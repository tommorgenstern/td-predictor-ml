{"custom_id": "keras#353cb10af7c0c7d435b31129c5970fcabe3fc961", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 103 | Lines Deleted: 114 | Files Changed: 9 | Hunks: 63 | Methods Changed: 26 | Complexity Δ (Sum/Max): -3/2 | Churn Δ: 217 | Churn Cumulative: 38732 | Contributors (this commit): 100 | Commits (past 90d): 78 | Contributors (cumulative): 151 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1606,7 +1606,8 @@ class BackendNNOpsTest(tf.test.TestCase, parameterized.TestCase):\n     # Test noise shape\n     outputs = backend.dropout(inputs, 0.2, noise_shape=(200, 1))\n     outputs_val = backend.eval(outputs)\n-    self.assertAllClose(outputs_val[2, :], outputs_val[3, :], atol=1e-5)\n+    # Make sure the whole column gets the same dropout\n+    self.assertEqual(np.min(outputs_val[0, :]), np.max(outputs_val[0, :]))\n \n \n class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n\n@@ -3414,6 +3414,42 @@ def _apply_name_scope_on_model_declaration(enable):\n   _is_name_scope_on_model_declaration_enabled = enable\n \n \n+class BaseRandomLayer(Layer):\n+  \"\"\"A layer handle the random nubmer creation and savemodel behavior.\"\"\"\n+\n+  @tf.__internal__.tracking.no_automatic_dependency_tracking\n+  def __init__(self, seed=None, force_generator=False, **kwargs):\n+    \"\"\"Initialize the BaseRandomLayer.\n+\n+    Note that the constructor is annotated with\n+    @no_automatic_dependency_tracking. This is to skip the auto\n+    tracking of self._random_generator instance, which is an AutoTrackable.\n+    The backend.RandomGenerator could contain a tf.random.Generator instance\n+    which will have tf.Variable as the internal state. We want to avoid saving\n+    that state into model.weights and checkpoints for backward compatibility\n+    reason. In the meantime, we still need to make them visible to SavedModel\n+    when it is tracing the tf.function for the `call()`.\n+    See _list_extra_dependencies_for_serialization below for more details.\n+\n+    Args:\n+      seed: optional integer, used to create RandomGenerator.\n+      force_generator: boolean, default to False, whether to force the\n+        RandomGenerator to use the code branch of tf.random.Generator.\n+      **kwargs: other keyward arguements that will be passed to the parent class\n+    \"\"\"\n+    super().__init__(**kwargs)\n+    self._random_generator = backend.RandomGenerator(\n+        seed, force_generator=force_generator)\n+\n+  def _list_extra_dependencies_for_serialization(self, serialization_cache):\n+    # This method exposes the self._random_generator to SavedModel only\n+    # (not layer.weights and checkpoint).\n+    deps = super()._list_extra_dependencies_for_serialization(\n+        serialization_cache)\n+    deps['_random_generator'] = self._random_generator\n+    return deps\n+\n+\n # Avoid breaking users who directly import this symbol from this file.\n # TODO(fchollet): remove this.\n InputSpec = input_spec.InputSpec  # pylint:disable=invalid-name\n\n@@ -14,24 +14,24 @@\n # ==============================================================================\n # pylint: disable=protected-access\n # pylint: disable=g-classes-have-attributes\n+# pylint: disable=g-direct-tensorflow-import\n \"\"\"Convolutional-recurrent layers.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n-import numpy as np\n-\n from keras import activations\n from keras import backend\n from keras import constraints\n from keras import initializers\n from keras import regularizers\n-from keras.engine.base_layer import Layer\n+from keras.engine import base_layer\n from keras.engine.input_spec import InputSpec\n from keras.layers.recurrent import DropoutRNNCellMixin\n from keras.layers.recurrent import RNN\n from keras.utils import conv_utils\n from keras.utils import generic_utils\n from keras.utils import tf_utils\n+\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n \n@@ -209,7 +209,7 @@ class ConvRNN(RNN):\n         shape=(batch_size, None) + input_shape[2:self.rank + 3])\n \n     # allow cell (if layer) to build before we set or validate state_spec\n-    if isinstance(self.cell, Layer):\n+    if isinstance(self.cell, base_layer.Layer):\n       step_input_shape = (input_shape[0],) + input_shape[2:]\n       if constants_shape is not None:\n         self.cell.build([step_input_shape] + constants_shape)\n@@ -397,7 +397,7 @@ class ConvRNN(RNN):\n         backend.set_value(state, value)\n \n \n-class ConvLSTMCell(DropoutRNNCellMixin, Layer):\n+class ConvLSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n   \"\"\"Cell class for the ConvLSTM layer.\n \n   Args:\n\n@@ -16,14 +16,14 @@\n # pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n \n from keras import backend\n-from keras.engine.base_layer import Layer\n+from keras.engine import base_layer\n from keras.utils import control_flow_util\n import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n \n @keras_export('keras.layers.Dropout')\n-class Dropout(Layer):\n+class Dropout(base_layer.BaseRandomLayer):\n   \"\"\"Applies Dropout to the input.\n \n   The Dropout layer randomly sets input units to 0 with a frequency of `rate`\n@@ -74,7 +74,6 @@ class Dropout(Layer):\n       training mode (adding dropout) or in inference mode (doing nothing).\n   \"\"\"\n \n-  @tf.__internal__.tracking.no_automatic_dependency_tracking\n   def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n     # Note that the constructor is annotated with\n     # @no_automatic_dependency_tracking. This is to skip the auto\n@@ -85,7 +84,7 @@ class Dropout(Layer):\n     # reason. In the meantime, we still need to make them visible to SavedModel\n     # when it is tracing the tf.function for the `call()`.\n     # See _list_extra_dependencies_for_serialization below for more details.\n-    super(Dropout, self).__init__(**kwargs)\n+    super(Dropout, self).__init__(seed=seed, **kwargs)\n     if isinstance(rate, (int, float)) and not 0 <= rate <= 1:\n       raise ValueError(f'Invalid value {rate} received for '\n                        f'`rate`, expected a value between 0 and 1.')\n@@ -93,7 +92,6 @@ class Dropout(Layer):\n     self.noise_shape = noise_shape\n     self.seed = seed\n     self.supports_masking = True\n-    self._random_generator = backend.RandomGenerator(seed)\n \n   def build(self, input_shape):\n     self._random_generator._maybe_init()  # pylint: disable=protected-access\n@@ -134,11 +132,3 @@ class Dropout(Layer):\n     }\n     base_config = super(Dropout, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n-\n-  def _list_extra_dependencies_for_serialization(self, serialization_cache):\n-    # This method exposes the self._random_generator to SavedModel only\n-    # (not layer.weights and checkpoint).\n-    deps = super()._list_extra_dependencies_for_serialization(\n-        serialization_cache)\n-    deps['_random_generator'] = self._random_generator\n-    return deps\n\n@@ -17,15 +17,16 @@\n This file follows the terminology of https://arxiv.org/abs/1706.03762 Figure 2.\n Attention is formed by three tensors: Query, Key and Value.\n \"\"\"\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n \n-import tensorflow.compat.v2 as tf\n from keras import backend\n-from keras.engine.base_layer import Layer\n+from keras.engine import base_layer\n from keras.utils import control_flow_util\n+import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n \n-class BaseDenseAttention(Layer):\n+class BaseDenseAttention(base_layer.BaseRandomLayer):\n   \"\"\"Base Attention class for Dense networks.\n \n   This class is suitable for Dense or CNN networks, and not for RNN networks.\n@@ -67,8 +68,7 @@ class BaseDenseAttention(Layer):\n       `[batch_size, Tq, Tv]`.\n   \"\"\"\n \n-  def __init__(self, causal=False, dropout=0.0,\n-               **kwargs):\n+  def __init__(self, causal=False, dropout=0.0, **kwargs):\n     super(BaseDenseAttention, self).__init__(**kwargs)\n     self.causal = causal\n     self.dropout = dropout\n@@ -126,7 +126,7 @@ class BaseDenseAttention(Layer):\n     weights = tf.nn.softmax(scores)\n \n     def dropped_weights():\n-      return tf.nn.dropout(weights, rate=self.dropout)\n+      return self._random_generator.dropout(weights, rate=self.dropout)\n \n     weights = control_flow_util.smart_cond(training, dropped_weights,\n                                            lambda: tf.identity(weights))\n\n@@ -25,34 +25,8 @@ import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n \n-class BaseRandomLayer(base_layer.Layer):\n-  \"\"\"A layer handle the random nubmer creation and savemodel behavior.\"\"\"\n-\n-  @tf.__internal__.tracking.no_automatic_dependency_tracking\n-  def __init__(self, seed=None, **kwargs):\n-    # Note that the constructor is annotated with\n-    # @no_automatic_dependency_tracking. This is to skip the auto\n-    # tracking of self._random_generator instance, which is an AutoTrackable.\n-    # The backend.RandomGenerator could contain a tf.random.Generator instance\n-    # which will have tf.Variable as the internal state. We want to avoid saving\n-    # that state into model.weights and checkpoints for backward compatibility\n-    # reason. In the meantime, we still need to make them visible to SavedModel\n-    # when it is tracing the tf.function for the `call()`.\n-    # See _list_extra_dependencies_for_serialization below for more details.\n-    super().__init__(**kwargs)\n-    self._random_generator = backend.RandomGenerator(seed)\n-\n-  def _list_extra_dependencies_for_serialization(self, serialization_cache):\n-    # This method exposes the self._random_generator to SavedModel\n-    # only (not layer.weights and checkpoint).\n-    deps = super()._list_extra_dependencies_for_serialization(\n-        serialization_cache)\n-    deps['_random_generator'] = self._random_generator\n-    return deps\n-\n-\n @keras_export('keras.layers.GaussianNoise')\n-class GaussianNoise(BaseRandomLayer):\n+class GaussianNoise(base_layer.BaseRandomLayer):\n   \"\"\"Apply additive zero-centered Gaussian noise.\n \n   This is useful to mitigate overfitting\n@@ -108,7 +82,7 @@ class GaussianNoise(BaseRandomLayer):\n \n \n @keras_export('keras.layers.GaussianDropout')\n-class GaussianDropout(BaseRandomLayer):\n+class GaussianDropout(base_layer.BaseRandomLayer):\n   \"\"\"Apply multiplicative 1-centered Gaussian noise.\n \n   As it is a regularization layer, it is only active at training time.\n@@ -164,7 +138,7 @@ class GaussianDropout(BaseRandomLayer):\n \n \n @keras_export('keras.layers.AlphaDropout')\n-class AlphaDropout(BaseRandomLayer):\n+class AlphaDropout(base_layer.BaseRandomLayer):\n   \"\"\"Applies Alpha Dropout to the input.\n \n   Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\n\n@@ -189,35 +189,9 @@ class CenterCrop(base_layer.Layer):\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n-class BaseRandomLayer(base_layer.Layer):\n-  \"\"\"A layer handle the random nubmer creation and savemodel behavior.\"\"\"\n-\n-  @tf.__internal__.tracking.no_automatic_dependency_tracking\n-  def __init__(self, seed=None, **kwargs):\n-    # Note that the constructor is annotated with\n-    # @no_automatic_dependency_tracking. This is to skip the auto\n-    # tracking of self._random_generator instance, which is an AutoTrackable.\n-    # The backend.RandomGenerator could contain a tf.random.Generator instance\n-    # which will have tf.Variable as the internal state. We want to avoid saving\n-    # that state into model.weights and checkpoints for backward compatibility\n-    # reason. In the meantime, we still need to make them visible to SavedModel\n-    # when it is tracing the tf.function for the `call()`.\n-    # See _list_extra_dependencies_for_serialization below for more details.\n-    super().__init__(**kwargs)\n-    self._random_generator = backend.RandomGenerator(seed, force_generator=True)\n-\n-  def _list_extra_dependencies_for_serialization(self, serialization_cache):\n-    # This method exposes the self._random_generator to SavedModel only\n-    # (not layer.weights and checkpoint).\n-    deps = super()._list_extra_dependencies_for_serialization(\n-        serialization_cache)\n-    deps['_random_generator'] = self._random_generator\n-    return deps\n-\n-\n @keras_export('keras.layers.RandomCrop',\n               'keras.layers.experimental.preprocessing.RandomCrop')\n-class RandomCrop(BaseRandomLayer):\n+class RandomCrop(base_layer.BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly crops images during training.\n \n   During training, this layer will randomly choose a location to crop images\n@@ -249,7 +223,8 @@ class RandomCrop(BaseRandomLayer):\n \n   def __init__(self, height, width, seed=None, **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomCrop').set(True)\n-    super(RandomCrop, self).__init__(**kwargs, autocast=False, seed=seed)\n+    super(RandomCrop, self).__init__(**kwargs, autocast=False, seed=seed,\n+                                     force_generator=True)\n     self.height = height\n     self.width = width\n     self.seed = seed\n@@ -354,7 +329,7 @@ HORIZONTAL_AND_VERTICAL = 'horizontal_and_vertical'\n \n @keras_export('keras.layers.RandomFlip',\n               'keras.layers.experimental.preprocessing.RandomFlip')\n-class RandomFlip(BaseRandomLayer):\n+class RandomFlip(base_layer.BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly flips images during training.\n \n   This layer will flip the images horizontally and or vertically based on the\n@@ -384,7 +359,7 @@ class RandomFlip(BaseRandomLayer):\n                mode=HORIZONTAL_AND_VERTICAL,\n                seed=None,\n                **kwargs):\n-    super(RandomFlip, self).__init__(seed=seed, **kwargs)\n+    super(RandomFlip, self).__init__(seed=seed, force_generator=True, **kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomFlip').set(True)\n     self.mode = mode\n     if mode == HORIZONTAL:\n@@ -445,7 +420,7 @@ class RandomFlip(BaseRandomLayer):\n # TODO(tanzheny): Add examples, here and everywhere.\n @keras_export('keras.layers.RandomTranslation',\n               'keras.layers.experimental.preprocessing.RandomTranslation')\n-class RandomTranslation(BaseRandomLayer):\n+class RandomTranslation(base_layer.BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly translates images during training.\n \n   This layer will apply random translations to each image during training,\n@@ -507,7 +482,8 @@ class RandomTranslation(BaseRandomLayer):\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomTranslation').set(\n         True)\n-    super(RandomTranslation, self).__init__(seed=seed, **kwargs)\n+    super(RandomTranslation, self).__init__(seed=seed, force_generator=True,\n+                                            **kwargs)\n     self.height_factor = height_factor\n     if isinstance(height_factor, (tuple, list)):\n       self.height_lower = height_factor[0]\n@@ -771,7 +747,7 @@ def get_rotation_matrix(angles, image_height, image_width, name=None):\n \n @keras_export('keras.layers.RandomRotation',\n               'keras.layers.experimental.preprocessing.RandomRotation')\n-class RandomRotation(BaseRandomLayer):\n+class RandomRotation(base_layer.BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly rotates images during training.\n \n   This layer will apply random rotations to each image, filling empty space\n@@ -827,7 +803,8 @@ class RandomRotation(BaseRandomLayer):\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomRotation').set(\n         True)\n-    super(RandomRotation, self).__init__(seed=seed, **kwargs)\n+    super(RandomRotation, self).__init__(seed=seed, force_generator=True,\n+                                         **kwargs)\n     self.factor = factor\n     if isinstance(factor, (tuple, list)):\n       self.lower = factor[0]\n@@ -897,7 +874,7 @@ class RandomRotation(BaseRandomLayer):\n \n @keras_export('keras.layers.RandomZoom',\n               'keras.layers.experimental.preprocessing.RandomZoom')\n-class RandomZoom(BaseRandomLayer):\n+class RandomZoom(base_layer.BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly zooms images during training.\n \n   This layer will randomly zoom in or out on each axis of an image\n@@ -964,7 +941,7 @@ class RandomZoom(BaseRandomLayer):\n                fill_value=0.0,\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomZoom').set(True)\n-    super(RandomZoom, self).__init__(seed=seed, **kwargs)\n+    super(RandomZoom, self).__init__(seed=seed, force_generator=True, **kwargs)\n     self.height_factor = height_factor\n     if isinstance(height_factor, (tuple, list)):\n       self.height_lower = height_factor[0]\n@@ -1103,7 +1080,7 @@ def get_zoom_matrix(zooms, image_height, image_width, name=None):\n \n @keras_export('keras.layers.RandomContrast',\n               'keras.layers.experimental.preprocessing.RandomContrast')\n-class RandomContrast(BaseRandomLayer):\n+class RandomContrast(base_layer.BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly adjusts contrast during training.\n \n   This layer will randomly adjust the contrast of an image or images by a random\n@@ -1136,7 +1113,8 @@ class RandomContrast(BaseRandomLayer):\n   def __init__(self, factor, seed=None, **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomContrast').set(\n         True)\n-    super(RandomContrast, self).__init__(seed=seed, **kwargs)\n+    super(RandomContrast, self).__init__(seed=seed, force_generator=True,\n+                                         **kwargs)\n     self.factor = factor\n     if isinstance(factor, (tuple, list)):\n       self.lower = factor[0]\n@@ -1181,7 +1159,7 @@ class RandomContrast(BaseRandomLayer):\n \n @keras_export('keras.layers.RandomHeight',\n               'keras.layers.experimental.preprocessing.RandomHeight')\n-class RandomHeight(BaseRandomLayer):\n+class RandomHeight(base_layer.BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly varies image height during training.\n \n   This layer adjusts the height of a batch of images by a random factor.\n@@ -1222,7 +1200,8 @@ class RandomHeight(BaseRandomLayer):\n                seed=None,\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomHeight').set(True)\n-    super(RandomHeight, self).__init__(seed=seed, **kwargs)\n+    super(RandomHeight, self).__init__(seed=seed, force_generator=True,\n+                                       **kwargs)\n     self.factor = factor\n     if isinstance(factor, (tuple, list)):\n       self.height_lower = factor[0]\n@@ -1283,7 +1262,7 @@ class RandomHeight(BaseRandomLayer):\n \n @keras_export('keras.layers.RandomWidth',\n               'keras.layers.experimental.preprocessing.RandomWidth')\n-class RandomWidth(BaseRandomLayer):\n+class RandomWidth(base_layer.BaseRandomLayer):\n   \"\"\"A preprocessing layer which randomly varies image width during training.\n \n   This layer will randomly adjusts the width of a batch of images of a\n@@ -1324,7 +1303,7 @@ class RandomWidth(BaseRandomLayer):\n                seed=None,\n                **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomWidth').set(True)\n-    super(RandomWidth, self).__init__(seed=seed, **kwargs)\n+    super(RandomWidth, self).__init__(seed=seed, force_generator=True, **kwargs)\n     self.factor = factor\n     if isinstance(factor, (tuple, list)):\n       self.width_lower = factor[0]\n\n@@ -28,7 +28,7 @@ from keras import backend\n from keras import constraints\n from keras import initializers\n from keras import regularizers\n-from keras.engine.base_layer import Layer\n+from keras.engine import base_layer\n from keras.engine.input_spec import InputSpec\n from keras.saving.saved_model import layer_serialization\n from keras.utils import control_flow_util\n@@ -45,7 +45,7 @@ RECURRENT_DROPOUT_WARNING_MSG = (\n \n \n @keras_export('keras.layers.StackedRNNCells')\n-class StackedRNNCells(Layer):\n+class StackedRNNCells(base_layer.Layer):\n   \"\"\"Wrapper allowing a stack of RNN cells to behave as a single cell.\n \n   Used to implement efficient stacked RNNs.\n@@ -158,7 +158,7 @@ class StackedRNNCells(Layer):\n       return tuple([batch_size] + shape)\n \n     for cell in self.cells:\n-      if isinstance(cell, Layer) and not cell.built:\n+      if isinstance(cell, base_layer.Layer) and not cell.built:\n         with backend.name_scope(cell.name):\n           cell.build(input_shape)\n           cell.built = True\n@@ -196,7 +196,7 @@ class StackedRNNCells(Layer):\n \n \n @keras_export('keras.layers.RNN')\n-class RNN(Layer):\n+class RNN(base_layer.Layer):\n   \"\"\"Base class for recurrent layers.\n \n   See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n@@ -587,7 +587,7 @@ class RNN(Layer):\n       step_input_shape = tf.nest.map_structure(get_step_input_shape, input_shape)\n \n     # allow cell (if layer) to build before we set or validate state_spec.\n-    if isinstance(self.cell, Layer) and not self.cell.built:\n+    if isinstance(self.cell, base_layer.Layer) and not self.cell.built:\n       with backend.name_scope(self.cell.name):\n         self.cell.build(step_input_shape)\n         self.cell.built = True\n@@ -1025,7 +1025,7 @@ class RNN(Layer):\n \n \n @keras_export('keras.layers.AbstractRNNCell')\n-class AbstractRNNCell(Layer):\n+class AbstractRNNCell(base_layer.Layer):\n   \"\"\"Abstract object representing an RNN cell.\n \n   See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n@@ -1126,6 +1126,8 @@ class DropoutRNNCellMixin:\n       tensor need to dropout.\n     recurrent_dropout: a float number within range [0, 1). The ratio that the\n       recurrent state weights need to dropout.\n+    _random_generator: A backend.RandomGenerator instance, which will be used\n+      to produce outputs based on the inputs and dropout rate.\n   This object will create and cache created dropout masks, and reuse them for\n   the incoming data, so that the same mask is used for every batch input.\n   \"\"\"\n@@ -1181,6 +1183,7 @@ class DropoutRNNCellMixin:\n \n   def _create_dropout_mask(self, inputs, training, count=1):\n     return _generate_dropout_mask(\n+        self._random_generator,\n         tf.ones_like(inputs),\n         self.dropout,\n         training=training,\n@@ -1188,6 +1191,7 @@ class DropoutRNNCellMixin:\n \n   def _create_recurrent_dropout_mask(self, inputs, training, count=1):\n     return _generate_dropout_mask(\n+        self._random_generator,\n         tf.ones_like(inputs),\n         self.recurrent_dropout,\n         training=training,\n@@ -1252,7 +1256,7 @@ class DropoutRNNCellMixin:\n \n \n @keras_export('keras.layers.SimpleRNNCell')\n-class SimpleRNNCell(DropoutRNNCellMixin, Layer):\n+class SimpleRNNCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n   \"\"\"Cell class for SimpleRNN.\n \n   See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n@@ -1711,7 +1715,7 @@ class SimpleRNN(RNN):\n \n \n @keras_export(v1=['keras.layers.GRUCell'])\n-class GRUCell(DropoutRNNCellMixin, Layer):\n+class GRUCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n   \"\"\"Cell class for the GRU layer.\n \n   Args:\n@@ -2271,7 +2275,7 @@ class GRU(RNN):\n \n \n @keras_export(v1=['keras.layers.LSTMCell'])\n-class LSTMCell(DropoutRNNCellMixin, Layer):\n+class LSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n   \"\"\"Cell class for the LSTM layer.\n \n   Args:\n@@ -2945,9 +2949,9 @@ class LSTM(RNN):\n     return cls(**config)\n \n \n-def _generate_dropout_mask(ones, rate, training=None, count=1):\n+def _generate_dropout_mask(generator, ones, rate, training=None, count=1):\n   def dropped_inputs():\n-    return backend.dropout(ones, rate)\n+    return generator.dropout(ones, rate)\n \n   if count > 1:\n     return [\n\n@@ -12,17 +12,20 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-# pylint: disable=g-classes-have-attributes\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n \"\"\"Recurrent layers for TF 2.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n \n import uuid\n-from tensorflow.python.eager.context import get_device_name\n+\n from keras import activations\n from keras import backend\n+from keras.engine import base_layer\n from keras.engine.input_spec import InputSpec\n from keras.layers import recurrent\n+\n+import tensorflow.compat.v2 as tf\n+from tensorflow.python.eager.context import get_device_name\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n@@ -201,7 +204,8 @@ class GRUCell(recurrent.GRUCell):\n \n \n @keras_export('keras.layers.GRU', v1=[])\n-class GRU(recurrent.DropoutRNNCellMixin, recurrent.GRU):\n+class GRU(recurrent.DropoutRNNCellMixin,\n+          recurrent.GRU, base_layer.BaseRandomLayer):\n   \"\"\"Gated Recurrent Unit - Cho et al. 2014.\n \n   See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n@@ -941,7 +945,8 @@ class LSTMCell(recurrent.LSTMCell):\n \n \n @keras_export('keras.layers.LSTM', v1=[])\n-class LSTM(recurrent.DropoutRNNCellMixin, recurrent.LSTM):\n+class LSTM(recurrent.DropoutRNNCellMixin,\n+           recurrent.LSTM, base_layer.BaseRandomLayer):\n   \"\"\"Long Short-Term Memory layer - Hochreiter 1997.\n \n   See [the Keras RNN API guide](https://www.tensorflow.org/guide/keras/rnn)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3925b94be9763628114ffd74b2099c20a5957bee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 4 | Churn Cumulative: 19318 | Contributors (this commit): 83 | Commits (past 90d): 25 | Contributors (cumulative): 83 | DMM Complexity: None\n\nDIFF:\n@@ -194,7 +194,9 @@ class Conv(Layer):\n     kernel_shape = self.kernel_size + (input_channel // self.groups,\n                                        self.filters)\n \n-    output_shape = self.compute_output_shape(input_shape)\n+    # compute_output_shape contains some validation logic for the input shape,\n+    # and make sure the output shape has all positive dimentions.\n+    self.compute_output_shape(input_shape)\n \n     self.kernel = self.add_weight(\n         name='kernel',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e988b59d03bbb6ce358f42749f9498624479feb7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 68 | Lines Deleted: 17 | Files Changed: 1 | Hunks: 15 | Methods Changed: 8 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 85 | Churn Cumulative: 16139 | Contributors (this commit): 114 | Commits (past 90d): 37 | Contributors (cumulative): 114 | DMM Complexity: 0.2222222222222222\n\nDIFF:\n@@ -870,13 +870,44 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       write_scalar_summaries(outputs, step=model._train_counter)  # pylint: disable=protected-access\n       return outputs\n \n+    # Special case if steps_per_execution is one.\n     if (self._steps_per_execution is None or\n         self._steps_per_execution.numpy().item() == 1):\n \n       def train_function(iterator):\n-        \"\"\"Runs a training execution with one step.\"\"\"\n+        \"\"\"Runs a training execution with a single step.\"\"\"\n         return step_function(self, iterator)\n \n+      if not self.run_eagerly:\n+        train_function = tf.function(\n+            train_function, experimental_relax_shapes=True)\n+        self.train_tf_function = train_function\n+\n+      if self._cluster_coordinator:\n+        self.train_function = lambda it: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda\n+            train_function, args=(it,))\n+      else:\n+        self.train_function = train_function\n+\n+    # If we're using a coordinator, use the value of self._steps_per_execution\n+    # at the time the function is called/scheduled, and not when it is actually\n+    # executed.\n+    elif self._cluster_coordinator:\n+\n+      def train_function(iterator, steps_per_execution):\n+        \"\"\"Runs a training execution with multiple steps.\"\"\"\n+        for _ in tf.range(steps_per_execution):\n+          outputs = step_function(self, iterator)\n+        return outputs\n+\n+      if not self.run_eagerly:\n+        train_function = tf.function(\n+            train_function, experimental_relax_shapes=True)\n+        self.train_tf_function = train_function\n+\n+      self.train_function = lambda it: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda\n+          train_function,\n+          args=(it, self._steps_per_execution.value()))\n     else:\n \n       def train_function(iterator):\n@@ -889,13 +920,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         train_function = tf.function(\n             train_function, experimental_relax_shapes=True)\n         self.train_tf_function = train_function\n-\n       self.train_function = train_function\n \n-    if self._cluster_coordinator:\n-      self.train_function = lambda iterator: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda\n-          train_function, args=(iterator,))\n-\n     return self.train_function\n \n   @traceback_utils.filter_traceback\n@@ -1358,18 +1384,32 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           outputs, self.distribute_strategy, reduction='first')\n       return outputs\n \n+    # Special case if steps_per_execution is one.\n     if (self._steps_per_execution is None or\n         self._steps_per_execution.numpy().item() == 1):\n \n       def test_function(iterator):\n-        \"\"\"Runs an evaluation execution with one step.\"\"\"\n+        \"\"\"Runs a test execution with a single step.\"\"\"\n         return step_function(self, iterator)\n \n-    else:\n+      if not self.run_eagerly:\n+        test_function = tf.function(\n+            test_function, experimental_relax_shapes=True)\n \n-      def test_function(iterator):\n-        \"\"\"Runs an evaluation execution with multiple steps.\"\"\"\n-        for _ in tf.range(self._steps_per_execution):\n+      if self._cluster_coordinator:\n+        self.test_function = lambda it: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda\n+            test_function, args=(it,))\n+      else:\n+        self.test_function = test_function\n+\n+    # If we're using a coordinator, use the value of self._steps_per_execution\n+    # at the time the function is called/scheduled, and not when it is actually\n+    # executed.\n+    elif self._cluster_coordinator:\n+\n+      def test_function(iterator, steps_per_execution):\n+        \"\"\"Runs a test execution with multiple steps.\"\"\"\n+        for _ in tf.range(steps_per_execution):\n           outputs = step_function(self, iterator)\n         return outputs\n \n@@ -1377,11 +1417,21 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         test_function = tf.function(\n             test_function, experimental_relax_shapes=True)\n \n-    self.test_function = test_function\n+      self.test_function = lambda it: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda\n+          test_function,\n+          args=(it, self._steps_per_execution.value()))\n+    else:\n \n-    if self._cluster_coordinator:\n-      self.test_function = lambda iterator: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda\n-          test_function, args=(iterator,))\n+      def test_function(iterator):\n+        \"\"\"Runs a test execution with multiple steps.\"\"\"\n+        for _ in tf.range(self._steps_per_execution):\n+          outputs = step_function(self, iterator)\n+        return outputs\n+\n+      if not self.run_eagerly:\n+        test_function = tf.function(\n+            test_function, experimental_relax_shapes=True)\n+      self.test_function = test_function\n \n     return self.test_function\n \n@@ -1613,11 +1663,12 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           outputs, self.distribute_strategy, reduction='concat')\n       return outputs\n \n+    # Special case if steps_per_execution is one.\n     if (self._steps_per_execution is None or\n         self._steps_per_execution.numpy().item() == 1):\n \n       def predict_function(iterator):\n-        \"\"\"Runs an evaluation execution with one step.\"\"\"\n+        \"\"\"Runs an evaluation execution with a single step.\"\"\"\n         return step_function(self, iterator)\n \n     else:\n@@ -1638,8 +1689,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     if not self.run_eagerly:\n       predict_function = tf.function(\n           predict_function, experimental_relax_shapes=True)\n-\n     self.predict_function = predict_function\n+\n     return self.predict_function\n \n   @traceback_utils.filter_traceback\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4c6eedc59aa5ba4481149b224a42245b9f102efb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 225 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -19,6 +19,7 @@ import tensorflow.compat.v2 as tf\n import numpy as np\n \n import keras\n+from keras import backend\n from keras import keras_parameterized\n from keras.distribute.strategy_combinations import all_strategies\n from keras.layers.preprocessing import category_crossing\n@@ -30,8 +31,7 @@ def batch_wrapper(dataset, batch_size, distribution, repeat=None):\n     dataset = dataset.repeat(repeat)\n   # TPUs currently require fully defined input shapes, drop_remainder ensures\n   # the input will have fully defined shapes.\n-  if isinstance(distribution,\n-                (tf.distribute.experimental.TPUStrategy, tf.compat.v1.distribute.experimental.TPUStrategy)):\n+  if backend.is_tpu_strategy(distribution):\n     return dataset.batch(batch_size, drop_remainder=True)\n   else:\n     return dataset.batch(batch_size)\n\n@@ -32,8 +32,7 @@ def batch_wrapper(dataset, batch_size, strategy, repeat=None):\n     dataset = dataset.repeat(repeat)\n   # TPUs currently require fully defined input shapes, drop_remainder ensures\n   # the input will have fully defined shapes.\n-  if isinstance(strategy, (tf.distribute.experimental.TPUStrategy,\n-                           tf.compat.v1.distribute.experimental.TPUStrategy)):\n+  if backend.is_tpu_strategy(strategy):\n     return dataset.batch(batch_size, drop_remainder=True)\n   else:\n     return dataset.batch(batch_size)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1ae6573b644c35748ce87c6160664920d9ca9d19", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 16159 | Contributors (this commit): 115 | Commits (past 90d): 38 | Contributors (cumulative): 115 | DMM Complexity: None\n\nDIFF:\n@@ -1705,14 +1705,24 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n               use_multiprocessing=False):\n     \"\"\"Generates output predictions for the input samples.\n \n-    Computation is done in batches. This method is designed for performance in\n-    large scale inputs. For small amount of inputs that fit in one batch,\n-    directly using `__call__()` is recommended for faster execution, e.g.,\n+    Computation is done in batches. This method is designed for batch processing\n+    of large numbers of inputs. It is not intended for use inside of loops\n+    that iterate over your data and process small numbers of inputs at a time.\n+\n+    For small numbers of inputs that fit in one batch,\n+    directly use `__call__()` for faster execution, e.g.,\n     `model(x)`, or `model(x, training=False)` if you have layers such as\n-    `tf.keras.layers.BatchNormalization` that behaves differently during\n-    inference. Also, note the fact that test loss is not affected by\n+    `tf.keras.layers.BatchNormalization` that behave differently during\n+    inference. You may pair the individual model call with a `tf.function`\n+    for additional performance inside your inner loop.\n+    If you need access to numpy array values instead of tensors after your\n+    model call, you can use `tensor.numpy()` to get the numpy array value of\n+    an eager tensor.\n+\n+    Also, note the fact that test loss is not affected by\n     regularization layers like noise and dropout.\n \n+\n     Args:\n         x: Input samples. It could be:\n           - A Numpy array (or array-like), or a list of arrays\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8172c5a33589aff6b1e4aff5055773a25de6a333", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 252 | Lines Deleted: 156 | Files Changed: 9 | Hunks: 157 | Methods Changed: 113 | Complexity Δ (Sum/Max): 15/7 | Churn Δ: 408 | Churn Cumulative: 12688 | Contributors (this commit): 7 | Commits (past 90d): 45 | Contributors (cumulative): 35 | DMM Complexity: 0.2923076923076923\n\nDIFF:\n@@ -17,6 +17,7 @@\n # pylint: disable=g-classes-have-attributes\n # pylint: disable=g-direct-tensorflow-import\n \n+from keras import backend\n from keras.engine import base_layer\n from keras.engine import base_preprocessing_layer\n from keras.layers.preprocessing import preprocessing_utils as utils\n@@ -125,6 +126,11 @@ class CategoryEncoding(base_layer.Layer):\n       num_tokens = kwargs[\"max_tokens\"]\n       del kwargs[\"max_tokens\"]\n \n+    # By default, output floats. This is already default for TF2, but in TF1\n+    # dtype is inferred from inputs, and would default to int.\n+    if \"dtype\" not in kwargs:\n+      kwargs[\"dtype\"] = backend.floatx()\n+\n     super(CategoryEncoding, self).__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell(\"CategoryEncoding\").set(\n         True)\n@@ -204,5 +210,6 @@ class CategoryEncoding(base_layer.Layer):\n           inputs,\n           output_mode=self.output_mode,\n           depth=depth,\n+          dtype=self.compute_dtype,\n           sparse=self.sparse,\n           count_weights=count_weights)\n\n@@ -301,6 +301,19 @@ class CategoryEncodingOutputTest(keras_parameterized.TestCase,\n                                  preprocessing_test_utils.PreprocessingLayerTest\n                                 ):\n \n+  @parameterized.named_parameters(\n+      (\"float32\", tf.float32),\n+      (\"float64\", tf.float64),\n+  )\n+  def test_output_dtype(self, dtype):\n+    inputs = keras.Input(shape=(1,), dtype=tf.int32)\n+    layer = category_encoding.CategoryEncoding(\n+        num_tokens=4,\n+        output_mode=category_encoding.ONE_HOT,\n+        dtype=dtype)\n+    outputs = layer(inputs)\n+    self.assertAllEqual(outputs.dtype, dtype)\n+\n   def test_one_hot_output(self):\n     input_data = np.array([[3], [2], [0], [1]])\n     expected_output = [\n\n@@ -73,7 +73,7 @@ class VocabWeightHandler(base_layer_utils.TrackableWeightHandler):\n \n   def __init__(self, lookup_layer):\n     self._layer = lookup_layer\n-    self._dtype = lookup_layer.dtype\n+    self._dtype = lookup_layer.vocabulary_dtype\n     self._distribute_strategy = tf.distribute.get_strategy()\n \n   @property\n@@ -118,6 +118,8 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n       containing the vocbulary terms. If passing a file path, the file should\n       contain one line per term in the vocabulary. If this argument is set,\n       there is no need to `adapt` the layer.\n+    vocabulary_dtype: The dtype of the vocabulary terms. For example, `\"int64\"`\n+      or `\"string\"`.\n     idf_weights: Only valid when `output_mode` is `\"tf_idf\"`. A tuple, list, 1D\n       numpy array, or 1D tensor or the same length as the vocabulary, containing\n       the floating point inverse document frequency weights, which will be\n@@ -160,6 +162,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n                num_oov_indices,\n                mask_token,\n                oov_token,\n+               vocabulary_dtype,\n                vocabulary=None,\n                idf_weights=None,\n                invert=False,\n@@ -216,6 +219,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n     self.output_mode = output_mode\n     self.sparse = sparse\n     self.pad_to_max_tokens = pad_to_max_tokens\n+    self.vocabulary_dtype = vocabulary_dtype\n     self._frozen_vocab_size = None\n \n     self.input_vocabulary = vocabulary\n@@ -230,21 +234,31 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n     kwargs.pop(\"vocabulary_size\", None)\n     kwargs.pop(\"has_static_table\", None)\n \n+    # By default, output int64 when output_mode='int' and floats otherwise.\n+    if \"dtype\" not in kwargs:\n+      kwargs[\"dtype\"] = tf.int64 if output_mode == INT else backend.floatx()\n+\n     super().__init__(**kwargs)\n \n+    # Check dtype only after base layer parses it; dtype parsing is complex.\n+    if output_mode == INT and not tf.as_dtype(self.compute_dtype).is_integer:\n+      input_dtype = kwargs[\"dtype\"]\n+      raise ValueError(\"When `output_mode='int'`, `dtype` should be an integer \"\n+                       f\"type. Received: dtype={input_dtype}\")\n+\n     if invert:\n-      self._key_dtype = tf.int64\n-      self._value_dtype = tf.as_dtype(self.dtype)\n+      self._key_dtype = self.dtype if output_mode == INT else tf.int64\n+      self._value_dtype = tf.as_dtype(self.vocabulary_dtype)\n       mask_key = 0\n       mask_value = mask_token\n       self._default_value = self.oov_token\n     else:\n-      self._key_dtype = tf.as_dtype(self.dtype)\n-      self._value_dtype = tf.int64\n+      self._key_dtype = tf.as_dtype(self.vocabulary_dtype)\n+      self._value_dtype = self.dtype if output_mode == INT else tf.int64\n       mask_key = mask_token\n       # Masks should map to 0 for int output and be dropped otherwise. Max ints\n       # will be dropped from the bincount op.\n-      mask_value = 0 if self.output_mode == INT else tf.int64.max\n+      mask_value = 0 if self.output_mode == INT else self._value_dtype.max\n       if self.num_oov_indices == 0:\n         # If there are no OOV indices, we map OOV tokens to -1 and error out\n         # during call if we find a negative index.\n@@ -267,7 +281,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n       self.idf_weights = tf.Variable(\n           [0] * self._token_start_index(),\n           shape=(None,),\n-          dtype=backend.floatx(),\n+          dtype=self.compute_dtype,\n           trainable=False)\n       self.idf_weights_const = self.idf_weights.value()\n \n@@ -285,10 +299,10 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n       self._add_trackable(VocabWeightHandler(self), False)\n       # Set adapt state.\n       self.token_counts = tf.lookup.experimental.MutableHashTable(\n-          key_dtype=self.dtype, value_dtype=tf.int64, default_value=0)\n+          key_dtype=vocabulary_dtype, value_dtype=tf.int64, default_value=0)\n       if self.output_mode == TF_IDF:\n         self.token_document_counts = tf.lookup.experimental.MutableHashTable(\n-            key_dtype=self.dtype, value_dtype=tf.int64, default_value=0)\n+            key_dtype=vocabulary_dtype, value_dtype=tf.int64, default_value=0)\n         self.num_documents = tf.Variable(0, dtype=tf.int64, trainable=False)\n \n   def compute_output_shape(self, input_shape):\n@@ -300,8 +314,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n \n   def compute_output_signature(self, input_spec):\n     output_shape = self.compute_output_shape(input_spec.shape.as_list())\n-    output_dtype = (\n-        self._value_dtype if self.output_mode == INT else backend.floatx())\n+    output_dtype = self.vocabulary_dtype if self.invert else self.compute_dtype\n     return tf.TensorSpec(shape=output_shape, dtype=output_dtype)\n \n   def get_vocabulary(self, include_special_tokens=True):\n@@ -352,6 +365,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n         \"output_mode\": self.output_mode,\n         \"pad_to_max_tokens\": self.pad_to_max_tokens,\n         \"vocabulary\": utils.listify_tensors(self.input_vocabulary),\n+        \"vocabulary_dtype\": self.vocabulary_dtype,\n         \"idf_weights\": utils.listify_tensors(self.input_idf_weights),\n     }\n \n@@ -513,7 +527,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n           \"Cannot adapt {} layer after setting a static vocabulary via init \"\n           \"argument or `set_vocabulary`.\".format(self.__class__.__name__))\n \n-    data = self._standardize_inputs(data, self.dtype)\n+    data = self._standardize_inputs(data, self.vocabulary_dtype)\n     if data.shape.rank == 0:\n       data = tf.expand_dims(data, -1)\n     if data.shape.rank == 1:\n@@ -545,10 +559,10 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n     # Remove special tokens from our counts.\n     if self.mask_token is not None:\n       self.token_counts.remove(\n-          tf.convert_to_tensor([self.mask_token], self.dtype))\n+          tf.convert_to_tensor([self.mask_token], self.vocabulary_dtype))\n     if self.oov_token is not None:\n       self.token_counts.remove(\n-          tf.convert_to_tensor([self.oov_token], self.dtype))\n+          tf.convert_to_tensor([self.oov_token], self.vocabulary_dtype))\n \n     tokens, counts = self.token_counts.export()\n     # To keep vocabs deterministic, we sort our tokens by count and break ties\n@@ -619,6 +633,7 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n         lookups,\n         output_mode=self.output_mode,\n         depth=depth,\n+        dtype=self.compute_dtype,\n         sparse=self.sparse,\n         idf_weights=idf_weights)\n \n@@ -675,7 +690,8 @@ class IndexLookup(base_preprocessing_layer.PreprocessingLayer):\n     with tf.init_scope():\n       token_start = self._token_start_index()\n       token_end = token_start + tf.size(tokens)\n-      indices = tf.range(token_start, token_end, dtype=tf.int64)\n+      indices_dtype = self._key_dtype if self.invert else self._value_dtype\n+      indices = tf.range(token_start, token_end, dtype=indices_dtype)\n       keys, values = (indices, tokens) if self.invert else (tokens, indices)\n       initializer = tf.lookup.KeyValueTensorInitializer(keys, values,\n                                                         self._key_dtype,\n\n@@ -74,7 +74,7 @@ class IndexLookupDistributionTest(\n           num_oov_indices=1,\n           mask_token=\"\",\n           oov_token=\"[OOV]\",\n-          dtype=tf.string)\n+          vocabulary_dtype=tf.string)\n       layer.adapt(vocab_dataset)\n       int_data = layer(input_data)\n       model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -105,7 +105,7 @@ class IndexLookupDistributionTest(\n           num_oov_indices=1,\n           mask_token=\"\",\n           oov_token=\"[OOV]\",\n-          dtype=tf.string,\n+          vocabulary_dtype=tf.string,\n           vocabulary=vocab_file)\n       int_data = layer(input_data)\n       model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -138,7 +138,7 @@ class IndexLookupDistributionTest(\n           num_oov_indices=2,\n           mask_token=\"\",\n           oov_token=\"[OOV]\",\n-          dtype=tf.string)\n+          vocabulary_dtype=tf.string)\n       layer.adapt(vocab_dataset)\n       int_data = layer(input_data)\n       model = keras.Model(inputs=input_data, outputs=int_data)\n\n@@ -14,15 +14,12 @@\n # ==============================================================================\n \"\"\"Tests for Keras text vectorization preprocessing layer.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n import itertools\n import os\n import random\n import string\n \n from absl.testing import parameterized\n-import numpy as np\n \n import keras\n from keras import keras_parameterized\n@@ -30,6 +27,8 @@ from keras import testing_utils\n from keras.layers.preprocessing import index_lookup\n from keras.layers.preprocessing import preprocessing_test_utils\n from keras.utils.generic_utils import CustomObjectScope\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n \n \n def zip_and_sort(weight_values):\n@@ -56,7 +55,7 @@ def _get_end_to_end_test_cases():\n               \"num_oov_indices\": 1,\n               \"mask_token\": \"\",\n               \"oov_token\": \"[OOV]\",\n-              \"dtype\": tf.string,\n+              \"vocabulary_dtype\": tf.string,\n           },\n           \"expected_output\": [[2], [3], [4], [5], [5], [4], [2], [1]],\n           \"input_dtype\":\n@@ -78,7 +77,7 @@ def _get_end_to_end_test_cases():\n               \"num_oov_indices\": 1,\n               \"mask_token\": \"\",\n               \"oov_token\": \"[OOV]\",\n-              \"dtype\": tf.string,\n+              \"vocabulary_dtype\": tf.string,\n               \"invert\": True\n           },\n           \"expected_output\":\n@@ -104,7 +103,7 @@ def _get_end_to_end_test_cases():\n               \"num_oov_indices\": 1,\n               \"mask_token\": \"\",\n               \"oov_token\": \"[OOV]\",\n-              \"dtype\": tf.string,\n+              \"vocabulary_dtype\": tf.string,\n           },\n           \"expected_output\": [[2], [0], [3], [1], [4], [0], [5], [4], [1], [1]],\n           \"input_dtype\":\n@@ -128,7 +127,7 @@ def _get_end_to_end_test_cases():\n               \"num_oov_indices\": 1,\n               \"mask_token\": 0,\n               \"oov_token\": -1,\n-              \"dtype\": tf.int64,\n+              \"vocabulary_dtype\": tf.int64,\n           },\n           \"expected_output\": [[2], [3], [4], [5], [5], [4], [2], [1]],\n           \"input_dtype\":\n@@ -152,7 +151,7 @@ def _get_end_to_end_test_cases():\n               \"num_oov_indices\": 1,\n               \"mask_token\": 0,\n               \"oov_token\": -1,\n-              \"dtype\": tf.int64,\n+              \"vocabulary_dtype\": tf.int64,\n           },\n           \"expected_output\": [[2], [0], [3], [1], [4], [0], [5], [4], [1], [1]],\n           \"input_dtype\":\n@@ -175,7 +174,7 @@ def _get_end_to_end_test_cases():\n               \"num_oov_indices\": 1,\n               \"mask_token\": \"\",\n               \"oov_token\": \"[OOV]\",\n-              \"dtype\": tf.string,\n+              \"vocabulary_dtype\": tf.string,\n           },\n           \"expected_output\": [[2], [3], [4], [1], [1], [4], [2], [1]],\n           \"input_dtype\":\n@@ -197,7 +196,7 @@ def _get_end_to_end_test_cases():\n               \"num_oov_indices\": 1,\n               \"mask_token\": \"\",\n               \"oov_token\": \"[OOV]\",\n-              \"dtype\": tf.string,\n+              \"vocabulary_dtype\": tf.string,\n               \"invert\": True\n           },\n           \"expected_output\":\n@@ -224,7 +223,7 @@ def _get_end_to_end_test_cases():\n               \"num_oov_indices\": 1,\n               \"mask_token\": 0,\n               \"oov_token\": -1,\n-              \"dtype\": tf.int64,\n+              \"vocabulary_dtype\": tf.int64,\n           },\n           \"expected_output\": [[2], [3], [4], [1], [1], [4], [2], [1]],\n           \"input_dtype\":\n@@ -245,7 +244,7 @@ def _get_end_to_end_test_cases():\n               \"mask_token\": 0,\n               \"oov_token\": -1,\n               \"output_mode\": index_lookup.TF_IDF,\n-              \"dtype\": tf.int64,\n+              \"vocabulary_dtype\": tf.int64,\n           },\n           \"expected_output\": [[0, 1.098612, 0, 0, 0], [0, 0, 1.252763, 0, 0],\n                               [0, 0, 0, 1.466337, 0], [0, 0, 0, 0, 1.7917595],\n@@ -270,7 +269,7 @@ def _get_end_to_end_test_cases():\n               \"mask_token\": \"\",\n               \"oov_token\": \"[OOV]\",\n               \"output_mode\": index_lookup.TF_IDF,\n-              \"dtype\": tf.string,\n+              \"vocabulary_dtype\": tf.string,\n           },\n           \"expected_output\": [[0, 1.098612, 0, 0, 0], [0, 0, 1.252763, 0, 0],\n                               [0, 0, 0, 1.466337, 0], [0, 0, 0, 0, 1.7917595],\n@@ -304,7 +303,7 @@ class IndexLookupLayerTest(keras_parameterized.TestCase,\n                                        input_dtype):\n     cls = index_lookup.IndexLookup\n     if \"invert\" in kwargs and kwargs[\"invert\"]:\n-      expected_output_dtype = kwargs[\"dtype\"]\n+      expected_output_dtype = kwargs[\"vocabulary_dtype\"]\n     elif \"output_mode\" in kwargs and kwargs[\"output_mode\"] != index_lookup.INT:\n       expected_output_dtype = tf.float32\n     else:\n@@ -368,7 +367,7 @@ class CategoricalEncodingInputTest(\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -391,7 +390,7 @@ class CategoricalEncodingInputTest(\n     input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)\n     layer = index_lookup.IndexLookup(\n         max_tokens=None,\n-        dtype=tf.int64,\n+        vocabulary_dtype=tf.int64,\n         num_oov_indices=1,\n         mask_token=0,\n         oov_token=-1)\n@@ -415,7 +414,7 @@ class CategoricalEncodingInputTest(\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -431,7 +430,7 @@ class CategoricalEncodingInputTest(\n     input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)\n     layer = index_lookup.IndexLookup(\n         max_tokens=None,\n-        dtype=tf.int64,\n+        vocabulary_dtype=tf.int64,\n         num_oov_indices=1,\n         mask_token=0,\n         oov_token=-1)\n@@ -450,7 +449,7 @@ class CategoricalEncodingInputTest(\n     input_data = keras.Input(shape=(None,), dtype=tf.int32, ragged=True)\n     layer = index_lookup.IndexLookup(\n         max_tokens=None,\n-        dtype=tf.int64,\n+        vocabulary_dtype=tf.int64,\n         num_oov_indices=1,\n         mask_token=0,\n         oov_token=-1)\n@@ -481,7 +480,7 @@ class CategoricalEncodingMultiOOVTest(\n         num_oov_indices=2,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -504,7 +503,7 @@ class CategoricalEncodingMultiOOVTest(\n     input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)\n     layer = index_lookup.IndexLookup(\n         max_tokens=None,\n-        dtype=tf.int64,\n+        vocabulary_dtype=tf.int64,\n         num_oov_indices=2,\n         mask_token=0,\n         oov_token=-1)\n@@ -519,8 +518,7 @@ class CategoricalEncodingMultiOOVTest(\n   def test_ragged_string_input_multi_bucket(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n     input_array = tf.ragged.constant([[\"earth\", \"wind\", \"fire\"],\n-                                               [\"fire\", \"and\", \"earth\",\n-                                                \"ohio\"]])\n+                                      [\"fire\", \"and\", \"earth\", \"ohio\"]])\n     expected_output = [[3, 4, 6], [6, 5, 3, 2]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string, ragged=True)\n@@ -529,7 +527,7 @@ class CategoricalEncodingMultiOOVTest(\n         num_oov_indices=2,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -545,7 +543,7 @@ class CategoricalEncodingMultiOOVTest(\n     input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)\n     layer = index_lookup.IndexLookup(\n         max_tokens=None,\n-        dtype=tf.int64,\n+        vocabulary_dtype=tf.int64,\n         num_oov_indices=2,\n         mask_token=0,\n         oov_token=-1)\n@@ -573,7 +571,7 @@ class CategoricalEncodingAdaptTest(\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.adapt(vocab_dataset)\n     expected_vocabulary = [\"\", \"[OOV]\", \"michigan\", \"fire\"]\n     self.assertAllEqual(expected_vocabulary, layer.get_vocabulary())\n@@ -588,7 +586,7 @@ class CategoricalEncodingAdaptTest(\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.adapt(vocab_dataset)\n     expected_vocabulary = [\"\", \"[OOV]\", \"michigan\", \"fire\"]\n     self.assertAllEqual(expected_vocabulary, layer.get_vocabulary())\n@@ -607,7 +605,7 @@ class CategoricalEncodingAdaptTest(\n     input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)\n     layer = index_lookup.IndexLookup(\n         max_tokens=None,\n-        dtype=tf.int64,\n+        vocabulary_dtype=tf.int64,\n         num_oov_indices=1,\n         mask_token=0,\n         oov_token=-1)\n@@ -631,7 +629,7 @@ class CategoricalEncodingAdaptTest(\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -647,7 +645,7 @@ class CategoricalEncodingAdaptTest(\n     input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)\n     layer = index_lookup.IndexLookup(\n         max_tokens=None,\n-        dtype=tf.int64,\n+        vocabulary_dtype=tf.int64,\n         num_oov_indices=1,\n         mask_token=0,\n         oov_token=-1)\n@@ -672,7 +670,7 @@ class CategoricalEncodingAdaptTest(\n         num_oov_indices=0,\n         mask_token=None,\n         oov_token=None,\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     _ = layer(input_t)\n     layer.adapt(batched_ds)\n \n@@ -731,7 +729,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     output_dataset = layer(input_array)\n     self.assertAllEqual(expected_output, output_dataset)\n@@ -751,10 +749,36 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     int_data = layer(input_data)\n     self.assertAllEqual(int_data.shape.as_list(), [16, 4])\n \n+  @parameterized.named_parameters(\n+      (\"int32\", tf.int32),\n+      (\"int64\", tf.int64),\n+  )\n+  def test_int_output_dtype(self, dtype):\n+    input_data = keras.Input(batch_size=16, shape=(4,), dtype=tf.string)\n+    layer = index_lookup.IndexLookup(\n+        max_tokens=2,\n+        num_oov_indices=1,\n+        mask_token=\"\",\n+        oov_token=\"[OOV]\",\n+        vocabulary_dtype=tf.string,\n+        dtype=dtype)\n+    int_data = layer(input_data)\n+    self.assertAllEqual(int_data.dtype, dtype)\n+\n+  def test_int_output_float_dtype_fails(self):\n+    with self.assertRaisesRegex(ValueError, \"`dtype` should be an integer\"):\n+      index_lookup.IndexLookup(\n+          max_tokens=2,\n+          num_oov_indices=1,\n+          mask_token=\"\",\n+          oov_token=\"[OOV]\",\n+          vocabulary_dtype=tf.string,\n+          dtype=tf.float32)\n+\n   def test_int_output_no_reserved_zero(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n     input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n@@ -767,7 +791,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=None,\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -788,7 +812,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         num_oov_indices=0,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -814,7 +838,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         num_oov_indices=0,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -840,7 +864,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         num_oov_indices=0,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -864,7 +888,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n     output_dataset = model.predict(input_array)\n@@ -891,7 +915,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         oov_token=\"[OOV]\",\n         output_mode=index_lookup.ONE_HOT,\n         pad_to_max_tokens=True,\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     binary_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=binary_data)\n@@ -918,7 +942,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n         output_mode=index_lookup.ONE_HOT,\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     binary_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=binary_data)\n@@ -937,7 +961,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n         output_mode=index_lookup.ONE_HOT,\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     output_data = layer(input_data)\n     self.assertAllEqual(expected_output, output_data)\n@@ -951,10 +975,28 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n         output_mode=index_lookup.ONE_HOT,\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     outputs = layer(inputs)\n     self.assertAllEqual(outputs.shape.as_list(), [16, 2])\n \n+  @parameterized.named_parameters(\n+      (\"float32\", tf.float32),\n+      (\"float64\", tf.float64),\n+  )\n+  def test_one_hot_output_dtype(self, dtype):\n+    inputs = keras.Input(batch_size=16, shape=(1,), dtype=tf.string)\n+    layer = index_lookup.IndexLookup(\n+        vocabulary=[\"earth\"],\n+        max_tokens=2,\n+        num_oov_indices=1,\n+        mask_token=\"\",\n+        oov_token=\"[OOV]\",\n+        output_mode=index_lookup.ONE_HOT,\n+        vocabulary_dtype=tf.string,\n+        dtype=dtype)\n+    outputs = layer(inputs)\n+    self.assertAllEqual(outputs.dtype, dtype)\n+\n   def test_multi_hot_output_hard_maximum(self):\n     \"\"\"Check binary output when pad_to_max_tokens=True.\"\"\"\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n@@ -973,7 +1015,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         oov_token=\"[OOV]\",\n         output_mode=index_lookup.MULTI_HOT,\n         pad_to_max_tokens=True,\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     binary_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=binary_data)\n@@ -1000,7 +1042,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         oov_token=\"[OOV]\",\n         output_mode=index_lookup.MULTI_HOT,\n         pad_to_max_tokens=True,\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     binary_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=binary_data)\n@@ -1035,7 +1077,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         oov_token=\"[OOV]\",\n         output_mode=index_lookup.MULTI_HOT,\n         pad_to_max_tokens=True,\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n \n@@ -1067,7 +1109,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n         output_mode=index_lookup.MULTI_HOT,\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     binary_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=binary_data)\n@@ -1083,7 +1125,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         oov_token=\"[OOV]\",\n         output_mode=index_lookup.MULTI_HOT,\n         vocabulary=[\"foo\"],\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     binary_data = layer(input_data)\n     self.assertAllEqual(binary_data.shape.as_list(), [16, 2])\n \n@@ -1105,7 +1147,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         oov_token=\"[OOV]\",\n         output_mode=index_lookup.COUNT,\n         pad_to_max_tokens=True,\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     count_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=count_data)\n@@ -1129,7 +1171,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n         output_mode=index_lookup.COUNT,\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     count_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=count_data)\n@@ -1145,7 +1187,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         oov_token=\"[OOV]\",\n         output_mode=index_lookup.COUNT,\n         vocabulary=[\"foo\"],\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     count_data = layer(input_data)\n     self.assertAllEqual(count_data.shape.as_list(), [16, 2])\n \n@@ -1173,7 +1215,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         oov_token=\"[OOV]\",\n         output_mode=index_lookup.TF_IDF,\n         pad_to_max_tokens=True,\n-        dtype=tf.string,\n+        vocabulary_dtype=tf.string,\n         sparse=sparse,\n         vocabulary=vocab_data,\n         idf_weights=idf_weights)\n@@ -1207,7 +1249,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n         output_mode=index_lookup.TF_IDF,\n-        dtype=tf.string,\n+        vocabulary_dtype=tf.string,\n         sparse=sparse,\n         vocabulary=vocab_data,\n         idf_weights=idf_weights)\n@@ -1226,7 +1268,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n         output_mode=index_lookup.TF_IDF,\n-        dtype=tf.string,\n+        vocabulary_dtype=tf.string,\n         vocabulary=[\"foo\"],\n         idf_weights=[1.0])\n     layer_output = layer(input_data)\n@@ -1247,7 +1289,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n     output_dataset = model.predict(input_array)\n@@ -1275,7 +1317,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n           mask_token=\"\",\n           oov_token=\"[OOV]\",\n           output_mode=index_lookup.MULTI_HOT,\n-          dtype=tf.string)\n+          vocabulary_dtype=tf.string)\n       return layer(data)\n \n     output_dataset = compute(input_array)\n@@ -1292,7 +1334,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n \n     list_layer = index_lookup.IndexLookup(\n         vocabulary=vocab_data,\n@@ -1300,7 +1342,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n \n     expected_vocab = [\"\", \"[OOV]\", \"earth\", \"wind\", \"and\", \"fire\"]\n     self.assertAllEqual(expected_vocab, list_layer.get_vocabulary())\n@@ -1322,7 +1364,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         num_oov_indices=2,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n \n     list_layer = index_lookup.IndexLookup(\n         vocabulary=vocab_data,\n@@ -1330,7 +1372,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         num_oov_indices=2,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n \n     expected_vocab = [\"\", \"[OOV]\", \"[OOV]\", \"earth\", \"wind\", \"and\", \"fire\"]\n     self.assertAllEqual(expected_vocab, list_layer.get_vocabulary())\n@@ -1352,7 +1394,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         num_oov_indices=2,\n         mask_token=None,\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n \n     list_layer = index_lookup.IndexLookup(\n         vocabulary=vocab_data,\n@@ -1360,7 +1402,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         num_oov_indices=2,\n         mask_token=None,\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n \n     expected_vocab = [\"[OOV]\", \"[OOV]\", \"earth\", \"wind\", \"and\", \"fire\"]\n     self.assertAllEqual(expected_vocab, list_layer.get_vocabulary())\n@@ -1386,7 +1428,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         mask_token=None,\n         num_oov_indices=1,\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n     output_dataset = model.predict(input_array)\n@@ -1407,7 +1449,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         mask_token=None,\n         num_oov_indices=0,\n         oov_token=None,\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n     output_dataset = model.predict(input_array)\n@@ -1427,7 +1469,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         mask_token=None,\n         num_oov_indices=1,\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     _ = layer(idata)\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n@@ -1439,7 +1481,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         mask_token=None,\n         num_oov_indices=1,\n         invert=True,\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     int_data = invert_layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n     output_dataset = model.predict(input_array)\n@@ -1458,7 +1500,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=0,\n         oov_token=-1,\n-        dtype=tf.int64)\n+        vocabulary_dtype=tf.int64)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n     output_dataset = model.predict(input_array)\n@@ -1472,7 +1514,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         mask_token=None,\n         oov_token=\"[OOV]\",\n         vocabulary=vocab_data,\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     ds = tf.data.Dataset.from_tensor_slices([[\"earth\"], [\"wind\"], [\"and\"]])\n     ds = ds.map(layer)\n     self.assertAllEqual(list(ds.as_numpy_iterator()), [[0], [1], [2]])\n@@ -1487,7 +1529,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n           mask_token=None,\n           oov_token=\"[OOV]\",\n           vocabulary=vocab_data,\n-          dtype=tf.string)\n+          vocabulary_dtype=tf.string)\n       return layer(data)\n \n     ds = tf.data.Dataset.from_tensor_slices([[\"earth\"], [\"wind\"], [\"and\"]])\n@@ -1513,7 +1555,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n     output_dataset = model.predict(input_array)\n@@ -1532,7 +1574,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n     output_dataset = model.predict(input_array)\n@@ -1545,7 +1587,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     returned_vocab = layer.get_vocabulary(include_special_tokens=False)\n     self.assertAllEqual(returned_vocab, [\"wind\", \"and\", \"fire\"])\n@@ -1558,7 +1600,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n         num_oov_indices=2,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     returned_vocab = layer.get_vocabulary()\n     self.assertAllEqual(returned_vocab, vocab_data)\n@@ -1570,7 +1612,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n         num_oov_indices=10,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     returned_vocab = layer.get_vocabulary()\n     self.assertAllEqual(returned_vocab,\n@@ -1583,7 +1625,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     returned_vocab = layer.get_vocabulary()\n     self.assertAllEqual(vocab_data, returned_vocab)\n@@ -1596,7 +1638,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=0,\n         oov_token=-1,\n-        dtype=tf.int64)\n+        vocabulary_dtype=tf.int64)\n     layer.set_vocabulary(vocab_data)\n     returned_vocab = layer.get_vocabulary()\n     self.assertAllEqual(vocab_data, returned_vocab)\n@@ -1609,7 +1651,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n         num_oov_indices=3,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     returned_vocab = layer.get_vocabulary()\n     self.assertAllEqual(vocab_data, returned_vocab)\n@@ -1621,7 +1663,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n         num_oov_indices=3,\n         mask_token=0,\n         oov_token=-1,\n-        dtype=tf.int64)\n+        vocabulary_dtype=tf.int64)\n     layer.set_vocabulary(vocab_data)\n     returned_vocab = layer.get_vocabulary()\n     self.assertAllEqual(vocab_data, returned_vocab)\n@@ -1635,7 +1677,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n           num_oov_indices=1,\n           mask_token=\"\",\n           oov_token=\"[OOV]\",\n-          dtype=tf.string)\n+          vocabulary_dtype=tf.string)\n \n   def test_vocab_with_repeated_element_fails(self):\n     vocab_data = [\"earth\", \"earth\", \"wind\", \"and\", \"fire\"]\n@@ -1644,7 +1686,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     with self.assertRaisesRegex(ValueError, \"repeated term.*earth\"):\n       layer.set_vocabulary(vocab_data)\n \n@@ -1656,7 +1698,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n         invert=True,\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     with self.assertRaisesRegex(ValueError, \"reserved OOV\"):\n       layer.set_vocabulary(vocab_data)\n \n@@ -1667,7 +1709,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"mask_token\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     with self.assertRaisesRegex(ValueError, \"reserved mask\"):\n       layer.set_vocabulary(vocab_data)\n \n@@ -1680,7 +1722,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n         oov_token=\"[OOV]\",\n         pad_to_max_tokens=False,\n         output_mode=index_lookup.MULTI_HOT,\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     # Calling the layer should lock the vocabulary size.\n     _ = layer([[\"earth\"]])\n@@ -1701,7 +1743,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n           mask_token=\"\",\n           oov_token=\"[OOV]\",\n           output_mode=index_lookup.MULTI_HOT,\n-          dtype=tf.string,\n+          vocabulary_dtype=tf.string,\n           vocabulary=vocab_data,\n           idf_weights=weight_data)\n \n@@ -1716,7 +1758,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n           mask_token=\"\",\n           oov_token=\"[OOV]\",\n           output_mode=index_lookup.TF_IDF,\n-          dtype=tf.string,\n+          vocabulary_dtype=tf.string,\n           vocabulary=vocab_data,\n           idf_weights=weight_data)\n \n@@ -1730,7 +1772,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n           mask_token=\"\",\n           oov_token=\"[OOV]\",\n           output_mode=index_lookup.TF_IDF,\n-          dtype=tf.string,\n+          vocabulary_dtype=tf.string,\n           vocabulary=vocab_data)\n \n   def test_non_unique_int_vocab_fails(self):\n@@ -1742,7 +1784,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n           num_oov_indices=1,\n           mask_token=0,\n           oov_token=-1,\n-          dtype=tf.int64)\n+          vocabulary_dtype=tf.int64)\n \n   def test_int_vocab_with_reserved_oov_element_and_invert_true_fails(self):\n     vocab_data = [14, 38, -1, 34, 3, 84]\n@@ -1752,7 +1794,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n         mask_token=0,\n         oov_token=-1,\n         invert=True,\n-        dtype=tf.int64)\n+        vocabulary_dtype=tf.int64)\n     with self.assertRaisesRegex(ValueError, \"reserved OOV\"):\n       layer.set_vocabulary(vocab_data)\n \n@@ -1763,7 +1805,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=0,\n         oov_token=-1,\n-        dtype=tf.int64)\n+        vocabulary_dtype=tf.int64)\n     with self.assertRaisesRegex(ValueError, \"reserved mask\"):\n       layer.set_vocabulary(vocab_data)\n \n@@ -1775,7 +1817,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n           num_oov_indices=1,\n           mask_token=0,\n           oov_token=-1,\n-          dtype=tf.int64)\n+          vocabulary_dtype=tf.int64)\n \n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n@@ -1796,7 +1838,7 @@ class IndexLookupInverseVocabularyTest(\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string,\n+        vocabulary_dtype=tf.string,\n         invert=True)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -1810,7 +1852,7 @@ class IndexLookupInverseVocabularyTest(\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string,\n+        vocabulary_dtype=tf.string,\n         invert=True)\n     layer.set_vocabulary(vocab_data)\n     returned_vocab = layer.get_vocabulary()\n@@ -1823,7 +1865,7 @@ class IndexLookupInverseVocabularyTest(\n         num_oov_indices=1,\n         mask_token=0,\n         oov_token=-1,\n-        dtype=tf.int64,\n+        vocabulary_dtype=tf.int64,\n         invert=True)\n     layer.set_vocabulary(vocab_data)\n     returned_vocab = layer.get_vocabulary()\n@@ -1838,7 +1880,7 @@ class IndexLookupInverseVocabularyTest(\n           num_oov_indices=1,\n           mask_token=\"\",\n           oov_token=\"[OOV]\",\n-          dtype=tf.string,\n+          vocabulary_dtype=tf.string,\n           invert=True)\n \n   def test_non_int_output_fails(self):\n@@ -1848,7 +1890,7 @@ class IndexLookupInverseVocabularyTest(\n           num_oov_indices=1,\n           mask_token=\"\",\n           oov_token=\"[OOV]\",\n-          dtype=tf.string,\n+          vocabulary_dtype=tf.string,\n           output_mode=index_lookup.COUNT,\n           invert=True)\n \n@@ -1859,7 +1901,7 @@ class IndexLookupInverseVocabularyTest(\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string,\n+        vocabulary_dtype=tf.string,\n         invert=True)\n     with self.assertRaisesRegex(ValueError, \"repeated term.*earth\"):\n       layer.set_vocabulary(vocab_data)\n@@ -1871,7 +1913,7 @@ class IndexLookupInverseVocabularyTest(\n         num_oov_indices=1,\n         mask_token=\"mask_token\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string,\n+        vocabulary_dtype=tf.string,\n         invert=True)\n     with self.assertRaisesRegex(ValueError, \"reserved mask\"):\n       layer.set_vocabulary(vocab_data)\n@@ -1885,7 +1927,7 @@ class IndexLookupInverseVocabularyTest(\n           num_oov_indices=1,\n           mask_token=0,\n           oov_token=-1,\n-          dtype=tf.int64,\n+          vocabulary_dtype=tf.int64,\n           invert=True)\n \n   def test_int_vocab_with_repeated_element_fails(self):\n@@ -1895,7 +1937,7 @@ class IndexLookupInverseVocabularyTest(\n         num_oov_indices=1,\n         mask_token=0,\n         oov_token=-1,\n-        dtype=tf.int64,\n+        vocabulary_dtype=tf.int64,\n         invert=True)\n     with self.assertRaisesRegex(ValueError, \"repeated term.*11\"):\n       layer.set_vocabulary(vocab_data)\n@@ -1913,7 +1955,7 @@ class IndexLookupErrorTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     with self.assertRaisesRegex(ValueError,\n                                 \"vocabulary larger than the maximum vocab\"):\n       layer.set_vocabulary(vocab_data)\n@@ -1925,7 +1967,7 @@ class IndexLookupErrorTest(keras_parameterized.TestCase,\n           num_oov_indices=1,\n           mask_token=\"\",\n           oov_token=\"[OOV]\",\n-          dtype=tf.string)\n+          vocabulary_dtype=tf.string)\n \n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n@@ -1954,7 +1996,7 @@ class IndexLookupSavingTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string)\n+        vocabulary_dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -1993,7 +2035,7 @@ class IndexLookupSavingTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string,\n+        vocabulary_dtype=tf.string,\n         vocabulary=vocab_file)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -2026,7 +2068,7 @@ class IndexLookupSavingTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string,\n+        vocabulary_dtype=tf.string,\n         vocabulary=vocab_file)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -2066,7 +2108,7 @@ class IndexLookupSavingTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string,\n+        vocabulary_dtype=tf.string,\n         vocabulary=vocab_file)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -2106,7 +2148,7 @@ class IndexLookupSavingTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string,\n+        vocabulary_dtype=tf.string,\n         vocabulary=vocab_file)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -2173,7 +2215,7 @@ class IndexLookupSavingTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string,\n+        vocabulary_dtype=tf.string,\n         vocabulary=vocab_file)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -2241,7 +2283,7 @@ class IndexLookupSavingTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string,\n+        vocabulary_dtype=tf.string,\n         vocabulary=vocab_file)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -2308,7 +2350,7 @@ class IndexLookupSavingTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        dtype=tf.string,\n+        vocabulary_dtype=tf.string,\n         vocabulary=vocab_file)\n     config = layer.get_config()\n     weights = layer.get_weights()\n@@ -2343,7 +2385,7 @@ class EagerExecutionDisabled(keras_parameterized.TestCase,\n             num_oov_indices=1,\n             mask_token=None,\n             oov_token=\"[OOV]\",\n-            dtype=tf.string,\n+            vocabulary_dtype=tf.string,\n             vocabulary=vocab_data)\n         int_data = layer(input_data)\n         model = keras.Model(inputs=input_data, outputs=int_data)\n\n@@ -15,6 +15,7 @@\n \"\"\"Keras string lookup preprocessing layer.\"\"\"\n \n # pylint: disable=g-classes-have-attributes\n+# pylint: disable=g-direct-tensorflow-import\n \n from keras.engine import base_preprocessing_layer\n from keras.layers.preprocessing import index_lookup\n@@ -85,6 +86,8 @@ class IntegerLookup(index_lookup.IndexLookup):\n       tensor containing the integer vocbulary terms. If passing a file path, the\n       file should contain one line per term in the vocabulary. If this argument\n       is set, there is no need to `adapt` the layer.\n+    vocabulary_dtype: The dtype of the vocabulary terms, for example\n+      `\"int64\"` or `\"int32\"`. Defaults to `\"int64\"`.\n     idf_weights: Only valid when `output_mode` is `\"tf_idf\"`. A tuple, list, 1D\n       numpy array, or 1D tensor or the same length as the vocabulary, containing\n       the floating point inverse document frequency weights, which will be\n@@ -309,13 +312,23 @@ class IntegerLookup(index_lookup.IndexLookup):\n                mask_token=None,\n                oov_token=-1,\n                vocabulary=None,\n+               vocabulary_dtype=\"int64\",\n                idf_weights=None,\n                invert=False,\n                output_mode=\"int\",\n                sparse=False,\n                pad_to_max_tokens=False,\n                **kwargs):\n-    allowed_dtypes = [tf.int64]\n+    if not tf.dtypes.as_dtype(vocabulary_dtype).is_integer:\n+      raise ValueError(\"`vocabulary_dtype` must be an integer dtype. \"\n+                       f\"Received: {vocabulary_dtype}\")\n+\n+    # Legacy versions of the IntegerLookup layer set layer dtype to int64,\n+    # instead of the output type. If we see this and output mode is not \"int\",\n+    # clear the setting so we don't switch types for old SavedModels.\n+    if output_mode != \"int\" and \"dtype\" in kwargs and (\n+        kwargs[\"dtype\"] == tf.int64 or kwargs[\"dtype\"] == \"int64\"):\n+      del kwargs[\"dtype\"]\n \n     # Support deprecated args for this layer.\n     if \"max_values\" in kwargs:\n@@ -336,13 +349,6 @@ class IntegerLookup(index_lookup.IndexLookup):\n       oov_token = kwargs[\"oov_value\"]\n       del kwargs[\"oov_value\"]\n \n-    if \"dtype\" in kwargs and kwargs[\"dtype\"] not in allowed_dtypes:\n-      raise ValueError(\"The value of the dtype argument for IntegerLookup may \"\n-                       \"only be one of %s.\" % (allowed_dtypes,))\n-\n-    if \"dtype\" not in kwargs:\n-      kwargs[\"dtype\"] = tf.int64\n-\n     # If max_tokens is set, the token must be greater than 1 - otherwise we\n     # are creating a 0-element vocab, which doesn't make sense.\n     if max_tokens is not None and max_tokens <= 1:\n@@ -366,6 +372,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n         mask_token=mask_token,\n         oov_token=oov_token,\n         vocabulary=vocabulary,\n+        vocabulary_dtype=vocabulary_dtype,\n         idf_weights=idf_weights,\n         invert=invert,\n         output_mode=output_mode,\n\n@@ -14,7 +14,6 @@\n # ==============================================================================\n \"\"\"Utils for preprocessing layers.\"\"\"\n \n-from keras import backend\n from keras.utils import tf_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n@@ -35,7 +34,7 @@ def listify_tensors(x):\n   return x\n \n \n-def sparse_bincount(inputs, depth, binary_output, count_weights=None):\n+def sparse_bincount(inputs, depth, binary_output, dtype, count_weights=None):\n   \"\"\"Apply binary or count encoding to an input and return a sparse tensor.\"\"\"\n   result = tf.sparse.bincount(\n       inputs,\n@@ -44,7 +43,7 @@ def sparse_bincount(inputs, depth, binary_output, count_weights=None):\n       maxlength=depth,\n       axis=-1,\n       binary_output=binary_output)\n-  result = tf.cast(result, backend.floatx())\n+  result = tf.cast(result, dtype)\n   if inputs.shape.rank == 1:\n     output_shape = (depth,)\n   else:\n@@ -55,14 +54,14 @@ def sparse_bincount(inputs, depth, binary_output, count_weights=None):\n   return result\n \n \n-def dense_bincount(inputs, depth, binary_output, count_weights=None):\n+def dense_bincount(inputs, depth, binary_output, dtype, count_weights=None):\n   \"\"\"Apply binary or count encoding to an input.\"\"\"\n   result = tf.math.bincount(\n       inputs,\n       weights=count_weights,\n       minlength=depth,\n       maxlength=depth,\n-      dtype=backend.floatx(),\n+      dtype=dtype,\n       axis=-1,\n       binary_output=binary_output)\n   if inputs.shape.rank == 1:\n@@ -83,12 +82,13 @@ def expand_dims(inputs, axis):\n def encode_categorical_inputs(inputs,\n                               output_mode,\n                               depth,\n+                              dtype=\"float32\",\n                               sparse=False,\n                               count_weights=None,\n                               idf_weights=None):\n   \"\"\"Encodes categoical inputs according to output_mode.\"\"\"\n   if output_mode == INT:\n-    return inputs\n+    return tf.cast(inputs, dtype)\n \n   original_shape = inputs.shape\n   # In all cases, we should uprank scalar input to a single sample.\n@@ -108,9 +108,11 @@ def encode_categorical_inputs(inputs,\n \n   binary_output = output_mode in (MULTI_HOT, ONE_HOT)\n   if sparse:\n-    bincounts = sparse_bincount(inputs, depth, binary_output, count_weights)\n+    bincounts = sparse_bincount(inputs, depth, binary_output, dtype,\n+                                count_weights)\n   else:\n-    bincounts = dense_bincount(inputs, depth, binary_output, count_weights)\n+    bincounts = dense_bincount(inputs, depth, binary_output, dtype,\n+                               count_weights)\n \n   if output_mode != TF_IDF:\n     return bincounts\n\n@@ -98,17 +98,27 @@ class EncodeCategoricalInputsTest(keras_parameterized.TestCase):\n       outputs = tf.sparse.to_dense(outputs)\n     self.assertAllClose([.1, 2, 30, 0], outputs)\n \n+  def test_output_dtype(self):\n+    inputs = tf.constant([0, 1, 2], dtype=tf.dtypes.int32)\n+    outputs = preprocessing_utils.encode_categorical_inputs(\n+        inputs, output_mode='int', depth=4, dtype=tf.dtypes.int64)\n+    self.assertAllEqual(outputs.dtype, tf.dtypes.int64)\n+    outputs = preprocessing_utils.encode_categorical_inputs(\n+        inputs, output_mode='one_hot', depth=4, dtype=tf.dtypes.float64)\n+    self.assertAllEqual(outputs.dtype, tf.dtypes.float64)\n+\n   def test_rank_3_output_fails(self):\n     inputs = tf.constant([[[0]], [[1]], [[2]]])\n     with self.assertRaisesRegex(ValueError,\n                                 'maximum supported output rank is 2'):\n-      preprocessing_utils.encode_categorical_inputs(inputs, 'multi_hot', 4)\n+      preprocessing_utils.encode_categorical_inputs(inputs, 'multi_hot', 4,\n+                                                    'float32')\n \n   def test_tf_idf_output_with_no_weights_fails(self):\n     inputs = tf.constant([0, 1, 2])\n-    with self.assertRaisesRegex(ValueError,\n-                                'idf_weights must be provided'):\n-      preprocessing_utils.encode_categorical_inputs(inputs, 'tf_idf', 4)\n+    with self.assertRaisesRegex(ValueError, 'idf_weights must be provided'):\n+      preprocessing_utils.encode_categorical_inputs(inputs, 'tf_idf', 4,\n+                                                    'float32')\n \n \n if __name__ == '__main__':\n\n@@ -310,15 +310,11 @@ class StringLookup(index_lookup.IndexLookup):\n                sparse=False,\n                pad_to_max_tokens=False,\n                **kwargs):\n-    allowed_dtypes = [tf.string]\n-\n-    if \"dtype\" in kwargs and kwargs[\"dtype\"] not in allowed_dtypes:\n-      raise ValueError(\n-          f\"The value of the `dtype` for `StringLookup` may \"\n-          f\"only be one of {allowed_dtypes}, but received {kwargs['dtype']}.\")\n-\n-    if \"dtype\" not in kwargs:\n-      kwargs[\"dtype\"] = tf.string\n+    # Legacy versions of the StringLookup layer set layer dtype to string,\n+    # instead of the output type. If we see this, clear it.\n+    if \"dtype\" in kwargs and (kwargs[\"dtype\"] == tf.string or\n+                              kwargs[\"dtype\"] == \"string\"):\n+      del kwargs[\"dtype\"]\n \n     if encoding is None:\n       encoding = \"utf-8\"\n@@ -331,6 +327,7 @@ class StringLookup(index_lookup.IndexLookup):\n         mask_token=mask_token,\n         oov_token=oov_token,\n         vocabulary=vocabulary,\n+        vocabulary_dtype=tf.string,\n         idf_weights=idf_weights,\n         invert=invert,\n         output_mode=output_mode,\n@@ -342,6 +339,8 @@ class StringLookup(index_lookup.IndexLookup):\n   def get_config(self):\n     config = {\"encoding\": self.encoding}\n     base_config = super(StringLookup, self).get_config()\n+    # There is only one valid dtype for strings, so we don't expose this.\n+    del base_config[\"vocabulary_dtype\"]\n     return dict(list(base_config.items()) + list(config.items()))\n \n   # Overridden methods from IndexLookup.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
