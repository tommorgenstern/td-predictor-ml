{"custom_id": "scrapy#dd3a46295c069561b0c278a8af0db784b57a6416", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 19 | Churn Cumulative: 27 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -94,6 +94,25 @@ class RFC2616Policy(object):\n         currentage = self._compute_current_age(cachedresponse, request, now)\n         if currentage < freshnesslifetime:\n             return True\n+\n+        if 'max-stale' in ccreq and 'must-revalidate' not in cc:\n+            # From RFC2616: \"Indicates that the client is willing to\n+            # accept a response that has exceeded its expiration time.\n+            # If max-stale is assigned a value, then the client is\n+            # willing to accept a response that has exceeded its\n+            # expiration time by no more than the specified number of\n+            # seconds. If no value is assigned to max-stale, then the\n+            # client is willing to accept a stale response of any age.\"\n+            staleage = ccreq['max-stale']\n+            if staleage is None:\n+                return True\n+\n+            try:\n+                if currentage < freshnesslifetime + max(0, int(staleage)):\n+                    return True\n+            except ValueError:\n+                pass\n+\n         # Cached response is stale, try to set validators if any\n         self._set_conditional_validators(request, cachedresponse)\n         return False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e23a38133726b716f5931e59e163cfe70169d17c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 10 | Churn Cumulative: 398 | Contributors (this commit): 18 | Commits (past 90d): 21 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,7 +7,7 @@ from time import time\n from weakref import WeakKeyDictionary\n from email.utils import mktime_tz, parsedate_tz\n from w3lib.http import headers_raw_to_dict, headers_dict_to_raw\n-from scrapy.http import Headers\n+from scrapy.http import Headers, Response\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.project import data_path\n@@ -39,12 +39,17 @@ class RFC2616Policy(object):\n \n     def __init__(self, settings):\n         self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n+        self.ignore_response_cache_controls = settings.getlist('HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS')\n         self._cc_parsed = WeakKeyDictionary()\n \n     def _parse_cachecontrol(self, r):\n         if r not in self._cc_parsed:\n             cch = r.headers.get('Cache-Control', '')\n-            self._cc_parsed[r] = parse_cachecontrol(cch)\n+            parsed = parse_cachecontrol(cch)\n+            if isinstance(r, Response):\n+                for key in self.ignore_response_cache_controls:\n+                    parsed.pop(key, None)\n+            self._cc_parsed[r] = parsed\n         return self._cc_parsed[r]\n \n     def should_cache_request(self, request):\n\n@@ -155,6 +155,7 @@ HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n HTTPCACHE_EXPIRATION_SECS = 0\n HTTPCACHE_IGNORE_HTTP_CODES = []\n HTTPCACHE_IGNORE_SCHEMES = ['file']\n+HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS = []\n HTTPCACHE_DBM_MODULE = 'anydbm'\n HTTPCACHE_POLICY = 'scrapy.extensions.httpcache.DummyPolicy'\n HTTPCACHE_GZIP = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c3b2cabf6c6600a5a2c6bbef2035ac7616ef6a06", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 403 | Contributors (this commit): 18 | Commits (past 90d): 23 | Contributors (cumulative): 20 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -38,6 +38,7 @@ class RFC2616Policy(object):\n     MAXAGE = 3600 * 24 * 365  # one year\n \n     def __init__(self, settings):\n+        self.always_store = settings.getbool('HTTPCACHE_ALWAYS_STORE')\n         self.ignore_schemes = settings.getlist('HTTPCACHE_IGNORE_SCHEMES')\n         self.ignore_response_cache_controls = settings.getlist('HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS')\n         self._cc_parsed = WeakKeyDictionary()\n@@ -73,6 +74,9 @@ class RFC2616Policy(object):\n         # Never cache 304 (Not Modified) responses\n         elif response.status == 304:\n             return False\n+        # Cache unconditionally if configured to do so\n+        elif self.always_store:\n+            return True\n         # Any hint on response expiration is good\n         elif 'max-age' in cc or 'Expires' in response.headers:\n             return True\n\n@@ -153,6 +153,7 @@ HTTPCACHE_DIR = 'httpcache'\n HTTPCACHE_IGNORE_MISSING = False\n HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n HTTPCACHE_EXPIRATION_SECS = 0\n+HTTPCACHE_ALWAYS_STORE = False\n HTTPCACHE_IGNORE_HTTP_CODES = []\n HTTPCACHE_IGNORE_SCHEMES = ['file']\n HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#19915504422ccf6735b11f176ae8170c43562c29", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 19 | Churn Cumulative: 59 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 0.8333333333333334\n\nDIFF:\n@@ -101,6 +101,11 @@ class RFC2616Policy(object):\n         now = time()\n         freshnesslifetime = self._compute_freshness_lifetime(cachedresponse, request, now)\n         currentage = self._compute_current_age(cachedresponse, request, now)\n+\n+        reqmaxage = self._get_max_age(ccreq)\n+        if reqmaxage is not None:\n+            freshnesslifetime = min(freshnesslifetime, reqmaxage)\n+\n         if currentage < freshnesslifetime:\n             return True\n \n@@ -144,15 +149,19 @@ class RFC2616Policy(object):\n         if 'ETag' in cachedresponse.headers:\n             request.headers['If-None-Match'] = cachedresponse.headers['ETag']\n \n+    def _get_max_age(self, cc):\n+        try:\n+            return max(0, int(cc['max-age']))\n+        except (KeyError, ValueError):\n+            return None\n+\n     def _compute_freshness_lifetime(self, response, request, now):\n         # Reference nsHttpResponseHead::ComputeFreshnessLifetime\n         # http://dxr.mozilla.org/mozilla-central/source/netwerk/protocol/http/nsHttpResponseHead.cpp#410\n         cc = self._parse_cachecontrol(response)\n-        if 'max-age' in cc:\n-            try:\n-                return max(0, int(cc['max-age']))\n-            except ValueError:\n-                pass\n+        maxage = self._get_max_age(cc)\n+        if maxage is not None:\n+            return maxage\n \n         # Parse date header or synthesize it if none exists\n         date = rfc1123_to_epoch(response.headers.get('Date')) or now\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bb3ebf13f97a06e8fce0e3dd6a734ab2f4a91fbd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 85 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 85 | Churn Cumulative: 126 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -343,6 +343,25 @@ class RFC2616PolicyTest(DefaultStorageTest):\n                     self.assertFalse(resc)\n                     assert 'cached' not in res2.flags\n \n+        # cache unconditionally unless response contains no-store or is a 304\n+        with self._middleware(HTTPCACHE_ALWAYS_STORE=True) as mw:\n+            for idx, (_, status, headers) in enumerate(responses):\n+                shouldcache = 'no-store' not in headers.get('Cache-Control', '') and status != 304\n+                req0 = Request('http://example2-%d.com' % idx)\n+                res0 = Response(req0.url, status=status, headers=headers)\n+                res1 = self._process_requestresponse(mw, req0, res0)\n+                res304 = res0.replace(status=304)\n+                res2 = self._process_requestresponse(mw, req0, res304 if shouldcache else res0)\n+                self.assertEqualResponse(res1, res0)\n+                self.assertEqualResponse(res2, res0)\n+                resc = mw.storage.retrieve_response(self.spider, req0)\n+                if shouldcache:\n+                    self.assertEqualResponse(resc, res1)\n+                    assert 'cached' in res2.flags and res2.status != 304\n+                else:\n+                    self.assertFalse(resc)\n+                    assert 'cached' not in res2.flags\n+\n     def test_cached_and_fresh(self):\n         sampledata = [\n             (200, {'Date': self.yesterday, 'Expires': self.tomorrow}),\n@@ -381,6 +400,13 @@ class RFC2616PolicyTest(DefaultStorageTest):\n                 res2 = self._process_requestresponse(mw, req0, None)\n                 self.assertEqualResponse(res1, res2)\n                 assert 'cached' in res2.flags\n+                # validate cached response if request max-age set as 0\n+                req1 = req0.replace(headers={'Cache-Control': 'max-age=0'})\n+                res304 = res0.replace(status=304)\n+                assert mw.process_request(req1, self.spider) is None\n+                res3 = self._process_requestresponse(mw, req1, res304)\n+                self.assertEqualResponse(res1, res3)\n+                assert 'cached' in res3.flags\n \n     def test_cached_and_stale(self):\n         sampledata = [\n@@ -395,6 +421,9 @@ class RFC2616PolicyTest(DefaultStorageTest):\n             (200, {'Cache-Control': 'no-cache'}),\n             (200, {'Cache-Control': 'no-cache', 'ETag': 'foo'}),\n             (200, {'Cache-Control': 'no-cache', 'Last-Modified': self.yesterday}),\n+            (200, {'Cache-Control': 'no-cache,must-revalidate', 'Last-Modified': self.yesterday}),\n+            (200, {'Cache-Control': 'must-revalidate', 'Expires': self.yesterday, 'Last-Modified': self.yesterday}),\n+            (200, {'Cache-Control': 'max-age=86400,must-revalidate', 'Age': '86405'}),\n         ]\n         with self._middleware() as mw:\n             for idx, (status, headers) in enumerate(sampledata):\n@@ -410,6 +439,7 @@ class RFC2616PolicyTest(DefaultStorageTest):\n                 res2 = self._process_requestresponse(mw, req0, res0b)\n                 self.assertEqualResponse(res2, res0b)\n                 assert 'cached' not in res2.flags\n+                cc = headers.get('Cache-Control', '')\n                 # Previous response expired too, subsequent request to same\n                 # resource must revalidate and succeed on 304 if validators\n                 # are present\n@@ -418,7 +448,62 @@ class RFC2616PolicyTest(DefaultStorageTest):\n                     res3 = self._process_requestresponse(mw, req0, res0c)\n                     self.assertEqualResponse(res3, res0b)\n                     assert 'cached' in res3.flags\n+                    # get cached response on server errors unless must-revalidate\n+                    # in cached response\n+                    res0d = res0b.replace(status=500)\n+                    res4 = self._process_requestresponse(mw, req0, res0d)\n+                    if 'must-revalidate' in cc:\n+                        assert 'cached' not in res4.flags\n+                        self.assertEqualResponse(res4, res0d)\n+                    else:\n+                        assert 'cached' in res4.flags\n+                        self.assertEqualResponse(res4, res0b)\n+                # Requests with max-stale can fetch expired cached responses\n+                # unless cached response has must-revalidate\n+                req1 = req0.replace(headers={'Cache-Control': 'max-stale'})\n+                res5 = self._process_requestresponse(mw, req1, res0b)\n+                self.assertEqualResponse(res5, res0b)\n+                if 'no-cache' in cc or 'must-revalidate' in cc:\n+                    assert 'cached' not in res5.flags\n+                else:\n+                    assert 'cached' in res5.flags\n \n+    def test_process_exception(self):\n+        with self._middleware() as mw:\n+            res0 = Response(self.request.url, headers={'Expires': self.yesterday})\n+            req0 = Request(self.request.url)\n+            self._process_requestresponse(mw, req0, res0)\n+            for e in mw.DOWNLOAD_EXCEPTIONS:\n+                # Simulate encountering an error on download attempts\n+                assert mw.process_request(req0, self.spider) is None\n+                res1 = mw.process_exception(req0, e('foo'), self.spider)\n+                # Use cached response as recovery\n+                assert 'cached' in res1.flags\n+                self.assertEqualResponse(res0, res1)\n+            # Do not use cached response for unhandled exceptions\n+            mw.process_request(req0, self.spider)\n+            assert mw.process_exception(req0, Exception('foo'), self.spider) is None\n+\n+    def test_ignore_response_cache_controls(self):\n+        sampledata = [\n+            (200, {'Date': self.yesterday, 'Expires': self.tomorrow}),\n+            (200, {'Date': self.yesterday, 'Cache-Control': 'no-store,max-age=86405'}),\n+            (200, {'Age': '299', 'Cache-Control': 'max-age=300,no-cache'}),\n+            (300, {'Cache-Control': 'no-cache'}),\n+            (200, {'Expires': self.tomorrow, 'Cache-Control': 'no-store'}),\n+        ]\n+        with self._middleware(HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS=['no-cache', 'no-store']) as mw:\n+            for idx, (status, headers) in enumerate(sampledata):\n+                req0 = Request('http://example-%d.com' % idx)\n+                res0 = Response(req0.url, status=status, headers=headers)\n+                # cache fresh response\n+                res1 = self._process_requestresponse(mw, req0, res0)\n+                self.assertEqualResponse(res1, res0)\n+                assert 'cached' not in res1.flags\n+                # return fresh cached response without network interaction\n+                res2 = self._process_requestresponse(mw, req0, None)\n+                self.assertEqualResponse(res1, res2)\n+                assert 'cached' in res2.flags\n \n if __name__ == '__main__':\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ffc7b7fd6c22264b5cd6f25c5165e04f0f070cff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 107 | Lines Deleted: 9 | Files Changed: 4 | Hunks: 12 | Methods Changed: 15 | Complexity Δ (Sum/Max): 20/7 | Churn Δ: 116 | Churn Cumulative: 527 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 14 | DMM Complexity: 0.8076923076923077\n\nDIFF:\n@@ -5,16 +5,30 @@ from operator import itemgetter\n import six\n from six.moves.configparser import SafeConfigParser\n \n+from scrapy.utils.deprecate import update_classpath\n \n-def build_component_list(base, custom):\n+\n+def build_component_list(base, custom, convert=update_classpath):\n     \"\"\"Compose a component list based on a custom and base dict of components\n     (typically middlewares or extensions), unless custom is already a list, in\n     which case it's returned.\n     \"\"\"\n+\n+    def _check_components(complist):\n+        if len({convert(c) for c in complist}) != len(complist):\n+            raise ValueError('Some paths in {!r} convert to the same object, '\n+                             'please update your settings'.format(complist))\n+\n     if isinstance(custom, (list, tuple)):\n-        return custom\n-    compdict = base.copy()\n-    compdict.update(custom)\n+        _check_components(custom)\n+        return type(custom)(convert(c) for c in custom)\n+\n+    def _map_keys(compdict):\n+        _check_components(compdict)\n+        return {convert(k): v for k, v in six.iteritems(compdict)}\n+\n+    compdict = _map_keys(base)\n+    compdict.update(_map_keys(custom))\n     items = (x for x in six.iteritems(compdict) if x[1] is not None)\n     return [x[0] for x in sorted(items, key=itemgetter(1))]\n \n\n@@ -121,3 +121,37 @@ def _clspath(cls, forced=None):\n     if forced is not None:\n         return forced\n     return '{}.{}'.format(cls.__module__, cls.__name__)\n+\n+\n+DEPRECATION_RULES = [\n+    ('scrapy.contrib_exp.downloadermiddleware.decompression.', 'scrapy.downloadermiddlewares.decompression.'),\n+    ('scrapy.contrib_exp.iterators.', 'scrapy.utils.iterators.'),\n+    ('scrapy.contrib.downloadermiddleware.', 'scrapy.downloadermiddlewares.'),\n+    ('scrapy.contrib.exporter.', 'scrapy.exporters.'),\n+    ('scrapy.contrib.linkextractors.', 'scrapy.linkextractors.'),\n+    ('scrapy.contrib.loader.processor.', 'scrapy.loader.processors.'),\n+    ('scrapy.contrib.loader.', 'scrapy.loader.'),\n+    ('scrapy.contrib.pipeline.', 'scrapy.pipelines.'),\n+    ('scrapy.contrib.spidermiddleware.', 'scrapy.spidermiddlewares.'),\n+    ('scrapy.contrib.spiders.', 'scrapy.spiders.'),\n+    ('scrapy.contrib.', 'scrapy.extensions.'),\n+    ('scrapy.command.', 'scrapy.commands.'),\n+    ('scrapy.dupefilter.', 'scrapy.dupefilters.'),\n+    ('scrapy.linkextractor.', 'scrapy.linkextractors.'),\n+    ('scrapy.spider.', 'scrapy.spiders.'),\n+    ('scrapy.squeue.', 'scrapy.squeues.'),\n+    ('scrapy.statscol.', 'scrapy.statscollectors.'),\n+    ('scrapy.utils.decorator.', 'scrapy.utils.decorators.'),\n+    ('scrapy.spidermanager.SpiderManager', 'scrapy.spiderloader.SpiderLoader'),\n+]\n+\n+\n+def update_classpath(path):\n+    \"\"\"Update a deprecated path from an object with its new location\"\"\"\n+    for prefix, replacement in DEPRECATION_RULES:\n+        if path.startswith(prefix):\n+            new_path = path.replace(prefix, replacement, 1)\n+            warnings.warn(\"`{}` class is deprecated, use `{}` instead\".format(path, new_path),\n+                          ScrapyDeprecationWarning)\n+            return new_path\n+    return path\n\n@@ -2,16 +2,41 @@ import unittest\n \n from scrapy.utils.conf import build_component_list, arglist_to_dict\n \n-class UtilsConfTestCase(unittest.TestCase):\n \n-    def test_build_component_list(self):\n+class BuildComponentListTest(unittest.TestCase):\n+\n+    def test_build_dict(self):\n         base = {'one': 1, 'two': 2, 'three': 3, 'five': 5, 'six': None}\n         custom = {'two': None, 'three': 8, 'four': 4}\n-        self.assertEqual(build_component_list(base, custom),\n+        self.assertEqual(build_component_list(base, custom, lambda x: x),\n                          ['one', 'four', 'five', 'three'])\n \n+    def test_return_list(self):\n         custom = ['a', 'b', 'c']\n-        self.assertEqual(build_component_list(base, custom), custom)\n+        self.assertEqual(build_component_list(None, custom, lambda x: x),\n+                         custom)\n+\n+    def test_map_dict(self):\n+        custom = {'one': 1, 'two': 2, 'three': 3}\n+        self.assertEqual(build_component_list({}, custom, lambda x: x.upper()),\n+                         ['ONE', 'TWO', 'THREE'])\n+\n+    def test_map_list(self):\n+        custom = ['a', 'b', 'c']\n+        self.assertEqual(build_component_list(None, custom, lambda x: x.upper()),\n+                         ['A', 'B', 'C'])\n+\n+    def test_duplicate_components_in_dict(self):\n+        duplicate_dict = {'one': 1, 'two': 2, 'ONE': 4}\n+        self.assertRaises(ValueError,\n+                          build_component_list, {}, duplicate_dict, lambda x: x.lower())\n+\n+    def test_duplicate_components_in_list(self):\n+        duplicate_list = ['a', 'b', 'a']\n+        self.assertRaises(ValueError,\n+                          build_component_list, None, duplicate_list, lambda x: x)\n+\n+class UtilsConfTestCase(unittest.TestCase):\n \n     def test_arglist_to_dict(self):\n         self.assertEqual(arglist_to_dict(['arg1=val1', 'arg2=val2']),\n\n@@ -3,7 +3,7 @@ from __future__ import absolute_import\n import inspect\n import unittest\n import warnings\n-from scrapy.utils.deprecate import create_deprecated_class\n+from scrapy.utils.deprecate import create_deprecated_class, update_classpath\n \n from tests import mock\n \n@@ -248,3 +248,28 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n                     pass\n \n         self.assertIn(\"Error detecting parent module\", str(w[0].message))\n+\n+\n+@mock.patch('scrapy.utils.deprecate.DEPRECATION_RULES',\n+            [('scrapy.contrib.pipeline.', 'scrapy.pipelines.'),\n+             ('scrapy.contrib.', 'scrapy.extensions.')])\n+class UpdateClassPathTest(unittest.TestCase):\n+\n+    def test_old_path_gets_fixed(self):\n+        with warnings.catch_warnings(record=True) as w:\n+            output = update_classpath('scrapy.contrib.debug.Debug')\n+        self.assertEqual(output, 'scrapy.extensions.debug.Debug')\n+        self.assertEqual(len(w), 1)\n+        self.assertIn(\"scrapy.contrib.debug.Debug\", str(w[0].message))\n+        self.assertIn(\"scrapy.extensions.debug.Debug\", str(w[0].message))\n+\n+    def test_sorted_replacement(self):\n+        with warnings.catch_warnings(record=True):\n+            output = update_classpath('scrapy.contrib.pipeline.Pipeline')\n+        self.assertEqual(output, 'scrapy.pipelines.Pipeline')\n+\n+    def test_unmatched_path_stays_the_same(self):\n+        with warnings.catch_warnings(record=True) as w:\n+            output = update_classpath('scrapy.unmatched.Path')\n+        self.assertEqual(output, 'scrapy.unmatched.Path')\n+        self.assertEqual(len(w), 0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cc2f3e1b46b6036fa78bdcc7686bd4ffa3774dbf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 14 | Churn Cumulative: 188 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,11 +1,13 @@\n import warnings\n import unittest\n \n+import scrapy\n from scrapy.crawler import Crawler, CrawlerRunner, CrawlerProcess\n from scrapy.settings import Settings, default_settings\n from scrapy.spiderloader import SpiderLoader\n from scrapy.utils.spider import DefaultSpider\n from scrapy.utils.misc import load_object\n+from scrapy.extensions.throttle import AutoThrottle\n \n \n class CrawlerTestCase(unittest.TestCase):\n@@ -52,6 +54,18 @@ class CrawlerTestCase(unittest.TestCase):\n         self.assertIsInstance(crawler.settings, Settings)\n \n \n+class SpiderSettingsTestCase(unittest.TestCase):\n+    def test_spider_custom_settings(self):\n+        class MySpider(scrapy.Spider):\n+            name = 'spider'\n+            custom_settings = {\n+                'AUTOTHROTTLE_ENABLED': True\n+            }\n+\n+        crawler = Crawler(MySpider, {})\n+        enabled_exts = [e.__class__ for e in crawler.extensions.middlewares]\n+        self.assertIn(AutoThrottle, enabled_exts)\n+\n \n class SpiderLoaderWithWrongInterface(object):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d42c420a6d45a818f59db71a3f9b30d88625598b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1308 | Contributors (this commit): 8 | Commits (past 90d): 20 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -29,6 +29,7 @@ class Crawler(object):\n \n         self.spidercls = spidercls\n         self.settings = settings.copy()\n+        self.spidercls.update_settings(self.settings)\n \n         self.signals = SignalManager(self)\n         self.stats = load_object(self.settings['STATS_CLASS'])(self)\n@@ -44,9 +45,7 @@ class Crawler(object):\n         self.logformatter = lf_cls.from_crawler(self)\n         self.extensions = ExtensionManager.from_crawler(self)\n \n-        self.spidercls.update_settings(self.settings)\n         self.settings.freeze()\n-\n         self.crawling = False\n         self.spider = None\n         self.engine = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#367ea81e716c5e08d6f47ed84d5b0bfb3211cf0c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 365 | Contributors (this commit): 18 | Commits (past 90d): 13 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -168,7 +168,7 @@ LOG_ENABLED = True\n LOG_ENCODING = 'utf-8'\n LOG_FORMATTER = 'scrapy.logformatter.LogFormatter'\n LOG_FORMAT = '%(asctime)s [%(name)s] %(levelname)s: %(message)s'\n-LOG_DATEFORMAT = '%Y-%m-%d %H:%M:%S%z'\n+LOG_DATEFORMAT = '%Y-%m-%d %H:%M:%S'\n LOG_STDOUT = False\n LOG_LEVEL = 'DEBUG'\n LOG_FILE = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eae25a04d96877a6d74627492307619d38c96e5c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 23/23 | Churn Δ: 7 | Churn Cumulative: 7 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -36,6 +36,7 @@ class MemoryUsage(object):\n         self.limit = crawler.settings.getint('MEMUSAGE_LIMIT_MB')*1024*1024\n         self.warning = crawler.settings.getint('MEMUSAGE_WARNING_MB')*1024*1024\n         self.report = crawler.settings.getbool('MEMUSAGE_REPORT')\n+        self.check_interval = crawler.settings.getfloat('MEMUSAGE_CHECK_INTERVAL_SECONDS', 60.0)\n         self.mail = MailSender.from_settings(crawler.settings)\n         crawler.signals.connect(self.engine_started, signal=signals.engine_started)\n         crawler.signals.connect(self.engine_stopped, signal=signals.engine_stopped)\n@@ -56,15 +57,15 @@ class MemoryUsage(object):\n         self.tasks = []\n         tsk = task.LoopingCall(self.update)\n         self.tasks.append(tsk)\n-        tsk.start(60.0, now=True)\n+        tsk.start(self.check_interval, now=True)\n         if self.limit:\n             tsk = task.LoopingCall(self._check_limit)\n             self.tasks.append(tsk)\n-            tsk.start(60.0, now=True)\n+            tsk.start(self.check_interval, now=True)\n         if self.warning:\n             tsk = task.LoopingCall(self._check_warning)\n             self.tasks.append(tsk)\n-            tsk.start(60.0, now=True)\n+            tsk.start(self.check_interval, now=True)\n \n     def engine_stopped(self):\n         for tsk in self.tasks:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#33d145e2f5a1f470398d96184b4da158e3ef7240", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 17 | Churn Cumulative: 1325 | Contributors (this commit): 8 | Commits (past 90d): 21 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -242,16 +242,25 @@ class CrawlerProcess(CrawlerRunner):\n             # Don't start the reactor if the deferreds are already fired\n             if d.called:\n                 return\n-            d.addBoth(lambda _: self._stop_reactor())\n+            d.addBoth(self._stop_reactor)\n \n-        cache_size = self.settings.getint('DNSCACHE_SIZE') if self.settings.getbool('DNSCACHE_ENABLED') else 0\n-        reactor.installResolver(CachingThreadedResolver(reactor, cache_size,\n-                                                            self.settings.getfloat('DNS_TIMEOUT')))\n+        reactor.installResolver(self._get_dns_resolver())\n         tp = reactor.getThreadPool()\n         tp.adjustPoolsize(maxthreads=self.settings.getint('REACTOR_THREADPOOL_MAXSIZE'))\n         reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n         reactor.run(installSignalHandlers=False)  # blocking call\n \n+    def _get_dns_resolver(self):\n+        if self.settings.getbool('DNSCACHE_ENABLED'):\n+            cache_size = self.settings.getint('DNSCACHE_SIZE')\n+        else:\n+            cache_size = 0\n+        return CachingThreadedResolver(\n+            reactor=reactor,\n+            cache_size=cache_size,\n+            timeout=self.settings.getfloat('DNS_TIMEOUT')\n+        )\n+\n     def _stop_reactor(self, _=None):\n         try:\n             reactor.stop()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#64399d18d81bda9fdb1771627949d36f9f412981", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 1332 | Contributors (this commit): 8 | Commits (past 90d): 22 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -216,7 +216,7 @@ class CrawlerProcess(CrawlerRunner):\n         signame = signal_names[signum]\n         logger.info(\"Received %(signame)s, shutting down gracefully. Send again to force \",\n                     {'signame': signame})\n-        reactor.callFromThread(self.stop)\n+        reactor.callFromThread(self._graceful_stop_reactor)\n \n     def _signal_kill(self, signum, _):\n         install_shutdown_handlers(signal.SIG_IGN)\n@@ -261,6 +261,11 @@ class CrawlerProcess(CrawlerRunner):\n             timeout=self.settings.getfloat('DNS_TIMEOUT')\n         )\n \n+    def _graceful_stop_reactor(self):\n+        d = self.stop()\n+        d.addBoth(self._stop_reactor)\n+        return d\n+\n     def _stop_reactor(self, _=None):\n         try:\n             reactor.stop()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d047665c027d0a88dedc515098d276eed481c134", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 23 | Files Changed: 2 | Hunks: 9 | Methods Changed: 11 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 53 | Churn Cumulative: 1573 | Contributors (this commit): 8 | Commits (past 90d): 33 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -23,8 +23,8 @@ logger = logging.getLogger(__name__)\n \n class Crawler(object):\n \n-    def __init__(self, spidercls, settings):\n-        if isinstance(settings, dict):\n+    def __init__(self, spidercls, settings=None):\n+        if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n \n         self.spidercls = spidercls\n@@ -108,8 +108,8 @@ class CrawlerRunner(object):\n             \":meth:`crawl` and managed by this class.\"\n     )\n \n-    def __init__(self, settings):\n-        if isinstance(settings, dict):\n+    def __init__(self, settings=None):\n+        if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n         self.settings = settings\n         self.spider_loader = _get_spider_loader(settings)\n@@ -205,7 +205,7 @@ class CrawlerProcess(CrawlerRunner):\n     process. See :ref:`run-from-script` for an example.\n     \"\"\"\n \n-    def __init__(self, settings):\n+    def __init__(self, settings=None):\n         super(CrawlerProcess, self).__init__(settings)\n         install_shutdown_handlers(self._signal_shutdown)\n         configure_logging(self.settings)\n\n@@ -10,7 +10,14 @@ from scrapy.utils.misc import load_object\n from scrapy.extensions.throttle import AutoThrottle\n \n \n-class CrawlerTestCase(unittest.TestCase):\n+class BaseCrawlerTest(unittest.TestCase):\n+\n+    def assertOptionIsDefault(self, settings, key):\n+        self.assertIsInstance(settings, Settings)\n+        self.assertEqual(settings[key], getattr(default_settings, key))\n+\n+\n+class CrawlerTestCase(BaseCrawlerTest):\n \n     def setUp(self):\n         self.crawler = Crawler(DefaultSpider, Settings())\n@@ -47,11 +54,11 @@ class CrawlerTestCase(unittest.TestCase):\n     def test_crawler_accepts_dict(self):\n         crawler = Crawler(DefaultSpider, {'foo': 'bar'})\n         self.assertEqual(crawler.settings['foo'], 'bar')\n-        self.assertEqual(\n-            crawler.settings['RETRY_ENABLED'],\n-            default_settings.RETRY_ENABLED\n-        )\n-        self.assertIsInstance(crawler.settings, Settings)\n+        self.assertOptionIsDefault(crawler.settings, 'RETRY_ENABLED')\n+\n+    def test_crawler_accepts_None(self):\n+        crawler = Crawler(DefaultSpider)\n+        self.assertOptionIsDefault(crawler.settings, 'RETRY_ENABLED')\n \n \n class SpiderSettingsTestCase(unittest.TestCase):\n@@ -77,7 +84,7 @@ class CustomSpiderLoader(SpiderLoader):\n     pass\n \n \n-class CrawlerRunnerTestCase(unittest.TestCase):\n+class CrawlerRunnerTestCase(BaseCrawlerTest):\n \n     def test_spider_manager_verify_interface(self):\n         settings = Settings({\n@@ -93,11 +100,11 @@ class CrawlerRunnerTestCase(unittest.TestCase):\n     def test_crawler_runner_accepts_dict(self):\n         runner = CrawlerRunner({'foo': 'bar'})\n         self.assertEqual(runner.settings['foo'], 'bar')\n-        self.assertEqual(\n-            runner.settings['RETRY_ENABLED'],\n-            default_settings.RETRY_ENABLED\n-        )\n-        self.assertIsInstance(runner.settings, Settings)\n+        self.assertOptionIsDefault(runner.settings, 'RETRY_ENABLED')\n+\n+    def test_crawler_runner_accepts_None(self):\n+        runner = CrawlerRunner()\n+        self.assertOptionIsDefault(runner.settings, 'RETRY_ENABLED')\n \n     def test_deprecated_attribute_spiders(self):\n         with warnings.catch_warnings(record=True) as w:\n@@ -119,12 +126,12 @@ class CrawlerRunnerTestCase(unittest.TestCase):\n             self.assertIn('Please use SPIDER_LOADER_CLASS', str(w[0].message))\n \n \n-class CrawlerProcessTest(unittest.TestCase):\n+class CrawlerProcessTest(BaseCrawlerTest):\n     def test_crawler_process_accepts_dict(self):\n         runner = CrawlerProcess({'foo': 'bar'})\n         self.assertEqual(runner.settings['foo'], 'bar')\n-        self.assertEqual(\n-            runner.settings['RETRY_ENABLED'],\n-            default_settings.RETRY_ENABLED\n-        )\n-        self.assertIsInstance(runner.settings, Settings)\n+        self.assertOptionIsDefault(runner.settings, 'RETRY_ENABLED')\n+\n+    def test_crawler_process_accepts_None(self):\n+        runner = CrawlerProcess()\n+        self.assertOptionIsDefault(runner.settings, 'RETRY_ENABLED')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#61dec83f706665e5c5238a17f2efdca3d642d9db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 375 | Contributors (this commit): 19 | Commits (past 90d): 17 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -36,7 +36,7 @@ class MemoryUsage(object):\n         self.limit = crawler.settings.getint('MEMUSAGE_LIMIT_MB')*1024*1024\n         self.warning = crawler.settings.getint('MEMUSAGE_WARNING_MB')*1024*1024\n         self.report = crawler.settings.getbool('MEMUSAGE_REPORT')\n-        self.check_interval = crawler.settings.getfloat('MEMUSAGE_CHECK_INTERVAL_SECONDS', 60.0)\n+        self.check_interval = crawler.settings.getfloat('MEMUSAGE_CHECK_INTERVAL_SECONDS')\n         self.mail = MailSender.from_settings(crawler.settings)\n         crawler.signals.connect(self.engine_started, signal=signals.engine_started)\n         crawler.signals.connect(self.engine_stopped, signal=signals.engine_stopped)\n\n@@ -186,6 +186,7 @@ MAIL_USER = None\n MEMDEBUG_ENABLED = False        # enable memory debugging\n MEMDEBUG_NOTIFY = []            # send memory debugging report by mail at engine shutdown\n \n+MEMUSAGE_CHECK_INTERVAL_SECONDS = 60.0\n MEMUSAGE_ENABLED = False\n MEMUSAGE_LIMIT_MB = 0\n MEMUSAGE_NOTIFY_MAIL = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e575f4444657261cfec39b977f3efbdf0e34961c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 368 | Contributors (this commit): 20 | Commits (past 90d): 15 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -209,7 +209,7 @@ REFERER_ENABLED = True\n \n RETRY_ENABLED = True\n RETRY_TIMES = 2  # initial response + 2 retries = 3 requests\n-RETRY_HTTP_CODES = [500, 502, 503, 504, 400, 408]\n+RETRY_HTTP_CODES = [500, 502, 503, 504, 408]\n RETRY_PRIORITY_ADJUST = -1\n \n ROBOTSTXT_OBEY = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3cbf8a0b2ba58afc83ac30a012ed23852cc60ad4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 1344 | Contributors (this commit): 8 | Commits (past 90d): 24 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -148,7 +148,9 @@ class CrawlerRunner(object):\n         crawler = crawler_or_spidercls\n         if not isinstance(crawler_or_spidercls, Crawler):\n             crawler = self._create_crawler(crawler_or_spidercls)\n+        return self._crawl(crawler, *args, **kwargs)\n \n+    def _crawl(self, crawler, *args, **kwargs):\n         self.crawlers.add(crawler)\n         d = crawler.crawl(*args, **kwargs)\n         self._active.add(d)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9a787893e3dccd8bcbfa6821806fde2e2ef4515e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 25 | Churn Cumulative: 245 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -56,14 +56,17 @@ DEFAULT_LOGGING = {\n }\n \n \n-def configure_logging(settings=None):\n+def configure_logging(settings=None, install_root_handler=True):\n     \"\"\"Initialize and configure default loggers\n \n     This function does:\n       - Route warnings and twisted logging through Python standard logging\n-      - Set FailureFormatter filter on Scrapy logger\n       - Assign DEBUG and ERROR level to Scrapy and Twisted loggers respectively\n-      - Create a handler for the root logger according to given settings\n+      - Route stdout to log if LOG_STDOUT setting is True\n+\n+    When ``install_root_handler`` is True (default), this function also\n+      - Sets FailureFormatter filter on Scrapy logger\n+      - Creates a handler for the root logger according to given settings\n     \"\"\"\n     if not sys.warnoptions:\n         # Route warnings through python logging\n@@ -74,16 +77,20 @@ def configure_logging(settings=None):\n \n     dictConfig(DEFAULT_LOGGING)\n \n-    if isinstance(settings, dict):\n+    if isinstance(settings, dict) or settings is None:\n         settings = Settings(settings)\n \n-    if settings:\n-        logging.root.setLevel(logging.NOTSET)\n-\n     if settings.getbool('LOG_STDOUT'):\n         sys.stdout = StreamLogger(logging.getLogger('stdout'))\n \n-        # Set up the default log handler\n+    if install_root_handler:\n+        logging.root.setLevel(logging.NOTSET)\n+        handler = _get_handler(settings)\n+        logging.root.addHandler(handler)\n+\n+\n+def _get_handler(settings):\n+    \"\"\" Return a log handler object according to settings \"\"\"\n     filename = settings.get('LOG_FILE')\n     if filename:\n         encoding = settings.get('LOG_ENCODING')\n@@ -100,7 +107,7 @@ def configure_logging(settings=None):\n     handler.setFormatter(formatter)\n     handler.setLevel(settings.get('LOG_LEVEL'))\n     handler.addFilter(TopLevelFormatter(['scrapy']))\n-        logging.root.addHandler(handler)\n+    return handler\n \n \n def log_scrapy_info(settings):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1740fcf1a6236ca7b847be2206b40b088dceec8c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 7 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 76 | Churn Cumulative: 107 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,27 +1,71 @@\n+from __future__ import absolute_import\n from scrapy.xlib.pydispatch import dispatcher\n-from scrapy.utils import signal\n+from scrapy.utils import signal as _signal\n+\n \n class SignalManager(object):\n \n     def __init__(self, sender=dispatcher.Anonymous):\n         self.sender = sender\n \n-    def connect(self, *a, **kw):\n-        kw.setdefault('sender', self.sender)\n-        return dispatcher.connect(*a, **kw)\n+    def connect(self, receiver, signal, **kwargs):\n+        \"\"\"\n+        Connect a receiver function to a signal.\n \n-    def disconnect(self, *a, **kw):\n-        kw.setdefault('sender', self.sender)\n-        return dispatcher.disconnect(*a, **kw)\n+        The signal can be any object, although Scrapy comes with some\n+        predefined signals that are documented in the :ref:`topics-signals`\n+        section.\n \n-    def send_catch_log(self, *a, **kw):\n-        kw.setdefault('sender', self.sender)\n-        return signal.send_catch_log(*a, **kw)\n+        :param receiver: the function to be connected\n+        :type receiver: callable\n \n-    def send_catch_log_deferred(self, *a, **kw):\n-        kw.setdefault('sender', self.sender)\n-        return signal.send_catch_log_deferred(*a, **kw)\n+        :param signal: the signal to connect to\n+        :type signal: object\n+        \"\"\"\n+        kwargs.setdefault('sender', self.sender)\n+        return dispatcher.connect(receiver, signal, **kwargs)\n \n-    def disconnect_all(self, *a, **kw):\n-        kw.setdefault('sender', self.sender)\n-        return signal.disconnect_all(*a, **kw)\n+    def disconnect(self, receiver, signal, **kwargs):\n+        \"\"\"\n+        Disconnect a receiver function from a signal. This has the\n+        opposite effect of the :meth:`connect` method, and the arguments\n+        are the same.\n+        \"\"\"\n+        kwargs.setdefault('sender', self.sender)\n+        return dispatcher.disconnect(receiver, signal, **kwargs)\n+\n+    def send_catch_log(self, signal, **kwargs):\n+        \"\"\"\n+        Send a signal, catch exceptions and log them.\n+\n+        The keyword arguments are passed to the signal handlers (connected\n+        through the :meth:`connect` method).\n+        \"\"\"\n+        kwargs.setdefault('sender', self.sender)\n+        return _signal.send_catch_log(signal, **kwargs)\n+\n+    def send_catch_log_deferred(self, signal, **kwargs):\n+        \"\"\"\n+        Like :meth:`send_catch_log` but supports returning `deferred`_ from\n+        signal handlers.\n+\n+        Returns a `deferred`_ that gets fired once all signal handlers\n+        deferreds were fired. Send a signal, catch exceptions and log them.\n+\n+        The keyword arguments are passed to the signal handlers (connected\n+        through the :meth:`connect` method).\n+\n+        .. _deferreds: http://twistedmatrix.com/documents/current/core/howto/defer.html\n+        \"\"\"\n+        kwargs.setdefault('sender', self.sender)\n+        return _signal.send_catch_log_deferred(signal, **kwargs)\n+\n+    def disconnect_all(self, signal, **kwargs):\n+        \"\"\"\n+        Disconnect all receivers from the given signal.\n+\n+        :param signal: the signal to disconnect from\n+        :type signal: object\n+        \"\"\"\n+        kwargs.setdefault('sender', self.sender)\n+        return _signal.disconnect_all(signal, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#66e5591eb9e459f36cec43d4c9e1d8dba82d6cff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 65 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -14,16 +14,16 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"--get\", dest=\"get\", metavar=\"SETTING\", \\\n+        parser.add_option(\"--get\", dest=\"get\", metavar=\"SETTING\",\n             help=\"print raw setting value\")\n-        parser.add_option(\"--getbool\", dest=\"getbool\", metavar=\"SETTING\", \\\n-            help=\"print setting value, intepreted as a boolean\")\n-        parser.add_option(\"--getint\", dest=\"getint\", metavar=\"SETTING\", \\\n-            help=\"print setting value, intepreted as an integer\")\n-        parser.add_option(\"--getfloat\", dest=\"getfloat\", metavar=\"SETTING\", \\\n-            help=\"print setting value, intepreted as an float\")\n-        parser.add_option(\"--getlist\", dest=\"getlist\", metavar=\"SETTING\", \\\n-            help=\"print setting value, intepreted as an float\")\n+        parser.add_option(\"--getbool\", dest=\"getbool\", metavar=\"SETTING\",\n+            help=\"print setting value, interpreted as a boolean\")\n+        parser.add_option(\"--getint\", dest=\"getint\", metavar=\"SETTING\",\n+            help=\"print setting value, interpreted as an integer\")\n+        parser.add_option(\"--getfloat\", dest=\"getfloat\", metavar=\"SETTING\",\n+            help=\"print setting value, interpreted as a float\")\n+        parser.add_option(\"--getlist\", dest=\"getlist\", metavar=\"SETTING\",\n+            help=\"print setting value, interpreted as a list\")\n \n     def run(self, args, opts):\n         settings = self.crawler_process.settings\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8a48d9c6a87211b3424ac7f2714c50915849fac0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 10 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 28 | Churn Cumulative: 144 | Contributors (this commit): 6 | Commits (past 90d): 8 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -111,7 +111,8 @@ class SgmlLinkExtractor(FilteringLinkExtractor):\n         tag_func = lambda x: x in tags\n         attr_func = lambda x: x in attrs\n \n-        with warnings.catch_warnings(record=True):\n+        with warnings.catch_warnings():\n+            warnings.simplefilter('ignore', ScrapyDeprecationWarning)\n             lx = BaseSgmlLinkExtractor(tag=tag_func, attr=attr_func,\n                 unique=unique, process_value=process_value)\n \n\n@@ -490,21 +490,24 @@ class DeprecatedXpathSelectorTest(unittest.TestCase):\n             self.assertTrue(isinstance(usel, XPathSelector))\n \n     def test_xpathselector(self):\n-        with warnings.catch_warnings(record=True):\n+        with warnings.catch_warnings():\n+            warnings.simplefilter('ignore', ScrapyDeprecationWarning)\n             hs = XPathSelector(text=self.text)\n             self.assertEqual(hs.select(\"//div\").extract(),\n                              [u'<div><img src=\"a.jpg\"><p>Hello</p></div>'])\n             self.assertRaises(RuntimeError, hs.css, 'div')\n \n     def test_htmlxpathselector(self):\n-        with warnings.catch_warnings(record=True):\n+        with warnings.catch_warnings():\n+            warnings.simplefilter('ignore', ScrapyDeprecationWarning)\n             hs = HtmlXPathSelector(text=self.text)\n             self.assertEqual(hs.select(\"//div\").extract(),\n                              [u'<div><img src=\"a.jpg\"><p>Hello</p></div>'])\n             self.assertRaises(RuntimeError, hs.css, 'div')\n \n     def test_xmlxpathselector(self):\n-        with warnings.catch_warnings(record=True):\n+        with warnings.catch_warnings():\n+            warnings.simplefilter('ignore', ScrapyDeprecationWarning)\n             xs = XmlXPathSelector(text=self.text)\n             self.assertEqual(xs.select(\"//div\").extract(),\n                              [u'<div><img src=\"a.jpg\"><p>Hello</p></img></div>'])\n\n@@ -3,6 +3,7 @@ from __future__ import absolute_import\n import inspect\n import unittest\n import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.deprecate import create_deprecated_class, update_classpath\n \n from tests import mock\n@@ -109,7 +110,8 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n                                              warn_category=MyWarning)\n \n         # ignore subclassing warnings\n-        with warnings.catch_warnings(record=True):\n+        with warnings.catch_warnings():\n+            warnings.simplefilter('ignore', ScrapyDeprecationWarning)\n             class UserClass(Deprecated):\n                 pass\n \n@@ -138,7 +140,8 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n         self.assertIn(\"tests.test_utils_deprecate.Deprecated\", msg)\n \n     def test_issubclass(self):\n-        with warnings.catch_warnings(record=True):\n+        with warnings.catch_warnings():\n+            warnings.simplefilter('ignore', ScrapyDeprecationWarning)\n             DeprecatedName = create_deprecated_class('DeprecatedName', NewName)\n \n             class UpdatedUserClass1(NewName):\n@@ -173,7 +176,8 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n         self.assertRaises(TypeError, issubclass, object(), DeprecatedName)\n \n     def test_isinstance(self):\n-        with warnings.catch_warnings(record=True):\n+        with warnings.catch_warnings():\n+            warnings.simplefilter('ignore', ScrapyDeprecationWarning)\n             DeprecatedName = create_deprecated_class('DeprecatedName', NewName)\n \n             class UpdatedUserClass2(NewName):\n@@ -206,7 +210,8 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n         assert not isinstance(OldStyleClass(), DeprecatedName)\n \n     def test_clsdict(self):\n-        with warnings.catch_warnings(record=True):\n+        with warnings.catch_warnings():\n+            warnings.simplefilter('ignore', ScrapyDeprecationWarning)\n             Deprecated = create_deprecated_class('Deprecated', NewName, {'foo': 'bar'})\n \n         self.assertEqual(Deprecated.foo, 'bar')\n@@ -264,7 +269,8 @@ class UpdateClassPathTest(unittest.TestCase):\n         self.assertIn(\"scrapy.extensions.debug.Debug\", str(w[0].message))\n \n     def test_sorted_replacement(self):\n-        with warnings.catch_warnings(record=True):\n+        with warnings.catch_warnings():\n+            warnings.simplefilter('ignore', ScrapyDeprecationWarning)\n             output = update_classpath('scrapy.contrib.pipeline.Pipeline')\n         self.assertEqual(output, 'scrapy.pipelines.Pipeline')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
