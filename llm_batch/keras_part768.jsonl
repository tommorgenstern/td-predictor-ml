{"custom_id": "keras#866b745ebdafb29248f6a0946fbed1ce1cbcba90", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 375 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,11 +1,11 @@\n \"\"\"\n Title: MobileViT: A mobile-friendly Transformer-based model for image classification\n Author: [Sayak Paul](https://twitter.com/RisingSayak)\n-Converted to Keras 3 by: [Pavan Kumar Singh](https://github.com/pksX01)\n Date created: 2021/10/20\n Last modified: 2023/11/23\n Description: MobileViT for image classification with combined benefits of convolutions and Transformers.\n Accelerator: GPU\n+Converted to Keras 3 by: [Pavan Kumar Singh](https://github.com/pksX01)\n \"\"\"\n \"\"\"\n ## Introduction\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9c675a9a45e5e8244163fea82efc6066722608a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 18 | Churn Cumulative: 62 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 0.9090909090909091\n\nDIFF:\n@@ -119,6 +119,7 @@ def affine_transform(\n         interpolation=interpolation.upper(),\n         fill_mode=fill_mode.upper(),\n     )\n+    affined = tf.ensure_shape(affined, image.shape)\n \n     if data_format == \"channels_first\":\n         affined = tf.transpose(affined, (0, 3, 1, 2))\n\n@@ -1,4 +1,5 @@\n import numpy as np\n+import pytest\n from absl.testing import parameterized\n from tensorflow import data as tf_data\n \n@@ -132,3 +133,19 @@ class RandomZoomTest(testing.TestCase, parameterized.TestCase):\n         )(inputs)\n         model = models.Model(inputs, outputs)\n         model.predict(np.random.random((1, 6, 6, 3)))\n+\n+    @pytest.mark.skipif(\n+        backend.backend() == \"numpy\",\n+        reason=\"The NumPy backend does not implement fit.\",\n+    )\n+    def test_connect_with_flatten(self):\n+        model = models.Sequential(\n+            [\n+                layers.RandomZoom((-0.5, 0.0), (-0.5, 0.0)),\n+                layers.Flatten(),\n+                layers.Dense(1, activation=\"relu\"),\n+            ],\n+        )\n+\n+        model.compile(loss=\"mse\")\n+        model.fit(np.random.random((2, 2, 2, 1)), y=np.random.random((2,)))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#86a2ed7e6ef0f94c89c0d643dc7e63ed79cc433e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 10 | Files Changed: 3 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 27 | Churn Cumulative: 1247 | Contributors (this commit): 6 | Commits (past 90d): 24 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -296,6 +296,7 @@ def cumsum(x, axis=None, dtype=None):\n \n \n def diag(x, k=0):\n+    x = convert_to_tensor(x)\n     return jnp.diag(x, k=k)\n \n \n\n@@ -1,5 +1,7 @@\n import numpy as np\n \n+from keras import layers\n+from keras import models\n from keras import testing\n from keras.metrics import iou_metrics as metrics\n \n@@ -93,6 +95,16 @@ class IoUTest(testing.TestCase):\n         expected_result = (1 / (1 + 1 - 1)) / 1\n         self.assertAllClose(result, expected_result, atol=1e-3)\n \n+    def test_compilation(self):\n+        m_obj = metrics.MeanIoU(num_classes=2)\n+        model = models.Sequential(\n+            [\n+                layers.Dense(2, activation=\"softmax\"),\n+            ]\n+        )\n+        model.compile(optimizer=\"rmsprop\", loss=\"mse\", metrics=[m_obj])\n+        model.fit(np.array([[1.0, 1.0]]), np.array([[1.0, 0.0]]))\n+\n \n class BinaryIoUTest(testing.TestCase):\n     def test_config(self):\n\n@@ -615,7 +615,7 @@ def _filter_top_k(x, k):\n def confusion_matrix(\n     labels,\n     predictions,\n-    num_classes=None,\n+    num_classes,\n     weights=None,\n     dtype=\"int32\",\n ):\n@@ -652,9 +652,8 @@ def confusion_matrix(\n     Args:\n         labels: 1-D tensor of real labels for the classification task.\n         predictions: 1-D tensor of predictions for a given classification.\n-        num_classes: The possible number of labels the classification task can\n-                    have. If this value is not provided, it will be calculated\n-                    using both predictions and labels array.\n+        num_classes: The possible number of labels the classification\n+            task can have.\n         weights: An optional tensor whose shape matches `predictions`.\n         dtype: Data type of the confusion matrix.\n \n@@ -670,11 +669,6 @@ def confusion_matrix(\n     predictions = ops.cast(predictions, dtype)\n     labels = ops.cast(labels, dtype)\n \n-    if num_classes is None:\n-        num_classes = ops.maximum(ops.max(predictions), ops.max(labels)) + 1\n-    else:\n-        num_classes = ops.cast(num_classes, dtype)\n-\n     if weights is not None:\n         weights = ops.convert_to_tensor(weights, dtype)\n \n@@ -682,6 +676,6 @@ def confusion_matrix(\n     values = ops.ones_like(predictions, dtype) if weights is None else weights\n     indices = ops.cast(indices, dtype=\"int64\")\n     values = ops.cast(values, dtype=dtype)\n-    num_classes = ops.cast(num_classes, \"int64\")\n+    num_classes = int(num_classes)\n     confusion_matrix = ops.scatter(indices, values, (num_classes, num_classes))\n     return confusion_matrix\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#331325055407a705745918d683e504f301ad4c9c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1472 | Contributors (this commit): 5 | Commits (past 90d): 23 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -4814,9 +4814,6 @@ def split(x, indices_or_sections, axis=0):\n \n     Args:\n         x: Input tensor.\n-        indices_or_sections: Either an integer indicating the number of\n-            sections along `axis` or a list of integers indicating the indices\n-            along `axis` at which the tensor is split.\n         indices_or_sections: If an integer, N, the tensor will be split into N\n             equal sections along `axis`. If a 1-D array of sorted integers,\n             the entries indicate indices at which the tensor will be split\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#df3394d0ee2c33268390811ee039448788150fcd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 364 | Lines Deleted: 193 | Files Changed: 6 | Hunks: 67 | Methods Changed: 48 | Complexity Δ (Sum/Max): 22/25 | Churn Δ: 557 | Churn Cumulative: 4220 | Contributors (this commit): 9 | Commits (past 90d): 64 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -101,7 +101,9 @@ class Variable(\n \n def convert_to_tensor(x, dtype=None, sparse=None):\n     if isinstance(x, tf.SparseTensor) and sparse is not None and not sparse:\n+        x_shape = x.shape\n         x = tf.sparse.to_dense(x)\n+        x.set_shape(x_shape)\n     if dtype is not None:\n         dtype = standardize_dtype(dtype)\n     if not tf.is_tensor(x):\n@@ -119,7 +121,9 @@ def convert_to_tensor(x, dtype=None, sparse=None):\n \n def convert_to_numpy(x):\n     if isinstance(x, tf.SparseTensor):\n+        x_shape = x.shape\n         x = tf.sparse.to_dense(x)\n+        x.set_shape(x_shape)\n     elif isinstance(x, tf.IndexedSlices):\n         x = tf.convert_to_tensor(x)\n     return np.array(x)\n\n@@ -52,21 +52,21 @@ def bincount(x, weights=None, minlength=0):\n     else:\n         dtype = \"int32\"\n     if isinstance(x, tf.SparseTensor):\n-        result = tf.sparse.bincount(\n+        output = tf.sparse.bincount(\n             x,\n             weights=weights,\n             minlength=minlength,\n             axis=-1,\n         )\n-        result = tf.cast(result, dtype)\n+        output = tf.cast(output, dtype)\n         if x.shape.rank == 1:\n             output_shape = (minlength,)\n         else:\n-            batch_size = tf.shape(result)[0]\n+            batch_size = tf.shape(output)[0]\n             output_shape = (batch_size, minlength)\n         return tf.SparseTensor(\n-            indices=result.indices,\n-            values=result.values,\n+            indices=output.indices,\n+            values=output.values,\n             dense_shape=output_shape,\n         )\n     return tf.cast(\n@@ -109,6 +109,8 @@ def subtract(x1, x2):\n \n \n def matmul(x1, x2):\n+    x1_shape = x1.shape\n+    x2_shape = x2.shape\n     x1 = convert_to_tensor(x1)\n     x2 = convert_to_tensor(x2)\n     # TODO: GPU and XLA only support float types\n@@ -117,15 +119,29 @@ def matmul(x1, x2):\n     x1 = tf.cast(x1, compute_dtype)\n     x2 = tf.cast(x2, compute_dtype)\n \n-    def with_combined_batch_dimensions(a, b, fn_3d):\n-        batch_shape = (\n-            b.shape[:-2] if isinstance(b, tf.SparseTensor) else a.shape[:-2]\n-        )\n+    def with_combined_batch_dimensions(a, b, output_shape, fn_3d):\n+        a_sparse = isinstance(a, tf.SparseTensor)\n+        b_sparse = isinstance(b, tf.SparseTensor)\n+        batch_shape = b.shape[:-2] if b_sparse else a.shape[:-2]\n         batch_size = math.prod(batch_shape)\n-        a_3d = reshape(a, [batch_size] + a.shape[-2:])\n-        b_3d = reshape(b, [batch_size] + b.shape[-2:])\n-        result = fn_3d(a_3d, b_3d)\n-        return reshape(result, batch_shape + result.shape[1:])\n+        a3d_shape = [batch_size] + a.shape[-2:]\n+        a_3d = (\n+            tf.sparse.reshape(a, a3d_shape)\n+            if a_sparse\n+            else tf.reshape(a, a3d_shape)\n+        )\n+        b3d_shape = [batch_size] + b.shape[-2:]\n+        b_3d = (\n+            tf.sparse.reshape(b, b3d_shape)\n+            if b_sparse\n+            else tf.reshape(b, b3d_shape)\n+        )\n+        result_3d = fn_3d(a_3d, b_3d)\n+        return (\n+            tf.sparse.reshape(result_3d, output_shape)\n+            if isinstance(result_3d, tf.SparseTensor)\n+            else tf.reshape(result_3d, output_shape)\n+        )\n \n     def sparse_sparse_matmul(a, b):\n         dtype = a.values.dtype\n@@ -172,31 +188,37 @@ def matmul(x1, x2):\n \n     x1_sparse = isinstance(x1, tf.SparseTensor)\n     x2_sparse = isinstance(x2, tf.SparseTensor)\n-    if x1_sparse and x2_sparse:\n-        if x1.shape.rank <= 3:\n-            result = sparse_sparse_matmul(x1, x2)\n-        else:\n-            result = with_combined_batch_dimensions(\n-                x1, x2, sparse_sparse_matmul\n-            )\n-    elif x1_sparse or x2_sparse:\n-        # Sparse * dense or dense * sparse\n-        sparse_rank = x1.shape.rank if x1_sparse else x2.shape.rank\n+    if x1_sparse or x2_sparse:\n+        from keras.ops.operation_utils import compute_matmul_output_shape\n \n-        # Special case: embedding_lookup_sparse for sparse * dense and rank 2\n-        if x1_sparse and sparse_rank == 2:\n-            result = embedding_lookup_sparse_dense_matmul(x1, x2)\n-        elif sparse_rank == 2:\n-            result = tf.sparse.sparse_dense_matmul(x1, x2)\n-        elif sparse_rank == 3:\n-            result = sparse_dense_matmul_3d(x1, x2)\n+        output_shape = compute_matmul_output_shape(x1_shape, x2_shape)\n+        if x1_sparse and x2_sparse:\n+            if x1_shape.rank <= 3:\n+                output = sparse_sparse_matmul(x1, x2)\n             else:\n-            result = with_combined_batch_dimensions(\n-                x1, x2, sparse_dense_matmul_3d\n+                output = with_combined_batch_dimensions(\n+                    x1, x2, output_shape, sparse_sparse_matmul\n                 )\n         else:\n-        result = tfnp.matmul(x1, x2)\n-    return tf.cast(result, result_dtype)\n+            # Sparse * dense or dense * sparse\n+            sparse_rank = x1_shape.rank if x1_sparse else x2_shape.rank\n+\n+            # Special case: embedding_lookup_sparse for sparse * dense, rank 2\n+            if x1_sparse and sparse_rank == 2:\n+                output = embedding_lookup_sparse_dense_matmul(x1, x2)\n+            elif sparse_rank == 2:\n+                output = tf.sparse.sparse_dense_matmul(x1, x2)\n+            elif sparse_rank == 3:\n+                output = sparse_dense_matmul_3d(x1, x2)\n+            else:\n+                output = with_combined_batch_dimensions(\n+                    x1, x2, output_shape, sparse_dense_matmul_3d\n+                )\n+        output = tf.cast(output, result_dtype)\n+        output.set_shape(output_shape)\n+        return output\n+    else:\n+        return tf.cast(tfnp.matmul(x1, x2), result_dtype)\n \n \n @sparse.elementwise_binary_intersection\n@@ -286,8 +308,8 @@ def mean(x, axis=None, keepdims=False):\n         result_dtype = compute_dtype\n     else:\n         result_dtype = ori_dtype\n-    result = tfnp.mean(x, axis=axis, keepdims=keepdims, dtype=compute_dtype)\n-    return tf.cast(result, result_dtype)\n+    output = tfnp.mean(x, axis=axis, keepdims=keepdims, dtype=compute_dtype)\n+    return tf.cast(output, result_dtype)\n \n \n def max(x, axis=None, keepdims=False, initial=None):\n@@ -521,7 +543,9 @@ def concatenate(xs, axis=0):\n             return tf.sparse.concat(axis=axis, sp_inputs=xs)\n         else:\n             xs = [\n-                tf.sparse.to_dense(x) if isinstance(x, tf.SparseTensor) else x\n+                convert_to_tensor(x, sparse=False)\n+                if isinstance(x, tf.SparseTensor)\n+                else x\n                 for x in xs\n             ]\n     xs = tf.nest.map_structure(convert_to_tensor, xs)\n@@ -642,11 +666,13 @@ def digitize(x, bins):\n             lambda y: tf.raw_ops.Bucketize(input=y, boundaries=bins), x\n         )\n     elif isinstance(x, tf.SparseTensor):\n-        return tf.SparseTensor(\n+        output = tf.SparseTensor(\n             indices=tf.identity(x.indices),\n             values=tf.raw_ops.Bucketize(input=x.values, boundaries=bins),\n             dense_shape=tf.identity(x.dense_shape),\n         )\n+        output.set_shape(x.shape)\n+        return output\n     return tf.raw_ops.Bucketize(input=x, boundaries=bins)\n \n \n@@ -688,7 +714,11 @@ def exp(x):\n \n def expand_dims(x, axis):\n     if isinstance(x, tf.SparseTensor):\n-        return tf.sparse.expand_dims(x, axis)\n+        from keras.ops.operation_utils import compute_expand_dims_output_shape\n+\n+        output = tf.sparse.expand_dims(x, axis)\n+        output.set_shape(compute_expand_dims_output_shape(x.shape, axis))\n+        return output\n     return tfnp.expand_dims(x, axis)\n \n \n@@ -1238,7 +1268,14 @@ def repeat(x, repeats, axis=None):\n \n def reshape(x, new_shape):\n     if isinstance(x, tf.SparseTensor):\n-        return tf.sparse.reshape(x, new_shape)\n+        from keras.ops.operation_utils import compute_reshape_output_shape\n+\n+        output_shape = compute_reshape_output_shape(\n+            x.shape, new_shape, \"new_shape\"\n+        )\n+        output = tf.sparse.reshape(x, new_shape)\n+        output.set_shape(output_shape)\n+        return output\n     return tfnp.reshape(x, new_shape)\n \n \n@@ -1325,7 +1362,9 @@ def take(x, indices, axis=None):\n                 f\"`x.dtype={x.dtype}` when `indices` is a sparse tensor; \"\n                 \"densifying `indices`.\"\n             )\n-            return tfnp.take(x, tf.sparse.to_dense(indices), axis=axis)\n+            return tfnp.take(\n+                x, convert_to_tensor(indices, sparse=False), axis=axis\n+            )\n         if axis is None:\n             x = tf.reshape(x, (-1,))\n         elif axis != 0:\n@@ -1334,12 +1373,16 @@ def take(x, indices, axis=None):\n                 f\"`axis={axis}` when `indices` is a sparse tensor; \"\n                 \"densifying `indices`.\"\n             )\n-            return tfnp.take(x, tf.sparse.to_dense(indices), axis=axis)\n-        return tf.nn.safe_embedding_lookup_sparse(\n+            return tfnp.take(\n+                x, convert_to_tensor(indices, sparse=False), axis=axis\n+            )\n+        output = tf.nn.safe_embedding_lookup_sparse(\n             embedding_weights=tf.convert_to_tensor(x),\n             sparse_ids=tf.sparse.expand_dims(indices, axis=-1),\n             default_id=0,\n         )\n+        output.set_shape(indices.shape + output.shape[len(indices.shape) :])\n+        return output\n     return tfnp.take(x, indices, axis=axis)\n \n \n@@ -1540,21 +1583,19 @@ def sqrt(x):\n \n def squeeze(x, axis=None):\n     if isinstance(x, tf.SparseTensor):\n-        new_shape = list(x.shape)\n-        gather_indices = list(range(len(new_shape)))\n-        if axis is None:\n-            for i in range(len(new_shape) - 1, -1, -1):\n-                if new_shape[i] == 1:\n-                    del new_shape[i]\n-                    del gather_indices[i]\n-        else:\n-            if new_shape[axis] != 1:\n+        static_shape = x.shape.as_list()\n+        if axis is not None and static_shape[axis] != 1:\n             raise ValueError(\n                 f\"Cannot squeeze axis {axis}, because the \"\n                 \"dimension is not 1.\"\n             )\n-            del new_shape[axis]\n-            del gather_indices[axis]\n+        dynamic_shape = tf.shape(x)\n+        new_shape = []\n+        gather_indices = []\n+        for i, dim in enumerate(static_shape):\n+            if not (dim == 1 if axis is None else i == axis):\n+                new_shape.append(dim if dim is not None else dynamic_shape[i])\n+                gather_indices.append(i)\n         new_indices = tf.gather(x.indices, gather_indices, axis=1)\n         return tf.SparseTensor(new_indices, x.values, tuple(new_shape))\n     return tfnp.squeeze(x, axis=axis)\n@@ -1562,7 +1603,11 @@ def squeeze(x, axis=None):\n \n def transpose(x, axes=None):\n     if isinstance(x, tf.SparseTensor):\n-        return tf.sparse.transpose(x, perm=axes)\n+        from keras.ops.operation_utils import compute_transpose_output_shape\n+\n+        output = tf.sparse.transpose(x, perm=axes)\n+        output.set_shape(compute_transpose_output_shape(x.shape, axes))\n+        return output\n     return tfnp.transpose(x, axes=axes)\n \n \n\n@@ -13,6 +13,26 @@ def empty_tensor(shape, dtype):\n     return tf.reshape(tf.convert_to_tensor((), dtype=dtype), shape=shape)\n \n \n+def sparse_to_dense(x, default_value=None):\n+    x_shape = x.shape\n+    x = tf.sparse.to_dense(x, default_value=default_value)\n+    x.set_shape(x_shape)\n+    return x\n+\n+\n+def sparse_with_values(x, values):\n+    x_shape = x.shape\n+    x = tf.SparseTensor(x.indices, values, x.dense_shape)\n+    x.set_shape(x_shape)\n+    return x\n+\n+\n+def broadcast_scalar_to_sparse_shape(scalar, sparse):\n+    output = tf.broadcast_to(scalar, sparse.dense_shape)\n+    output.set_shape(sparse.shape)\n+    return output\n+\n+\n def sparse_subtract(x1, x2):\n     \"\"\"Subtraction for `tf.SparseTensor`s.\n \n@@ -164,7 +184,7 @@ def sparse_intersection_indices_and_values(x1, x2):\n \n     def empty_intersection():\n         return (\n-            empty_tensor((0, x1.indices.shape[1]), dtype=tf.int64),\n+            empty_tensor((0, x1.shape.rank), dtype=tf.int64),\n             empty_tensor((0,), dtype=x1.values.dtype),\n             empty_tensor((0,), dtype=x2.values.dtype),\n         )\n@@ -300,10 +320,10 @@ def densifying_unary(default_value):\n         @functools.wraps(func)\n         def sparse_wrapper(x, *args, **kwargs):\n             if isinstance(x, tf.SparseTensor):\n-                sparse_output = tf.SparseTensor(\n-                    x.indices, func(x.values, *args, **kwargs), x.dense_shape\n+                sparse_output = sparse_with_values(\n+                    x, func(x.values, *args, **kwargs)\n                 )\n-                return tf.sparse.to_dense(\n+                return sparse_to_dense(\n                     sparse_output,\n                     tf.cast(default_value, sparse_output.values.dtype),\n                 )\n@@ -356,9 +376,7 @@ def elementwise_unary(func):\n     @functools.wraps(func)\n     def sparse_wrapper(x, *args, **kwargs):\n         if isinstance(x, tf.SparseTensor):\n-            return tf.SparseTensor(\n-                x.indices, func(x.values, *args, **kwargs), x.dense_shape\n-            )\n+            return sparse_with_values(x, func(x.values, *args, **kwargs))\n         elif isinstance(x, tf.IndexedSlices):\n             return tf.IndexedSlices(\n                 func(x.values, *args, **kwargs), x.indices, x.dense_shape\n@@ -412,30 +430,30 @@ def elementwise_binary_union(sparse_op, densify_mixed=False):\n                 if isinstance(x2, tf.SparseTensor):\n                     # x1 is a SparseTensor and x2 is a SparseTensor.\n                     if x1.indices is x2.indices:\n-                        return tf.SparseTensor(\n-                            x1.indices,\n-                            func(x1.values, x2.values),\n-                            x1.dense_shape,\n+                        return sparse_with_values(\n+                            x1, func(x1.values, x2.values)\n                         )\n                     else:\n-                        return sparse_op(x1, x2)\n+                        output = sparse_op(x1, x2)\n+                        output.set_shape(x1.shape)\n+                        return output\n                 else:\n                     # x1 is a SparseTensor.\n                     if densify_mixed:\n-                        x1 = tf.sparse.to_dense(x1)\n+                        x1 = sparse_to_dense(x1)\n                     else:\n                         if not hasattr(x2, \"shape\") or len(x2.shape) == 0:\n                             # x2 is a scalar, broadcast.\n-                            x2 = tf.broadcast_to(x2, x1.dense_shape)\n+                            x2 = broadcast_scalar_to_sparse_shape(x2, x1)\n                         return sparse_op(x1, x2)\n             elif isinstance(x2, tf.SparseTensor):\n                 # x2 is a SparseTensor.\n                 if densify_mixed:\n-                    x2 = tf.sparse.to_dense(x2)\n+                    x2 = sparse_to_dense(x2)\n                 else:\n                     if not hasattr(x1, \"shape\") or len(x1.shape) == 0:\n                         # x1 is a scalar, broadcast.\n-                        x1 = tf.broadcast_to(x1, x2.dense_shape)\n+                        x1 = broadcast_scalar_to_sparse_shape(x1, x2)\n                     return sparse_op(x1, x2)\n             elif isinstance(x1, tf.IndexedSlices):\n                 if isinstance(x2, tf.IndexedSlices):\n@@ -513,9 +531,7 @@ def elementwise_binary_intersection(func):\n             if isinstance(x2, tf.SparseTensor):\n                 # x1 is a SparseTensor and x2 is a SparseTensor.\n                 if x1.indices is x2.indices:\n-                    return tf.SparseTensor(\n-                        x1.indices, func(x1.values, x2.values), x1.dense_shape\n-                    )\n+                    return sparse_with_values(x1, func(x1.values, x2.values))\n                 else:\n                     # Compute the intersection of indices.\n                     (\n@@ -524,7 +540,7 @@ def elementwise_binary_intersection(func):\n                         x2_values_for_intersection,\n                     ) = sparse_intersection_indices_and_values(x1, x2)\n                     # Now, it is an element-wise operation on the intersection.\n-                    return tf.SparseTensor(\n+                    output = tf.SparseTensor(\n                         intersection_indices,\n                         func(\n                             x1_values_for_intersection,\n@@ -532,37 +548,27 @@ def elementwise_binary_intersection(func):\n                         ),\n                         x1.dense_shape,\n                     )\n+                    output.set_shape(x1.shape)\n+                    return output\n             else:\n                 # x1 is a SparseTensor.\n                 if not hasattr(x2, \"shape\") or len(x2.shape) == 0:\n                     # x2 is a scalar, apply func element-wise.\n-                    return tf.SparseTensor(\n-                        x1.indices,\n-                        func(x1.values, x2),\n-                        x1.dense_shape,\n-                    )\n+                    return sparse_with_values(x1, func(x1.values, x2))\n                 else:\n                     # x2 is dense, gather values from x1 indices.\n-                    return tf.SparseTensor(\n-                        x1.indices,\n-                        func(x1.values, tf.gather_nd(x2, x1.indices)),\n-                        x1.dense_shape,\n+                    return sparse_with_values(\n+                        x1, func(x1.values, tf.gather_nd(x2, x1.indices))\n                     )\n         elif isinstance(x2, tf.SparseTensor):\n             # x2 is a SparseTensor.\n             if not hasattr(x1, \"shape\") or len(x1.shape) == 0:\n                 # x1 is a scalar, apply func element-wise.\n-                return tf.SparseTensor(\n-                    x2.indices,\n-                    func(x1, x2.values),\n-                    x2.dense_shape,\n-                )\n+                return sparse_with_values(x2, func(x1, x2.values))\n             else:\n                 # x1 is dense, gather values from x2 indices.\n-                return tf.SparseTensor(\n-                    x2.indices,\n-                    func(tf.gather_nd(x1, x2.indices), x2.values),\n-                    x2.dense_shape,\n+                return sparse_with_values(\n+                    x2, func(tf.gather_nd(x1, x2.indices), x2.values)\n                 )\n         elif isinstance(x1, tf.IndexedSlices):\n             if isinstance(x2, tf.IndexedSlices):\n@@ -654,17 +660,13 @@ def elementwise_division(func):\n                 # x1 is a SparseTensor and x2 is a SparseTensor.\n                 # Divisor is sparse, meaning we're doing divisions by zero\n                 # outside of x2.indices, so the result is dense. Densify both.\n-                x1 = tf.sparse.to_dense(x1)\n-                x2 = tf.sparse.to_dense(x2)\n+                x1 = sparse_to_dense(x1)\n+                x2 = sparse_to_dense(x2)\n             else:\n                 # x1 is a SparseTensor.\n                 if not hasattr(x2, \"shape\") or len(x2.shape) == 0:\n                     # x2 is a scalar, apply func element-wise.\n-                    return tf.SparseTensor(\n-                        x1.indices,\n-                        func(x1.values, x2),\n-                        x1.dense_shape,\n-                    )\n+                    return sparse_with_values(x1, func(x1.values, x2))\n                 else:\n                     # x2 is dense.\n                     x2_zeros_and_nans = tf.equal(x2, 0)\n@@ -675,10 +677,8 @@ def elementwise_division(func):\n \n                     def func_for_x1_indices():\n                         # Gather values from x1 indices.\n-                        return tf.SparseTensor(\n-                            x1.indices,\n-                            func(x1.values, tf.gather_nd(x2, x1.indices)),\n-                            x1.dense_shape,\n+                        return sparse_with_values(\n+                            x1, func(x1.values, tf.gather_nd(x2, x1.indices))\n                         )\n \n                     def func_for_union_indices():\n@@ -691,7 +691,7 @@ def elementwise_division(func):\n                         ) = sparse_union_indices_and_values(\n                             x1, x2_zeros_and_nan_indices\n                         )\n-                        return tf.SparseTensor(\n+                        output = tf.SparseTensor(\n                             union_indices,\n                             func(\n                                 x1_values_for_union,\n@@ -699,6 +699,8 @@ def elementwise_division(func):\n                             ),\n                             x1.dense_shape,\n                         )\n+                        output.set_shape(x1.shape)\n+                        return output\n \n                     return tf.cond(\n                         tf.reduce_any(x2_zeros_and_nans),\n@@ -708,7 +710,7 @@ def elementwise_division(func):\n         elif isinstance(x2, tf.SparseTensor):\n             # x2 is a SparseTensor.\n             # Divisor is sparse, densify to do the divisions by zero correctly.\n-            x2 = tf.sparse.to_dense(x2)\n+            x2 = sparse_to_dense(x2)\n         elif isinstance(x1, tf.IndexedSlices):\n             if isinstance(x2, tf.IndexedSlices):\n                 # x1 is an IndexedSlices and x2 is an IndexedSlices.\n\n@@ -1,18 +1,21 @@\n import unittest\n \n+import pytest\n+\n from keras import backend\n+from keras import ops\n from keras.backend.common.keras_tensor import KerasTensor\n \n \n def single_arg_test_fn(x):\n-    return backend.numpy.concatenate([(x + 1) ** 2, x], axis=-1)\n+    return ops.concatenate([(x + 1) ** 2, x], axis=-1)\n \n \n def three_args_2_kwarg_test_fn(x1, x2, x3=None):\n-    x1 = backend.numpy.max(x1, axis=1)\n-    x2 = backend.numpy.max(x2, axis=1)\n+    x1 = ops.max(x1, axis=1)\n+    x2 = ops.max(x2, axis=1)\n     if x3 is not None:\n-        x1 += backend.numpy.max(x3, axis=1)\n+        x1 += ops.max(x3, axis=1)\n     return x1 + x2\n \n \n@@ -42,3 +45,75 @@ class ComputeOutputSpecTest(unittest.TestCase):\n             three_args_2_kwarg_test_fn, x1, x2, x3=x3\n         )\n         self.assertEqual(y.shape, (None, 5))\n+\n+    @pytest.mark.skipif(\n+        not backend.SUPPORTS_SPARSE_TENSORS,\n+        reason=\"Backend does not support sparse tensors.\",\n+    )\n+    def test_sparse_to_sparse(self):\n+        def single_arg_sparse_fn(x):\n+            y0 = ops.transpose(x, axes=(0, 2, 1))\n+            y1 = ops.squeeze(ops.expand_dims(x, axis=3), axis=3)\n+            y2 = ops.reshape(ops.reshape(x, (-1, 9)), (-1, 3, 3))\n+            return (y0, y1, y2)\n+\n+        x = KerasTensor(shape=(None, 3, 3), sparse=True)\n+        ys = backend.compute_output_spec(single_arg_sparse_fn, x)\n+        for y in ys:\n+            self.assertEqual(y.shape, (None, 3, 3))\n+            self.assertTrue(y.sparse)\n+\n+        def three_args_2_kwarg_sparse_fn(x1, x2, x3=None):\n+            y0 = ops.add(x1, x2)  # sparse, sparse\n+            y1 = ops.concatenate([x1, x2], axis=0)  # sparse, sparse\n+            y2 = ops.divide(x1, x3)  # sparse, dense\n+            y3 = ops.matmul(x1, x2)  # sparse, sparse\n+            y4 = ops.multiply(x1, x2)  # sparse, sparse\n+            y5 = ops.multiply(x1, x3)  # sparse, dense\n+            return (y0, y1, y2, y3, y4, y5)\n+\n+        x1 = KerasTensor(shape=(None, 3, 3), sparse=True)\n+        x2 = KerasTensor(shape=(None, 3, 3), sparse=True)\n+        x3 = KerasTensor(shape=(None, 3, 3), sparse=False)\n+        ys = backend.compute_output_spec(\n+            three_args_2_kwarg_sparse_fn, x1, x2, x3=x3\n+        )\n+        for y in ys:\n+            self.assertEqual(y.shape, (None, 3, 3))\n+            self.assertTrue(y.sparse)\n+\n+    @pytest.mark.skipif(\n+        not backend.SUPPORTS_SPARSE_TENSORS,\n+        reason=\"Backend does not support sparse tensors.\",\n+    )\n+    def test_sparse_to_dense(self):\n+        def single_arg_dense_fn(x):\n+            y0 = ops.exp(x)\n+            return (y0,)\n+\n+        x = KerasTensor(shape=(None, 3, 3), sparse=True)\n+        ys = backend.compute_output_spec(single_arg_dense_fn, x)\n+        for y in ys:\n+            self.assertEqual(y.shape, (None, 3, 3))\n+            self.assertFalse(y.sparse)\n+\n+        def three_args_2_kwarg_dense_fn(x1, x2, x3=None):\n+            y0 = ops.add(x1, x3)  # sparse, dense\n+            y1 = ops.add(x3, x1)  # dense, sparse\n+            y2 = ops.concatenate([x1, x3], axis=0)  # sparse, dense\n+            y3 = ops.matmul(x1, x3)  # sparse, dense\n+            y4 = ops.matmul(x3, x1)  # dense, sparse\n+            indices = KerasTensor((3,), \"int64\", sparse=True)\n+            y5 = ops.take(x3, indices=indices, axis=1)  # dense, sparse\n+            y6 = ops.divide(x1, x2)\n+            return (y0, y1, y2, y3, y4, y5, y6)\n+\n+        x1 = KerasTensor(shape=(None, 3, 3), sparse=True)\n+        x2 = KerasTensor(shape=(None, 3, 3), sparse=True)\n+        x3 = KerasTensor(shape=(None, 3, 3), sparse=False)\n+        ys = backend.compute_output_spec(\n+            three_args_2_kwarg_dense_fn, x1, x2, x3=x3\n+        )\n+        for y in ys:\n+            self.assertEqual(y.shape, (None, 3, 3))\n+            self.assertFalse(y.sparse)\n\n@@ -152,53 +152,10 @@ from keras.backend import any_symbolic_tensors\n from keras.backend.common import dtypes\n from keras.ops import operation_utils\n from keras.ops.operation import Operation\n+from keras.ops.operation_utils import broadcast_shapes\n from keras.ops.operation_utils import reduce_shape\n \n \n-def broadcast_shapes(shape1, shape2):\n-    \"\"\"Broadcast input shapes to a unified shape.\n-\n-    Convert to list for mutability.\n-\n-    Args:\n-        shape1: A tuple or list of integers.\n-        shape2: A tuple or list of integers.\n-\n-    Returns:\n-        output_shape (list of integers or `None`): The broadcasted shape.\n-\n-    Example:\n-    >>> broadcast_shapes((5, 3), (1, 3))\n-    [5, 3]\n-    \"\"\"\n-    shape1 = list(shape1)\n-    shape2 = list(shape2)\n-    origin_shape1 = shape1\n-    origin_shape2 = shape2\n-\n-    if len(shape1) > len(shape2):\n-        shape2 = [1] * (len(shape1) - len(shape2)) + shape2\n-    if len(shape1) < len(shape2):\n-        shape1 = [1] * (len(shape2) - len(shape1)) + shape1\n-    output_shape = list(shape1)\n-    for i in range(len(shape1)):\n-        if shape1[i] == 1:\n-            output_shape[i] = shape2[i]\n-        elif shape1[i] is None:\n-            output_shape[i] = None if shape2[i] == 1 else shape2[i]\n-        else:\n-            if shape2[i] == 1 or shape2[i] is None or shape2[i] == shape1[i]:\n-                output_shape[i] = shape1[i]\n-            else:\n-                raise ValueError(\n-                    \"Cannot broadcast shape, the failure dim has value \"\n-                    f\"{shape1[i]}, which cannot be broadcasted to {shape2[i]}. \"\n-                    f\"Input shapes are: {origin_shape1} and {origin_shape2}.\"\n-                )\n-\n-    return output_shape\n-\n-\n def shape_equal(shape1, shape2, axis=None, allow_none=True):\n     \"\"\"Check if two shapes are equal.\n \n@@ -2546,12 +2503,9 @@ class ExpandDims(Operation):\n         return backend.numpy.expand_dims(x, self.axis)\n \n     def compute_output_spec(self, x):\n-        x_shape = list(x.shape)\n-        if self.axis < 0:\n-            axis = len(x.shape) + 1 + self.axis\n-        else:\n-            axis = self.axis\n-        output_shape = x_shape[:axis] + [1] + x_shape[axis:]\n+        output_shape = operation_utils.compute_expand_dims_output_shape(\n+            x.shape, self.axis\n+        )\n         sparse = getattr(x, \"sparse\", False)\n         return KerasTensor(output_shape, dtype=x.dtype, sparse=sparse)\n \n@@ -3552,28 +3506,9 @@ class Matmul(Operation):\n     def compute_output_spec(self, x1, x2):\n         x1_shape = getattr(x1, \"shape\", [])\n         x2_shape = getattr(x2, \"shape\", [])\n-        if len(x1_shape) == 1:\n-            x1_shape = (1, x1_shape[0])\n-        if len(x2_shape) == 1:\n-            x2_shape = (x2_shape[0], 1)\n-        if (\n-            x1_shape[-1] is not None\n-            and x2_shape[-2] is not None\n-            and x1_shape[-1] != x2_shape[-2]\n-        ):\n-            raise ValueError(\n-                \"Inner dimensions (`x1.shape[-1]` and `x2.shape[-2]`) must be \"\n-                f\"equal, but received `x1.shape={x1.shape}` and \"\n-                f\"`x2.shape={x2.shape}`.\"\n+        output_shape = operation_utils.compute_matmul_output_shape(\n+            x1_shape, x2_shape\n         )\n-\n-        leading_shape = broadcast_shapes(x1_shape[:-2], x2_shape[:-2])\n-        last_2_dims_shape = [x1_shape[-2], x2_shape[-1]]\n-        output_shape = leading_shape + last_2_dims_shape\n-        if len(x1.shape) == 1:\n-            del output_shape[-2]\n-        if len(x2.shape) == 1:\n-            del output_shape[-1]\n         x1_sparse = getattr(x1, \"sparse\", True)\n         x2_sparse = getattr(x2, \"sparse\", True)\n         output_sparse = x1_sparse and x2_sparse\n@@ -5807,19 +5742,10 @@ class Transpose(Operation):\n         return backend.numpy.transpose(x, axes=self.axes)\n \n     def compute_output_spec(self, x):\n-        x_shape = x.shape\n-        sparse = getattr(x, \"sparse\", False)\n-        if self.axes is None:\n-            return KerasTensor(x_shape[::-1], dtype=x.dtype, sparse=sparse)\n-\n-        if len(self.axes) != len(x_shape):\n-            raise ValueError(\n-                \"axis must be a list of the same length as the input shape, \"\n-                f\"expected {len(x_shape)}, but received {len(self.axes)}.\"\n+        output_shape = operation_utils.compute_transpose_output_shape(\n+            x.shape, self.axes\n         )\n-        output_shape = []\n-        for ax in self.axes:\n-            output_shape.append(x_shape[ax])\n+        sparse = getattr(x, \"sparse\", False)\n         return KerasTensor(output_shape, dtype=x.dtype, sparse=sparse)\n \n \n\n@@ -6,6 +6,68 @@ import tree\n from keras.api_export import keras_export\n \n \n+def broadcast_shapes(shape1, shape2):\n+    \"\"\"Broadcast input shapes to a unified shape.\n+\n+    Convert to list for mutability.\n+\n+    Args:\n+        shape1: A tuple or list of integers.\n+        shape2: A tuple or list of integers.\n+\n+    Returns:\n+        output_shape (list of integers or `None`): The broadcasted shape.\n+\n+    Example:\n+    >>> broadcast_shapes((5, 3), (1, 3))\n+    [5, 3]\n+    \"\"\"\n+    shape1 = list(shape1)\n+    shape2 = list(shape2)\n+    origin_shape1 = shape1\n+    origin_shape2 = shape2\n+\n+    if len(shape1) > len(shape2):\n+        shape2 = [1] * (len(shape1) - len(shape2)) + shape2\n+    if len(shape1) < len(shape2):\n+        shape1 = [1] * (len(shape2) - len(shape1)) + shape1\n+    output_shape = list(shape1)\n+    for i in range(len(shape1)):\n+        if shape1[i] == 1:\n+            output_shape[i] = shape2[i]\n+        elif shape1[i] is None:\n+            output_shape[i] = None if shape2[i] == 1 else shape2[i]\n+        else:\n+            if shape2[i] == 1 or shape2[i] is None or shape2[i] == shape1[i]:\n+                output_shape[i] = shape1[i]\n+            else:\n+                raise ValueError(\n+                    \"Cannot broadcast shape, the failure dim has value \"\n+                    f\"{shape1[i]}, which cannot be broadcasted to {shape2[i]}. \"\n+                    f\"Input shapes are: {origin_shape1} and {origin_shape2}.\"\n+                )\n+\n+    return output_shape\n+\n+\n+def compute_expand_dims_output_shape(input_shape, axis):\n+    \"\"\"Compute the output shape for the `expand_dims` operation.\n+\n+    Args:\n+        input_shape: Input shape.\n+        axis: int for the axis to expand.\n+\n+    Returns:\n+        Tuple of ints: The output shape after the `expand_dims` operation.\n+    \"\"\"\n+    input_shape = list(input_shape)\n+    if axis is None:\n+        axis = len(input_shape)\n+    elif axis < 0:\n+        axis = len(input_shape) + 1 + axis\n+    return tuple(input_shape[:axis] + [1] + input_shape[axis:])\n+\n+\n def compute_pooling_output_shape(\n     input_shape,\n     pool_size,\n@@ -170,6 +232,41 @@ def compute_conv_output_shape(\n     return output_shape\n \n \n+def compute_matmul_output_shape(shape1, shape2):\n+    \"\"\"Compute the output shape of a `matmul` operation.\n+\n+    Args:\n+        shape1: Shape of the left operand.\n+        shape2: Shape of the right operand.\n+\n+    Returns:\n+        Tuple of ints: The output shape for the `matmul` operation.\n+    \"\"\"\n+    if len(shape1) == 1:\n+        shape1 = (1, shape1[0])\n+    if len(shape2) == 1:\n+        shape2 = (shape2[0], 1)\n+    if (\n+        shape1[-1] is not None\n+        and shape2[-2] is not None\n+        and shape1[-1] != shape2[-2]\n+    ):\n+        raise ValueError(\n+            \"Inner dimensions (`x1.shape[-1]` and `x2.shape[-2]`) must be \"\n+            f\"equal, but received `x1.shape={shape1}` and \"\n+            f\"`x2.shape={shape2}`.\"\n+        )\n+\n+    leading_shape = broadcast_shapes(shape1[:-2], shape2[:-2])\n+    last_2_dims_shape = [shape1[-2], shape2[-1]]\n+    output_shape = leading_shape + last_2_dims_shape\n+    if len(shape1) == 1:\n+        del output_shape[-2]\n+    if len(shape2) == 1:\n+        del output_shape[-1]\n+    return tuple(output_shape)\n+\n+\n def compute_reshape_output_shape(input_shape, new_shape, new_shape_arg_name):\n     \"\"\"Converts `-1` in `new_shape` to either an actual dimension or `None`.\n \n@@ -218,6 +315,28 @@ def compute_reshape_output_shape(input_shape, new_shape, new_shape_arg_name):\n     return tuple(output_shape)\n \n \n+def compute_transpose_output_shape(input_shape, axes):\n+    \"\"\"Compute the output shape for the `transpose` operation.\n+\n+    Args:\n+        input_shape: Input shape.\n+        axes: Permutation of the dimensions for the `transpose` operation.\n+\n+    Returns:\n+        Tuple of ints: The output shape after the `transpose` operation.\n+    \"\"\"\n+    input_shape = list(input_shape)\n+    if axes is None:\n+        return tuple(input_shape[::-1])\n+\n+    if len(axes) != len(input_shape):\n+        raise ValueError(\n+            \"axis must be a list of the same length as the input shape, \"\n+            f\"expected {len(input_shape)}, but received {len(axes)}.\"\n+        )\n+    return tuple(input_shape[ax] for ax in axes)\n+\n+\n def reduce_shape(shape, axis=None, keepdims=False):\n     shape = list(shape)\n     if axis is None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5b9f1b70dca1ac3b7a307f0ddbbe5df816dfe5b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 80 | Lines Deleted: 42 | Files Changed: 6 | Hunks: 38 | Methods Changed: 30 | Complexity Δ (Sum/Max): -3/3 | Churn Δ: 122 | Churn Cumulative: 9859 | Contributors (this commit): 7 | Commits (past 90d): 145 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -130,6 +130,7 @@ def arange(start, stop=None, step=1, dtype=None):\n \n \n def arccos(x):\n+    x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n         dtype = config.floatx()\n     else:\n@@ -139,6 +140,7 @@ def arccos(x):\n \n \n def arccosh(x):\n+    x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n         dtype = config.floatx()\n     else:\n@@ -148,6 +150,7 @@ def arccosh(x):\n \n \n def arcsin(x):\n+    x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n         dtype = config.floatx()\n     else:\n@@ -157,6 +160,7 @@ def arcsin(x):\n \n \n def arcsinh(x):\n+    x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n         dtype = config.floatx()\n     else:\n@@ -166,6 +170,7 @@ def arcsinh(x):\n \n \n def arctan(x):\n+    x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n         dtype = config.floatx()\n     else:\n@@ -184,6 +189,7 @@ def arctan2(x1, x2):\n \n \n def arctanh(x):\n+    x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n         dtype = config.floatx()\n     else:\n@@ -235,6 +241,9 @@ def ceil(x):\n \n \n def clip(x, x_min, x_max):\n+    x = convert_to_tensor(x)\n+    if standardize_dtype(x.dtype) == \"bool\":\n+        x = cast(x, \"int32\")\n     return jnp.clip(x, x_min, x_max)\n \n \n@@ -255,6 +264,7 @@ def copy(x):\n \n \n def cos(x):\n+    x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n         dtype = config.floatx()\n     else:\n@@ -264,6 +274,7 @@ def cos(x):\n \n \n def cosh(x):\n+    x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n         dtype = config.floatx()\n     else:\n@@ -518,6 +529,7 @@ def median(x, axis=None, keepdims=False):\n     # axis of jnp.median must be hashable\n     if isinstance(axis, list):\n         axis = tuple(axis)\n+    x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n         x = cast(x, config.floatx())\n \n@@ -645,6 +657,7 @@ def sign(x):\n \n \n def sin(x):\n+    x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n         dtype = config.floatx()\n     else:\n@@ -654,6 +667,7 @@ def sin(x):\n \n \n def sinh(x):\n+    x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n         dtype = config.floatx()\n     else:\n@@ -700,6 +714,7 @@ def take_along_axis(x, indices, axis=None):\n \n \n def tan(x):\n+    x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n         dtype = config.floatx()\n     else:\n@@ -709,6 +724,7 @@ def tan(x):\n \n \n def tanh(x):\n+    x = convert_to_tensor(x)\n     if standardize_dtype(x.dtype) == \"int64\":\n         dtype = config.floatx()\n     else:\n@@ -805,14 +821,15 @@ def transpose(x, axes=None):\n \n \n def var(x, axis=None, keepdims=False):\n+    x = convert_to_tensor(x)\n     # `jnp.var` does not handle low precision (e.g., float16) overflow\n     # correctly, so we compute with float32 and cast back to the original type.\n-    outputs = jnp.var(x, axis=axis, keepdims=keepdims, dtype=jnp.float32)\n-    dtype = getattr(x, \"dtype\", None)\n-    if hasattr(dtype, \"name\") and \"float\" in dtype.name:\n-        return cast(outputs, dtype)\n-    else:\n-        return cast(outputs, config.floatx())\n+    compute_dtype = dtypes.result_type(x.dtype, \"float32\")\n+    result_dtype = dtypes.result_type(x.dtype, float)\n+    return cast(\n+        jnp.var(x, axis=axis, keepdims=keepdims, dtype=compute_dtype),\n+        result_dtype,\n+    )\n \n \n def sum(x, axis=None, keepdims=False):\n\n@@ -287,13 +287,15 @@ def ceil(x):\n         dtype = config.floatx()\n     else:\n         dtype = dtypes.result_type(x.dtype, float)\n-    return np.ceil(x).astype(dtype)\n+    x = x.astype(dtype)\n+    return np.ceil(x)\n \n \n def clip(x, x_min, x_max):\n+    x = convert_to_tensor(x)\n     dtype = standardize_dtype(x.dtype)\n     if dtype == \"bool\":\n-        dtype = \"int64\"\n+        dtype = \"int32\"\n     return np.clip(x, x_min, x_max).astype(dtype)\n \n \n@@ -728,9 +730,7 @@ def prod(x, axis=None, keepdims=False, dtype=None):\n     x = convert_to_tensor(x)\n     if dtype is None:\n         dtype = dtypes.result_type(x.dtype)\n-        if dtype == \"bool\":\n-            dtype = \"int32\"\n-        elif dtype in (\"int8\", \"int16\"):\n+        if dtype in (\"bool\", \"int8\", \"int16\"):\n             dtype = \"int32\"\n         elif dtype in (\"uint8\", \"uint16\"):\n             dtype = \"uint32\"\n@@ -891,11 +891,7 @@ def trace(x, offset=0, axis1=0, axis2=1):\n     axis2 = tuple(axis2) if isinstance(axis2, list) else axis2\n     x = convert_to_tensor(x)\n     dtype = standardize_dtype(x.dtype)\n-    if dtype == \"int64\":\n-        dtype = \"int64\"\n-    elif dtype == \"uint32\":\n-        dtype = \"uint32\"\n-    else:\n+    if dtype not in (\"int64\", \"uint32\", \"uint64\"):\n         dtype = dtypes.result_type(dtype, \"int32\")\n     return np.trace(x, offset=offset, axis1=axis1, axis2=axis2, dtype=dtype)\n \n@@ -1016,7 +1012,12 @@ def transpose(x, axes=None):\n \n def var(x, axis=None, keepdims=False):\n     axis = tuple(axis) if isinstance(axis, list) else axis\n-    return np.var(x, axis=axis, keepdims=keepdims)\n+    x = convert_to_tensor(x)\n+    compute_dtype = dtypes.result_type(x.dtype, \"float32\")\n+    result_dtype = dtypes.result_type(x.dtype, float)\n+    return np.var(x, axis=axis, keepdims=keepdims, dtype=compute_dtype).astype(\n+        result_dtype\n+    )\n \n \n def sum(x, axis=None, keepdims=False):\n\n@@ -532,7 +532,7 @@ def ceil(x):\n def clip(x, x_min, x_max):\n     dtype = standardize_dtype(x.dtype)\n     if dtype == \"bool\":\n-        x = tf.cast(x, \"int64\")\n+        x = tf.cast(x, \"int32\")\n     return tf.clip_by_value(x, x_min, x_max)\n \n \n@@ -1450,11 +1450,7 @@ def tile(x, repeats):\n def trace(x, offset=0, axis1=0, axis2=1):\n     x = convert_to_tensor(x)\n     dtype = standardize_dtype(x.dtype)\n-    if dtype == \"int64\":\n-        dtype = \"int64\"\n-    elif dtype == \"uint32\":\n-        dtype = \"uint32\"\n-    else:\n+    if dtype not in (\"int64\", \"uint32\", \"uint64\"):\n         dtype = dtypes.result_type(dtype, \"int32\")\n     return tfnp.trace(x, offset=offset, axis1=axis1, axis2=axis2, dtype=dtype)\n \n@@ -1612,7 +1608,13 @@ def transpose(x, axes=None):\n \n \n def var(x, axis=None, keepdims=False):\n-    return tfnp.var(x, axis=axis, keepdims=keepdims)\n+    x = convert_to_tensor(x)\n+    compute_dtype = dtypes.result_type(x.dtype, \"float32\")\n+    result_dtype = dtypes.result_type(x.dtype, float)\n+    return tf.cast(\n+        tfnp.var(x, axis=axis, keepdims=keepdims, dtype=compute_dtype),\n+        result_dtype,\n+    )\n \n \n def sum(x, axis=None, keepdims=False):\n\n@@ -391,9 +391,11 @@ def clip(x, x_min, x_max):\n     # TODO: torch.clip doesn't support float16 with cpu\n     if get_device() == \"cpu\" and ori_dtype == \"float16\":\n         x = cast(x, \"float32\")\n+        return cast(torch.clip(x, min=x_min, max=x_max), \"float16\")\n \n-    dtype = \"int64\" if ori_dtype == \"bool\" else ori_dtype\n-    return cast(torch.clip(x, min=x_min, max=x_max), dtype=dtype)\n+    if ori_dtype == \"bool\":\n+        x = cast(x, \"int32\")\n+    return torch.clip(x, min=x_min, max=x_max)\n \n \n def concatenate(xs, axis=0):\n@@ -1290,9 +1292,7 @@ def tile(x, repeats):\n def trace(x, offset=None, axis1=None, axis2=None):\n     x = convert_to_tensor(x)\n     dtype = standardize_dtype(x.dtype)\n-    if dtype == \"int64\":\n-        dtype = \"int64\"\n-    else:\n+    if dtype != \"int64\":\n         dtype = dtypes.result_type(dtype, \"int32\")\n     return torch.sum(\n         torch.diagonal(x, offset, axis1, axis2),\n@@ -1380,7 +1380,7 @@ def square(x):\n \n def sqrt(x):\n     x = convert_to_tensor(x)\n-    if x.dtype == torch.int64:\n+    if standardize_dtype(x.dtype) == \"int64\":\n         x = cast(x, config.floatx())\n     return torch.sqrt(x)\n \n@@ -1400,14 +1400,17 @@ def transpose(x, axes=None):\n \n \n def var(x, axis=None, keepdims=False):\n-    x = convert_to_tensor(x, dtype=\"float32\")\n-    # Conversion to float necessary for `torch.var`\n-    x = cast(x, \"float32\") if x.dtype in TORCH_INT_TYPES else x\n+    x = convert_to_tensor(x)\n+    compute_dtype = dtypes.result_type(x.dtype, \"float32\")\n+    result_dtype = dtypes.result_type(x.dtype, float)\n     if axis == [] or axis == ():\n         # Torch handles the empty axis case differently from numpy.\n-        return zeros_like(x)\n+        return zeros_like(x, result_dtype)\n     # Bessel correction removed for numpy compatibility\n-    return torch.var(x, dim=axis, keepdim=keepdims, correction=0)\n+    x = cast(x, compute_dtype)\n+    return cast(\n+        torch.var(x, dim=axis, keepdim=keepdims, correction=0), result_dtype\n+    )\n \n \n def sum(x, axis=None, keepdims=False):\n\n@@ -1385,7 +1385,7 @@ class Clip(Operation):\n     def compute_output_spec(self, x):\n         dtype = backend.standardize_dtype(x.dtype)\n         if dtype == \"bool\":\n-            dtype = \"int64\"\n+            dtype = \"int32\"\n         return KerasTensor(x.shape, dtype=dtype)\n \n \n@@ -5187,11 +5187,7 @@ class Trace(Operation):\n         x_shape[self.axis2] = -1\n         output_shape = list(filter((-1).__ne__, x_shape))\n         output_dtype = backend.standardize_dtype(x.dtype)\n-        if output_dtype == \"int64\":\n-            output_dtype = \"int64\"\n-        elif output_dtype == \"uint32\":\n-            output_dtype = \"uint32\"\n-        else:\n+        if output_dtype not in (\"int64\", \"uint32\", \"uint64\"):\n             output_dtype = dtypes.result_type(output_dtype, \"int32\")\n         return KerasTensor(output_shape, dtype=output_dtype)\n \n@@ -5821,9 +5817,10 @@ class Var(Operation):\n         return backend.numpy.var(x, axis=self.axis, keepdims=self.keepdims)\n \n     def compute_output_spec(self, x):\n+        output_dtype = backend.result_type(getattr(x, \"dtype\", type(x)), float)\n         return KerasTensor(\n             reduce_shape(x.shape, axis=self.axis, keepdims=self.keepdims),\n-            dtype=x.dtype,\n+            dtype=output_dtype,\n         )\n \n \n\n@@ -5085,6 +5085,8 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n         x = knp.ones((1,), dtype=dtype)\n         x_jax = jnp.ones((1,), dtype=dtype)\n         expected_dtype = standardize_dtype(jnp.clip(x_jax, -2, 2).dtype)\n+        if dtype == \"bool\":\n+            expected_dtype = \"int32\"\n \n         self.assertEqual(\n             standardize_dtype(knp.clip(x, -2, 2).dtype), expected_dtype\n@@ -6969,6 +6971,22 @@ class NumpyDtypeTest(testing.TestCase, parameterized.TestCase):\n                 knp.TrueDivide().symbolic_call(x1, x2).dtype, expected_dtype\n             )\n \n+    @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n+    def test_var(self, dtype):\n+        import jax.numpy as jnp\n+\n+        x = knp.ones((2,), dtype=dtype)\n+        x_jax = jnp.ones((2,), dtype=dtype)\n+        expected_dtype = standardize_dtype(jnp.var(x_jax).dtype)\n+        if dtype == \"int64\":\n+            expected_dtype = backend.floatx()\n+\n+        self.assertEqual(standardize_dtype(knp.var(x).dtype), expected_dtype)\n+        self.assertEqual(\n+            standardize_dtype(knp.Var().symbolic_call(x).dtype),\n+            expected_dtype,\n+        )\n+\n     @parameterized.named_parameters(\n         named_product(dtypes=itertools.combinations(ALL_DTYPES, 2))\n     )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
