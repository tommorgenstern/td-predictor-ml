{"custom_id": "keras#088f0fcf09c991475a3eb7b814d2d453bbf0a6e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 19 | Churn Cumulative: 503 | Contributors (this commit): 5 | Commits (past 90d): 10 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -20,7 +20,7 @@ ARRAY_TYPES = (np.ndarray,)\n if backend.backend() == \"tensorflow\":\n     from keras.utils.module_utils import tensorflow as tf\n \n-    ARRAY_TYPES = ARRAY_TYPES + (tf.Tensor, tf.RaggedTensor)\n+    ARRAY_TYPES = ARRAY_TYPES + (tf.RaggedTensor,)\n if pandas:\n     ARRAY_TYPES = ARRAY_TYPES + (pandas.Series, pandas.DataFrame)\n \n@@ -158,7 +158,7 @@ def train_validation_split(arrays, validation_split):\n     \"\"\"\n \n     def _can_split(t):\n-        return isinstance(t, ARRAY_TYPES) or t is None\n+        return backend.is_tensor(t) or isinstance(t, ARRAY_TYPES) or t is None\n \n     flat_arrays = tree.flatten(arrays)\n     unsplitable = [type(t) for t in flat_arrays if not _can_split(t)]\n\n@@ -291,6 +291,21 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         self.assertIn(\"loss\", history)\n         self.assertIn(\"val_loss\", history)\n \n+        # Test with backend-native tensors.\n+        x = ops.ones((dataset_size, 4))\n+        y = ops.zeros((dataset_size, 3))\n+        history = model.fit(\n+            x,\n+            y,\n+            batch_size=batch_size,\n+            steps_per_epoch=steps_per_epoch if use_steps_per_epoch else None,\n+            epochs=epochs,\n+            validation_split=0.2,\n+        )\n+        history = history.history\n+        self.assertIn(\"loss\", history)\n+        self.assertIn(\"val_loss\", history)\n+\n     @parameterized.named_parameters(\n         [\n             (\"eager_tf_sparse\", True, False),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aa39acb14b7c8ea661faa970f77619581eaf321a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 827 | Contributors (this commit): 10 | Commits (past 90d): 20 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -233,6 +233,7 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(x, np.ones((2, 3)) * 10)\n         self.assertAllClose(y, np.ones((3, 2)) * 10)\n \n+        # Test max iterations\n         x = np.ones((2, 3))\n         y = np.ones((3, 2))\n         x, y = core.while_loop(cond, body, (x, y), maximum_iterations=5)\n\n@@ -952,7 +952,7 @@ class MathOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         expected_result = np.linalg.norm(\n             x, ord=ord, axis=axis, keepdims=keepdims\n         )\n-        self.assertAllClose(output, expected_result)\n+        self.assertAllClose(output, expected_result, atol=1e-5, rtol=1e-5)\n \n     @parameterized.named_parameters(\n         named_product(\n@@ -1243,7 +1243,7 @@ class SolveTest(testing.TestCase):\n         b = np.array([[9, 8], [5, 4]], dtype=np.float32)\n         output = solve_op.call(a, b)\n         expected_output = np.linalg.solve(a, b)\n-        self.assertAllClose(output, expected_output, atol=1e-6)\n+        self.assertAllClose(output, expected_output, atol=1e-6, rtol=1e-6)\n \n \n class FFT2Test(testing.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cd670f8abb67f18ee2ef668c2685826873b7cd67", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 45 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -246,6 +246,13 @@ class ModelCheckpoint(Callback):\n                         \"available, skipping.\",\n                         stacklevel=2,\n                     )\n+                elif len(current.numpy()) > 0:\n+                    warnings.warn(\n+                        f\"Can save best model only when `monitor` is \"\n+                        f\"a scalar value, got {current}. \"\n+                        f\"Falling back to `save_best_only=False`. \"\n+                    )\n+                    self.model.save(filepath, overwrite=True)\n                 else:\n                     if self.monitor_op(current, self.best):\n                         if self.verbose > 0:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3979c994e2b166cbe7ca8a5fa3806c4db25b083b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 51 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -248,9 +248,9 @@ class ModelCheckpoint(Callback):\n                     )\n                 elif len(current.numpy()) > 0:\n                     warnings.warn(\n-                        f\"Can save best model only when `monitor` is \"\n-                        f\"a scalar value, got {current}. \"\n-                        f\"Falling back to `save_best_only=False`. \"\n+                        \"Can save best model only when `monitor` is \"\n+                        f\"a scalar value. Received: {current}. \"\n+                        \"Falling back to `save_best_only=False`.\"\n                     )\n                     self.model.save(filepath, overwrite=True)\n                 else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bc314e5a8ee815c4054288f138b19b8c91f5a591", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 6 | Churn Cumulative: 57 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,6 +4,7 @@ import warnings\n \n import numpy as np\n \n+from keras import backend\n from keras.api_export import keras_export\n from keras.callbacks.callback import Callback\n from keras.utils import file_utils\n@@ -246,7 +247,10 @@ class ModelCheckpoint(Callback):\n                         \"available, skipping.\",\n                         stacklevel=2,\n                     )\n-                elif len(current.numpy()) > 0:\n+                elif (\n+                    isinstance(current, np.ndarray)\n+                    or backend.is_tensor(current)\n+                ) and len(current.shape) > 0:\n                     warnings.warn(\n                         \"Can save best model only when `monitor` is \"\n                         f\"a scalar value. Received: {current}. \"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3a3ca1f1bb737910f4429882f32fd2ff38e4a940", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 249 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 32 | Complexity Δ (Sum/Max): 36/35 | Churn Δ: 252 | Churn Cumulative: 637 | Contributors (this commit): 8 | Commits (past 90d): 18 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -541,9 +541,8 @@ class Cond(Operation):\n         return true_fn_spec\n \n     def _check_output_spec(self, true_fn_spec, false_fn_spec):\n-        if true_fn_spec is None:\n-            if false_fn_spec is not None:\n-                return False\n+        if true_fn_spec is None or false_fn_spec is None:\n+            return true_fn_spec is None and false_fn_spec is None\n         elif isinstance(true_fn_spec, dict):\n             if not isinstance(false_fn_spec, dict):\n                 return False\n\n@@ -1,4 +1,5 @@\n import contextlib\n+from unittest.mock import Mock\n \n import numpy as np\n import pytest\n@@ -506,3 +507,249 @@ class CoreOpsDtypeTest(testing.TestCase, parameterized.TestCase):\n                 ),\n                 expected_dtype,\n             )\n+\n+\n+class CoreOpsCallsTests(testing.TestCase):\n+    def test_scatter_basic_call(self):\n+        indices = np.array([[1, 0], [0, 1]])\n+        values = np.array([10, 20])\n+        shape = (2, 2)\n+        scatter = core.Scatter()\n+        result = scatter.call(indices, values, shape)\n+        expected_output = np.array([[0, 20], [10, 0]])\n+        self.assertAllClose(core.convert_to_numpy(result), expected_output)\n+\n+    def test_scatter_update_basic_call(self):\n+        inputs = np.array([[0, 0], [0, 0]])\n+        indices = np.array([[1, 0], [0, 1]])\n+        updates = np.array([10, 20])\n+        scatter_update = core.ScatterUpdate()\n+        result = scatter_update.call(inputs, indices, updates)\n+        expected_output = np.array([[0, 20], [10, 0]])\n+        self.assertAllClose(core.convert_to_numpy(result), expected_output)\n+\n+    def test_slice_basic_call(self):\n+        inputs = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n+        start_indices = np.array([1, 1])\n+        shape = (2, 2)\n+        slice_op = core.Slice()\n+        result = slice_op.call(inputs, start_indices, shape)\n+        expected_output = np.array([[5, 6], [8, 9]])\n+        self.assertAllClose(core.convert_to_numpy(result), expected_output)\n+\n+    def test_slice_compute_output_spec(self):\n+        inputs = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float32)\n+        start_indices = np.array([1, 1])\n+        shape = (2, 2)\n+        slice_op = core.Slice()\n+        output_spec = slice_op.compute_output_spec(inputs, start_indices, shape)\n+        self.assertEqual(output_spec.shape, shape)\n+        self.assertEqual(output_spec.dtype, inputs.dtype)\n+\n+    def test_slice_with_symbolic_tensors(self):\n+        inputs = KerasTensor(shape=(3, 3), dtype=np.float32)\n+        start_indices = KerasTensor(shape=(2,), dtype=np.int32)\n+        shape = (2, 2)\n+        result = core.slice(inputs, start_indices, shape)\n+        self.assertTrue(isinstance(result, KerasTensor))\n+\n+    def test_slice_with_non_symbolic_tensors(self):\n+        inputs = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n+        start_indices = np.array([1, 1])\n+        shape = (2, 2)\n+        result = core.slice(inputs, start_indices, shape)\n+        expected_output = np.array([[5, 6], [8, 9]])\n+        self.assertAllClose(result, expected_output)\n+\n+    def test_slice_update_basic_call(self):\n+        inputs = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n+        start_indices = np.array([1, 1])\n+        updates = np.array([[10, 11], [12, 13]])\n+        slice_update = core.SliceUpdate()\n+        result = slice_update.call(inputs, start_indices, updates)\n+        expected_output = np.array([[1, 2, 3], [4, 10, 11], [7, 12, 13]])\n+        self.assertAllClose(core.convert_to_numpy(result), expected_output)\n+\n+    def test_while_loop_basic_functionality(self):\n+        # Loop condition: continue if i < 5\n+        def cond(i):\n+            return i < 5\n+\n+        # Loop body: increment i by 1\n+        def body(i):\n+            return (i + 1,)\n+\n+        while_loop = core.WhileLoop(cond, body, maximum_iterations=None)\n+        # Initial loop variable (i = 0)\n+        loop_vars = (0,)\n+        result = while_loop.call(loop_vars)\n+        self.assertEqual(result[0], 5)\n+\n+    def test_while_loop_output_spec(self):\n+        # Define dummy cond and body functions\n+        def cond(x):\n+            return True\n+\n+        def body(x):\n+            return (x,)\n+\n+        while_loop = core.WhileLoop(cond, body, maximum_iterations=None)\n+        loop_vars = (KerasTensor(shape=(10,), dtype=np.float32),)\n+        output_spec = while_loop.compute_output_spec(loop_vars)\n+        self.assertEqual(output_spec[0].shape, loop_vars[0].shape)\n+        self.assertEqual(output_spec[0].dtype, loop_vars[0].dtype)\n+\n+    def test_while_loop_with_max_iterations(self):\n+        # loop condition: continue if i < 10\n+        def cond(i):\n+            return i < 10\n+\n+        def body(i):\n+            return (i + 1,)\n+\n+        while_loop = core.WhileLoop(cond, body, maximum_iterations=5)\n+        result = while_loop.call((0,))\n+        self.assertEqual(result[0], 5)\n+\n+    def test_whileloop_compute_output_spec(self):\n+        # Define loop variables with different shapes and data types\n+        loop_vars = (np.random.rand(5, 5), np.random.randint(10, size=(3, 7)))\n+        keras_loop_vars = [\n+            KerasTensor(v.shape, dtype=v.dtype) for v in loop_vars\n+        ]\n+\n+        def cond(v):\n+            return v[0] < 5\n+\n+        def body(v):\n+            return (v[0] + 1, v[1])\n+\n+        while_loop = core.WhileLoop(cond, body, maximum_iterations=None)\n+        output_specs = while_loop.compute_output_spec(keras_loop_vars)\n+        self.assertEqual(output_specs[0].shape, keras_loop_vars[0].shape)\n+        self.assertEqual(output_specs[0].dtype, keras_loop_vars[0].dtype)\n+        self.assertEqual(output_specs[1].shape, keras_loop_vars[1].shape)\n+        self.assertEqual(output_specs[1].dtype, keras_loop_vars[1].dtype)\n+\n+    def test_stop_gradient_call(self):\n+        variable_np = np.array([1.0, 2.0, 3.0], dtype=np.float32)\n+        variable = core.convert_to_tensor(variable_np)\n+        stop_gradient = core.StopGradient()\n+        result = stop_gradient.call(variable)\n+        result_np = core.convert_to_numpy(result)\n+        self.assertTrue(np.array_equal(result_np, variable_np))\n+        self.assertEqual(result_np.dtype, variable_np.dtype)\n+\n+    def test_stop_gradient_compute_output_spec(self):\n+        variable = KerasTensor(shape=(3,), dtype=np.float32)\n+        stop_gradient = core.StopGradient()\n+        output_spec = stop_gradient.compute_output_spec(variable)\n+        self.assertEqual(output_spec.shape, variable.shape)\n+        self.assertEqual(output_spec.dtype, variable.dtype)\n+\n+    def test_fori_loop_basic_functionality(self):\n+        lower = 0\n+        upper = 5\n+\n+        def body_fun(index, val):\n+            return val + 1\n+\n+        fori_loop = core.ForiLoop(lower, upper, body_fun)\n+        init_val = 0\n+        result = fori_loop.call(init_val)\n+        self.assertEqual(result, upper)\n+\n+    def test_unstack_basic_functionality(self):\n+        x = np.random.rand(2, 3, 4)\n+        x = core.convert_to_tensor(x)\n+        axis = 1\n+        unstack = core.Unstack(axis=axis)\n+        result = unstack.call(x)\n+        self.assertEqual(len(result), x.shape[axis])\n+        result = core.convert_to_numpy(result)\n+        expected_shape = x.shape[:axis] + x.shape[axis + 1 :]\n+        # Check that all tensors have the same shape\n+        if len(result) > 0:\n+            self.assertEqual(result[0].shape, expected_shape)\n+        if len(result) > 1:\n+            self.assertEqual(result[1].shape, expected_shape)\n+        if len(result) > 2:\n+            self.assertEqual(result[2].shape, expected_shape)\n+\n+    def test_cast_basic_functionality(self):\n+        x = np.array([1.0, 2.0, 3.0], dtype=np.float32)\n+        target_dtype = np.int32\n+        cast = core.Cast(target_dtype)\n+        result = cast.call(x)\n+        result = core.convert_to_numpy(result)\n+        self.assertEqual(result.dtype, target_dtype)\n+        # Check that the values are the same\n+        expected_values = x.astype(target_dtype)\n+        self.assertTrue(np.array_equal(result, expected_values))\n+\n+    def test_cond_check_output_spec_list_tuple(self):\n+        cond_op = core.Cond()\n+        mock_spec = Mock(dtype=\"float32\", shape=(2, 2))\n+        self.assertTrue(\n+            cond_op._check_output_spec(\n+                [mock_spec, mock_spec], [mock_spec, mock_spec]\n+            )\n+        )\n+\n+    def test_cond_check_output_spec_other_types(self):\n+        cond_op = core.Cond()\n+        # Create mock objects with dtype and shape attributes\n+        mock_spec1 = Mock(dtype=\"float32\", shape=(2, 2))\n+        mock_spec2 = Mock(dtype=\"float32\", shape=(2, 2))\n+        self.assertTrue(cond_op._check_output_spec(mock_spec1, mock_spec2))\n+\n+    def test_cond_check_output_spec_none(self):\n+        cond_op = core.Cond()\n+        self.assertTrue(cond_op._check_output_spec(None, None))\n+        self.assertFalse(\n+            cond_op._check_output_spec(\n+                None, Mock(dtype=\"float32\", shape=(2, 2))\n+            )\n+        )\n+        self.assertFalse(\n+            cond_op._check_output_spec(\n+                Mock(dtype=\"float32\", shape=(2, 2)), None\n+            )\n+        )\n+\n+    def test_cond_check_output_spec_dict(self):\n+        cond_op = core.Cond()\n+        mock_spec = Mock(dtype=\"float32\", shape=(2, 2))\n+        self.assertTrue(\n+            cond_op._check_output_spec({\"a\": mock_spec}, {\"a\": mock_spec})\n+        )\n+        self.assertFalse(\n+            cond_op._check_output_spec({\"a\": mock_spec}, {\"b\": mock_spec})\n+        )\n+        self.assertFalse(\n+            cond_op._check_output_spec(\n+                {\"a\": mock_spec}, {\"a\": mock_spec, \"b\": mock_spec}\n+            )\n+        )\n+\n+    def test_cond_check_output_spec_list(self):\n+        cond_op = core.Cond()\n+        mock_spec = Mock(dtype=\"float32\", shape=(2, 2))\n+        mock_spec_different = Mock(dtype=\"int32\", shape=(3, 3))\n+        self.assertTrue(cond_op._check_output_spec([mock_spec], [mock_spec]))\n+        self.assertFalse(\n+            cond_op._check_output_spec(\n+                [mock_spec], [mock_spec, mock_spec_different]\n+            )\n+        )\n+\n+    def test_cond_check_output_spec_tuple(self):\n+        cond_op = core.Cond()\n+        mock_spec = Mock(dtype=\"float32\", shape=(2, 2))\n+        mock_spec_different = Mock(dtype=\"int32\", shape=(3, 3))\n+        self.assertTrue(cond_op._check_output_spec((mock_spec,), (mock_spec,)))\n+        self.assertFalse(\n+            cond_op._check_output_spec(\n+                (mock_spec,), (mock_spec, mock_spec_different)\n+            )\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#186f8723410527b0292f990fd4c951ad0f1edd24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 6 | Churn Cumulative: 154 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -336,6 +336,7 @@ class TestCase(unittest.TestCase):\n             # test the \"default\" path for each backend by setting\n             # jit_compile=\"auto\".\n             # for tensorflow and jax backends auto is jitted\n+            # Note that tensorflow cannot be jitted with sparse tensors\n             # for torch backend auto is eager\n             #\n             # NB: for torch, jit_compile=True turns on torchdynamo\n@@ -345,7 +346,10 @@ class TestCase(unittest.TestCase):\n             #    TORCH_LOGS=\"+dynamo\"\n             #    TORCHDYNAMO_VERBOSE=1\n             #    TORCHDYNAMO_REPORT_GUARD_FAILURES=1\n-            model.compile(optimizer=\"sgd\", loss=\"mse\", jit_compile=\"auto\")\n+            jit_compile = \"auto\"\n+            if backend.backend() == \"tensorflow\" and input_sparse:\n+                jit_compile = False\n+            model.compile(optimizer=\"sgd\", loss=\"mse\", jit_compile=jit_compile)\n             model.fit(data_generator(), steps_per_epoch=1, verbose=0)\n \n         # Build test.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7736f67cfb4fa847bf53dbba2ce4bf73aace3adb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 482 | Contributors (this commit): 6 | Commits (past 90d): 16 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -322,6 +322,7 @@ def average(x, axis=None, weights=None):\n \n \n def broadcast_to(x, shape):\n+    x = convert_to_tensor(x)\n     return jnp.broadcast_to(x, shape)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5da120237e3b3ed2261b4799b95ea3d39a5058df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 150 | Lines Deleted: 108 | Files Changed: 5 | Hunks: 37 | Methods Changed: 30 | Complexity Δ (Sum/Max): 15/12 | Churn Δ: 258 | Churn Cumulative: 1110 | Contributors (this commit): 7 | Commits (past 90d): 18 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -42,7 +42,11 @@ class GeneratorDataAdapter(DataAdapter):\n             dtype = backend.standardize_dtype(x.dtype)\n             if isinstance(x, tf.RaggedTensor):\n                 return tf.RaggedTensorSpec(shape=shape, dtype=dtype)\n-            if isinstance(x, tf.SparseTensor) or is_scipy_sparse(x):\n+            if (\n+                isinstance(x, tf.SparseTensor)\n+                or is_scipy_sparse(x)\n+                or is_jax_sparse(x)\n+            ):\n                 return tf.SparseTensorSpec(shape=shape, dtype=dtype)\n             else:\n                 return tf.TensorSpec(shape=shape, dtype=dtype)\n@@ -55,15 +59,27 @@ class GeneratorDataAdapter(DataAdapter):\n         return data_adapter_utils.get_numpy_iterator(self.generator)\n \n     def get_jax_iterator(self):\n-        return data_adapter_utils.get_jax_iterator(self.generator)\n+        from keras.backend.jax.core import convert_to_tensor\n+\n+        def convert_to_jax(x):\n+            if is_scipy_sparse(x):\n+                return scipy_sparse_to_jax_sparse(x)\n+            elif is_tf_sparse(x):\n+                return tf_sparse_to_jax_sparse(x)\n+            return convert_to_tensor(x)\n+\n+        for batch in self.generator:\n+            yield tree.map_structure(convert_to_jax, batch)\n \n     def get_tf_dataset(self):\n         from keras.utils.module_utils import tensorflow as tf\n \n-        def convert_to_tf(batch):\n-            if is_scipy_sparse(batch):\n-                batch = scipy_sparse_to_tf_sparse(batch)\n-            return batch\n+        def convert_to_tf(x):\n+            if is_scipy_sparse(x):\n+                x = scipy_sparse_to_tf_sparse(x)\n+            elif is_jax_sparse(x):\n+                x = jax_sparse_to_tf_sparse(x)\n+            return x\n \n         def get_tf_iterator():\n             for batch in self.generator:\n@@ -102,13 +118,50 @@ def is_scipy_sparse(x):\n     )\n \n \n+def is_tf_sparse(x):\n+    return (\n+        x.__class__.__name__ == \"SparseTensor\"\n+        and x.__class__.__module__.startswith(\"tensorflow\")\n+    )\n+\n+\n+def is_jax_sparse(x):\n+    return x.__class__.__module__.startswith(\"jax.experimental.sparse\")\n+\n+\n def scipy_sparse_to_tf_sparse(x):\n     from keras.utils.module_utils import tensorflow as tf\n \n-    sparse_coo = x.tocoo()\n-    row, col = sparse_coo.row, sparse_coo.col\n-    data, shape = sparse_coo.data, sparse_coo.shape\n+    coo = x.tocoo()\n     indices = np.concatenate(\n-        (np.expand_dims(row, axis=1), np.expand_dims(col, axis=1)), axis=1\n+        (np.expand_dims(coo.row, 1), np.expand_dims(coo.col, 1)),\n+        axis=1,\n     )\n-    return tf.SparseTensor(indices, data, shape)\n+    return tf.SparseTensor(indices, coo.data, coo.shape)\n+\n+\n+def scipy_sparse_to_jax_sparse(x):\n+    import jax.experimental.sparse as jax_sparse\n+\n+    coo = x.tocoo()\n+    indices = np.concatenate(\n+        (np.expand_dims(coo.row, 1), np.expand_dims(coo.col, 1)),\n+        axis=1,\n+    )\n+    return jax_sparse.BCOO((coo.data, indices), shape=coo.shape)\n+\n+\n+def tf_sparse_to_jax_sparse(x):\n+    import jax.experimental.sparse as jax_sparse\n+\n+    from keras.backend.tensorflow.core import convert_to_numpy\n+\n+    values = convert_to_numpy(x.values)\n+    indices = convert_to_numpy(x.indices)\n+    return jax_sparse.BCOO((values, indices), shape=x.shape)\n+\n+\n+def jax_sparse_to_tf_sparse(x):\n+    from keras.utils.module_utils import tensorflow as tf\n+\n+    return tf.SparseTensor(x.indices, x.data, x.shape)\n\n@@ -1,6 +1,7 @@\n import math\n \n import jax\n+import jax.experimental.sparse as jax_sparse\n import numpy as np\n import scipy\n import tensorflow as tf\n@@ -100,48 +101,39 @@ class GeneratorDataAdapterTest(testing.TestCase, parameterized.TestCase):\n                 sample_order.append(by[i, 0])\n         self.assertAllClose(sample_order, list(range(34)))\n \n-    def test_tf_sparse_tensors_with_tf_dataset(self):\n-        def generate_tf():\n-            for i in range(4):\n-                x = tf.SparseTensor(\n-                    indices=[[0, 0], [1, 2]],\n-                    values=[1.0, 2.0],\n-                    dense_shape=(2, 4),\n+    @parameterized.named_parameters(\n+        named_product(\n+            generator_type=[\"tf\", \"jax\", \"scipy\"], iterator_type=[\"tf\", \"jax\"]\n         )\n-                y = tf.SparseTensor(\n-                    indices=[[0, 0], [1, 1]],\n-                    values=[3.0, 4.0],\n-                    dense_shape=(2, 2),\n     )\n+    def test_scipy_sparse_tensors(self, generator_type, iterator_type):\n+        if generator_type == \"tf\":\n+            x = tf.SparseTensor([[0, 0], [1, 2]], [1.0, 2.0], (2, 4))\n+            y = tf.SparseTensor([[0, 0], [1, 1]], [3.0, 4.0], (2, 2))\n+        elif generator_type == \"jax\":\n+            x = jax_sparse.BCOO(([1.0, 2.0], [[0, 0], [1, 2]]), shape=(2, 4))\n+            y = jax_sparse.BCOO(([3.0, 4.0], [[0, 0], [1, 1]]), shape=(2, 2))\n+        elif generator_type == \"scipy\":\n+            x = scipy.sparse.coo_matrix(([1.0, 2.0], ([0, 1], [0, 2])), (2, 4))\n+            y = scipy.sparse.coo_matrix(([3.0, 4.0], ([0, 1], [0, 1])), (2, 2))\n+\n+        def generate():\n+            for _ in range(4):\n                 yield x, y\n \n-        adapter = generator_data_adapter.GeneratorDataAdapter(generate_tf())\n-        ds = adapter.get_tf_dataset()\n-        for batch in ds:\n+        adapter = generator_data_adapter.GeneratorDataAdapter(generate())\n+\n+        if iterator_type == \"tf\":\n+            it = adapter.get_tf_dataset()\n+            expected_class = tf.SparseTensor\n+        elif iterator_type == \"jax\":\n+            it = adapter.get_jax_iterator()\n+            expected_class = jax_sparse.BCOO\n+\n+        for batch in it:\n             self.assertEqual(len(batch), 2)\n             bx, by = batch\n-            self.assertIsInstance(bx, tf.SparseTensor)\n-            self.assertIsInstance(by, tf.SparseTensor)\n-            self.assertEqual(bx.shape, (2, 4))\n-            self.assertEqual(by.shape, (2, 2))\n-\n-    def test_scipy_sparse_tensors_with_tf_dataset(self):\n-        def generate_scipy():\n-            for i in range(4):\n-                x = scipy.sparse.coo_matrix(\n-                    ([1.0, 2.0], ([0, 1], [0, 2])), shape=[2, 4]\n-                )\n-                y = scipy.sparse.coo_matrix(\n-                    ([3.0, 4.0], ([0, 1], [0, 1])), shape=[2, 2]\n-                )\n-                yield x, y\n-\n-        adapter = generator_data_adapter.GeneratorDataAdapter(generate_scipy())\n-        ds = adapter.get_tf_dataset()\n-        for batch in ds:\n-            self.assertEqual(len(batch), 2)\n-            bx, by = batch\n-            self.assertIsInstance(bx, tf.SparseTensor)\n-            self.assertIsInstance(by, tf.SparseTensor)\n+            self.assertIsInstance(bx, expected_class)\n+            self.assertIsInstance(by, expected_class)\n             self.assertEqual(bx.shape, (2, 4))\n             self.assertEqual(by.shape, (2, 2))\n\n@@ -42,9 +42,23 @@ class TFDatasetAdapter(DataAdapter):\n             yield tree.map_structure(convert_to_numpy, batch)\n \n     def get_jax_iterator(self):\n+        import jax.experimental.sparse as jax_sparse\n+\n+        from keras.backend.jax.core import convert_to_tensor\n+        from keras.backend.tensorflow.core import convert_to_numpy\n+        from keras.utils.module_utils import tensorflow as tf\n+\n+        def convert_to_jax(x):\n             # We use numpy as an intermediary because the conversion\n             # tf -> numpy -> jax is more than 2x faster than tf -> jax.\n-        return data_adapter_utils.get_jax_iterator(self.get_numpy_iterator())\n+            if isinstance(x, tf.SparseTensor):\n+                values = convert_to_numpy(x.values)\n+                indices = convert_to_numpy(x.indices)\n+                return jax_sparse.BCOO((values, indices), shape=x.shape)\n+            return convert_to_tensor(convert_to_numpy(x))\n+\n+        for batch in self._dataset:\n+            yield tree.map_structure(convert_to_jax, batch)\n \n     def get_tf_dataset(self):\n         return self._dataset\n\n@@ -258,7 +258,10 @@ class TestTFDatasetAdapter(testing.TestCase, parameterized.TestCase):\n                 self.assertEqual(tuple(bx.shape), (2, 4))\n                 self.assertEqual(tuple(by.shape), (2, 2))\n \n-    def test_tf_sparse_tensors(self):\n+    @parameterized.named_parameters(\n+        named_product(iterator_type=[\"np\", \"tf\", \"jax\"])\n+    )\n+    def test_tf_sparse_tensors(self, iterator_type):\n         x = tf.SparseTensor(\n             indices=[[0, 0], [1, 2]], values=[1.0, 2.0], dense_shape=(2, 4)\n         )\n@@ -268,19 +271,20 @@ class TestTFDatasetAdapter(testing.TestCase, parameterized.TestCase):\n         base_ds = tf.data.Dataset.from_tensors((x, y))\n         adapter = tf_dataset_adapter.TFDatasetAdapter(base_ds)\n \n-        gen = adapter.get_numpy_iterator()\n-        for batch in gen:\n+        if iterator_type == \"np\":\n+            it = adapter.get_numpy_iterator()\n+            expected_class = np.ndarray\n+        elif iterator_type == \"tf\":\n+            it = adapter.get_tf_dataset()\n+            expected_class = tf.SparseTensor\n+        elif iterator_type == \"jax\":\n+            it = adapter.get_jax_iterator()\n+            expected_class = jax.experimental.sparse.BCOO\n+\n+        for batch in it:\n             self.assertEqual(len(batch), 2)\n             bx, by = batch\n-            self.assertIsInstance(bx, np.ndarray)\n-            self.assertIsInstance(by, np.ndarray)\n-            self.assertEqual(bx.shape, (2, 4))\n-            self.assertEqual(by.shape, (2, 2))\n-        ds = adapter.get_tf_dataset()\n-        for batch in ds:\n-            self.assertEqual(len(batch), 2)\n-            bx, by = batch\n-            self.assertIsInstance(bx, tf.SparseTensor)\n-            self.assertIsInstance(by, tf.SparseTensor)\n+            self.assertIsInstance(bx, expected_class)\n+            self.assertIsInstance(by, expected_class)\n             self.assertEqual(bx.shape, (2, 4))\n             self.assertEqual(by.shape, (2, 2))\n\n@@ -16,6 +16,7 @@ from keras import optimizers\n from keras import testing\n from keras.callbacks.callback import Callback\n from keras.optimizers.rmsprop import RMSprop\n+from keras.testing.test_utils import named_product\n \n if backend.backend() == \"jax\":\n     from keras.backend.jax.trainer import JAXTrainer as Trainer\n@@ -98,26 +99,25 @@ def sparse_generator(generator_type):\n     if generator_type == \"scipy\":\n         import scipy\n \n-        for i in range(4):\n+        for _ in range(4):\n             x = scipy.sparse.random(2, 4, density=0.25, dtype=\"float32\")\n             y = np.random.rand(2, 3).astype(\"float32\")\n             yield x, y\n-    elif generator_type == \"tensorflow\":\n+    elif generator_type == \"tf\":\n         import tensorflow as tf\n \n-        for i in range(4):\n+        for _ in range(4):\n             x = tf.random.uniform((2, 4), dtype=\"float32\")\n             x = tf.sparse.from_dense(tf.nn.dropout(x, 0.25))\n             y = tf.random.uniform((2, 3), dtype=\"float32\")\n             yield x, y\n     elif generator_type == \"jax\":\n         import jax\n+        import jax.experimental.sparse as jax_sparse\n \n-        for i in range(4):\n+        for _ in range(4):\n             seed = jax.random.PRNGKey(0)\n-            x = jax.experimental.sparse.random_bcoo(\n-                seed, (2, 4), dtype=\"float32\", nse=0.25\n-            )\n+            x = jax_sparse.random_bcoo(seed, (2, 4), dtype=\"float32\", nse=0.25)\n             y = jax.random.uniform(seed, (2, 3), dtype=\"float32\")\n             yield x, y\n     else:\n@@ -307,32 +307,25 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         self.assertIn(\"val_loss\", history)\n \n     @parameterized.named_parameters(\n-        [\n-            (\"eager_tf_sparse\", True, False),\n-            (\"graph_fn_tf_sparse\", False, False),\n-            (\"eager_scipy_sparse\", True, False),\n-            (\"graph_fn_scipy_sparse\", False, False),\n-        ]\n+        named_product(\n+            generator_type=[\"tf\", \"jax\", \"scipy\"], mode=[\"eager\", \"graph\"]\n+        )\n     )\n     @pytest.mark.skipif(\n         not backend.SUPPORTS_SPARSE_TENSORS,\n         reason=\"Backend does not support sparse tensors.\",\n     )\n-    def test_fit_sparse(self, run_eagerly, use_scipy_sparse):\n+    def test_fit_sparse(self, generator_type, mode):\n         model = ExampleModel(units=3)\n         optimizer = optimizers.Adagrad()\n         model.compile(\n             optimizer=optimizer,\n             loss=losses.MeanSquaredError(),\n             metrics=[metrics.MeanSquaredError()],\n-            run_eagerly=run_eagerly,\n+            run_eagerly=(mode == \"eager\"),\n             jit_compile=False,\n         )\n-        dataset = (\n-            sparse_generator(\"scipy\")\n-            if use_scipy_sparse\n-            else sparse_generator(backend.backend())\n-        )\n+        dataset = sparse_generator(generator_type)\n \n         sparse_variable_updates = False\n \n@@ -381,31 +374,24 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(output[\"mean_squared_error\"], 16.0)\n \n     @parameterized.named_parameters(\n-        [\n-            (\"eager_tf_sparse\", True, False),\n-            (\"graph_fn_tf_sparse\", False, False),\n-            (\"eager_scipy_sparse\", True, False),\n-            (\"graph_fn_scipy_sparse\", False, False),\n-        ]\n+        named_product(\n+            generator_type=[\"tf\", \"jax\", \"scipy\"], mode=[\"eager\", \"graph\"]\n+        )\n     )\n     @pytest.mark.skipif(\n         not backend.SUPPORTS_SPARSE_TENSORS,\n         reason=\"Backend does not support sparse tensors.\",\n     )\n-    def test_evaluate_sparse(self, run_eagerly, use_scipy_sparse):\n+    def test_evaluate_sparse(self, generator_type, mode):\n         model = ExampleModel(units=3)\n         model.compile(\n             optimizer=optimizers.Adagrad(),\n             loss=losses.MeanSquaredError(),\n             metrics=[metrics.MeanSquaredError()],\n-            run_eagerly=run_eagerly,\n+            run_eagerly=(mode == \"eager\"),\n             jit_compile=False,\n         )\n-        dataset = (\n-            sparse_generator(\"scipy\")\n-            if use_scipy_sparse\n-            else sparse_generator(backend.backend())\n-        )\n+        dataset = sparse_generator(generator_type)\n         model.evaluate(dataset)\n \n     @parameterized.named_parameters(\n@@ -451,31 +437,24 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         self.assertAllClose(outputs[\"y_two\"], 4 * np.ones((100, 3)))\n \n     @parameterized.named_parameters(\n-        [\n-            (\"eager_tf_sparse\", True, False),\n-            (\"graph_fn_tf_sparse\", False, False),\n-            (\"eager_scipy_sparse\", True, False),\n-            (\"graph_fn_scipy_sparse\", False, False),\n-        ]\n+        named_product(\n+            generator_type=[\"tf\", \"jax\", \"scipy\"], mode=[\"eager\", \"graph\"]\n+        )\n     )\n     @pytest.mark.skipif(\n         not backend.SUPPORTS_SPARSE_TENSORS,\n         reason=\"Backend does not support sparse tensors.\",\n     )\n-    def test_predict_sparse(self, run_eagerly, use_scipy_sparse):\n+    def test_predict_sparse(self, generator_type, mode):\n         model = ExampleModel(units=3)\n         model.compile(\n             optimizer=optimizers.Adagrad(),\n             loss=losses.MeanSquaredError(),\n             metrics=[metrics.MeanSquaredError()],\n-            run_eagerly=run_eagerly,\n+            run_eagerly=(mode == \"eager\"),\n             jit_compile=False,\n         )\n-        dataset = (\n-            sparse_generator(\"scipy\")\n-            if use_scipy_sparse\n-            else sparse_generator(backend.backend())\n-        )\n+        dataset = sparse_generator(generator_type)\n         model.predict(dataset)\n \n     @pytest.mark.skipif(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c8700f40de66aa8a2e907564a874116bea2664fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 25 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -168,10 +168,10 @@ class Operation:\n \n             def get_config(self):\n                 config = super().get_config()\n-                config.update({\n+                config.update({{\n                     \"arg1\": self.arg1,\n                     \"arg2\": self.arg2,\n-                })\n+                }})\n                 return config\"\"\"\n                 )\n             )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#32bca81b819c8501714304e44a9ef380f54bb9e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 140 | Lines Deleted: 43 | Files Changed: 3 | Hunks: 22 | Methods Changed: 19 | Complexity Δ (Sum/Max): 18/10 | Churn Δ: 183 | Churn Cumulative: 957 | Contributors (this commit): 7 | Commits (past 90d): 19 | Contributors (cumulative): 12 | DMM Complexity: 0.7222222222222222\n\nDIFF:\n@@ -372,25 +372,23 @@ class JAXTrainer(base_trainer.Trainer):\n             self.reset_metrics()\n             callbacks.on_epoch_begin(epoch)\n \n-            trainable_variables = [v.value for v in self.trainable_variables]\n-            non_trainable_variables = [\n-                v.value for v in self.non_trainable_variables\n-            ]\n-            optimizer_variables = [v.value for v in self.optimizer.variables]\n-            metrics_variables = [v.value for v in self.metrics_variables]\n-\n-            self._purge_model_variables()\n+            self._jax_state_synced = True\n             for step, data in epoch_iterator.enumerate_epoch():\n                 # Callbacks\n                 callbacks.on_train_batch_begin(step)\n \n                 # Train step\n-                state = (\n-                    trainable_variables,\n-                    non_trainable_variables,\n-                    optimizer_variables,\n-                    metrics_variables,\n+                if self._jax_state_synced:\n+                    # The state may have been synced by a callback.\n+                    state = self._get_jax_state(\n+                        trainable_variables=True,\n+                        non_trainable_variables=True,\n+                        optimizer_variables=True,\n+                        metrics_variables=True,\n+                        purge_model_variables=True,\n                     )\n+                    self._jax_state_synced = False\n+\n                 logs, state = self.train_function(state, data)\n                 (\n                     trainable_variables,\n@@ -413,9 +411,10 @@ class JAXTrainer(base_trainer.Trainer):\n                 if self.stop_training:\n                     break\n \n-            # Reattach state to model variables.\n+            # Reattach state to the model (if not already done by a callback).\n             # NOTE: doing this after each step would be a big performance\n             # bottleneck.\n+            if not self._jax_state_synced:\n                 self.jax_state_sync()\n \n             # Override with model metrics instead of last step logs\n@@ -525,21 +524,20 @@ class JAXTrainer(base_trainer.Trainer):\n         logs = None\n         self.reset_metrics()\n \n-        trainable_variables = [v.value for v in self.trainable_variables]\n-        non_trainable_variables = [\n-            v.value for v in self.non_trainable_variables\n-        ]\n-        metrics_variables = [v.value for v in self.metrics_variables]\n-\n-        self._purge_model_variables(optimizer_variables=False)\n+        self._jax_state_synced = True\n         for step, data in epoch_iterator.enumerate_epoch():\n             callbacks.on_test_batch_begin(step)\n \n-            state = (\n-                trainable_variables,\n-                non_trainable_variables,\n-                metrics_variables,\n+            if self._jax_state_synced:\n+                # The state may have been synced by a callback.\n+                state = self._get_jax_state(\n+                    trainable_variables=True,\n+                    non_trainable_variables=True,\n+                    metrics_variables=True,\n+                    purge_model_variables=True,\n                 )\n+                self._jax_state_synced = False\n+\n             logs, state = self.test_function(state, data)\n             (\n                 trainable_variables,\n@@ -560,7 +558,8 @@ class JAXTrainer(base_trainer.Trainer):\n             if self.stop_evaluating:\n                 break\n \n-        # Reattach state back to model.\n+        # Reattach state back to model (if not already done by a callback).\n+        if not self._jax_state_synced:\n             self.jax_state_sync()\n \n         # The jax spmd_mode is need for multi-process context, since the\n@@ -628,19 +627,21 @@ class JAXTrainer(base_trainer.Trainer):\n                 )\n             return outputs\n \n-        trainable_variables = [v.value for v in self.trainable_variables]\n-        non_trainable_variables = [\n-            v.value for v in self.non_trainable_variables\n-        ]\n-        self._purge_model_variables(\n-            trainable_variables=False,\n-            optimizer_variables=False,\n-            metric_variables=False,\n-        )\n+        self._jax_state_synced = True\n         outputs = None\n+        non_trainable_variables = None\n         for step, x in epoch_iterator.enumerate_epoch():\n-            state = (trainable_variables, non_trainable_variables)\n             callbacks.on_predict_batch_begin(step)\n+            if self._jax_state_synced:\n+                # The state may have been synced by a callback.\n+                state = self._get_jax_state(\n+                    trainable_variables=True,\n+                    non_trainable_variables=True,\n+                )\n+                self._purge_model_variables(non_trainable_variables=True)\n+                self._jax_state_synced = False\n+            else:\n+                state = (state[0], non_trainable_variables)\n             batch_outputs, non_trainable_variables = self.predict_function(\n                 state, x\n             )\n@@ -654,6 +655,7 @@ class JAXTrainer(base_trainer.Trainer):\n             # during predict(), but it's allowed.\n             \"non_trainable_variables\": non_trainable_variables,\n         }\n+        if not self._jax_state_synced:\n             self.jax_state_sync()\n         callbacks.on_predict_end()\n         self._jax_state = None\n@@ -814,6 +816,7 @@ class JAXTrainer(base_trainer.Trainer):\n         if metrics_variables:\n             for ref_v, v in zip(self.metrics_variables, metrics_variables):\n                 ref_v.assign(v)\n+        self._jax_state_synced = True\n \n     def _record_training_state_sharding_spec(self):\n         self._trainable_variable_shardings = [\n@@ -882,10 +885,10 @@ class JAXTrainer(base_trainer.Trainer):\n \n     def _purge_model_variables(\n         self,\n-        trainable_variables=True,\n-        non_trainable_variables=True,\n-        optimizer_variables=True,\n-        metric_variables=True,\n+        trainable_variables=False,\n+        non_trainable_variables=False,\n+        optimizer_variables=False,\n+        metrics_variables=False,\n     ):\n         \"\"\"Remove all the model variable for memory saving.\n \n@@ -906,10 +909,36 @@ class JAXTrainer(base_trainer.Trainer):\n         if optimizer_variables:\n             for v in self.optimizer.variables:\n                 v._value = None\n-        if metric_variables:\n+        if metrics_variables:\n             for v in self.metrics_variables:\n                 v._value = None\n \n+    def _get_jax_state(\n+        self,\n+        trainable_variables=False,\n+        non_trainable_variables=False,\n+        optimizer_variables=False,\n+        metrics_variables=False,\n+        purge_model_variables=False,\n+    ):\n+        state = []\n+        if trainable_variables:\n+            state.append([v.value for v in self.trainable_variables])\n+        if non_trainable_variables:\n+            state.append([v.value for v in self.non_trainable_variables])\n+        if optimizer_variables:\n+            state.append([v.value for v in self.optimizer.variables])\n+        if metrics_variables:\n+            state.append([v.value for v in self.metrics_variables])\n+        if purge_model_variables:\n+            self._purge_model_variables(\n+                trainable_variables=trainable_variables,\n+                non_trainable_variables=non_trainable_variables,\n+                optimizer_variables=optimizer_variables,\n+                metrics_variables=metrics_variables,\n+            )\n+        return tuple(state)\n+\n \n def _distribute_data(data):\n     distribution = distribution_lib.distribution()\n\n@@ -82,6 +82,7 @@ class Callback:\n             # attached to the model in the middle of an\n             # epoch. We have to force a sync before\n             # accessing model state for e.g. checkpointing.\n+            if not self._model._jax_state_synced:\n                 self._model.jax_state_sync()\n         return self._model\n \n\n@@ -497,7 +497,7 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n \n             # Note that we access model via self._model since self.model\n             # will trigger a sync of the jax training state back to the model.\n-            def on_train_batch_begin(self, batch, logs=None):\n+            def on_train_batch_end(self, batch, logs=None):\n                 for v in self._model.trainable_variables:\n                     assert v._value is None\n                 for v in self._model.non_trainable_variables:\n@@ -507,7 +507,7 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n                 for v in self._model.metrics_variables:\n                     assert v._value is None\n \n-            def on_test_batch_begin(self, batch, logs=None):\n+            def on_test_batch_end(self, batch, logs=None):\n                 for v in self._model.non_trainable_variables:\n                     assert v._value is None\n                 for v in self._model.metrics_variables:\n@@ -1203,3 +1203,70 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         out_1 = model.predict(x)\n         out_2 = model.predict(x)\n         self.assertGreater(np.mean(np.abs(out_1 - out_2)), 0.01)\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_callbacks_can_update_state_at_batch_boundary(self):\n+\n+        class CounterModel(keras.Model):\n+            def __init__(self):\n+                super().__init__()\n+                self.train_counter = self.add_weight(\n+                    shape=(),\n+                    initializer=\"zeros\",\n+                )\n+                self.test_counter = self.add_weight(\n+                    shape=(),\n+                    initializer=\"zeros\",\n+                )\n+                self.predict_counter = self.add_weight(\n+                    shape=(),\n+                    initializer=\"zeros\",\n+                )\n+                self.dense = layers.Dense(3)\n+\n+            def call(self, x):\n+                return self.dense(x)\n+\n+        class CounterCallback(keras.callbacks.Callback):\n+            def __init__(self):\n+                self.eager_call_counter_train = 0\n+                self.eager_call_counter_test = 0\n+                self.eager_call_counter_predict = 0\n+\n+            def on_train_batch_end(self, *args, **kwargs):\n+                self.model.train_counter.assign_add(1)\n+                self.eager_call_counter_train += 1\n+\n+            def on_test_batch_end(self, *args, **kwargs):\n+                self.model.test_counter.assign_add(1)\n+                self.eager_call_counter_test += 1\n+\n+            def on_predict_batch_end(self, *args, **kwargs):\n+                self.model.predict_counter.assign_add(1)\n+                self.eager_call_counter_predict += 1\n+\n+        model = CounterModel()\n+        model.compile(\n+            optimizer=\"sgd\", loss=\"mse\", metrics=[\"mse\"], run_eagerly=True\n+        )\n+        cbk = CounterCallback()\n+        model.fit(\n+            np.ones((4, 3)),\n+            np.ones((4, 3)),\n+            callbacks=[cbk],\n+            epochs=3,\n+            batch_size=1,\n+            verbose=0,\n+            validation_data=(np.ones((2, 3)), np.ones((2, 3))),\n+        )\n+        self.assertAlmostEqual(cbk.eager_call_counter_train, 12)\n+        self.assertAlmostEqual(model.train_counter.numpy(), 12)\n+        self.assertAlmostEqual(cbk.eager_call_counter_test, 6)\n+        self.assertAlmostEqual(model.test_counter.numpy(), 6)\n+        model.predict(\n+            np.ones((4, 3)),\n+            callbacks=[cbk],\n+            batch_size=1,\n+        )\n+        self.assertAlmostEqual(cbk.eager_call_counter_predict, 4)\n+        self.assertAlmostEqual(model.predict_counter.numpy(), 4)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
