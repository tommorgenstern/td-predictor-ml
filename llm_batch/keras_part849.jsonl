{"custom_id": "keras#4b1e65e8fdaef0b2e4eba459d433b7eb343f09d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 23 | Churn Cumulative: 110 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: 0.5\n\nDIFF:\n@@ -211,13 +211,6 @@ class MultiHeadAttention(Layer):\n         \"\"\"\n         key_shape = value_shape if key_shape is None else key_shape\n \n-        if query_shape[-1] != value_shape[-1]:\n-            raise ValueError(\n-                \"The last dimension of `query_shape` and `value_shape` \"\n-                f\"must be equal, but are {query_shape[-1]}, {value_shape[-1]}. \"\n-                \"Received: query_shape={query_shape}, value_shape={value_shape}\"\n-            )\n-\n         if value_shape[1:-1] != key_shape[1:-1]:\n             raise ValueError(\n                 \"All dimensions of `value` and `key`, except the last one, \"\n@@ -604,13 +597,6 @@ class MultiHeadAttention(Layer):\n         if key_shape is None:\n             key_shape = value_shape\n \n-        if query_shape[-1] != value_shape[-1]:\n-            raise ValueError(\n-                \"The last dimension of `query_shape` and `value_shape` \"\n-                f\"must be equal, but are {query_shape[-1]}, {value_shape[-1]}. \"\n-                \"Received: query_shape={query_shape}, value_shape={value_shape}\"\n-            )\n-\n         if value_shape[1:-1] != key_shape[1:-1]:\n             raise ValueError(\n                 \"All dimensions of `value` and `key`, except the last one, \"\n\n@@ -104,6 +104,13 @@ class MultiHeadAttentionTest(testing.TestCase):\n             (1, 1, 5, 2),\n             (3, 2),\n         ),\n+        (\n+            \"different_qv_last_dims\",\n+            (4, 2, 3, 8),\n+            (4, 2, 3, 7),\n+            (4, 2, 3, 8),\n+            None,\n+        ),\n     )\n     def test_compute_output_shape(\n         self, query_dims, value_dims, key_dims, output_shape\n@@ -130,7 +137,7 @@ class MultiHeadAttentionTest(testing.TestCase):\n         self.assertEqual(output.shape, comp_output_shape)\n \n     @parameterized.named_parameters(\n-        (\"query_value_dim_mismatch\", (2, 4, 8), (2, 2, 7), 2),\n+        (\"query_value_dim_mismatch\", (2, 4, 8), (2, 2, 7), (2,)),\n         (\"key_value_dim_mismatch\", (2, 4, 8), (2, 2, 8), (2, 1, 7)),\n         (\n             \"key_value_dim_mismatch_high_dim\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#acceb5a995b82a006bb174b396844afc9f1fd052", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): 6/3 | Churn Δ: 45 | Churn Cumulative: 631 | Contributors (this commit): 9 | Commits (past 90d): 16 | Contributors (cumulative): 15 | DMM Complexity: 0.64\n\nDIFF:\n@@ -132,7 +132,7 @@ class Functional(Function, Model):\n         if not all(is_input_keras_tensor(t) for t in flat_inputs):\n             inputs, outputs = clone_graph_nodes(inputs, outputs)\n \n-        Function.__init__(self, inputs, outputs, name=name, **kwargs)\n+        Function.__init__(self, inputs, outputs, name=name)\n \n         if trainable is not None:\n             self.trainable = trainable\n@@ -494,8 +494,20 @@ def functional_from_config(cls, config, custom_objects=None):\n             # (e.g. a model such as A(B(A(B(x)))))\n             add_unprocessed_node(layer, node_data)\n \n+    # Extract config used to instantiate Functional model from the config. The\n+    # remaining config will be passed as keyword arguments to the Model\n+    # constructor.\n+    functional_config = {}\n+    for key in [\"layers\", \"input_layers\", \"output_layers\"]:\n+        functional_config[key] = config.pop(key)\n+    for key in [\"name\", \"trainable\"]:\n+        if key in config:\n+            functional_config[key] = config.pop(key)\n+        else:\n+            functional_config[key] = None\n+\n     # First, we create all layers and enqueue nodes to be processed\n-    for layer_data in config[\"layers\"]:\n+    for layer_data in functional_config[\"layers\"]:\n         process_layer(layer_data)\n \n     # Then we process nodes in order of layer depth.\n@@ -503,7 +515,7 @@ def functional_from_config(cls, config, custom_objects=None):\n     # does not yet exist) are re-enqueued, and the process\n     # is repeated until all nodes are processed.\n     while unprocessed_nodes:\n-        for layer_data in config[\"layers\"]:\n+        for layer_data in functional_config[\"layers\"]:\n             layer = created_layers[layer_data[\"name\"]]\n \n             # Process all nodes in layer, if not yet processed\n@@ -532,8 +544,8 @@ def functional_from_config(cls, config, custom_objects=None):\n                     del unprocessed_nodes[layer]\n \n     # Create list of input and output tensors and return new class\n-    name = config.get(\"name\")\n-    trainable = config.get(\"trainable\")\n+    name = functional_config[\"name\"]\n+    trainable = functional_config[\"trainable\"]\n \n     def get_tensor(layer_name, node_index, tensor_index):\n         assert layer_name in created_layers\n@@ -558,8 +570,8 @@ def functional_from_config(cls, config, custom_objects=None):\n             return tuple([map_tensors(v) for v in tensors])\n         return [map_tensors(v) for v in tensors]\n \n-    input_tensors = map_tensors(config[\"input_layers\"])\n-    output_tensors = map_tensors(config[\"output_layers\"])\n+    input_tensors = map_tensors(functional_config[\"input_layers\"])\n+    output_tensors = map_tensors(functional_config[\"output_layers\"])\n     if isinstance(input_tensors, list) and len(input_tensors) == 1:\n         input_tensors = input_tensors[0]\n     if isinstance(output_tensors, list) and len(output_tensors) == 1:\n@@ -570,6 +582,7 @@ def functional_from_config(cls, config, custom_objects=None):\n         outputs=output_tensors,\n         name=name,\n         trainable=trainable,\n+        **config,\n     )\n \n \n\n@@ -169,6 +169,24 @@ class ModelTest(testing.TestCase):\n         )\n         self.assertIsInstance(new_model, Functional)\n \n+    def test_reviving_functional_from_config_custom_model(self):\n+        class CustomModel(Model):\n+            def __init__(self, *args, param=1, **kwargs):\n+                super().__init__(*args, **kwargs)\n+                self.param = param\n+\n+            def get_config(self):\n+                base_config = super().get_config()\n+                config = {\"param\": self.param}\n+                return base_config | config\n+\n+        inputs = layers.Input((3,))\n+        outputs = layers.Dense(5)(inputs)\n+        model = CustomModel(inputs=inputs, outputs=outputs, param=3)\n+\n+        new_model = CustomModel.from_config(model.get_config())\n+        self.assertEqual(new_model.param, 3)\n+\n     @parameterized.named_parameters(\n         (\"single_output_1\", _get_model_single_output),\n         (\"single_output_2\", _get_model_single_output),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8e7843c9f8a3b3298730ab993fd6e6d7bf5c2b1c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 409 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -620,7 +620,7 @@ class CompileLoss(losses_module.Loss):\n     def call(self, y_true, y_pred, sample_weight=None):\n         if not self.built:\n             self.build(y_true, y_pred)\n-        else:\n+\n         # Filter unused inputs.\n         y_true, y_pred = self._filter_unused_inputs(\n             y_true,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#95ca6a6da5c128c695e92c5142898abf0837ba6a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 43 | Files Changed: 33 | Hunks: 43 | Methods Changed: 34 | Complexity Δ (Sum/Max): -17/1 | Churn Δ: 95 | Churn Cumulative: 3054 | Contributors (this commit): 27 | Commits (past 90d): 94 | Contributors (cumulative): 128 | DMM Complexity: 1.0\n\nDIFF:\n@@ -287,6 +287,7 @@ EPOCHS = 5\n print(\"\\nTraining:\")\n data_iter = iter(train_data)\n for epoch in range(EPOCHS):\n+    loss_value = None  # default\n     for i in tqdm(range(STEPS_PER_EPOCH)):\n         x, y = next(data_iter)\n         sharded_x = jax.device_put(x.numpy(), data_sharding)\n\n@@ -124,7 +124,7 @@ class CustomModel(keras.Model):\n         )\n \n         # Update metrics.\n-        new_metrics_vars = []\n+        new_metrics_vars, logs = [], []\n         for metric in self.metrics:\n             this_metric_vars = metrics_variables[\n                 len(new_metrics_vars) : len(new_metrics_vars)\n@@ -314,7 +314,7 @@ class CustomModel(keras.Model):\n         loss = self.compute_loss(x, y, y_pred)\n \n         # Update metrics.\n-        new_metrics_vars = []\n+        new_metrics_vars, logs = [], []\n         for metric in self.metrics:\n             this_metric_vars = metrics_variables[\n                 len(new_metrics_vars) : len(new_metrics_vars)\n\n@@ -252,6 +252,7 @@ train_state = get_replicated_train_state(devices)\n # Custom training loop\n for epoch in range(num_epochs):\n     data_iter = iter(train_data)\n+    loss_value = None  # default\n     for data in data_iter:\n         x, y = data\n         sharded_x = jax.device_put(x.numpy(), data_sharding)\n\n@@ -83,6 +83,8 @@ class ReLU(ops.Operation):\n                 negative_part = backend.nn.relu(-x + threshold)\n             else:\n                 negative_part = backend.nn.relu(-x)\n+        else:\n+            negative_part = 1\n \n         clip_max = max_value is not None\n         if threshold != 0:\n\n@@ -289,6 +289,8 @@ def DenseNet(\n                     cache_subdir=\"models\",\n                     file_hash=\"1ceb130c1ea1b78c3bf6114dbdfd8807\",\n                 )\n+            else:\n+                raise ValueError(\"weights_path undefined\")\n         else:\n             if blocks == [6, 12, 24, 16]:\n                 weights_path = file_utils.get_file(\n@@ -311,6 +313,8 @@ def DenseNet(\n                     cache_subdir=\"models\",\n                     file_hash=\"c13680b51ded0fb44dff2d8f86ac8bb1\",\n                 )\n+            else:\n+                raise ValueError(\"weights_path undefined\")\n         model.load_weights(weights_path)\n     elif weights is not None:\n         model.load_weights(weights)\n\n@@ -397,6 +397,7 @@ class JAXTrainer(base_trainer.Trainer):\n \n         self.make_train_function()\n         self.stop_training = False\n+        training_logs = {}\n         callbacks.on_train_begin()\n         initial_epoch = self._initial_epoch or initial_epoch\n         for epoch in range(initial_epoch, epochs):\n\n@@ -236,6 +236,7 @@ class TorchTrainer(base_trainer.Trainer):\n             )\n \n         self.stop_training = False\n+        training_logs = {}\n         self.make_train_function()\n         callbacks.on_train_begin()\n         initial_epoch = self._initial_epoch or initial_epoch\n\n@@ -134,6 +134,8 @@ class Attention(Layer):\n                 scores = self.concat_score_weight * ops.sum(\n                     ops.tanh(q_reshaped + k_reshaped), axis=-1\n                 )\n+        else:\n+            raise ValueError(\"scores not computed\")\n \n         return scores\n \n\n@@ -517,8 +517,7 @@ class FeatureSpace(Layer):\n             preprocessor = self.preprocessors[name]\n             # TODO: consider adding an adapt progress bar.\n             # Sample 1 element to check the rank\n-            for x in feature_dataset.take(1):\n-                pass\n+            x = next(iter(feature_dataset))\n             if len(x.shape) == 0:\n                 # The dataset yields unbatched scalars; batch it.\n                 feature_dataset = feature_dataset.batch(32)\n\n@@ -86,8 +86,7 @@ class HashedCrossingTest(testing.TestCase):\n             .batch(5)\n             .map(lambda x1, x2: layer((x1, x2)))\n         )\n-        for output in ds.take(1):\n-            output = output.numpy()\n+        output = next(iter(ds)).numpy()\n         self.assertAllClose(np.array([1, 4, 1, 1, 3]), output)\n \n     def test_unsupported_shape_input_fails(self):\n\n@@ -60,8 +60,7 @@ class HashingTest(testing.TestCase):\n         layer = layers.Hashing(num_bins=3)\n         inp = [[\"A\"], [\"B\"], [\"C\"], [\"D\"], [\"E\"]]\n         ds = tf.data.Dataset.from_tensor_slices(inp).batch(5).map(layer)\n-        for output in ds.take(1):\n-            output = output.numpy()\n+        output = next(iter(ds)).numpy()\n         self.assertAllClose(output, np.array([[1], [0], [1], [1], [2]]))\n \n     @parameterized.named_parameters(\n@@ -306,6 +305,8 @@ class HashingTest(testing.TestCase):\n             symbolic_sample_shape = ()\n         elif input_array.ndim == 2:\n             symbolic_sample_shape = (None,)\n+        else:\n+            raise TypeError(\"Unknown `symbolic_sample_shape`\")\n         inputs = layers.Input(shape=symbolic_sample_shape, dtype=\"int32\")\n         layer = layers.Hashing(num_bins=3, output_mode=\"count\")\n         outputs = layer(inputs)\n\n@@ -171,8 +171,7 @@ class CenterCropTest(testing.TestCase):\n         layer = layers.CenterCrop(8, 9)\n         input_data = np.random.random(input_shape)\n         ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n-        for output in ds.take(1):\n-            output = output.numpy()\n+        output = next(iter(ds)).numpy()\n         self.assertEqual(tuple(output.shape), output_shape)\n \n     # TODO\n\n@@ -54,8 +54,7 @@ class RandomContrastTest(testing.TestCase):\n         layer = layers.RandomContrast(factor=0.5, seed=1337)\n         input_data = np.random.random((2, 8, 8, 3))\n         ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n-        for output in ds.take(1):\n-            output.numpy()\n+        next(iter(ds)).numpy()\n \n     def test_dict_input(self):\n         layer = layers.RandomContrast(factor=0.1, bounding_box_format=\"xyxy\")\n\n@@ -136,8 +136,7 @@ class RandomCropTest(testing.TestCase):\n             output_shape = (2, 3, 8, 9)\n         input_data = np.random.random(input_shape)\n         ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n-        for output in ds.take(1):\n-            output = output.numpy()\n+        output = next(iter(ds)).numpy()\n         self.assertEqual(tuple(output.shape), output_shape)\n \n     def test_dict_input(self):\n\n@@ -141,8 +141,7 @@ class RandomFlipTest(testing.TestCase):\n         input_data = np.array([[[2, 3, 4]], [[5, 6, 7]]])\n         expected_output = np.array([[[5, 6, 7]], [[2, 3, 4]]])\n         ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n-        for output in ds.take(1):\n-            output = output.numpy()\n+        output = next(iter(ds)).numpy()\n         self.assertAllClose(output, expected_output)\n         # Test 4D input: shape (2, 2, 1, 3)\n         layer = layers.RandomFlip(\n@@ -167,6 +166,5 @@ class RandomFlipTest(testing.TestCase):\n             ]\n         )\n         ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n-        for output in ds.take(1):\n-            output = output.numpy()\n+        output = next(iter(ds)).numpy()\n         self.assertAllClose(output, expected_output)\n\n@@ -73,6 +73,5 @@ class RandomRotationTest(testing.TestCase):\n                 [4, 3, 2, 1, 0],\n             ]\n         ).reshape(input_shape[1:])\n-        for output in ds.take(1):\n-            output = output.numpy()\n+        output = next(iter(ds)).numpy()\n         self.assertAllClose(expected_output, output)\n\n@@ -327,5 +327,4 @@ class RandomTranslationTest(testing.TestCase):\n         layer = layers.RandomTranslation(0.2, 0.1)\n         input_data = np.random.random((1, 4, 4, 3))\n         ds = tf_data.Dataset.from_tensor_slices(input_data).batch(1).map(layer)\n-        for output in ds.take(1):\n-            output.numpy()\n+        next(iter(ds)).numpy()\n\n@@ -119,8 +119,7 @@ class RandomZoomTest(testing.TestCase):\n                 [0, 0, 0, 0, 0],\n             ]\n         ).reshape(input_shape)\n-        for output in ds.take(1):\n-            output = output.numpy()\n+        output = next(iter(ds)).numpy()\n         self.assertAllClose(expected_output, output)\n \n     def test_dynamic_shape(self):\n\n@@ -186,8 +186,7 @@ class ResizingTest(testing.TestCase):\n         layer = layers.Resizing(8, 9)\n         input_data = np.random.random(input_shape)\n         ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n-        for output in ds.take(1):\n-            output = output.numpy()\n+        output = next(iter(ds)).numpy()\n         self.assertEqual(tuple(output.shape), output_shape)\n \n     @pytest.mark.skipif(\n@@ -210,8 +209,7 @@ class ResizingTest(testing.TestCase):\n             .batch(2)\n             .map(Sequential([layer]))\n         )\n-        for output in ds.take(1):\n-            output = output.numpy()\n+        output = next(iter(ds)).numpy()\n         self.assertEqual(tuple(output.shape), output_shape)\n \n     @parameterized.parameters(\n\n@@ -102,6 +102,5 @@ class IntegerLookupTest(testing.TestCase):\n         )\n         input_data = [2, 3, 4, 5]\n         ds = tf_data.Dataset.from_tensor_slices(input_data).batch(4).map(layer)\n-        for output in ds.take(1):\n-            output = output.numpy()\n+        output = next(iter(ds)).numpy()\n         self.assertAllClose(output, np.array([2, 3, 4, 0]))\n\n@@ -277,6 +277,8 @@ class Normalization(TFDataLayer):\n                     batch_var + (batch_mean - new_total_mean) ** 2\n                 ) * batch_weight\n                 total_mean = new_total_mean\n+        else:\n+            raise NotImplementedError(type(data))\n \n         self.adapt_mean.assign(total_mean)\n         self.adapt_variance.assign(total_var)\n\n@@ -65,6 +65,8 @@ class NormalizationTest(testing.TestCase):\n             data = backend.convert_to_tensor(x)\n         elif input_type == \"tf.data\":\n             data = tf_data.Dataset.from_tensor_slices(x).batch(8)\n+        else:\n+            raise NotImplementedError(input_type)\n \n         layer = layers.Normalization()\n         layer.adapt(data)\n\n@@ -72,8 +72,7 @@ class RescalingTest(testing.TestCase):\n         layer = layers.Rescaling(scale=1.0 / 255, offset=0.5)\n         x = np.random.random((3, 10, 10, 3)) * 255\n         ds = tf_data.Dataset.from_tensor_slices(x).batch(3).map(layer)\n-        for output in ds.take(1):\n-            output.numpy()\n+        next(iter(ds)).numpy()\n \n     def test_rescaling_with_channels_first_and_vector_scale(self):\n         config = backend.image_data_format()\n\n@@ -77,8 +77,7 @@ class StringLookupTest(testing.TestCase):\n         )\n         input_data = [\"b\", \"c\", \"d\"]\n         ds = tf_data.Dataset.from_tensor_slices(input_data).batch(3).map(layer)\n-        for output in ds.take(1):\n-            output = output.numpy()\n+        output = next(iter(ds)).numpy()\n         self.assertAllClose(output, np.array([2, 3, 0]))\n \n     @pytest.mark.skipif(not backend.backend() == \"tensorflow\", reason=\"tf only\")\n\n@@ -95,8 +95,7 @@ class TextVectorizationTest(testing.TestCase):\n         )\n         input_data = [[\"foo qux bar\"], [\"qux baz\"]]\n         ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n-        for output in ds.take(1):\n-            output = output.numpy()\n+        output = next(iter(ds)).numpy()\n         self.assertAllClose(output, np.array([[4, 1, 3, 0], [1, 2, 0, 0]]))\n \n         # Test adapt flow\n@@ -107,8 +106,7 @@ class TextVectorizationTest(testing.TestCase):\n         )\n         layer.adapt(input_data)\n         ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n-        for output in ds.take(1):\n-            output.numpy()\n+        next(iter(ds)).numpy()\n \n     @pytest.mark.skipif(\n         backend.backend() != \"tensorflow\", reason=\"Requires string tensors.\"\n\n@@ -1279,6 +1279,8 @@ def relu(x, alpha=0.0, max_value=None, threshold=0.0):\n             negative_part = tf.nn.relu(-x + threshold)\n         else:\n             negative_part = tf.nn.relu(-x)\n+    else:\n+        negative_part = 1\n \n     clip_max = max_value is not None\n \n\n@@ -397,6 +397,7 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n     def build_from_config(self, config):\n         if not config:\n             return\n+        status = False\n         if \"input_shape\" in config:\n             # Case: all inputs are in the first arg (possibly nested).\n             if utils.is_default(self.build):\n@@ -408,7 +409,7 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n                     self.build(config[\"input_shape\"])\n                     status = True\n                 except:\n-                    status = False\n+                    pass\n             self._build_shapes_dict = config\n \n         elif \"shapes_dict\" in config:\n@@ -420,7 +421,7 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n                     self.build(**config[\"shapes_dict\"])\n                     status = True\n                 except:\n-                    status = False\n+                    pass\n             self._build_shapes_dict = config[\"shapes_dict\"]\n \n         if not status:\n\n@@ -359,6 +359,7 @@ class Sequential(Model):\n             model.add(layer)\n         if (\n             not model._functional\n+            and \"build_input_shape\" in locals()\n             and build_input_shape\n             and isinstance(build_input_shape, (tuple, list))\n         ):\n\n@@ -318,7 +318,7 @@ def map_graph(inputs, outputs):\n                         \"The following previous operations were accessed \"\n                         f\"without issue: {operations_with_complete_input}\"\n                     )\n-                operations_with_complete_input.append(operation.name)\n+                operations_with_complete_input.append(node.operation.name)\n \n             for x in tree.flatten(node.outputs):\n                 computable_tensors.add(x)\n\n@@ -914,6 +914,8 @@ class BaseOptimizer(KerasSaveable):\n             learning_rate = serialization_lib.serialize_keras_object(\n                 self._learning_rate\n             )\n+        else:\n+            learning_rate = 0.5\n \n         config = {\n             \"name\": self.name,\n\n@@ -170,6 +170,7 @@ class CompileMetrics(metrics_module.Metric):\n         return vars\n \n     def build(self, y_true, y_pred):\n+        num_outputs = 1  # default\n         if self.output_names:\n             output_names = self.output_names\n         elif isinstance(y_pred, dict):\n@@ -182,7 +183,6 @@ class CompileMetrics(metrics_module.Metric):\n                 output_names = None\n         else:\n             output_names = None\n-            num_outputs = 1\n         if output_names:\n             num_outputs = len(output_names)\n \n\n@@ -60,6 +60,7 @@ def pack_sequence_as(structure, flat_sequence, sequence_fn=None):\n             )\n         return flat_sequence[0]\n \n+    packed = []\n     try:\n         final_index, packed = packed_nest_with_indices(\n             structure, flat_sequence, 0, is_nested_fn, sequence_fn\n\n@@ -100,9 +100,11 @@ def extract_archive(file_path, path=\".\", archive_format=\"auto\"):\n         if archive_type == \"tar\":\n             open_fn = tarfile.open\n             is_match_fn = tarfile.is_tarfile\n-        if archive_type == \"zip\":\n+        elif archive_type == \"zip\":\n             open_fn = zipfile.ZipFile\n             is_match_fn = zipfile.is_zipfile\n+        else:\n+            raise NotImplementedError(archive_type)\n \n         if is_match_fn(file_path):\n             with open_fn(file_path) as archive:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e0533f86b2c08076efb59dd9305a9d4fe635f2c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 7 | Churn Cumulative: 1012 | Contributors (this commit): 10 | Commits (past 90d): 6 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4348,6 +4348,13 @@ class Pad(Operation):\n         return pad_width\n \n     def call(self, x, constant_values=None):\n+        if len(self.pad_width) > 1 and len(self.pad_width) != len(x.shape):\n+            raise ValueError(\n+                \"`pad_width` must have the same length as `x.shape`. \"\n+                f\"Received: pad_width={self.pad_width} \"\n+                f\"(of length {len(self.pad_width)}) and x.shape={x.shape} \"\n+                f\"(of length {len(x.shape)})\"\n+            )\n         return backend.numpy.pad(\n             x,\n             pad_width=self.pad_width,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#713382b7cb3da0665a5657b7d8210baa4315a5ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 219 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -146,7 +146,7 @@ class Embedding(Layer):\n         return ops.not_equal(inputs, 0)\n \n     def compute_output_shape(self, input_shape):\n-        return input_shape + (self.output_dim,)\n+        return (*input_shape, self.output_dim)\n \n     def enable_lora(\n         self, rank, a_initializer=\"he_uniform\", b_initializer=\"zeros\"\n\n@@ -2,6 +2,7 @@ import types\n \n from keras.src.distribution import distribution_lib\n from keras.src.trainers.data_adapters import array_data_adapter\n+from keras.src.trainers.data_adapters import data_adapter\n from keras.src.trainers.data_adapters import py_dataset_adapter\n from keras.src.trainers.data_adapters.array_data_adapter import ArrayDataAdapter\n from keras.src.trainers.data_adapters.generator_data_adapter import (\n@@ -23,6 +24,10 @@ def get_data_adapter(\n     shuffle=False,\n     class_weight=None,\n ):\n+    # Allow passing a custom data adapter.\n+    if isinstance(x, data_adapter.DataAdapter):\n+        return x\n+\n     # Check for multi-process/worker distribution. Since only tf.dataset\n     # is supported at the moment, we will raise error if the inputs fail\n     # the type check\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3de677e047976098bb5871ef710ffe7f4e96301c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 16 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -105,8 +105,8 @@ class SpectralNormalization(Wrapper):\n                 ops.matmul(vector_u, ops.transpose(weights)), axis=None\n             )\n             vector_u = normalize(ops.matmul(vector_v, weights), axis=None)\n-        # vector_u = tf.stop_gradient(vector_u)\n-        # vector_v = tf.stop_gradient(vector_v)\n+        vector_u = ops.stop_gradient(vector_u)\n+        vector_v = ops.stop_gradient(vector_v)\n         sigma = ops.matmul(\n             ops.matmul(vector_v, weights), ops.transpose(vector_u)\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#813fbc548f4b30ccd95dd1b09ed5d43ee5ec6f34", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 111 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -17,7 +17,8 @@ class Loss(KerasSaveable):\n     Args:\n         reduction: Type of reduction to apply to the loss. In almost all cases\n             this should be `\"sum_over_batch_size\"`.\n-            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            Supported options are `\"sum\"`, `\"sum_over_batch_size\"`, `\"mean\"`\n+            or `None`.\n         name: Optional name for the loss instance.\n         dtype: The dtype of the loss's computations. Defaults to `None`, which\n             means using `keras.backend.floatx()`. `keras.backend.floatx()` is a\n@@ -95,7 +96,7 @@ class Loss(KerasSaveable):\n \n \n def standardize_reduction(reduction):\n-    allowed = {\"sum_over_batch_size\", \"sum\", None, \"none\"}\n+    allowed = {\"sum_over_batch_size\", \"sum\", None, \"none\", \"mean\"}\n     if reduction not in allowed:\n         raise ValueError(\n             \"Invalid value for argument `reduction`. \"\n@@ -135,7 +136,7 @@ def reduce_values(values, reduction=\"sum_over_batch_size\"):\n     ):\n         return values\n     loss = ops.sum(values)\n-    if reduction == \"sum_over_batch_size\":\n+    if reduction in (\"mean\", \"sum_over_batch_size\"):\n         loss /= ops.cast(\n             ops.prod(ops.convert_to_tensor(ops.shape(values), dtype=\"int32\")),\n             loss.dtype,\n@@ -180,7 +181,7 @@ def apply_mask(sample_weight, mask, dtype, reduction):\n     \"\"\"Applies any mask on predictions to sample weights.\"\"\"\n     if mask is not None:\n         mask = ops.cast(mask, dtype=dtype)\n-        if reduction == \"sum_over_batch_size\":\n+        if reduction in (\"mean\", \"sum_over_batch_size\"):\n             # Valid entries have weight `total/valid`, while invalid ones\n             # have 0. When summed over batch, they will be reduced to:\n             #\n\n@@ -69,7 +69,7 @@ class LossTest(testing.TestCase):\n         self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n         self.assertAllClose(np.sum((y_true - y_pred) ** 2), loss)\n \n-        # sum_over_batch_size\n+        # sum_over_batch_size or mean\n         loss_fn = ExampleLoss(reduction=\"sum_over_batch_size\")\n         loss = loss_fn(y_true, y_pred)\n         self.assertEqual(backend.standardize_dtype(loss.dtype), \"float32\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3d32481cfb0672e552d3965146bb30f25ee7d589", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 10 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -93,9 +93,9 @@ class DepthwiseConv2D(BaseDepthwiseConv):\n     Example:\n \n     >>> x = np.random.rand(4, 10, 10, 12)\n-    >>> y = keras.layers.DepthwiseConv2D(3, 3, activation='relu')(x)\n+    >>> y = keras.layers.DepthwiseConv2D(kernel_size=3, activation='relu')(x)\n     >>> print(y.shape)\n-    (4, 8, 8, 36)\n+    (4, 8, 8, 12)\n     \"\"\"\n \n     def __init__(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#90dca1581ac714f1157eaf2e1b37c9d87953f65e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 307 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 6 | Methods Changed: 12 | Complexity Δ (Sum/Max): 14/8 | Churn Δ: 307 | Churn Cumulative: 613 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -51,6 +51,10 @@ from keras.src.metrics.confusion_metrics import SensitivityAtSpecificity\n from keras.src.metrics.confusion_metrics import SpecificityAtSensitivity\n from keras.src.metrics.confusion_metrics import TrueNegatives\n from keras.src.metrics.confusion_metrics import TruePositives\n+from keras.src.metrics.correlation_metrics import ConcordanceCorrelation\n+from keras.src.metrics.correlation_metrics import PearsonCorrelation\n+from keras.src.metrics.correlation_metrics import concordance_correlation\n+from keras.src.metrics.correlation_metrics import pearson_correlation\n from keras.src.metrics.f_score_metrics import F1Score\n from keras.src.metrics.f_score_metrics import FBetaScore\n from keras.src.metrics.hinge_metrics import CategoricalHinge\n\n@@ -45,6 +45,10 @@ from keras.src.metrics.confusion_metrics import SensitivityAtSpecificity\n from keras.src.metrics.confusion_metrics import SpecificityAtSensitivity\n from keras.src.metrics.confusion_metrics import TrueNegatives\n from keras.src.metrics.confusion_metrics import TruePositives\n+from keras.src.metrics.correlation_metrics import ConcordanceCorrelation\n+from keras.src.metrics.correlation_metrics import PearsonCorrelation\n+from keras.src.metrics.correlation_metrics import concordance_correlation\n+from keras.src.metrics.correlation_metrics import pearson_correlation\n from keras.src.metrics.f_score_metrics import F1Score\n from keras.src.metrics.f_score_metrics import FBetaScore\n from keras.src.metrics.hinge_metrics import CategoricalHinge\n\n@@ -18,6 +18,8 @@ from keras.src.metrics.confusion_metrics import SensitivityAtSpecificity\n from keras.src.metrics.confusion_metrics import SpecificityAtSensitivity\n from keras.src.metrics.confusion_metrics import TrueNegatives\n from keras.src.metrics.confusion_metrics import TruePositives\n+from keras.src.metrics.correlation_metrics import ConcordanceCorrelation\n+from keras.src.metrics.correlation_metrics import PearsonCorrelation\n from keras.src.metrics.f_score_metrics import F1Score\n from keras.src.metrics.f_score_metrics import FBetaScore\n from keras.src.metrics.hinge_metrics import CategoricalHinge\n@@ -77,6 +79,9 @@ ALL_OBJECTS = {\n     SpecificityAtSensitivity,\n     TrueNegatives,\n     TruePositives,\n+    # Correlation\n+    ConcordanceCorrelation,\n+    PearsonCorrelation,\n     # Hinge\n     Hinge,\n     SquaredHinge,\n\n@@ -0,0 +1,215 @@\n+from keras.src import backend\n+from keras.src import ops\n+from keras.src.api_export import keras_export\n+from keras.src.losses.loss import squeeze_or_expand_to_same_rank\n+from keras.src.metrics import reduction_metrics\n+\n+\n+@keras_export(\"keras.metrics.pearson_correlation\")\n+def pearson_correlation(y_true, y_pred, axis=-1):\n+    \"\"\"Computes the Pearson coefficient between labels and predictions.\n+\n+    Formula:\n+\n+    ```python\n+    loss = mean(l2norm(y_true - mean(y_true) * l2norm(y_pred - mean(y_pred)))\n+    ```\n+\n+    Args:\n+        y_true: Tensor of true targets.\n+        y_pred: Tensor of predicted targets.\n+        axis: Axis along which to determine similarity. Defaults to `-1`.\n+\n+    Returns:\n+        Pearson Correlation Coefficient tensor.\n+\n+    Example:\n+\n+    >>> y_true = [[0, 1, 0.5], [1, 1, 0.2]]\n+    >>> y_pred = [[0.1, 0.9, 0.5], [1, 0.9, 0.2]]\n+    >>> loss = keras.losses.concordance_correlation(\n+    ...     y_true, y_pred, axis=-1\n+    ... ).numpy()\n+    [1.         0.99339927]\n+    \"\"\"\n+    y_pred = ops.convert_to_tensor(y_pred)\n+    y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n+    y_true, y_pred = squeeze_or_expand_to_same_rank(y_true, y_pred)\n+\n+    y_true_norm = y_true - ops.mean(y_true, axis=axis, keepdims=True)\n+    y_pred_norm = y_pred - ops.mean(y_pred, axis=axis, keepdims=True)\n+\n+    y_true_norm = y_true_norm / ops.std(y_true_norm, axis=axis, keepdims=True)\n+    y_pred_norm = y_pred_norm / ops.std(y_pred_norm, axis=axis, keepdims=True)\n+\n+    return ops.mean(y_true_norm * y_pred_norm, axis=axis)\n+\n+\n+@keras_export(\"keras.metrics.concordance_correlation\")\n+def concordance_correlation(y_true, y_pred, axis=-1):\n+    \"\"\"Computes the Concordance coefficient between labels and predictions.\n+\n+    Formula:\n+\n+    ```python\n+    loss = mean(\n+        2 * (y_true - mean(y_true) * (y_pred - mean(y_pred)) / (\n+            var(y_true) + var(y_pred) + square(mean(y_true) - mean(y_pred))\n+        )\n+    )\n+    ```\n+\n+    Args:\n+        y_true: Tensor of true targets.\n+        y_pred: Tensor of predicted targets.\n+        axis: Axis along which to determine similarity. Defaults to `-1`.\n+\n+    Returns:\n+        Concordance Correlation Coefficient tensor.\n+\n+    Example:\n+\n+    >>> y_true = [[0, 1, 0.5], [1, 1, 0.2]]\n+    >>> y_pred = [[0.1, 0.9, 0.5], [1, 0.9, 0.2]]\n+    >>> loss = keras.losses.concordance_correlation(\n+    ...     y_true, y_pred, axis=-1\n+    ... ).numpy()\n+    [0.97560976 0.98765432]\n+    \"\"\"\n+    y_pred = ops.convert_to_tensor(y_pred)\n+    y_true = ops.convert_to_tensor(y_true, dtype=y_pred.dtype)\n+    y_true, y_pred = squeeze_or_expand_to_same_rank(y_true, y_pred)\n+\n+    y_true_mean = ops.mean(y_true, axis=axis, keepdims=True)\n+    y_pred_mean = ops.mean(y_pred, axis=axis, keepdims=True)\n+\n+    y_true_var = ops.var(y_true - y_true_mean, axis=axis, keepdims=True)\n+    y_pred_var = ops.var(y_pred - y_pred_mean, axis=axis, keepdims=True)\n+\n+    covar = (y_true - y_pred_mean) * (y_pred - y_pred_mean)\n+    norm = y_true_var + y_pred_var + ops.square(y_true_mean - y_pred_mean)\n+\n+    return ops.mean(2 * covar / (norm + backend.epsilon()), axis=axis)\n+\n+\n+@keras_export(\"keras.metrics.PearsonCorrelation\")\n+class PearsonCorrelation(reduction_metrics.MeanMetricWrapper):\n+    \"\"\"Calculates the Pearson Correlation Coefficient (PCC).\n+\n+    PCC measures the linear relationship between the true values (`y_true`) and\n+    the predicted values (`y_pred`). The coefficient ranges from -1 to 1, where\n+    a value of 1 implies a perfect positive linear correlation, 0 indicates no\n+    linear correlation, and -1 indicates a perfect negative linear correlation.\n+\n+    This metric is widely used in regression tasks where the strength of the\n+    linear relationship between predictions and true labels is an\n+    important evaluation criterion.\n+\n+    Args:\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+        axis: (Optional) integer or tuple of integers of the axis/axes along\n+            which to compute the metric. Defaults to `-1`.\n+\n+    Example:\n+\n+    >>> pcc = keras.metrics.PearsonCorrelation(axis=-1)\n+    >>> y_true = [[0, 1, 0.5], [1, 1, 0.2]]\n+    >>> y_pred = [[0.1, 0.9, 0.5], [1, 0.9, 0.2]]\n+    >>> pcc.update_state(y_true, y_pred)\n+    >>> pcc.result()\n+    0.9966996338993913\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(optimizer='sgd',\n+                  loss='mean_squared_error',\n+                  metrics=[keras.metrics.PearsonCorrelation()])\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        name=\"pearson_correlation\",\n+        dtype=None,\n+        axis=-1,\n+    ):\n+        super().__init__(\n+            fn=pearson_correlation,\n+            name=name,\n+            dtype=dtype,\n+            axis=axis,\n+        )\n+        self.axis = axis\n+        # Metric should be maximized during optimization.\n+        self._direction = \"up\"\n+\n+    def get_config(self):\n+        return {\n+            \"name\": self.name,\n+            \"dtype\": self.dtype,\n+            \"axis\": self.axis,\n+        }\n+\n+\n+@keras_export(\"keras.metrics.ConcordanceCorrelation\")\n+class ConcordanceCorrelation(reduction_metrics.MeanMetricWrapper):\n+    \"\"\"Calculates the Concordance Correlation Coefficient (CCC).\n+\n+    CCC evaluates the agreement between true values (`y_true`) and predicted\n+    values (`y_pred`) by considering both precision and accuracy. The\n+    coefficient ranges from -1 to 1, where a value of 1 indicates perfect\n+    agreement.\n+\n+    This metric is useful in regression tasks where it is important to assess\n+    how well the predictions match the true values, taking into account both\n+    their correlation and proximity to the 45-degree line of perfect\n+    concordance.\n+\n+    Args:\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+        axis: (Optional) integer or tuple of integers of the axis/axes along\n+            which to compute the metric. Defaults to `-1`.\n+\n+    Example:\n+\n+    >>> ccc = keras.metrics.ConcordanceCorrelation(axis=-1)\n+    >>> y_true = [[0, 1, 0.5], [1, 1, 0.2]]\n+    >>> y_pred = [[0.1, 0.9, 0.5], [1, 0.9, 0.2]]\n+    >>> ccc.update_state(y_true, y_pred)\n+    >>> ccc.result()\n+    0.9816320385426076\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(optimizer='sgd',\n+                  loss='mean_squared_error',\n+                  metrics=[keras.metrics.ConcordanceCorrelation()])\n+    ```\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        name=\"concordance_correlation\",\n+        dtype=None,\n+        axis=-1,\n+    ):\n+        super().__init__(\n+            fn=concordance_correlation,\n+            name=name,\n+            dtype=dtype,\n+            axis=axis,\n+        )\n+        self.axis = axis\n+        # Metric should be maximized during optimization.\n+        self._direction = \"up\"\n+\n+    def get_config(self):\n+        return {\n+            \"name\": self.name,\n+            \"dtype\": self.dtype,\n+            \"axis\": self.axis,\n+        }\n\n@@ -0,0 +1,79 @@\n+import numpy as np\n+from scipy.stats import pearsonr\n+\n+from keras.src import testing\n+from keras.src.metrics import ConcordanceCorrelation\n+from keras.src.metrics import PearsonCorrelation\n+from keras.src.metrics import correlation_metrics\n+\n+\n+class CorrelationsTest(testing.TestCase):\n+    def _get_data(self):\n+        # Sample data for testing\n+        y_true = np.array(\n+            [[0, 1, 0.5], [1, 1, 0.2], [1, 1, 0.1], [0.1, 0.7, 0.0]],\n+            dtype=\"float32\",\n+        )\n+        y_pred = np.array(\n+            [[0.1, 0.9, 0.5], [1, 0.9, 0.2], [0.2, 0.8, 0], [0.3, 0.3, 0.9]],\n+            dtype=\"float32\",\n+        )\n+\n+        ccc_expected = np.array(\n+            [0.97560976, 0.98765432, 0.46511628, -0.46376812]\n+        )\n+        # pcc_expected = np.array([1, 0.99339927, 0.69337525, -0.60999428])\n+        pcc_expected = np.array(\n+            [pearsonr(yt, yp).statistic for yt, yp in zip(y_true, y_pred)]\n+        )\n+        return y_true, y_pred, ccc_expected, pcc_expected\n+\n+    def test_pearson_function(self):\n+        \"\"\"Test the functional API for Pearson Correlation Coefficient.\"\"\"\n+        y_true, y_pred, _, pcc_expected = self._get_data()\n+        result = correlation_metrics.pearson_correlation(\n+            y_true, y_pred, axis=-1\n+        )\n+        self.assertAllClose(result, pcc_expected)\n+\n+    def test_concordance_function(self):\n+        \"\"\"Test the functional API for Concordance Correlation Coefficient.\"\"\"\n+        y_true, y_pred, ccc_expected, _ = self._get_data()\n+        result = correlation_metrics.concordance_correlation(\n+            y_true, y_pred, axis=-1\n+        )\n+        self.assertAllClose(result, ccc_expected)\n+\n+    def test_pearson_class(self):\n+        \"\"\"Test the PearsonCorrelation metric class.\"\"\"\n+        y_true, y_pred, _, pcc_expected = self._get_data()\n+        m = PearsonCorrelation(axis=-1, dtype=\"float32\")\n+        m.update_state(y_true[:2], y_pred[:2])\n+        self.assertAllClose(m.result(), np.mean(pcc_expected[:2]))\n+        m.update_state(y_true[2:], y_pred[2:])\n+        self.assertAllClose(m.result(), np.mean(pcc_expected))\n+\n+    def test_concordance_class(self):\n+        \"\"\"Test the ConcordanceCorrelation metric class.\"\"\"\n+        y_true, y_pred, ccc_expected, _ = self._get_data()\n+        m = ConcordanceCorrelation(axis=-1, dtype=\"float32\")\n+        m.update_state(y_true[:2], y_pred[:2])\n+        self.assertAllClose(m.result(), np.mean(ccc_expected[:2]))\n+        m.update_state(y_true[2:], y_pred[2:])\n+        self.assertAllClose(m.result(), np.mean(ccc_expected))\n+\n+    def test_pearson_config(self):\n+        \"\"\"Test the get_config method for PearsonCorrelation.\"\"\"\n+        m = PearsonCorrelation(axis=-1, dtype=\"float16\")\n+        config = m.get_config()\n+        self.assertEqual(config[\"axis\"], -1)\n+        self.assertEqual(config[\"dtype\"], \"float16\")\n+        self.assertEqual(config[\"name\"], \"pearson_correlation\")\n+\n+    def test_concordance_config(self):\n+        \"\"\"Test the get_config method for ConcordanceCorrelation.\"\"\"\n+        m = ConcordanceCorrelation(axis=-1, dtype=\"float32\")\n+        config = m.get_config()\n+        self.assertEqual(config[\"axis\"], -1)\n+        self.assertEqual(config[\"dtype\"], \"float32\")\n+        self.assertEqual(config[\"name\"], \"concordance_correlation\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bce176f7a239b32ad321e8d7a019588ee4217baa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 22 | Churn Cumulative: 186 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -66,6 +66,14 @@ class Pipeline(Layer):\n             mask = tree.map_structure(_get_mask_from_keras_tensor, outputs)\n         return outputs\n \n+    @classmethod\n+    def from_config(cls, config):\n+        config[\"layers\"] = [\n+            serialization_lib.deserialize_keras_object(x)\n+            for x in config[\"layers\"]\n+        ]\n+        return cls(**config)\n+\n     def get_config(self):\n         config = {\n             \"layers\": serialization_lib.serialize_keras_object(\n\n@@ -72,3 +72,17 @@ class PipelineTest(testing.TestCase):\n         for output in ds.take(1):\n             output = output.numpy()\n         self.assertEqual(tuple(output.shape), output_shape)\n+\n+    def test_from_config(self):\n+        pipeline = layers.Pipeline(\n+            [\n+                layers.AutoContrast(),\n+                layers.CenterCrop(8, 9),\n+            ]\n+        )\n+        x = np.ones((2, 10, 12, 3))\n+        output = pipeline(x)\n+        restored = layers.Pipeline.from_config(pipeline.get_config())\n+        restored_output = restored(x)\n+        self.assertEqual(tuple(output.shape), (2, 8, 9, 3))\n+        self.assertAllClose(output, restored_output)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c03eccc06f9b1313ca9b4a2642903caf709f1d64", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 9 | Churn Cumulative: 44 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.8\n\nDIFF:\n@@ -3,6 +3,12 @@ import weakref\n from keras.src.backend.common import global_state\n \n \n+def _clear_tensor_attr(tensor_id, attr):\n+    attr_dict = global_state.get_global_attribute(f\"{attr}_dict\")\n+    if attr_dict is not None and tensor_id in attr_dict:\n+        del attr_dict[tensor_id]\n+\n+\n def set_tensor_attr(tensor, attr, value):\n     try:\n         setattr(tensor, attr, value)\n@@ -11,10 +17,11 @@ def set_tensor_attr(tensor, attr, value):\n         if attr_dict is None:\n             if value is None:\n                 return\n-            attr_dict = weakref.WeakValueDictionary()\n+            attr_dict = {}\n             global_state.set_global_attribute(f\"{attr}_dict\", attr_dict)\n         if value is not None:\n             attr_dict[id(tensor)] = value\n+            weakref.finalize(tensor, _clear_tensor_attr, id(tensor), attr)\n         elif id(tensor) in attr_dict:\n             del attr_dict[id(tensor)]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
