{"custom_id": "scrapy#af0e1c40f5082910b22000df4b0068003611948f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 756 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -183,8 +183,9 @@ class Scraper(object):\n         errors that got propagated thru here)\n         \"\"\"\n         if spider_failure is download_failure:\n-            log.msg(\"Error downloading %s: %s\" % \\\n-                (request, spider_failure.getErrorMessage()), log.ERROR, spider=spider)\n+            errmsg = spider_failure.getErrorMessage()\n+            if errmsg:\n+                log.msg(\"Error downloading %s: %s\" % (request, errmsg), log.ERROR, spider=spider)\n             return\n         return spider_failure\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6e8edbd72ef2214422c0267b908aaa1a4a5125d6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 540 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -18,13 +18,13 @@ elif settings['SELECTORS_BACKEND'] == 'dummy':\n     from scrapy.selector.dummysel import *\n else:\n     try:\n-        import libxml2\n+        import lxml\n     except ImportError:\n         try:\n-            import lxml\n+            import libxml2\n         except ImportError:\n             from scrapy.selector.dummysel import *\n-        else:\n-            from scrapy.selector.lxmlsel import *\n         else:\n             from scrapy.selector.libxml2sel import *\n+    else:\n+        from scrapy.selector.lxmlsel import *\n\n@@ -120,12 +120,8 @@ try:\n except ImportError:\n     from distutils.core import setup\n else:\n-    setup_args['install_requires'] = ['Twisted>=8.0', 'w3lib>=1.1', 'pyOpenSSL']\n+    setup_args['install_requires'] = ['Twisted>=8.0', 'w3lib>=1.1', 'lxml', 'pyOpenSSL']\n     if sys.version_info < (2, 6):\n         setup_args['install_requires'] += ['simplejson']\n-    try:\n-        import libxml2\n-    except ImportError:\n-        setup_args['install_requires'] += ['lxml']\n  \n setup(**setup_args)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4f28ffcb2cc476e5c6e02ffddbef963b4a5a4e5b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 350 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -12,7 +12,7 @@ from scrapy.http import Request, Response\n class SpiderReferencer(object):\n     \"\"\"Class to serialize (and deserialize) objects (typically dicts)\n     containing references to running spiders (ie. Spider objects). This is\n-    required because simplejson fails to serialize dicts containing\n+    required because json library fails to serialize dicts containing\n     non-primitive types as keys, even when you override\n     ScrapyJSONEncoder.default() with a custom encoding mechanism.\n     \"\"\"\n\n@@ -121,7 +121,5 @@ except ImportError:\n     from distutils.core import setup\n else:\n     setup_args['install_requires'] = ['Twisted>=8.0', 'w3lib>=1.1', 'lxml', 'pyOpenSSL']\n-    if sys.version_info < (2, 6):\n-        setup_args['install_requires'] += ['simplejson']\n  \n setup(**setup_args)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a0a1a5026bc710d8478d3da2b1bdf7afb41b8e67", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 45 | Files Changed: 1 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 66 | Churn Cumulative: 321 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 0.8260869565217391\n\nDIFF:\n@@ -10,6 +10,7 @@ import urllib\n from lxml import html\n \n from scrapy.http.request import Request\n+from scrapy.utils.python import unicode_to_str\n \n \n class MultipleElementsFound(Exception):\n@@ -20,66 +21,41 @@ class FormRequest(Request):\n \n     def __init__(self, *args, **kwargs):\n         formdata = kwargs.pop('formdata', None)\n+        if formdata and kwargs.get('method') is None:\n+            kwargs['method'] = 'POST'\n+\n         super(FormRequest, self).__init__(*args, **kwargs)\n \n         if formdata:\n             items = formdata.iteritems() if isinstance(formdata, dict) else formdata\n-            query = [(unicode_to_str(k, self.encoding), _unicode_to_str(v, self.encoding))\n-                    for k, v in items]\n-            self.method = 'POST'\n-            self._set_body(urllib.urlencode(query, doseq=1))\n-            self.headers['Content-Type'] = 'application/x-www-form-urlencoded'\n+            querystr = _urlencode(items, self.encoding)\n+            if self.method == 'POST':\n+                self.headers.setdefault('Content-Type', 'application/x-www-form-urlencoded')\n+                self._set_body(querystr)\n+            else:\n+                self._set_url(self.url + ('&' if '?' in self.url else '?') + querystr)\n \n     @classmethod\n     def from_response(cls, response, formname=None, formnumber=0, formdata=None,\n                       clickdata=None, dont_click=False, **kwargs):\n         if not hasattr(formdata, \"items\"):\n             try:\n-                if formdata:\n-                    formdata = dict(formdata)\n-                else: formdata = {}\n+                formdata = dict(formdata) if formdata else {}\n             except (ValueError, TypeError):\n                 raise ValueError('formdata should be a dict or iterable of tuples')\n \n-        encoding = kwargs.get('encoding', response.encoding or 'UTF-8')\n-        hxs = html.fromstring(response.body_as_unicode(),\n-                              base_url=response.url)\n+        kwargs.setdefault('encoding', response.encoding)\n+        hxs = html.fromstring(response.body_as_unicode(), base_url=response.url)\n         form = _get_form(hxs, formname, formnumber, response)\n-        inputs = _get_inputs(form, formdata, dont_click, clickdata, response)\n-        values = [(_unicode_to_str(key, encoding), _unicode_to_str(value, encoding))\n-                  for key,value in inputs]\n-        if form.action:\n-            url = form.action\n-        else:\n-            url = form.base_url\n-        if form.method == \"POST\":\n-            kwargs.setdefault('headers', {}).update(\n-                        {'Content-Type':'application/x-www-form-urlencoded'})\n-            body = urllib.urlencode(values, doseq=1)\n-        else:\n-            if '?' in url:\n-                url += '&'\n-            else:\n-                url += '?'\n-            url += urllib.urlencode(values, doseq=1)\n-            body=None\n+        formdata = _get_inputs(form, formdata, dont_click, clickdata, response)\n+        url = form.action or form.base_url\n+        return cls(url, method=form.method, formdata=formdata, **kwargs)\n \n-        return cls(url, method=form.method, body=body, encoding=encoding, **kwargs)\n-\n-\n-def unicode_to_str(text, encoding='utf-8', errors='strict'):\n-    if isinstance(text, unicode):\n-        return text.encode(encoding, errors)\n-    elif isinstance(text, str):\n-        return unicode(text, 'utf-8', errors).encode(encoding, errors)\n-    else:\n-        raise TypeError('unicode_to_str must receive a unicode or str object, got %s' % type(text).__name__)\n-\n-def _unicode_to_str(string, encoding):\n-    if hasattr(string, '__iter__'):\n-        return [unicode_to_str(k, encoding) for k in string]\n-    else:\n-        return unicode_to_str(string, encoding)\n+def _urlencode(seq, enc):\n+    values = [(unicode_to_str(k, enc), unicode_to_str(v, enc))\n+              for k, vs in seq\n+              for v in (vs if hasattr(vs, '__iter__') else [vs])]\n+    return urllib.urlencode(values, doseq=1)\n \n def _get_form(hxs, formname, formnumber, response):\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#02833e3265eece72c7a0ea992adea53824ad883f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 112 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,4 @@\n-\"\"\"Helper functinos for working with signals\"\"\"\n+\"\"\"Helper functions for working with signals\"\"\"\n \n from twisted.internet.defer import maybeDeferred, DeferredList, Deferred\n from twisted.python.failure import Failure\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f13a54720362998c8be27ae39fe8ee96b7e1be30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 2 | Churn Cumulative: 323 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -133,7 +133,7 @@ def _get_inputs(form, formdata, dont_click, clickdata, response):\n         clickable = _get_clickable(clickdata, clickables, form)\n         inputs.append(clickable)\n \n-    inputs.extend([(key, value) for key, value in formdata.iteritems()])\n+    inputs.extend(formdata.iteritems())\n     return inputs\n \n def _get_clickable(clickdata, clickables, form):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c760cc5cd83d8753a30eba3bc6d285b0dbcccc32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 15 | Churn Cumulative: 338 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 0.8333333333333334\n\nDIFF:\n@@ -12,6 +12,8 @@ from lxml import html\n from scrapy.http.request import Request\n from scrapy.utils.python import unicode_to_str\n \n+XHTML_NAMESPACE = \"http://www.w3.org/1999/xhtml\"\n+\n \n class MultipleElementsFound(Exception):\n     pass\n@@ -51,6 +53,13 @@ class FormRequest(Request):\n         url = form.action or form.base_url\n         return cls(url, method=form.method, formdata=formdata, **kwargs)\n \n+# Copied from lxml.html to avoid relying on a non-public function\n+def _nons(tag):\n+    if isinstance(tag, basestring):\n+        if tag[0] == '{' and tag[1:len(XHTML_NAMESPACE)+1] == XHTML_NAMESPACE:\n+            return tag.split('}')[-1]\n+    return tag\n+\n def _urlencode(seq, enc):\n     values = [(unicode_to_str(k, enc), unicode_to_str(v, enc))\n               for k, vs in seq\n@@ -89,15 +98,17 @@ def _get_inputs(form, formdata, dont_click, clickdata, response):\n     \"\"\"\n     clickables = []\n     inputs = []\n+    xmlns_in_resp = u' xmlns' in response.body_as_unicode()[:200]\n+\n     for el in form.inputs:\n         name = el.name\n         if not name or name in formdata:\n             continue\n-        tag = html._nons(el.tag)\n+        tag = _nons(el.tag)\n         if tag == 'textarea':\n             inputs.append((name, el.value))\n         elif tag == 'select':\n-            if u' xmlns' in response.body_as_unicode()[:200]:\n+            if xmlns_in_resp:\n                 #use builtin select parser with namespaces\n                 value = el.value\n             else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7cca916ed5e6ac5b896e77824f5b1baca71c9235", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 16 | Churn Cumulative: 48 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,9 +23,23 @@ def setup(app):\n         indextemplate = \"pair: %s; reqmeta\",\n     )\n     app.add_role('source', source_role)\n+    app.add_role('commit', commit_role)\n+    app.add_role('rev', rev_role)\n \n def source_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n-    url = 'https://github.com/scrapy/scrapy/blob/master/' + text\n+    ref = 'https://github.com/scrapy/scrapy/blob/master/' + text\n     set_classes(options)\n     node = nodes.reference(rawtext, text, refuri=ref, **options)\n     return [node], []\n+\n+def commit_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n+    ref = 'https://github.com/scrapy/scrapy/commit/' + text\n+    set_classes(options)\n+    node = nodes.reference(rawtext, 'commit ' + text, refuri=ref, **options)\n+    return [node], []\n+\n+def rev_role(name, rawtext, text, lineno, inliner, options={}, content=[]):\n+    ref = 'http://hg.scrapy.org/scrapy/changeset/' + text\n+    set_classes(options)\n+    node = nodes.reference(rawtext, 'r' + text, refuri=ref, **options)\n+    return [node], []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3dbe211d29c84aaceceb80fc9572ed597aac94d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 13 | Churn Cumulative: 517 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -68,6 +68,11 @@ class XPathSelector(object_ref):\n             return etree.tostring(self.root, method=self._tostring_method, \\\n                 encoding=unicode)\n         except (AttributeError, TypeError):\n+            if self.root is True:\n+                return u'1'\n+            elif self.root is False:\n+                return u'0'\n+            else:\n                 return unicode(self.root)\n \n     def register_namespace(self, prefix, uri):\n\n@@ -63,12 +63,8 @@ class XPathSelectorTestCase(unittest.TestCase):\n         body = \"<p><input name='a'value='1'/><input name='b'value='2'/></p>\"\n         response = TextResponse(url=\"http://example.com\", body=body)\n         xs = self.hxs_cls(response)\n-        true = xs.select(\"//input[@name='a']/@name='a'\").extract()[0]\n-        false = xs.select(\"//input[@name='a']/@name='n'\").extract()[0]\n-\n-        # the actual result depends on the backend used\n-        assert true in [u'1', u'True'], true\n-        assert false in [u'0', u'False'], false\n+        self.assertEquals(xs.select(\"//input[@name='a']/@name='a'\").extract(), [u'1'])\n+        self.assertEquals(xs.select(\"//input[@name='a']/@name='n'\").extract(), [u'0'])\n \n     @libxml2debug\n     def test_selector_xml_html(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e4d22cb16a1e312ac6fbb1325a55d98cd97156ea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 46 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -14/0 | Churn Δ: 48 | Churn Cumulative: 386 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -91,52 +91,8 @@ def _get_form(hxs, formname, formnumber, response):\n             return form\n \n def _get_inputs(form, formdata, dont_click, clickdata, response):\n-    \"\"\"\n-    Returns all the inputs that will be sent with the request,\n-    both those already present in the form and those given by\n-    the user\n-    \"\"\"\n-    clickables = []\n-    inputs = []\n-    xmlns_in_resp = u' xmlns' in response.body_as_unicode()[:200]\n-\n-    for el in form.inputs:\n-        name = el.name\n-        if not name or name in formdata:\n-            continue\n-        tag = _nons(el.tag)\n-        if tag == 'textarea':\n-            inputs.append((name, el.value))\n-        elif tag == 'select':\n-            if xmlns_in_resp:\n-                #use builtin select parser with namespaces\n-                value = el.value\n-            else:\n-                value = el.xpath(\".//option[@selected]\") or None\n-\n-            if el.multiple:\n-                for v in value:\n-                    if v is not None:\n-                        inputs.append((name, v))\n-            elif value is not None:\n-                inputs.append((name, value[0] if isinstance(value, list)\n-                                               else value))\n-            else:\n-                option = el.xpath(\".//option[1]/@value\")\n-                if option:\n-                    inputs.append((name, option[0]))\n-        else:\n-            assert tag == 'input', (\"Unexpected tag: %r\" % el)\n-            if el.checkable and not el.checked:\n-                continue\n-            if el.type in ('image', 'reset'):\n-                continue\n-            elif el.type == 'submit':\n-                clickables.append(el)\n-            else:\n-                value = el.value\n-                if value is not None:\n-                    inputs.append((name, el.value))\n+    inputs = [(n, v) for n, v in form.form_values() if n not in formdata]\n+    clickables = [el for el in form.inputs if el.type == 'submit']\n \n     # If we are allowed to click on buttons and we have clickable\n     # elements, we move on to see if we have any clickdata\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#32b9f788be278ede969af4c1d1bf703a44e75dfb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 32 | Files Changed: 1 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 48 | Churn Cumulative: 434 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 0.2857142857142857\n\nDIFF:\n@@ -12,8 +12,6 @@ from lxml import html\n from scrapy.http.request import Request\n from scrapy.utils.python import unicode_to_str\n \n-XHTML_NAMESPACE = \"http://www.w3.org/1999/xhtml\"\n-\n \n class MultipleElementsFound(Exception):\n     pass\n@@ -53,13 +51,6 @@ class FormRequest(Request):\n         url = form.action or form.base_url\n         return cls(url, method=form.method, formdata=formdata, **kwargs)\n \n-# Copied from lxml.html to avoid relying on a non-public function\n-def _nons(tag):\n-    if isinstance(tag, basestring):\n-        if tag[0] == '{' and tag[1:len(XHTML_NAMESPACE)+1] == XHTML_NAMESPACE:\n-            return tag.split('}')[-1]\n-    return tag\n-\n def _urlencode(seq, enc):\n     values = [(unicode_to_str(k, enc), unicode_to_str(v, enc))\n               for k, vs in seq\n@@ -92,11 +83,10 @@ def _get_form(hxs, formname, formnumber, response):\n \n def _get_inputs(form, formdata, dont_click, clickdata, response):\n     inputs = [(n, v) for n, v in form.form_values() if n not in formdata]\n-    clickables = [el for el in form.inputs if el.type == 'submit']\n \n-    # If we are allowed to click on buttons and we have clickable\n-    # elements, we move on to see if we have any clickdata\n-    if not dont_click and clickables:\n+    if not dont_click:\n+        clickables = [el for el in form.inputs if el.type == 'submit']\n+        if clickables:\n             clickable = _get_clickable(clickdata, clickables, form)\n             inputs.append(clickable)\n \n@@ -109,11 +99,14 @@ def _get_clickable(clickdata, clickables, form):\n     if the latter is given. If not, it returns the first\n     clickable element found\n     \"\"\"\n-    # If clickdata is given, we compare it to the clickable elements\n-    # to find a match\n-    if clickdata is not None:\n-        # We first look to see if the number is specified in\n-        # clickdata, because that uniquely identifies the element\n+    # If we don't have clickdata, we just use the first clickable element\n+    if clickdata is None:\n+        el = clickables.pop(0)\n+        return (el.name, el.value)\n+\n+    # If clickdata is given, we compare it to the clickable elements to find a\n+    # match. We first look to see if the number is specified in clickdata,\n+    # because that uniquely identifies the element\n     nr = clickdata.get('nr', None)\n     if nr is not None:\n         try:\n@@ -123,10 +116,8 @@ def _get_clickable(clickdata, clickables, form):\n         else:\n             return (el.name, el.value)\n \n-        # We didn't find it, so now we build an XPath expression\n-        # out of the other arguments, because they can be used\n-        # as such\n-        else:\n+    # We didn't find it, so now we build an XPath expression out of the other\n+    # arguments, because they can be used as such\n     xpath_pred = []\n     for k, v in clickdata.items():\n         if k == 'coord':\n@@ -136,15 +127,8 @@ def _get_clickable(clickdata, clickables, form):\n     xpath_expr = '//*%s' % ''.join(xpath_pred)\n     el = form.xpath(xpath_expr)\n     if len(el) > 1:\n-                raise MultipleElementsFound(\n-                        \"Multiple elements found (%r) matching the criteria\"\n-                        \" in clickdata: %r\" % (el, clickdata)\n-                )\n+        raise MultipleElementsFound(\"Multiple elements found (%r) \"\n+                                    \"matching the criteria in clickdata: %r\"\n+                                    % (el, clickdata))\n     else:\n         return (el[0].name, el[0].value)\n-\n-    # If we don't have clickdata, we just use the first\n-    # clickable element\n-    else:\n-        el = clickables.pop(0)\n-        return (el.name, el.value)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ee1f7847a437f81b8d6b7be1dce3d4a49b39701c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 71 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/4 | Churn Δ: 81 | Churn Cumulative: 940 | Contributors (this commit): 6 | Commits (past 90d): 11 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -118,17 +118,14 @@ def _get_clickable(clickdata, clickables, form):\n \n     # We didn't find it, so now we build an XPath expression out of the other\n     # arguments, because they can be used as such\n-    xpath_pred = []\n-    for k, v in clickdata.items():\n-        if k == 'coord':\n-            v = ','.join(str(c) for c in v)\n-        xpath_pred.append('[@%s=\"%s\"]' % (k, v))\n-\n-    xpath_expr = '//*%s' % ''.join(xpath_pred)\n-    el = form.xpath(xpath_expr)\n-    if len(el) > 1:\n+    xpath = u'.//*' + \\\n+            u''.join(u'[@%s=\"%s\"]' % tuple(c) for c in clickdata.iteritems())\n+    el = form.xpath(xpath)\n+    if len(el) == 1:\n+        return (el[0].name, el[0].value)\n+    elif len(el) > 1:\n         raise MultipleElementsFound(\"Multiple elements found (%r) \"\n                                     \"matching the criteria in clickdata: %r\"\n                                     % (el, clickdata))\n     else:\n-        return (el[0].name, el[0].value)\n+        raise ValueError('No clickeable element matching clickdata')\n\n@@ -306,6 +306,59 @@ class FormRequestTest(RequestTest):\n         self.assertEqual(urlargs['one'], ['1'])\n         self.assertEqual(urlargs['two'], ['2'])\n \n+    def test_from_response_multiple_clickdata(self):\n+        respbody = \"\"\"\n+<form action=\"get.php\" method=\"GET\">\n+<input type=\"submit\" name=\"clickeable\" value=\"clicked1\">\n+<input type=\"submit\" name=\"clickeable\" value=\"clicked2\">\n+<input type=\"hidden\" name=\"one\" value=\"clicked1\">\n+<input type=\"hidden\" name=\"two\" value=\"clicked2\">\n+</form>\n+        \"\"\"\n+        response = HtmlResponse(\"http://www.example.com/this/list.html\", body=respbody)\n+        r1 = self.request_class.from_response(response, \\\n+                clickdata={'name': 'clickeable', 'value': 'clicked2'})\n+        urlargs = cgi.parse_qs(urlparse(r1.url).query)\n+        self.assertEqual(urlargs['clickeable'], ['clicked2'])\n+        self.assertEqual(urlargs['one'], ['clicked1'])\n+        self.assertEqual(urlargs['two'], ['clicked2'])\n+\n+    def test_from_response_unicode_clickdata(self):\n+        body = u\"\"\"\n+<form action=\"get.php\" method=\"GET\">\n+<input type=\"submit\" name=\"price in \\u00a3\" value=\"\\u00a3 1000\">\n+<input type=\"submit\" name=\"price in \\u20ac\" value=\"\\u20ac 2000\">\n+<input type=\"hidden\" name=\"poundsign\" value=\"\\u00a3\">\n+<input type=\"hidden\" name=\"eurosign\" value=\"\\u20ac\">\n+</form>\n+        \"\"\"\n+        response = HtmlResponse(\"http://www.example.com\", body=body, \\\n+                                encoding='utf-8')\n+        r1 = self.request_class.from_response(response, \\\n+                clickdata={'name': u'price in \\u00a3'})\n+        urlargs = cgi.parse_qs(urlparse(r1.url).query)\n+        self.assertTrue(urlargs[u'price in \\u00a3'.encode('utf-8')])\n+\n+    def test_from_response_multiple_forms_clickdata(self):\n+        body = u\"\"\"\n+        <form name=\"form1\">\n+          <input type=\"submit\" name=\"clickeable\" value=\"clicked\">\n+          <input type=\"hidden\" name=\"field1\" value=\"value1\">\n+        </form>\n+        <form name=\"form2\">\n+          <input type=\"submit\" name=\"clickeable\" value=\"clicked\">\n+          <input type=\"hidden\" name=\"field2\" value=\"value2\">\n+        </form>\n+        \"\"\"\n+        res = HtmlResponse(\"http://example.com\", body=body, encoding='utf-8')\n+        req = self.request_class.from_response(res, \\\n+                                              formname='form2', \\\n+                                              clickdata={'name': 'clickeable'})\n+        urlargs = cgi.parse_qs(urlparse(req.url).query)\n+        self.assertEqual(urlargs['clickeable'], ['clicked'])\n+        self.assertEqual(urlargs['field2'], ['value2'])\n+        self.assertFalse('field1' in urlargs, urlargs)\n+\n     def test_from_response_dont_click(self):\n         respbody = \"\"\"\n <form action=\"get.php\" method=\"GET\">\n@@ -336,6 +389,17 @@ class FormRequestTest(RequestTest):\n                           response,\n                           clickdata={'type': 'submit'})\n \n+    def test_from_response_non_matching_clickdata(self):\n+        body = \"\"\"\n+        <form>\n+          <input type=\"submit\" name=\"clickeable\" value=\"clicked\">\n+        </form>\n+        \"\"\"\n+        res = HtmlResponse(\"http://example.com\", body=body)\n+        self.assertRaises(ValueError,\n+                          self.request_class.from_response, res,\n+                                clickdata={'nonexistent': 'notme'})\n+\n     def test_from_response_errors_noform(self):\n         respbody = \"\"\"<html></html>\"\"\"\n         response = HtmlResponse(\"http://www.example.com/lala.html\", body=respbody)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2904dc2dc0c5241e1ab403ea9d27de348df5e8b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 954 | Contributors (this commit): 6 | Commits (past 90d): 13 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,10 +13,6 @@ from scrapy.http.request import Request\n from scrapy.utils.python import unicode_to_str\n \n \n-class MultipleElementsFound(Exception):\n-    pass\n-\n-\n class FormRequest(Request):\n \n     def __init__(self, *args, **kwargs):\n@@ -124,8 +120,7 @@ def _get_clickable(clickdata, clickables, form):\n     if len(el) == 1:\n         return (el[0].name, el[0].value)\n     elif len(el) > 1:\n-        raise MultipleElementsFound(\"Multiple elements found (%r) \"\n-                                    \"matching the criteria in clickdata: %r\"\n-                                    % (el, clickdata))\n+        raise ValueError(\"Multiple elements found (%r) matching the criteria \"\n+                         \"in clickdata: %r\" % (el, clickdata))\n     else:\n-        raise ValueError('No clickeable element matching clickdata')\n+        raise ValueError('No clickeable element matching clickdata: %r' % (clickdata,))\n\n@@ -5,7 +5,6 @@ from cStringIO import StringIO\n from urlparse import urlparse\n \n from scrapy.http import Request, FormRequest, XmlRpcRequest, Headers, HtmlResponse\n-from scrapy.http.request.form import MultipleElementsFound\n \n \n class RequestTest(unittest.TestCase):\n@@ -384,7 +383,7 @@ class FormRequestTest(RequestTest):\n </form>\n         \"\"\"\n         response = HtmlResponse(\"http://www.example.com/this/list.html\", body=respbody)\n-        self.assertRaises(MultipleElementsFound,\n+        self.assertRaises(ValueError,\n                           self.request_class.from_response,\n                           response,\n                           clickdata={'type': 'submit'})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ac4f6cc17cffed99d6ba218d2323a7383c1a80cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 83 | Lines Deleted: 88 | Files Changed: 5 | Hunks: 6 | Methods Changed: 12 | Complexity Δ (Sum/Max): 0/14 | Churn Δ: 171 | Churn Cumulative: 631 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -1,40 +0,0 @@\n-\"\"\"\n-This module contains a simple class (Libxml2Document) which provides cache and\n-garbage collection to libxml2 documents (xmlDoc).\n-\"\"\"\n-\n-import weakref\n-\n-from scrapy.utils.trackref import object_ref\n-from .factories import xmlDoc_from_html\n-\n-class Libxml2Document(object_ref):\n-\n-    cache = weakref.WeakKeyDictionary()\n-    __slots__ = ['xmlDoc', 'xpathContext', '__weakref__']\n-\n-    def __new__(cls, response, factory=xmlDoc_from_html):\n-        cache = cls.cache.setdefault(response, {})\n-        if factory not in cache:\n-            obj = object_ref.__new__(cls)\n-            obj.xmlDoc = factory(response)\n-            obj.xpathContext = obj.xmlDoc.xpathNewContext()\n-            cache[factory] = obj\n-        return cache[factory]\n-\n-    def __del__(self):\n-        # we must call both cleanup functions, so we try/except all exceptions\n-        # to make sure one doesn't prevent the other from being called\n-        # this call sometimes raises a \"NoneType is not callable\" TypeError\n-        # so the try/except block silences them\n-        try:\n-            self.xmlDoc.freeDoc()\n-        except:\n-            pass\n-        try:\n-            self.xpathContext.xpathFreeContext()\n-        except:\n-            pass\n-\n-    def __str__(self):\n-        return \"<Libxml2Document %s>\" % self.xmlDoc.name\n\n@@ -1,45 +0,0 @@\n-\"\"\"\n-This module provides functions for generating libxml2 documents (xmlDoc).\n-\n-Constructors must receive a Response object and return a xmlDoc object.\n-\"\"\"\n-\n-import libxml2\n-\n-xml_parser_options = libxml2.XML_PARSE_RECOVER + \\\n-                     libxml2.XML_PARSE_NOERROR + \\\n-                     libxml2.XML_PARSE_NOWARNING\n-\n-html_parser_options = libxml2.HTML_PARSE_RECOVER + \\\n-                      libxml2.HTML_PARSE_NOERROR + \\\n-                      libxml2.HTML_PARSE_NOWARNING\n-\n-utf8_encodings = set(('utf-8', 'UTF-8', 'utf8', 'UTF8'))\n-\n-def body_as_utf8(response):\n-    if response.encoding in utf8_encodings:\n-        return response.body\n-    else:\n-        return response.body_as_unicode().encode('utf-8')\n-        \n-def xmlDoc_from_html(response):\n-    \"\"\"Return libxml2 doc for HTMLs\"\"\"\n-    utf8body = body_as_utf8(response) or ' '\n-    try:\n-        lxdoc = libxml2.htmlReadDoc(utf8body, response.url, 'utf-8', \\\n-            html_parser_options)\n-    except TypeError:  # libxml2 doesn't parse text with null bytes\n-        lxdoc = libxml2.htmlReadDoc(utf8body.replace(\"\\x00\", \"\"), response.url, \\\n-            'utf-8', html_parser_options)\n-    return lxdoc\n-\n-def xmlDoc_from_xml(response):\n-    \"\"\"Return libxml2 doc for XMLs\"\"\"\n-    utf8body = body_as_utf8(response) or ' '\n-    try:\n-        lxdoc = libxml2.readDoc(utf8body, response.url, 'utf-8', \\\n-            xml_parser_options)\n-    except TypeError:  # libxml2 doesn't parse text with null bytes\n-        lxdoc = libxml2.readDoc(utf8body.replace(\"\\x00\", \"\"), response.url, \\\n-            'utf-8', xml_parser_options)\n-    return lxdoc\n\n@@ -0,0 +1,80 @@\n+\"\"\"\n+This module contains a simple class (Libxml2Document) which provides cache and\n+garbage collection to libxml2 documents (xmlDoc).\n+\"\"\"\n+\n+import weakref\n+import libxml2\n+from scrapy.utils.trackref import object_ref\n+\n+xml_parser_options = libxml2.XML_PARSE_RECOVER + \\\n+                     libxml2.XML_PARSE_NOERROR + \\\n+                     libxml2.XML_PARSE_NOWARNING\n+\n+html_parser_options = libxml2.HTML_PARSE_RECOVER + \\\n+                      libxml2.HTML_PARSE_NOERROR + \\\n+                      libxml2.HTML_PARSE_NOWARNING\n+\n+\n+_UTF8_ENCODINGS = set(('utf-8', 'UTF-8', 'utf8', 'UTF8'))\n+def _body_as_utf8(response):\n+    if response.encoding in _UTF8_ENCODINGS:\n+        return response.body\n+    else:\n+        return response.body_as_unicode().encode('utf-8')\n+\n+\n+def xmlDoc_from_html(response):\n+    \"\"\"Return libxml2 doc for HTMLs\"\"\"\n+    utf8body = _body_as_utf8(response) or ' '\n+    try:\n+        lxdoc = libxml2.htmlReadDoc(utf8body, response.url, 'utf-8', \\\n+            html_parser_options)\n+    except TypeError:  # libxml2 doesn't parse text with null bytes\n+        lxdoc = libxml2.htmlReadDoc(utf8body.replace(\"\\x00\", \"\"), response.url, \\\n+            'utf-8', html_parser_options)\n+    return lxdoc\n+\n+\n+def xmlDoc_from_xml(response):\n+    \"\"\"Return libxml2 doc for XMLs\"\"\"\n+    utf8body = _body_as_utf8(response) or ' '\n+    try:\n+        lxdoc = libxml2.readDoc(utf8body, response.url, 'utf-8', \\\n+            xml_parser_options)\n+    except TypeError:  # libxml2 doesn't parse text with null bytes\n+        lxdoc = libxml2.readDoc(utf8body.replace(\"\\x00\", \"\"), response.url, \\\n+            'utf-8', xml_parser_options)\n+    return lxdoc\n+\n+\n+class Libxml2Document(object_ref):\n+\n+    cache = weakref.WeakKeyDictionary()\n+    __slots__ = ['xmlDoc', 'xpathContext', '__weakref__']\n+\n+    def __new__(cls, response, factory=xmlDoc_from_html):\n+        cache = cls.cache.setdefault(response, {})\n+        if factory not in cache:\n+            obj = object_ref.__new__(cls)\n+            obj.xmlDoc = factory(response)\n+            obj.xpathContext = obj.xmlDoc.xpathNewContext()\n+            cache[factory] = obj\n+        return cache[factory]\n+\n+    def __del__(self):\n+        # we must call both cleanup functions, so we try/except all exceptions\n+        # to make sure one doesn't prevent the other from being called\n+        # this call sometimes raises a \"NoneType is not callable\" TypeError\n+        # so the try/except block silences them\n+        try:\n+            self.xmlDoc.freeDoc()\n+        except:\n+            pass\n+        try:\n+            self.xpathContext.xpathFreeContext()\n+        except:\n+            pass\n+\n+    def __str__(self):\n+        return \"<Libxml2Document %s>\" % self.xmlDoc.name\n\n@@ -9,8 +9,7 @@ from scrapy.utils.python import unicode_to_str\n from scrapy.utils.misc import extract_regex\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.decorator import deprecated\n-from .factories import xmlDoc_from_html, xmlDoc_from_xml\n-from .document import Libxml2Document\n+from .libxml2document import Libxml2Document, xmlDoc_from_html, xmlDoc_from_xml\n from .list import XPathSelectorList\n \n __all__ = ['HtmlXPathSelector', 'XmlXPathSelector', 'XPathSelector', \\\n\n@@ -7,10 +7,11 @@ import unittest\n from scrapy.http import TextResponse, HtmlResponse, XmlResponse\n from scrapy.selector.libxml2sel import XmlXPathSelector, HtmlXPathSelector, \\\n     XPathSelector\n-from scrapy.selector.document import Libxml2Document\n+from scrapy.selector.libxml2document import Libxml2Document\n from scrapy.utils.test import libxml2debug\n from scrapy.tests import test_selector\n \n+\n class Libxml2XPathSelectorTestCase(test_selector.XPathSelectorTestCase):\n \n     xs_cls = XPathSelector\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4c7d29b7f7fd6071449e925ce4f6df6e5b36c808", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 80 | Lines Deleted: 51 | Files Changed: 3 | Hunks: 20 | Methods Changed: 11 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 131 | Churn Cumulative: 896 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,31 @@\n+\"\"\"\n+This module contains a simple class (LxmlDocument) which provides cache and\n+garbage collection to lxml element tree documents.\n+\"\"\"\n+\n+import weakref\n+from lxml import etree\n+from scrapy.utils.trackref import object_ref\n+\n+\n+def _factory(response, parser_cls):\n+    url = response.url\n+    body = response.body_as_unicode().strip().encode('utf8') or '<html/>'\n+    parser = parser_cls(recover=True, encoding='utf8')\n+    return etree.fromstring(body, parser=parser, base_url=url)\n+\n+\n+class LxmlDocument(object_ref):\n+\n+    cache = weakref.WeakKeyDictionary()\n+    __slots__ = ['xmlDoc', 'xpathContext', '__weakref__']\n+\n+    def __new__(cls, response, parser=etree.HTMLParser):\n+        cache = cls.cache.setdefault(response, {})\n+        if parser not in cache:\n+            obj = object_ref.__new__(cls)\n+            cache[parser] = _factory(response, parser)\n+        return cache[parser]\n+\n+    def __str__(self):\n+        return \"<LxmlDocument %s>\" % self.root.tag\n\n@@ -9,42 +9,35 @@ from scrapy.utils.trackref import object_ref\n from scrapy.utils.python import unicode_to_str\n from scrapy.utils.decorator import deprecated\n from scrapy.http import TextResponse\n+from .lxmldocument import LxmlDocument\n from .list import XPathSelectorList\n \n+\n __all__ = ['HtmlXPathSelector', 'XmlXPathSelector', 'XPathSelector', \\\n            'XPathSelectorList']\n \n+\n class XPathSelector(object_ref):\n \n-    __slots__ = ['response', 'text', 'expr', 'namespaces', '_root', '_xpathev', \\\n-        '__weakref__']\n+    __slots__ = ['response', 'text', 'namespaces', '_expr', '_root', '__weakref__']\n     _parser = etree.HTMLParser\n     _tostring_method = 'html'\n \n-    def __init__(self, response=None, text=None, root=None, expr=None, namespaces=None):\n-        if text:\n-            self.response = TextResponse(url='about:blank', \\\n+    def __init__(self, response=None, text=None, namespaces=None, _root=None, _expr=None):\n+        if text is not None:\n+            response = TextResponse(url='about:blank', \\\n                 body=unicode_to_str(text, 'utf-8'), encoding='utf-8')\n-        else:\n-            self.response = response\n-        self._root = root\n-        self._xpathev = None\n-        self.namespaces = namespaces\n-        self.expr = expr\n+        if response is not None:\n+            _root = LxmlDocument(response, self._parser)\n \n-    @property\n-    def root(self):\n-        if self._root is None:\n-            url = self.response.url\n-            body = self.response.body_as_unicode().strip().encode('utf8') or '<html/>'\n-            parser = self._parser(recover=True, encoding='utf8')\n-            self._root = etree.fromstring(body, parser=parser, base_url=url)\n-            assert self._root is not None, 'BUG lxml selector with None root'\n-        return self._root\n+        self.namespaces = namespaces\n+        self.response = response\n+        self._root = _root\n+        self._expr = _expr\n \n     def select(self, xpath):\n         try:\n-            xpathev = self.root.xpath\n+            xpathev = self._root.xpath\n         except AttributeError:\n             return XPathSelectorList([])\n \n@@ -56,7 +49,7 @@ class XPathSelector(object_ref):\n         if type(result) is not list:\n             result = [result]\n \n-        result = [self.__class__(root=x, expr=xpath, namespaces=self.namespaces)\n+        result = [self.__class__(_root=x, _expr=xpath, namespaces=self.namespaces)\n                   for x in result]\n         return XPathSelectorList(result)\n \n@@ -65,15 +58,15 @@ class XPathSelector(object_ref):\n \n     def extract(self):\n         try:\n-            return etree.tostring(self.root, method=self._tostring_method, \\\n+            return etree.tostring(self._root, method=self._tostring_method, \\\n                 encoding=unicode)\n         except (AttributeError, TypeError):\n-            if self.root is True:\n+            if self._root is True:\n                 return u'1'\n-            elif self.root is False:\n+            elif self._root is False:\n                 return u'0'\n             else:\n-                return unicode(self.root)\n+                return unicode(self._root)\n \n     def register_namespace(self, prefix, uri):\n         if self.namespaces is None:\n@@ -85,7 +78,7 @@ class XPathSelector(object_ref):\n \n     def __str__(self):\n         data = repr(self.extract()[:40])\n-        return \"<%s xpath=%r data=%s>\" % (type(self).__name__, self.expr, data)\n+        return \"<%s xpath=%r data=%s>\" % (type(self).__name__, self._expr, data)\n \n     __repr__ = __str__\n \n\n@@ -2,35 +2,40 @@\n Selectors tests, specific for lxml backend\n \"\"\"\n \n-from scrapy.http import TextResponse, XmlResponse\n-has_lxml = True\n-try:\n-    from scrapy.selector.lxmlsel import XmlXPathSelector, HtmlXPathSelector, \\\n-        XPathSelector\n-except ImportError:\n-    has_lxml = False\n-from scrapy.utils.test import libxml2debug\n+import unittest\n from scrapy.tests import test_selector\n+from scrapy.http import TextResponse, HtmlResponse\n+from scrapy.selector.lxmldocument import LxmlDocument\n+from scrapy.selector.lxmlsel import XmlXPathSelector, HtmlXPathSelector, XPathSelector\n+\n \n class LxmlXPathSelectorTestCase(test_selector.XPathSelectorTestCase):\n \n-    if has_lxml:\n     xs_cls = XPathSelector\n     hxs_cls = HtmlXPathSelector\n     xxs_cls = XmlXPathSelector\n-    else:\n-        skip = \"lxml not available\"\n \n-    # XXX: this test was disabled because lxml behaves inconsistently when\n-    # handling null bytes between different 2.2.x versions, but it may be due\n-    # to differences in libxml2 too. it's also unclear what should be the\n-    # proper behaviour (pablo - 26 oct 2010)\n-    #@libxml2debug\n-    #def test_null_bytes(self):\n-    #    hxs = HtmlXPathSelector(text='<root>la\\x00la</root>')\n-    #    self.assertEqual(hxs.extract(),\n-    #                     u'<html><body><root>la</root></body></html>')\n-    #\n-    #    xxs = XmlXPathSelector(text='<root>la\\x00la</root>')\n-    #    self.assertEqual(xxs.extract(),\n-    #                     u'<root>la</root>')\n+\n+class Libxml2DocumentTest(unittest.TestCase):\n+\n+    def test_caching(self):\n+        r1 = HtmlResponse('http://www.example.com', body='<html><head></head><body></body></html>')\n+        r2 = r1.copy()\n+\n+        doc1 = LxmlDocument(r1)\n+        doc2 = LxmlDocument(r1)\n+        doc3 = LxmlDocument(r2)\n+\n+        # make sure it's cached\n+        assert doc1 is doc2\n+        assert doc1 is not doc3\n+\n+        # don't leave documents in memory to avoid wrong libxml2 leaks reports\n+        del doc1, doc2, doc3\n+\n+    def test_null_char(self):\n+        # make sure bodies with null char ('\\x00') don't raise a TypeError exception\n+        self.body_content = 'test problematic \\x00 body'\n+        response = TextResponse('http://example.com/catalog/product/blabla-123',\n+                            headers={'Content-Type': 'text/plain; charset=utf-8'}, body=self.body_content)\n+        LxmlDocument(response)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a11ef7fba704c68049516a323bcca69d6fc4d03b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 513 | Contributors (this commit): 6 | Commits (past 90d): 11 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,9 +6,7 @@ See documentation in docs/topics/request-response.rst\n \"\"\"\n \n import urllib\n-\n-from lxml import html\n-\n+import lxml.html\n from scrapy.http.request import Request\n from scrapy.utils.python import unicode_to_str\n \n@@ -34,6 +32,7 @@ class FormRequest(Request):\n     @classmethod\n     def from_response(cls, response, formname=None, formnumber=0, formdata=None,\n                       clickdata=None, dont_click=False, **kwargs):\n+        from scrapy.selector.lxmldocument import LxmlDocument\n         if not hasattr(formdata, \"items\"):\n             try:\n                 formdata = dict(formdata) if formdata else {}\n@@ -41,28 +40,29 @@ class FormRequest(Request):\n                 raise ValueError('formdata should be a dict or iterable of tuples')\n \n         kwargs.setdefault('encoding', response.encoding)\n-        hxs = html.fromstring(response.body_as_unicode(), base_url=response.url)\n-        form = _get_form(hxs, formname, formnumber, response)\n+        root = LxmlDocument(response, lxml.html.HTMLParser)\n+        form = _get_form(root, formname, formnumber, response)\n         formdata = _get_inputs(form, formdata, dont_click, clickdata, response)\n         url = form.action or form.base_url\n         return cls(url, method=form.method, formdata=formdata, **kwargs)\n \n+\n def _urlencode(seq, enc):\n     values = [(unicode_to_str(k, enc), unicode_to_str(v, enc))\n               for k, vs in seq\n               for v in (vs if hasattr(vs, '__iter__') else [vs])]\n     return urllib.urlencode(values, doseq=1)\n \n-def _get_form(hxs, formname, formnumber, response):\n+def _get_form(root, formname, formnumber, response):\n     \"\"\"\n     Uses all the passed arguments to get the required form\n     element\n     \"\"\"\n-    if not hxs.forms:\n+    if not root.forms:\n         raise ValueError(\"No <form> element found in %s\" % response)\n \n     if formname is not None:\n-        f = hxs.xpath('//form[@name=\"%s\"]' % formname)\n+        f = root.xpath('//form[@name=\"%s\"]' % formname)\n         if f:\n             return f[0]\n \n@@ -70,7 +70,7 @@ def _get_form(hxs, formname, formnumber, response):\n     # or invalid\n     if formnumber is not None:\n         try:\n-            form = hxs.forms[formnumber]\n+            form = root.forms[formnumber]\n         except IndexError:\n             raise IndexError(\"Form number %d not found in %s\" %\n                                 (formnumber, response))\n\n@@ -18,7 +18,7 @@ def _factory(response, parser_cls):\n class LxmlDocument(object_ref):\n \n     cache = weakref.WeakKeyDictionary()\n-    __slots__ = ['xmlDoc', 'xpathContext', '__weakref__']\n+    __slots__ = ['__weakref__']\n \n     def __new__(cls, response, parser=etree.HTMLParser):\n         cache = cls.cache.setdefault(response, {})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
