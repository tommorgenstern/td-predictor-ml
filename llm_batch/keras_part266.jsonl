{"custom_id": "keras#cb9d79efa87043f76989ed530de2bdb4b1fe8d19", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 480 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -119,18 +119,13 @@ def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n \n     # check with the functional API\n     model = Model(x, y)\n-    _layer_in_model_test(model)\n+    actual_output = _layer_in_model_test(model)\n \n-    # test as first layer in Sequential API\n+    # test instantiation from layer config\n     layer_config = layer.get_config()\n     layer_config['batch_input_shape'] = input_shape\n     layer = layer.__class__.from_config(layer_config)\n \n-    # check with the sequential API\n-    model = Sequential()\n-    model.add(layer)\n-    actual_output = _layer_in_model_test(model)\n-\n     # for further checks in the caller function\n     return actual_output\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9149ff85210c1cc8b294b26ea37b8f6d7d51c8f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 27 | Files Changed: 1 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 46 | Churn Cumulative: 5032 | Contributors (this commit): 30 | Commits (past 90d): 24 | Contributors (cumulative): 30 | DMM Complexity: 0.0\n\nDIFF:\n@@ -45,28 +45,20 @@ def check_dtype(var, dtype):\n         assert var.dtype.name == '%s_ref' % dtype\n \n \n-def cntk_func_single_tensor(function_name, x_shape, **kwargs):\n-    xc = KC.placeholder(x_shape)\n-    output_cntk = getattr(KC, function_name)(xc, **kwargs)\n-    return output_cntk, KC.function([xc], [output_cntk])\n-\n-\n-def cntk_func_two_tensor(function_name, x_shape, y, **kwargs):\n-    if isinstance(y, (np.generic, np.ndarray)):\n-        xc = KC.placeholder(x_shape)\n-        output_cntk = getattr(KC, function_name)(xc, KC.variable(y), **kwargs)\n-        return output_cntk, KC.function([xc], [output_cntk])\n+def cntk_func_tensors(function_name, shapes_or_vals, **kwargs):\n+    placeholders = []\n+    variables = []\n+    for shape_or_val in shapes_or_vals:\n+        if isinstance(shape_or_val, tuple):\n+            shape = shape_or_val\n+            placeholders.append(KC.placeholder(shape))\n         else:\n-        xc = KC.placeholder(ndim=len(x_shape))\n-        yc = KC.placeholder(y)\n-        output_cntk = getattr(KC, function_name)(xc, yc, **kwargs)\n-        return output_cntk, KC.function([xc, yc], [output_cntk])\n+            value = shape_or_val\n+            variables.append(KC.variable(value))\n \n-\n-def cntk_func_three_tensor(function_name, x_shape, y, z, **kwargs):\n-    xc = KC.placeholder(x_shape)\n-    output_cntk = getattr(KC, function_name)(xc, KC.variable(y), KC.variable(z), **kwargs)\n-    return KC.function([xc], [output_cntk])\n+    output_cntk = getattr(KC, function_name)(*(placeholders + variables), **kwargs)\n+    cntk_func = KC.function(placeholders, [output_cntk])\n+    return output_cntk, cntk_func\n \n \n def parse_shape_or_val(shape_or_val):\n@@ -108,7 +100,7 @@ def check_single_tensor_operation(function_name, x_shape_or_val, backend_list, *\n     for k in backend_list:\n         if shape_or_val:\n             if (k == KC) & (cntk_dynamicity):\n-                t, f = cntk_func_single_tensor(function_name, x_shape, **kwargs)\n+                t, f = cntk_func_tensors(function_name, [x_shape], **kwargs)\n                 z = f([x_val])[0]\n             else:\n                 t = getattr(k, function_name)(k.variable(x_val), **kwargs)\n@@ -137,10 +129,10 @@ def check_two_tensor_operation(function_name, x_shape_or_val,\n     z_list = []\n     for k in backend_list:\n         if (k == KC) & (cntk_dynamicity):\n-            t, f = cntk_func_two_tensor(function_name, x_shape, y=y_val, **kwargs)\n+            t, f = cntk_func_tensors(function_name, [x_shape, y_val], **kwargs)\n             z = f([x_val])[0]\n         elif (k == KC) & (cntk_two_dynamicity):\n-            t, f = cntk_func_two_tensor(function_name, x_shape, y=y_shape, **kwargs)\n+            t, f = cntk_func_tensors(function_name, [x_shape, y_shape], **kwargs)\n             z = f([x_val, y_val])[0]\n         elif (k == KTH) & (function_name[:4] == 'conv'):\n             t = getattr(k, function_name)(\n@@ -1131,10 +1123,10 @@ class TestBackend(object):\n         y1 = KNP.separable_conv(x, depthwise, pointwise,\n                                 padding=padding, data_format=data_format)\n         if K.backend() == 'cntk':\n-            y2 = cntk_func_three_tensor(\n-                op, input_shape,\n-                depthwise, pointwise,\n-                padding=padding, data_format=data_format)([x])[0]\n+            _, cntk_func = cntk_func_tensors(\n+                op, [input_shape, depthwise, pointwise],\n+                padding=padding, data_format=data_format)\n+            y2 = cntk_func([x])[0]\n         else:\n             y2 = K.eval(getattr(K, op)(\n                 K.variable(x),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#06c3a804e7d56cc77f76a28ee34aa67a34daedee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 10363 | Contributors (this commit): 121 | Commits (past 90d): 19 | Contributors (cumulative): 125 | DMM Complexity: None\n\nDIFF:\n@@ -1905,17 +1905,17 @@ def batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=1e-3):\n             # The mean / var / beta / gamma may be processed by broadcast\n             # so it may have extra axes with 1, it is not needed and should be removed\n             if ndim(mean) > 1:\n-                mean = tf.squeeze(mean)\n+                mean = tf.reshape(mean, (-1))\n             if ndim(var) > 1:\n-                var = tf.squeeze(var)\n+                var = tf.reshape(var, (-1))\n             if beta is None:\n                 beta = zeros_like(mean)\n             elif ndim(beta) > 1:\n-                beta = tf.squeeze(beta)\n+                beta = tf.reshape(beta, (-1))\n             if gamma is None:\n                 gamma = ones_like(mean)\n             elif ndim(gamma) > 1:\n-                gamma = tf.squeeze(gamma)\n+                gamma = tf.reshape(gamma, (-1))\n             y, _, _ = tf.nn.fused_batch_norm(\n                 x,\n                 gamma,\n\n@@ -28,7 +28,7 @@ def test_basic_batchnorm():\n                kwargs={'momentum': 0.9,\n                        'epsilon': 0.1,\n                        'axis': 1},\n-               input_shape=(3, 4, 2))\n+               input_shape=(1, 4, 1))\n     layer_test(normalization.BatchNormalization,\n                kwargs={'gamma_initializer': 'ones',\n                        'beta_initializer': 'ones',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ba7ab2ffe62143949b9730a55291c474352f31e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 18 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): -8/0 | Churn Δ: 24 | Churn Cumulative: 6267 | Contributors (this commit): 88 | Commits (past 90d): 10 | Contributors (cumulative): 88 | DMM Complexity: 1.0\n\nDIFF:\n@@ -589,24 +589,7 @@ def any(x, axis=None, keepdims=False):\n     \"\"\"Bitwise reduction (logical OR).\n     \"\"\"\n     y = T.any(x, axis=axis, keepdims=keepdims)\n-    if hasattr(x, '_keras_shape'):\n-        if axis is None:\n-            y._keras_shape = (1,) * len(x._keras_shape) if keepdims else (1,)\n-        else:\n-            if isinstance(axis, int):\n-                axis_list = [axis]\n-            else:\n-                axis_list = list(set(int(a) for a in axis))\n-            keras_shape_list = list(x._keras_shape)\n-            if keepdims:\n-                for a in axis_list:\n-                    keras_shape_list[a] = 1\n-            else:\n-                for a in axis_list[::-1]:\n-                    keras_shape_list.pop(a)\n-                if not keras_shape_list:\n-                    keras_shape_list = (1,)\n-            y._keras_shape = tuple(keras_shape_list)\n+    y = _set_keras_shape_for_reduction(x, y, axis, keepdims)\n     return y\n \n \n@@ -614,6 +597,11 @@ def all(x, axis=None, keepdims=False):\n     \"\"\"Bitwise reduction (logical AND).\n     \"\"\"\n     y = T.all(x, axis=axis, keepdims=keepdims)\n+    y = _set_keras_shape_for_reduction(x, y, axis, keepdims)\n+    return y\n+\n+\n+def _set_keras_shape_for_reduction(x, y, axis, keepdims):\n     if hasattr(x, '_keras_shape'):\n         if axis is None:\n             y._keras_shape = (1,) * len(x._keras_shape) if keepdims else (1,)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8be9909909243348710b8b6572b6c97bdcaf36a3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 260 | Contributors (this commit): 11 | Commits (past 90d): 4 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,6 @@\n '''Tests for functions in io_utils.py.\n '''\n import os\n-import sys\n import pytest\n from keras.models import Sequential\n from keras.layers import Dense\n@@ -13,6 +12,7 @@ import numpy as np\n import six\n import warnings\n import h5py\n+import tempfile\n try:\n     from unittest.mock import patch\n except:\n@@ -46,7 +46,7 @@ def test_io_utils(in_tmpdir):\n     '''Tests the HDF5Matrix code using the sample from @jfsantos at\n     https://gist.github.com/jfsantos/e2ef822c744357a4ed16ec0c885100a3\n     '''\n-    h5_path = 'test.h5'\n+    _, h5_path = tempfile.mkstemp('.h5')\n     create_dataset(h5_path)\n \n     # Instantiating HDF5Matrix for the training set,\n@@ -135,7 +135,9 @@ def test_ask_to_proceed_with_overwrite():\n         assert not ask_to_proceed_with_overwrite('/tmp/not_exists')\n \n \n-def test_h5dict_attrs(h5_path='test.h5'):\n+def test_h5dict_attrs():\n+    _, h5_path = tempfile.mkstemp('.h5')\n+\n     # test both HDF5 and dict implementations\n     paths = [h5_path, dict()]\n \n@@ -163,7 +165,9 @@ def test_h5dict_attrs(h5_path='test.h5'):\n         f.close()\n \n \n-def test_h5dict_groups(h5_path='test.h5'):\n+def test_h5dict_groups():\n+    _, h5_path = tempfile.mkstemp('.h5')\n+\n     # test both HDF5 and dict implementations\n     paths = [h5_path, dict()]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2c791b0b05f1296af0a82b4e271c5522ea737a69", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 263 | Contributors (this commit): 11 | Commits (past 90d): 5 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,7 +10,6 @@ from keras.utils.io_utils import ask_to_proceed_with_overwrite\n from numpy.testing import assert_allclose\n import numpy as np\n import six\n-import warnings\n import h5py\n import tempfile\n try:\n@@ -163,6 +162,7 @@ def test_h5dict_attrs():\n         assert_allclose(f['z'], array)\n \n         f.close()\n+    os.remove(h5_path)\n \n \n def test_h5dict_groups():\n@@ -206,6 +206,7 @@ def test_h5dict_groups():\n         assert_allclose(group4['z'], array)\n \n         f.close()\n+    os.remove(h5_path)\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ff89e2de51e5ccdb7a68510d5539365002881b61", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 2072 | Contributors (this commit): 23 | Commits (past 90d): 8 | Contributors (cumulative): 25 | DMM Complexity: None\n\nDIFF:\n@@ -71,7 +71,7 @@ def _serialize_model(model, f, include_optimizer=True):\n         if type(obj).__name__ == type.__name__:\n             return obj.__name__\n \n-        raise TypeError('Not JSON Serializable:', obj)\n+        raise TypeError('Not JSON Serializable: %s' % (obj,))\n \n     from .. import __version__ as keras_version\n \n\n@@ -192,7 +192,7 @@ class H5Dict(object):\n             self.data['_is_group'] = True\n         else:\n             raise TypeError('Required Group, str or dict. '\n-                            ' Received {}.'.format(type(path)))\n+                            'Received: {}.'.format(type(path)))\n         self.read_only = mode == 'r'\n \n     def __setitem__(self, attr, val):\n@@ -211,7 +211,7 @@ class H5Dict(object):\n             return\n         if attr in self:\n             raise KeyError('Cannot set attribute. '\n-                           ' Group with name {} exists.'.format(attr))\n+                           'Group with name \"{}\" exists.'.format(attr))\n         if is_np:\n             dataset = self.data.create_dataset(attr, val.shape, dtype=val.dtype)\n             if not val.shape:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#514aca20c6f076a86819d7180f36c3b2e8bcc33b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 51 | Churn Cumulative: 51 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,51 @@\n+from __future__ import print_function\n+\n+import os\n+import tempfile\n+import pytest\n+import keras\n+from keras import layers\n+from keras.utils.test_utils import get_test_data\n+from keras.utils.test_utils import keras_test\n+\n+\n+@pytest.mark.skipif(keras.backend.backend() != 'tensorflow', reason='Requires TF backend')\n+@keras_test\n+def test_tf_optimizer():\n+    import tensorflow as tf\n+\n+    num_hidden = 10\n+    output_dim = 2\n+    input_dim = 10\n+    target = 0.8\n+    optimizer = tf.train.AdadeltaOptimizer(\n+        learning_rate=1., rho=0.95, epsilon=1e-08)\n+\n+    (x_train, y_train), (x_test, y_test) = get_test_data(\n+        num_train=1000, num_test=200,\n+        input_shape=(input_dim,),\n+        classification=True, num_classes=output_dim)\n+\n+    model = keras.Sequential()\n+    model.add(layers.Dense(num_hidden,\n+                           activation='relu',\n+                           input_shape=(input_dim,)))\n+    model.add(layers.Dense(output_dim, activation='softmax'))\n+\n+    model.compile(loss='sparse_categorical_crossentropy',\n+                  optimizer=optimizer,\n+                  metrics=['accuracy'])\n+    history = model.fit(x_train, y_train, epochs=8, batch_size=16,\n+                        validation_data=(x_test, y_test), verbose=2)\n+    assert history.history['val_acc'][-1] >= target\n+\n+    # Test saving.\n+    _, fname = tempfile.mkstemp('.h5')\n+    model.save(fname)\n+    model = keras.models.load_model(fname)\n+    assert len(model.weights) == 4\n+    os.remove(fname)\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fe38f9dfc8c732a77ac03507b63c79b1d2acfba2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 12 | Methods Changed: 7 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 38 | Churn Cumulative: 4115 | Contributors (this commit): 15 | Commits (past 90d): 6 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -85,6 +85,7 @@ class Sequential(Model):\n \n     def __init__(self, layers=None, name=None):\n         super(Sequential, self).__init__(name=name)\n+        self._build_input_shape = None\n \n         # Add to the model any layers passed to the constructor.\n         if layers:\n@@ -219,8 +220,7 @@ class Sequential(Model):\n             for layer in self._layers:\n                 x = layer(x)\n             self.outputs = [x]\n-            if self._layers:\n-                self._layers[0].batch_input_shape = batch_shape\n+            self._build_input_shape = input_shape\n \n         if self.inputs:\n             self._init_graph_network(self.inputs,\n@@ -271,19 +271,31 @@ class Sequential(Model):\n             return (proba > 0.5).astype('int32')\n \n     def get_config(self):\n-        config = []\n+        layer_configs = []\n         for layer in self.layers:\n-            config.append({\n+            layer_configs.append({\n                 'class_name': layer.__class__.__name__,\n                 'config': layer.get_config()\n             })\n-        return copy.deepcopy(config)\n+        config = {\n+            'name': self.name,\n+            'layers': copy.deepcopy(layer_configs)\n+        }\n+        if self._build_input_shape:\n+            config['build_input_shape'] = self._build_input_shape\n+        return config\n \n     @classmethod\n     def from_config(cls, config, custom_objects=None):\n-        model = cls()\n-        for conf in config:\n+        if 'name' in config:\n+            name = config['name']\n+            build_input_shape = config.get('build_input_shape')\n+            layer_configs = config['layers']\n+        model = cls(name=name)\n+        for conf in layer_configs:\n             layer = layer_module.deserialize(conf,\n                                              custom_objects=custom_objects)\n             model.add(layer)\n+        if not model.inputs and build_input_shape:\n+            model.build(build_input_shape)\n         return model\n\n@@ -168,9 +168,11 @@ def test_sequential(in_tmpdir):\n     nloss = model.evaluate(x_test, y_test, verbose=0)\n     assert(loss == nloss)\n \n-    # test serialization\n+    # Test serialization\n     config = model.get_config()\n-    Sequential.from_config(config)\n+    assert 'name' in config\n+    new_model = Sequential.from_config(config)\n+    assert new_model.weights  # Model should be built.\n \n     model.summary()\n     json_str = model.to_json()\n@@ -234,7 +236,7 @@ def test_nested_sequential(in_tmpdir):\n     nloss = model.evaluate(x_test, y_test, verbose=0)\n     assert(loss == nloss)\n \n-    # test serialization\n+    # Test serialization\n     config = model.get_config()\n     Sequential.from_config(config)\n \n@@ -426,8 +428,10 @@ def test_sequential_deferred_build():\n     assert len(model.layers) == 2\n     assert len(model.weights) == 4\n \n+    # Test serialization\n     config = model.get_config()\n-    new_model = keras.models.Sequential.from_config(config)\n+    assert 'name' in config\n+    new_model = Sequential.from_config(config)\n     assert new_model.built is True\n     assert len(new_model.layers) == 2\n     assert len(new_model.weights) == 4\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#98e17b1be99368d69bc292443cfc23dcefdf3bb1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 54 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -9,7 +9,8 @@ from keras.utils.test_utils import get_test_data\n from keras.utils.test_utils import keras_test\n \n \n-@pytest.mark.skipif(keras.backend.backend() != 'tensorflow', reason='Requires TF backend')\n+@pytest.mark.skipif(keras.backend.backend() != 'tensorflow',\n+                    reason='Requires TF backend')\n @keras_test\n def test_tf_optimizer():\n     import tensorflow as tf\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#842d360e4388f255ae69055136909525cb66b276", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 315 | Contributors (this commit): 17 | Commits (past 90d): 2 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -152,6 +152,12 @@ def hard_sigmoid(x):\n     return K.hard_sigmoid(x)\n \n \n+def exponential(x):\n+    \"\"\"Exponential (base e) activation function.\n+    \"\"\"\n+    return K.exp(x)\n+\n+\n def linear(x):\n     \"\"\"Linear (i.e. identity) activation function.\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5b6243485acc20cc36f2db4f258512c332d691ec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 54 | Files Changed: 8 | Hunks: 23 | Methods Changed: 15 | Complexity Δ (Sum/Max): -14/1 | Churn Δ: 81 | Churn Cumulative: 27839 | Contributors (this commit): 180 | Commits (past 90d): 28 | Contributors (cumulative): 255 | DMM Complexity: 0.9142857142857143\n\nDIFF:\n@@ -139,14 +139,8 @@ class Network(Layer):\n     def _init_graph_network(self, inputs, outputs, name=None):\n         self._uses_inputs_arg = True\n         # Normalize and set self.inputs, self.outputs.\n-        if isinstance(inputs, (list, tuple)):\n-            self.inputs = list(inputs)  # Tensor or list of tensors.\n-        else:\n-            self.inputs = [inputs]\n-        if isinstance(outputs, (list, tuple)):\n-            self.outputs = list(outputs)\n-        else:\n-            self.outputs = [outputs]\n+        self.inputs = to_list(inputs, allow_tuple=True)\n+        self.outputs = to_list(outputs, allow_tuple=True)\n \n         # User-provided argument validation.\n         # Check for redundancy in inputs.\n\n@@ -596,10 +596,7 @@ class Model(Network):\n         self._feed_inputs = []\n         self._feed_input_names = []\n         self._feed_input_shapes = []\n-        if isinstance(inputs, (list, tuple)):\n-            inputs = list(inputs)\n-        else:\n-            inputs = [inputs]\n+        inputs = to_list(inputs, allow_tuple=True)\n \n         for i, v in enumerate(inputs):\n             name = 'input_%d' % (i + 1)\n@@ -633,10 +630,7 @@ class Model(Network):\n                 outputs = self.call(unpack_singleton(self.inputs), training=training)\n             else:\n                 outputs = self.call(unpack_singleton(self.inputs))\n-        if isinstance(outputs, (list, tuple)):\n-            outputs = list(outputs)\n-        else:\n-            outputs = [outputs]\n+        outputs = to_list(outputs, allow_tuple=True)\n         self.outputs = outputs\n         self.output_names = [\n             'output_%d' % (i + 1) for i in range(len(self.outputs))]\n@@ -704,10 +698,7 @@ class Model(Network):\n                                          'You passed: y=' + str(y))\n                 # Typecheck that all inputs are *either* value *or* symbolic.\n                 if y is not None:\n-                    if isinstance(y, (list, tuple)):\n-                        all_inputs += list(y)\n-                    else:\n-                        all_inputs.append(y)\n+                    all_inputs += to_list(y, allow_tuple=True)\n                 if any(K.is_tensor(v) for v in all_inputs):\n                     if not all(K.is_tensor(v) for v in all_inputs):\n                         raise ValueError('Do not pass inputs that mix Numpy '\n@@ -716,8 +707,7 @@ class Model(Network):\n                                          '; y=' + str(y))\n \n                 # Handle target tensors if any passed.\n-                if not isinstance(y, (list, tuple)):\n-                    y = [y]\n+                y = to_list(y, allow_tuple=True)\n                 target_tensors = [v for v in y if K.is_tensor(v)]\n                 if not target_tensors:\n                     target_tensors = None\n\n@@ -13,6 +13,7 @@ from ..engine.base_layer import Layer\n from ..engine.base_layer import InputSpec\n from .. import backend as K\n from ..legacy import interfaces\n+from ..utils.generic_utils import to_list\n \n \n class LeakyReLU(Layer):\n@@ -100,10 +101,8 @@ class PReLU(Layer):\n         self.alpha_constraint = constraints.get(alpha_constraint)\n         if shared_axes is None:\n             self.shared_axes = None\n-        elif not isinstance(shared_axes, (list, tuple)):\n-            self.shared_axes = [shared_axes]\n         else:\n-            self.shared_axes = list(shared_axes)\n+            self.shared_axes = to_list(shared_axes, allow_tuple=True)\n \n     def build(self, input_shape):\n         param_shape = list(input_shape[1:])\n\n@@ -21,6 +21,7 @@ from ..legacy import interfaces\n from ..legacy.layers import Recurrent, ConvRecurrent2D\n from .recurrent import RNN\n from ..utils.generic_utils import has_arg\n+from ..utils.generic_utils import to_list\n from ..utils.generic_utils import transpose_shape\n \n \n@@ -387,10 +388,7 @@ class ConvRNN2D(RNN):\n             output._uses_learning_phase = True\n \n         if self.return_state:\n-            if not isinstance(states, (list, tuple)):\n-                states = [states]\n-            else:\n-                states = list(states)\n+            states = to_list(states, allow_tuple=True)\n             return [output] + states\n         else:\n             return output\n@@ -443,8 +441,7 @@ class ConvRNN2D(RNN):\n                 K.set_value(self.states[0],\n                             np.zeros(get_tuple_shape(self.cell.state_size)))\n         else:\n-            if not isinstance(states, (list, tuple)):\n-                states = [states]\n+            states = to_list(states, allow_tuple=True)\n             if len(states) != len(self.states):\n                 raise ValueError('Layer ' + self.name + ' expects ' +\n                                  str(len(self.states)) + ' states, '\n\n@@ -10,6 +10,7 @@ from .. import regularizers\n from .. import constraints\n from ..engine.base_layer import Layer\n from ..legacy import interfaces\n+from ..utils.generic_utils import to_list\n \n \n class Embedding(Layer):\n@@ -117,10 +118,7 @@ class Embedding(Layer):\n             return input_shape + (self.output_dim,)\n         else:\n             # input_length can be tuple if input is 3D or higher\n-            if isinstance(self.input_length, (list, tuple)):\n-                in_lens = list(self.input_length)\n-            else:\n-                in_lens = [self.input_length]\n+            in_lens = to_list(self.input_length, allow_tuple=True)\n             if len(in_lens) != len(input_shape) - 1:\n                 raise ValueError('\"input_length\" is %s, but received input has shape %s' %\n                                  (str(self.input_length), str(input_shape)))\n\n@@ -16,6 +16,7 @@ from .. import constraints\n from ..engine.base_layer import Layer\n from ..engine.base_layer import InputSpec\n from ..utils.generic_utils import has_arg\n+from ..utils.generic_utils import to_list\n \n # Legacy support.\n from ..legacy.layers import Recurrent\n@@ -664,10 +665,7 @@ class RNN(Layer):\n                 state._uses_learning_phase = True\n \n         if self.return_state:\n-            if not isinstance(states, (list, tuple)):\n-                states = [states]\n-            else:\n-                states = list(states)\n+            states = to_list(states, allow_tuple=True)\n             return [output] + states\n         else:\n             return output\n@@ -702,8 +700,7 @@ class RNN(Layer):\n                 K.set_value(self.states[0],\n                             np.zeros((batch_size, self.cell.state_size)))\n         else:\n-            if not isinstance(states, (list, tuple)):\n-                states = [states]\n+            states = to_list(states, allow_tuple=True)\n             if len(states) != len(self.states):\n                 raise ValueError('Layer ' + self.name + ' expects ' +\n                                  str(len(self.states)) + ' states, '\n\n@@ -508,8 +508,7 @@ class Recurrent(Layer):\n         if initial_state is None:\n             return super(Recurrent, self).__call__(inputs, **kwargs)\n \n-        if not isinstance(initial_state, (list, tuple)):\n-            initial_state = [initial_state]\n+        initial_state = to_list(initial_state, allow_tuple=True)\n \n         is_keras_tensor = hasattr(initial_state[0], '_keras_history')\n         for tensor in initial_state:\n@@ -602,10 +601,7 @@ class Recurrent(Layer):\n             output = last_output\n \n         if self.return_state:\n-            if not isinstance(states, (list, tuple)):\n-                states = [states]\n-            else:\n-                states = list(states)\n+            states = to_list(states, allow_tuple=True)\n             return [output] + states\n         else:\n             return output\n@@ -633,8 +629,7 @@ class Recurrent(Layer):\n             for state in self.states:\n                 K.set_value(state, np.zeros((batch_size, self.units)))\n         else:\n-            if not isinstance(states, (list, tuple)):\n-                states = [states]\n+            states = to_list(states, allow_tuple=True)\n             if len(states) != len(self.states):\n                 raise ValueError('Layer ' + self.name + ' expects ' +\n                                  str(len(self.states)) + ' states, '\n\n@@ -444,7 +444,7 @@ class Progbar(object):\n         self.update(self._seen_so_far + n, values)\n \n \n-def to_list(x):\n+def to_list(x, allow_tuple=False):\n     \"\"\"Normalizes a list/tensor into a list.\n \n     If a tensor is passed, we return\n@@ -452,12 +452,18 @@ def to_list(x):\n \n     # Arguments\n         x: target object to be normalized.\n+        allow_tuple: If False and x is a tuple,\n+            it will be converted into a list\n+            with a single element (the tuple).\n+            Else converts the tuple to a list.\n \n     # Returns\n         A list.\n     \"\"\"\n     if isinstance(x, list):\n         return x\n+    if allow_tuple and isinstance(x, tuple):\n+        return list(x)\n     return [x]\n \n \n@@ -483,10 +489,7 @@ def object_list_uid(object_list):\n \n \n def is_all_none(iterable_or_element):\n-    if not isinstance(iterable_or_element, (list, tuple)):\n-        iterable = [iterable_or_element]\n-    else:\n-        iterable = iterable_or_element\n+    iterable = to_list(iterable_or_element, allow_tuple=True)\n     for element in iterable:\n         if element is not None:\n             return False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f60313e29657b2afb6a02f28dba5936bc0dd09e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 31 | Churn Cumulative: 4884 | Contributors (this commit): 82 | Commits (past 90d): 11 | Contributors (cumulative): 109 | DMM Complexity: 0.75\n\nDIFF:\n@@ -12,6 +12,8 @@ import numpy as np\n import time\n import json\n import warnings\n+import io\n+import sys\n \n from collections import deque\n from collections import OrderedDict\n@@ -1122,7 +1124,12 @@ class CSVLogger(Callback):\n         self.writer = None\n         self.keys = None\n         self.append_header = True\n-        self.file_flags = 'b' if six.PY2 and os.name == 'nt' else ''\n+        if six.PY2:\n+            self.file_flags = 'b'\n+            self._open_args = {}\n+        else:\n+            self.file_flags = ''\n+            self._open_args = {'newline': '\\n'}\n         super(CSVLogger, self).__init__()\n \n     def on_train_begin(self, logs=None):\n@@ -1130,9 +1137,12 @@ class CSVLogger(Callback):\n             if os.path.exists(self.filename):\n                 with open(self.filename, 'r' + self.file_flags) as f:\n                     self.append_header = not bool(len(f.readline()))\n-            self.csv_file = open(self.filename, 'a' + self.file_flags)\n+            mode = 'a'\n         else:\n-            self.csv_file = open(self.filename, 'w' + self.file_flags)\n+            mode = 'w'\n+        self.csv_file = io.open(self.filename,\n+                                mode + self.file_flags,\n+                                **self._open_args)\n \n     def on_epoch_end(self, epoch, logs=None):\n         logs = logs or {}\n@@ -1156,9 +1166,12 @@ class CSVLogger(Callback):\n         if not self.writer:\n             class CustomDialect(csv.excel):\n                 delimiter = self.sep\n-\n+            fieldnames = ['epoch'] + self.keys\n+            if six.PY2:\n+                fieldnames = [unicode(x) for x in fieldnames]\n             self.writer = csv.DictWriter(self.csv_file,\n-                                         fieldnames=['epoch'] + self.keys, dialect=CustomDialect)\n+                                         fieldnames=fieldnames,\n+                                         dialect=CustomDialect)\n             if self.append_header:\n                 self.writer.writeheader()\n \n\n@@ -556,11 +556,15 @@ def test_CSVLogger(tmpdir):\n \n     # case 3, reuse of CSVLogger object\n     model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n+              validation_data=(X_test, y_test), callbacks=cbks, epochs=2)\n \n     import re\n     with open(filepath) as csvfile:\n-        output = \" \".join(csvfile.readlines())\n+        list_lines = csvfile.readlines()\n+        for line in list_lines:\n+            assert line.count(sep) == 4\n+        assert len(list_lines) == 5\n+        output = \" \".join(list_lines)\n         assert len(re.findall('epoch', output)) == 1\n \n     os.remove(filepath)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#40a9ecc684d2c5e7fdfc09f46d0912940b74ab63", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 60 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -25,10 +25,9 @@ def load_data(path='boston_housing.npz', test_split=0.2, seed=113):\n     path = get_file(path,\n                     origin='https://s3.amazonaws.com/keras-datasets/boston_housing.npz',\n                     file_hash='f553886a1f8d56431e820c5b82552d9d95cfcb96d1e678153f8839538947dff5')\n-    f = np.load(path)\n+    with np.load(path) as f:\n         x = f['x']\n         y = f['y']\n-    f.close()\n \n     np.random.seed(seed)\n     indices = np.arange(len(x))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cd22c5a53bf738ea678cb5ce0e799093d527f2a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 65 | Lines Deleted: 7 | Files Changed: 3 | Hunks: 13 | Methods Changed: 5 | Complexity Δ (Sum/Max): 6/7 | Churn Δ: 72 | Churn Cumulative: 10436 | Contributors (this commit): 51 | Commits (past 90d): 49 | Contributors (cumulative): 73 | DMM Complexity: 0.95\n\nDIFF:\n@@ -1494,6 +1494,11 @@ def conv1d(x, kernel, strides=1, padding='valid',\n \n     if data_format == 'channels_last':\n         x = C.swapaxes(x, 0, 1)\n+\n+    # As of Keras 2.0.0, all kernels are normalized\n+    # on the format `(steps, input_depth, depth)`,\n+    # independently of `data_format`.\n+    # CNTK expects `(depth, input_depth, steps)`.\n     kernel = C.swapaxes(kernel, 0, 2)\n \n     padding = _preprocess_border_mode(padding)\n@@ -1502,6 +1507,8 @@ def conv1d(x, kernel, strides=1, padding='valid',\n         raise ValueError('Dilated convolution on CPU is not supported by CNTK backend. '\n                          'Please set `dilation_rate` to 1. You passed: %s' % (dilation_rate,))\n \n+    dilation_rate = (1, dilation_rate)\n+\n     x = C.convolution(\n         kernel,\n         x,\n@@ -1527,6 +1534,8 @@ def conv2d(x, kernel, strides=(1, 1), padding='valid',\n                          'Please set `dilation_rate` to (1, 1). '\n                          'You passed: %s' % (dilation_rate,))\n \n+    dilation_rate = (1,) + dilation_rate\n+\n     x = C.convolution(kernel,\n                       x,\n                       strides,\n@@ -1662,6 +1671,8 @@ def conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n                          'Please set `dilation_rate` to (1, 1, 1). '\n                          'You passed: %s' % (dilation_rate,))\n \n+    dilation_rate = (1,) + dilation_rate\n+\n     x = C.convolution(\n         kernel,\n         x,\n@@ -2200,6 +2211,8 @@ def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n         output_shape = transpose_shape(output_shape, 'channels_first',\n                                        spatial_axes=(0, 1))\n \n+    dilation_rate = (1,) + dilation_rate\n+\n     x = C.convolution_transpose(\n         kernel,\n         x,\n\n@@ -38,6 +38,21 @@ except ImportError:\n WITH_NP = [KTH if K.backend() == 'theano' else KC if K.backend() == 'cntk' else KTF, KNP]\n \n \n+# CNTK only supports dilated convolution on GPU\n+def get_dilated_conv_backends():\n+    backend_list = []\n+    if KTF is not None:\n+        backend_list.append(KTF)\n+    if KTH is not None:\n+        backend_list.append(KTH)\n+    if KC is not None and KC.dev.type() == 1:\n+        backend_list.append(KC)\n+    return backend_list\n+\n+\n+DILATED_CONV_BACKENDS = get_dilated_conv_backends()\n+\n+\n def check_dtype(var, dtype):\n     if K._BACKEND == 'theano':\n         assert var.dtype == dtype\n@@ -1017,6 +1032,7 @@ class TestBackend(object):\n     @pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format', [\n         ('conv1d', (2, 8, 2), (3, 2, 3), 'same', 'channels_last'),\n         ('conv1d', (1, 8, 2), (3, 2, 3), 'valid', 'channels_last'),\n+        ('conv1d', (1, 2, 8), (3, 2, 3), 'valid', 'channels_first'),\n         ('conv2d', (2, 3, 4, 5), (3, 3, 3, 2), 'same', 'channels_first'),\n         ('conv2d', (2, 3, 5, 6), (4, 3, 3, 4), 'valid', 'channels_first'),\n         ('conv2d', (1, 6, 5, 3), (3, 4, 3, 2), 'valid', 'channels_last'),\n@@ -1032,6 +1048,38 @@ class TestBackend(object):\n             padding=padding, data_format=data_format,\n             cntk_dynamicity=True)\n \n+    @pytest.mark.skipif((K.backend() == 'cntk' and K.dev.type() == 0),\n+                        reason='cntk only supports dilated conv on GPU')\n+    @pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format,dilation_rate', [\n+        ('conv1d', (2, 8, 3), (4, 3, 2), 'valid', 'channels_last', 2),\n+        ('conv1d', (2, 3, 8), (4, 3, 2), 'valid', 'channels_first', 2),\n+        ('conv2d', (2, 8, 9, 3), (3, 3, 3, 2), 'same', 'channels_last', (2, 2)),\n+        ('conv2d', (2, 3, 9, 8), (4, 3, 3, 4), 'valid', 'channels_first', (2, 2)),\n+        ('conv3d', (2, 5, 4, 6, 3), (2, 2, 3, 3, 4), 'valid', 'channels_last', (2, 2, 2)),\n+        ('conv3d', (2, 3, 5, 4, 6), (2, 2, 3, 3, 4), 'same', 'channels_first', (2, 2, 2)),\n+    ])\n+    def test_conv_dilation(self, op, input_shape, kernel_shape, padding,\n+                           data_format, dilation_rate):\n+        check_two_tensor_operation(\n+            op, input_shape, kernel_shape, DILATED_CONV_BACKENDS, padding=padding,\n+            data_format=data_format, dilation_rate=dilation_rate, cntk_dynamicity=True)\n+\n+    @pytest.mark.skipif((K.backend() == 'cntk' and K.dev.type() == 0),\n+                        reason='cntk only supports dilated conv transpose on GPU')\n+    @pytest.mark.parametrize(\n+        'op,input_shape,kernel_shape,output_shape,padding,data_format,dilation_rate', [\n+            ('conv2d_transpose', (2, 5, 6, 3), (3, 3, 2, 3), (2, 5, 6, 2),\n+             'same', 'channels_last', (2, 2)),\n+            ('conv2d_transpose', (2, 3, 8, 9), (3, 3, 2, 3), (2, 2, 8, 9),\n+             'same', 'channels_first', (2, 2)),\n+        ])\n+    def test_conv_transpose_dilation(self, op, input_shape, kernel_shape, output_shape,\n+                                     padding, data_format, dilation_rate):\n+        check_two_tensor_operation(\n+            op, input_shape, kernel_shape, DILATED_CONV_BACKENDS, output_shape=output_shape,\n+            padding=padding, data_format=data_format, dilation_rate=dilation_rate,\n+            cntk_dynamicity=True)\n+\n     @pytest.mark.parametrize('op,input_shape,kernel_shape,padding,data_format', [\n         ('depthwise_conv2d', (2, 3, 4, 5), (3, 3, 3, 2), 'same', 'channels_first'),\n         ('depthwise_conv2d', (2, 3, 5, 6), (4, 3, 3, 4), 'valid', 'channels_first'),\n\n@@ -17,7 +17,7 @@ else:\n \n \n @keras_test\n-@pytest.mark.skipif((K.backend() == 'cntk'),\n+@pytest.mark.skipif((K.backend() == 'cntk' and K.dev.type() == 0),\n                     reason='cntk only support dilated conv on GPU')\n @pytest.mark.parametrize(\n     'layer_kwargs,input_length,expected_output',\n@@ -79,7 +79,7 @@ def test_conv_1d(padding, strides):\n \n \n @keras_test\n-@pytest.mark.skipif((K.backend() == 'cntk'),\n+@pytest.mark.skipif((K.backend() == 'cntk' and K.dev.type() == 0),\n                     reason='cntk only support dilated conv on GPU')\n def test_conv_1d_dilation():\n     batch_size = 2\n@@ -163,7 +163,7 @@ def test_convolution_2d_channels_last():\n \n \n @keras_test\n-@pytest.mark.skipif((K.backend() == 'cntk'),\n+@pytest.mark.skipif((K.backend() == 'cntk' and K.dev.type() == 0),\n                     reason='cntk only supports dilated conv on GPU')\n def test_convolution_2d_dilation():\n     num_samples = 2\n@@ -174,9 +174,6 @@ def test_convolution_2d_dilation():\n     num_col = 6\n     padding = 'valid'\n \n-    # Test dilation\n-    if K.backend() != 'cntk':\n-        # cntk only support dilated conv on GPU\n     layer_test(convolutional.Conv2D,\n                kwargs={'filters': filters,\n                        'kernel_size': kernel_size,\n@@ -226,7 +223,7 @@ def test_conv2d_transpose(padding, out_padding, strides):\n \n \n @keras_test\n-@pytest.mark.skipif((K.backend() == 'cntk'),\n+@pytest.mark.skipif((K.backend() == 'cntk' and K.dev.type() == 0),\n                     reason='cntk only supports dilated conv transpose on GPU')\n def test_conv2d_transpose_dilation():\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ef0c95aca2fd19666711422c14a6b16f8025ed84", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 125 | Lines Deleted: 69 | Files Changed: 6 | Hunks: 50 | Methods Changed: 11 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 194 | Churn Cumulative: 19078 | Contributors (this commit): 104 | Commits (past 90d): 19 | Contributors (cumulative): 152 | DMM Complexity: 0.7948717948717948\n\nDIFF:\n@@ -35,7 +35,8 @@ class LeakyReLU(Layer):\n         alpha: float >= 0. Negative slope coefficient.\n \n     # References\n-        - [Rectifier Nonlinearities Improve Neural Network Acoustic Models](https://web.stanford.edu/~awni/papers/relu_hybrid_icml2013_final.pdf)\n+        - [Rectifier Nonlinearities Improve Neural Network Acoustic Models]\n+          (https://ai.stanford.edu/~amaas/papers/relu_hybrid_icml2013_final.pdf)\n     \"\"\"\n \n     def __init__(self, alpha=0.3, **kwargs):\n@@ -85,7 +86,8 @@ class PReLU(Layer):\n             set `shared_axes=[1, 2]`.\n \n     # References\n-        - [Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification](https://arxiv.org/abs/1502.01852)\n+        - [Delving Deep into Rectifiers: Surpassing Human-Level Performance on\n+           ImageNet Classification](https://arxiv.org/abs/1502.01852)\n     \"\"\"\n \n     @interfaces.legacy_prelu_support\n@@ -167,7 +169,8 @@ class ELU(Layer):\n         alpha: scale for the negative factor.\n \n     # References\n-        - [Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)](https://arxiv.org/abs/1511.07289v1)\n+        - [Fast and Accurate Deep Network Learning by Exponential Linear Units\n+           (ELUs)](https://arxiv.org/abs/1511.07289v1)\n     \"\"\"\n \n     def __init__(self, alpha=1.0, **kwargs):\n@@ -206,7 +209,8 @@ class ThresholdedReLU(Layer):\n         theta: float >= 0. Threshold location of activation.\n \n     # References\n-        - [Zero-Bias Autoencoders and the Benefits of Co-Adapting Features](http://arxiv.org/abs/1402.3337)\n+        - [Zero-Bias Autoencoders and the Benefits of Co-Adapting Features]\n+          (https://arxiv.org/abs/1402.3337)\n     \"\"\"\n \n     def __init__(self, theta=1.0, **kwargs):\n\n@@ -34,14 +34,12 @@ class ConvRNN2D(RNN):\n               `(output_at_t, states_at_t_plus_1)`. The call method of the\n               cell can also take the optional argument `constants`, see\n               section \"Note on passing external constants\" below.\n-            - a `state_size` attribute. This can be a single integer\n-                (single state) in which case it is\n-                the number of channels of the recurrent state\n-                (which should be the same as the number of channels of the cell output).\n-                This can also be a list/tuple of integers\n-                (one size per state). In this case, the first entry\n-                (`state_size[0]`) should be the same as\n-                the size of the cell output.\n+            - a `state_size` attribute. This can be a single integer (single state)\n+              in which case it is the number of channels of the recurrent state\n+              (which should be the same as the number of channels of the cell\n+              output). This can also be a list/tuple of integers\n+              (one size per state). In this case, the first entry (`state_size[0]`)\n+              should be the same as the size of the cell output.\n         return_sequences: Boolean. Whether to return the last output.\n             in the output sequence, or the full sequence.\n         return_state: Boolean. Whether to return the last state\n@@ -65,14 +63,18 @@ class ConvRNN2D(RNN):\n         - if `return_state`: a list of tensors. The first tensor is\n             the output. The remaining tensors are the last states,\n             each 5D tensor with shape:\n-            `(samples, timesteps, filters, new_rows, new_cols)` if data_format='channels_first'\n+            `(samples, timesteps,\n+              filters, new_rows, new_cols)` if data_format='channels_first'\n             or 5D tensor with shape:\n-            `(samples, timesteps, new_rows, new_cols, filters)` if data_format='channels_last'.\n+            `(samples, timesteps,\n+              new_rows, new_cols, filters)` if data_format='channels_last'.\n             `rows` and `cols` values might have changed due to padding.\n         - if `return_sequences`: 5D tensor with shape:\n-            `(samples, timesteps, filters, new_rows, new_cols)` if data_format='channels_first'\n+            `(samples, timesteps,\n+              filters, new_rows, new_cols)` if data_format='channels_first'\n             or 5D tensor with shape:\n-            `(samples, timesteps, new_rows, new_cols, filters)` if data_format='channels_last'.\n+            `(samples, timesteps,\n+              new_rows, new_cols, filters)` if data_format='channels_last'.\n         - else, 4D tensor with shape:\n             `(samples, filters, new_rows, new_cols)` if data_format='channels_first'\n             or 4D tensor with shape:\n@@ -224,7 +226,8 @@ class ConvRNN2D(RNN):\n                     'An initial_state was passed that is not compatible with '\n                     '`cell.state_size`. Received `state_spec`={}; '\n                     'However `cell.state_size` is '\n-                    '{}'.format([spec.shape for spec in self.state_spec], self.cell.state_size))\n+                    '{}'.format([spec.shape for spec in self.state_spec],\n+                                self.cell.state_size))\n         else:\n             if self.cell.data_format == 'channels_first':\n                 self.state_spec = [InputSpec(shape=(None, dim, None, None))\n@@ -413,7 +416,8 @@ class ConvRNN2D(RNN):\n                              '- If using the functional API, specify '\n                              'the time dimension by passing a '\n                              '`batch_shape` argument to your Input layer.\\n'\n-                             'The same thing goes for the number of rows and columns.')\n+                             'The same thing goes for the number of rows '\n+                             'and columns.')\n \n         # helper function\n         def get_tuple_shape(nb_channels):\n@@ -505,7 +509,8 @@ class ConvLSTM2DCell(Layer):\n         unit_forget_bias: Boolean.\n             If True, add 1 to the bias of the forget gate at initialization.\n             Use in combination with `bias_initializer=\"zeros\"`.\n-            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n+            This is recommended in [Jozefowicz et al.]\n+            (http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n         kernel_regularizer: Regularizer function applied to\n             the `kernel` weights matrix\n             (see [regularizer](../regularizers.md)).\n@@ -558,7 +563,8 @@ class ConvLSTM2DCell(Layer):\n         self.strides = conv_utils.normalize_tuple(strides, 2, 'strides')\n         self.padding = conv_utils.normalize_padding(padding)\n         self.data_format = K.normalize_data_format(data_format)\n-        self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, 2, 'dilation_rate')\n+        self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, 2,\n+                                                        'dilation_rate')\n         self.activation = activations.get(activation)\n         self.recurrent_activation = activations.get(recurrent_activation)\n         self.use_bias = use_bias\n@@ -636,9 +642,11 @@ class ConvLSTM2DCell(Layer):\n         self.kernel_i = self.kernel[:, :, :, :self.filters]\n         self.recurrent_kernel_i = self.recurrent_kernel[:, :, :, :self.filters]\n         self.kernel_f = self.kernel[:, :, :, self.filters: self.filters * 2]\n-        self.recurrent_kernel_f = self.recurrent_kernel[:, :, :, self.filters: self.filters * 2]\n+        self.recurrent_kernel_f = (\n+            self.recurrent_kernel[:, :, :, self.filters: self.filters * 2])\n         self.kernel_c = self.kernel[:, :, :, self.filters * 2: self.filters * 3]\n-        self.recurrent_kernel_c = self.recurrent_kernel[:, :, :, self.filters * 2: self.filters * 3]\n+        self.recurrent_kernel_c = (\n+            self.recurrent_kernel[:, :, :, self.filters * 2: self.filters * 3])\n         self.kernel_o = self.kernel[:, :, :, self.filters * 3:]\n         self.recurrent_kernel_o = self.recurrent_kernel[:, :, :, self.filters * 3:]\n \n@@ -752,17 +760,24 @@ class ConvLSTM2DCell(Layer):\n                   'data_format': self.data_format,\n                   'dilation_rate': self.dilation_rate,\n                   'activation': activations.serialize(self.activation),\n-                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n+                  'recurrent_activation':\n+                      activations.serialize(self.recurrent_activation),\n                   'use_bias': self.use_bias,\n-                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n-                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n+                  'kernel_initializer':\n+                      initializers.serialize(self.kernel_initializer),\n+                  'recurrent_initializer':\n+                      initializers.serialize(self.recurrent_initializer),\n                   'bias_initializer': initializers.serialize(self.bias_initializer),\n                   'unit_forget_bias': self.unit_forget_bias,\n-                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n-                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n+                  'kernel_regularizer':\n+                      regularizers.serialize(self.kernel_regularizer),\n+                  'recurrent_regularizer':\n+                      regularizers.serialize(self.recurrent_regularizer),\n                   'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n-                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n-                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n+                  'kernel_constraint':\n+                      constraints.serialize(self.kernel_constraint),\n+                  'recurrent_constraint':\n+                      constraints.serialize(self.recurrent_constraint),\n                   'bias_constraint': constraints.serialize(self.bias_constraint),\n                   'dropout': self.dropout,\n                   'recurrent_dropout': self.recurrent_dropout}\n@@ -820,7 +835,8 @@ class ConvLSTM2D(ConvRNN2D):\n         unit_forget_bias: Boolean.\n             If True, add 1 to the bias of the forget gate at initialization.\n             Use in combination with `bias_initializer=\"zeros\"`.\n-            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n+            This is recommended in [Jozefowicz et al.]\n+            (http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n         kernel_regularizer: Regularizer function applied to\n             the `kernel` weights matrix\n             (see [regularizer](../regularizers.md)).\n@@ -1043,18 +1059,26 @@ class ConvLSTM2D(ConvRNN2D):\n                   'data_format': self.data_format,\n                   'dilation_rate': self.dilation_rate,\n                   'activation': activations.serialize(self.activation),\n-                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n+                  'recurrent_activation':\n+                      activations.serialize(self.recurrent_activation),\n                   'use_bias': self.use_bias,\n-                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n-                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n+                  'kernel_initializer':\n+                      initializers.serialize(self.kernel_initializer),\n+                  'recurrent_initializer':\n+                      initializers.serialize(self.recurrent_initializer),\n                   'bias_initializer': initializers.serialize(self.bias_initializer),\n                   'unit_forget_bias': self.unit_forget_bias,\n-                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n-                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n+                  'kernel_regularizer':\n+                      regularizers.serialize(self.kernel_regularizer),\n+                  'recurrent_regularizer':\n+                      regularizers.serialize(self.recurrent_regularizer),\n                   'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n-                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n-                  'kernel_constraint': constraints.serialize(self.kernel_constraint),\n-                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n+                  'activity_regularizer':\n+                      regularizers.serialize(self.activity_regularizer),\n+                  'kernel_constraint':\n+                      constraints.serialize(self.kernel_constraint),\n+                  'recurrent_constraint':\n+                      constraints.serialize(self.recurrent_constraint),\n                   'bias_constraint': constraints.serialize(self.bias_constraint),\n                   'dropout': self.dropout,\n                   'recurrent_dropout': self.recurrent_dropout}\n\n@@ -95,7 +95,8 @@ class Dropout(Layer):\n         seed: A Python integer to use as random seed.\n \n     # References\n-        - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n+        - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting]\n+          (http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)\n     \"\"\"\n     @interfaces.legacy_dropout_support\n     def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n@@ -158,7 +159,8 @@ class SpatialDropout1D(Dropout):\n         Same as input\n \n     # References\n-        - [Efficient Object Localization Using Convolutional Networks](https://arxiv.org/abs/1411.4280)\n+        - [Efficient Object Localization Using Convolutional Networks]\n+          (https://arxiv.org/abs/1411.4280)\n     \"\"\"\n \n     @interfaces.legacy_spatialdropout1d_support\n@@ -203,7 +205,8 @@ class SpatialDropout2D(Dropout):\n         Same as input\n \n     # References\n-        - [Efficient Object Localization Using Convolutional Networks](https://arxiv.org/abs/1411.4280)\n+        - [Efficient Object Localization Using Convolutional Networks]\n+          (https://arxiv.org/abs/1411.4280)\n     \"\"\"\n \n     @interfaces.legacy_spatialdropoutNd_support\n@@ -251,7 +254,8 @@ class SpatialDropout3D(Dropout):\n         Same as input\n \n     # References\n-        - [Efficient Object Localization Using Convolutional Networks](https://arxiv.org/abs/1411.4280)\n+        - [Efficient Object Localization Using Convolutional Networks]\n+          (https://arxiv.org/abs/1411.4280)\n     \"\"\"\n \n     @interfaces.legacy_spatialdropoutNd_support\n@@ -669,7 +673,8 @@ class Lambda(Layer):\n         else:\n             shape = self._output_shape(input_shape)\n             if not isinstance(shape, (list, tuple)):\n-                raise ValueError('`output_shape` function must return a tuple or a list of tuples.')\n+                raise ValueError('`output_shape` function must return a tuple or '\n+                                 'a list of tuples.')\n             if isinstance(shape, list):\n                 if isinstance(shape[0], int) or shape[0] is None:\n                     shape = tuple(shape)\n@@ -894,7 +899,8 @@ class Dense(Layer):\n             'bias_initializer': initializers.serialize(self.bias_initializer),\n             'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n             'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n-            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n+            'activity_regularizer':\n+                regularizers.serialize(self.activity_regularizer),\n             'kernel_constraint': constraints.serialize(self.kernel_constraint),\n             'bias_constraint': constraints.serialize(self.bias_constraint)\n         }\n\n@@ -308,21 +308,25 @@ class CuDNNGRU(_CuDNNRNN):\n         config = {\n             'units': self.units,\n             'kernel_initializer': initializers.serialize(self.kernel_initializer),\n-            'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n+            'recurrent_initializer':\n+                initializers.serialize(self.recurrent_initializer),\n             'bias_initializer': initializers.serialize(self.bias_initializer),\n             'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n-            'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n+            'recurrent_regularizer':\n+                regularizers.serialize(self.recurrent_regularizer),\n             'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n-            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n+            'activity_regularizer':\n+                regularizers.serialize(self.activity_regularizer),\n             'kernel_constraint': constraints.serialize(self.kernel_constraint),\n-            'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n+            'recurrent_constraint':\n+                constraints.serialize(self.recurrent_constraint),\n             'bias_constraint': constraints.serialize(self.bias_constraint)}\n         base_config = super(CuDNNGRU, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n class CuDNNLSTM(_CuDNNRNN):\n-    \"\"\"Fast LSTM implementation backed by [CuDNN](https://developer.nvidia.com/cudnn).\n+    \"\"\"Fast LSTM implementation with [CuDNN](https://developer.nvidia.com/cudnn).\n \n     Can only be run on GPU, with the TensorFlow backend.\n \n@@ -334,7 +338,8 @@ class CuDNNLSTM(_CuDNNRNN):\n         unit_forget_bias: Boolean.\n             If True, add 1 to the bias of the forget gate at initialization.\n             Setting it to true will also force `bias_initializer=\"zeros\"`.\n-            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n+            This is recommended in [Jozefowicz et al.]\n+            (http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n         recurrent_initializer: Initializer for the `recurrent_kernel`\n             weights matrix,\n             used for the linear transformation of the recurrent state.\n@@ -457,8 +462,10 @@ class CuDNNLSTM(_CuDNNRNN):\n         self.kernel_o = self.kernel[:, self.units * 3:]\n \n         self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n-        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n-        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n+        self.recurrent_kernel_f = (\n+            self.recurrent_kernel[:, self.units: self.units * 2])\n+        self.recurrent_kernel_c = (\n+            self.recurrent_kernel[:, self.units * 2: self.units * 3])\n         self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n \n         self.bias_i_i = self.bias[:self.units]\n@@ -522,13 +529,16 @@ class CuDNNLSTM(_CuDNNRNN):\n         config = {\n             'units': self.units,\n             'kernel_initializer': initializers.serialize(self.kernel_initializer),\n-            'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n+            'recurrent_initializer':\n+                initializers.serialize(self.recurrent_initializer),\n             'bias_initializer': initializers.serialize(self.bias_initializer),\n             'unit_forget_bias': self.unit_forget_bias,\n             'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n-            'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n+            'recurrent_regularizer':\n+                regularizers.serialize(self.recurrent_regularizer),\n             'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n-            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n+            'activity_regularizer':\n+                regularizers.serialize(self.activity_regularizer),\n             'kernel_constraint': constraints.serialize(self.kernel_constraint),\n             'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n             'bias_constraint': constraints.serialize(self.bias_constraint)}\n\n@@ -25,7 +25,8 @@ class Embedding(Layer):\n       model = Sequential()\n       model.add(Embedding(1000, 64, input_length=10))\n       # the model will take as input an integer matrix of size (batch, input_length).\n-      # the largest integer (i.e. word index) in the input should be no larger than 999 (vocabulary size).\n+      # the largest integer (i.e. word index) in the input should be\n+      # no larger than 999 (vocabulary size).\n       # now model.output_shape == (None, 10, 64), where None is the batch dimension.\n \n       input_array = np.random.randint(1000, size=(32, 10))\n@@ -68,7 +69,8 @@ class Embedding(Layer):\n         3D tensor with shape: `(batch_size, sequence_length, output_dim)`.\n \n     # References\n-        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n+        - [A Theoretically Grounded Application of Dropout in\n+           Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n     \"\"\"\n \n     @interfaces.legacy_embedding_support\n@@ -120,12 +122,14 @@ class Embedding(Layer):\n             # input_length can be tuple if input is 3D or higher\n             in_lens = to_list(self.input_length, allow_tuple=True)\n             if len(in_lens) != len(input_shape) - 1:\n-                raise ValueError('\"input_length\" is %s, but received input has shape %s' %\n+                raise ValueError(\n+                    '\"input_length\" is %s, but received input has shape %s' %\n                     (str(self.input_length), str(input_shape)))\n             else:\n                 for i, (s1, s2) in enumerate(zip(in_lens, input_shape[1:])):\n                     if s1 is not None and s2 is not None and s1 != s2:\n-                        raise ValueError('\"input_length\" is %s, but received input has shape %s' %\n+                        raise ValueError(\n+                            '\"input_length\" is %s, but received input has shape %s' %\n                             (str(self.input_length), str(input_shape)))\n                     elif s1 is None:\n                         in_lens[i] = s2\n@@ -140,10 +144,14 @@ class Embedding(Layer):\n     def get_config(self):\n         config = {'input_dim': self.input_dim,\n                   'output_dim': self.output_dim,\n-                  'embeddings_initializer': initializers.serialize(self.embeddings_initializer),\n-                  'embeddings_regularizer': regularizers.serialize(self.embeddings_regularizer),\n-                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n-                  'embeddings_constraint': constraints.serialize(self.embeddings_constraint),\n+                  'embeddings_initializer':\n+                      initializers.serialize(self.embeddings_initializer),\n+                  'embeddings_regularizer':\n+                      regularizers.serialize(self.embeddings_regularizer),\n+                  'activity_regularizer':\n+                      regularizers.serialize(self.activity_regularizer),\n+                  'embeddings_constraint':\n+                      constraints.serialize(self.embeddings_constraint),\n                   'mask_zero': self.mask_zero,\n                   'input_length': self.input_length}\n         base_config = super(Embedding, self).get_config()\n\n@@ -170,7 +170,8 @@ class LocallyConnected1D(Layer):\n             'bias_initializer': initializers.serialize(self.bias_initializer),\n             'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n             'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n-            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n+            'activity_regularizer':\n+                regularizers.serialize(self.activity_regularizer),\n             'kernel_constraint': constraints.serialize(self.kernel_constraint),\n             'bias_constraint': constraints.serialize(self.bias_constraint)\n         }\n@@ -188,12 +189,13 @@ class LocallyConnected2D(Layer):\n \n     # Examples\n     ```python\n-        # apply a 3x3 unshared weights convolution with 64 output filters on a 32x32 image\n-        # with `data_format=\"channels_last\"`:\n+        # apply a 3x3 unshared weights convolution with 64 output filters\n+        # on a 32x32 image with `data_format=\"channels_last\"`:\n         model = Sequential()\n         model.add(LocallyConnected2D(64, (3, 3), input_shape=(32, 32, 3)))\n         # now model.output_shape == (None, 30, 30, 64)\n-        # notice that this layer will consume (30*30)*(3*3*3*64) + (30*30)*64 parameters\n+        # notice that this layer will consume (30*30)*(3*3*3*64)\n+        # + (30*30)*64 parameters\n \n         # add a 3x3 unshared weights convolution on top, with 32 output filters:\n         model.add(LocallyConnected2D(32, (3, 3)))\n@@ -313,7 +315,8 @@ class LocallyConnected2D(Layer):\n                                                    self.padding, self.strides[1])\n         self.output_row = output_row\n         self.output_col = output_col\n-        self.kernel_shape = (output_row * output_col,\n+        self.kernel_shape = (\n+            output_row * output_col,\n             self.kernel_size[0] * self.kernel_size[1] * input_filter,\n             self.filters)\n         self.kernel = self.add_weight(shape=self.kernel_shape,\n@@ -380,7 +383,8 @@ class LocallyConnected2D(Layer):\n             'bias_initializer': initializers.serialize(self.bias_initializer),\n             'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n             'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n-            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n+            'activity_regularizer':\n+                regularizers.serialize(self.activity_regularizer),\n             'kernel_constraint': constraints.serialize(self.kernel_constraint),\n             'bias_constraint': constraints.serialize(self.bias_constraint)\n         }\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
