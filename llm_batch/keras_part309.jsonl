{"custom_id": "keras#9a2b5092913884fa30f6790e8128e8d39655b68c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 20 | Files Changed: 1 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 413 | Contributors (this commit): 19 | Commits (past 90d): 2 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -2,37 +2,24 @@ from setuptools import setup\n from setuptools import find_packages\n \n long_description = '''\n-Keras is a high-level neural networks API,\n-written in Python and capable of running on top of\n-TensorFlow, CNTK, or Theano.\n-\n-Use Keras if you need a deep learning library that:\n-\n-- Allows for easy and fast prototyping\n-  (through user friendliness, modularity, and extensibility).\n-- Supports both convolutional networks and recurrent networks,\n-  as well as combinations of the two.\n-- Runs seamlessly on CPU and GPU.\n+Keras is a high-level neural networks API for Python.\n \n Read the documentation at: https://keras.io/\n \n-For a detailed overview of what makes Keras special, see:\n-https://keras.io/why-use-keras/\n-\n-Keras is compatible with Python 2.7-3.6\n+Keras is compatible with Python 3.6+\n and is distributed under the MIT license.\n '''\n \n setup(name='Keras',\n-      version='2.4.0',\n+      version='2.4.1',\n       description='Deep Learning for humans',\n       long_description=long_description,\n       author='Francois Chollet',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/keras-team/keras',\n-      download_url='https://github.com/keras-team/keras/tarball/2.3.0',\n+      download_url='https://github.com/keras-team/keras/tarball/2.4.1',\n       license='MIT',\n-      install_requires=['tensorflow>=2.2.0',\n+      install_requires=['tensorflow',\n                         'numpy>=1.9.1',\n                         'scipy>=0.14',\n                         'pyyaml',\n@@ -54,8 +41,6 @@ setup(name='Keras',\n           'Intended Audience :: Education',\n           'Intended Audience :: Science/Research',\n           'License :: OSI Approved :: MIT License',\n-          'Programming Language :: Python :: 2',\n-          'Programming Language :: Python :: 2.7',\n           'Programming Language :: Python :: 3',\n           'Programming Language :: Python :: 3.6',\n           'Topic :: Software Development :: Libraries',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7b76bed70c1e9528bddbfe0019da550865d0963a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 596 | Contributors (this commit): 22 | Commits (past 90d): 5 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -1,3 +1,11 @@\n+\r\n+try:\r\n+    from tensorflow.keras.layers.experimental.preprocessing import RandomRotation\r\n+except ImportError:\r\n+    raise ImportError(\r\n+        'Keras requires TensorFlow 2.2 or higher. '\r\n+        'Install TensorFlow via `pip install tensorflow`')\r\n+\r\n from . import utils\r\n from . import activations\r\n from . import applications\r\n@@ -21,4 +29,4 @@ from .layers import Input\n from .models import Model\r\n from .models import Sequential\r\n \r\n-from tensorflow.keras import __version__\r\n+__version__ = '2.4.2'\r\n\n@@ -11,16 +11,15 @@ and is distributed under the MIT license.\n '''\n \n setup(name='Keras',\n-      version='2.4.1',\n+      version='2.4.2',\n       description='Deep Learning for humans',\n       long_description=long_description,\n       author='Francois Chollet',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/keras-team/keras',\n-      download_url='https://github.com/keras-team/keras/tarball/2.4.1',\n+      download_url='https://github.com/keras-team/keras/tarball/2.4.2',\n       license='MIT',\n-      install_requires=['tensorflow',\n-                        'numpy>=1.9.1',\n+      install_requires=['numpy>=1.9.1',\n                         'scipy>=0.14',\n                         'pyyaml',\n                         'h5py'],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1a3ee8441933fc007be6b2beb47af67998d50737", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 602 | Contributors (this commit): 22 | Commits (past 90d): 7 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -29,4 +29,4 @@ from .layers import Input\n from .models import Model\r\n from .models import Sequential\r\n \r\n-__version__ = '2.4.2'\r\n+__version__ = '2.4.3'\r\n\n@@ -11,13 +11,13 @@ and is distributed under the MIT license.\n '''\n \n setup(name='Keras',\n-      version='2.4.2',\n+      version='2.4.3',\n       description='Deep Learning for humans',\n       long_description=long_description,\n       author='Francois Chollet',\n       author_email='francois.chollet@gmail.com',\n       url='https://github.com/keras-team/keras',\n-      download_url='https://github.com/keras-team/keras/tarball/2.4.2',\n+      download_url='https://github.com/keras-team/keras/tarball/2.4.3',\n       license='MIT',\n       install_requires=['numpy>=1.9.1',\n                         'scipy>=0.14',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#33c4e162e36452f6ad8377c05a25a20d86e754a7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 27092 | Files Changed: 172 | Hunks: 171 | Methods Changed: 945 | Complexity Δ (Sum/Max): -1858/0 | Churn Δ: 27092 | Churn Cumulative: 230244 | Contributors (this commit): 687 | Commits (past 90d): 172 | Contributors (cumulative): 2228 | DMM Complexity: 0.10249231819733698\n\nDIFF:\n\n@@ -1,473 +0,0 @@\n-# -*- coding: utf-8 -*-\n-from __future__ import print_function\n-from __future__ import unicode_literals\n-\n-import re\n-import inspect\n-import os\n-import shutil\n-import six\n-\n-try:\n-    import pathlib\n-except ImportError:\n-    import pathlib2 as pathlib\n-\n-import keras\n-from keras import backend as K\n-from keras.backend import numpy_backend\n-\n-from docs.structure import EXCLUDE\n-from docs.structure import PAGES\n-from docs.structure import template_np_implementation\n-from docs.structure import template_hidden_np_implementation\n-\n-import sys\n-if sys.version[0] == '2':\n-    reload(sys)\n-    sys.setdefaultencoding('utf8')\n-\n-keras_dir = pathlib.Path(__file__).resolve().parents[1]\n-\n-\n-def get_function_signature(function, method=True):\n-    wrapped = getattr(function, '_original_function', None)\n-    if wrapped is None:\n-        signature = inspect.getfullargspec(function)\n-    else:\n-        signature = inspect.getfullargspec(wrapped)\n-    defaults = signature.defaults\n-    if method:\n-        args = signature.args[1:]\n-    else:\n-        args = signature.args\n-    if defaults:\n-        kwargs = zip(args[-len(defaults):], defaults)\n-        args = args[:-len(defaults)]\n-    else:\n-        kwargs = []\n-    st = '%s.%s(' % (clean_module_name(function.__module__), function.__name__)\n-\n-    for a in args:\n-        st += str(a) + ', '\n-    for a, v in kwargs:\n-        if isinstance(v, str):\n-            v = '\\'' + v + '\\''\n-        st += str(a) + '=' + str(v) + ', '\n-    if kwargs or args:\n-        signature = st[:-2] + ')'\n-    else:\n-        signature = st + ')'\n-    return post_process_signature(signature)\n-\n-\n-def get_class_signature(cls):\n-    try:\n-        class_signature = get_function_signature(cls.__init__)\n-        class_signature = class_signature.replace('__init__', cls.__name__)\n-    except (TypeError, AttributeError):\n-        # in case the class inherits from object and does not\n-        # define __init__\n-        class_signature = \"{clean_module_name}.{cls_name}()\".format(\n-            clean_module_name=cls.__module__,\n-            cls_name=cls.__name__\n-        )\n-    return post_process_signature(class_signature)\n-\n-\n-def post_process_signature(signature):\n-    parts = re.split(r'\\.(?!\\d)', signature)\n-    if len(parts) >= 4:\n-        if parts[1] == 'layers':\n-            signature = 'keras.layers.' + '.'.join(parts[3:])\n-        if parts[1] == 'utils':\n-            signature = 'keras.utils.' + '.'.join(parts[3:])\n-        if parts[1] == 'backend':\n-            signature = 'keras.backend.' + '.'.join(parts[3:])\n-        if parts[1] == 'callbacks':\n-            signature = 'keras.callbacks.' + '.'.join(parts[3:])\n-    return signature\n-\n-\n-def clean_module_name(name):\n-    if name.startswith('keras_applications'):\n-        name = name.replace('keras_applications', 'keras.applications')\n-    if name.startswith('keras_preprocessing'):\n-        name = name.replace('keras_preprocessing', 'keras.preprocessing')\n-    return name\n-\n-\n-def class_to_source_link(cls):\n-    module_name = clean_module_name(cls.__module__)\n-    path = module_name.replace('.', '/')\n-    path += '.py'\n-    line = inspect.getsourcelines(cls)[-1]\n-    link = ('https://github.com/keras-team/'\n-            'keras/blob/master/' + path + '#L' + str(line))\n-    return '[[source]](' + link + ')'\n-\n-\n-def code_snippet(snippet):\n-    result = '```python\\n'\n-    result += snippet.encode('unicode_escape').decode('utf8') + '\\n'\n-    result += '```\\n'\n-    return result\n-\n-\n-def count_leading_spaces(s):\n-    ws = re.search(r'\\S', s)\n-    if ws:\n-        return ws.start()\n-    else:\n-        return 0\n-\n-\n-def process_list_block(docstring, starting_point, section_end,\n-                       leading_spaces, marker):\n-    ending_point = docstring.find('\\n\\n', starting_point)\n-    block = docstring[starting_point:\n-                      (ending_point - 1 if ending_point > -1\n-                       else section_end)]\n-    # Place marker for later reinjection.\n-    docstring_slice = docstring[\n-        starting_point:section_end].replace(block, marker)\n-    docstring = (docstring[:starting_point] +\n-                 docstring_slice +\n-                 docstring[section_end:])\n-    lines = block.split('\\n')\n-    # Remove the computed number of leading white spaces from each line.\n-    lines = [re.sub('^' + ' ' * leading_spaces, '', line) for line in lines]\n-    # Usually lines have at least 4 additional leading spaces.\n-    # These have to be removed, but first the list roots have to be detected.\n-    top_level_regex = r'^    ([^\\s\\\\\\(]+):(.*)'\n-    top_level_replacement = r'- __\\1__:\\2'\n-    lines = [re.sub(top_level_regex, top_level_replacement, line)\n-             for line in lines]\n-    # All the other lines get simply the 4 leading space (if present) removed\n-    lines = [re.sub(r'^    ', '', line) for line in lines]\n-    # Fix text lines after lists\n-    indent = 0\n-    text_block = False\n-    for i in range(len(lines)):\n-        line = lines[i]\n-        spaces = re.search(r'\\S', line)\n-        if spaces:\n-            # If it is a list element\n-            if line[spaces.start()] == '-':\n-                indent = spaces.start() + 1\n-                if text_block:\n-                    text_block = False\n-                    lines[i] = '\\n' + line\n-            elif spaces.start() < indent:\n-                text_block = True\n-                indent = spaces.start()\n-                lines[i] = '\\n' + line\n-        else:\n-            text_block = False\n-            indent = 0\n-    block = '\\n'.join(lines)\n-    return docstring, block\n-\n-\n-def process_docstring(docstring):\n-    # First, extract code blocks and process them.\n-    code_blocks = []\n-    if '```' in docstring:\n-        tmp = docstring[:]\n-        while '```' in tmp:\n-            tmp = tmp[tmp.find('```'):]\n-            index = tmp[3:].find('```') + 6\n-            snippet = tmp[:index]\n-            # Place marker in docstring for later reinjection.\n-            docstring = docstring.replace(\n-                snippet, '$CODE_BLOCK_%d' % len(code_blocks))\n-            snippet_lines = snippet.split('\\n')\n-            # Remove leading spaces.\n-            num_leading_spaces = snippet_lines[-1].find('`')\n-            snippet_lines = ([snippet_lines[0]] +\n-                             [line[num_leading_spaces:]\n-                             for line in snippet_lines[1:]])\n-            # Most code snippets have 3 or 4 more leading spaces\n-            # on inner lines, but not all. Remove them.\n-            inner_lines = snippet_lines[1:-1]\n-            leading_spaces = None\n-            for line in inner_lines:\n-                if not line or line[0] == '\\n':\n-                    continue\n-                spaces = count_leading_spaces(line)\n-                if leading_spaces is None:\n-                    leading_spaces = spaces\n-                if spaces < leading_spaces:\n-                    leading_spaces = spaces\n-            if leading_spaces:\n-                snippet_lines = ([snippet_lines[0]] +\n-                                 [line[leading_spaces:]\n-                                  for line in snippet_lines[1:-1]] +\n-                                 [snippet_lines[-1]])\n-            snippet = '\\n'.join(snippet_lines)\n-            code_blocks.append(snippet)\n-            tmp = tmp[index:]\n-\n-    # Format docstring lists.\n-    section_regex = r'\\n( +)# (.*)\\n'\n-    section_idx = re.search(section_regex, docstring)\n-    shift = 0\n-    sections = {}\n-    while section_idx and section_idx.group(2):\n-        anchor = section_idx.group(2)\n-        leading_spaces = len(section_idx.group(1))\n-        shift += section_idx.end()\n-        next_section_idx = re.search(section_regex, docstring[shift:])\n-        if next_section_idx is None:\n-            section_end = -1\n-        else:\n-            section_end = shift + next_section_idx.start()\n-        marker = '$' + anchor.replace(' ', '_') + '$'\n-        docstring, content = process_list_block(docstring,\n-                                                shift,\n-                                                section_end,\n-                                                leading_spaces,\n-                                                marker)\n-        sections[marker] = content\n-        # `docstring` has changed, so we can't use `next_section_idx` anymore\n-        # we have to recompute it\n-        section_idx = re.search(section_regex, docstring[shift:])\n-\n-    # Format docstring section titles.\n-    docstring = re.sub(r'\\n(\\s+)# (.*)\\n',\n-                       r'\\n\\1__\\2__\\n\\n',\n-                       docstring)\n-\n-    # Strip all remaining leading spaces.\n-    lines = docstring.split('\\n')\n-    docstring = '\\n'.join([line.lstrip(' ') for line in lines])\n-\n-    # Reinject list blocks.\n-    for marker, content in sections.items():\n-        docstring = docstring.replace(marker, content)\n-\n-    # Reinject code blocks.\n-    for i, code_block in enumerate(code_blocks):\n-        docstring = docstring.replace(\n-            '$CODE_BLOCK_%d' % i, code_block)\n-    return docstring\n-\n-\n-def add_np_implementation(function, docstring):\n-    np_implementation = getattr(numpy_backend, function.__name__)\n-    code = inspect.getsource(np_implementation)\n-    code_lines = code.split('\\n')\n-    for i in range(len(code_lines)):\n-        if code_lines[i]:\n-            # if there is something on the line, add 8 spaces.\n-            code_lines[i] = '        ' + code_lines[i]\n-    code = '\\n'.join(code_lines[:-1])\n-\n-    if len(code_lines) < 10:\n-        section = template_np_implementation.replace('{{code}}', code)\n-    else:\n-        section = template_hidden_np_implementation.replace('{{code}}', code)\n-    return docstring.replace('{{np_implementation}}', section)\n-\n-\n-def read_file(path):\n-    with open(path, encoding='utf-8') as f:\n-        return f.read()\n-\n-\n-def collect_class_methods(cls, methods):\n-    if isinstance(methods, (list, tuple)):\n-        return [getattr(cls, m) if isinstance(m, str) else m for m in methods]\n-    methods = []\n-    for _, method in inspect.getmembers(cls, predicate=inspect.isroutine):\n-        if method.__name__[0] == '_' or method.__name__ in EXCLUDE:\n-            continue\n-        methods.append(method)\n-    return methods\n-\n-\n-def render_function(function, method=True):\n-    subblocks = []\n-    signature = get_function_signature(function, method=method)\n-    if method:\n-        signature = signature.replace(\n-            clean_module_name(function.__module__) + '.', '')\n-    subblocks.append('### ' + function.__name__ + '\\n')\n-    subblocks.append(code_snippet(signature))\n-    docstring = function.__doc__\n-    if docstring:\n-        if ('backend' in signature and\n-                '{{np_implementation}}' in docstring):\n-            docstring = add_np_implementation(function, docstring)\n-        subblocks.append(process_docstring(docstring))\n-    return '\\n\\n'.join(subblocks)\n-\n-\n-def read_page_data(page_data, type):\n-    assert type in ['classes', 'functions', 'methods']\n-    data = page_data.get(type, [])\n-    for module in page_data.get('all_module_{}'.format(type), []):\n-        module_data = []\n-        for name in dir(module):\n-            if name[0] == '_' or name in EXCLUDE:\n-                continue\n-            module_member = getattr(module, name)\n-            if (inspect.isclass(module_member) and type == 'classes' or\n-               inspect.isfunction(module_member) and type == 'functions'):\n-                instance = module_member\n-                if module.__name__ in instance.__module__:\n-                    if instance not in module_data:\n-                        module_data.append(instance)\n-        module_data.sort(key=lambda x: id(x))\n-        data += module_data\n-    return data\n-\n-\n-def get_module_docstring(filepath):\n-    \"\"\"Extract the module docstring.\n-\n-    Also finds the line at which the docstring ends.\n-    \"\"\"\n-    co = compile(open(filepath, encoding='utf-8').read(), filepath, 'exec')\n-    if co.co_consts and isinstance(co.co_consts[0], six.string_types):\n-        docstring = co.co_consts[0]\n-    else:\n-        print('Could not get the docstring from ' + filepath)\n-        docstring = ''\n-    return docstring, co.co_firstlineno\n-\n-\n-def copy_examples(examples_dir, destination_dir):\n-    \"\"\"Copy the examples directory in the documentation.\n-\n-    Prettify files by extracting the docstrings written in Markdown.\n-    \"\"\"\n-    pathlib.Path(destination_dir).mkdir(exist_ok=True)\n-    for file in os.listdir(examples_dir):\n-        if not file.endswith('.py'):\n-            continue\n-        module_path = os.path.join(examples_dir, file)\n-        docstring, starting_line = get_module_docstring(module_path)\n-        destination_file = os.path.join(destination_dir, file[:-2] + 'md')\n-        with open(destination_file, 'w+', encoding='utf-8') as f_out, \\\n-                open(os.path.join(examples_dir, file),\n-                     'r+', encoding='utf-8') as f_in:\n-\n-            f_out.write(docstring + '\\n\\n')\n-\n-            # skip docstring\n-            for _ in range(starting_line):\n-                next(f_in)\n-\n-            f_out.write('```python\\n')\n-            # next line might be empty.\n-            line = next(f_in)\n-            if line != '\\n':\n-                f_out.write(line)\n-\n-            # copy the rest of the file.\n-            for line in f_in:\n-                f_out.write(line)\n-            f_out.write('```')\n-\n-\n-def generate(sources_dir):\n-    \"\"\"Generates the markdown files for the documentation.\n-\n-    # Arguments\n-        sources_dir: Where to put the markdown files.\n-    \"\"\"\n-    template_dir = os.path.join(str(keras_dir), 'docs', 'templates')\n-\n-    if K.backend() != 'tensorflow':\n-        raise RuntimeError('The documentation must be built '\n-                           'with the TensorFlow backend because this '\n-                           'is the only backend with docstrings.')\n-\n-    print('Cleaning up existing sources directory.')\n-    if os.path.exists(sources_dir):\n-        shutil.rmtree(sources_dir)\n-\n-    print('Populating sources directory with templates.')\n-    shutil.copytree(template_dir, sources_dir)\n-\n-    readme = read_file(os.path.join(str(keras_dir), 'README.md'))\n-    index = read_file(os.path.join(template_dir, 'index.md'))\n-    index = index.replace('{{autogenerated}}', readme[readme.find('##'):])\n-    with open(os.path.join(sources_dir, 'index.md'), 'w', encoding='utf-8') as f:\n-        f.write(index)\n-\n-    print('Generating docs for Keras %s.' % keras.__version__)\n-    for page_data in PAGES:\n-        classes = read_page_data(page_data, 'classes')\n-\n-        blocks = []\n-        for element in classes:\n-            if not isinstance(element, (list, tuple)):\n-                element = (element, [])\n-            cls = element[0]\n-            subblocks = []\n-            signature = get_class_signature(cls)\n-            subblocks.append('<span style=\"float:right;\">' +\n-                             class_to_source_link(cls) + '</span>')\n-            if element[1]:\n-                subblocks.append('## ' + cls.__name__ + ' class\\n')\n-            else:\n-                subblocks.append('### ' + cls.__name__ + '\\n')\n-            subblocks.append(code_snippet(signature))\n-            docstring = cls.__doc__\n-            if docstring:\n-                subblocks.append(process_docstring(docstring))\n-            methods = collect_class_methods(cls, element[1])\n-            if methods:\n-                subblocks.append('\\n---')\n-                subblocks.append('## ' + cls.__name__ + ' methods\\n')\n-                subblocks.append('\\n---\\n'.join(\n-                    [render_function(method, method=True)\n-                     for method in methods]))\n-            blocks.append('\\n'.join(subblocks))\n-\n-        methods = read_page_data(page_data, 'methods')\n-\n-        for method in methods:\n-            blocks.append(render_function(method, method=True))\n-\n-        functions = read_page_data(page_data, 'functions')\n-\n-        for function in functions:\n-            blocks.append(render_function(function, method=False))\n-\n-        if not blocks:\n-            raise RuntimeError('Found no content for page ' +\n-                               page_data['page'])\n-\n-        mkdown = '\\n----\\n\\n'.join(blocks)\n-        # Save module page.\n-        # Either insert content into existing page,\n-        # or create page otherwise.\n-        page_name = page_data['page']\n-        path = os.path.join(sources_dir, page_name)\n-        if os.path.exists(path):\n-            template = read_file(path)\n-            if '{{autogenerated}}' not in template:\n-                raise RuntimeError('Template found for ' + path +\n-                                   ' but missing {{autogenerated}}'\n-                                   ' tag.')\n-            mkdown = template.replace('{{autogenerated}}', mkdown)\n-            print('...inserting autogenerated content into template:', path)\n-        else:\n-            print('...creating new page with autogenerated content:', path)\n-        subdir = os.path.dirname(path)\n-        if not os.path.exists(subdir):\n-            os.makedirs(subdir)\n-        with open(path, 'w', encoding='utf-8') as f:\n-            f.write(mkdown)\n-\n-    shutil.copyfile(os.path.join(str(keras_dir), 'CONTRIBUTING.md'),\n-                    os.path.join(str(sources_dir), 'contributing.md'))\n-    copy_examples(os.path.join(str(keras_dir), 'examples'),\n-                  os.path.join(str(sources_dir), 'examples'))\n-\n-\n-if __name__ == '__main__':\n-    generate(os.path.join(str(keras_dir), 'docs', 'sources'))\n\n@@ -1,358 +0,0 @@\n-# -*- coding: utf-8 -*-\r\n-'''\r\n-General documentation architecture:\r\n-\r\n-Home\r\n-Index\r\n-\r\n-- Getting started\r\n-    Getting started with the sequential model\r\n-    Getting started with the functional api\r\n-    FAQ\r\n-\r\n-- Models\r\n-    About Keras models\r\n-        explain when one should use Sequential or functional API\r\n-        explain compilation step\r\n-        explain weight saving, weight loading\r\n-        explain serialization, deserialization\r\n-    Sequential\r\n-    Model (functional API)\r\n-\r\n-- Layers\r\n-    About Keras layers\r\n-        explain common layer functions: get_weights, set_weights, get_config\r\n-        explain input_shape\r\n-        explain usage on non-Keras tensors\r\n-    Core Layers\r\n-    Convolutional Layers\r\n-    Pooling Layers\r\n-    Locally-connected Layers\r\n-    Recurrent Layers\r\n-    Embedding Layers\r\n-    Merge Layers\r\n-    Advanced Activations Layers\r\n-    Normalization Layers\r\n-    Noise Layers\r\n-    Layer Wrappers\r\n-    Writing your own Keras layers\r\n-\r\n-- Preprocessing\r\n-    Sequence Preprocessing\r\n-    Text Preprocessing\r\n-    Image Preprocessing\r\n-\r\n-Losses\r\n-Metrics\r\n-Optimizers\r\n-Activations\r\n-Callbacks\r\n-Datasets\r\n-Applications\r\n-Backend\r\n-Initializers\r\n-Regularizers\r\n-Constraints\r\n-Visualization\r\n-Scikit-learn API\r\n-Utils\r\n-Contributing\r\n-\r\n-'''\r\n-from keras import utils\r\n-from keras import layers\r\n-from keras.layers import advanced_activations\r\n-from keras.layers import noise\r\n-from keras.layers import wrappers\r\n-from keras import initializers\r\n-from keras import optimizers\r\n-from keras import callbacks\r\n-from keras import models\r\n-from keras import losses\r\n-from keras import metrics\r\n-from keras import backend\r\n-from keras import constraints\r\n-from keras import activations\r\n-from keras import preprocessing\r\n-\r\n-\r\n-EXCLUDE = {\r\n-    'Optimizer',\r\n-    'TFOptimizer',\r\n-    'Wrapper',\r\n-    'get_session',\r\n-    'set_session',\r\n-    'CallbackList',\r\n-    'serialize',\r\n-    'deserialize',\r\n-    'get',\r\n-    'set_image_dim_ordering',\r\n-    'normalize_data_format',\r\n-    'image_dim_ordering',\r\n-    'get_variable_shape',\r\n-    'Constraint'\r\n-}\r\n-\r\n-# For each class to document, it is possible to:\r\n-# 1) Document only the class: [classA, classB, ...]\r\n-# 2) Document all its methods: [classA, (classB, \"*\")]\r\n-# 3) Choose which methods to document (methods listed as strings):\r\n-# [classA, (classB, [\"method1\", \"method2\", ...]), ...]\r\n-# 4) Choose which methods to document (methods listed as qualified names):\r\n-# [classA, (classB, [module.classB.method1, module.classB.method2, ...]), ...]\r\n-PAGES = [\r\n-    {\r\n-        'page': 'models/sequential.md',\r\n-        'methods': [\r\n-            models.Sequential.compile,\r\n-            models.Sequential.fit,\r\n-            models.Sequential.evaluate,\r\n-            models.Sequential.predict,\r\n-            models.Sequential.train_on_batch,\r\n-            models.Sequential.test_on_batch,\r\n-            models.Sequential.predict_on_batch,\r\n-            models.Sequential.fit_generator,\r\n-            models.Sequential.evaluate_generator,\r\n-            models.Sequential.predict_generator,\r\n-            models.Sequential.get_layer,\r\n-        ],\r\n-    },\r\n-    {\r\n-        'page': 'models/model.md',\r\n-        'methods': [\r\n-            models.Model.compile,\r\n-            models.Model.fit,\r\n-            models.Model.evaluate,\r\n-            models.Model.predict,\r\n-            models.Model.train_on_batch,\r\n-            models.Model.test_on_batch,\r\n-            models.Model.predict_on_batch,\r\n-            models.Model.fit_generator,\r\n-            models.Model.evaluate_generator,\r\n-            models.Model.predict_generator,\r\n-            models.Model.get_layer,\r\n-        ]\r\n-    },\r\n-    {\r\n-        'page': 'layers/core.md',\r\n-        'classes': [\r\n-            layers.Dense,\r\n-            layers.Activation,\r\n-            layers.Dropout,\r\n-            layers.Flatten,\r\n-            layers.Input,\r\n-            layers.Reshape,\r\n-            layers.Permute,\r\n-            layers.RepeatVector,\r\n-            layers.Lambda,\r\n-            layers.ActivityRegularization,\r\n-            layers.Masking,\r\n-            layers.SpatialDropout1D,\r\n-            layers.SpatialDropout2D,\r\n-            layers.SpatialDropout3D,\r\n-        ],\r\n-    },\r\n-    {\r\n-        'page': 'layers/convolutional.md',\r\n-        'classes': [\r\n-            layers.Conv1D,\r\n-            layers.Conv2D,\r\n-            layers.SeparableConv1D,\r\n-            layers.SeparableConv2D,\r\n-            layers.DepthwiseConv2D,\r\n-            layers.Conv2DTranspose,\r\n-            layers.Conv3D,\r\n-            layers.Conv3DTranspose,\r\n-            layers.Cropping1D,\r\n-            layers.Cropping2D,\r\n-            layers.Cropping3D,\r\n-            layers.UpSampling1D,\r\n-            layers.UpSampling2D,\r\n-            layers.UpSampling3D,\r\n-            layers.ZeroPadding1D,\r\n-            layers.ZeroPadding2D,\r\n-            layers.ZeroPadding3D,\r\n-        ],\r\n-    },\r\n-    {\r\n-        'page': 'layers/pooling.md',\r\n-        'classes': [\r\n-            layers.MaxPooling1D,\r\n-            layers.MaxPooling2D,\r\n-            layers.MaxPooling3D,\r\n-            layers.AveragePooling1D,\r\n-            layers.AveragePooling2D,\r\n-            layers.AveragePooling3D,\r\n-            layers.GlobalMaxPooling1D,\r\n-            layers.GlobalAveragePooling1D,\r\n-            layers.GlobalMaxPooling2D,\r\n-            layers.GlobalAveragePooling2D,\r\n-            layers.GlobalMaxPooling3D,\r\n-            layers.GlobalAveragePooling3D,\r\n-        ],\r\n-    },\r\n-    {\r\n-        'page': 'layers/local.md',\r\n-        'classes': [\r\n-            layers.LocallyConnected1D,\r\n-            layers.LocallyConnected2D,\r\n-        ],\r\n-    },\r\n-    {\r\n-        'page': 'layers/recurrent.md',\r\n-        'classes': [\r\n-            layers.RNN,\r\n-            layers.SimpleRNN,\r\n-            layers.GRU,\r\n-            layers.LSTM,\r\n-            layers.ConvLSTM2D,\r\n-            layers.ConvLSTM2DCell,\r\n-            layers.SimpleRNNCell,\r\n-            layers.GRUCell,\r\n-            layers.LSTMCell,\r\n-            layers.CuDNNGRU,\r\n-            layers.CuDNNLSTM,\r\n-        ],\r\n-    },\r\n-    {\r\n-        'page': 'layers/embeddings.md',\r\n-        'classes': [\r\n-            layers.Embedding,\r\n-        ],\r\n-    },\r\n-    {\r\n-        'page': 'layers/normalization.md',\r\n-        'classes': [\r\n-            layers.BatchNormalization,\r\n-        ],\r\n-    },\r\n-    {\r\n-        'page': 'layers/advanced-activations.md',\r\n-        'all_module_classes': [advanced_activations],\r\n-    },\r\n-    {\r\n-        'page': 'layers/noise.md',\r\n-        'all_module_classes': [noise],\r\n-    },\r\n-    {\r\n-        'page': 'layers/merge.md',\r\n-        'classes': [\r\n-            layers.Add,\r\n-            layers.Subtract,\r\n-            layers.Multiply,\r\n-            layers.Average,\r\n-            layers.Maximum,\r\n-            layers.Minimum,\r\n-            layers.Concatenate,\r\n-            layers.Dot,\r\n-        ],\r\n-        'functions': [\r\n-            layers.add,\r\n-            layers.subtract,\r\n-            layers.multiply,\r\n-            layers.average,\r\n-            layers.maximum,\r\n-            layers.minimum,\r\n-            layers.concatenate,\r\n-            layers.dot,\r\n-        ]\r\n-    },\r\n-    {\r\n-        'page': 'preprocessing/sequence.md',\r\n-        'functions': [\r\n-            preprocessing.sequence.pad_sequences,\r\n-            preprocessing.sequence.skipgrams,\r\n-            preprocessing.sequence.make_sampling_table,\r\n-        ],\r\n-        'classes': [\r\n-            preprocessing.sequence.TimeseriesGenerator,\r\n-        ]\r\n-    },\r\n-    {\r\n-        'page': 'preprocessing/image.md',\r\n-        'classes': [\r\n-            (preprocessing.image.ImageDataGenerator, '*')\r\n-        ]\r\n-    },\r\n-    {\r\n-        'page': 'preprocessing/text.md',\r\n-        'functions': [\r\n-            preprocessing.text.hashing_trick,\r\n-            preprocessing.text.one_hot,\r\n-            preprocessing.text.text_to_word_sequence,\r\n-        ],\r\n-        'classes': [\r\n-            preprocessing.text.Tokenizer,\r\n-        ]\r\n-    },\r\n-    {\r\n-        'page': 'layers/wrappers.md',\r\n-        'all_module_classes': [wrappers],\r\n-    },\r\n-    {\r\n-        'page': 'metrics.md',\r\n-        'all_module_functions': [metrics],\r\n-    },\r\n-    {\r\n-        'page': 'losses.md',\r\n-        'all_module_functions': [losses],\r\n-    },\r\n-    {\r\n-        'page': 'initializers.md',\r\n-        'all_module_functions': [initializers],\r\n-        'all_module_classes': [initializers],\r\n-    },\r\n-    {\r\n-        'page': 'optimizers.md',\r\n-        'all_module_classes': [optimizers],\r\n-    },\r\n-    {\r\n-        'page': 'callbacks.md',\r\n-        'all_module_classes': [callbacks],\r\n-    },\r\n-    {\r\n-        'page': 'activations.md',\r\n-        'all_module_functions': [activations],\r\n-    },\r\n-    {\r\n-        'page': 'backend.md',\r\n-        'all_module_functions': [backend],\r\n-    },\r\n-    {\r\n-        'page': 'constraints.md',\r\n-        'all_module_classes': [constraints],\r\n-    },\r\n-    {\r\n-        'page': 'utils.md',\r\n-        'functions': [utils.to_categorical,\r\n-                      utils.normalize,\r\n-                      utils.get_file,\r\n-                      utils.print_summary,\r\n-                      utils.plot_model,\r\n-                      utils.multi_gpu_model],\r\n-        'classes': [utils.CustomObjectScope,\r\n-                    utils.HDF5Matrix,\r\n-                    utils.Sequence],\r\n-    },\r\n-]\r\n-\r\n-ROOT = 'http://keras.io/'\r\n-\r\n-template_np_implementation = \"\"\"# Numpy implementation\r\n-\r\n-    ```python\r\n-{{code}}\r\n-    ```\r\n-\"\"\"\r\n-\r\n-template_hidden_np_implementation = \"\"\"# Numpy implementation\r\n-\r\n-    <details>\r\n-    <summary>Show the Numpy implementation</summary>\r\n-\r\n-    ```python\r\n-{{code}}\r\n-    ```\r\n-\r\n-    </details>\r\n-\"\"\"\r\n\n@@ -1,211 +0,0 @@\n-# -*- coding: utf-8 -*-\n-'''\n-# An implementation of sequence to sequence learning for performing addition\n-\n-Input: \"535+61\"\n-Output: \"596\"\n-Padding is handled by using a repeated sentinel character (space)\n-\n-Input may optionally be reversed, shown to increase performance in many tasks in:\n-\"Learning to Execute\"\n-http://arxiv.org/abs/1410.4615\n-and\n-\"Sequence to Sequence Learning with Neural Networks\"\n-http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf\n-Theoretically it introduces shorter term dependencies between source and target.\n-\n-Two digits reversed:\n-+ One layer LSTM (128 HN), 5k training examples = 99% train/test accuracy in 55 epochs\n-\n-Three digits reversed:\n-+ One layer LSTM (128 HN), 50k training examples = 99% train/test accuracy in 100 epochs\n-\n-Four digits reversed:\n-+ One layer LSTM (128 HN), 400k training examples = 99% train/test accuracy in 20 epochs\n-\n-Five digits reversed:\n-+ One layer LSTM (128 HN), 550k training examples = 99% train/test accuracy in 30 epochs\n-'''  # noqa\n-\n-from __future__ import print_function\n-from keras.models import Sequential\n-from keras import layers\n-import numpy as np\n-from six.moves import range\n-\n-\n-class CharacterTable(object):\n-    \"\"\"Given a set of characters:\n-    + Encode them to a one-hot integer representation\n-    + Decode the one-hot or integer representation to their character output\n-    + Decode a vector of probabilities to their character output\n-    \"\"\"\n-    def __init__(self, chars):\n-        \"\"\"Initialize character table.\n-\n-        # Arguments\n-            chars: Characters that can appear in the input.\n-        \"\"\"\n-        self.chars = sorted(set(chars))\n-        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n-        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n-\n-    def encode(self, C, num_rows):\n-        \"\"\"One-hot encode given string C.\n-\n-        # Arguments\n-            C: string, to be encoded.\n-            num_rows: Number of rows in the returned one-hot encoding. This is\n-                used to keep the # of rows for each data the same.\n-        \"\"\"\n-        x = np.zeros((num_rows, len(self.chars)))\n-        for i, c in enumerate(C):\n-            x[i, self.char_indices[c]] = 1\n-        return x\n-\n-    def decode(self, x, calc_argmax=True):\n-        \"\"\"Decode the given vector or 2D array to their character output.\n-\n-        # Arguments\n-            x: A vector or a 2D array of probabilities or one-hot representations;\n-                or a vector of character indices (used with `calc_argmax=False`).\n-            calc_argmax: Whether to find the character index with maximum\n-                probability, defaults to `True`.\n-        \"\"\"\n-        if calc_argmax:\n-            x = x.argmax(axis=-1)\n-        return ''.join(self.indices_char[x] for x in x)\n-\n-\n-class colors:\n-    ok = '\\033[92m'\n-    fail = '\\033[91m'\n-    close = '\\033[0m'\n-\n-# Parameters for the model and dataset.\n-TRAINING_SIZE = 50000\n-DIGITS = 3\n-REVERSE = True\n-\n-# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n-# int is DIGITS.\n-MAXLEN = DIGITS + 1 + DIGITS\n-\n-# All the numbers, plus sign and space for padding.\n-chars = '0123456789+ '\n-ctable = CharacterTable(chars)\n-\n-questions = []\n-expected = []\n-seen = set()\n-print('Generating data...')\n-while len(questions) < TRAINING_SIZE:\n-    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n-                    for i in range(np.random.randint(1, DIGITS + 1))))\n-    a, b = f(), f()\n-    # Skip any addition questions we've already seen\n-    # Also skip any such that x+Y == Y+x (hence the sorting).\n-    key = tuple(sorted((a, b)))\n-    if key in seen:\n-        continue\n-    seen.add(key)\n-    # Pad the data with spaces such that it is always MAXLEN.\n-    q = '{}+{}'.format(a, b)\n-    query = q + ' ' * (MAXLEN - len(q))\n-    ans = str(a + b)\n-    # Answers can be of maximum size DIGITS + 1.\n-    ans += ' ' * (DIGITS + 1 - len(ans))\n-    if REVERSE:\n-        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n-        # space used for padding.)\n-        query = query[::-1]\n-    questions.append(query)\n-    expected.append(ans)\n-print('Total addition questions:', len(questions))\n-\n-print('Vectorization...')\n-x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n-y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n-for i, sentence in enumerate(questions):\n-    x[i] = ctable.encode(sentence, MAXLEN)\n-for i, sentence in enumerate(expected):\n-    y[i] = ctable.encode(sentence, DIGITS + 1)\n-\n-# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n-# digits.\n-indices = np.arange(len(y))\n-np.random.shuffle(indices)\n-x = x[indices]\n-y = y[indices]\n-\n-# Explicitly set apart 10% for validation data that we never train over.\n-split_at = len(x) - len(x) // 10\n-(x_train, x_val) = x[:split_at], x[split_at:]\n-(y_train, y_val) = y[:split_at], y[split_at:]\n-\n-print('Training Data:')\n-print(x_train.shape)\n-print(y_train.shape)\n-\n-print('Validation Data:')\n-print(x_val.shape)\n-print(y_val.shape)\n-\n-# Try replacing GRU, or SimpleRNN.\n-RNN = layers.LSTM\n-HIDDEN_SIZE = 128\n-BATCH_SIZE = 128\n-LAYERS = 1\n-\n-print('Build model...')\n-model = Sequential()\n-# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n-# Note: In a situation where your input sequences have a variable length,\n-# use input_shape=(None, num_feature).\n-model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n-# As the decoder RNN's input, repeatedly provide with the last output of\n-# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n-# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n-model.add(layers.RepeatVector(DIGITS + 1))\n-# The decoder RNN could be multiple layers stacked or a single layer.\n-for _ in range(LAYERS):\n-    # By setting return_sequences to True, return not only the last output but\n-    # all the outputs so far in the form of (num_samples, timesteps,\n-    # output_dim). This is necessary as TimeDistributed in the below expects\n-    # the first dimension to be the timesteps.\n-    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n-\n-# Apply a dense layer to the every temporal slice of an input. For each of step\n-# of the output sequence, decide which character should be chosen.\n-model.add(layers.TimeDistributed(layers.Dense(len(chars), activation='softmax')))\n-model.compile(loss='categorical_crossentropy',\n-              optimizer='adam',\n-              metrics=['accuracy'])\n-model.summary()\n-\n-# Train the model each generation and show predictions against the validation\n-# dataset.\n-for iteration in range(1, 200):\n-    print()\n-    print('-' * 50)\n-    print('Iteration', iteration)\n-    model.fit(x_train, y_train,\n-              batch_size=BATCH_SIZE,\n-              epochs=1,\n-              validation_data=(x_val, y_val))\n-    # Select 10 samples from the validation set at random so we can visualize\n-    # errors.\n-    for i in range(10):\n-        ind = np.random.randint(0, len(x_val))\n-        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n-        preds = model.predict_classes(rowx, verbose=0)\n-        q = ctable.decode(rowx[0])\n-        correct = ctable.decode(rowy[0])\n-        guess = ctable.decode(preds[0], calc_argmax=False)\n-        print('Q', q[::-1] if REVERSE else q, end=' ')\n-        print('T', correct, end=' ')\n-        if correct == guess:\n-            print(colors.ok + '☑' + colors.close, end=' ')\n-        else:\n-            print(colors.fail + '☒' + colors.close, end=' ')\n-        print(guess)\n\n@@ -1,108 +0,0 @@\n-'''\n-#This example demonstrates how to write custom layers for Keras.\n-\n-We build a custom activation layer called 'Antirectifier',\n-which modifies the shape of the tensor that passes through it.\n-We need to specify two methods: `compute_output_shape` and `call`.\n-\n-Note that the same result can also be achieved via a Lambda layer.\n-\n-Because our custom layer is written with primitives from the Keras\n-backend (`K`), our code can run both on TensorFlow and Theano.\n-'''\n-\n-from __future__ import print_function\n-import keras\n-from keras.models import Sequential\n-from keras import layers\n-from keras.datasets import mnist\n-from keras import backend as K\n-\n-\n-class Antirectifier(layers.Layer):\n-    '''This is the combination of a sample-wise\n-    L2 normalization with the concatenation of the\n-    positive part of the input with the negative part\n-    of the input. The result is a tensor of samples that are\n-    twice as large as the input samples.\n-\n-    It can be used in place of a ReLU.\n-\n-    # Input shape\n-        2D tensor of shape (samples, n)\n-\n-    # Output shape\n-        2D tensor of shape (samples, 2*n)\n-\n-    # Theoretical justification\n-        When applying ReLU, assuming that the distribution\n-        of the previous output is approximately centered around 0.,\n-        you are discarding half of your input. This is inefficient.\n-\n-        Antirectifier allows to return all-positive outputs like ReLU,\n-        without discarding any data.\n-\n-        Tests on MNIST show that Antirectifier allows to train networks\n-        with twice less parameters yet with comparable\n-        classification accuracy as an equivalent ReLU-based network.\n-    '''\n-\n-    def compute_output_shape(self, input_shape):\n-        shape = list(input_shape)\n-        assert len(shape) == 2  # only valid for 2D tensors\n-        shape[-1] *= 2\n-        return tuple(shape)\n-\n-    def call(self, inputs):\n-        inputs -= K.mean(inputs, axis=1, keepdims=True)\n-        inputs = K.l2_normalize(inputs, axis=1)\n-        pos = K.relu(inputs)\n-        neg = K.relu(-inputs)\n-        return K.concatenate([pos, neg], axis=1)\n-\n-# global parameters\n-batch_size = 128\n-num_classes = 10\n-epochs = 40\n-\n-# the data, split between train and test sets\n-(x_train, y_train), (x_test, y_test) = mnist.load_data()\n-\n-x_train = x_train.reshape(60000, 784)\n-x_test = x_test.reshape(10000, 784)\n-x_train = x_train.astype('float32')\n-x_test = x_test.astype('float32')\n-x_train /= 255\n-x_test /= 255\n-print(x_train.shape[0], 'train samples')\n-print(x_test.shape[0], 'test samples')\n-\n-# convert class vectors to binary class matrices\n-y_train = keras.utils.to_categorical(y_train, num_classes)\n-y_test = keras.utils.to_categorical(y_test, num_classes)\n-\n-# build the model\n-model = Sequential()\n-model.add(layers.Dense(256, input_shape=(784,)))\n-model.add(Antirectifier())\n-model.add(layers.Dropout(0.1))\n-model.add(layers.Dense(256))\n-model.add(Antirectifier())\n-model.add(layers.Dropout(0.1))\n-model.add(layers.Dense(num_classes))\n-model.add(layers.Activation('softmax'))\n-\n-# compile the model\n-model.compile(loss='categorical_crossentropy',\n-              optimizer='rmsprop',\n-              metrics=['accuracy'])\n-\n-# train the model\n-model.fit(x_train, y_train,\n-          batch_size=batch_size,\n-          epochs=epochs,\n-          verbose=1,\n-          validation_data=(x_test, y_test))\n-\n-# next, compare with an equivalent network\n-# with2x bigger Dense layers and ReLU\n\n@@ -1,231 +0,0 @@\n-'''\n-#Trains a memory network on the bAbI dataset.\n-\n-References:\n-\n-- Jason Weston, Antoine Bordes, Sumit Chopra, Tomas Mikolov, Alexander M. Rush,\n-  [\"Towards AI-Complete Question Answering:\n-  A Set of Prerequisite Toy Tasks\"](http://arxiv.org/abs/1502.05698)\n-\n-- Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, Rob Fergus,\n-  [\"End-To-End Memory Networks\"](http://arxiv.org/abs/1503.08895)\n-\n-Reaches 98.6% accuracy on task 'single_supporting_fact_10k' after 120 epochs.\n-Time per epoch: 3s on CPU (core i7).\n-'''\n-from __future__ import print_function\n-\n-from keras.models import Sequential, Model\n-from keras.layers.embeddings import Embedding\n-from keras.layers import Input, Activation, Dense, Permute, Dropout\n-from keras.layers import add, dot, concatenate\n-from keras.layers import LSTM\n-from keras.utils.data_utils import get_file\n-from keras.preprocessing.sequence import pad_sequences\n-from functools import reduce\n-import tarfile\n-import numpy as np\n-import re\n-\n-\n-def tokenize(sent):\n-    '''Return the tokens of a sentence including punctuation.\n-\n-    >>> tokenize('Bob dropped the apple. Where is the apple?')\n-    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n-    '''\n-    return [x.strip() for x in re.split(r'(\\W+)?', sent) if x.strip()]\n-\n-\n-def parse_stories(lines, only_supporting=False):\n-    '''Parse stories provided in the bAbi tasks format\n-\n-    If only_supporting is true, only the sentences\n-    that support the answer are kept.\n-    '''\n-    data = []\n-    story = []\n-    for line in lines:\n-        line = line.decode('utf-8').strip()\n-        nid, line = line.split(' ', 1)\n-        nid = int(nid)\n-        if nid == 1:\n-            story = []\n-        if '\\t' in line:\n-            q, a, supporting = line.split('\\t')\n-            q = tokenize(q)\n-            if only_supporting:\n-                # Only select the related substory\n-                supporting = map(int, supporting.split())\n-                substory = [story[i - 1] for i in supporting]\n-            else:\n-                # Provide all the substories\n-                substory = [x for x in story if x]\n-            data.append((substory, q, a))\n-            story.append('')\n-        else:\n-            sent = tokenize(line)\n-            story.append(sent)\n-    return data\n-\n-\n-def get_stories(f, only_supporting=False, max_length=None):\n-    '''Given a file name, read the file,\n-    retrieve the stories,\n-    and then convert the sentences into a single story.\n-\n-    If max_length is supplied,\n-    any stories longer than max_length tokens will be discarded.\n-    '''\n-    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n-    flatten = lambda data: reduce(lambda x, y: x + y, data)\n-    data = [(flatten(story), q, answer) for story, q, answer in data\n-            if not max_length or len(flatten(story)) < max_length]\n-    return data\n-\n-\n-def vectorize_stories(data):\n-    inputs, queries, answers = [], [], []\n-    for story, query, answer in data:\n-        inputs.append([word_idx[w] for w in story])\n-        queries.append([word_idx[w] for w in query])\n-        answers.append(word_idx[answer])\n-    return (pad_sequences(inputs, maxlen=story_maxlen),\n-            pad_sequences(queries, maxlen=query_maxlen),\n-            np.array(answers))\n-\n-try:\n-    path = get_file('babi-tasks-v1-2.tar.gz',\n-                    origin='https://s3.amazonaws.com/text-datasets/'\n-                           'babi_tasks_1-20_v1-2.tar.gz')\n-except:\n-    print('Error downloading dataset, please download it manually:\\n'\n-          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2'\n-          '.tar.gz\\n'\n-          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n-    raise\n-\n-\n-challenges = {\n-    # QA1 with 10,000 samples\n-    'single_supporting_fact_10k': 'tasks_1-20_v1-2/en-10k/qa1_'\n-                                  'single-supporting-fact_{}.txt',\n-    # QA2 with 10,000 samples\n-    'two_supporting_facts_10k': 'tasks_1-20_v1-2/en-10k/qa2_'\n-                                'two-supporting-facts_{}.txt',\n-}\n-challenge_type = 'single_supporting_fact_10k'\n-challenge = challenges[challenge_type]\n-\n-print('Extracting stories for the challenge:', challenge_type)\n-with tarfile.open(path) as tar:\n-    train_stories = get_stories(tar.extractfile(challenge.format('train')))\n-    test_stories = get_stories(tar.extractfile(challenge.format('test')))\n-\n-vocab = set()\n-for story, q, answer in train_stories + test_stories:\n-    vocab |= set(story + q + [answer])\n-vocab = sorted(vocab)\n-\n-# Reserve 0 for masking via pad_sequences\n-vocab_size = len(vocab) + 1\n-story_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n-query_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories)))\n-\n-print('-')\n-print('Vocab size:', vocab_size, 'unique words')\n-print('Story max length:', story_maxlen, 'words')\n-print('Query max length:', query_maxlen, 'words')\n-print('Number of training stories:', len(train_stories))\n-print('Number of test stories:', len(test_stories))\n-print('-')\n-print('Here\\'s what a \"story\" tuple looks like (input, query, answer):')\n-print(train_stories[0])\n-print('-')\n-print('Vectorizing the word sequences...')\n-\n-word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n-inputs_train, queries_train, answers_train = vectorize_stories(train_stories)\n-inputs_test, queries_test, answers_test = vectorize_stories(test_stories)\n-\n-print('-')\n-print('inputs: integer tensor of shape (samples, max_length)')\n-print('inputs_train shape:', inputs_train.shape)\n-print('inputs_test shape:', inputs_test.shape)\n-print('-')\n-print('queries: integer tensor of shape (samples, max_length)')\n-print('queries_train shape:', queries_train.shape)\n-print('queries_test shape:', queries_test.shape)\n-print('-')\n-print('answers: binary (1 or 0) tensor of shape (samples, vocab_size)')\n-print('answers_train shape:', answers_train.shape)\n-print('answers_test shape:', answers_test.shape)\n-print('-')\n-print('Compiling...')\n-\n-# placeholders\n-input_sequence = Input((story_maxlen,))\n-question = Input((query_maxlen,))\n-\n-# encoders\n-# embed the input sequence into a sequence of vectors\n-input_encoder_m = Sequential()\n-input_encoder_m.add(Embedding(input_dim=vocab_size,\n-                              output_dim=64))\n-input_encoder_m.add(Dropout(0.3))\n-# output: (samples, story_maxlen, embedding_dim)\n-\n-# embed the input into a sequence of vectors of size query_maxlen\n-input_encoder_c = Sequential()\n-input_encoder_c.add(Embedding(input_dim=vocab_size,\n-                              output_dim=query_maxlen))\n-input_encoder_c.add(Dropout(0.3))\n-# output: (samples, story_maxlen, query_maxlen)\n-\n-# embed the question into a sequence of vectors\n-question_encoder = Sequential()\n-question_encoder.add(Embedding(input_dim=vocab_size,\n-                               output_dim=64,\n-                               input_length=query_maxlen))\n-question_encoder.add(Dropout(0.3))\n-# output: (samples, query_maxlen, embedding_dim)\n-\n-# encode input sequence and questions (which are indices)\n-# to sequences of dense vectors\n-input_encoded_m = input_encoder_m(input_sequence)\n-input_encoded_c = input_encoder_c(input_sequence)\n-question_encoded = question_encoder(question)\n-\n-# compute a 'match' between the first input vector sequence\n-# and the question vector sequence\n-# shape: `(samples, story_maxlen, query_maxlen)`\n-match = dot([input_encoded_m, question_encoded], axes=(2, 2))\n-match = Activation('softmax')(match)\n-\n-# add the match matrix with the second input vector sequence\n-response = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\n-response = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)\n-\n-# concatenate the match matrix with the question vector sequence\n-answer = concatenate([response, question_encoded])\n-\n-# the original paper uses a matrix multiplication for this reduction step.\n-# we choose to use a RNN instead.\n-answer = LSTM(32)(answer)  # (samples, 32)\n-\n-# one regularization layer -- more would probably be needed.\n-answer = Dropout(0.3)(answer)\n-answer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n-# we output a probability distribution over the vocabulary\n-answer = Activation('softmax')(answer)\n-\n-# build the final model\n-model = Model([input_sequence, question], answer)\n-model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',\n-              metrics=['accuracy'])\n-\n-# train\n-model.fit([inputs_train, queries_train], answers_train,\n-          batch_size=32,\n-          epochs=120,\n-          validation_data=([inputs_test, queries_test], answers_test))\n\n@@ -1,228 +0,0 @@\n-'''\n-# Trains two recurrent neural networks based upon a story and a question.\n-\n-The resulting merged vector is then queried to answer a range of bAbI tasks.\n-\n-The results are comparable to those for an LSTM model provided in Weston et al.:\n-\"Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks\"\n-http://arxiv.org/abs/1502.05698\n-\n-Task Number                  | FB LSTM Baseline | Keras QA\n----                          | ---              | ---\n-QA1 - Single Supporting Fact | 50               | 52.1\n-QA2 - Two Supporting Facts   | 20               | 37.0\n-QA3 - Three Supporting Facts | 20               | 20.5\n-QA4 - Two Arg. Relations     | 61               | 62.9\n-QA5 - Three Arg. Relations   | 70               | 61.9\n-QA6 - yes/No Questions       | 48               | 50.7\n-QA7 - Counting               | 49               | 78.9\n-QA8 - Lists/Sets             | 45               | 77.2\n-QA9 - Simple Negation        | 64               | 64.0\n-QA10 - Indefinite Knowledge  | 44               | 47.7\n-QA11 - Basic Coreference     | 72               | 74.9\n-QA12 - Conjunction           | 74               | 76.4\n-QA13 - Compound Coreference  | 94               | 94.4\n-QA14 - Time Reasoning        | 27               | 34.8\n-QA15 - Basic Deduction       | 21               | 32.4\n-QA16 - Basic Induction       | 23               | 50.6\n-QA17 - Positional Reasoning  | 51               | 49.1\n-QA18 - Size Reasoning        | 52               | 90.8\n-QA19 - Path Finding          | 8                | 9.0\n-QA20 - Agent's Motivations   | 91               | 90.7\n-\n-For the resources related to the bAbI project, refer to:\n-https://research.facebook.com/researchers/1543934539189348\n-\n-### Notes\n-\n-- With default word, sentence, and query vector sizes, the GRU model achieves:\n-  - 52.1% test accuracy on QA1 in 20 epochs (2 seconds per epoch on CPU)\n-  - 37.0% test accuracy on QA2 in 20 epochs (16 seconds per epoch on CPU)\n-In comparison, the Facebook paper achieves 50% and 20% for the LSTM baseline.\n-\n-- The task does not traditionally parse the question separately. This likely\n-improves accuracy and is a good example of merging two RNNs.\n-\n-- The word vector embeddings are not shared between the story and question RNNs.\n-\n-- See how the accuracy changes given 10,000 training samples (en-10k) instead\n-of only 1000. 1000 was used in order to be comparable to the original paper.\n-\n-- Experiment with GRU, LSTM, and JZS1-3 as they give subtly different results.\n-\n-- The length and noise (i.e. 'useless' story components) impact the ability of\n-LSTMs / GRUs to provide the correct answer. Given only the supporting facts,\n-these RNNs can achieve 100% accuracy on many tasks. Memory networks and neural\n-networks that use attentional processes can efficiently search through this\n-noise to find the relevant statements, improving performance substantially.\n-This becomes especially obvious on QA2 and QA3, both far longer than QA1.\n-'''\n-\n-from __future__ import print_function\n-from functools import reduce\n-import re\n-import tarfile\n-\n-import numpy as np\n-\n-from keras.utils.data_utils import get_file\n-from keras.layers.embeddings import Embedding\n-from keras import layers\n-from keras.layers import recurrent\n-from keras.models import Model\n-from keras.preprocessing.sequence import pad_sequences\n-\n-\n-def tokenize(sent):\n-    '''Return the tokens of a sentence including punctuation.\n-\n-    >>> tokenize('Bob dropped the apple. Where is the apple?')\n-    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n-    '''\n-    return [x.strip() for x in re.split(r'(\\W+)', sent) if x.strip()]\n-\n-\n-def parse_stories(lines, only_supporting=False):\n-    '''Parse stories provided in the bAbi tasks format\n-\n-    If only_supporting is true,\n-    only the sentences that support the answer are kept.\n-    '''\n-    data = []\n-    story = []\n-    for line in lines:\n-        line = line.decode('utf-8').strip()\n-        nid, line = line.split(' ', 1)\n-        nid = int(nid)\n-        if nid == 1:\n-            story = []\n-        if '\\t' in line:\n-            q, a, supporting = line.split('\\t')\n-            q = tokenize(q)\n-            if only_supporting:\n-                # Only select the related substory\n-                supporting = map(int, supporting.split())\n-                substory = [story[i - 1] for i in supporting]\n-            else:\n-                # Provide all the substories\n-                substory = [x for x in story if x]\n-            data.append((substory, q, a))\n-            story.append('')\n-        else:\n-            sent = tokenize(line)\n-            story.append(sent)\n-    return data\n-\n-\n-def get_stories(f, only_supporting=False, max_length=None):\n-    '''Given a file name, read the file, retrieve the stories,\n-    and then convert the sentences into a single story.\n-\n-    If max_length is supplied,\n-    any stories longer than max_length tokens will be discarded.\n-    '''\n-    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n-    flatten = lambda data: reduce(lambda x, y: x + y, data)\n-    data = [(flatten(story), q, answer) for story, q, answer in data\n-            if not max_length or len(flatten(story)) < max_length]\n-    return data\n-\n-\n-def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n-    xs = []\n-    xqs = []\n-    ys = []\n-    for story, query, answer in data:\n-        x = [word_idx[w] for w in story]\n-        xq = [word_idx[w] for w in query]\n-        # let's not forget that index 0 is reserved\n-        y = np.zeros(len(word_idx) + 1)\n-        y[word_idx[answer]] = 1\n-        xs.append(x)\n-        xqs.append(xq)\n-        ys.append(y)\n-    return (pad_sequences(xs, maxlen=story_maxlen),\n-            pad_sequences(xqs, maxlen=query_maxlen), np.array(ys))\n-\n-RNN = recurrent.LSTM\n-EMBED_HIDDEN_SIZE = 50\n-SENT_HIDDEN_SIZE = 100\n-QUERY_HIDDEN_SIZE = 100\n-BATCH_SIZE = 32\n-EPOCHS = 20\n-print('RNN / Embed / Sent / Query = {}, {}, {}, {}'.format(RNN,\n-                                                           EMBED_HIDDEN_SIZE,\n-                                                           SENT_HIDDEN_SIZE,\n-                                                           QUERY_HIDDEN_SIZE))\n-\n-try:\n-    path = get_file('babi-tasks-v1-2.tar.gz',\n-                    origin='https://s3.amazonaws.com/text-datasets/'\n-                           'babi_tasks_1-20_v1-2.tar.gz')\n-except:\n-    print('Error downloading dataset, please download it manually:\\n'\n-          '$ wget http://www.thespermwhale.com/jaseweston/babi/tasks_1-20_v1-2'\n-          '.tar.gz\\n'\n-          '$ mv tasks_1-20_v1-2.tar.gz ~/.keras/datasets/babi-tasks-v1-2.tar.gz')\n-    raise\n-\n-# Default QA1 with 1000 samples\n-# challenge = 'tasks_1-20_v1-2/en/qa1_single-supporting-fact_{}.txt'\n-# QA1 with 10,000 samples\n-# challenge = 'tasks_1-20_v1-2/en-10k/qa1_single-supporting-fact_{}.txt'\n-# QA2 with 1000 samples\n-challenge = 'tasks_1-20_v1-2/en/qa2_two-supporting-facts_{}.txt'\n-# QA2 with 10,000 samples\n-# challenge = 'tasks_1-20_v1-2/en-10k/qa2_two-supporting-facts_{}.txt'\n-with tarfile.open(path) as tar:\n-    train = get_stories(tar.extractfile(challenge.format('train')))\n-    test = get_stories(tar.extractfile(challenge.format('test')))\n-\n-vocab = set()\n-for story, q, answer in train + test:\n-    vocab |= set(story + q + [answer])\n-vocab = sorted(vocab)\n-\n-# Reserve 0 for masking via pad_sequences\n-vocab_size = len(vocab) + 1\n-word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n-story_maxlen = max(map(len, (x for x, _, _ in train + test)))\n-query_maxlen = max(map(len, (x for _, x, _ in train + test)))\n-\n-x, xq, y = vectorize_stories(train, word_idx, story_maxlen, query_maxlen)\n-tx, txq, ty = vectorize_stories(test, word_idx, story_maxlen, query_maxlen)\n-\n-print('vocab = {}'.format(vocab))\n-print('x.shape = {}'.format(x.shape))\n-print('xq.shape = {}'.format(xq.shape))\n-print('y.shape = {}'.format(y.shape))\n-print('story_maxlen, query_maxlen = {}, {}'.format(story_maxlen, query_maxlen))\n-\n-print('Build model...')\n-\n-sentence = layers.Input(shape=(story_maxlen,), dtype='int32')\n-encoded_sentence = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(sentence)\n-encoded_sentence = RNN(SENT_HIDDEN_SIZE)(encoded_sentence)\n-\n-question = layers.Input(shape=(query_maxlen,), dtype='int32')\n-encoded_question = layers.Embedding(vocab_size, EMBED_HIDDEN_SIZE)(question)\n-encoded_question = RNN(QUERY_HIDDEN_SIZE)(encoded_question)\n-\n-merged = layers.concatenate([encoded_sentence, encoded_question])\n-preds = layers.Dense(vocab_size, activation='softmax')(merged)\n-\n-model = Model([sentence, question], preds)\n-model.compile(optimizer='adam',\n-              loss='categorical_crossentropy',\n-              metrics=['accuracy'])\n-\n-print('Training')\n-model.fit([x, xq], y,\n-          batch_size=BATCH_SIZE,\n-          epochs=EPOCHS,\n-          validation_split=0.05)\n-\n-print('Evaluation')\n-loss, acc = model.evaluate([tx, txq], ty,\n-                           batch_size=BATCH_SIZE)\n-print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))\n\n@@ -1,131 +0,0 @@\n-'''\n-#Train a simple deep CNN on the CIFAR10 small images dataset.\n-\n-It gets to 75% validation accuracy in 25 epochs, and 79% after 50 epochs.\n-(it's still underfitting at that point, though).\n-'''\n-\n-from __future__ import print_function\n-import keras\n-from keras.datasets import cifar10\n-from keras.preprocessing.image import ImageDataGenerator\n-from keras.models import Sequential\n-from keras.layers import Dense, Dropout, Activation, Flatten\n-from keras.layers import Conv2D, MaxPooling2D\n-import os\n-\n-batch_size = 32\n-num_classes = 10\n-epochs = 100\n-data_augmentation = True\n-num_predictions = 20\n-save_dir = os.path.join(os.getcwd(), 'saved_models')\n-model_name = 'keras_cifar10_trained_model.h5'\n-\n-# The data, split between train and test sets:\n-(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n-print('x_train shape:', x_train.shape)\n-print(x_train.shape[0], 'train samples')\n-print(x_test.shape[0], 'test samples')\n-\n-# Convert class vectors to binary class matrices.\n-y_train = keras.utils.to_categorical(y_train, num_classes)\n-y_test = keras.utils.to_categorical(y_test, num_classes)\n-\n-model = Sequential()\n-model.add(Conv2D(32, (3, 3), padding='same',\n-                 input_shape=x_train.shape[1:]))\n-model.add(Activation('relu'))\n-model.add(Conv2D(32, (3, 3)))\n-model.add(Activation('relu'))\n-model.add(MaxPooling2D(pool_size=(2, 2)))\n-model.add(Dropout(0.25))\n-\n-model.add(Conv2D(64, (3, 3), padding='same'))\n-model.add(Activation('relu'))\n-model.add(Conv2D(64, (3, 3)))\n-model.add(Activation('relu'))\n-model.add(MaxPooling2D(pool_size=(2, 2)))\n-model.add(Dropout(0.25))\n-\n-model.add(Flatten())\n-model.add(Dense(512))\n-model.add(Activation('relu'))\n-model.add(Dropout(0.5))\n-model.add(Dense(num_classes))\n-model.add(Activation('softmax'))\n-\n-# initiate RMSprop optimizer\n-opt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n-\n-# Let's train the model using RMSprop\n-model.compile(loss='categorical_crossentropy',\n-              optimizer=opt,\n-              metrics=['accuracy'])\n-\n-x_train = x_train.astype('float32')\n-x_test = x_test.astype('float32')\n-x_train /= 255\n-x_test /= 255\n-\n-if not data_augmentation:\n-    print('Not using data augmentation.')\n-    model.fit(x_train, y_train,\n-              batch_size=batch_size,\n-              epochs=epochs,\n-              validation_data=(x_test, y_test),\n-              shuffle=True)\n-else:\n-    print('Using real-time data augmentation.')\n-    # This will do preprocessing and realtime data augmentation:\n-    datagen = ImageDataGenerator(\n-        featurewise_center=False,  # set input mean to 0 over the dataset\n-        samplewise_center=False,  # set each sample mean to 0\n-        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n-        samplewise_std_normalization=False,  # divide each input by its std\n-        zca_whitening=False,  # apply ZCA whitening\n-        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n-        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n-        # randomly shift images horizontally (fraction of total width)\n-        width_shift_range=0.1,\n-        # randomly shift images vertically (fraction of total height)\n-        height_shift_range=0.1,\n-        shear_range=0.,  # set range for random shear\n-        zoom_range=0.,  # set range for random zoom\n-        channel_shift_range=0.,  # set range for random channel shifts\n-        # set mode for filling points outside the input boundaries\n-        fill_mode='nearest',\n-        cval=0.,  # value used for fill_mode = \"constant\"\n-        horizontal_flip=True,  # randomly flip images\n-        vertical_flip=False,  # randomly flip images\n-        # set rescaling factor (applied before any other transformation)\n-        rescale=None,\n-        # set function that will be applied on each input\n-        preprocessing_function=None,\n-        # image data format, either \"channels_first\" or \"channels_last\"\n-        data_format=None,\n-        # fraction of images reserved for validation (strictly between 0 and 1)\n-        validation_split=0.0)\n-\n-    # Compute quantities required for feature-wise normalization\n-    # (std, mean, and principal components if ZCA whitening is applied).\n-    datagen.fit(x_train)\n-\n-    # Fit the model on the batches generated by datagen.flow().\n-    model.fit_generator(datagen.flow(x_train, y_train,\n-                                     batch_size=batch_size),\n-                        epochs=epochs,\n-                        validation_data=(x_test, y_test),\n-                        workers=4)\n-\n-# Save model and weights\n-if not os.path.isdir(save_dir):\n-    os.makedirs(save_dir)\n-model_path = os.path.join(save_dir, model_name)\n-model.save(model_path)\n-print('Saved trained model at %s ' % model_path)\n-\n-# Score trained model.\n-scores = model.evaluate(x_test, y_test, verbose=1)\n-print('Test loss:', scores[0])\n-print('Test accuracy:', scores[1])\n\n@@ -1,455 +0,0 @@\n-\"\"\"\n-#Trains a ResNet on the CIFAR10 dataset.\n-\n-ResNet v1:\n-[Deep Residual Learning for Image Recognition\n-](https://arxiv.org/pdf/1512.03385.pdf)\n-\n-ResNet v2:\n-[Identity Mappings in Deep Residual Networks\n-](https://arxiv.org/pdf/1603.05027.pdf)\n-\n-\n-Model|n|200-epoch accuracy|Original paper accuracy |sec/epoch GTX1080Ti\n-:------------|--:|-------:|-----------------------:|---:\n-ResNet20   v1|  3| 92.16 %|                 91.25 %|35\n-ResNet32   v1|  5| 92.46 %|                 92.49 %|50\n-ResNet44   v1|  7| 92.50 %|                 92.83 %|70\n-ResNet56   v1|  9| 92.71 %|                 93.03 %|90\n-ResNet110  v1| 18| 92.65 %|            93.39+-.16 %|165\n-ResNet164  v1| 27|     - %|                 94.07 %|  -\n-ResNet1001 v1|N/A|     - %|                 92.39 %|  -\n-\n-&nbsp;\n-\n-Model|n|200-epoch accuracy|Original paper accuracy |sec/epoch GTX1080Ti\n-:------------|--:|-------:|-----------------------:|---:\n-ResNet20   v2|  2|     - %|                     - %|---\n-ResNet32   v2|N/A| NA    %|            NA         %| NA\n-ResNet44   v2|N/A| NA    %|            NA         %| NA\n-ResNet56   v2|  6| 93.01 %|            NA         %|100\n-ResNet110  v2| 12| 93.15 %|            93.63      %|180\n-ResNet164  v2| 18|     - %|            94.54      %|  -\n-ResNet1001 v2|111|     - %|            95.08+-.14 %|  -\n-\"\"\"\n-\n-from __future__ import print_function\n-import keras\n-from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n-from keras.layers import AveragePooling2D, Input, Flatten\n-from keras.optimizers import Adam\n-from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n-from keras.callbacks import ReduceLROnPlateau\n-from keras.preprocessing.image import ImageDataGenerator\n-from keras.regularizers import l2\n-from keras import backend as K\n-from keras.models import Model\n-from keras.datasets import cifar10\n-import numpy as np\n-import os\n-\n-# Training parameters\n-batch_size = 32  # orig paper trained all networks with batch_size=128\n-epochs = 200\n-data_augmentation = True\n-num_classes = 10\n-\n-# Subtracting pixel mean improves accuracy\n-subtract_pixel_mean = True\n-\n-# Model parameter\n-# ----------------------------------------------------------------------------\n-#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n-# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n-#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n-# ----------------------------------------------------------------------------\n-# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n-# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n-# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n-# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n-# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n-# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n-# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n-# ---------------------------------------------------------------------------\n-n = 3\n-\n-# Model version\n-# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n-version = 1\n-\n-# Computed depth from supplied model parameter n\n-if version == 1:\n-    depth = n * 6 + 2\n-elif version == 2:\n-    depth = n * 9 + 2\n-\n-# Model name, depth and version\n-model_type = 'ResNet%dv%d' % (depth, version)\n-\n-# Load the CIFAR10 data.\n-(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n-\n-# Input image dimensions.\n-input_shape = x_train.shape[1:]\n-\n-# Normalize data.\n-x_train = x_train.astype('float32') / 255\n-x_test = x_test.astype('float32') / 255\n-\n-# If subtract pixel mean is enabled\n-if subtract_pixel_mean:\n-    x_train_mean = np.mean(x_train, axis=0)\n-    x_train -= x_train_mean\n-    x_test -= x_train_mean\n-\n-print('x_train shape:', x_train.shape)\n-print(x_train.shape[0], 'train samples')\n-print(x_test.shape[0], 'test samples')\n-print('y_train shape:', y_train.shape)\n-\n-# Convert class vectors to binary class matrices.\n-y_train = keras.utils.to_categorical(y_train, num_classes)\n-y_test = keras.utils.to_categorical(y_test, num_classes)\n-\n-\n-def lr_schedule(epoch):\n-    \"\"\"Learning Rate Schedule\n-\n-    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n-    Called automatically every epoch as part of callbacks during training.\n-\n-    # Arguments\n-        epoch (int): The number of epochs\n-\n-    # Returns\n-        lr (float32): learning rate\n-    \"\"\"\n-    lr = 1e-3\n-    if epoch > 180:\n-        lr *= 0.5e-3\n-    elif epoch > 160:\n-        lr *= 1e-3\n-    elif epoch > 120:\n-        lr *= 1e-2\n-    elif epoch > 80:\n-        lr *= 1e-1\n-    print('Learning rate: ', lr)\n-    return lr\n-\n-\n-def resnet_layer(inputs,\n-                 num_filters=16,\n-                 kernel_size=3,\n-                 strides=1,\n-                 activation='relu',\n-                 batch_normalization=True,\n-                 conv_first=True):\n-    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n-\n-    # Arguments\n-        inputs (tensor): input tensor from input image or previous layer\n-        num_filters (int): Conv2D number of filters\n-        kernel_size (int): Conv2D square kernel dimensions\n-        strides (int): Conv2D square stride dimensions\n-        activation (string): activation name\n-        batch_normalization (bool): whether to include batch normalization\n-        conv_first (bool): conv-bn-activation (True) or\n-            bn-activation-conv (False)\n-\n-    # Returns\n-        x (tensor): tensor as input to the next layer\n-    \"\"\"\n-    conv = Conv2D(num_filters,\n-                  kernel_size=kernel_size,\n-                  strides=strides,\n-                  padding='same',\n-                  kernel_initializer='he_normal',\n-                  kernel_regularizer=l2(1e-4))\n-\n-    x = inputs\n-    if conv_first:\n-        x = conv(x)\n-        if batch_normalization:\n-            x = BatchNormalization()(x)\n-        if activation is not None:\n-            x = Activation(activation)(x)\n-    else:\n-        if batch_normalization:\n-            x = BatchNormalization()(x)\n-        if activation is not None:\n-            x = Activation(activation)(x)\n-        x = conv(x)\n-    return x\n-\n-\n-def resnet_v1(input_shape, depth, num_classes=10):\n-    \"\"\"ResNet Version 1 Model builder [a]\n-\n-    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n-    Last ReLU is after the shortcut connection.\n-    At the beginning of each stage, the feature map size is halved (downsampled)\n-    by a convolutional layer with strides=2, while the number of filters is\n-    doubled. Within each stage, the layers have the same number filters and the\n-    same number of filters.\n-    Features maps sizes:\n-    stage 0: 32x32, 16\n-    stage 1: 16x16, 32\n-    stage 2:  8x8,  64\n-    The Number of parameters is approx the same as Table 6 of [a]:\n-    ResNet20 0.27M\n-    ResNet32 0.46M\n-    ResNet44 0.66M\n-    ResNet56 0.85M\n-    ResNet110 1.7M\n-\n-    # Arguments\n-        input_shape (tensor): shape of input image tensor\n-        depth (int): number of core convolutional layers\n-        num_classes (int): number of classes (CIFAR10 has 10)\n-\n-    # Returns\n-        model (Model): Keras model instance\n-    \"\"\"\n-    if (depth - 2) % 6 != 0:\n-        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n-    # Start model definition.\n-    num_filters = 16\n-    num_res_blocks = int((depth - 2) / 6)\n-\n-    inputs = Input(shape=input_shape)\n-    x = resnet_layer(inputs=inputs)\n-    # Instantiate the stack of residual units\n-    for stack in range(3):\n-        for res_block in range(num_res_blocks):\n-            strides = 1\n-            if stack > 0 and res_block == 0:  # first layer but not first stack\n-                strides = 2  # downsample\n-            y = resnet_layer(inputs=x,\n-                             num_filters=num_filters,\n-                             strides=strides)\n-            y = resnet_layer(inputs=y,\n-                             num_filters=num_filters,\n-                             activation=None)\n-            if stack > 0 and res_block == 0:  # first layer but not first stack\n-                # linear projection residual shortcut connection to match\n-                # changed dims\n-                x = resnet_layer(inputs=x,\n-                                 num_filters=num_filters,\n-                                 kernel_size=1,\n-                                 strides=strides,\n-                                 activation=None,\n-                                 batch_normalization=False)\n-            x = keras.layers.add([x, y])\n-            x = Activation('relu')(x)\n-        num_filters *= 2\n-\n-    # Add classifier on top.\n-    # v1 does not use BN after last shortcut connection-ReLU\n-    x = AveragePooling2D(pool_size=8)(x)\n-    y = Flatten()(x)\n-    outputs = Dense(num_classes,\n-                    activation='softmax',\n-                    kernel_initializer='he_normal')(y)\n-\n-    # Instantiate model.\n-    model = Model(inputs=inputs, outputs=outputs)\n-    return model\n-\n-\n-def resnet_v2(input_shape, depth, num_classes=10):\n-    \"\"\"ResNet Version 2 Model builder [b]\n-\n-    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n-    bottleneck layer\n-    First shortcut connection per layer is 1 x 1 Conv2D.\n-    Second and onwards shortcut connection is identity.\n-    At the beginning of each stage, the feature map size is halved (downsampled)\n-    by a convolutional layer with strides=2, while the number of filter maps is\n-    doubled. Within each stage, the layers have the same number filters and the\n-    same filter map sizes.\n-    Features maps sizes:\n-    conv1  : 32x32,  16\n-    stage 0: 32x32,  64\n-    stage 1: 16x16, 128\n-    stage 2:  8x8,  256\n-\n-    # Arguments\n-        input_shape (tensor): shape of input image tensor\n-        depth (int): number of core convolutional layers\n-        num_classes (int): number of classes (CIFAR10 has 10)\n-\n-    # Returns\n-        model (Model): Keras model instance\n-    \"\"\"\n-    if (depth - 2) % 9 != 0:\n-        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n-    # Start model definition.\n-    num_filters_in = 16\n-    num_res_blocks = int((depth - 2) / 9)\n-\n-    inputs = Input(shape=input_shape)\n-    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n-    x = resnet_layer(inputs=inputs,\n-                     num_filters=num_filters_in,\n-                     conv_first=True)\n-\n-    # Instantiate the stack of residual units\n-    for stage in range(3):\n-        for res_block in range(num_res_blocks):\n-            activation = 'relu'\n-            batch_normalization = True\n-            strides = 1\n-            if stage == 0:\n-                num_filters_out = num_filters_in * 4\n-                if res_block == 0:  # first layer and first stage\n-                    activation = None\n-                    batch_normalization = False\n-            else:\n-                num_filters_out = num_filters_in * 2\n-                if res_block == 0:  # first layer but not first stage\n-                    strides = 2    # downsample\n-\n-            # bottleneck residual unit\n-            y = resnet_layer(inputs=x,\n-                             num_filters=num_filters_in,\n-                             kernel_size=1,\n-                             strides=strides,\n-                             activation=activation,\n-                             batch_normalization=batch_normalization,\n-                             conv_first=False)\n-            y = resnet_layer(inputs=y,\n-                             num_filters=num_filters_in,\n-                             conv_first=False)\n-            y = resnet_layer(inputs=y,\n-                             num_filters=num_filters_out,\n-                             kernel_size=1,\n-                             conv_first=False)\n-            if res_block == 0:\n-                # linear projection residual shortcut connection to match\n-                # changed dims\n-                x = resnet_layer(inputs=x,\n-                                 num_filters=num_filters_out,\n-                                 kernel_size=1,\n-                                 strides=strides,\n-                                 activation=None,\n-                                 batch_normalization=False)\n-            x = keras.layers.add([x, y])\n-\n-        num_filters_in = num_filters_out\n-\n-    # Add classifier on top.\n-    # v2 has BN-ReLU before Pooling\n-    x = BatchNormalization()(x)\n-    x = Activation('relu')(x)\n-    x = AveragePooling2D(pool_size=8)(x)\n-    y = Flatten()(x)\n-    outputs = Dense(num_classes,\n-                    activation='softmax',\n-                    kernel_initializer='he_normal')(y)\n-\n-    # Instantiate model.\n-    model = Model(inputs=inputs, outputs=outputs)\n-    return model\n-\n-\n-if version == 2:\n-    model = resnet_v2(input_shape=input_shape, depth=depth)\n-else:\n-    model = resnet_v1(input_shape=input_shape, depth=depth)\n-\n-model.compile(loss='categorical_crossentropy',\n-              optimizer=Adam(learning_rate=lr_schedule(0)),\n-              metrics=['accuracy'])\n-model.summary()\n-print(model_type)\n-\n-# Prepare model model saving directory.\n-save_dir = os.path.join(os.getcwd(), 'saved_models')\n-model_name = 'cifar10_%s_model.{epoch:03d}.h5' % model_type\n-if not os.path.isdir(save_dir):\n-    os.makedirs(save_dir)\n-filepath = os.path.join(save_dir, model_name)\n-\n-# Prepare callbacks for model saving and for learning rate adjustment.\n-checkpoint = ModelCheckpoint(filepath=filepath,\n-                             monitor='val_acc',\n-                             verbose=1,\n-                             save_best_only=True)\n-\n-lr_scheduler = LearningRateScheduler(lr_schedule)\n-\n-lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n-                               cooldown=0,\n-                               patience=5,\n-                               min_lr=0.5e-6)\n-\n-callbacks = [checkpoint, lr_reducer, lr_scheduler]\n-\n-# Run training, with or without data augmentation.\n-if not data_augmentation:\n-    print('Not using data augmentation.')\n-    model.fit(x_train, y_train,\n-              batch_size=batch_size,\n-              epochs=epochs,\n-              validation_data=(x_test, y_test),\n-              shuffle=True,\n-              callbacks=callbacks)\n-else:\n-    print('Using real-time data augmentation.')\n-    # This will do preprocessing and realtime data augmentation:\n-    datagen = ImageDataGenerator(\n-        # set input mean to 0 over the dataset\n-        featurewise_center=False,\n-        # set each sample mean to 0\n-        samplewise_center=False,\n-        # divide inputs by std of dataset\n-        featurewise_std_normalization=False,\n-        # divide each input by its std\n-        samplewise_std_normalization=False,\n-        # apply ZCA whitening\n-        zca_whitening=False,\n-        # epsilon for ZCA whitening\n-        zca_epsilon=1e-06,\n-        # randomly rotate images in the range (deg 0 to 180)\n-        rotation_range=0,\n-        # randomly shift images horizontally\n-        width_shift_range=0.1,\n-        # randomly shift images vertically\n-        height_shift_range=0.1,\n-        # set range for random shear\n-        shear_range=0.,\n-        # set range for random zoom\n-        zoom_range=0.,\n-        # set range for random channel shifts\n-        channel_shift_range=0.,\n-        # set mode for filling points outside the input boundaries\n-        fill_mode='nearest',\n-        # value used for fill_mode = \"constant\"\n-        cval=0.,\n-        # randomly flip images\n-        horizontal_flip=True,\n-        # randomly flip images\n-        vertical_flip=False,\n-        # set rescaling factor (applied before any other transformation)\n-        rescale=None,\n-        # set function that will be applied on each input\n-        preprocessing_function=None,\n-        # image data format, either \"channels_first\" or \"channels_last\"\n-        data_format=None,\n-        # fraction of images reserved for validation (strictly between 0 and 1)\n-        validation_split=0.0)\n-\n-    # Compute quantities required for featurewise normalization\n-    # (std, mean, and principal components if ZCA whitening is applied).\n-    datagen.fit(x_train)\n-\n-    # Fit the model on the batches generated by datagen.flow().\n-    model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n-                        validation_data=(x_test, y_test),\n-                        epochs=epochs, verbose=1, workers=4,\n-                        callbacks=callbacks)\n-\n-# Score trained model.\n-scores = model.evaluate(x_test, y_test, verbose=1)\n-print('Test loss:', scores[0])\n-print('Test accuracy:', scores[1])\n\n@@ -1,93 +0,0 @@\n-# -*- coding: utf-8 -*-\r\n-\r\n-import numpy as np\r\n-import argparse\r\n-import cv2\r\n-import matplotlib.pyplot as plt\r\n-\r\n-from keras.models import Model\r\n-\r\n-import keras.applications.resnet50 as resnet\r\n-from keras.layers import UpSampling2D, Conv2D\r\n-\r\n-# Set an appropriate image file\r\n-parser = argparse.ArgumentParser(description='Class activation maps with Keras.')\r\n-parser.add_argument('input_image', metavar='base', type=str,\r\n-                    help='Path to the image to use.')\r\n-args = parser.parse_args()\r\n-input_image = args.input_image\r\n-\r\n-\r\n-################################################################\r\n-# The following parameters can be changed to other models\r\n-# that use global average pooling.\r\n-# e.g.) InceptionResnetV2 / NASNetLarge\r\n-NETWORK_INPUT_SIZE = 224\r\n-MODEL_CLASS = resnet.ResNet50\r\n-PREPROCESS_FN = resnet.preprocess_input\r\n-LAST_CONV_LAYER = 'activation_49'\r\n-PRED_LAYER = 'fc1000'\r\n-################################################################\r\n-\r\n-# number of imagenet classes\r\n-N_CLASSES = 1000\r\n-\r\n-\r\n-def load_img(fname, input_size, preprocess_fn):\r\n-    original_img = cv2.imread(fname)[:, :, ::-1]\r\n-    original_size = (original_img.shape[1], original_img.shape[0])\r\n-    img = cv2.resize(original_img, (input_size, input_size))\r\n-    imgs = np.expand_dims(preprocess_fn(img), axis=0)\r\n-    return imgs, original_img, original_size\r\n-\r\n-\r\n-def get_cam_model(model_class,\r\n-                  input_size=224,\r\n-                  last_conv_layer='activation_49',\r\n-                  pred_layer='fc1000'):\r\n-    model = model_class(input_shape=(input_size, input_size, 3))\r\n-\r\n-    final_params = model.get_layer(pred_layer).get_weights()\r\n-    final_params = (final_params[0].reshape(\r\n-        1, 1, -1, N_CLASSES), final_params[1])\r\n-\r\n-    last_conv_output = model.get_layer(last_conv_layer).output\r\n-    x = UpSampling2D(size=(32, 32), interpolation='bilinear')(\r\n-        last_conv_output)\r\n-    x = Conv2D(filters=N_CLASSES, kernel_size=(\r\n-        1, 1), name='predictions_2')(x)\r\n-\r\n-    cam_model = Model(inputs=model.input,\r\n-                      outputs=[model.output, x])\r\n-    cam_model.get_layer('predictions_2').set_weights(final_params)\r\n-    return cam_model\r\n-\r\n-\r\n-def postprocess(preds, cams, top_k=1):\r\n-    idxes = np.argsort(preds[0])[-top_k:]\r\n-    class_activation_map = np.zeros_like(cams[0, :, :, 0])\r\n-    for i in idxes:\r\n-        class_activation_map += cams[0, :, :, i]\r\n-    return class_activation_map\r\n-\r\n-\r\n-# 1. load image\r\n-imgs, original_img, original_size = load_img(input_image,\r\n-                                             input_size=NETWORK_INPUT_SIZE,\r\n-                                             preprocess_fn=resnet.preprocess_input)\r\n-\r\n-# 2. prediction\r\n-model = get_cam_model(resnet.ResNet50,\r\n-                      NETWORK_INPUT_SIZE,\r\n-                      LAST_CONV_LAYER,\r\n-                      PRED_LAYER)\r\n-preds, cams = model.predict(imgs)\r\n-\r\n-# 4. post processing\r\n-class_activation_map = postprocess(preds, cams)\r\n-\r\n-# 5. plot image+cam to original size\r\n-plt.imshow(original_img, alpha=0.5)\r\n-plt.imshow(cv2.resize(class_activation_map,\r\n-                      original_size), cmap='jet', alpha=0.5)\r\n-plt.show()\r\n\n@@ -1,207 +0,0 @@\n-'''# Sequence-to-sequence example in Keras (character-level).\n-\n-This script demonstrates how to implement a basic character-level CNN\n-sequence-to-sequence model. We apply it to translating\n-short English sentences into short French sentences,\n-character-by-character. Note that it is fairly unusual to\n-do character-level machine translation, as word-level\n-models are much more common in this domain. This example\n-is for demonstration purposes only.\n-\n-**Summary of the algorithm**\n-\n-- We start with input sequences from a domain (e.g. English sentences)\n-    and corresponding target sequences from another domain\n-    (e.g. French sentences).\n-- An encoder CNN encodes the input character sequence.\n-- A decoder CNN is trained to turn the target sequences into\n-    the same sequence but offset by one timestep in the future,\n-    a training process called \"teacher forcing\" in this context.\n-    It uses the output from the encoder.\n-    Effectively, the decoder learns to generate `targets[t+1...]`\n-    given `targets[...t]`, conditioned on the input sequence.\n-- In inference mode, when we want to decode unknown input sequences, we:\n-    - Encode the input sequence.\n-    - Start with a target sequence of size 1\n-        (just the start-of-sequence character)\n-    - Feed the input sequence and 1-char target sequence\n-        to the decoder to produce predictions for the next character\n-    - Sample the next character using these predictions\n-        (we simply use argmax).\n-    - Append the sampled character to the target sequence\n-    - Repeat until we hit the character limit.\n-\n-**Data download**\n-\n-[English to French sentence pairs.\n-](http://www.manythings.org/anki/fra-eng.zip)\n-\n-[Lots of neat sentence pairs datasets.\n-](http://www.manythings.org/anki/)\n-\n-**References**\n-\n-- lstm_seq2seq.py\n-- https://wanasit.github.io/attention-based-sequence-to-sequence-in-keras.html\n-'''\n-from __future__ import print_function\n-\n-import numpy as np\n-\n-from keras.layers import Input, Convolution1D, Dot, Dense, Activation, Concatenate\n-from keras.models import Model\n-\n-batch_size = 64  # Batch size for training.\n-epochs = 100  # Number of epochs to train for.\n-num_samples = 10000  # Number of samples to train on.\n-# Path to the data txt file on disk.\n-data_path = 'fra-eng/fra.txt'\n-\n-# Vectorize the data.\n-input_texts = []\n-target_texts = []\n-input_characters = set()\n-target_characters = set()\n-with open(data_path, 'r', encoding='utf-8') as f:\n-    lines = f.read().split('\\n')\n-for line in lines[: min(num_samples, len(lines) - 1)]:\n-    input_text, target_text = line.split('\\t')\n-    # We use \"tab\" as the \"start sequence\" character\n-    # for the targets, and \"\\n\" as \"end sequence\" character.\n-    target_text = '\\t' + target_text + '\\n'\n-    input_texts.append(input_text)\n-    target_texts.append(target_text)\n-    for char in input_text:\n-        if char not in input_characters:\n-            input_characters.add(char)\n-    for char in target_text:\n-        if char not in target_characters:\n-            target_characters.add(char)\n-\n-input_characters = sorted(list(input_characters))\n-target_characters = sorted(list(target_characters))\n-num_encoder_tokens = len(input_characters)\n-num_decoder_tokens = len(target_characters)\n-max_encoder_seq_length = max([len(txt) for txt in input_texts])\n-max_decoder_seq_length = max([len(txt) for txt in target_texts])\n-\n-print('Number of samples:', len(input_texts))\n-print('Number of unique input tokens:', num_encoder_tokens)\n-print('Number of unique output tokens:', num_decoder_tokens)\n-print('Max sequence length for inputs:', max_encoder_seq_length)\n-print('Max sequence length for outputs:', max_decoder_seq_length)\n-\n-input_token_index = dict(\n-    [(char, i) for i, char in enumerate(input_characters)])\n-target_token_index = dict(\n-    [(char, i) for i, char in enumerate(target_characters)])\n-\n-encoder_input_data = np.zeros(\n-    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n-    dtype='float32')\n-decoder_input_data = np.zeros(\n-    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n-    dtype='float32')\n-decoder_target_data = np.zeros(\n-    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n-    dtype='float32')\n-\n-for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n-    for t, char in enumerate(input_text):\n-        encoder_input_data[i, t, input_token_index[char]] = 1.\n-    for t, char in enumerate(target_text):\n-        # decoder_target_data is ahead of decoder_input_data by one timestep\n-        decoder_input_data[i, t, target_token_index[char]] = 1.\n-        if t > 0:\n-            # decoder_target_data will be ahead by one timestep\n-            # and will not include the start character.\n-            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n-\n-# Define an input sequence and process it.\n-encoder_inputs = Input(shape=(None, num_encoder_tokens))\n-# Encoder\n-x_encoder = Convolution1D(256, kernel_size=3, activation='relu',\n-                          padding='causal')(encoder_inputs)\n-x_encoder = Convolution1D(256, kernel_size=3, activation='relu',\n-                          padding='causal', dilation_rate=2)(x_encoder)\n-x_encoder = Convolution1D(256, kernel_size=3, activation='relu',\n-                          padding='causal', dilation_rate=4)(x_encoder)\n-\n-decoder_inputs = Input(shape=(None, num_decoder_tokens))\n-# Decoder\n-x_decoder = Convolution1D(256, kernel_size=3, activation='relu',\n-                          padding='causal')(decoder_inputs)\n-x_decoder = Convolution1D(256, kernel_size=3, activation='relu',\n-                          padding='causal', dilation_rate=2)(x_decoder)\n-x_decoder = Convolution1D(256, kernel_size=3, activation='relu',\n-                          padding='causal', dilation_rate=4)(x_decoder)\n-# Attention\n-attention = Dot(axes=[2, 2])([x_decoder, x_encoder])\n-attention = Activation('softmax')(attention)\n-\n-context = Dot(axes=[2, 1])([attention, x_encoder])\n-decoder_combined_context = Concatenate(axis=-1)([context, x_decoder])\n-\n-decoder_outputs = Convolution1D(64, kernel_size=3, activation='relu',\n-                                padding='causal')(decoder_combined_context)\n-decoder_outputs = Convolution1D(64, kernel_size=3, activation='relu',\n-                                padding='causal')(decoder_outputs)\n-# Output\n-decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n-decoder_outputs = decoder_dense(decoder_outputs)\n-\n-# Define the model that will turn\n-# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n-model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n-model.summary()\n-\n-# Run training\n-model.compile(optimizer='adam', loss='categorical_crossentropy')\n-model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n-          batch_size=batch_size,\n-          epochs=epochs,\n-          validation_split=0.2)\n-# Save model\n-model.save('cnn_s2s.h5')\n-\n-# Next: inference mode (sampling).\n-\n-# Define sampling models\n-reverse_input_char_index = dict(\n-    (i, char) for char, i in input_token_index.items())\n-reverse_target_char_index = dict(\n-    (i, char) for char, i in target_token_index.items())\n-\n-nb_examples = 100\n-in_encoder = encoder_input_data[:nb_examples]\n-in_decoder = np.zeros(\n-    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n-    dtype='float32')\n-\n-in_decoder[:, 0, target_token_index[\"\\t\"]] = 1\n-\n-predict = np.zeros(\n-    (len(input_texts), max_decoder_seq_length),\n-    dtype='float32')\n-\n-for i in range(max_decoder_seq_length - 1):\n-    predict = model.predict([in_encoder, in_decoder])\n-    predict = predict.argmax(axis=-1)\n-    predict_ = predict[:, i].ravel().tolist()\n-    for j, x in enumerate(predict_):\n-        in_decoder[j, i + 1, x] = 1\n-\n-for seq_index in range(nb_examples):\n-    # Take one sequence (part of the training set)\n-    # for trying out decoding.\n-    output_seq = predict[seq_index, :].ravel().tolist()\n-    decoded = []\n-    for x in output_seq:\n-        if reverse_target_char_index[x] == \"\\n\":\n-            break\n-        else:\n-            decoded.append(reverse_target_char_index[x])\n-    decoded_sentence = \"\".join(decoded)\n-    print('-')\n-    print('Input sentence:', input_texts[seq_index])\n-    print('Decoded sentence:', decoded_sentence)\n\n@@ -1,259 +0,0 @@\n-\"\"\"\n-#Visualization of the filters of VGG16, via gradient ascent in input space.\n-\n-This script can run on CPU in a few minutes.\n-\n-Results example: ![Visualization](http://i.imgur.com/4nj4KjN.jpg)\n-\"\"\"\n-from __future__ import print_function\n-\n-import time\n-import numpy as np\n-from PIL import Image as pil_image\n-from keras.preprocessing.image import save_img\n-from keras import layers\n-from keras.applications import vgg16\n-from keras import backend as K\n-\n-\n-def normalize(x):\n-    \"\"\"utility function to normalize a tensor.\n-\n-    # Arguments\n-        x: An input tensor.\n-\n-    # Returns\n-        The normalized input tensor.\n-    \"\"\"\n-    return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())\n-\n-\n-def deprocess_image(x):\n-    \"\"\"utility function to convert a float array into a valid uint8 image.\n-\n-    # Arguments\n-        x: A numpy-array representing the generated image.\n-\n-    # Returns\n-        A processed numpy-array, which could be used in e.g. imshow.\n-    \"\"\"\n-    # normalize tensor: center on 0., ensure std is 0.25\n-    x -= x.mean()\n-    x /= (x.std() + K.epsilon())\n-    x *= 0.25\n-\n-    # clip to [0, 1]\n-    x += 0.5\n-    x = np.clip(x, 0, 1)\n-\n-    # convert to RGB array\n-    x *= 255\n-    if K.image_data_format() == 'channels_first':\n-        x = x.transpose((1, 2, 0))\n-    x = np.clip(x, 0, 255).astype('uint8')\n-    return x\n-\n-\n-def process_image(x, former):\n-    \"\"\"utility function to convert a valid uint8 image back into a float array.\n-       Reverses `deprocess_image`.\n-\n-    # Arguments\n-        x: A numpy-array, which could be used in e.g. imshow.\n-        former: The former numpy-array.\n-                Need to determine the former mean and variance.\n-\n-    # Returns\n-        A processed numpy-array representing the generated image.\n-    \"\"\"\n-    if K.image_data_format() == 'channels_first':\n-        x = x.transpose((2, 0, 1))\n-    return (x / 255 - 0.5) * 4 * former.std() + former.mean()\n-\n-\n-def visualize_layer(model,\n-                    layer_name,\n-                    step=1.,\n-                    epochs=15,\n-                    upscaling_steps=9,\n-                    upscaling_factor=1.2,\n-                    output_dim=(412, 412),\n-                    filter_range=(0, None)):\n-    \"\"\"Visualizes the most relevant filters of one conv-layer in a certain model.\n-\n-    # Arguments\n-        model: The model containing layer_name.\n-        layer_name: The name of the layer to be visualized.\n-                    Has to be a part of model.\n-        step: step size for gradient ascent.\n-        epochs: Number of iterations for gradient ascent.\n-        upscaling_steps: Number of upscaling steps.\n-                         Starting image is in this case (80, 80).\n-        upscaling_factor: Factor to which to slowly upgrade\n-                          the image towards output_dim.\n-        output_dim: [img_width, img_height] The output image dimensions.\n-        filter_range: Tupel[lower, upper]\n-                      Determines the to be computed filter numbers.\n-                      If the second value is `None`,\n-                      the last filter will be inferred as the upper boundary.\n-    \"\"\"\n-\n-    def _generate_filter_image(input_img,\n-                               layer_output,\n-                               filter_index):\n-        \"\"\"Generates image for one particular filter.\n-\n-        # Arguments\n-            input_img: The input-image Tensor.\n-            layer_output: The output-image Tensor.\n-            filter_index: The to be processed filter number.\n-                          Assumed to be valid.\n-\n-        #Returns\n-            Either None if no image could be generated.\n-            or a tuple of the image (array) itself and the last loss.\n-        \"\"\"\n-        s_time = time.time()\n-\n-        # we build a loss function that maximizes the activation\n-        # of the nth filter of the layer considered\n-        if K.image_data_format() == 'channels_first':\n-            loss = K.mean(layer_output[:, filter_index, :, :])\n-        else:\n-            loss = K.mean(layer_output[:, :, :, filter_index])\n-\n-        # we compute the gradient of the input picture wrt this loss\n-        grads = K.gradients(loss, input_img)[0]\n-\n-        # normalization trick: we normalize the gradient\n-        grads = normalize(grads)\n-\n-        # this function returns the loss and grads given the input picture\n-        iterate = K.function([input_img], [loss, grads])\n-\n-        # we start from a gray image with some random noise\n-        intermediate_dim = tuple(\n-            int(x / (upscaling_factor ** upscaling_steps)) for x in output_dim)\n-        if K.image_data_format() == 'channels_first':\n-            input_img_data = np.random.random(\n-                (1, 3, intermediate_dim[0], intermediate_dim[1]))\n-        else:\n-            input_img_data = np.random.random(\n-                (1, intermediate_dim[0], intermediate_dim[1], 3))\n-        input_img_data = (input_img_data - 0.5) * 20 + 128\n-\n-        # Slowly upscaling towards the original size prevents\n-        # a dominating high-frequency of the to visualized structure\n-        # as it would occur if we directly compute the 412d-image.\n-        # Behaves as a better starting point for each following dimension\n-        # and therefore avoids poor local minima\n-        for up in reversed(range(upscaling_steps)):\n-            # we run gradient ascent for e.g. 20 steps\n-            for _ in range(epochs):\n-                loss_value, grads_value = iterate([input_img_data])\n-                input_img_data += grads_value * step\n-\n-                # some filters get stuck to 0, we can skip them\n-                if loss_value <= K.epsilon():\n-                    return None\n-\n-            # Calculate upscaled dimension\n-            intermediate_dim = tuple(\n-                int(x / (upscaling_factor ** up)) for x in output_dim)\n-            # Upscale\n-            img = deprocess_image(input_img_data[0])\n-            img = np.array(pil_image.fromarray(img).resize(intermediate_dim,\n-                                                           pil_image.BICUBIC))\n-            input_img_data = np.expand_dims(\n-                process_image(img, input_img_data[0]), 0)\n-\n-        # decode the resulting input image\n-        img = deprocess_image(input_img_data[0])\n-        e_time = time.time()\n-        print('Costs of filter {:3}: {:5.0f} ( {:4.2f}s )'.format(filter_index,\n-                                                                  loss_value,\n-                                                                  e_time - s_time))\n-        return img, loss_value\n-\n-    def _draw_filters(filters, n=None):\n-        \"\"\"Draw the best filters in a nxn grid.\n-\n-        # Arguments\n-            filters: A List of generated images and their corresponding losses\n-                     for each processed filter.\n-            n: dimension of the grid.\n-               If none, the largest possible square will be used\n-        \"\"\"\n-        if n is None:\n-            n = int(np.floor(np.sqrt(len(filters))))\n-\n-        # the filters that have the highest loss are assumed to be better-looking.\n-        # we will only keep the top n*n filters.\n-        filters.sort(key=lambda x: x[1], reverse=True)\n-        filters = filters[:n * n]\n-\n-        # build a black picture with enough space for\n-        # e.g. our 8 x 8 filters of size 412 x 412, with a 5px margin in between\n-        MARGIN = 5\n-        width = n * output_dim[0] + (n - 1) * MARGIN\n-        height = n * output_dim[1] + (n - 1) * MARGIN\n-        stitched_filters = np.zeros((width, height, 3), dtype='uint8')\n-\n-        # fill the picture with our saved filters\n-        for i in range(n):\n-            for j in range(n):\n-                img, _ = filters[i * n + j]\n-                width_margin = (output_dim[0] + MARGIN) * i\n-                height_margin = (output_dim[1] + MARGIN) * j\n-                stitched_filters[\n-                    width_margin: width_margin + output_dim[0],\n-                    height_margin: height_margin + output_dim[1], :] = img\n-\n-        # save the result to disk\n-        save_img('vgg_{0:}_{1:}x{1:}.png'.format(layer_name, n), stitched_filters)\n-\n-    # this is the placeholder for the input images\n-    assert len(model.inputs) == 1\n-    input_img = model.inputs[0]\n-\n-    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n-    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n-\n-    output_layer = layer_dict[layer_name]\n-    assert isinstance(output_layer, layers.Conv2D)\n-\n-    # Compute to be processed filter range\n-    filter_lower = filter_range[0]\n-    filter_upper = (filter_range[1]\n-                    if filter_range[1] is not None\n-                    else len(output_layer.get_weights()[1]))\n-    assert(filter_lower >= 0\n-           and filter_upper <= len(output_layer.get_weights()[1])\n-           and filter_upper > filter_lower)\n-    print('Compute filters {:} to {:}'.format(filter_lower, filter_upper))\n-\n-    # iterate through each filter and generate its corresponding image\n-    processed_filters = []\n-    for f in range(filter_lower, filter_upper):\n-        img_loss = _generate_filter_image(input_img, output_layer.output, f)\n-\n-        if img_loss is not None:\n-            processed_filters.append(img_loss)\n-\n-    print('{} filter processed.'.format(len(processed_filters)))\n-    # Finally draw and store the best filters to disk\n-    _draw_filters(processed_filters)\n-\n-\n-if __name__ == '__main__':\n-    # the name of the layer we want to visualize\n-    # (see model definition at keras/applications/vgg16.py)\n-    LAYER_NAME = 'block5_conv1'\n-\n-    # build the VGG16 network with ImageNet weights\n-    vgg = vgg16.VGG16(weights='imagenet', include_top=False)\n-    print('Model loaded.')\n-    vgg.summary()\n-\n-    # example function call\n-    visualize_layer(vgg, LAYER_NAME)\n\n@@ -1,143 +0,0 @@\n-\"\"\"\n-#This script demonstrates the use of a convolutional LSTM network.\n-\n-This network is used to predict the next frame of an artificially\n-generated movie which contains moving squares.\n-\"\"\"\n-from keras.models import Sequential\n-from keras.layers.convolutional import Conv3D\n-from keras.layers.convolutional_recurrent import ConvLSTM2D\n-from keras.layers.normalization import BatchNormalization\n-import numpy as np\n-import pylab as plt\n-\n-# We create a layer which take as input movies of shape\n-# (n_frames, width, height, channels) and returns a movie\n-# of identical shape.\n-\n-seq = Sequential()\n-seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n-                   input_shape=(None, 40, 40, 1),\n-                   padding='same', return_sequences=True))\n-seq.add(BatchNormalization())\n-\n-seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n-                   padding='same', return_sequences=True))\n-seq.add(BatchNormalization())\n-\n-seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n-                   padding='same', return_sequences=True))\n-seq.add(BatchNormalization())\n-\n-seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n-                   padding='same', return_sequences=True))\n-seq.add(BatchNormalization())\n-\n-seq.add(Conv3D(filters=1, kernel_size=(3, 3, 3),\n-               activation='sigmoid',\n-               padding='same', data_format='channels_last'))\n-seq.compile(loss='binary_crossentropy', optimizer='adadelta')\n-\n-\n-# Artificial data generation:\n-# Generate movies with 3 to 7 moving squares inside.\n-# The squares are of shape 1x1 or 2x2 pixels,\n-# which move linearly over time.\n-# For convenience we first create movies with bigger width and height (80x80)\n-# and at the end we select a 40x40 window.\n-\n-def generate_movies(n_samples=1200, n_frames=15):\n-    row = 80\n-    col = 80\n-    noisy_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float)\n-    shifted_movies = np.zeros((n_samples, n_frames, row, col, 1),\n-                              dtype=np.float)\n-\n-    for i in range(n_samples):\n-        # Add 3 to 7 moving squares\n-        n = np.random.randint(3, 8)\n-\n-        for j in range(n):\n-            # Initial position\n-            xstart = np.random.randint(20, 60)\n-            ystart = np.random.randint(20, 60)\n-            # Direction of motion\n-            directionx = np.random.randint(0, 3) - 1\n-            directiony = np.random.randint(0, 3) - 1\n-\n-            # Size of the square\n-            w = np.random.randint(2, 4)\n-\n-            for t in range(n_frames):\n-                x_shift = xstart + directionx * t\n-                y_shift = ystart + directiony * t\n-                noisy_movies[i, t, x_shift - w: x_shift + w,\n-                             y_shift - w: y_shift + w, 0] += 1\n-\n-                # Make it more robust by adding noise.\n-                # The idea is that if during inference,\n-                # the value of the pixel is not exactly one,\n-                # we need to train the network to be robust and still\n-                # consider it as a pixel belonging to a square.\n-                if np.random.randint(0, 2):\n-                    noise_f = (-1)**np.random.randint(0, 2)\n-                    noisy_movies[i, t,\n-                                 x_shift - w - 1: x_shift + w + 1,\n-                                 y_shift - w - 1: y_shift + w + 1,\n-                                 0] += noise_f * 0.1\n-\n-                # Shift the ground truth by 1\n-                x_shift = xstart + directionx * (t + 1)\n-                y_shift = ystart + directiony * (t + 1)\n-                shifted_movies[i, t, x_shift - w: x_shift + w,\n-                               y_shift - w: y_shift + w, 0] += 1\n-\n-    # Cut to a 40x40 window\n-    noisy_movies = noisy_movies[::, ::, 20:60, 20:60, ::]\n-    shifted_movies = shifted_movies[::, ::, 20:60, 20:60, ::]\n-    noisy_movies[noisy_movies >= 1] = 1\n-    shifted_movies[shifted_movies >= 1] = 1\n-    return noisy_movies, shifted_movies\n-\n-# Train the network\n-noisy_movies, shifted_movies = generate_movies(n_samples=1200)\n-seq.fit(noisy_movies[:1000], shifted_movies[:1000], batch_size=10,\n-        epochs=300, validation_split=0.05)\n-\n-# Testing the network on one movie\n-# feed it with the first 7 positions and then\n-# predict the new positions\n-which = 1004\n-track = noisy_movies[which][:7, ::, ::, ::]\n-\n-for j in range(16):\n-    new_pos = seq.predict(track[np.newaxis, ::, ::, ::, ::])\n-    new = new_pos[::, -1, ::, ::, ::]\n-    track = np.concatenate((track, new), axis=0)\n-\n-\n-# And then compare the predictions\n-# to the ground truth\n-track2 = noisy_movies[which][::, ::, ::, ::]\n-for i in range(15):\n-    fig = plt.figure(figsize=(10, 5))\n-\n-    ax = fig.add_subplot(121)\n-\n-    if i >= 7:\n-        ax.text(1, 3, 'Predictions !', fontsize=20, color='w')\n-    else:\n-        ax.text(1, 3, 'Initial trajectory', fontsize=20)\n-\n-    toplot = track[i, ::, ::, 0]\n-\n-    plt.imshow(toplot)\n-    ax = fig.add_subplot(122)\n-    plt.text(1, 3, 'Ground truth', fontsize=20)\n-\n-    toplot = track2[i, ::, ::, 0]\n-    if i >= 2:\n-        toplot = shifted_movies[which][i - 1, ::, ::, 0]\n-\n-    plt.imshow(toplot)\n-    plt.savefig('%i_animate.png' % (i + 1))\n\n@@ -1,192 +0,0 @@\n-'''\n-#Deep Dreaming in Keras.\n-\n-Run the script with:\n-```python\n-python deep_dream.py path_to_your_base_image.jpg prefix_for_results\n-```\n-e.g.:\n-```python\n-python deep_dream.py img/mypic.jpg results/dream\n-```\n-'''\n-from __future__ import print_function\n-\n-from keras.preprocessing.image import load_img, save_img, img_to_array\n-import numpy as np\n-import scipy\n-import argparse\n-\n-from keras.applications import inception_v3\n-from keras import backend as K\n-\n-parser = argparse.ArgumentParser(description='Deep Dreams with Keras.')\n-parser.add_argument('base_image_path', metavar='base', type=str,\n-                    help='Path to the image to transform.')\n-parser.add_argument('result_prefix', metavar='res_prefix', type=str,\n-                    help='Prefix for the saved results.')\n-\n-args = parser.parse_args()\n-base_image_path = args.base_image_path\n-result_prefix = args.result_prefix\n-\n-# These are the names of the layers\n-# for which we try to maximize activation,\n-# as well as their weight in the final loss\n-# we try to maximize.\n-# You can tweak these setting to obtain new visual effects.\n-settings = {\n-    'features': {\n-        'mixed2': 0.2,\n-        'mixed3': 0.5,\n-        'mixed4': 2.,\n-        'mixed5': 1.5,\n-    },\n-}\n-\n-\n-def preprocess_image(image_path):\n-    # Util function to open, resize and format pictures\n-    # into appropriate tensors.\n-    img = load_img(image_path)\n-    img = img_to_array(img)\n-    img = np.expand_dims(img, axis=0)\n-    img = inception_v3.preprocess_input(img)\n-    return img\n-\n-\n-def deprocess_image(x):\n-    # Util function to convert a tensor into a valid image.\n-    if K.image_data_format() == 'channels_first':\n-        x = x.reshape((3, x.shape[2], x.shape[3]))\n-        x = x.transpose((1, 2, 0))\n-    else:\n-        x = x.reshape((x.shape[1], x.shape[2], 3))\n-    x /= 2.\n-    x += 0.5\n-    x *= 255.\n-    x = np.clip(x, 0, 255).astype('uint8')\n-    return x\n-\n-K.set_learning_phase(0)\n-\n-# Build the InceptionV3 network with our placeholder.\n-# The model will be loaded with pre-trained ImageNet weights.\n-model = inception_v3.InceptionV3(weights='imagenet',\n-                                 include_top=False)\n-dream = model.input\n-print('Model loaded.')\n-\n-# Get the symbolic outputs of each \"key\" layer (we gave them unique names).\n-layer_dict = dict([(layer.name, layer) for layer in model.layers])\n-\n-# Define the loss.\n-loss = K.variable(0.)\n-for layer_name in settings['features']:\n-    # Add the L2 norm of the features of a layer to the loss.\n-    if layer_name not in layer_dict:\n-        raise ValueError('Layer ' + layer_name + ' not found in model.')\n-    coeff = settings['features'][layer_name]\n-    x = layer_dict[layer_name].output\n-    # We avoid border artifacts by only involving non-border pixels in the loss.\n-    scaling = K.prod(K.cast(K.shape(x), 'float32'))\n-    if K.image_data_format() == 'channels_first':\n-        loss = loss + coeff * K.sum(K.square(x[:, :, 2: -2, 2: -2])) / scaling\n-    else:\n-        loss = loss + coeff * K.sum(K.square(x[:, 2: -2, 2: -2, :])) / scaling\n-\n-# Compute the gradients of the dream wrt the loss.\n-grads = K.gradients(loss, dream)[0]\n-# Normalize gradients.\n-grads /= K.maximum(K.mean(K.abs(grads)), K.epsilon())\n-\n-# Set up function to retrieve the value\n-# of the loss and gradients given an input image.\n-outputs = [loss, grads]\n-fetch_loss_and_grads = K.function([dream], outputs)\n-\n-\n-def eval_loss_and_grads(x):\n-    outs = fetch_loss_and_grads([x])\n-    loss_value = outs[0]\n-    grad_values = outs[1]\n-    return loss_value, grad_values\n-\n-\n-def resize_img(img, size):\n-    img = np.copy(img)\n-    if K.image_data_format() == 'channels_first':\n-        factors = (1, 1,\n-                   float(size[0]) / img.shape[2],\n-                   float(size[1]) / img.shape[3])\n-    else:\n-        factors = (1,\n-                   float(size[0]) / img.shape[1],\n-                   float(size[1]) / img.shape[2],\n-                   1)\n-    return scipy.ndimage.zoom(img, factors, order=1)\n-\n-\n-def gradient_ascent(x, iterations, step, max_loss=None):\n-    for i in range(iterations):\n-        loss_value, grad_values = eval_loss_and_grads(x)\n-        if max_loss is not None and loss_value > max_loss:\n-            break\n-        print('..Loss value at', i, ':', loss_value)\n-        x += step * grad_values\n-    return x\n-\n-\n-\"\"\"Process:\n-\n-- Load the original image.\n-- Define a number of processing scales (i.e. image shapes),\n-    from smallest to largest.\n-- Resize the original image to the smallest scale.\n-- For every scale, starting with the smallest (i.e. current one):\n-    - Run gradient ascent\n-    - Upscale image to the next scale\n-    - Reinject the detail that was lost at upscaling time\n-- Stop when we are back to the original size.\n-\n-To obtain the detail lost during upscaling, we simply\n-take the original image, shrink it down, upscale it,\n-and compare the result to the (resized) original image.\n-\"\"\"\n-\n-\n-# Playing with these hyperparameters will also allow you to achieve new effects\n-step = 0.01  # Gradient ascent step size\n-num_octave = 3  # Number of scales at which to run gradient ascent\n-octave_scale = 1.4  # Size ratio between scales\n-iterations = 20  # Number of ascent steps per scale\n-max_loss = 10.\n-\n-img = preprocess_image(base_image_path)\n-if K.image_data_format() == 'channels_first':\n-    original_shape = img.shape[2:]\n-else:\n-    original_shape = img.shape[1:3]\n-successive_shapes = [original_shape]\n-for i in range(1, num_octave):\n-    shape = tuple([int(dim / (octave_scale ** i)) for dim in original_shape])\n-    successive_shapes.append(shape)\n-successive_shapes = successive_shapes[::-1]\n-original_img = np.copy(img)\n-shrunk_original_img = resize_img(img, successive_shapes[0])\n-\n-for shape in successive_shapes:\n-    print('Processing image shape', shape)\n-    img = resize_img(img, shape)\n-    img = gradient_ascent(img,\n-                          iterations=iterations,\n-                          step=step,\n-                          max_loss=max_loss)\n-    upscaled_shrunk_original_img = resize_img(shrunk_original_img, shape)\n-    same_size_original = resize_img(original_img, shape)\n-    lost_detail = same_size_original - upscaled_shrunk_original_img\n-\n-    img += lost_detail\n-    shrunk_original_img = resize_img(original_img, shape)\n-\n-save_img(result_prefix + '.png', deprocess_image(np.copy(img)))\n\n@@ -1,561 +0,0 @@\n-# -*- coding: utf-8 -*-\n-'''\n-# Optical character recognition\n-This example uses a convolutional stack followed by a recurrent stack\n-and a CTC logloss function to perform optical character recognition\n-of generated text images. I have no evidence of whether it actually\n-learns general shapes of text, or just is able to recognize all\n-the different fonts thrown at it...the purpose is more to demonstrate CTC\n-inside of Keras.  Note that the font list may need to be updated\n-for the particular OS in use.\n-\n-This starts off with 4 letter words.  For the first 12 epochs, the\n-difficulty is gradually increased using the TextImageGenerator class\n-which is both a generator class for test/train data and a Keras\n-callback class. After 20 epochs, longer sequences are thrown at it\n-by recompiling the model to handle a wider image and rebuilding\n-the word list to include two words separated by a space.\n-\n-The table below shows normalized edit distance values. Theano uses\n-a slightly different CTC implementation, hence the different results.\n-\n-Epoch |   TF   |   TH\n------:|-------:|-------:\n-    10|  0.027 | 0.064\n-    15|  0.038 | 0.035\n-    20|  0.043 | 0.045\n-    25|  0.014 | 0.019\n-\n-# Additional dependencies\n-\n-This requires ```cairo``` and ```editdistance``` packages:\n-\n-First, install the Cairo library: https://cairographics.org/\n-\n-Then install Python dependencies:\n-\n-```python\n-pip install cairocffi\n-pip install editdistance\n-```\n-\n-Created by Mike Henry\n-https://github.com/mbhenry/\n-'''\n-import os\n-import itertools\n-import codecs\n-import re\n-import datetime\n-import cairocffi as cairo\n-import editdistance\n-import numpy as np\n-from scipy import ndimage\n-import pylab\n-from keras import backend as K\n-from keras.layers.convolutional import Conv2D, MaxPooling2D\n-from keras.layers import Input, Dense, Activation\n-from keras.layers import Reshape, Lambda\n-from keras.layers.merge import add, concatenate\n-from keras.models import Model\n-from keras.layers.recurrent import GRU\n-from keras.optimizers import SGD\n-from keras.utils.data_utils import get_file\n-from keras.preprocessing import image\n-import keras.callbacks\n-\n-\n-OUTPUT_DIR = 'image_ocr'\n-\n-# character classes and matching regex filter\n-regex = r'^[a-z ]+$'\n-alphabet = u'abcdefghijklmnopqrstuvwxyz '\n-\n-np.random.seed(55)\n-\n-\n-# this creates larger \"blotches\" of noise which look\n-# more realistic than just adding gaussian noise\n-# assumes greyscale with pixels ranging from 0 to 1\n-\n-def speckle(img):\n-    severity = np.random.uniform(0, 0.6)\n-    blur = ndimage.gaussian_filter(np.random.randn(*img.shape) * severity, 1)\n-    img_speck = (img + blur)\n-    img_speck[img_speck > 1] = 1\n-    img_speck[img_speck <= 0] = 0\n-    return img_speck\n-\n-\n-# paints the string in a random location the bounding box\n-# also uses a random font, a slight random rotation,\n-# and a random amount of speckle noise\n-\n-def paint_text(text, w, h, rotate=False, ud=False, multi_fonts=False):\n-    surface = cairo.ImageSurface(cairo.FORMAT_RGB24, w, h)\n-    with cairo.Context(surface) as context:\n-        context.set_source_rgb(1, 1, 1)  # White\n-        context.paint()\n-        # this font list works in CentOS 7\n-        if multi_fonts:\n-            fonts = [\n-                'Century Schoolbook', 'Courier', 'STIX',\n-                'URW Chancery L', 'FreeMono']\n-            context.select_font_face(\n-                np.random.choice(fonts),\n-                cairo.FONT_SLANT_NORMAL,\n-                np.random.choice([cairo.FONT_WEIGHT_BOLD, cairo.FONT_WEIGHT_NORMAL]))\n-        else:\n-            context.select_font_face('Courier',\n-                                     cairo.FONT_SLANT_NORMAL,\n-                                     cairo.FONT_WEIGHT_BOLD)\n-        context.set_font_size(25)\n-        box = context.text_extents(text)\n-        border_w_h = (4, 4)\n-        if box[2] > (w - 2 * border_w_h[1]) or box[3] > (h - 2 * border_w_h[0]):\n-            raise IOError(('Could not fit string into image.'\n-                           'Max char count is too large for given image width.'))\n-\n-        # teach the RNN translational invariance by\n-        # fitting text box randomly on canvas, with some room to rotate\n-        max_shift_x = w - box[2] - border_w_h[0]\n-        max_shift_y = h - box[3] - border_w_h[1]\n-        top_left_x = np.random.randint(0, int(max_shift_x))\n-        if ud:\n-            top_left_y = np.random.randint(0, int(max_shift_y))\n-        else:\n-            top_left_y = h // 2\n-        context.move_to(top_left_x - int(box[0]), top_left_y - int(box[1]))\n-        context.set_source_rgb(0, 0, 0)\n-        context.show_text(text)\n-\n-    buf = surface.get_data()\n-    a = np.frombuffer(buf, np.uint8)\n-    a.shape = (h, w, 4)\n-    a = a[:, :, 0]  # grab single channel\n-    a = a.astype(np.float32) / 255\n-    a = np.expand_dims(a, 0)\n-    if rotate:\n-        a = image.random_rotation(a, 3 * (w - top_left_x) / w + 1)\n-    a = speckle(a)\n-\n-    return a\n-\n-\n-def shuffle_mats_or_lists(matrix_list, stop_ind=None):\n-    ret = []\n-    assert all([len(i) == len(matrix_list[0]) for i in matrix_list])\n-    len_val = len(matrix_list[0])\n-    if stop_ind is None:\n-        stop_ind = len_val\n-    assert stop_ind <= len_val\n-\n-    a = list(range(stop_ind))\n-    np.random.shuffle(a)\n-    a += list(range(stop_ind, len_val))\n-    for mat in matrix_list:\n-        if isinstance(mat, np.ndarray):\n-            ret.append(mat[a])\n-        elif isinstance(mat, list):\n-            ret.append([mat[i] for i in a])\n-        else:\n-            raise TypeError('`shuffle_mats_or_lists` only supports '\n-                            'numpy.array and list objects.')\n-    return ret\n-\n-\n-# Translation of characters to unique integer values\n-def text_to_labels(text):\n-    ret = []\n-    for char in text:\n-        ret.append(alphabet.find(char))\n-    return ret\n-\n-\n-# Reverse translation of numerical classes back to characters\n-def labels_to_text(labels):\n-    ret = []\n-    for c in labels:\n-        if c == len(alphabet):  # CTC Blank\n-            ret.append(\"\")\n-        else:\n-            ret.append(alphabet[c])\n-    return \"\".join(ret)\n-\n-\n-# only a-z and space..probably not to difficult\n-# to expand to uppercase and symbols\n-\n-def is_valid_str(in_str):\n-    search = re.compile(regex, re.UNICODE).search\n-    return bool(search(in_str))\n-\n-\n-# Uses generator functions to supply train/test with\n-# data. Image renderings and text are created on the fly\n-# each time with random perturbations\n-\n-class TextImageGenerator(keras.callbacks.Callback):\n-\n-    def __init__(self, monogram_file, bigram_file, minibatch_size,\n-                 img_w, img_h, downsample_factor, val_split,\n-                 absolute_max_string_len=16):\n-\n-        self.minibatch_size = minibatch_size\n-        self.img_w = img_w\n-        self.img_h = img_h\n-        self.monogram_file = monogram_file\n-        self.bigram_file = bigram_file\n-        self.downsample_factor = downsample_factor\n-        self.val_split = val_split\n-        self.blank_label = self.get_output_size() - 1\n-        self.absolute_max_string_len = absolute_max_string_len\n-\n-    def get_output_size(self):\n-        return len(alphabet) + 1\n-\n-    # num_words can be independent of the epoch size due to the use of generators\n-    # as max_string_len grows, num_words can grow\n-    def build_word_list(self, num_words, max_string_len=None, mono_fraction=0.5):\n-        assert max_string_len <= self.absolute_max_string_len\n-        assert num_words % self.minibatch_size == 0\n-        assert (self.val_split * num_words) % self.minibatch_size == 0\n-        self.num_words = num_words\n-        self.string_list = [''] * self.num_words\n-        tmp_string_list = []\n-        self.max_string_len = max_string_len\n-        self.Y_data = np.ones([self.num_words, self.absolute_max_string_len]) * -1\n-        self.X_text = []\n-        self.Y_len = [0] * self.num_words\n-\n-        def _is_length_of_word_valid(word):\n-            return (max_string_len == -1 or\n-                    max_string_len is None or\n-                    len(word) <= max_string_len)\n-\n-        # monogram file is sorted by frequency in english speech\n-        with codecs.open(self.monogram_file, mode='r', encoding='utf-8') as f:\n-            for line in f:\n-                if len(tmp_string_list) == int(self.num_words * mono_fraction):\n-                    break\n-                word = line.rstrip()\n-                if _is_length_of_word_valid(word):\n-                    tmp_string_list.append(word)\n-\n-        # bigram file contains common word pairings in english speech\n-        with codecs.open(self.bigram_file, mode='r', encoding='utf-8') as f:\n-            lines = f.readlines()\n-            for line in lines:\n-                if len(tmp_string_list) == self.num_words:\n-                    break\n-                columns = line.lower().split()\n-                word = columns[0] + ' ' + columns[1]\n-                if is_valid_str(word) and _is_length_of_word_valid(word):\n-                    tmp_string_list.append(word)\n-        if len(tmp_string_list) != self.num_words:\n-            raise IOError('Could not pull enough words'\n-                          'from supplied monogram and bigram files.')\n-        # interlace to mix up the easy and hard words\n-        self.string_list[::2] = tmp_string_list[:self.num_words // 2]\n-        self.string_list[1::2] = tmp_string_list[self.num_words // 2:]\n-\n-        for i, word in enumerate(self.string_list):\n-            self.Y_len[i] = len(word)\n-            self.Y_data[i, 0:len(word)] = text_to_labels(word)\n-            self.X_text.append(word)\n-        self.Y_len = np.expand_dims(np.array(self.Y_len), 1)\n-\n-        self.cur_val_index = self.val_split\n-        self.cur_train_index = 0\n-\n-    # each time an image is requested from train/val/test, a new random\n-    # painting of the text is performed\n-    def get_batch(self, index, size, train):\n-        # width and height are backwards from typical Keras convention\n-        # because width is the time dimension when it gets fed into the RNN\n-        if K.image_data_format() == 'channels_first':\n-            X_data = np.ones([size, 1, self.img_w, self.img_h])\n-        else:\n-            X_data = np.ones([size, self.img_w, self.img_h, 1])\n-\n-        labels = np.ones([size, self.absolute_max_string_len])\n-        input_length = np.zeros([size, 1])\n-        label_length = np.zeros([size, 1])\n-        source_str = []\n-        for i in range(size):\n-            # Mix in some blank inputs.  This seems to be important for\n-            # achieving translational invariance\n-            if train and i > size - 4:\n-                if K.image_data_format() == 'channels_first':\n-                    X_data[i, 0, 0:self.img_w, :] = self.paint_func('')[0, :, :].T\n-                else:\n-                    X_data[i, 0:self.img_w, :, 0] = self.paint_func('',)[0, :, :].T\n-                labels[i, 0] = self.blank_label\n-                input_length[i] = self.img_w // self.downsample_factor - 2\n-                label_length[i] = 1\n-                source_str.append('')\n-            else:\n-                if K.image_data_format() == 'channels_first':\n-                    X_data[i, 0, 0:self.img_w, :] = (\n-                        self.paint_func(self.X_text[index + i])[0, :, :].T)\n-                else:\n-                    X_data[i, 0:self.img_w, :, 0] = (\n-                        self.paint_func(self.X_text[index + i])[0, :, :].T)\n-                labels[i, :] = self.Y_data[index + i]\n-                input_length[i] = self.img_w // self.downsample_factor - 2\n-                label_length[i] = self.Y_len[index + i]\n-                source_str.append(self.X_text[index + i])\n-        inputs = {'the_input': X_data,\n-                  'the_labels': labels,\n-                  'input_length': input_length,\n-                  'label_length': label_length,\n-                  'source_str': source_str  # used for visualization only\n-                  }\n-        outputs = {'ctc': np.zeros([size])}  # dummy data for dummy loss function\n-        return (inputs, outputs)\n-\n-    def next_train(self):\n-        while 1:\n-            ret = self.get_batch(self.cur_train_index,\n-                                 self.minibatch_size, train=True)\n-            self.cur_train_index += self.minibatch_size\n-            if self.cur_train_index >= self.val_split:\n-                self.cur_train_index = self.cur_train_index % 32\n-                (self.X_text, self.Y_data, self.Y_len) = shuffle_mats_or_lists(\n-                    [self.X_text, self.Y_data, self.Y_len], self.val_split)\n-            yield ret\n-\n-    def next_val(self):\n-        while 1:\n-            ret = self.get_batch(self.cur_val_index,\n-                                 self.minibatch_size, train=False)\n-            self.cur_val_index += self.minibatch_size\n-            if self.cur_val_index >= self.num_words:\n-                self.cur_val_index = self.val_split + self.cur_val_index % 32\n-            yield ret\n-\n-    def on_train_begin(self, logs={}):\n-        self.build_word_list(16000, 4, 1)\n-        self.paint_func = lambda text: paint_text(\n-            text, self.img_w, self.img_h,\n-            rotate=False, ud=False, multi_fonts=False)\n-\n-    def on_epoch_begin(self, epoch, logs={}):\n-        # rebind the paint function to implement curriculum learning\n-        if 3 <= epoch < 6:\n-            self.paint_func = lambda text: paint_text(\n-                text, self.img_w, self.img_h,\n-                rotate=False, ud=True, multi_fonts=False)\n-        elif 6 <= epoch < 9:\n-            self.paint_func = lambda text: paint_text(\n-                text, self.img_w, self.img_h,\n-                rotate=False, ud=True, multi_fonts=True)\n-        elif epoch >= 9:\n-            self.paint_func = lambda text: paint_text(\n-                text, self.img_w, self.img_h,\n-                rotate=True, ud=True, multi_fonts=True)\n-        if epoch >= 21 and self.max_string_len < 12:\n-            self.build_word_list(32000, 12, 0.5)\n-\n-\n-# the actual loss calc occurs here despite it not being\n-# an internal Keras loss function\n-\n-def ctc_lambda_func(args):\n-    y_pred, labels, input_length, label_length = args\n-    # the 2 is critical here since the first couple outputs of the RNN\n-    # tend to be garbage:\n-    y_pred = y_pred[:, 2:, :]\n-    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n-\n-\n-# For a real OCR application, this should be beam search with a dictionary\n-# and language model.  For this example, best path is sufficient.\n-\n-def decode_batch(test_func, word_batch):\n-    out = test_func([word_batch])[0]\n-    ret = []\n-    for j in range(out.shape[0]):\n-        out_best = list(np.argmax(out[j, 2:], 1))\n-        out_best = [k for k, g in itertools.groupby(out_best)]\n-        outstr = labels_to_text(out_best)\n-        ret.append(outstr)\n-    return ret\n-\n-\n-class VizCallback(keras.callbacks.Callback):\n-\n-    def __init__(self, run_name, test_func, text_img_gen, num_display_words=6):\n-        self.test_func = test_func\n-        self.output_dir = os.path.join(\n-            OUTPUT_DIR, run_name)\n-        self.text_img_gen = text_img_gen\n-        self.num_display_words = num_display_words\n-        if not os.path.exists(self.output_dir):\n-            os.makedirs(self.output_dir)\n-\n-    def show_edit_distance(self, num):\n-        num_left = num\n-        mean_norm_ed = 0.0\n-        mean_ed = 0.0\n-        while num_left > 0:\n-            word_batch = next(self.text_img_gen)[0]\n-            num_proc = min(word_batch['the_input'].shape[0], num_left)\n-            decoded_res = decode_batch(self.test_func,\n-                                       word_batch['the_input'][0:num_proc])\n-            for j in range(num_proc):\n-                edit_dist = editdistance.eval(decoded_res[j],\n-                                              word_batch['source_str'][j])\n-                mean_ed += float(edit_dist)\n-                mean_norm_ed += float(edit_dist) / len(word_batch['source_str'][j])\n-            num_left -= num_proc\n-        mean_norm_ed = mean_norm_ed / num\n-        mean_ed = mean_ed / num\n-        print('\\nOut of %d samples:  Mean edit distance:'\n-              '%.3f Mean normalized edit distance: %0.3f'\n-              % (num, mean_ed, mean_norm_ed))\n-\n-    def on_epoch_end(self, epoch, logs={}):\n-        self.model.save_weights(\n-            os.path.join(self.output_dir, 'weights%02d.h5' % (epoch)))\n-        self.show_edit_distance(256)\n-        word_batch = next(self.text_img_gen)[0]\n-        res = decode_batch(self.test_func,\n-                           word_batch['the_input'][0:self.num_display_words])\n-        if word_batch['the_input'][0].shape[0] < 256:\n-            cols = 2\n-        else:\n-            cols = 1\n-        for i in range(self.num_display_words):\n-            pylab.subplot(self.num_display_words // cols, cols, i + 1)\n-            if K.image_data_format() == 'channels_first':\n-                the_input = word_batch['the_input'][i, 0, :, :]\n-            else:\n-                the_input = word_batch['the_input'][i, :, :, 0]\n-            pylab.imshow(the_input.T, cmap='Greys_r')\n-            pylab.xlabel(\n-                'Truth = \\'%s\\'\\nDecoded = \\'%s\\'' %\n-                (word_batch['source_str'][i], res[i]))\n-        fig = pylab.gcf()\n-        fig.set_size_inches(10, 13)\n-        pylab.savefig(os.path.join(self.output_dir, 'e%02d.png' % (epoch)))\n-        pylab.close()\n-\n-\n-def train(run_name, start_epoch, stop_epoch, img_w):\n-    # Input Parameters\n-    img_h = 64\n-    words_per_epoch = 16000\n-    val_split = 0.2\n-    val_words = int(words_per_epoch * (val_split))\n-\n-    # Network parameters\n-    conv_filters = 16\n-    kernel_size = (3, 3)\n-    pool_size = 2\n-    time_dense_size = 32\n-    rnn_size = 512\n-    minibatch_size = 32\n-\n-    if K.image_data_format() == 'channels_first':\n-        input_shape = (1, img_w, img_h)\n-    else:\n-        input_shape = (img_w, img_h, 1)\n-\n-    fdir = os.path.dirname(\n-        get_file('wordlists.tgz',\n-                 origin='http://www.mythic-ai.com/datasets/wordlists.tgz',\n-                 untar=True))\n-\n-    img_gen = TextImageGenerator(\n-        monogram_file=os.path.join(fdir, 'wordlist_mono_clean.txt'),\n-        bigram_file=os.path.join(fdir, 'wordlist_bi_clean.txt'),\n-        minibatch_size=minibatch_size,\n-        img_w=img_w,\n-        img_h=img_h,\n-        downsample_factor=(pool_size ** 2),\n-        val_split=words_per_epoch - val_words)\n-    act = 'relu'\n-    input_data = Input(name='the_input', shape=input_shape, dtype='float32')\n-    inner = Conv2D(conv_filters, kernel_size, padding='same',\n-                   activation=act, kernel_initializer='he_normal',\n-                   name='conv1')(input_data)\n-    inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max1')(inner)\n-    inner = Conv2D(conv_filters, kernel_size, padding='same',\n-                   activation=act, kernel_initializer='he_normal',\n-                   name='conv2')(inner)\n-    inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max2')(inner)\n-\n-    conv_to_rnn_dims = (img_w // (pool_size ** 2),\n-                        (img_h // (pool_size ** 2)) * conv_filters)\n-    inner = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(inner)\n-\n-    # cuts down input size going into RNN:\n-    inner = Dense(time_dense_size, activation=act, name='dense1')(inner)\n-\n-    # Two layers of bidirectional GRUs\n-    # GRU seems to work as well, if not better than LSTM:\n-    gru_1 = GRU(rnn_size, return_sequences=True,\n-                kernel_initializer='he_normal', name='gru1')(inner)\n-    gru_1b = GRU(rnn_size, return_sequences=True,\n-                 go_backwards=True, kernel_initializer='he_normal',\n-                 name='gru1_b')(inner)\n-    gru1_merged = add([gru_1, gru_1b])\n-    gru_2 = GRU(rnn_size, return_sequences=True,\n-                kernel_initializer='he_normal', name='gru2')(gru1_merged)\n-    gru_2b = GRU(rnn_size, return_sequences=True, go_backwards=True,\n-                 kernel_initializer='he_normal', name='gru2_b')(gru1_merged)\n-\n-    # transforms RNN output to character activations:\n-    inner = Dense(img_gen.get_output_size(), kernel_initializer='he_normal',\n-                  name='dense2')(concatenate([gru_2, gru_2b]))\n-    y_pred = Activation('softmax', name='softmax')(inner)\n-    Model(inputs=input_data, outputs=y_pred).summary()\n-\n-    labels = Input(name='the_labels',\n-                   shape=[img_gen.absolute_max_string_len], dtype='float32')\n-    input_length = Input(name='input_length', shape=[1], dtype='int64')\n-    label_length = Input(name='label_length', shape=[1], dtype='int64')\n-    # Keras doesn't currently support loss funcs with extra parameters\n-    # so CTC loss is implemented in a lambda layer\n-    loss_out = Lambda(\n-        ctc_lambda_func, output_shape=(1,),\n-        name='ctc')([y_pred, labels, input_length, label_length])\n-\n-    # clipnorm seems to speeds up convergence\n-    sgd = SGD(learning_rate=0.02,\n-              decay=1e-6,\n-              momentum=0.9,\n-              nesterov=True)\n-\n-    model = Model(inputs=[input_data, labels, input_length, label_length],\n-                  outputs=loss_out)\n-\n-    # the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n-    model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd)\n-    if start_epoch > 0:\n-        weight_file = os.path.join(\n-            OUTPUT_DIR,\n-            os.path.join(run_name, 'weights%02d.h5' % (start_epoch - 1)))\n-        model.load_weights(weight_file)\n-    # captures output of softmax so we can decode the output during visualization\n-    test_func = K.function([input_data], [y_pred])\n-\n-    viz_cb = VizCallback(run_name, test_func, img_gen.next_val())\n-\n-    model.fit_generator(\n-        generator=img_gen.next_train(),\n-        steps_per_epoch=(words_per_epoch - val_words) // minibatch_size,\n-        epochs=stop_epoch,\n-        validation_data=img_gen.next_val(),\n-        validation_steps=val_words // minibatch_size,\n-        callbacks=[viz_cb, img_gen],\n-        initial_epoch=start_epoch)\n-\n-\n-if __name__ == '__main__':\n-    run_name = datetime.datetime.now().strftime('%Y:%m:%d:%H:%M:%S')\n-    train(run_name, 0, 20, 128)\n-    # increase to wider images and start at epoch 20.\n-    # The learned weights are reloaded\n-    train(run_name, 20, 25, 512)\n\n@@ -1,49 +0,0 @@\n-'''\n-#Trains a Bidirectional LSTM on the IMDB sentiment classification task.\n-\n-Output after 4 epochs on CPU: ~0.8146\n-Time per epoch on CPU (Core i7): ~150s.\n-'''\n-\n-from __future__ import print_function\n-import numpy as np\n-\n-from keras.preprocessing import sequence\n-from keras.models import Sequential\n-from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n-from keras.datasets import imdb\n-\n-\n-max_features = 20000\n-# cut texts after this number of words\n-# (among top max_features most common words)\n-maxlen = 100\n-batch_size = 32\n-\n-print('Loading data...')\n-(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n-print(len(x_train), 'train sequences')\n-print(len(x_test), 'test sequences')\n-\n-print('Pad sequences (samples x time)')\n-x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n-x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n-print('x_train shape:', x_train.shape)\n-print('x_test shape:', x_test.shape)\n-y_train = np.array(y_train)\n-y_test = np.array(y_test)\n-\n-model = Sequential()\n-model.add(Embedding(max_features, 128, input_length=maxlen))\n-model.add(Bidirectional(LSTM(64)))\n-model.add(Dropout(0.5))\n-model.add(Dense(1, activation='sigmoid'))\n-\n-# try using different optimizers and different optimizer configs\n-model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n-\n-print('Train...')\n-model.fit(x_train, y_train,\n-          batch_size=batch_size,\n-          epochs=4,\n-          validation_data=[x_test, y_test])\n\n@@ -1,73 +0,0 @@\n-'''\n-#This example demonstrates the use of Convolution1D for text classification.\n-\n-Gets to 0.89 test accuracy after 2 epochs. </br>\n-90s/epoch on Intel i5 2.4Ghz CPU. </br>\n-10s/epoch on Tesla K40 GPU.\n-'''\n-from __future__ import print_function\n-\n-from keras.preprocessing import sequence\n-from keras.models import Sequential\n-from keras.layers import Dense, Dropout, Activation\n-from keras.layers import Embedding\n-from keras.layers import Conv1D, GlobalMaxPooling1D\n-from keras.datasets import imdb\n-\n-# set parameters:\n-max_features = 5000\n-maxlen = 400\n-batch_size = 32\n-embedding_dims = 50\n-filters = 250\n-kernel_size = 3\n-hidden_dims = 250\n-epochs = 2\n-\n-print('Loading data...')\n-(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n-print(len(x_train), 'train sequences')\n-print(len(x_test), 'test sequences')\n-\n-print('Pad sequences (samples x time)')\n-x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n-x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n-print('x_train shape:', x_train.shape)\n-print('x_test shape:', x_test.shape)\n-\n-print('Build model...')\n-model = Sequential()\n-\n-# we start off with an efficient embedding layer which maps\n-# our vocab indices into embedding_dims dimensions\n-model.add(Embedding(max_features,\n-                    embedding_dims,\n-                    input_length=maxlen))\n-model.add(Dropout(0.2))\n-\n-# we add a Convolution1D, which will learn filters\n-# word group filters of size filter_length:\n-model.add(Conv1D(filters,\n-                 kernel_size,\n-                 padding='valid',\n-                 activation='relu',\n-                 strides=1))\n-# we use max pooling:\n-model.add(GlobalMaxPooling1D())\n-\n-# We add a vanilla hidden layer:\n-model.add(Dense(hidden_dims))\n-model.add(Dropout(0.2))\n-model.add(Activation('relu'))\n-\n-# We project onto a single unit output layer, and squash it with a sigmoid:\n-model.add(Dense(1))\n-model.add(Activation('sigmoid'))\n-\n-model.compile(loss='binary_crossentropy',\n-              optimizer='adam',\n-              metrics=['accuracy'])\n-model.fit(x_train, y_train,\n-          batch_size=batch_size,\n-          epochs=epochs,\n-          validation_data=(x_test, y_test))\n\n@@ -1,76 +0,0 @@\n-'''\n-#Train a recurrent convolutional network on the IMDB sentiment classification task.\n-\n-Gets to 0.8498 test accuracy after 2 epochs. 41 s/epoch on K520 GPU.\n-'''\n-from __future__ import print_function\n-\n-from keras.preprocessing import sequence\n-from keras.models import Sequential\n-from keras.layers import Dense, Dropout, Activation\n-from keras.layers import Embedding\n-from keras.layers import LSTM\n-from keras.layers import Conv1D, MaxPooling1D\n-from keras.datasets import imdb\n-\n-# Embedding\n-max_features = 20000\n-maxlen = 100\n-embedding_size = 128\n-\n-# Convolution\n-kernel_size = 5\n-filters = 64\n-pool_size = 4\n-\n-# LSTM\n-lstm_output_size = 70\n-\n-# Training\n-batch_size = 30\n-epochs = 2\n-\n-'''\n-Note:\n-batch_size is highly sensitive.\n-Only 2 epochs are needed as the dataset is very small.\n-'''\n-\n-print('Loading data...')\n-(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n-print(len(x_train), 'train sequences')\n-print(len(x_test), 'test sequences')\n-\n-print('Pad sequences (samples x time)')\n-x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n-x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n-print('x_train shape:', x_train.shape)\n-print('x_test shape:', x_test.shape)\n-\n-print('Build model...')\n-\n-model = Sequential()\n-model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n-model.add(Dropout(0.25))\n-model.add(Conv1D(filters,\n-                 kernel_size,\n-                 padding='valid',\n-                 activation='relu',\n-                 strides=1))\n-model.add(MaxPooling1D(pool_size=pool_size))\n-model.add(LSTM(lstm_output_size))\n-model.add(Dense(1))\n-model.add(Activation('sigmoid'))\n-\n-model.compile(loss='binary_crossentropy',\n-              optimizer='adam',\n-              metrics=['accuracy'])\n-\n-print('Train...')\n-model.fit(x_train, y_train,\n-          batch_size=batch_size,\n-          epochs=epochs,\n-          validation_data=(x_test, y_test))\n-score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n-print('Test score:', score)\n-print('Test accuracy:', acc)\n\n@@ -1,144 +0,0 @@\n-'''\n-#This example demonstrates the use of fasttext for text classification\n-\n-Based on Joulin et al's paper:\n-\n-[Bags of Tricks for Efficient Text Classification\n-](https://arxiv.org/abs/1607.01759)\n-\n-Results on IMDB datasets with uni and bi-gram embeddings:\n-\n-Embedding|Accuracy, 5 epochs|Speed (s/epoch)|Hardware\n-:--------|-----------------:|----:|:-------\n-Uni-gram |            0.8813|    8|i7 CPU\n-Bi-gram  |            0.9056|    2|GTx 980M GPU\n-\n-'''\n-\n-from __future__ import print_function\n-import numpy as np\n-\n-from keras.preprocessing import sequence\n-from keras.models import Sequential\n-from keras.layers import Dense\n-from keras.layers import Embedding\n-from keras.layers import GlobalAveragePooling1D\n-from keras.datasets import imdb\n-\n-\n-def create_ngram_set(input_list, ngram_value=2):\n-    \"\"\"\n-    Extract a set of n-grams from a list of integers.\n-\n-    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n-    {(4, 9), (4, 1), (1, 4), (9, 4)}\n-\n-    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n-    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n-    \"\"\"\n-    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n-\n-\n-def add_ngram(sequences, token_indice, ngram_range=2):\n-    \"\"\"\n-    Augment the input list of list (sequences) by appending n-grams values.\n-\n-    Example: adding bi-gram\n-    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n-    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n-    >>> add_ngram(sequences, token_indice, ngram_range=2)\n-    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n-\n-    Example: adding tri-gram\n-    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n-    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n-    >>> add_ngram(sequences, token_indice, ngram_range=3)\n-    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]\n-    \"\"\"\n-    new_sequences = []\n-    for input_list in sequences:\n-        new_list = input_list[:]\n-        for ngram_value in range(2, ngram_range + 1):\n-            for i in range(len(new_list) - ngram_value + 1):\n-                ngram = tuple(new_list[i:i + ngram_value])\n-                if ngram in token_indice:\n-                    new_list.append(token_indice[ngram])\n-        new_sequences.append(new_list)\n-\n-    return new_sequences\n-\n-# Set parameters:\n-# ngram_range = 2 will add bi-grams features\n-ngram_range = 1\n-max_features = 20000\n-maxlen = 400\n-batch_size = 32\n-embedding_dims = 50\n-epochs = 5\n-\n-print('Loading data...')\n-(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n-print(len(x_train), 'train sequences')\n-print(len(x_test), 'test sequences')\n-print('Average train sequence length: {}'.format(\n-    np.mean(list(map(len, x_train)), dtype=int)))\n-print('Average test sequence length: {}'.format(\n-    np.mean(list(map(len, x_test)), dtype=int)))\n-\n-if ngram_range > 1:\n-    print('Adding {}-gram features'.format(ngram_range))\n-    # Create set of unique n-gram from the training set.\n-    ngram_set = set()\n-    for input_list in x_train:\n-        for i in range(2, ngram_range + 1):\n-            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n-            ngram_set.update(set_of_ngram)\n-\n-    # Dictionary mapping n-gram token to a unique integer.\n-    # Integer values are greater than max_features in order\n-    # to avoid collision with existing features.\n-    start_index = max_features + 1\n-    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n-    indice_token = {token_indice[k]: k for k in token_indice}\n-\n-    # max_features is the highest integer that could be found in the dataset.\n-    max_features = np.max(list(indice_token.keys())) + 1\n-\n-    # Augmenting x_train and x_test with n-grams features\n-    x_train = add_ngram(x_train, token_indice, ngram_range)\n-    x_test = add_ngram(x_test, token_indice, ngram_range)\n-    print('Average train sequence length: {}'.format(\n-        np.mean(list(map(len, x_train)), dtype=int)))\n-    print('Average test sequence length: {}'.format(\n-        np.mean(list(map(len, x_test)), dtype=int)))\n-\n-print('Pad sequences (samples x time)')\n-x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n-x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n-print('x_train shape:', x_train.shape)\n-print('x_test shape:', x_test.shape)\n-\n-print('Build model...')\n-model = Sequential()\n-\n-# we start off with an efficient embedding layer which maps\n-# our vocab indices into embedding_dims dimensions\n-model.add(Embedding(max_features,\n-                    embedding_dims,\n-                    input_length=maxlen))\n-\n-# we add a GlobalAveragePooling1D, which will average the embeddings\n-# of all words in the document\n-model.add(GlobalAveragePooling1D())\n-\n-# We project onto a single unit output layer, and squash it with a sigmoid:\n-model.add(Dense(1, activation='sigmoid'))\n-\n-model.compile(loss='binary_crossentropy',\n-              optimizer='adam',\n-              metrics=['accuracy'])\n-\n-model.fit(x_train, y_train,\n-          batch_size=batch_size,\n-          epochs=epochs,\n-          validation_data=(x_test, y_test))\n\n@@ -1,60 +0,0 @@\n-'''\n-#Trains an LSTM model on the IMDB sentiment classification task.\n-\n-The dataset is actually too small for LSTM to be of any advantage\n-compared to simpler, much faster methods such as TF-IDF + LogReg.\n-\n-**Notes**\n-\n-- RNNs are tricky. Choice of batch size is important,\n-choice of loss and optimizer is critical, etc.\n-Some configurations won't converge.\n-\n-- LSTM loss decrease patterns during training can be quite different\n-from what you see with CNNs/MLPs/etc.\n-\n-'''\n-from __future__ import print_function\n-\n-from keras.preprocessing import sequence\n-from keras.models import Sequential\n-from keras.layers import Dense, Embedding\n-from keras.layers import LSTM\n-from keras.datasets import imdb\n-\n-max_features = 20000\n-# cut texts after this number of words (among top max_features most common words)\n-maxlen = 80\n-batch_size = 32\n-\n-print('Loading data...')\n-(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n-print(len(x_train), 'train sequences')\n-print(len(x_test), 'test sequences')\n-\n-print('Pad sequences (samples x time)')\n-x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n-x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n-print('x_train shape:', x_train.shape)\n-print('x_test shape:', x_test.shape)\n-\n-print('Build model...')\n-model = Sequential()\n-model.add(Embedding(max_features, 128))\n-model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n-model.add(Dense(1, activation='sigmoid'))\n-\n-# try using different optimizers and different optimizer configs\n-model.compile(loss='binary_crossentropy',\n-              optimizer='adam',\n-              metrics=['accuracy'])\n-\n-print('Train...')\n-model.fit(x_train, y_train,\n-          batch_size=batch_size,\n-          epochs=15,\n-          validation_data=(x_test, y_test))\n-score, acc = model.evaluate(x_test, y_test,\n-                            batch_size=batch_size)\n-print('Test score:', score)\n-print('Test accuracy:', acc)\n\n@@ -1,234 +0,0 @@\n-'''\n-#Sequence to sequence example in Keras (character-level).\n-\n-This script demonstrates how to implement a basic character-level\n-sequence-to-sequence model. We apply it to translating\n-short English sentences into short French sentences,\n-character-by-character. Note that it is fairly unusual to\n-do character-level machine translation, as word-level\n-models are more common in this domain.\n-\n-**Summary of the algorithm**\n-\n-- We start with input sequences from a domain (e.g. English sentences)\n-    and corresponding target sequences from another domain\n-    (e.g. French sentences).\n-- An encoder LSTM turns input sequences to 2 state vectors\n-    (we keep the last LSTM state and discard the outputs).\n-- A decoder LSTM is trained to turn the target sequences into\n-    the same sequence but offset by one timestep in the future,\n-    a training process called \"teacher forcing\" in this context.\n-    It uses as initial state the state vectors from the encoder.\n-    Effectively, the decoder learns to generate `targets[t+1...]`\n-    given `targets[...t]`, conditioned on the input sequence.\n-- In inference mode, when we want to decode unknown input sequences, we:\n-    - Encode the input sequence into state vectors\n-    - Start with a target sequence of size 1\n-        (just the start-of-sequence character)\n-    - Feed the state vectors and 1-char target sequence\n-        to the decoder to produce predictions for the next character\n-    - Sample the next character using these predictions\n-        (we simply use argmax).\n-    - Append the sampled character to the target sequence\n-    - Repeat until we generate the end-of-sequence character or we\n-        hit the character limit.\n-\n-**Data download**\n-\n-[English to French sentence pairs.\n-](http://www.manythings.org/anki/fra-eng.zip)\n-\n-[Lots of neat sentence pairs datasets.\n-](http://www.manythings.org/anki/)\n-\n-**References**\n-\n-- [Sequence to Sequence Learning with Neural Networks\n-   ](https://arxiv.org/abs/1409.3215)\n-- [Learning Phrase Representations using\n-    RNN Encoder-Decoder for Statistical Machine Translation\n-    ](https://arxiv.org/abs/1406.1078)\n-'''\n-from __future__ import print_function\n-\n-from keras.models import Model\n-from keras.layers import Input, LSTM, Dense\n-import numpy as np\n-\n-batch_size = 64  # Batch size for training.\n-epochs = 100  # Number of epochs to train for.\n-latent_dim = 256  # Latent dimensionality of the encoding space.\n-num_samples = 10000  # Number of samples to train on.\n-# Path to the data txt file on disk.\n-data_path = 'fra-eng/fra.txt'\n-\n-# Vectorize the data.\n-input_texts = []\n-target_texts = []\n-input_characters = set()\n-target_characters = set()\n-with open(data_path, 'r', encoding='utf-8') as f:\n-    lines = f.read().split('\\n')\n-for line in lines[: min(num_samples, len(lines) - 1)]:\n-    input_text, target_text, _ = line.split('\\t')\n-    # We use \"tab\" as the \"start sequence\" character\n-    # for the targets, and \"\\n\" as \"end sequence\" character.\n-    target_text = '\\t' + target_text + '\\n'\n-    input_texts.append(input_text)\n-    target_texts.append(target_text)\n-    for char in input_text:\n-        if char not in input_characters:\n-            input_characters.add(char)\n-    for char in target_text:\n-        if char not in target_characters:\n-            target_characters.add(char)\n-\n-input_characters = sorted(list(input_characters))\n-target_characters = sorted(list(target_characters))\n-num_encoder_tokens = len(input_characters)\n-num_decoder_tokens = len(target_characters)\n-max_encoder_seq_length = max([len(txt) for txt in input_texts])\n-max_decoder_seq_length = max([len(txt) for txt in target_texts])\n-\n-print('Number of samples:', len(input_texts))\n-print('Number of unique input tokens:', num_encoder_tokens)\n-print('Number of unique output tokens:', num_decoder_tokens)\n-print('Max sequence length for inputs:', max_encoder_seq_length)\n-print('Max sequence length for outputs:', max_decoder_seq_length)\n-\n-input_token_index = dict(\n-    [(char, i) for i, char in enumerate(input_characters)])\n-target_token_index = dict(\n-    [(char, i) for i, char in enumerate(target_characters)])\n-\n-encoder_input_data = np.zeros(\n-    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n-    dtype='float32')\n-decoder_input_data = np.zeros(\n-    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n-    dtype='float32')\n-decoder_target_data = np.zeros(\n-    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n-    dtype='float32')\n-\n-for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n-    for t, char in enumerate(input_text):\n-        encoder_input_data[i, t, input_token_index[char]] = 1.\n-    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n-    for t, char in enumerate(target_text):\n-        # decoder_target_data is ahead of decoder_input_data by one timestep\n-        decoder_input_data[i, t, target_token_index[char]] = 1.\n-        if t > 0:\n-            # decoder_target_data will be ahead by one timestep\n-            # and will not include the start character.\n-            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n-    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n-    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n-# Define an input sequence and process it.\n-encoder_inputs = Input(shape=(None, num_encoder_tokens))\n-encoder = LSTM(latent_dim, return_state=True)\n-encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n-# We discard `encoder_outputs` and only keep the states.\n-encoder_states = [state_h, state_c]\n-\n-# Set up the decoder, using `encoder_states` as initial state.\n-decoder_inputs = Input(shape=(None, num_decoder_tokens))\n-# We set up our decoder to return full output sequences,\n-# and to return internal states as well. We don't use the\n-# return states in the training model, but we will use them in inference.\n-decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n-decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n-                                     initial_state=encoder_states)\n-decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n-decoder_outputs = decoder_dense(decoder_outputs)\n-\n-# Define the model that will turn\n-# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n-model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n-\n-# Run training\n-model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n-              metrics=['accuracy'])\n-model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n-          batch_size=batch_size,\n-          epochs=epochs,\n-          validation_split=0.2)\n-# Save model\n-model.save('s2s.h5')\n-\n-# Next: inference mode (sampling).\n-# Here's the drill:\n-# 1) encode input and retrieve initial decoder state\n-# 2) run one step of decoder with this initial state\n-# and a \"start of sequence\" token as target.\n-# Output will be the next target token\n-# 3) Repeat with the current target token and current states\n-\n-# Define sampling models\n-encoder_model = Model(encoder_inputs, encoder_states)\n-\n-decoder_state_input_h = Input(shape=(latent_dim,))\n-decoder_state_input_c = Input(shape=(latent_dim,))\n-decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n-decoder_outputs, state_h, state_c = decoder_lstm(\n-    decoder_inputs, initial_state=decoder_states_inputs)\n-decoder_states = [state_h, state_c]\n-decoder_outputs = decoder_dense(decoder_outputs)\n-decoder_model = Model(\n-    [decoder_inputs] + decoder_states_inputs,\n-    [decoder_outputs] + decoder_states)\n-\n-# Reverse-lookup token index to decode sequences back to\n-# something readable.\n-reverse_input_char_index = dict(\n-    (i, char) for char, i in input_token_index.items())\n-reverse_target_char_index = dict(\n-    (i, char) for char, i in target_token_index.items())\n-\n-\n-def decode_sequence(input_seq):\n-    # Encode the input as state vectors.\n-    states_value = encoder_model.predict(input_seq)\n-\n-    # Generate empty target sequence of length 1.\n-    target_seq = np.zeros((1, 1, num_decoder_tokens))\n-    # Populate the first character of target sequence with the start character.\n-    target_seq[0, 0, target_token_index['\\t']] = 1.\n-\n-    # Sampling loop for a batch of sequences\n-    # (to simplify, here we assume a batch of size 1).\n-    stop_condition = False\n-    decoded_sentence = ''\n-    while not stop_condition:\n-        output_tokens, h, c = decoder_model.predict(\n-            [target_seq] + states_value)\n-\n-        # Sample a token\n-        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n-        sampled_char = reverse_target_char_index[sampled_token_index]\n-        decoded_sentence += sampled_char\n-\n-        # Exit condition: either hit max length\n-        # or find stop character.\n-        if (sampled_char == '\\n' or\n-           len(decoded_sentence) > max_decoder_seq_length):\n-            stop_condition = True\n-\n-        # Update the target sequence (of length 1).\n-        target_seq = np.zeros((1, 1, num_decoder_tokens))\n-        target_seq[0, 0, sampled_token_index] = 1.\n-\n-        # Update states\n-        states_value = [h, c]\n-\n-    return decoded_sentence\n-\n-\n-for seq_index in range(100):\n-    # Take one sequence (part of the training set)\n-    # for trying out decoding.\n-    input_seq = encoder_input_data[seq_index: seq_index + 1]\n-    decoded_sentence = decode_sequence(input_seq)\n-    print('-')\n-    print('Input sentence:', input_texts[seq_index])\n-    print('Decoded sentence:', decoded_sentence)\n\n@@ -1,151 +0,0 @@\n-'''\n-#Restore a character-level sequence to sequence model from to generate predictions.\n-\n-This script loads the ```s2s.h5``` model saved by [lstm_seq2seq.py\n-](/examples/lstm_seq2seq/) and generates sequences from it. It assumes\n-that no changes have been made (for example: ```latent_dim``` is unchanged,\n-and the input data and model architecture are unchanged).\n-\n-See [lstm_seq2seq.py](/examples/lstm_seq2seq/) for more details on the\n-model architecture and how it is trained.\n-'''\n-from __future__ import print_function\n-\n-from keras.models import Model, load_model\n-from keras.layers import Input\n-import numpy as np\n-\n-batch_size = 64  # Batch size for training.\n-epochs = 100  # Number of epochs to train for.\n-latent_dim = 256  # Latent dimensionality of the encoding space.\n-num_samples = 10000  # Number of samples to train on.\n-# Path to the data txt file on disk.\n-data_path = 'fra-eng/fra.txt'\n-\n-# Vectorize the data.  We use the same approach as the training script.\n-# NOTE: the data must be identical, in order for the character -> integer\n-# mappings to be consistent.\n-# We omit encoding target_texts since they are not needed.\n-input_texts = []\n-target_texts = []\n-input_characters = set()\n-target_characters = set()\n-with open(data_path, 'r', encoding='utf-8') as f:\n-    lines = f.read().split('\\n')\n-for line in lines[: min(num_samples, len(lines) - 1)]:\n-    input_text, target_text, _ = line.split('\\t')\n-    # We use \"tab\" as the \"start sequence\" character\n-    # for the targets, and \"\\n\" as \"end sequence\" character.\n-    target_text = '\\t' + target_text + '\\n'\n-    input_texts.append(input_text)\n-    target_texts.append(target_text)\n-    for char in input_text:\n-        if char not in input_characters:\n-            input_characters.add(char)\n-    for char in target_text:\n-        if char not in target_characters:\n-            target_characters.add(char)\n-\n-input_characters = sorted(list(input_characters))\n-target_characters = sorted(list(target_characters))\n-num_encoder_tokens = len(input_characters)\n-num_decoder_tokens = len(target_characters)\n-max_encoder_seq_length = max([len(txt) for txt in input_texts])\n-max_decoder_seq_length = max([len(txt) for txt in target_texts])\n-\n-print('Number of samples:', len(input_texts))\n-print('Number of unique input tokens:', num_encoder_tokens)\n-print('Number of unique output tokens:', num_decoder_tokens)\n-print('Max sequence length for inputs:', max_encoder_seq_length)\n-print('Max sequence length for outputs:', max_decoder_seq_length)\n-\n-input_token_index = dict(\n-    [(char, i) for i, char in enumerate(input_characters)])\n-target_token_index = dict(\n-    [(char, i) for i, char in enumerate(target_characters)])\n-\n-encoder_input_data = np.zeros(\n-    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n-    dtype='float32')\n-\n-for i, input_text in enumerate(input_texts):\n-    for t, char in enumerate(input_text):\n-        encoder_input_data[i, t, input_token_index[char]] = 1.\n-\n-# Restore the model and construct the encoder and decoder.\n-model = load_model('s2s.h5')\n-\n-encoder_inputs = model.input[0]   # input_1\n-encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output   # lstm_1\n-encoder_states = [state_h_enc, state_c_enc]\n-encoder_model = Model(encoder_inputs, encoder_states)\n-\n-decoder_inputs = model.input[1]   # input_2\n-decoder_state_input_h = Input(shape=(latent_dim,), name='input_3')\n-decoder_state_input_c = Input(shape=(latent_dim,), name='input_4')\n-decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n-decoder_lstm = model.layers[3]\n-decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n-    decoder_inputs, initial_state=decoder_states_inputs)\n-decoder_states = [state_h_dec, state_c_dec]\n-decoder_dense = model.layers[4]\n-decoder_outputs = decoder_dense(decoder_outputs)\n-decoder_model = Model(\n-    [decoder_inputs] + decoder_states_inputs,\n-    [decoder_outputs] + decoder_states)\n-\n-# Reverse-lookup token index to decode sequences back to\n-# something readable.\n-reverse_input_char_index = dict(\n-    (i, char) for char, i in input_token_index.items())\n-reverse_target_char_index = dict(\n-    (i, char) for char, i in target_token_index.items())\n-\n-\n-# Decodes an input sequence.  Future work should support beam search.\n-def decode_sequence(input_seq):\n-    # Encode the input as state vectors.\n-    states_value = encoder_model.predict(input_seq)\n-\n-    # Generate empty target sequence of length 1.\n-    target_seq = np.zeros((1, 1, num_decoder_tokens))\n-    # Populate the first character of target sequence with the start character.\n-    target_seq[0, 0, target_token_index['\\t']] = 1.\n-\n-    # Sampling loop for a batch of sequences\n-    # (to simplify, here we assume a batch of size 1).\n-    stop_condition = False\n-    decoded_sentence = ''\n-    while not stop_condition:\n-        output_tokens, h, c = decoder_model.predict(\n-            [target_seq] + states_value)\n-\n-        # Sample a token\n-        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n-        sampled_char = reverse_target_char_index[sampled_token_index]\n-        decoded_sentence += sampled_char\n-\n-        # Exit condition: either hit max length\n-        # or find stop character.\n-        if (sampled_char == '\\n' or\n-           len(decoded_sentence) > max_decoder_seq_length):\n-            stop_condition = True\n-\n-        # Update the target sequence (of length 1).\n-        target_seq = np.zeros((1, 1, num_decoder_tokens))\n-        target_seq[0, 0, sampled_token_index] = 1.\n-\n-        # Update states\n-        states_value = [h, c]\n-\n-    return decoded_sentence\n-\n-\n-for seq_index in range(100):\n-    # Take one sequence (part of the training set)\n-    # for trying out decoding.\n-    input_seq = encoder_input_data[seq_index: seq_index + 1]\n-    decoded_sentence = decode_sequence(input_seq)\n-    print('-')\n-    print('Input sentence:', input_texts[seq_index])\n-    print('Decoded sentence:', decoded_sentence)\n\n@@ -1,243 +0,0 @@\n-'''\n-#How to use a stateful LSTM model, stateful vs stateless LSTM performance comparison\n-\n-[More documentation about the Keras LSTM model](/layers/recurrent/#lstm)\n-\n-The models are trained on an input/output pair, where\n-the input is a generated uniformly distributed\n-random sequence of length = `input_len`,\n-and the output is a moving average of the input with window length = `tsteps`.\n-Both `input_len` and `tsteps` are defined in the \"editable parameters\"\n-section.\n-\n-A larger `tsteps` value means that the LSTM will need more memory\n-to figure out the input-output relationship.\n-This memory length is controlled by the `lahead` variable (more details below).\n-\n-The rest of the parameters are:\n-\n-- `input_len`: the length of the generated input sequence\n-- `lahead`: the input sequence length that the LSTM\n-  is trained on for each output point\n-- `batch_size`, `epochs`: same parameters as in the `model.fit(...)`\n-  function\n-\n-When `lahead > 1`, the model input is preprocessed to a \"rolling window view\"\n-of the data, with the window length = `lahead`.\n-This is similar to sklearn's `view_as_windows`\n-with `window_shape` [being a single number.](\n-http://scikit-image.org/docs/0.10.x/api/skimage.util.html#view-as-windows)\n-\n-When `lahead < tsteps`, only the stateful LSTM converges because its\n-statefulness allows it to see beyond the capability that lahead\n-gave it to fit the n-point average. The stateless LSTM does not have\n-this capability, and hence is limited by its `lahead` parameter,\n-which is not sufficient to see the n-point average.\n-\n-When `lahead >= tsteps`, both the stateful and stateless LSTM converge.\n-'''\n-from __future__ import print_function\n-import numpy as np\n-import matplotlib.pyplot as plt\n-import pandas as pd\n-from keras.models import Sequential\n-from keras.layers import Dense, LSTM\n-\n-# ----------------------------------------------------------\n-# EDITABLE PARAMETERS\n-# Read the documentation in the script head for more details\n-# ----------------------------------------------------------\n-\n-# length of input\n-input_len = 1000\n-\n-# The window length of the moving average used to generate\n-# the output from the input in the input/output pair used\n-# to train the LSTM\n-# e.g. if tsteps=2 and input=[1, 2, 3, 4, 5],\n-#      then output=[1.5, 2.5, 3.5, 4.5]\n-tsteps = 2\n-\n-# The input sequence length that the LSTM is trained on for each output point\n-lahead = 1\n-\n-# training parameters passed to \"model.fit(...)\"\n-batch_size = 1\n-epochs = 10\n-\n-# ------------\n-# MAIN PROGRAM\n-# ------------\n-\n-print(\"*\" * 33)\n-if lahead >= tsteps:\n-    print(\"STATELESS LSTM WILL ALSO CONVERGE\")\n-else:\n-    print(\"STATELESS LSTM WILL NOT CONVERGE\")\n-print(\"*\" * 33)\n-\n-np.random.seed(1986)\n-\n-print('Generating Data...')\n-\n-\n-def gen_uniform_amp(amp=1, xn=10000):\n-    \"\"\"Generates uniform random data between\n-    -amp and +amp\n-    and of length xn\n-\n-    # Arguments\n-        amp: maximum/minimum range of uniform data\n-        xn: length of series\n-    \"\"\"\n-    data_input = np.random.uniform(-1 * amp, +1 * amp, xn)\n-    data_input = pd.DataFrame(data_input)\n-    return data_input\n-\n-# Since the output is a moving average of the input,\n-# the first few points of output will be NaN\n-# and will be dropped from the generated data\n-# before training the LSTM.\n-# Also, when lahead > 1,\n-# the preprocessing step later of \"rolling window view\"\n-# will also cause some points to be lost.\n-# For aesthetic reasons,\n-# in order to maintain generated data length = input_len after pre-processing,\n-# add a few points to account for the values that will be lost.\n-to_drop = max(tsteps - 1, lahead - 1)\n-data_input = gen_uniform_amp(amp=0.1, xn=input_len + to_drop)\n-\n-# set the target to be a N-point average of the input\n-expected_output = data_input.rolling(window=tsteps, center=False).mean()\n-\n-# when lahead > 1, need to convert the input to \"rolling window view\"\n-# https://docs.scipy.org/doc/numpy/reference/generated/numpy.repeat.html\n-if lahead > 1:\n-    data_input = np.repeat(data_input.values, repeats=lahead, axis=1)\n-    data_input = pd.DataFrame(data_input)\n-    for i, c in enumerate(data_input.columns):\n-        data_input[c] = data_input[c].shift(i)\n-\n-# drop the nan\n-expected_output = expected_output[to_drop:]\n-data_input = data_input[to_drop:]\n-\n-print('Input shape:', data_input.shape)\n-print('Output shape:', expected_output.shape)\n-print('Input head: ')\n-print(data_input.head())\n-print('Output head: ')\n-print(expected_output.head())\n-print('Input tail: ')\n-print(data_input.tail())\n-print('Output tail: ')\n-print(expected_output.tail())\n-\n-print('Plotting input and expected output')\n-plt.plot(data_input[0][:10], '.')\n-plt.plot(expected_output[0][:10], '-')\n-plt.legend(['Input', 'Expected output'])\n-plt.title('Input')\n-plt.show()\n-\n-\n-def create_model(stateful):\n-    model = Sequential()\n-    model.add(LSTM(20,\n-              input_shape=(lahead, 1),\n-              batch_size=batch_size,\n-              stateful=stateful))\n-    model.add(Dense(1))\n-    model.compile(loss='mse', optimizer='adam')\n-    return model\n-\n-print('Creating Stateful Model...')\n-model_stateful = create_model(stateful=True)\n-\n-\n-# split train/test data\n-def split_data(x, y, ratio=0.8):\n-    to_train = int(input_len * ratio)\n-    # tweak to match with batch_size\n-    to_train -= to_train % batch_size\n-\n-    x_train = x[:to_train]\n-    y_train = y[:to_train]\n-    x_test = x[to_train:]\n-    y_test = y[to_train:]\n-\n-    # tweak to match with batch_size\n-    to_drop = x.shape[0] % batch_size\n-    if to_drop > 0:\n-        x_test = x_test[:-1 * to_drop]\n-        y_test = y_test[:-1 * to_drop]\n-\n-    # some reshaping\n-    reshape_3 = lambda x: x.values.reshape((x.shape[0], x.shape[1], 1))\n-    x_train = reshape_3(x_train)\n-    x_test = reshape_3(x_test)\n-\n-    reshape_2 = lambda x: x.values.reshape((x.shape[0], 1))\n-    y_train = reshape_2(y_train)\n-    y_test = reshape_2(y_test)\n-\n-    return (x_train, y_train), (x_test, y_test)\n-\n-\n-(x_train, y_train), (x_test, y_test) = split_data(data_input, expected_output)\n-print('x_train.shape: ', x_train.shape)\n-print('y_train.shape: ', y_train.shape)\n-print('x_test.shape: ', x_test.shape)\n-print('y_test.shape: ', y_test.shape)\n-\n-print('Training')\n-for i in range(epochs):\n-    print('Epoch', i + 1, '/', epochs)\n-    # Note that the last state for sample i in a batch will\n-    # be used as initial state for sample i in the next batch.\n-    # Thus we are simultaneously training on batch_size series with\n-    # lower resolution than the original series contained in data_input.\n-    # Each of these series are offset by one step and can be\n-    # extracted with data_input[i::batch_size].\n-    model_stateful.fit(x_train,\n-                       y_train,\n-                       batch_size=batch_size,\n-                       epochs=1,\n-                       verbose=1,\n-                       validation_data=(x_test, y_test),\n-                       shuffle=False)\n-    model_stateful.reset_states()\n-\n-print('Predicting')\n-predicted_stateful = model_stateful.predict(x_test, batch_size=batch_size)\n-\n-print('Creating Stateless Model...')\n-model_stateless = create_model(stateful=False)\n-\n-print('Training')\n-model_stateless.fit(x_train,\n-                    y_train,\n-                    batch_size=batch_size,\n-                    epochs=epochs,\n-                    verbose=1,\n-                    validation_data=(x_test, y_test),\n-                    shuffle=False)\n-\n-print('Predicting')\n-predicted_stateless = model_stateless.predict(x_test, batch_size=batch_size)\n-\n-# ----------------------------\n-\n-print('Plotting Results')\n-plt.subplot(3, 1, 1)\n-plt.plot(y_test)\n-plt.title('Expected')\n-plt.subplot(3, 1, 2)\n-# drop the first \"tsteps-1\" because it is not possible to predict them\n-# since the \"previous\" timesteps to use do not exist\n-plt.plot((y_test - predicted_stateful).flatten()[tsteps - 1:])\n-plt.title('Stateful: Expected - Predicted')\n-plt.subplot(3, 1, 3)\n-plt.plot((y_test - predicted_stateless).flatten())\n-plt.title('Stateless: Expected - Predicted')\n-plt.show()\n\n@@ -1,112 +0,0 @@\n-'''\n-#Example script to generate text from Nietzsche's writings.\n-\n-At least 20 epochs are required before the generated text\n-starts sounding coherent.\n-\n-It is recommended to run this script on GPU, as recurrent\n-networks are quite computationally intensive.\n-\n-If you try this script on new data, make sure your corpus\n-has at least ~100k characters. ~1M is better.\n-'''\n-\n-from __future__ import print_function\n-from keras.callbacks import LambdaCallback\n-from keras.models import Sequential\n-from keras.layers import Dense\n-from keras.layers import LSTM\n-from keras.optimizers import RMSprop\n-from keras.utils.data_utils import get_file\n-import numpy as np\n-import random\n-import sys\n-import io\n-\n-path = get_file(\n-    'nietzsche.txt',\n-    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n-with io.open(path, encoding='utf-8') as f:\n-    text = f.read().lower()\n-print('corpus length:', len(text))\n-\n-chars = sorted(list(set(text)))\n-print('total chars:', len(chars))\n-char_indices = dict((c, i) for i, c in enumerate(chars))\n-indices_char = dict((i, c) for i, c in enumerate(chars))\n-\n-# cut the text in semi-redundant sequences of maxlen characters\n-maxlen = 40\n-step = 3\n-sentences = []\n-next_chars = []\n-for i in range(0, len(text) - maxlen, step):\n-    sentences.append(text[i: i + maxlen])\n-    next_chars.append(text[i + maxlen])\n-print('nb sequences:', len(sentences))\n-\n-print('Vectorization...')\n-x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n-y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n-for i, sentence in enumerate(sentences):\n-    for t, char in enumerate(sentence):\n-        x[i, t, char_indices[char]] = 1\n-    y[i, char_indices[next_chars[i]]] = 1\n-\n-\n-# build the model: a single LSTM\n-print('Build model...')\n-model = Sequential()\n-model.add(LSTM(128, input_shape=(maxlen, len(chars))))\n-model.add(Dense(len(chars), activation='softmax'))\n-\n-optimizer = RMSprop(learning_rate=0.01)\n-model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n-\n-\n-def sample(preds, temperature=1.0):\n-    # helper function to sample an index from a probability array\n-    preds = np.asarray(preds).astype('float64')\n-    preds = np.log(preds) / temperature\n-    exp_preds = np.exp(preds)\n-    preds = exp_preds / np.sum(exp_preds)\n-    probas = np.random.multinomial(1, preds, 1)\n-    return np.argmax(probas)\n-\n-\n-def on_epoch_end(epoch, _):\n-    # Function invoked at end of each epoch. Prints generated text.\n-    print()\n-    print('----- Generating text after Epoch: %d' % epoch)\n-\n-    start_index = random.randint(0, len(text) - maxlen - 1)\n-    for diversity in [0.2, 0.5, 1.0, 1.2]:\n-        print('----- diversity:', diversity)\n-\n-        generated = ''\n-        sentence = text[start_index: start_index + maxlen]\n-        generated += sentence\n-        print('----- Generating with seed: \"' + sentence + '\"')\n-        sys.stdout.write(generated)\n-\n-        for i in range(400):\n-            x_pred = np.zeros((1, maxlen, len(chars)))\n-            for t, char in enumerate(sentence):\n-                x_pred[0, t, char_indices[char]] = 1.\n-\n-            preds = model.predict(x_pred, verbose=0)[0]\n-            next_index = sample(preds, diversity)\n-            next_char = indices_char[next_index]\n-\n-            sentence = sentence[1:] + next_char\n-\n-            sys.stdout.write(next_char)\n-            sys.stdout.flush()\n-        print()\n-\n-print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n-\n-model.fit(x, y,\n-          batch_size=128,\n-          epochs=60,\n-          callbacks=[print_callback])\n\n@@ -1,347 +0,0 @@\n-# -*- coding: utf-8 -*-\n-\"\"\"\n-#Train an Auxiliary Classifier GAN (ACGAN) on the MNIST dataset.\n-\n-[More details on Auxiliary Classifier GANs.](https://arxiv.org/abs/1610.09585)\n-\n-You should start to see reasonable images after ~5 epochs, and good images\n-by ~15 epochs. You should use a GPU, as the convolution-heavy operations are\n-very slow on the CPU. Prefer the TensorFlow backend if you plan on iterating,\n-as the compilation time can be a blocker using Theano.\n-\n-Timings:\n-\n-Hardware           | Backend | Time / Epoch\n-:------------------|:--------|------------:\n- CPU               | TF      | 3 hrs\n- Titan X (maxwell) | TF      | 4 min\n- Titan X (maxwell) | TH      | 7 min\n-\n-Consult [Auxiliary Classifier Generative Adversarial Networks in Keras\n-](https://github.com/lukedeo/keras-acgan) for more information and example output.\n-\"\"\"\n-from __future__ import print_function\n-\n-from collections import defaultdict\n-try:\n-    import cPickle as pickle\n-except ImportError:\n-    import pickle\n-from PIL import Image\n-\n-from six.moves import range\n-\n-from keras.datasets import mnist\n-from keras import layers\n-from keras.layers import Input, Dense, Reshape, Flatten, Embedding, Dropout\n-from keras.layers import BatchNormalization\n-from keras.layers.advanced_activations import LeakyReLU\n-from keras.layers.convolutional import Conv2DTranspose, Conv2D\n-from keras.models import Sequential, Model\n-from keras.optimizers import Adam\n-from keras.utils.generic_utils import Progbar\n-import numpy as np\n-\n-np.random.seed(1337)\n-num_classes = 10\n-\n-\n-def build_generator(latent_size):\n-    # we will map a pair of (z, L), where z is a latent vector and L is a\n-    # label drawn from P_c, to image space (..., 28, 28, 1)\n-    cnn = Sequential()\n-\n-    cnn.add(Dense(3 * 3 * 384, input_dim=latent_size, activation='relu'))\n-    cnn.add(Reshape((3, 3, 384)))\n-\n-    # upsample to (7, 7, ...)\n-    cnn.add(Conv2DTranspose(192, 5, strides=1, padding='valid',\n-                            activation='relu',\n-                            kernel_initializer='glorot_normal'))\n-    cnn.add(BatchNormalization())\n-\n-    # upsample to (14, 14, ...)\n-    cnn.add(Conv2DTranspose(96, 5, strides=2, padding='same',\n-                            activation='relu',\n-                            kernel_initializer='glorot_normal'))\n-    cnn.add(BatchNormalization())\n-\n-    # upsample to (28, 28, ...)\n-    cnn.add(Conv2DTranspose(1, 5, strides=2, padding='same',\n-                            activation='tanh',\n-                            kernel_initializer='glorot_normal'))\n-\n-    # this is the z space commonly referred to in GAN papers\n-    latent = Input(shape=(latent_size, ))\n-\n-    # this will be our label\n-    image_class = Input(shape=(1,), dtype='int32')\n-\n-    cls = Embedding(num_classes, latent_size,\n-                    embeddings_initializer='glorot_normal')(image_class)\n-\n-    # hadamard product between z-space and a class conditional embedding\n-    h = layers.multiply([latent, cls])\n-\n-    fake_image = cnn(h)\n-\n-    return Model([latent, image_class], fake_image)\n-\n-\n-def build_discriminator():\n-    # build a relatively standard conv net, with LeakyReLUs as suggested in\n-    # the reference paper\n-    cnn = Sequential()\n-\n-    cnn.add(Conv2D(32, 3, padding='same', strides=2,\n-                   input_shape=(28, 28, 1)))\n-    cnn.add(LeakyReLU(0.2))\n-    cnn.add(Dropout(0.3))\n-\n-    cnn.add(Conv2D(64, 3, padding='same', strides=1))\n-    cnn.add(LeakyReLU(0.2))\n-    cnn.add(Dropout(0.3))\n-\n-    cnn.add(Conv2D(128, 3, padding='same', strides=2))\n-    cnn.add(LeakyReLU(0.2))\n-    cnn.add(Dropout(0.3))\n-\n-    cnn.add(Conv2D(256, 3, padding='same', strides=1))\n-    cnn.add(LeakyReLU(0.2))\n-    cnn.add(Dropout(0.3))\n-\n-    cnn.add(Flatten())\n-\n-    image = Input(shape=(28, 28, 1))\n-\n-    features = cnn(image)\n-\n-    # first output (name=generation) is whether or not the discriminator\n-    # thinks the image that is being shown is fake, and the second output\n-    # (name=auxiliary) is the class that the discriminator thinks the image\n-    # belongs to.\n-    fake = Dense(1, activation='sigmoid', name='generation')(features)\n-    aux = Dense(num_classes, activation='softmax', name='auxiliary')(features)\n-\n-    return Model(image, [fake, aux])\n-\n-\n-if __name__ == '__main__':\n-    # batch and latent size taken from the paper\n-    epochs = 100\n-    batch_size = 100\n-    latent_size = 100\n-\n-    # Adam parameters suggested in https://arxiv.org/abs/1511.06434\n-    adam_lr = 0.0002\n-    adam_beta_1 = 0.5\n-\n-    # build the discriminator\n-    print('Discriminator model:')\n-    discriminator = build_discriminator()\n-    discriminator.compile(\n-        optimizer=Adam(learning_rate=adam_lr, beta_1=adam_beta_1),\n-        loss=['binary_crossentropy', 'sparse_categorical_crossentropy']\n-    )\n-    discriminator.summary()\n-\n-    # build the generator\n-    generator = build_generator(latent_size)\n-\n-    latent = Input(shape=(latent_size, ))\n-    image_class = Input(shape=(1,), dtype='int32')\n-\n-    # get a fake image\n-    fake = generator([latent, image_class])\n-\n-    # we only want to be able to train generation for the combined model\n-    discriminator.trainable = False\n-    fake, aux = discriminator(fake)\n-    combined = Model([latent, image_class], [fake, aux])\n-\n-    print('Combined model:')\n-    combined.compile(\n-        optimizer=Adam(learning_rate=adam_lr, beta_1=adam_beta_1),\n-        loss=['binary_crossentropy', 'sparse_categorical_crossentropy']\n-    )\n-    combined.summary()\n-\n-    # get our mnist data, and force it to be of shape (..., 28, 28, 1) with\n-    # range [-1, 1]\n-    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n-    x_train = (x_train.astype(np.float32) - 127.5) / 127.5\n-    x_train = np.expand_dims(x_train, axis=-1)\n-\n-    x_test = (x_test.astype(np.float32) - 127.5) / 127.5\n-    x_test = np.expand_dims(x_test, axis=-1)\n-\n-    num_train, num_test = x_train.shape[0], x_test.shape[0]\n-\n-    train_history = defaultdict(list)\n-    test_history = defaultdict(list)\n-\n-    for epoch in range(1, epochs + 1):\n-        print('Epoch {}/{}'.format(epoch, epochs))\n-\n-        num_batches = int(np.ceil(x_train.shape[0] / float(batch_size)))\n-        progress_bar = Progbar(target=num_batches)\n-\n-        epoch_gen_loss = []\n-        epoch_disc_loss = []\n-\n-        for index in range(num_batches):\n-            # get a batch of real images\n-            image_batch = x_train[index * batch_size:(index + 1) * batch_size]\n-            label_batch = y_train[index * batch_size:(index + 1) * batch_size]\n-\n-            # generate a new batch of noise\n-            noise = np.random.uniform(-1, 1, (len(image_batch), latent_size))\n-\n-            # sample some labels from p_c\n-            sampled_labels = np.random.randint(0, num_classes, len(image_batch))\n-\n-            # generate a batch of fake images, using the generated labels as a\n-            # conditioner. We reshape the sampled labels to be\n-            # (len(image_batch), 1) so that we can feed them into the embedding\n-            # layer as a length one sequence\n-            generated_images = generator.predict(\n-                [noise, sampled_labels.reshape((-1, 1))], verbose=0)\n-\n-            x = np.concatenate((image_batch, generated_images))\n-\n-            # use one-sided soft real/fake labels\n-            # Salimans et al., 2016\n-            # https://arxiv.org/pdf/1606.03498.pdf (Section 3.4)\n-            soft_zero, soft_one = 0, 0.95\n-            y = np.array(\n-                [soft_one] * len(image_batch) + [soft_zero] * len(image_batch))\n-            aux_y = np.concatenate((label_batch, sampled_labels), axis=0)\n-\n-            # we don't want the discriminator to also maximize the classification\n-            # accuracy of the auxiliary classifier on generated images, so we\n-            # don't train discriminator to produce class labels for generated\n-            # images (see https://openreview.net/forum?id=rJXTf9Bxg).\n-            # To preserve sum of sample weights for the auxiliary classifier,\n-            # we assign sample weight of 2 to the real images.\n-            disc_sample_weight = [np.ones(2 * len(image_batch)),\n-                                  np.concatenate((np.ones(len(image_batch)) * 2,\n-                                                  np.zeros(len(image_batch))))]\n-\n-            # see if the discriminator can figure itself out...\n-            epoch_disc_loss.append(discriminator.train_on_batch(\n-                x, [y, aux_y], sample_weight=disc_sample_weight))\n-\n-            # make new noise. we generate 2 * batch size here such that we have\n-            # the generator optimize over an identical number of images as the\n-            # discriminator\n-            noise = np.random.uniform(-1, 1, (2 * len(image_batch), latent_size))\n-            sampled_labels = np.random.randint(0, num_classes, 2 * len(image_batch))\n-\n-            # we want to train the generator to trick the discriminator\n-            # For the generator, we want all the {fake, not-fake} labels to say\n-            # not-fake\n-            trick = np.ones(2 * len(image_batch)) * soft_one\n-\n-            epoch_gen_loss.append(combined.train_on_batch(\n-                [noise, sampled_labels.reshape((-1, 1))],\n-                [trick, sampled_labels]))\n-\n-            progress_bar.update(index + 1)\n-\n-        print('Testing for epoch {}:'.format(epoch))\n-\n-        # evaluate the testing loss here\n-\n-        # generate a new batch of noise\n-        noise = np.random.uniform(-1, 1, (num_test, latent_size))\n-\n-        # sample some labels from p_c and generate images from them\n-        sampled_labels = np.random.randint(0, num_classes, num_test)\n-        generated_images = generator.predict(\n-            [noise, sampled_labels.reshape((-1, 1))], verbose=False)\n-\n-        x = np.concatenate((x_test, generated_images))\n-        y = np.array([1] * num_test + [0] * num_test)\n-        aux_y = np.concatenate((y_test, sampled_labels), axis=0)\n-\n-        # see if the discriminator can figure itself out...\n-        discriminator_test_loss = discriminator.evaluate(\n-            x, [y, aux_y], verbose=False)\n-\n-        discriminator_train_loss = np.mean(np.array(epoch_disc_loss), axis=0)\n-\n-        # make new noise\n-        noise = np.random.uniform(-1, 1, (2 * num_test, latent_size))\n-        sampled_labels = np.random.randint(0, num_classes, 2 * num_test)\n-\n-        trick = np.ones(2 * num_test)\n-\n-        generator_test_loss = combined.evaluate(\n-            [noise, sampled_labels.reshape((-1, 1))],\n-            [trick, sampled_labels], verbose=False)\n-\n-        generator_train_loss = np.mean(np.array(epoch_gen_loss), axis=0)\n-\n-        # generate an epoch report on performance\n-        train_history['generator'].append(generator_train_loss)\n-        train_history['discriminator'].append(discriminator_train_loss)\n-\n-        test_history['generator'].append(generator_test_loss)\n-        test_history['discriminator'].append(discriminator_test_loss)\n-\n-        print('{0:<22s} | {1:4s} | {2:15s} | {3:5s}'.format(\n-            'component', *discriminator.metrics_names))\n-        print('-' * 65)\n-\n-        ROW_FMT = '{0:<22s} | {1:<4.2f} | {2:<15.4f} | {3:<5.4f}'\n-        print(ROW_FMT.format('generator (train)',\n-                             *train_history['generator'][-1]))\n-        print(ROW_FMT.format('generator (test)',\n-                             *test_history['generator'][-1]))\n-        print(ROW_FMT.format('discriminator (train)',\n-                             *train_history['discriminator'][-1]))\n-        print(ROW_FMT.format('discriminator (test)',\n-                             *test_history['discriminator'][-1]))\n-\n-        # save weights every epoch\n-        generator.save_weights(\n-            'params_generator_epoch_{0:03d}.hdf5'.format(epoch), True)\n-        discriminator.save_weights(\n-            'params_discriminator_epoch_{0:03d}.hdf5'.format(epoch), True)\n-\n-        # generate some digits to display\n-        num_rows = 40\n-        noise = np.tile(np.random.uniform(-1, 1, (num_rows, latent_size)),\n-                        (num_classes, 1))\n-\n-        sampled_labels = np.array([\n-            [i] * num_rows for i in range(num_classes)\n-        ]).reshape(-1, 1)\n-\n-        # get a batch to display\n-        generated_images = generator.predict(\n-            [noise, sampled_labels], verbose=0)\n-\n-        # prepare real images sorted by class label\n-        real_labels = y_train[(epoch - 1) * num_rows * num_classes:\n-                              epoch * num_rows * num_classes]\n-        indices = np.argsort(real_labels, axis=0)\n-        real_images = x_train[(epoch - 1) * num_rows * num_classes:\n-                              epoch * num_rows * num_classes][indices]\n-\n-        # display generated images, white separator, real images\n-        img = np.concatenate(\n-            (generated_images,\n-             np.repeat(np.ones_like(x_train[:1]), num_rows, axis=0),\n-             real_images))\n-\n-        # arrange them into a grid\n-        img = (np.concatenate([r.reshape(-1, 28)\n-                               for r in np.split(img, 2 * num_classes + 1)\n-                               ], axis=-1) * 127.5 + 127.5).astype(np.uint8)\n-\n-        Image.fromarray(img).save(\n-            'plot_epoch_{0:03d}_generated.png'.format(epoch))\n-\n-    with open('acgan-history.pkl', 'wb') as f:\n-        pickle.dump({'train': train_history, 'test': test_history}, f)\n\n@@ -1,70 +0,0 @@\n-'''Trains a simple convnet on the MNIST dataset.\n-\n-Gets to 99.25% test accuracy after 12 epochs\n-(there is still a lot of margin for parameter tuning).\n-16 seconds per epoch on a GRID K520 GPU.\n-'''\n-\n-from __future__ import print_function\n-import keras\n-from keras.datasets import mnist\n-from keras.models import Sequential\n-from keras.layers import Dense, Dropout, Flatten\n-from keras.layers import Conv2D, MaxPooling2D\n-from keras import backend as K\n-\n-batch_size = 128\n-num_classes = 10\n-epochs = 12\n-\n-# input image dimensions\n-img_rows, img_cols = 28, 28\n-\n-# the data, split between train and test sets\n-(x_train, y_train), (x_test, y_test) = mnist.load_data()\n-\n-if K.image_data_format() == 'channels_first':\n-    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n-    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n-    input_shape = (1, img_rows, img_cols)\n-else:\n-    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n-    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n-    input_shape = (img_rows, img_cols, 1)\n-\n-x_train = x_train.astype('float32')\n-x_test = x_test.astype('float32')\n-x_train /= 255\n-x_test /= 255\n-print('x_train shape:', x_train.shape)\n-print(x_train.shape[0], 'train samples')\n-print(x_test.shape[0], 'test samples')\n-\n-# convert class vectors to binary class matrices\n-y_train = keras.utils.to_categorical(y_train, num_classes)\n-y_test = keras.utils.to_categorical(y_test, num_classes)\n-\n-model = Sequential()\n-model.add(Conv2D(32, kernel_size=(3, 3),\n-                 activation='relu',\n-                 input_shape=input_shape))\n-model.add(Conv2D(64, (3, 3), activation='relu'))\n-model.add(MaxPooling2D(pool_size=(2, 2)))\n-model.add(Dropout(0.25))\n-model.add(Flatten())\n-model.add(Dense(128, activation='relu'))\n-model.add(Dropout(0.5))\n-model.add(Dense(num_classes, activation='softmax'))\n-\n-model.compile(loss=keras.losses.categorical_crossentropy,\n-              optimizer=keras.optimizers.Adadelta(),\n-              metrics=['accuracy'])\n-\n-model.fit(x_train, y_train,\n-          batch_size=batch_size,\n-          epochs=epochs,\n-          verbose=1,\n-          validation_data=(x_test, y_test))\n-score = model.evaluate(x_test, y_test, verbose=0)\n-print('Test loss:', score[0])\n-print('Test accuracy:', score[1])\n\n@@ -1,149 +0,0 @@\n-'''Trains a denoising autoencoder on MNIST dataset.\n-\n-Denoising is one of the classic applications of autoencoders.\n-The denoising process removes unwanted noise that corrupted the\n-true signal.\n-\n-Noise + Data ---> Denoising Autoencoder ---> Data\n-\n-Given a training dataset of corrupted data as input and\n-true signal as output, a denoising autoencoder can recover the\n-hidden structure to generate clean data.\n-\n-This example has modular design. The encoder, decoder and autoencoder\n-are 3 models that share weights. For example, after training the\n-autoencoder, the encoder can be used to  generate latent vectors\n-of input data for low-dim visualization like PCA or TSNE.\n-'''\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-import keras\n-from keras.layers import Activation, Dense, Input\n-from keras.layers import Conv2D, Flatten\n-from keras.layers import Reshape, Conv2DTranspose\n-from keras.models import Model\n-from keras import backend as K\n-from keras.datasets import mnist\n-import numpy as np\n-import matplotlib.pyplot as plt\n-from PIL import Image\n-\n-np.random.seed(1337)\n-\n-# MNIST dataset\n-(x_train, _), (x_test, _) = mnist.load_data()\n-\n-image_size = x_train.shape[1]\n-x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n-x_test = np.reshape(x_test, [-1, image_size, image_size, 1])\n-x_train = x_train.astype('float32') / 255\n-x_test = x_test.astype('float32') / 255\n-\n-# Generate corrupted MNIST images by adding noise with normal dist\n-# centered at 0.5 and std=0.5\n-noise = np.random.normal(loc=0.5, scale=0.5, size=x_train.shape)\n-x_train_noisy = x_train + noise\n-noise = np.random.normal(loc=0.5, scale=0.5, size=x_test.shape)\n-x_test_noisy = x_test + noise\n-\n-x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n-x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n-\n-# Network parameters\n-input_shape = (image_size, image_size, 1)\n-batch_size = 128\n-kernel_size = 3\n-latent_dim = 16\n-# Encoder/Decoder number of CNN layers and filters per layer\n-layer_filters = [32, 64]\n-\n-# Build the Autoencoder Model\n-# First build the Encoder Model\n-inputs = Input(shape=input_shape, name='encoder_input')\n-x = inputs\n-# Stack of Conv2D blocks\n-# Notes:\n-# 1) Use Batch Normalization before ReLU on deep networks\n-# 2) Use MaxPooling2D as alternative to strides>1\n-# - faster but not as good as strides>1\n-for filters in layer_filters:\n-    x = Conv2D(filters=filters,\n-               kernel_size=kernel_size,\n-               strides=2,\n-               activation='relu',\n-               padding='same')(x)\n-\n-# Shape info needed to build Decoder Model\n-shape = K.int_shape(x)\n-\n-# Generate the latent vector\n-x = Flatten()(x)\n-latent = Dense(latent_dim, name='latent_vector')(x)\n-\n-# Instantiate Encoder Model\n-encoder = Model(inputs, latent, name='encoder')\n-encoder.summary()\n-\n-# Build the Decoder Model\n-latent_inputs = Input(shape=(latent_dim,), name='decoder_input')\n-x = Dense(shape[1] * shape[2] * shape[3])(latent_inputs)\n-x = Reshape((shape[1], shape[2], shape[3]))(x)\n-\n-# Stack of Transposed Conv2D blocks\n-# Notes:\n-# 1) Use Batch Normalization before ReLU on deep networks\n-# 2) Use UpSampling2D as alternative to strides>1\n-# - faster but not as good as strides>1\n-for filters in layer_filters[::-1]:\n-    x = Conv2DTranspose(filters=filters,\n-                        kernel_size=kernel_size,\n-                        strides=2,\n-                        activation='relu',\n-                        padding='same')(x)\n-\n-x = Conv2DTranspose(filters=1,\n-                    kernel_size=kernel_size,\n-                    padding='same')(x)\n-\n-outputs = Activation('sigmoid', name='decoder_output')(x)\n-\n-# Instantiate Decoder Model\n-decoder = Model(latent_inputs, outputs, name='decoder')\n-decoder.summary()\n-\n-# Autoencoder = Encoder + Decoder\n-# Instantiate Autoencoder Model\n-autoencoder = Model(inputs, decoder(encoder(inputs)), name='autoencoder')\n-autoencoder.summary()\n-\n-autoencoder.compile(loss='mse', optimizer='adam')\n-\n-# Train the autoencoder\n-autoencoder.fit(x_train_noisy,\n-                x_train,\n-                validation_data=(x_test_noisy, x_test),\n-                epochs=30,\n-                batch_size=batch_size)\n-\n-# Predict the Autoencoder output from corrupted test images\n-x_decoded = autoencoder.predict(x_test_noisy)\n-\n-# Display the 1st 8 corrupted and denoised images\n-rows, cols = 10, 30\n-num = rows * cols\n-imgs = np.concatenate([x_test[:num], x_test_noisy[:num], x_decoded[:num]])\n-imgs = imgs.reshape((rows * 3, cols, image_size, image_size))\n-imgs = np.vstack(np.split(imgs, rows, axis=1))\n-imgs = imgs.reshape((rows * 3, -1, image_size, image_size))\n-imgs = np.vstack([np.hstack(i) for i in imgs])\n-imgs = (imgs * 255).astype(np.uint8)\n-plt.figure()\n-plt.axis('off')\n-plt.title('Original images: top rows, '\n-          'Corrupted Input: middle rows, '\n-          'Denoised Input:  third rows')\n-plt.imshow(imgs, interpolation='none', cmap='gray')\n-Image.fromarray(imgs).save('corrupted_and_denoised.png')\n-plt.show()\n\n@@ -1,98 +0,0 @@\n-\"\"\"Example of using Hierarchical RNN (HRNN) to classify MNIST digits.\n-\n-HRNNs can learn across multiple levels\n-of temporal hierarchy over a complex sequence.\n-Usually, the first recurrent layer of an HRNN\n-encodes a sentence (e.g. of word vectors)\n-into a  sentence vector.\n-The second recurrent layer then encodes a sequence of\n-such vectors (encoded by the first layer) into a document vector.\n-This document vector is considered to preserve both\n-the word-level and sentence-level structure of the context.\n-\n-# References\n-\n-- [A Hierarchical Neural Autoencoder for Paragraphs and Documents]\n-    (https://arxiv.org/abs/1506.01057)\n-    Encodes paragraphs and documents with HRNN.\n-    Results have shown that HRNN outperforms standard\n-    RNNs and may play some role in more sophisticated generation tasks like\n-    summarization or question answering.\n-- [Hierarchical recurrent neural network for skeleton based action recognition]\n-    (http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7298714)\n-    Achieved state-of-the-art results on\n-    skeleton based action recognition with 3 levels\n-    of bidirectional HRNN combined with fully connected layers.\n-\n-In the below MNIST example the first LSTM layer first encodes every\n-column of pixels of shape (28, 1) to a column vector of shape (128,).\n-The second LSTM layer encodes then these 28 column vectors of shape (28, 128)\n-to a image vector representing the whole image.\n-A final Dense layer is added for prediction.\n-\n-After 5 epochs: train acc: 0.9858, val acc: 0.9864\n-\"\"\"\n-from __future__ import print_function\n-\n-import keras\n-from keras.datasets import mnist\n-from keras.models import Model\n-from keras.layers import Input, Dense, TimeDistributed\n-from keras.layers import LSTM\n-\n-# Training parameters.\n-batch_size = 32\n-num_classes = 10\n-epochs = 5\n-\n-# Embedding dimensions.\n-row_hidden = 128\n-col_hidden = 128\n-\n-# The data, split between train and test sets.\n-(x_train, y_train), (x_test, y_test) = mnist.load_data()\n-\n-# Reshapes data to 4D for Hierarchical RNN.\n-x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n-x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n-x_train = x_train.astype('float32')\n-x_test = x_test.astype('float32')\n-x_train /= 255\n-x_test /= 255\n-print('x_train shape:', x_train.shape)\n-print(x_train.shape[0], 'train samples')\n-print(x_test.shape[0], 'test samples')\n-\n-# Converts class vectors to binary class matrices.\n-y_train = keras.utils.to_categorical(y_train, num_classes)\n-y_test = keras.utils.to_categorical(y_test, num_classes)\n-\n-row, col, pixel = x_train.shape[1:]\n-\n-# 4D input.\n-x = Input(shape=(row, col, pixel))\n-\n-# Encodes a row of pixels using TimeDistributed Wrapper.\n-encoded_rows = TimeDistributed(LSTM(row_hidden))(x)\n-\n-# Encodes columns of encoded rows.\n-encoded_columns = LSTM(col_hidden)(encoded_rows)\n-\n-# Final predictions and model.\n-prediction = Dense(num_classes, activation='softmax')(encoded_columns)\n-model = Model(x, prediction)\n-model.compile(loss='categorical_crossentropy',\n-              optimizer='rmsprop',\n-              metrics=['accuracy'])\n-\n-# Training.\n-model.fit(x_train, y_train,\n-          batch_size=batch_size,\n-          epochs=epochs,\n-          verbose=1,\n-          validation_data=(x_test, y_test))\n-\n-# Evaluation.\n-scores = model.evaluate(x_test, y_test, verbose=0)\n-print('Test loss:', scores[0])\n-print('Test accuracy:', scores[1])\n\n@@ -1,73 +0,0 @@\n-'''This is a reproduction of the IRNN experiment\n-with pixel-by-pixel sequential MNIST in\n-\"A Simple Way to Initialize Recurrent Networks of Rectified Linear Units\"\n-by Quoc V. Le, Navdeep Jaitly, Geoffrey E. Hinton\n-\n-arxiv:1504.00941v2 [cs.NE] 7 Apr 2015\n-http://arxiv.org/pdf/1504.00941v2.pdf\n-\n-Optimizer is replaced with RMSprop which yields more stable and steady\n-improvement.\n-\n-Reaches 0.93 train/test accuracy after 900 epochs\n-(which roughly corresponds to 1687500 steps in the original paper.)\n-'''\n-\n-from __future__ import print_function\n-\n-import keras\n-from keras.datasets import mnist\n-from keras.models import Sequential\n-from keras.layers import Dense, Activation\n-from keras.layers import SimpleRNN\n-from keras import initializers\n-from keras.optimizers import RMSprop\n-\n-batch_size = 32\n-num_classes = 10\n-epochs = 200\n-hidden_units = 100\n-\n-learning_rate = 1e-6\n-clip_norm = 1.0\n-\n-# the data, split between train and test sets\n-(x_train, y_train), (x_test, y_test) = mnist.load_data()\n-\n-x_train = x_train.reshape(x_train.shape[0], -1, 1)\n-x_test = x_test.reshape(x_test.shape[0], -1, 1)\n-x_train = x_train.astype('float32')\n-x_test = x_test.astype('float32')\n-x_train /= 255\n-x_test /= 255\n-print('x_train shape:', x_train.shape)\n-print(x_train.shape[0], 'train samples')\n-print(x_test.shape[0], 'test samples')\n-\n-# convert class vectors to binary class matrices\n-y_train = keras.utils.to_categorical(y_train, num_classes)\n-y_test = keras.utils.to_categorical(y_test, num_classes)\n-\n-print('Evaluate IRNN...')\n-model = Sequential()\n-model.add(SimpleRNN(hidden_units,\n-                    kernel_initializer=initializers.RandomNormal(stddev=0.001),\n-                    recurrent_initializer=initializers.Identity(gain=1.0),\n-                    activation='relu',\n-                    input_shape=x_train.shape[1:]))\n-model.add(Dense(num_classes))\n-model.add(Activation('softmax'))\n-rmsprop = RMSprop(learning_rate=learning_rate)\n-model.compile(loss='categorical_crossentropy',\n-              optimizer=rmsprop,\n-              metrics=['accuracy'])\n-\n-model.fit(x_train, y_train,\n-          batch_size=batch_size,\n-          epochs=epochs,\n-          verbose=1,\n-          validation_data=(x_test, y_test))\n-\n-scores = model.evaluate(x_test, y_test, verbose=0)\n-print('IRNN test score:', scores[0])\n-print('IRNN test accuracy:', scores[1])\n\n@@ -1,56 +0,0 @@\n-'''Trains a simple deep NN on the MNIST dataset.\n-\n-Gets to 98.40% test accuracy after 20 epochs\n-(there is *a lot* of margin for parameter tuning).\n-2 seconds per epoch on a K520 GPU.\n-'''\n-\n-from __future__ import print_function\n-\n-import keras\n-from keras.datasets import mnist\n-from keras.models import Sequential\n-from keras.layers import Dense, Dropout\n-from keras.optimizers import RMSprop\n-\n-batch_size = 128\n-num_classes = 10\n-epochs = 20\n-\n-# the data, split between train and test sets\n-(x_train, y_train), (x_test, y_test) = mnist.load_data()\n-\n-x_train = x_train.reshape(60000, 784)\n-x_test = x_test.reshape(10000, 784)\n-x_train = x_train.astype('float32')\n-x_test = x_test.astype('float32')\n-x_train /= 255\n-x_test /= 255\n-print(x_train.shape[0], 'train samples')\n-print(x_test.shape[0], 'test samples')\n-\n-# convert class vectors to binary class matrices\n-y_train = keras.utils.to_categorical(y_train, num_classes)\n-y_test = keras.utils.to_categorical(y_test, num_classes)\n-\n-model = Sequential()\n-model.add(Dense(512, activation='relu', input_shape=(784,)))\n-model.add(Dropout(0.2))\n-model.add(Dense(512, activation='relu'))\n-model.add(Dropout(0.2))\n-model.add(Dense(num_classes, activation='softmax'))\n-\n-model.summary()\n-\n-model.compile(loss='categorical_crossentropy',\n-              optimizer=RMSprop(),\n-              metrics=['accuracy'])\n-\n-history = model.fit(x_train, y_train,\n-                    batch_size=batch_size,\n-                    epochs=epochs,\n-                    verbose=1,\n-                    validation_data=(x_test, y_test))\n-score = model.evaluate(x_test, y_test, verbose=0)\n-print('Test loss:', score[0])\n-print('Test accuracy:', score[1])\n\n@@ -1,401 +0,0 @@\n-'''This is an implementation of Net2Net experiment with MNIST in\n-'Net2Net: Accelerating Learning via Knowledge Transfer'\n-by Tianqi Chen, Ian Goodfellow, and Jonathon Shlens\n-\n-arXiv:1511.05641v4 [cs.LG] 23 Apr 2016\n-http://arxiv.org/abs/1511.05641\n-\n-# Notes\n-\n-- What:\n-  + Net2Net is a group of methods to transfer knowledge from a teacher neural\n-    net to a student net,so that the student net can be trained faster than\n-    from scratch.\n-  + The paper discussed two specific methods of Net2Net, i.e. Net2WiderNet\n-    and Net2DeeperNet.\n-  + Net2WiderNet replaces a model with an equivalent wider model that has\n-    more units in each hidden layer.\n-  + Net2DeeperNet replaces a model with an equivalent deeper model.\n-  + Both are based on the idea of 'function-preserving transformations of\n-    neural nets'.\n-- Why:\n-  + Enable fast exploration of multiple neural nets in experimentation and\n-    design process,by creating a series of wider and deeper models with\n-    transferable knowledge.\n-  + Enable 'lifelong learning system' by gradually adjusting model complexity\n-    to data availability,and reusing transferable knowledge.\n-\n-# Experiments\n-\n-- Teacher model: a basic CNN model trained on MNIST for 3 epochs.\n-- Net2WiderNet experiment:\n-  + Student model has a wider Conv2D layer and a wider FC layer.\n-  + Comparison of 'random-padding' vs 'net2wider' weight initialization.\n-  + With both methods, after 1 epoch, student model should perform as well as\n-    teacher model, but 'net2wider' is slightly better.\n-- Net2DeeperNet experiment:\n-  + Student model has an extra Conv2D layer and an extra FC layer.\n-  + Comparison of 'random-init' vs 'net2deeper' weight initialization.\n-  + After 1 epoch, performance of 'net2deeper' is better than 'random-init'.\n-- Hyper-parameters:\n-  + SGD with momentum=0.9 is used for training teacher and student models.\n-  + Learning rate adjustment: it's suggested to reduce learning rate\n-    to 1/10 for student model.\n-  + Addition of noise in 'net2wider' is used to break weight symmetry\n-    and thus enable full capacity of student models. It is optional\n-    when a Dropout layer is used.\n-\n-# Results\n-\n-- Tested with TF backend and 'channels_last' image_data_format.\n-- Running on GPU GeForce GTX Titan X Maxwell\n-- Performance Comparisons - validation loss values during first 3 epochs:\n-\n-Teacher model ...\n-(0) teacher_model:             0.0537   0.0354   0.0356\n-\n-Experiment of Net2WiderNet ...\n-(1) wider_random_pad:          0.0320   0.0317   0.0289\n-(2) wider_net2wider:           0.0271   0.0274   0.0270\n-\n-Experiment of Net2DeeperNet ...\n-(3) deeper_random_init:        0.0682   0.0506   0.0468\n-(4) deeper_net2deeper:         0.0292   0.0294   0.0286\n-'''\n-\n-from __future__ import print_function\n-import numpy as np\n-import keras\n-from keras import backend as K\n-from keras.models import Sequential\n-from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n-from keras.optimizers import SGD\n-from keras.datasets import mnist\n-\n-if K.image_data_format() == 'channels_first':\n-    input_shape = (1, 28, 28)  # image shape\n-else:\n-    input_shape = (28, 28, 1)  # image shape\n-num_classes = 10  # number of classes\n-epochs = 3\n-\n-\n-# load and pre-process data\n-def preprocess_input(x):\n-    return x.astype('float32').reshape((-1,) + input_shape) / 255\n-\n-\n-def preprocess_output(y):\n-    return keras.utils.to_categorical(y)\n-\n-(x_train, y_train), (x_test, y_test) = mnist.load_data()\n-x_train, x_test = map(preprocess_input, [x_train, x_test])\n-y_train, y_test = map(preprocess_output, [y_train, y_test])\n-print('Loading MNIST data...')\n-print('x_train shape:', x_train.shape, 'y_train shape:', y_train.shape)\n-print('x_test shape:', x_test.shape, 'y_test shape', y_test.shape)\n-\n-\n-# knowledge transfer algorithms\n-def wider2net_conv2d(teacher_w1, teacher_b1, teacher_w2, new_width, init):\n-    '''Get initial weights for a wider conv2d layer with a bigger filters,\n-    by 'random-padding' or 'net2wider'.\n-\n-    # Arguments\n-        teacher_w1: `weight` of conv2d layer to become wider,\n-          of shape (filters1, num_channel1, kh1, kw1)\n-        teacher_b1: `bias` of conv2d layer to become wider,\n-          of shape (filters1, )\n-        teacher_w2: `weight` of next connected conv2d layer,\n-          of shape (filters2, num_channel2, kh2, kw2)\n-        new_width: new `filters` for the wider conv2d layer\n-        init: initialization algorithm for new weights,\n-          either 'random-pad' or 'net2wider'\n-    '''\n-    assert teacher_w1.shape[0] == teacher_w2.shape[1], (\n-        'successive layers from teacher model should have compatible shapes')\n-    assert teacher_w1.shape[3] == teacher_b1.shape[0], (\n-        'weight and bias from same layer should have compatible shapes')\n-    assert new_width > teacher_w1.shape[3], (\n-        'new width (filters) should be bigger than the existing one')\n-\n-    n = new_width - teacher_w1.shape[3]\n-    if init == 'random-pad':\n-        new_w1 = np.random.normal(0, 0.1, size=teacher_w1.shape[:3] + (n,))\n-        new_b1 = np.ones(n) * 0.1\n-        new_w2 = np.random.normal(\n-            0, 0.1,\n-            size=teacher_w2.shape[:2] + (n, teacher_w2.shape[3]))\n-    elif init == 'net2wider':\n-        index = np.random.randint(teacher_w1.shape[3], size=n)\n-        factors = np.bincount(index)[index] + 1.\n-        new_w1 = teacher_w1[:, :, :, index]\n-        new_b1 = teacher_b1[index]\n-        new_w2 = teacher_w2[:, :, index, :] / factors.reshape((1, 1, -1, 1))\n-    else:\n-        raise ValueError('Unsupported weight initializer: %s' % init)\n-\n-    student_w1 = np.concatenate((teacher_w1, new_w1), axis=3)\n-    if init == 'random-pad':\n-        student_w2 = np.concatenate((teacher_w2, new_w2), axis=2)\n-    elif init == 'net2wider':\n-        # add small noise to break symmetry, so that student model will have\n-        # full capacity later\n-        noise = np.random.normal(0, 5e-2 * new_w2.std(), size=new_w2.shape)\n-        student_w2 = np.concatenate((teacher_w2, new_w2 + noise), axis=2)\n-        student_w2[:, :, index, :] = new_w2\n-    student_b1 = np.concatenate((teacher_b1, new_b1), axis=0)\n-\n-    return student_w1, student_b1, student_w2\n-\n-\n-def wider2net_fc(teacher_w1, teacher_b1, teacher_w2, new_width, init):\n-    '''Get initial weights for a wider fully connected (dense) layer\n-       with a bigger nout, by 'random-padding' or 'net2wider'.\n-\n-    # Arguments\n-        teacher_w1: `weight` of fc layer to become wider,\n-          of shape (nin1, nout1)\n-        teacher_b1: `bias` of fc layer to become wider,\n-          of shape (nout1, )\n-        teacher_w2: `weight` of next connected fc layer,\n-          of shape (nin2, nout2)\n-        new_width: new `nout` for the wider fc layer\n-        init: initialization algorithm for new weights,\n-          either 'random-pad' or 'net2wider'\n-    '''\n-    assert teacher_w1.shape[1] == teacher_w2.shape[0], (\n-        'successive layers from teacher model should have compatible shapes')\n-    assert teacher_w1.shape[1] == teacher_b1.shape[0], (\n-        'weight and bias from same layer should have compatible shapes')\n-    assert new_width > teacher_w1.shape[1], (\n-        'new width (nout) should be bigger than the existing one')\n-\n-    n = new_width - teacher_w1.shape[1]\n-    if init == 'random-pad':\n-        new_w1 = np.random.normal(0, 0.1, size=(teacher_w1.shape[0], n))\n-        new_b1 = np.ones(n) * 0.1\n-        new_w2 = np.random.normal(0, 0.1, size=(n, teacher_w2.shape[1]))\n-    elif init == 'net2wider':\n-        index = np.random.randint(teacher_w1.shape[1], size=n)\n-        factors = np.bincount(index)[index] + 1.\n-        new_w1 = teacher_w1[:, index]\n-        new_b1 = teacher_b1[index]\n-        new_w2 = teacher_w2[index, :] / factors[:, np.newaxis]\n-    else:\n-        raise ValueError('Unsupported weight initializer: %s' % init)\n-\n-    student_w1 = np.concatenate((teacher_w1, new_w1), axis=1)\n-    if init == 'random-pad':\n-        student_w2 = np.concatenate((teacher_w2, new_w2), axis=0)\n-    elif init == 'net2wider':\n-        # add small noise to break symmetry, so that student model will have\n-        # full capacity later\n-        noise = np.random.normal(0, 5e-2 * new_w2.std(), size=new_w2.shape)\n-        student_w2 = np.concatenate((teacher_w2, new_w2 + noise), axis=0)\n-        student_w2[index, :] = new_w2\n-    student_b1 = np.concatenate((teacher_b1, new_b1), axis=0)\n-\n-    return student_w1, student_b1, student_w2\n-\n-\n-def deeper2net_conv2d(teacher_w):\n-    '''Get initial weights for a deeper conv2d layer by net2deeper'.\n-\n-    # Arguments\n-        teacher_w: `weight` of previous conv2d layer,\n-          of shape (kh, kw, num_channel, filters)\n-    '''\n-    kh, kw, num_channel, filters = teacher_w.shape\n-    student_w = np.zeros_like(teacher_w)\n-    for i in range(filters):\n-        student_w[(kh - 1) // 2, (kw - 1) // 2, i, i] = 1.\n-    student_b = np.zeros(filters)\n-    return student_w, student_b\n-\n-\n-def copy_weights(teacher_model, student_model, layer_names):\n-    '''Copy weights from teacher_model to student_model,\n-     for layers with names listed in layer_names\n-    '''\n-    for name in layer_names:\n-        weights = teacher_model.get_layer(name=name).get_weights()\n-        student_model.get_layer(name=name).set_weights(weights)\n-\n-\n-# methods to construct teacher_model and student_models\n-def make_teacher_model(x_train, y_train,\n-                       x_test, y_test,\n-                       epochs):\n-    '''Train and benchmark performance of a simple CNN.\n-    (0) Teacher model\n-    '''\n-    model = Sequential()\n-    model.add(Conv2D(64, 3, input_shape=input_shape,\n-                     padding='same', name='conv1'))\n-    model.add(MaxPooling2D(2, name='pool1'))\n-    model.add(Conv2D(64, 3, padding='same', name='conv2'))\n-    model.add(MaxPooling2D(2, name='pool2'))\n-    model.add(Flatten(name='flatten'))\n-    model.add(Dense(64, activation='relu', name='fc1'))\n-    model.add(Dense(num_classes, activation='softmax', name='fc2'))\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer=SGD(learning_rate=0.01, momentum=0.9),\n-                  metrics=['accuracy'])\n-\n-    model.fit(x_train, y_train,\n-              epochs=epochs,\n-              validation_data=(x_test, y_test))\n-    return model\n-\n-\n-def make_wider_student_model(teacher_model,\n-                             x_train, y_train,\n-                             x_test, y_test,\n-                             init, epochs):\n-    '''Train a wider student model based on teacher_model,\n-       with either 'random-pad' (baseline) or 'net2wider'\n-    '''\n-    new_conv1_width = 128\n-    new_fc1_width = 128\n-\n-    model = Sequential()\n-    # a wider conv1 compared to teacher_model\n-    model.add(Conv2D(new_conv1_width, 3, input_shape=input_shape,\n-                     padding='same', name='conv1'))\n-    model.add(MaxPooling2D(2, name='pool1'))\n-    model.add(Conv2D(64, 3, padding='same', name='conv2'))\n-    model.add(MaxPooling2D(2, name='pool2'))\n-    model.add(Flatten(name='flatten'))\n-    # a wider fc1 compared to teacher model\n-    model.add(Dense(new_fc1_width, activation='relu', name='fc1'))\n-    model.add(Dense(num_classes, activation='softmax', name='fc2'))\n-\n-    # The weights for other layers need to be copied from teacher_model\n-    # to student_model, except for widened layers\n-    # and their immediate downstreams, which will be initialized separately.\n-    # For this example there are no other layers that need to be copied.\n-\n-    w_conv1, b_conv1 = teacher_model.get_layer('conv1').get_weights()\n-    w_conv2, b_conv2 = teacher_model.get_layer('conv2').get_weights()\n-    new_w_conv1, new_b_conv1, new_w_conv2 = wider2net_conv2d(\n-        w_conv1, b_conv1, w_conv2, new_conv1_width, init)\n-    model.get_layer('conv1').set_weights([new_w_conv1, new_b_conv1])\n-    model.get_layer('conv2').set_weights([new_w_conv2, b_conv2])\n-\n-    w_fc1, b_fc1 = teacher_model.get_layer('fc1').get_weights()\n-    w_fc2, b_fc2 = teacher_model.get_layer('fc2').get_weights()\n-    new_w_fc1, new_b_fc1, new_w_fc2 = wider2net_fc(\n-        w_fc1, b_fc1, w_fc2, new_fc1_width, init)\n-    model.get_layer('fc1').set_weights([new_w_fc1, new_b_fc1])\n-    model.get_layer('fc2').set_weights([new_w_fc2, b_fc2])\n-\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer=SGD(learning_rate=0.001, momentum=0.9),\n-                  metrics=['accuracy'])\n-\n-    model.fit(x_train, y_train,\n-              epochs=epochs,\n-              validation_data=(x_test, y_test))\n-\n-\n-def make_deeper_student_model(teacher_model,\n-                              x_train, y_train,\n-                              x_test, y_test,\n-                              init, epochs):\n-    '''Train a deeper student model based on teacher_model,\n-       with either 'random-init' (baseline) or 'net2deeper'\n-    '''\n-    model = Sequential()\n-    model.add(Conv2D(64, 3, input_shape=input_shape,\n-                     padding='same', name='conv1'))\n-    model.add(MaxPooling2D(2, name='pool1'))\n-    model.add(Conv2D(64, 3, padding='same', name='conv2'))\n-    # add another conv2d layer to make original conv2 deeper\n-    if init == 'net2deeper':\n-        prev_w, _ = model.get_layer('conv2').get_weights()\n-        new_weights = deeper2net_conv2d(prev_w)\n-        model.add(Conv2D(64, 3, padding='same',\n-                         name='conv2-deeper', weights=new_weights))\n-    elif init == 'random-init':\n-        model.add(Conv2D(64, 3, padding='same', name='conv2-deeper'))\n-    else:\n-        raise ValueError('Unsupported weight initializer: %s' % init)\n-    model.add(MaxPooling2D(2, name='pool2'))\n-    model.add(Flatten(name='flatten'))\n-    model.add(Dense(64, activation='relu', name='fc1'))\n-    # add another fc layer to make original fc1 deeper\n-    if init == 'net2deeper':\n-        # net2deeper for fc layer with relu, is just an identity initializer\n-        model.add(Dense(64, kernel_initializer='identity',\n-                        activation='relu', name='fc1-deeper'))\n-    elif init == 'random-init':\n-        model.add(Dense(64, activation='relu', name='fc1-deeper'))\n-    else:\n-        raise ValueError('Unsupported weight initializer: %s' % init)\n-    model.add(Dense(num_classes, activation='softmax', name='fc2'))\n-\n-    # copy weights for other layers\n-    copy_weights(teacher_model, model, layer_names=[\n-                 'conv1', 'conv2', 'fc1', 'fc2'])\n-\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer=SGD(learning_rate=0.001, momentum=0.9),\n-                  metrics=['accuracy'])\n-\n-    model.fit(x_train, y_train,\n-              epochs=epochs,\n-              validation_data=(x_test, y_test))\n-\n-\n-# experiments setup\n-def net2wider_experiment():\n-    '''Benchmark performances of\n-    (1) a wider student model with `random_pad` initializer\n-    (2) a wider student model with `Net2WiderNet` initializer\n-    '''\n-    print('\\nExperiment of Net2WiderNet ...')\n-\n-    print('\\n(1) building wider student model by random padding ...')\n-    make_wider_student_model(teacher_model,\n-                             x_train, y_train,\n-                             x_test, y_test,\n-                             init='random-pad',\n-                             epochs=epochs)\n-    print('\\n(2) building wider student model by net2wider ...')\n-    make_wider_student_model(teacher_model,\n-                             x_train, y_train,\n-                             x_test, y_test,\n-                             init='net2wider',\n-                             epochs=epochs)\n-\n-\n-def net2deeper_experiment():\n-    '''Benchmark performances of\n-    (3) a deeper student model with `random_init` initializer\n-    (4) a deeper student model with `Net2DeeperNet` initializer\n-    '''\n-    print('\\nExperiment of Net2DeeperNet ...')\n-\n-    print('\\n(3) building deeper student model by random init ...')\n-    make_deeper_student_model(teacher_model,\n-                              x_train, y_train,\n-                              x_test, y_test,\n-                              init='random-init',\n-                              epochs=epochs)\n-    print('\\n(4) building deeper student model by net2deeper ...')\n-    make_deeper_student_model(teacher_model,\n-                              x_train, y_train,\n-                              x_test, y_test,\n-                              init='net2deeper',\n-                              epochs=epochs)\n-\n-\n-print('\\n(0) building teacher model ...')\n-teacher_model = make_teacher_model(x_train, y_train,\n-                                   x_test, y_test,\n-                                   epochs=epochs)\n-\n-# run the experiments\n-net2wider_experiment()\n-net2deeper_experiment()\n\n@@ -1,143 +0,0 @@\n-'''Trains a Siamese MLP on pairs of digits from the MNIST dataset.\n-\n-It follows Hadsell-et-al.'06 [1] by computing the Euclidean distance on the\n-output of the shared network and by optimizing the contrastive loss (see paper\n-for more details).\n-\n-# References\n-\n-- Dimensionality Reduction by Learning an Invariant Mapping\n-    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n-\n-Gets to 97.2% test accuracy after 20 epochs.\n-2 seconds per epoch on a Titan X Maxwell GPU\n-'''\n-from __future__ import absolute_import\n-from __future__ import print_function\n-import numpy as np\n-\n-import random\n-from keras.datasets import mnist\n-from keras.models import Model\n-from keras.layers import Input, Flatten, Dense, Dropout, Lambda\n-from keras.optimizers import RMSprop\n-from keras import backend as K\n-\n-num_classes = 10\n-epochs = 20\n-\n-\n-def euclidean_distance(vects):\n-    x, y = vects\n-    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n-    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n-\n-\n-def eucl_dist_output_shape(shapes):\n-    shape1, shape2 = shapes\n-    return (shape1[0], 1)\n-\n-\n-def contrastive_loss(y_true, y_pred):\n-    '''Contrastive loss from Hadsell-et-al.'06\n-    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n-    '''\n-    margin = 1\n-    square_pred = K.square(y_pred)\n-    margin_square = K.square(K.maximum(margin - y_pred, 0))\n-    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)\n-\n-\n-def create_pairs(x, digit_indices):\n-    '''Positive and negative pair creation.\n-    Alternates between positive and negative pairs.\n-    '''\n-    pairs = []\n-    labels = []\n-    n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1\n-    for d in range(num_classes):\n-        for i in range(n):\n-            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n-            pairs += [[x[z1], x[z2]]]\n-            inc = random.randrange(1, num_classes)\n-            dn = (d + inc) % num_classes\n-            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n-            pairs += [[x[z1], x[z2]]]\n-            labels += [1, 0]\n-    return np.array(pairs), np.array(labels)\n-\n-\n-def create_base_network(input_shape):\n-    '''Base network to be shared (eq. to feature extraction).\n-    '''\n-    input = Input(shape=input_shape)\n-    x = Flatten()(input)\n-    x = Dense(128, activation='relu')(x)\n-    x = Dropout(0.1)(x)\n-    x = Dense(128, activation='relu')(x)\n-    x = Dropout(0.1)(x)\n-    x = Dense(128, activation='relu')(x)\n-    return Model(input, x)\n-\n-\n-def compute_accuracy(y_true, y_pred):\n-    '''Compute classification accuracy with a fixed threshold on distances.\n-    '''\n-    pred = y_pred.ravel() < 0.5\n-    return np.mean(pred == y_true)\n-\n-\n-def accuracy(y_true, y_pred):\n-    '''Compute classification accuracy with a fixed threshold on distances.\n-    '''\n-    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))\n-\n-\n-# the data, split between train and test sets\n-(x_train, y_train), (x_test, y_test) = mnist.load_data()\n-x_train = x_train.astype('float32')\n-x_test = x_test.astype('float32')\n-x_train /= 255\n-x_test /= 255\n-input_shape = x_train.shape[1:]\n-\n-# create training+test positive and negative pairs\n-digit_indices = [np.where(y_train == i)[0] for i in range(num_classes)]\n-tr_pairs, tr_y = create_pairs(x_train, digit_indices)\n-\n-digit_indices = [np.where(y_test == i)[0] for i in range(num_classes)]\n-te_pairs, te_y = create_pairs(x_test, digit_indices)\n-\n-# network definition\n-base_network = create_base_network(input_shape)\n-\n-input_a = Input(shape=input_shape)\n-input_b = Input(shape=input_shape)\n-\n-# because we re-use the same instance `base_network`,\n-# the weights of the network\n-# will be shared across the two branches\n-processed_a = base_network(input_a)\n-processed_b = base_network(input_b)\n-\n-distance = Lambda(euclidean_distance,\n-                  output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n-\n-model = Model([input_a, input_b], distance)\n-\n-# train\n-rms = RMSprop()\n-model.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy])\n-model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n-          batch_size=128,\n-          epochs=epochs,\n-          validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y))\n-\n-# compute final accuracy on training and test sets\n-y_pred = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\n-tr_acc = compute_accuracy(tr_y, y_pred)\n-y_pred = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\n-te_acc = compute_accuracy(te_y, y_pred)\n-\n-print('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\n-print('* Accuracy on test set: %0.2f%%' % (100 * te_acc))\n\n@@ -1,102 +0,0 @@\n-'''Example of how to use sklearn wrapper\n-\n-Builds simple CNN models on MNIST and uses sklearn's GridSearchCV to find best model\n-'''\n-\n-from __future__ import print_function\n-\n-import keras\n-from keras.datasets import mnist\n-from keras.models import Sequential\n-from keras.layers import Dense, Dropout, Activation, Flatten\n-from keras.layers import Conv2D, MaxPooling2D\n-from keras.wrappers.scikit_learn import KerasClassifier\n-from keras import backend as K\n-from sklearn.model_selection import GridSearchCV\n-\n-\n-num_classes = 10\n-\n-# input image dimensions\n-img_rows, img_cols = 28, 28\n-\n-# load training data and do basic data normalization\n-(x_train, y_train), (x_test, y_test) = mnist.load_data()\n-\n-if K.image_data_format() == 'channels_first':\n-    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n-    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n-    input_shape = (1, img_rows, img_cols)\n-else:\n-    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n-    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n-    input_shape = (img_rows, img_cols, 1)\n-\n-x_train = x_train.astype('float32')\n-x_test = x_test.astype('float32')\n-x_train /= 255\n-x_test /= 255\n-\n-# convert class vectors to binary class matrices\n-y_train = keras.utils.to_categorical(y_train, num_classes)\n-y_test = keras.utils.to_categorical(y_test, num_classes)\n-\n-\n-def make_model(dense_layer_sizes, filters, kernel_size, pool_size):\n-    '''Creates model comprised of 2 convolutional layers followed by dense layers\n-\n-    dense_layer_sizes: List of layer sizes.\n-        This list has one number for each layer\n-    filters: Number of convolutional filters in each convolutional layer\n-    kernel_size: Convolutional kernel size\n-    pool_size: Size of pooling area for max pooling\n-    '''\n-\n-    model = Sequential()\n-    model.add(Conv2D(filters, kernel_size,\n-                     padding='valid',\n-                     input_shape=input_shape))\n-    model.add(Activation('relu'))\n-    model.add(Conv2D(filters, kernel_size))\n-    model.add(Activation('relu'))\n-    model.add(MaxPooling2D(pool_size=pool_size))\n-    model.add(Dropout(0.25))\n-\n-    model.add(Flatten())\n-    for layer_size in dense_layer_sizes:\n-        model.add(Dense(layer_size))\n-        model.add(Activation('relu'))\n-    model.add(Dropout(0.5))\n-    model.add(Dense(num_classes))\n-    model.add(Activation('softmax'))\n-\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer='adadelta',\n-                  metrics=['accuracy'])\n-    return model\n-\n-\n-dense_size_candidates = [[32], [64], [32, 32], [64, 64]]\n-my_classifier = KerasClassifier(make_model, batch_size=32)\n-validator = GridSearchCV(my_classifier,\n-                         param_grid={'dense_layer_sizes': dense_size_candidates,\n-                                     # epochs is avail for tuning even when not\n-                                     # an argument to model building function\n-                                     'epochs': [3, 6],\n-                                     'filters': [8],\n-                                     'kernel_size': [3],\n-                                     'pool_size': [2]},\n-                         scoring='neg_log_loss',\n-                         n_jobs=1)\n-validator.fit(x_train, y_train)\n-\n-print('The parameters of the best model are: ')\n-print(validator.best_params_)\n-\n-# validator.best_estimator_ returns sklearn-wrapped version of best model.\n-# validator.best_estimator_.model returns the (unwrapped) keras model\n-best_model = validator.best_estimator_.model\n-metric_names = best_model.metrics_names\n-metric_values = best_model.evaluate(x_test, y_test)\n-for metric, value in zip(metric_names, metric_values):\n-    print(metric, ': ', value)\n\n@@ -1,190 +0,0 @@\n-'''Trains a stacked what-where autoencoder built on residual blocks on the\r\n-MNIST dataset. It exemplifies two influential methods that have been developed\r\n-in the past few years.\r\n-\r\n-The first is the idea of properly 'unpooling.' During any max pool, the\r\n-exact location (the 'where') of the maximal value in a pooled receptive field\r\n-is lost, however it can be very useful in the overall reconstruction of an\r\n-input image. Therefore, if the 'where' is handed from the encoder\r\n-to the corresponding decoder layer, features being decoded can be 'placed' in\r\n-the right location, allowing for reconstructions of much higher fidelity.\r\n-\r\n-# References\r\n-\r\n-- Visualizing and Understanding Convolutional Networks\r\n-  Matthew D Zeiler, Rob Fergus\r\n-  https://arxiv.org/abs/1311.2901v3\r\n-- Stacked What-Where Auto-encoders\r\n-  Junbo Zhao, Michael Mathieu, Ross Goroshin, Yann LeCun\r\n-  https://arxiv.org/abs/1506.02351v8\r\n-\r\n-The second idea exploited here is that of residual learning. Residual blocks\r\n-ease the training process by allowing skip connections that give the network\r\n-the ability to be as linear (or non-linear) as the data sees fit.  This allows\r\n-for much deep networks to be easily trained. The residual element seems to\r\n-be advantageous in the context of this example as it allows a nice symmetry\r\n-between the encoder and decoder. Normally, in the decoder, the final\r\n-projection to the space where the image is reconstructed is linear, however\r\n-this does not have to be the case for a residual block as the degree to which\r\n-its output is linear or non-linear is determined by the data it is fed.\r\n-However, in order to cap the reconstruction in this example, a hard softmax is\r\n-applied as a bias because we know the MNIST digits are mapped to [0, 1].\r\n-\r\n-# References\r\n-- Deep Residual Learning for Image Recognition\r\n-  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\r\n-  https://arxiv.org/abs/1512.03385v1\r\n-- Identity Mappings in Deep Residual Networks\r\n-  Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\r\n-  https://arxiv.org/abs/1603.05027v3\r\n-'''\r\n-from __future__ import print_function\r\n-import numpy as np\r\n-\r\n-from keras.datasets import mnist\r\n-from keras.models import Model\r\n-from keras.layers import Activation\r\n-from keras.layers import UpSampling2D, Conv2D, MaxPooling2D\r\n-from keras.layers import Input, BatchNormalization, ELU\r\n-import matplotlib.pyplot as plt\r\n-import keras.backend as K\r\n-from keras import layers\r\n-\r\n-\r\n-def convresblock(x, nfeats=8, ksize=3, nskipped=2, elu=True):\r\n-    \"\"\"The proposed residual block from [4].\r\n-\r\n-    Running with elu=True will use ELU nonlinearity and running with\r\n-    elu=False will use BatchNorm + RELU nonlinearity.  While ELU's are fast\r\n-    due to the fact they do not suffer from BatchNorm overhead, they may\r\n-    overfit because they do not offer the stochastic element of the batch\r\n-    formation process of BatchNorm, which acts as a good regularizer.\r\n-\r\n-    # Arguments\r\n-        x: 4D tensor, the tensor to feed through the block\r\n-        nfeats: Integer, number of feature maps for conv layers.\r\n-        ksize: Integer, width and height of conv kernels in first convolution.\r\n-        nskipped: Integer, number of conv layers for the residual function.\r\n-        elu: Boolean, whether to use ELU or BN+RELU.\r\n-\r\n-    # Input shape\r\n-        4D tensor with shape:\r\n-        `(batch, channels, rows, cols)`\r\n-\r\n-    # Output shape\r\n-        4D tensor with shape:\r\n-        `(batch, filters, rows, cols)`\r\n-    \"\"\"\r\n-    y0 = Conv2D(nfeats, ksize, padding='same')(x)\r\n-    y = y0\r\n-    for i in range(nskipped):\r\n-        if elu:\r\n-            y = ELU()(y)\r\n-        else:\r\n-            y = BatchNormalization(axis=1)(y)\r\n-            y = Activation('relu')(y)\r\n-        y = Conv2D(nfeats, 1, padding='same')(y)\r\n-    return layers.add([y0, y])\r\n-\r\n-\r\n-def getwhere(x):\r\n-    ''' Calculate the 'where' mask that contains switches indicating which\r\n-    index contained the max value when MaxPool2D was applied.  Using the\r\n-    gradient of the sum is a nice trick to keep everything high level.'''\r\n-    y_prepool, y_postpool = x\r\n-    return K.gradients(K.sum(y_postpool), y_prepool)\r\n-\r\n-# This example assume 'channels_first' data format.\r\n-K.set_image_data_format('channels_first')\r\n-\r\n-# input image dimensions\r\n-img_rows, img_cols = 28, 28\r\n-\r\n-# the data, split between train and test sets\r\n-(x_train, _), (x_test, _) = mnist.load_data()\r\n-\r\n-x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\n-x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\r\n-x_train = x_train.astype('float32')\r\n-x_test = x_test.astype('float32')\r\n-x_train /= 255\r\n-x_test /= 255\r\n-print('x_train shape:', x_train.shape)\r\n-print(x_train.shape[0], 'train samples')\r\n-print(x_test.shape[0], 'test samples')\r\n-\r\n-# The size of the kernel used for the MaxPooling2D\r\n-pool_size = 2\r\n-# The total number of feature maps at each layer\r\n-nfeats = [8, 16, 32, 64, 128]\r\n-# The sizes of the pooling kernel at each layer\r\n-pool_sizes = np.array([1, 1, 1, 1, 1]) * pool_size\r\n-# The convolution kernel size\r\n-ksize = 3\r\n-# Number of epochs to train for\r\n-epochs = 5\r\n-# Batch size during training\r\n-batch_size = 128\r\n-\r\n-if pool_size == 2:\r\n-    # if using a 5 layer net of pool_size = 2\r\n-    x_train = np.pad(x_train, [[0, 0], [0, 0], [2, 2], [2, 2]],\r\n-                     mode='constant')\r\n-    x_test = np.pad(x_test, [[0, 0], [0, 0], [2, 2], [2, 2]], mode='constant')\r\n-    nlayers = 5\r\n-elif pool_size == 3:\r\n-    # if using a 3 layer net of pool_size = 3\r\n-    x_train = x_train[:, :, :-1, :-1]\r\n-    x_test = x_test[:, :, :-1, :-1]\r\n-    nlayers = 3\r\n-else:\r\n-    import sys\r\n-    sys.exit('Script supports pool_size of 2 and 3.')\r\n-\r\n-# Shape of input to train on (note that model is fully convolutional however)\r\n-input_shape = x_train.shape[1:]\r\n-# The final list of the size of axis=1 for all layers, including input\r\n-nfeats_all = [input_shape[0]] + nfeats\r\n-\r\n-# First build the encoder, all the while keeping track of the 'where' masks\r\n-img_input = Input(shape=input_shape)\r\n-\r\n-# We push the 'where' masks to the following list\r\n-wheres = [None] * nlayers\r\n-y = img_input\r\n-for i in range(nlayers):\r\n-    y_prepool = convresblock(y, nfeats=nfeats_all[i + 1], ksize=ksize)\r\n-    y = MaxPooling2D(pool_size=(pool_sizes[i], pool_sizes[i]))(y_prepool)\r\n-    wheres[i] = layers.Lambda(\r\n-        getwhere, output_shape=lambda x: x[0])([y_prepool, y])\r\n-\r\n-# Now build the decoder, and use the stored 'where' masks to place the features\r\n-for i in range(nlayers):\r\n-    ind = nlayers - 1 - i\r\n-    y = UpSampling2D(size=(pool_sizes[ind], pool_sizes[ind]))(y)\r\n-    y = layers.multiply([y, wheres[ind]])\r\n-    y = convresblock(y, nfeats=nfeats_all[ind], ksize=ksize)\r\n-\r\n-# Use hard_simgoid to clip range of reconstruction\r\n-y = Activation('hard_sigmoid')(y)\r\n-\r\n-# Define the model and it's mean square error loss, and compile it with Adam\r\n-model = Model(img_input, y)\r\n-model.compile('adam', 'mse')\r\n-\r\n-# Fit the model\r\n-model.fit(x_train, x_train,\r\n-          batch_size=batch_size,\r\n-          epochs=epochs,\r\n-          validation_data=(x_test, x_test))\r\n-\r\n-# Plot\r\n-x_recon = model.predict(x_test[:25])\r\n-x_plot = np.concatenate((x_test[:25], x_recon), axis=1)\r\n-x_plot = x_plot.reshape((5, 10, input_shape[-2], input_shape[-1]))\r\n-x_plot = np.vstack([np.hstack(x) for x in x_plot])\r\n-plt.figure()\r\n-plt.axis('off')\r\n-plt.title('Test Samples: Originals/Reconstructions')\r\n-plt.imshow(x_plot, interpolation='none', cmap='gray')\r\n-plt.savefig('reconstructions.png')\r\n\n@@ -1,124 +0,0 @@\n-'''Transfer learning toy example.\n-\n-1 - Train a simple convnet on the MNIST dataset the first 5 digits [0..4].\n-2 - Freeze convolutional layers and fine-tune dense layers\n-   for the classification of digits [5..9].\n-\n-Get to 99.8% test accuracy after 5 epochs\n-for the first five digits classifier\n-and 99.2% for the last five digits after transfer + fine-tuning.\n-'''\n-\n-from __future__ import print_function\n-\n-import datetime\n-import keras\n-from keras.datasets import mnist\n-from keras.models import Sequential\n-from keras.layers import Dense, Dropout, Activation, Flatten\n-from keras.layers import Conv2D, MaxPooling2D\n-from keras import backend as K\n-\n-now = datetime.datetime.now\n-\n-batch_size = 128\n-num_classes = 5\n-epochs = 5\n-\n-# input image dimensions\n-img_rows, img_cols = 28, 28\n-# number of convolutional filters to use\n-filters = 32\n-# size of pooling area for max pooling\n-pool_size = 2\n-# convolution kernel size\n-kernel_size = 3\n-\n-if K.image_data_format() == 'channels_first':\n-    input_shape = (1, img_rows, img_cols)\n-else:\n-    input_shape = (img_rows, img_cols, 1)\n-\n-\n-def train_model(model, train, test, num_classes):\n-    x_train = train[0].reshape((train[0].shape[0],) + input_shape)\n-    x_test = test[0].reshape((test[0].shape[0],) + input_shape)\n-    x_train = x_train.astype('float32')\n-    x_test = x_test.astype('float32')\n-    x_train /= 255\n-    x_test /= 255\n-    print('x_train shape:', x_train.shape)\n-    print(x_train.shape[0], 'train samples')\n-    print(x_test.shape[0], 'test samples')\n-\n-    # convert class vectors to binary class matrices\n-    y_train = keras.utils.to_categorical(train[1], num_classes)\n-    y_test = keras.utils.to_categorical(test[1], num_classes)\n-\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer='adadelta',\n-                  metrics=['accuracy'])\n-\n-    t = now()\n-    model.fit(x_train, y_train,\n-              batch_size=batch_size,\n-              epochs=epochs,\n-              verbose=1,\n-              validation_data=(x_test, y_test))\n-    print('Training time: %s' % (now() - t))\n-    score = model.evaluate(x_test, y_test, verbose=0)\n-    print('Test score:', score[0])\n-    print('Test accuracy:', score[1])\n-\n-\n-# the data, split between train and test sets\n-(x_train, y_train), (x_test, y_test) = mnist.load_data()\n-\n-# create two datasets one with digits below 5 and one with 5 and above\n-x_train_lt5 = x_train[y_train < 5]\n-y_train_lt5 = y_train[y_train < 5]\n-x_test_lt5 = x_test[y_test < 5]\n-y_test_lt5 = y_test[y_test < 5]\n-\n-x_train_gte5 = x_train[y_train >= 5]\n-y_train_gte5 = y_train[y_train >= 5] - 5\n-x_test_gte5 = x_test[y_test >= 5]\n-y_test_gte5 = y_test[y_test >= 5] - 5\n-\n-# define two groups of layers: feature (convolutions) and classification (dense)\n-feature_layers = [\n-    Conv2D(filters, kernel_size,\n-           padding='valid',\n-           input_shape=input_shape),\n-    Activation('relu'),\n-    Conv2D(filters, kernel_size),\n-    Activation('relu'),\n-    MaxPooling2D(pool_size=pool_size),\n-    Dropout(0.25),\n-    Flatten(),\n-]\n-\n-classification_layers = [\n-    Dense(128),\n-    Activation('relu'),\n-    Dropout(0.5),\n-    Dense(num_classes),\n-    Activation('softmax')\n-]\n-\n-# create complete model\n-model = Sequential(feature_layers + classification_layers)\n-\n-# train model for 5-digit classification [0..4]\n-train_model(model,\n-            (x_train_lt5, y_train_lt5),\n-            (x_test_lt5, y_test_lt5), num_classes)\n-\n-# freeze feature layers and rebuild model\n-for l in feature_layers:\n-    l.trainable = False\n-\n-# transfer: train dense layers for new classification task [5..9]\n-train_model(model,\n-            (x_train_gte5, y_train_gte5),\n-            (x_test_gte5, y_test_gte5), num_classes)\n\n@@ -1,382 +0,0 @@\n-'''Neural doodle with Keras\r\n-\r\n-# Script Usage\r\n-\r\n-## Arguments\r\n-```\r\n---nlabels:              # of regions (colors) in mask images\r\n---style-image:          image to learn style from\r\n---style-mask:           semantic labels for style image\r\n---target-mask:          semantic labels for target image (your doodle)\r\n---content-image:        optional image to learn content from\r\n---target-image-prefix:  path prefix for generated target images\r\n-```\r\n-\r\n-## Example 1: doodle using a style image, style mask\r\n-and target mask.\r\n-```\r\n-python neural_doodle.py --nlabels 4 --style-image Monet/style.png \\\r\n---style-mask Monet/style_mask.png --target-mask Monet/target_mask.png \\\r\n---target-image-prefix generated/monet\r\n-```\r\n-\r\n-## Example 2: doodle using a style image, style mask,\r\n-target mask and an optional content image.\r\n-```\r\n-python neural_doodle.py --nlabels 4 --style-image Renoir/style.png \\\r\n---style-mask Renoir/style_mask.png --target-mask Renoir/target_mask.png \\\r\n---content-image Renoir/creek.jpg \\\r\n---target-image-prefix generated/renoir\r\n-```\r\n-\r\n-# References\r\n-\r\n-- [Dmitry Ulyanov's blog on fast-neural-doodle]\r\n-    (http://dmitryulyanov.github.io/feed-forward-neural-doodle/)\r\n-- [Torch code for fast-neural-doodle]\r\n-    (https://github.com/DmitryUlyanov/fast-neural-doodle)\r\n-- [Torch code for online-neural-doodle]\r\n-    (https://github.com/DmitryUlyanov/online-neural-doodle)\r\n-- [Paper Texture Networks: Feed-forward Synthesis of Textures and Stylized Images]\r\n-    (http://arxiv.org/abs/1603.03417)\r\n-- [Discussion on parameter tuning]\r\n-    (https://github.com/keras-team/keras/issues/3705)\r\n-\r\n-# Resources\r\n-\r\n-Example images can be downloaded from\r\n-https://github.com/DmitryUlyanov/fast-neural-doodle/tree/master/data\r\n-'''\r\n-from __future__ import print_function\r\n-import time\r\n-import argparse\r\n-import numpy as np\r\n-from scipy.optimize import fmin_l_bfgs_b\r\n-\r\n-from keras import backend as K\r\n-from keras.layers import Input, AveragePooling2D\r\n-from keras.models import Model\r\n-from keras.preprocessing.image import load_img, save_img, img_to_array\r\n-from keras.applications import vgg19\r\n-\r\n-# Command line arguments\r\n-parser = argparse.ArgumentParser(description='Keras neural doodle example')\r\n-parser.add_argument('--nlabels', type=int,\r\n-                    help='number of semantic labels'\r\n-                    ' (regions in differnet colors)'\r\n-                    ' in style_mask/target_mask')\r\n-parser.add_argument('--style-image', type=str,\r\n-                    help='path to image to learn style from')\r\n-parser.add_argument('--style-mask', type=str,\r\n-                    help='path to semantic mask of style image')\r\n-parser.add_argument('--target-mask', type=str,\r\n-                    help='path to semantic mask of target image')\r\n-parser.add_argument('--content-image', type=str, default=None,\r\n-                    help='path to optional content image')\r\n-parser.add_argument('--target-image-prefix', type=str,\r\n-                    help='path prefix for generated results')\r\n-args = parser.parse_args()\r\n-\r\n-style_img_path = args.style_image\r\n-style_mask_path = args.style_mask\r\n-target_mask_path = args.target_mask\r\n-content_img_path = args.content_image\r\n-target_img_prefix = args.target_image_prefix\r\n-use_content_img = content_img_path is not None\r\n-\r\n-num_labels = args.nlabels\r\n-num_colors = 3  # RGB\r\n-# determine image sizes based on target_mask\r\n-ref_img = img_to_array(load_img(target_mask_path))\r\n-img_nrows, img_ncols = ref_img.shape[:2]\r\n-\r\n-num_iterations = 50\r\n-\r\n-total_variation_weight = 50.\r\n-style_weight = 1.\r\n-content_weight = 0.1 if use_content_img else 0\r\n-\r\n-content_feature_layers = ['block5_conv2']\r\n-# To get better generation qualities, use more conv layers for style features\r\n-style_feature_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1',\r\n-                        'block4_conv1', 'block5_conv1']\r\n-\r\n-\r\n-# helper functions for reading/processing images\r\n-def preprocess_image(image_path):\r\n-    img = load_img(image_path, target_size=(img_nrows, img_ncols))\r\n-    img = img_to_array(img)\r\n-    img = np.expand_dims(img, axis=0)\r\n-    img = vgg19.preprocess_input(img)\r\n-    return img\r\n-\r\n-\r\n-def deprocess_image(x):\r\n-    if K.image_data_format() == 'channels_first':\r\n-        x = x.reshape((3, img_nrows, img_ncols))\r\n-        x = x.transpose((1, 2, 0))\r\n-    else:\r\n-        x = x.reshape((img_nrows, img_ncols, 3))\r\n-    # Remove zero-center by mean pixel\r\n-    x[:, :, 0] += 103.939\r\n-    x[:, :, 1] += 116.779\r\n-    x[:, :, 2] += 123.68\r\n-    # 'BGR'->'RGB'\r\n-    x = x[:, :, ::-1]\r\n-    x = np.clip(x, 0, 255).astype('uint8')\r\n-    return x\r\n-\r\n-\r\n-def kmeans(xs, k):\r\n-    assert xs.ndim == 2\r\n-    try:\r\n-        from sklearn.cluster import k_means\r\n-        _, labels, _ = k_means(xs.astype('float64'), k)\r\n-    except ImportError:\r\n-        from scipy.cluster.vq import kmeans2\r\n-        _, labels = kmeans2(xs, k, missing='raise')\r\n-    return labels\r\n-\r\n-\r\n-def load_mask_labels():\r\n-    '''Load both target and style masks.\r\n-    A mask image (nr x nc) with m labels/colors will be loaded\r\n-    as a 4D boolean tensor:\r\n-        (1, m, nr, nc) for 'channels_first' or (1, nr, nc, m) for 'channels_last'\r\n-    '''\r\n-    target_mask_img = load_img(target_mask_path,\r\n-                               target_size=(img_nrows, img_ncols))\r\n-    target_mask_img = img_to_array(target_mask_img)\r\n-    style_mask_img = load_img(style_mask_path,\r\n-                              target_size=(img_nrows, img_ncols))\r\n-    style_mask_img = img_to_array(style_mask_img)\r\n-    if K.image_data_format() == 'channels_first':\r\n-        mask_vecs = np.vstack([style_mask_img.reshape((3, -1)).T,\r\n-                               target_mask_img.reshape((3, -1)).T])\r\n-    else:\r\n-        mask_vecs = np.vstack([style_mask_img.reshape((-1, 3)),\r\n-                               target_mask_img.reshape((-1, 3))])\r\n-\r\n-    labels = kmeans(mask_vecs, num_labels)\r\n-    style_mask_label = labels[:img_nrows *\r\n-                              img_ncols].reshape((img_nrows, img_ncols))\r\n-    target_mask_label = labels[img_nrows *\r\n-                               img_ncols:].reshape((img_nrows, img_ncols))\r\n-\r\n-    stack_axis = 0 if K.image_data_format() == 'channels_first' else -1\r\n-    style_mask = np.stack([style_mask_label == r for r in range(num_labels)],\r\n-                          axis=stack_axis)\r\n-    target_mask = np.stack([target_mask_label == r for r in range(num_labels)],\r\n-                           axis=stack_axis)\r\n-\r\n-    return (np.expand_dims(style_mask, axis=0),\r\n-            np.expand_dims(target_mask, axis=0))\r\n-\r\n-\r\n-# Create tensor variables for images\r\n-if K.image_data_format() == 'channels_first':\r\n-    shape = (1, num_colors, img_nrows, img_ncols)\r\n-else:\r\n-    shape = (1, img_nrows, img_ncols, num_colors)\r\n-\r\n-style_image = K.variable(preprocess_image(style_img_path))\r\n-target_image = K.placeholder(shape=shape)\r\n-if use_content_img:\r\n-    content_image = K.variable(preprocess_image(content_img_path))\r\n-else:\r\n-    content_image = K.zeros(shape=shape)\r\n-\r\n-images = K.concatenate([style_image, target_image, content_image], axis=0)\r\n-\r\n-# Create tensor variables for masks\r\n-raw_style_mask, raw_target_mask = load_mask_labels()\r\n-style_mask = K.variable(raw_style_mask.astype('float32'))\r\n-target_mask = K.variable(raw_target_mask.astype('float32'))\r\n-masks = K.concatenate([style_mask, target_mask], axis=0)\r\n-\r\n-# index constants for images and tasks variables\r\n-STYLE, TARGET, CONTENT = 0, 1, 2\r\n-\r\n-# Build image model, mask model and use layer outputs as features\r\n-# image model as VGG19\r\n-image_model = vgg19.VGG19(include_top=False, input_tensor=images)\r\n-\r\n-# mask model as a series of pooling\r\n-mask_input = Input(tensor=masks, shape=(None, None, None), name='mask_input')\r\n-x = mask_input\r\n-for layer in image_model.layers[1:]:\r\n-    name = 'mask_%s' % layer.name\r\n-    if 'conv' in layer.name:\r\n-        x = AveragePooling2D((3, 3), padding='same', strides=(\r\n-            1, 1), name=name)(x)\r\n-    elif 'pool' in layer.name:\r\n-        x = AveragePooling2D((2, 2), name=name)(x)\r\n-mask_model = Model(mask_input, x)\r\n-\r\n-# Collect features from image_model and task_model\r\n-image_features = {}\r\n-mask_features = {}\r\n-for img_layer, mask_layer in zip(image_model.layers, mask_model.layers):\r\n-    if 'conv' in img_layer.name:\r\n-        assert 'mask_' + img_layer.name == mask_layer.name\r\n-        layer_name = img_layer.name\r\n-        img_feat, mask_feat = img_layer.output, mask_layer.output\r\n-        image_features[layer_name] = img_feat\r\n-        mask_features[layer_name] = mask_feat\r\n-\r\n-\r\n-# Define loss functions\r\n-def gram_matrix(x):\r\n-    assert K.ndim(x) == 3\r\n-    features = K.batch_flatten(x)\r\n-    gram = K.dot(features, K.transpose(features))\r\n-    return gram\r\n-\r\n-\r\n-def region_style_loss(style_image, target_image, style_mask, target_mask):\r\n-    '''Calculate style loss between style_image and target_image,\r\n-    for one common region specified by their (boolean) masks\r\n-    '''\r\n-    assert 3 == K.ndim(style_image) == K.ndim(target_image)\r\n-    assert 2 == K.ndim(style_mask) == K.ndim(target_mask)\r\n-    if K.image_data_format() == 'channels_first':\r\n-        masked_style = style_image * style_mask\r\n-        masked_target = target_image * target_mask\r\n-        num_channels = K.shape(style_image)[0]\r\n-    else:\r\n-        masked_style = K.permute_dimensions(\r\n-            style_image, (2, 0, 1)) * style_mask\r\n-        masked_target = K.permute_dimensions(\r\n-            target_image, (2, 0, 1)) * target_mask\r\n-        num_channels = K.shape(style_image)[-1]\r\n-    num_channels = K.cast(num_channels, dtype='float32')\r\n-    s = gram_matrix(masked_style) / K.mean(style_mask) / num_channels\r\n-    c = gram_matrix(masked_target) / K.mean(target_mask) / num_channels\r\n-    return K.mean(K.square(s - c))\r\n-\r\n-\r\n-def style_loss(style_image, target_image, style_masks, target_masks):\r\n-    '''Calculate style loss between style_image and target_image,\r\n-    in all regions.\r\n-    '''\r\n-    assert 3 == K.ndim(style_image) == K.ndim(target_image)\r\n-    assert 3 == K.ndim(style_masks) == K.ndim(target_masks)\r\n-    loss = K.variable(0)\r\n-    for i in range(num_labels):\r\n-        if K.image_data_format() == 'channels_first':\r\n-            style_mask = style_masks[i, :, :]\r\n-            target_mask = target_masks[i, :, :]\r\n-        else:\r\n-            style_mask = style_masks[:, :, i]\r\n-            target_mask = target_masks[:, :, i]\r\n-        loss = loss + region_style_loss(style_image,\r\n-                                        target_image,\r\n-                                        style_mask,\r\n-                                        target_mask)\r\n-    return loss\r\n-\r\n-\r\n-def content_loss(content_image, target_image):\r\n-    return K.sum(K.square(target_image - content_image))\r\n-\r\n-\r\n-def total_variation_loss(x):\r\n-    assert 4 == K.ndim(x)\r\n-    if K.image_data_format() == 'channels_first':\r\n-        a = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] -\r\n-                     x[:, :, 1:, :img_ncols - 1])\r\n-        b = K.square(x[:, :, :img_nrows - 1, :img_ncols - 1] -\r\n-                     x[:, :, :img_nrows - 1, 1:])\r\n-    else:\r\n-        a = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] -\r\n-                     x[:, 1:, :img_ncols - 1, :])\r\n-        b = K.square(x[:, :img_nrows - 1, :img_ncols - 1, :] -\r\n-                     x[:, :img_nrows - 1, 1:, :])\r\n-    return K.sum(K.pow(a + b, 1.25))\r\n-\r\n-\r\n-# Overall loss is the weighted sum of content_loss, style_loss and tv_loss\r\n-# Each individual loss uses features from image/mask models.\r\n-loss = K.variable(0)\r\n-for layer in content_feature_layers:\r\n-    content_feat = image_features[layer][CONTENT, :, :, :]\r\n-    target_feat = image_features[layer][TARGET, :, :, :]\r\n-    loss = loss + content_weight * content_loss(content_feat, target_feat)\r\n-\r\n-for layer in style_feature_layers:\r\n-    style_feat = image_features[layer][STYLE, :, :, :]\r\n-    target_feat = image_features[layer][TARGET, :, :, :]\r\n-    style_masks = mask_features[layer][STYLE, :, :, :]\r\n-    target_masks = mask_features[layer][TARGET, :, :, :]\r\n-    sl = style_loss(style_feat, target_feat, style_masks, target_masks)\r\n-    loss = loss + (style_weight / len(style_feature_layers)) * sl\r\n-\r\n-loss = loss + total_variation_weight * total_variation_loss(target_image)\r\n-loss_grads = K.gradients(loss, target_image)\r\n-\r\n-# Evaluator class for computing efficiency\r\n-outputs = [loss]\r\n-if isinstance(loss_grads, (list, tuple)):\r\n-    outputs += loss_grads\r\n-else:\r\n-    outputs.append(loss_grads)\r\n-\r\n-f_outputs = K.function([target_image], outputs)\r\n-\r\n-\r\n-def eval_loss_and_grads(x):\r\n-    if K.image_data_format() == 'channels_first':\r\n-        x = x.reshape((1, 3, img_nrows, img_ncols))\r\n-    else:\r\n-        x = x.reshape((1, img_nrows, img_ncols, 3))\r\n-    outs = f_outputs([x])\r\n-    loss_value = outs[0]\r\n-    if len(outs[1:]) == 1:\r\n-        grad_values = outs[1].flatten().astype('float64')\r\n-    else:\r\n-        grad_values = np.array(outs[1:]).flatten().astype('float64')\r\n-    return loss_value, grad_values\r\n-\r\n-\r\n-class Evaluator(object):\r\n-\r\n-    def __init__(self):\r\n-        self.loss_value = None\r\n-        self.grads_values = None\r\n-\r\n-    def loss(self, x):\r\n-        assert self.loss_value is None\r\n-        loss_value, grad_values = eval_loss_and_grads(x)\r\n-        self.loss_value = loss_value\r\n-        self.grad_values = grad_values\r\n-        return self.loss_value\r\n-\r\n-    def grads(self, x):\r\n-        assert self.loss_value is not None\r\n-        grad_values = np.copy(self.grad_values)\r\n-        self.loss_value = None\r\n-        self.grad_values = None\r\n-        return grad_values\r\n-\r\n-\r\n-evaluator = Evaluator()\r\n-\r\n-# Generate images by iterative optimization\r\n-if K.image_data_format() == 'channels_first':\r\n-    x = np.random.uniform(0, 255, (1, 3, img_nrows, img_ncols)) - 128.\r\n-else:\r\n-    x = np.random.uniform(0, 255, (1, img_nrows, img_ncols, 3)) - 128.\r\n-\r\n-for i in range(num_iterations):\r\n-    print('Start of iteration', i, '/', num_iterations)\r\n-    start_time = time.time()\r\n-    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\r\n-                                     fprime=evaluator.grads, maxfun=20)\r\n-    print('Current loss value:', min_val)\r\n-    # save current generated image\r\n-    img = deprocess_image(x.copy())\r\n-    fname = target_img_prefix + '_at_iteration_%d.png' % i\r\n-    save_img(fname, img)\r\n-    end_time = time.time()\r\n-    print('Image saved as', fname)\r\n-    print('Iteration %d completed in %ds' % (i, end_time - start_time))\r\n\n@@ -1,298 +0,0 @@\n-'''Neural style transfer with Keras.\n-\n-Run the script with:\n-```\n-python neural_style_transfer.py path_to_your_base_image.jpg \\\n-    path_to_your_reference.jpg prefix_for_results\n-```\n-e.g.:\n-```\n-python neural_style_transfer.py img/tuebingen.jpg \\\n-    img/starry_night.jpg results/my_result\n-```\n-Optional parameters:\n-```\n---iter, To specify the number of iterations \\\n-    the style transfer takes place (Default is 10)\n---content_weight, The weight given to the content loss (Default is 0.025)\n---style_weight, The weight given to the style loss (Default is 1.0)\n---tv_weight, The weight given to the total variation loss (Default is 1.0)\n-```\n-\n-It is preferable to run this script on GPU, for speed.\n-\n-Example result: https://twitter.com/fchollet/status/686631033085677568\n-\n-# Details\n-\n-Style transfer consists in generating an image\n-with the same \"content\" as a base image, but with the\n-\"style\" of a different picture (typically artistic).\n-\n-This is achieved through the optimization of a loss function\n-that has 3 components: \"style loss\", \"content loss\",\n-and \"total variation loss\":\n-\n-- The total variation loss imposes local spatial continuity between\n-the pixels of the combination image, giving it visual coherence.\n-\n-- The style loss is where the deep learning keeps in --that one is defined\n-using a deep convolutional neural network. Precisely, it consists in a sum of\n-L2 distances between the Gram matrices of the representations of\n-the base image and the style reference image, extracted from\n-different layers of a convnet (trained on ImageNet). The general idea\n-is to capture color/texture information at different spatial\n-scales (fairly large scales --defined by the depth of the layer considered).\n-\n- - The content loss is a L2 distance between the features of the base\n-image (extracted from a deep layer) and the features of the combination image,\n-keeping the generated image close enough to the original one.\n-\n-# References\n-    - [A Neural Algorithm of Artistic Style](http://arxiv.org/abs/1508.06576)\n-'''\n-\n-from __future__ import print_function\n-from keras.preprocessing.image import load_img, save_img, img_to_array\n-import numpy as np\n-from scipy.optimize import fmin_l_bfgs_b\n-import time\n-import argparse\n-\n-from keras.applications import vgg19\n-from keras import backend as K\n-\n-parser = argparse.ArgumentParser(description='Neural style transfer with Keras.')\n-parser.add_argument('base_image_path', metavar='base', type=str,\n-                    help='Path to the image to transform.')\n-parser.add_argument('style_reference_image_path', metavar='ref', type=str,\n-                    help='Path to the style reference image.')\n-parser.add_argument('result_prefix', metavar='res_prefix', type=str,\n-                    help='Prefix for the saved results.')\n-parser.add_argument('--iter', type=int, default=10, required=False,\n-                    help='Number of iterations to run.')\n-parser.add_argument('--content_weight', type=float, default=0.025, required=False,\n-                    help='Content weight.')\n-parser.add_argument('--style_weight', type=float, default=1.0, required=False,\n-                    help='Style weight.')\n-parser.add_argument('--tv_weight', type=float, default=1.0, required=False,\n-                    help='Total Variation weight.')\n-\n-args = parser.parse_args()\n-base_image_path = args.base_image_path\n-style_reference_image_path = args.style_reference_image_path\n-result_prefix = args.result_prefix\n-iterations = args.iter\n-\n-# these are the weights of the different loss components\n-total_variation_weight = args.tv_weight\n-style_weight = args.style_weight\n-content_weight = args.content_weight\n-\n-# dimensions of the generated picture.\n-width, height = load_img(base_image_path).size\n-img_nrows = 400\n-img_ncols = int(width * img_nrows / height)\n-\n-# util function to open, resize and format pictures into appropriate tensors\n-\n-\n-def preprocess_image(image_path):\n-    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n-    img = img_to_array(img)\n-    img = np.expand_dims(img, axis=0)\n-    img = vgg19.preprocess_input(img)\n-    return img\n-\n-# util function to convert a tensor into a valid image\n-\n-\n-def deprocess_image(x):\n-    if K.image_data_format() == 'channels_first':\n-        x = x.reshape((3, img_nrows, img_ncols))\n-        x = x.transpose((1, 2, 0))\n-    else:\n-        x = x.reshape((img_nrows, img_ncols, 3))\n-    # Remove zero-center by mean pixel\n-    x[:, :, 0] += 103.939\n-    x[:, :, 1] += 116.779\n-    x[:, :, 2] += 123.68\n-    # 'BGR'->'RGB'\n-    x = x[:, :, ::-1]\n-    x = np.clip(x, 0, 255).astype('uint8')\n-    return x\n-\n-# get tensor representations of our images\n-base_image = K.variable(preprocess_image(base_image_path))\n-style_reference_image = K.variable(preprocess_image(style_reference_image_path))\n-\n-# this will contain our generated image\n-if K.image_data_format() == 'channels_first':\n-    combination_image = K.placeholder((1, 3, img_nrows, img_ncols))\n-else:\n-    combination_image = K.placeholder((1, img_nrows, img_ncols, 3))\n-\n-# combine the 3 images into a single Keras tensor\n-input_tensor = K.concatenate([base_image,\n-                              style_reference_image,\n-                              combination_image], axis=0)\n-\n-# build the VGG19 network with our 3 images as input\n-# the model will be loaded with pre-trained ImageNet weights\n-model = vgg19.VGG19(input_tensor=input_tensor,\n-                    weights='imagenet', include_top=False)\n-print('Model loaded.')\n-\n-# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n-outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n-\n-# compute the neural style loss\n-# first we need to define 4 util functions\n-\n-# the gram matrix of an image tensor (feature-wise outer product)\n-\n-\n-def gram_matrix(x):\n-    assert K.ndim(x) == 3\n-    if K.image_data_format() == 'channels_first':\n-        features = K.batch_flatten(x)\n-    else:\n-        features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n-    gram = K.dot(features, K.transpose(features))\n-    return gram\n-\n-# the \"style loss\" is designed to maintain\n-# the style of the reference image in the generated image.\n-# It is based on the gram matrices (which capture style) of\n-# feature maps from the style reference image\n-# and from the generated image\n-\n-\n-def style_loss(style, combination):\n-    assert K.ndim(style) == 3\n-    assert K.ndim(combination) == 3\n-    S = gram_matrix(style)\n-    C = gram_matrix(combination)\n-    channels = 3\n-    size = img_nrows * img_ncols\n-    return K.sum(K.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))\n-\n-# an auxiliary loss function\n-# designed to maintain the \"content\" of the\n-# base image in the generated image\n-\n-\n-def content_loss(base, combination):\n-    return K.sum(K.square(combination - base))\n-\n-# the 3rd loss function, total variation loss,\n-# designed to keep the generated image locally coherent\n-\n-\n-def total_variation_loss(x):\n-    assert K.ndim(x) == 4\n-    if K.image_data_format() == 'channels_first':\n-        a = K.square(\n-            x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1])\n-        b = K.square(\n-            x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:])\n-    else:\n-        a = K.square(\n-            x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\n-        b = K.square(\n-            x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\n-    return K.sum(K.pow(a + b, 1.25))\n-\n-\n-# combine these loss functions into a single scalar\n-loss = K.variable(0.0)\n-layer_features = outputs_dict['block5_conv2']\n-base_image_features = layer_features[0, :, :, :]\n-combination_features = layer_features[2, :, :, :]\n-loss = loss + content_weight * content_loss(base_image_features,\n-                                            combination_features)\n-\n-feature_layers = ['block1_conv1', 'block2_conv1',\n-                  'block3_conv1', 'block4_conv1',\n-                  'block5_conv1']\n-for layer_name in feature_layers:\n-    layer_features = outputs_dict[layer_name]\n-    style_reference_features = layer_features[1, :, :, :]\n-    combination_features = layer_features[2, :, :, :]\n-    sl = style_loss(style_reference_features, combination_features)\n-    loss = loss + (style_weight / len(feature_layers)) * sl\n-loss = loss + total_variation_weight * total_variation_loss(combination_image)\n-\n-# get the gradients of the generated image wrt the loss\n-grads = K.gradients(loss, combination_image)\n-\n-outputs = [loss]\n-if isinstance(grads, (list, tuple)):\n-    outputs += grads\n-else:\n-    outputs.append(grads)\n-\n-f_outputs = K.function([combination_image], outputs)\n-\n-\n-def eval_loss_and_grads(x):\n-    if K.image_data_format() == 'channels_first':\n-        x = x.reshape((1, 3, img_nrows, img_ncols))\n-    else:\n-        x = x.reshape((1, img_nrows, img_ncols, 3))\n-    outs = f_outputs([x])\n-    loss_value = outs[0]\n-    if len(outs[1:]) == 1:\n-        grad_values = outs[1].flatten().astype('float64')\n-    else:\n-        grad_values = np.array(outs[1:]).flatten().astype('float64')\n-    return loss_value, grad_values\n-\n-# this Evaluator class makes it possible\n-# to compute loss and gradients in one pass\n-# while retrieving them via two separate functions,\n-# \"loss\" and \"grads\". This is done because scipy.optimize\n-# requires separate functions for loss and gradients,\n-# but computing them separately would be inefficient.\n-\n-\n-class Evaluator(object):\n-\n-    def __init__(self):\n-        self.loss_value = None\n-        self.grads_values = None\n-\n-    def loss(self, x):\n-        assert self.loss_value is None\n-        loss_value, grad_values = eval_loss_and_grads(x)\n-        self.loss_value = loss_value\n-        self.grad_values = grad_values\n-        return self.loss_value\n-\n-    def grads(self, x):\n-        assert self.loss_value is not None\n-        grad_values = np.copy(self.grad_values)\n-        self.loss_value = None\n-        self.grad_values = None\n-        return grad_values\n-\n-\n-evaluator = Evaluator()\n-\n-# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n-# so as to minimize the neural style loss\n-x = preprocess_image(base_image_path)\n-\n-for i in range(iterations):\n-    print('Start of iteration', i)\n-    start_time = time.time()\n-    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n-                                     fprime=evaluator.grads, maxfun=20)\n-    print('Current loss value:', min_val)\n-    # save current generated image\n-    img = deprocess_image(x.copy())\n-    fname = result_prefix + '_at_iteration_%d.png' % i\n-    save_img(fname, img)\n-    end_time = time.time()\n-    print('Image saved as', fname)\n-    print('Iteration %d completed in %ds' % (i, end_time - start_time))\n\n@@ -1,144 +0,0 @@\n-'''This script loads pre-trained word embeddings (GloVe embeddings)\n-into a frozen Keras Embedding layer, and uses it to\n-train a text classification model on the 20 Newsgroup dataset\n-(classification of newsgroup messages into 20 different categories).\n-\n-GloVe embedding data can be found at:\n-http://nlp.stanford.edu/data/glove.6B.zip\n-(source page: http://nlp.stanford.edu/projects/glove/)\n-\n-20 Newsgroup data can be found at:\n-http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html\n-'''\n-\n-from __future__ import print_function\n-\n-import os\n-import sys\n-import numpy as np\n-from keras.preprocessing.text import Tokenizer\n-from keras.preprocessing.sequence import pad_sequences\n-from keras.utils import to_categorical\n-from keras.layers import Dense, Input, GlobalMaxPooling1D\n-from keras.layers import Conv1D, MaxPooling1D, Embedding\n-from keras.models import Model\n-from keras.initializers import Constant\n-\n-\n-BASE_DIR = ''\n-GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n-TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup')\n-MAX_SEQUENCE_LENGTH = 1000\n-MAX_NUM_WORDS = 20000\n-EMBEDDING_DIM = 100\n-VALIDATION_SPLIT = 0.2\n-\n-# first, build index mapping words in the embeddings set\n-# to their embedding vector\n-\n-print('Indexing word vectors.')\n-\n-embeddings_index = {}\n-with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')) as f:\n-    for line in f:\n-        word, coefs = line.split(maxsplit=1)\n-        coefs = np.fromstring(coefs, 'f', sep=' ')\n-        embeddings_index[word] = coefs\n-\n-print('Found %s word vectors.' % len(embeddings_index))\n-\n-# second, prepare text samples and their labels\n-print('Processing text dataset')\n-\n-texts = []  # list of text samples\n-labels_index = {}  # dictionary mapping label name to numeric id\n-labels = []  # list of label ids\n-for name in sorted(os.listdir(TEXT_DATA_DIR)):\n-    path = os.path.join(TEXT_DATA_DIR, name)\n-    if os.path.isdir(path):\n-        label_id = len(labels_index)\n-        labels_index[name] = label_id\n-        for fname in sorted(os.listdir(path)):\n-            if fname.isdigit():\n-                fpath = os.path.join(path, fname)\n-                args = {} if sys.version_info < (3,) else {'encoding': 'latin-1'}\n-                with open(fpath, **args) as f:\n-                    t = f.read()\n-                    i = t.find('\\n\\n')  # skip header\n-                    if 0 < i:\n-                        t = t[i:]\n-                    texts.append(t)\n-                labels.append(label_id)\n-\n-print('Found %s texts.' % len(texts))\n-\n-# finally, vectorize the text samples into a 2D integer tensor\n-tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n-tokenizer.fit_on_texts(texts)\n-sequences = tokenizer.texts_to_sequences(texts)\n-\n-word_index = tokenizer.word_index\n-print('Found %s unique tokens.' % len(word_index))\n-\n-data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n-\n-labels = to_categorical(np.asarray(labels))\n-print('Shape of data tensor:', data.shape)\n-print('Shape of label tensor:', labels.shape)\n-\n-# split the data into a training set and a validation set\n-indices = np.arange(data.shape[0])\n-np.random.shuffle(indices)\n-data = data[indices]\n-labels = labels[indices]\n-num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n-\n-x_train = data[:-num_validation_samples]\n-y_train = labels[:-num_validation_samples]\n-x_val = data[-num_validation_samples:]\n-y_val = labels[-num_validation_samples:]\n-\n-print('Preparing embedding matrix.')\n-\n-# prepare embedding matrix\n-num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n-embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n-for word, i in word_index.items():\n-    if i >= MAX_NUM_WORDS:\n-        continue\n-    embedding_vector = embeddings_index.get(word)\n-    if embedding_vector is not None:\n-        # words not found in embedding index will be all-zeros.\n-        embedding_matrix[i] = embedding_vector\n-\n-# load pre-trained word embeddings into an Embedding layer\n-# note that we set trainable = False so as to keep the embeddings fixed\n-embedding_layer = Embedding(num_words,\n-                            EMBEDDING_DIM,\n-                            embeddings_initializer=Constant(embedding_matrix),\n-                            input_length=MAX_SEQUENCE_LENGTH,\n-                            trainable=False)\n-\n-print('Training model.')\n-\n-# train a 1D convnet with global maxpooling\n-sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n-embedded_sequences = embedding_layer(sequence_input)\n-x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n-x = MaxPooling1D(5)(x)\n-x = Conv1D(128, 5, activation='relu')(x)\n-x = MaxPooling1D(5)(x)\n-x = Conv1D(128, 5, activation='relu')(x)\n-x = GlobalMaxPooling1D()(x)\n-x = Dense(128, activation='relu')(x)\n-preds = Dense(len(labels_index), activation='softmax')(x)\n-\n-model = Model(sequence_input, preds)\n-model.compile(loss='categorical_crossentropy',\n-              optimizer='rmsprop',\n-              metrics=['acc'])\n-\n-model.fit(x_train, y_train,\n-          batch_size=128,\n-          epochs=10,\n-          validation_data=(x_val, y_val))\n\n@@ -1,60 +0,0 @@\n-'''Trains and evaluate a simple MLP\n-on the Reuters newswire topic classification task.\n-'''\n-from __future__ import print_function\n-\n-import numpy as np\n-import keras\n-from keras.datasets import reuters\n-from keras.models import Sequential\n-from keras.layers import Dense, Dropout, Activation\n-from keras.preprocessing.text import Tokenizer\n-\n-max_words = 1000\n-batch_size = 32\n-epochs = 5\n-\n-print('Loading data...')\n-(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n-                                                         test_split=0.2)\n-print(len(x_train), 'train sequences')\n-print(len(x_test), 'test sequences')\n-\n-num_classes = np.max(y_train) + 1\n-print(num_classes, 'classes')\n-\n-print('Vectorizing sequence data...')\n-tokenizer = Tokenizer(num_words=max_words)\n-x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n-x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n-print('x_train shape:', x_train.shape)\n-print('x_test shape:', x_test.shape)\n-\n-print('Convert class vector to binary class matrix '\n-      '(for use with categorical_crossentropy)')\n-y_train = keras.utils.to_categorical(y_train, num_classes)\n-y_test = keras.utils.to_categorical(y_test, num_classes)\n-print('y_train shape:', y_train.shape)\n-print('y_test shape:', y_test.shape)\n-\n-print('Building model...')\n-model = Sequential()\n-model.add(Dense(512, input_shape=(max_words,)))\n-model.add(Activation('relu'))\n-model.add(Dropout(0.5))\n-model.add(Dense(num_classes))\n-model.add(Activation('softmax'))\n-\n-model.compile(loss='categorical_crossentropy',\n-              optimizer='adam',\n-              metrics=['accuracy'])\n-\n-history = model.fit(x_train, y_train,\n-                    batch_size=batch_size,\n-                    epochs=epochs,\n-                    verbose=1,\n-                    validation_split=0.1)\n-score = model.evaluate(x_test, y_test,\n-                       batch_size=batch_size, verbose=1)\n-print('Test score:', score[0])\n-print('Test accuracy:', score[1])\n\n@@ -1,175 +0,0 @@\n-'''Compares self-normalizing MLPs with regular MLPs.\n-\n-Compares the performance of a simple MLP using two\n-different activation functions: RELU and SELU\n-on the Reuters newswire topic classification task.\n-\n-# Reference\n-\n-- Klambauer, G., Unterthiner, T., Mayr, A., & Hochreiter, S. (2017).\n-  Self-Normalizing Neural Networks. arXiv preprint arXiv:1706.02515.\n-  https://arxiv.org/abs/1706.02515\n-'''\n-from __future__ import print_function\n-\n-import numpy as np\n-import matplotlib.pyplot as plt\n-import keras\n-from keras.datasets import reuters\n-from keras.models import Sequential\n-from keras.layers import Dense, Activation, Dropout\n-from keras.layers.noise import AlphaDropout\n-from keras.preprocessing.text import Tokenizer\n-\n-max_words = 1000\n-batch_size = 16\n-epochs = 40\n-plot = True\n-\n-\n-def create_network(n_dense=6,\n-                   dense_units=16,\n-                   activation='selu',\n-                   dropout=AlphaDropout,\n-                   dropout_rate=0.1,\n-                   kernel_initializer='lecun_normal',\n-                   optimizer='adam',\n-                   num_classes=1,\n-                   max_words=max_words):\n-    \"\"\"Generic function to create a fully-connected neural network.\n-\n-    # Arguments\n-        n_dense: int > 0. Number of dense layers.\n-        dense_units: int > 0. Number of dense units per layer.\n-        dropout: keras.layers.Layer. A dropout layer to apply.\n-        dropout_rate: 0 <= float <= 1. The rate of dropout.\n-        kernel_initializer: str. The initializer for the weights.\n-        optimizer: str/keras.optimizers.Optimizer. The optimizer to use.\n-        num_classes: int > 0. The number of classes to predict.\n-        max_words: int > 0. The maximum number of words per data point.\n-\n-    # Returns\n-        A Keras model instance (compiled).\n-    \"\"\"\n-    model = Sequential()\n-    model.add(Dense(dense_units, input_shape=(max_words,),\n-                    kernel_initializer=kernel_initializer))\n-    model.add(Activation(activation))\n-    model.add(dropout(dropout_rate))\n-\n-    for i in range(n_dense - 1):\n-        model.add(Dense(dense_units, kernel_initializer=kernel_initializer))\n-        model.add(Activation(activation))\n-        model.add(dropout(dropout_rate))\n-\n-    model.add(Dense(num_classes))\n-    model.add(Activation('softmax'))\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer=optimizer,\n-                  metrics=['accuracy'])\n-    return model\n-\n-\n-network1 = {\n-    'n_dense': 6,\n-    'dense_units': 16,\n-    'activation': 'relu',\n-    'dropout': Dropout,\n-    'dropout_rate': 0.5,\n-    'kernel_initializer': 'glorot_uniform',\n-    'optimizer': 'sgd'\n-}\n-\n-network2 = {\n-    'n_dense': 6,\n-    'dense_units': 16,\n-    'activation': 'selu',\n-    'dropout': AlphaDropout,\n-    'dropout_rate': 0.1,\n-    'kernel_initializer': 'lecun_normal',\n-    'optimizer': 'sgd'\n-}\n-\n-print('Loading data...')\n-(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n-                                                         test_split=0.2)\n-print(len(x_train), 'train sequences')\n-print(len(x_test), 'test sequences')\n-\n-num_classes = np.max(y_train) + 1\n-print(num_classes, 'classes')\n-\n-print('Vectorizing sequence data...')\n-tokenizer = Tokenizer(num_words=max_words)\n-x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n-x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n-print('x_train shape:', x_train.shape)\n-print('x_test shape:', x_test.shape)\n-\n-print('Convert class vector to binary class matrix '\n-      '(for use with categorical_crossentropy)')\n-y_train = keras.utils.to_categorical(y_train, num_classes)\n-y_test = keras.utils.to_categorical(y_test, num_classes)\n-print('y_train shape:', y_train.shape)\n-print('y_test shape:', y_test.shape)\n-\n-print('\\nBuilding network 1...')\n-\n-model1 = create_network(num_classes=num_classes, **network1)\n-history_model1 = model1.fit(x_train,\n-                            y_train,\n-                            batch_size=batch_size,\n-                            epochs=epochs,\n-                            verbose=1,\n-                            validation_split=0.1)\n-\n-score_model1 = model1.evaluate(x_test,\n-                               y_test,\n-                               batch_size=batch_size,\n-                               verbose=1)\n-\n-\n-print('\\nBuilding network 2...')\n-model2 = create_network(num_classes=num_classes, **network2)\n-\n-history_model2 = model2.fit(x_train,\n-                            y_train,\n-                            batch_size=batch_size,\n-                            epochs=epochs,\n-                            verbose=1,\n-                            validation_split=0.1)\n-\n-score_model2 = model2.evaluate(x_test,\n-                               y_test,\n-                               batch_size=batch_size,\n-                               verbose=1)\n-\n-print('\\nNetwork 1 results')\n-print('Hyperparameters:', network1)\n-print('Test score:', score_model1[0])\n-print('Test accuracy:', score_model1[1])\n-print('Network 2 results')\n-print('Hyperparameters:', network2)\n-print('Test score:', score_model2[0])\n-print('Test accuracy:', score_model2[1])\n-\n-plt.plot(range(epochs),\n-         history_model1.history['val_loss'],\n-         'g-',\n-         label='Network 1 Val Loss')\n-plt.plot(range(epochs),\n-         history_model2.history['val_loss'],\n-         'r-',\n-         label='Network 2 Val Loss')\n-plt.plot(range(epochs),\n-         history_model1.history['loss'],\n-         'g--',\n-         label='Network 1 Loss')\n-plt.plot(range(epochs),\n-         history_model2.history['loss'],\n-         'r--',\n-         label='Network 2 Loss')\n-plt.xlabel('Epochs')\n-plt.ylabel('Loss')\n-plt.legend()\n-plt.savefig('comparison_of_networks.png')\n\n@@ -1,204 +0,0 @@\n-'''Example of VAE on MNIST dataset using MLP\n-\n-The VAE has a modular design. The encoder, decoder and VAE\n-are 3 models that share weights. After training the VAE model,\n-the encoder can be used to generate latent vectors.\n-The decoder can be used to generate MNIST digits by sampling the\n-latent vector from a Gaussian distribution with mean = 0 and std = 1.\n-\n-# Reference\n-\n-[1] Kingma, Diederik P., and Max Welling.\n-\"Auto-Encoding Variational Bayes.\"\n-https://arxiv.org/abs/1312.6114\n-'''\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import keras\n-from keras.layers import Lambda, Input, Dense\n-from keras.models import Model\n-from keras.datasets import mnist\n-from keras.losses import mse, binary_crossentropy\n-from keras.utils import plot_model\n-from keras import backend as K\n-\n-import numpy as np\n-import matplotlib.pyplot as plt\n-import argparse\n-import os\n-\n-\n-# reparameterization trick\n-# instead of sampling from Q(z|X), sample epsilon = N(0,I)\n-# z = z_mean + sqrt(var) * epsilon\n-def sampling(args):\n-    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n-\n-    # Arguments\n-        args (tensor): mean and log of variance of Q(z|X)\n-\n-    # Returns\n-        z (tensor): sampled latent vector\n-    \"\"\"\n-\n-    z_mean, z_log_var = args\n-    batch = K.shape(z_mean)[0]\n-    dim = K.int_shape(z_mean)[1]\n-    # by default, random_normal has mean = 0 and std = 1.0\n-    epsilon = K.random_normal(shape=(batch, dim))\n-    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n-\n-\n-def plot_results(models,\n-                 data,\n-                 batch_size=128,\n-                 model_name=\"vae_mnist\"):\n-    \"\"\"Plots labels and MNIST digits as a function of the 2D latent vector\n-\n-    # Arguments\n-        models (tuple): encoder and decoder models\n-        data (tuple): test data and label\n-        batch_size (int): prediction batch size\n-        model_name (string): which model is using this function\n-    \"\"\"\n-\n-    encoder, decoder = models\n-    x_test, y_test = data\n-    os.makedirs(model_name, exist_ok=True)\n-\n-    filename = os.path.join(model_name, \"vae_mean.png\")\n-    # display a 2D plot of the digit classes in the latent space\n-    z_mean, _, _ = encoder.predict(x_test,\n-                                   batch_size=batch_size)\n-    plt.figure(figsize=(12, 10))\n-    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n-    plt.colorbar()\n-    plt.xlabel(\"z[0]\")\n-    plt.ylabel(\"z[1]\")\n-    plt.savefig(filename)\n-    plt.show()\n-\n-    filename = os.path.join(model_name, \"digits_over_latent.png\")\n-    # display a 30x30 2D manifold of digits\n-    n = 30\n-    digit_size = 28\n-    figure = np.zeros((digit_size * n, digit_size * n))\n-    # linearly spaced coordinates corresponding to the 2D plot\n-    # of digit classes in the latent space\n-    grid_x = np.linspace(-4, 4, n)\n-    grid_y = np.linspace(-4, 4, n)[::-1]\n-\n-    for i, yi in enumerate(grid_y):\n-        for j, xi in enumerate(grid_x):\n-            z_sample = np.array([[xi, yi]])\n-            x_decoded = decoder.predict(z_sample)\n-            digit = x_decoded[0].reshape(digit_size, digit_size)\n-            figure[i * digit_size: (i + 1) * digit_size,\n-                   j * digit_size: (j + 1) * digit_size] = digit\n-\n-    plt.figure(figsize=(10, 10))\n-    start_range = digit_size // 2\n-    end_range = (n - 1) * digit_size + start_range + 1\n-    pixel_range = np.arange(start_range, end_range, digit_size)\n-    sample_range_x = np.round(grid_x, 1)\n-    sample_range_y = np.round(grid_y, 1)\n-    plt.xticks(pixel_range, sample_range_x)\n-    plt.yticks(pixel_range, sample_range_y)\n-    plt.xlabel(\"z[0]\")\n-    plt.ylabel(\"z[1]\")\n-    plt.imshow(figure, cmap='Greys_r')\n-    plt.savefig(filename)\n-    plt.show()\n-\n-\n-# MNIST dataset\n-(x_train, y_train), (x_test, y_test) = mnist.load_data()\n-\n-image_size = x_train.shape[1]\n-original_dim = image_size * image_size\n-x_train = np.reshape(x_train, [-1, original_dim])\n-x_test = np.reshape(x_test, [-1, original_dim])\n-x_train = x_train.astype('float32') / 255\n-x_test = x_test.astype('float32') / 255\n-\n-# network parameters\n-input_shape = (original_dim, )\n-intermediate_dim = 512\n-batch_size = 128\n-latent_dim = 2\n-epochs = 50\n-\n-# VAE model = encoder + decoder\n-# build encoder model\n-inputs = Input(shape=input_shape, name='encoder_input')\n-x = Dense(intermediate_dim, activation='relu')(inputs)\n-z_mean = Dense(latent_dim, name='z_mean')(x)\n-z_log_var = Dense(latent_dim, name='z_log_var')(x)\n-\n-# use reparameterization trick to push the sampling out as input\n-# note that \"output_shape\" isn't necessary with the TensorFlow backend\n-z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n-\n-# instantiate encoder model\n-encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n-encoder.summary()\n-plot_model(encoder, to_file='vae_mlp_encoder.png', show_shapes=True)\n-\n-# build decoder model\n-latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n-x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n-outputs = Dense(original_dim, activation='sigmoid')(x)\n-\n-# instantiate decoder model\n-decoder = Model(latent_inputs, outputs, name='decoder')\n-decoder.summary()\n-plot_model(decoder, to_file='vae_mlp_decoder.png', show_shapes=True)\n-\n-# instantiate VAE model\n-outputs = decoder(encoder(inputs)[2])\n-vae = Model(inputs, outputs, name='vae_mlp')\n-\n-if __name__ == '__main__':\n-    parser = argparse.ArgumentParser()\n-    help_ = \"Load h5 model trained weights\"\n-    parser.add_argument(\"-w\", \"--weights\", help=help_)\n-    help_ = \"Use mse loss instead of binary cross entropy (default)\"\n-    parser.add_argument(\"-m\",\n-                        \"--mse\",\n-                        help=help_, action='store_true')\n-    args = parser.parse_args()\n-    models = (encoder, decoder)\n-    data = (x_test, y_test)\n-\n-    # VAE loss = mse_loss or xent_loss + kl_loss\n-    if args.mse:\n-        reconstruction_loss = mse(inputs, outputs)\n-    else:\n-        reconstruction_loss = binary_crossentropy(inputs,\n-                                                  outputs)\n-\n-    reconstruction_loss *= original_dim\n-    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n-    kl_loss = K.sum(kl_loss, axis=-1)\n-    kl_loss *= -0.5\n-    vae_loss = K.mean(reconstruction_loss + kl_loss)\n-    vae.add_loss(vae_loss)\n-    vae.compile(optimizer='adam')\n-\n-    if args.weights:\n-        vae.load_weights(args.weights)\n-    else:\n-        # train the autoencoder\n-        vae.fit(x_train,\n-                epochs=epochs,\n-                batch_size=batch_size,\n-                validation_data=(x_test, None))\n-        vae.save_weights('vae_mlp_mnist.h5')\n-\n-    plot_results(models,\n-                 data,\n-                 batch_size=batch_size,\n-                 model_name=\"vae_mlp\")\n\n@@ -1,230 +0,0 @@\n-'''Example of VAE on MNIST dataset using CNN\n-\n-The VAE has a modular design. The encoder, decoder and VAE\n-are 3 models that share weights. After training the VAE model,\n-the encoder can be used to  generate latent vectors.\n-The decoder can be used to generate MNIST digits by sampling the\n-latent vector from a Gaussian distribution with mean=0 and std=1.\n-\n-# Reference\n-\n-[1] Kingma, Diederik P., and Max Welling.\n-\"Auto-encoding variational bayes.\"\n-https://arxiv.org/abs/1312.6114\n-'''\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from keras.layers import Dense, Input\n-from keras.layers import Conv2D, Flatten, Lambda\n-from keras.layers import Reshape, Conv2DTranspose\n-from keras.models import Model\n-from keras.datasets import mnist\n-from keras.losses import mse, binary_crossentropy\n-from keras.utils import plot_model\n-from keras import backend as K\n-\n-import numpy as np\n-import matplotlib.pyplot as plt\n-import argparse\n-import os\n-\n-\n-# reparameterization trick\n-# instead of sampling from Q(z|X), sample eps = N(0,I)\n-# then z = z_mean + sqrt(var)*eps\n-def sampling(args):\n-    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n-\n-    # Arguments\n-        args (tensor): mean and log of variance of Q(z|X)\n-\n-    # Returns\n-        z (tensor): sampled latent vector\n-    \"\"\"\n-\n-    z_mean, z_log_var = args\n-    batch = K.shape(z_mean)[0]\n-    dim = K.int_shape(z_mean)[1]\n-    # by default, random_normal has mean=0 and std=1.0\n-    epsilon = K.random_normal(shape=(batch, dim))\n-    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n-\n-\n-def plot_results(models,\n-                 data,\n-                 batch_size=128,\n-                 model_name=\"vae_mnist\"):\n-    \"\"\"Plots labels and MNIST digits as function of 2-dim latent vector\n-\n-    # Arguments\n-        models (tuple): encoder and decoder models\n-        data (tuple): test data and label\n-        batch_size (int): prediction batch size\n-        model_name (string): which model is using this function\n-    \"\"\"\n-\n-    encoder, decoder = models\n-    x_test, y_test = data\n-    os.makedirs(model_name, exist_ok=True)\n-\n-    filename = os.path.join(model_name, \"vae_mean.png\")\n-    # display a 2D plot of the digit classes in the latent space\n-    z_mean, _, _ = encoder.predict(x_test,\n-                                   batch_size=batch_size)\n-    plt.figure(figsize=(12, 10))\n-    plt.scatter(z_mean[:, 0], z_mean[:, 1], c=y_test)\n-    plt.colorbar()\n-    plt.xlabel(\"z[0]\")\n-    plt.ylabel(\"z[1]\")\n-    plt.savefig(filename)\n-    plt.show()\n-\n-    filename = os.path.join(model_name, \"digits_over_latent.png\")\n-    # display a 30x30 2D manifold of digits\n-    n = 30\n-    digit_size = 28\n-    figure = np.zeros((digit_size * n, digit_size * n))\n-    # linearly spaced coordinates corresponding to the 2D plot\n-    # of digit classes in the latent space\n-    grid_x = np.linspace(-4, 4, n)\n-    grid_y = np.linspace(-4, 4, n)[::-1]\n-\n-    for i, yi in enumerate(grid_y):\n-        for j, xi in enumerate(grid_x):\n-            z_sample = np.array([[xi, yi]])\n-            x_decoded = decoder.predict(z_sample)\n-            digit = x_decoded[0].reshape(digit_size, digit_size)\n-            figure[i * digit_size: (i + 1) * digit_size,\n-                   j * digit_size: (j + 1) * digit_size] = digit\n-\n-    plt.figure(figsize=(10, 10))\n-    start_range = digit_size // 2\n-    end_range = n * digit_size + start_range + 1\n-    pixel_range = np.arange(start_range, end_range, digit_size)\n-    sample_range_x = np.round(grid_x, 1)\n-    sample_range_y = np.round(grid_y, 1)\n-    plt.xticks(pixel_range, sample_range_x)\n-    plt.yticks(pixel_range, sample_range_y)\n-    plt.xlabel(\"z[0]\")\n-    plt.ylabel(\"z[1]\")\n-    plt.imshow(figure, cmap='Greys_r')\n-    plt.savefig(filename)\n-    plt.show()\n-\n-\n-# MNIST dataset\n-(x_train, y_train), (x_test, y_test) = mnist.load_data()\n-\n-image_size = x_train.shape[1]\n-x_train = np.reshape(x_train, [-1, image_size, image_size, 1])\n-x_test = np.reshape(x_test, [-1, image_size, image_size, 1])\n-x_train = x_train.astype('float32') / 255\n-x_test = x_test.astype('float32') / 255\n-\n-# network parameters\n-input_shape = (image_size, image_size, 1)\n-batch_size = 128\n-kernel_size = 3\n-filters = 16\n-latent_dim = 2\n-epochs = 30\n-\n-# VAE model = encoder + decoder\n-# build encoder model\n-inputs = Input(shape=input_shape, name='encoder_input')\n-x = inputs\n-for i in range(2):\n-    filters *= 2\n-    x = Conv2D(filters=filters,\n-               kernel_size=kernel_size,\n-               activation='relu',\n-               strides=2,\n-               padding='same')(x)\n-\n-# shape info needed to build decoder model\n-shape = K.int_shape(x)\n-\n-# generate latent vector Q(z|X)\n-x = Flatten()(x)\n-x = Dense(16, activation='relu')(x)\n-z_mean = Dense(latent_dim, name='z_mean')(x)\n-z_log_var = Dense(latent_dim, name='z_log_var')(x)\n-\n-# use reparameterization trick to push the sampling out as input\n-# note that \"output_shape\" isn't necessary with the TensorFlow backend\n-z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n-\n-# instantiate encoder model\n-encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n-encoder.summary()\n-plot_model(encoder, to_file='vae_cnn_encoder.png', show_shapes=True)\n-\n-# build decoder model\n-latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n-x = Dense(shape[1] * shape[2] * shape[3], activation='relu')(latent_inputs)\n-x = Reshape((shape[1], shape[2], shape[3]))(x)\n-\n-for i in range(2):\n-    x = Conv2DTranspose(filters=filters,\n-                        kernel_size=kernel_size,\n-                        activation='relu',\n-                        strides=2,\n-                        padding='same')(x)\n-    filters //= 2\n-\n-outputs = Conv2DTranspose(filters=1,\n-                          kernel_size=kernel_size,\n-                          activation='sigmoid',\n-                          padding='same',\n-                          name='decoder_output')(x)\n-\n-# instantiate decoder model\n-decoder = Model(latent_inputs, outputs, name='decoder')\n-decoder.summary()\n-plot_model(decoder, to_file='vae_cnn_decoder.png', show_shapes=True)\n-\n-# instantiate VAE model\n-outputs = decoder(encoder(inputs)[2])\n-vae = Model(inputs, outputs, name='vae')\n-\n-if __name__ == '__main__':\n-    parser = argparse.ArgumentParser()\n-    help_ = \"Load h5 model trained weights\"\n-    parser.add_argument(\"-w\", \"--weights\", help=help_)\n-    help_ = \"Use mse loss instead of binary cross entropy (default)\"\n-    parser.add_argument(\"-m\", \"--mse\", help=help_, action='store_true')\n-    args = parser.parse_args()\n-    models = (encoder, decoder)\n-    data = (x_test, y_test)\n-\n-    # VAE loss = mse_loss or xent_loss + kl_loss\n-    if args.mse:\n-        reconstruction_loss = mse(K.flatten(inputs), K.flatten(outputs))\n-    else:\n-        reconstruction_loss = binary_crossentropy(K.flatten(inputs),\n-                                                  K.flatten(outputs))\n-\n-    reconstruction_loss *= image_size * image_size\n-    kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n-    kl_loss = K.sum(kl_loss, axis=-1)\n-    kl_loss *= -0.5\n-    vae_loss = K.mean(reconstruction_loss + kl_loss)\n-    vae.add_loss(vae_loss)\n-    vae.compile(optimizer='rmsprop')\n-    vae.summary()\n-    plot_model(vae, to_file='vae_cnn.png', show_shapes=True)\n-\n-    if args.weights:\n-        vae.load_weights(args.weights)\n-    else:\n-        # train the autoencoder\n-        vae.fit(x_train,\n-                epochs=epochs,\n-                batch_size=batch_size,\n-                validation_data=(x_test, None))\n-        vae.save_weights('vae_cnn_mnist.h5')\n-\n-    plot_results(models, data, batch_size=batch_size, model_name=\"vae_cnn\")\n\n@@ -1,32 +0,0 @@\n-\r\n-try:\r\n-    from tensorflow.keras.layers.experimental.preprocessing import RandomRotation\r\n-except ImportError:\r\n-    raise ImportError(\r\n-        'Keras requires TensorFlow 2.2 or higher. '\r\n-        'Install TensorFlow via `pip install tensorflow`')\r\n-\r\n-from . import utils\r\n-from . import activations\r\n-from . import applications\r\n-from . import backend\r\n-from . import datasets\r\n-from . import engine\r\n-from . import layers\r\n-from . import preprocessing\r\n-from . import wrappers\r\n-from . import callbacks\r\n-from . import constraints\r\n-from . import initializers\r\n-from . import metrics\r\n-from . import models\r\n-from . import losses\r\n-from . import optimizers\r\n-from . import regularizers\r\n-\r\n-# Also importable from root\r\n-from .layers import Input\r\n-from .models import Model\r\n-from .models import Sequential\r\n-\r\n-__version__ = '2.4.3'\r\n\n@@ -1,17 +0,0 @@\n-\"\"\"Built-in activation functions.\"\"\"\n-\n-from tensorflow.keras.activations import softmax\n-from tensorflow.keras.activations import elu\n-from tensorflow.keras.activations import selu\n-from tensorflow.keras.activations import softplus\n-from tensorflow.keras.activations import softsign\n-from tensorflow.keras.activations import relu\n-from tensorflow.keras.activations import tanh\n-from tensorflow.keras.activations import sigmoid\n-from tensorflow.keras.activations import hard_sigmoid\n-from tensorflow.keras.activations import exponential\n-from tensorflow.keras.activations import linear\n-\n-from tensorflow.keras.activations import get\n-from tensorflow.keras.activations import serialize\n-from tensorflow.keras.activations import deserialize\n\n@@ -1,12 +0,0 @@\n-from .vgg16 import VGG16\n-from .vgg19 import VGG19\n-from .resnet50 import ResNet50\n-from .inception_v3 import InceptionV3\n-from .inception_resnet_v2 import InceptionResNetV2\n-from .xception import Xception\n-from .mobilenet import MobileNet\n-from .mobilenet_v2 import MobileNetV2\n-from .densenet import DenseNet121, DenseNet169, DenseNet201\n-from .nasnet import NASNetMobile, NASNetLarge\n-from .resnet import ResNet101, ResNet152\n-from .resnet_v2 import ResNet50V2, ResNet101V2, ResNet152V2\n\n@@ -1,5 +0,0 @@\n-from tensorflow.keras.applications.densenet import DenseNet121\n-from tensorflow.keras.applications.densenet import DenseNet169\n-from tensorflow.keras.applications.densenet import DenseNet201\n-from tensorflow.keras.applications.densenet import decode_predictions\n-from tensorflow.keras.applications.densenet import preprocess_input\n\n@@ -1,3 +0,0 @@\n-\"\"\"Utilities for ImageNet data preprocessing & prediction decoding.\"\"\"\n-from tensorflow.keras.applications.imagenet_utils import decode_predictions\n-from tensorflow.keras.applications.imagenet_utils import preprocess_input\n\n@@ -1,3 +0,0 @@\n-from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n-from tensorflow.keras.applications.inception_resnet_v2 import decode_predictions\n-from tensorflow.keras.applications.inception_resnet_v2 import preprocess_input\n\n@@ -1,3 +0,0 @@\n-from tensorflow.keras.applications.inception_v3 import InceptionV3\n-from tensorflow.keras.applications.inception_v3 import decode_predictions\n-from tensorflow.keras.applications.inception_v3 import preprocess_input\n\n@@ -1,3 +0,0 @@\n-from tensorflow.keras.applications.mobilenet import MobileNet\n-from tensorflow.keras.applications.mobilenet import decode_predictions\n-from tensorflow.keras.applications.mobilenet import preprocess_input\n\n@@ -1,3 +0,0 @@\n-from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\n-from tensorflow.keras.applications.mobilenet_v2 import decode_predictions\n-from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n\n@@ -1,4 +0,0 @@\n-from tensorflow.keras.applications.nasnet import NASNetMobile\n-from tensorflow.keras.applications.nasnet import NASNetLarge\n-from tensorflow.keras.applications.nasnet import decode_predictions\n-from tensorflow.keras.applications.nasnet import preprocess_input\n\n@@ -1,5 +0,0 @@\n-from tensorflow.keras.applications.resnet import ResNet50\n-from tensorflow.keras.applications.resnet import ResNet101\n-from tensorflow.keras.applications.resnet import ResNet152\n-from tensorflow.keras.applications.resnet import decode_predictions\n-from tensorflow.keras.applications.resnet import preprocess_input\n\n@@ -1,3 +0,0 @@\n-from tensorflow.keras.applications.resnet import ResNet50\n-from tensorflow.keras.applications.resnet import decode_predictions\n-from tensorflow.keras.applications.resnet import preprocess_input\n\n@@ -1,5 +0,0 @@\n-from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n-from tensorflow.keras.applications.resnet_v2 import ResNet101V2\n-from tensorflow.keras.applications.resnet_v2 import ResNet152V2\n-from tensorflow.keras.applications.resnet_v2 import decode_predictions\n-from tensorflow.keras.applications.resnet_v2 import preprocess_input\n\n@@ -1,3 +0,0 @@\n-from tensorflow.keras.applications.vgg16 import VGG16\n-from tensorflow.keras.applications.vgg16 import decode_predictions\n-from tensorflow.keras.applications.vgg16 import preprocess_input\n\n@@ -1,3 +0,0 @@\n-from tensorflow.keras.applications.vgg19 import VGG19\n-from tensorflow.keras.applications.vgg19 import decode_predictions\n-from tensorflow.keras.applications.vgg19 import preprocess_input\n\n@@ -1,3 +0,0 @@\n-from tensorflow.keras.applications.xception import Xception\n-from tensorflow.keras.applications.xception import decode_predictions\n-from tensorflow.keras.applications.xception import preprocess_input\n\n@@ -1 +0,0 @@\n-from tensorflow.keras.backend import *\n\\ No newline at end of file\n\n@@ -1 +0,0 @@\n-from tensorflow.keras.callbacks import *\n\n@@ -1,30 +0,0 @@\n-\"\"\"Constraints: functions that impose constraints on weight values.\n-\"\"\"\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from tensorflow.keras.constraints import Constraint\n-\n-from tensorflow.keras.constraints import MaxNorm\n-from tensorflow.keras.constraints import NonNeg\n-from tensorflow.keras.constraints import UnitNorm\n-from tensorflow.keras.constraints import MinMaxNorm\n-\n-from tensorflow.keras.constraints import get\n-from tensorflow.keras.constraints import serialize\n-from tensorflow.keras.constraints import deserialize\n-\n-# Aliases.\n-\n-max_norm = MaxNorm\n-non_neg = NonNeg\n-unit_norm = UnitNorm\n-min_max_norm = MinMaxNorm\n-\n-\n-# Legacy aliases.\n-maxnorm = max_norm\n-nonneg = non_neg\n-unitnorm = unit_norm\n-\n\n@@ -1,7 +0,0 @@\n-from . import mnist\n-from . import imdb\n-from . import reuters\n-from . import cifar10\n-from . import cifar100\n-from . import boston_housing\n-from . import fashion_mnist\n\n@@ -1,3 +0,0 @@\n-\"\"\"Boston housing price regression dataset.\"\"\"\n-\n-from tensorflow.keras.datasets.boston_housing import load_data\n\n@@ -1,3 +0,0 @@\n-\"\"\"CIFAR10 small images classification dataset.\"\"\"\n-\n-from tensorflow.keras.datasets.cifar10 import load_data\n\n@@ -1,3 +0,0 @@\n-\"\"\"CIFAR100 small images classification dataset.\"\"\"\n-\n-from tensorflow.keras.datasets.cifar100 import load_data\n\n@@ -1,3 +0,0 @@\n-\"\"\"Fashion-MNIST dataset.\"\"\"\n-\n-from tensorflow.keras.datasets.fashion_mnist import load_data\n\n@@ -1,4 +0,0 @@\n-\"\"\"IMDB sentiment classification dataset.\"\"\"\n-\n-from tensorflow.keras.datasets.imdb import load_data\n-from tensorflow.keras.datasets.imdb import get_word_index\n\n@@ -1,3 +0,0 @@\n-\"\"\"MNIST handwritten digits dataset.\"\"\"\n-\n-from tensorflow.keras.datasets.mnist import load_data\n\n@@ -1,4 +0,0 @@\n-\"\"\"Reuters topic classification dataset.\"\"\"\n-\n-from tensorflow.keras.datasets.reuters import load_data\n-from tensorflow.keras.datasets.reuters import get_word_index\n\n@@ -1,8 +0,0 @@\n-# note: `Node` is an internal class,\n-# it isn't meant to be used by Keras users.\n-from .input_layer import Input\n-from .input_layer import InputLayer\n-from .base_layer import InputSpec\n-from .base_layer import Layer\n-from .network import get_source_inputs\n-from .training import Model\n\n@@ -1,3 +0,0 @@\n-\"\"\"Contains the base Layer class, from which all layers inherit.\"\"\"\n-from tensorflow.keras.layers import Layer\n-from tensorflow.keras.layers import InputSpec\n\n@@ -1,4 +0,0 @@\n-\"\"\"Input layer code (`Input` and `InputLayer`).\"\"\"\n-from tensorflow.keras.layers import InputLayer\n-from tensorflow.keras import Input\n-\n\n@@ -1,4 +0,0 @@\n-\"\"\"A `Network` is way to compose layers: the topological form of a `Model`.\"\"\"\n-\n-from tensorflow.keras import Model as Network\n-from tensorflow.keras.utils import get_source_inputs\n\n@@ -1,6 +0,0 @@\n-\"\"\"Model saving utilities.\"\"\"\n-from tensorflow.keras.models import save_model\n-from tensorflow.keras.models import load_model\n-from tensorflow.keras.models import model_from_config\n-from tensorflow.keras.models import model_from_yaml\n-from tensorflow.keras.models import model_from_json\n\n@@ -1,2 +0,0 @@\n-\"\"\"Sequential model class.\"\"\"\n-from tensorflow.keras import Sequential\n\n@@ -1,5 +0,0 @@\n-\"\"\"This module is deprecated, but kept around for backwards compatibility.\n-\"\"\"\n-from .base_layer import Layer, InputSpec\n-from .input_layer import Input, InputLayer\n-from .network import Network, get_source_inputs\n\n@@ -1,2 +0,0 @@\n-\"\"\"Training-related part of the Keras engine.\"\"\"\n-from tensorflow.keras import Model\n\n@@ -1,3 +0,0 @@\n-\"\"\"Built-in weight initializers.\"\"\"\n-\n-from tensorflow.keras.initializers import *\n\\ No newline at end of file\n\n@@ -1,15 +0,0 @@\n-from tensorflow.keras.layers import *\n-\n-from . import advanced_activations\n-from . import convolutional\n-from . import convolutional_recurrent\n-from . import core\n-from . import embeddings\n-from . import experimental\n-from . import local\n-from . import merge\n-from . import noise\n-from . import normalization\n-from . import pooling\n-from . import recurrent\n-from . import wrappers\n\n@@ -1,8 +0,0 @@\n-\"\"\"Layers that act as activation functions.\"\"\"\n-\n-from tensorflow.keras.layers import LeakyReLU\n-from tensorflow.keras.layers import PReLU\n-from tensorflow.keras.layers import ELU\n-from tensorflow.keras.layers import ThresholdedReLU\n-from tensorflow.keras.layers import Softmax\n-from tensorflow.keras.layers import ReLU\n\n@@ -1,38 +0,0 @@\n-\"\"\"Convolutional layers.\"\"\"\n-from tensorflow.keras.layers import Conv1D\n-from tensorflow.keras.layers import Conv2D\n-from tensorflow.keras.layers import Conv3D\n-from tensorflow.keras.layers import Conv2DTranspose\n-from tensorflow.keras.layers import Conv3DTranspose\n-from tensorflow.keras.layers import SeparableConv1D\n-from tensorflow.keras.layers import SeparableConv2D\n-from tensorflow.keras.layers import DepthwiseConv2D\n-from tensorflow.keras.layers import UpSampling1D\n-from tensorflow.keras.layers import UpSampling2D\n-from tensorflow.keras.layers import UpSampling3D\n-from tensorflow.keras.layers import ZeroPadding1D\n-from tensorflow.keras.layers import ZeroPadding2D\n-from tensorflow.keras.layers import ZeroPadding3D\n-from tensorflow.keras.layers import Cropping1D\n-from tensorflow.keras.layers import Cropping2D\n-from tensorflow.keras.layers import Cropping3D\n-\n-# Aliases\n-\n-Convolution1D = Conv1D\n-Convolution2D = Conv2D\n-Convolution3D = Conv3D\n-SeparableConvolution1D = SeparableConv1D\n-SeparableConvolution2D = SeparableConv2D\n-Convolution2DTranspose = Conv2DTranspose\n-Deconvolution2D = Deconv2D = Conv2DTranspose\n-Deconvolution3D = Deconv3D = Conv3DTranspose\n-\n-# imports for backwards namespace compatibility\n-\n-from .pooling import AveragePooling1D\n-from .pooling import AveragePooling2D\n-from .pooling import AveragePooling3D\n-from .pooling import MaxPooling1D\n-from .pooling import MaxPooling2D\n-from .pooling import MaxPooling3D\n\n@@ -1,3 +0,0 @@\n-\"\"\"Convolutional-recurrent layers.\"\"\"\n-\n-from tensorflow.keras.layers import ConvLSTM2D\n\n@@ -1,17 +0,0 @@\n-\"\"\"Core Keras layers.\"\"\"\n-\n-from tensorflow.keras.layers import Dense\n-from tensorflow.keras.layers import Activation\n-from tensorflow.keras.layers import ActivityRegularization\n-from tensorflow.keras.layers import Lambda\n-from tensorflow.keras.layers import Masking\n-\n-from tensorflow.keras.layers import Dropout\n-from tensorflow.keras.layers import SpatialDropout1D\n-from tensorflow.keras.layers import SpatialDropout2D\n-from tensorflow.keras.layers import SpatialDropout3D\n-\n-from tensorflow.keras.layers import Flatten\n-from tensorflow.keras.layers import Permute\n-from tensorflow.keras.layers import RepeatVector\n-from tensorflow.keras.layers import Reshape\n\n@@ -1,4 +0,0 @@\n-\"\"\"Recurrent layers backed by cuDNN.\"\"\"\n-\n-from tensorflow.keras.layers import GRU as CuDNNGRU\n-from tensorflow.keras.layers import LSTM as CuDNNLSTM\n\n@@ -1,3 +0,0 @@\n-\"\"\"Embedding layer.\"\"\"\n-\n-from tensorflow.keras.layers import Embedding\n\n@@ -1 +0,0 @@\n-from tensorflow.keras.layers.experimental import *\n\\ No newline at end of file\n\n@@ -1 +0,0 @@\n-from tensorflow.keras.layers.experimental.preprocessing import *\n\n@@ -1,4 +0,0 @@\n-\"\"\"Locally-connected layers.\"\"\"\n-\n-from tensorflow.keras.layers import LocallyConnected1D\n-from tensorflow.keras.layers import LocallyConnected2D\n\n@@ -1,18 +0,0 @@\n-\"\"\"Layers that can merge several inputs into one.\"\"\"\n-\n-from tensorflow.keras.layers import Add\n-from tensorflow.keras.layers import Subtract\n-from tensorflow.keras.layers import Multiply\n-from tensorflow.keras.layers import Average\n-from tensorflow.keras.layers import Maximum\n-from tensorflow.keras.layers import Minimum\n-from tensorflow.keras.layers import Concatenate\n-from tensorflow.keras.layers import Dot\n-from tensorflow.keras.layers import add\n-from tensorflow.keras.layers import subtract\n-from tensorflow.keras.layers import multiply\n-from tensorflow.keras.layers import average\n-from tensorflow.keras.layers import maximum\n-from tensorflow.keras.layers import minimum\n-from tensorflow.keras.layers import concatenate\n-from tensorflow.keras.layers import dot\n\n@@ -1,5 +0,0 @@\n-\"\"\"Layers that operate regularization via the addition of noise.\"\"\"\n-\n-from tensorflow.keras.layers import GaussianNoise\n-from tensorflow.keras.layers import GaussianDropout\n-from tensorflow.keras.layers import AlphaDropout\n\n@@ -1,3 +0,0 @@\n-\"\"\"Normalization layers.\"\"\"\n-\n-from tensorflow.keras.layers import BatchNormalization\n\n@@ -1,33 +0,0 @@\n-\"\"\"Pooling layers.\"\"\"\n-\n-from tensorflow.keras.layers import MaxPooling1D\n-from tensorflow.keras.layers import MaxPooling2D\n-from tensorflow.keras.layers import MaxPooling3D\n-\n-from tensorflow.keras.layers import AveragePooling1D\n-from tensorflow.keras.layers import AveragePooling2D\n-from tensorflow.keras.layers import AveragePooling3D\n-\n-from tensorflow.keras.layers import GlobalAveragePooling1D\n-from tensorflow.keras.layers import GlobalAveragePooling2D\n-from tensorflow.keras.layers import GlobalAveragePooling3D\n-\n-from tensorflow.keras.layers import GlobalMaxPooling1D\n-from tensorflow.keras.layers import GlobalMaxPooling2D\n-from tensorflow.keras.layers import GlobalMaxPooling3D\n-\n-\n-# Aliases\n-\n-AvgPool1D = AveragePooling1D\n-MaxPool1D = MaxPooling1D\n-AvgPool2D = AveragePooling2D\n-MaxPool2D = MaxPooling2D\n-AvgPool3D = AveragePooling3D\n-MaxPool3D = MaxPooling3D\n-GlobalMaxPool1D = GlobalMaxPooling1D\n-GlobalMaxPool2D = GlobalMaxPooling2D\n-GlobalMaxPool3D = GlobalMaxPooling3D\n-GlobalAvgPool1D = GlobalAveragePooling1D\n-GlobalAvgPool2D = GlobalAveragePooling2D\n-GlobalAvgPool3D = GlobalAveragePooling3D\n\n@@ -1,12 +0,0 @@\n-\"\"\"Recurrent layers and their base classes.\"\"\"\n-\n-from tensorflow.keras.layers import RNN\n-from tensorflow.keras.layers import StackedRNNCells\n-\n-from tensorflow.keras.layers import SimpleRNN\n-from tensorflow.keras.layers import GRU\n-from tensorflow.keras.layers import LSTM\n-\n-from tensorflow.keras.layers import SimpleRNNCell\n-from tensorflow.keras.layers import GRUCell\n-from tensorflow.keras.layers import LSTMCell\n\n@@ -1,5 +0,0 @@\n-\"\"\"Layers that augment the functionality of a base layer.\"\"\"\n-\n-from tensorflow.keras.layers import Wrapper\n-from tensorflow.keras.layers import TimeDistributed\n-from tensorflow.keras.layers import Bidirectional\n\n@@ -1,2 +0,0 @@\n-\"\"\"Built-in loss functions.\"\"\"\n-from tensorflow.keras.losses import *\n\n@@ -1,2 +0,0 @@\n-\"\"\"Built-in metrics.\"\"\"\n-from tensorflow.keras.metrics import *\n\n@@ -1,12 +0,0 @@\n-\"\"\"Model-related utilities.\"\"\"\n-from .engine.input_layer import Input\n-from .engine.input_layer import InputLayer\n-from .engine.training import Model\n-from .engine.sequential import Sequential\n-from .engine.saving import save_model\n-from .engine.saving import load_model\n-from .engine.saving import model_from_config\n-from .engine.saving import model_from_yaml\n-from .engine.saving import model_from_json\n-\n-from tensorflow.keras.models import clone_model\n\n@@ -1,6 +0,0 @@\n-\"\"\"Legacy objectives module.\n-\n-Only kept for backwards API compatibility.\n-\"\"\"\n-from __future__ import absolute_import\n-from .losses import *\n\n@@ -1 +0,0 @@\n-from tensorflow.keras.optimizers import *\n\n@@ -1 +0,0 @@\n-from tensorflow.keras.optimizers.schedules import *\n\n@@ -1 +0,0 @@\n-from tensorflow.keras.preprocessing import *\n\n@@ -1,3 +0,0 @@\n-\"\"\"Utilities for real-time data augmentation on image data.\"\"\"\n-\n-from tensorflow.keras.preprocessing.image import *\n\n@@ -1,3 +0,0 @@\n-\"\"\"Utilities for preprocessing sequence data.\"\"\"\n-\n-from tensorflow.keras.preprocessing.sequence import *\n\n@@ -1,3 +0,0 @@\n-\"\"\"Utilities for text input preprocessing.\"\"\"\n-\n-from tensorflow.keras.preprocessing.text import *\n\\ No newline at end of file\n\n@@ -1,2 +0,0 @@\n-\"\"\"Built-in regularizers.\"\"\"\n-from tensorflow.keras.regularizers import *\n\n@@ -1 +0,0 @@\n-from tensorflow.keras.utils import *\n\n@@ -1,173 +0,0 @@\n-\"\"\"Utilities used in convolutional layers.\"\"\"\n-\n-import numpy as np\n-\n-\n-def normalize_tuple(value, n, name):\n-    \"\"\"Transforms a single int or iterable of ints into an int tuple.\n-\n-    # Arguments\n-        value: The value to validate and convert. Could be an int, or any iterable\n-          of ints.\n-        n: The size of the tuple to be returned.\n-        name: The name of the argument being validated, e.g. `strides` or\n-          `kernel_size`. This is only used to format error messages.\n-\n-    # Returns\n-        A tuple of n integers.\n-\n-    # Raises\n-        ValueError: If something else than an int/long or iterable thereof was\n-        passed.\n-    \"\"\"\n-    if isinstance(value, int):\n-        return (value,) * n\n-    else:\n-        try:\n-            value_tuple = tuple(value)\n-        except TypeError:\n-            raise ValueError('The `{}` argument must be a tuple of {} '\n-                             'integers. Received: {}'.format(name, n, value))\n-        if len(value_tuple) != n:\n-            raise ValueError('The `{}` argument must be a tuple of {} '\n-                             'integers. Received: {}'.format(name, n, value))\n-        for single_value in value_tuple:\n-            try:\n-                int(single_value)\n-            except ValueError:\n-                raise ValueError('The `{}` argument must be a tuple of {} '\n-                                 'integers. Received: {} including element {} '\n-                                 'of type {}'.format(name, n, value, single_value,\n-                                                     type(single_value)))\n-    return value_tuple\n-\n-\n-def normalize_padding(value):\n-    padding = value.lower()\n-    allowed = {'valid', 'same', 'causal'}\n-    if padding not in allowed:\n-        raise ValueError('The `padding` argument must be one of \"valid\", \"same\" '\n-                         '(or \"causal\" for Conv1D). Received: {}'.format(padding))\n-    return padding\n-\n-\n-def convert_kernel(kernel):\n-    \"\"\"Converts a Numpy kernel matrix from Theano format to TensorFlow format.\n-\n-    Also works reciprocally, since the transformation is its own inverse.\n-\n-    # Arguments\n-        kernel: Numpy array (3D, 4D or 5D).\n-\n-    # Returns\n-        The converted kernel.\n-\n-    # Raises\n-        ValueError: in case of invalid kernel shape or invalid data_format.\n-    \"\"\"\n-    kernel = np.asarray(kernel)\n-    if not 3 <= kernel.ndim <= 5:\n-        raise ValueError('Invalid kernel shape:', kernel.shape)\n-    slices = [slice(None, None, -1) for _ in range(kernel.ndim)]\n-    no_flip = (slice(None, None), slice(None, None))\n-    slices[-2:] = no_flip\n-    return np.copy(kernel[tuple(slices)])\n-\n-\n-def conv_output_length(input_length, filter_size,\n-                       padding, stride, dilation=1):\n-    \"\"\"Determines output length of a convolution given input length.\n-\n-    # Arguments\n-        input_length: integer.\n-        filter_size: integer.\n-        padding: one of `\"same\"`, `\"valid\"`, `\"full\"`.\n-        stride: integer.\n-        dilation: dilation rate, integer.\n-\n-    # Returns\n-        The output length (integer).\n-    \"\"\"\n-    if input_length is None:\n-        return None\n-    assert padding in {'same', 'valid', 'full', 'causal'}\n-    dilated_filter_size = (filter_size - 1) * dilation + 1\n-    if padding == 'same':\n-        output_length = input_length\n-    elif padding == 'valid':\n-        output_length = input_length - dilated_filter_size + 1\n-    elif padding == 'causal':\n-        output_length = input_length\n-    elif padding == 'full':\n-        output_length = input_length + dilated_filter_size - 1\n-    return (output_length + stride - 1) // stride\n-\n-\n-def conv_input_length(output_length, filter_size, padding, stride):\n-    \"\"\"Determines input length of a convolution given output length.\n-\n-    # Arguments\n-        output_length: integer.\n-        filter_size: integer.\n-        padding: one of `\"same\"`, `\"valid\"`, `\"full\"`.\n-        stride: integer.\n-\n-    # Returns\n-        The input length (integer).\n-    \"\"\"\n-    if output_length is None:\n-        return None\n-    assert padding in {'same', 'valid', 'full'}\n-    if padding == 'same':\n-        pad = filter_size // 2\n-    elif padding == 'valid':\n-        pad = 0\n-    elif padding == 'full':\n-        pad = filter_size - 1\n-    return (output_length - 1) * stride - 2 * pad + filter_size\n-\n-\n-def deconv_length(dim_size, stride_size, kernel_size, padding,\n-                  output_padding, dilation=1):\n-    \"\"\"Determines output length of a transposed convolution given input length.\n-\n-    # Arguments\n-        dim_size: Integer, the input length.\n-        stride_size: Integer, the stride along the dimension of `dim_size`.\n-        kernel_size: Integer, the kernel size along the dimension of\n-            `dim_size`.\n-        padding: One of `\"same\"`, `\"valid\"`, `\"full\"`.\n-        output_padding: Integer, amount of padding along the output dimension,\n-            Can be set to `None` in which case the output length is inferred.\n-        dilation: dilation rate, integer.\n-\n-    # Returns\n-        The output length (integer).\n-    \"\"\"\n-    assert padding in {'same', 'valid', 'full'}\n-    if dim_size is None:\n-        return None\n-\n-    # Get the dilated kernel size\n-    kernel_size = (kernel_size - 1) * dilation + 1\n-\n-    # Infer length if output padding is None, else compute the exact length\n-    if output_padding is None:\n-        if padding == 'valid':\n-            dim_size = dim_size * stride_size + max(kernel_size - stride_size, 0)\n-        elif padding == 'full':\n-            dim_size = dim_size * stride_size - (stride_size + kernel_size - 2)\n-        elif padding == 'same':\n-            dim_size = dim_size * stride_size\n-    else:\n-        if padding == 'same':\n-            pad = kernel_size // 2\n-        elif padding == 'valid':\n-            pad = 0\n-        elif padding == 'full':\n-            pad = kernel_size - 1\n-\n-        dim_size = ((dim_size - 1) * stride_size + kernel_size - 2 * pad +\n-                    output_padding)\n-\n-    return dim_size\n\n@@ -1,8 +0,0 @@\n-\"\"\"Utilities for file download and caching.\"\"\"\n-\n-from tensorflow.keras.utils import get_file\n-\n-from tensorflow.keras.utils import Sequence\n-from tensorflow.keras.utils import SequenceEnqueuer\n-from tensorflow.keras.utils import OrderedEnqueuer\n-from tensorflow.keras.utils import GeneratorEnqueuer\n\n@@ -1,199 +0,0 @@\n-\"\"\"Python utilities required by Keras.\"\"\"\n-\n-import inspect\n-import sys\n-\n-from tensorflow.keras.utils import CustomObjectScope\n-from tensorflow.keras.utils import custom_object_scope\n-from tensorflow.keras.utils import get_custom_objects\n-from tensorflow.keras.utils import serialize_keras_object\n-from tensorflow.keras.utils import deserialize_keras_object\n-from tensorflow.keras.utils import Progbar\n-\n-\n-def to_list(x, allow_tuple=False):\n-    \"\"\"Normalizes a list/tensor into a list.\n-\n-    If a tensor is passed, we return\n-    a list of size 1 containing the tensor.\n-\n-    # Arguments\n-        x: target object to be normalized.\n-        allow_tuple: If False and x is a tuple,\n-            it will be converted into a list\n-            with a single element (the tuple).\n-            Else converts the tuple to a list.\n-\n-    # Returns\n-        A list.\n-    \"\"\"\n-    if isinstance(x, list):\n-        return x\n-    if allow_tuple and isinstance(x, tuple):\n-        return list(x)\n-    return [x]\n-\n-\n-def unpack_singleton(x):\n-    \"\"\"Gets the first element if the iterable has only one value.\n-\n-    Otherwise return the iterable.\n-\n-    # Argument\n-        x: A list or tuple.\n-\n-    # Returns\n-        The same iterable or the first element.\n-    \"\"\"\n-    if len(x) == 1:\n-        return x[0]\n-    return x\n-\n-\n-def object_list_uid(object_list):\n-    object_list = to_list(object_list)\n-    return ', '.join((str(abs(id(x))) for x in object_list))\n-\n-\n-def is_all_none(iterable_or_element):\n-    iterable = to_list(iterable_or_element, allow_tuple=True)\n-    for element in iterable:\n-        if element is not None:\n-            return False\n-    return True\n-\n-\n-def slice_arrays(arrays, start=None, stop=None):\n-    \"\"\"Slices an array or list of arrays.\n-\n-    This takes an array-like, or a list of\n-    array-likes, and outputs:\n-        - arrays[start:stop] if `arrays` is an array-like\n-        - [x[start:stop] for x in arrays] if `arrays` is a list\n-\n-    Can also work on list/array of indices: `_slice_arrays(x, indices)`\n-\n-    # Arguments\n-        arrays: Single array or list of arrays.\n-        start: can be an integer index (start index)\n-            or a list/array of indices\n-        stop: integer (stop index); should be None if\n-            `start` was a list.\n-\n-    # Returns\n-        A slice of the array(s).\n-    \"\"\"\n-    if arrays is None:\n-        return [None]\n-    elif isinstance(arrays, list):\n-        if hasattr(start, '__len__'):\n-            # hdf5 datasets only support list objects as indices\n-            if hasattr(start, 'shape'):\n-                start = start.tolist()\n-            return [None if x is None else x[start] for x in arrays]\n-        else:\n-            return [None if x is None else x[start:stop] for x in arrays]\n-    else:\n-        if hasattr(start, '__len__'):\n-            if hasattr(start, 'shape'):\n-                start = start.tolist()\n-            return arrays[start]\n-        elif hasattr(start, '__getitem__'):\n-            return arrays[start:stop]\n-        else:\n-            return [None]\n-\n-\n-def transpose_shape(shape, target_format, spatial_axes):\n-    \"\"\"Converts a tuple or a list to the correct `data_format`.\n-\n-    It does so by switching the positions of its elements.\n-\n-    # Arguments\n-        shape: Tuple or list, often representing shape,\n-            corresponding to `'channels_last'`.\n-        target_format: A string, either `'channels_first'` or `'channels_last'`.\n-        spatial_axes: A tuple of integers.\n-            Correspond to the indexes of the spatial axes.\n-            For example, if you pass a shape\n-            representing (batch_size, timesteps, rows, cols, channels),\n-            then `spatial_axes=(2, 3)`.\n-\n-    # Returns\n-        A tuple or list, with the elements permuted according\n-        to `target_format`.\n-\n-    # Example\n-    ```python\n-        >>> from keras.utils.generic_utils import transpose_shape\n-        >>> transpose_shape((16, 128, 128, 32),'channels_first', spatial_axes=(1, 2))\n-        (16, 32, 128, 128)\n-        >>> transpose_shape((16, 128, 128, 32), 'channels_last', spatial_axes=(1, 2))\n-        (16, 128, 128, 32)\n-        >>> transpose_shape((128, 128, 32), 'channels_first', spatial_axes=(0, 1))\n-        (32, 128, 128)\n-    ```\n-\n-    # Raises\n-        ValueError: if `value` or the global `data_format` invalid.\n-    \"\"\"\n-    if target_format == 'channels_first':\n-        new_values = shape[:spatial_axes[0]]\n-        new_values += (shape[-1],)\n-        new_values += tuple(shape[x] for x in spatial_axes)\n-\n-        if isinstance(shape, list):\n-            return list(new_values)\n-        return new_values\n-    elif target_format == 'channels_last':\n-        return shape\n-    else:\n-        raise ValueError('The `data_format` argument must be one of '\n-                         '\"channels_first\", \"channels_last\". Received: ' +\n-                         str(target_format))\n-\n-\n-def check_for_unexpected_keys(name, input_dict, expected_values):\n-    unknown = set(input_dict.keys()).difference(expected_values)\n-    if unknown:\n-        raise ValueError('Unknown entries in {} dictionary: {}. Only expected '\n-                         'following keys: {}'.format(name, list(unknown),\n-                                                     expected_values))\n-\n-\n-def has_arg(fn, name, accept_all=False):\n-    \"\"\"Checks if a callable accepts a given keyword argument.\n-    For Python 2, checks if there is an argument with the given name.\n-    For Python 3, checks if there is an argument with the given name, and\n-    also whether this argument can be called with a keyword (i.e. if it is\n-    not a positional-only argument).\n-    # Arguments\n-        fn: Callable to inspect.\n-        name: Check if `fn` can be called with `name` as a keyword argument.\n-        accept_all: What to return if there is no parameter called `name`\n-                    but the function accepts a `**kwargs` argument.\n-    # Returns\n-        bool, whether `fn` accepts a `name` keyword argument.\n-    \"\"\"\n-    if sys.version_info < (3,):\n-        arg_spec = inspect.getargspec(fn)\n-        if accept_all and arg_spec.keywords is not None:\n-            return True\n-        return (name in arg_spec.args)\n-    elif sys.version_info < (3, 3):\n-        arg_spec = inspect.getfullargspec(fn)\n-        if accept_all and arg_spec.varkw is not None:\n-            return True\n-        return (name in arg_spec.args or\n-                name in arg_spec.kwonlyargs)\n-    else:\n-        signature = inspect.signature(fn)\n-        parameter = signature.parameters.get(name)\n-        if parameter is None:\n-            if accept_all:\n-                for param in signature.parameters.values():\n-                    if param.kind == inspect.Parameter.VAR_KEYWORD:\n-                        return True\n-            return False\n-        return (parameter.kind in (inspect.Parameter.POSITIONAL_OR_KEYWORD,\n-                                   inspect.Parameter.KEYWORD_ONLY))\n\n@@ -1,3 +0,0 @@\n-\"\"\"Utilities related to disk I/O.\"\"\"\n-\n-from tensorflow.keras.utils import HDF5Matrix\n\n@@ -1,91 +0,0 @@\n-\"\"\"Utilities related to layer/model functionality.\n-\"\"\"\n-from .conv_utils import convert_kernel\n-from .. import backend as K\n-import numpy as np\n-\n-\n-from tensorflow.keras.utils import get_source_inputs\n-from tensorflow.python.keras.utils.layer_utils import print_summary\n-\n-\n-def count_params(weights):\n-    \"\"\"Count the total number of scalars composing the weights.\n-\n-    # Arguments\n-        weights: An iterable containing the weights on which to compute params\n-\n-    # Returns\n-        The total number of scalars composing the weights\n-    \"\"\"\n-    weight_ids = set()\n-    total = 0\n-    for w in weights:\n-        if id(w) not in weight_ids:\n-            weight_ids.add(id(w))\n-            total += int(K.count_params(w))\n-    return total\n-\n-\n-def convert_all_kernels_in_model(model):\n-    \"\"\"Converts all convolution kernels in a model from Theano to TensorFlow.\n-\n-    Also works from TensorFlow to Theano.\n-\n-    # Arguments\n-        model: target model for the conversion.\n-    \"\"\"\n-    # Note: SeparableConvolution not included\n-    # since only supported by TF.\n-    conv_classes = {\n-        'Conv1D',\n-        'Conv2D',\n-        'Conv3D',\n-        'Conv2DTranspose',\n-    }\n-    to_assign = []\n-    for layer in model.layers:\n-        if layer.__class__.__name__ in conv_classes:\n-            original_kernel = K.get_value(layer.kernel)\n-            converted_kernel = convert_kernel(original_kernel)\n-            to_assign.append((layer.kernel, converted_kernel))\n-    K.batch_set_value(to_assign)\n-\n-\n-def convert_dense_weights_data_format(dense,\n-                                      previous_feature_map_shape,\n-                                      target_data_format='channels_first'):\n-    \"\"\"Utility useful when changing a convnet's `data_format`.\n-\n-    When porting the weights of a convnet from one data format to the other,\n-    if the convnet includes a `Flatten` layer\n-    (applied to the last convolutional feature map)\n-    followed by a `Dense` layer, the weights of that `Dense` layer\n-    should be updated to reflect the new dimension ordering.\n-\n-    # Arguments\n-        dense: The target `Dense` layer.\n-        previous_feature_map_shape: A shape tuple of 3 integers,\n-            e.g. `(512, 7, 7)`. The shape of the convolutional\n-            feature map right before the `Flatten` layer that\n-            came before the target `Dense` layer.\n-        target_data_format: One of \"channels_last\", \"channels_first\".\n-            Set it \"channels_last\"\n-            if converting a \"channels_first\" model to \"channels_last\",\n-            or reciprocally.\n-    \"\"\"\n-    assert target_data_format in {'channels_last', 'channels_first'}\n-    kernel, bias = dense.get_weights()\n-    for i in range(kernel.shape[1]):\n-        if target_data_format == 'channels_first':\n-            c, h, w = previous_feature_map_shape\n-            original_fm_shape = (h, w, c)\n-            ki = kernel[:, i].reshape(original_fm_shape)\n-            ki = np.transpose(ki, (2, 0, 1))  # last -> first\n-        else:\n-            h, w, c = previous_feature_map_shape\n-            original_fm_shape = (c, h, w)\n-            ki = kernel[:, i].reshape(original_fm_shape)\n-            ki = np.transpose(ki, (1, 2, 0))  # first -> last\n-        kernel[:, i] = np.reshape(ki, (np.prod(previous_feature_map_shape),))\n-    dense.set_weights([kernel, bias])\n\n@@ -1,3 +0,0 @@\n-\"\"\"Multi-GPU training utilities.\"\"\"\n-\n-from tensorflow.keras.utils import multi_gpu_model\n\n@@ -1,4 +0,0 @@\n-\"\"\"Numpy-related utilities.\"\"\"\n-\n-from tensorflow.keras.utils import to_categorical\n-from tensorflow.keras.utils import normalize\n\n@@ -1,117 +0,0 @@\n-\"\"\"Utilities related to Keras unit tests.\"\"\"\n-from __future__ import absolute_import\n-\n-import numpy as np\n-from numpy.testing import assert_allclose\n-\n-from .generic_utils import has_arg\n-from ..engine import Model, Input\n-from .. import backend as K\n-\n-\n-\n-def get_test_data(num_train=1000, num_test=500, input_shape=(10,),\n-                  output_shape=(2,),\n-                  classification=True, num_classes=2):\n-    \"\"\"Generates test data to train a model on.\n-\n-    classification=True overrides output_shape\n-    (i.e. output_shape is set to (1,)) and the output\n-    consists in integers in [0, num_classes-1].\n-\n-    Otherwise: float output with shape output_shape.\n-    \"\"\"\n-    samples = num_train + num_test\n-    if classification:\n-        y = np.random.randint(0, num_classes, size=(samples,))\n-        X = np.zeros((samples,) + input_shape, dtype=np.float32)\n-        for i in range(samples):\n-            X[i] = np.random.normal(loc=y[i], scale=0.7, size=input_shape)\n-    else:\n-        y_loc = np.random.random((samples,))\n-        X = np.zeros((samples,) + input_shape, dtype=np.float32)\n-        y = np.zeros((samples,) + output_shape, dtype=np.float32)\n-        for i in range(samples):\n-            X[i] = np.random.normal(loc=y_loc[i], scale=0.7, size=input_shape)\n-            y[i] = np.random.normal(loc=y_loc[i], scale=0.7, size=output_shape)\n-\n-    return (X[:num_train], y[:num_train]), (X[num_train:], y[num_train:])\n-\n-\n-def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n-               input_data=None, expected_output=None,\n-               expected_output_dtype=None, fixed_batch_size=False):\n-    \"\"\"Test routine for a layer with a single input tensor\n-    and single output tensor.\n-    \"\"\"\n-    # generate input data\n-    if input_data is None:\n-        assert input_shape\n-        if not input_dtype:\n-            input_dtype = K.floatx()\n-        input_data_shape = list(input_shape)\n-        for i, e in enumerate(input_data_shape):\n-            if e is None:\n-                input_data_shape[i] = np.random.randint(1, 4)\n-        input_data = (10 * np.random.random(input_data_shape))\n-        input_data = input_data.astype(input_dtype)\n-    else:\n-        if input_shape is None:\n-            input_shape = input_data.shape\n-        if input_dtype is None:\n-            input_dtype = input_data.dtype\n-    if expected_output_dtype is None:\n-        expected_output_dtype = input_dtype\n-\n-    # instantiation\n-    layer = layer_cls(**kwargs)\n-\n-    # test get_weights , set_weights at layer level\n-    weights = layer.get_weights()\n-    layer.set_weights(weights)\n-\n-    expected_output_shape = layer.compute_output_shape(input_shape)\n-\n-    # test in functional API\n-    if fixed_batch_size:\n-        x = Input(batch_shape=input_shape, dtype=input_dtype)\n-    else:\n-        x = Input(shape=input_shape[1:], dtype=input_dtype)\n-    y = layer(x)\n-    assert K.dtype(y) == expected_output_dtype\n-\n-    # check with the functional API\n-    model = Model(x, y)\n-\n-    actual_output = model.predict(input_data)\n-    actual_output_shape = actual_output.shape\n-    for expected_dim, actual_dim in zip(expected_output_shape,\n-                                        actual_output_shape):\n-        if expected_dim is not None:\n-            assert expected_dim == actual_dim\n-\n-    if expected_output is not None:\n-        assert_allclose(actual_output, expected_output, rtol=1e-3)\n-\n-    # test serialization, weight setting at model level\n-    model_config = model.get_config()\n-    recovered_model = model.__class__.from_config(model_config)\n-    if model.weights:\n-        weights = model.get_weights()\n-        recovered_model.set_weights(weights)\n-        _output = recovered_model.predict(input_data)\n-        assert_allclose(_output, actual_output, rtol=1e-3)\n-\n-    # test training mode (e.g. useful when the layer has a\n-    # different behavior at training and testing time).\n-    if has_arg(layer.call, 'training'):\n-        model.compile('rmsprop', 'mse')\n-        model.train_on_batch(input_data, actual_output)\n-\n-    # test instantiation from layer config\n-    layer_config = layer.get_config()\n-    layer_config['batch_input_shape'] = input_shape\n-    layer = layer.__class__.from_config(layer_config)\n-\n-    # for further checks in the caller function\n-    return actual_output\n\n@@ -1,4 +0,0 @@\n-\"\"\"Utilities related to model visualization.\"\"\"\n-\n-from tensorflow.keras.utils import model_to_dot\n-from tensorflow.keras.utils import plot_model\n\n@@ -1 +0,0 @@\n-from . import scikit_learn\r\n\n@@ -1,2 +0,0 @@\n-\"\"\"Wrapper for using the Scikit-Learn API with Keras models.\"\"\"\n-from tensorflow.keras.wrappers.scikit_learn import *\n\n@@ -1,48 +0,0 @@\n-from setuptools import setup\n-from setuptools import find_packages\n-\n-long_description = '''\n-Keras is a high-level neural networks API for Python.\n-\n-Read the documentation at: https://keras.io/\n-\n-Keras is compatible with Python 3.6+\n-and is distributed under the MIT license.\n-'''\n-\n-setup(name='Keras',\n-      version='2.4.3',\n-      description='Deep Learning for humans',\n-      long_description=long_description,\n-      author='Francois Chollet',\n-      author_email='francois.chollet@gmail.com',\n-      url='https://github.com/keras-team/keras',\n-      download_url='https://github.com/keras-team/keras/tarball/2.4.3',\n-      license='MIT',\n-      install_requires=['numpy>=1.9.1',\n-                        'scipy>=0.14',\n-                        'pyyaml',\n-                        'h5py'],\n-      extras_require={\n-          'visualize': ['pydot>=1.2.4'],\n-          'tests': ['pytest',\n-                    'pytest-pep8',\n-                    'pytest-xdist',\n-                    'flaky',\n-                    'pytest-cov',\n-                    'pandas',\n-                    'requests',\n-                    'markdown'],\n-      },\n-      classifiers=[\n-          'Development Status :: 5 - Production/Stable',\n-          'Intended Audience :: Developers',\n-          'Intended Audience :: Education',\n-          'Intended Audience :: Science/Research',\n-          'License :: OSI Approved :: MIT License',\n-          'Programming Language :: Python :: 3',\n-          'Programming Language :: Python :: 3.6',\n-          'Topic :: Software Development :: Libraries',\n-          'Topic :: Software Development :: Libraries :: Python Modules'\n-      ],\n-      packages=find_packages())\n\n@@ -1,13 +0,0 @@\n-import pytest\r\n-from keras import backend as K\r\n-\r\n-\r\n-@pytest.fixture(autouse=True)\r\n-def clear_session_after_test():\r\n-    \"\"\"Test wrapper to clean up after TensorFlow and CNTK tests.\r\n-\r\n-    This wrapper runs for all the tests in the keras test suite.\r\n-    \"\"\"\r\n-    yield\r\n-    if K.backend() == 'tensorflow' or K.backend() == 'cntk':\r\n-        K.clear_session()\r\n\n@@ -1,78 +0,0 @@\n-import pytest\n-import random\n-import os\n-from multiprocessing import Process, Queue\n-from keras import applications\n-from keras import backend as K\n-\n-\n-MODEL_LIST = [\n-    (applications.ResNet50, 2048),\n-    (applications.ResNet101, 2048),\n-    (applications.ResNet152, 2048),\n-    (applications.ResNet50V2, 2048),\n-    (applications.ResNet101V2, 2048),\n-    (applications.ResNet152V2, 2048),\n-    (applications.VGG16, 512),\n-    (applications.VGG19, 512),\n-    (applications.Xception, 2048),\n-    (applications.InceptionV3, 2048),\n-    (applications.InceptionResNetV2, 1536),\n-    (applications.MobileNet, 1024),\n-    (applications.MobileNetV2, 1280),\n-    (applications.DenseNet121, 1024),\n-    (applications.DenseNet169, 1664),\n-    (applications.DenseNet201, 1920),\n-    # Note that NASNetLarge is too heavy to test on Travis.\n-    (applications.NASNetMobile, 1056)\n-]\n-\n-\n-def _get_output_shape(model_fn):\n-    if K.backend() == 'cntk':\n-        # Create model in a subprocess so that\n-        # the memory consumed by InceptionResNetV2 will be\n-        # released back to the system after this test\n-        # (to deal with OOM error on CNTK backend).\n-        # TODO: remove the use of multiprocessing from these tests\n-        # once a memory clearing mechanism\n-        # is implemented in the CNTK backend.\n-        def target(queue):\n-            model = model_fn()\n-            queue.put(model.output_shape)\n-        queue = Queue()\n-        p = Process(target=target, args=(queue,))\n-        p.start()\n-        p.join()\n-        # The error in a subprocess won't propagate\n-        # to the main process, so we check if the model\n-        # is successfully created by checking if the output shape\n-        # has been put into the queue\n-        assert not queue.empty(), 'Model creation failed.'\n-        return queue.get_nowait()\n-    else:\n-        model = model_fn()\n-        return model.output_shape\n-\n-\n-def _test_application_basic(app, last_dim=1000):\n-    output_shape = _get_output_shape(lambda: app(weights=None))\n-    assert output_shape == (None, last_dim)\n-\n-\n-def _test_application_notop(app, last_dim):\n-    output_shape = _get_output_shape(\n-        lambda: app(weights=None, include_top=False))\n-    assert len(output_shape) == 4\n-    assert output_shape[-1] == last_dim\n-\n-\n-def test_applications():\n-    for _ in range(3):\n-        app, last_dim = random.choice(MODEL_LIST)\n-        _test_application_basic(app)\n-        _test_application_notop(app, last_dim)\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,113 +0,0 @@\n-import pytest\n-import numpy as np\n-from numpy.testing import assert_allclose\n-\n-from keras.applications import imagenet_utils as utils\n-from keras.models import Model\n-from keras.layers import Input, Lambda\n-\n-\n-def test_preprocess_input():\n-    # Test image batch with float and int image input\n-    x = np.random.uniform(0, 255, (2, 10, 10, 3))\n-    xint = x.astype('int32')\n-    assert utils.preprocess_input(x).shape == x.shape\n-    assert utils.preprocess_input(xint).shape == xint.shape\n-\n-    out1 = utils.preprocess_input(x, 'channels_last')\n-    out1int = utils.preprocess_input(xint, 'channels_last')\n-    out2 = utils.preprocess_input(np.transpose(x, (0, 3, 1, 2)),\n-                                  'channels_first')\n-    out2int = utils.preprocess_input(np.transpose(xint, (0, 3, 1, 2)),\n-                                     'channels_first')\n-    assert_allclose(out1, out2.transpose(0, 2, 3, 1))\n-    assert_allclose(out1int, out2int.transpose(0, 2, 3, 1))\n-\n-    # Test single image\n-    x = np.random.uniform(0, 255, (10, 10, 3))\n-    xint = x.astype('int32')\n-    assert utils.preprocess_input(x).shape == x.shape\n-    assert utils.preprocess_input(xint).shape == xint.shape\n-\n-    out1 = utils.preprocess_input(x, 'channels_last')\n-    out1int = utils.preprocess_input(xint, 'channels_last')\n-    out2 = utils.preprocess_input(np.transpose(x, (2, 0, 1)),\n-                                  'channels_first')\n-    out2int = utils.preprocess_input(np.transpose(xint, (2, 0, 1)),\n-                                     'channels_first')\n-    assert_allclose(out1, out2.transpose(1, 2, 0))\n-    assert_allclose(out1int, out2int.transpose(1, 2, 0))\n-\n-    # Test that writing over the input data works predictably\n-    for mode in ['torch', 'tf']:\n-        x = np.random.uniform(0, 255, (2, 10, 10, 3))\n-        xint = x.astype('int')\n-        x2 = utils.preprocess_input(x, mode=mode)\n-        xint2 = utils.preprocess_input(xint)\n-        assert_allclose(x, x2)\n-        assert xint.astype('float').max() != xint2.max()\n-    # Caffe mode works differently from the others\n-    x = np.random.uniform(0, 255, (2, 10, 10, 3))\n-    xint = x.astype('int')\n-    x2 = utils.preprocess_input(x, data_format='channels_last', mode='caffe')\n-    xint2 = utils.preprocess_input(xint)\n-    assert_allclose(x, x2[..., ::-1])\n-    assert xint.astype('float').max() != xint2.max()\n-\n-\n-def test_preprocess_input_symbolic():\n-    # Test image batch\n-    x = np.random.uniform(0, 255, (2, 10, 10, 3))\n-    inputs = Input(shape=x.shape[1:])\n-    outputs = Lambda(utils.preprocess_input, output_shape=x.shape[1:])(inputs)\n-    model = Model(inputs, outputs)\n-    assert model.predict(x).shape == x.shape\n-\n-    outputs1 = Lambda(lambda x: utils.preprocess_input(x, 'channels_last'),\n-                      output_shape=x.shape[1:])(inputs)\n-    model1 = Model(inputs, outputs1)\n-    out1 = model1.predict(x)\n-    x2 = np.transpose(x, (0, 3, 1, 2))\n-    inputs2 = Input(shape=x2.shape[1:])\n-    outputs2 = Lambda(lambda x: utils.preprocess_input(x, 'channels_first'),\n-                      output_shape=x2.shape[1:])(inputs2)\n-    model2 = Model(inputs2, outputs2)\n-    out2 = model2.predict(x2)\n-    assert_allclose(out1, out2.transpose(0, 2, 3, 1))\n-\n-    # Test single image\n-    x = np.random.uniform(0, 255, (10, 10, 3))\n-    inputs = Input(shape=x.shape)\n-    outputs = Lambda(utils.preprocess_input, output_shape=x.shape)(inputs)\n-    model = Model(inputs, outputs)\n-    assert model.predict(x[np.newaxis])[0].shape == x.shape\n-\n-    outputs1 = Lambda(lambda x: utils.preprocess_input(x, 'channels_last'),\n-                      output_shape=x.shape)(inputs)\n-    model1 = Model(inputs, outputs1)\n-    out1 = model1.predict(x[np.newaxis])[0]\n-    x2 = np.transpose(x, (2, 0, 1))\n-    inputs2 = Input(shape=x2.shape)\n-    outputs2 = Lambda(lambda x: utils.preprocess_input(x, 'channels_first'),\n-                      output_shape=x2.shape)(inputs2)\n-    model2 = Model(inputs2, outputs2)\n-    out2 = model2.predict(x2[np.newaxis])[0]\n-    assert_allclose(out1, out2.transpose(1, 2, 0))\n-\n-\n-def DISABLED_test_decode_predictions():\n-    # Disabled due to SSL issues on Travis.\n-    x = np.zeros((2, 1000))\n-    x[0, 372] = 1.0\n-    x[1, 549] = 1.0\n-    outs = utils.decode_predictions(x, top=1)\n-    scores = [out[0][2] for out in outs]\n-    assert scores[0] == scores[1]\n-\n-    # the numbers of columns and ImageNet classes are not identical.\n-    with pytest.raises(ValueError):\n-        utils.decode_predictions(np.ones((2, 100)))\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,603 +0,0 @@\n-import pytest\n-from keras.preprocessing import image\n-from PIL import Image\n-import numpy as np\n-import os\n-import tempfile\n-import shutil\n-\n-\n-class TestImage(object):\n-\n-    def setup_class(cls):\n-        cls.img_w = cls.img_h = 20\n-        rgb_images = []\n-        gray_images = []\n-        for n in range(8):\n-            bias = np.random.rand(cls.img_w, cls.img_h, 1) * 64\n-            variance = np.random.rand(cls.img_w, cls.img_h, 1) * (255 - 64)\n-            imarray = np.random.rand(cls.img_w, cls.img_h, 3) * variance + bias\n-            im = Image.fromarray(imarray.astype('uint8')).convert('RGB')\n-            rgb_images.append(im)\n-\n-            imarray = np.random.rand(cls.img_w, cls.img_h, 1) * variance + bias\n-            im = Image.fromarray(imarray.astype('uint8').squeeze()).convert('L')\n-            gray_images.append(im)\n-\n-        cls.all_test_images = [rgb_images, gray_images]\n-\n-    def teardown_class(cls):\n-        del cls.all_test_images\n-\n-    def test_image_data_generator(self, tmpdir):\n-        for test_images in self.all_test_images:\n-            img_list = []\n-            for im in test_images:\n-                img_list.append(image.img_to_array(im)[None, ...])\n-\n-            images = np.vstack(img_list)\n-            generator = image.ImageDataGenerator(\n-                featurewise_center=True,\n-                samplewise_center=True,\n-                featurewise_std_normalization=True,\n-                samplewise_std_normalization=True,\n-                zca_whitening=True,\n-                rotation_range=90.,\n-                width_shift_range=0.1,\n-                height_shift_range=0.1,\n-                shear_range=0.5,\n-                zoom_range=0.2,\n-                channel_shift_range=0.,\n-                brightness_range=(1, 5),\n-                fill_mode='nearest',\n-                cval=0.5,\n-                horizontal_flip=True,\n-                vertical_flip=True)\n-            generator.fit(images, augment=True)\n-\n-            num_samples = images.shape[0]\n-            for x, y in generator.flow(images, np.arange(num_samples),\n-                                       shuffle=False, save_to_dir=str(tmpdir),\n-                                       batch_size=3):\n-                assert x.shape == images[:3].shape\n-                assert list(y) == [0, 1, 2]\n-                break\n-\n-            # Test with sample weights\n-            for x, y, w in generator.flow(images, np.arange(num_samples),\n-                                          shuffle=False,\n-                                          sample_weight=np.arange(num_samples) + 1,\n-                                          save_to_dir=str(tmpdir),\n-                                          batch_size=3):\n-                assert x.shape == images[:3].shape\n-                assert list(y) == [0, 1, 2]\n-                assert list(w) == [1, 2, 3]\n-                break\n-\n-            # Test with `shuffle=True`\n-            for x, y in generator.flow(images, np.arange(num_samples),\n-                                       shuffle=True, save_to_dir=str(tmpdir),\n-                                       batch_size=3):\n-                assert x.shape == images[:3].shape\n-                # Check that the sequence is shuffled.\n-                assert list(y) != [0, 1, 2]\n-                break\n-\n-            # Test without y\n-            for x in generator.flow(images, None,\n-                                    shuffle=True, save_to_dir=str(tmpdir),\n-                                    batch_size=3):\n-                assert type(x) is np.ndarray\n-                assert x.shape == images[:3].shape\n-                # Check that the sequence is shuffled.\n-                break\n-\n-            # Test with a single miscellaneous input data array\n-            dsize = images.shape[0]\n-            x_misc1 = np.random.random(dsize)\n-\n-            for i, (x, y) in enumerate(generator.flow((images, x_misc1),\n-                                                      np.arange(dsize),\n-                                                      shuffle=False, batch_size=2)):\n-                assert x[0].shape == images[:2].shape\n-                assert (x[1] == x_misc1[(i * 2):((i + 1) * 2)]).all()\n-                if i == 2:\n-                    break\n-\n-            # Test with two miscellaneous inputs\n-            x_misc2 = np.random.random((dsize, 3, 3))\n-\n-            for i, (x, y) in enumerate(generator.flow((images, [x_misc1, x_misc2]),\n-                                                      np.arange(dsize),\n-                                                      shuffle=False, batch_size=2)):\n-                assert x[0].shape == images[:2].shape\n-                assert (x[1] == x_misc1[(i * 2):((i + 1) * 2)]).all()\n-                assert (x[2] == x_misc2[(i * 2):((i + 1) * 2)]).all()\n-                if i == 2:\n-                    break\n-\n-            # Test cases with `y = None`\n-            x = generator.flow(images, None, batch_size=3).next()\n-            assert type(x) is np.ndarray\n-            assert x.shape == images[:3].shape\n-            x = generator.flow((images, x_misc1), None,\n-                               batch_size=3, shuffle=False).next()\n-            assert type(x) is list\n-            assert x[0].shape == images[:3].shape\n-            assert (x[1] == x_misc1[:3]).all()\n-            x = generator.flow((images, [x_misc1, x_misc2]), None,\n-                               batch_size=3, shuffle=False).next()\n-            assert type(x) is list\n-            assert x[0].shape == images[:3].shape\n-            assert (x[1] == x_misc1[:3]).all()\n-            assert (x[2] == x_misc2[:3]).all()\n-\n-            # Test some failure cases:\n-            x_misc_err = np.random.random((dsize + 1, 3, 3))\n-\n-            with pytest.raises(ValueError) as e_info:\n-                generator.flow((images, x_misc_err), np.arange(dsize), batch_size=3)\n-            assert 'All of the arrays in' in str(e_info.value)\n-\n-            with pytest.raises(ValueError) as e_info:\n-                generator.flow((images, x_misc1),\n-                               np.arange(dsize + 1),\n-                               batch_size=3)\n-            assert '`x` (images tensor) and `y` (labels) ' in str(e_info.value)\n-\n-            # Test `flow` behavior as Sequence\n-            seq = generator.flow(images, np.arange(images.shape[0]),\n-                                 shuffle=False, save_to_dir=str(tmpdir),\n-                                 batch_size=3)\n-            assert len(seq) == images.shape[0] // 3 + 1\n-            x, y = seq[0]\n-            assert x.shape == images[:3].shape\n-            assert list(y) == [0, 1, 2]\n-\n-            # Test with `shuffle=True`\n-            seq = generator.flow(images, np.arange(images.shape[0]),\n-                                 shuffle=True, save_to_dir=str(tmpdir),\n-                                 batch_size=3, seed=123)\n-            x, y = seq[0]\n-            # Check that the sequence is shuffled.\n-            assert list(y) != [0, 1, 2]\n-\n-            # `on_epoch_end` should reshuffle the sequence.\n-            seq.on_epoch_end()\n-            x2, y2 = seq[0]\n-            assert list(y) != list(y2)\n-\n-    def test_image_data_generator_with_split_value_error(self):\n-        with pytest.raises(ValueError):\n-            generator = image.ImageDataGenerator(validation_split=5)\n-\n-    def test_image_data_generator_invalid_data(self):\n-        generator = image.ImageDataGenerator(\n-            featurewise_center=True,\n-            samplewise_center=True,\n-            featurewise_std_normalization=True,\n-            samplewise_std_normalization=True,\n-            zca_whitening=True,\n-            data_format='channels_last')\n-        # Test fit with invalid data\n-        with pytest.raises(ValueError):\n-            x = np.random.random((3, 10, 10))\n-            generator.fit(x)\n-\n-        # Test flow with invalid data\n-        with pytest.raises(ValueError):\n-            x = np.random.random((32, 10, 10))\n-            generator.flow(np.arange(x.shape[0]))\n-\n-    def test_image_data_generator_fit(self):\n-        generator = image.ImageDataGenerator(\n-            featurewise_center=True,\n-            samplewise_center=True,\n-            featurewise_std_normalization=True,\n-            samplewise_std_normalization=True,\n-            zca_whitening=True,\n-            zoom_range=(0.2, 0.2),\n-            data_format='channels_last')\n-        # Test grayscale\n-        x = np.random.random((32, 10, 10, 1))\n-        generator.fit(x)\n-        # Test RBG\n-        x = np.random.random((32, 10, 10, 3))\n-        generator.fit(x)\n-        # Test more samples than dims\n-        x = np.random.random((32, 4, 4, 1))\n-        generator.fit(x)\n-        generator = image.ImageDataGenerator(\n-            featurewise_center=True,\n-            samplewise_center=True,\n-            featurewise_std_normalization=True,\n-            samplewise_std_normalization=True,\n-            zca_whitening=True,\n-            data_format='channels_first')\n-        # Test grayscale\n-        x = np.random.random((32, 1, 10, 10))\n-        generator.fit(x)\n-        # Test RBG\n-        x = np.random.random((32, 3, 10, 10))\n-        generator.fit(x)\n-        # Test more samples than dims\n-        x = np.random.random((32, 1, 4, 4))\n-        generator.fit(x)\n-\n-    def test_directory_iterator(self, tmpdir):\n-        num_classes = 2\n-\n-        # create folders and subfolders\n-        paths = []\n-        for cl in range(num_classes):\n-            class_directory = 'class-{}'.format(cl)\n-            classpaths = [\n-                class_directory,\n-                os.path.join(class_directory, 'subfolder-1'),\n-                os.path.join(class_directory, 'subfolder-2'),\n-                os.path.join(class_directory, 'subfolder-1', 'sub-subfolder')\n-            ]\n-            for path in classpaths:\n-                tmpdir.join(path).mkdir()\n-            paths.append(classpaths)\n-\n-        # save the images in the paths\n-        count = 0\n-        filenames = []\n-        for test_images in self.all_test_images:\n-            for im in test_images:\n-                # rotate image class\n-                im_class = count % num_classes\n-                # rotate subfolders\n-                classpaths = paths[im_class]\n-                filename = os.path.join(classpaths[count % len(classpaths)],\n-                                        'image-{}.jpg'.format(count))\n-                filenames.append(filename)\n-                im.save(str(tmpdir / filename))\n-                count += 1\n-\n-        # create iterator\n-        generator = image.ImageDataGenerator()\n-        dir_iterator = generator.flow_from_directory(str(tmpdir))\n-\n-        # check number of classes and images\n-        assert len(dir_iterator.class_indices) == num_classes\n-        assert len(dir_iterator.classes) == count\n-        assert set(dir_iterator.filenames) == set(filenames)\n-\n-        # Test invalid use cases\n-        with pytest.raises(ValueError):\n-            generator.flow_from_directory(str(tmpdir), color_mode='cmyk')\n-        with pytest.raises(ValueError):\n-            generator.flow_from_directory(str(tmpdir), class_mode='output')\n-\n-        def preprocessing_function(x):\n-            \"\"\"This will fail if not provided by a Numpy array.\n-            Note: This is made to enforce backward compatibility.\n-            \"\"\"\n-\n-            assert x.shape == (26, 26, 3)\n-            assert type(x) is np.ndarray\n-\n-            return np.zeros_like(x)\n-\n-        # Test usage as Sequence\n-        generator = image.ImageDataGenerator(\n-            preprocessing_function=preprocessing_function)\n-        dir_seq = generator.flow_from_directory(str(tmpdir),\n-                                                target_size=(26, 26),\n-                                                color_mode='rgb',\n-                                                batch_size=3,\n-                                                class_mode='categorical')\n-        assert len(dir_seq) == count // 3 + 1\n-        x1, y1 = dir_seq[1]\n-        assert x1.shape == (3, 26, 26, 3)\n-        assert y1.shape == (3, num_classes)\n-        x1, y1 = dir_seq[5]\n-        assert (x1 == 0).all()\n-\n-        with pytest.raises(ValueError):\n-            x1, y1 = dir_seq[9]\n-\n-    def test_directory_iterator_class_mode_input(self, tmpdir):\n-        tmpdir.join('class-1').mkdir()\n-\n-        # save the images in the paths\n-        count = 0\n-        for test_images in self.all_test_images:\n-            for im in test_images:\n-                filename = str(tmpdir / 'class-1' / 'image-{}.jpg'.format(count))\n-                im.save(filename)\n-                count += 1\n-\n-        # create iterator\n-        generator = image.ImageDataGenerator()\n-        dir_iterator = generator.flow_from_directory(str(tmpdir),\n-                                                     class_mode='input')\n-        batch = next(dir_iterator)\n-\n-        # check if input and output have the same shape\n-        assert(batch[0].shape == batch[1].shape)\n-        # check if the input and output images are not the same numpy array\n-        input_img = batch[0][0]\n-        output_img = batch[1][0]\n-        output_img[0][0][0] += 1\n-        assert(input_img[0][0][0] != output_img[0][0][0])\n-\n-    @pytest.mark.parametrize('validation_split,num_training', [\n-        (0.25, 12),\n-        (0.40, 10),\n-        (0.50, 8),\n-    ])\n-    def test_directory_iterator_with_validation_split(self, validation_split,\n-                                                      num_training):\n-        num_classes = 2\n-        tmp_folder = tempfile.mkdtemp(prefix='test_images')\n-\n-        # create folders and subfolders\n-        paths = []\n-        for cl in range(num_classes):\n-            class_directory = 'class-{}'.format(cl)\n-            classpaths = [\n-                class_directory,\n-                os.path.join(class_directory, 'subfolder-1'),\n-                os.path.join(class_directory, 'subfolder-2'),\n-                os.path.join(class_directory, 'subfolder-1', 'sub-subfolder')\n-            ]\n-            for path in classpaths:\n-                os.mkdir(os.path.join(tmp_folder, path))\n-            paths.append(classpaths)\n-\n-        # save the images in the paths\n-        count = 0\n-        filenames = []\n-        for test_images in self.all_test_images:\n-            for im in test_images:\n-                # rotate image class\n-                im_class = count % num_classes\n-                # rotate subfolders\n-                classpaths = paths[im_class]\n-                filename = os.path.join(classpaths[count % len(classpaths)],\n-                                        'image-{}.jpg'.format(count))\n-                filenames.append(filename)\n-                im.save(os.path.join(tmp_folder, filename))\n-                count += 1\n-\n-        # create iterator\n-        generator = image.ImageDataGenerator(validation_split=validation_split)\n-\n-        with pytest.raises(ValueError):\n-            generator.flow_from_directory(tmp_folder, subset='foo')\n-\n-        train_iterator = generator.flow_from_directory(tmp_folder,\n-                                                       subset='training')\n-        assert train_iterator.samples == num_training\n-\n-        valid_iterator = generator.flow_from_directory(tmp_folder,\n-                                                       subset='validation')\n-        assert valid_iterator.samples == count - num_training\n-\n-        # check number of classes and images\n-        assert len(train_iterator.class_indices) == num_classes\n-        assert len(train_iterator.classes) == num_training\n-        assert len(set(train_iterator.filenames) & set(filenames)) == num_training\n-\n-        shutil.rmtree(tmp_folder)\n-\n-    def test_img_utils(self):\n-        height, width = 10, 8\n-\n-        # Test th data format\n-        x = np.random.random((3, height, width))\n-        img = image.array_to_img(x, data_format='channels_first')\n-        assert img.size == (width, height)\n-        x = image.img_to_array(img, data_format='channels_first')\n-        assert x.shape == (3, height, width)\n-        # Test 2D\n-        x = np.random.random((1, height, width))\n-        img = image.array_to_img(x, data_format='channels_first')\n-        assert img.size == (width, height)\n-        x = image.img_to_array(img, data_format='channels_first')\n-        assert x.shape == (1, height, width)\n-\n-        # Test tf data format\n-        x = np.random.random((height, width, 3))\n-        img = image.array_to_img(x, data_format='channels_last')\n-        assert img.size == (width, height)\n-        x = image.img_to_array(img, data_format='channels_last')\n-        assert x.shape == (height, width, 3)\n-        # Test 2D\n-        x = np.random.random((height, width, 1))\n-        img = image.array_to_img(x, data_format='channels_last')\n-        assert img.size == (width, height)\n-        x = image.img_to_array(img, data_format='channels_last')\n-        assert x.shape == (height, width, 1)\n-\n-        # Test invalid use case\n-        with pytest.raises(ValueError):\n-            x = np.random.random((height, width))  # not 3D\n-            img = image.array_to_img(x, data_format='channels_first')\n-        with pytest.raises(ValueError):  # unknown data_format\n-            x = np.random.random((height, width, 3))\n-            img = image.array_to_img(x, data_format='channels')\n-        with pytest.raises(ValueError):  # neither RGB nor gray-scale\n-            x = np.random.random((height, width, 5))\n-            img = image.array_to_img(x, data_format='channels_last')\n-        with pytest.raises(ValueError):  # unknown data_format\n-            x = np.random.random((height, width, 3))\n-            img = image.img_to_array(x, data_format='channels')\n-        with pytest.raises(ValueError):  # neither RGB nor gray-scale\n-            x = np.random.random((height, width, 5, 3))\n-            img = image.img_to_array(x, data_format='channels_last')\n-\n-    def test_random_transforms(self):\n-        x = np.random.random((2, 28, 28))\n-        assert image.random_rotation(x, 45).shape == (2, 28, 28)\n-        assert image.random_shift(x, 1, 1).shape == (2, 28, 28)\n-        assert image.random_shear(x, 20).shape == (2, 28, 28)\n-        assert image.random_zoom(x, (5, 5)).shape == (2, 28, 28)\n-        assert image.random_channel_shift(x, 20).shape == (2, 28, 28)\n-\n-        # Test get_random_transform with predefined seed\n-        seed = 1\n-        generator = image.ImageDataGenerator(\n-            rotation_range=90.,\n-            width_shift_range=0.1,\n-            height_shift_range=0.1,\n-            shear_range=0.5,\n-            zoom_range=0.2,\n-            channel_shift_range=0.1,\n-            brightness_range=(1, 5),\n-            horizontal_flip=True,\n-            vertical_flip=True)\n-        transform_dict = generator.get_random_transform(x.shape, seed)\n-        transform_dict2 = generator.get_random_transform(x.shape, seed * 2)\n-        assert transform_dict['theta'] != 0\n-        assert transform_dict['theta'] != transform_dict2['theta']\n-        assert transform_dict['tx'] != 0\n-        assert transform_dict['tx'] != transform_dict2['tx']\n-        assert transform_dict['ty'] != 0\n-        assert transform_dict['ty'] != transform_dict2['ty']\n-        assert transform_dict['shear'] != 0\n-        assert transform_dict['shear'] != transform_dict2['shear']\n-        assert transform_dict['zx'] != 0\n-        assert transform_dict['zx'] != transform_dict2['zx']\n-        assert transform_dict['zy'] != 0\n-        assert transform_dict['zy'] != transform_dict2['zy']\n-        assert transform_dict['channel_shift_intensity'] != 0\n-        assert (transform_dict['channel_shift_intensity'] !=\n-                transform_dict2['channel_shift_intensity'])\n-        assert transform_dict['brightness'] != 0\n-        assert transform_dict['brightness'] != transform_dict2['brightness']\n-\n-        # Test get_random_transform without any randomness\n-        generator = image.ImageDataGenerator()\n-        transform_dict = generator.get_random_transform(x.shape, seed)\n-        assert transform_dict['theta'] == 0\n-        assert transform_dict['tx'] == 0\n-        assert transform_dict['ty'] == 0\n-        assert transform_dict['shear'] == 0\n-        assert transform_dict['zx'] == 1\n-        assert transform_dict['zy'] == 1\n-        assert transform_dict['channel_shift_intensity'] is None\n-        assert transform_dict['brightness'] is None\n-\n-    def test_deterministic_transform(self):\n-        x = np.ones((32, 32, 3))\n-        generator = image.ImageDataGenerator(\n-            rotation_range=90,\n-            fill_mode='constant')\n-        x = np.random.random((32, 32, 3))\n-        assert np.allclose(generator.apply_transform(x, {'flip_vertical': True}),\n-                           x[::-1, :, :])\n-        assert np.allclose(generator.apply_transform(x, {'flip_horizontal': True}),\n-                           x[:, ::-1, :])\n-        x = np.ones((3, 3, 3))\n-        x_rotated = np.array([[[0., 0., 0.],\n-                               [0., 0., 0.],\n-                               [1., 1., 1.]],\n-                              [[0., 0., 0.],\n-                               [1., 1., 1.],\n-                               [1., 1., 1.]],\n-                              [[0., 0., 0.],\n-                               [0., 0., 0.],\n-                               [1., 1., 1.]]])\n-        assert np.allclose(generator.apply_transform(x, {'theta': 45}),\n-                           x_rotated)\n-        assert np.allclose(image.apply_affine_transform(\n-            x, theta=45, channel_axis=2, fill_mode='constant'), x_rotated)\n-\n-    def test_batch_standardize(self):\n-        # ImageDataGenerator.standardize should work on batches\n-        for test_images in self.all_test_images:\n-            img_list = []\n-            for im in test_images:\n-                img_list.append(image.img_to_array(im)[None, ...])\n-\n-            images = np.vstack(img_list)\n-            generator = image.ImageDataGenerator(\n-                featurewise_center=True,\n-                samplewise_center=True,\n-                featurewise_std_normalization=True,\n-                samplewise_std_normalization=True,\n-                zca_whitening=True,\n-                rotation_range=90.,\n-                width_shift_range=0.1,\n-                height_shift_range=0.1,\n-                shear_range=0.5,\n-                zoom_range=0.2,\n-                channel_shift_range=0.,\n-                brightness_range=(1, 5),\n-                fill_mode='nearest',\n-                cval=0.5,\n-                horizontal_flip=True,\n-                vertical_flip=True)\n-            generator.fit(images, augment=True)\n-\n-            transformed = np.copy(images)\n-            for i, im in enumerate(transformed):\n-                transformed[i] = generator.random_transform(im)\n-            transformed = generator.standardize(transformed)\n-\n-    def test_load_img(self, tmpdir):\n-        filename = str(tmpdir / 'image.png')\n-\n-        original_im_array = np.array(255 * np.random.rand(100, 100, 3),\n-                                     dtype=np.uint8)\n-        original_im = image.array_to_img(original_im_array, scale=False)\n-        original_im.save(filename)\n-\n-        # Test that loaded image is exactly equal to original.\n-\n-        loaded_im = image.load_img(filename)\n-        loaded_im_array = image.img_to_array(loaded_im)\n-        assert loaded_im_array.shape == original_im_array.shape\n-        assert np.all(loaded_im_array == original_im_array)\n-\n-        loaded_im = image.load_img(filename, grayscale=True)\n-        loaded_im_array = image.img_to_array(loaded_im)\n-        assert loaded_im_array.shape == (original_im_array.shape[0],\n-                                         original_im_array.shape[1], 1)\n-\n-        # Test that nothing is changed when target size is equal to original.\n-\n-        loaded_im = image.load_img(filename, target_size=(100, 100))\n-        loaded_im_array = image.img_to_array(loaded_im)\n-        assert loaded_im_array.shape == original_im_array.shape\n-        assert np.all(loaded_im_array == original_im_array)\n-\n-        loaded_im = image.load_img(filename, grayscale=True,\n-                                   target_size=(100, 100))\n-        loaded_im_array = image.img_to_array(loaded_im)\n-        assert loaded_im_array.shape == (original_im_array.shape[0],\n-                                         original_im_array.shape[1], 1)\n-\n-        # Test down-sampling with bilinear interpolation.\n-\n-        loaded_im = image.load_img(filename, target_size=(25, 25))\n-        loaded_im_array = image.img_to_array(loaded_im)\n-        assert loaded_im_array.shape == (25, 25, 3)\n-\n-        loaded_im = image.load_img(filename, grayscale=True,\n-                                   target_size=(25, 25))\n-        loaded_im_array = image.img_to_array(loaded_im)\n-        assert loaded_im_array.shape == (25, 25, 1)\n-\n-        # Test down-sampling with nearest neighbor interpolation.\n-\n-        loaded_im_nearest = image.load_img(filename, target_size=(25, 25),\n-                                           interpolation=\"nearest\")\n-        loaded_im_array_nearest = image.img_to_array(loaded_im_nearest)\n-        assert loaded_im_array_nearest.shape == (25, 25, 3)\n-        assert np.any(loaded_im_array_nearest != loaded_im_array)\n-\n-        # Check that exception is raised if interpolation not supported.\n-\n-        loaded_im = image.load_img(filename, interpolation=\"unsupported\")\n-        with pytest.raises(ValueError):\n-            loaded_im = image.load_img(filename, target_size=(25, 25),\n-                                       interpolation=\"unsupported\")\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,229 +0,0 @@\n-from math import ceil\n-\n-import numpy as np\n-from numpy.testing import assert_allclose, assert_raises\n-\n-import pytest\n-\n-from keras.preprocessing.sequence import pad_sequences\n-from keras.preprocessing.sequence import make_sampling_table\n-from keras.preprocessing.sequence import skipgrams\n-from keras.preprocessing.sequence import TimeseriesGenerator\n-\n-\n-def test_pad_sequences():\n-    a = [[1], [1, 2], [1, 2, 3]]\n-\n-    # test padding\n-    b = pad_sequences(a, maxlen=3, padding='pre')\n-    assert_allclose(b, [[0, 0, 1], [0, 1, 2], [1, 2, 3]])\n-    b = pad_sequences(a, maxlen=3, padding='post')\n-    assert_allclose(b, [[1, 0, 0], [1, 2, 0], [1, 2, 3]])\n-\n-    # test truncating\n-    b = pad_sequences(a, maxlen=2, truncating='pre')\n-    assert_allclose(b, [[0, 1], [1, 2], [2, 3]])\n-    b = pad_sequences(a, maxlen=2, truncating='post')\n-    assert_allclose(b, [[0, 1], [1, 2], [1, 2]])\n-\n-    # test value\n-    b = pad_sequences(a, maxlen=3, value=1)\n-    assert_allclose(b, [[1, 1, 1], [1, 1, 2], [1, 2, 3]])\n-\n-\n-def test_pad_sequences_vector():\n-    a = [[[1, 1]],\n-         [[2, 1], [2, 2]],\n-         [[3, 1], [3, 2], [3, 3]]]\n-\n-    # test padding\n-    b = pad_sequences(a, maxlen=3, padding='pre')\n-    assert_allclose(b, [[[0, 0], [0, 0], [1, 1]],\n-                        [[0, 0], [2, 1], [2, 2]],\n-                        [[3, 1], [3, 2], [3, 3]]])\n-    b = pad_sequences(a, maxlen=3, padding='post')\n-    assert_allclose(b, [[[1, 1], [0, 0], [0, 0]],\n-                        [[2, 1], [2, 2], [0, 0]],\n-                        [[3, 1], [3, 2], [3, 3]]])\n-\n-    # test truncating\n-    b = pad_sequences(a, maxlen=2, truncating='pre')\n-    assert_allclose(b, [[[0, 0], [1, 1]],\n-                        [[2, 1], [2, 2]],\n-                        [[3, 2], [3, 3]]])\n-\n-    b = pad_sequences(a, maxlen=2, truncating='post')\n-    assert_allclose(b, [[[0, 0], [1, 1]],\n-                        [[2, 1], [2, 2]],\n-                        [[3, 1], [3, 2]]])\n-\n-    # test value\n-    b = pad_sequences(a, maxlen=3, value=1)\n-    assert_allclose(b, [[[1, 1], [1, 1], [1, 1]],\n-                        [[1, 1], [2, 1], [2, 2]],\n-                        [[3, 1], [3, 2], [3, 3]]])\n-\n-\n-def test_make_sampling_table():\n-    a = make_sampling_table(3)\n-    assert_allclose(a, np.asarray([0.00315225, 0.00315225, 0.00547597]),\n-                    rtol=.1)\n-\n-\n-def test_skipgrams():\n-    # test with no window size and binary labels\n-    couples, labels = skipgrams(np.arange(3), vocabulary_size=3)\n-    for couple in couples:\n-        assert couple[0] in [0, 1, 2] and couple[1] in [0, 1, 2]\n-\n-    # test window size and categorical labels\n-    couples, labels = skipgrams(np.arange(5), vocabulary_size=5, window_size=1,\n-                                categorical=True)\n-    for couple in couples:\n-        assert couple[0] - couple[1] <= 3\n-    for l in labels:\n-        assert len(l) == 2\n-\n-\n-def test_TimeseriesGenerator():\n-    data = np.array([[i] for i in range(50)])\n-    targets = np.array([[i] for i in range(50)])\n-\n-    data_gen = TimeseriesGenerator(data, targets,\n-                                   length=10, sampling_rate=2,\n-                                   batch_size=2)\n-    assert len(data_gen) == 20\n-    assert (np.allclose(data_gen[0][0],\n-                        np.array([[[0], [2], [4], [6], [8]],\n-                                  [[1], [3], [5], [7], [9]]])))\n-    assert (np.allclose(data_gen[0][1],\n-                        np.array([[10], [11]])))\n-    assert (np.allclose(data_gen[1][0],\n-                        np.array([[[2], [4], [6], [8], [10]],\n-                                  [[3], [5], [7], [9], [11]]])))\n-    assert (np.allclose(data_gen[1][1],\n-                        np.array([[12], [13]])))\n-\n-    data_gen = TimeseriesGenerator(data, targets,\n-                                   length=10, sampling_rate=2, reverse=True,\n-                                   batch_size=2)\n-    assert len(data_gen) == 20\n-    assert (np.allclose(data_gen[0][0],\n-                        np.array([[[8], [6], [4], [2], [0]],\n-                                  [[9], [7], [5], [3], [1]]])))\n-    assert (np.allclose(data_gen[0][1],\n-                        np.array([[10], [11]])))\n-\n-    data_gen = TimeseriesGenerator(data, targets,\n-                                   length=10, sampling_rate=2, shuffle=True,\n-                                   batch_size=1)\n-    batch = data_gen[0]\n-    r = batch[1][0][0]\n-    assert (np.allclose(batch[0],\n-                        np.array([[[r - 10],\n-                                   [r - 8],\n-                                   [r - 6],\n-                                   [r - 4],\n-                                   [r - 2]]])))\n-    assert (np.allclose(batch[1], np.array([[r], ])))\n-\n-    data_gen = TimeseriesGenerator(data, targets,\n-                                   length=10, sampling_rate=2, stride=2,\n-                                   batch_size=2)\n-    assert len(data_gen) == 10\n-    assert (np.allclose(data_gen[1][0],\n-                        np.array([[[4], [6], [8], [10], [12]],\n-                                  [[6], [8], [10], [12], [14]]])))\n-    assert (np.allclose(data_gen[1][1],\n-                        np.array([[14], [16]])))\n-\n-    data_gen = TimeseriesGenerator(data, targets,\n-                                   length=10, sampling_rate=2,\n-                                   start_index=10, end_index=30,\n-                                   batch_size=2)\n-    assert len(data_gen) == 6\n-    assert (np.allclose(data_gen[0][0],\n-                        np.array([[[10], [12], [14], [16], [18]],\n-                                  [[11], [13], [15], [17], [19]]])))\n-    assert (np.allclose(data_gen[0][1],\n-                        np.array([[20], [21]])))\n-\n-    data = np.array([np.random.random_sample((1, 2, 3, 4)) for i in range(50)])\n-    targets = np.array([np.random.random_sample((3, 2, 1)) for i in range(50)])\n-    data_gen = TimeseriesGenerator(data, targets,\n-                                   length=10, sampling_rate=2,\n-                                   start_index=10, end_index=30,\n-                                   batch_size=2)\n-    assert len(data_gen) == 6\n-    assert np.allclose(data_gen[0][0], np.array(\n-        [np.array(data[10:19:2]), np.array(data[11:20:2])]))\n-    assert (np.allclose(data_gen[0][1],\n-                        np.array([targets[20], targets[21]])))\n-\n-    with assert_raises(ValueError) as context:\n-        TimeseriesGenerator(data, targets, length=50)\n-    error = str(context.exception)\n-    assert '`start_index+length=50 > end_index=49` is disallowed' in error\n-\n-\n-def test_TimeSeriesGenerator_doesnt_miss_any_sample():\n-    x = np.array([[i] for i in range(10)])\n-\n-    for length in range(3, 10):\n-        g = TimeseriesGenerator(x, x,\n-                                length=length,\n-                                batch_size=1)\n-        expected = max(0, len(x) - length)\n-        actual = len(g)\n-\n-        assert expected == actual\n-\n-        if len(g) > 0:\n-            # All elements in range(length, 10) should be used as current step\n-            expected = np.arange(length, 10).reshape(-1, 1)\n-\n-            y = np.concatenate([g[ix][1] for ix in range(len(g))], axis=0)\n-            assert_allclose(y, expected)\n-\n-    x = np.array([[i] for i in range(23)])\n-\n-    strides = (1, 1, 5, 7, 3, 5, 3)\n-    lengths = (3, 3, 4, 3, 1, 3, 7)\n-    batch_sizes = (6, 6, 6, 5, 6, 6, 6)\n-    shuffles = (False, True, True, False, False, False, False)\n-\n-    for stride, length, batch_size, shuffle in zip(strides,\n-                                                   lengths,\n-                                                   batch_sizes,\n-                                                   shuffles):\n-        g = TimeseriesGenerator(x, x,\n-                                length=length,\n-                                sampling_rate=1,\n-                                stride=stride,\n-                                start_index=0,\n-                                end_index=None,\n-                                shuffle=shuffle,\n-                                reverse=False,\n-                                batch_size=batch_size)\n-        if shuffle:\n-            # all batches have the same size when shuffle is True.\n-            expected_sequences = ceil(\n-                (23 - length) / float(batch_size * stride)) * batch_size\n-        else:\n-            # last batch will be different if `(samples - length) / stride`\n-            # is not a multiple of `batch_size`.\n-            expected_sequences = ceil((23 - length) / float(stride))\n-\n-        expected_batches = ceil(expected_sequences / float(batch_size))\n-\n-        y = [g[ix][1] for ix in range(len(g))]\n-\n-        actual_sequences = sum(len(_y) for _y in y)\n-        actual_batches = len(y)\n-\n-        assert expected_sequences == actual_sequences\n-        assert expected_batches == actual_batches\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,126 +0,0 @@\n-# -*- coding: utf-8 -*-\n-\n-import numpy as np\n-import pytest\n-\n-from keras.preprocessing.text import Tokenizer\n-from keras.preprocessing.text import one_hot\n-from keras.preprocessing.text import hashing_trick\n-from keras.preprocessing.text import text_to_word_sequence\n-\n-\n-def test_one_hot():\n-    text = 'The cat sat on the mat.'\n-    encoded = one_hot(text, 5)\n-    assert len(encoded) == 6\n-    assert np.max(encoded) <= 4\n-    assert np.min(encoded) >= 0\n-\n-\n-def test_hashing_trick_hash():\n-    text = 'The cat sat on the mat.'\n-    encoded = hashing_trick(text, 5)\n-    assert len(encoded) == 6\n-    assert np.max(encoded) <= 4\n-    assert np.min(encoded) >= 1\n-\n-\n-def test_hashing_trick_md5():\n-    text = 'The cat sat on the mat.'\n-    encoded = hashing_trick(text, 5, hash_function='md5')\n-    assert len(encoded) == 6\n-    assert np.max(encoded) <= 4\n-    assert np.min(encoded) >= 1\n-\n-\n-def test_tokenizer():\n-    texts = ['The cat sat on the mat.',\n-             'The dog sat on the log.',\n-             'Dogs and cats living together.']\n-    tokenizer = Tokenizer(num_words=10)\n-    tokenizer.fit_on_texts(texts)\n-\n-    sequences = []\n-    for seq in tokenizer.texts_to_sequences_generator(texts):\n-        sequences.append(seq)\n-    assert np.max(np.max(sequences)) < 10\n-    assert np.min(np.min(sequences)) == 1\n-\n-    tokenizer.fit_on_sequences(sequences)\n-\n-    for mode in ['binary', 'count', 'tfidf', 'freq']:\n-        matrix = tokenizer.texts_to_matrix(texts, mode)\n-\n-\n-def test_sequential_fit():\n-    texts = ['The cat sat on the mat.',\n-             'The dog sat on the log.',\n-             'Dogs and cats living together.']\n-    word_sequences = [\n-        ['The', 'cat', 'is', 'sitting'],\n-        ['The', 'dog', 'is', 'standing']\n-    ]\n-\n-    tokenizer = Tokenizer()\n-    tokenizer.fit_on_texts(texts)\n-    tokenizer.fit_on_texts(word_sequences)\n-\n-    assert tokenizer.document_count == 5\n-\n-    tokenizer.texts_to_matrix(texts)\n-    tokenizer.texts_to_matrix(word_sequences)\n-\n-\n-def test_text_to_word_sequence():\n-    text = 'hello! ? world!'\n-    assert text_to_word_sequence(text) == ['hello', 'world']\n-\n-\n-def test_text_to_word_sequence_multichar_split():\n-    text = 'hello!stop?world!'\n-    assert text_to_word_sequence(text, split='stop') == ['hello', 'world']\n-\n-\n-def test_text_to_word_sequence_unicode():\n-    text = u'ali! veli? kırk dokuz elli'\n-    assert (text_to_word_sequence(text) ==\n-            [u'ali', u'veli', u'kırk', u'dokuz', u'elli'])\n-\n-\n-def test_text_to_word_sequence_unicode_multichar_split():\n-    text = u'ali!stopveli?stopkırkstopdokuzstopelli'\n-    assert (text_to_word_sequence(text, split='stop') ==\n-            [u'ali', u'veli', u'kırk', u'dokuz', u'elli'])\n-\n-\n-def test_tokenizer_unicode():\n-    texts = [u'ali veli kırk dokuz elli',\n-             u'ali veli kırk dokuz elli veli kırk dokuz']\n-    tokenizer = Tokenizer(num_words=5)\n-    tokenizer.fit_on_texts(texts)\n-\n-    assert len(tokenizer.word_counts) == 5\n-\n-\n-def test_tokenizer_oov_flag():\n-    \"\"\"\n-    Test of Out of Vocabulary (OOV) flag in Tokenizer\n-    \"\"\"\n-    x_train = ['This text has only known words']\n-    x_test = ['This text has some unknown words']  # 2 OOVs: some, unknown\n-\n-    # Default, without OOV flag\n-    tokenizer = Tokenizer()\n-    tokenizer.fit_on_texts(x_train)\n-    x_test_seq = tokenizer.texts_to_sequences(x_test)\n-    assert len(x_test_seq[0]) == 4  # discards 2 OOVs\n-\n-    # With OOV feature\n-    tokenizer = Tokenizer(oov_token='<unk>')\n-    tokenizer.fit_on_texts(x_train)\n-    x_test_seq = tokenizer.texts_to_sequences(x_test)\n-    assert len(x_test_seq[0]) == 6  # OOVs marked in place\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,90 +0,0 @@\n-from __future__ import print_function\n-import pytest\n-import time\n-import random\n-from keras.datasets import cifar10\n-from keras.datasets import cifar100\n-from keras.datasets import reuters\n-from keras.datasets import imdb\n-from keras.datasets import mnist\n-from keras.datasets import boston_housing\n-from keras.datasets import fashion_mnist\n-\n-\n-def test_cifar():\n-    # only run data download tests 20% of the time\n-    # to speed up frequent testing\n-    random.seed(time.time())\n-    if random.random() > 0.8:\n-        (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n-        assert len(x_train) == len(y_train) == 50000\n-        assert len(x_test) == len(y_test) == 10000\n-        (x_train, y_train), (x_test, y_test) = cifar100.load_data('fine')\n-        assert len(x_train) == len(y_train) == 50000\n-        assert len(x_test) == len(y_test) == 10000\n-        (x_train, y_train), (x_test, y_test) = cifar100.load_data('coarse')\n-        assert len(x_train) == len(y_train) == 50000\n-        assert len(x_test) == len(y_test) == 10000\n-\n-\n-def test_reuters():\n-    # only run data download tests 20% of the time\n-    # to speed up frequent testing\n-    random.seed(time.time())\n-    if random.random() > 0.8:\n-        (x_train, y_train), (x_test, y_test) = reuters.load_data()\n-        assert len(x_train) == len(y_train)\n-        assert len(x_test) == len(y_test)\n-        assert len(x_train) + len(x_test) == 11228\n-        (x_train, y_train), (x_test, y_test) = reuters.load_data(maxlen=10)\n-        assert len(x_train) == len(y_train)\n-        assert len(x_test) == len(y_test)\n-        word_index = reuters.get_word_index()\n-        assert isinstance(word_index, dict)\n-\n-\n-def test_mnist():\n-    # only run data download tests 20% of the time\n-    # to speed up frequent testing\n-    random.seed(time.time())\n-    if random.random() > 0.8:\n-        (x_train, y_train), (x_test, y_test) = mnist.load_data()\n-        assert len(x_train) == len(y_train) == 60000\n-        assert len(x_test) == len(y_test) == 10000\n-\n-\n-def test_imdb():\n-    # only run data download tests 20% of the time\n-    # to speed up frequent testing\n-    random.seed(time.time())\n-    if random.random() > 0.8:\n-        (x_train, y_train), (x_test, y_test) = imdb.load_data()\n-        (x_train, y_train), (x_test, y_test) = imdb.load_data(maxlen=40)\n-        assert len(x_train) == len(y_train)\n-        assert len(x_test) == len(y_test)\n-        word_index = imdb.get_word_index()\n-        assert isinstance(word_index, dict)\n-\n-\n-def test_boston_housing():\n-    # only run data download tests 20% of the time\n-    # to speed up frequent testing\n-    random.seed(time.time())\n-    if random.random() > 0.8:\n-        (x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n-        assert len(x_train) == len(y_train)\n-        assert len(x_test) == len(y_test)\n-\n-\n-def test_fashion_mnist():\n-    # only run data download tests 20% of the time\n-    # to speed up frequent testing\n-    random.seed(time.time())\n-    if random.random() > 0.8:\n-        (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n-        assert len(x_train) == len(y_train) == 60000\n-        assert len(x_test) == len(y_test) == 10000\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,80 +0,0 @@\n-from __future__ import print_function\n-import numpy as np\n-import pytest\n-\n-from keras.preprocessing.image import ImageDataGenerator\n-from keras.utils.test_utils import get_test_data\n-from keras.models import Sequential\n-from keras import layers\n-from keras.utils.np_utils import to_categorical\n-\n-\n-def test_image_classification():\n-    np.random.seed(1337)\n-    input_shape = (16, 16, 3)\n-    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=500,\n-                                                         num_test=200,\n-                                                         input_shape=input_shape,\n-                                                         classification=True,\n-                                                         num_classes=4)\n-    y_train = to_categorical(y_train)\n-    y_test = to_categorical(y_test)\n-\n-    model = Sequential([\n-        layers.Conv2D(filters=8, kernel_size=3,\n-                      activation='relu',\n-                      input_shape=input_shape),\n-        layers.MaxPooling2D(pool_size=2),\n-        layers.Conv2D(filters=4, kernel_size=(3, 3),\n-                      activation='relu', padding='same'),\n-        layers.GlobalAveragePooling2D(),\n-        layers.Dense(y_test.shape[-1], activation='softmax')\n-    ])\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer='rmsprop',\n-                  metrics=['accuracy'])\n-    model.summary()\n-    history = model.fit(x_train, y_train, epochs=15, batch_size=16,\n-                        validation_data=(x_test, y_test),\n-                        verbose=0)\n-    assert history.history['val_accuracy'][-1] > 0.6\n-    config = model.get_config()\n-    model = Sequential.from_config(config)\n-\n-\n-def test_image_data_generator_training():\n-    np.random.seed(1337)\n-    img_gen = ImageDataGenerator(rescale=1.)  # Dummy ImageDataGenerator\n-    input_shape = (16, 16, 3)\n-    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=500,\n-                                                         num_test=200,\n-                                                         input_shape=input_shape,\n-                                                         classification=True,\n-                                                         num_classes=4)\n-    y_train = to_categorical(y_train)\n-    y_test = to_categorical(y_test)\n-\n-    model = Sequential([\n-        layers.Conv2D(filters=8, kernel_size=3,\n-                      activation='relu',\n-                      input_shape=input_shape),\n-        layers.MaxPooling2D(pool_size=2),\n-        layers.Conv2D(filters=4, kernel_size=(3, 3),\n-                      activation='relu', padding='same'),\n-        layers.GlobalAveragePooling2D(),\n-        layers.Dense(y_test.shape[-1], activation='softmax')\n-    ])\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer='rmsprop',\n-                  metrics=['accuracy'])\n-    history = model.fit_generator(img_gen.flow(x_train, y_train, batch_size=16),\n-                                  epochs=15,\n-                                  validation_data=img_gen.flow(x_test, y_test,\n-                                                               batch_size=16),\n-                                  verbose=0)\n-    assert history.history['val_accuracy'][-1] > 0.6\n-    model.evaluate_generator(img_gen.flow(x_train, y_train, batch_size=16))\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,180 +0,0 @@\n-from __future__ import print_function\n-import numpy as np\n-import pytest\n-import string\n-\n-from keras.utils.test_utils import get_test_data\n-from keras.utils.np_utils import to_categorical\n-from keras.models import Sequential\n-from keras import layers, optimizers\n-import keras.backend as K\n-import keras\n-\n-\n-def test_temporal_classification():\n-    '''\n-    Classify temporal sequences of float numbers\n-    of length 3 into 2 classes using\n-    single layer of GRU units and softmax applied\n-    to the last activations of the units\n-    '''\n-    np.random.seed(1337)\n-    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=200,\n-                                                         num_test=20,\n-                                                         input_shape=(3, 4),\n-                                                         classification=True,\n-                                                         num_classes=2)\n-    y_train = to_categorical(y_train)\n-    y_test = to_categorical(y_test)\n-\n-    model = Sequential()\n-    model.add(layers.GRU(8,\n-                         input_shape=(x_train.shape[1], x_train.shape[2])))\n-    model.add(layers.Dense(y_train.shape[-1], activation='softmax'))\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer='rmsprop',\n-                  metrics=['accuracy'])\n-    model.summary()\n-    history = model.fit(x_train, y_train, epochs=5, batch_size=10,\n-                        validation_data=(x_test, y_test),\n-                        verbose=0)\n-    assert(history.history['accuracy'][-1] >= 0.8)\n-    config = model.get_config()\n-    model = Sequential.from_config(config)\n-\n-\n-def test_temporal_classification_functional():\n-    '''\n-    Classify temporal sequences of float numbers\n-    of length 3 into 2 classes using\n-    single layer of GRU units and softmax applied\n-    to the last activations of the units\n-    '''\n-    np.random.seed(1337)\n-    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=200,\n-                                                         num_test=20,\n-                                                         input_shape=(3, 4),\n-                                                         classification=True,\n-                                                         num_classes=2)\n-    y_train = to_categorical(y_train)\n-    y_test = to_categorical(y_test)\n-\n-    inputs = layers.Input(shape=(x_train.shape[1], x_train.shape[2]))\n-    x = layers.SimpleRNN(8)(inputs)\n-    outputs = layers.Dense(y_train.shape[-1], activation='softmax')(x)\n-    model = keras.models.Model(inputs, outputs)\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer='rmsprop',\n-                  metrics=['accuracy'])\n-    history = model.fit(x_train, y_train, epochs=5, batch_size=10,\n-                        validation_data=(x_test, y_test),\n-                        verbose=0)\n-    assert(history.history['accuracy'][-1] >= 0.6)\n-\n-\n-def test_temporal_regression():\n-    '''\n-    Predict float numbers (regression) based on sequences\n-    of float numbers of length 3 using a single layer of GRU units\n-    '''\n-    np.random.seed(1337)\n-    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=200,\n-                                                         num_test=20,\n-                                                         input_shape=(3, 5),\n-                                                         output_shape=(2,),\n-                                                         classification=False)\n-    model = Sequential()\n-    model.add(layers.LSTM(y_train.shape[-1],\n-                          input_shape=(x_train.shape[1], x_train.shape[2])))\n-    model.compile(loss='hinge', optimizer='adam')\n-    history = model.fit(x_train, y_train, epochs=5, batch_size=16,\n-                        validation_data=(x_test, y_test), verbose=0)\n-    assert(history.history['loss'][-1] < 1.1)\n-\n-\n-def test_3d_to_3d():\n-    '''\n-    Apply a same Dense layer for each element of time dimension of the input\n-    and make predictions of the output sequence elements.\n-    This does not make use of the temporal structure of the sequence\n-    (see TimeDistributedDense for more details)\n-    '''\n-    np.random.seed(1337)\n-    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=100,\n-                                                         num_test=20,\n-                                                         input_shape=(3, 5),\n-                                                         output_shape=(3, 5),\n-                                                         classification=False)\n-\n-    model = Sequential()\n-    model.add(layers.TimeDistributed(\n-        layers.Dense(y_train.shape[-1]), input_shape=x_train.shape[1:3]))\n-    model.compile(loss='hinge', optimizer='rmsprop')\n-    history = model.fit(x_train, y_train, epochs=20, batch_size=16,\n-                        validation_data=(x_test, y_test), verbose=0)\n-    assert(history.history['loss'][-1] < 1.2)\n-\n-\n-def test_stacked_lstm_char_prediction():\n-    '''\n-    Learn alphabetical char sequence with stacked LSTM.\n-    Predict the whole alphabet based on the first two letters ('ab' -> 'ab...z')\n-    See non-toy example in examples/lstm_text_generation.py\n-    '''\n-    # generate alphabet:\n-    # http://stackoverflow.com/questions/16060899/alphabet-range-python\n-    alphabet = string.ascii_lowercase\n-    number_of_chars = len(alphabet)\n-\n-    # generate char sequences of length 'sequence_length' out of alphabet and\n-    # store the next char as label (e.g. 'ab'->'c')\n-    sequence_length = 2\n-    sentences = [alphabet[i: i + sequence_length]\n-                 for i in range(len(alphabet) - sequence_length)]\n-    next_chars = [alphabet[i + sequence_length]\n-                  for i in range(len(alphabet) - sequence_length)]\n-\n-    # Transform sequences and labels into 'one-hot' encoding\n-    x = np.zeros((len(sentences), sequence_length, number_of_chars), dtype=np.bool)\n-    y = np.zeros((len(sentences), number_of_chars), dtype=np.bool)\n-    for i, sentence in enumerate(sentences):\n-        for t, char in enumerate(sentence):\n-            x[i, t, ord(char) - ord('a')] = 1\n-        y[i, ord(next_chars[i]) - ord('a')] = 1\n-\n-    # learn the alphabet with stacked LSTM\n-    model = Sequential([\n-        layers.LSTM(16, return_sequences=True,\n-                    input_shape=(sequence_length, number_of_chars)),\n-        layers.LSTM(16, return_sequences=False),\n-        layers.Dense(number_of_chars, activation='softmax')\n-    ])\n-    model.compile(loss='categorical_crossentropy', optimizer='adam')\n-    model.fit(x, y, batch_size=1, epochs=60, verbose=1)\n-\n-    # prime the model with 'ab' sequence and let it generate the learned alphabet\n-    sentence = alphabet[:sequence_length]\n-    generated = sentence\n-    for iteration in range(number_of_chars - sequence_length):\n-        x = np.zeros((1, sequence_length, number_of_chars))\n-        for t, char in enumerate(sentence):\n-            x[0, t, ord(char) - ord('a')] = 1.\n-        preds = model.predict(x, verbose=0)[0]\n-        next_char = chr(np.argmax(preds) + ord('a'))\n-        generated += next_char\n-        sentence = sentence[1:] + next_char\n-\n-    # check that it did generate the alphabet correctly\n-    assert(generated == alphabet)\n-\n-\n-@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Requires TF backend')\n-def test_embedding_with_clipnorm():\n-    model = Sequential()\n-    model.add(layers.Embedding(input_dim=1, output_dim=1))\n-    model.compile(optimizer=optimizers.SGD(clipnorm=0.1), loss='mse')\n-    model.fit(np.array([[0]]), np.array([[[0.5]]]), epochs=1)\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,55 +0,0 @@\n-from __future__ import print_function\n-\n-import os\n-import tempfile\n-import pytest\n-import keras\n-from keras import layers\n-from keras.utils.test_utils import get_test_data\n-\n-\n-@pytest.mark.skipif(keras.backend.backend() != 'tensorflow',\n-                    reason='Requires TF backend')\n-def test_tf_optimizer():\n-    import tensorflow as tf\n-\n-    num_hidden = 10\n-    output_dim = 2\n-    input_dim = 10\n-    target = 0.8\n-\n-    if tf.__version__.startswith('1.'):\n-        optimizer = tf.train.AdadeltaOptimizer(\n-            learning_rate=1., rho=0.95, epsilon=1e-08)\n-    else:\n-        optimizer = tf.keras.optimizers.Adadelta(\n-            learning_rate=1., rho=0.95, epsilon=1e-08)\n-\n-    (x_train, y_train), (x_test, y_test) = get_test_data(\n-        num_train=1000, num_test=200,\n-        input_shape=(input_dim,),\n-        classification=True, num_classes=output_dim)\n-\n-    model = keras.Sequential()\n-    model.add(layers.Dense(num_hidden,\n-                           activation='relu',\n-                           input_shape=(input_dim,)))\n-    model.add(layers.Dense(output_dim, activation='softmax'))\n-\n-    model.compile(loss='sparse_categorical_crossentropy',\n-                  optimizer=optimizer,\n-                  metrics=['accuracy'])\n-    history = model.fit(x_train, y_train, epochs=8, batch_size=16,\n-                        validation_data=(x_test, y_test), verbose=2)\n-    assert history.history['val_accuracy'][-1] >= target\n-\n-    # Test saving.\n-    _, fname = tempfile.mkstemp('.h5')\n-    model.save(fname)\n-    model = keras.models.load_model(fname)\n-    assert len(model.weights) == 4\n-    os.remove(fname)\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,90 +0,0 @@\n-from __future__ import print_function\n-import pytest\n-\n-from keras.utils.test_utils import get_test_data\n-from keras.models import Sequential\n-from keras import layers\n-import keras\n-from keras.utils.np_utils import to_categorical\n-\n-num_classes = 2\n-\n-\n-def test_vector_classification():\n-    '''\n-    Classify random float vectors into 2 classes with logistic regression\n-    using 2 layer neural network with ReLU hidden units.\n-    '''\n-    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=500,\n-                                                         num_test=200,\n-                                                         input_shape=(20,),\n-                                                         classification=True,\n-                                                         num_classes=num_classes)\n-    y_train = to_categorical(y_train)\n-    y_test = to_categorical(y_test)\n-\n-    # Test with Sequential API\n-    model = Sequential([\n-        layers.Dense(16, input_shape=(x_train.shape[-1],), activation='relu'),\n-        layers.Dense(8),\n-        layers.Activation('relu'),\n-        layers.Dense(num_classes, activation='softmax')\n-    ])\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer=keras.optimizers.Adam(1e-3),\n-                  metrics=['accuracy'])\n-    model.summary()\n-    history = model.fit(x_train, y_train, epochs=15, batch_size=16,\n-                        validation_data=(x_test, y_test),\n-                        verbose=0)\n-    assert(history.history['val_accuracy'][-1] > 0.8)\n-    config = model.get_config()\n-    model = Sequential.from_config(config)\n-\n-\n-def test_vector_classification_functional():\n-    (x_train, y_train), _ = get_test_data(num_train=500,\n-                                          num_test=200,\n-                                          input_shape=(20,),\n-                                          classification=True,\n-                                          num_classes=num_classes)\n-    # Test with functional API\n-    inputs = layers.Input(shape=(x_train.shape[-1],))\n-    x = layers.Dense(16, activation=keras.activations.relu)(inputs)\n-    x = layers.Dense(8)(x)\n-    x = layers.Activation('relu')(x)\n-    outputs = layers.Dense(num_classes, activation='softmax')(x)\n-    model = keras.models.Model(inputs, outputs)\n-    model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n-                  optimizer=keras.optimizers.Adam(1e-3),\n-                  metrics=['accuracy'])\n-    history = model.fit(x_train, y_train, epochs=15, batch_size=16,\n-                        validation_data=(x_train, y_train),\n-                        verbose=0)\n-    assert(history.history['val_accuracy'][-1] > 0.8)\n-\n-\n-def test_vector_regression():\n-    '''\n-    Perform float data prediction (regression) using 2 layer MLP\n-    with tanh and sigmoid activations.\n-    '''\n-    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=500,\n-                                                         num_test=200,\n-                                                         input_shape=(20,),\n-                                                         output_shape=(num_classes,),\n-                                                         classification=False)\n-\n-    model = Sequential([\n-        layers.Dense(16, input_shape=(x_train.shape[-1],), activation='tanh'),\n-        layers.Dense(num_classes)\n-    ])\n-\n-    model.compile(loss='hinge', optimizer=keras.optimizers.Adam(1e-3))\n-    history = model.fit(x_train, y_train, epochs=20, batch_size=16,\n-                        validation_data=(x_test, y_test), verbose=0)\n-    assert (history.history['val_loss'][-1] < 0.9)\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,248 +0,0 @@\n-import pytest\n-import numpy as np\n-from numpy.testing import assert_allclose\n-\n-from keras import backend as K\n-from keras import activations\n-\n-from keras.layers.core import Dense\n-\n-\n-def get_standard_values():\n-    \"\"\"A set of floats used for testing the activations.\n-    \"\"\"\n-    return np.array([[0, 0.1, 0.5, 0.9, 1.0]], dtype=K.floatx())\n-\n-\n-def test_serialization():\n-    all_activations = ['softmax', 'relu', 'elu', 'tanh',\n-                       'sigmoid', 'hard_sigmoid', 'linear',\n-                       'softplus', 'softsign', 'selu']\n-    for name in all_activations:\n-        fn = activations.get(name)\n-        ref_fn = getattr(activations, name)\n-        assert fn == ref_fn\n-        config = activations.serialize(fn)\n-        fn = activations.deserialize(config)\n-        assert fn == ref_fn\n-\n-\n-def test_get_fn():\n-    \"\"\"Activations has a convenience \"get\" function. All paths of this\n-    function are tested here, although the behaviour in some instances\n-    seems potentially surprising (e.g. situation 3)\n-    \"\"\"\n-\n-    # 1. Default returns linear\n-    a = activations.get(None)\n-    assert a == activations.linear\n-\n-    # # 2. Passing in a layer raises a warning\n-    # layer = Dense(32)\n-    # with pytest.warns(UserWarning):\n-    #     a = activations.get(layer)\n-\n-    # 3. Callables return themselves\n-    a = activations.get(lambda x: 5)\n-    assert a(None) == 5\n-\n-    # 4. Anything else is not a valid argument\n-    with pytest.raises(TypeError):\n-        a = activations.get(6)\n-\n-\n-def test_softmax_valid():\n-    \"\"\"Test using a reference implementation of softmax.\n-    \"\"\"\n-    def softmax(values):\n-        m = np.max(values)\n-        e = np.exp(values - m)\n-        return e / np.sum(e)\n-\n-    x = K.placeholder(ndim=2)\n-    f = K.function([x], [activations.softmax(x)])\n-    test_values = get_standard_values()\n-\n-    result = f([test_values])[0]\n-    expected = softmax(test_values)\n-    assert_allclose(result, expected, rtol=1e-05)\n-\n-\n-def test_softmax_invalid():\n-    \"\"\"Test for the expected exception behaviour on invalid input\n-    \"\"\"\n-\n-    x = K.placeholder(ndim=1)\n-\n-    # One dimensional arrays are supposed to raise a value error\n-    with pytest.raises(ValueError):\n-        f = K.function([x], [activations.softmax(x)])\n-\n-\n-def test_softmax_3d():\n-    \"\"\"Test using a reference implementation of softmax.\n-    \"\"\"\n-    def softmax(values, axis):\n-        m = np.max(values, axis=axis, keepdims=True)\n-        e = np.exp(values - m)\n-        return e / np.sum(e, axis=axis, keepdims=True)\n-\n-    x = K.placeholder(ndim=3)\n-    f = K.function([x], [activations.softmax(x, axis=1)])\n-    test_values = get_standard_values()[:, :, np.newaxis].copy()\n-\n-    result = f([test_values])[0]\n-    expected = softmax(test_values, axis=1)\n-    assert_allclose(result, expected, rtol=1e-05)\n-\n-\n-def test_time_distributed_softmax():\n-    x = K.placeholder(shape=(1, 1, 5))\n-    f = K.function([x], [activations.softmax(x)])\n-    test_values = get_standard_values()\n-    test_values = np.reshape(test_values, (1, 1, np.size(test_values)))\n-    f([test_values])[0]\n-\n-\n-def test_softplus():\n-    \"\"\"Test using a reference softplus implementation.\n-    \"\"\"\n-    def softplus(x):\n-        return np.log(np.ones_like(x) + np.exp(x))\n-\n-    x = K.placeholder(ndim=2)\n-    f = K.function([x], [activations.softplus(x)])\n-    test_values = get_standard_values()\n-\n-    result = f([test_values])[0]\n-    expected = softplus(test_values)\n-    assert_allclose(result, expected, rtol=1e-05)\n-\n-\n-def test_softsign():\n-    \"\"\"Test using a reference softsign implementation.\n-    \"\"\"\n-    def softsign(x):\n-        return np.divide(x, np.ones_like(x) + np.absolute(x))\n-\n-    x = K.placeholder(ndim=2)\n-    f = K.function([x], [activations.softsign(x)])\n-    test_values = get_standard_values()\n-\n-    result = f([test_values])[0]\n-    expected = softsign(test_values)\n-    assert_allclose(result, expected, rtol=1e-05)\n-\n-\n-def test_sigmoid():\n-    \"\"\"Test using a numerically stable reference sigmoid implementation.\n-    \"\"\"\n-    def ref_sigmoid(x):\n-        if x >= 0:\n-            return 1 / (1 + np.exp(-x))\n-        else:\n-            z = np.exp(x)\n-            return z / (1 + z)\n-    sigmoid = np.vectorize(ref_sigmoid)\n-\n-    x = K.placeholder(ndim=2)\n-    f = K.function([x], [activations.sigmoid(x)])\n-    test_values = get_standard_values()\n-\n-    result = f([test_values])[0]\n-    expected = sigmoid(test_values)\n-    assert_allclose(result, expected, rtol=1e-05)\n-\n-\n-def test_hard_sigmoid():\n-    \"\"\"Test using a reference hard sigmoid implementation.\n-    \"\"\"\n-    def ref_hard_sigmoid(x):\n-        x = (x * 0.2) + 0.5\n-        z = 0.0 if x <= 0 else (1.0 if x >= 1 else x)\n-        return z\n-    hard_sigmoid = np.vectorize(ref_hard_sigmoid)\n-\n-    x = K.placeholder(ndim=2)\n-    f = K.function([x], [activations.hard_sigmoid(x)])\n-    test_values = get_standard_values()\n-\n-    result = f([test_values])[0]\n-    expected = hard_sigmoid(test_values)\n-    assert_allclose(result, expected, rtol=1e-05)\n-\n-\n-def test_relu():\n-    x = K.placeholder(ndim=2)\n-    f = K.function([x], [activations.relu(x)])\n-\n-    test_values = get_standard_values()\n-    result = f([test_values])[0]\n-    assert_allclose(result, test_values, rtol=1e-05)\n-\n-    # Test max_value\n-    test_values = np.array([[0.5, 1.5]], dtype=K.floatx())\n-    f = K.function([x], [activations.relu(x, max_value=1.)])\n-    result = f([test_values])[0]\n-    assert np.max(result) <= 1.\n-\n-    # Test max_value == 6.\n-    test_values = np.array([[0.5, 6.]], dtype=K.floatx())\n-    f = K.function([x], [activations.relu(x, max_value=1.)])\n-    result = f([test_values])[0]\n-    assert np.max(result) <= 6.\n-\n-\n-def test_elu():\n-    x = K.placeholder(ndim=2)\n-    f = K.function([x], [activations.elu(x, 0.5)])\n-\n-    test_values = get_standard_values()\n-    result = f([test_values])[0]\n-    assert_allclose(result, test_values, rtol=1e-05)\n-\n-    negative_values = np.array([[-1, -2]], dtype=K.floatx())\n-    result = f([negative_values])[0]\n-    true_result = (np.exp(negative_values) - 1) / 2\n-\n-    assert_allclose(result, true_result)\n-\n-\n-def test_selu():\n-    x = K.placeholder(ndim=2)\n-    f = K.function([x], [activations.selu(x)])\n-    alpha = 1.6732632423543772848170429916717\n-    scale = 1.0507009873554804934193349852946\n-\n-    positive_values = get_standard_values()\n-    result = f([positive_values])[0]\n-    assert_allclose(result, positive_values * scale, rtol=1e-05)\n-\n-    negative_values = np.array([[-1, -2]], dtype=K.floatx())\n-\n-    result = f([negative_values])[0]\n-    true_result = (np.exp(negative_values) - 1) * scale * alpha\n-\n-    assert_allclose(result, true_result)\n-\n-\n-def test_tanh():\n-    test_values = get_standard_values()\n-\n-    x = K.placeholder(ndim=2)\n-    exp = activations.tanh(x)\n-    f = K.function([x], [exp])\n-\n-    result = f([test_values])[0]\n-    expected = np.tanh(test_values)\n-    assert_allclose(result, expected, rtol=1e-05)\n-\n-\n-def test_linear():\n-    xs = [1, 5, True, None]\n-    for x in xs:\n-        assert(x == activations.linear(x))\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,998 +0,0 @@\n-import os\n-import multiprocessing\n-\n-import numpy as np\n-import pytest\n-from numpy.testing import assert_allclose\n-from csv import reader\n-from csv import Sniffer\n-import shutil\n-from collections import defaultdict\n-from keras import optimizers\n-from keras import initializers\n-from keras import callbacks\n-from keras.models import Sequential, Model\n-from keras.layers import Input, Dense, Dropout, add, dot, Lambda, Layer\n-from keras.layers import Conv2D\n-from keras.layers import MaxPooling2D\n-from keras.layers import GlobalAveragePooling1D\n-from keras.layers import GlobalAveragePooling2D\n-from keras.layers import BatchNormalization\n-from keras.utils.test_utils import get_test_data\n-from keras.utils.generic_utils import to_list\n-from keras.utils.generic_utils import unpack_singleton\n-from keras import backend as K\n-from keras.utils import np_utils\n-\n-try:\n-    from unittest.mock import patch\n-except ImportError:\n-    from mock import patch\n-\n-\n-input_dim = 2\n-num_hidden = 4\n-num_classes = 2\n-batch_size = 5\n-train_samples = 20\n-test_samples = 20\n-\n-\n-def data_generator(x, y, batch_size):\n-    x = to_list(x)\n-    y = to_list(y)\n-    max_batch_index = len(x[0]) // batch_size\n-    i = 0\n-    while 1:\n-        x_batch = [array[i * batch_size: (i + 1) * batch_size] for array in x]\n-        x_batch = unpack_singleton(x_batch)\n-\n-        y_batch = [array[i * batch_size: (i + 1) * batch_size] for array in y]\n-        y_batch = unpack_singleton(y_batch)\n-        yield x_batch, y_batch\n-        i += 1\n-        i = i % max_batch_index\n-\n-\n-# Changing the default arguments of get_test_data.\n-def get_data_callbacks(num_train=train_samples,\n-                       num_test=test_samples,\n-                       input_shape=(input_dim,),\n-                       classification=True,\n-                       num_classes=num_classes):\n-    return get_test_data(num_train=num_train,\n-                         num_test=num_test,\n-                         input_shape=input_shape,\n-                         classification=classification,\n-                         num_classes=num_classes)\n-\n-\n-class Counter(callbacks.Callback):\n-    \"\"\"Counts the number of times each callback method was run.\n-\n-    # Arguments\n-        method_counts: dict, contains the counts of time\n-            each callback method was run.\n-    \"\"\"\n-\n-    def __init__(self):\n-        self.method_counts = defaultdict(int)\n-        methods_to_count = [\n-            'on_batch_begin', 'on_batch_end', 'on_epoch_begin', 'on_epoch_end',\n-            'on_train_batch_begin', 'on_train_batch_end',\n-            'on_test_batch_begin', 'on_test_batch_end',\n-            'on_predict_batch_begin', 'on_predict_batch_end',\n-            'on_train_begin', 'on_train_end',\n-            'on_predict_begin', 'on_predict_end',\n-            'on_test_begin', 'on_test_end',\n-        ]\n-        for method_name in methods_to_count:\n-            setattr(self, method_name,\n-                    self.wrap_with_counts(\n-                        method_name, getattr(self, method_name)))\n-\n-    def wrap_with_counts(self, method_name, method):\n-\n-        def _call_and_count(*args, **kwargs):\n-            self.method_counts[method_name] += 1\n-            return method(*args, **kwargs)\n-\n-        return _call_and_count\n-\n-\n-class TestCallbackCounts(object):\n-\n-    def _check_counts(self, counter, expected_counts):\n-        \"\"\"Checks that counts registered by `counter` are those expected.\"\"\"\n-        for method_name, expected_count in expected_counts.items():\n-            count = counter.method_counts[method_name]\n-            assert count == expected_count, \\\n-                'For method {}: expected {}, got: {}'.format(\n-                    method_name, expected_count, count)\n-\n-    def _get_model(self):\n-        layers = [\n-            Dense(10, activation='relu', input_dim=input_dim),\n-            Dense(num_classes, activation='softmax')\n-        ]\n-        model = Sequential(layers=layers)\n-        model.compile(optimizer='adam', loss='binary_crossentropy')\n-        return model\n-\n-    def test_callback_hooks_are_called_in_fit(self):\n-        np.random.seed(1337)\n-        (X_train, y_train), (X_test, y_test) = get_data_callbacks(num_train=10,\n-                                                                  num_test=4)\n-        y_train = np_utils.to_categorical(y_train)\n-        y_test = np_utils.to_categorical(y_test)\n-\n-        model = self._get_model()\n-        counter = Counter()\n-        model.fit(X_train, y_train, validation_data=(X_test, y_test),\n-                  batch_size=2, epochs=5, callbacks=[counter])\n-\n-        self._check_counts(\n-            counter, {\n-                'on_batch_begin': 25,\n-                'on_batch_end': 25,\n-                'on_epoch_begin': 5,\n-                'on_epoch_end': 5,\n-                'on_predict_batch_begin': 0,\n-                'on_predict_batch_end': 0,\n-                'on_predict_begin': 0,\n-                'on_predict_end': 0,\n-                'on_test_batch_begin': 10,\n-                'on_test_batch_end': 10,\n-                'on_test_begin': 5,\n-                'on_test_end': 5,\n-                'on_train_batch_begin': 25,\n-                'on_train_batch_end': 25,\n-                'on_train_begin': 1,\n-                'on_train_end': 1,\n-            })\n-\n-    def test_callback_hooks_are_called_in_evaluate(self):\n-        np.random.seed(1337)\n-        (_, _), (X_test, y_test) = get_data_callbacks(num_test=10)\n-\n-        y_test = np_utils.to_categorical(y_test)\n-\n-        model = self._get_model()\n-        counter = Counter()\n-        model.evaluate(X_test, y_test, batch_size=2, callbacks=[counter])\n-        self._check_counts(\n-            counter, {\n-                'on_test_batch_begin': 5,\n-                'on_test_batch_end': 5,\n-                'on_test_begin': 1,\n-                'on_test_end': 1,\n-                'on_batch_begin': 0,\n-                'on_batch_end': 0,\n-                'on_epoch_begin': 0,\n-                'on_epoch_end': 0,\n-                'on_predict_batch_begin': 0,\n-                'on_predict_batch_end': 0,\n-                'on_predict_begin': 0,\n-                'on_predict_end': 0,\n-                'on_train_batch_begin': 0,\n-                'on_train_batch_end': 0,\n-                'on_train_begin': 0,\n-                'on_train_end': 0,\n-            })\n-\n-    def test_callback_hooks_are_called_in_predict(self):\n-        np.random.seed(1337)\n-        (_, _), (X_test, _) = get_data_callbacks(num_test=10)\n-\n-        model = self._get_model()\n-        counter = Counter()\n-        model.predict(X_test, batch_size=2, callbacks=[counter])\n-        self._check_counts(\n-            counter, {\n-                'on_predict_batch_begin': 5,\n-                'on_predict_batch_end': 5,\n-                'on_predict_begin': 1,\n-                'on_predict_end': 1,\n-                'on_batch_begin': 0,\n-                'on_batch_end': 0,\n-                'on_epoch_begin': 0,\n-                'on_epoch_end': 0,\n-                'on_test_batch_begin': 0,\n-                'on_test_batch_end': 0,\n-                'on_test_begin': 0,\n-                'on_test_end': 0,\n-                'on_train_batch_begin': 0,\n-                'on_train_batch_end': 0,\n-                'on_train_begin': 0,\n-                'on_train_end': 0,\n-            })\n-\n-    def test_callback_hooks_are_called_in_fit_generator(self):\n-        np.random.seed(1337)\n-        (X_train, y_train), (X_test, y_test) = get_data_callbacks(num_train=10,\n-                                                                  num_test=4)\n-        y_train = np_utils.to_categorical(y_train)\n-        y_test = np_utils.to_categorical(y_test)\n-        train_generator = data_generator(X_train, y_train, batch_size=2)\n-        validation_generator = data_generator(X_test, y_test, batch_size=2)\n-\n-        model = self._get_model()\n-        counter = Counter()\n-        model.fit_generator(train_generator,\n-                            steps_per_epoch=len(X_train) // 2,\n-                            epochs=5,\n-                            validation_data=validation_generator,\n-                            validation_steps=len(X_test) // 2,\n-                            callbacks=[counter])\n-\n-        self._check_counts(\n-            counter, {\n-                'on_batch_begin': 25,\n-                'on_batch_end': 25,\n-                'on_epoch_begin': 5,\n-                'on_epoch_end': 5,\n-                'on_predict_batch_begin': 0,\n-                'on_predict_batch_end': 0,\n-                'on_predict_begin': 0,\n-                'on_predict_end': 0,\n-                'on_test_batch_begin': 10,\n-                'on_test_batch_end': 10,\n-                'on_test_begin': 5,\n-                'on_test_end': 5,\n-                'on_train_batch_begin': 25,\n-                'on_train_batch_end': 25,\n-                'on_train_begin': 1,\n-                'on_train_end': 1,\n-            })\n-\n-    def test_callback_hooks_are_called_in_evaluate_generator(self):\n-        np.random.seed(1337)\n-        (_, _), (X_test, y_test) = get_data_callbacks(num_test=10)\n-        y_test = np_utils.to_categorical(y_test)\n-\n-        model = self._get_model()\n-        counter = Counter()\n-        model.evaluate_generator(data_generator(X_test, y_test, batch_size=2),\n-                                 steps=len(X_test) // 2, callbacks=[counter])\n-        self._check_counts(\n-            counter, {\n-                'on_test_batch_begin': 5,\n-                'on_test_batch_end': 5,\n-                'on_test_begin': 1,\n-                'on_test_end': 1,\n-                'on_batch_begin': 0,\n-                'on_batch_end': 0,\n-                'on_epoch_begin': 0,\n-                'on_epoch_end': 0,\n-                'on_predict_batch_begin': 0,\n-                'on_predict_batch_end': 0,\n-                'on_predict_begin': 0,\n-                'on_predict_end': 0,\n-                'on_train_batch_begin': 0,\n-                'on_train_batch_end': 0,\n-                'on_train_begin': 0,\n-                'on_train_end': 0,\n-            })\n-\n-    def test_callback_hooks_are_called_in_predict_generator(self):\n-        np.random.seed(1337)\n-        (_, _), (X_test, _) = get_data_callbacks(num_test=10)\n-\n-        def data_generator(x, batch_size):\n-            x = to_list(x)\n-            max_batch_index = len(x[0]) // batch_size\n-            i = 0\n-            while 1:\n-                x_batch = [\n-                    array[i * batch_size: (i + 1) * batch_size] for array in x]\n-                x_batch = unpack_singleton(x_batch)\n-\n-                yield x_batch\n-                i += 1\n-                i = i % max_batch_index\n-\n-        model = self._get_model()\n-        counter = Counter()\n-        model.predict_generator(data_generator(X_test, batch_size=2),\n-                                steps=len(X_test) // 2, callbacks=[counter])\n-        self._check_counts(\n-            counter, {\n-                'on_predict_batch_begin': 5,\n-                'on_predict_batch_end': 5,\n-                'on_predict_begin': 1,\n-                'on_predict_end': 1,\n-                'on_batch_begin': 0,\n-                'on_batch_end': 0,\n-                'on_epoch_begin': 0,\n-                'on_epoch_end': 0,\n-                'on_test_batch_begin': 0,\n-                'on_test_batch_end': 0,\n-                'on_test_begin': 0,\n-                'on_test_end': 0,\n-                'on_train_batch_begin': 0,\n-                'on_train_batch_end': 0,\n-                'on_train_begin': 0,\n-                'on_train_end': 0,\n-            })\n-\n-    def test_callback_list_methods(self):\n-        counter = Counter()\n-        callback_list = callbacks.CallbackList([counter])\n-\n-        batch = 0\n-        callback_list.on_test_batch_begin(batch)\n-        callback_list.on_test_batch_end(batch)\n-        callback_list.on_predict_batch_begin(batch)\n-        callback_list.on_predict_batch_end(batch)\n-\n-        self._check_counts(\n-            counter, {\n-                'on_test_batch_begin': 1,\n-                'on_test_batch_end': 1,\n-                'on_predict_batch_begin': 1,\n-                'on_predict_batch_end': 1,\n-                'on_predict_begin': 0,\n-                'on_predict_end': 0,\n-                'on_batch_begin': 0,\n-                'on_batch_end': 0,\n-                'on_epoch_begin': 0,\n-                'on_epoch_end': 0,\n-                'on_test_begin': 0,\n-                'on_test_end': 0,\n-                'on_train_batch_begin': 0,\n-                'on_train_batch_end': 0,\n-                'on_train_begin': 0,\n-                'on_train_end': 0,\n-            })\n-\n-\n-def test_TerminateOnNaN():\n-    np.random.seed(1337)\n-    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n-\n-    y_test = np_utils.to_categorical(y_test)\n-    y_train = np_utils.to_categorical(y_train)\n-    cbks = [callbacks.TerminateOnNaN()]\n-    model = Sequential()\n-    initializer = initializers.Constant(value=1e5)\n-    for _ in range(5):\n-        model.add(Dense(num_hidden, input_dim=input_dim, activation='relu',\n-                        kernel_initializer=initializer))\n-    model.add(Dense(num_classes, activation='linear'))\n-    model.compile(loss='mean_squared_error',\n-                  optimizer='rmsprop')\n-\n-    # case 1 fit\n-    history = model.fit(X_train, y_train,\n-                        batch_size=batch_size,\n-                        validation_data=(X_test, y_test),\n-                        callbacks=cbks,\n-                        epochs=20)\n-    loss = history.history['loss']\n-    assert len(loss) == 1\n-    assert np.isnan(loss[0])\n-    assert len(history.history) < 20\n-\n-    history = model.fit_generator(data_generator(X_train, y_train, batch_size),\n-                                  len(X_train),\n-                                  validation_data=(X_test, y_test),\n-                                  callbacks=cbks,\n-                                  epochs=20)\n-    loss = history.history['loss']\n-    assert len(loss) == 1\n-    assert np.isnan(loss[0])\n-    assert len(history.history) < 20\n-\n-\n-def test_stop_training_csv(tmpdir):\n-    np.random.seed(1337)\n-    fp = str(tmpdir / 'test.csv')\n-    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n-\n-    y_test = np_utils.to_categorical(y_test)\n-    y_train = np_utils.to_categorical(y_train)\n-    cbks = [callbacks.TerminateOnNaN(), callbacks.CSVLogger(fp)]\n-    model = Sequential()\n-    for _ in range(5):\n-        model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n-    model.add(Dense(num_classes, activation='linear'))\n-    model.compile(loss='mean_squared_error',\n-                  optimizer='rmsprop')\n-\n-    def data_generator():\n-        i = 0\n-        max_batch_index = len(X_train) // batch_size\n-        tot = 0\n-        while 1:\n-            if tot > 3 * len(X_train):\n-                yield (np.ones([batch_size, input_dim]) * np.nan,\n-                       np.ones([batch_size, num_classes]) * np.nan)\n-            else:\n-                yield (X_train[i * batch_size: (i + 1) * batch_size],\n-                       y_train[i * batch_size: (i + 1) * batch_size])\n-            i += 1\n-            tot += 1\n-            i = i % max_batch_index\n-\n-    history = model.fit_generator(data_generator(),\n-                                  len(X_train) // batch_size,\n-                                  validation_data=(X_test, y_test),\n-                                  callbacks=cbks,\n-                                  epochs=20)\n-    loss = history.history['loss']\n-    assert len(loss) > 1\n-    assert loss[-1] == np.inf or np.isnan(loss[-1])\n-\n-    values = []\n-    with open(fp) as f:\n-        for x in reader(f):\n-            values.append(x)\n-\n-    assert 'nan' in values[-1], 'The last epoch was not logged.'\n-    os.remove(fp)\n-\n-\n-def test_ModelCheckpoint(tmpdir):\n-    np.random.seed(1337)\n-    filepath = str(tmpdir / 'checkpoint.h5')\n-    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n-    y_test = np_utils.to_categorical(y_test)\n-    y_train = np_utils.to_categorical(y_train)\n-    # case 1\n-    monitor = 'val_loss'\n-    save_best_only = False\n-    mode = 'auto'\n-\n-    model = Sequential()\n-    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n-    model.add(Dense(num_classes, activation='softmax'))\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer='rmsprop',\n-                  metrics=['accuracy'])\n-\n-    cbks = [callbacks.ModelCheckpoint(filepath, monitor=monitor,\n-                                      save_best_only=save_best_only, mode=mode)]\n-    model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n-    assert os.path.isfile(filepath)\n-    os.remove(filepath)\n-\n-    # case 2\n-    mode = 'min'\n-    cbks = [callbacks.ModelCheckpoint(filepath, monitor=monitor,\n-                                      save_best_only=save_best_only, mode=mode)]\n-    model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n-    assert os.path.isfile(filepath)\n-    os.remove(filepath)\n-\n-    # case 3\n-    mode = 'max'\n-    monitor = 'val_accuracy'\n-    cbks = [callbacks.ModelCheckpoint(filepath,\n-                                      monitor=monitor,\n-                                      save_best_only=save_best_only,\n-                                      mode=mode)]\n-    model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n-    assert os.path.isfile(filepath)\n-    os.remove(filepath)\n-\n-    # case 4\n-    save_best_only = True\n-    cbks = [callbacks.ModelCheckpoint(filepath,\n-                                      monitor=monitor,\n-                                      save_best_only=save_best_only,\n-                                      mode=mode)]\n-    model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n-    assert os.path.isfile(filepath)\n-    os.remove(filepath)\n-\n-    # case 5\n-    save_best_only = False\n-    period = 2\n-    mode = 'auto'\n-    filepath = 'checkpoint.{epoch:02d}.h5'\n-    cbks = [callbacks.ModelCheckpoint(filepath,\n-                                      monitor=monitor,\n-                                      save_best_only=save_best_only,\n-                                      mode=mode,\n-                                      period=period)]\n-    model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, epochs=4)\n-    assert os.path.isfile(filepath.format(epoch=2))\n-    assert os.path.isfile(filepath.format(epoch=4))\n-    assert not os.path.exists(filepath.format(epoch=1))\n-    assert not os.path.exists(filepath.format(epoch=3))\n-    os.remove(filepath.format(epoch=2))\n-    os.remove(filepath.format(epoch=4))\n-    assert not tmpdir.listdir()\n-\n-\n-def test_EarlyStopping():\n-    np.random.seed(1337)\n-    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n-    y_test = np_utils.to_categorical(y_test)\n-    y_train = np_utils.to_categorical(y_train)\n-    model = Sequential()\n-    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n-    model.add(Dense(num_classes, activation='softmax'))\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer='rmsprop',\n-                  metrics=['accuracy'])\n-    mode = 'max'\n-    monitor = 'val_acc'\n-    patience = 0\n-    cbks = [callbacks.EarlyStopping(patience=patience,\n-                                    monitor=monitor,\n-                                    mode=mode)]\n-    history = model.fit(X_train, y_train,\n-                        batch_size=batch_size,\n-                        validation_data=(X_test, y_test),\n-                        callbacks=cbks,\n-                        epochs=20)\n-    mode = 'auto'\n-    monitor = 'val_acc'\n-    patience = 2\n-    cbks = [callbacks.EarlyStopping(patience=patience,\n-                                    monitor=monitor,\n-                                    mode=mode)]\n-    history = model.fit(X_train, y_train,\n-                        batch_size=batch_size,\n-                        validation_data=(X_test, y_test),\n-                        callbacks=cbks,\n-                        epochs=20)\n-\n-\n-def test_EarlyStopping_reuse():\n-    np.random.seed(1337)\n-    patience = 3\n-    data = np.random.random((100, 1))\n-    labels = np.where(data > 0.5, 1, 0)\n-    model = Sequential((\n-        Dense(1, input_dim=1, activation='relu'),\n-        Dense(1, activation='sigmoid'),\n-    ))\n-    model.compile(optimizer='sgd',\n-                  loss='binary_crossentropy',\n-                  metrics=['accuracy'])\n-    stopper = callbacks.EarlyStopping(monitor='acc', patience=patience)\n-    weights = model.get_weights()\n-\n-    hist = model.fit(data, labels, callbacks=[stopper], epochs=20)\n-    assert len(hist.epoch) >= patience\n-\n-    # This should allow training to go for at least `patience` epochs\n-    model.set_weights(weights)\n-    hist = model.fit(data, labels, callbacks=[stopper], epochs=20)\n-    assert len(hist.epoch) >= patience\n-\n-\n-def test_EarlyStopping_patience():\n-    class DummyModel(object):\n-        def __init__(self):\n-            self.stop_training = False\n-\n-        def get_weights(self):\n-            return []\n-\n-        def set_weights(self, weights):\n-            pass\n-\n-    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=2)\n-    early_stop.model = DummyModel()\n-\n-    losses = [0.0860, 0.1096, 0.1040, 0.1019]\n-\n-    # Should stop after epoch 3,\n-    # as the loss has not improved after patience=2 epochs.\n-    epochs_trained = 0\n-    early_stop.on_train_begin()\n-\n-    for epoch in range(len(losses)):\n-        epochs_trained += 1\n-        early_stop.on_epoch_end(epoch, logs={'val_loss': losses[epoch]})\n-\n-        if early_stop.model.stop_training:\n-            break\n-\n-    assert epochs_trained == 3\n-\n-\n-def test_EarlyStopping_baseline():\n-    class DummyModel(object):\n-        def __init__(self):\n-            self.stop_training = False\n-\n-        def get_weights(self):\n-            return []\n-\n-        def set_weights(self, weights):\n-            pass\n-\n-    def baseline_tester(acc_levels):\n-        early_stop = callbacks.EarlyStopping(monitor='val_acc', baseline=0.75,\n-                                             patience=2)\n-        early_stop.model = DummyModel()\n-        epochs_trained = 0\n-        early_stop.on_train_begin()\n-        for epoch in range(len(acc_levels)):\n-            epochs_trained += 1\n-            early_stop.on_epoch_end(epoch, logs={'val_acc': acc_levels[epoch]})\n-            if early_stop.model.stop_training:\n-                break\n-        return epochs_trained\n-\n-    acc_levels = [0.55, 0.76, 0.81, 0.81]\n-    baseline_met = baseline_tester(acc_levels)\n-    acc_levels = [0.55, 0.74, 0.81, 0.81]\n-    baseline_not_met = baseline_tester(acc_levels)\n-\n-    # All epochs should run because baseline was met in second epoch\n-    assert baseline_met == 4\n-    # Baseline was not met by second epoch and should stop\n-    assert baseline_not_met == 2\n-\n-\n-def test_EarlyStopping_final_weights():\n-    class DummyModel(object):\n-        def __init__(self):\n-            self.stop_training = False\n-            self.weights = -1\n-\n-        def get_weights(self):\n-            return self.weights\n-\n-        def set_weights(self, weights):\n-            self.weights = weights\n-\n-        def set_weight_to_epoch(self, epoch):\n-            self.weights = epoch\n-\n-    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=2)\n-    early_stop.model = DummyModel()\n-\n-    losses = [0.2, 0.15, 0.1, 0.11, 0.12]\n-\n-    epochs_trained = 0\n-    early_stop.on_train_begin()\n-\n-    for epoch in range(len(losses)):\n-        epochs_trained += 1\n-        early_stop.model.set_weight_to_epoch(epoch=epoch)\n-        early_stop.on_epoch_end(epoch, logs={'val_loss': losses[epoch]})\n-\n-        if early_stop.model.stop_training:\n-            break\n-\n-    # The best configuration is in the epoch 2 (loss = 0.1000),\n-    # so with patience=2 we need to end up at epoch 4\n-    assert early_stop.model.get_weights() == 4\n-\n-\n-def test_EarlyStopping_final_weights_when_restoring_model_weights():\n-    class DummyModel(object):\n-        def __init__(self):\n-            self.stop_training = False\n-            self.weights = -1\n-\n-        def get_weights(self):\n-            return self.weights\n-\n-        def set_weights(self, weights):\n-            self.weights = weights\n-\n-        def set_weight_to_epoch(self, epoch):\n-            self.weights = epoch\n-\n-    early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=2,\n-                                         restore_best_weights=True)\n-    early_stop.model = DummyModel()\n-\n-    losses = [0.2, 0.15, 0.1, 0.11, 0.12]\n-\n-    # The best configuration is in the epoch 2 (loss = 0.1000).\n-\n-    epochs_trained = 0\n-    early_stop.on_train_begin()\n-\n-    for epoch in range(len(losses)):\n-        epochs_trained += 1\n-        early_stop.model.set_weight_to_epoch(epoch=epoch)\n-        early_stop.on_epoch_end(epoch, logs={'val_loss': losses[epoch]})\n-\n-        if early_stop.model.stop_training:\n-            break\n-\n-    # The best configuration is in epoch 2 (loss = 0.1000),\n-    # and while patience = 2, we're restoring the best weights,\n-    # so we end up at the epoch with the best weights, i.e. epoch 2\n-    assert early_stop.model.get_weights() == 2\n-\n-\n-def test_LearningRateScheduler():\n-    np.random.seed(1337)\n-    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n-    y_test = np_utils.to_categorical(y_test)\n-    y_train = np_utils.to_categorical(y_train)\n-    model = Sequential()\n-    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n-    model.add(Dense(num_classes, activation='softmax'))\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer='sgd',\n-                  metrics=['accuracy'])\n-\n-    cbks = [callbacks.LearningRateScheduler(lambda x: 1. / (1. + x))]\n-    model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, epochs=5)\n-    assert (float(K.get_value(model.optimizer.lr)) - 0.2) < K.epsilon()\n-\n-\n-def test_ReduceLROnPlateau():\n-    np.random.seed(1337)\n-    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n-    y_test = np_utils.to_categorical(y_test)\n-    y_train = np_utils.to_categorical(y_train)\n-\n-    def make_model():\n-        np.random.seed(1337)\n-        model = Sequential()\n-        model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n-        model.add(Dense(num_classes, activation='softmax'))\n-\n-        model.compile(loss='categorical_crossentropy',\n-                      optimizer=optimizers.SGD(lr=0.1),\n-                      metrics=['accuracy'])\n-        return model\n-\n-    model = make_model()\n-\n-    # This should reduce the LR after the first epoch (due to high epsilon).\n-    cbks = [callbacks.ReduceLROnPlateau(monitor='val_loss',\n-                                        factor=0.1,\n-                                        min_delta=10,\n-                                        patience=1,\n-                                        cooldown=5)]\n-    model.fit(X_train, y_train,\n-              batch_size=batch_size,\n-              validation_data=(X_test, y_test),\n-              callbacks=cbks,\n-              epochs=5,\n-              verbose=2)\n-    assert_allclose(\n-        float(K.get_value(model.optimizer.lr)), 0.01, atol=K.epsilon())\n-\n-    model = make_model()\n-    cbks = [callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n-                                        min_delta=0, patience=1, cooldown=5)]\n-    model.fit(X_train, y_train,\n-              batch_size=batch_size,\n-              validation_data=(X_test, y_test),\n-              callbacks=cbks,\n-              epochs=5,\n-              verbose=2)\n-    assert_allclose(\n-        float(K.get_value(model.optimizer.lr)), 0.1, atol=K.epsilon())\n-\n-\n-def test_ReduceLROnPlateau_patience():\n-    class DummyOptimizer(object):\n-        def __init__(self):\n-            self.lr = K.variable(1.0)\n-\n-    class DummyModel(object):\n-        def __init__(self):\n-            self.optimizer = DummyOptimizer()\n-\n-    reduce_on_plateau = callbacks.ReduceLROnPlateau(monitor='val_loss',\n-                                                    patience=2)\n-    reduce_on_plateau.model = DummyModel()\n-\n-    losses = [0.0860, 0.1096, 0.1040]\n-    lrs = []\n-\n-    for epoch in range(len(losses)):\n-        reduce_on_plateau.on_epoch_end(epoch, logs={'val_loss': losses[epoch]})\n-        lrs.append(K.get_value(reduce_on_plateau.model.optimizer.lr))\n-\n-    # The learning rates should be 1.0 except the last one\n-    assert all([lr == 1.0 for lr in lrs[:-1]]) and lrs[-1] < 1.0\n-\n-\n-def DISABLED_test_ReduceLROnPlateau_backwards_compatibility():\n-    import warnings\n-    with warnings.catch_warnings(record=True) as ws:\n-        reduce_on_plateau = callbacks.ReduceLROnPlateau(epsilon=1e-13)\n-        # Check if warnings are disabled\n-        if os.environ.get(\"PYTHONWARNINGS\") != \"ignore\":\n-            assert \"`epsilon` argument is deprecated\" in str(ws[0].message)\n-    assert not hasattr(reduce_on_plateau, 'epsilon')\n-    assert hasattr(reduce_on_plateau, 'min_delta')\n-    assert reduce_on_plateau.min_delta == 1e-13\n-\n-\n-def test_CSVLogger(tmpdir):\n-    np.random.seed(1337)\n-    filepath = str(tmpdir / 'log.tsv')\n-    sep = '\\t'\n-    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n-    y_test = np_utils.to_categorical(y_test)\n-    y_train = np_utils.to_categorical(y_train)\n-\n-    def make_model():\n-        np.random.seed(1337)\n-        model = Sequential()\n-        model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n-        model.add(Dense(num_classes, activation='softmax'))\n-\n-        model.compile(loss='categorical_crossentropy',\n-                      optimizer=optimizers.SGD(lr=0.1),\n-                      metrics=['accuracy'])\n-        return model\n-\n-    # case 1, create new file with defined separator\n-    model = make_model()\n-    cbks = [callbacks.CSVLogger(filepath, separator=sep)]\n-    model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n-\n-    assert os.path.isfile(filepath)\n-    with open(filepath) as csvfile:\n-        dialect = Sniffer().sniff(csvfile.read())\n-    assert dialect.delimiter == sep\n-    del model\n-    del cbks\n-\n-    # case 2, append data to existing file, skip header\n-    model = make_model()\n-    cbks = [callbacks.CSVLogger(filepath, separator=sep, append=True)]\n-    model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n-\n-    # case 3, reuse of CSVLogger object\n-    model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, epochs=2)\n-\n-    import re\n-    with open(filepath) as csvfile:\n-        list_lines = csvfile.readlines()\n-        for line in list_lines:\n-            assert line.count(sep) == 4\n-        assert len(list_lines) == 5\n-        output = \" \".join(list_lines)\n-        assert len(re.findall('epoch', output)) == 1\n-\n-    os.remove(filepath)\n-    assert not tmpdir.listdir()\n-\n-\n-def DISABLED_test_CallbackValData():\n-    np.random.seed(1337)\n-    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n-    y_test = np_utils.to_categorical(y_test)\n-    y_train = np_utils.to_categorical(y_train)\n-    model = Sequential()\n-    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n-    model.add(Dense(num_classes, activation='softmax'))\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer='sgd',\n-                  metrics=['accuracy'])\n-\n-    cbk = callbacks.LambdaCallback(on_train_end=lambda x: 1)\n-    model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=[cbk], epochs=1)\n-\n-    cbk2 = callbacks.LambdaCallback(on_train_end=lambda x: 1)\n-    train_generator = data_generator(X_train, y_train, batch_size)\n-    model.fit_generator(train_generator, len(X_train), epochs=1,\n-                        validation_data=(X_test, y_test),\n-                        callbacks=[cbk2])\n-\n-    # callback validation data should always have x, y, and sample weights\n-    assert len(cbk.validation_data) == len(cbk2.validation_data) == 3\n-    assert cbk.validation_data[0] is cbk2.validation_data[0]\n-    assert cbk.validation_data[1] is cbk2.validation_data[1]\n-    assert cbk.validation_data[2].shape == cbk2.validation_data[2].shape\n-\n-\n-def test_LambdaCallback():\n-    np.random.seed(1337)\n-    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n-    y_test = np_utils.to_categorical(y_test)\n-    y_train = np_utils.to_categorical(y_train)\n-    model = Sequential()\n-    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n-    model.add(Dense(num_classes, activation='softmax'))\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer='sgd',\n-                  metrics=['accuracy'])\n-\n-    # Start an arbitrary process that should run during model training and\n-    # be terminated after training has completed.\n-    def f():\n-        while True:\n-            pass\n-\n-    p = multiprocessing.Process(target=f)\n-    p.start()\n-    cleanup_callback = callbacks.LambdaCallback(\n-        on_train_end=lambda logs: p.terminate())\n-\n-    cbks = [cleanup_callback]\n-    model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, epochs=5)\n-    p.join()\n-    assert not p.is_alive()\n-\n-\n-@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Uses TensorBoard')\n-def test_TensorBoard_with_ReduceLROnPlateau(tmpdir):\n-    import shutil\n-    np.random.seed(np.random.randint(1, 1e7))\n-    filepath = str(tmpdir / 'logs')\n-\n-    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n-    y_test = np_utils.to_categorical(y_test)\n-    y_train = np_utils.to_categorical(y_train)\n-\n-    model = Sequential()\n-    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n-    model.add(Dense(num_classes, activation='softmax'))\n-    model.compile(loss='binary_crossentropy',\n-                  optimizer='sgd',\n-                  metrics=['accuracy'])\n-\n-    cbks = [\n-        callbacks.ReduceLROnPlateau(\n-            monitor='val_loss',\n-            factor=0.5,\n-            patience=4,\n-            verbose=1),\n-        callbacks.TensorBoard(\n-            log_dir=filepath)]\n-\n-    model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test), callbacks=cbks, epochs=2)\n-\n-    assert os.path.isdir(filepath)\n-    shutil.rmtree(filepath)\n-    assert not tmpdir.listdir()\n-\n-\n-def tests_RemoteMonitor():\n-    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n-    y_test = np_utils.to_categorical(y_test)\n-    y_train = np_utils.to_categorical(y_train)\n-    model = Sequential()\n-    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n-    model.add(Dense(num_classes, activation='softmax'))\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer='rmsprop',\n-                  metrics=['accuracy'])\n-    cbks = [callbacks.RemoteMonitor()]\n-\n-    with patch('requests.post'):\n-        model.fit(X_train, y_train, batch_size=batch_size,\n-                  validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n-\n-\n-def tests_RemoteMonitorWithJsonPayload():\n-    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n-    y_test = np_utils.to_categorical(y_test)\n-    y_train = np_utils.to_categorical(y_train)\n-    model = Sequential()\n-    model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n-    model.add(Dense(num_classes, activation='softmax'))\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer='rmsprop',\n-                  metrics=['accuracy'])\n-    cbks = [callbacks.RemoteMonitor(send_as_json=True)]\n-\n-    with patch('requests.post'):\n-        model.fit(X_train, y_train, batch_size=batch_size,\n-                  validation_data=(X_test, y_test), callbacks=cbks, epochs=1)\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,273 +0,0 @@\n-import os\n-import numpy as np\n-import pytest\n-import shutil\n-\n-from keras import callbacks\n-from keras.models import Sequential, Model\n-from keras import layers\n-from keras import backend as K\n-from keras.utils import np_utils\n-from keras.utils.test_utils import get_test_data\n-from keras.utils.generic_utils import to_list\n-from keras.utils.generic_utils import unpack_singleton\n-\n-\n-input_dim = 2\n-num_hidden = 4\n-num_classes = 2\n-batch_size = 5\n-train_samples = 20\n-test_samples = 20\n-\n-\n-if K.backend() != 'tensorflow':\n-    pytestmark = pytest.mark.skip\n-\n-\n-def data_generator(x, y, batch_size):\n-    x = to_list(x)\n-    y = to_list(y)\n-    max_batch_index = len(x[0]) // batch_size\n-    i = 0\n-    while 1:\n-        x_batch = [array[i * batch_size: (i + 1) * batch_size] for array in x]\n-        x_batch = unpack_singleton(x_batch)\n-\n-        y_batch = [array[i * batch_size: (i + 1) * batch_size] for array in y]\n-        y_batch = unpack_singleton(y_batch)\n-        yield x_batch, y_batch\n-        i += 1\n-        i = i % max_batch_index\n-\n-\n-# Changing the default arguments of get_test_data.\n-def get_data_callbacks(num_train=train_samples,\n-                       num_test=test_samples,\n-                       input_shape=(input_dim,),\n-                       classification=True,\n-                       num_classes=num_classes):\n-    return get_test_data(num_train=num_train,\n-                         num_test=num_test,\n-                         input_shape=input_shape,\n-                         classification=classification,\n-                         num_classes=num_classes)\n-\n-\n-@pytest.mark.parametrize('update_freq', ['batch', 'epoch', 9])\n-def DISABLED_test_TensorBoard(tmpdir, update_freq):\n-    np.random.seed(np.random.randint(1, 1e7))\n-    filepath = str(tmpdir / 'logs')\n-\n-    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n-    y_test = np_utils.to_categorical(y_test)\n-    y_train = np_utils.to_categorical(y_train)\n-\n-    class DummyStatefulMetric(layers.Layer):\n-\n-        def __init__(self, name='dummy_stateful_metric', **kwargs):\n-            super(DummyStatefulMetric, self).__init__(name=name, **kwargs)\n-            self.stateful = True\n-            self.state = K.variable(value=0, dtype='int32')\n-\n-        def reset_states(self):\n-            pass\n-\n-        def __call__(self, y_true, y_pred):\n-            return self.state\n-\n-    inp = layers.Input((input_dim,))\n-    hidden = layers.Dense(num_hidden, activation='relu')(inp)\n-    hidden = layers.Dropout(0.1)(hidden)\n-    hidden = layers.BatchNormalization()(hidden)\n-    output = layers.Dense(num_classes, activation='softmax')(hidden)\n-    model = Model(inputs=inp, outputs=output)\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer='sgd',\n-                  metrics=['accuracy', DummyStatefulMetric()])\n-\n-    # we must generate new callbacks for each test, as they aren't stateless\n-    def callbacks_factory(histogram_freq=0,\n-                          embeddings_freq=0,\n-                          write_images=False,\n-                          write_grads=False):\n-        if embeddings_freq:\n-            embeddings_layer_names = ['dense_1']\n-            embeddings_data = X_test\n-        else:\n-            embeddings_layer_names = None\n-            embeddings_data = None\n-        return [callbacks.TensorBoard(log_dir=filepath,\n-                                      histogram_freq=histogram_freq,\n-                                      write_images=write_images,\n-                                      write_grads=write_grads,\n-                                      embeddings_freq=embeddings_freq,\n-                                      embeddings_layer_names=embeddings_layer_names,\n-                                      embeddings_data=embeddings_data,\n-                                      update_freq=update_freq)]\n-\n-    # fit without validation data\n-    model.fit(X_train, y_train, batch_size=batch_size,\n-              callbacks=callbacks_factory(),\n-              epochs=2)\n-\n-    # fit with validation data and accuracy\n-    model.fit(X_train, y_train, batch_size=batch_size,\n-              validation_data=(X_test, y_test),\n-              callbacks=callbacks_factory(),\n-              epochs=2)\n-\n-    # fit generator without validation data\n-    train_generator = data_generator(X_train, y_train, batch_size)\n-    model.fit_generator(train_generator, len(X_train), epochs=2,\n-                        callbacks=callbacks_factory())\n-\n-    # fit generator with validation data and accuracy\n-    train_generator = data_generator(X_train, y_train, batch_size)\n-    model.fit_generator(train_generator, len(X_train), epochs=2,\n-                        validation_data=(X_test, y_test),\n-                        callbacks=callbacks_factory(histogram_freq=1))\n-\n-    assert os.path.isdir(filepath)\n-    shutil.rmtree(filepath)\n-    assert not tmpdir.listdir()\n-\n-\n-def test_TensorBoard_multi_input_output(tmpdir):\n-    np.random.seed(np.random.randint(1, 1e7))\n-    filepath = str(tmpdir / 'logs')\n-\n-    (X_train, y_train), (X_test, y_test) = get_data_callbacks(\n-        input_shape=(input_dim, input_dim))\n-\n-    y_test = np_utils.to_categorical(y_test)\n-    y_train = np_utils.to_categorical(y_train)\n-\n-    inp1 = layers.Input((input_dim, input_dim))\n-    inp2 = layers.Input((input_dim, input_dim))\n-    inp_3d = layers.add([inp1, inp2])\n-    inp_2d = layers.GlobalAveragePooling1D()(inp_3d)\n-    # test a layer with a list of output tensors\n-    inp_pair = layers.Lambda(lambda x: x)([inp_3d, inp_2d])\n-    hidden = layers.dot(inp_pair, axes=-1)\n-    hidden = layers.Dense(num_hidden, activation='relu')(hidden)\n-    hidden = layers.Dropout(0.1)(hidden)\n-    output1 = layers.Dense(num_classes, activation='softmax')(hidden)\n-    output2 = layers.Dense(num_classes, activation='softmax')(hidden)\n-    model = Model(inputs=[inp1, inp2], outputs=[output1, output2])\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer='sgd',\n-                  metrics=['accuracy'])\n-\n-    # we must generate new callbacks for each test, as they aren't stateless\n-    def callbacks_factory(histogram_freq=0,\n-                          embeddings_freq=0,\n-                          write_images=False,\n-                          write_grads=False):\n-        if embeddings_freq:\n-            embeddings_layer_names = ['dense_1']\n-            embeddings_data = [X_test] * 2\n-        else:\n-            embeddings_layer_names = None\n-            embeddings_data = None\n-        return [callbacks.TensorBoard(log_dir=filepath,\n-                                      histogram_freq=histogram_freq,\n-                                      write_images=write_images,\n-                                      write_grads=write_grads,\n-                                      embeddings_freq=embeddings_freq,\n-                                      embeddings_layer_names=embeddings_layer_names,\n-                                      embeddings_data=embeddings_data)]\n-\n-    # fit without validation data\n-    model.fit([X_train] * 2, [y_train] * 2, batch_size=batch_size,\n-              callbacks=callbacks_factory(),\n-              epochs=3)\n-\n-    # fit with validation data and accuracy\n-    model.fit([X_train] * 2, [y_train] * 2, batch_size=batch_size,\n-              validation_data=([X_test] * 2, [y_test] * 2),\n-              callbacks=callbacks_factory(histogram_freq=1),\n-              epochs=2)\n-\n-    train_generator = data_generator([X_train] * 2, [y_train] * 2, batch_size)\n-\n-    # fit generator without validation data\n-    model.fit_generator(train_generator, len(X_train), epochs=2,\n-                        callbacks=callbacks_factory())\n-\n-    # fit generator with validation data and accuracy\n-    model.fit_generator(train_generator, len(X_train), epochs=2,\n-                        validation_data=([X_test] * 2, [y_test] * 2),\n-                        callbacks=callbacks_factory())\n-\n-    assert os.path.isdir(filepath)\n-    shutil.rmtree(filepath)\n-    assert not tmpdir.listdir()\n-\n-\n-def test_TensorBoard_convnet(tmpdir):\n-    np.random.seed(np.random.randint(1, 1e7))\n-    filepath = str(tmpdir / 'logs')\n-\n-    input_shape = (16, 16, 3)\n-    (x_train, y_train), (x_test, y_test) = get_data_callbacks(\n-        num_train=500,\n-        num_test=200,\n-        input_shape=input_shape)\n-    y_train = np_utils.to_categorical(y_train)\n-    y_test = np_utils.to_categorical(y_test)\n-\n-    model = Sequential([\n-        layers.Conv2D(filters=8, kernel_size=3,\n-                      activation='relu',\n-                      input_shape=input_shape),\n-        layers.MaxPooling2D(pool_size=2),\n-        layers.Conv2D(filters=4, kernel_size=(3, 3),\n-                      activation='relu', padding='same'),\n-        layers.BatchNormalization(),\n-        layers.GlobalAveragePooling2D(),\n-        layers.Dense(num_classes, activation='softmax')\n-    ])\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer='rmsprop',\n-                  metrics=['accuracy'])\n-    tsb = callbacks.TensorBoard(filepath, histogram_freq=1)\n-    cbks = [tsb]\n-    model.summary()\n-    history = model.fit(x_train, y_train, epochs=2, batch_size=16,\n-                        validation_data=(x_test, y_test),\n-                        callbacks=cbks,\n-                        verbose=0)\n-    assert os.path.isdir(filepath)\n-    shutil.rmtree(filepath)\n-    assert not tmpdir.listdir()\n-\n-\n-def test_TensorBoard_display_float_from_logs(tmpdir):\n-    filepath = str(tmpdir / 'logs')\n-\n-    input_shape = (3,)\n-    (x_train, y_train), _ = get_data_callbacks(num_train=10,\n-                                               num_test=0,\n-                                               input_shape=input_shape)\n-    y_train = np_utils.to_categorical(y_train)\n-\n-    model = Sequential([\n-        layers.Dense(num_classes, activation='softmax')\n-    ])\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer='rmsprop')\n-\n-    class CustomCallback(callbacks.Callback):\n-\n-        def on_epoch_end(self, epoch, logs=None):\n-            logs['test'] = 0.\n-\n-    tsb = callbacks.TensorBoard(log_dir=filepath)\n-    cbks = [CustomCallback(), tsb]\n-    model.fit(x_train, y_train, epochs=2, batch_size=16,\n-              callbacks=cbks,\n-              verbose=0)\n-    assert os.path.isdir(filepath)\n-    shutil.rmtree(filepath)\n-    assert not tmpdir.listdir()\n\n@@ -1,79 +0,0 @@\n-import pytest\n-import numpy as np\n-from numpy.testing import assert_allclose\n-\n-from keras import backend as K\n-from keras import constraints\n-\n-\n-def get_test_values():\n-    return [0.1, 0.5, 3, 8, 1e-7]\n-\n-\n-def get_example_array():\n-    np.random.seed(3537)\n-    example_array = np.random.random((100, 100)) * 100. - 50.\n-    example_array[0, 0] = 0.  # 0 could possibly cause trouble\n-    return example_array\n-\n-\n-def test_serialization():\n-    all_activations = ['max_norm', 'non_neg',\n-                       'unit_norm', 'min_max_norm']\n-    for name in all_activations:\n-        fn = constraints.get(name)\n-        ref_fn = getattr(constraints, name)()\n-        assert fn.__class__ == ref_fn.__class__\n-        config = constraints.serialize(fn)\n-        fn = constraints.deserialize(config)\n-        assert fn.__class__ == ref_fn.__class__\n-\n-\n-def test_max_norm():\n-    array = get_example_array()\n-    for m in get_test_values():\n-        norm_instance = constraints.max_norm(m)\n-        normed = norm_instance(K.variable(array))\n-        assert(np.all(K.eval(normed) < m))\n-\n-    # a more explicit example\n-    norm_instance = constraints.max_norm(2.0)\n-    x = np.array([[0, 0, 0], [1.0, 0, 0], [3, 0, 0], [3, 3, 3]]).T\n-    x_normed_target = np.array([[0, 0, 0], [1.0, 0, 0],\n-                                [2.0, 0, 0],\n-                                [2. / np.sqrt(3),\n-                                 2. / np.sqrt(3),\n-                                 2. / np.sqrt(3)]]).T\n-    x_normed_actual = K.eval(norm_instance(K.variable(x)))\n-    assert_allclose(x_normed_actual, x_normed_target, rtol=1e-05)\n-\n-\n-def test_non_neg():\n-    non_neg_instance = constraints.non_neg()\n-    normed = non_neg_instance(K.variable(get_example_array()))\n-    assert(np.all(np.min(K.eval(normed), axis=1) == 0.))\n-\n-\n-def test_unit_norm():\n-    unit_norm_instance = constraints.unit_norm()\n-    normalized = unit_norm_instance(K.variable(get_example_array()))\n-    norm_of_normalized = np.sqrt(np.sum(K.eval(normalized) ** 2, axis=0))\n-    # In the unit norm constraint, it should be equal to 1.\n-    difference = norm_of_normalized - 1.\n-    largest_difference = np.max(np.abs(difference))\n-    assert(np.abs(largest_difference) < 10e-5)\n-\n-\n-def test_min_max_norm():\n-    array = get_example_array()\n-    for m in get_test_values():\n-        norm_instance = constraints.min_max_norm(min_value=m, max_value=m * 2)\n-        normed = norm_instance(K.variable(array))\n-        value = K.eval(normed)\n-        l2 = np.sqrt(np.sum(np.square(value), axis=0))\n-        assert l2[l2 < m].size == 0\n-        assert l2[l2 > m * 2 + 1e-5].size == 0\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,90 +0,0 @@\n-import tempfile\n-\n-import numpy as np\n-import pytest\n-\n-from keras.datasets import boston_housing\n-from keras.datasets import imdb\n-from keras.datasets import reuters\n-\n-\n-@pytest.fixture\n-def fake_downloaded_boston_path(monkeypatch):\n-    num_rows = 100\n-    num_cols = 10\n-    rng = np.random.RandomState(123)\n-\n-    x = rng.randint(1, 100, size=(num_rows, num_cols))\n-    y = rng.normal(loc=100, scale=15, size=num_rows)\n-\n-    with tempfile.NamedTemporaryFile('wb', delete=True) as f:\n-        np.savez(f, x=x, y=y)\n-        monkeypatch.setattr(boston_housing, 'get_file',\n-                            lambda *args, **kwargs: f.name)\n-        yield f.name\n-\n-\n-@pytest.fixture\n-def fake_downloaded_imdb_path(monkeypatch):\n-    train_rows = 100\n-    test_rows = 20\n-    seq_length = 10\n-    rng = np.random.RandomState(123)\n-\n-    x_train = rng.randint(1, 100, size=(train_rows, seq_length))\n-    y_train = rng.binomial(n=1, p=0.5, size=train_rows)\n-    x_test = rng.randint(1, 100, size=(test_rows, seq_length))\n-    y_test = rng.binomial(n=1, p=0.5, size=test_rows)\n-\n-    with tempfile.NamedTemporaryFile('wb', delete=True) as f:\n-        np.savez(f, x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test)\n-        monkeypatch.setattr(imdb, 'get_file', lambda *args, **kwargs: f.name)\n-        yield f.name\n-\n-\n-@pytest.fixture\n-def fake_downloaded_reuters_path(monkeypatch):\n-    num_rows = 100\n-    seq_length = 10\n-    rng = np.random.RandomState(123)\n-\n-    x = rng.randint(1, 100, size=(num_rows, seq_length))\n-    y = rng.binomial(n=1, p=0.5, size=num_rows)\n-\n-    with tempfile.NamedTemporaryFile('wb', delete=True) as f:\n-        np.savez(f, x=x, y=y)\n-        monkeypatch.setattr(reuters, 'get_file', lambda *args, **kwargs: f.name)\n-        yield f.name\n-\n-\n-def DISABLED_test_boston_load_does_not_affect_global_rng(fake_downloaded_boston_path):\n-    np.random.seed(1337)\n-    before = np.random.randint(0, 100, size=10)\n-\n-    np.random.seed(1337)\n-    boston_housing.load_data(path=fake_downloaded_boston_path, seed=9876)\n-    after = np.random.randint(0, 100, size=10)\n-\n-    assert np.array_equal(before, after)\n-\n-\n-def DISABLED_test_imdb_load_does_not_affect_global_rng(fake_downloaded_imdb_path):\n-    np.random.seed(1337)\n-    before = np.random.randint(0, 100, size=10)\n-\n-    np.random.seed(1337)\n-    imdb.load_data(path=fake_downloaded_imdb_path, seed=9876)\n-    after = np.random.randint(0, 100, size=10)\n-\n-    assert np.array_equal(before, after)\n-\n-\n-def DISABLED_test_reuters_load_does_not_affect_global_rng(fake_downloaded_reuters_path):\n-    np.random.seed(1337)\n-    before = np.random.randint(0, 100, size=10)\n-\n-    np.random.seed(1337)\n-    reuters.load_data(path=fake_downloaded_reuters_path, seed=9876)\n-    after = np.random.randint(0, 100, size=10)\n-\n-    assert np.array_equal(before, after)\n\n@@ -1,203 +0,0 @@\n-import pytest\n-import keras\n-import numpy as np\n-from keras import layers\n-from keras import backend as K\n-\n-\n-def test_sublayer_tracking():\n-    # basic case\n-    class MyLayer(layers.Layer):\n-\n-        def __init__(self):\n-            super(MyLayer, self).__init__()\n-            self._input_shape = (2, 4)\n-            self.dense = layers.Dense(3)\n-            self.bidir = layers.Bidirectional(keras.layers.LSTM(2))\n-\n-        def call(self, inputs):\n-            return self.dense(self.bidir(inputs))\n-\n-    layer = MyLayer()\n-    assert len(layer._layers) == 2\n-    layer(K.constant(np.random.random((2,) + layer._input_shape)))\n-    assert len(layer.weights) == 2 + 3 + 3\n-    assert len(layer._layers[0].weights) == 2\n-    assert len(layer._layers[1].weights) == 6\n-\n-    # recursive case\n-    class MyRecursiveLayer(layers.Layer):\n-\n-        def __init__(self):\n-            super(MyRecursiveLayer, self).__init__()\n-            self._input_shape = (2, 4)\n-            self.my_layer = MyLayer()\n-            self.dense = layers.Dense(3)\n-            self.bidir = layers.Bidirectional(\n-                keras.layers.LSTM(2, return_sequences=True))\n-\n-        def call(self, inputs):\n-            return self.my_layer(self.dense(self.bidir(inputs)))\n-\n-    layer = MyRecursiveLayer()\n-    assert len(layer._layers) == 3\n-    layer(K.constant(np.random.random((2,) + layer._input_shape)))\n-    assert len(layer.weights) == 16\n-\n-    # subnetwork case\n-    class MyLayerWithSubnetwork(keras.layers.Layer):\n-\n-        def __init__(self):\n-            super(MyLayerWithSubnetwork, self).__init__()\n-            self._input_shape = (2,)\n-            self.dense = layers.Dense(3)\n-            self.sequential = keras.Sequential(\n-                [layers.Dense(5), layers.Dense(1)], name='seq')\n-            inputs = keras.Input((1,))\n-            outputs = layers.Dense(1)(inputs)\n-            self.functional = keras.Model(inputs, outputs, name='func')\n-\n-        def call(self, inputs):\n-            x = self.dense(inputs)\n-            x = self.sequential(x)\n-            return self.functional(x)\n-\n-    layer = MyLayerWithSubnetwork()\n-    assert len(layer._layers) == 3\n-    layer(K.constant(np.random.random((2,) + layer._input_shape)))\n-    assert len(layer.weights) == 2 + (2 + 2) + 2\n-    assert len(layer._layers[0].weights) == 2\n-    assert len(layer._layers[1].weights) == 4\n-    assert len(layer._layers[2].weights) == 2\n-\n-\n-def test_weight_tracking():\n-\n-    class MyLayer(layers.Layer):\n-\n-        def __init__(self):\n-            super(MyLayer, self).__init__()\n-            self._input_shape = (2,)\n-            self.dense = layers.Dense(3)\n-            self.w1 = K.variable(0, name='w1')\n-\n-        def build(self, input_shape):\n-            self.w2 = K.variable(1, name='w2')\n-            self.w3 = self.add_weight(\n-                'w3', shape=(), trainable=False, initializer='zeros')\n-\n-        def call(self, inputs):\n-            return self.dense(inputs) + self.w1 + self.w2\n-\n-    layer = MyLayer()\n-    layer(K.constant(np.random.random((2,) + layer._input_shape)))\n-    assert len(layer.weights) == 5\n-    assert len(layer.trainable_weights) == 4\n-    assert len(layer.non_trainable_weights) == 1\n-    assert len(layer._trainable_weights) == 2\n-    assert layer._trainable_weights[0] is layer.w1\n-    assert layer._trainable_weights[1] is layer.w2\n-    assert len(layer._non_trainable_weights) == 1\n-    assert layer._non_trainable_weights[0] is layer.w3\n-\n-\n-def test_loss_tracking():\n-    # basic case\n-    class MyLayer(layers.Layer):\n-\n-        def __init__(self):\n-            super(MyLayer, self).__init__()\n-            self.dense = layers.Dense(\n-                3, kernel_regularizer='l2', activity_regularizer='l2')\n-\n-        def call(self, inputs):\n-            return self.dense(inputs)\n-\n-    inputs = keras.Input((2,))\n-    outputs = MyLayer()(inputs)\n-    model = keras.Model(inputs, outputs)\n-\n-    assert len(model.layers) == 2  # includes input layer\n-    assert len(model.weights) == 2\n-    assert len(model.losses) == 2\n-    assert len(model.get_losses_for(None)) == 1\n-    assert len(model.get_losses_for(inputs)) == 1\n-\n-\n-@pytest.mark.skipif(K.backend() != 'tensorflow',\n-                    reason='Requires TF symbols')\n-def test_tf_keras_guide():\n-    import tensorflow as tf\n-\n-    class Linear(layers.Layer):\n-\n-        def __init__(self, units=32, input_dim=32):\n-            super(Linear, self).__init__()\n-            w_init = tf.random_normal_initializer()\n-            self.w = tf.Variable(initial_value=w_init(shape=(input_dim, units),\n-                                                      dtype='float32'),\n-                                 trainable=True)\n-            b_init = tf.zeros_initializer()\n-            self.b = tf.Variable(initial_value=b_init(shape=(units,),\n-                                                      dtype='float32'),\n-                                 trainable=True)\n-\n-        def call(self, inputs):\n-            return tf.matmul(inputs, self.w) + self.b\n-\n-    x = tf.ones((2, 2))\n-    linear_layer = Linear(4, 2)\n-    y = linear_layer(x)\n-\n-    assert len(linear_layer.trainable_weights) == 2\n-\n-    class Linear(layers.Layer):\n-\n-        def __init__(self, units=32):\n-            super(Linear, self).__init__()\n-            self.units = units\n-\n-        def build(self, input_shape):\n-            self.w = self.add_weight(shape=(input_shape[-1], self.units),\n-                                     initializer='random_normal',\n-                                     trainable=True)\n-            self.b = self.add_weight(shape=(self.units,),\n-                                     initializer='random_normal',\n-                                     trainable=True)\n-\n-        def call(self, inputs):\n-            return tf.matmul(inputs, self.w) + self.b\n-\n-    class MLPBlock(layers.Layer):\n-\n-        def __init__(self):\n-            super(MLPBlock, self).__init__()\n-            self.linear_1 = Linear(32)\n-            self.linear_2 = Linear(32)\n-            self.linear_3 = Linear(1)\n-\n-        def call(self, inputs):\n-            x = self.linear_1(inputs)\n-            x = tf.nn.relu(x)\n-            x = self.linear_2(x)\n-            x = tf.nn.relu(x)\n-            return self.linear_3(x)\n-\n-    mlp = MLPBlock()\n-    y = mlp(tf.ones(shape=(3, 64)))\n-    assert len(mlp.weights) == 6\n-    assert len(mlp.trainable_weights) == 6\n-\n-    class OuterLayer(layers.Layer):\n-\n-        def __init__(self):\n-            super(OuterLayer, self).__init__()\n-            self.dense = layers.Dense(\n-                32, kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n-\n-        def call(self, inputs):\n-            return self.dense(inputs)\n-\n-    layer = OuterLayer()\n-    _ = layer(tf.zeros((1, 1)))\n-    assert len(layer.losses) == 1\n\n@@ -1,800 +0,0 @@\n-import pytest\n-import json\n-import numpy as np\n-\n-from keras.layers import Dense, Dropout, Conv2D, InputLayer\n-from keras import layers\n-from keras.engine import Input, Layer, saving, get_source_inputs\n-from keras.models import Model, Sequential\n-from keras import backend as K\n-from keras.models import model_from_json, model_from_yaml\n-from keras.initializers import Constant\n-from tensorflow.python.keras.saving.hdf5_format import preprocess_weights_for_loading\n-\n-\n-\n-def test_get_updates_for():\n-    a = Input(shape=(2,))\n-    dense_layer = Dense(1)\n-    dense_layer.add_update(0, inputs=a)\n-    dense_layer.add_update(1, inputs=None)\n-\n-\n-def test_get_losses_for():\n-    a = Input(shape=(2,))\n-    dense_layer = Dense(1)\n-    dense_layer.add_loss(lambda: 0, inputs=a)\n-    dense_layer.add_loss(lambda: 1, inputs=None)\n-\n-\n-def test_trainable_weights():\n-    a = Input(shape=(2,))\n-    b = Dense(1)(a)\n-    model = Model(a, b)\n-\n-    weights = model.weights\n-    assert model.trainable_weights == weights\n-    assert model.non_trainable_weights == []\n-\n-    model.trainable = False\n-    assert model.trainable_weights == []\n-    assert model.non_trainable_weights == weights\n-\n-    model.trainable = True\n-    assert model.trainable_weights == weights\n-    assert model.non_trainable_weights == []\n-\n-    model.layers[1].trainable = False\n-    assert model.trainable_weights == []\n-    assert model.non_trainable_weights == weights\n-\n-    # sequential model\n-    model = Sequential()\n-    model.add(Dense(1, input_dim=2))\n-    weights = model.weights\n-\n-    assert model.trainable_weights == weights\n-    assert model.non_trainable_weights == []\n-\n-    model.trainable = False\n-    assert model.trainable_weights == []\n-    assert model.non_trainable_weights == weights\n-\n-    model.trainable = True\n-    assert model.trainable_weights == weights\n-    assert model.non_trainable_weights == []\n-\n-    model.layers[0].trainable = False\n-    assert model.trainable_weights == []\n-    assert model.non_trainable_weights == weights\n-\n-\n-def test_valid_compute_mask():\n-    model = Sequential()\n-    model.add(Dense(1, input_dim=2))\n-    assert model.layers[0].supports_masking is True\n-    assert model.layers[0].compute_mask([model.input], [0., 1.]) == [0., 1.]\n-\n-\n-def test_invalid_compute_mask():\n-    model = Sequential()\n-    model.add(Conv2D(1, [2, 2], input_shape=[3, 3, 1]))\n-    assert model.layers[0].supports_masking is False\n-    assert model.layers[0].compute_mask([model.input], [None]) is None\n-\n-    mask = np.array([[0., 1.], [1., 0.]])\n-    with pytest.raises(TypeError):\n-        model.layers[0].compute_mask([model.input], [mask])\n-    with pytest.raises(TypeError):\n-        model.layers[0].compute_mask([model.input], mask)\n-\n-\n-def test_get_layer():\n-    model = Sequential()\n-    model.add(Dense(1, input_dim=2))\n-    with pytest.raises(ValueError):\n-        model.get_layer(index=5)\n-    with pytest.raises(ValueError):\n-        model.get_layer(index=None)\n-    with pytest.raises(ValueError):\n-        model.get_layer(name='conv')\n-\n-\n-def DISABLED_test_learning_phase():\n-    a = Input(shape=(32,), name='input_a')\n-    b = Input(shape=(32,), name='input_b')\n-\n-    a_2 = Dense(16, name='dense_1')(a)\n-    dp = Dropout(0.5, name='dropout')\n-    b_2 = dp(b)\n-\n-    # test merge\n-    m = layers.concatenate([a_2, b_2])\n-\n-    # Test recursion\n-    model = Model([a, b], [a_2, b_2])\n-    print(model.input_spec)\n-\n-    c = Input(shape=(32,), name='input_c')\n-    d = Input(shape=(32,), name='input_d')\n-\n-    c_2, b_2 = model([c, d])\n-\n-    # try actually running graph\n-    fn = K.function(model.inputs + [K.learning_phase()], model.outputs)\n-    input_a_np = np.random.random((10, 32))\n-    input_b_np = np.random.random((10, 32))\n-    fn_outputs_no_dp = fn([input_a_np, input_b_np, 0])\n-    fn_outputs_dp = fn([input_a_np, input_b_np, 1])\n-    # output a: nothing changes\n-    assert fn_outputs_no_dp[0].sum() == fn_outputs_dp[0].sum()\n-    # output b: dropout applied\n-    assert fn_outputs_no_dp[1].sum() != fn_outputs_dp[1].sum()\n-\n-\n-def test_layer_call_arguments():\n-    # Test the ability to pass and serialize arguments to `call`.\n-    inp = layers.Input(shape=(2,))\n-    x = layers.Dense(3)(inp)\n-    x = layers.Dropout(0.5)(x, training=True)\n-    model = Model(inp, x)\n-\n-    # Test that argument is kept when applying the model\n-    inp2 = layers.Input(shape=(2,))\n-    out2 = model(inp2)\n-\n-    # Test that argument is kept after loading a model\n-    config = model.get_config()\n-    model = Model.from_config(config)\n-\n-\n-def test_node_construction():\n-    ####################################################\n-    # test basics\n-\n-    a = Input(shape=(32,), name='input_a')\n-    b = Input(shape=(32,), name='input_b')\n-\n-    assert tuple(a.shape) == (None, 32)\n-    a_layer, a_node_index, a_tensor_index = a._keras_history\n-    b_layer, b_node_index, b_tensor_index = b._keras_history\n-    assert len(a_layer._inbound_nodes) == 1\n-    assert a_tensor_index is 0\n-    node = a_layer._inbound_nodes[a_node_index]\n-    assert node.outbound_layer == a_layer\n-\n-    assert isinstance(node.inbound_layers, list)\n-    assert node.inbound_layers == []\n-    assert isinstance(node.input_tensors, list)\n-    assert node.input_tensors == [a]\n-    assert isinstance(node.input_shapes, list)\n-    assert node.input_shapes == [(None, 32)]\n-\n-    assert isinstance(node.output_tensors, list)\n-    assert node.output_tensors == [a]\n-    assert isinstance(node.output_shapes, list)\n-    assert node.output_shapes == [(None, 32)]\n-\n-    dense = Dense(16, name='dense_1')\n-    a_2 = dense(a)\n-    b_2 = dense(b)\n-\n-    assert len(dense._inbound_nodes) == 2\n-    assert len(dense._outbound_nodes) == 0\n-    assert dense._inbound_nodes[0].inbound_layers == a_layer\n-    assert dense._inbound_nodes[0].outbound_layer == dense\n-    assert dense._inbound_nodes[1].inbound_layers == b_layer\n-    assert dense._inbound_nodes[1].outbound_layer == dense\n-\n-    # test layer properties\n-    test_layer = Dense(16, name='test_layer')\n-    a_test = test_layer(a)\n-    assert K.int_shape(test_layer.kernel) == (32, 16)\n-    assert test_layer.input is a\n-    assert test_layer.output is a_test\n-    assert test_layer.input_mask is None\n-    assert test_layer.output_mask is None\n-    assert test_layer.input_shape == (None, 32)\n-    assert test_layer.output_shape == (None, 16)\n-\n-    # with pytest.raises(AttributeError):\n-    #     dense.input\n-    # with pytest.raises(AttributeError):\n-    #     dense.output\n-    # with pytest.raises(AttributeError):\n-    #     dense.input_mask\n-    # with pytest.raises(AttributeError):\n-    #     dense.output_mask\n-\n-    assert dense.get_input_at(0) is a\n-    assert dense.get_input_at(1)is b\n-    assert dense.get_output_at(0) is a_2\n-    assert dense.get_output_at(1) is b_2\n-    assert dense.get_input_shape_at(0) == (None, 32)\n-    assert dense.get_input_shape_at(1) == (None, 32)\n-    assert dense.get_output_shape_at(0) == (None, 16)\n-    assert dense.get_output_shape_at(1) == (None, 16)\n-    assert dense.get_input_mask_at(0) is None\n-    assert dense.get_input_mask_at(1) is None\n-    assert dense.get_output_mask_at(0) is None\n-    assert dense.get_output_mask_at(1) is None\n-\n-\n-def test_multi_input_layer():\n-    ####################################################\n-    # test multi-input layer\n-    a = Input(shape=(32,), name='input_a')\n-    b = Input(shape=(32,), name='input_b')\n-\n-    dense = Dense(16, name='dense_1')\n-    a_2 = dense(a)\n-    b_2 = dense(b)\n-\n-    merged = layers.concatenate([a_2, b_2], name='merge')\n-    assert tuple(merged.shape) == (None, 16 * 2)\n-    merge_layer, merge_node_index, merge_tensor_index = merged._keras_history\n-\n-    assert merge_node_index == 0\n-    assert merge_tensor_index == 0\n-\n-    assert len(merge_layer._inbound_nodes) == 1\n-    assert len(merge_layer._outbound_nodes) == 0\n-\n-    assert len(merge_layer._inbound_nodes[0].input_tensors) == 2\n-    assert len(merge_layer._inbound_nodes[0].inbound_layers) == 2\n-\n-    c = Dense(64, name='dense_2')(merged)\n-    d = Dense(5, name='dense_3')(c)\n-\n-    model = Model(inputs=[a, b], outputs=[c, d], name='model')\n-    assert len(model.layers) == 6\n-    expected_shapes = [(None, 64), (None, 5)]\n-    assert [tuple(s) for s in model.compute_output_shape([(None, 32), (None, 32)])] == expected_shapes\n-    assert model.compute_mask([a, b], [None, None]) == [None, None]\n-    assert [tuple(s) for s in model.compute_output_shape([(None, 32), (None, 32)])] == expected_shapes\n-\n-    # we don't check names of first 2 layers (inputs) because\n-    # ordering of same-level layers is not fixed\n-    expected_names = ['dense_1', 'merge', 'dense_2', 'dense_3']\n-    assert [l.name for l in model.layers][2:] == expected_names\n-    assert [l.name for l in model._input_layers] == ['input_a', 'input_b']\n-    assert [l.name for l in model._output_layers] == ['dense_2', 'dense_3']\n-\n-    # actually run model\n-    fn = K.function(model.inputs, model.outputs)\n-    input_a_np = np.random.random((10, 32))\n-    input_b_np = np.random.random((10, 32))\n-    fn_outputs = fn([input_a_np, input_b_np])\n-    assert [x.shape for x in fn_outputs] == [(10, 64), (10, 5)]\n-\n-    # test get_source_inputs\n-    source_inputs = get_source_inputs(c)\n-    assert source_inputs[0] is a\n-    assert source_inputs[1] is b\n-\n-    # serialization / deserialization\n-    json_config = model.to_json()\n-    recreated_model = model_from_json(json_config)\n-    recreated_model.compile('rmsprop', 'mse')\n-\n-    assert [l.name for l in recreated_model.layers][2:] == expected_names\n-    assert [l.name for l in recreated_model._input_layers] == ['input_a', 'input_b']\n-    assert [l.name for l in recreated_model._output_layers] == ['dense_2', 'dense_3']\n-\n-    fn = K.function(recreated_model.inputs, recreated_model.outputs)\n-    input_a_np = np.random.random((10, 32))\n-    input_b_np = np.random.random((10, 32))\n-    fn_outputs = fn([input_a_np, input_b_np])\n-    assert [x.shape for x in fn_outputs] == [(10, 64), (10, 5)]\n-\n-\n-def test_recursion():\n-    ####################################################\n-    # test recursion\n-\n-    a = Input(shape=(32,), name='input_a')\n-    b = Input(shape=(32,), name='input_b')\n-\n-    dense = Dense(16, name='dense_1')\n-    a_2 = dense(a)\n-    b_2 = dense(b)\n-    merged = layers.concatenate([a_2, b_2], name='merge')\n-    c = Dense(64, name='dense_2')(merged)\n-    d = Dense(5, name='dense_3')(c)\n-\n-    model = Model(inputs=[a, b], outputs=[c, d], name='model')\n-\n-    e = Input(shape=(32,), name='input_e')\n-    f = Input(shape=(32,), name='input_f')\n-    g, h = model([e, f])\n-\n-    # g2, h2 = model([e, f])\n-\n-    assert tuple(g.shape) == tuple(c.shape)\n-    assert tuple(h.shape) == tuple(d.shape)\n-\n-    # test separate manipulation of different layer outputs\n-    i = Dense(7, name='dense_4')(h)\n-\n-    final_model = Model(inputs=[e, f], outputs=[i, g], name='final')\n-    assert len(final_model.inputs) == 2\n-    assert len(final_model.outputs) == 2\n-    assert len(final_model.layers) == 4\n-\n-    # we don't check names of first 2 layers (inputs) because\n-    # ordering of same-level layers is not fixed\n-    expected_shapes = [(10, 7), (10, 64)]\n-    assert [layer.name for layer in final_model.layers][2:] == ['model', 'dense_4']\n-    assert model.compute_mask([e, f], [None, None]) == [None, None]\n-    assert final_model.compute_output_shape([(10, 32), (10, 32)]) == expected_shapes\n-\n-    # run recursive model\n-    fn = K.function(final_model.inputs, final_model.outputs)\n-    input_a_np = np.random.random((10, 32))\n-    input_b_np = np.random.random((10, 32))\n-    fn_outputs = fn([input_a_np, input_b_np])\n-    assert [x.shape for x in fn_outputs] == [(10, 7), (10, 64)]\n-\n-    # test serialization\n-    model_config = final_model.get_config()\n-    print(json.dumps(model_config, indent=4))\n-    recreated_model = Model.from_config(model_config)\n-\n-    fn = K.function(recreated_model.inputs, recreated_model.outputs)\n-    input_a_np = np.random.random((10, 32))\n-    input_b_np = np.random.random((10, 32))\n-    fn_outputs = fn([input_a_np, input_b_np])\n-    assert [x.shape for x in fn_outputs] == [(10, 7), (10, 64)]\n-\n-    ####################################################\n-    # test multi-input multi-output\n-\n-    j = Input(shape=(32,), name='input_j')\n-    k = Input(shape=(32,), name='input_k')\n-    m, n = model([j, k])\n-\n-    o = Input(shape=(32,), name='input_o')\n-    p = Input(shape=(32,), name='input_p')\n-    q, r = model([o, p])\n-\n-    assert tuple(n.shape) == (None, 5)\n-    assert tuple(q.shape) == (None, 64)\n-    s = layers.concatenate([n, q], name='merge_nq')\n-    assert tuple(s.shape) == (None, 64 + 5)\n-\n-    # test with single output as 1-elem list\n-    multi_io_model = Model([j, k, o, p], [s])\n-\n-    fn = K.function(multi_io_model.inputs, multi_io_model.outputs)\n-    fn_outputs = fn([np.random.random((10, 32)), np.random.random((10, 32)),\n-                     np.random.random((10, 32)), np.random.random((10, 32))])\n-    assert [x.shape for x in fn_outputs] == [(10, 69)]\n-\n-    # test with single output as tensor\n-    multi_io_model = Model([j, k, o, p], s)\n-\n-    fn = K.function(multi_io_model.inputs, multi_io_model.outputs)\n-    fn_outputs = fn([np.random.random((10, 32)), np.random.random((10, 32)),\n-                     np.random.random((10, 32)), np.random.random((10, 32))])\n-    # note that the output of the K.function will still be a 1-elem list\n-    assert [x.shape for x in fn_outputs] == [(10, 69)]\n-\n-    # test serialization\n-    model_config = multi_io_model.get_config()\n-    recreated_model = Model.from_config(model_config)\n-\n-    fn = K.function(recreated_model.inputs, recreated_model.outputs)\n-    fn_outputs = fn([np.random.random((10, 32)), np.random.random((10, 32)),\n-                     np.random.random((10, 32)), np.random.random((10, 32))])\n-    # note that the output of the K.function will still be a 1-elem list\n-    assert [x.shape for x in fn_outputs] == [(10, 69)]\n-\n-    config = model.get_config()\n-    Model.from_config(config)\n-\n-    model.summary()\n-    json_str = model.to_json()\n-    model_from_json(json_str)\n-\n-    yaml_str = model.to_yaml()\n-    model_from_yaml(yaml_str)\n-\n-    ####################################################\n-    # test invalid graphs\n-\n-    # input is not an Input tensor\n-    j = Input(shape=(32,), name='input_j')\n-    j = Dense(32)(j)\n-    k = Input(shape=(32,), name='input_k')\n-    m, n = model([j, k])\n-\n-    with pytest.raises(ValueError):\n-        Model([j, k], [m, n])\n-\n-    # disconnected graph\n-    j = Input(shape=(32,), name='input_j')\n-    k = Input(shape=(32,), name='input_k')\n-    m, n = model([j, k])\n-    with pytest.raises(ValueError):\n-        Model([j], [m, n])\n-\n-    # redundant outputs\n-    j = Input(shape=(32,), name='input_j')\n-    k = Input(shape=(32,), name='input_k')\n-    m, n = model([j, k])\n-    # this should work with a warning\n-    Model([j, k], [m, n, n])\n-\n-    # redundant inputs\n-    j = Input(shape=(32,), name='input_j')\n-    k = Input(shape=(32,), name='input_k')\n-    m, n = model([j, k])\n-    with pytest.raises(ValueError):\n-        Model([j, k, j], [m, n])\n-\n-    ####################################################\n-    # test calling layers/models on placeholders\n-    j = Input(shape=(32,), name='input_j')\n-    k = Input(shape=(32,), name='input_k')\n-    m, n = model([j, k])\n-    outer_model = Model([j, k], [m, n])\n-\n-    j_tf = K.placeholder(shape=(None, 32), dtype=K.floatx())\n-    k_tf = K.placeholder(shape=(None, 32), dtype=K.floatx())\n-    m_tf, n_tf = outer_model([j_tf, k_tf])\n-    assert K.int_shape(m_tf) == (None, 64)\n-    assert K.int_shape(n_tf) == (None, 5)\n-\n-    # test merge\n-    layers.concatenate([j_tf, k_tf], axis=1)\n-    layers.add([j_tf, k_tf])\n-\n-    # test tensor input\n-    x = K.placeholder(shape=(None, 2), dtype=K.floatx())\n-    InputLayer(input_tensor=x)\n-\n-    x = Input(tensor=x)\n-    Dense(2)(x)\n-\n-\n-def test_load_layers():\n-    from keras.layers import ConvLSTM2D, TimeDistributed\n-    from keras.layers import Bidirectional, Conv2D, Input\n-    from keras.models import Model\n-\n-    if K.backend() == 'tensorflow' or K.backend() == 'cntk':\n-        inputs = Input(shape=(10, 20, 20, 1))\n-    else:\n-        inputs = Input(shape=(10, 1, 20, 20))\n-    td_conv = TimeDistributed(Conv2D(15, (5, 5)))(inputs)\n-    bi_conv = Bidirectional(ConvLSTM2D(10, (3, 3)), merge_mode='concat')(td_conv)\n-    model = Model(inputs=inputs, outputs=bi_conv)\n-\n-    weight_value_tuples = []\n-\n-    # TimeDistributed Conv2D layer\n-    # use 'channels_first' data format to check that\n-    # the function is being called correctly for Conv2D\n-    # old: (filters, stack_size, kernel_rows, kernel_cols)\n-    # new: (kernel_rows, kernel_cols, stack_size, filters)\n-    weight_tensor_td_conv_old = list()\n-    weight_tensor_td_conv_old.append(np.zeros((15, 1, 5, 5)))\n-    weight_tensor_td_conv_old.append(np.zeros((15,)))\n-    td_conv_layer = model.layers[1]\n-    td_conv_layer.layer.data_format = 'channels_first'\n-    weight_tensor_td_conv_new = preprocess_weights_for_loading(\n-        td_conv_layer,\n-        weight_tensor_td_conv_old,\n-        original_keras_version='1')\n-    symbolic_weights = td_conv_layer.weights\n-    assert (len(symbolic_weights) == len(weight_tensor_td_conv_new))\n-    weight_value_tuples += zip(symbolic_weights, weight_tensor_td_conv_new)\n-\n-    # Bidirectional ConvLSTM2D layer\n-    # old ConvLSTM2D took a list of 12 weight tensors,\n-    # returns a list of 3 concatenated larger tensors.\n-    weights_bi_conv_old = []\n-    for j in range(2):  # bidirectional\n-        for i in range(4):\n-            weights_bi_conv_old.append(np.zeros((3, 3, 15, 10)))  # kernel\n-            weights_bi_conv_old.append(np.zeros((3, 3, 10, 10)))  # recurrent kernel\n-            weights_bi_conv_old.append(np.zeros((10,)))  # bias\n-\n-    bi_convlstm_layer = model.layers[2]\n-    weights_bi_conv_new = preprocess_weights_for_loading(\n-        bi_convlstm_layer,\n-        weights_bi_conv_old,\n-        original_keras_version='1')\n-\n-    symbolic_weights = bi_convlstm_layer.weights\n-    assert (len(symbolic_weights) == len(weights_bi_conv_new))\n-    weight_value_tuples += zip(symbolic_weights, weights_bi_conv_new)\n-\n-    K.batch_set_value(weight_value_tuples)\n-\n-    assert np.all(K.eval(model.layers[1].weights[0]) == weight_tensor_td_conv_new[0])\n-    assert np.all(K.eval(model.layers[1].weights[1]) == weight_tensor_td_conv_new[1])\n-    assert np.all(K.eval(model.layers[2].weights[0]) == weights_bi_conv_new[0])\n-    assert np.all(K.eval(model.layers[2].weights[1]) == weights_bi_conv_new[1])\n-    assert np.all(K.eval(model.layers[2].weights[2]) == weights_bi_conv_new[2])\n-    assert np.all(K.eval(model.layers[2].weights[3]) == weights_bi_conv_new[3])\n-    assert np.all(K.eval(model.layers[2].weights[4]) == weights_bi_conv_new[4])\n-    assert np.all(K.eval(model.layers[2].weights[5]) == weights_bi_conv_new[5])\n-\n-\n-def convert_weights(layer, weights):\n-    if layer.__class__.__name__ == 'GRU':\n-        W = [np.split(w, 3, axis=-1) for w in weights]\n-        return sum(map(list, zip(*W)), [])\n-    elif layer.__class__.__name__ in ('LSTM', 'ConvLSTM2D'):\n-        W = [np.split(w, 4, axis=-1) for w in weights]\n-        for w in W:\n-            w[2], w[1] = w[1], w[2]\n-        return sum(map(list, zip(*W)), [])\n-    elif layer.__class__.__name__ == 'Conv2DTranspose':\n-        return [np.transpose(weights[0], (2, 3, 0, 1)), weights[1]]\n-    return weights\n-\n-\n-@pytest.mark.parametrize(\"layer\", [\n-    layers.GRU(2, input_shape=[3, 5]),\n-    layers.LSTM(2, input_shape=[3, 5]),\n-    layers.ConvLSTM2D(5, (3, 3),\n-                      input_shape=[6, 6, 6, 6],\n-                      data_format='channels_first'),\n-], ids=['GRU', 'LSTM', 'ConvLSTM2D'])\n-def test_preprocess_weights_for_loading(layer):\n-    # A model is needed to initialize weights.\n-    _ = Sequential([layer])\n-    weights1 = layer.get_weights()\n-    weights2 = preprocess_weights_for_loading(\n-        layer, convert_weights(layer, weights1),\n-        original_keras_version='1')\n-    assert all([np.allclose(x, y, 1e-5)\n-                for (x, y) in zip(weights1, weights2)])\n-\n-\n-@pytest.mark.parametrize(\"layer\", [\n-    layers.Conv2D(2, (3, 3), input_shape=[5, 5, 3]),\n-    layers.Conv2DTranspose(2, (5, 5),\n-                           input_shape=[7, 7, 3],\n-                           data_format='channels_first'),\n-], ids=['Conv2D', 'Conv2DTranspose'])\n-def test_preprocess_weights_for_loading_for_model(layer):\n-    model = Sequential([layer])\n-    weights1 = model.get_weights()\n-    weights2 = preprocess_weights_for_loading(\n-        model, convert_weights(layer, weights1),\n-        original_keras_version='1')\n-    assert all([np.allclose(x, y, 1e-5)\n-                for (x, y) in zip(weights1, weights2)])\n-\n-\n-@pytest.mark.parametrize('layer_class,args', [\n-    (layers.GRU, {'units': 2, 'input_shape': [3, 5]}),\n-    (layers.GRU, {'units': 2, 'input_shape': [3, 5], 'reset_after': True}),\n-    (layers.LSTM, {'units': 2, 'input_shape': [3, 5]}),\n-])\n-def test_preprocess_weights_for_loading_rnn_should_be_idempotent(layer_class, args):\n-    \"\"\"\n-    Loading weights from a RNN class to itself should not convert the weights.\n-    \"\"\"\n-    # layer can be instantiated only for supported backends\n-    layer = layer_class(**args)\n-    # A model is needed to initialize weights.\n-    _ = Sequential([layer])\n-    weights1 = layer.get_weights()\n-    weights2 = preprocess_weights_for_loading(layer, weights1)\n-    assert all([np.allclose(x, y, 1e-5) for (x, y) in zip(weights1, weights2)])\n-\n-\n-def test_recursion_with_bn_and_loss():\n-    model1 = Sequential([\n-        layers.Dense(5, input_dim=5, activity_regularizer='l1'),\n-        layers.BatchNormalization(),\n-        layers.Dense(5),\n-    ])\n-\n-    inputs = layers.Input(shape=(5,))\n-    outputs = model1(inputs)\n-    model2 = Model(inputs=inputs, outputs=outputs)\n-\n-    model1.compile(optimizer='sgd', loss='categorical_crossentropy')\n-    model2.compile(optimizer='sgd', loss='categorical_crossentropy')\n-\n-    x = np.ones((3, 5))\n-    y = np.ones((3, 5))\n-    model1.fit(x, y, verbose=0, epochs=1)\n-    model2.fit(x, y, verbose=0, epochs=1)\n-\n-\n-def test_activity_regularization_with_model_composition():\n-\n-    def reg(x):\n-        return K.sum(x)\n-\n-    net_a_input = Input((2,))\n-    net_a = net_a_input\n-    net_a = Dense(2, kernel_initializer='ones',\n-                  use_bias=False,\n-                  activity_regularizer=reg)(net_a)\n-    model_a = Model([net_a_input], [net_a])\n-\n-    net_b_input = Input((2,))\n-    net_b = model_a(net_b_input)\n-    model_b = Model([net_b_input], [net_b])\n-\n-    model_b.compile(optimizer='sgd', loss=None)\n-    x = np.ones((1, 2))\n-    loss = model_b.evaluate(x)\n-    assert loss == 4\n-\n-\n-def test_shared_layer_depth_is_correct():\n-    # Basic outline here: we have a shared embedding layer, and two inputs that\n-    # go through different depths of computation in the graph before\n-    # the final output.  We need the computed depth of the input layers to be\n-    # the same, because they both pass through the embedding layer before anything\n-    # else happens.  That's what we're testing.\n-    from keras.layers import Embedding, Input, Dense, Concatenate\n-    from keras.models import Model\n-    input1 = Input(shape=(10,), name='input1')\n-    input2 = Input(shape=(10,), name='input2')\n-    embedding_layer = Embedding(name='embedding', input_dim=5, output_dim=10)\n-    embedded_input1 = embedding_layer(input1)\n-    embedded_input2 = embedding_layer(input2)\n-    transformed_input2 = Dense(6)(Dense(5)(Dense(3)(embedded_input2)))\n-    final_output = Dense(2)(Concatenate()([embedded_input1, transformed_input2]))\n-    model = Model(inputs=[input1, input2], outputs=final_output)\n-\n-\n-def test_layer_sharing_at_heterogeneous_depth():\n-    x_val = np.random.random((10, 5))\n-\n-    x = Input(shape=(5,))\n-    A = Dense(5, name='A')\n-    B = Dense(5, name='B')\n-    output = A(B(A(B(x))))\n-    M = Model(x, output)\n-\n-    output_val = M.predict(x_val)\n-\n-    config = M.get_config()\n-    weights = M.get_weights()\n-\n-    M2 = Model.from_config(config)\n-    M2.set_weights(weights)\n-\n-    output_val_2 = M2.predict(x_val)\n-    np.testing.assert_allclose(output_val, output_val_2, atol=1e-6)\n-\n-\n-def test_layer_sharing_at_heterogeneous_depth_with_concat():\n-    input_shape = (16, 9, 3)\n-    input_layer = Input(shape=input_shape)\n-\n-    A = Dense(3, name='dense_A')\n-    B = Dense(3, name='dense_B')\n-    C = Dense(3, name='dense_C')\n-\n-    x1 = B(A(input_layer))\n-    x2 = A(C(input_layer))\n-    output = layers.concatenate([x1, x2])\n-\n-    M = Model(inputs=input_layer, outputs=output)\n-\n-    x_val = np.random.random((10, 16, 9, 3))\n-    output_val = M.predict(x_val)\n-\n-    config = M.get_config()\n-    weights = M.get_weights()\n-\n-    M2 = Model.from_config(config)\n-    M2.set_weights(weights)\n-\n-    output_val_2 = M2.predict(x_val)\n-    np.testing.assert_allclose(output_val, output_val_2, atol=1e-6)\n-\n-\n-def DISABLED_test_layer_sharing_at_heterogeneous_depth_order():\n-    # This tests for the bug in this issue\n-    # https://github.com/keras-team/keras/issues/11159\n-    # It occurs with layer sharing at heterogeneous depth when\n-    # the layers need to be applied in an order that differs from\n-    # the order that occurs in the config.\n-\n-    input_shape = (1, 12)\n-    input_layer = Input(shape=input_shape)\n-\n-    A = Dense(12, name='layer_a')\n-    r1 = layers.Reshape((12,))(input_layer)\n-    Aout1 = A(r1)\n-\n-    r2 = layers.Reshape((12,))(A(input_layer))\n-    Aout2 = A(r2)\n-\n-    # Note: if the order of the layers in the concat is\n-    # changed to ([Aout1, Aout2]) the bug doesn't trigger\n-    c1 = layers.concatenate([Aout2, Aout1])\n-    output = Dense(2, name='layer_b')(c1)\n-\n-    M = Model(inputs=input_layer, outputs=output)\n-\n-    x_val = np.random.random((10,) + input_shape)\n-    output_val = M.predict(x_val)\n-\n-    config = M.get_config()\n-    weights = M.get_weights()\n-\n-    M2 = Model.from_config(config)\n-    M2.set_weights(weights)\n-\n-    output_val_2 = M2.predict(x_val)\n-    np.testing.assert_allclose(output_val, output_val_2, atol=1e-6)\n-\n-\n-def test_multi_output_mask():\n-    \"\"\"Fixes #7589\"\"\"\n-    class TestMultiOutputLayer(Layer):\n-        def __init__(self, **kwargs):\n-            super(TestMultiOutputLayer, self).__init__(**kwargs)\n-\n-        def call(self, inputs, **kwargs):\n-            return [K.abs(inputs), K.abs(inputs)]\n-\n-        def compute_output_shape(self, input_shape):\n-            out_shape = super(TestMultiOutputLayer, self).compute_output_shape(\n-                input_shape)\n-            return [out_shape, out_shape]\n-\n-    class TestMultiInputLayer(Layer):\n-        def __init__(self, **kwargs):\n-            super(TestMultiInputLayer, self).__init__(**kwargs)\n-\n-        def call(self, inputs, **kwargs):\n-            negative, positive = inputs\n-            return negative + positive\n-\n-    input_layer = Input(shape=(16, 16, 3))\n-    x, y = TestMultiOutputLayer()(input_layer)\n-    z = TestMultiInputLayer()([x, y])\n-    _ = Model(inputs=input_layer, outputs=z)\n-    assert K.int_shape(z)[1:] == (16, 16, 3)\n-\n-\n-def test_constant_initializer_with_numpy():\n-    model = Sequential()\n-    model.add(Dense(2, input_shape=(3,),\n-                    kernel_initializer=Constant(1.)))\n-    model.add(Dense(3))\n-    model.compile(loss='mse', optimizer='sgd', metrics=['acc'])\n-\n-    json_str = model.to_json()\n-    model_from_json(json_str).summary()\n-\n-    yaml_str = model.to_yaml()\n-    model_from_yaml(yaml_str).summary()\n-\n-\n-@pytest.mark.skipif(K.backend() == 'cntk',\n-                    reason='Float64 not supported with CNTK.')\n-def test_initialization_dtype():\n-    class TestLayer(Layer):\n-        def __init__(self):\n-            super(TestLayer, self).__init__(dtype='int64')\n-            self.w = self.add_weight('w', [], initializer=Constant(1))\n-\n-    layer = TestLayer()\n-    assert K.dtype(layer.w) == 'int64'\n-\n-    class TestModel(Model):\n-        def __init__(self):\n-            super(TestModel, self).__init__(dtype='int64')\n-            self.w = self.add_weight('w', [], initializer=Constant(1))\n-\n-    model = TestModel()\n-    assert K.dtype(model.w) == 'int64'\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,2141 +0,0 @@\n-import threading\n-\n-import pytest\n-import numpy as np\n-import pandas as pd\n-from numpy.testing import assert_allclose\n-import sys\n-import scipy.sparse as sparse\n-from flaky import flaky\n-\n-import keras\n-from keras import losses\n-from keras import metrics\n-from keras.layers import Layer, Activation, Dense, Dropout, Conv2D, Concatenate\n-from keras.engine import Input\n-from keras.engine.training import Model\n-from keras.utils.generic_utils import slice_arrays\n-from keras.models import Sequential\n-from keras import backend as K\n-from keras.utils import Sequence\n-from keras.callbacks import Callback\n-\n-if K.backend() == 'tensorflow':\n-    import tensorflow as tf\n-\n-\n-class RandomSequence(Sequence):\n-    def __init__(self, batch_size, sequence_length=12):\n-        self.batch_size = batch_size\n-        self.sequence_length = sequence_length\n-        self.logs = []  # It will work for use_multiprocessing=False\n-\n-    def __len__(self):\n-        return self.sequence_length\n-\n-    def __getitem__(self, idx):\n-        self.logs.append(idx)\n-        return ([np.random.random((self.batch_size, 3)),\n-                 np.random.random((self.batch_size, 3))],\n-                [np.random.random((self.batch_size, 4)),\n-                 np.random.random((self.batch_size, 3))])\n-\n-    def on_epoch_end(self):\n-        pass\n-\n-\n-class IncreaseBatchSizeRandomSequence(Sequence):\n-    def __init__(self, initial_batch_size, initial_sequence_length=12,\n-                 batch_size_func=lambda x: x + 2):\n-        self.batch_size = initial_batch_size\n-        self.initial_sequence_length = initial_sequence_length\n-        self.batch_size_func = batch_size_func\n-        self.logs = []\n-\n-    def __len__(self):\n-        return int(np.ceil(self.initial_sequence_length / float(self.batch_size)))\n-\n-    def __getitem__(self, idx):\n-        self.logs.append(idx)\n-        return ([np.random.random((self.batch_size, 3)),\n-                 np.random.random((self.batch_size, 3))],\n-                [np.random.random((self.batch_size, 4)),\n-                 np.random.random((self.batch_size, 3))])\n-\n-    def on_epoch_end(self):\n-        self.batch_size = self.batch_size_func(self.batch_size)\n-\n-\n-class threadsafe_iter:\n-    \"\"\"Takes an iterator/generator and makes it thread-safe by\n-    serializing call to the `next` method of given iterator/generator.\n-    \"\"\"\n-\n-    def __init__(self, it):\n-        self.it = it\n-        self.lock = threading.Lock()\n-\n-    def __iter__(self):\n-        return self\n-\n-    def __next__(self):\n-        return self.next()\n-\n-    def next(self):\n-        with self.lock:\n-            return next(self.it)\n-\n-\n-def threadsafe_generator(f):\n-    \"\"\"A decorator that takes a generator function and makes it thread-safe.\n-    \"\"\"\n-\n-    def g(*a, **kw):\n-        return threadsafe_iter(f(*a, **kw))\n-\n-    return g\n-\n-\n-def DISABLED_test_check_array_length_consistency():\n-    training_utils.check_array_length_consistency(None, None, None)\n-    a_np = np.random.random((4, 3, 3))\n-    training_utils.check_array_length_consistency(a_np, a_np, a_np)\n-    training_utils.check_array_length_consistency(\n-        [a_np, a_np], [a_np, a_np], [a_np, a_np])\n-    training_utils.check_array_length_consistency([None], [None], [None])\n-\n-    b_np = np.random.random((3, 4))\n-    with pytest.raises(ValueError):\n-        training_utils.check_array_length_consistency(a_np, None, None)\n-    with pytest.raises(ValueError):\n-        training_utils.check_array_length_consistency(a_np, a_np, None)\n-    with pytest.raises(ValueError):\n-        training_utils.check_array_length_consistency([a_np], [None], None)\n-    with pytest.raises(ValueError):\n-        training_utils.check_array_length_consistency([a_np], [b_np], None)\n-    with pytest.raises(ValueError):\n-        training_utils.check_array_length_consistency([a_np], None, [b_np])\n-\n-\n-def testslice_arrays():\n-    input_a = np.random.random((10, 3))\n-    slice_arrays(None)\n-    slice_arrays(input_a, 0)\n-    slice_arrays(input_a, 0, 1)\n-    slice_arrays(input_a, stop=2)\n-    input_a = [None, [1, 1], None, [1, 1]]\n-    slice_arrays(input_a, 0)\n-    slice_arrays(input_a, 0, 1)\n-    slice_arrays(input_a, stop=2)\n-    input_a = [None]\n-    slice_arrays(input_a, 0)\n-    slice_arrays(input_a, 0, 1)\n-    slice_arrays(input_a, stop=2)\n-    input_a = None\n-    slice_arrays(input_a, 0)\n-    slice_arrays(input_a, 0, 1)\n-    slice_arrays(input_a, stop=2)\n-\n-\n-def DISABLED_test_weighted_masked_objective():\n-    a = Input(shape=(3,), name='input_a')\n-\n-    # weighted_masked_objective\n-    def mask_dummy(y_true=None, y_pred=None, weight=None):\n-        return K.placeholder(y_true.shape)\n-\n-    weighted_function = training_utils.weighted_masked_objective(\n-        losses.categorical_crossentropy)\n-    weighted_function(a, a, None)\n-\n-\n-def get_model(num_outputs=1):\n-    a = Input(shape=(3,), name='input_a')\n-    b = Input(shape=(3,), name='input_b')\n-\n-    a_2 = Dense(4, name='dense_1')(a)\n-    dp = Dropout(0.5, name='dropout')\n-    b_2 = dp(b)\n-\n-    if num_outputs == 1:\n-        model = Model([a, b], a_2)\n-    else:\n-        model = Model([a, b], [a_2, b_2])\n-    return model\n-\n-\n-class TrackerCallback(Callback):\n-\n-    def __init__(self):\n-        # test starting from non-zero initial epoch\n-        self.trained_epochs = []\n-        self.trained_batches = []\n-        self.steps_per_epoch_log = []\n-        super(TrackerCallback, self).__init__()\n-\n-    def set_params(self, params):\n-        super(TrackerCallback, self).set_params(params)\n-        self.steps_per_epoch_log.append(params['steps'])\n-\n-    # define tracer callback\n-    def on_epoch_begin(self, epoch, logs):\n-        self.trained_epochs.append(epoch)\n-\n-    def on_batch_begin(self, batch, logs):\n-        self.trained_batches.append(batch)\n-\n-\n-# TODO: resolve flakyness issue. Tracked with #11560\n-@flaky(rerun_filter=lambda err, *args: issubclass(err[0], AssertionError))\n-def test_model_methods():\n-    model = get_model(num_outputs=2)\n-\n-    optimizer = 'rmsprop'\n-    loss = 'mse'\n-    loss_weights = [1., 0.5]\n-\n-    input_a_np = np.random.random((10, 3))\n-    input_b_np = np.random.random((10, 3))\n-\n-    output_a_np = np.random.random((10, 4))\n-    output_b_np = np.random.random((10, 3))\n-\n-    # training/testing doesn't work before compiling.\n-    with pytest.raises(RuntimeError):\n-        model.train_on_batch([input_a_np, input_b_np],\n-                             [output_a_np, output_b_np])\n-\n-    model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n-                  sample_weight_mode=None)\n-\n-    # test train_on_batch\n-    out = model.train_on_batch([input_a_np, input_b_np],\n-                               [output_a_np, output_b_np])\n-    out = model.train_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n-                               [output_a_np, output_b_np])\n-    out = model.train_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n-                               {'dense_1': output_a_np, 'dropout': output_b_np})\n-\n-    # test fit\n-    out = model.fit([input_a_np, input_b_np],\n-                    [output_a_np, output_b_np], epochs=1, batch_size=4)\n-    out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n-                    [output_a_np, output_b_np], epochs=1, batch_size=4)\n-    out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n-                    {'dense_1': output_a_np, 'dropout': output_b_np},\n-                    epochs=1, batch_size=4)\n-\n-    # test validation_split\n-    out = model.fit([input_a_np, input_b_np],\n-                    [output_a_np, output_b_np],\n-                    epochs=1, batch_size=4, validation_split=0.5)\n-    out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n-                    [output_a_np, output_b_np],\n-                    epochs=1, batch_size=4, validation_split=0.5)\n-\n-    # test validation data\n-    out = model.fit([input_a_np, input_b_np],\n-                    [output_a_np, output_b_np],\n-                    epochs=1, batch_size=4,\n-                    validation_data=([input_a_np, input_b_np],\n-                                     [output_a_np, output_b_np]))\n-    out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n-                    [output_a_np, output_b_np],\n-                    epochs=1, batch_size=4, validation_split=0.5,\n-                    validation_data=({'input_a': input_a_np,\n-                                      'input_b': input_b_np},\n-                                     [output_a_np, output_b_np]))\n-    out = model.fit({'input_a': input_a_np, 'input_b': input_b_np},\n-                    {'dense_1': output_a_np, 'dropout': output_b_np},\n-                    epochs=1, batch_size=4, validation_split=0.5,\n-                    validation_data=(\n-                        {'input_a': input_a_np, 'input_b': input_b_np},\n-                        {'dense_1': output_a_np, 'dropout': output_b_np}))\n-\n-    # test_on_batch\n-    out = model.test_on_batch([input_a_np, input_b_np],\n-                              [output_a_np, output_b_np])\n-    out = model.test_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n-                              [output_a_np, output_b_np])\n-    out = model.test_on_batch({'input_a': input_a_np, 'input_b': input_b_np},\n-                              {'dense_1': output_a_np, 'dropout': output_b_np})\n-\n-    # predict_on_batch\n-    out = model.predict_on_batch([input_a_np, input_b_np])\n-    out = model.predict_on_batch({'input_a': input_a_np,\n-                                  'input_b': input_b_np})\n-\n-    # predict, evaluate\n-    input_a_np = np.random.random((10, 3))\n-    input_b_np = np.random.random((10, 3))\n-\n-    output_a_np = np.random.random((10, 4))\n-    output_b_np = np.random.random((10, 3))\n-\n-    out = model.evaluate([input_a_np, input_b_np],\n-                         [output_a_np, output_b_np],\n-                         batch_size=4)\n-    out = model.predict([input_a_np, input_b_np], batch_size=4)\n-\n-    # with sample_weight\n-    input_a_np = np.random.random((10, 3))\n-    input_b_np = np.random.random((10, 3))\n-\n-    output_a_np = np.random.random((10, 4))\n-    output_b_np = np.random.random((10, 3))\n-\n-    sample_weight = [None, np.random.random((10,))]\n-    out = model.train_on_batch([input_a_np, input_b_np],\n-                               [output_a_np, output_b_np],\n-                               sample_weight=sample_weight)\n-\n-    out = model.test_on_batch([input_a_np, input_b_np],\n-                              [output_a_np, output_b_np],\n-                              sample_weight=sample_weight)\n-\n-    # test accuracy metric\n-    model.compile(optimizer, loss, metrics=['acc'],\n-                  sample_weight_mode=None)\n-\n-    out = model.train_on_batch([input_a_np, input_b_np],\n-                               [output_a_np, output_b_np])\n-    assert len(out) == 5\n-    out = model.test_on_batch([input_a_np, input_b_np],\n-                              [output_a_np, output_b_np])\n-    assert len(out) == 5\n-\n-    # this should also work\n-    model.compile(optimizer, loss, metrics={'dense_1': 'acc'},\n-                  sample_weight_mode=None)\n-\n-    out = model.train_on_batch([input_a_np, input_b_np],\n-                               [output_a_np, output_b_np])\n-    assert len(out) == 4\n-    out = model.test_on_batch([input_a_np, input_b_np],\n-                              [output_a_np, output_b_np])\n-    assert len(out) == 4\n-\n-    # and this as well\n-    model.compile(optimizer, loss, metrics={'dense_1': ['acc']},\n-                  sample_weight_mode=None)\n-\n-    out = model.train_on_batch([input_a_np, input_b_np],\n-                               [output_a_np, output_b_np])\n-    assert len(out) == 4\n-    out = model.test_on_batch([input_a_np, input_b_np],\n-                              [output_a_np, output_b_np])\n-    assert len(out) == 4\n-\n-    tracker_cb = TrackerCallback()\n-\n-    out = model.fit([input_a_np, input_b_np],\n-                    [output_a_np, output_b_np], epochs=5, batch_size=4,\n-                    initial_epoch=2, callbacks=[tracker_cb])\n-    assert tracker_cb.trained_epochs == [2, 3, 4]\n-\n-    # test starting from non-zero initial epoch for generator too\n-    tracker_cb = TrackerCallback()\n-\n-    @threadsafe_generator\n-    def gen_data(batch_sz):\n-        while True:\n-            yield ([np.random.random((batch_sz, 3)),\n-                    np.random.random((batch_sz, 3))],\n-                   [np.random.random((batch_sz, 4)),\n-                    np.random.random((batch_sz, 3))])\n-\n-    out = model.fit_generator(gen_data(4), steps_per_epoch=3, epochs=5,\n-                              initial_epoch=2, callbacks=[tracker_cb])\n-    assert tracker_cb.trained_epochs == [2, 3, 4]\n-\n-    # test with a custom metric function\n-    def mse(y_true, y_pred):\n-        return K.mean(K.pow(y_true - y_pred, 2))\n-\n-    model.compile(optimizer, loss, metrics=[mse],\n-                  sample_weight_mode=None)\n-\n-    out = model.train_on_batch([input_a_np, input_b_np],\n-                               [output_a_np, output_b_np])\n-    out_len = 1 + 2 * (1 + 1)  # total loss + 2 outputs * (loss + metric)\n-    assert len(out) == out_len\n-    out = model.test_on_batch([input_a_np, input_b_np],\n-                              [output_a_np, output_b_np])\n-    assert len(out) == out_len\n-\n-    input_a_np = np.random.random((10, 3))\n-    input_b_np = np.random.random((10, 3))\n-\n-    output_a_np = np.random.random((10, 4))\n-    output_b_np = np.random.random((10, 3))\n-\n-    out = model.fit([input_a_np, input_b_np],\n-                    [output_a_np, output_b_np],\n-                    batch_size=4, epochs=1)\n-    out = model.evaluate([input_a_np, input_b_np],\n-                         [output_a_np, output_b_np],\n-                         batch_size=4)\n-    out = model.predict([input_a_np, input_b_np], batch_size=4)\n-\n-    # enable verbose for evaluate_generator\n-    out = model.evaluate_generator(gen_data(4), steps=3, verbose=1)\n-    # pass generator directly so `is_generator_or_sequence`\n-    # doesn't get confused.\n-    out = model.evaluate(gen_data(4).it, steps=3, verbose=1)\n-\n-    # empty batch\n-    with pytest.raises(ValueError):\n-        @threadsafe_generator\n-        def gen_data():\n-            while True:\n-                yield (np.asarray([]), np.asarray([]))\n-\n-        out = model.evaluate_generator(gen_data(), steps=1)\n-    with pytest.raises(ValueError):\n-        @threadsafe_generator\n-        def gen_data():\n-            while True:\n-                yield (np.asarray([]), np.asarray([]))\n-\n-        out = model.evaluate(gen_data().it, steps=1)\n-\n-    # x is not a list of numpy arrays.\n-    with pytest.raises(ValueError):\n-        out = model.predict([None])\n-\n-    # x does not match _feed_input_names.\n-    with pytest.raises(ValueError):\n-        out = model.predict([input_a_np, None, input_b_np])\n-    with pytest.raises(ValueError):\n-        out = model.predict([None, input_a_np, input_b_np])\n-\n-    # # all input/output/weight arrays should have the same number of samples.\n-    # with pytest.raises(ValueError):\n-    #     out = model.train_on_batch([input_a_np, input_b_np[:2]],\n-    #                                [output_a_np, output_b_np],\n-    #                                sample_weight=sample_weight)\n-    # with pytest.raises(ValueError):\n-    #     out = model.train_on_batch([input_a_np, input_b_np],\n-    #                                [output_a_np, output_b_np[:2]],\n-    #                                sample_weight=sample_weight)\n-    # with pytest.raises(ValueError):\n-    #     out = model.train_on_batch([input_a_np, input_b_np],\n-    #                                [output_a_np, output_b_np],\n-    #                                sample_weight=[sample_weight[1],\n-    #                                               sample_weight[1][:2]])\n-\n-    # # `sample_weight` is neither a dict nor a list.\n-    # with pytest.raises(TypeError):\n-    #     out = model.train_on_batch([input_a_np, input_b_np],\n-    #                                [output_a_np, output_b_np],\n-    #                                sample_weight=tuple(sample_weight))\n-\n-    # # `validation_data` is neither a tuple nor a triple.\n-    # with pytest.raises(ValueError):\n-    #     out = model.fit([input_a_np, input_b_np],\n-    #                     [output_a_np, output_b_np],\n-    #                     epochs=1, batch_size=4,\n-    #                     validation_data=([input_a_np, input_b_np],))\n-\n-    # # `loss` does not match outputs.\n-    # with pytest.raises(ValueError):\n-    #     model.compile(optimizer, loss=['mse', 'mae', 'mape'])\n-\n-    # # `loss_weights` does not match output_names.\n-    # with pytest.raises(ValueError):\n-    #     model.compile(optimizer, loss='mse', loss_weights={'lstm': 0.5})\n-\n-    # # `loss_weights` does not match outputs.\n-    # with pytest.raises(ValueError):\n-    #     model.compile(optimizer, loss='mse', loss_weights=[0.5])\n-\n-    # # `loss_weights` is invalid type.\n-    # with pytest.raises(TypeError):\n-    #     model.compile(optimizer, loss='mse', loss_weights=(0.5, 0.5))\n-\n-    # # `sample_weight_mode` does not match output_names.\n-    # with pytest.raises(ValueError):\n-    #     model.compile(optimizer, loss='mse',\n-    #                   sample_weight_mode={'lstm': 'temporal'})\n-\n-    # # `sample_weight_mode` does not match output_names.\n-    # with pytest.raises(ValueError):\n-    #     model.compile(optimizer, loss='mse', sample_weight_mode=['temporal'])\n-\n-    # # `sample_weight_mode` matches output_names partially.\n-    # with pytest.raises(ValueError):\n-    #     model.compile(optimizer, loss='mse',\n-    #                   sample_weight_mode={'dense_1': 'temporal'})\n-\n-    # # `loss` does not exist.\n-    # with pytest.raises(ValueError):\n-    #     model.compile(optimizer, loss=[])\n-\n-    model.compile(optimizer, loss=['mse', 'mae'])\n-    model.compile(optimizer, loss='mse', loss_weights={'dense_1': 0.2,\n-                                                       'dropout': 0.8})\n-    model.compile(optimizer, loss='mse', loss_weights=[0.2, 0.8])\n-\n-    # the rank of weight arrays should be 1.\n-    with pytest.raises(ValueError):\n-        out = model.train_on_batch(\n-            [input_a_np, input_b_np],\n-            [output_a_np, output_b_np],\n-            sample_weight=[None, np.random.random((10, 20, 30))])\n-\n-    model.compile(optimizer, loss='mse',\n-                  sample_weight_mode={'dense_1': None, 'dropout': 'temporal'})\n-    model.compile(optimizer, loss='mse', sample_weight_mode=[None, 'temporal'])\n-\n-    # # the rank of output arrays should be at least 3D.\n-    # with pytest.raises(ValueError):\n-    #     out = model.train_on_batch([input_a_np, input_b_np],\n-    #                                [output_a_np, output_b_np],\n-    #                                sample_weight=sample_weight)\n-\n-\n-# TODO: resolve flakyness issue. Tracked with #11560\n-@flaky(rerun_filter=lambda err, *args: issubclass(err[0], AssertionError))\n-def DISABLED_test_fit_generator():\n-    model = get_model(num_outputs=2)\n-    optimizer = 'rmsprop'\n-    loss = 'mse'\n-    loss_weights = [1., 0.5]\n-\n-    model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n-                  sample_weight_mode=None)\n-    tracker_cb = TrackerCallback()\n-    val_seq = RandomSequence(4)\n-    out = model.fit_generator(generator=RandomSequence(3),\n-                              steps_per_epoch=3,\n-                              epochs=5,\n-                              initial_epoch=0,\n-                              validation_data=val_seq,\n-                              validation_steps=3,\n-                              max_queue_size=1,\n-                              callbacks=[tracker_cb])\n-    assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]\n-    assert tracker_cb.trained_batches == list(range(3)) * 5\n-    # print('val_seq.logs', val_seq.logs)\n-    # assert len(val_seq.logs) <= 4 * 5\n-\n-    tracker_cb = TrackerCallback()\n-    val_seq = RandomSequence(4)\n-    out = model.fit(RandomSequence(3),\n-                    steps_per_epoch=3,\n-                    epochs=5,\n-                    initial_epoch=0,\n-                    validation_data=val_seq,\n-                    validation_steps=3,\n-                    max_queue_size=1,\n-                    callbacks=[tracker_cb])\n-    assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]\n-    assert tracker_cb.trained_batches == list(range(3)) * 5\n-    # assert len(val_seq.logs) <= 4 * 5\n-\n-    # steps_per_epoch will be equal to len of sequence if it's unspecified\n-    tracker_cb = TrackerCallback()\n-    val_seq = RandomSequence(4)\n-    out = model.fit_generator(generator=RandomSequence(3),\n-                              epochs=5,\n-                              initial_epoch=0,\n-                              validation_data=val_seq,\n-                              callbacks=[tracker_cb],\n-                              max_queue_size=1)\n-    assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]\n-    assert tracker_cb.trained_batches == list(range(12)) * 5\n-    # assert 12 * 5 <= len(val_seq.logs) <= (12 * 5) + 2  # the queue may be full.\n-\n-    tracker_cb = TrackerCallback()\n-    val_seq = RandomSequence(4)\n-    out = model.fit(RandomSequence(3),\n-                    epochs=5,\n-                    initial_epoch=0,\n-                    validation_data=val_seq,\n-                    callbacks=[tracker_cb],\n-                    max_queue_size=1)\n-    assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]\n-    assert tracker_cb.trained_batches == list(range(12)) * 5\n-    # assert 12 * 5 <= len(val_seq.logs) <= (12 * 5) + 2  # the queue may be full.\n-\n-    # test for workers = 0\n-    tracker_cb = TrackerCallback()\n-    val_seq = RandomSequence(4)\n-    out = model.fit_generator(generator=RandomSequence(3),\n-                              epochs=5,\n-                              validation_data=val_seq,\n-                              callbacks=[tracker_cb],\n-                              workers=0)\n-    assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]\n-    assert tracker_cb.trained_batches == list(range(12)) * 5\n-    # assert len(val_seq.logs) == 12 * 5\n-\n-    tracker_cb = TrackerCallback()\n-    val_seq = RandomSequence(4)\n-    out = model.fit(RandomSequence(3),\n-                    steps_per_epoch=3,\n-                    epochs=5,\n-                    initial_epoch=0,\n-                    validation_data=val_seq,\n-                    validation_steps=3,\n-                    max_queue_size=1,\n-                    callbacks=[tracker_cb])\n-    assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]\n-    assert tracker_cb.trained_batches == list(range(3)) * 5\n-    # assert len(val_seq.logs) <= 4 * 5\n-\n-    # fit_generator will throw an exception\n-    # if steps is unspecified for regular generator\n-    with pytest.raises(ValueError):\n-        @threadsafe_generator\n-        def gen_data():\n-            while True:\n-                yield (np.asarray([]), np.asarray([]))\n-\n-        out = model.fit_generator(generator=gen_data(), epochs=5,\n-                                  initial_epoch=0, validation_data=gen_data(),\n-                                  callbacks=[tracker_cb])\n-\n-    # Check if generator is only accessed an expected number of times\n-    gen_counters = [0, 0]\n-\n-    @threadsafe_generator\n-    def gen_data(i):\n-        while True:\n-            gen_counters[i] += 1\n-            yield ([np.random.random((1, 3)), np.random.random((1, 3))],\n-                   [np.random.random((1, 4)), np.random.random((1, 3))])\n-    out = model.fit_generator(generator=gen_data(0), epochs=3,\n-                              steps_per_epoch=2,\n-                              validation_data=gen_data(1),\n-                              validation_steps=1,\n-                              max_queue_size=2,\n-                              workers=2)\n-\n-    # Need range check here as filling\n-    # of the queue depends on sleep in the enqueuers\n-    max_train = 3 * 2 + 2 * 2\n-    min_train = 2 * 3\n-    assert min_train <= gen_counters[0] <= max_train\n-    # 12 = (epoch * workers * validation steps * max_queue_size)\n-    assert 3 <= gen_counters[1] <= 12\n-\n-    gen_counters = [0]\n-    out = model.fit_generator(generator=RandomSequence(3), epochs=3,\n-                              validation_data=gen_data(0),\n-                              validation_steps=1,\n-                              max_queue_size=2,\n-                              workers=2)\n-\n-    # 12 = (epoch * workers * validation steps * max_queue_size)\n-    # Need range check here as filling\n-    # of the queue depends on sleep in the enqueuers\n-    assert 3 <= gen_counters[0] <= 12\n-\n-\n-def DISABLED_test_fit_generator_dynamic_size_sequence_with_workers():\n-    model = get_model(num_outputs=2)\n-    optimizer = 'rmsprop'\n-    loss = 'mse'\n-    loss_weights = [1., 0.5]\n-\n-    model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n-                  sample_weight_mode=None)\n-    tracker_cb = TrackerCallback()\n-    val_seq = RandomSequence(4)\n-    train_seq = IncreaseBatchSizeRandomSequence(3, 20)\n-    out = model.fit_generator(generator=train_seq,\n-                              epochs=5,\n-                              initial_epoch=0,\n-                              validation_data=val_seq,\n-                              validation_steps=3,\n-                              max_queue_size=1,\n-                              callbacks=[tracker_cb])\n-    assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]\n-    assert tracker_cb.trained_batches == [\n-        0, 1, 2, 3, 4, 5, 6,  # 1st epoch -> ceil(20 / 3) = 7 batches\n-        0, 1, 2, 3,           # 2nd epoch -> ceil(20 / 5) = 4 batches\n-        0, 1, 2,              # 3d  epoch -> ceil(20 / 7) = 3 batches\n-        0, 1, 2,              # 4th epoch -> ceil(20 / 9) = 3 batches\n-        0, 1,                 # 5th epoch -> ceil(20 /11) = 2 batches\n-    ]\n-    assert tracker_cb.steps_per_epoch_log[0:5] == [7, 4, 3, 3, 2]\n-\n-    tracker_cb = TrackerCallback()\n-    val_seq = RandomSequence(4)\n-    train_seq = IncreaseBatchSizeRandomSequence(3, 30)\n-    out = model.fit_generator(generator=train_seq,\n-                              epochs=5,\n-                              initial_epoch=0,\n-                              validation_data=val_seq,\n-                              validation_steps=3,\n-                              max_queue_size=1,\n-                              callbacks=[tracker_cb])\n-    assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]\n-    assert tracker_cb.trained_batches == [\n-        0, 1, 2, 3, 4, 5, 6, 7, 8, 9,  # 1st epoch -> ceil(30 / 3) = 10 batches\n-        0, 1, 2, 3, 4, 5,              # 2nd epoch -> ceil(30 / 5) =  6 batches\n-        0, 1, 2, 3, 4,                 # 3d  epoch -> ceil(30 / 7) =  5 batches\n-        0, 1, 2, 3,                    # 4th epoch -> ceil(30 / 9) =  4 batches\n-        0, 1, 2,                       # 5th epoch -> ceil(30 /11) =  3 batches\n-    ]\n-    assert tracker_cb.steps_per_epoch_log[0:5] == [10, 6, 5, 4, 3]\n-\n-    tracker_cb = TrackerCallback()\n-    val_seq = RandomSequence(4)\n-    train_seq = IncreaseBatchSizeRandomSequence(2, 404, lambda x: x * 2)\n-    out = model.fit_generator(generator=train_seq,\n-                              epochs=5,\n-                              initial_epoch=0,\n-                              validation_data=val_seq,\n-                              validation_steps=3,\n-                              max_queue_size=1,\n-                              callbacks=[tracker_cb])\n-    assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]\n-    # number of trained batches should match sum of steps per each epoch\n-    assert len(tracker_cb.trained_batches) == 202 + 101 + 51 + 26 + 13\n-    assert tracker_cb.steps_per_epoch_log[0:5] == [202, 101, 51, 26, 13]\n-\n-\n-def DISABLED_test_fit_generator_dynamic_size_sequence_main_thread():\n-    model = get_model(num_outputs=2)\n-    optimizer = 'rmsprop'\n-    loss = 'mse'\n-    loss_weights = [1., 0.5]\n-\n-    model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n-                  sample_weight_mode=None)\n-    tracker_cb = TrackerCallback()\n-    val_seq = RandomSequence(4)\n-    train_seq = IncreaseBatchSizeRandomSequence(3, 20)\n-    out = model.fit_generator(generator=train_seq,\n-                              epochs=5,\n-                              initial_epoch=0,\n-                              validation_data=val_seq,\n-                              validation_steps=3,\n-                              workers=0,\n-                              callbacks=[tracker_cb])\n-    assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]\n-    assert tracker_cb.trained_batches == [\n-        0, 1, 2, 3, 4, 5, 6,  # 1st epoch -> ceil(20 / 3) = 7 batches\n-        0, 1, 2, 3,           # 2nd epoch -> ceil(20 / 5) = 4 batches\n-        0, 1, 2,              # 3d  epoch -> ceil(20 / 7) = 3 batches\n-        0, 1, 2,              # 4th epoch -> ceil(20 / 9) = 3 batches\n-        0, 1,                 # 5th epoch -> ceil(20 /11) = 2 batches\n-    ]\n-    assert tracker_cb.steps_per_epoch_log[0:5] == [7, 4, 3, 3, 2]\n-\n-    tracker_cb = TrackerCallback()\n-    val_seq = RandomSequence(4)\n-    train_seq = IncreaseBatchSizeRandomSequence(3, 30)\n-    out = model.fit_generator(generator=train_seq,\n-                              epochs=5,\n-                              initial_epoch=0,\n-                              validation_data=val_seq,\n-                              validation_steps=3,\n-                              workers=0,\n-                              callbacks=[tracker_cb])\n-    assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]\n-    assert tracker_cb.trained_batches == [\n-        0, 1, 2, 3, 4, 5, 6, 7, 8, 9,  # 1st epoch -> ceil(30 / 3) = 10 batches\n-        0, 1, 2, 3, 4, 5,              # 2nd epoch -> ceil(30 / 5) =  6 batches\n-        0, 1, 2, 3, 4,                 # 3d  epoch -> ceil(30 / 7) =  5 batches\n-        0, 1, 2, 3,                    # 4th epoch -> ceil(30 / 9) =  4 batches\n-        0, 1, 2,                       # 5th epoch -> ceil(30 /11) =  3 batches\n-    ]\n-    assert tracker_cb.steps_per_epoch_log[0:5] == [10, 6, 5, 4, 3]\n-\n-    tracker_cb = TrackerCallback()\n-    val_seq = RandomSequence(4)\n-    train_seq = IncreaseBatchSizeRandomSequence(2, 404, lambda x: x * 2)\n-    out = model.fit_generator(generator=train_seq,\n-                              epochs=5,\n-                              initial_epoch=0,\n-                              validation_data=val_seq,\n-                              validation_steps=3,\n-                              workers=0,\n-                              callbacks=[tracker_cb])\n-    assert tracker_cb.trained_epochs == [0, 1, 2, 3, 4]\n-    # number of trained batches should match sum of steps per each epoch\n-    assert len(tracker_cb.trained_batches) == 202 + 101 + 51 + 26 + 13\n-    assert tracker_cb.steps_per_epoch_log[0:5] == [202, 101, 51, 26, 13]\n-\n-\n-def test_fit_generator_shape():\n-    # predict_generator output shape behavior should be consistent\n-    def expected_shape(batch_size, n_batches):\n-        return (batch_size * n_batches, 4), (batch_size * n_batches, 3)\n-\n-    model = get_model(num_outputs=2)\n-    optimizer = 'rmsprop'\n-    loss = 'mse'\n-\n-    # Multiple outputs and one step.\n-    batch_size = 5\n-    sequence_length = 1\n-    shape_0, shape_1 = expected_shape(batch_size, sequence_length)\n-    out = model.predict_generator(\n-        RandomSequence(batch_size, sequence_length=sequence_length))\n-    assert np.shape(out[0]) == shape_0 and np.shape(out[1]) == shape_1\n-\n-    out = model.predict(\n-        RandomSequence(batch_size, sequence_length=sequence_length))\n-    assert np.shape(out[0]) == shape_0 and np.shape(out[1]) == shape_1\n-\n-    # Multiple outputs and multiple steps.\n-    batch_size = 5\n-    sequence_length = 2\n-    shape_0, shape_1 = expected_shape(batch_size, sequence_length)\n-    out = model.predict_generator(\n-        RandomSequence(batch_size, sequence_length=sequence_length))\n-    assert np.shape(out[0]) == shape_0 and np.shape(out[1]) == shape_1\n-\n-    out = model.predict(\n-        RandomSequence(batch_size, sequence_length=sequence_length))\n-    assert np.shape(out[0]) == shape_0 and np.shape(out[1]) == shape_1\n-\n-    # Create a model with a single output.\n-    single_output_model = get_model(num_outputs=1)\n-    single_output_model.compile(optimizer, loss,\n-                                metrics=[], sample_weight_mode=None)\n-\n-    # Single output and one step.\n-    batch_size = 5\n-    sequence_length = 1\n-    shape_0, _ = expected_shape(batch_size, sequence_length)\n-    out = single_output_model.predict_generator(\n-        RandomSequence(batch_size, sequence_length=sequence_length))\n-    assert np.shape(out) == shape_0\n-\n-    out = single_output_model.predict(\n-        RandomSequence(batch_size, sequence_length=sequence_length))\n-    assert np.shape(out) == shape_0\n-\n-    # Single output and multiple steps.\n-    batch_size = 5\n-    sequence_length = 2\n-    shape_0, _ = expected_shape(batch_size, sequence_length)\n-    out = single_output_model.predict_generator(\n-        RandomSequence(batch_size, sequence_length=sequence_length))\n-    assert np.shape(out) == shape_0\n-\n-    out = single_output_model.predict(\n-        RandomSequence(batch_size, sequence_length=sequence_length))\n-    assert np.shape(out) == shape_0\n-\n-\n-def test_training_with_loss_instance():\n-    a = Input(shape=(3,), name='input_a')\n-    b = Input(shape=(3,), name='input_b')\n-\n-    dense = Dense(4, name='dense')\n-    c = dense(a)\n-    d = dense(b)\n-    e = Dropout(0.5, name='dropout')(c)\n-\n-    model = Model([a, b], [d, e])\n-    loss_weights = [1., 0.5]\n-    model.compile(\n-        'sgd',\n-        loss=losses.MeanSquaredError(),\n-        metrics=['mae'],\n-        loss_weights=loss_weights)\n-\n-    input_a_np = np.random.random((10, 3))\n-    input_b_np = np.random.random((10, 3))\n-\n-    output_d_np = np.random.random((10, 4))\n-    output_e_np = np.random.random((10, 4))\n-\n-    model.fit([input_a_np, input_b_np], [output_d_np, output_e_np],\n-              epochs=1,\n-              batch_size=5)\n-\n-\n-@pytest.mark.skipif(sys.version_info < (3,),\n-                    reason='Cannot catch warnings in python 2')\n-def DISABLED_test_warnings():\n-    \"\"\"This test hangs Travis.\"\"\"\n-    a = Input(shape=(3,), name='input_a')\n-    b = Input(shape=(3,), name='input_b')\n-\n-    a_2 = Dense(4, name='dense_1')(a)\n-    dp = Dropout(0.5, name='dropout')\n-    b_2 = dp(b)\n-\n-    model = Model([a, b], [a_2, b_2])\n-\n-    optimizer = 'rmsprop'\n-    loss = 'mse'\n-    loss_weights = [1., 0.5]\n-    model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n-                  sample_weight_mode=None)\n-\n-    @threadsafe_generator\n-    def gen_data(batch_sz):\n-        while True:\n-            yield ([np.random.random((batch_sz, 3)),\n-                    np.random.random((batch_sz, 3))],\n-                   [np.random.random((batch_sz, 4)),\n-                    np.random.random((batch_sz, 3))])\n-\n-    with pytest.warns(Warning) as w:\n-        out = model.fit_generator(gen_data(4),\n-                                  steps_per_epoch=10,\n-                                  use_multiprocessing=True,\n-                                  workers=2)\n-    warning_raised = any(['Sequence' in str(w_.message) for w_ in w])\n-    assert warning_raised, 'No warning raised when using generator with processes.'\n-\n-    with pytest.warns(None) as w:\n-        out = model.fit_generator(RandomSequence(3),\n-                                  steps_per_epoch=4,\n-                                  use_multiprocessing=True,\n-                                  workers=2)\n-    assert all(['Sequence' not in str(w_.message) for w_ in w]), (\n-        'A warning was raised for Sequence.')\n-\n-\n-@pytest.mark.skipif(K.backend() == 'tensorflow',\n-                    reason='Must for for tf.keras to support sparse ops.')\n-def test_sparse_inputs_targets():\n-    test_inputs = [sparse.random(6, 3, density=0.25).tocsr() for _ in range(2)]\n-    test_outputs = [sparse.random(6, i, density=0.25).tocsr() for i in range(3, 5)]\n-    in1 = Input(shape=(3,))\n-    in2 = Input(shape=(3,))\n-    out1 = Dropout(0.5, name='dropout')(in1)\n-    out2 = Dense(4, name='dense_1')(in2)\n-    model = Model([in1, in2], [out1, out2])\n-    model.predict(test_inputs, batch_size=2)\n-    model.compile('rmsprop', 'mse')\n-    model.fit(test_inputs, test_outputs,\n-              epochs=1, batch_size=2, validation_split=0.5)\n-    model.evaluate(test_inputs, test_outputs, batch_size=2)\n-\n-\n-@pytest.mark.skipif(K.backend() != 'tensorflow',\n-                    reason='sparse operations supported only by TensorFlow')\n-def DISABLED_test_sparse_placeholder_fit():\n-    \"\"\"Must wait for tf.keras to support sparse operations.\"\"\"\n-    test_inputs = [sparse.random(6, 3, density=0.25).tocsr() for _ in range(2)]\n-    test_outputs = [sparse.random(6, i, density=0.25).tocsr() for i in range(3, 5)]\n-    in1 = Input(shape=(3,))\n-    in2 = Input(shape=(3,), sparse=True)\n-    out1 = Dropout(0.5, name='dropout')(in1)\n-    out2 = Dense(4, name='dense_1')(in2)\n-    model = Model([in1, in2], [out1, out2])\n-    model.predict(test_inputs, batch_size=2)\n-    model.compile('rmsprop', 'mse')\n-    model.fit(test_inputs, test_outputs,\n-              epochs=1, batch_size=2, validation_split=0.5)\n-    model.evaluate(test_inputs, test_outputs, batch_size=2)\n-\n-\n-def test_trainable_argument():\n-    x = np.random.random((5, 3))\n-    y = np.random.random((5, 2))\n-\n-    model = Sequential()\n-    model.add(Dense(2, input_dim=3, trainable=False))\n-    model.compile('rmsprop', 'mse')\n-    out = model.predict(x)\n-    model.train_on_batch(x, y)\n-    out_2 = model.predict(x)\n-    assert_allclose(out, out_2)\n-\n-    # test with nesting\n-    inputs = Input(shape=(3,))\n-    outputs = model(inputs)\n-    model = Model(inputs, outputs)\n-    model.compile('rmsprop', 'mse')\n-    out = model.predict(x)\n-    model.train_on_batch(x, y)\n-    out_2 = model.predict(x)\n-    assert_allclose(out, out_2)\n-\n-\n-def DISABLED_test_with_list_as_targets():\n-    model = Sequential()\n-    model.add(Dense(1, input_dim=3, trainable=False))\n-    model.compile('rmsprop', 'mse')\n-\n-    x = np.random.random((2, 3))\n-    y = [0, 1]\n-    model.train_on_batch(x, y)\n-\n-\n-def DISABLED_test_check_not_failing():\n-    a = np.random.random((2, 1, 3))\n-    training_utils.check_loss_and_target_compatibility(\n-        [a], [losses.categorical_crossentropy], [a.shape])\n-    training_utils.check_loss_and_target_compatibility(\n-        [a], [losses.categorical_crossentropy], [(2, None, 3)])\n-\n-\n-def DISABLED_test_check_last_is_one():\n-    a = np.random.random((2, 3, 1))\n-    with pytest.raises(ValueError,\n-                       match='You are passing a target array'):\n-        training_utils.check_loss_and_target_compatibility(\n-            [a], [losses.CategoricalCrossentropy()], [a.shape])\n-\n-\n-def DISABLED_test_check_bad_shape():\n-    a = np.random.random((2, 3, 5))\n-    with pytest.raises(ValueError,\n-                       match='targets to have the same shape'):\n-        training_utils.check_loss_and_target_compatibility(\n-            [a], [losses.CategoricalCrossentropy()], [(2, 3, 6)])\n-\n-\n-@pytest.mark.skipif(K.backend() != 'tensorflow',\n-                    reason='Requires TensorFlow backend')\n-def DISABLED_test_model_with_input_feed_tensor():\n-    \"\"\"We test building a model with a TF variable as input.\n-    We should be able to call fit, evaluate, predict,\n-    by only passing them data for the placeholder inputs\n-    in the model.\n-    \"\"\"\n-    import tensorflow as tf\n-\n-    input_a_np = np.random.random((10, 3))\n-    input_b_np = np.random.random((10, 3))\n-\n-    output_a_np = np.random.random((10, 4))\n-    output_b_np = np.random.random((10, 3))\n-\n-    a = Input(tensor=tf.Variable(input_a_np, dtype=tf.float32))\n-    b = Input(shape=(3,), name='input_b')\n-\n-    a_2 = Dense(4, name='dense_1')(a)\n-    dp = Dropout(0.5, name='dropout')\n-    b_2 = dp(b)\n-\n-    model = Model([a, b], [a_2, b_2])\n-    model.summary()\n-\n-    optimizer = 'rmsprop'\n-    loss = 'mse'\n-    loss_weights = [1., 0.5]\n-    model.compile(optimizer, loss, metrics=['mean_squared_error'],\n-                  loss_weights=loss_weights,\n-                  sample_weight_mode=None)\n-\n-    # test train_on_batch\n-    out = model.train_on_batch(input_b_np,\n-                               [output_a_np, output_b_np])\n-    out = model.train_on_batch({'input_b': input_b_np},\n-                               [output_a_np, output_b_np])\n-    out = model.test_on_batch({'input_b': input_b_np},\n-                              [output_a_np, output_b_np])\n-    out = model.predict_on_batch({'input_b': input_b_np})\n-\n-    # test fit\n-    out = model.fit({'input_b': input_b_np},\n-                    [output_a_np, output_b_np], epochs=1, batch_size=10)\n-    out = model.fit(input_b_np,\n-                    [output_a_np, output_b_np], epochs=1, batch_size=10)\n-\n-    # test evaluate\n-    out = model.evaluate({'input_b': input_b_np},\n-                         [output_a_np, output_b_np], batch_size=10)\n-    out = model.evaluate(input_b_np,\n-                         [output_a_np, output_b_np], batch_size=10)\n-\n-    # test predict\n-    out = model.predict({'input_b': input_b_np}, batch_size=10)\n-    out = model.predict(input_b_np, batch_size=10)\n-    assert len(out) == 2\n-\n-    # Now test a model with a single input\n-    # i.e. we don't pass any data to fit the model.\n-    a = Input(tensor=tf.Variable(input_a_np, dtype=tf.float32))\n-    a_2 = Dense(4, name='dense_1')(a)\n-    a_2 = Dropout(0.5, name='dropout')(a_2)\n-    model = Model(a, a_2)\n-    model.summary()\n-\n-    optimizer = 'rmsprop'\n-    loss = 'mse'\n-    model.compile(optimizer, loss, metrics=['mean_squared_error'])\n-\n-    # test train_on_batch\n-    out = model.train_on_batch(None,\n-                               output_a_np)\n-    out = model.train_on_batch(None,\n-                               output_a_np)\n-    out = model.test_on_batch(None,\n-                              output_a_np)\n-    out = model.predict_on_batch(None)\n-    out = model.train_on_batch([],\n-                               output_a_np)\n-    out = model.train_on_batch({},\n-                               output_a_np)\n-\n-    # test fit\n-    out = model.fit(None,\n-                    output_a_np, epochs=1, batch_size=10)\n-    out = model.fit(None,\n-                    output_a_np, epochs=1, batch_size=10)\n-\n-    # test evaluate\n-    out = model.evaluate(None,\n-                         output_a_np, batch_size=10)\n-    out = model.evaluate(None,\n-                         output_a_np, batch_size=10)\n-\n-    # test predict\n-    out = model.predict(None, steps=3)\n-    out = model.predict(None, steps=3)\n-    assert out.shape == (10 * 3, 4)\n-\n-    # Same, without learning phase\n-    # i.e. we don't pass any data to fit the model.\n-    a = Input(tensor=tf.Variable(input_a_np, dtype=tf.float32))\n-    a_2 = Dense(4, name='dense_1')(a)\n-    model = Model(a, a_2)\n-    model.summary()\n-\n-    optimizer = 'rmsprop'\n-    loss = 'mse'\n-    model.compile(optimizer, loss, metrics=['mean_squared_error'])\n-\n-    # test train_on_batch\n-    out = model.train_on_batch(None,\n-                               output_a_np)\n-    out = model.train_on_batch(None,\n-                               output_a_np)\n-    out = model.test_on_batch(None,\n-                              output_a_np)\n-    out = model.predict_on_batch(None)\n-    out = model.train_on_batch([],\n-                               output_a_np)\n-    out = model.train_on_batch({},\n-                               output_a_np)\n-\n-    # test fit\n-    out = model.fit(None,\n-                    output_a_np, epochs=1, batch_size=10)\n-    out = model.fit(None,\n-                    output_a_np, epochs=1, batch_size=10)\n-\n-    # test evaluate\n-    out = model.evaluate(None,\n-                         output_a_np, batch_size=10)\n-    out = model.evaluate(None,\n-                         output_a_np, batch_size=10)\n-\n-    # test predict\n-    out = model.predict(None, steps=3)\n-    out = model.predict(None, steps=3)\n-    assert out.shape == (10 * 3, 4)\n-\n-\n-def DISABLED_test_model_with_partial_loss():\n-    a = Input(shape=(3,), name='input_a')\n-    a_2 = Dense(4, name='dense_1')(a)\n-    dp = Dropout(0.5, name='dropout')\n-    a_3 = dp(a_2)\n-    model = Model(a, [a_2, a_3])\n-\n-    optimizer = 'rmsprop'\n-    loss = {'dropout': 'mse'}\n-    model.compile(optimizer, loss, metrics=['mae'])\n-\n-    input_a_np = np.random.random((10, 3))\n-    output_a_np = np.random.random((10, 4))\n-\n-    # test train_on_batch\n-    out = model.train_on_batch(input_a_np, output_a_np)\n-    out = model.test_on_batch(input_a_np, output_a_np)\n-    # fit\n-    out = model.fit(input_a_np, output_a_np)\n-    # evaluate\n-    out = model.evaluate(input_a_np, output_a_np)\n-\n-    # Same without dropout.\n-    a = Input(shape=(3,), name='input_a')\n-    a_2 = Dense(4, name='dense_1')(a)\n-    a_3 = Dense(4, name='dense_2')(a_2)\n-    model = Model(a, [a_2, a_3])\n-\n-    optimizer = 'rmsprop'\n-    loss = {'dense_2': 'mse'}\n-    model.compile(optimizer, loss, metrics={'dense_1': 'mae'})\n-\n-    # test train_on_batch\n-    out = model.train_on_batch(input_a_np, output_a_np)\n-    out = model.test_on_batch(input_a_np, output_a_np)\n-    # fit\n-    out = model.fit(input_a_np, output_a_np)\n-    # evaluate\n-    out = model.evaluate(input_a_np, output_a_np)\n-\n-\n-@pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason='cntk does not support external loss yet')\n-def DISABLED_test_model_with_external_loss():\n-    # None loss, only regularization loss.\n-    a = Input(shape=(3,), name='input_a')\n-    a_2 = Dense(4, name='dense_1',\n-                kernel_regularizer='l1',\n-                bias_regularizer='l2')(a)\n-    dp = Dropout(0.5, name='dropout')\n-    a_3 = dp(a_2)\n-\n-    model = Model(a, [a_2, a_3])\n-\n-    optimizer = 'rmsprop'\n-    loss = None\n-    model.compile(optimizer, loss, metrics=['mae'])\n-\n-    input_a_np = np.random.random((10, 3))\n-\n-    # test train_on_batch\n-    out = model.train_on_batch(input_a_np, None)\n-    out = model.test_on_batch(input_a_np, None)\n-    # fit\n-    out = model.fit(input_a_np, None)\n-    # evaluate\n-    out = model.evaluate(input_a_np, None)\n-\n-    # No dropout, external loss.\n-    a = Input(shape=(3,), name='input_a')\n-    a_2 = Dense(4, name='dense_1')(a)\n-    a_3 = Dense(4, name='dense_2')(a)\n-\n-    model = Model(a, [a_2, a_3])\n-    model.add_loss(K.mean(a_3 + a_2))\n-\n-    optimizer = 'rmsprop'\n-    loss = None\n-    model.compile(optimizer, loss, metrics=['mae'])\n-\n-    # test train_on_batch\n-    out = model.train_on_batch(input_a_np, None)\n-    out = model.test_on_batch(input_a_np, None)\n-    # fit\n-    out = model.fit(input_a_np, None)\n-    # evaluate\n-    out = model.evaluate(input_a_np, None)\n-\n-    # Test fit with no external data at all.\n-    if K.backend() == 'tensorflow':\n-        import tensorflow as tf\n-\n-        a = Input(tensor=tf.Variable(input_a_np, dtype=tf.float32))\n-        a_2 = Dense(4, name='dense_1')(a)\n-        a_2 = Dropout(0.5, name='dropout')(a_2)\n-        model = Model(a, a_2)\n-        model.add_loss(K.mean(a_2))\n-\n-        model.compile(optimizer='rmsprop',\n-                      loss=None,\n-                      metrics=['mean_squared_error'])\n-\n-        # test train_on_batch\n-        out = model.train_on_batch(None, None)\n-        out = model.test_on_batch(None, None)\n-        out = model.predict_on_batch(None)\n-\n-        # test fit\n-        with pytest.raises(ValueError):\n-            out = model.fit(None, None, epochs=1, batch_size=10)\n-        out = model.fit(None, None, epochs=1, steps_per_epoch=1)\n-\n-        # define a generator to produce x=None and y=None\n-        @threadsafe_generator\n-        def data_tensors_generator():\n-            while True:\n-                yield (None, None)\n-\n-        generator = data_tensors_generator()\n-\n-        # test fit_generator for framework-native data tensors\n-        out = model.fit_generator(generator, epochs=1,\n-                                  steps_per_epoch=3)\n-\n-        # test evaluate_generator for framework-native data tensors\n-        out = model.evaluate_generator(generator, steps=3)\n-        out = model.evaluate(generator, steps=3)\n-\n-        # test fit with validation data\n-        with pytest.raises(ValueError):\n-            out = model.fit(None, None,\n-                            epochs=1,\n-                            steps_per_epoch=None,\n-                            validation_steps=2)\n-        out = model.fit(None, None,\n-                        epochs=1,\n-                        steps_per_epoch=2,\n-                        validation_steps=2)\n-\n-        # test evaluate\n-        with pytest.raises(ValueError):\n-            out = model.evaluate(None, None, batch_size=10)\n-        out = model.evaluate(None, None, steps=3)\n-\n-        # test predict\n-        with pytest.raises(ValueError):\n-            out = model.predict(None, batch_size=10)\n-        out = model.predict(None, steps=3)\n-        assert out.shape == (10 * 3, 4)\n-\n-        # Test multi-output model without external data.\n-        a = Input(tensor=tf.Variable(input_a_np, dtype=tf.float32))\n-        a_1 = Dense(4, name='dense_1')(a)\n-        a_2 = Dropout(0.5, name='dropout')(a_1)\n-        model = Model(a, [a_1, a_2])\n-        model.add_loss(K.mean(a_2))\n-        model.compile(optimizer='rmsprop',\n-                      loss=None,\n-                      metrics=['mean_squared_error'])\n-\n-        # test train_on_batch\n-        out = model.train_on_batch(None, None)\n-        out = model.test_on_batch(None, None)\n-        out = model.predict_on_batch(None)\n-\n-        # test fit\n-        with pytest.raises(ValueError):\n-            out = model.fit(None, None, epochs=1, batch_size=10)\n-        out = model.fit(None, None, epochs=1, steps_per_epoch=1)\n-\n-        # test fit with validation data\n-        with pytest.raises(ValueError):\n-            out = model.fit(None, None,\n-                            epochs=1,\n-                            steps_per_epoch=None,\n-                            validation_steps=2)\n-        out = model.fit(None, None,\n-                        epochs=1,\n-                        steps_per_epoch=2,\n-                        validation_steps=2)\n-\n-        # test evaluate\n-        with pytest.raises(ValueError):\n-            out = model.evaluate(None, None, batch_size=10)\n-        out = model.evaluate(None, None, steps=3)\n-\n-        # test predict\n-        with pytest.raises(ValueError):\n-            out = model.predict(None, batch_size=10)\n-        out = model.predict(None, steps=3)\n-        assert len(out) == 2\n-        assert out[0].shape == (10 * 3, 4)\n-        assert out[1].shape == (10 * 3, 4)\n-\n-\n-def DISABLED_test_target_tensors():\n-    # single-output, as list\n-    model = keras.models.Sequential()\n-    model.add(keras.layers.Dense(4, input_shape=(4,), name='dense'))\n-    input_val = np.random.random((10, 4))\n-    target_val = np.random.random((10, 4))\n-    target = keras.backend.variable(target_val)\n-    model.compile(optimizer='rmsprop', loss='mse', target_tensors=[target])\n-    model.train_on_batch(input_val, None)\n-\n-    # single-output, as dict\n-    model.compile(optimizer='rmsprop', loss='mse',\n-                  target_tensors={'dense': target})\n-    model.train_on_batch(input_val, None)\n-\n-    # single-output, as tensor\n-    model.compile(optimizer='rmsprop', loss='mse',\n-                  target_tensors=target)\n-    model.train_on_batch(input_val, None)\n-\n-    # test invalid arguments\n-    with pytest.raises(TypeError):\n-        model.compile(optimizer='rmsprop', loss='mse',\n-                      target_tensors=set())\n-    with pytest.raises(ValueError):\n-        model.compile(optimizer='rmsprop', loss='mse',\n-                      target_tensors=[target, target])\n-    with pytest.raises(ValueError):\n-        model.compile(optimizer='rmsprop', loss='mse',\n-                      target_tensors={'dense2': None})\n-    with pytest.raises(ValueError):\n-        model.compile(optimizer='rmsprop', loss='mse',\n-                      target_tensors=[target])\n-        model.train_on_batch(input_val, target_val)\n-\n-    # multi-output, as list\n-    input_val = np.random.random((10, 4))\n-    target_val_a = np.random.random((10, 4))\n-    target_val_b = np.random.random((10, 4))\n-    target_a = keras.backend.variable(target_val_a)\n-    target_b = keras.backend.variable(target_val_b)\n-\n-    inputs = keras.layers.Input(shape=(4,))\n-    output_a = keras.layers.Dense(4, name='dense_a')(inputs)\n-    output_b = keras.layers.Dense(4, name='dense_b')(inputs)\n-    model = keras.models.Model(inputs, [output_a, output_b])\n-    model.compile(optimizer='rmsprop', loss='mse',\n-                  target_tensors=[target_a, target_b])\n-    model.train_on_batch(input_val, None)\n-\n-    # multi-output, as dict\n-    model.compile(optimizer='rmsprop', loss='mse',\n-                  target_tensors={'dense_a': target_a,\n-                                  'dense_b': target_b})\n-    model.train_on_batch(input_val, None)\n-\n-    # multi-output, not enough target tensors when `target_tensors` is not a dict\n-    with pytest.raises(ValueError,\n-                       match='When passing a list as `target_tensors`, it should '\n-                             'have one entry per model output. The model has \\\\d '\n-                             'outputs, but you passed target_tensors='):\n-        model.compile(optimizer='rmsprop', loss='mse',\n-                      target_tensors=[target_a])\n-    with pytest.raises(ValueError,\n-                       match='The model has \\\\d outputs, but you passed a single '\n-                             'tensor as `target_tensors`. Expected a list or '\n-                             'a dict of tensors.'):\n-        model.compile(optimizer='rmsprop', loss='mse',\n-                      target_tensors=target_a)\n-\n-    # test with sample weights\n-    model.compile(optimizer='rmsprop', loss='mse',\n-                  target_tensors=[target_a, target_b])\n-    model.train_on_batch(input_val, None,\n-                         sample_weight={'dense_a': np.random.random((10,))})\n-\n-\n-@pytest.mark.skipif(K.backend() == 'tensorflow' and\n-                    tf.__version__.startswith('2'),\n-                    reason='Cannot have tensors as dict keys in TF2')\n-def DISABLED_test_model_custom_target_tensors():\n-    a = Input(shape=(3,), name='input_a')\n-    b = Input(shape=(3,), name='input_b')\n-\n-    a_2 = Dense(4, name='dense_1')(a)\n-    dp = Dropout(0.5, name='dropout')\n-    b_2 = dp(b)\n-\n-    y = K.placeholder([10, 4], name='y')\n-    y1 = K.placeholder([10, 3], name='y1')\n-    y2 = K.placeholder([7, 5], name='y2')\n-    model = Model([a, b], [a_2, b_2])\n-\n-    optimizer = 'rmsprop'\n-    loss = 'mse'\n-    loss_weights = [1., 0.5]\n-\n-    # test list of target tensors\n-    with pytest.raises(ValueError):\n-        model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n-                      sample_weight_mode=None, target_tensors=[y, y1, y2])\n-    model.compile(optimizer, loss, metrics=[], loss_weights=loss_weights,\n-                  sample_weight_mode=None, target_tensors=[y, y1])\n-    input_a_np = np.random.random((10, 3))\n-    input_b_np = np.random.random((10, 3))\n-\n-    output_a_np = np.random.random((10, 4))\n-    output_b_np = np.random.random((10, 3))\n-\n-    out = model.train_on_batch([input_a_np, input_b_np],\n-                               [output_a_np, output_b_np],\n-                               {y: np.random.random((10, 4)),\n-                                y1: np.random.random((10, 3))})\n-    # test dictionary of target_tensors\n-    with pytest.raises(ValueError):\n-        model.compile(optimizer, loss,\n-                      metrics=[],\n-                      loss_weights=loss_weights,\n-                      sample_weight_mode=None,\n-                      target_tensors={'does_not_exist': y2})\n-    # test dictionary of target_tensors\n-    model.compile(optimizer, loss,\n-                  metrics=[],\n-                  loss_weights=loss_weights,\n-                  sample_weight_mode=None,\n-                  target_tensors={'dense_1': y, 'dropout': y1})\n-    out = model.train_on_batch([input_a_np, input_b_np],\n-                               [output_a_np, output_b_np],\n-                               {y: np.random.random((10, 4)),\n-                                y1: np.random.random((10, 3))})\n-\n-    # test with custom placeholder as target\n-    pl_target_a = K.placeholder(shape=(None, 4))\n-    model.compile(optimizer='rmsprop', loss='mse',\n-                  target_tensors={'dense_1': pl_target_a})\n-    model.train_on_batch([input_a_np, input_b_np],\n-                         [output_a_np, output_b_np])\n-\n-\n-@pytest.mark.skipif(sys.version_info < (3,),\n-                    reason='Cannot catch warnings in python 2')\n-def DISABLED_test_trainable_weights_count_consistency():\n-    \"\"\"Tests the trainable weights consistency check of Model.\n-\n-    This verifies that a warning is shown if model.trainable is modified\n-    and the model is summarized/run without a new call to .compile()\n-\n-    Reproduce issue #8121\n-    \"\"\"\n-    a = Input(shape=(3,), name='input_a')\n-    model1 = Model(inputs=a, outputs=Dense(1)(a))\n-\n-    model1.trainable = False\n-    b = Input(shape=(3,), name='input_b')\n-    y = model1(b)\n-    model2 = Model(inputs=b, outputs=Dense(1)(y))\n-\n-    model2.compile(optimizer='adam', loss='mse')\n-\n-    model1.trainable = True\n-\n-    # Should warn on .summary()\n-    with pytest.warns(UserWarning) as w:\n-        model2.summary()\n-    warning_raised = any(['Discrepancy' in str(w_.message) for w_ in w])\n-    assert warning_raised, (\n-        'No warning raised when trainable is modified without .compile.')\n-\n-    # And on .fit()\n-    with pytest.warns(UserWarning) as w:\n-        model2.fit(x=np.zeros((5, 3)), y=np.zeros((5, 1)))\n-    warning_raised = any(['Discrepancy' in str(w_.message) for w_ in w])\n-    assert warning_raised, (\n-        'No warning raised when trainable is modified without .compile.')\n-\n-    # And shouldn't warn if we recompile\n-    model2.compile(optimizer='adam', loss='mse')\n-    with pytest.warns(None) as w:\n-        model2.summary()\n-    assert len(w) == 0, (\n-        'Warning raised even when .compile() is called after modifying .trainable')\n-\n-\n-def test_pandas_dataframe():\n-    input_a = Input(shape=(3,), name='input_a')\n-    input_b = Input(shape=(3,), name='input_b')\n-\n-    x = Dense(4, name='dense_1')(input_a)\n-    y = Dense(3, name='desne_2')(input_b)\n-\n-    model_1 = Model(inputs=input_a, outputs=x)\n-    model_2 = Model(inputs=[input_a, input_b], outputs=[x, y])\n-\n-    optimizer = 'rmsprop'\n-    loss = 'mse'\n-\n-    model_1.compile(optimizer=optimizer, loss=loss)\n-    model_2.compile(optimizer=optimizer, loss=loss)\n-\n-    input_a_df = pd.DataFrame(np.random.random((10, 3)))\n-    input_b_df = pd.DataFrame(np.random.random((10, 3)))\n-\n-    output_a_df = pd.DataFrame(np.random.random((10, 4)))\n-    output_b_df = pd.DataFrame(np.random.random((10, 3)))\n-\n-    model_1.fit(input_a_df,\n-                output_a_df)\n-    model_2.fit([input_a_df, input_b_df],\n-                [output_a_df, output_b_df])\n-    model_1.fit([input_a_df],\n-                [output_a_df])\n-    model_1.fit({'input_a': input_a_df},\n-                output_a_df)\n-    model_2.fit({'input_a': input_a_df, 'input_b': input_b_df},\n-                [output_a_df, output_b_df])\n-\n-    model_1.predict(input_a_df)\n-    model_2.predict([input_a_df, input_b_df])\n-    model_1.predict([input_a_df])\n-    model_1.predict({'input_a': input_a_df})\n-    model_2.predict({'input_a': input_a_df, 'input_b': input_b_df})\n-\n-    model_1.predict_on_batch(input_a_df)\n-    model_2.predict_on_batch([input_a_df, input_b_df])\n-    model_1.predict_on_batch([input_a_df])\n-    model_1.predict_on_batch({'input_a': input_a_df})\n-    model_2.predict_on_batch({'input_a': input_a_df, 'input_b': input_b_df})\n-\n-    model_1.evaluate(input_a_df,\n-                     output_a_df)\n-    model_2.evaluate([input_a_df, input_b_df],\n-                     [output_a_df, output_b_df])\n-    model_1.evaluate([input_a_df],\n-                     [output_a_df])\n-    model_1.evaluate({'input_a': input_a_df},\n-                     output_a_df)\n-    model_2.evaluate({'input_a': input_a_df, 'input_b': input_b_df},\n-                     [output_a_df, output_b_df])\n-\n-    model_1.train_on_batch(input_a_df,\n-                           output_a_df)\n-    model_2.train_on_batch([input_a_df, input_b_df],\n-                           [output_a_df, output_b_df])\n-    model_1.train_on_batch([input_a_df],\n-                           [output_a_df])\n-    model_1.train_on_batch({'input_a': input_a_df},\n-                           output_a_df)\n-    model_2.train_on_batch({'input_a': input_a_df, 'input_b': input_b_df},\n-                           [output_a_df, output_b_df])\n-\n-    model_1.test_on_batch(input_a_df,\n-                          output_a_df)\n-    model_2.test_on_batch([input_a_df, input_b_df],\n-                          [output_a_df, output_b_df])\n-    model_1.test_on_batch([input_a_df],\n-                          [output_a_df])\n-    model_1.test_on_batch({'input_a': input_a_df},\n-                          output_a_df)\n-    model_2.test_on_batch({'input_a': input_a_df, 'input_b': input_b_df},\n-                          [output_a_df, output_b_df])\n-\n-\n-@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Requires TensorFlow')\n-def DISABLED_test_training_and_eval_methods_on_backend_tensors_single_io():\n-    x = keras.layers.Input(shape=(3,), name='input')\n-    y = keras.layers.Dense(4, name='dense')(x)\n-    model = keras.Model(x, y)\n-\n-    optimizer = 'rmsprop'\n-    loss = 'mse'\n-    metrics = ['mae']\n-    model.compile(optimizer, loss, metrics=metrics)\n-\n-    inputs = keras.backend.zeros(shape=(10, 3))\n-    targets = keras.backend.zeros(shape=(10, 4))\n-\n-    model.fit(inputs, targets, epochs=1, steps_per_epoch=2, verbose=0)\n-    model.evaluate(inputs, targets, steps=2, verbose=0)\n-    model.predict(inputs, steps=2)\n-    model.train_on_batch(inputs, targets)\n-    model.test_on_batch(inputs, targets)\n-    model.fit(inputs, targets,\n-              epochs=1, steps_per_epoch=2, verbose=1,\n-              validation_data=(inputs, targets), validation_steps=2)\n-\n-\n-@pytest.mark.skipif(K.backend() != 'tensorflow', reason='Requires TensorFlow')\n-def DISABLED_test_training_and_eval_methods_on_backend_tensors_multi_io():\n-    a = keras.layers.Input(shape=(3,), name='input_a')\n-    b = keras.layers.Input(shape=(3,), name='input_b')\n-\n-    dense = keras.layers.Dense(4, name='dense')\n-    c = dense(a)\n-    d = dense(b)\n-    e = keras.layers.Dropout(0.5, name='dropout')(c)\n-\n-    model = keras.models.Model([a, b], [d, e])\n-\n-    optimizer = 'rmsprop'\n-    loss = 'mse'\n-    loss_weights = [1., 0.5]\n-    metrics = ['mae']\n-    model.compile(optimizer, loss, metrics=metrics, loss_weights=loss_weights)\n-\n-    input_a_tf = keras.backend.zeros(shape=(10, 3))\n-    input_b_tf = keras.backend.zeros(shape=(10, 3))\n-\n-    output_d_tf = keras.backend.zeros(shape=(10, 4))\n-    output_e_tf = keras.backend.zeros(shape=(10, 4))\n-\n-    model.fit(\n-        [input_a_tf, input_b_tf], [output_d_tf, output_e_tf],\n-        epochs=1,\n-        steps_per_epoch=2,\n-        verbose=0)\n-    with pytest.raises(ValueError,\n-                       match='should specify the `steps_per_epoch`'):\n-        model.fit(\n-            [input_a_tf, input_b_tf], [output_d_tf, output_e_tf],\n-            epochs=1,\n-            batch_size=5,\n-            verbose=0)\n-\n-    model.train_on_batch([input_a_tf, input_b_tf], [output_d_tf, output_e_tf])\n-\n-    # Test with dictionary inputs\n-    model.fit(\n-        {'input_a': input_a_tf,\n-         'input_b': input_b_tf},\n-        {'dense': output_d_tf,\n-         'dropout': output_e_tf},\n-        epochs=1,\n-        steps_per_epoch=2,\n-        verbose=0)\n-    model.fit(\n-        {'input_a': input_a_tf,\n-         'input_b': input_b_tf},\n-        {'dense': output_d_tf,\n-         'dropout': output_e_tf},\n-        validation_data=({'input_a': input_a_tf,\n-                          'input_b': input_b_tf},\n-                         {'dense': output_d_tf,\n-                          'dropout': output_e_tf}),\n-        epochs=1,\n-        steps_per_epoch=2,\n-        validation_steps=2,\n-        verbose=0)\n-    model.train_on_batch(\n-        {'input_a': input_a_tf,\n-         'input_b': input_b_tf},\n-        {'dense': output_d_tf,\n-         'dropout': output_e_tf})\n-\n-    # Test with validation data\n-    model.fit(\n-        [input_a_tf, input_b_tf], [output_d_tf, output_e_tf],\n-        validation_data=([input_a_tf, input_b_tf],\n-                         [output_d_tf, output_e_tf]),\n-        epochs=1,\n-        steps_per_epoch=2,\n-        validation_steps=2,\n-        verbose=0)\n-    # Test with validation split\n-    with pytest.raises(ValueError,\n-                       match='you cannot use `validation_split`'):\n-        model.fit(\n-            [input_a_tf, input_b_tf], [output_d_tf, output_e_tf],\n-            epochs=2,\n-            steps_per_epoch=2,\n-            verbose=0,\n-            validation_split=0.2,\n-            validation_steps=2)\n-\n-    # Test evaluation / prediction methods\n-    model.evaluate([input_a_tf, input_b_tf], [output_d_tf, output_e_tf],\n-                   steps=2, verbose=0)\n-    model.predict([input_a_tf, input_b_tf], steps=2)\n-    model.test_on_batch([input_a_tf, input_b_tf], [output_d_tf, output_e_tf])\n-\n-\n-def DISABLED_test_model_with_crossentropy_losses_channels_first():\n-    \"\"\"Tests use of all crossentropy losses with `channels_first`.\n-\n-    Tests `sparse_categorical_crossentropy`, `categorical_crossentropy`,\n-    and `binary_crossentropy`.\n-    Verifies that evaluate gives the same result with either\n-    `channels_first` or `channels_last` image_data_format.\n-    Tests PR #9715.\n-    \"\"\"\n-\n-    def prepare_simple_model(input_tensor, loss_name, target):\n-        axis = 1 if K.image_data_format() == 'channels_first' else -1\n-        if loss_name == 'sparse_categorical_crossentropy':\n-            loss = lambda y_true, y_pred: K.sparse_categorical_crossentropy(\n-                y_true, y_pred, axis=axis)\n-            num_channels = np.amax(target) + 1\n-            activation = 'softmax'\n-        elif loss_name == 'categorical_crossentropy':\n-            loss = lambda y_true, y_pred: K.categorical_crossentropy(\n-                y_true, y_pred, axis=axis)\n-            num_channels = target.shape[axis]\n-            activation = 'softmax'\n-        elif loss_name == 'binary_crossentropy':\n-            loss = lambda y_true, y_pred: K.binary_crossentropy(y_true, y_pred)\n-            num_channels = target.shape[axis]\n-            activation = 'sigmoid'\n-        predictions = Conv2D(num_channels, 1, activation=activation,\n-                             kernel_initializer='ones',\n-                             bias_initializer='ones')(input_tensor)\n-        simple_model = Model(inputs=input_tensor, outputs=predictions)\n-        simple_model.compile(optimizer='rmsprop', loss=loss)\n-        return simple_model\n-\n-    losses_to_test = ['sparse_categorical_crossentropy',\n-                      'categorical_crossentropy', 'binary_crossentropy']\n-\n-    data_channels_first = np.array([[[[8., 7.1, 0.], [4.5, 2.6, 0.55],\n-                                      [0.9, 4.2, 11.2]]]], dtype=np.float32)\n-    # Labels for testing 4-class sparse_categorical_crossentropy, 4-class\n-    # categorical_crossentropy, and 2-class binary_crossentropy:\n-    labels_channels_first = [np.array([[[[0, 1, 3], [2, 1, 0], [2, 2, 1]]]]),\n-                             np.array([[[[0, 1, 0], [0, 1, 0], [0, 0, 0]],\n-                                        [[1, 0, 0], [0, 0, 1], [0, 1, 0]],\n-                                        [[0, 0, 0], [1, 0, 0], [0, 0, 1]],\n-                                        [[0, 0, 1], [0, 0, 0], [1, 0, 0]]]]),\n-                             np.array([[[[0, 1, 0], [0, 1, 0], [0, 0, 1]],\n-                                        [[1, 0, 1], [1, 0, 1], [1, 1, 0]]]])]\n-    # Compute one loss for each loss function in the list `losses_to_test`:\n-    loss_channels_last = [0., 0., 0.]\n-    loss_channels_first = [0., 0., 0.]\n-\n-    old_data_format = K.image_data_format()\n-\n-    # Evaluate a simple network with channels last, with all three loss\n-    # functions:\n-    K.set_image_data_format('channels_last')\n-    data = np.moveaxis(data_channels_first, 1, -1)\n-    for index, loss_function in enumerate(losses_to_test):\n-        labels = np.moveaxis(labels_channels_first[index], 1, -1)\n-        inputs = Input(shape=(3, 3, 1))\n-        model = prepare_simple_model(inputs, loss_function, labels)\n-        loss_channels_last[index] = model.evaluate(x=data, y=labels,\n-                                                   batch_size=1, verbose=0)\n-\n-    # Evaluate the same network with channels first, with all three loss\n-    # functions:\n-    K.set_image_data_format('channels_first')\n-    assert K.image_data_format() == 'channels_first'\n-    data = data_channels_first\n-    for index, loss_function in enumerate(losses_to_test):\n-        labels = labels_channels_first[index]\n-        inputs = Input(shape=(1, 3, 3))\n-        model = prepare_simple_model(inputs, loss_function, labels)\n-        loss_channels_first[index] = model.evaluate(x=data, y=labels,\n-                                                    batch_size=1, verbose=0)\n-\n-    K.set_image_data_format(old_data_format)\n-\n-    assert_allclose(loss_channels_first, loss_channels_last,\n-                    err_msg='{}{}'.format('Computed different losses for ',\n-                                          'channels_first and channels_last.'))\n-\n-\n-def DISABLED_test_dynamic_set_inputs():\n-    model = Sequential()\n-    model.add(Dense(16, input_dim=32))\n-    model.add(Activation('relu'))\n-\n-    model2 = Sequential()\n-    model2.add(model.layers[-1])\n-    model2.add(Dense(8))\n-    preds2 = model2.predict(np.random.random((1, 32)))\n-    assert preds2.shape == (1, 8)\n-\n-    model3 = Model(inputs=model.inputs, outputs=model.outputs)\n-    with pytest.raises(ValueError):\n-        model3._set_inputs(model.inputs)\n-\n-    model3.inputs = None\n-    model3._set_inputs(model.inputs)\n-    preds3 = model3.predict(np.random.random((1, 32)))\n-    assert preds3.shape == (1, 16)\n-\n-    model3.inputs = None\n-    model3._set_inputs(model.input)\n-    preds3 = model3.predict(np.random.random((1, 32)))\n-    assert preds3.shape == (1, 16)\n-\n-    aux_input = Input(shape=(5,), name='aux_input')\n-    aux_model = Dense(3)(aux_input)\n-    model4 = Model(inputs=model.inputs + [aux_input],\n-                   outputs=Concatenate()(model.outputs + [aux_model]))\n-    model4.inputs = None\n-    model4._set_inputs(model.inputs + [aux_input])\n-    preds4 = model4.predict([np.random.random((1, 32)),\n-                             np.random.random((1, 5))])\n-    assert preds4.shape == (1, 19)\n-\n-\n-def DISABLED_test_sample_weights():\n-    y = np.array([0, 1, 0, 0, 2])\n-    sample_weights = np.array([0.5, 1., 1., 0., 2.])\n-    class_weights = {0: 0.5, 1: 1., 2: 1.5}\n-\n-    # Only `sample_weights`.\n-    weights = training_utils.standardize_weights(y, sample_weights)\n-    assert np.allclose(weights, sample_weights)\n-\n-    # Only `class_weights`.\n-    weights = training_utils.standardize_weights(y, class_weight=class_weights)\n-    assert np.allclose(weights, np.array([0.5, 1., 0.5, 0.5, 1.5]))\n-\n-    # Both 'sample_weights` and 'class_weights`.\n-    weights = training_utils.standardize_weights(y, sample_weights,\n-                                                 class_weights)\n-    expected = sample_weights * np.array([0.5, 1., 0.5, 0.5, 1.5])\n-    assert np.allclose(weights, expected)\n-\n-\n-def test_validation_freq():\n-    model = Sequential([Dense(1)])\n-    model.compile('sgd', 'mse')\n-\n-    def _gen():\n-        while True:\n-            yield np.ones((2, 10)), np.ones((2, 1))\n-\n-    x, y = np.ones((10, 10)), np.ones((10, 1))\n-\n-    class ValCounter(Callback):\n-\n-        def __init__(self):\n-            self.val_runs = 0\n-\n-        def on_test_begin(self, logs=None):\n-            self.val_runs += 1\n-\n-    # Test in training_arrays.py\n-    val_counter = ValCounter()\n-    model.fit(\n-        x,\n-        y,\n-        batch_size=2,\n-        epochs=4,\n-        validation_data=(x, y),\n-        validation_freq=2,\n-        callbacks=[val_counter])\n-    assert val_counter.val_runs == 2\n-\n-    # Test in training_generator.py\n-    val_counter = ValCounter()\n-    model.fit_generator(\n-        _gen(),\n-        epochs=4,\n-        steps_per_epoch=5,\n-        validation_data=(x, y),\n-        validation_freq=[4, 2, 2, 1],\n-        callbacks=[val_counter])\n-    assert val_counter.val_runs == 3\n-\n-\n-def test_loss_correctness():\n-    class Bias(Layer):\n-\n-        def build(self, input_shape):\n-            self.bias = self.add_weight('bias', (1,), initializer='zeros')\n-\n-        def call(self, inputs):\n-            return inputs + self.bias\n-\n-    inp = Input(shape=(1,))\n-    out = Bias()(inp)\n-    model = Model(inp, out)\n-    model.compile(\n-        keras.optimizers.SGD(lr=0.1),\n-        loss=keras.losses.MeanAbsoluteError())\n-\n-    x = np.array([[0.], [1.], [2.]])\n-    y = np.array([[0.5], [2.], [3.5]])\n-    history = model.fit(x, y, batch_size=3, epochs=5)\n-    np.allclose(history.history['loss'], [1., 0.9, 0.8, 0.7, 0.6])\n-\n-\n-def DISABLED_test_model_metrics_list():\n-\n-    class LayerWithAddMetric(Layer):\n-\n-        def __init__(self):\n-            super(LayerWithAddMetric, self).__init__()\n-            self.dense = keras.layers.Dense(1, kernel_initializer='ones')\n-\n-        def __call__(self, inputs):\n-            outputs = self.dense(inputs)\n-            return outputs\n-\n-    class LayerWithNestedAddMetricLayer(Layer):\n-\n-        def __init__(self):\n-            super(LayerWithNestedAddMetricLayer, self).__init__()\n-            self.layer = LayerWithAddMetric()\n-\n-        def call(self, inputs):\n-            outputs = self.layer(inputs)\n-            self.add_metric(K.sum(outputs), name='metric_4')\n-            return outputs\n-\n-    x = Input(shape=(1,))\n-    y = LayerWithNestedAddMetricLayer()(x)\n-\n-    model = keras.models.Model(x, y)\n-    model.add_metric(K.sum(y), name='metric_2')\n-    model.add_metric(metrics.Mean(name='metric_3')(y))\n-\n-    model.compile(\n-        'sgd',\n-        loss='mse',\n-        metrics=[metrics.MeanSquaredError('metric_1')])\n-\n-    # # Verify that the metrics added using `compile` and `add_metric` API are\n-    # # included\n-    # for m1, m2 in zip([m.name for m in model._compile_metrics], ['metric_1']):\n-    #     assert m1 == m2\n-\n-    for m1, m2 in zip(\n-            [m.name for m in model.metrics],\n-            ['metric_1', 'metric_2', 'metric_3', 'metric_4']):\n-        assert m1 == m2\n-\n-\n-def test_model_metrics_list_in_call():\n-\n-    class TestModel(Model):\n-\n-        def __init__(self):\n-            super(TestModel, self).__init__(name='test_model')\n-            self.dense1 = keras.layers.Dense(2)\n-\n-        def call(self, x):\n-            self.add_metric(K.sum(x), name='metric_2')\n-            return self.dense1(x)\n-\n-    model = TestModel()\n-    model.compile(\n-        loss='mse',\n-        optimizer='adam',\n-        metrics=[metrics.MeanSquaredError('metric_1')])\n-    x = np.ones(shape=(10, 1))\n-    y = np.ones(shape=(10, 2))\n-    model.fit(x, y, epochs=2, batch_size=5, validation_data=(x, y))\n-\n-    # # Verify that the metrics added using `compile` and `add_metric` API are\n-    # # included\n-    # for m1, m2 in zip([m.name for m in model._compile_metrics], ['metric_1']):\n-    #     assert m1 == m2\n-\n-    names = [m.name for m in model.metrics]\n-    assert 'metric_1' in names\n-    assert 'metric_2' in names\n-\n-\n-def test_duplicate_metric_name_in_add_metric():\n-\n-    class TestModel(Model):\n-\n-        def __init__(self):\n-            super(TestModel, self).__init__(name='test_model')\n-            self.dense1 = keras.layers.Dense(2, kernel_initializer='ones')\n-            self.mean = metrics.Mean(name='metric_1')\n-            self.mean2 = metrics.Mean(name='metric_1')\n-\n-        def call(self, x):\n-            self.add_metric(self.mean(x), name='metric_1')\n-            return self.dense1(x)\n-\n-    model = TestModel()\n-    model.compile(loss='mse', optimizer='adam')\n-\n-    x = np.ones(shape=(10, 1))\n-    y = np.ones(shape=(10, 2))\n-    with pytest.raises(ValueError):\n-        model.fit(x, y, epochs=2, batch_size=5, validation_data=(x, y))\n-\n-\n-def test_add_metric_on_model():\n-    x = Input(shape=(1,))\n-    y = Dense(1, kernel_initializer='ones', trainable=False)(x)\n-    model = Model(x, y)\n-    model.add_metric(K.sum(y), name='metric_1')\n-    model.add_metric(K.mean(y), name='metric_2')\n-    model.compile('sgd', loss='mse', metrics=['mse'])\n-\n-    inputs = np.ones(shape=(10, 1))\n-    targets = np.zeros(shape=(10, 1))\n-    history = model.fit(\n-        inputs,\n-        targets,\n-        epochs=2,\n-        batch_size=5,\n-        validation_data=(inputs, targets))\n-    assert history.history['metric_1'][-1] == 5\n-    assert history.history['val_metric_1'][-1] == 5\n-\n-    assert history.history['metric_2'][-1] == 1\n-    assert history.history['val_metric_2'][-1] == 1\n-\n-    eval_results = model.evaluate(inputs, targets, batch_size=5)\n-    assert eval_results[-2] == 5\n-    assert eval_results[-1] == 1\n-\n-    model.predict(inputs, batch_size=5)\n-    model.train_on_batch(inputs, targets)\n-    model.test_on_batch(inputs, targets)\n-\n-\n-def test_add_metric_in_model_call():\n-\n-    class TestModel(Model):\n-\n-        def __init__(self):\n-            super(TestModel, self).__init__(name='test_model')\n-            self.dense1 = keras.layers.Dense(2, kernel_initializer='ones')\n-            self.mean = metrics.Mean(name='metric_1')\n-\n-        def call(self, x):\n-            self.add_metric(K.sum(x), name='metric_2')\n-            # Provide same name as in the instance created in __init__\n-            # for eager mode\n-            self.add_metric(self.mean(x), name='metric_1')\n-            return self.dense1(x)\n-\n-    model = TestModel()\n-    model.compile(loss='mse', optimizer='sgd')\n-\n-    x = np.ones(shape=(10, 1))\n-    y = np.ones(shape=(10, 2))\n-    history = model.fit(x, y, epochs=2, batch_size=5, validation_data=(x, y))\n-    assert np.isclose(history.history['metric_1'][-1], 1, 0)\n-    assert np.isclose(history.history['val_metric_1'][-1], 1, 0)\n-    assert np.isclose(history.history['metric_2'][-1], 5, 0)\n-    assert np.isclose(history.history['val_metric_2'][-1], 5, 0)\n-\n-    eval_results = model.evaluate(x, y, batch_size=5)\n-    assert np.isclose(eval_results[1], 1, 0)\n-    assert np.isclose(eval_results[2], 5, 0)\n-\n-    model.predict(x, batch_size=5)\n-    model.train_on_batch(x, y)\n-    model.test_on_batch(x, y)\n-\n-\n-def test_multiple_add_metric_calls():\n-\n-    class TestModel(Model):\n-\n-        def __init__(self):\n-            super(TestModel, self).__init__(name='test_model')\n-            self.dense1 = keras.layers.Dense(2, kernel_initializer='ones')\n-            self.mean1 = metrics.Mean(name='metric_1')\n-            self.mean2 = metrics.Mean(name='metric_2')\n-\n-        def call(self, x):\n-            self.add_metric(self.mean2(x), name='metric_2')\n-            self.add_metric(self.mean1(x), name='metric_1')\n-            self.add_metric(K.sum(x), name='metric_3')\n-            return self.dense1(x)\n-\n-    model = TestModel()\n-    model.compile(loss='mse', optimizer='sgd')\n-\n-    x = np.ones(shape=(10, 1))\n-    y = np.ones(shape=(10, 2))\n-    history = model.fit(x, y, epochs=2, batch_size=5, validation_data=(x, y))\n-    assert np.isclose(history.history['metric_1'][-1], 1, 0)\n-    assert np.isclose(history.history['metric_2'][-1], 1, 0)\n-    assert np.isclose(history.history['metric_3'][-1], 5, 0)\n-\n-    eval_results = model.evaluate(x, y, batch_size=5)\n-    assert np.allclose(eval_results[1:4], [1, 1, 5], 0.1)\n-\n-    model.predict(x, batch_size=5)\n-    model.train_on_batch(x, y)\n-    model.test_on_batch(x, y)\n-\n-\n-def test_add_metric_in_layer_call():\n-\n-    class TestLayer(Layer):\n-\n-        def build(self, input_shape):\n-            self.a = self.add_weight(\n-                'a', (1, 1), initializer='ones', trainable=False)\n-            self.built = True\n-\n-        def call(self, inputs):\n-            self.add_metric(K.sum(inputs), name='metric_1')\n-            return inputs + 1\n-\n-    inp = Input(shape=(1,))\n-    x = TestLayer(input_shape=(1,))(inp)\n-    x = keras.layers.Dense(2, kernel_initializer='ones')(x)\n-\n-    model = Model(inp, x)\n-    model.compile('adam', loss='mse')\n-\n-    x = np.ones(shape=(10, 1))\n-    y = np.ones(shape=(10, 2))\n-    history = model.fit(x, y, epochs=2, batch_size=5, validation_data=(x, y))\n-    assert np.isclose(history.history['metric_1'][-1], 5, 0)\n-    assert np.isclose(history.history['val_metric_1'][-1], 5, 0)\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,64 +0,0 @@\n-import pytest\n-from keras.utils.test_utils import layer_test\n-from keras import layers\n-from keras import backend as K\n-\n-\n-@pytest.mark.parametrize('activation_layer',\n-                         [layers.LeakyReLU,\n-                          layers.ELU])\n-@pytest.mark.parametrize('alpha', [0., .5, -1.])\n-def test_linear_unit_activations(activation_layer,\n-                                 alpha):\n-    layer_test(activation_layer, kwargs={'alpha': alpha},\n-               input_shape=(2, 3, 4))\n-\n-\n-def test_prelu():\n-    layer_test(layers.PReLU, kwargs={},\n-               input_shape=(2, 3, 4))\n-\n-\n-def test_prelu_share():\n-    layer_test(layers.PReLU, kwargs={'shared_axes': 1},\n-               input_shape=(2, 3, 4))\n-\n-\n-def test_thresholded_relu():\n-    layer_test(layers.ThresholdedReLU, kwargs={'theta': 0.5},\n-               input_shape=(2, 3, 4))\n-\n-\n-@pytest.mark.parametrize('axis', [1, -1])\n-def test_softmax(axis):\n-    layer_test(layers.Softmax, kwargs={'axis': axis},\n-               input_shape=(2, 3, 4))\n-\n-\n-def test_relu():\n-    layer_test(layers.ReLU,\n-               kwargs={'max_value': 10,\n-                       'negative_slope': 0.2,\n-                       'threshold': 3.0},\n-               input_shape=(2, 3, 4))\n-    layer_test(layers.ReLU,\n-               kwargs={'max_value': 6},\n-               input_shape=(2, 3, 4))\n-    layer_test(layers.ReLU,\n-               kwargs={'negative_slope': 0.2},\n-               input_shape=(2, 3, 4))\n-\n-    # max_value of ReLU layer cannot be negative value\n-    with pytest.raises(ValueError):\n-        layer_test(layers.ReLU, kwargs={'max_value': -2.0},\n-                   input_shape=(2, 3, 4))\n-\n-    # negative_slope of ReLU layer cannot be negative value\n-    with pytest.raises(ValueError):\n-        layer_test(layers.ReLU, kwargs={'negative_slope': -2.0},\n-                   input_shape=(2, 3, 4))\n-\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,168 +0,0 @@\n-import pytest\n-import numpy as np\n-from numpy.testing import assert_allclose\n-\n-from keras import backend as K\n-from keras.models import Sequential, Model\n-from keras.layers import convolutional_recurrent, Input, Masking, Lambda\n-from keras.utils.test_utils import layer_test\n-from keras import regularizers\n-\n-num_row = 3\n-num_col = 3\n-filters = 2\n-num_samples = 1\n-input_channel = 2\n-input_num_row = 5\n-input_num_col = 5\n-sequence_len = 2\n-\n-\n-@pytest.mark.parametrize('data_format', ['channels_first', 'channels_last'])\n-@pytest.mark.parametrize('return_sequences', [True, False])\n-@pytest.mark.parametrize('use_mask', [True, False])\n-def DISABLED_test_convolutional_recurrent(data_format, return_sequences, use_mask):\n-\n-    class Masking5D(Masking):\n-        \"\"\"Regular masking layer returns wrong shape of mask for RNN\"\"\"\n-        def compute_mask(self, inputs, mask=None):\n-            return K.any(K.not_equal(inputs, 0.), axis=[2, 3, 4])\n-\n-    if data_format == 'channels_first':\n-        inputs = np.random.rand(num_samples, sequence_len,\n-                                input_channel,\n-                                input_num_row, input_num_col)\n-    else:\n-        inputs = np.random.rand(num_samples, sequence_len,\n-                                input_num_row, input_num_col,\n-                                input_channel)\n-\n-    # test for return state:\n-    x = Input(batch_shape=inputs.shape)\n-    kwargs = {'data_format': data_format,\n-              'return_sequences': return_sequences,\n-              'return_state': True,\n-              'stateful': True,\n-              'filters': filters,\n-              'kernel_size': (num_row, num_col),\n-              'padding': 'valid'}\n-    layer = convolutional_recurrent.ConvLSTM2D(**kwargs)\n-    layer.build(inputs.shape)\n-    if use_mask:\n-        outputs = layer(Masking5D()(x))\n-    else:\n-        outputs = layer(x)\n-    output, states = outputs[0], outputs[1:]\n-    assert len(states) == 2\n-    model = Model(x, states[0])\n-    state = model.predict(inputs)\n-    np.testing.assert_allclose(K.eval(layer.states[0]), state, atol=1e-4)\n-\n-    # test for output shape:\n-    output = layer_test(convolutional_recurrent.ConvLSTM2D,\n-                        kwargs={'data_format': data_format,\n-                                'return_sequences': return_sequences,\n-                                'filters': filters,\n-                                'kernel_size': (num_row, num_col),\n-                                'padding': 'valid'},\n-                        input_shape=inputs.shape)\n-\n-\n-def test_convolutional_recurrent_statefulness():\n-\n-    data_format = 'channels_last'\n-    return_sequences = False\n-    inputs = np.random.rand(num_samples, sequence_len,\n-                            input_num_row, input_num_col,\n-                            input_channel)\n-    # Tests for statefulness\n-    model = Sequential()\n-    kwargs = {'data_format': data_format,\n-              'return_sequences': return_sequences,\n-              'filters': filters,\n-              'kernel_size': (num_row, num_col),\n-              'stateful': True,\n-              'batch_input_shape': inputs.shape,\n-              'padding': 'same'}\n-    layer = convolutional_recurrent.ConvLSTM2D(**kwargs)\n-\n-    model.add(layer)\n-    model.compile(optimizer='sgd', loss='mse')\n-    out1 = model.predict(np.ones_like(inputs))\n-\n-    # train once so that the states change\n-    model.train_on_batch(np.ones_like(inputs),\n-                         np.random.random(out1.shape))\n-    out2 = model.predict(np.ones_like(inputs))\n-\n-    # if the state is not reset, output should be different\n-    assert(out1.max() != out2.max())\n-\n-    # check that output changes after states are reset\n-    # (even though the model itself didn't change)\n-    layer.reset_states()\n-    out3 = model.predict(np.ones_like(inputs))\n-    assert(out2.max() != out3.max())\n-\n-    # check that container-level reset_states() works\n-    model.reset_states()\n-    out4 = model.predict(np.ones_like(inputs))\n-    assert_allclose(out3, out4, atol=1e-5)\n-\n-    # check that the call to `predict` updated the states\n-    out5 = model.predict(np.ones_like(inputs))\n-    assert(out4.max() != out5.max())\n-\n-    # cntk doesn't support eval convolution with static\n-    # variable, will enable it later\n-    if K.backend() != 'cntk':\n-        # check regularizers\n-        kwargs = {'data_format': data_format,\n-                  'return_sequences': return_sequences,\n-                  'kernel_size': (num_row, num_col),\n-                  'stateful': True,\n-                  'filters': filters,\n-                  'batch_input_shape': inputs.shape,\n-                  'kernel_regularizer': regularizers.L1L2(l1=0.01),\n-                  'recurrent_regularizer': regularizers.L1L2(l1=0.01),\n-                  'bias_regularizer': 'l2',\n-                  'activity_regularizer': 'l2',\n-                  'kernel_constraint': 'max_norm',\n-                  'recurrent_constraint': 'max_norm',\n-                  'bias_constraint': 'max_norm',\n-                  'padding': 'same'}\n-\n-        layer = convolutional_recurrent.ConvLSTM2D(**kwargs)\n-        layer.build(inputs.shape)\n-        assert len(layer.losses) == 3\n-        assert layer.activity_regularizer\n-        output = layer(K.variable(np.ones(inputs.shape)))\n-        assert len(layer.losses) == 4\n-        K.eval(output)\n-\n-    # check dropout\n-    layer_test(convolutional_recurrent.ConvLSTM2D,\n-               kwargs={'data_format': data_format,\n-                       'return_sequences': return_sequences,\n-                       'filters': filters,\n-                       'kernel_size': (num_row, num_col),\n-                       'padding': 'same',\n-                       'dropout': 0.1,\n-                       'recurrent_dropout': 0.1},\n-               input_shape=inputs.shape)\n-\n-    # check state initialization\n-    layer = convolutional_recurrent.ConvLSTM2D(\n-        filters=filters, kernel_size=(num_row, num_col),\n-        data_format=data_format, return_sequences=return_sequences)\n-    layer.build(inputs.shape)\n-    x = Input(batch_shape=inputs.shape)\n-    initial_state = layer.get_initial_state(x)\n-    y = layer(x, initial_state=initial_state)\n-    model = Model(x, y)\n-    assert (model.predict(inputs).shape ==\n-            layer.compute_output_shape(inputs.shape))\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,1103 +0,0 @@\n-import pytest\n-import numpy as np\n-from numpy.testing import assert_allclose\n-\n-from keras.utils.test_utils import layer_test\n-from keras import backend as K\n-from keras.layers import convolutional\n-from keras.models import Sequential\n-\n-\n-# TensorFlow does not support full convolution.\n-if K.backend() == 'theano':\n-    _convolution_paddings = ['valid', 'same', 'full']\n-else:\n-    _convolution_paddings = ['valid', 'same']\n-\n-\n-@pytest.mark.parametrize(\n-    'layer_kwargs,input_length,expected_output',\n-    [\n-        # Causal\n-        ({'filters': 1, 'kernel_size': 2, 'dilation_rate': 1, 'padding': 'causal',\n-          'kernel_initializer': 'ones', 'use_bias': False},\n-         4, [[[0], [1], [3], [5]]]),\n-        # Non-causal\n-        ({'filters': 1, 'kernel_size': 2, 'dilation_rate': 1, 'padding': 'valid',\n-          'kernel_initializer': 'ones', 'use_bias': False},\n-         4, [[[1], [3], [5]]]),\n-        # Causal dilated with larger kernel size\n-        ({'filters': 1, 'kernel_size': 3, 'dilation_rate': 2, 'padding': 'causal',\n-          'kernel_initializer': 'ones', 'use_bias': False},\n-         10, np.float32([[[0], [1], [2], [4], [6], [9], [12], [15], [18], [21]]])),\n-    ]\n-)\n-def test_causal_dilated_conv(layer_kwargs, input_length, expected_output):\n-    input_data = np.reshape(np.arange(input_length, dtype='float32'),\n-                            (1, input_length, 1))\n-    layer_test(convolutional.Conv1D, input_data=input_data,\n-               kwargs=layer_kwargs, expected_output=expected_output)\n-\n-\n-@pytest.mark.parametrize(\n-    'padding,strides',\n-    [(padding, strides)\n-     for padding in _convolution_paddings\n-     for strides in [1, 2]\n-     if not (padding == 'same' and strides != 1)]\n-)\n-def test_conv_1d(padding, strides):\n-    batch_size = 2\n-    steps = 8\n-    input_dim = 2\n-    kernel_size = 3\n-    filters = 3\n-\n-    layer_test(convolutional.Conv1D,\n-               kwargs={'filters': filters,\n-                       'kernel_size': kernel_size,\n-                       'padding': padding,\n-                       'strides': strides},\n-               input_shape=(batch_size, steps, input_dim))\n-\n-    layer_test(convolutional.Conv1D,\n-               kwargs={'filters': filters,\n-                       'kernel_size': kernel_size,\n-                       'padding': padding,\n-                       'kernel_regularizer': 'l2',\n-                       'bias_regularizer': 'l2',\n-                       'activity_regularizer': 'l2',\n-                       'kernel_constraint': 'max_norm',\n-                       'bias_constraint': 'max_norm',\n-                       'strides': strides},\n-               input_shape=(batch_size, steps, input_dim))\n-\n-\n-def test_conv_1d_dilation():\n-    batch_size = 2\n-    steps = 8\n-    input_dim = 2\n-    kernel_size = 3\n-    filters = 3\n-    padding = _convolution_paddings[-1]\n-\n-    layer_test(convolutional.Conv1D,\n-               kwargs={'filters': filters,\n-                       'kernel_size': kernel_size,\n-                       'padding': padding,\n-                       'dilation_rate': 2},\n-               input_shape=(batch_size, steps, input_dim))\n-\n-\n-def DISABLED_test_conv_1d_channels_first():\n-    batch_size = 2\n-    steps = 8\n-    input_dim = 2\n-    kernel_size = 3\n-    filters = 3\n-\n-    layer_test(convolutional.Conv1D,\n-               kwargs={'filters': filters,\n-                       'kernel_size': kernel_size,\n-                       'data_format': 'channels_first'},\n-               input_shape=(batch_size, input_dim, steps))\n-\n-\n-@pytest.mark.parametrize(\n-    'strides,padding',\n-    [(strides, padding)\n-     for padding in _convolution_paddings\n-     for strides in [(1, 1), (2, 2)]\n-     if not (padding == 'same' and strides != (1, 1))]\n-)\n-def test_convolution_2d(strides, padding):\n-    num_samples = 2\n-    filters = 2\n-    stack_size = 3\n-    kernel_size = (3, 2)\n-    num_row = 7\n-    num_col = 6\n-\n-    layer_test(convolutional.Conv2D,\n-               kwargs={'filters': filters,\n-                       'kernel_size': kernel_size,\n-                       'padding': padding,\n-                       'strides': strides,\n-                       'data_format': 'channels_last'},\n-               input_shape=(num_samples, stack_size, num_row, num_col))\n-\n-\n-def test_convolution_2d_dilation():\n-    num_samples = 2\n-    filters = 2\n-    stack_size = 3\n-    kernel_size = (3, 2)\n-    num_row = 7\n-    num_col = 6\n-    padding = 'valid'\n-\n-    layer_test(convolutional.Conv2D,\n-               kwargs={'filters': filters,\n-                       'kernel_size': kernel_size,\n-                       'padding': padding,\n-                       'dilation_rate': (2, 2)},\n-               input_shape=(num_samples, num_row, num_col, stack_size))\n-\n-\n-def test_convolution_2d_invalid():\n-    filters = 2\n-    padding = _convolution_paddings[-1]\n-    kernel_size = (3, 2)\n-\n-    with pytest.raises(ValueError):\n-        model = Sequential([convolutional.Conv2D(\n-            filters=filters, kernel_size=kernel_size, padding=padding,\n-            batch_input_shape=(None, None, 5, None))])\n-\n-\n-@pytest.mark.parametrize(\n-    'padding,out_padding,strides',\n-    [(padding, out_padding, strides)\n-     for padding in _convolution_paddings\n-     for out_padding in [None, (0, 0), (1, 1)]\n-     for strides in [(1, 1), (2, 2)]\n-     if (not (padding == 'same' and strides != (1, 1))\n-         and not(strides == (1, 1) and out_padding == (1, 1)))]\n-)\n-def test_conv2d_transpose(padding, out_padding, strides):\n-    num_samples = 2\n-    filters = 2\n-    stack_size = 3\n-    num_row = 5\n-    num_col = 6\n-\n-    layer_test(convolutional.Conv2DTranspose,\n-               kwargs={'filters': filters,\n-                       'kernel_size': 3,\n-                       'padding': padding,\n-                       'output_padding': out_padding,\n-                       'strides': strides,\n-                       'data_format': 'channels_last'},\n-               input_shape=(num_samples, num_row, num_col, stack_size),\n-               fixed_batch_size=True)\n-\n-\n-def test_conv2d_transpose_dilation():\n-\n-    layer_test(convolutional.Conv2DTranspose,\n-               kwargs={'filters': 2,\n-                       'kernel_size': 3,\n-                       'padding': 'same',\n-                       'data_format': 'channels_last',\n-                       'dilation_rate': (2, 2)},\n-               input_shape=(2, 5, 6, 3))\n-\n-    # Check dilated conv transpose returns expected output\n-    input_data = np.arange(48).reshape((1, 4, 4, 3)).astype(np.float32)\n-    expected_output = np.float32([[192, 228, 192, 228],\n-                                  [336, 372, 336, 372],\n-                                  [192, 228, 192, 228],\n-                                  [336, 372, 336, 372]]).reshape((1, 4, 4, 1))\n-\n-    layer_test(convolutional.Conv2DTranspose,\n-               input_data=input_data,\n-               kwargs={'filters': 1,\n-                       'kernel_size': 3,\n-                       'padding': 'same',\n-                       'data_format': 'channels_last',\n-                       'dilation_rate': (2, 2),\n-                       'kernel_initializer': 'ones'},\n-               expected_output=expected_output)\n-\n-\n-def test_conv2d_transpose_channels_first():\n-    num_samples = 2\n-    filters = 2\n-    stack_size = 3\n-    num_row = 5\n-    num_col = 6\n-    padding = 'valid'\n-    strides = (2, 2)\n-\n-    layer_test(convolutional.Conv2DTranspose,\n-               kwargs={'filters': filters,\n-                       'kernel_size': 3,\n-                       'padding': padding,\n-                       'data_format': 'channels_first',\n-                       'activation': None,\n-                       'kernel_regularizer': 'l2',\n-                       'bias_regularizer': 'l2',\n-                       'activity_regularizer': 'l2',\n-                       'kernel_constraint': 'max_norm',\n-                       'bias_constraint': 'max_norm',\n-                       'strides': strides},\n-               input_shape=(num_samples, stack_size, num_row, num_col),\n-               fixed_batch_size=True)\n-\n-\n-def test_conv2d_transpose_invalid():\n-    filters = 2\n-    stack_size = 3\n-    num_row = 5\n-    num_col = 6\n-    padding = 'valid'\n-\n-    with pytest.raises(ValueError):\n-        model = Sequential([convolutional.Conv2DTranspose(\n-            filters=filters,\n-            kernel_size=3,\n-            padding=padding,\n-            use_bias=True,\n-            batch_input_shape=(None, None, 5, None))])\n-\n-    # Test invalid output padding for given stride. Output padding equal to stride\n-    with pytest.raises(ValueError):\n-        model = Sequential([convolutional.Conv2DTranspose(\n-            filters=filters,\n-            kernel_size=3,\n-            padding=padding,\n-            output_padding=(0, 3),\n-            strides=(1, 3),\n-            batch_input_shape=(None, num_row, num_col, stack_size))])\n-\n-    # Output padding greater than stride\n-    with pytest.raises(ValueError):\n-        model = Sequential([convolutional.Conv2DTranspose(\n-            filters=filters,\n-            kernel_size=3,\n-            padding=padding,\n-            output_padding=(2, 2),\n-            strides=(1, 3),\n-            batch_input_shape=(None, num_row, num_col, stack_size))])\n-\n-\n-@pytest.mark.parametrize(\n-    'padding,strides,multiplier,dilation_rate',\n-    [(padding, strides, multiplier, dilation_rate)\n-     for padding in _convolution_paddings\n-     for strides in [1, 2]\n-     for multiplier in [1, 2]\n-     for dilation_rate in [1, 2]\n-     if (not (padding == 'same' and strides != 1)\n-         and not (dilation_rate != 1 and strides != 1)\n-         and not (dilation_rate != 1 and K.backend() == 'cntk'))]\n-)\n-def test_separable_conv_1d(padding, strides, multiplier, dilation_rate):\n-    num_samples = 2\n-    filters = 6\n-    stack_size = 3\n-    num_step = 9\n-\n-    layer_test(convolutional.SeparableConv1D,\n-               kwargs={'filters': filters,\n-                       'kernel_size': 3,\n-                       'padding': padding,\n-                       'strides': strides,\n-                       'depth_multiplier': multiplier,\n-                       'dilation_rate': dilation_rate},\n-               input_shape=(num_samples, num_step, stack_size))\n-\n-\n-def test_separable_conv_1d_additional_args():\n-    num_samples = 2\n-    filters = 6\n-    stack_size = 3\n-    num_step = 9\n-    padding = 'valid'\n-    multiplier = 2\n-\n-    layer_test(convolutional.SeparableConv1D,\n-               kwargs={'filters': filters,\n-                       'kernel_size': 3,\n-                       'padding': padding,\n-                       # 'data_format': 'channels_first',\n-                       'activation': None,\n-                       'depthwise_regularizer': 'l2',\n-                       'pointwise_regularizer': 'l2',\n-                       'bias_regularizer': 'l2',\n-                       'activity_regularizer': 'l2',\n-                       'pointwise_constraint': 'unit_norm',\n-                       'depthwise_constraint': 'unit_norm',\n-                       'strides': 1,\n-                       'use_bias': True,\n-                       'depth_multiplier': multiplier},\n-               input_shape=(num_samples, stack_size, num_step))\n-\n-\n-def test_separable_conv_1d_invalid():\n-    filters = 6\n-    padding = 'valid'\n-    with pytest.raises(ValueError):\n-        model = Sequential([convolutional.SeparableConv1D(\n-            filters=filters, kernel_size=3, padding=padding,\n-            batch_input_shape=(None, 5, None))])\n-\n-\n-@pytest.mark.parametrize(\n-    'padding,strides,multiplier,dilation_rate',\n-    [(padding, strides, multiplier, dilation_rate)\n-     for padding in _convolution_paddings\n-     for strides in [(1, 1), (2, 2)]\n-     for multiplier in [1, 2]\n-     for dilation_rate in [(1, 1), (2, 2), (2, 1), (1, 2)]\n-     if (not (padding == 'same' and strides != (1, 1))\n-         and not (dilation_rate != (1, 1) and strides != (1, 1))\n-         and not (dilation_rate != (1, 1) and multiplier == dilation_rate[0])\n-         and not (dilation_rate != (1, 1) and K.backend() == 'cntk'))]\n-)\n-def test_separable_conv_2d(padding, strides, multiplier, dilation_rate):\n-    num_samples = 2\n-    filters = 6\n-    stack_size = 3\n-    num_row = 7\n-    num_col = 6\n-\n-    layer_test(\n-        convolutional.SeparableConv2D,\n-        kwargs={'filters': filters,\n-                'kernel_size': (3, 3),\n-                'padding': padding,\n-                'strides': strides,\n-                'depth_multiplier': multiplier,\n-                'dilation_rate': dilation_rate},\n-        input_shape=(num_samples, num_row, num_col, stack_size))\n-\n-\n-def test_separable_conv_2d_additional_args():\n-    num_samples = 2\n-    filters = 6\n-    stack_size = 3\n-    num_row = 7\n-    num_col = 6\n-    padding = 'valid'\n-    strides = (2, 2)\n-    multiplier = 2\n-\n-    layer_test(convolutional.SeparableConv2D,\n-               kwargs={'filters': filters,\n-                       'kernel_size': 3,\n-                       'padding': padding,\n-                       # 'data_format': 'channels_first',\n-                       'activation': None,\n-                       'depthwise_regularizer': 'l2',\n-                       'pointwise_regularizer': 'l2',\n-                       'bias_regularizer': 'l2',\n-                       'activity_regularizer': 'l2',\n-                       'pointwise_constraint': 'unit_norm',\n-                       'depthwise_constraint': 'unit_norm',\n-                       'strides': strides,\n-                       'depth_multiplier': multiplier},\n-               input_shape=(num_samples, stack_size, num_row, num_col))\n-\n-\n-def test_separable_conv_2d_invalid():\n-    filters = 6\n-    padding = 'valid'\n-    with pytest.raises(ValueError):\n-        model = Sequential([convolutional.SeparableConv2D(\n-            filters=filters, kernel_size=3, padding=padding,\n-            batch_input_shape=(None, None, 5, None))])\n-\n-\n-@pytest.mark.parametrize(\n-    'padding,strides,multiplier,dilation_rate',\n-    [(padding, strides, multiplier, dilation_rate)\n-     for padding in _convolution_paddings\n-     for strides in [(1, 1), (2, 2)]\n-     for multiplier in [1, 2]\n-     for dilation_rate in [(1, 1), (2, 2), (2, 1), (1, 2)]\n-     if (not (padding == 'same' and strides != (1, 1))\n-         and not (dilation_rate != (1, 1) and strides != (1, 1))\n-         and not (dilation_rate != (1, 1) and multiplier == dilation_rate[0])\n-         and not (dilation_rate != (1, 1) and K.backend() == 'cntk'))]\n-)\n-def test_depthwise_conv_2d(padding, strides, multiplier, dilation_rate):\n-    num_samples = 2\n-    stack_size = 3\n-    num_row = 7\n-    num_col = 6\n-\n-    layer_test(convolutional.DepthwiseConv2D,\n-               kwargs={'kernel_size': (3, 3),\n-                       'padding': padding,\n-                       'strides': strides,\n-                       'depth_multiplier': multiplier,\n-                       'dilation_rate': dilation_rate},\n-               input_shape=(num_samples,\n-                            num_row,\n-                            num_col,\n-                            stack_size))\n-\n-\n-def test_depthwise_conv_2d_additional_args():\n-    num_samples = 2\n-    stack_size = 3\n-    num_row = 7\n-    num_col = 6\n-    padding = 'valid'\n-    strides = (2, 2)\n-    multiplier = 2\n-\n-    layer_test(convolutional.DepthwiseConv2D,\n-               kwargs={'kernel_size': 3,\n-                       'padding': padding,\n-                       # 'data_format': 'channels_first',\n-                       'activation': None,\n-                       'depthwise_regularizer': 'l2',\n-                       'bias_regularizer': 'l2',\n-                       'activity_regularizer': 'l2',\n-                       'depthwise_constraint': 'unit_norm',\n-                       'use_bias': True,\n-                       'strides': strides,\n-                       'depth_multiplier': multiplier},\n-               input_shape=(num_samples, stack_size, num_row, num_col))\n-\n-\n-def test_depthwise_conv_2d_invalid():\n-    padding = 'valid'\n-    with pytest.raises(ValueError):\n-        Sequential([convolutional.DepthwiseConv2D(\n-            kernel_size=3,\n-            padding=padding,\n-            batch_input_shape=(None, None, 5, None))])\n-\n-\n-@pytest.mark.parametrize(\n-    'padding,strides',\n-    [(padding, strides)\n-     for padding in _convolution_paddings\n-     for strides in [(1, 1, 1), (2, 2, 2)]\n-     if not (padding == 'same' and strides != (1, 1, 1))]\n-)\n-def test_convolution_3d(padding, strides):\n-    num_samples = 2\n-    filters = 2\n-    stack_size = 3\n-\n-    input_len_dim1 = 9\n-    input_len_dim2 = 8\n-    input_len_dim3 = 8\n-\n-    layer_test(convolutional.Convolution3D,\n-               kwargs={'filters': filters,\n-                       'kernel_size': 3,\n-                       'padding': padding,\n-                       'strides': strides},\n-               input_shape=(num_samples,\n-                            input_len_dim1, input_len_dim2, input_len_dim3,\n-                            stack_size))\n-\n-\n-def test_convolution_3d_additional_args():\n-    num_samples = 2\n-    filters = 2\n-    stack_size = 3\n-    padding = 'valid'\n-    strides = (2, 2, 2)\n-\n-    input_len_dim1 = 9\n-    input_len_dim2 = 8\n-    input_len_dim3 = 8\n-\n-    layer_test(convolutional.Convolution3D,\n-               kwargs={'filters': filters,\n-                       'kernel_size': (1, 2, 3),\n-                       'padding': padding,\n-                       'activation': None,\n-                       'kernel_regularizer': 'l2',\n-                       'bias_regularizer': 'l2',\n-                       'activity_regularizer': 'l2',\n-                       'kernel_constraint': 'max_norm',\n-                       'bias_constraint': 'max_norm',\n-                       'strides': strides},\n-               input_shape=(num_samples,\n-                            input_len_dim1, input_len_dim2, input_len_dim3,\n-                            stack_size))\n-\n-\n-@pytest.mark.parametrize(\n-    'padding,out_padding,strides,data_format',\n-    [(padding, out_padding, strides, data_format)\n-     for padding in _convolution_paddings\n-     for out_padding in [None, (0, 0, 0), (1, 1, 1)]\n-     for strides in [(1, 1, 1), (2, 2, 2)]\n-     # for data_format in ['channels_first', 'channels_last']\n-     for data_format in ['channels_last']\n-     if (not (padding == 'same' and strides != (1, 1, 1))\n-         and not (strides == (1, 1, 1) and out_padding == (1, 1, 1)))]\n-)\n-def test_conv3d_transpose(padding, out_padding, strides, data_format):\n-    filters = 2\n-    stack_size = 3\n-    num_depth = 7\n-    num_row = 5\n-    num_col = 6\n-\n-    layer_test(\n-        convolutional.Conv3DTranspose,\n-        kwargs={'filters': filters,\n-                'kernel_size': 3,\n-                'padding': padding,\n-                'output_padding': out_padding,\n-                'strides': strides,\n-                'data_format': data_format},\n-        input_shape=(None, num_depth, num_row, num_col, stack_size),\n-        fixed_batch_size=True)\n-\n-\n-def test_conv3d_transpose_additional_args():\n-    filters = 2\n-    stack_size = 3\n-    num_depth = 7\n-    num_row = 5\n-    num_col = 6\n-    padding = 'valid'\n-    strides = (2, 2, 2)\n-\n-    layer_test(convolutional.Conv3DTranspose,\n-               kwargs={'filters': filters,\n-                       'kernel_size': 3,\n-                       'padding': padding,\n-                       # 'data_format': 'channels_first',\n-                       'activation': None,\n-                       'kernel_regularizer': 'l2',\n-                       'bias_regularizer': 'l2',\n-                       'activity_regularizer': 'l2',\n-                       'kernel_constraint': 'max_norm',\n-                       'bias_constraint': 'max_norm',\n-                       'use_bias': True,\n-                       'strides': strides},\n-               input_shape=(None, stack_size, num_depth, num_row, num_col),\n-               fixed_batch_size=True)\n-\n-\n-def test_conv3d_transpose_invalid():\n-    filters = 2\n-    stack_size = 3\n-    num_depth = 7\n-    num_row = 5\n-    num_col = 6\n-    padding = 'valid'\n-\n-    # Test invalid use case\n-    with pytest.raises(ValueError):\n-        model = Sequential([convolutional.Conv3DTranspose(\n-            filters=filters,\n-            kernel_size=3,\n-            padding=padding,\n-            batch_input_shape=(None, None, 5, None, None))])\n-\n-    # Test invalid output padding for given stride. Output padding equal\n-    # to stride\n-    with pytest.raises(ValueError):\n-        model = Sequential([convolutional.Conv3DTranspose(\n-            filters=filters,\n-            kernel_size=3,\n-            padding=padding,\n-            output_padding=(0, 3, 3),\n-            strides=(1, 3, 4),\n-            batch_input_shape=(None, num_depth, num_row, num_col, stack_size))])\n-\n-    # Output padding greater than stride\n-    with pytest.raises(ValueError):\n-        model = Sequential([convolutional.Conv3DTranspose(\n-            filters=filters,\n-            kernel_size=3,\n-            padding=padding,\n-            output_padding=(2, 2, 3),\n-            strides=(1, 3, 4),\n-            batch_input_shape=(None, num_depth, num_row, num_col, stack_size))])\n-\n-\n-def test_zero_padding_1d():\n-    num_samples = 2\n-    input_dim = 2\n-    num_steps = 5\n-    shape = (num_samples, num_steps, input_dim)\n-    inputs = np.ones(shape)\n-\n-    # basic test\n-    layer_test(convolutional.ZeroPadding1D,\n-               kwargs={'padding': 2},\n-               input_shape=inputs.shape)\n-    layer_test(convolutional.ZeroPadding1D,\n-               kwargs={'padding': (1, 2)},\n-               input_shape=inputs.shape)\n-\n-    # correctness test\n-    layer = convolutional.ZeroPadding1D(padding=2)\n-    layer.build(shape)\n-    outputs = layer(K.variable(inputs))\n-    np_output = K.eval(outputs)\n-    for offset in [0, 1, -1, -2]:\n-        assert_allclose(np_output[:, offset, :], 0.)\n-    assert_allclose(np_output[:, 2:-2, :], 1.)\n-\n-    layer = convolutional.ZeroPadding1D(padding=(1, 2))\n-    layer.build(shape)\n-    outputs = layer(K.variable(inputs))\n-    np_output = K.eval(outputs)\n-    for left_offset in [0]:\n-        assert_allclose(np_output[:, left_offset, :], 0.)\n-    for right_offset in [-1, -2]:\n-        assert_allclose(np_output[:, right_offset, :], 0.)\n-    assert_allclose(np_output[:, 1:-2, :], 1.)\n-    layer.get_config()\n-\n-\n-@pytest.mark.parametrize(\n-    'data_format,padding',\n-    [(data_format, padding)\n-     for data_format in ['channels_first', 'channels_last']\n-     for padding in [(2, 2), ((1, 2), (3, 4))]]\n-)\n-def test_zero_padding_2d(data_format, padding):\n-    num_samples = 2\n-    stack_size = 2\n-    input_num_row = 4\n-    input_num_col = 5\n-\n-    if data_format == 'channels_last':\n-        inputs = np.ones((num_samples, input_num_row, input_num_col, stack_size))\n-    else:\n-        inputs = np.ones((num_samples, stack_size, input_num_row, input_num_col))\n-\n-    layer_test(convolutional.ZeroPadding2D,\n-               kwargs={'padding': padding, 'data_format': data_format},\n-               input_shape=inputs.shape)\n-\n-\n-@pytest.mark.parametrize('data_format',\n-                         ['channels_first', 'channels_last'])\n-def test_zero_padding_2d_correctness(data_format):\n-    num_samples = 2\n-    stack_size = 2\n-    input_num_row = 4\n-    input_num_col = 5\n-    inputs = np.ones((num_samples, stack_size, input_num_row, input_num_col))\n-\n-    layer = convolutional.ZeroPadding2D(padding=(2, 2),\n-                                        data_format=data_format)\n-    layer.build(inputs.shape)\n-    outputs = layer(K.variable(inputs))\n-    np_output = K.eval(outputs)\n-    if data_format == 'channels_last':\n-        for offset in [0, 1, -1, -2]:\n-            assert_allclose(np_output[:, offset, :, :], 0.)\n-            assert_allclose(np_output[:, :, offset, :], 0.)\n-        assert_allclose(np_output[:, 2:-2, 2:-2, :], 1.)\n-    elif data_format == 'channels_first':\n-        for offset in [0, 1, -1, -2]:\n-            assert_allclose(np_output[:, :, offset, :], 0.)\n-            assert_allclose(np_output[:, :, :, offset], 0.)\n-        assert_allclose(np_output[:, 2:-2, 2:-2, :], 1.)\n-\n-    layer = convolutional.ZeroPadding2D(padding=((1, 2), (3, 4)),\n-                                        data_format=data_format)\n-    layer.build(inputs.shape)\n-    outputs = layer(K.variable(inputs))\n-    np_output = K.eval(outputs)\n-    if data_format == 'channels_last':\n-        for top_offset in [0]:\n-            assert_allclose(np_output[:, top_offset, :, :], 0.)\n-        for bottom_offset in [-1, -2]:\n-            assert_allclose(np_output[:, bottom_offset, :, :], 0.)\n-        for left_offset in [0, 1, 2]:\n-            assert_allclose(np_output[:, :, left_offset, :], 0.)\n-        for right_offset in [-1, -2, -3, -4]:\n-            assert_allclose(np_output[:, :, right_offset, :], 0.)\n-        assert_allclose(np_output[:, 1:-2, 3:-4, :], 1.)\n-    elif data_format == 'channels_first':\n-        for top_offset in [0]:\n-            assert_allclose(np_output[:, :, top_offset, :], 0.)\n-        for bottom_offset in [-1, -2]:\n-            assert_allclose(np_output[:, :, bottom_offset, :], 0.)\n-        for left_offset in [0, 1, 2]:\n-            assert_allclose(np_output[:, :, :, left_offset], 0.)\n-        for right_offset in [-1, -2, -3, -4]:\n-            assert_allclose(np_output[:, :, :, right_offset], 0.)\n-        assert_allclose(np_output[:, :, 1:-2, 3:-4], 1.)\n-\n-\n-@pytest.mark.parametrize(\n-    'data_format,padding',\n-    [(data_format, padding)\n-     for data_format in ['channels_first', 'channels_last']\n-     for padding in [(2, 2, 2), ((1, 2), (3, 4), (0, 2))]]\n-)\n-def DISABLED_test_zero_padding_3d(data_format, padding):\n-    num_samples = 2\n-    stack_size = 2\n-    input_len_dim1 = 4\n-    input_len_dim2 = 5\n-    input_len_dim3 = 3\n-    inputs = np.ones((num_samples,\n-                     input_len_dim1, input_len_dim2, input_len_dim3,\n-                     stack_size))\n-\n-    layer_test(convolutional.ZeroPadding3D,\n-               kwargs={'padding': padding, 'data_format': data_format},\n-               input_shape=inputs.shape)\n-\n-\n-@pytest.mark.parametrize('data_format',\n-                         ['channels_first', 'channels_last'])\n-def test_zero_padding_3d_correctness(data_format):\n-    num_samples = 2\n-    stack_size = 2\n-    input_len_dim1 = 4\n-    input_len_dim2 = 5\n-    input_len_dim3 = 3\n-    inputs = np.ones((num_samples,\n-                      input_len_dim1, input_len_dim2, input_len_dim3,\n-                      stack_size))\n-\n-    layer = convolutional.ZeroPadding3D(padding=(2, 2, 2),\n-                                        data_format=data_format)\n-    layer.build(inputs.shape)\n-    outputs = layer(K.variable(inputs))\n-    np_output = K.eval(outputs)\n-    if data_format == 'channels_last':\n-        for offset in [0, 1, -1, -2]:\n-            assert_allclose(np_output[:, offset, :, :, :], 0.)\n-            assert_allclose(np_output[:, :, offset, :, :], 0.)\n-            assert_allclose(np_output[:, :, :, offset, :], 0.)\n-        assert_allclose(np_output[:, 2:-2, 2:-2, 2:-2, :], 1.)\n-    elif data_format == 'channels_first':\n-        for offset in [0, 1, -1, -2]:\n-            assert_allclose(np_output[:, :, offset, :, :], 0.)\n-            assert_allclose(np_output[:, :, :, offset, :], 0.)\n-            assert_allclose(np_output[:, :, :, :, offset], 0.)\n-        assert_allclose(np_output[:, :, 2:-2, 2:-2, 2:-2], 1.)\n-\n-    layer = convolutional.ZeroPadding3D(padding=((1, 2), (3, 4), (0, 2)),\n-                                        data_format=data_format)\n-    layer.build(inputs.shape)\n-    outputs = layer(K.variable(inputs))\n-    np_output = K.eval(outputs)\n-    if data_format == 'channels_last':\n-        for dim1_offset in [0, -1, -2]:\n-            assert_allclose(np_output[:, dim1_offset, :, :, :], 0.)\n-        for dim2_offset in [0, 1, 2, -1, -2, -3, -4]:\n-            assert_allclose(np_output[:, :, dim2_offset, :, :], 0.)\n-        for dim3_offset in [-1, -2]:\n-            assert_allclose(np_output[:, :, :, dim3_offset, :], 0.)\n-        assert_allclose(np_output[:, 1:-2, 3:-4, 0:-2, :], 1.)\n-    elif data_format == 'channels_first':\n-        for dim1_offset in [0, -1, -2]:\n-            assert_allclose(np_output[:, :, dim1_offset, :, :], 0.)\n-        for dim2_offset in [0, 1, 2, -1, -2, -3, -4]:\n-            assert_allclose(np_output[:, :, :, dim2_offset, :], 0.)\n-        for dim3_offset in [-1, -2]:\n-            assert_allclose(np_output[:, :, :, :, dim3_offset], 0.)\n-        assert_allclose(np_output[:, :, 1:-2, 3:-4, 0:-2], 1.)\n-\n-\n-def test_upsampling_1d():\n-    layer_test(convolutional.UpSampling1D,\n-               kwargs={'size': 2},\n-               input_shape=(3, 5, 4))\n-\n-\n-@pytest.mark.parametrize('data_format',\n-                         ['channels_first', 'channels_last'])\n-def test_upsampling_2d(data_format):\n-    num_samples = 2\n-    stack_size = 2\n-    input_num_row = 11\n-    input_num_col = 12\n-\n-    if data_format == 'channels_first':\n-        inputs = np.random.rand(num_samples, stack_size, input_num_row,\n-                                input_num_col)\n-    else:  # tf\n-        inputs = np.random.rand(num_samples, input_num_row, input_num_col,\n-                                stack_size)\n-\n-    # basic test\n-    layer_test(convolutional.UpSampling2D,\n-               kwargs={'size': (2, 2), 'data_format': data_format},\n-               input_shape=inputs.shape)\n-\n-    for length_row in [2]:\n-        for length_col in [2, 3]:\n-            layer = convolutional.UpSampling2D(\n-                size=(length_row, length_col),\n-                data_format=data_format)\n-            layer.build(inputs.shape)\n-            outputs = layer(K.variable(inputs))\n-            np_output = K.eval(outputs)\n-            if data_format == 'channels_first':\n-                assert np_output.shape[2] == length_row * input_num_row\n-                assert np_output.shape[3] == length_col * input_num_col\n-            else:  # tf\n-                assert np_output.shape[1] == length_row * input_num_row\n-                assert np_output.shape[2] == length_col * input_num_col\n-\n-            # compare with numpy\n-            if data_format == 'channels_first':\n-                expected_out = np.repeat(inputs, length_row, axis=2)\n-                expected_out = np.repeat(expected_out, length_col, axis=3)\n-            else:  # tf\n-                expected_out = np.repeat(inputs, length_row, axis=1)\n-                expected_out = np.repeat(expected_out, length_col, axis=2)\n-\n-            assert_allclose(np_output, expected_out)\n-\n-\n-@pytest.mark.parametrize('data_format',\n-                         ['channels_first', 'channels_last'])\n-def test_upsampling_2d_bilinear(data_format):\n-    num_samples = 2\n-    stack_size = 2\n-    input_num_row = 11\n-    input_num_col = 12\n-\n-    if data_format == 'channels_first':\n-        inputs = np.random.rand(num_samples, stack_size, input_num_row,\n-                                input_num_col)\n-    else:  # tf\n-        inputs = np.random.rand(num_samples, input_num_row, input_num_col,\n-                                stack_size)\n-\n-    # basic test\n-    layer_test(convolutional.UpSampling2D,\n-               kwargs={'size': (2, 2),\n-                       'data_format': data_format,\n-                       'interpolation': 'bilinear'},\n-               input_shape=inputs.shape)\n-\n-    for length_row in [2]:\n-        for length_col in [2, 3]:\n-            layer = convolutional.UpSampling2D(\n-                size=(length_row, length_col),\n-                data_format=data_format)\n-            layer.build(inputs.shape)\n-            outputs = layer(K.variable(inputs))\n-            np_output = K.eval(outputs)\n-            if data_format == 'channels_first':\n-                assert np_output.shape[2] == length_row * input_num_row\n-                assert np_output.shape[3] == length_col * input_num_col\n-            else:  # tf\n-                assert np_output.shape[1] == length_row * input_num_row\n-                assert np_output.shape[2] == length_col * input_num_col\n-\n-\n-@pytest.mark.parametrize('data_format',\n-                         ['channels_first', 'channels_last'])\n-def test_upsampling_3d(data_format):\n-    num_samples = 2\n-    stack_size = 2\n-    input_len_dim1 = 10\n-    input_len_dim2 = 11\n-    input_len_dim3 = 12\n-\n-    if data_format == 'channels_first':\n-        inputs = np.random.rand(num_samples,\n-                                stack_size,\n-                                input_len_dim1, input_len_dim2, input_len_dim3)\n-    else:  # tf\n-        inputs = np.random.rand(num_samples,\n-                                input_len_dim1, input_len_dim2, input_len_dim3,\n-                                stack_size)\n-\n-    # basic test\n-    layer_test(convolutional.UpSampling3D,\n-               kwargs={'size': (2, 2, 2), 'data_format': data_format},\n-               input_shape=inputs.shape)\n-\n-    for length_dim1 in [2, 3]:\n-        for length_dim2 in [2]:\n-            for length_dim3 in [3]:\n-                layer = convolutional.UpSampling3D(\n-                    size=(length_dim1, length_dim2, length_dim3),\n-                    data_format=data_format)\n-                layer.build(inputs.shape)\n-                outputs = layer(K.variable(inputs))\n-                np_output = K.eval(outputs)\n-                if data_format == 'channels_first':\n-                    assert np_output.shape[2] == length_dim1 * input_len_dim1\n-                    assert np_output.shape[3] == length_dim2 * input_len_dim2\n-                    assert np_output.shape[4] == length_dim3 * input_len_dim3\n-                else:  # tf\n-                    assert np_output.shape[1] == length_dim1 * input_len_dim1\n-                    assert np_output.shape[2] == length_dim2 * input_len_dim2\n-                    assert np_output.shape[3] == length_dim3 * input_len_dim3\n-\n-                # compare with numpy\n-                if data_format == 'channels_first':\n-                    expected_out = np.repeat(inputs, length_dim1, axis=2)\n-                    expected_out = np.repeat(expected_out, length_dim2, axis=3)\n-                    expected_out = np.repeat(expected_out, length_dim3, axis=4)\n-                else:  # tf\n-                    expected_out = np.repeat(inputs, length_dim1, axis=1)\n-                    expected_out = np.repeat(expected_out, length_dim2, axis=2)\n-                    expected_out = np.repeat(expected_out, length_dim3, axis=3)\n-\n-                assert_allclose(np_output, expected_out)\n-\n-\n-def test_cropping_1d():\n-    num_samples = 2\n-    time_length = 4\n-    input_len_dim1 = 2\n-    inputs = np.random.rand(num_samples, time_length, input_len_dim1)\n-\n-    layer_test(convolutional.Cropping1D,\n-               kwargs={'cropping': (2, 2)},\n-               input_shape=inputs.shape)\n-\n-\n-def test_cropping_2d():\n-    num_samples = 2\n-    stack_size = 2\n-    input_len_dim1 = 9\n-    input_len_dim2 = 9\n-    cropping = ((2, 2), (3, 3))\n-\n-    for data_format in ['channels_first', 'channels_last']:\n-        if data_format == 'channels_first':\n-            inputs = np.random.rand(num_samples, stack_size,\n-                                    input_len_dim1, input_len_dim2)\n-        else:\n-            inputs = np.random.rand(num_samples,\n-                                    input_len_dim1, input_len_dim2,\n-                                    stack_size)\n-        # basic test\n-        layer_test(convolutional.Cropping2D,\n-                   kwargs={'cropping': cropping,\n-                           'data_format': data_format},\n-                   input_shape=inputs.shape)\n-        # correctness test\n-        layer = convolutional.Cropping2D(cropping=cropping,\n-                                         data_format=data_format)\n-        layer.build(inputs.shape)\n-        outputs = layer(K.variable(inputs))\n-        np_output = K.eval(outputs)\n-        # compare with numpy\n-        if data_format == 'channels_first':\n-            expected_out = inputs[:,\n-                                  :,\n-                                  cropping[0][0]: -cropping[0][1],\n-                                  cropping[1][0]: -cropping[1][1]]\n-        else:\n-            expected_out = inputs[:,\n-                                  cropping[0][0]: -cropping[0][1],\n-                                  cropping[1][0]: -cropping[1][1],\n-                                  :]\n-        assert_allclose(np_output, expected_out)\n-\n-    for data_format in ['channels_first', 'channels_last']:\n-        if data_format == 'channels_first':\n-            inputs = np.random.rand(num_samples, stack_size,\n-                                    input_len_dim1, input_len_dim2)\n-        else:\n-            inputs = np.random.rand(num_samples,\n-                                    input_len_dim1, input_len_dim2,\n-                                    stack_size)\n-        # another correctness test (no cropping)\n-        cropping = ((0, 0), (0, 0))\n-        layer = convolutional.Cropping2D(cropping=cropping,\n-                                         data_format=data_format)\n-        layer.build(inputs.shape)\n-        outputs = layer(K.variable(inputs))\n-        np_output = K.eval(outputs)\n-        # compare with input\n-        assert_allclose(np_output, inputs)\n-\n-    # Test invalid use cases\n-    with pytest.raises(ValueError):\n-        layer = convolutional.Cropping2D(cropping=((1, 1),))\n-    with pytest.raises(ValueError):\n-        layer = convolutional.Cropping2D(cropping=lambda x: x)\n-\n-\n-def test_cropping_3d():\n-    num_samples = 2\n-    stack_size = 2\n-    input_len_dim1 = 8\n-    input_len_dim2 = 8\n-    input_len_dim3 = 8\n-    cropping = ((2, 2), (3, 3), (2, 3))\n-\n-    for data_format in ['channels_last', 'channels_first']:\n-        if data_format == 'channels_first':\n-            inputs = np.random.rand(num_samples, stack_size,\n-                                    input_len_dim1, input_len_dim2, input_len_dim3)\n-        else:\n-            inputs = np.random.rand(num_samples,\n-                                    input_len_dim1, input_len_dim2,\n-                                    input_len_dim3, stack_size)\n-        # basic test\n-        layer_test(convolutional.Cropping3D,\n-                   kwargs={'cropping': cropping,\n-                           'data_format': data_format},\n-                   input_shape=inputs.shape)\n-        # correctness test\n-        layer = convolutional.Cropping3D(cropping=cropping,\n-                                         data_format=data_format)\n-        layer.build(inputs.shape)\n-        outputs = layer(K.variable(inputs))\n-        np_output = K.eval(outputs)\n-        # compare with numpy\n-        if data_format == 'channels_first':\n-            expected_out = inputs[:,\n-                                  :,\n-                                  cropping[0][0]: -cropping[0][1],\n-                                  cropping[1][0]: -cropping[1][1],\n-                                  cropping[2][0]: -cropping[2][1]]\n-        else:\n-            expected_out = inputs[:,\n-                                  cropping[0][0]: -cropping[0][1],\n-                                  cropping[1][0]: -cropping[1][1],\n-                                  cropping[2][0]: -cropping[2][1],\n-                                  :]\n-        assert_allclose(np_output, expected_out)\n-\n-    for data_format in ['channels_last', 'channels_first']:\n-        if data_format == 'channels_first':\n-            inputs = np.random.rand(num_samples, stack_size,\n-                                    input_len_dim1, input_len_dim2, input_len_dim3)\n-        else:\n-            inputs = np.random.rand(num_samples,\n-                                    input_len_dim1, input_len_dim2,\n-                                    input_len_dim3, stack_size)\n-        # another correctness test (no cropping)\n-        cropping = ((0, 0), (0, 0), (0, 0))\n-        layer = convolutional.Cropping3D(cropping=cropping,\n-                                         data_format=data_format)\n-        layer.build(inputs.shape)\n-        outputs = layer(K.variable(inputs))\n-        np_output = K.eval(outputs)\n-        # compare with input\n-        assert_allclose(np_output, inputs)\n-\n-    # Test invalid use cases\n-    with pytest.raises(ValueError):\n-        layer = convolutional.Cropping3D(cropping=((1, 1),))\n-    with pytest.raises(ValueError):\n-        layer = convolutional.Cropping3D(cropping=lambda x: x)\n-\n-\n-@pytest.mark.parametrize(\n-    'input_shape,conv_class',\n-    [((2, 4, 2), convolutional.Conv1D),\n-     ((2, 4, 4, 2), convolutional.Conv2D),\n-     ((2, 4, 4, 4, 2), convolutional.Conv3D)]\n-)\n-def test_conv_float64(input_shape, conv_class):\n-    kernel_size = 3\n-    strides = 1\n-    filters = 3\n-    K.set_floatx('float64')\n-    layer_test(conv_class,\n-               kwargs={'filters': filters,\n-                       'kernel_size': kernel_size,\n-                       'padding': 'valid',\n-                       'strides': strides},\n-               input_shape=input_shape)\n-    K.set_floatx('float32')\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,353 +0,0 @@\n-import pytest\n-import numpy as np\n-from numpy.testing import assert_allclose\n-\n-from keras import backend as K\n-from keras import layers\n-from keras.models import Model\n-from keras.models import Sequential\n-from keras.utils.test_utils import layer_test\n-from keras import regularizers\n-from keras import constraints\n-from keras.layers import deserialize as deserialize_layer\n-\n-\n-def test_masking():\n-    layer_test(layers.Masking,\n-               kwargs={},\n-               input_shape=(3, 2, 3))\n-\n-\n-def test_dropout():\n-    layer_test(layers.Dropout,\n-               kwargs={'rate': 0.5},\n-               input_shape=(3, 2))\n-\n-    layer_test(layers.Dropout,\n-               kwargs={'rate': 0.5, 'noise_shape': [3, 1]},\n-               input_shape=(3, 2))\n-\n-    layer_test(layers.Dropout,\n-               kwargs={'rate': 0.5, 'noise_shape': [None, 1]},\n-               input_shape=(3, 2))\n-\n-    layer_test(layers.SpatialDropout1D,\n-               kwargs={'rate': 0.5},\n-               input_shape=(2, 3, 4))\n-\n-    for data_format in ['channels_last', 'channels_first']:\n-        for shape in [(4, 5), (4, 5, 6)]:\n-            if data_format == 'channels_last':\n-                input_shape = (2,) + shape + (3,)\n-            else:\n-                input_shape = (2, 3) + shape\n-            if len(shape) == 2:\n-                layer = layers.SpatialDropout2D\n-            else:\n-                layer = layers.SpatialDropout3D\n-            layer_test(layer,\n-                       kwargs={'rate': 0.5,\n-                               'data_format': data_format},\n-                       input_shape=input_shape)\n-\n-            # Test invalid use cases\n-            with pytest.raises(ValueError):\n-                layer_test(layer,\n-                           kwargs={'rate': 0.5,\n-                                   'data_format': 'channels_middle'},\n-                           input_shape=input_shape)\n-\n-\n-def test_activation():\n-    # with string argument\n-    layer_test(layers.Activation,\n-               kwargs={'activation': 'relu'},\n-               input_shape=(3, 2))\n-\n-    # with function argument\n-    layer_test(layers.Activation,\n-               kwargs={'activation': K.relu},\n-               input_shape=(3, 2))\n-\n-\n-@pytest.mark.parametrize('target_shape,input_shape',\n-                         [((8, 1), (3, 2, 4)),\n-                          ((-1, 1), (3, 2, 4)),\n-                          ((1, -1), (3, 2, 4)),\n-                          ((-1, 1), (None, None, 4))])\n-def test_reshape(target_shape, input_shape):\n-    layer_test(layers.Reshape,\n-               kwargs={'target_shape': target_shape},\n-               input_shape=input_shape)\n-\n-\n-def test_permute():\n-    layer_test(layers.Permute,\n-               kwargs={'dims': (2, 1)},\n-               input_shape=(3, 2, 4))\n-\n-\n-def test_flatten():\n-\n-    def test_4d():\n-        np_inp_channels_last = np.arange(24, dtype='float32').reshape(\n-                                        (1, 4, 3, 2))\n-\n-        np_output_cl = layer_test(layers.Flatten,\n-                                  kwargs={'data_format':\n-                                          'channels_last'},\n-                                  input_data=np_inp_channels_last)\n-\n-        np_inp_channels_first = np.transpose(np_inp_channels_last,\n-                                             [0, 3, 1, 2])\n-\n-        np_output_cf = layer_test(layers.Flatten,\n-                                  kwargs={'data_format':\n-                                          'channels_first'},\n-                                  input_data=np_inp_channels_first,\n-                                  expected_output=np_output_cl)\n-\n-    def test_3d():\n-        np_inp_channels_last = np.arange(12, dtype='float32').reshape(\n-            (1, 4, 3))\n-\n-        np_output_cl = layer_test(layers.Flatten,\n-                                  kwargs={'data_format':\n-                                          'channels_last'},\n-                                  input_data=np_inp_channels_last)\n-\n-        np_inp_channels_first = np.transpose(np_inp_channels_last,\n-                                             [0, 2, 1])\n-\n-        np_output_cf = layer_test(layers.Flatten,\n-                                  kwargs={'data_format':\n-                                          'channels_first'},\n-                                  input_data=np_inp_channels_first,\n-                                  expected_output=np_output_cl)\n-\n-    def test_5d():\n-        np_inp_channels_last = np.arange(120, dtype='float32').reshape(\n-            (1, 5, 4, 3, 2))\n-\n-        np_output_cl = layer_test(layers.Flatten,\n-                                  kwargs={'data_format':\n-                                          'channels_last'},\n-                                  input_data=np_inp_channels_last)\n-\n-        np_inp_channels_first = np.transpose(np_inp_channels_last,\n-                                             [0, 4, 1, 2, 3])\n-\n-        np_output_cf = layer_test(layers.Flatten,\n-                                  kwargs={'data_format':\n-                                          'channels_first'},\n-                                  input_data=np_inp_channels_first,\n-                                  expected_output=np_output_cl)\n-    test_3d()\n-    test_4d()\n-    test_5d()\n-\n-\n-def test_repeat_vector():\n-    layer_test(layers.RepeatVector,\n-               kwargs={'n': 3},\n-               input_shape=(3, 2))\n-\n-\n-def test_lambda():\n-    layer_test(layers.Lambda,\n-               kwargs={'function': lambda x: x + 1},\n-               input_shape=(3, 2))\n-\n-    layer_test(layers.Lambda,\n-               kwargs={'function': lambda x, a, b: x * a + b,\n-                       'arguments': {'a': 0.6, 'b': 0.4}},\n-               input_shape=(3, 2))\n-\n-    def antirectifier(x):\n-        x -= K.mean(x, axis=1, keepdims=True)\n-        x = K.l2_normalize(x, axis=1)\n-        pos = K.relu(x)\n-        neg = K.relu(-x)\n-        return K.concatenate([pos, neg], axis=1)\n-\n-    def antirectifier_output_shape(input_shape):\n-        shape = list(input_shape)\n-        assert len(shape) == 2  # only valid for 2D tensors\n-        shape[-1] *= 2\n-        return tuple(shape)\n-\n-    layer_test(layers.Lambda,\n-               kwargs={'function': antirectifier,\n-                       'output_shape': antirectifier_output_shape},\n-               input_shape=(3, 2))\n-\n-    # test layer with multiple outputs\n-    def test_multiple_outputs():\n-        def func(x):\n-            return [x * 0.2, x * 0.3]\n-\n-        def output_shape(input_shape):\n-            return [input_shape, input_shape]\n-\n-        def mask(inputs, mask=None):\n-            return [None, None]\n-\n-        i = layers.Input(shape=(3, 2, 1))\n-        o = layers.Lambda(function=func,\n-                          output_shape=output_shape,\n-                          mask=mask)(i)\n-\n-        o1, o2 = o\n-\n-        model = Model(i, o)\n-\n-        x = np.random.random((4, 3, 2, 1))\n-        out1, out2 = model.predict(x)\n-        assert out1.shape == (4, 3, 2, 1)\n-        assert out2.shape == (4, 3, 2, 1)\n-        assert_allclose(out1, x * 0.2, atol=1e-4)\n-        assert_allclose(out2, x * 0.3, atol=1e-4)\n-\n-    test_multiple_outputs()\n-\n-    # test layer with multiple outputs and no\n-    # explicit mask\n-    def test_multiple_outputs_no_mask():\n-        def func(x):\n-            return [x * 0.2, x * 0.3]\n-\n-        def output_shape(input_shape):\n-            return [input_shape, input_shape]\n-\n-        i = layers.Input(shape=(3, 2, 1))\n-        o = layers.Lambda(function=func,\n-                          output_shape=output_shape)(i)\n-\n-        o = layers.add(o)\n-        model = Model(i, o)\n-\n-        i2 = layers.Input(shape=(3, 2, 1))\n-        o2 = model(i2)\n-        model2 = Model(i2, o2)\n-\n-        x = np.random.random((4, 3, 2, 1))\n-        out = model2.predict(x)\n-        assert out.shape == (4, 3, 2, 1)\n-        assert_allclose(out, x * 0.2 + x * 0.3, atol=1e-4)\n-\n-    test_multiple_outputs_no_mask()\n-\n-    # test serialization with function\n-    def f(x):\n-        return x + 1\n-\n-    ld = layers.Lambda(f)\n-    config = ld.get_config()\n-    ld = deserialize_layer({'class_name': 'Lambda', 'config': config})\n-\n-    # test with lambda\n-    ld = layers.Lambda(\n-        lambda x: K.concatenate([K.square(x), x]),\n-        output_shape=lambda s: tuple(list(s)[:-1] + [2 * s[-1]]))\n-    config = ld.get_config()\n-    ld = layers.Lambda.from_config(config)\n-\n-    # test serialization with output_shape function\n-    def f(x):\n-        return K.concatenate([K.square(x), x])\n-\n-    def f_shape(s):\n-        return tuple(list(s)[:-1] + [2 * s[-1]])\n-\n-    ld = layers.Lambda(f, output_shape=f_shape)\n-    config = ld.get_config()\n-    ld = deserialize_layer({'class_name': 'Lambda', 'config': config})\n-\n-\n-@pytest.mark.skipif((K.backend() == 'theano'),\n-                    reason=\"theano cannot compute \"\n-                           \"the output shape automatically.\")\n-def test_lambda_output_shape():\n-    layer_test(layers.Lambda,\n-               kwargs={'function': lambda x: K.mean(x, axis=-1)},\n-               input_shape=(3, 2, 4))\n-\n-\n-def test_dense():\n-    layer_test(layers.Dense,\n-               kwargs={'units': 3},\n-               input_shape=(3, 2))\n-\n-    layer_test(layers.Dense,\n-               kwargs={'units': 3},\n-               input_shape=(3, 4, 2))\n-\n-    layer_test(layers.Dense,\n-               kwargs={'units': 3},\n-               input_shape=(None, None, 2))\n-\n-    layer_test(layers.Dense,\n-               kwargs={'units': 3},\n-               input_shape=(3, 4, 5, 2))\n-\n-    layer_test(layers.Dense,\n-               kwargs={'units': 3,\n-                       'kernel_regularizer': regularizers.l2(0.01),\n-                       'bias_regularizer': regularizers.l1(0.01),\n-                       'activity_regularizer': regularizers.L1L2(l1=0.01, l2=0.01),\n-                       'kernel_constraint': constraints.MaxNorm(1),\n-                       'bias_constraint': constraints.max_norm(1)},\n-               input_shape=(3, 2))\n-\n-    layer = layers.Dense(3,\n-                         kernel_regularizer=regularizers.l1(0.01),\n-                         bias_regularizer='l1')\n-    layer.build((None, 4))\n-    assert len(layer.losses) == 2\n-\n-\n-def test_activity_regularization():\n-    layer = layers.ActivityRegularization(l1=0.01, l2=0.01)\n-\n-    # test in functional API\n-    x = layers.Input(shape=(3,))\n-    z = layers.Dense(2)(x)\n-    y = layer(z)\n-    model = Model(x, y)\n-    model.compile('rmsprop', 'mse')\n-\n-    model.predict(np.random.random((2, 3)))\n-\n-    # test serialization\n-    model_config = model.get_config()\n-    model = Model.from_config(model_config)\n-    model.compile('rmsprop', 'mse')\n-\n-\n-def DISABLED_test_sequential_as_downstream_of_masking_layer():\n-\n-    inputs = layers.Input(shape=(3, 4))\n-    x = layers.Masking(mask_value=0., input_shape=(3, 4))(inputs)\n-    s = Sequential()\n-    s.add(layers.Dense(5, input_shape=(4,)))\n-    s.add(layers.Activation('relu'))\n-    x = layers.TimeDistributed(s)(x)\n-    model = Model(inputs=inputs, outputs=x)\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model_input = np.random.randint(low=1, high=5, size=(10, 3, 4))\n-    for i in range(4):\n-        model_input[i, i:, :] = 0.\n-    model.fit(model_input,\n-              np.random.random((10, 3, 5)), epochs=1, batch_size=6)\n-\n-    mask_outputs = [model.layers[1].compute_mask(model.layers[1].input)]\n-    mask_outputs += [model.layers[2].compute_mask(model.layers[2].input,\n-                                                  mask_outputs[-1])]\n-    func = K.function([inputs], mask_outputs)\n-    mask_outputs_val = func([model_input])\n-    assert np.array_equal(mask_outputs_val[0], np.any(model_input, axis=-1))\n-    assert np.array_equal(mask_outputs_val[1], np.any(model_input, axis=-1))\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,47 +0,0 @@\n-import pytest\n-from keras.utils.test_utils import layer_test\n-from keras.layers.embeddings import Embedding\n-from keras.models import Sequential\n-import keras.backend as K\n-\n-\n-def test_embedding():\n-    layer_test(Embedding,\n-               kwargs={'output_dim': 4, 'input_dim': 10, 'input_length': 2},\n-               input_shape=(3, 2),\n-               input_dtype='int32',\n-               expected_output_dtype=K.floatx())\n-    layer_test(Embedding,\n-               kwargs={'output_dim': 4, 'input_dim': 10, 'mask_zero': True},\n-               input_shape=(3, 2),\n-               input_dtype='int32',\n-               expected_output_dtype=K.floatx())\n-    layer_test(Embedding,\n-               kwargs={'output_dim': 4, 'input_dim': 10, 'mask_zero': True},\n-               input_shape=(3, 2, 5),\n-               input_dtype='int32',\n-               expected_output_dtype=K.floatx())\n-    layer_test(Embedding,\n-               kwargs={'output_dim': 4, 'input_dim': 10, 'mask_zero': True,\n-                       'input_length': (None, 5)},\n-               input_shape=(3, 2, 5),\n-               input_dtype='int32',\n-               expected_output_dtype=K.floatx())\n-\n-\n-@pytest.mark.parametrize('input_shape',\n-                         [(3, 4, 5),\n-                          (3, 5)])\n-def DISABLED_test_embedding_invalid(input_shape):\n-\n-    # len(input_length) should be equal to len(input_shape) - 1\n-    with pytest.raises(ValueError):\n-        model = Sequential([Embedding(\n-            input_dim=10,\n-            output_dim=4,\n-            input_length=2,\n-            input_shape=input_shape)])\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,60 +0,0 @@\n-import pytest\n-\n-from keras.utils.test_utils import layer_test\n-from keras.layers import local\n-\n-\n-def test_locallyconnected_1d():\n-    num_samples = 2\n-    num_steps = 8\n-    input_dim = 5\n-    filter_length = 3\n-    filters = 4\n-    padding = 'valid'\n-    strides = 1\n-\n-    layer_test(local.LocallyConnected1D,\n-               kwargs={'filters': filters,\n-                       'kernel_size': filter_length,\n-                       'padding': padding,\n-                       'kernel_regularizer': 'l2',\n-                       'bias_regularizer': 'l2',\n-                       'activity_regularizer': 'l2',\n-                       'strides': strides},\n-               input_shape=(num_samples, num_steps, input_dim))\n-\n-\n-def test_locallyconnected_2d():\n-    num_samples = 5\n-    filters = 3\n-    stack_size = 4\n-    num_row = 6\n-    num_col = 8\n-    padding = 'valid'\n-\n-    for strides in [(1, 1), (2, 2)]:\n-        layer_test(local.LocallyConnected2D,\n-                   kwargs={'filters': filters,\n-                           'kernel_size': 3,\n-                           'padding': padding,\n-                           'kernel_regularizer': 'l2',\n-                           'bias_regularizer': 'l2',\n-                           'activity_regularizer': 'l2',\n-                           'strides': strides,\n-                           'data_format': 'channels_last'},\n-                   input_shape=(num_samples, num_row, num_col, stack_size))\n-\n-        layer_test(local.LocallyConnected2D,\n-                   kwargs={'filters': filters,\n-                           'kernel_size': (3, 3),\n-                           'padding': padding,\n-                           'kernel_regularizer': 'l2',\n-                           'bias_regularizer': 'l2',\n-                           'activity_regularizer': 'l2',\n-                           'strides': strides,\n-                           'data_format': 'channels_first'},\n-                   input_shape=(num_samples, stack_size, num_row, num_col))\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,293 +0,0 @@\n-import pytest\n-import numpy as np\n-from numpy.testing import assert_allclose\n-from keras import layers\n-from keras import models\n-from keras import backend as K\n-from keras.utils.test_utils import layer_test\n-from keras.layers import merge\n-\n-\n-def test_merge_add():\n-    i1 = layers.Input(shape=(4, 5))\n-    i2 = layers.Input(shape=(4, 5))\n-    i3 = layers.Input(shape=(4, 5))\n-    o = layers.add([i1, i2, i3])\n-    assert K.int_shape(o) == (None, 4, 5)\n-    model = models.Model([i1, i2, i3], o)\n-\n-    add_layer = layers.Add()\n-    o2 = add_layer([i1, i2, i3])\n-    assert add_layer.output_shape == (None, 4, 5)\n-\n-    x1 = np.random.random((2, 4, 5))\n-    x2 = np.random.random((2, 4, 5))\n-    x3 = np.random.random((2, 4, 5))\n-    out = model.predict([x1, x2, x3])\n-    assert out.shape == (2, 4, 5)\n-    assert_allclose(out, x1 + x2 + x3, atol=1e-4)\n-\n-    assert add_layer.compute_mask([i1, i2, i3], [None, None, None]) is None\n-    assert np.all(K.eval(add_layer.compute_mask(\n-        [i1, i2, i3], [K.variable(x1), K.variable(x2), K.variable(x3)])))\n-\n-    # Test invalid use case\n-    with pytest.raises(ValueError):\n-        add_layer.compute_mask([i1, i2, i3], x1)\n-    with pytest.raises(ValueError):\n-        add_layer.compute_mask(i1, [None, None, None])\n-    with pytest.raises(ValueError):\n-        add_layer.compute_mask([i1, i2, i3], [None, None])\n-\n-\n-def test_merge_subtract():\n-    i1 = layers.Input(shape=(4, 5))\n-    i2 = layers.Input(shape=(4, 5))\n-    i3 = layers.Input(shape=(4, 5))\n-    i4 = layers.Input(shape=(3, 5))\n-    o = layers.subtract([i1, i2])\n-    assert K.int_shape(o) == (None, 4, 5)\n-    model = models.Model([i1, i2], o)\n-\n-    subtract_layer = layers.Subtract()\n-    o2 = subtract_layer([i1, i2])\n-    assert subtract_layer.output_shape == (None, 4, 5)\n-\n-    x1 = np.random.random((2, 4, 5))\n-    x2 = np.random.random((2, 4, 5))\n-    out = model.predict([x1, x2])\n-    assert out.shape == (2, 4, 5)\n-    assert_allclose(out, x1 - x2, atol=1e-4)\n-\n-    assert subtract_layer.compute_mask([i1, i2], [None, None]) is None\n-    assert np.all(K.eval(subtract_layer.compute_mask(\n-        [i1, i2], [K.variable(x1), K.variable(x2)])))\n-\n-    # Test invalid use case\n-    with pytest.raises(ValueError):\n-        subtract_layer.compute_mask([i1, i2], x1)\n-    with pytest.raises(ValueError):\n-        subtract_layer.compute_mask(i1, [None, None])\n-    with pytest.raises(ValueError):\n-        subtract_layer([i1, i2, i3])\n-    with pytest.raises(ValueError):\n-        subtract_layer([i1])\n-\n-\n-def test_merge_multiply():\n-    i1 = layers.Input(shape=(4, 5))\n-    i2 = layers.Input(shape=(4, 5))\n-    i3 = layers.Input(shape=(4, 5))\n-    o = layers.multiply([i1, i2, i3])\n-    assert K.int_shape(o) == (None, 4, 5)\n-    model = models.Model([i1, i2, i3], o)\n-\n-    mul_layer = layers.Multiply()\n-    o2 = mul_layer([i1, i2, i3])\n-    assert mul_layer.output_shape == (None, 4, 5)\n-\n-    x1 = np.random.random((2, 4, 5))\n-    x2 = np.random.random((2, 4, 5))\n-    x3 = np.random.random((2, 4, 5))\n-    out = model.predict([x1, x2, x3])\n-    assert out.shape == (2, 4, 5)\n-    assert_allclose(out, x1 * x2 * x3, atol=1e-4)\n-\n-\n-def test_merge_average():\n-    i1 = layers.Input(shape=(4, 5))\n-    i2 = layers.Input(shape=(4, 5))\n-    o = layers.average([i1, i2])\n-    assert K.int_shape(o) == (None, 4, 5)\n-    model = models.Model([i1, i2], o)\n-\n-    avg_layer = layers.Average()\n-    o2 = avg_layer([i1, i2])\n-    assert avg_layer.output_shape == (None, 4, 5)\n-\n-    x1 = np.random.random((2, 4, 5))\n-    x2 = np.random.random((2, 4, 5))\n-    out = model.predict([x1, x2])\n-    assert out.shape == (2, 4, 5)\n-    assert_allclose(out, 0.5 * (x1 + x2), atol=1e-4)\n-\n-\n-def test_merge_maximum():\n-    i1 = layers.Input(shape=(4, 5))\n-    i2 = layers.Input(shape=(4, 5))\n-    o = layers.maximum([i1, i2])\n-    assert K.int_shape(o) == (None, 4, 5)\n-    model = models.Model([i1, i2], o)\n-\n-    max_layer = layers.Maximum()\n-    o2 = max_layer([i1, i2])\n-    assert max_layer.output_shape == (None, 4, 5)\n-\n-    x1 = np.random.random((2, 4, 5))\n-    x2 = np.random.random((2, 4, 5))\n-    out = model.predict([x1, x2])\n-    assert out.shape == (2, 4, 5)\n-    assert_allclose(out, np.maximum(x1, x2), atol=1e-4)\n-\n-\n-def test_merge_minimum():\n-    i1 = layers.Input(shape=(4, 5))\n-    i2 = layers.Input(shape=(4, 5))\n-    o = layers.minimum([i1, i2])\n-    assert K.int_shape(o) == (None, 4, 5)\n-    model = models.Model([i1, i2], o)\n-\n-    max_layer = layers.Minimum()\n-    o2 = max_layer([i1, i2])\n-    assert max_layer.output_shape == (None, 4, 5)\n-\n-    x1 = np.random.random((2, 4, 5))\n-    x2 = np.random.random((2, 4, 5))\n-    out = model.predict([x1, x2])\n-    assert out.shape == (2, 4, 5)\n-    assert_allclose(out, np.minimum(x1, x2), atol=1e-4)\n-\n-\n-def test_merge_concatenate():\n-    i1 = layers.Input(shape=(None, 5))\n-    i2 = layers.Input(shape=(None, 5))\n-    o = layers.concatenate([i1, i2], axis=1)\n-    assert K.int_shape(o) == (None, None, 5)\n-    model = models.Model([i1, i2], o)\n-\n-    i1 = layers.Input(shape=(4, 5))\n-    i2 = layers.Input(shape=(4, 5))\n-    o = layers.concatenate([i1, i2], axis=1)\n-    assert K.int_shape(o) == (None, 8, 5)\n-    model = models.Model([i1, i2], o)\n-\n-    concat_layer = layers.Concatenate(axis=1)\n-    o2 = concat_layer([i1, i2])\n-    assert concat_layer.output_shape == (None, 8, 5)\n-\n-    x1 = np.random.random((2, 4, 5))\n-    x2 = np.random.random((2, 4, 5))\n-    out = model.predict([x1, x2])\n-    assert out.shape == (2, 8, 5)\n-    assert_allclose(out, np.concatenate([x1, x2], axis=1), atol=1e-4)\n-\n-    x3 = np.random.random((1, 1, 1))\n-    nb_layers = 4\n-    x_i = layers.Input(shape=(None, None))\n-    x_list = [x_i]\n-    x = x_i\n-    for i in range(nb_layers):\n-        x_list.append(x)\n-        x = layers.concatenate(x_list, axis=1)\n-    concat_model = models.Model(x_i, x)\n-    concat_out = concat_model.predict([x3])\n-    x3 = np.repeat(x3, 16, axis=1)\n-    assert concat_out.shape == (1, 16, 1)\n-    assert_allclose(concat_out, x3)\n-\n-    assert concat_layer.compute_mask([i1, i2], [None, None]) is None\n-    assert np.all(K.eval(concat_layer.compute_mask(\n-        [i1, i2], [K.variable(x1), K.variable(x2)])).reshape(-1))\n-\n-    # Test invalid use case\n-    concat_layer = layers.Concatenate(axis=1)\n-    with pytest.raises(ValueError):\n-        concat_layer.compute_mask([i1, i2], x1)\n-    with pytest.raises(ValueError):\n-        concat_layer.compute_mask(i1, [None, None])\n-    with pytest.raises(ValueError):\n-        concat_layer.compute_mask([i1, i2], [None])\n-    with pytest.raises(ValueError):\n-        concat_layer([i1])\n-\n-\n-def test_merge_dot():\n-    i1 = layers.Input(shape=(4,))\n-    i2 = layers.Input(shape=(4,))\n-    o = layers.dot([i1, i2], axes=1)\n-    assert K.int_shape(o) == (None, 1)\n-    model = models.Model([i1, i2], o)\n-\n-    dot_layer = layers.Dot(axes=1)\n-    o2 = dot_layer([i1, i2])\n-    assert dot_layer.output_shape == (None, 1)\n-\n-    x1 = np.random.random((2, 4))\n-    x2 = np.random.random((2, 4))\n-    out = model.predict([x1, x2])\n-    assert out.shape == (2, 1)\n-    expected = np.zeros((2, 1))\n-    expected[0, 0] = np.dot(x1[0], x2[0])\n-    expected[1, 0] = np.dot(x1[1], x2[1])\n-    assert_allclose(out, expected, atol=1e-4)\n-\n-    # Test with negative tuple of axes.\n-    o = layers.dot([i1, i2], axes=(-1, -1))\n-    assert K.int_shape(o) == (None, 1)\n-    model = models.Model([i1, i2], o)\n-    out = model.predict([x1, x2])\n-    assert out.shape == (2, 1)\n-    assert_allclose(out, expected, atol=1e-4)\n-\n-\n-def test_merge_broadcast():\n-    # shapes provided\n-    i1 = layers.Input(shape=(4, 5))\n-    i2 = layers.Input(shape=(5,))\n-    ops = [layers.add, layers.maximum]\n-    for op in ops:\n-        o = op([i1, i2])\n-        assert K.int_shape(o) == (None, 4, 5)\n-        model = models.Model([i1, i2], o)\n-\n-        x1 = np.random.random((2, 4, 5))\n-        x2 = np.random.random((2, 5))\n-        out = model.predict([x1, x2])\n-        assert out.shape == (2, 4, 5)\n-\n-    # shapes not provided\n-    i1 = layers.Input(shape=(None, None))\n-    i2 = layers.Input(shape=(None,))\n-    ops = [layers.add, layers.maximum]\n-    for op in ops:\n-        o = op([i1, i2])\n-        assert K.int_shape(o) == (None, None, None)\n-        model = models.Model([i1, i2], o)\n-\n-        x1 = np.random.random((2, 4, 5))\n-        x2 = np.random.random((2, 5))\n-        out = model.predict([x1, x2])\n-        assert out.shape == (2, 4, 5)\n-\n-    # ndim not provided\n-    if K.backend() == 'tensorflow':\n-        k_ndim = K.ndim\n-        K.ndim = lambda _: None\n-\n-        i1 = layers.Input(shape=(None, None))\n-        i2 = layers.Input(shape=(None,))\n-        ops = [layers.add, layers.maximum]\n-        for op in ops:\n-            o = op([i1, i2])\n-            assert K.int_shape(o) == (None, None, None)\n-            model = models.Model([i1, i2], o)\n-\n-            x1 = np.random.random((2, 4, 5))\n-            x2 = np.random.random((2, 5))\n-            out = model.predict([x1, x2])\n-            assert out.shape == (2, 4, 5)\n-        K.ndim = k_ndim\n-\n-\n-def test_masking_concatenate():\n-    input1 = layers.Input(shape=(6,))\n-    input2 = layers.Input(shape=(6,))\n-    x1 = layers.Embedding(10, 5, input_length=6, mask_zero=True)(input1)\n-    x2 = layers.Embedding(10, 5, input_length=6, mask_zero=True)(input2)\n-    x = layers.concatenate([x1, x2])\n-    x = layers.TimeDistributed(layers.Dense(3, activation='softmax'))(x)\n-    models.Model(inputs=[input1, input2], outputs=[x])\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,32 +0,0 @@\n-import pytest\n-from keras.utils.test_utils import layer_test\n-from keras.layers import noise\n-from keras import backend as K\n-\n-\n-@pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason=\"cntk does not support it yet\")\n-def test_GaussianNoise():\n-    layer_test(noise.GaussianNoise,\n-               kwargs={'stddev': 1.},\n-               input_shape=(3, 2, 3))\n-\n-\n-@pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason=\"cntk does not support it yet\")\n-def test_GaussianDropout():\n-    layer_test(noise.GaussianDropout,\n-               kwargs={'rate': 0.5},\n-               input_shape=(3, 2, 3))\n-\n-\n-@pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason=\"cntk does not support it yet\")\n-def test_AlphaDropout():\n-    layer_test(noise.AlphaDropout,\n-               kwargs={'rate': 0.1},\n-               input_shape=(3, 2, 3))\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,241 +0,0 @@\n-import pytest\n-import numpy as np\n-from numpy.testing import assert_allclose\n-\n-from keras.layers import Input\n-from keras import regularizers\n-from keras.utils.test_utils import layer_test\n-from keras.layers import normalization\n-from keras.models import Sequential, Model\n-from keras import backend as K\n-\n-input_1 = np.arange(10)\n-input_2 = np.zeros(10)\n-input_3 = np.ones((10))\n-input_4 = np.expand_dims(np.arange(10.), axis=1)\n-input_shapes = [np.ones((10, 10)), np.ones((10, 10, 10))]\n-\n-\n-def test_basic_batchnorm():\n-    layer_test(normalization.BatchNormalization,\n-               kwargs={'momentum': 0.9,\n-                       'epsilon': 0.1,\n-                       'gamma_regularizer': regularizers.l2(0.01),\n-                       'beta_regularizer': regularizers.l2(0.01)},\n-               input_shape=(3, 4, 2))\n-    layer_test(normalization.BatchNormalization,\n-               kwargs={'momentum': 0.9,\n-                       'epsilon': 0.1,\n-                       'axis': 1},\n-               input_shape=(1, 4, 1))\n-    layer_test(normalization.BatchNormalization,\n-               kwargs={'gamma_initializer': 'ones',\n-                       'beta_initializer': 'ones',\n-                       'moving_mean_initializer': 'zeros',\n-                       'moving_variance_initializer': 'ones'},\n-               input_shape=(3, 4, 2, 4))\n-    if K.backend() != 'theano':\n-        layer_test(normalization.BatchNormalization,\n-                   kwargs={'momentum': 0.9,\n-                           'epsilon': 0.1,\n-                           'axis': 1,\n-                           'scale': False,\n-                           'center': False},\n-                   input_shape=(3, 4, 2, 4))\n-\n-\n-def test_batchnorm_correctness_1d():\n-    np.random.seed(1337)\n-    model = Sequential()\n-    norm = normalization.BatchNormalization(input_shape=(10,), momentum=0.8)\n-    model.add(norm)\n-    model.compile(loss='mse', optimizer='rmsprop')\n-\n-    # centered on 5.0, variance 10.0\n-    x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 10))\n-    model.fit(x, x, epochs=5, verbose=0)\n-    out = model.predict(x)\n-    out -= K.eval(norm.beta)\n-    out /= K.eval(norm.gamma)\n-\n-    assert_allclose(out.mean(), 0.0, atol=1e-1)\n-    assert_allclose(out.std(), 1.0, atol=1e-1)\n-\n-\n-def test_batchnorm_correctness_2d():\n-    np.random.seed(1337)\n-    model = Sequential()\n-    norm = normalization.BatchNormalization(axis=1, input_shape=(10, 6),\n-                                            momentum=0.8)\n-    model.add(norm)\n-    model.compile(loss='mse', optimizer='rmsprop')\n-\n-    # centered on 5.0, variance 10.0\n-    x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 10, 6))\n-    model.fit(x, x, epochs=5, verbose=0)\n-    out = model.predict(x)\n-    out -= np.reshape(K.eval(norm.beta), (1, 10, 1))\n-    out /= np.reshape(K.eval(norm.gamma), (1, 10, 1))\n-\n-    assert_allclose(out.mean(axis=(0, 2)), 0.0, atol=1.1e-1)\n-    assert_allclose(out.std(axis=(0, 2)), 1.0, atol=1.1e-1)\n-\n-\n-def test_batchnorm_training_argument():\n-    np.random.seed(1337)\n-    bn1 = normalization.BatchNormalization(input_shape=(10,))\n-    x1 = Input(shape=(10,))\n-    y1 = bn1(x1, training=True)\n-    assert bn1.updates\n-\n-    model1 = Model(x1, y1)\n-    x = np.random.normal(loc=5.0, scale=10.0, size=(20, 10))\n-    output_a = model1.predict(x)\n-\n-    model1.compile(loss='mse', optimizer='rmsprop')\n-    model1.fit(x, x, epochs=1, verbose=0)\n-    output_b = model1.predict(x)\n-    assert np.abs(np.sum(output_a - output_b)) > 0.1\n-    assert_allclose(output_b.mean(), 0.0, atol=1e-1)\n-    assert_allclose(output_b.std(), 1.0, atol=1e-1)\n-\n-    bn2 = normalization.BatchNormalization(input_shape=(10,))\n-    x2 = Input(shape=(10,))\n-    bn2(x2, training=False)\n-    assert not bn2.updates\n-\n-\n-def test_batchnorm_mode_twice():\n-    # This is a regression test for issue #4881 with the old\n-    # batch normalization functions in the Theano backend.\n-    model = Sequential()\n-    model.add(normalization.BatchNormalization(input_shape=(10, 5, 5), axis=1))\n-    model.add(normalization.BatchNormalization(input_shape=(10, 5, 5), axis=1))\n-    model.compile(loss='mse', optimizer='sgd')\n-\n-    x = np.random.normal(loc=5.0, scale=10.0, size=(20, 10, 5, 5))\n-    model.fit(x, x, epochs=1, verbose=0)\n-    model.predict(x)\n-\n-\n-def test_batchnorm_convnet():\n-    np.random.seed(1337)\n-    model = Sequential()\n-    norm = normalization.BatchNormalization(axis=1, input_shape=(3, 4, 4),\n-                                            momentum=0.8)\n-    model.add(norm)\n-    model.compile(loss='mse', optimizer='sgd')\n-\n-    # centered on 5.0, variance 10.0\n-    x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 3, 4, 4))\n-    model.fit(x, x, epochs=4, verbose=0)\n-    out = model.predict(x)\n-    out -= np.reshape(K.eval(norm.beta), (1, 3, 1, 1))\n-    out /= np.reshape(K.eval(norm.gamma), (1, 3, 1, 1))\n-\n-    assert_allclose(np.mean(out, axis=(0, 2, 3)), 0.0, atol=1e-1)\n-    assert_allclose(np.std(out, axis=(0, 2, 3)), 1.0, atol=1e-1)\n-\n-\n-@pytest.mark.skipif((K.backend() == 'theano'),\n-                    reason='Bug with theano backend')\n-def test_batchnorm_convnet_no_center_no_scale():\n-    np.random.seed(1337)\n-    model = Sequential()\n-    norm = normalization.BatchNormalization(axis=-1, center=False, scale=False,\n-                                            input_shape=(3, 4, 4), momentum=0.8)\n-    model.add(norm)\n-    model.compile(loss='mse', optimizer='sgd')\n-\n-    # centered on 5.0, variance 10.0\n-    x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 3, 4, 4))\n-    model.fit(x, x, epochs=4, verbose=0)\n-    out = model.predict(x)\n-\n-    assert_allclose(np.mean(out, axis=(0, 2, 3)), 0.0, atol=1e-1)\n-    assert_allclose(np.std(out, axis=(0, 2, 3)), 1.0, atol=1e-1)\n-\n-\n-def test_shared_batchnorm():\n-    '''Test that a BN layer can be shared\n-    across different data streams.\n-    '''\n-    # Test single layer reuse\n-    bn = normalization.BatchNormalization(input_shape=(10,))\n-    x1 = Input(shape=(10,))\n-    bn(x1)\n-\n-    x2 = Input(shape=(10,))\n-    y2 = bn(x2)\n-\n-    x = np.random.normal(loc=5.0, scale=10.0, size=(2, 10))\n-    model = Model(x2, y2)\n-    model.compile('sgd', 'mse')\n-    model.train_on_batch(x, x)\n-\n-    # Test model-level reuse\n-    x3 = Input(shape=(10,))\n-    y3 = model(x3)\n-    new_model = Model(x3, y3)\n-    new_model.compile('sgd', 'mse')\n-    new_model.train_on_batch(x, x)\n-\n-\n-def test_that_trainable_disables_updates():\n-    val_a = np.random.random((10, 4))\n-    val_out = np.random.random((10, 4))\n-\n-    a = Input(shape=(4,))\n-    layer = normalization.BatchNormalization(input_shape=(4,))\n-    b = layer(a)\n-    model = Model(a, b)\n-\n-    model.trainable = False\n-    assert not model.updates\n-\n-    model.compile('sgd', 'mse')\n-    assert not model.updates\n-\n-    x1 = model.predict(val_a)\n-    model.train_on_batch(val_a, val_out)\n-    x2 = model.predict(val_a)\n-    assert_allclose(x1, x2, atol=1e-7)\n-\n-    model.trainable = True\n-    model.compile('sgd', 'mse')\n-    assert model.updates\n-\n-    model.train_on_batch(val_a, val_out)\n-    x2 = model.predict(val_a)\n-    assert np.abs(np.sum(x1 - x2)) > 1e-5\n-\n-    layer.trainable = False\n-    model.compile('sgd', 'mse')\n-    assert not model.updates\n-\n-    x1 = model.predict(val_a)\n-    model.train_on_batch(val_a, val_out)\n-    x2 = model.predict(val_a)\n-    assert_allclose(x1, x2, atol=1e-7)\n-\n-\n-def test_batchnorm_trainable():\n-    bn_mean = 0.5\n-    bn_std = 10.\n-\n-    def get_model(bn_mean, bn_std):\n-        input = Input(shape=(1,))\n-        x = normalization.BatchNormalization()(input)\n-        model = Model(input, x)\n-        model.set_weights([np.array([1.]), np.array([0.]),\n-                           np.array([bn_mean]), np.array([bn_std ** 2])])\n-        return model\n-    # Simulates training-mode with trainable layer. Should use mini-batch statistics.\n-    model = get_model(bn_mean, bn_std)\n-    model.compile(loss='mse', optimizer='rmsprop')\n-    out = model(input_4, training=True).numpy()\n-    assert_allclose((input_4 - np.mean(input_4)) / np.std(input_4), out, atol=1e-3)\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,150 +0,0 @@\n-import numpy as np\n-import pytest\n-\n-from keras.utils.test_utils import layer_test\n-from keras import layers\n-from keras.models import Sequential\n-\n-\n-@pytest.mark.parametrize(\n-    'padding,stride,data_format',\n-    [(padding, stride, data_format)\n-     for padding in ['valid', 'same']\n-     for stride in [1, 2]\n-     for data_format in ['channels_first', 'channels_last']]\n-)\n-def test_maxpooling_1d(padding, stride, data_format):\n-    layer_test(layers.MaxPooling1D,\n-               kwargs={'strides': stride,\n-                       'padding': padding,\n-                       'data_format': data_format},\n-               input_shape=(3, 5, 4))\n-\n-\n-@pytest.mark.parametrize(\n-    'strides',\n-    [(1, 1), (2, 3)]\n-)\n-def test_maxpooling_2d(strides):\n-    pool_size = (3, 3)\n-    layer_test(layers.MaxPooling2D,\n-               kwargs={'strides': strides,\n-                       'padding': 'valid',\n-                       'pool_size': pool_size},\n-               input_shape=(3, 5, 6, 4))\n-\n-\n-@pytest.mark.parametrize(\n-    'strides,data_format,input_shape',\n-    [(2, None, (3, 11, 12, 10, 4)),\n-     (3, 'channels_first', (3, 4, 11, 12, 10))]\n-)\n-def test_maxpooling_3d(strides, data_format, input_shape):\n-    pool_size = (3, 3, 3)\n-    layer_test(layers.MaxPooling3D,\n-               kwargs={'strides': strides,\n-                       'padding': 'valid',\n-                       'data_format': data_format,\n-                       'pool_size': pool_size},\n-               input_shape=input_shape)\n-\n-\n-@pytest.mark.parametrize(\n-    'padding,stride,data_format',\n-    [(padding, stride, data_format)\n-     for padding in ['valid', 'same']\n-     for stride in [1, 2]\n-     for data_format in ['channels_first', 'channels_last']]\n-)\n-def test_averagepooling_1d(padding, stride, data_format):\n-    layer_test(layers.AveragePooling1D,\n-               kwargs={'strides': stride,\n-                       'padding': padding,\n-                       'data_format': data_format},\n-               input_shape=(3, 5, 4))\n-\n-\n-@pytest.mark.parametrize(\n-    'strides,padding,data_format,input_shape',\n-    [((2, 2), 'same', None, (3, 5, 6, 4)),\n-     ((2, 2), 'valid', None, (3, 5, 6, 4))]\n-)\n-def test_averagepooling_2d(strides, padding, data_format, input_shape):\n-    layer_test(layers.AveragePooling2D,\n-               kwargs={'strides': strides,\n-                       'padding': padding,\n-                       'pool_size': (2, 2),\n-                       'data_format': data_format},\n-               input_shape=input_shape)\n-\n-\n-@pytest.mark.parametrize(\n-    'strides,data_format,input_shape',\n-    [(2, None, (3, 11, 12, 10, 4)),\n-     (3, 'channels_first', (3, 4, 11, 12, 10))]\n-)\n-def test_averagepooling_3d(strides, data_format, input_shape):\n-    pool_size = (3, 3, 3)\n-\n-    layer_test(layers.AveragePooling3D,\n-               kwargs={'strides': strides,\n-                       'padding': 'valid',\n-                       'data_format': data_format,\n-                       'pool_size': pool_size},\n-               input_shape=input_shape)\n-\n-\n-@pytest.mark.parametrize(\n-    'data_format,pooling_class',\n-    [(data_format, pooling_class)\n-     for data_format in ['channels_first', 'channels_last']\n-     for pooling_class in [layers.GlobalMaxPooling1D,\n-                           layers.GlobalAveragePooling1D]]\n-)\n-def test_globalpooling_1d(data_format, pooling_class):\n-    layer_test(pooling_class,\n-               kwargs={'data_format': data_format},\n-               input_shape=(3, 4, 5))\n-\n-\n-def test_globalpooling_1d_supports_masking():\n-    # Test GlobalAveragePooling1D supports masking\n-    model = Sequential()\n-    model.add(layers.Masking(mask_value=0., input_shape=(3, 4)))\n-    model.add(layers.GlobalAveragePooling1D())\n-    model.compile(loss='mae', optimizer='adam')\n-\n-    model_input = np.random.randint(low=1, high=5, size=(2, 3, 4))\n-    model_input[0, 1:, :] = 0\n-    output = model.predict(model_input)\n-    assert np.array_equal(output[0], model_input[0, 0, :])\n-\n-\n-@pytest.mark.parametrize(\n-    'data_format,pooling_class',\n-    [(data_format, pooling_class)\n-     for data_format in ['channels_first', 'channels_last']\n-     for pooling_class in [layers.GlobalMaxPooling2D,\n-                           layers.GlobalAveragePooling2D]]\n-)\n-def test_globalpooling_2d(data_format, pooling_class):\n-    layer_test(pooling_class,\n-               kwargs={'data_format': data_format},\n-               input_shape=(3, 4, 5, 6))\n-\n-\n-@pytest.mark.parametrize(\n-    'data_format,pooling_class',\n-    [(data_format, pooling_class)\n-     for data_format in ['channels_first', 'channels_last']\n-     for pooling_class in [layers.GlobalMaxPooling3D,\n-                           layers.GlobalAveragePooling3D]]\n-)\n-def test_globalpooling_3d(data_format, pooling_class):\n-    layer_test(pooling_class,\n-               kwargs={'data_format': data_format},\n-               input_shape=(3, 4, 3, 4, 3))\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,1052 +0,0 @@\n-import pytest\n-import numpy as np\n-from numpy.testing import assert_allclose\n-\n-import keras\n-from keras.utils.test_utils import layer_test\n-from keras.layers import recurrent\n-from keras.layers import embeddings\n-from keras.models import Sequential\n-from keras.models import Model\n-from keras.engine import Input\n-from keras.layers import Masking\n-from keras import regularizers\n-from keras import backend as K\n-\n-num_samples, timesteps, embedding_dim, units = 2, 5, 4, 3\n-embedding_num = 12\n-\n-\n-rnn_test = pytest.mark.parametrize('layer_class',\n-                                   [recurrent.SimpleRNN,\n-                                    recurrent.GRU,\n-                                    recurrent.LSTM])\n-\n-\n-rnn_cell_test = pytest.mark.parametrize('cell_class',\n-                                        [recurrent.SimpleRNNCell,\n-                                         recurrent.GRUCell,\n-                                         recurrent.LSTMCell])\n-\n-\n-@rnn_test\n-def test_return_sequences(layer_class):\n-    layer_test(layer_class,\n-               kwargs={'units': units,\n-                       'return_sequences': True},\n-               input_shape=(num_samples, timesteps, embedding_dim))\n-\n-\n-@rnn_test\n-def test_dynamic_behavior(layer_class):\n-    layer = layer_class(units, input_shape=(None, embedding_dim))\n-    model = Sequential()\n-    model.add(layer)\n-    model.compile('sgd', 'mse')\n-    x = np.random.random((num_samples, timesteps, embedding_dim))\n-    y = np.random.random((num_samples, units))\n-    model.train_on_batch(x, y)\n-\n-\n-@rnn_test\n-def DISABLED_test_stateful_invalid_use(layer_class):\n-    layer = layer_class(units,\n-                        stateful=True,\n-                        batch_input_shape=(num_samples,\n-                                           timesteps,\n-                                           embedding_dim))\n-    model = Sequential()\n-    model.add(layer)\n-    model.compile('sgd', 'mse')\n-    x = np.random.random((num_samples * 2, timesteps, embedding_dim))\n-    y = np.random.random((num_samples * 2, units))\n-    with pytest.raises(ValueError):\n-        model.fit(x, y)\n-    with pytest.raises(ValueError):\n-        model.predict(x, batch_size=num_samples + 1)\n-\n-\n-@rnn_test\n-@pytest.mark.skipif((K.backend() in ['theano']),\n-                    reason='Not supported.')\n-def test_dropout(layer_class):\n-    for unroll in [True, False]:\n-        layer_test(layer_class,\n-                   kwargs={'units': units,\n-                           'dropout': 0.1,\n-                           'recurrent_dropout': 0.1,\n-                           'unroll': unroll},\n-                   input_shape=(num_samples, timesteps, embedding_dim))\n-\n-        # Test that dropout is applied during training\n-        x = K.ones((num_samples, timesteps, embedding_dim))\n-        layer = layer_class(units, dropout=0.5, recurrent_dropout=0.5,\n-                            input_shape=(timesteps, embedding_dim))\n-        y = layer(x)\n-\n-        y = layer(x, training=True)\n-\n-        # Test that dropout is not applied during testing\n-        x = np.random.random((num_samples, timesteps, embedding_dim))\n-        layer = layer_class(units, dropout=0.5, recurrent_dropout=0.5,\n-                            unroll=unroll,\n-                            input_shape=(timesteps, embedding_dim))\n-        model = Sequential([layer])\n-        y1 = model.predict(x)\n-        y2 = model.predict(x)\n-        assert_allclose(y1, y2)\n-\n-\n-@rnn_test\n-def test_statefulness(layer_class):\n-    model = Sequential()\n-    model.add(embeddings.Embedding(embedding_num, embedding_dim,\n-                                   mask_zero=True,\n-                                   input_length=timesteps,\n-                                   batch_input_shape=(num_samples, timesteps)))\n-    layer = layer_class(units, return_sequences=False,\n-                        stateful=True,\n-                        weights=None)\n-    model.add(layer)\n-    model.compile(optimizer='sgd', loss='mse')\n-    out1 = model.predict(np.ones((num_samples, timesteps)))\n-    assert(out1.shape == (num_samples, units))\n-\n-    # train once so that the states change\n-    model.train_on_batch(np.ones((num_samples, timesteps)),\n-                         np.ones((num_samples, units)))\n-    out2 = model.predict(np.ones((num_samples, timesteps)))\n-\n-    # if the state is not reset, output should be different\n-    assert(out1.max() != out2.max())\n-\n-    # check that output changes after states are reset\n-    # (even though the model itself didn't change)\n-    layer.reset_states()\n-    out3 = model.predict(np.ones((num_samples, timesteps)))\n-    assert(out2.max() != out3.max())\n-\n-    # check that container-level reset_states() works\n-    model.reset_states()\n-    out4 = model.predict(np.ones((num_samples, timesteps)))\n-    assert_allclose(out3, out4, atol=1e-5)\n-\n-    # check that the call to `predict` updated the states\n-    out5 = model.predict(np.ones((num_samples, timesteps)))\n-    assert(out4.max() != out5.max())\n-\n-\n-@rnn_test\n-def test_masking_correctness(layer_class):\n-    # Check masking: output with left padding and right padding\n-    # should be the same.\n-    model = Sequential()\n-    model.add(embeddings.Embedding(embedding_num, embedding_dim,\n-                                   mask_zero=True,\n-                                   input_length=timesteps,\n-                                   batch_input_shape=(num_samples, timesteps)))\n-    layer = layer_class(units, return_sequences=False)\n-    model.add(layer)\n-    model.compile(optimizer='sgd', loss='mse')\n-\n-    left_padded_input = np.ones((num_samples, timesteps))\n-    left_padded_input[0, :1] = 0\n-    left_padded_input[1, :2] = 0\n-    out6 = model.predict(left_padded_input)\n-\n-    right_padded_input = np.ones((num_samples, timesteps))\n-    right_padded_input[0, -1:] = 0\n-    right_padded_input[1, -2:] = 0\n-    out7 = model.predict(right_padded_input)\n-\n-    assert_allclose(out7, out6, atol=1e-5)\n-\n-\n-@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\n-def test_masking_correctness_output_not_equal_to_first_state():\n-\n-    class Cell(keras.layers.Layer):\n-\n-        def __init__(self):\n-            self.state_size = None\n-            self.output_size = None\n-            super(Cell, self).__init__()\n-\n-        def build(self, input_shape):\n-            self.state_size = input_shape[-1]\n-            self.output_size = input_shape[-1]\n-\n-        def call(self, inputs, states):\n-            return inputs, [s + 1 for s in states]\n-\n-    num_samples = 5\n-    num_timesteps = 4\n-    state_size = input_size = 3  # also equal to `output_size`\n-\n-    # random inputs and state values\n-    x_vals = np.random.random((num_samples, num_timesteps, input_size))\n-    # last timestep masked for first sample (all zero inputs masked by Masking layer)\n-    x_vals[0, -1, :] = 0\n-    s_initial_vals = np.random.random((num_samples, state_size))\n-\n-    # final outputs equal to last inputs\n-    y_vals_expected = x_vals[:, -1].copy()\n-    # except for first sample, where it is equal to second to last value due to mask\n-    y_vals_expected[0] = x_vals[0, -2]\n-\n-    s_final_vals_expected = s_initial_vals.copy()\n-    # states are incremented `num_timesteps - 1` times for first sample\n-    s_final_vals_expected[0] += (num_timesteps - 1)\n-    # and `num_timesteps - 1` times for remaining samples\n-    s_final_vals_expected[1:] += num_timesteps\n-\n-    for unroll in [True, False]:\n-        x = Input((num_timesteps, input_size), name=\"x\")\n-        x_masked = Masking()(x)\n-        s_initial = Input((state_size,), name=\"s_initial\")\n-        y, s_final = recurrent.RNN(Cell(),\n-                                   return_state=True,\n-                                   unroll=unroll)(x_masked, initial_state=s_initial)\n-        model = Model([x, s_initial], [y, s_final])\n-        model.compile(optimizer='sgd', loss='mse')\n-\n-        y_vals, s_final_vals = model.predict([x_vals, s_initial_vals])\n-        assert_allclose(y_vals,\n-                        y_vals_expected,\n-                        err_msg=\"Unexpected output for unroll={}\".format(unroll))\n-        assert_allclose(s_final_vals,\n-                        s_final_vals_expected,\n-                        err_msg=\"Unexpected state for unroll={}\".format(unroll))\n-\n-\n-@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\n-def test_masking_correctness_output_size_not_equal_to_first_state_size():\n-\n-    class Cell(keras.layers.Layer):\n-\n-        def __init__(self):\n-            self.state_size = None\n-            self.output_size = None\n-            super(Cell, self).__init__()\n-\n-        def build(self, input_shape):\n-            self.state_size = input_shape[-1]\n-            self.output_size = input_shape[-1] * 2\n-\n-        def call(self, inputs, states):\n-            return keras.layers.concatenate([inputs] * 2), [s + 1 for s in states]\n-\n-    num_samples = 5\n-    num_timesteps = 6\n-    input_size = state_size = 7\n-\n-    # random inputs and state values\n-    x_vals = np.random.random((num_samples, num_timesteps, input_size))\n-    # last timestep masked for first sample (all zero inputs masked by Masking layer)\n-    x_vals[0, -1, :] = 0\n-    s_initial_vals = np.random.random((num_samples, state_size))\n-\n-    # final outputs equal to last inputs concatenated\n-    y_vals_expected = np.concatenate([x_vals[:, -1]] * 2, axis=-1)\n-    # except for first sample, where it is equal to second to last value due to mask\n-    y_vals_expected[0] = np.concatenate([x_vals[0, -2]] * 2, axis=-1)\n-\n-    s_final_vals_expected = s_initial_vals.copy()\n-    # states are incremented `num_timesteps - 1` times for first sample\n-    s_final_vals_expected[0] += (num_timesteps - 1)\n-    # and `num_timesteps - 1` times for remaining samples\n-    s_final_vals_expected[1:] += num_timesteps\n-\n-    for unroll in [True, False]:\n-        x = Input((num_timesteps, input_size), name=\"x\")\n-        x_masked = Masking()(x)\n-        s_initial = Input((state_size,), name=\"s_initial\")\n-        y, s_final = recurrent.RNN(Cell(),\n-                                   return_state=True,\n-                                   unroll=unroll)(x_masked, initial_state=s_initial)\n-        model = Model([x, s_initial], [y, s_final])\n-        model.compile(optimizer='sgd', loss='mse')\n-\n-        y_vals, s_final_vals = model.predict([x_vals, s_initial_vals])\n-        assert_allclose(y_vals,\n-                        y_vals_expected,\n-                        err_msg=\"Unexpected output for unroll={}\".format(unroll))\n-        assert_allclose(s_final_vals,\n-                        s_final_vals_expected,\n-                        err_msg=\"Unexpected state for unroll={}\".format(unroll))\n-\n-\n-@rnn_test\n-def test_implementation_mode(layer_class):\n-    for mode in [1, 2]:\n-        # Without dropout\n-        layer_test(layer_class,\n-                   kwargs={'units': units,\n-                           'implementation': mode},\n-                   input_shape=(num_samples, timesteps, embedding_dim))\n-        # With dropout\n-        layer_test(layer_class,\n-                   kwargs={'units': units,\n-                           'implementation': mode,\n-                           'dropout': 0.1,\n-                           'recurrent_dropout': 0.1},\n-                   input_shape=(num_samples, timesteps, embedding_dim))\n-        # Without bias\n-        layer_test(layer_class,\n-                   kwargs={'units': units,\n-                           'implementation': mode,\n-                           'use_bias': False},\n-                   input_shape=(num_samples, timesteps, embedding_dim))\n-\n-\n-@rnn_test\n-def test_regularizer(layer_class):\n-    layer = layer_class(units, return_sequences=False, weights=None,\n-                        input_shape=(timesteps, embedding_dim),\n-                        kernel_regularizer=regularizers.l1(0.01),\n-                        recurrent_regularizer=regularizers.l1(0.01),\n-                        bias_regularizer='l2')\n-    layer.build((None, None, embedding_dim))\n-    assert len(layer.losses) == 3\n-    assert len(layer.cell.losses) == 3\n-\n-    layer = layer_class(units, return_sequences=False, weights=None,\n-                        input_shape=(timesteps, embedding_dim),\n-                        activity_regularizer='l2')\n-    assert layer.activity_regularizer\n-    x = K.variable(np.ones((num_samples, timesteps, embedding_dim)))\n-    layer(x)\n-\n-\n-@rnn_test\n-def test_trainability(layer_class):\n-    layer = layer_class(units)\n-    layer.build((None, None, embedding_dim))\n-    assert len(layer.weights) == 3\n-    assert len(layer.trainable_weights) == 3\n-    assert len(layer.non_trainable_weights) == 0\n-    layer.trainable = False\n-    assert len(layer.weights) == 3\n-    assert len(layer.trainable_weights) == 0\n-    assert len(layer.non_trainable_weights) == 3\n-    layer.trainable = True\n-    assert len(layer.weights) == 3\n-    assert len(layer.trainable_weights) == 3\n-    assert len(layer.non_trainable_weights) == 0\n-\n-\n-def test_masking_layer():\n-    ''' This test based on a previously failing issue here:\n-    https://github.com/keras-team/keras/issues/1567\n-    '''\n-    inputs = np.random.random((6, 3, 4))\n-    targets = np.abs(np.random.random((6, 3, 5)))\n-    targets /= targets.sum(axis=-1, keepdims=True)\n-\n-    model = Sequential()\n-    model.add(Masking(input_shape=(3, 4)))\n-    model.add(recurrent.SimpleRNN(units=5, return_sequences=True, unroll=False))\n-    model.compile(loss='categorical_crossentropy', optimizer='adam')\n-    model.fit(inputs, targets, epochs=1, batch_size=100, verbose=1)\n-\n-    model = Sequential()\n-    model.add(Masking(input_shape=(3, 4)))\n-    model.add(recurrent.SimpleRNN(units=5, return_sequences=True, unroll=True))\n-    model.compile(loss='categorical_crossentropy', optimizer='adam')\n-    model.fit(inputs, targets, epochs=1, batch_size=100, verbose=1)\n-\n-\n-@rnn_test\n-def test_from_config(layer_class):\n-    stateful_flags = (False, True)\n-    for stateful in stateful_flags:\n-        l1 = layer_class(units=1, stateful=stateful)\n-        l2 = layer_class.from_config(l1.get_config())\n-        assert l1.get_config() == l2.get_config()\n-\n-\n-@rnn_test\n-def test_specify_initial_state_keras_tensor(layer_class):\n-    num_states = 2 if layer_class is recurrent.LSTM else 1\n-\n-    # Test with Keras tensor\n-    inputs = Input((timesteps, embedding_dim))\n-    initial_state = [Input((units,)) for _ in range(num_states)]\n-    layer = layer_class(units)\n-    if len(initial_state) == 1:\n-        output = layer(inputs, initial_state=initial_state[0])\n-    else:\n-        output = layer(inputs, initial_state=initial_state)\n-    assert id(initial_state[0]) in [\n-        id(x) for x in layer._inbound_nodes[0].input_tensors]\n-\n-    model = Model([inputs] + initial_state, output)\n-    model.compile(loss='categorical_crossentropy', optimizer='adam')\n-\n-    inputs = np.random.random((num_samples, timesteps, embedding_dim))\n-    initial_state = [np.random.random((num_samples, units))\n-                     for _ in range(num_states)]\n-    targets = np.random.random((num_samples, units))\n-    model.fit([inputs] + initial_state, targets)\n-\n-\n-@rnn_test\n-def test_specify_initial_state_non_keras_tensor(layer_class):\n-    num_states = 2 if layer_class is recurrent.LSTM else 1\n-\n-    # Test with non-Keras tensor\n-    inputs = Input((timesteps, embedding_dim))\n-    initial_state = [K.random_normal_variable((num_samples, units), 0, 1)\n-                     for _ in range(num_states)]\n-    layer = layer_class(units)\n-    output = layer(inputs, initial_state=initial_state)\n-\n-    model = Model(inputs, output)\n-    model.compile(loss='categorical_crossentropy', optimizer='adam')\n-\n-    inputs = np.random.random((num_samples, timesteps, embedding_dim))\n-    targets = np.random.random((num_samples, units))\n-    model.fit(inputs, targets)\n-\n-\n-@rnn_test\n-def test_reset_states_with_values(layer_class):\n-    num_states = 2 if layer_class is recurrent.LSTM else 1\n-\n-    layer = layer_class(units, stateful=True)\n-    layer.build((num_samples, timesteps, embedding_dim))\n-    layer.reset_states()\n-    assert len(layer.states) == num_states\n-    assert layer.states[0] is not None\n-    np.testing.assert_allclose(K.eval(layer.states[0]),\n-                               np.zeros(K.int_shape(layer.states[0])),\n-                               atol=1e-4)\n-    state_shapes = [K.int_shape(state) for state in layer.states]\n-    values = [np.ones(shape) for shape in state_shapes]\n-    if len(values) == 1:\n-        values = values[0]\n-    layer.reset_states(values)\n-    np.testing.assert_allclose(K.eval(layer.states[0]),\n-                               np.ones(K.int_shape(layer.states[0])),\n-                               atol=1e-4)\n-\n-    # Test fit with invalid data\n-    with pytest.raises(ValueError):\n-        layer.reset_states([1] * (len(layer.states) + 1))\n-\n-\n-@rnn_test\n-def test_initial_states_as_other_inputs(layer_class):\n-    num_states = 2 if layer_class is recurrent.LSTM else 1\n-\n-    # Test with Keras tensor\n-    main_inputs = Input((timesteps, embedding_dim))\n-    initial_state = [Input((units,)) for _ in range(num_states)]\n-    inputs = [main_inputs] + initial_state\n-\n-    layer = layer_class(units)\n-    output = layer(inputs)\n-    assert id(initial_state[0]) in [\n-        id(x) for x in layer._inbound_nodes[0].input_tensors]\n-\n-    model = Model(inputs, output)\n-    model.compile(loss='categorical_crossentropy', optimizer='adam')\n-\n-    main_inputs = np.random.random((num_samples, timesteps, embedding_dim))\n-    initial_state = [np.random.random((num_samples, units))\n-                     for _ in range(num_states)]\n-    targets = np.random.random((num_samples, units))\n-    model.train_on_batch([main_inputs] + initial_state, targets)\n-\n-\n-@rnn_test\n-def test_specify_state_with_masking(layer_class):\n-    ''' This test based on a previously failing issue here:\n-    https://github.com/keras-team/keras/issues/1567\n-    '''\n-    num_states = 2 if layer_class is recurrent.LSTM else 1\n-\n-    inputs = Input((timesteps, embedding_dim))\n-    _ = Masking()(inputs)\n-    initial_state = [Input((units,)) for _ in range(num_states)]\n-    output = layer_class(units)(inputs, initial_state=initial_state)\n-\n-    model = Model([inputs] + initial_state, output)\n-    model.compile(loss='categorical_crossentropy', optimizer='adam')\n-\n-    inputs = np.random.random((num_samples, timesteps, embedding_dim))\n-    initial_state = [np.random.random((num_samples, units))\n-                     for _ in range(num_states)]\n-    targets = np.random.random((num_samples, units))\n-    model.fit([inputs] + initial_state, targets)\n-\n-\n-@rnn_test\n-def test_return_state(layer_class):\n-    num_states = 2 if layer_class is recurrent.LSTM else 1\n-\n-    inputs = Input(batch_shape=(num_samples, timesteps, embedding_dim))\n-    layer = layer_class(units, return_state=True, stateful=True)\n-    outputs = layer(inputs)\n-    output, state = outputs[0], outputs[1:]\n-    assert len(state) == num_states\n-    model = Model(inputs, state[0])\n-\n-    inputs = np.random.random((num_samples, timesteps, embedding_dim))\n-    state = model.predict(inputs)\n-    np.testing.assert_allclose(K.eval(layer.states[0]), state, atol=1e-4)\n-\n-\n-@rnn_test\n-def test_state_reuse(layer_class):\n-    inputs = Input(batch_shape=(num_samples, timesteps, embedding_dim))\n-    layer = layer_class(units, return_state=True, return_sequences=True)\n-    outputs = layer(inputs)\n-    output, state = outputs[0], outputs[1:]\n-    output = layer_class(units)(output, initial_state=state)\n-    model = Model(inputs, output)\n-\n-    inputs = np.random.random((num_samples, timesteps, embedding_dim))\n-    outputs = model.predict(inputs)\n-\n-\n-@rnn_test\n-@pytest.mark.skipif((K.backend() in ['theano']),\n-                    reason='Not supported.')\n-def test_state_reuse_with_dropout(layer_class):\n-    input1 = Input(batch_shape=(num_samples, timesteps, embedding_dim))\n-    layer = layer_class(units, return_state=True, return_sequences=True, dropout=0.2)\n-    state = layer(input1)[1:]\n-\n-    input2 = Input(batch_shape=(num_samples, timesteps, embedding_dim))\n-    output = layer_class(units)(input2, initial_state=state)\n-    model = Model([input1, input2], output)\n-\n-    inputs = [np.random.random((num_samples, timesteps, embedding_dim)),\n-              np.random.random((num_samples, timesteps, embedding_dim))]\n-    outputs = model.predict(inputs)\n-\n-\n-def test_minimal_rnn_cell_non_layer():\n-\n-    class MinimalRNNCell(object):\n-\n-        def __init__(self, units, input_dim):\n-            self.units = units\n-            self.state_size = units\n-            self.kernel = keras.backend.variable(\n-                np.random.random((input_dim, units)))\n-\n-        def call(self, inputs, states):\n-            prev_output = states[0]\n-            output = keras.backend.dot(inputs, self.kernel) + prev_output\n-            return output, [output]\n-\n-    # Basic test case.\n-    cell = MinimalRNNCell(32, 5)\n-    x = keras.Input((None, 5))\n-    layer = recurrent.RNN(cell)\n-    y = layer(x)\n-    model = keras.models.Model(x, y)\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n-\n-    # Test stacking.\n-    cells = [MinimalRNNCell(8, 5),\n-             MinimalRNNCell(32, 8),\n-             MinimalRNNCell(32, 32)]\n-    layer = recurrent.RNN(cells)\n-    y = layer(x)\n-    model = keras.models.Model(x, y)\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n-\n-\n-def test_minimal_rnn_cell_non_layer_multiple_states():\n-\n-    class MinimalRNNCell(object):\n-\n-        def __init__(self, units, input_dim):\n-            self.units = units\n-            self.state_size = (units, units)\n-            self.kernel = keras.backend.variable(\n-                np.random.random((input_dim, units)))\n-\n-        def call(self, inputs, states):\n-            prev_output_1 = states[0]\n-            prev_output_2 = states[1]\n-            output = keras.backend.dot(inputs, self.kernel)\n-            output += prev_output_1\n-            output -= prev_output_2\n-            return output, [output * 2, output * 3]\n-\n-    # Basic test case.\n-    cell = MinimalRNNCell(32, 5)\n-    x = keras.Input((None, 5))\n-    layer = recurrent.RNN(cell)\n-    y = layer(x)\n-    model = keras.models.Model(x, y)\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n-\n-    # Test stacking.\n-    cells = [MinimalRNNCell(8, 5),\n-             MinimalRNNCell(16, 8),\n-             MinimalRNNCell(32, 16)]\n-    layer = recurrent.RNN(cells)\n-    y = layer(x)\n-    model = keras.models.Model(x, y)\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n-\n-\n-def test_minimal_rnn_cell_layer():\n-\n-    class MinimalRNNCell(keras.layers.Layer):\n-\n-        def __init__(self, units, **kwargs):\n-            self.units = units\n-            self.state_size = units\n-            super(MinimalRNNCell, self).__init__(**kwargs)\n-\n-        def build(self, input_shape):\n-            # no time axis in the input shape passed to RNN cells\n-            assert len(input_shape) == 2\n-\n-            self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n-                                          initializer='uniform',\n-                                          name='kernel')\n-            self.recurrent_kernel = self.add_weight(\n-                shape=(self.units, self.units),\n-                initializer='uniform',\n-                name='recurrent_kernel')\n-            self.built = True\n-\n-        def call(self, inputs, states):\n-            prev_output = states[0]\n-            h = keras.backend.dot(inputs, self.kernel)\n-            output = h + keras.backend.dot(prev_output, self.recurrent_kernel)\n-            return output, [output]\n-\n-        def get_config(self):\n-            config = {'units': self.units}\n-            base_config = super(MinimalRNNCell, self).get_config()\n-            return dict(list(base_config.items()) + list(config.items()))\n-\n-    # Test basic case.\n-    x = keras.Input((None, 5))\n-    cell = MinimalRNNCell(32)\n-    layer = recurrent.RNN(cell)\n-    y = layer(x)\n-    model = keras.models.Model(x, y)\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n-\n-    # Test basic case serialization.\n-    x_np = np.random.random((6, 5, 5))\n-    y_np = model.predict(x_np)\n-    weights = model.get_weights()\n-    config = layer.get_config()\n-    with keras.utils.CustomObjectScope({'MinimalRNNCell': MinimalRNNCell}):\n-        layer = recurrent.RNN.from_config(config)\n-    y = layer(x)\n-    model = keras.models.Model(x, y)\n-    model.set_weights(weights)\n-    y_np_2 = model.predict(x_np)\n-    assert_allclose(y_np, y_np_2, atol=1e-4)\n-\n-    # Test stacking.\n-    cells = [MinimalRNNCell(8),\n-             MinimalRNNCell(12),\n-             MinimalRNNCell(32)]\n-    layer = recurrent.RNN(cells)\n-    y = layer(x)\n-    model = keras.models.Model(x, y)\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n-\n-    # Test stacked RNN serialization.\n-    x_np = np.random.random((6, 5, 5))\n-    y_np = model.predict(x_np)\n-    weights = model.get_weights()\n-    config = layer.get_config()\n-    with keras.utils.CustomObjectScope({'MinimalRNNCell': MinimalRNNCell}):\n-        layer = recurrent.RNN.from_config(config)\n-    y = layer(x)\n-    model = keras.models.Model(x, y)\n-    model.set_weights(weights)\n-    y_np_2 = model.predict(x_np)\n-    assert_allclose(y_np, y_np_2, atol=1e-4)\n-\n-\n-@rnn_cell_test\n-def test_builtin_rnn_cell_layer(cell_class):\n-    # Test basic case.\n-    x = keras.Input((None, 5))\n-    cell = cell_class(32)\n-    layer = recurrent.RNN(cell)\n-    y = layer(x)\n-    model = keras.models.Model(x, y)\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n-\n-    # Test basic case serialization.\n-    x_np = np.random.random((6, 5, 5))\n-    y_np = model.predict(x_np)\n-    weights = model.get_weights()\n-    config = layer.get_config()\n-    layer = recurrent.RNN.from_config(config)\n-    y = layer(x)\n-    model = keras.models.Model(x, y)\n-    model.set_weights(weights)\n-    y_np_2 = model.predict(x_np)\n-    assert_allclose(y_np, y_np_2, atol=1e-4)\n-\n-    # Test stacking.\n-    cells = [cell_class(8),\n-             cell_class(12),\n-             cell_class(32)]\n-    layer = recurrent.RNN(cells)\n-    y = layer(x)\n-    model = keras.models.Model(x, y)\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n-\n-    # Test stacked RNN serialization.\n-    x_np = np.random.random((6, 5, 5))\n-    y_np = model.predict(x_np)\n-    weights = model.get_weights()\n-    config = layer.get_config()\n-    layer = recurrent.RNN.from_config(config)\n-    y = layer(x)\n-    model = keras.models.Model(x, y)\n-    model.set_weights(weights)\n-    y_np_2 = model.predict(x_np)\n-    assert_allclose(y_np, y_np_2, atol=1e-4)\n-\n-\n-@pytest.mark.skipif((K.backend() in ['cntk', 'theano']),\n-                    reason='Not supported.')\n-def test_stacked_rnn_dropout():\n-    cells = [recurrent.LSTMCell(3, dropout=0.1, recurrent_dropout=0.1),\n-             recurrent.LSTMCell(3, dropout=0.1, recurrent_dropout=0.1)]\n-    layer = recurrent.RNN(cells)\n-\n-    x = keras.Input((None, 5))\n-    y = layer(x)\n-    model = keras.models.Model(x, y)\n-    model.compile('sgd', 'mse')\n-    x_np = np.random.random((6, 5, 5))\n-    y_np = np.random.random((6, 3))\n-    model.train_on_batch(x_np, y_np)\n-\n-\n-def test_stacked_rnn_attributes():\n-    cells = [recurrent.LSTMCell(3),\n-             recurrent.LSTMCell(3, kernel_regularizer='l2')]\n-    layer = recurrent.RNN(cells)\n-    layer.build((None, None, 5))\n-\n-    # Test regularization losses\n-    assert len(layer.losses) == 1\n-\n-    # Test weights\n-    assert len(layer.trainable_weights) == 6\n-    cells[0].trainable = False\n-    assert len(layer.trainable_weights) == 3\n-    assert len(layer.non_trainable_weights) == 3\n-\n-    x = keras.Input((None, 5))\n-    y = K.sum(x)\n-    cells[0].add_loss(y, inputs=x)\n-\n-\n-def test_stacked_rnn_compute_output_shape():\n-    cells = [recurrent.LSTMCell(3),\n-             recurrent.LSTMCell(6)]\n-    layer = recurrent.RNN(cells, return_state=True, return_sequences=True)\n-    output_shape = layer.compute_output_shape((None, timesteps, embedding_dim))\n-    expected_output_shape = [(None, timesteps, 6),\n-                             (None, 3),\n-                             (None, 3),\n-                             (None, 6),\n-                             (None, 6)]\n-    assert [tuple(s) for s in output_shape] == expected_output_shape\n-\n-    # Test reverse_state_order = True for stacked cell.\n-    stacked_cell = recurrent.StackedRNNCells(\n-        cells, reverse_state_order=True)\n-    layer = recurrent.RNN(\n-        stacked_cell, return_state=True, return_sequences=True)\n-    output_shape = layer.compute_output_shape((None, timesteps, embedding_dim))\n-    expected_output_shape = [(None, timesteps, 6),\n-                             (None, 6),\n-                             (None, 6),\n-                             (None, 3),\n-                             (None, 3)]\n-    assert [tuple(s) for s in output_shape] == expected_output_shape\n-\n-\n-@rnn_test\n-def test_batch_size_equal_one(layer_class):\n-    inputs = Input(batch_shape=(1, timesteps, embedding_dim))\n-    layer = layer_class(units)\n-    outputs = layer(inputs)\n-    model = Model(inputs, outputs)\n-    model.compile('sgd', 'mse')\n-    x = np.random.random((1, timesteps, embedding_dim))\n-    y = np.random.random((1, units))\n-    model.train_on_batch(x, y)\n-\n-\n-def DISABLED_test_rnn_cell_with_constants_layer():\n-\n-    class RNNCellWithConstants(keras.layers.Layer):\n-\n-        def __init__(self, units, **kwargs):\n-            self.units = units\n-            self.state_size = units\n-            super(RNNCellWithConstants, self).__init__(**kwargs)\n-\n-        def build(self, input_shape):\n-            if not isinstance(input_shape, list):\n-                raise TypeError('expects `constants` shape')\n-            [input_shape, constant_shape] = input_shape\n-            # will (and should) raise if more than one constant passed\n-\n-            self.input_kernel = self.add_weight(\n-                shape=(input_shape[-1], self.units),\n-                initializer='uniform',\n-                name='kernel')\n-            self.recurrent_kernel = self.add_weight(\n-                shape=(self.units, self.units),\n-                initializer='uniform',\n-                name='recurrent_kernel')\n-            self.constant_kernel = self.add_weight(\n-                shape=(constant_shape[-1], self.units),\n-                initializer='uniform',\n-                name='constant_kernel')\n-            self.built = True\n-\n-        def call(self, inputs, states, constants):\n-            [prev_output] = states\n-            [constant] = constants\n-            h_input = keras.backend.dot(inputs, self.input_kernel)\n-            h_state = keras.backend.dot(prev_output, self.recurrent_kernel)\n-            h_const = keras.backend.dot(constant, self.constant_kernel)\n-            output = h_input + h_state + h_const\n-            return output, [output]\n-\n-        def get_config(self):\n-            config = {'units': self.units}\n-            base_config = super(RNNCellWithConstants, self).get_config()\n-            return dict(list(base_config.items()) + list(config.items()))\n-\n-    # Test basic case.\n-    x = keras.Input((None, 5))\n-    c = keras.Input((3,))\n-    cell = RNNCellWithConstants(32)\n-    layer = recurrent.RNN(cell)\n-    y = layer(x, constants=c)\n-    model = keras.models.Model([x, c], y)\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model.train_on_batch(\n-        [np.zeros((6, 5, 5)), np.zeros((6, 3))],\n-        np.zeros((6, 32))\n-    )\n-\n-    # Test basic case serialization.\n-    x_np = np.random.random((6, 5, 5))\n-    c_np = np.random.random((6, 3))\n-    y_np = model.predict([x_np, c_np])\n-    weights = model.get_weights()\n-    config = layer.get_config()\n-    custom_objects = {'RNNCellWithConstants': RNNCellWithConstants}\n-    with keras.utils.CustomObjectScope(custom_objects):\n-        layer = recurrent.RNN.from_config(config.copy())\n-    y = layer(x, constants=c)\n-    model = keras.models.Model([x, c], y)\n-    model.set_weights(weights)\n-    y_np_2 = model.predict([x_np, c_np])\n-    assert_allclose(y_np, y_np_2, atol=1e-4)\n-\n-    # test flat list inputs\n-    with keras.utils.CustomObjectScope(custom_objects):\n-        layer = recurrent.RNN.from_config(config.copy())\n-    y = layer([x, c])\n-    model = keras.models.Model([x, c], y)\n-    model.set_weights(weights)\n-    y_np_3 = model.predict([x_np, c_np])\n-    assert_allclose(y_np, y_np_3, atol=1e-4)\n-\n-    # Test stacking.\n-    cells = [recurrent.GRUCell(8),\n-             RNNCellWithConstants(12),\n-             RNNCellWithConstants(32)]\n-    layer = recurrent.RNN(cells)\n-    y = layer(x, constants=c)\n-    model = keras.models.Model([x, c], y)\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model.train_on_batch(\n-        [np.zeros((6, 5, 5)), np.zeros((6, 3))],\n-        np.zeros((6, 32))\n-    )\n-\n-    # Test stacked RNN serialization.\n-    x_np = np.random.random((6, 5, 5))\n-    c_np = np.random.random((6, 3))\n-    y_np = model.predict([x_np, c_np])\n-    weights = model.get_weights()\n-    config = layer.get_config()\n-    with keras.utils.CustomObjectScope(custom_objects):\n-        layer = recurrent.RNN.from_config(config.copy())\n-    y = layer(x, constants=c)\n-    model = keras.models.Model([x, c], y)\n-    model.set_weights(weights)\n-    y_np_2 = model.predict([x_np, c_np])\n-    assert_allclose(y_np, y_np_2, atol=1e-4)\n-\n-\n-def DISABLED_test_rnn_cell_with_constants_layer_passing_initial_state():\n-\n-    class RNNCellWithConstants(keras.layers.Layer):\n-\n-        def __init__(self, units, **kwargs):\n-            self.units = units\n-            self.state_size = units\n-            super(RNNCellWithConstants, self).__init__(**kwargs)\n-\n-        def build(self, input_shape):\n-            if not isinstance(input_shape, list):\n-                raise TypeError('expects constants shape')\n-            [input_shape, constant_shape] = input_shape\n-            # will (and should) raise if more than one constant passed\n-\n-            self.input_kernel = self.add_weight(\n-                shape=(input_shape[-1], self.units),\n-                initializer='uniform',\n-                name='kernel')\n-            self.recurrent_kernel = self.add_weight(\n-                shape=(self.units, self.units),\n-                initializer='uniform',\n-                name='recurrent_kernel')\n-            self.constant_kernel = self.add_weight(\n-                shape=(constant_shape[-1], self.units),\n-                initializer='uniform',\n-                name='constant_kernel')\n-            self.built = True\n-\n-        def call(self, inputs, states, constants):\n-            [prev_output] = states\n-            [constant] = constants\n-            h_input = keras.backend.dot(inputs, self.input_kernel)\n-            h_state = keras.backend.dot(prev_output, self.recurrent_kernel)\n-            h_const = keras.backend.dot(constant, self.constant_kernel)\n-            output = h_input + h_state + h_const\n-            return output, [output]\n-\n-        def get_config(self):\n-            config = {'units': self.units}\n-            base_config = super(RNNCellWithConstants, self).get_config()\n-            return dict(list(base_config.items()) + list(config.items()))\n-\n-    # Test basic case.\n-    x = keras.Input((None, 5))\n-    c = keras.Input((3,))\n-    s = keras.Input((32,))\n-    cell = RNNCellWithConstants(32)\n-    layer = recurrent.RNN(cell)\n-    y = layer(x, initial_state=s, constants=c)\n-    model = keras.models.Model([x, s, c], y)\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model.train_on_batch(\n-        [np.zeros((6, 5, 5)), np.zeros((6, 32)), np.zeros((6, 3))],\n-        np.zeros((6, 32))\n-    )\n-\n-    # Test basic case serialization.\n-    x_np = np.random.random((6, 5, 5))\n-    s_np = np.random.random((6, 32))\n-    c_np = np.random.random((6, 3))\n-    y_np = model.predict([x_np, s_np, c_np])\n-    weights = model.get_weights()\n-    config = layer.get_config()\n-    custom_objects = {'RNNCellWithConstants': RNNCellWithConstants}\n-    with keras.utils.CustomObjectScope(custom_objects):\n-        layer = recurrent.RNN.from_config(config.copy())\n-    y = layer(x, initial_state=s, constants=c)\n-    model = keras.models.Model([x, s, c], y)\n-    model.set_weights(weights)\n-    y_np_2 = model.predict([x_np, s_np, c_np])\n-    assert_allclose(y_np, y_np_2, atol=1e-4)\n-\n-    # verify that state is used\n-    y_np_2_different_s = model.predict([x_np, s_np + 10., c_np])\n-    with pytest.raises(AssertionError):\n-        assert_allclose(y_np, y_np_2_different_s, atol=1e-4)\n-\n-    # test flat list inputs\n-    with keras.utils.CustomObjectScope(custom_objects):\n-        layer = recurrent.RNN.from_config(config.copy())\n-    y = layer([x, s, c])\n-    model = keras.models.Model([x, s, c], y)\n-    model.set_weights(weights)\n-    y_np_3 = model.predict([x_np, s_np, c_np])\n-    assert_allclose(y_np, y_np_3, atol=1e-4)\n-\n-\n-@rnn_test\n-def DISABLED_test_rnn_cell_identity_initializer(layer_class):\n-    inputs = Input(shape=(1, 2))\n-    layer = layer_class(2, recurrent_initializer='identity')\n-    layer(inputs)\n-    recurrent_kernel = layer.get_weights()[1]\n-    num_kernels = recurrent_kernel.shape[1] // recurrent_kernel.shape[0]\n-    assert np.array_equal(recurrent_kernel,\n-                          np.concatenate([np.identity(2)] * num_kernels, axis=1))\n-\n-\n-@pytest.mark.skipif(K.backend() == 'cntk', reason='Not supported.')\n-def test_inconsistent_output_state_size():\n-\n-    class PlusOneRNNCell(keras.layers.Layer):\n-        \"\"\"Add one to the input and state.\n-\n-        This cell is used for testing state_size and output_size.\"\"\"\n-\n-        def __init__(self, num_unit, **kwargs):\n-            self.state_size = num_unit\n-            super(PlusOneRNNCell, self).__init__(**kwargs)\n-\n-        def build(self, input_shape):\n-            self.output_size = input_shape[-1]\n-\n-        def call(self, inputs, states):\n-            return inputs + 1, [states[0] + 1]\n-\n-    batch = 32\n-    time_step = 4\n-    state_size = 5\n-    input_size = 6\n-    cell = PlusOneRNNCell(state_size)\n-    x = keras.Input((None, input_size))\n-    layer = recurrent.RNN(cell)\n-    y = layer(x)\n-\n-    assert cell.state_size == state_size\n-    init_state = layer.get_initial_state(x)\n-    assert len(init_state) == 1\n-    if K.backend() != 'theano':\n-        # theano does not support static shape inference.\n-        assert K.int_shape(init_state[0]) == (None, state_size)\n-\n-    model = keras.models.Model(x, y)\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model.train_on_batch(\n-        np.zeros((batch, time_step, input_size)),\n-        np.zeros((batch, input_size)))\n-    assert model.output_shape == (None, input_size)\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,606 +0,0 @@\n-import pytest\n-import numpy as np\n-import copy\n-from numpy.testing import assert_allclose\n-from keras.utils import CustomObjectScope\n-from keras.layers import wrappers, Input, Layer\n-from keras.layers import RNN\n-from keras import layers\n-from keras.models import Sequential, Model, model_from_json\n-from keras import backend as K\n-from keras.utils.generic_utils import object_list_uid, to_list\n-\n-\n-def test_TimeDistributed():\n-    # first, test with Dense layer\n-    model = Sequential()\n-    model.add(wrappers.TimeDistributed(layers.Dense(2), input_shape=(3, 4)))\n-    model.add(layers.Activation('relu'))\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model.fit(np.random.random((10, 3, 4)), np.random.random((10, 3, 2)),\n-              epochs=1,\n-              batch_size=10)\n-\n-    # test config\n-    model.get_config()\n-\n-    # test when specifying a batch_input_shape\n-    test_input = np.random.random((1, 3, 4))\n-    test_output = model.predict(test_input)\n-    weights = model.layers[0].get_weights()\n-\n-    reference = Sequential()\n-    reference.add(wrappers.TimeDistributed(layers.Dense(2),\n-                                           batch_input_shape=(1, 3, 4)))\n-    reference.add(layers.Activation('relu'))\n-    reference.compile(optimizer='rmsprop', loss='mse')\n-    reference.layers[0].set_weights(weights)\n-\n-    reference_output = reference.predict(test_input)\n-    assert_allclose(test_output, reference_output, atol=1e-05)\n-\n-    # test with Embedding\n-    model = Sequential()\n-    model.add(wrappers.TimeDistributed(layers.Embedding(5, 6),\n-                                       batch_input_shape=(10, 3, 4),\n-                                       dtype='int32'))\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model.fit(np.random.randint(5, size=(10, 3, 4), dtype='int32'),\n-              np.random.random((10, 3, 4, 6)), epochs=1, batch_size=10)\n-\n-    # compare to not using batch_input_shape\n-    test_input = np.random.randint(5, size=(10, 3, 4), dtype='int32')\n-    test_output = model.predict(test_input)\n-    weights = model.layers[0].get_weights()\n-\n-    reference = Sequential()\n-    reference.add(wrappers.TimeDistributed(layers.Embedding(5, 6),\n-                                           input_shape=(3, 4), dtype='int32'))\n-    reference.compile(optimizer='rmsprop', loss='mse')\n-    reference.layers[0].set_weights(weights)\n-\n-    reference_output = reference.predict(test_input)\n-    assert_allclose(test_output, reference_output, atol=1e-05)\n-\n-    # test with Conv2D\n-    model = Sequential()\n-    model.add(wrappers.TimeDistributed(layers.Conv2D(5, (2, 2),\n-                                                     padding='same'),\n-                                       input_shape=(2, 4, 4, 3)))\n-    model.add(layers.Activation('relu'))\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model.train_on_batch(np.random.random((1, 2, 4, 4, 3)),\n-                         np.random.random((1, 2, 4, 4, 5)))\n-\n-    model = model_from_json(model.to_json())\n-    model.summary()\n-\n-    # test stacked layers\n-    model = Sequential()\n-    model.add(wrappers.TimeDistributed(layers.Dense(2), input_shape=(3, 4)))\n-    model.add(wrappers.TimeDistributed(layers.Dense(3)))\n-    model.add(layers.Activation('relu'))\n-    model.compile(optimizer='rmsprop', loss='mse')\n-\n-    model.fit(np.random.random((10, 3, 4)), np.random.random((10, 3, 3)),\n-              epochs=1, batch_size=10)\n-\n-    # test wrapping Sequential model\n-    model = Sequential()\n-    model.add(layers.Dense(3, input_dim=2))\n-    outer_model = Sequential()\n-    outer_model.add(wrappers.TimeDistributed(model, input_shape=(3, 2)))\n-    outer_model.compile(optimizer='rmsprop', loss='mse')\n-    outer_model.fit(np.random.random((10, 3, 2)), np.random.random((10, 3, 3)),\n-                    epochs=1, batch_size=10)\n-\n-    # test with functional API\n-    x = Input(shape=(3, 2))\n-    y = wrappers.TimeDistributed(model)(x)\n-    outer_model = Model(x, y)\n-    outer_model.compile(optimizer='rmsprop', loss='mse')\n-    outer_model.fit(np.random.random((10, 3, 2)), np.random.random((10, 3, 3)),\n-                    epochs=1, batch_size=10)\n-\n-    # test with BatchNormalization\n-    model = Sequential()\n-    model.add(wrappers.TimeDistributed(\n-        layers.BatchNormalization(center=True, scale=True),\n-        name='bn', input_shape=(10, 2)))\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    # Assert that mean and variance are 0 and 1.\n-    td = model.layers[0]\n-    assert np.array_equal(td.get_weights()[2], np.array([0, 0]))\n-    assert np.array_equal(td.get_weights()[3], np.array([1, 1]))\n-    # Train\n-    model.train_on_batch(np.random.normal(loc=2, scale=2, size=(1, 10, 2)),\n-                         np.broadcast_to(np.array([0, 1]), (1, 10, 2)))\n-    # Assert that mean and variance changed.\n-    assert not np.array_equal(td.get_weights()[2], np.array([0, 0]))\n-    assert not np.array_equal(td.get_weights()[3], np.array([1, 1]))\n-\n-\n-@pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason='Flaky with CNTK backend')\n-def test_TimeDistributed_learning_phase():\n-    # test layers that need learning_phase to be set\n-    np.random.seed(1234)\n-    x = Input(shape=(3, 2))\n-    y = wrappers.TimeDistributed(layers.Dropout(.999))(x, training=True)\n-    model = Model(x, y)\n-    y = model.predict(np.random.random((10, 3, 2)))\n-    assert_allclose(np.mean(y), 0., atol=1e-1, rtol=1e-1)\n-\n-\n-def test_TimeDistributed_trainable():\n-    # test layers that need learning_phase to be set\n-    x = Input(shape=(3, 2))\n-    layer = wrappers.TimeDistributed(layers.BatchNormalization())\n-    _ = layer(x)\n-    assert len(layer.trainable_weights) == 2\n-    layer.trainable = False\n-    assert len(layer.trainable_weights) == 0\n-    layer.trainable = True\n-    assert len(layer.trainable_weights) == 2\n-\n-\n-@pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason='Unknown timestamps for RNN not supported in CNTK.')\n-def test_TimeDistributed_with_masked_embedding_and_unspecified_shape():\n-    # test with unspecified shape and Embeddings with mask_zero\n-    model = Sequential()\n-    model.add(wrappers.TimeDistributed(layers.Embedding(5, 6, mask_zero=True),\n-                                       input_shape=(None, None)))\n-    # the shape so far: (N, t_1, t_2, 6)\n-    model.add(wrappers.TimeDistributed(layers.SimpleRNN(7, return_sequences=True)))\n-    model.add(wrappers.TimeDistributed(layers.SimpleRNN(8, return_sequences=False)))\n-    model.add(layers.SimpleRNN(1, return_sequences=False))\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model_input = np.random.randint(low=1, high=5, size=(10, 3, 4), dtype='int32')\n-    for i in range(4):\n-        model_input[i, i:, i:] = 0\n-    model.fit(model_input,\n-              np.random.random((10, 1)), epochs=1, batch_size=10)\n-    mask_outputs = [model.layers[0].compute_mask(model.input)]\n-    for layer in model.layers[1:]:\n-        mask_outputs.append(layer.compute_mask(layer.input, mask_outputs[-1]))\n-    func = K.function([model.input], mask_outputs[:-1])\n-    mask_outputs_val = func([model_input])\n-    ref_mask_val_0 = model_input > 0         # embedding layer\n-    ref_mask_val_1 = ref_mask_val_0          # first RNN layer\n-    ref_mask_val_2 = np.any(ref_mask_val_1, axis=-1)     # second RNN layer\n-    ref_mask_val = [ref_mask_val_0, ref_mask_val_1, ref_mask_val_2]\n-    for i in range(3):\n-        assert np.array_equal(mask_outputs_val[i], ref_mask_val[i])\n-    assert mask_outputs[-1] is None  # final layer\n-\n-\n-def test_TimeDistributed_with_masking_layer():\n-    # test with Masking layer\n-    model = Sequential()\n-    model.add(wrappers.TimeDistributed(layers.Masking(mask_value=0.,),\n-                                       input_shape=(None, 4)))\n-    model.add(wrappers.TimeDistributed(layers.Dense(5)))\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model_input = np.random.randint(low=1, high=5, size=(10, 3, 4))\n-    for i in range(4):\n-        model_input[i, i:, :] = 0.\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model.fit(model_input,\n-              np.random.random((10, 3, 5)), epochs=1, batch_size=6)\n-    mask_outputs = [model.layers[0].compute_mask(model.input)]\n-    mask_outputs += [model.layers[1].compute_mask(model.layers[1].input,\n-                                                  mask_outputs[-1])]\n-    func = K.function([model.input], mask_outputs)\n-    mask_outputs_val = func([model_input])\n-    assert np.array_equal(mask_outputs_val[0], np.any(model_input, axis=-1))\n-    assert np.array_equal(mask_outputs_val[1], np.any(model_input, axis=-1))\n-\n-\n-def test_regularizers():\n-    model = Sequential()\n-    model.add(wrappers.TimeDistributed(\n-        layers.Dense(2, kernel_regularizer='l1'), input_shape=(3, 4)))\n-    model.add(layers.Activation('relu'))\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    assert len(model.layers[0].layer.losses) == 1\n-    assert len(model.layers[0].losses) == 1\n-    assert len(model.layers[0].get_losses_for(None)) == 1\n-    assert len(model.losses) == 1\n-\n-    model = Sequential()\n-    model.add(wrappers.TimeDistributed(\n-        layers.Dense(2, activity_regularizer='l1'), input_shape=(3, 4)))\n-    model.add(layers.Activation('relu'))\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    assert len(model.losses) == 1\n-\n-\n-def test_Bidirectional():\n-    rnn = layers.SimpleRNN\n-    samples = 2\n-    dim = 2\n-    timesteps = 2\n-    output_dim = 2\n-    dropout_rate = 0.2\n-    for mode in ['sum', 'concat']:\n-        x = np.random.random((samples, timesteps, dim))\n-        target_dim = 2 * output_dim if mode == 'concat' else output_dim\n-        y = np.random.random((samples, target_dim))\n-\n-        # test with Sequential model\n-        model = Sequential()\n-        model.add(wrappers.Bidirectional(rnn(output_dim, dropout=dropout_rate,\n-                                             recurrent_dropout=dropout_rate),\n-                                         merge_mode=mode,\n-                                         input_shape=(timesteps, dim)))\n-        model.compile(loss='mse', optimizer='sgd')\n-        model.fit(x, y, epochs=1, batch_size=1)\n-\n-        # test config\n-        model.get_config()\n-        model = model_from_json(model.to_json())\n-        model.summary()\n-\n-        # test stacked bidirectional layers\n-        model = Sequential()\n-        model.add(wrappers.Bidirectional(rnn(output_dim,\n-                                             return_sequences=True),\n-                                         merge_mode=mode,\n-                                         input_shape=(timesteps, dim)))\n-        model.add(wrappers.Bidirectional(rnn(output_dim), merge_mode=mode))\n-        model.compile(loss='mse', optimizer='sgd')\n-        model.fit(x, y, epochs=1, batch_size=1)\n-\n-        # Bidirectional and stateful\n-        inputs = Input(batch_shape=(1, timesteps, dim))\n-        outputs = wrappers.Bidirectional(rnn(output_dim, stateful=True),\n-                                         merge_mode=mode)(inputs)\n-        model = Model(inputs, outputs)\n-        model.compile(loss='mse', optimizer='sgd')\n-        model.fit(x, y, epochs=1, batch_size=1)\n-\n-\n-@pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason='Unknown timestamps not supported in CNTK.')\n-def test_Bidirectional_dynamic_timesteps():\n-    # test with functional API with dynamic length\n-    rnn = layers.SimpleRNN\n-    samples = 2\n-    dim = 2\n-    timesteps = 2\n-    output_dim = 2\n-    dropout_rate = 0.2\n-    for mode in ['sum', 'concat']:\n-        x = np.random.random((samples, timesteps, dim))\n-        target_dim = 2 * output_dim if mode == 'concat' else output_dim\n-        y = np.random.random((samples, target_dim))\n-\n-        inputs = Input((None, dim))\n-        outputs = wrappers.Bidirectional(rnn(output_dim, dropout=dropout_rate,\n-                                             recurrent_dropout=dropout_rate),\n-                                         merge_mode=mode)(inputs)\n-        model = Model(inputs, outputs)\n-        model.compile(loss='mse', optimizer='sgd')\n-        model.fit(x, y, epochs=1, batch_size=1)\n-\n-\n-@pytest.mark.parametrize('merge_mode', ['sum', 'mul', 'ave', 'concat', None])\n-def test_Bidirectional_merged_value(merge_mode):\n-    rnn = layers.LSTM\n-    samples = 2\n-    dim = 5\n-    timesteps = 3\n-    units = 3\n-    X = [np.random.rand(samples, timesteps, dim)]\n-\n-    if merge_mode == 'sum':\n-        merge_func = lambda y, y_rev: y + y_rev\n-    elif merge_mode == 'mul':\n-        merge_func = lambda y, y_rev: y * y_rev\n-    elif merge_mode == 'ave':\n-        merge_func = lambda y, y_rev: (y + y_rev) / 2\n-    elif merge_mode == 'concat':\n-        merge_func = lambda y, y_rev: np.concatenate((y, y_rev), axis=-1)\n-    else:\n-        merge_func = lambda y, y_rev: [y, y_rev]\n-\n-    # basic case\n-    inputs = Input((timesteps, dim))\n-    layer = wrappers.Bidirectional(rnn(units, return_sequences=True),\n-                                   merge_mode=merge_mode)\n-    f_merged = K.function([inputs], to_list(layer(inputs)))\n-    f_forward = K.function([inputs], [layer.forward_layer(inputs)])\n-    f_backward = K.function([inputs],\n-                            [K.reverse(layer.backward_layer(inputs), 1)])\n-\n-    y_merged = f_merged(X)\n-    y_expected = to_list(merge_func(f_forward(X)[0], f_backward(X)[0]))\n-    assert len(y_merged) == len(y_expected)\n-    for x1, x2 in zip(y_merged, y_expected):\n-        assert_allclose(x1, x2, atol=1e-5)\n-\n-    # test return_state\n-    inputs = Input((timesteps, dim))\n-    layer = wrappers.Bidirectional(rnn(units, return_state=True),\n-                                   merge_mode=merge_mode)\n-    f_merged = K.function([inputs], layer(inputs))\n-    f_forward = K.function([inputs], layer.forward_layer(inputs))\n-    f_backward = K.function([inputs], layer.backward_layer(inputs))\n-    n_states = len(layer.layer.states)\n-\n-    y_merged = f_merged(X)\n-    y_forward = f_forward(X)\n-    y_backward = f_backward(X)\n-    y_expected = to_list(merge_func(y_forward[0], y_backward[0]))\n-    assert len(y_merged) == len(y_expected) + n_states * 2\n-    for x1, x2 in zip(y_merged, y_expected):\n-        assert_allclose(x1, x2, atol=1e-5)\n-\n-    # test if the state of a BiRNN is the concatenation of the underlying RNNs\n-    y_merged = y_merged[-n_states * 2:]\n-    y_forward = y_forward[-n_states:]\n-    y_backward = y_backward[-n_states:]\n-    for state_birnn, state_inner in zip(y_merged, y_forward + y_backward):\n-        assert_allclose(state_birnn, state_inner, atol=1e-5)\n-\n-\n-@pytest.mark.skipif(K.backend() == 'theano', reason='Not supported.')\n-@pytest.mark.parametrize('merge_mode', ['sum', 'concat', None])\n-def test_Bidirectional_dropout(merge_mode):\n-    rnn = layers.LSTM\n-    samples = 2\n-    dim = 5\n-    timesteps = 3\n-    units = 3\n-    X = [np.random.rand(samples, timesteps, dim)]\n-\n-    inputs = Input((timesteps, dim))\n-    wrapped = wrappers.Bidirectional(rnn(units, dropout=0.2, recurrent_dropout=0.2),\n-                                     merge_mode=merge_mode)\n-    outputs = to_list(wrapped(inputs, training=True))\n-\n-    inputs = Input((timesteps, dim))\n-    wrapped = wrappers.Bidirectional(rnn(units, dropout=0.2, return_state=True),\n-                                     merge_mode=merge_mode)\n-    outputs = to_list(wrapped(inputs))\n-\n-    model = Model(inputs, outputs)\n-\n-    y1 = to_list(model.predict(X))\n-    y2 = to_list(model.predict(X))\n-    for x1, x2 in zip(y1, y2):\n-        assert_allclose(x1, x2, atol=1e-5)\n-\n-\n-def test_Bidirectional_state_reuse():\n-    rnn = layers.LSTM\n-    samples = 2\n-    dim = 5\n-    timesteps = 3\n-    units = 3\n-\n-    input1 = Input((timesteps, dim))\n-    layer = wrappers.Bidirectional(rnn(units, return_state=True,\n-                                       return_sequences=True))\n-    state = layer(input1)[1:]\n-\n-    # test passing invalid initial_state: passing a tensor\n-    input2 = Input((timesteps, dim))\n-    with pytest.raises(ValueError):\n-        output = wrappers.Bidirectional(rnn(units))(input2, initial_state=state[0])\n-\n-    # test valid usage: passing a list\n-    output = wrappers.Bidirectional(rnn(units))(input2, initial_state=state)\n-    model = Model([input1, input2], output)\n-    assert len(model.layers) == 4\n-    assert isinstance(model.layers[-1].input, list)\n-    inputs = [np.random.rand(samples, timesteps, dim),\n-              np.random.rand(samples, timesteps, dim)]\n-    outputs = model.predict(inputs)\n-\n-\n-def DISABLED_test_Bidirectional_with_constants():\n-    class RNNCellWithConstants(Layer):\n-        def __init__(self, units, **kwargs):\n-            self.units = units\n-            self.state_size = units\n-            super(RNNCellWithConstants, self).__init__(**kwargs)\n-\n-        def build(self, input_shape):\n-            if not isinstance(input_shape, list):\n-                raise TypeError('expects constants shape')\n-            [input_shape, constant_shape] = input_shape\n-            # will (and should) raise if more than one constant passed\n-\n-            self.input_kernel = self.add_weight(\n-                shape=(input_shape[-1], self.units),\n-                initializer='uniform',\n-                name='kernel')\n-            self.recurrent_kernel = self.add_weight(\n-                shape=(self.units, self.units),\n-                initializer='uniform',\n-                name='recurrent_kernel')\n-            self.constant_kernel = self.add_weight(\n-                shape=(constant_shape[-1], self.units),\n-                initializer='uniform',\n-                name='constant_kernel')\n-            self.built = True\n-\n-        def call(self, inputs, states, constants):\n-            [prev_output] = states\n-            [constant] = constants\n-            h_input = K.dot(inputs, self.input_kernel)\n-            h_state = K.dot(prev_output, self.recurrent_kernel)\n-            h_const = K.dot(constant, self.constant_kernel)\n-            output = h_input + h_state + h_const\n-            return output, [output]\n-\n-        def get_config(self):\n-            config = {'units': self.units}\n-            base_config = super(RNNCellWithConstants, self).get_config()\n-            return dict(list(base_config.items()) + list(config.items()))\n-\n-    # Test basic case.\n-    x = Input((5, 5))\n-    c = Input((3,))\n-    cell = RNNCellWithConstants(32)\n-    custom_objects = {'RNNCellWithConstants': RNNCellWithConstants}\n-    with CustomObjectScope(custom_objects):\n-        layer = wrappers.Bidirectional(RNN(cell))\n-    y = layer(x, constants=c)\n-    model = Model([x, c], y)\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model.train_on_batch(\n-        [np.zeros((6, 5, 5)), np.zeros((6, 3))],\n-        np.zeros((6, 64))\n-    )\n-\n-    # Test basic case serialization.\n-    x_np = np.random.random((6, 5, 5))\n-    c_np = np.random.random((6, 3))\n-    y_np = model.predict([x_np, c_np])\n-    weights = model.get_weights()\n-    config = layer.get_config()\n-    with CustomObjectScope(custom_objects):\n-        layer = wrappers.Bidirectional.from_config(copy.deepcopy(config))\n-    y = layer(x, constants=c)\n-    model = Model([x, c], y)\n-    model.set_weights(weights)\n-    y_np_2 = model.predict([x_np, c_np])\n-    assert_allclose(y_np, y_np_2, atol=1e-4)\n-\n-    # test flat list inputs\n-    with CustomObjectScope(custom_objects):\n-        layer = wrappers.Bidirectional.from_config(copy.deepcopy(config))\n-    y = layer([x, c])\n-    model = Model([x, c], y)\n-    model.set_weights(weights)\n-    y_np_3 = model.predict([x_np, c_np])\n-    assert_allclose(y_np, y_np_3, atol=1e-4)\n-\n-\n-def DISABLED_test_Bidirectional_with_constants_layer_passing_initial_state():\n-    class RNNCellWithConstants(Layer):\n-        def __init__(self, units, **kwargs):\n-            self.units = units\n-            self.state_size = units\n-            super(RNNCellWithConstants, self).__init__(**kwargs)\n-\n-        def build(self, input_shape):\n-            if not isinstance(input_shape, list):\n-                raise TypeError('expects constants shape')\n-            [input_shape, constant_shape] = input_shape\n-            # will (and should) raise if more than one constant passed\n-\n-            self.input_kernel = self.add_weight(\n-                shape=(input_shape[-1], self.units),\n-                initializer='uniform',\n-                name='kernel')\n-            self.recurrent_kernel = self.add_weight(\n-                shape=(self.units, self.units),\n-                initializer='uniform',\n-                name='recurrent_kernel')\n-            self.constant_kernel = self.add_weight(\n-                shape=(constant_shape[-1], self.units),\n-                initializer='uniform',\n-                name='constant_kernel')\n-            self.built = True\n-\n-        def call(self, inputs, states, constants):\n-            [prev_output] = states\n-            [constant] = constants\n-            h_input = K.dot(inputs, self.input_kernel)\n-            h_state = K.dot(prev_output, self.recurrent_kernel)\n-            h_const = K.dot(constant, self.constant_kernel)\n-            output = h_input + h_state + h_const\n-            return output, [output]\n-\n-        def get_config(self):\n-            config = {'units': self.units}\n-            base_config = super(RNNCellWithConstants, self).get_config()\n-            return dict(list(base_config.items()) + list(config.items()))\n-\n-    # Test basic case.\n-    x = Input((5, 5))\n-    c = Input((3,))\n-    s_for = Input((32,))\n-    s_bac = Input((32,))\n-    cell = RNNCellWithConstants(32)\n-    custom_objects = {'RNNCellWithConstants': RNNCellWithConstants}\n-    with CustomObjectScope(custom_objects):\n-        layer = wrappers.Bidirectional(RNN(cell))\n-    y = layer(x, initial_state=[s_for, s_bac], constants=c)\n-    model = Model([x, s_for, s_bac, c], y)\n-    model.compile(optimizer='rmsprop', loss='mse')\n-    model.train_on_batch(\n-        [np.zeros((6, 5, 5)), np.zeros((6, 32)),\n-         np.zeros((6, 32)), np.zeros((6, 3))],\n-        np.zeros((6, 64))\n-    )\n-\n-    # Test basic case serialization.\n-    x_np = np.random.random((6, 5, 5))\n-    s_fw_np = np.random.random((6, 32))\n-    s_bk_np = np.random.random((6, 32))\n-    c_np = np.random.random((6, 3))\n-    y_np = model.predict([x_np, s_fw_np, s_bk_np, c_np])\n-    weights = model.get_weights()\n-    config = layer.get_config()\n-    with CustomObjectScope(custom_objects):\n-        layer = wrappers.Bidirectional.from_config(copy.deepcopy(config))\n-    y = layer(x, initial_state=[s_for, s_bac], constants=c)\n-    model = Model([x, s_for, s_bac, c], y)\n-    model.set_weights(weights)\n-    y_np_2 = model.predict([x_np, s_fw_np, s_bk_np, c_np])\n-    assert_allclose(y_np, y_np_2, atol=1e-4)\n-\n-    # verify that state is used\n-    y_np_2_different_s = model.predict([x_np, s_fw_np + 10., s_bk_np + 10., c_np])\n-    with pytest.raises(AssertionError):\n-        assert_allclose(y_np, y_np_2_different_s, atol=1e-4)\n-\n-    # test flat list inputs\n-    with CustomObjectScope(custom_objects):\n-        layer = wrappers.Bidirectional.from_config(copy.deepcopy(config))\n-    y = layer([x, s_for, s_bac, c])\n-    model = Model([x, s_for, s_bac, c], y)\n-    model.set_weights(weights)\n-    y_np_3 = model.predict([x_np, s_fw_np, s_bk_np, c_np])\n-    assert_allclose(y_np, y_np_3, atol=1e-4)\n-\n-\n-def test_Bidirectional_trainable():\n-    # test layers that need learning_phase to be set\n-    x = Input(shape=(3, 2))\n-    layer = wrappers.Bidirectional(layers.SimpleRNN(3))\n-    _ = layer(x)\n-    assert len(layer.trainable_weights) == 6\n-    layer.trainable = False\n-    assert len(layer.trainable_weights) == 0\n-    layer.trainable = True\n-    assert len(layer.trainable_weights) == 6\n-\n-\n-def test_Bidirectional_updates():\n-    x = Input(shape=(3, 2))\n-    layer = wrappers.Bidirectional(layers.SimpleRNN(3))\n-    layer.forward_layer.add_update(0, inputs=x)\n-    layer.forward_layer.add_update(1, inputs=None)\n-    layer.backward_layer.add_update(0, inputs=x)\n-    layer.backward_layer.add_update(1, inputs=None)\n-\n-\n-def test_Bidirectional_losses():\n-    x = Input(shape=(3, 2))\n-    layer = wrappers.Bidirectional(\n-        layers.SimpleRNN(3, kernel_regularizer='l1', bias_regularizer='l1'))\n-    _ = layer(x)\n-    layer.forward_layer.add_loss(lambda: 0)\n-    layer.forward_layer.add_loss(lambda: 1)\n-    layer.backward_layer.add_loss(lambda: 0)\n-    layer.backward_layer.add_loss(lambda: 1)\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,765 +0,0 @@\n-import pytest\n-import numpy as np\n-\n-import keras\n-from keras import losses\n-from keras.losses import Reduction\n-from keras import backend as K\n-from keras.utils import custom_object_scope\n-\n-\n-all_functions = [losses.mean_squared_error,\n-                 losses.mean_absolute_error,\n-                 losses.mean_absolute_percentage_error,\n-                 losses.mean_squared_logarithmic_error,\n-                 losses.squared_hinge,\n-                 losses.hinge,\n-                 losses.categorical_crossentropy,\n-                 losses.binary_crossentropy,\n-                 losses.kullback_leibler_divergence,\n-                 losses.poisson,\n-                 losses.cosine_similarity,\n-                 losses.logcosh,\n-                 losses.categorical_hinge]\n-all_classes = [\n-    losses.Hinge,\n-    losses.SquaredHinge,\n-    losses.CategoricalHinge,\n-    losses.Poisson,\n-    losses.LogCosh,\n-    losses.KLDivergence,\n-    losses.Huber,\n-    # losses.SparseCategoricalCrossentropy,\n-    losses.BinaryCrossentropy,\n-    losses.MeanSquaredLogarithmicError,\n-    losses.MeanAbsolutePercentageError,\n-    losses.MeanAbsoluteError,\n-    losses.MeanSquaredError,\n-]\n-\n-\n-class MSE_MAE_loss(losses.Loss):\n-    \"\"\"Loss function with internal state, for testing serialization code.\"\"\"\n-\n-    def __init__(self, mse_fraction):\n-        self.mse_fraction = mse_fraction\n-        super(MSE_MAE_loss, self).__init__()\n-\n-    def __call__(self, y_true, y_pred, sample_weight=None):\n-        return (self.mse_fraction * losses.mse(y_true, y_pred) +\n-                (1 - self.mse_fraction) * losses.mae(y_true, y_pred))\n-\n-    def get_config(self):\n-        return {'mse_fraction': self.mse_fraction}\n-\n-\n-class TestLossFunctions(object):\n-\n-    @pytest.mark.parametrize('loss_fn', all_functions)\n-    def test_objective_shapes_3d(self, loss_fn):\n-        y_a = K.variable(np.random.random((5, 6, 7)))\n-        y_b = K.variable(np.random.random((5, 6, 7)))\n-        objective_output = loss_fn(y_a, y_b)\n-        assert K.eval(objective_output).shape == (5, 6)\n-\n-    @pytest.mark.parametrize('loss_fn', all_functions)\n-    def test_objective_shapes_2d(self, loss_fn):\n-        y_a = K.variable(np.random.random((6, 7)))\n-        y_b = K.variable(np.random.random((6, 7)))\n-        objective_output = loss_fn(y_a, y_b)\n-        assert K.eval(objective_output).shape == (6,)\n-\n-    def test_cce_one_hot(self):\n-        y_a = K.variable(np.random.randint(0, 7, (5, 6)))\n-        y_b = K.variable(np.random.random((5, 6, 7)))\n-        objective_output = losses.sparse_categorical_crossentropy(y_a, y_b)\n-        assert K.eval(objective_output).shape == (5, 6)\n-\n-        y_a = K.variable(np.random.randint(0, 7, (6,)))\n-        y_b = K.variable(np.random.random((6, 7)))\n-        assert K.eval(losses.sparse_categorical_crossentropy(y_a, y_b)).shape == (6,)\n-\n-    def test_categorical_hinge(self):\n-        y_pred = K.variable(np.array([[0.3, 0.2, 0.1],\n-                                      [0.1, 0.2, 0.7]]))\n-        y_true = K.variable(np.array([[0, 1, 0],\n-                                      [1, 0, 0]]))\n-        expected_loss = ((0.3 - 0.2 + 1) + (0.7 - 0.1 + 1)) / 2.0\n-        loss = K.eval(losses.categorical_hinge(y_true, y_pred))\n-        assert np.isclose(expected_loss, np.mean(loss))\n-\n-    def test_sparse_categorical_crossentropy(self):\n-        y_pred = K.variable(np.array([[0.3, 0.6, 0.1],\n-                                      [0.1, 0.2, 0.7]]))\n-        y_true = K.variable(np.array([1, 2]))\n-        expected_loss = - (np.log(0.6) + np.log(0.7)) / 2\n-        loss = K.eval(losses.sparse_categorical_crossentropy(y_true, y_pred))\n-        assert np.isclose(expected_loss, np.mean(loss))\n-\n-    def test_sparse_categorical_crossentropy_4d(self):\n-        y_pred = K.variable(np.array([[[[0.7, 0.1, 0.2],\n-                                        [0.0, 0.3, 0.7],\n-                                        [0.1, 0.1, 0.8]],\n-                                       [[0.3, 0.7, 0.0],\n-                                        [0.3, 0.4, 0.3],\n-                                        [0.2, 0.5, 0.3]],\n-                                       [[0.8, 0.1, 0.1],\n-                                        [1.0, 0.0, 0.0],\n-                                        [0.4, 0.3, 0.3]]]]))\n-        y_true = K.variable(np.array([[[0, 1, 0],\n-                                       [2, 1, 0],\n-                                       [2, 2, 1]]]))\n-        expected_loss = - (np.log(0.7) + np.log(0.3) + np.log(0.1) +\n-                           np.log(K.epsilon()) + np.log(0.4) + np.log(0.2) +\n-                           np.log(0.1) + np.log(K.epsilon()) + np.log(0.3)) / 9\n-        loss = K.eval(losses.sparse_categorical_crossentropy(y_true, y_pred))\n-        assert np.isclose(expected_loss, np.mean(loss))\n-\n-    def test_serializing_loss_class(self):\n-        orig_loss_class = MSE_MAE_loss(0.3)\n-        with custom_object_scope({'MSE_MAE_loss': MSE_MAE_loss}):\n-            serialized = losses.serialize(orig_loss_class)\n-\n-        with custom_object_scope({'MSE_MAE_loss': MSE_MAE_loss}):\n-            deserialized = losses.deserialize(serialized)\n-        assert isinstance(deserialized, MSE_MAE_loss)\n-        assert deserialized.mse_fraction == 0.3\n-\n-    def test_serializing_model_with_loss_class(self, tmpdir):\n-        model_filename = str(tmpdir / 'custom_loss.hdf')\n-\n-        with custom_object_scope({'MSE_MAE_loss': MSE_MAE_loss}):\n-            loss = MSE_MAE_loss(0.3)\n-            inputs = keras.layers.Input((2,))\n-            outputs = keras.layers.Dense(1, name='model_output')(inputs)\n-            model = keras.models.Model(inputs, outputs)\n-            model.compile(optimizer='sgd', loss={'model_output': loss})\n-            model.fit(np.random.rand(256, 2), np.random.rand(256, 1))\n-            model.save(model_filename)\n-\n-        with custom_object_scope({'MSE_MAE_loss': MSE_MAE_loss}):\n-            loaded_model = keras.models.load_model(model_filename)\n-            loaded_model.predict(np.random.rand(128, 2))\n-\n-\n-skipif_not_tf = pytest.mark.skipif(\n-    K.backend() != 'tensorflow',\n-    reason='Need TensorFlow to __call__ a loss')\n-\n-\n-class TestLossClasses(object):\n-\n-    @pytest.mark.parametrize('cls', all_classes)\n-    def test_objective_shapes_3d(self, cls):\n-        y_a = K.variable(np.random.random((5, 6, 7)))\n-        y_b = K.variable(np.random.random((5, 6, 7)))\n-        sw = K.variable(np.random.random((5, 6)))\n-        obj_fn = cls(name='test')\n-        objective_output = obj_fn(y_a, y_b, sample_weight=sw)\n-        assert K.eval(objective_output).shape == ()\n-\n-    @pytest.mark.parametrize('cls', all_classes)\n-    def test_objective_shapes_2d(self, cls):\n-        y_a = K.variable(np.random.random((6, 7)))\n-        y_b = K.variable(np.random.random((6, 7)))\n-        sw = K.variable(np.random.random((6,)))\n-        obj_fn = cls(name='test')\n-        objective_output = obj_fn(y_a, y_b, sample_weight=sw)\n-        assert K.eval(objective_output).shape == ()\n-\n-\n-@skipif_not_tf\n-class TestMeanSquaredError:\n-\n-    def test_config(self):\n-        mse_obj = losses.MeanSquaredError(\n-            reduction=Reduction.SUM, name='mse_1')\n-        assert mse_obj.name == 'mse_1'\n-        assert mse_obj.reduction == Reduction.SUM\n-\n-    def test_all_correct_unweighted(self):\n-        mse_obj = losses.MeanSquaredError()\n-        y_true = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n-        loss = mse_obj(y_true, y_true)\n-        assert np.isclose(K.eval(loss), 0.0)\n-\n-    def test_unweighted(self):\n-        mse_obj = losses.MeanSquaredError()\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n-        loss = mse_obj(y_true, y_pred)\n-        assert np.isclose(K.eval(loss), 49.5, atol=1e-3)\n-\n-    def test_scalar_weighted(self):\n-        mse_obj = losses.MeanSquaredError()\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n-        loss = mse_obj(y_true, y_pred, sample_weight=2.3)\n-        assert np.isclose(K.eval(loss), 113.85, atol=1e-3)\n-\n-    def test_sample_weighted(self):\n-        mse_obj = losses.MeanSquaredError()\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n-        sample_weight = K.constant([1.2, 3.4], shape=(2, 1))\n-        loss = mse_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.isclose(K.eval(loss), 767.8 / 6, atol=1e-3)\n-\n-    def test_timestep_weighted(self):\n-        mse_obj = losses.MeanSquaredError()\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3, 1))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3, 1))\n-        sample_weight = K.constant([3, 6, 5, 0, 4, 2], shape=(2, 3))\n-        loss = mse_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.isclose(K.eval(loss), 97.833, atol=1e-3)\n-\n-    def test_zero_weighted(self):\n-        mse_obj = losses.MeanSquaredError()\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n-        loss = mse_obj(y_true, y_pred, sample_weight=0)\n-        assert np.isclose(K.eval(loss), 0.0)\n-\n-    def test_invalid_sample_weight(self):\n-        mse_obj = losses.MeanSquaredError()\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3, 1))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3, 1))\n-        sample_weight = K.constant([3, 6, 5, 0], shape=(2, 2))\n-        with pytest.raises(Exception):\n-            mse_obj(y_true, y_pred, sample_weight=sample_weight)\n-\n-    def test_no_reduction(self):\n-        mse_obj = losses.MeanSquaredError(\n-            reduction=Reduction.NONE)\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n-        loss = mse_obj(y_true, y_pred, sample_weight=2.3)\n-        assert np.allclose(K.eval(loss), [84.3333, 143.3666], atol=1e-3)\n-\n-    def test_sum_reduction(self):\n-        mse_obj = losses.MeanSquaredError(\n-            reduction=Reduction.SUM)\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n-        loss = mse_obj(y_true, y_pred, sample_weight=2.3)\n-        assert np.isclose(K.eval(loss), 227.69998, atol=1e-3)\n-\n-\n-@skipif_not_tf\n-class TestMeanAbsoluteError(object):\n-\n-    def test_config(self):\n-        mae_obj = losses.MeanAbsoluteError(\n-            reduction=Reduction.SUM, name='mae_1')\n-        assert mae_obj.name == 'mae_1'\n-        assert mae_obj.reduction == Reduction.SUM\n-\n-    def test_all_correct_unweighted(self):\n-        mae_obj = losses.MeanAbsoluteError()\n-        y_true = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n-        loss = mae_obj(y_true, y_true)\n-        assert np.isclose(K.eval(loss), 0.0, atol=1e-3)\n-\n-    def test_unweighted(self):\n-        mae_obj = losses.MeanAbsoluteError()\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n-        loss = mae_obj(y_true, y_pred)\n-        assert np.isclose(K.eval(loss), 5.5, atol=1e-3)\n-\n-    def test_scalar_weighted(self):\n-        mae_obj = losses.MeanAbsoluteError()\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n-        loss = mae_obj(y_true, y_pred, sample_weight=2.3)\n-        assert np.isclose(K.eval(loss), 12.65, atol=1e-3)\n-\n-    def test_sample_weighted(self):\n-        mae_obj = losses.MeanAbsoluteError()\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n-        sample_weight = K.constant([1.2, 3.4], shape=(2, 1))\n-        loss = mae_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.isclose(K.eval(loss), 81.4 / 6, atol=1e-3)\n-\n-    def test_timestep_weighted(self):\n-        mae_obj = losses.MeanAbsoluteError()\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3, 1))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3, 1))\n-        sample_weight = K.constant([3, 6, 5, 0, 4, 2], shape=(2, 3))\n-        loss = mae_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.isclose(K.eval(loss), 13.833, atol=1e-3)\n-\n-    def test_zero_weighted(self):\n-        mae_obj = losses.MeanAbsoluteError()\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n-        loss = mae_obj(y_true, y_pred, sample_weight=0)\n-        assert np.isclose(K.eval(loss), 0.0, atol=1e-3)\n-\n-    def test_invalid_sample_weight(self):\n-        mae_obj = losses.MeanAbsoluteError()\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3, 1))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3, 1))\n-        sample_weight = K.constant([3, 6, 5, 0], shape=(2, 2))\n-        with pytest.raises(Exception):\n-            mae_obj(y_true, y_pred, sample_weight=sample_weight)\n-\n-    def test_no_reduction(self):\n-        mae_obj = losses.MeanAbsoluteError(\n-            reduction=Reduction.NONE)\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n-        loss = mae_obj(y_true, y_pred, sample_weight=2.3)\n-        assert np.allclose(K.eval(loss), [10.7333, 14.5666], atol=1e-3)\n-\n-    def test_sum_reduction(self):\n-        mae_obj = losses.MeanAbsoluteError(\n-            reduction=Reduction.SUM)\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n-        loss = mae_obj(y_true, y_pred, sample_weight=2.3)\n-        assert np.isclose(K.eval(loss), 25.29999, atol=1e-3)\n-\n-\n-@skipif_not_tf\n-class TestMeanAbsolutePercentageError(object):\n-\n-    def test_config(self):\n-        mape_obj = losses.MeanAbsolutePercentageError(\n-            reduction=Reduction.SUM, name='mape_1')\n-        assert mape_obj.name == 'mape_1'\n-        assert mape_obj.reduction == Reduction.SUM\n-\n-    def test_all_correct_unweighted(self):\n-        mape_obj = losses.MeanAbsolutePercentageError()\n-        y_true = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n-        loss = mape_obj(y_true, y_true)\n-        assert np.allclose(K.eval(loss), 0.0, atol=1e-3)\n-\n-    def test_unweighted(self):\n-        mape_obj = losses.MeanAbsolutePercentageError()\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n-        loss = mape_obj(y_true, y_pred)\n-        assert np.allclose(K.eval(loss), 211.8518, atol=1e-3)\n-\n-    def test_scalar_weighted(self):\n-        mape_obj = losses.MeanAbsolutePercentageError()\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n-        loss = mape_obj(y_true, y_pred, sample_weight=2.3)\n-        assert np.allclose(K.eval(loss), 487.259, atol=1e-3)\n-\n-    def test_sample_weighted(self):\n-        mape_obj = losses.MeanAbsolutePercentageError()\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n-        sample_weight = K.constant([1.2, 3.4], shape=(2, 1))\n-        loss = mape_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose(K.eval(loss), 422.8888, atol=1e-3)\n-\n-    def test_timestep_weighted(self):\n-        mape_obj = losses.MeanAbsolutePercentageError()\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3, 1))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3, 1))\n-        sample_weight = K.constant([3, 6, 5, 0, 4, 2], shape=(2, 3))\n-        loss = mape_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose(K.eval(loss), 694.4445, atol=1e-3)\n-\n-    def test_zero_weighted(self):\n-        mape_obj = losses.MeanAbsolutePercentageError()\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n-        loss = mape_obj(y_true, y_pred, sample_weight=0)\n-        assert np.allclose(K.eval(loss), 0.0, atol=1e-3)\n-\n-    def test_no_reduction(self):\n-        mape_obj = losses.MeanAbsolutePercentageError(\n-            reduction=Reduction.NONE)\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n-        loss = mape_obj(y_true, y_pred, sample_weight=2.3)\n-        assert np.allclose(K.eval(loss), [621.8518, 352.6666], atol=1e-3)\n-\n-\n-@skipif_not_tf\n-class TestMeanSquaredLogarithmicError(object):\n-\n-    def test_config(self):\n-        msle_obj = losses.MeanSquaredLogarithmicError(\n-            reduction=Reduction .SUM, name='mape_1')\n-        assert msle_obj.name == 'mape_1'\n-        assert msle_obj.reduction == Reduction .SUM\n-\n-    def test_unweighted(self):\n-        msle_obj = losses.MeanSquaredLogarithmicError()\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n-        loss = msle_obj(y_true, y_pred)\n-        assert np.allclose(K.eval(loss), 1.4370, atol=1e-3)\n-\n-    def test_scalar_weighted(self):\n-        msle_obj = losses.MeanSquaredLogarithmicError()\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n-        loss = msle_obj(y_true, y_pred, sample_weight=2.3)\n-        assert np.allclose(K.eval(loss), 3.3051, atol=1e-3)\n-\n-    def test_sample_weighted(self):\n-        msle_obj = losses.MeanSquaredLogarithmicError()\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n-        sample_weight = K.constant([1.2, 3.4], shape=(2, 1))\n-        loss = msle_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose(K.eval(loss), 3.7856, atol=1e-3)\n-\n-    def test_timestep_weighted(self):\n-        msle_obj = losses.MeanSquaredLogarithmicError()\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3, 1))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3, 1))\n-        sample_weight = K.constant([3, 6, 5, 0, 4, 2], shape=(2, 3))\n-        loss = msle_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose(K.eval(loss), 2.6473, atol=1e-3)\n-\n-    def test_zero_weighted(self):\n-        msle_obj = losses.MeanSquaredLogarithmicError()\n-        y_true = K.constant([1, 9, 2, -5, -2, 6], shape=(2, 3))\n-        y_pred = K.constant([4, 8, 12, 8, 1, 3], shape=(2, 3))\n-        loss = msle_obj(y_true, y_pred, sample_weight=0)\n-        assert np.allclose(K.eval(loss), 0.0, atol=1e-3)\n-\n-\n-@skipif_not_tf\n-class TestBinaryCrossentropy(object):\n-\n-    def test_config(self):\n-        bce_obj = losses.BinaryCrossentropy(\n-            reduction=Reduction.SUM, name='bce_1')\n-        assert bce_obj.name == 'bce_1'\n-        assert bce_obj.reduction == Reduction.SUM\n-\n-    def test_all_correct_unweighted(self):\n-        y_true = K.constant([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n-        bce_obj = losses.BinaryCrossentropy()\n-        loss = bce_obj(y_true, y_true)\n-        assert np.isclose(K.eval(loss), 0.0, atol=1e-3)\n-\n-        # Test with logits.\n-        logits = K.constant([[100.0, -100.0, -100.0],\n-                             [-100.0, 100.0, -100.0],\n-                             [-100.0, -100.0, 100.0]])\n-        bce_obj = losses.BinaryCrossentropy(from_logits=True)\n-        loss = bce_obj(y_true, logits)\n-        assert np.isclose(K.eval(loss), 0.0, 3)\n-\n-    def test_unweighted(self):\n-        y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n-        y_pred = np.asarray([1, 1, 1, 0], dtype=np.float32).reshape([2, 2])\n-        bce_obj = losses.BinaryCrossentropy()\n-        loss = bce_obj(y_true, y_pred)\n-\n-        # EPSILON = 1e-7, y = y_true, y` = y_pred, Y_MAX = 0.9999999\n-        # y` = clip(output, EPSILON, 1. - EPSILON)\n-        # y` = [Y_MAX, Y_MAX, Y_MAX, EPSILON]\n-\n-        # Loss = -(y log(y` + EPSILON) + (1 - y) log(1 - y` + EPSILON))\n-        #      = [-log(Y_MAX + EPSILON), -log(1 - Y_MAX + EPSILON),\n-        #         -log(Y_MAX + EPSILON), -log(1)]\n-        #      = [0, 15.33, 0, 0]\n-        # Reduced loss = 15.33 / 4\n-\n-        assert np.isclose(K.eval(loss), 3.833, atol=1e-3)\n-\n-        # Test with logits.\n-        y_true = K.constant([[1., 0., 1.], [0., 1., 1.]])\n-        logits = K.constant([[100.0, -100.0, 100.0], [100.0, 100.0, -100.0]])\n-        bce_obj = losses.BinaryCrossentropy(from_logits=True)\n-        loss = bce_obj(y_true, logits)\n-\n-        # Loss = max(x, 0) - x * z + log(1 + exp(-abs(x)))\n-        #            (where x = logits and z = y_true)\n-        #      = [((100 - 100 * 1 + log(1 + exp(-100))) +\n-        #          (0 + 100 * 0 + log(1 + exp(-100))) +\n-        #          (100 - 100 * 1 + log(1 + exp(-100))),\n-        #         ((100 - 100 * 0 + log(1 + exp(-100))) +\n-        #          (100 - 100 * 1 + log(1 + exp(-100))) +\n-        #          (0 + 100 * 1 + log(1 + exp(-100))))]\n-        #      = [(0 + 0 + 0) / 3, 200 / 3]\n-        # Reduced loss = (0 + 66.666) / 2\n-\n-        assert np.isclose(K.eval(loss), 33.333, atol=1e-3)\n-\n-    def test_scalar_weighted(self):\n-        bce_obj = losses.BinaryCrossentropy()\n-        y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n-        y_pred = np.asarray([1, 1, 1, 0], dtype=np.float32).reshape([2, 2])\n-        loss = bce_obj(y_true, y_pred, sample_weight=2.3)\n-\n-        # EPSILON = 1e-7, y = y_true, y` = y_pred, Y_MAX = 0.9999999\n-        # y` = clip(output, EPSILON, 1. - EPSILON)\n-        # y` = [Y_MAX, Y_MAX, Y_MAX, EPSILON]\n-\n-        # Loss = -(y log(y` + EPSILON) + (1 - y) log(1 - y` + EPSILON))\n-        #      = [-log(Y_MAX + EPSILON), -log(1 - Y_MAX + EPSILON),\n-        #         -log(Y_MAX + EPSILON), -log(1)]\n-        #      = [0, 15.33, 0, 0]\n-        # Weighted loss = [0, 15.33 * 2.3, 0, 0]\n-        # Reduced loss = 15.33 * 2.3 / 4\n-\n-        assert np.isclose(K.eval(loss), 8.817, atol=1e-3)\n-\n-        # Test with logits.\n-        y_true = K.constant([[1, 0, 1], [0, 1, 1]])\n-        logits = K.constant([[100.0, -100.0, 100.0], [100.0, 100.0, -100.0]])\n-        bce_obj = losses.BinaryCrossentropy(from_logits=True)\n-        loss = bce_obj(y_true, logits, sample_weight=2.3)\n-\n-        # Loss = max(x, 0) - x * z + log(1 + exp(-abs(x)))\n-        #            (where x = logits and z = y_true)\n-        # Loss = [(0 + 0 + 0) / 3, 200 / 3]\n-        # Weighted loss = [0 * 2.3, 66.666 * 2.3]\n-        # Reduced loss = (0 + 66.666 * 2.3) / 2\n-\n-        assert np.isclose(K.eval(loss), 76.667, atol=1e-3)\n-\n-    def test_sample_weighted(self):\n-        bce_obj = losses.BinaryCrossentropy()\n-        y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n-        y_pred = np.asarray([1, 1, 1, 0], dtype=np.float32).reshape([2, 2])\n-        sample_weight = K.constant([1.2, 3.4], shape=(2, 1))\n-        loss = bce_obj(y_true, y_pred, sample_weight=sample_weight)\n-\n-        # EPSILON = 1e-7, y = y_true, y` = y_pred, Y_MAX = 0.9999999\n-        # y` = clip(output, EPSILON, 1. - EPSILON)\n-        # y` = [Y_MAX, Y_MAX, Y_MAX, EPSILON]\n-\n-        # Loss = -(y log(y` + EPSILON) + (1 - y) log(1 - y` + EPSILON))\n-        #      = [-log(Y_MAX + EPSILON), -log(1 - Y_MAX + EPSILON),\n-        #         -log(Y_MAX + EPSILON), -log(1)]\n-        #      = [0, 15.33, 0, 0]\n-        # Reduced loss = 15.33 * 1.2 / 4\n-\n-        assert np.isclose(K.eval(loss), 4.6, atol=1e-3)\n-\n-        # Test with logits.\n-        y_true = K.constant([[1, 0, 1], [0, 1, 1]])\n-        logits = K.constant([[100.0, -100.0, 100.0], [100.0, 100.0, -100.0]])\n-        weights = K.constant([4, 3])\n-        bce_obj = losses.BinaryCrossentropy(from_logits=True)\n-        loss = bce_obj(y_true, logits, sample_weight=weights)\n-\n-        # Loss = max(x, 0) - x * z + log(1 + exp(-abs(x)))\n-        #            (where x = logits and z = y_true)\n-        # Loss = [(0 + 0 + 0)/3, 200 / 3]\n-        # Weighted loss = [0 * 4, 66.666 * 3]\n-        # Reduced loss = (0 + 66.666 * 3) / 2\n-\n-        assert np.isclose(K.eval(loss), 100, atol=1e-3)\n-\n-    def test_no_reduction(self):\n-        y_true = K.constant([[1, 0, 1], [0, 1, 1]])\n-        logits = K.constant([[100.0, -100.0, 100.0], [100.0, 100.0, -100.0]])\n-        bce_obj = losses.BinaryCrossentropy(\n-            from_logits=True, reduction=Reduction.NONE)\n-        loss = bce_obj(y_true, logits)\n-\n-        # Loss = max(x, 0) - x * z + log(1 + exp(-abs(x)))\n-        #            (where x = logits and z = y_true)\n-        # Loss = [(0 + 0 + 0)/3, (200)/3]\n-\n-        assert np.allclose(K.eval(loss), (0., 66.6666), atol=1e-3)\n-\n-    def test_label_smoothing(self):\n-        logits = K.constant([[100.0, -100.0, -100.0]])\n-        y_true = K.constant([[1, 0, 1]])\n-        label_smoothing = 0.1\n-        # Loss: max(x, 0) - x * z + log(1 + exp(-abs(x)))\n-        #            (where x = logits and z = y_true)\n-        # Label smoothing: z' = z * (1 - L) + 0.5L\n-        #                  1  = 1 - 0.5L\n-        #                  0  = 0.5L\n-        # Applying the above two fns to the given input:\n-        # (100 - 100 * (1 - 0.5 L)  + 0 +\n-        #  0   + 100 * (0.5 L)      + 0 +\n-        #  0   + 100 * (1 - 0.5 L)  + 0) * (1/3)\n-        #  = (100 + 50L) * 1/3\n-        bce_obj = losses.BinaryCrossentropy(\n-            from_logits=True, label_smoothing=label_smoothing)\n-        loss = bce_obj(y_true, logits)\n-        expected_value = (100.0 + 50.0 * label_smoothing) / 3.0\n-        assert np.isclose(K.eval(loss), expected_value, atol=1e-3)\n-\n-\n-@skipif_not_tf\n-class TestCategoricalCrossentropy(object):\n-\n-    def test_config(self):\n-        cce_obj = losses.CategoricalCrossentropy(\n-            reduction=Reduction.SUM, name='bce_1')\n-        assert cce_obj.name == 'bce_1'\n-        assert cce_obj.reduction == Reduction.SUM\n-\n-    def test_all_correct_unweighted(self):\n-        y_true = K.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n-        y_pred = K.constant([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n-        cce_obj = losses.CategoricalCrossentropy()\n-        loss = cce_obj(y_true, y_pred)\n-        assert np.isclose(K.eval(loss), 0.0, atol=1e-3)\n-\n-        # Test with logits.\n-        logits = K.constant([[10., 0., 0.], [0., 10., 0.], [0., 0., 10.]])\n-        cce_obj = losses.CategoricalCrossentropy(from_logits=True)\n-        loss = cce_obj(y_true, logits)\n-        assert np.isclose(K.eval(loss), 0.0, atol=1e-3)\n-\n-    def test_unweighted(self):\n-        cce_obj = losses.CategoricalCrossentropy()\n-        y_true = K.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n-        y_pred = K.constant(\n-            [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])\n-        loss = cce_obj(y_true, y_pred)\n-        assert np.isclose(K.eval(loss), .3239, atol=1e-3)\n-\n-        # Test with logits.\n-        logits = K.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])\n-        cce_obj = losses.CategoricalCrossentropy(from_logits=True)\n-        loss = cce_obj(y_true, logits)\n-        assert np.isclose(K.eval(loss), .05737, atol=1e-3)\n-\n-    def test_scalar_weighted(self):\n-        cce_obj = losses.CategoricalCrossentropy()\n-        y_true = K.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n-        y_pred = K.constant(\n-            [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])\n-        loss = cce_obj(y_true, y_pred, sample_weight=2.3)\n-        assert np.isclose(K.eval(loss), .7449, atol=1e-3)\n-\n-        # Test with logits.\n-        logits = K.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])\n-        cce_obj = losses.CategoricalCrossentropy(from_logits=True)\n-        loss = cce_obj(y_true, logits, sample_weight=2.3)\n-        assert np.isclose(K.eval(loss), .132, atol=1e-3)\n-\n-    def test_sample_weighted(self):\n-        cce_obj = losses.CategoricalCrossentropy()\n-        y_true = K.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n-        y_pred = K.constant(\n-            [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])\n-        sample_weight = K.constant([[1.2], [3.4], [5.6]], shape=(3, 1))\n-        loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.isclose(K.eval(loss), 1.0696, atol=1e-3)\n-\n-        # Test with logits.\n-        logits = K.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])\n-        cce_obj = losses.CategoricalCrossentropy(from_logits=True)\n-        loss = cce_obj(y_true, logits, sample_weight=sample_weight)\n-        assert np.isclose(K.eval(loss), 0.31829, atol=1e-3)\n-\n-    def test_no_reduction(self):\n-        y_true = K.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n-        logits = K.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])\n-        cce_obj = losses.CategoricalCrossentropy(\n-            from_logits=True, reduction=Reduction.NONE)\n-        loss = cce_obj(y_true, logits)\n-        assert np.allclose(K.eval(loss), (0.001822, 0.000459, 0.169846), atol=1e-3)\n-\n-    def test_label_smoothing(self):\n-        logits = K.constant([[100.0, -100.0, -100.0]])\n-        y_true = K.constant([[1, 0, 0]])\n-        label_smoothing = 0.1\n-        # Softmax Cross Entropy Loss: -\\sum_i p_i \\log q_i\n-        # where for a softmax activation\n-        # \\log q_i = x_i - \\log \\sum_j \\exp x_j\n-        #          = x_i - x_max - \\log \\sum_j \\exp (x_j - x_max)\n-        # For our activations, [100, -100, -100]\n-        # \\log ( exp(0) + exp(-200) + exp(-200) ) = 0\n-        # so our log softmaxes become: [0, -200, -200]\n-        # Label smoothing: z' = z * (1 - L) + L/n\n-        #                  1  = 1 - L + L/n\n-        #                  0  = L/n\n-        # Applying the above two fns to the given input:\n-        # -0 * (1 - L + L/n) + 200 * L/n + 200 * L/n = 400 L/n\n-        cce_obj = losses.CategoricalCrossentropy(\n-            from_logits=True, label_smoothing=label_smoothing)\n-        loss = cce_obj(y_true, logits)\n-        expected_value = 400.0 * label_smoothing / 3.0\n-        assert np.isclose(K.eval(loss), expected_value, atol=1e-3)\n-\n-\n-@skipif_not_tf\n-class TestSparseCategoricalCrossentropy(object):\n-\n-    def test_config(self):\n-        cce_obj = losses.SparseCategoricalCrossentropy(\n-            reduction=Reduction.SUM, name='scc')\n-        assert cce_obj.name == 'scc'\n-        assert cce_obj.reduction == Reduction.SUM\n-\n-    def test_all_correct_unweighted(self):\n-        y_true = K.constant([[0], [1], [2]])\n-        y_pred = K.constant([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]])\n-        cce_obj = losses.SparseCategoricalCrossentropy()\n-        loss = cce_obj(y_true, y_pred)\n-        assert np.isclose(K.eval(loss), 0.0, atol=1e-3)\n-\n-        # Test with logits.\n-        logits = K.constant([[10., 0., 0.], [0., 10., 0.], [0., 0., 10.]])\n-        cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)\n-        loss = cce_obj(y_true, logits)\n-        assert np.isclose(K.eval(loss), 0.0, atol=1e-3)\n-\n-    def test_unweighted(self):\n-        cce_obj = losses.SparseCategoricalCrossentropy()\n-        y_true = K.constant([0, 1, 2])\n-        y_pred = K.constant(\n-            [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])\n-        loss = cce_obj(y_true, y_pred)\n-        assert np.isclose(K.eval(loss), .3239, atol=1e-3)\n-\n-        # Test with logits.\n-        logits = K.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])\n-        cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)\n-        loss = cce_obj(y_true, logits)\n-        assert np.isclose(K.eval(loss), .0573, atol=1e-3)\n-\n-    def test_scalar_weighted(self):\n-        cce_obj = losses.SparseCategoricalCrossentropy()\n-        y_true = K.constant([[0], [1], [2]])\n-        y_pred = K.constant(\n-            [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])\n-        loss = cce_obj(y_true, y_pred, sample_weight=2.3)\n-        assert np.isclose(K.eval(loss), .7449, atol=1e-3)\n-\n-        # Test with logits.\n-        logits = K.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])\n-        cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)\n-        loss = cce_obj(y_true, logits, sample_weight=2.3)\n-        assert np.isclose(K.eval(loss), .1317, atol=1e-3)\n-\n-    def test_sample_weighted(self):\n-        cce_obj = losses.SparseCategoricalCrossentropy()\n-        y_true = K.constant([[0], [1], [2]])\n-        y_pred = K.constant(\n-            [[.9, .05, .05], [.5, .89, .6], [.05, .01, .94]])\n-        sample_weight = K.constant([[1.2], [3.4], [5.6]], shape=(3, 1))\n-        loss = cce_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.isclose(K.eval(loss), 1.0696, atol=1e-3)\n-\n-        # Test with logits.\n-        logits = K.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])\n-        cce_obj = losses.SparseCategoricalCrossentropy(from_logits=True)\n-        loss = cce_obj(y_true, logits, sample_weight=sample_weight)\n-        assert np.isclose(K.eval(loss), 0.31829, atol=1e-3)\n-\n-    def test_no_reduction(self):\n-        y_true = K.constant([[0], [1], [2]])\n-        logits = K.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])\n-        cce_obj = losses.SparseCategoricalCrossentropy(\n-            from_logits=True, reduction=Reduction.NONE)\n-        loss = cce_obj(y_true, logits)\n-        assert np.allclose(K.eval(loss), (0.001822, 0.000459, 0.169846), atol=1e-3)\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,968 +0,0 @@\n-\"\"\"Tests for Keras confusion matrix metrics classes.\"\"\"\n-import pytest\n-import numpy as np\n-\n-from keras import metrics\n-from keras import backend as K\n-from tensorflow.python.keras.utils import metrics_utils\n-\n-if K.backend() != 'tensorflow':\n-    # Need TensorFlow to use metric.__call__\n-    pytestmark = pytest.mark.skip\n-\n-import tensorflow as tf\n-\n-\n-class TestFalsePositives(object):\n-\n-    def test_config(self):\n-        fp_obj = metrics.FalsePositives(name='my_fp', thresholds=[0.4, 0.9])\n-        assert fp_obj.name == 'my_fp'\n-        assert len(fp_obj.weights) == 1\n-        assert fp_obj.thresholds == [0.4, 0.9]\n-\n-        # Check save and restore config\n-        fp_obj2 = metrics.FalsePositives.from_config(fp_obj.get_config())\n-        assert fp_obj2.name == 'my_fp'\n-        assert len(fp_obj2.weights) == 1\n-        assert fp_obj2.thresholds == [.4, 0.9]\n-\n-    def test_unweighted(self):\n-        fp_obj = metrics.FalsePositives()\n-\n-        y_true = ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n-        y_pred = ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n-\n-        result = fp_obj(y_true, y_pred)\n-        assert np.allclose(7., K.eval(result))\n-\n-    def test_weighted(self):\n-        fp_obj = metrics.FalsePositives()\n-        y_true = ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n-        y_pred = ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n-        sample_weight = (1., 1.5, 2., 2.5)\n-        result = fp_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose(14., K.eval(result))\n-\n-    def test_unweighted_with_thresholds(self):\n-        fp_obj = metrics.FalsePositives(thresholds=[0.15, 0.5, 0.85])\n-\n-        y_pred = ((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),\n-                  (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3))\n-        y_true = ((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0), (1, 1, 1, 1))\n-\n-        result = fp_obj(y_true, y_pred)\n-        assert np.allclose([7., 4., 2.], K.eval(result))\n-\n-    def test_weighted_with_thresholds(self):\n-        fp_obj = metrics.FalsePositives(thresholds=[0.15, 0.5, 0.85])\n-\n-        y_pred = ((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),\n-                  (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3))\n-        y_true = ((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0), (1, 1, 1, 1))\n-        sample_weight = ((1.0, 2.0, 3.0, 5.0), (7.0, 11.0, 13.0, 17.0),\n-                         (19.0, 23.0, 29.0, 31.0), (5.0, 15.0, 10.0, 0))\n-\n-        result = fp_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose([125., 42., 12.], K.eval(result))\n-\n-    def test_threshold_limit(self):\n-        with pytest.raises(Exception):\n-            metrics.FalsePositives(thresholds=[-1, 0.5, 2])\n-\n-        with pytest.raises(Exception):\n-            metrics.FalsePositives(thresholds=[None])\n-\n-\n-class TestTruePositives(object):\n-\n-    def test_config(self):\n-        tp_obj = metrics.TruePositives(name='my_tp', thresholds=[0.4, 0.9])\n-        assert tp_obj.name == 'my_tp'\n-        assert len(tp_obj.weights) == 1\n-        assert tp_obj.thresholds == [0.4, 0.9]\n-\n-        # Check save and restore config\n-        tp_obj2 = metrics.TruePositives.from_config(tp_obj.get_config())\n-        assert tp_obj2.name == 'my_tp'\n-        assert len(tp_obj2.weights) == 1\n-        assert tp_obj2.thresholds == [0.4, 0.9]\n-\n-    def test_unweighted(self):\n-        tp_obj = metrics.TruePositives()\n-\n-        y_true = ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),\n-                  (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n-        y_pred = ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),\n-                  (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n-\n-        result = tp_obj(y_true, y_pred)\n-        assert np.allclose(7., K.eval(result))\n-\n-    def test_weighted(self):\n-        tp_obj = metrics.TruePositives()\n-        y_true = ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),\n-                  (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n-        y_pred = ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),\n-                  (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n-        sample_weight = (1., 1.5, 2., 2.5)\n-        result = tp_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose(12., K.eval(result))\n-\n-    def test_unweighted_with_thresholds(self):\n-        tp_obj = metrics.TruePositives(thresholds=[0.15, 0.5, 0.85])\n-\n-        y_pred = ((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),\n-                  (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3))\n-        y_true = ((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0),\n-                  (1, 1, 1, 1))\n-\n-        result = tp_obj(y_true, y_pred)\n-        assert np.allclose([6., 3., 1.], K.eval(result))\n-\n-    def test_weighted_with_thresholds(self):\n-        tp_obj = metrics.TruePositives(thresholds=[0.15, 0.5, 0.85])\n-\n-        y_pred = ((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),\n-                  (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3))\n-        y_true = ((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0),\n-                  (1, 1, 1, 1))\n-\n-        result = tp_obj(y_true, y_pred, sample_weight=37.)\n-        assert np.allclose([222., 111., 37.], K.eval(result))\n-\n-\n-class TestTrueNegatives(object):\n-\n-    def test_config(self):\n-        tn_obj = metrics.TrueNegatives(name='my_tn', thresholds=[0.4, 0.9])\n-        assert tn_obj.name == 'my_tn'\n-        assert len(tn_obj.weights) == 1\n-        assert tn_obj.thresholds == [0.4, 0.9]\n-\n-        # Check save and restore config\n-        tn_obj2 = metrics.TrueNegatives.from_config(tn_obj.get_config())\n-        assert tn_obj2.name == 'my_tn'\n-        assert len(tn_obj2.weights) == 1\n-        assert tn_obj2.thresholds == [0.4, 0.9]\n-\n-    def test_unweighted(self):\n-        tn_obj = metrics.TrueNegatives()\n-\n-        y_true = ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),\n-                  (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n-        y_pred = ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),\n-                  (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n-\n-        result = tn_obj(y_true, y_pred)\n-        assert np.allclose(3., K.eval(result))\n-\n-    def test_weighted(self):\n-        tn_obj = metrics.TrueNegatives()\n-        y_true = ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),\n-                  (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n-        y_pred = ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),\n-                  (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n-        sample_weight = (1., 1.5, 2., 2.5)\n-        result = tn_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose(4., K.eval(result))\n-\n-    def test_unweighted_with_thresholds(self):\n-        tn_obj = metrics.TrueNegatives(thresholds=[0.15, 0.5, 0.85])\n-\n-        y_pred = ((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),\n-                  (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3))\n-        y_true = ((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0), (1, 1, 1, 1))\n-\n-        result = tn_obj(y_true, y_pred)\n-        assert np.allclose([2., 5., 7.], K.eval(result))\n-\n-    def test_weighted_with_thresholds(self):\n-        tn_obj = metrics.TrueNegatives(thresholds=[0.15, 0.5, 0.85])\n-\n-        y_pred = ((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),\n-                  (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3))\n-        y_true = ((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0), (1, 1, 1, 1))\n-        sample_weight = ((0.0, 2.0, 3.0, 5.0),)\n-\n-        result = tn_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose([5., 15., 23.], K.eval(result))\n-\n-\n-class TestFalseNegatives(object):\n-\n-    def test_config(self):\n-        fn_obj = metrics.FalseNegatives(name='my_fn', thresholds=[0.4, 0.9])\n-        assert fn_obj.name == 'my_fn'\n-        assert len(fn_obj.weights) == 1\n-        assert fn_obj.thresholds == [0.4, 0.9]\n-\n-        # Check save and restore config\n-        fn_obj2 = metrics.FalseNegatives.from_config(fn_obj.get_config())\n-        assert fn_obj2.name == 'my_fn'\n-        assert len(fn_obj2.weights) == 1\n-        assert fn_obj2.thresholds == [0.4, 0.9]\n-\n-    def test_unweighted(self):\n-        fn_obj = metrics.FalseNegatives()\n-\n-        y_true = ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),\n-                  (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n-        y_pred = ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),\n-                  (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n-\n-        result = fn_obj(y_true, y_pred)\n-        assert np.allclose(3., K.eval(result))\n-\n-    def test_weighted(self):\n-        fn_obj = metrics.FalseNegatives()\n-        y_true = ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),\n-                  (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n-        y_pred = ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),\n-                  (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n-        sample_weight = (1., 1.5, 2., 2.5)\n-        result = fn_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose(5., K.eval(result))\n-\n-    def test_unweighted_with_thresholds(self):\n-        fn_obj = metrics.FalseNegatives(thresholds=[0.15, 0.5, 0.85])\n-\n-        y_pred = ((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),\n-                  (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3))\n-        y_true = ((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0), (1, 1, 1, 1))\n-\n-        result = fn_obj(y_true, y_pred)\n-        assert np.allclose([1., 4., 6.], K.eval(result))\n-\n-    def test_weighted_with_thresholds(self):\n-        fn_obj = metrics.FalseNegatives(thresholds=[0.15, 0.5, 0.85])\n-\n-        y_pred = ((0.9, 0.2, 0.8, 0.1), (0.2, 0.9, 0.7, 0.6),\n-                  (0.1, 0.2, 0.4, 0.3), (0, 1, 0.7, 0.3))\n-        y_true = ((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0), (1, 1, 1, 1))\n-        sample_weight = ((3.0,), (5.0,), (7.0,), (4.0,))\n-\n-        result = fn_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose([4., 16., 23.], K.eval(result))\n-\n-\n-class TestSensitivityAtSpecificity(object):\n-\n-    def test_config(self):\n-        s_obj = metrics.SensitivityAtSpecificity(\n-            0.4, num_thresholds=100, name='sensitivity_at_specificity_1')\n-        assert s_obj.name == 'sensitivity_at_specificity_1'\n-        assert len(s_obj.weights) == 4\n-        assert s_obj.specificity == 0.4\n-        assert s_obj.num_thresholds == 100\n-\n-        # Check save and restore config\n-        s_obj2 = metrics.SensitivityAtSpecificity.from_config(s_obj.get_config())\n-        assert s_obj2.name == 'sensitivity_at_specificity_1'\n-        assert len(s_obj2.weights) == 4\n-        assert s_obj2.specificity == 0.4\n-        assert s_obj2.num_thresholds == 100\n-\n-    def test_unweighted_all_correct(self):\n-        s_obj = metrics.SensitivityAtSpecificity(0.7, num_thresholds=1)\n-        inputs = np.random.randint(0, 2, size=(100, 1))\n-        y_pred = K.constant(inputs, dtype='float32')\n-        y_true = K.constant(inputs)\n-        result = s_obj(y_true, y_pred)\n-        assert np.isclose(1, K.eval(result))\n-\n-    def test_unweighted_high_specificity(self):\n-        s_obj = metrics.SensitivityAtSpecificity(0.8)\n-        pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.1, 0.45, 0.5, 0.8, 0.9]\n-        label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n-\n-        y_pred = K.constant(pred_values, dtype='float32')\n-        y_true = K.constant(label_values)\n-        result = s_obj(y_true, y_pred)\n-        assert np.isclose(0.8, K.eval(result))\n-\n-    def test_unweighted_low_specificity(self):\n-        s_obj = metrics.SensitivityAtSpecificity(0.4)\n-        pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]\n-        label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n-\n-        y_pred = K.constant(pred_values, dtype='float32')\n-        y_true = K.constant(label_values)\n-        result = s_obj(y_true, y_pred)\n-        assert np.isclose(0.6, K.eval(result))\n-\n-    def test_weighted(self):\n-        s_obj = metrics.SensitivityAtSpecificity(0.4)\n-        pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]\n-        label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n-        weight_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n-\n-        y_pred = K.constant(pred_values, dtype='float32')\n-        y_true = K.constant(label_values, dtype='float32')\n-        weights = K.constant(weight_values)\n-        result = s_obj(y_true, y_pred, sample_weight=weights)\n-        assert np.isclose(0.675, K.eval(result))\n-\n-    def test_invalid_specificity(self):\n-        with pytest.raises(Exception):\n-            metrics.SensitivityAtSpecificity(-1)\n-\n-    def test_invalid_num_thresholds(self):\n-        with pytest.raises(Exception):\n-            metrics.SensitivityAtSpecificity(0.4, num_thresholds=-1)\n-\n-\n-class TestSpecificityAtSensitivity(object):\n-\n-    def test_config(self):\n-        s_obj = metrics.SpecificityAtSensitivity(\n-            0.4, num_thresholds=100, name='specificity_at_sensitivity_1')\n-        assert s_obj.name == 'specificity_at_sensitivity_1'\n-        assert len(s_obj.weights) == 4\n-        assert s_obj.sensitivity == 0.4\n-        assert s_obj.num_thresholds == 100\n-\n-        # Check save and restore config\n-        s_obj2 = metrics.SpecificityAtSensitivity.from_config(s_obj.get_config())\n-        assert s_obj2.name == 'specificity_at_sensitivity_1'\n-        assert len(s_obj2.weights) == 4\n-        assert s_obj2.sensitivity == 0.4\n-        assert s_obj2.num_thresholds == 100\n-\n-    def test_unweighted_all_correct(self):\n-        s_obj = metrics.SpecificityAtSensitivity(0.7, num_thresholds=1)\n-        inputs = np.random.randint(0, 2, size=(100, 1))\n-        y_pred = K.constant(inputs, dtype='float32')\n-        y_true = K.constant(inputs)\n-        result = s_obj(y_true, y_pred)\n-        assert np.isclose(1, K.eval(result))\n-\n-    def DISABLED_test_unweighted_high_sensitivity(self):\n-        s_obj = metrics.SpecificityAtSensitivity(0.8)\n-        pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.1, 0.45, 0.5, 0.8, 0.9]\n-        label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n-\n-        y_pred = K.constant(pred_values, dtype='float32')\n-        y_true = K.constant(label_values)\n-        result = s_obj(y_true, y_pred)\n-        assert np.isclose(0.4, K.eval(result))\n-\n-    def test_unweighted_low_sensitivity(self):\n-        s_obj = metrics.SpecificityAtSensitivity(0.4)\n-        pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]\n-        label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n-\n-        y_pred = K.constant(pred_values, dtype='float32')\n-        y_true = K.constant(label_values)\n-        result = s_obj(y_true, y_pred)\n-        assert np.isclose(0.6, K.eval(result))\n-\n-    def test_weighted(self):\n-        s_obj = metrics.SpecificityAtSensitivity(0.4)\n-        pred_values = [0.0, 0.1, 0.2, 0.3, 0.4, 0.01, 0.02, 0.25, 0.26, 0.26]\n-        label_values = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n-        weight_values = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n-\n-        y_pred = K.constant(pred_values, dtype='float32')\n-        y_true = K.constant(label_values, dtype='float32')\n-        weights = K.constant(weight_values)\n-        result = s_obj(y_true, y_pred, sample_weight=weights)\n-        assert np.isclose(0.4, K.eval(result))\n-\n-    def test_invalid_sensitivity(self):\n-        with pytest.raises(Exception):\n-            metrics.SpecificityAtSensitivity(-1)\n-\n-    def test_invalid_num_thresholds(self):\n-        with pytest.raises(Exception):\n-            metrics.SpecificityAtSensitivity(0.4, num_thresholds=-1)\n-\n-\n-class TestAUC(object):\n-\n-    def setup(self):\n-        self.num_thresholds = 3\n-        self.y_pred = K.constant([0, 0.5, 0.3, 0.9], dtype='float32')\n-        self.y_true = K.constant([0, 0, 1, 1])\n-        self.sample_weight = [1, 2, 3, 4]\n-\n-        # threshold values are [0 - 1e-7, 0.5, 1 + 1e-7]\n-        # y_pred when threshold = 0 - 1e-7  : [1, 1, 1, 1]\n-        # y_pred when threshold = 0.5       : [0, 0, 0, 1]\n-        # y_pred when threshold = 1 + 1e-7  : [0, 0, 0, 0]\n-\n-        # without sample_weight:\n-        # tp = np.sum([[0, 0, 1, 1], [0, 0, 0, 1], [0, 0, 0, 0]], axis=1)\n-        # fp = np.sum([[1, 1, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], axis=1)\n-        # fn = np.sum([[0, 0, 0, 0], [0, 0, 1, 0], [0, 0, 1, 1]], axis=1)\n-        # tn = np.sum([[0, 0, 0, 0], [1, 1, 0, 0], [1, 1, 0, 0]], axis=1)\n-\n-        # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]\n-\n-        # with sample_weight:\n-        # tp = np.sum([[0, 0, 3, 4], [0, 0, 0, 4], [0, 0, 0, 0]], axis=1)\n-        # fp = np.sum([[1, 2, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], axis=1)\n-        # fn = np.sum([[0, 0, 0, 0], [0, 0, 3, 0], [0, 0, 3, 4]], axis=1)\n-        # tn = np.sum([[0, 0, 0, 0], [1, 2, 0, 0], [1, 2, 0, 0]], axis=1)\n-\n-        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n-\n-    def test_config(self):\n-        auc_obj = metrics.AUC(\n-            num_thresholds=100,\n-            curve='PR',\n-            summation_method='majoring',\n-            name='auc_1')\n-        assert auc_obj.name == 'auc_1'\n-        assert len(auc_obj.weights) == 4\n-        assert auc_obj.num_thresholds == 100\n-        assert auc_obj.curve == metrics_utils.AUCCurve.PR\n-        assert auc_obj.summation_method == metrics_utils.AUCSummationMethod.MAJORING\n-\n-        # Check save and restore config.\n-        auc_obj2 = metrics.AUC.from_config(auc_obj.get_config())\n-        assert auc_obj2.name == 'auc_1'\n-        assert len(auc_obj2.weights) == 4\n-        assert auc_obj2.num_thresholds == 100\n-        assert auc_obj2.curve == metrics_utils.AUCCurve.PR\n-        assert auc_obj2.summation_method == metrics_utils.AUCSummationMethod.MAJORING\n-\n-    def test_config_manual_thresholds(self):\n-        auc_obj = metrics.AUC(\n-            num_thresholds=None,\n-            curve='PR',\n-            summation_method='majoring',\n-            name='auc_1',\n-            thresholds=[0.3, 0.5])\n-        assert auc_obj.name == 'auc_1'\n-        assert len(auc_obj.weights) == 4\n-        assert auc_obj.num_thresholds == 4\n-        assert np.allclose(auc_obj.thresholds, [0.0, 0.3, 0.5, 1.0], atol=1e-3)\n-        assert auc_obj.curve == metrics_utils.AUCCurve.PR\n-        assert auc_obj.summation_method == metrics_utils.AUCSummationMethod.MAJORING\n-\n-        # Check save and restore config.\n-        auc_obj2 = metrics.AUC.from_config(auc_obj.get_config())\n-        assert auc_obj2.name == 'auc_1'\n-        assert len(auc_obj2.weights) == 4\n-        assert auc_obj2.num_thresholds == 4\n-        assert auc_obj2.curve == metrics_utils.AUCCurve.PR\n-        assert auc_obj2.summation_method == metrics_utils.AUCSummationMethod.MAJORING\n-\n-    def test_unweighted_all_correct(self):\n-        self.setup()\n-        auc_obj = metrics.AUC()\n-        result = auc_obj(self.y_true, self.y_true)\n-        assert K.eval(result) == 1\n-\n-    def test_unweighted(self):\n-        self.setup()\n-        auc_obj = metrics.AUC(num_thresholds=self.num_thresholds)\n-        result = auc_obj(self.y_true, self.y_pred)\n-\n-        # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]\n-        # recall = [2/2, 1/(1+1), 0] = [1, 0.5, 0]\n-        # fp_rate = [2/2, 0, 0] = [1, 0, 0]\n-        # heights = [(1 + 0.5)/2, (0.5 + 0)/2] = [0.75, 0.25]\n-        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n-        expected_result = (0.75 * 1 + 0.25 * 0)\n-        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n-\n-    def test_manual_thresholds(self):\n-        self.setup()\n-        # Verify that when specified, thresholds are used instead of num_thresholds.\n-        auc_obj = metrics.AUC(num_thresholds=2, thresholds=[0.5])\n-        assert auc_obj.num_thresholds == 3\n-        assert np.allclose(auc_obj.thresholds, [0.0, 0.5, 1.0], atol=1e-3)\n-        result = auc_obj(self.y_true, self.y_pred)\n-\n-        # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]\n-        # recall = [2/2, 1/(1+1), 0] = [1, 0.5, 0]\n-        # fp_rate = [2/2, 0, 0] = [1, 0, 0]\n-        # heights = [(1 + 0.5)/2, (0.5 + 0)/2] = [0.75, 0.25]\n-        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n-        expected_result = (0.75 * 1 + 0.25 * 0)\n-        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n-\n-    def test_weighted_roc_interpolation(self):\n-        self.setup()\n-        auc_obj = metrics.AUC(num_thresholds=self.num_thresholds)\n-        result = auc_obj(self.y_true, self.y_pred, sample_weight=self.sample_weight)\n-\n-        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n-        # recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]\n-        # fp_rate = [3/3, 0, 0] = [1, 0, 0]\n-        # heights = [(1 + 0.571)/2, (0.571 + 0)/2] = [0.7855, 0.2855]\n-        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n-        expected_result = (0.7855 * 1 + 0.2855 * 0)\n-        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n-\n-    def test_weighted_roc_majoring(self):\n-        self.setup()\n-        auc_obj = metrics.AUC(\n-            num_thresholds=self.num_thresholds, summation_method='majoring')\n-        result = auc_obj(self.y_true, self.y_pred, sample_weight=self.sample_weight)\n-\n-        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n-        # recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]\n-        # fp_rate = [3/3, 0, 0] = [1, 0, 0]\n-        # heights = [max(1, 0.571), max(0.571, 0)] = [1, 0.571]\n-        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n-        expected_result = (1 * 1 + 0.571 * 0)\n-        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n-\n-    def test_weighted_roc_minoring(self):\n-        self.setup()\n-        auc_obj = metrics.AUC(\n-            num_thresholds=self.num_thresholds, summation_method='minoring')\n-        result = auc_obj(self.y_true, self.y_pred, sample_weight=self.sample_weight)\n-\n-        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n-        # recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]\n-        # fp_rate = [3/3, 0, 0] = [1, 0, 0]\n-        # heights = [min(1, 0.571), min(0.571, 0)] = [0.571, 0]\n-        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n-        expected_result = (0.571 * 1 + 0 * 0)\n-        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n-\n-    def test_weighted_pr_majoring(self):\n-        self.setup()\n-        auc_obj = metrics.AUC(\n-            num_thresholds=self.num_thresholds,\n-            curve='PR',\n-            summation_method='majoring')\n-        result = auc_obj(self.y_true, self.y_pred, sample_weight=self.sample_weight)\n-\n-        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n-        # precision = [7/(7+3), 4/4, 0] = [0.7, 1, 0]\n-        # recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]\n-        # heights = [max(0.7, 1), max(1, 0)] = [1, 1]\n-        # widths = [(1 - 0.571), (0.571 - 0)] = [0.429, 0.571]\n-        expected_result = (1 * 0.429 + 1 * 0.571)\n-        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n-\n-    def test_weighted_pr_minoring(self):\n-        self.setup()\n-        auc_obj = metrics.AUC(\n-            num_thresholds=self.num_thresholds,\n-            curve='PR',\n-            summation_method='minoring')\n-        result = auc_obj(self.y_true, self.y_pred, sample_weight=self.sample_weight)\n-\n-        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n-        # precision = [7/(7+3), 4/4, 0] = [0.7, 1, 0]\n-        # recall = [7/7, 4/(4+3), 0] = [1, 0.571, 0]\n-        # heights = [min(0.7, 1), min(1, 0)] = [0.7, 0]\n-        # widths = [(1 - 0.571), (0.571 - 0)] = [0.429, 0.571]\n-        expected_result = (0.7 * 0.429 + 0 * 0.571)\n-        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n-\n-    def test_weighted_pr_interpolation(self):\n-        self.setup()\n-        auc_obj = metrics.AUC(num_thresholds=self.num_thresholds, curve='PR')\n-        result = auc_obj(self.y_true, self.y_pred, sample_weight=self.sample_weight)\n-\n-        # auc = (slope / Total Pos) * [dTP - intercept * log(Pb/Pa)]\n-\n-        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n-        # P = tp + fp = [10, 4, 0]\n-        # dTP = [7-4, 4-0] = [3, 4]\n-        # dP = [10-4, 4-0] = [6, 4]\n-        # slope = dTP/dP = [0.5, 1]\n-        # intercept = (TPa+(slope*Pa) = [(4 - 0.5*4), (0 - 1*0)] = [2, 0]\n-        # (Pb/Pa) = (Pb/Pa) if Pb > 0 AND Pa > 0 else 1 = [10/4, 4/0] = [2.5, 1]\n-        # auc * TotalPos = [(0.5 * (3 + 2 * log(2.5))), (1 * (4 + 0))]\n-        #                = [2.416, 4]\n-        # auc = [2.416, 4]/(tp[1:]+fn[1:])\n-        # expected_result = (2.416 / 7 + 4 / 7)\n-        expected_result = 0.345 + 0.571\n-        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n-\n-    def test_invalid_num_thresholds(self):\n-        with pytest.raises(Exception):\n-            metrics.AUC(num_thresholds=-1)\n-\n-        with pytest.raises(Exception):\n-            metrics.AUC(num_thresholds=1)\n-\n-    def test_invalid_curve(self):\n-        with pytest.raises(Exception):\n-            metrics.AUC(curve='Invalid')\n-\n-    def test_invalid_summation_method(self):\n-        with pytest.raises(Exception):\n-            metrics.AUC(summation_method='Invalid')\n-\n-\n-class TestPrecisionTest(object):\n-\n-    def test_config(self):\n-        p_obj = metrics.Precision(\n-            name='my_precision', thresholds=[0.4, 0.9], top_k=15, class_id=12)\n-        assert p_obj.name == 'my_precision'\n-        assert len(p_obj.weights) == 2\n-        assert ([v.name for v in p_obj.weights] ==\n-                ['true_positives:0', 'false_positives:0'])\n-        assert p_obj.thresholds == [0.4, 0.9]\n-        assert p_obj.top_k == 15\n-        assert p_obj.class_id == 12\n-\n-        # Check save and restore config\n-        p_obj2 = metrics.Precision.from_config(p_obj.get_config())\n-        assert p_obj2.name == 'my_precision'\n-        assert len(p_obj2.weights) == 2\n-        assert p_obj2.thresholds == [0.4, 0.9]\n-        assert p_obj2.top_k == 15\n-        assert p_obj2.class_id == 12\n-\n-    def test_unweighted(self):\n-        p_obj = metrics.Precision()\n-        y_pred = K.constant([1, 0, 1, 0], shape=(1, 4))\n-        y_true = K.constant([0, 1, 1, 0], shape=(1, 4))\n-        result = p_obj(y_true, y_pred)\n-        assert np.isclose(0.5, K.eval(result))\n-\n-    def test_unweighted_all_incorrect(self):\n-        p_obj = metrics.Precision(thresholds=[0.5])\n-        inputs = np.random.randint(0, 2, size=(100, 1))\n-        y_pred = K.constant(inputs)\n-        y_true = K.constant(1 - inputs)\n-        result = p_obj(y_true, y_pred)\n-        assert np.isclose(0, K.eval(result))\n-\n-    def test_weighted(self):\n-        p_obj = metrics.Precision()\n-        y_pred = K.constant([[1, 0, 1, 0], [1, 0, 1, 0]])\n-        y_true = K.constant([[0, 1, 1, 0], [1, 0, 0, 1]])\n-        result = p_obj(\n-            y_true,\n-            y_pred,\n-            sample_weight=K.constant([[1, 2, 3, 4], [4, 3, 2, 1]]))\n-        weighted_tp = 3.0 + 4.0\n-        weighted_positives = (1.0 + 3.0) + (4.0 + 2.0)\n-        expected_precision = weighted_tp / weighted_positives\n-        assert np.isclose(expected_precision, K.eval(result))\n-\n-    def test_unweighted_with_threshold(self):\n-        p_obj = metrics.Precision(thresholds=[0.5, 0.7])\n-        y_pred = K.constant([1, 0, 0.6, 0], shape=(1, 4))\n-        y_true = K.constant([0, 1, 1, 0], shape=(1, 4))\n-        result = p_obj(y_true, y_pred)\n-        assert np.allclose([0.5, 0.], K.eval(result), 0)\n-\n-    def test_weighted_with_threshold(self):\n-        p_obj = metrics.Precision(thresholds=[0.5, 1.])\n-        y_true = K.constant([[0, 1], [1, 0]], shape=(2, 2))\n-        y_pred = K.constant([[1, 0], [0.6, 0]],\n-                            shape=(2, 2),\n-                            dtype='float32')\n-        weights = K.constant([[4, 0], [3, 1]],\n-                             shape=(2, 2),\n-                             dtype='float32')\n-        result = p_obj(y_true, y_pred, sample_weight=weights)\n-        weighted_tp = 0 + 3.\n-        weighted_positives = (0 + 3.) + (4. + 0.)\n-        expected_precision = weighted_tp / weighted_positives\n-        assert np.allclose([expected_precision, 0], K.eval(result), 1e-3)\n-\n-    def test_unweighted_top_k(self):\n-        p_obj = metrics.Precision(top_k=3)\n-        y_pred = K.constant([0.2, 0.1, 0.5, 0, 0.2], shape=(1, 5))\n-        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-        result = p_obj(y_true, y_pred)\n-        assert np.isclose(1. / 3, K.eval(result))\n-\n-    def test_weighted_top_k(self):\n-        p_obj = metrics.Precision(top_k=3)\n-        y_pred1 = K.constant([0.2, 0.1, 0.4, 0, 0.2], shape=(1, 5))\n-        y_true1 = K.constant([0, 1, 1, 0, 1], shape=(1, 5))\n-        K.eval(\n-            p_obj(\n-                y_true1,\n-                y_pred1,\n-                sample_weight=K.constant([[1, 4, 2, 3, 5]])))\n-\n-        y_pred2 = K.constant([0.2, 0.6, 0.4, 0.2, 0.2], shape=(1, 5))\n-        y_true2 = K.constant([1, 0, 1, 1, 1], shape=(1, 5))\n-        result = p_obj(y_true2, y_pred2, sample_weight=K.constant(3))\n-\n-        tp = (2 + 5) + (3 + 3)\n-        predicted_positives = (1 + 2 + 5) + (3 + 3 + 3)\n-        expected_precision = float(tp) / predicted_positives\n-        assert np.isclose(expected_precision, K.eval(result))\n-\n-    def test_unweighted_class_id(self):\n-        p_obj = metrics.Precision(class_id=2)\n-\n-        y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))\n-        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-        result = p_obj(y_true, y_pred)\n-        assert np.isclose(1, K.eval(result))\n-        assert np.isclose(1, K.eval(p_obj.true_positives))\n-        assert np.isclose(0, K.eval(p_obj.false_positives))\n-\n-        y_pred = K.constant([0.2, 0.1, 0, 0, 0.2], shape=(1, 5))\n-        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-        result = p_obj(y_true, y_pred)\n-        assert np.isclose(1, K.eval(result))\n-        assert np.isclose(1, K.eval(p_obj.true_positives))\n-        assert np.isclose(0, K.eval(p_obj.false_positives))\n-\n-        y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))\n-        y_true = K.constant([0, 1, 0, 0, 0], shape=(1, 5))\n-        result = p_obj(y_true, y_pred)\n-        assert np.isclose(0.5, K.eval(result))\n-        assert np.isclose(1, K.eval(p_obj.true_positives))\n-        assert np.isclose(1, K.eval(p_obj.false_positives))\n-\n-    def test_unweighted_top_k_and_class_id(self):\n-        p_obj = metrics.Precision(class_id=2, top_k=2)\n-\n-        y_pred = K.constant([0.2, 0.6, 0.3, 0, 0.2], shape=(1, 5))\n-        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-        result = p_obj(y_true, y_pred)\n-        assert np.isclose(1, K.eval(result))\n-        assert np.isclose(1, K.eval(p_obj.true_positives))\n-        assert np.isclose(0, K.eval(p_obj.false_positives))\n-\n-        y_pred = K.constant([1, 1, 0.9, 1, 1], shape=(1, 5))\n-        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-        result = p_obj(y_true, y_pred)\n-        assert np.isclose(1, K.eval(result))\n-        assert np.isclose(1, K.eval(p_obj.true_positives))\n-        assert np.isclose(0, K.eval(p_obj.false_positives))\n-\n-    def test_unweighted_top_k_and_threshold(self):\n-        p_obj = metrics.Precision(thresholds=.7, top_k=2)\n-\n-        y_pred = K.constant([0.2, 0.8, 0.6, 0, 0.2], shape=(1, 5))\n-        y_true = K.constant([0, 1, 1, 0, 1], shape=(1, 5))\n-        result = p_obj(y_true, y_pred)\n-        assert np.isclose(1, K.eval(result))\n-        assert np.isclose(1, K.eval(p_obj.true_positives))\n-        assert np.isclose(0, K.eval(p_obj.false_positives))\n-\n-\n-class TestRecall(object):\n-\n-    def test_config(self):\n-        r_obj = metrics.Recall(\n-            name='my_recall', thresholds=[0.4, 0.9], top_k=15, class_id=12)\n-        assert r_obj.name == 'my_recall'\n-        assert len(r_obj.weights) == 2\n-        assert ([v.name for v in r_obj.weights] ==\n-                ['true_positives:0', 'false_negatives:0'])\n-        assert r_obj.thresholds == [0.4, 0.9]\n-        assert r_obj.top_k == 15\n-        assert r_obj.class_id == 12\n-\n-        # Check save and restore config\n-        r_obj2 = metrics.Recall.from_config(r_obj.get_config())\n-        assert r_obj2.name == 'my_recall'\n-        assert len(r_obj2.weights) == 2\n-        assert r_obj2.thresholds == [0.4, 0.9]\n-        assert r_obj2.top_k == 15\n-        assert r_obj2.class_id == 12\n-\n-    def test_unweighted(self):\n-        r_obj = metrics.Recall()\n-        y_pred = K.constant([1, 0, 1, 0], shape=(1, 4))\n-        y_true = K.constant([0, 1, 1, 0], shape=(1, 4))\n-        result = r_obj(y_true, y_pred)\n-        assert np.isclose(0.5, K.eval(result))\n-\n-    def test_unweighted_all_incorrect(self):\n-        r_obj = metrics.Recall(thresholds=[0.5])\n-        inputs = np.random.randint(0, 2, size=(100, 1))\n-        y_pred = K.constant(inputs)\n-        y_true = K.constant(1 - inputs)\n-        result = r_obj(y_true, y_pred)\n-        assert np.isclose(0, K.eval(result))\n-\n-    def test_weighted(self):\n-        r_obj = metrics.Recall()\n-        y_pred = K.constant([[1, 0, 1, 0], [0, 1, 0, 1]])\n-        y_true = K.constant([[0, 1, 1, 0], [1, 0, 0, 1]])\n-        result = r_obj(\n-            y_true,\n-            y_pred,\n-            sample_weight=K.constant([[1, 2, 3, 4], [4, 3, 2, 1]]))\n-        weighted_tp = 3.0 + 1.0\n-        weighted_t = (2.0 + 3.0) + (4.0 + 1.0)\n-        expected_recall = weighted_tp / weighted_t\n-        assert np.isclose(expected_recall, K.eval(result))\n-\n-    def test_unweighted_with_threshold(self):\n-        r_obj = metrics.Recall(thresholds=[0.5, 0.7])\n-        y_pred = K.constant([1, 0, 0.6, 0], shape=(1, 4))\n-        y_true = K.constant([0, 1, 1, 0], shape=(1, 4))\n-        result = r_obj(y_true, y_pred)\n-        assert np.allclose([0.5, 0.], K.eval(result), 0)\n-\n-    def test_weighted_with_threshold(self):\n-        r_obj = metrics.Recall(thresholds=[0.5, 1.])\n-        y_true = K.constant([[0, 1], [1, 0]], shape=(2, 2))\n-        y_pred = K.constant([[1, 0], [0.6, 0]],\n-                            shape=(2, 2),\n-                            dtype='float32')\n-        weights = K.constant([[1, 4], [3, 2]],\n-                             shape=(2, 2),\n-                             dtype='float32')\n-        result = r_obj(y_true, y_pred, sample_weight=weights)\n-        weighted_tp = 0 + 3.\n-        weighted_positives = (0 + 3.) + (4. + 0.)\n-        expected_recall = weighted_tp / weighted_positives\n-        assert np.allclose([expected_recall, 0], K.eval(result), 1e-3)\n-\n-    def test_unweighted_top_k(self):\n-        r_obj = metrics.Recall(top_k=3)\n-        y_pred = K.constant([0.2, 0.1, 0.5, 0, 0.2], shape=(1, 5))\n-        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-        result = r_obj(y_true, y_pred)\n-        assert np.isclose(0.5, K.eval(result))\n-\n-    def test_weighted_top_k(self):\n-        r_obj = metrics.Recall(top_k=3)\n-        y_pred1 = K.constant([0.2, 0.1, 0.4, 0, 0.2], shape=(1, 5))\n-        y_true1 = K.constant([0, 1, 1, 0, 1], shape=(1, 5))\n-        K.eval(\n-            r_obj(\n-                y_true1,\n-                y_pred1,\n-                sample_weight=K.constant([[1, 4, 2, 3, 5]])))\n-\n-        y_pred2 = K.constant([0.2, 0.6, 0.4, 0.2, 0.2], shape=(1, 5))\n-        y_true2 = K.constant([1, 0, 1, 1, 1], shape=(1, 5))\n-        result = r_obj(y_true2, y_pred2, sample_weight=K.constant(3))\n-\n-        tp = (2 + 5) + (3 + 3)\n-        positives = (4 + 2 + 5) + (3 + 3 + 3 + 3)\n-        expected_recall = float(tp) / positives\n-        assert np.isclose(expected_recall, K.eval(result))\n-\n-    def test_unweighted_class_id(self):\n-        r_obj = metrics.Recall(class_id=2)\n-\n-        y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))\n-        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-        result = r_obj(y_true, y_pred)\n-        assert np.isclose(1, K.eval(result))\n-        assert np.isclose(1, K.eval(r_obj.true_positives))\n-        assert np.isclose(0, K.eval(r_obj.false_negatives))\n-\n-        y_pred = K.constant([0.2, 0.1, 0, 0, 0.2], shape=(1, 5))\n-        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-        result = r_obj(y_true, y_pred)\n-        assert np.isclose(0.5, K.eval(result))\n-        assert np.isclose(1, K.eval(r_obj.true_positives))\n-        assert np.isclose(1, K.eval(r_obj.false_negatives))\n-\n-        y_pred = K.constant([0.2, 0.1, 0.6, 0, 0.2], shape=(1, 5))\n-        y_true = K.constant([0, 1, 0, 0, 0], shape=(1, 5))\n-        result = r_obj(y_true, y_pred)\n-        assert np.isclose(0.5, K.eval(result))\n-        assert np.isclose(1, K.eval(r_obj.true_positives))\n-        assert np.isclose(1, K.eval(r_obj.false_negatives))\n-\n-    def test_unweighted_top_k_and_class_id(self):\n-        r_obj = metrics.Recall(class_id=2, top_k=2)\n-\n-        y_pred = K.constant([0.2, 0.6, 0.3, 0, 0.2], shape=(1, 5))\n-        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-        result = r_obj(y_true, y_pred)\n-        assert np.isclose(1, K.eval(result))\n-        assert np.isclose(1, K.eval(r_obj.true_positives))\n-        assert np.isclose(0, K.eval(r_obj.false_negatives))\n-\n-        y_pred = K.constant([1, 1, 0.9, 1, 1], shape=(1, 5))\n-        y_true = K.constant([0, 1, 1, 0, 0], shape=(1, 5))\n-        result = r_obj(y_true, y_pred)\n-        assert np.isclose(0.5, K.eval(result))\n-        assert np.isclose(1, K.eval(r_obj.true_positives))\n-        assert np.isclose(1, K.eval(r_obj.false_negatives))\n-\n-    def test_unweighted_top_k_and_threshold(self):\n-        r_obj = metrics.Recall(thresholds=.7, top_k=2)\n-\n-        y_pred = K.constant([0.2, 0.8, 0.6, 0, 0.2], shape=(1, 5))\n-        y_true = K.constant([1, 1, 1, 0, 1], shape=(1, 5))\n-        result = r_obj(y_true, y_pred)\n-        assert np.isclose(0.25, K.eval(result))\n-        assert np.isclose(1, K.eval(r_obj.true_positives))\n-        assert np.isclose(3, K.eval(r_obj.false_negatives))\n-\n-\n-@pytest.mark.skipif(not tf.__version__.startswith('2.'),\n-                    reason='Requires TF 2')\n-class TestMeanIoU(object):\n-\n-    def test_config(self):\n-        m_obj = metrics.MeanIoU(num_classes=2, name='mean_iou')\n-        assert m_obj.name == 'mean_iou'\n-        assert m_obj.num_classes == 2\n-\n-        m_obj2 = metrics.MeanIoU.from_config(m_obj.get_config())\n-        assert m_obj2.name == 'mean_iou'\n-        assert m_obj2.num_classes == 2\n-\n-    def test_unweighted(self):\n-        y_pred = K.constant([0, 1, 0, 1], shape=(1, 4))\n-        y_true = K.constant([0, 0, 1, 1], shape=(1, 4))\n-\n-        m_obj = metrics.MeanIoU(num_classes=2)\n-        result = m_obj(y_true, y_pred)\n-\n-        # cm = [[1, 1],\n-        #       [1, 1]]\n-        # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = (1. / (2 + 2 - 1) + 1. / (2 + 2 - 1)) / 2\n-        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n-\n-    def test_weighted(self):\n-        y_pred = K.constant([0, 1, 0, 1], dtype='float32')\n-        y_true = K.constant([0, 0, 1, 1])\n-        sample_weight = K.constant([0.2, 0.3, 0.4, 0.1])\n-\n-        m_obj = metrics.MeanIoU(num_classes=2)\n-        result = m_obj(y_true, y_pred, sample_weight=sample_weight)\n-\n-        # cm = [[0.2, 0.3],\n-        #       [0.4, 0.1]]\n-        # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2, 0.1]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = (0.2 / (0.6 + 0.5 - 0.2) + 0.1 / (0.4 + 0.5 - 0.1)) / 2\n-        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n-\n-    def test_multi_dim_input(self):\n-        y_pred = K.constant([[0, 1], [0, 1]], dtype='float32')\n-        y_true = K.constant([[0, 0], [1, 1]])\n-        sample_weight = K.constant([[0.2, 0.3], [0.4, 0.1]])\n-\n-        m_obj = metrics.MeanIoU(num_classes=2)\n-        result = m_obj(y_true, y_pred, sample_weight=sample_weight)\n-\n-        # cm = [[0.2, 0.3],\n-        #       [0.4, 0.1]]\n-        # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2, 0.1]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = (0.2 / (0.6 + 0.5 - 0.2) + 0.1 / (0.4 + 0.5 - 0.1)) / 2\n-        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n-\n-    def test_zero_valid_entries(self):\n-        m_obj = metrics.MeanIoU(num_classes=2)\n-        assert np.allclose(K.eval(m_obj.result()), 0, atol=1e-3)\n-\n-    def test_zero_and_non_zero_entries(self):\n-        y_pred = K.constant([1], dtype='float32')\n-        y_true = K.constant([1])\n-\n-        m_obj = metrics.MeanIoU(num_classes=2)\n-        result = m_obj(y_true, y_pred)\n-\n-        # cm = [[0, 0],\n-        #       [0, 1]]\n-        # sum_row = [0, 1], sum_col = [0, 1], true_positives = [0, 1]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = (0. + 1. / (1 + 1 - 1)) / 1\n-        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n\n@@ -1,379 +0,0 @@\n-\"\"\"Tests for Keras metrics correctness.\"\"\"\n-\n-import numpy as np\n-\n-import keras\n-from keras import layers\n-from keras import losses\n-from keras import metrics\n-from keras import backend as K\n-\n-\n-def get_multi_io_model():\n-    inp_1 = layers.Input(shape=(1,), name='input_1')\n-    inp_2 = layers.Input(shape=(1,), name='input_2')\n-    dense = layers.Dense(3, kernel_initializer='ones', trainable=False)\n-    x_1 = dense(inp_1)\n-    x_2 = dense(inp_2)\n-    out_1 = layers.Dense(\n-        1, kernel_initializer='ones', name='output_1', trainable=False)(x_1)\n-    out_2 = layers.Dense(\n-        1, kernel_initializer='ones', name='output_2', trainable=False)(x_2)\n-    return keras.Model([inp_1, inp_2], [out_1, out_2])\n-\n-\n-def custom_generator_multi_io(sample_weights=None):\n-    batch_size = 2\n-    num_samples = 4\n-    inputs = np.asarray([[1.], [2.], [3.], [4.]])\n-    targets_1 = np.asarray([[2.], [4.], [6.], [8.]])\n-    targets_2 = np.asarray([[1.], [2.], [3.], [4.]])\n-    w1 = sample_weights[0] if sample_weights else None\n-    w2 = sample_weights[1] if sample_weights else None\n-    i = 0\n-    while True:\n-        batch_index = i * batch_size % num_samples\n-        i += 1\n-        start = batch_index\n-        end = start + batch_size\n-        x = [inputs[start:end], inputs[start:end]]\n-        y = [targets_1[start:end], targets_2[start:end]]\n-        if sample_weights:\n-            w = [\n-                None if w1 is None else w1[start:end],\n-                None if w2 is None else w2[start:end]\n-            ]\n-        else:\n-            w = None\n-        yield x, y, w\n-\n-\n-class TestMetricsCorrectnessMultiIO(object):\n-\n-    def _get_compiled_multi_io_model(self):\n-        model = get_multi_io_model()\n-        model.compile(\n-            optimizer='rmsprop',\n-            loss=losses.MeanSquaredError(),\n-            metrics=[metrics.MeanSquaredError(name='mean_squared_error')],\n-            weighted_metrics=[\n-                metrics.MeanSquaredError(name='mean_squared_error_2')\n-            ])\n-        return model\n-\n-    def setUp(self):\n-        self.x = np.asarray([[1.], [2.], [3.], [4.]])\n-        self.y1 = np.asarray([[2.], [4.], [6.], [8.]])\n-        self.y2 = np.asarray([[1.], [2.], [3.], [4.]])\n-        self.sample_weight_1 = np.asarray([2., 3., 4., 5.])\n-        self.sample_weight_2 = np.asarray([3.5, 2.5, 1.5, 0.5])\n-        self.class_weight_1 = {2: 2, 4: 3, 6: 4, 8: 5}\n-        self.class_weight_2 = {1: 3.5, 2: 2.5, 3: 1.5, 4: 0.5}\n-\n-        # y_true_1 = [[2.], [4.], [6.], [8.]], y_pred = [[3.], [6.], [9.], [12.]]\n-        # y_true_2 = [[1.], [2.], [3.], [4.]], y_pred = [[3.], [6.], [9.], [12.]]\n-\n-        # Weighted metric `output_1`:\n-        #   Total = ((3 - 2)^2 * 2  + (6 - 4)^2 * 3) +\n-        #           ((9 - 6)^2 * 4 + (12 - 8)^2 * 5)\n-        #         = 130\n-        #   Count = (2 + 3) + (4 + 5)\n-        #   Result = 9.2857141\n-\n-        # Weighted metric `output_2`:\n-        #   Total = ((3 - 1)^2 * 3.5 + (6 - 2)^2 * 2.5) +\n-        #           ((9 - 3)^2 * 1.5 + (12 - 4)^2 * 0.5)\n-        #         = 140\n-        #   Count = (3.5 + 2.5) + (1.5 + 0.5)\n-        #   Result = 17.5\n-\n-        # Loss `output_1` with weights:\n-        #   Total = ((3 - 2)^2 * 2  + (6 - 4)^2 * 3) +\n-        #           ((9 - 6)^2 * 4 + (12 - 8)^2 * 5)\n-        #         = 130\n-        #   Count = 2 + 2\n-        #   Result = 32.5\n-\n-        # Loss `output_1` without weights/Metric `output_1`:\n-        #   Total = ((3 - 2)^2 + (6 - 4)^2) + ((9 - 6)^2 + (12 - 8)^2) = 30\n-        #   Count = 2 + 2\n-        #   Result = 7.5\n-\n-        # Loss `output_2` with weights:\n-        #   Total = ((3 - 1)^2 * 3.5 + (6 - 2)^2 * 2.5) +\n-        #           ((9 - 3)^2 * 1.5 + (12 - 4)^2 * 0.5)\n-        #         = 140\n-        #   Count = 2 + 2\n-        #   Result = 35\n-\n-        # Loss `output_2` without weights/Metric `output_2`:\n-        #   Total = ((3 - 1)^2 + (6 - 2)^2) + ((9 - 3)^2 + (12 - 4)^2) = 120\n-        #   Count = 2 + 2\n-        #   Result = 30\n-\n-        # Total loss with weights = 32.5 + 35 = 67.5\n-        # Total loss without weights = 7.5 + 30 = 37.5\n-\n-        self.expected_fit_result_with_weights = {\n-            'output_1_mean_squared_error': [7.5, 7.5],\n-            'output_2_mean_squared_error': [30, 30],\n-            'output_1_mean_squared_error_2': [9.286, 9.286],\n-            'output_2_mean_squared_error_2': [17.5, 17.5],\n-            'loss': [67.5, 67.5],\n-            'output_1_loss': [32.5, 32.5],\n-            'output_2_loss': [35, 35],\n-        }\n-\n-        self.expected_fit_result_with_weights_output_2 = {\n-            'output_1_mean_squared_error': [7.5, 7.5],\n-            'output_2_mean_squared_error': [30, 30],\n-            'output_1_mean_squared_error_2': [7.5, 7.5],\n-            'output_2_mean_squared_error_2': [17.5, 17.5],\n-            'loss': [42.5, 42.5],\n-            'output_1_loss': [7.5, 7.5],\n-            'output_2_loss': [35, 35],\n-        }\n-\n-        self.expected_fit_result = {\n-            'output_1_mean_squared_error': [7.5, 7.5],\n-            'output_2_mean_squared_error': [30, 30],\n-            'output_1_mean_squared_error_2': [7.5, 7.5],\n-            'output_2_mean_squared_error_2': [30, 30],\n-            'loss': [37.5, 37.5],\n-            'output_1_loss': [7.5, 7.5],\n-            'output_2_loss': [30, 30],\n-        }\n-\n-        # In the order: 'loss', 'output_1_loss', 'output_2_loss',\n-        # 'output_1_mean_squared_error', 'output_1_mean_squared_error_2',\n-        # 'output_2_mean_squared_error', 'output_2_mean_squared_error_2'\n-        self.expected_batch_result_with_weights = [\n-            67.5, 32.5, 35, 7.5, 9.286, 30, 17.5\n-        ]\n-        self.expected_batch_result_with_weights_output_2 = [\n-            42.5, 7.5, 35, 7.5, 7.5, 30, 17.5\n-        ]\n-        self.expected_batch_result = [37.5, 7.5, 30, 7.5, 7.5, 30, 30]\n-\n-    def test_fit(self):\n-        self.setUp()\n-        model = self._get_compiled_multi_io_model()\n-        history = model.fit([self.x, self.x], [self.y1, self.y2],\n-                            batch_size=2,\n-                            epochs=2,\n-                            shuffle=False)\n-        for key, value in self.expected_fit_result.items():\n-            np.allclose(history.history[key], value, 1e-3)\n-\n-    def test_fit_with_sample_weight(self):\n-        self.setUp()\n-        model = self._get_compiled_multi_io_model()\n-        history = model.fit([self.x, self.x], [self.y1, self.y2],\n-                            sample_weight={\n-                                'output_1': self.sample_weight_1,\n-                                'output_2': self.sample_weight_2},\n-                            batch_size=2,\n-                            epochs=2,\n-                            shuffle=False)\n-        for key, value in self.expected_fit_result_with_weights.items():\n-            np.allclose(history.history[key], value, 1e-3)\n-\n-        # Set weights for one output (use batch size).\n-        history = model.fit([self.x, self.x], [self.y1, self.y2],\n-                            sample_weight={'output_2': self.sample_weight_2},\n-                            batch_size=2,\n-                            epochs=2,\n-                            shuffle=False)\n-\n-        for key, value in self.expected_fit_result_with_weights_output_2.items():\n-            np.allclose(history.history[key], value, 1e-3)\n-\n-    def DISABLED_test_fit_with_class_weight(self):\n-        self.setUp()\n-        model = self._get_compiled_multi_io_model()\n-        history = model.fit([self.x, self.x], [self.y1, self.y2],\n-                            class_weight={\n-                                'output_1': self.class_weight_1,\n-                                'output_2': self.class_weight_2},\n-                            batch_size=2,\n-                            epochs=2,\n-                            shuffle=False)\n-        for key, value in self.expected_fit_result_with_weights.items():\n-            np.allclose(history.history[key], value, 1e-3)\n-\n-        # Set weights for one output.\n-        history = model.fit([self.x, self.x], [self.y1, self.y2],\n-                            class_weight={'output_2': self.class_weight_2},\n-                            batch_size=2,\n-                            epochs=2,\n-                            shuffle=False)\n-\n-        for key, value in self.expected_fit_result_with_weights_output_2.items():\n-            np.allclose(history.history[key], value, 1e-3)\n-\n-    def test_eval(self):\n-        self.setUp()\n-        model = self._get_compiled_multi_io_model()\n-        eval_result = model.evaluate([self.x, self.x], [self.y1, self.y2],\n-                                     batch_size=2)\n-        np.allclose(eval_result, self.expected_batch_result, 1e-3)\n-\n-    def test_eval_with_sample_weight(self):\n-        self.setUp()\n-        model = self._get_compiled_multi_io_model()\n-        eval_result = model.evaluate([self.x, self.x], [self.y1, self.y2],\n-                                     batch_size=2,\n-                                     sample_weight={\n-                                         'output_1': self.sample_weight_1,\n-                                         'output_2': self.sample_weight_2})\n-        np.allclose(eval_result, self.expected_batch_result_with_weights,\n-                    1e-3)\n-\n-        # Set weights for one output.\n-        model = self._get_compiled_multi_io_model()\n-        eval_result = model.evaluate([self.x, self.x], [self.y1, self.y2],\n-                                     batch_size=2,\n-                                     sample_weight={\n-                                         'output_2': self.sample_weight_2})\n-        np.allclose(eval_result,\n-                    self.expected_batch_result_with_weights_output_2, 1e-3)\n-\n-        # Verify that metric value is same with arbitrary weights and batch size.\n-        x = np.random.random((50, 1))\n-        y = np.random.random((50, 1))\n-        w = np.random.random((50,))\n-        mse1 = model.evaluate([x, x], [y, y], sample_weight=[w, w], batch_size=5)[3]\n-        mse2 = model.evaluate([x, x], [y, y], sample_weight=[w, w],\n-                              batch_size=10)[3]\n-        np.allclose(mse1, mse2, 1e-3)\n-\n-    def test_train_on_batch(self):\n-        self.setUp()\n-        model = self._get_compiled_multi_io_model()\n-        result = model.train_on_batch([self.x, self.x], [self.y1, self.y2])\n-        np.allclose(result, self.expected_batch_result, 1e-3)\n-\n-    def test_train_on_batch_with_sample_weight(self):\n-        self.setUp()\n-        model = self._get_compiled_multi_io_model()\n-        result = model.train_on_batch([self.x, self.x], [self.y1, self.y2],\n-                                      sample_weight={\n-                                          'output_1': self.sample_weight_1,\n-                                          'output_2': self.sample_weight_2})\n-        np.allclose(result, self.expected_batch_result_with_weights, 1e-3)\n-\n-        # Set weights for one output.\n-        result = model.train_on_batch([self.x, self.x], [self.y1, self.y2],\n-                                      sample_weight={\n-                                          'output_2': self.sample_weight_2})\n-        np.allclose(result, self.expected_batch_result_with_weights_output_2, 1e-3)\n-\n-    def DISABLED_test_train_on_batch_with_class_weight(self):\n-        self.setUp()\n-        model = self._get_compiled_multi_io_model()\n-        result = model.train_on_batch([self.x, self.x], [self.y1, self.y2],\n-                                      class_weight={\n-                                          'output_1': self.class_weight_1,\n-                                          'output_2': self.class_weight_2})\n-        np.allclose(result, self.expected_batch_result_with_weights, 1e-3)\n-\n-        # Set weights for one output.\n-        result = model.train_on_batch([self.x, self.x], [self.y1, self.y2],\n-                                      class_weight={\n-                                          'output_2': self.class_weight_2})\n-        np.allclose(result,\n-                    self.expected_batch_result_with_weights_output_2, 1e-3)\n-\n-    def test_test_on_batch(self):\n-        self.setUp()\n-        model = self._get_compiled_multi_io_model()\n-        result = model.test_on_batch([self.x, self.x], [self.y1, self.y2])\n-        np.allclose(result, self.expected_batch_result, 1e-3)\n-\n-    def test_test_on_batch_with_sample_weight(self):\n-        self.setUp()\n-        model = self._get_compiled_multi_io_model()\n-        result = model.test_on_batch([self.x, self.x], [self.y1, self.y2],\n-                                     sample_weight={\n-                                         'output_1': self.sample_weight_1,\n-                                         'output_2': self.sample_weight_2})\n-        np.allclose(result, self.expected_batch_result_with_weights, 1e-3)\n-\n-        # Set weights for one output.\n-        result = model.test_on_batch([self.x, self.x], [self.y1, self.y2],\n-                                     sample_weight={\n-                                         'output_2': self.sample_weight_2})\n-        np.allclose(result,\n-                    self.expected_batch_result_with_weights_output_2, 1e-3)\n-\n-    def test_fit_generator(self):\n-        self.setUp()\n-        model = self._get_compiled_multi_io_model()\n-        history = model.fit_generator(\n-            custom_generator_multi_io(), steps_per_epoch=2, epochs=2)\n-        for key, value in self.expected_fit_result.items():\n-            np.allclose(history.history[key], value, 1e-3)\n-\n-    def DISABLED_test_fit_generator_with_sample_weight(self):\n-        self.setUp()\n-        model = self._get_compiled_multi_io_model()\n-        history = model.fit_generator(\n-            custom_generator_multi_io(\n-                sample_weights=[self.sample_weight_1, self.sample_weight_2]),\n-            steps_per_epoch=2,\n-            epochs=2)\n-        for key, value in self.expected_fit_result_with_weights.items():\n-            np.allclose(history.history[key], value, 1e-3)\n-\n-        # Set weights for one output.\n-        history = model.fit_generator(\n-            custom_generator_multi_io(sample_weights=[None, self.sample_weight_2]),\n-            steps_per_epoch=2,\n-            epochs=2)\n-        for key, value in self.expected_fit_result_with_weights_output_2.items():\n-            np.allclose(history.history[key], value, 1e-3)\n-\n-    def DISABLED_test_fit_generator_with_class_weight(self):\n-        self.setUp()\n-        model = self._get_compiled_multi_io_model()\n-        history = model.fit_generator(\n-            custom_generator_multi_io(),\n-            class_weight={\n-                'output_1': self.class_weight_1,\n-                'output_2': self.class_weight_2,\n-            },\n-            steps_per_epoch=2,\n-            epochs=2)\n-        for key, value in self.expected_fit_result_with_weights.items():\n-            np.allclose(history.history[key], value, 1e-3)\n-\n-        # Set weights for one output.\n-        history = model.fit_generator(\n-            custom_generator_multi_io(),\n-            class_weight={'output_2': self.class_weight_2},\n-            steps_per_epoch=2,\n-            epochs=2)\n-        for key, value in self.expected_fit_result_with_weights_output_2.items():\n-            np.allclose(history.history[key], value, 1e-3)\n-\n-    def test_eval_generator(self):\n-        self.setUp()\n-        model = self._get_compiled_multi_io_model()\n-        eval_result = model.evaluate_generator(custom_generator_multi_io(), steps=2)\n-        np.allclose(eval_result, self.expected_batch_result, 1e-3)\n-\n-    def DISABLED_test_eval_generator_with_sample_weight(self):\n-        self.setUp()\n-        model = self._get_compiled_multi_io_model()\n-        eval_result = model.evaluate_generator(\n-            custom_generator_multi_io(\n-                sample_weights=[self.sample_weight_1, self.sample_weight_2]),\n-            steps=2)\n-        np.allclose(eval_result, self.expected_batch_result_with_weights, 1e-3)\n-\n-        # Set weights for one output.\n-        eval_result = model.evaluate_generator(\n-            custom_generator_multi_io(sample_weights=[None, self.sample_weight_2]),\n-            steps=2)\n-        np.allclose(eval_result,\n-                    self.expected_batch_result_with_weights_output_2, 1e-3)\n\n@@ -1,123 +0,0 @@\n-import pytest\n-import numpy as np\n-from numpy.testing import assert_allclose\n-from flaky import flaky\n-\n-import keras\n-from keras import metrics\n-from keras import backend as K\n-\n-all_metrics = [\n-    metrics.binary_accuracy,\n-    metrics.categorical_accuracy,\n-    metrics.mean_squared_error,\n-    metrics.mean_absolute_error,\n-    metrics.mean_absolute_percentage_error,\n-    metrics.mean_squared_logarithmic_error,\n-    metrics.squared_hinge,\n-    metrics.hinge,\n-    metrics.categorical_crossentropy,\n-    metrics.binary_crossentropy,\n-    metrics.poisson,\n-]\n-\n-all_sparse_metrics = [\n-    metrics.sparse_categorical_accuracy,\n-    metrics.sparse_categorical_crossentropy,\n-]\n-\n-\n-@pytest.mark.parametrize('metric', all_metrics)\n-def test_metrics(metric):\n-    y_a = K.variable(np.random.random((6, 7)))\n-    y_b = K.variable(np.random.random((6, 7)))\n-    output = metric(y_a, y_b)\n-    assert K.eval(output).shape == (6,)\n-\n-\n-@pytest.mark.parametrize('metric', all_sparse_metrics)\n-def test_sparse_metrics(metric):\n-    y_a = K.variable(np.random.randint(0, 7, (6,)), dtype=K.floatx())\n-    y_b = K.variable(np.random.random((6, 7)), dtype=K.floatx())\n-    assert K.eval(metric(y_a, y_b)).shape == (6,)\n-\n-\n-@pytest.mark.parametrize('shape', [(6,), (6, 3), (6, 3, 1)])\n-def test_sparse_categorical_accuracy_correctness(shape):\n-    y_a = K.variable(np.random.randint(0, 7, shape), dtype=K.floatx())\n-    y_b_shape = shape + (7,)\n-    y_b = K.variable(np.random.random(y_b_shape), dtype=K.floatx())\n-    # use one_hot embedding to convert sparse labels to equivalent dense labels\n-    y_a_dense_labels = K.cast(K.one_hot(K.cast(y_a, dtype='int32'), 7),\n-                              dtype=K.floatx())\n-    sparse_categorical_acc = metrics.sparse_categorical_accuracy(y_a, y_b)\n-    categorical_acc = metrics.categorical_accuracy(y_a_dense_labels, y_b)\n-    assert np.allclose(K.eval(sparse_categorical_acc), K.eval(categorical_acc))\n-\n-\n-def test_serialize():\n-    '''This is a mock 'round trip' of serialize and deserialize.\n-    '''\n-\n-    class MockMetric:\n-        def __init__(self):\n-            self.__name__ = \"mock_metric\"\n-\n-    mock = MockMetric()\n-    found = metrics.serialize(mock)\n-    assert found == \"mock_metric\"\n-\n-    found = metrics.deserialize('mock_metric',\n-                                custom_objects={'mock_metric': True})\n-    assert found is True\n-\n-\n-def test_invalid_get():\n-\n-    with pytest.raises(ValueError):\n-        metrics.get(5)\n-\n-\n-@pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason='CNTK backend does not support top_k yet')\n-def test_top_k_categorical_accuracy():\n-    y_pred = K.variable(np.array([[0.3, 0.2, 0.1], [0.1, 0.2, 0.7]]))\n-    y_true = K.variable(np.array([[0, 1, 0], [1, 0, 0]]))\n-    success_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n-                                                               k=3))\n-    assert np.mean(success_result) == 1\n-    partial_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n-                                                               k=2))\n-    assert np.mean(partial_result) == 0.5\n-    failure_result = K.eval(metrics.top_k_categorical_accuracy(y_true, y_pred,\n-                                                               k=1))\n-    assert np.mean(failure_result) == 0\n-\n-\n-@pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason='CNTK backend does not support top_k yet')\n-@pytest.mark.parametrize('y_pred, y_true', [\n-    # Test correctness if the shape of y_true is (num_samples, 1)\n-    (np.array([[0.3, 0.2, 0.1], [0.1, 0.2, 0.7]]), np.array([[1], [0]])),\n-    # Test correctness if the shape of y_true is (num_samples,)\n-    (np.array([[0.3, 0.2, 0.1], [0.1, 0.2, 0.7]]), np.array([1, 0])),\n-])\n-def test_sparse_top_k_categorical_accuracy(y_pred, y_true):\n-    y_pred = K.variable(y_pred)\n-    y_true = K.variable(y_true)\n-    success_result = K.eval(\n-        metrics.sparse_top_k_categorical_accuracy(y_true, y_pred, k=3))\n-\n-    assert np.mean(success_result) == 1\n-    partial_result = K.eval(\n-        metrics.sparse_top_k_categorical_accuracy(y_true, y_pred, k=2))\n-\n-    assert np.mean(partial_result) == 0.5\n-    failure_result = K.eval(\n-        metrics.sparse_top_k_categorical_accuracy(y_true, y_pred, k=1))\n-\n-    assert np.mean(failure_result) == 0\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,1028 +0,0 @@\n-\"\"\"Tests for Keras metrics classes.\"\"\"\n-import pytest\n-import numpy as np\n-import math\n-\n-from keras import metrics\n-from keras import backend as K\n-\n-\n-if K.backend() != 'tensorflow':\n-    # Need TensorFlow to use metric.__call__\n-    pytestmark = pytest.mark.skip\n-\n-\n-class TestSum(object):\n-\n-    def test_sum(self):\n-        m = metrics.Sum(name='my_sum', dtype='float32')\n-\n-        # check config\n-        assert m.name == 'my_sum'\n-        assert m.stateful\n-        assert m.dtype == 'float32'\n-        assert len(m.weights) == 1\n-\n-        # check initial state\n-        assert K.eval(m.total) == 0\n-\n-        # check __call__\n-        assert K.eval(m(100.0)) == 100\n-        assert K.eval(m.total) == 100\n-\n-        # check update_state() and result() + state accumulation + tensor input\n-        result = m([1, 5])\n-        assert np.isclose(K.eval(result), 106)\n-        assert K.eval(m.total) == 106  # 100 + 1 + 5\n-\n-        # check reset_states()\n-        m.reset_states()\n-        assert K.eval(m.total) == 0\n-\n-    def test_sum_with_sample_weight(self):\n-        m = metrics.Sum(dtype='float64')\n-        assert m.dtype == 'float64'\n-\n-        # check scalar weight\n-        result_t = m(100, sample_weight=0.5)\n-        assert K.eval(result_t) == 50\n-        assert K.eval(m.total) == 50\n-\n-        # check weights not scalar and weights rank matches values rank\n-        result_t = m([1, 5], sample_weight=[1, 0.2])\n-        result = K.eval(result_t)\n-        assert np.isclose(result, 52.)  # 50 + 1 + 5 * 0.2\n-        assert np.isclose(K.eval(m.total), 52.)\n-\n-        # check weights broadcast\n-        result_t = m([1, 2], sample_weight=0.5)\n-        assert np.isclose(K.eval(result_t), 53.5)  # 52 + 0.5 + 1\n-        assert np.isclose(K.eval(m.total), 53.5)\n-\n-        # check weights squeeze\n-        result_t = m([1, 5], sample_weight=[[1], [0.2]])\n-        assert np.isclose(K.eval(result_t), 55.5)  # 53.5 + 1 + 1\n-        assert np.isclose(K.eval(m.total), 55.5)\n-\n-        # check weights expand\n-        result_t = m([[1], [5]], sample_weight=[1, 0.2])\n-        assert np.isclose(K.eval(result_t), 57.5, 2)  # 55.5 + 1 + 1\n-        assert np.isclose(K.eval(m.total), 57.5, 1)\n-\n-        # check values reduced to the dimensions of weight\n-        result_t = m([[[1., 2.], [3., 2.], [0.5, 4.]]], sample_weight=[0.5])\n-        result = np.round(K.eval(result_t), decimals=2)\n-        # result = (prev: 57.5) + 0.5 + 1 + 1.5 + 1 + 0.25 + 2\n-        assert np.isclose(result, 63.75, 2)\n-        assert np.isclose(K.eval(m.total), 63.75, 2)\n-\n-\n-class TestMean(object):\n-\n-    def test_mean(self):\n-        m = metrics.Mean(name='my_mean')\n-\n-        # check config\n-        assert m.name == 'my_mean'\n-        assert m.stateful\n-        assert m.dtype == 'float32'\n-        assert len(m.weights) == 2\n-\n-        # check initial state\n-        assert K.eval(m.total) == 0\n-        assert K.eval(m.count) == 0\n-\n-        # check __call__()\n-        assert K.eval(m(100)) == 100\n-        assert K.eval(m.total) == 100\n-        assert K.eval(m.count) == 1\n-\n-        # check update_state() and result()\n-        result = m([1, 5])\n-        assert np.isclose(K.eval(result), 106. / 3)\n-        assert K.eval(m.total) == 106  # 100 + 1 + 5\n-        assert K.eval(m.count) == 3\n-\n-        # check reset_states()\n-        m.reset_states()\n-        assert K.eval(m.total) == 0\n-        assert K.eval(m.count) == 0\n-\n-        # Check save and restore config\n-        m2 = metrics.Mean.from_config(m.get_config())\n-        assert m2.name == 'my_mean'\n-        assert m2.stateful\n-        assert m2.dtype == 'float32'\n-        assert len(m2.weights) == 2\n-\n-    def test_mean_with_sample_weight(self):\n-        m = metrics.Mean(dtype='float64')\n-        assert m.dtype == 'float64'\n-\n-        # check scalar weight\n-        result_t = m(100, sample_weight=0.5)\n-        assert K.eval(result_t) == 50. / 0.5\n-        assert K.eval(m.total) == 50\n-        assert K.eval(m.count) == 0.5\n-\n-        # check weights not scalar and weights rank matches values rank\n-        result_t = m([1, 5], sample_weight=[1, 0.2])\n-        result = K.eval(result_t)\n-        assert np.isclose(result, 52. / 1.7)\n-        assert np.isclose(K.eval(m.total), 52)  # 50 + 1 + 5 * 0.2\n-        assert np.isclose(K.eval(m.count), 1.7)  # 0.5 + 1.2\n-\n-        # check weights broadcast\n-        result_t = m([1, 2], sample_weight=0.5)\n-        assert np.isclose(K.eval(result_t), 53.5 / 2.7, rtol=3)\n-        assert np.isclose(K.eval(m.total), 53.5, rtol=3)  # 52 + 0.5 + 1\n-        assert np.isclose(K.eval(m.count), 2.7, rtol=3)  # 1.7 + 0.5 + 0.5\n-\n-        # check weights squeeze\n-        result_t = m([1, 5], sample_weight=[[1], [0.2]])\n-        assert np.isclose(K.eval(result_t), 55.5 / 3.9, rtol=3)\n-        assert np.isclose(K.eval(m.total), 55.5, rtol=3)  # 53.5 + 1 + 1\n-        assert np.isclose(K.eval(m.count), 3.9, rtol=3)  # 2.7 + 1.2\n-\n-        # check weights expand\n-        result_t = m([[1], [5]], sample_weight=[1, 0.2])\n-        assert np.isclose(K.eval(result_t), 57.5 / 5.1, rtol=3)\n-        assert np.isclose(K.eval(m.total), 57.5, rtol=3)  # 55.5 + 1 + 1\n-        assert np.isclose(K.eval(m.count), 5.1, rtol=3)  # 3.9 + 1.2\n-\n-    def test_multiple_instances(self):\n-        m = metrics.Mean()\n-        m2 = metrics.Mean()\n-\n-        assert m.name == 'mean'\n-        assert m2.name == 'mean'\n-\n-        # check initial state\n-        assert K.eval(m.total) == 0\n-        assert K.eval(m.count) == 0\n-        assert K.eval(m2.total) == 0\n-        assert K.eval(m2.count) == 0\n-\n-        # check __call__()\n-        assert K.eval(m(100)) == 100\n-        assert K.eval(m.total) == 100\n-        assert K.eval(m.count) == 1\n-        assert K.eval(m2.total) == 0\n-        assert K.eval(m2.count) == 0\n-\n-        assert K.eval(m2([63, 10])) == 36.5\n-        assert K.eval(m2.total) == 73\n-        assert K.eval(m2.count) == 2\n-        assert K.eval(m.result()) == 100\n-        assert K.eval(m.total) == 100\n-        assert K.eval(m.count) == 1\n-\n-\n-class TestAccuracy(object):\n-\n-    def test_accuracy(self):\n-        acc_obj = metrics.Accuracy(name='my_acc')\n-\n-        # check config\n-        assert acc_obj.name == 'my_acc'\n-        assert acc_obj.stateful\n-        assert len(acc_obj.weights) == 2\n-        assert acc_obj.dtype == 'float32'\n-\n-        # verify that correct value is returned\n-        result = K.eval(acc_obj([[1], [2], [3], [4]], [[1], [2], [3], [4]]))\n-        assert result == 1  # 2/2\n-\n-        # Check save and restore config\n-        a2 = metrics.Accuracy.from_config(acc_obj.get_config())\n-        assert a2.name == 'my_acc'\n-        assert a2.stateful\n-        assert len(a2.weights) == 2\n-        assert a2.dtype, 'float32'\n-\n-        # check with sample_weight\n-        result_t = acc_obj([[2], [1]], [[2], [0]], sample_weight=[[0.5], [0.2]])\n-        result = K.eval(result_t)\n-        assert np.isclose(result, 4.5 / 4.7, atol=1e-3)\n-\n-    def test_binary_accuracy(self):\n-        acc_obj = metrics.BinaryAccuracy(name='my_acc')\n-\n-        # check config\n-        assert acc_obj.name == 'my_acc'\n-        assert acc_obj.stateful\n-        assert len(acc_obj.weights) == 2\n-        assert acc_obj.dtype == 'float32'\n-\n-        # verify that correct value is returned\n-        result_t = acc_obj([[1], [0]], [[1], [0]])\n-        result = K.eval(result_t)\n-        assert result == 1  # 2/2\n-\n-        # check y_pred squeeze\n-        result_t = acc_obj([[1], [1]], [[[1]], [[0]]])\n-        result = K.eval(result_t)\n-        assert np.isclose(result, 3. / 4., atol=1e-3)\n-\n-        # check y_true squeeze\n-        result_t = acc_obj([[[1]], [[1]]], [[1], [0]])\n-        result = K.eval(result_t)\n-        assert np.isclose(result, 4. / 6., atol=1e-3)\n-\n-        # check with sample_weight\n-        result_t = acc_obj([[1], [1]], [[1], [0]], [[0.5], [0.2]])\n-        result = K.eval(result_t)\n-        assert np.isclose(result, 4.5 / 6.7, atol=1e-3)\n-\n-    def test_binary_accuracy_threshold(self):\n-        acc_obj = metrics.BinaryAccuracy(threshold=0.7)\n-        result_t = acc_obj([[1], [1], [0], [0]], [[0.9], [0.6], [0.4], [0.8]])\n-        result = K.eval(result_t)\n-        assert np.isclose(result, 0.5, atol=1e-3)\n-\n-    def test_categorical_accuracy(self):\n-        acc_obj = metrics.CategoricalAccuracy(name='my_acc')\n-\n-        # check config\n-        assert acc_obj.name == 'my_acc'\n-        assert acc_obj.stateful\n-        assert len(acc_obj.weights) == 2\n-        assert acc_obj.dtype == 'float32'\n-\n-        # verify that correct value is returned\n-        result_t = acc_obj([[0, 0, 1], [0, 1, 0]],\n-                           [[0.1, 0.1, 0.8], [0.05, 0.95, 0]])\n-        result = K.eval(result_t)\n-        assert result == 1  # 2/2\n-\n-        # check with sample_weight\n-        result_t = acc_obj([[0, 0, 1], [0, 1, 0]],\n-                           [[0.1, 0.1, 0.8], [0.05, 0, 0.95]],\n-                           [[0.5], [0.2]])\n-        result = K.eval(result_t)\n-        assert np.isclose(result, 2.5 / 2.7, atol=1e-3)  # 2.5/2.7\n-\n-    def test_sparse_categorical_accuracy(self):\n-        acc_obj = metrics.SparseCategoricalAccuracy(name='my_acc')\n-\n-        # check config\n-        assert acc_obj.name == 'my_acc'\n-        assert acc_obj.stateful\n-        assert len(acc_obj.weights) == 2\n-        assert acc_obj.dtype == 'float32'\n-\n-        # verify that correct value is returned\n-        result_t = acc_obj([[2], [1]],\n-                           [[0.1, 0.1, 0.8],\n-                           [0.05, 0.95, 0]])\n-        result = K.eval(result_t)\n-        assert result == 1  # 2/2\n-\n-        # check with sample_weight\n-        result_t = acc_obj([[2], [1]],\n-                           [[0.1, 0.1, 0.8], [0.05, 0, 0.95]],\n-                           [[0.5], [0.2]])\n-        result = K.eval(result_t)\n-        assert np.isclose(result, 2.5 / 2.7, atol=1e-3)\n-\n-    def test_sparse_categorical_accuracy_mismatched_dims(self):\n-        acc_obj = metrics.SparseCategoricalAccuracy(name='my_acc')\n-\n-        # check config\n-        assert acc_obj.name == 'my_acc'\n-        assert acc_obj.stateful\n-        assert len(acc_obj.weights) == 2\n-        assert acc_obj.dtype == 'float32'\n-\n-        # verify that correct value is returned\n-        result_t = acc_obj([2, 1], [[0.1, 0.1, 0.8], [0.05, 0.95, 0]])\n-        result = K.eval(result_t)\n-        assert result == 1  # 2/2\n-\n-        # check with sample_weight\n-        result_t = acc_obj([2, 1], [[0.1, 0.1, 0.8], [0.05, 0, 0.95]],\n-                           [[0.5], [0.2]])\n-        result = K.eval(result_t)\n-        assert np.isclose(result, 2.5 / 2.7, atol=1e-3)\n-\n-\n-class TestMeanSquaredErrorTest(object):\n-\n-    def test_config(self):\n-        mse_obj = metrics.MeanSquaredError(name='my_mse', dtype='int32')\n-        assert mse_obj.name == 'my_mse'\n-        assert mse_obj.dtype == 'int32'\n-\n-        # Check save and restore config\n-        mse_obj2 = metrics.MeanSquaredError.from_config(mse_obj.get_config())\n-        assert mse_obj2.name == 'my_mse'\n-        assert mse_obj2.dtype == 'int32'\n-\n-    def test_unweighted(self):\n-        mse_obj = metrics.MeanSquaredError()\n-        y_true = ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),\n-                  (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n-        y_pred = ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),\n-                  (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n-\n-        result = mse_obj(y_true, y_pred)\n-        np.isclose(0.5, K.eval(result), atol=1e-5)\n-\n-    def test_weighted(self):\n-        mse_obj = metrics.MeanSquaredError()\n-        y_true = ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),\n-                  (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n-        y_pred = ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),\n-                  (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n-        sample_weight = (1., 1.5, 2., 2.5)\n-        result = mse_obj(y_true, y_pred, sample_weight=sample_weight)\n-        np.isclose(0.54285, K.eval(result), atol=1e-5)\n-\n-\n-class TestHinge(object):\n-\n-    def test_config(self):\n-        hinge_obj = metrics.Hinge(name='hinge', dtype='int32')\n-        assert hinge_obj.name == 'hinge'\n-        assert hinge_obj.dtype == 'int32'\n-\n-        # Check save and restore config\n-        hinge_obj2 = metrics.Hinge.from_config(hinge_obj.get_config())\n-        assert hinge_obj2.name == 'hinge'\n-        assert hinge_obj2.dtype == 'int32'\n-\n-    def test_unweighted(self):\n-        hinge_obj = metrics.Hinge()\n-        y_true = K.constant([[0, 1, 0, 1], [0, 0, 1, 1]])\n-        y_pred = K.constant([[-0.3, 0.2, -0.1, 1.6],\n-                             [-0.25, -1., 0.5, 0.6]])\n-\n-        result = hinge_obj(y_true, y_pred)\n-        assert np.allclose(0.506, K.eval(result), atol=1e-3)\n-\n-    def test_weighted(self):\n-        hinge_obj = metrics.Hinge()\n-        y_true = K.constant([[-1, 1, -1, 1], [-1, -1, 1, 1]])\n-        y_pred = K.constant([[-0.3, 0.2, -0.1, 1.6],\n-                             [-0.25, -1., 0.5, 0.6]])\n-        sample_weight = K.constant([1.5, 2.])\n-\n-        result = hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose(0.493, K.eval(result), atol=1e-3)\n-\n-\n-class TestSquaredHinge(object):\n-\n-    def test_config(self):\n-        sq_hinge_obj = metrics.SquaredHinge(name='sq_hinge', dtype='int32')\n-        assert sq_hinge_obj.name == 'sq_hinge'\n-        assert sq_hinge_obj.dtype == 'int32'\n-\n-        # Check save and restore config\n-        sq_hinge_obj2 = metrics.SquaredHinge.from_config(\n-            sq_hinge_obj.get_config())\n-        assert sq_hinge_obj2.name == 'sq_hinge'\n-        assert sq_hinge_obj2.dtype == 'int32'\n-\n-    def test_unweighted(self):\n-        sq_hinge_obj = metrics.SquaredHinge()\n-        y_true = K.constant([[0, 1, 0, 1], [0, 0, 1, 1]])\n-        y_pred = K.constant([[-0.3, 0.2, -0.1, 1.6],\n-                             [-0.25, -1., 0.5, 0.6]])\n-\n-        result = sq_hinge_obj(y_true, y_pred)\n-        assert np.allclose(0.364, K.eval(result), atol=1e-3)\n-\n-    def test_weighted(self):\n-        sq_hinge_obj = metrics.SquaredHinge()\n-        y_true = K.constant([[-1, 1, -1, 1], [-1, -1, 1, 1]])\n-        y_pred = K.constant([[-0.3, 0.2, -0.1, 1.6],\n-                             [-0.25, -1., 0.5, 0.6]])\n-        sample_weight = K.constant([1.5, 2.])\n-\n-        result = sq_hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose(0.347, K.eval(result), atol=1e-3)\n-\n-\n-class TestCategoricalHinge(object):\n-\n-    def test_config(self):\n-        cat_hinge_obj = metrics.CategoricalHinge(\n-            name='cat_hinge', dtype='int32')\n-        assert cat_hinge_obj.name == 'cat_hinge'\n-        assert cat_hinge_obj.dtype == 'int32'\n-\n-        # Check save and restore config\n-        cat_hinge_obj2 = metrics.CategoricalHinge.from_config(\n-            cat_hinge_obj.get_config())\n-        assert cat_hinge_obj2.name == 'cat_hinge'\n-        assert cat_hinge_obj2.dtype == 'int32'\n-\n-    def test_unweighted(self):\n-        cat_hinge_obj = metrics.CategoricalHinge()\n-        y_true = K.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),\n-                             (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))\n-        y_pred = K.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),\n-                             (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))\n-\n-        result = cat_hinge_obj(y_true, y_pred)\n-        assert np.allclose(0.5, K.eval(result), atol=1e-5)\n-\n-    def test_weighted(self):\n-        cat_hinge_obj = metrics.CategoricalHinge()\n-        y_true = K.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),\n-                             (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))\n-        y_pred = K.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),\n-                             (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))\n-        sample_weight = K.constant((1., 1.5, 2., 2.5))\n-        result = cat_hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose(0.5, K.eval(result), atol=1e-5)\n-\n-\n-class TestTopKCategoricalAccuracy(object):\n-\n-    def test_config(self):\n-        a_obj = metrics.TopKCategoricalAccuracy(name='topkca', dtype='int32')\n-        assert a_obj.name == 'topkca'\n-        assert a_obj.dtype == 'int32'\n-\n-        a_obj2 = metrics.TopKCategoricalAccuracy.from_config(a_obj.get_config())\n-        assert a_obj2.name == 'topkca'\n-        assert a_obj2.dtype == 'int32'\n-\n-    def test_correctness(self):\n-        a_obj = metrics.TopKCategoricalAccuracy()\n-        y_true = [[0, 0, 1], [0, 1, 0]]\n-        y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]\n-\n-        result = a_obj(y_true, y_pred)\n-        assert 1 == K.eval(result)  # both the samples match\n-\n-        # With `k` < 5.\n-        a_obj = metrics.TopKCategoricalAccuracy(k=1)\n-        result = a_obj(y_true, y_pred)\n-        assert 0.5 == K.eval(result)  # only sample #2 matches\n-\n-        # With `k` > 5.\n-        y_true = ([[0, 0, 1, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0]])\n-        y_pred = [[0.5, 0.9, 0.1, 0.7, 0.6, 0.5, 0.4],\n-                  [0.05, 0.95, 0, 0, 0, 0, 0]]\n-        a_obj = metrics.TopKCategoricalAccuracy(k=6)\n-        result = a_obj(y_true, y_pred)\n-        assert 0.5 == K.eval(result)  # only 1 sample matches.\n-\n-    def test_weighted(self):\n-        a_obj = metrics.TopKCategoricalAccuracy(k=2)\n-        y_true = [[0, 1, 0], [1, 0, 0], [0, 0, 1]]\n-        y_pred = [[0, 0.9, 0.1], [0, 0.9, 0.1], [0, 0.9, 0.1]]\n-        sample_weight = (1.0, 0.0, 1.0)\n-        result = a_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose(1.0, K.eval(result), atol=1e-5)\n-\n-\n-class TestSparseTopKCategoricalAccuracy(object):\n-\n-    def test_config(self):\n-        a_obj = metrics.SparseTopKCategoricalAccuracy(\n-            name='stopkca', dtype='int32')\n-        assert a_obj.name == 'stopkca'\n-        assert a_obj.dtype == 'int32'\n-\n-        a_obj2 = metrics.SparseTopKCategoricalAccuracy.from_config(\n-            a_obj.get_config())\n-        assert a_obj2.name == 'stopkca'\n-        assert a_obj2.dtype == 'int32'\n-\n-    def test_correctness(self):\n-        a_obj = metrics.SparseTopKCategoricalAccuracy()\n-        y_true = [2, 1]\n-        y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]\n-\n-        result = a_obj(y_true, y_pred)\n-        assert 1 == K.eval(result)  # both the samples match\n-\n-        # With `k` < 5.\n-        a_obj = metrics.SparseTopKCategoricalAccuracy(k=1)\n-        result = a_obj(y_true, y_pred)\n-        assert 0.5 == K.eval(result)  # only sample #2 matches\n-\n-        # With `k` > 5.\n-        y_pred = [[0.5, 0.9, 0.1, 0.7, 0.6, 0.5, 0.4],\n-                  [0.05, 0.95, 0, 0, 0, 0, 0]]\n-        a_obj = metrics.SparseTopKCategoricalAccuracy(k=6)\n-        result = a_obj(y_true, y_pred)\n-        assert 0.5 == K.eval(result)  # only 1 sample matches.\n-\n-    def test_weighted(self):\n-        a_obj = metrics.SparseTopKCategoricalAccuracy(k=2)\n-        y_true = [1, 0, 2]\n-        y_pred = [[0, 0.9, 0.1], [0, 0.9, 0.1], [0, 0.9, 0.1]]\n-        sample_weight = (1.0, 0.0, 1.0)\n-        result = a_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose(1.0, K.eval(result), atol=1e-5)\n-\n-\n-class TestLogCoshError(object):\n-\n-    def setup(self):\n-        self.y_pred = np.asarray([1, 9, 2, -5, -2, 6]).reshape((2, 3))\n-        self.y_true = np.asarray([4, 8, 12, 8, 1, 3]).reshape((2, 3))\n-        self.batch_size = 6\n-        error = self.y_pred - self.y_true\n-        self.expected_results = np.log((np.exp(error) + np.exp(-error)) / 2)\n-\n-    def test_config(self):\n-        logcosh_obj = metrics.LogCoshError(name='logcosh', dtype='int32')\n-        assert logcosh_obj.name == 'logcosh'\n-        assert logcosh_obj.dtype == 'int32'\n-\n-    def test_unweighted(self):\n-        self.setup()\n-        logcosh_obj = metrics.LogCoshError()\n-\n-        result = logcosh_obj(self.y_true, self.y_pred)\n-        expected_result = np.sum(self.expected_results) / self.batch_size\n-        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n-\n-    def test_weighted(self):\n-        self.setup()\n-        logcosh_obj = metrics.LogCoshError()\n-        sample_weight = [[1.2], [3.4]]\n-        result = logcosh_obj(self.y_true, self.y_pred, sample_weight=sample_weight)\n-\n-        sample_weight = np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape((2, 3))\n-        expected_result = np.multiply(self.expected_results, sample_weight)\n-        expected_result = np.sum(expected_result) / np.sum(sample_weight)\n-        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n-\n-\n-class TestPoisson(object):\n-\n-    def setup(self):\n-        self.y_pred = np.asarray([1, 9, 2, 5, 2, 6]).reshape((2, 3))\n-        self.y_true = np.asarray([4, 8, 12, 8, 1, 3]).reshape((2, 3))\n-        self.batch_size = 6\n-        self.expected_results = self.y_pred - np.multiply(\n-            self.y_true, np.log(self.y_pred))\n-\n-    def test_config(self):\n-        poisson_obj = metrics.Poisson(name='poisson', dtype='int32')\n-        assert poisson_obj.name == 'poisson'\n-        assert poisson_obj.dtype == 'int32'\n-\n-        poisson_obj2 = metrics.Poisson.from_config(poisson_obj.get_config())\n-        assert poisson_obj2.name == 'poisson'\n-        assert poisson_obj2.dtype == 'int32'\n-\n-    def test_unweighted(self):\n-        self.setup()\n-        poisson_obj = metrics.Poisson()\n-\n-        result = poisson_obj(self.y_true, self.y_pred)\n-        expected_result = np.sum(self.expected_results) / self.batch_size\n-        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n-\n-    def test_weighted(self):\n-        self.setup()\n-        poisson_obj = metrics.Poisson()\n-        sample_weight = [[1.2], [3.4]]\n-\n-        result = poisson_obj(self.y_true, self.y_pred, sample_weight=sample_weight)\n-        sample_weight = np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape((2, 3))\n-        expected_result = np.multiply(self.expected_results, sample_weight)\n-        expected_result = np.sum(expected_result) / np.sum(sample_weight)\n-        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n-\n-\n-class TestKLDivergence(object):\n-\n-    def setup(self):\n-        self.y_pred = np.asarray([.4, .9, .12, .36, .3, .4]).reshape((2, 3))\n-        self.y_true = np.asarray([.5, .8, .12, .7, .43, .8]).reshape((2, 3))\n-        self.batch_size = 2\n-        self.expected_results = np.multiply(\n-            self.y_true, np.log(self.y_true / self.y_pred))\n-\n-    def test_config(self):\n-        k_obj = metrics.KLDivergence(name='kld', dtype='int32')\n-        assert k_obj.name == 'kld'\n-        assert k_obj.dtype == 'int32'\n-\n-        k_obj2 = metrics.KLDivergence.from_config(k_obj.get_config())\n-        assert k_obj2.name == 'kld'\n-        assert k_obj2.dtype == 'int32'\n-\n-    def test_unweighted(self):\n-        self.setup()\n-        k_obj = metrics.KLDivergence()\n-\n-        result = k_obj(self.y_true, self.y_pred)\n-        expected_result = np.sum(self.expected_results) / self.batch_size\n-        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n-\n-    def test_weighted(self):\n-        self.setup()\n-        k_obj = metrics.KLDivergence()\n-        sample_weight = [[1.2], [3.4]]\n-        result = k_obj(self.y_true, self.y_pred, sample_weight=sample_weight)\n-\n-        sample_weight = np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape((2, 3))\n-        expected_result = np.multiply(self.expected_results, sample_weight)\n-        expected_result = np.sum(expected_result) / (1.2 + 3.4)\n-        assert np.allclose(K.eval(result), expected_result, atol=1e-3)\n-\n-\n-class TestCosineSimilarity(object):\n-\n-    def l2_norm(self, x, axis):\n-        epsilon = 1e-12\n-        square_sum = np.sum(np.square(x), axis=axis, keepdims=True)\n-        x_inv_norm = 1 / np.sqrt(np.maximum(square_sum, epsilon))\n-        return np.multiply(x, x_inv_norm)\n-\n-    def setup(self, axis=1):\n-        self.np_y_true = np.asarray([[1, 9, 2], [-5, -2, 6]], dtype=np.float32)\n-        self.np_y_pred = np.asarray([[4, 8, 12], [8, 1, 3]], dtype=np.float32)\n-\n-        y_true = self.l2_norm(self.np_y_true, axis)\n-        y_pred = self.l2_norm(self.np_y_pred, axis)\n-        self.expected_loss = np.sum(np.multiply(y_true, y_pred), axis=(axis,))\n-\n-        self.y_true = K.constant(self.np_y_true)\n-        self.y_pred = K.constant(self.np_y_pred)\n-\n-    def test_config(self):\n-        cosine_obj = metrics.CosineSimilarity(\n-            axis=2, name='my_cos', dtype='int32')\n-        assert cosine_obj.name == 'my_cos'\n-        assert cosine_obj.dtype == 'int32'\n-\n-        # Check save and restore config\n-        cosine_obj2 = metrics.CosineSimilarity.from_config(cosine_obj.get_config())\n-        assert cosine_obj2.name == 'my_cos'\n-        assert cosine_obj2.dtype == 'int32'\n-\n-    def test_unweighted(self):\n-        self.setup()\n-        cosine_obj = metrics.CosineSimilarity()\n-        loss = cosine_obj(self.y_true, self.y_pred)\n-        expected_loss = np.mean(self.expected_loss)\n-        assert np.allclose(K.eval(loss), expected_loss, 3)\n-\n-    def test_weighted(self):\n-        self.setup()\n-        cosine_obj = metrics.CosineSimilarity()\n-        sample_weight = np.asarray([1.2, 3.4])\n-        loss = cosine_obj(\n-            self.y_true,\n-            self.y_pred,\n-            sample_weight=K.constant(sample_weight))\n-        expected_loss = np.sum(\n-            self.expected_loss * sample_weight) / np.sum(sample_weight)\n-        assert np.allclose(K.eval(loss), expected_loss, 3)\n-\n-    def test_axis(self):\n-        self.setup(axis=1)\n-        cosine_obj = metrics.CosineSimilarity(axis=1)\n-        loss = cosine_obj(self.y_true, self.y_pred)\n-        expected_loss = np.mean(self.expected_loss)\n-        assert np.allclose(K.eval(loss), expected_loss, 3)\n-\n-\n-class TestMeanAbsoluteError(object):\n-\n-    def test_config(self):\n-        mae_obj = metrics.MeanAbsoluteError(name='my_mae', dtype='int32')\n-        assert mae_obj.name == 'my_mae'\n-        assert mae_obj.dtype == 'int32'\n-\n-        # Check save and restore config\n-        mae_obj2 = metrics.MeanAbsoluteError.from_config(mae_obj.get_config())\n-        assert mae_obj2.name == 'my_mae'\n-        assert mae_obj2.dtype == 'int32'\n-\n-    def test_unweighted(self):\n-        mae_obj = metrics.MeanAbsoluteError()\n-        y_true = K.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),\n-                             (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))\n-        y_pred = K.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),\n-                             (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))\n-\n-        result = mae_obj(y_true, y_pred)\n-        assert np.allclose(0.5, K.eval(result), atol=1e-5)\n-\n-    def test_weighted(self):\n-        mae_obj = metrics.MeanAbsoluteError()\n-        y_true = K.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),\n-                             (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))\n-        y_pred = K.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),\n-                             (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))\n-        sample_weight = K.constant((1., 1.5, 2., 2.5))\n-        result = mae_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose(0.54285, K.eval(result), atol=1e-5)\n-\n-\n-class TestMeanAbsolutePercentageError(object):\n-\n-    def test_config(self):\n-        mape_obj = metrics.MeanAbsolutePercentageError(\n-            name='my_mape', dtype='int32')\n-        assert mape_obj.name == 'my_mape'\n-        assert mape_obj.dtype == 'int32'\n-\n-        # Check save and restore config\n-        mape_obj2 = metrics.MeanAbsolutePercentageError.from_config(\n-            mape_obj.get_config())\n-        assert mape_obj2.name == 'my_mape'\n-        assert mape_obj2.dtype == 'int32'\n-\n-    def test_unweighted(self):\n-        mape_obj = metrics.MeanAbsolutePercentageError()\n-        y_true = K.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),\n-                            (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))\n-        y_pred = K.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),\n-                            (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))\n-\n-        result = mape_obj(y_true, y_pred)\n-        assert np.allclose(35e7, K.eval(result), atol=1e-5)\n-\n-    def test_weighted(self):\n-        mape_obj = metrics.MeanAbsolutePercentageError()\n-        y_true = K.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),\n-                            (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))\n-        y_pred = K.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),\n-                            (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))\n-        sample_weight = K.constant((1., 1.5, 2., 2.5))\n-        result = mape_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose(40e7, K.eval(result), atol=1e-5)\n-\n-\n-class TestMeanSquaredError(object):\n-\n-    def test_config(self):\n-        mse_obj = metrics.MeanSquaredError(name='my_mse', dtype='int32')\n-        assert mse_obj.name == 'my_mse'\n-        assert mse_obj.dtype == 'int32'\n-\n-        # Check save and restore config\n-        mse_obj2 = metrics.MeanSquaredError.from_config(mse_obj.get_config())\n-        assert mse_obj2.name == 'my_mse'\n-        assert mse_obj2.dtype == 'int32'\n-\n-    def test_unweighted(self):\n-        mse_obj = metrics.MeanSquaredError()\n-        y_true = K.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),\n-                             (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))\n-        y_pred = K.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),\n-                            (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))\n-\n-        result = mse_obj(y_true, y_pred)\n-        assert np.allclose(0.5, K.eval(result), atol=1e-5)\n-\n-    def test_weighted(self):\n-        mse_obj = metrics.MeanSquaredError()\n-        y_true = K.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),\n-                            (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))\n-        y_pred = K.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),\n-                            (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))\n-        sample_weight = K.constant((1., 1.5, 2., 2.5))\n-        result = mse_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose(0.54285, K.eval(result), atol=1e-5)\n-\n-\n-class TestMeanSquaredLogarithmicError(object):\n-\n-    def test_config(self):\n-        msle_obj = metrics.MeanSquaredLogarithmicError(\n-            name='my_msle', dtype='int32')\n-        assert msle_obj.name == 'my_msle'\n-        assert msle_obj.dtype == 'int32'\n-\n-        # Check save and restore config\n-        msle_obj2 = metrics.MeanSquaredLogarithmicError.from_config(\n-            msle_obj.get_config())\n-        assert msle_obj2.name == 'my_msle'\n-        assert msle_obj2.dtype == 'int32'\n-\n-    def test_unweighted(self):\n-        msle_obj = metrics.MeanSquaredLogarithmicError()\n-        y_true = K.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),\n-                            (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))\n-        y_pred = K.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),\n-                            (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))\n-\n-        result = msle_obj(y_true, y_pred)\n-        assert np.allclose(0.24022, K.eval(result), atol=1e-5)\n-\n-    def test_weighted(self):\n-        msle_obj = metrics.MeanSquaredLogarithmicError()\n-        y_true = K.constant(((0, 1, 0, 1, 0), (0, 0, 1, 1, 1),\n-                            (1, 1, 1, 1, 0), (0, 0, 0, 0, 1)))\n-        y_pred = K.constant(((0, 0, 1, 1, 0), (1, 1, 1, 1, 1),\n-                            (0, 1, 0, 1, 0), (1, 1, 1, 1, 1)))\n-        sample_weight = K.constant((1., 1.5, 2., 2.5))\n-        result = msle_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose(0.26082, K.eval(result), atol=1e-5)\n-\n-\n-class TestRootMeanSquaredError(object):\n-\n-    def test_config(self):\n-        rmse_obj = metrics.RootMeanSquaredError(name='rmse', dtype='int32')\n-        assert rmse_obj.name == 'rmse'\n-        assert rmse_obj.dtype == 'int32'\n-\n-        rmse_obj2 = metrics.RootMeanSquaredError.from_config(rmse_obj.get_config())\n-        assert rmse_obj2.name == 'rmse'\n-        assert rmse_obj2.dtype == 'int32'\n-\n-    def test_unweighted(self):\n-        rmse_obj = metrics.RootMeanSquaredError()\n-        y_true = K.constant((2, 4, 6))\n-        y_pred = K.constant((1, 3, 2))\n-\n-        update_op = rmse_obj(y_true, y_pred)\n-        K.eval(update_op)\n-        result = rmse_obj(y_true, y_pred)\n-        # error = [-1, -1, -4], square(error) = [1, 1, 16], mean = 18/3 = 6\n-        assert np.allclose(math.sqrt(6), K.eval(result), atol=1e-3)\n-\n-    def test_weighted(self):\n-        rmse_obj = metrics.RootMeanSquaredError()\n-        y_true = K.constant((2, 4, 6, 8))\n-        y_pred = K.constant((1, 3, 2, 3))\n-        sample_weight = K.constant((0, 1, 0, 1))\n-        result = rmse_obj(y_true, y_pred, sample_weight=sample_weight)\n-        assert np.allclose(math.sqrt(13), K.eval(result), atol=1e-3)\n-\n-\n-class TestBinaryCrossentropy(object):\n-\n-    def test_config(self):\n-        bce_obj = metrics.BinaryCrossentropy(\n-            name='bce', dtype='int32', label_smoothing=0.2)\n-        assert bce_obj.name == 'bce'\n-        assert bce_obj.dtype == 'int32'\n-\n-        old_config = bce_obj.get_config()\n-        assert np.allclose(old_config['label_smoothing'], 0.2, atol=1e-3)\n-\n-    def test_unweighted(self):\n-        bce_obj = metrics.BinaryCrossentropy()\n-        y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n-        y_pred = np.asarray([1, 1, 1, 0], dtype=np.float32).reshape([2, 2])\n-        result = bce_obj(y_true, y_pred)\n-\n-        assert np.allclose(K.eval(result), 3.833, atol=1e-3)\n-\n-    def test_unweighted_with_logits(self):\n-        bce_obj = metrics.BinaryCrossentropy(from_logits=True)\n-\n-        y_true = [[1, 0, 1], [0, 1, 1]]\n-        y_pred = [[100.0, -100.0, 100.0], [100.0, 100.0, -100.0]]\n-        result = bce_obj(y_true, y_pred)\n-\n-        assert np.allclose(K.eval(result), 33.333, atol=1e-3)\n-\n-    def test_weighted(self):\n-        bce_obj = metrics.BinaryCrossentropy()\n-        y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n-        y_pred = np.asarray([1, 1, 1, 0], dtype=np.float32).reshape([2, 2])\n-        sample_weight = [1.5, 2.]\n-        result = bce_obj(y_true, y_pred, sample_weight=sample_weight)\n-\n-        assert np.allclose(K.eval(result), 3.285, atol=1e-3)\n-\n-    def test_weighted_from_logits(self):\n-        bce_obj = metrics.BinaryCrossentropy(from_logits=True)\n-        y_true = [[1, 0, 1], [0, 1, 1]]\n-        y_pred = [[100.0, -100.0, 100.0], [100.0, 100.0, -100.0]]\n-        sample_weight = [2., 2.5]\n-        result = bce_obj(y_true, y_pred, sample_weight=sample_weight)\n-\n-        assert np.allclose(K.eval(result), 37.037, atol=1e-3)\n-\n-    def test_label_smoothing(self):\n-        logits = ((100., -100., -100.))\n-        y_true = ((1, 0, 1))\n-        label_smoothing = 0.1\n-        bce_obj = metrics.BinaryCrossentropy(\n-            from_logits=True, label_smoothing=label_smoothing)\n-        result = bce_obj(y_true, logits)\n-        expected_value = (100.0 + 50.0 * label_smoothing) / 3.0\n-        assert np.allclose(expected_value, K.eval(result), atol=1e-3)\n-\n-\n-class TestCategoricalCrossentropy(object):\n-\n-    def test_config(self):\n-        cce_obj = metrics.CategoricalCrossentropy(\n-            name='cce', dtype='int32', label_smoothing=0.2)\n-        assert cce_obj.name == 'cce'\n-        assert cce_obj.dtype == 'int32'\n-\n-        old_config = cce_obj.get_config()\n-        assert np.allclose(old_config['label_smoothing'], 0.2, 1e-3)\n-\n-    def test_unweighted(self):\n-        cce_obj = metrics.CategoricalCrossentropy()\n-\n-        y_true = np.asarray([[0, 1, 0], [0, 0, 1]])\n-        y_pred = np.asarray([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n-        result = cce_obj(y_true, y_pred)\n-\n-        assert np.allclose(K.eval(result), 1.176, atol=1e-3)\n-\n-    def test_unweighted_from_logits(self):\n-        cce_obj = metrics.CategoricalCrossentropy(from_logits=True)\n-\n-        y_true = np.asarray([[0, 1, 0], [0, 0, 1]])\n-        logits = np.asarray([[1, 9, 0], [1, 8, 1]], dtype=np.float32)\n-        result = cce_obj(y_true, logits)\n-\n-        assert np.allclose(K.eval(result), 3.5011, atol=1e-3)\n-\n-    def test_weighted(self):\n-        cce_obj = metrics.CategoricalCrossentropy()\n-\n-        y_true = np.asarray([[0, 1, 0], [0, 0, 1]])\n-        y_pred = np.asarray([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n-        sample_weight = [1.5, 2.]\n-        result = cce_obj(y_true, y_pred, sample_weight=sample_weight)\n-\n-        assert np.allclose(K.eval(result), 1.338, atol=1e-3)\n-\n-    def test_weighted_from_logits(self):\n-        cce_obj = metrics.CategoricalCrossentropy(from_logits=True)\n-\n-        y_true = np.asarray([[0, 1, 0], [0, 0, 1]])\n-        logits = np.asarray([[1, 9, 0], [1, 8, 1]], dtype=np.float32)\n-        sample_weight = [1.5, 2.]\n-        result = cce_obj(y_true, logits, sample_weight=sample_weight)\n-\n-        assert np.allclose(K.eval(result), 4.0012, atol=1e-3)\n-\n-    def test_label_smoothing(self):\n-        y_true = np.asarray([[0, 1, 0], [0, 0, 1]])\n-        logits = np.asarray([[1, 9, 0], [1, 8, 1]], dtype=np.float32)\n-        label_smoothing = 0.1\n-\n-        cce_obj = metrics.CategoricalCrossentropy(\n-            from_logits=True, label_smoothing=label_smoothing)\n-        loss = cce_obj(y_true, logits)\n-        assert np.allclose(K.eval(loss), 3.667, atol=1e-3)\n-\n-\n-class TestSparseCategoricalCrossentropy(object):\n-\n-    def test_config(self):\n-        scce_obj = metrics.SparseCategoricalCrossentropy(\n-            name='scce', dtype='int32')\n-        assert scce_obj.name == 'scce'\n-        assert scce_obj.dtype == 'int32'\n-\n-    def test_unweighted(self):\n-        scce_obj = metrics.SparseCategoricalCrossentropy()\n-\n-        y_true = np.asarray([1, 2])\n-        y_pred = np.asarray([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n-        result = scce_obj(y_true, y_pred)\n-\n-        assert np.allclose(K.eval(result), 1.176, atol=1e-3)\n-\n-    def test_unweighted_from_logits(self):\n-        scce_obj = metrics.SparseCategoricalCrossentropy(from_logits=True)\n-\n-        y_true = np.asarray([1, 2])\n-        logits = np.asarray([[1, 9, 0], [1, 8, 1]], dtype=np.float32)\n-        result = scce_obj(y_true, logits)\n-\n-        assert np.allclose(K.eval(result), 3.5011, atol=1e-3)\n-\n-    def test_weighted(self):\n-        scce_obj = metrics.SparseCategoricalCrossentropy()\n-\n-        y_true = np.asarray([1, 2])\n-        y_pred = np.asarray([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n-        sample_weight = [1.5, 2.]\n-        result = scce_obj(y_true, y_pred, sample_weight=sample_weight)\n-\n-        assert np.allclose(K.eval(result), 1.338, atol=1e-3)\n-\n-    def test_weighted_from_logits(self):\n-        scce_obj = metrics.SparseCategoricalCrossentropy(from_logits=True)\n-        y_true = np.asarray([1, 2])\n-        logits = np.asarray([[1, 9, 0], [1, 8, 1]], dtype=np.float32)\n-        sample_weight = [1.5, 2.]\n-        result = scce_obj(y_true, logits, sample_weight=sample_weight)\n-\n-        assert np.allclose(K.eval(result), 4.0012, atol=1e-3)\n-\n-    def test_axis(self):\n-        scce_obj = metrics.SparseCategoricalCrossentropy(axis=0)\n-\n-        y_true = np.asarray([1, 2])\n-        y_pred = np.asarray([[0.05, 0.1], [0.95, 0.8], [0, 0.1]])\n-        result = scce_obj(y_true, y_pred)\n-\n-        assert np.allclose(K.eval(result), 1.176, atol=1e-3)\n\n@@ -1,94 +0,0 @@\n-\"\"\"Tests for metric objects training and evaluation.\"\"\"\n-import pytest\n-import numpy as np\n-\n-from keras import metrics\n-from keras import backend as K\n-from keras.layers import Dense\n-from keras.models import Sequential\n-\n-\n-if K.backend() == 'cntk':\n-    pytestmark = pytest.mark.skip\n-\n-\n-METRICS = [\n-    metrics.Accuracy,\n-    metrics.MeanSquaredError,\n-    metrics.Hinge,\n-    metrics.CategoricalHinge,\n-    metrics.SquaredHinge,\n-    metrics.FalsePositives,\n-    metrics.TruePositives,\n-    metrics.FalseNegatives,\n-    metrics.TrueNegatives,\n-    metrics.BinaryAccuracy,\n-    metrics.CategoricalAccuracy,\n-    metrics.TopKCategoricalAccuracy,\n-    metrics.LogCoshError,\n-    metrics.Poisson,\n-    metrics.KLDivergence,\n-    metrics.CosineSimilarity,\n-    metrics.MeanAbsoluteError,\n-    metrics.MeanAbsolutePercentageError,\n-    metrics.MeanSquaredError,\n-    metrics.MeanSquaredLogarithmicError,\n-    metrics.RootMeanSquaredError,\n-    metrics.BinaryCrossentropy,\n-    metrics.CategoricalCrossentropy,\n-    metrics.Precision,\n-    metrics.Recall,\n-    metrics.AUC,\n-]\n-SPARSE_METRICS = [\n-    metrics.SparseCategoricalAccuracy,\n-    metrics.SparseTopKCategoricalAccuracy,\n-    metrics.SparseCategoricalCrossentropy\n-]\n-\n-\n-@pytest.mark.parametrize('metric_cls', METRICS)\n-def test_training_and_eval(metric_cls):\n-    model = Sequential([Dense(2, activation='sigmoid', input_shape=(3,))])\n-    model.compile('rmsprop', 'mse', metrics=[metric_cls()])\n-    x = np.random.uniform(0, 1, size=(10, 3))\n-    y = np.random.uniform(0, 1, size=(10, 2))\n-    model.fit(x, y)\n-    model.evaluate(x, y)\n-\n-\n-@pytest.mark.parametrize('metric_cls', SPARSE_METRICS)\n-def test_sparse_metrics(metric_cls):\n-    model = Sequential([Dense(1, input_shape=(3,))])\n-    model.compile('rmsprop', 'mse', metrics=[metric_cls()])\n-    x = np.random.uniform(0, 1, size=(10, 3))\n-    y = np.random.uniform(0, 1, size=(10,))\n-    model.fit(x, y)\n-    model.evaluate(x, y)\n-\n-\n-def test_sensitivity_metrics():\n-    metrics_list = [\n-        metrics.SensitivityAtSpecificity(0.5),\n-        metrics.SpecificityAtSensitivity(0.5),\n-    ]\n-    model = Sequential([Dense(2, activation='sigmoid', input_shape=(3,))])\n-    model.compile('rmsprop', 'mse', metrics=metrics_list)\n-    x = np.random.uniform(0, 1, size=(10, 3))\n-    y = np.random.uniform(0, 1, size=(10, 2))\n-    model.fit(x, y)\n-    model.evaluate(x, y)\n-\n-\n-@pytest.mark.skipif(True, reason='It is a flaky test, see #13477 for more context.')\n-def test_mean_iou():\n-    import tensorflow as tf\n-    if not tf.__version__.startswith('2.'):\n-        return\n-\n-    model = Sequential([Dense(1, activation='sigmoid', input_shape=(3,))])\n-    model.compile('rmsprop', 'mse', metrics=[metrics.MeanIoU(2)])\n-    x = np.random.uniform(0, 1, size=(10, 3))\n-    y = np.random.uniform(0, 1, size=(10,))\n-    model.fit(x, y)\n-    model.evaluate(x, y)\n\n@@ -1,181 +0,0 @@\n-from __future__ import print_function\n-import pytest\n-import numpy as np\n-from numpy.testing import assert_allclose\n-\n-from keras.utils import test_utils\n-from keras import optimizers, Input\n-from keras.models import Sequential, Model, load_model\n-from keras.layers.core import Dense, Activation, Lambda\n-from keras.utils.np_utils import to_categorical\n-from keras import backend as K\n-import tempfile\n-\n-\n-num_classes = 2\n-\n-\n-def get_test_data():\n-    np.random.seed(1337)\n-    (x_train, y_train), _ = test_utils.get_test_data(num_train=1000,\n-                                                     num_test=200,\n-                                                     input_shape=(10,),\n-                                                     classification=True,\n-                                                     num_classes=num_classes)\n-    y_train = to_categorical(y_train)\n-    return x_train, y_train\n-\n-\n-def _test_optimizer(optimizer, target=0.6):\n-    x_train, y_train = get_test_data()\n-\n-    model = Sequential()\n-    model.add(Dense(10, input_shape=(x_train.shape[1],)))\n-    model.add(Activation('relu'))\n-    model.add(Dense(y_train.shape[1]))\n-    model.add(Activation('softmax'))\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer=optimizer,\n-                  metrics=['accuracy'])\n-\n-    history = model.fit(x_train, y_train, epochs=3, batch_size=16, verbose=0)\n-    assert history.history['accuracy'][-1] >= target\n-    config = optimizers.serialize(optimizer)\n-    optim = optimizers.deserialize(config)\n-    new_config = optimizers.serialize(optim)\n-    new_config['class_name'] = new_config['class_name'].lower()\n-    assert sorted(config.keys()) == sorted(new_config.keys())\n-    # for k in config['config'].keys():\n-    #     assert config['config'][k] == new_config['config'][k]\n-\n-    # Test constraints.\n-    model = Sequential()\n-    dense = Dense(10,\n-                  input_shape=(x_train.shape[1],),\n-                  kernel_constraint=lambda x: 0. * x + 1.,\n-                  bias_constraint=lambda x: 0. * x + 2.,)\n-    model.add(dense)\n-    model.add(Activation('relu'))\n-    model.add(Dense(y_train.shape[1]))\n-    model.add(Activation('softmax'))\n-    model.compile(loss='categorical_crossentropy',\n-                  optimizer=optimizer,\n-                  metrics=['accuracy'])\n-    model.train_on_batch(x_train[:10], y_train[:10])\n-    kernel, bias = dense.get_weights()\n-    assert_allclose(kernel, 1.)\n-    assert_allclose(bias, 2.)\n-\n-    # Test saving.\n-    model = Sequential()\n-    model.add(Dense(1, input_dim=1))\n-    model.compile(loss='mse', optimizer=optimizer)\n-    model.fit(np.zeros((1, 1)), np.zeros((1, 1)))\n-\n-    _, fname = tempfile.mkstemp('.h5')\n-    model.save(fname)\n-    model2 = load_model(fname)\n-\n-    for w1, w2 in zip(model.get_weights(), model2.get_weights()):\n-        assert_allclose(w1, w2)\n-\n-\n-@pytest.mark.skipif((K.backend() != 'tensorflow'),\n-                    reason='Only Tensorflow raises a '\n-                           'ValueError if the gradient is null.')\n-def test_no_grad():\n-    inp = Input([3])\n-    x = Dense(10)(inp)\n-    x = Lambda(\n-        lambda l: 1.0 * K.reshape(K.cast(K.argmax(l), 'float32'), [-1, 1]),\n-        output_shape=lambda x: [x[0], 1])(x)\n-    mod = Model(inp, x)\n-    mod.compile('sgd', 'mse')\n-    with pytest.raises(ValueError):\n-        mod.fit(np.zeros([10, 3]), np.zeros([10, 1], np.float32),\n-                batch_size=10, epochs=10)\n-\n-\n-@pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason='Flaky with CNTK')\n-def test_sgd():\n-    sgd = optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n-    _test_optimizer(sgd)\n-\n-\n-def test_rmsprop():\n-    _test_optimizer(optimizers.RMSprop())\n-    _test_optimizer(optimizers.RMSprop(decay=1e-3))\n-\n-\n-def test_adagrad():\n-    _test_optimizer(optimizers.Adagrad(lr=1.))\n-    _test_optimizer(optimizers.Adagrad(lr=1., decay=1e-3))\n-\n-\n-def test_adadelta():\n-    _test_optimizer(optimizers.Adadelta(lr=1.), target=0.4)\n-    _test_optimizer(optimizers.Adadelta(lr=1., decay=1e-3), target=0.4)\n-\n-\n-def test_adam():\n-    _test_optimizer(optimizers.Adam())\n-    _test_optimizer(optimizers.Adam(decay=1e-3))\n-\n-\n-def test_adamax():\n-    _test_optimizer(optimizers.Adamax(lr=1.))\n-    _test_optimizer(optimizers.Adamax(lr=1., decay=1e-3))\n-\n-\n-def test_nadam():\n-    _test_optimizer(optimizers.Nadam())\n-\n-\n-def test_adam_amsgrad():\n-    _test_optimizer(optimizers.Adam(amsgrad=True))\n-    _test_optimizer(optimizers.Adam(amsgrad=True, decay=1e-3))\n-\n-\n-@pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason='Flaky with CNTK')\n-def test_clipnorm():\n-    sgd = optimizers.SGD(lr=0.01, momentum=0.9, clipnorm=0.5)\n-    _test_optimizer(sgd)\n-\n-\n-@pytest.mark.skipif((K.backend() == 'cntk'),\n-                    reason='Flaky with CNTK')\n-def test_clipvalue():\n-    sgd = optimizers.SGD(lr=0.01, momentum=0.9, clipvalue=0.5)\n-    _test_optimizer(sgd)\n-\n-\n-@pytest.mark.skipif((K.backend() != 'tensorflow'),\n-                    reason='Requires TensorFlow backend')\n-def test_tfoptimizer():\n-    from keras import constraints\n-    import tensorflow as tf\n-    if tf.__version__.startswith('1.'):\n-        optimizer = optimizers.TFOptimizer(tf.train.AdamOptimizer())\n-    else:\n-        optimizer = tf.keras.optimizers.Adam()\n-\n-    model = Sequential()\n-    model.add(Dense(num_classes, input_shape=(3,),\n-                    kernel_constraint=constraints.MaxNorm(1)))\n-    model.compile(loss='mean_squared_error', optimizer=optimizer)\n-    model.fit(np.random.random((5, 3)), np.random.random((5, num_classes)),\n-              epochs=1, batch_size=5, verbose=0)\n-\n-    if tf.__version__.startswith('1.'):\n-        with pytest.raises(NotImplementedError):\n-            optimizer.weights\n-        with pytest.raises(NotImplementedError):\n-            optimizer.get_config()\n-        with pytest.raises(NotImplementedError):\n-            optimizer.from_config(None)\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,109 +0,0 @@\n-import pytest\n-\n-from keras.models import Sequential, Model\n-from keras.layers import Dense, Input, Average\n-from keras.utils import np_utils\n-from keras.utils import test_utils\n-from keras import regularizers\n-from keras import backend as K\n-\n-data_dim = 5\n-num_classes = 2\n-batch_size = 10\n-\n-\n-def get_data():\n-    (x_train, y_train), _ = test_utils.get_test_data(\n-        num_train=batch_size,\n-        num_test=batch_size,\n-        input_shape=(data_dim,),\n-        classification=True,\n-        num_classes=num_classes)\n-    y_train = np_utils.to_categorical(y_train, num_classes)\n-\n-    return x_train, y_train\n-\n-\n-def create_model(kernel_regularizer=None, activity_regularizer=None):\n-    model = Sequential()\n-    model.add(Dense(num_classes,\n-                    kernel_regularizer=kernel_regularizer,\n-                    activity_regularizer=activity_regularizer,\n-                    input_shape=(data_dim,)))\n-    return model\n-\n-\n-def create_multi_input_model_from(layer1, layer2):\n-    input_1 = Input(shape=(data_dim,))\n-    input_2 = Input(shape=(data_dim,))\n-    out1 = layer1(input_1)\n-    out2 = layer2(input_2)\n-    out = Average()([out1, out2])\n-    model = Model([input_1, input_2], out)\n-    model.add_loss(K.mean(out2))\n-    model.add_loss(lambda: 1)\n-    model.add_loss(lambda: 1)\n-    return model\n-\n-\n-def test_kernel_regularization():\n-    x_train, y_train = get_data()\n-    for reg in [regularizers.l1(),\n-                regularizers.l2(),\n-                regularizers.l1_l2()]:\n-        model = create_model(kernel_regularizer=reg)\n-        model.compile(loss='categorical_crossentropy', optimizer='sgd')\n-        assert len(model.losses) == 1\n-        model.train_on_batch(x_train, y_train)\n-\n-\n-def test_activity_regularization():\n-    x_train, y_train = get_data()\n-    for reg in [regularizers.l1(), regularizers.l2()]:\n-        model = create_model(activity_regularizer=reg)\n-        model.compile(loss='categorical_crossentropy', optimizer='sgd')\n-        assert len(model.losses) == 1\n-        model.train_on_batch(x_train, y_train)\n-\n-\n-def test_regularization_shared_layer():\n-    dense_layer = Dense(num_classes,\n-                        kernel_regularizer=regularizers.l1(),\n-                        activity_regularizer=regularizers.l1())\n-\n-    model = create_multi_input_model_from(dense_layer, dense_layer)\n-    model.compile(loss='categorical_crossentropy', optimizer='sgd')\n-    assert len(model.losses) == 6\n-\n-\n-def test_regularization_shared_model():\n-    dense_layer = Dense(num_classes,\n-                        kernel_regularizer=regularizers.l1(),\n-                        activity_regularizer=regularizers.l1())\n-\n-    input_tensor = Input(shape=(data_dim,))\n-    dummy_model = Model(input_tensor, dense_layer(input_tensor))\n-\n-    model = create_multi_input_model_from(dummy_model, dummy_model)\n-    model.compile(loss='categorical_crossentropy', optimizer='sgd')\n-    # assert len(model.losses) == 6\n-\n-\n-def test_regularization_shared_layer_in_different_models():\n-    shared_dense = Dense(num_classes,\n-                         kernel_regularizer=regularizers.l1(),\n-                         activity_regularizer=regularizers.l1())\n-    models = []\n-    for _ in range(2):\n-        input_tensor = Input(shape=(data_dim,))\n-        unshared_dense = Dense(num_classes, kernel_regularizer=regularizers.l1())\n-        out = unshared_dense(shared_dense(input_tensor))\n-        models.append(Model(input_tensor, out))\n-\n-    model = create_multi_input_model_from(*models)\n-    model.compile(loss='categorical_crossentropy', optimizer='sgd')\n-    # assert len(model.losses) == 8\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,506 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import print_function\n-import pytest\n-import os\n-import numpy as np\n-from numpy.testing import assert_allclose\n-\n-from keras import backend as K\n-import keras\n-from keras.models import Sequential\n-from keras.layers import Dense, Activation\n-from keras.utils import np_utils\n-from keras.utils.test_utils import get_test_data\n-from keras.models import model_from_json, model_from_yaml\n-from keras import losses\n-\n-\n-input_dim = 16\n-num_hidden = 8\n-num_classes = 4\n-batch_size = 32\n-epochs = 1\n-\n-\n-def make_batches(size, batch_size):\n-    \"\"\"Returns a list of batch indices (tuples of indices).\n-    # Arguments\n-        size: Integer, total size of the data to slice into batches.\n-        batch_size: Integer, batch size.\n-    # Returns\n-        A list of tuples of array indices.\n-    \"\"\"\n-    num_batches = (size + batch_size - 1) // batch_size  # round up\n-    return [(i * batch_size, min(size, (i + 1) * batch_size))\n-            for i in range(num_batches)]\n-\n-\n-@pytest.fixture\n-def in_tmpdir(tmpdir):\n-    \"\"\"Runs a function in a temporary directory.\n-\n-    Checks that the directory is empty afterwards.\n-    \"\"\"\n-    with tmpdir.as_cwd():\n-        yield None\n-    assert not tmpdir.listdir()\n-\n-\n-def test_sequential_pop():\n-    model = Sequential()\n-    model.add(Dense(num_hidden, input_dim=input_dim))\n-    model.add(Dense(num_classes))\n-    model.compile(loss='mse', optimizer='sgd')\n-    x = np.random.random((batch_size, input_dim))\n-    y = np.random.random((batch_size, num_classes))\n-    model.fit(x, y, epochs=1)\n-    model.pop()\n-    assert len(model.layers) == 1\n-    assert model.output_shape == (None, num_hidden)\n-    model.compile(loss='mse', optimizer='sgd')\n-    y = np.random.random((batch_size, num_hidden))\n-    model.fit(x, y, epochs=1)\n-\n-\n-def _get_test_data():\n-    np.random.seed(1234)\n-\n-    train_samples = 100\n-    test_samples = 50\n-\n-    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=train_samples,\n-                                                         num_test=test_samples,\n-                                                         input_shape=(input_dim,),\n-                                                         classification=True,\n-                                                         num_classes=num_classes)\n-    y_test = np_utils.to_categorical(y_test)\n-    y_train = np_utils.to_categorical(y_train)\n-    return (x_train, y_train), (x_test, y_test)\n-\n-\n-def test_sequential_fit_generator():\n-    (x_train, y_train), (x_test, y_test) = _get_test_data()\n-\n-    def data_generator(train):\n-        if train:\n-            max_batch_index = len(x_train) // batch_size\n-        else:\n-            max_batch_index = len(x_test) // batch_size\n-        i = 0\n-        while 1:\n-            if train:\n-                yield (x_train[i * batch_size: (i + 1) * batch_size],\n-                       y_train[i * batch_size: (i + 1) * batch_size])\n-            else:\n-                yield (x_test[i * batch_size: (i + 1) * batch_size],\n-                       y_test[i * batch_size: (i + 1) * batch_size])\n-            i += 1\n-            i = i % max_batch_index\n-\n-    model = Sequential()\n-    model.add(Dense(num_hidden, input_shape=(input_dim,)))\n-    model.add(Activation('relu'))\n-    model.add(Dense(num_classes))\n-    model.pop()\n-    model.add(Dense(num_classes))\n-    model.add(Activation('softmax'))\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-    model.fit_generator(data_generator(True), 5, epochs)\n-    model.fit_generator(data_generator(True), 5, epochs,\n-                        validation_data=(x_test, y_test))\n-    model.fit_generator(data_generator(True), 5, epochs,\n-                        validation_data=data_generator(False),\n-                        validation_steps=3)\n-    model.fit_generator(data_generator(True), 5, epochs, max_queue_size=2)\n-    model.evaluate(x_train, y_train)\n-\n-\n-def test_sequential(in_tmpdir):\n-    (x_train, y_train), (x_test, y_test) = _get_test_data()\n-\n-    # TODO: factor out\n-    def data_generator(x, y, batch_size=50):\n-        index_array = np.arange(len(x))\n-        while 1:\n-            batches = make_batches(len(x_test), batch_size)\n-            for batch_index, (batch_start, batch_end) in enumerate(batches):\n-                batch_ids = index_array[batch_start:batch_end]\n-                x_batch = x[batch_ids]\n-                y_batch = y[batch_ids]\n-                yield (x_batch, y_batch)\n-\n-    model = Sequential()\n-    model.add(Dense(num_hidden, input_shape=(input_dim,)))\n-    model.add(Activation('relu'))\n-    model.add(Dense(num_classes))\n-    model.add(Activation('softmax'))\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n-              validation_data=(x_test, y_test))\n-    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2,\n-              validation_split=0.1)\n-    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n-    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n-              shuffle=False)\n-\n-    model.train_on_batch(x_train[:32], y_train[:32])\n-\n-    loss_np = model.evaluate(x_test, y_test)\n-    predict_np = model.predict(x_test)\n-\n-    generator_pred_np = model.predict_generator(\n-        data_generator(x_test, y_test), 1,\n-        max_queue_size=2, verbose=1)\n-    generator_loss_np = model.evaluate_generator(\n-        data_generator(x_test, y_test, 50), 1,\n-        max_queue_size=2)\n-\n-    assert_allclose(loss_np, generator_loss_np, atol=1e-5)\n-    assert_allclose(predict_np, generator_pred_np, atol=1e-5)\n-\n-    model.predict(x_test, verbose=0)\n-    model.predict_classes(x_test, verbose=0)\n-    model.predict_proba(x_test, verbose=0)\n-\n-    fname = 'test_sequential_temp.h5'\n-    model.save_weights(fname, overwrite=True)\n-    model = Sequential()\n-    model.add(Dense(num_hidden, input_shape=(input_dim,)))\n-    model.add(Activation('relu'))\n-    model.add(Dense(num_classes))\n-    model.add(Activation('softmax'))\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-    model.load_weights(fname)\n-    os.remove(fname)\n-\n-    nloss = model.evaluate(x_test, y_test, verbose=0)\n-    assert(loss_np == nloss)\n-\n-    # Test serialization\n-    config = model.get_config()\n-    assert 'name' in config\n-    new_model = Sequential.from_config(config)\n-    assert new_model.weights  # Model should be built.\n-\n-    model.summary()\n-    json_str = model.to_json()\n-    model_from_json(json_str)\n-\n-    yaml_str = model.to_yaml()\n-    model_from_yaml(yaml_str)\n-\n-\n-def test_nested_sequential(in_tmpdir):\n-    (x_train, y_train), (x_test, y_test) = _get_test_data()\n-\n-    inner = Sequential()\n-    inner.add(Dense(num_hidden, input_shape=(input_dim,)))\n-    inner.add(Activation('relu'))\n-    inner.add(Dense(num_classes))\n-\n-    middle = Sequential()\n-    middle.add(inner)\n-\n-    model = Sequential()\n-    model.add(middle)\n-    model.add(Activation('softmax'))\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n-              validation_data=(x_test, y_test))\n-    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=2,\n-              validation_split=0.1)\n-    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=0)\n-    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n-              shuffle=False)\n-\n-    model.train_on_batch(x_train[:32], y_train[:32])\n-\n-    loss = model.evaluate(x_test, y_test, verbose=0)\n-\n-    model.predict(x_test, verbose=0)\n-    model.predict_classes(x_test, verbose=0)\n-    model.predict_proba(x_test, verbose=0)\n-\n-    fname = 'test_nested_sequential_temp.h5'\n-    model.save_weights(fname, overwrite=True)\n-\n-    inner = Sequential()\n-    inner.add(Dense(num_hidden, input_shape=(input_dim,)))\n-    inner.add(Activation('relu'))\n-    inner.add(Dense(num_classes))\n-\n-    middle = Sequential()\n-    middle.add(inner)\n-\n-    model = Sequential()\n-    model.add(middle)\n-    model.add(Activation('softmax'))\n-    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-    model.load_weights(fname)\n-    os.remove(fname)\n-\n-    nloss = model.evaluate(x_test, y_test, verbose=0)\n-    assert(loss == nloss)\n-\n-    # Test serialization\n-    config = model.get_config()\n-    Sequential.from_config(config)\n-\n-    model.summary()\n-    json_str = model.to_json()\n-    model_from_json(json_str)\n-\n-    yaml_str = model.to_yaml()\n-    model_from_yaml(yaml_str)\n-\n-\n-def test_sequential_count_params():\n-    input_dim = 20\n-    num_units = 10\n-    num_classes = 2\n-\n-    n = input_dim * num_units + num_units\n-    n += num_units * num_units + num_units\n-    n += num_units * num_classes + num_classes\n-\n-    model = Sequential()\n-    model.add(Dense(num_units, input_shape=(input_dim,)))\n-    model.add(Dense(num_units))\n-    model.add(Dense(num_classes))\n-    model.add(Activation('softmax'))\n-    model.build()\n-\n-    assert(n == model.count_params())\n-\n-    model.compile('sgd', 'binary_crossentropy')\n-    assert(n == model.count_params())\n-\n-\n-def test_nested_sequential_trainability():\n-    input_dim = 20\n-    num_units = 10\n-    num_classes = 2\n-\n-    inner_model = Sequential()\n-    inner_model.add(Dense(num_units, input_shape=(input_dim,)))\n-\n-    model = Sequential()\n-    model.add(inner_model)\n-    model.add(Dense(num_classes))\n-\n-    assert len(model.trainable_weights) == 4\n-    inner_model.trainable = False\n-    assert len(model.trainable_weights) == 2\n-    inner_model.trainable = True\n-    assert len(model.trainable_weights) == 4\n-\n-\n-def test_rebuild_model():\n-    model = Sequential()\n-    model.add(Dense(128, input_shape=(784,)))\n-    model.add(Dense(64))\n-    assert(model.get_layer(index=-1).output_shape == (None, 64))\n-\n-    model.add(Dense(32))\n-    assert(model.get_layer(index=-1).output_shape == (None, 32))\n-\n-\n-def test_clone_functional_model():\n-    val_a = np.random.random((10, 4))\n-    val_b = np.random.random((10, 4))\n-    val_out = np.random.random((10, 4))\n-\n-    input_a = keras.Input(shape=(4,))\n-    input_b = keras.Input(shape=(4,))\n-    dense_1 = keras.layers.Dense(4)\n-    dense_2 = keras.layers.Dense(4)\n-\n-    x_a = dense_1(input_a)\n-    x_a = keras.layers.Dropout(0.5)(x_a)\n-    x_a = keras.layers.BatchNormalization()(x_a)\n-    x_b = dense_1(input_b)\n-    x_a = dense_2(x_a)\n-    outputs = keras.layers.add([x_a, x_b])\n-    model = keras.models.Model([input_a, input_b], outputs)\n-\n-    if K.backend() == 'tensorflow':\n-        # Everything should work in a new session.\n-        K.clear_session()\n-\n-    # With placeholder creation\n-    new_model = keras.models.clone_model(model)\n-    new_model.compile('rmsprop', 'mse')\n-    new_model.train_on_batch([val_a, val_b], val_out)\n-\n-    # On top of new tensors\n-    input_a = keras.Input(shape=(4,), name='a')\n-    input_b = keras.Input(shape=(4,), name='b')\n-    new_model = keras.models.clone_model(\n-        model, input_tensors=[input_a, input_b])\n-    new_model.compile('rmsprop', 'mse')\n-    new_model.train_on_batch([val_a, val_b], val_out)\n-\n-    # # On top of new, non-Keras tensors\n-    # input_a = keras.backend.variable(val_a)\n-    # input_b = keras.backend.variable(val_b)\n-    # new_model = keras.models.clone_model(\n-    #     model, input_tensors=[input_a, input_b])\n-    # new_model.compile('rmsprop', 'mse')\n-    # new_model.train_on_batch(None, val_out)\n-\n-\n-def test_clone_functional_model_with_multi_outputs():\n-    input_layer = keras.Input(shape=(4,))\n-\n-    # Layer with single input and multiple outputs\n-    layer1 = keras.layers.Lambda(lambda x: [x + 1, x],\n-                                 lambda shapes: [shapes, shapes])\n-    x_a, x_b = layer1(input_layer)\n-\n-    class SwapLayer(keras.layers.Layer):\n-        def call(self, inputs, **kwargs):\n-            return [inputs[1], inputs[0]]\n-\n-        def compute_output_shape(self, input_shape):\n-            return [input_shape[1], input_shape[0]]\n-\n-    # Layer with multiple inputs and outputs\n-    x_a, x_b = SwapLayer()([x_a, x_b])\n-    model = keras.Model(inputs=[input_layer], outputs=[x_a, x_b])\n-    new_model = keras.models.clone_model(model)\n-\n-    x_test = np.random.random((10, 4))\n-    pred_a, pred_b = model.predict(x_test)\n-    pred_new_a, pred_new_b = new_model.predict(x_test)\n-    assert(pred_a.all() == pred_new_a.all())\n-    assert(pred_b.all() == pred_new_b.all())\n-\n-\n-def test_clone_sequential_model():\n-    val_a = np.random.random((10, 4))\n-    val_out = np.random.random((10, 4))\n-\n-    model = keras.models.Sequential()\n-    model.add(keras.layers.Dense(4, input_shape=(4,)))\n-    model.add(keras.layers.BatchNormalization())\n-    model.add(keras.layers.Dropout(0.5))\n-    model.add(keras.layers.Dense(4))\n-\n-    if K.backend() == 'tensorflow':\n-        # Everything should work in a new session.\n-        K.clear_session()\n-\n-    # With placeholder creation\n-    new_model = keras.models.clone_model(model)\n-    new_model.compile('rmsprop', 'mse')\n-    new_model.train_on_batch(val_a, val_out)\n-\n-    # On top of new tensor\n-    input_a = keras.Input(shape=(4,))\n-    new_model = keras.models.clone_model(\n-        model, input_tensors=input_a)\n-    new_model.compile('rmsprop', 'mse')\n-    new_model.train_on_batch(val_a, val_out)\n-\n-    # # On top of new, non-Keras tensor\n-    # input_a = keras.backend.variable(val_a)\n-    # new_model = keras.models.clone_model(\n-    #     model, input_tensors=input_a)\n-    # new_model.compile('rmsprop', 'mse')\n-    # new_model.train_on_batch(None, val_out)\n-\n-\n-def test_sequential_update_disabling():\n-    val_a = np.random.random((10, 4))\n-    val_out = np.random.random((10, 4))\n-\n-    model = keras.models.Sequential()\n-    model.add(keras.layers.BatchNormalization(input_shape=(4,)))\n-\n-    model.trainable = False\n-    assert not model.updates\n-\n-    model.compile('sgd', 'mse')\n-    assert not model.updates\n-\n-    x1 = model.predict(val_a)\n-    model.train_on_batch(val_a, val_out)\n-    x2 = model.predict(val_a)\n-    assert_allclose(x1, x2, atol=1e-7)\n-\n-    model.trainable = True\n-    model.compile('sgd', 'mse')\n-    assert model.updates\n-\n-    model.train_on_batch(val_a, val_out)\n-    x2 = model.predict(val_a)\n-    assert np.abs(np.sum(x1 - x2)) > 1e-5\n-\n-\n-def test_sequential_deferred_build():\n-    model = keras.models.Sequential()\n-    model.add(keras.layers.Dense(3))\n-    model.add(keras.layers.Dense(3))\n-    model.compile('sgd', 'mse')\n-\n-    assert model.built is False\n-    assert len(model.layers) == 2\n-\n-    model.train_on_batch(\n-        np.random.random((2, 4)), np.random.random((2, 3)))\n-\n-    assert model.built is True\n-    assert len(model.layers) == 2\n-    assert len(model.weights) == 4\n-\n-    # Test serialization\n-    config = model.get_config()\n-    assert 'name' in config\n-    new_model = Sequential.from_config(config)\n-    assert new_model.built is True\n-    assert len(new_model.layers) == 2\n-    assert len(new_model.weights) == 4\n-\n-\n-def test_nested_sequential_deferred_build():\n-    inner_model = keras.models.Sequential()\n-    inner_model.add(keras.layers.Dense(3))\n-    inner_model.add(keras.layers.Dense(3))\n-\n-    model = keras.models.Sequential()\n-    model.add(inner_model)\n-    model.add(keras.layers.Dense(5))\n-    model.compile('sgd', 'mse')\n-\n-    assert inner_model.built is False\n-    assert len(inner_model.layers) == 2\n-    assert model.built is False\n-    assert len(model.layers) == 2\n-\n-    model.train_on_batch(\n-        np.random.random((2, 4)), np.random.random((2, 5)))\n-\n-    assert inner_model.built is True\n-    assert len(inner_model.layers) == 2\n-    assert len(inner_model.weights) == 4\n-    assert model.built is True\n-    assert len(model.layers) == 2\n-    assert len(model.weights) == 6\n-\n-    config = model.get_config()\n-    new_model = keras.models.Sequential.from_config(config)\n-    assert new_model.built is True\n-    assert len(new_model.layers) == 2\n-    assert len(new_model.weights) == 6\n-\n-    new_inner_model = new_model.layers[0]\n-    assert new_inner_model.built is True\n-    assert len(new_inner_model.layers) == 2\n-    assert len(new_inner_model.weights) == 4\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,77 +0,0 @@\n-import pytest\n-import numpy as np\n-from keras.utils import conv_utils\n-from keras import backend as K\n-\n-\n-def test_normalize_tuple():\n-    assert conv_utils.normalize_tuple(5, 2, 'kernel_size') == (5, 5)\n-    assert conv_utils.normalize_tuple([7, 9], 2, 'kernel_size') == (7, 9)\n-\n-    with pytest.raises(ValueError):\n-        conv_utils.normalize_tuple(None, 2, 'kernel_size')\n-    with pytest.raises(ValueError):\n-        conv_utils.normalize_tuple([2, 3, 4], 2, 'kernel_size')\n-    with pytest.raises(ValueError):\n-        conv_utils.normalize_tuple(['str', 'impossible'], 2, 'kernel_size')\n-\n-\n-def test_invalid_padding():\n-    with pytest.raises(ValueError):\n-        conv_utils.normalize_padding('diagonal')\n-\n-\n-def test_invalid_convert_kernel():\n-    with pytest.raises(ValueError):\n-        conv_utils.convert_kernel(np.zeros((10, 20)))\n-\n-\n-def test_conv_output_length():\n-    assert conv_utils.conv_output_length(None, 7, 'same', 1) is None\n-    assert conv_utils.conv_output_length(224, 7, 'same', 1) == 224\n-    assert conv_utils.conv_output_length(224, 7, 'same', 2) == 112\n-    assert conv_utils.conv_output_length(32, 5, 'valid', 1) == 28\n-    assert conv_utils.conv_output_length(32, 5, 'valid', 2) == 14\n-    assert conv_utils.conv_output_length(32, 5, 'causal', 1) == 32\n-    assert conv_utils.conv_output_length(32, 5, 'causal', 2) == 16\n-    assert conv_utils.conv_output_length(32, 5, 'full', 1) == 36\n-    assert conv_utils.conv_output_length(32, 5, 'full', 2) == 18\n-\n-    with pytest.raises(AssertionError):\n-        conv_utils.conv_output_length(32, 5, 'diagonal', 2)\n-\n-\n-def test_conv_input_length():\n-    assert conv_utils.conv_input_length(None, 7, 'same', 1) is None\n-    assert conv_utils.conv_input_length(112, 7, 'same', 1) == 112\n-    assert conv_utils.conv_input_length(112, 7, 'same', 2) == 223\n-    assert conv_utils.conv_input_length(28, 5, 'valid', 1) == 32\n-    assert conv_utils.conv_input_length(14, 5, 'valid', 2) == 31\n-    assert conv_utils.conv_input_length(36, 5, 'full', 1) == 32\n-    assert conv_utils.conv_input_length(18, 5, 'full', 2) == 31\n-\n-    with pytest.raises(AssertionError):\n-        conv_utils.conv_output_length(18, 5, 'diagonal', 2)\n-\n-\n-def test_deconv_length():\n-    assert conv_utils.deconv_length(None, 1, 7, 'same', None) is None\n-    assert conv_utils.deconv_length(224, 1, 7, 'same', None) == 224\n-    assert conv_utils.deconv_length(224, 2, 7, 'same', None) == 448\n-    assert conv_utils.deconv_length(32, 1, 5, 'valid', None) == 36\n-    assert conv_utils.deconv_length(32, 2, 5, 'valid', None) == 67\n-    assert conv_utils.deconv_length(32, 1, 5, 'full', None) == 28\n-    assert conv_utils.deconv_length(32, 2, 5, 'full', None) == 59\n-    assert conv_utils.deconv_length(224, 1, 7, 'same', 0) == 224\n-    assert conv_utils.deconv_length(224, 2, 7, 'same', 0) == 447\n-    assert conv_utils.deconv_length(224, 2, 7, 'same', 1) == 448\n-    assert conv_utils.deconv_length(32, 1, 5, 'valid', 0) == 36\n-    assert conv_utils.deconv_length(32, 2, 5, 'valid', 0) == 67\n-    assert conv_utils.deconv_length(32, 2, 5, 'valid', 1) == 68\n-    assert conv_utils.deconv_length(6, 1, 3, 'full', 0) == 4\n-    assert conv_utils.deconv_length(6, 2, 3, 'full', 1) == 10\n-    assert conv_utils.deconv_length(6, 2, 3, 'full', 2) == 11\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,542 +0,0 @@\n-\"\"\"Tests for functions in data_utils.py.\n-\"\"\"\n-import os\n-import time\n-import sys\n-import tarfile\n-import threading\n-import signal\n-import shutil\n-import zipfile\n-from itertools import cycle\n-import multiprocessing as mp\n-import numpy as np\n-import pytest\n-import six\n-from six.moves.urllib.parse import urljoin\n-from six.moves.urllib.request import pathname2url\n-from six.moves import reload_module\n-\n-from flaky import flaky\n-\n-from keras.utils import GeneratorEnqueuer\n-from keras.utils import OrderedEnqueuer\n-from keras.utils import Sequence\n-from tensorflow.python.keras.utils.data_utils import _hash_file\n-from keras.utils.data_utils import get_file\n-from tensorflow.python.keras.utils.data_utils import validate_file\n-from keras import backend as K\n-\n-pytestmark = pytest.mark.skipif(\n-    six.PY2 and 'TRAVIS_PYTHON_VERSION' in os.environ,\n-    reason='Temporarily disabled until the use_multiprocessing problem is solved')\n-\n-skip_generators = pytest.mark.skipif(K.backend() in {'tensorflow', 'cntk'} and\n-                                     'TRAVIS_PYTHON_VERSION' in os.environ,\n-                                     reason='Generators do not work with `spawn`.')\n-\n-\n-def use_spawn(func):\n-    \"\"\"Decorator which uses `spawn` when possible.\n-    This is useful on Travis to avoid memory issues.\n-    \"\"\"\n-\n-    @six.wraps(func)\n-    def wrapper(*args, **kwargs):\n-        if sys.version_info > (3, 4) and os.name != 'nt':\n-            mp.set_start_method('spawn', force=True)\n-            out = func(*args, **kwargs)\n-            mp.set_start_method('fork', force=True)\n-        else:\n-            out = func(*args, **kwargs)\n-        return out\n-\n-    return wrapper\n-\n-\n-if sys.version_info < (3,):\n-    def next(x):\n-        return x.next()\n-\n-\n-@pytest.fixture\n-def in_tmpdir(tmpdir):\n-    \"\"\"Runs a function in a temporary directory.\n-\n-    Checks that the directory is empty afterwards.\n-    \"\"\"\n-    with tmpdir.as_cwd():\n-        yield None\n-    assert not tmpdir.listdir()\n-\n-\n-def test_data_utils(in_tmpdir):\n-    \"\"\"Tests get_file from a url, plus extraction and validation.\n-    \"\"\"\n-    dirname = 'data_utils'\n-\n-    with open('test.txt', 'w') as text_file:\n-        text_file.write('Float like a butterfly, sting like a bee.')\n-\n-    with tarfile.open('test.tar.gz', 'w:gz') as tar_file:\n-        tar_file.add('test.txt')\n-\n-    with zipfile.ZipFile('test.zip', 'w') as zip_file:\n-        zip_file.write('test.txt')\n-\n-    origin = urljoin('file://', pathname2url(os.path.abspath('test.tar.gz')))\n-\n-    path = get_file(dirname, origin, untar=True)\n-    filepath = path + '.tar.gz'\n-    data_keras_home = os.path.dirname(os.path.dirname(os.path.abspath(filepath)))\n-    os.remove(filepath)\n-\n-    _keras_home = os.path.join(os.path.abspath('.'), '.keras')\n-    if not os.path.exists(_keras_home):\n-        os.makedirs(_keras_home)\n-    os.environ['KERAS_HOME'] = _keras_home\n-    path = get_file(dirname, origin, untar=True)\n-    filepath = path + '.tar.gz'\n-    data_keras_home = os.path.dirname(os.path.dirname(os.path.abspath(filepath)))\n-    os.environ.pop('KERAS_HOME')\n-    shutil.rmtree(_keras_home)\n-\n-    path = get_file(dirname, origin, untar=True)\n-    filepath = path + '.tar.gz'\n-    hashval_sha256 = _hash_file(filepath)\n-    hashval_md5 = _hash_file(filepath, algorithm='md5')\n-    path = get_file(dirname, origin, md5_hash=hashval_md5, untar=True)\n-    path = get_file(filepath, origin, file_hash=hashval_sha256, extract=True)\n-    assert os.path.exists(filepath)\n-    assert validate_file(filepath, hashval_sha256)\n-    assert validate_file(filepath, hashval_md5)\n-    os.remove(filepath)\n-    os.remove('test.tar.gz')\n-\n-    origin = urljoin('file://', pathname2url(os.path.abspath('test.zip')))\n-\n-    hashval_sha256 = _hash_file('test.zip')\n-    hashval_md5 = _hash_file('test.zip', algorithm='md5')\n-    path = get_file(dirname, origin, md5_hash=hashval_md5, extract=True)\n-    path = get_file(dirname, origin, file_hash=hashval_sha256, extract=True)\n-    assert os.path.exists(path)\n-    assert validate_file(path, hashval_sha256)\n-    assert validate_file(path, hashval_md5)\n-\n-    os.remove(path)\n-    os.remove(os.path.join(os.path.dirname(path), 'test.txt'))\n-    os.remove('test.txt')\n-    os.remove('test.zip')\n-\n-\n-\"\"\"Enqueuers Tests\"\"\"\n-\n-\n-class threadsafe_iter:\n-    \"\"\"Takes an iterator/generator and makes it thread-safe by\n-    serializing call to the `next` method of given iterator/generator.\n-    \"\"\"\n-\n-    def __init__(self, it):\n-        self.it = it\n-        self.lock = threading.Lock()\n-\n-    def __iter__(self):\n-        return self\n-\n-    def __next__(self):\n-        return self.next()\n-\n-    def next(self):\n-        with self.lock:\n-            return next(self.it)\n-\n-\n-def threadsafe_generator(f):\n-    \"\"\"A decorator that takes a generator function and makes it thread-safe.\n-    \"\"\"\n-\n-    def g(*a, **kw):\n-        return threadsafe_iter(f(*a, **kw))\n-\n-    return g\n-\n-\n-class DummySequence(Sequence):\n-    def __init__(self, shape, value=1.0):\n-        self.shape = shape\n-        self.inner = value\n-\n-    def __getitem__(self, item):\n-        time.sleep(0.05)\n-        return np.ones(self.shape, dtype=np.uint32) * item * self.inner\n-\n-    def __len__(self):\n-        return 100\n-\n-    def on_epoch_end(self):\n-        self.inner *= 5.0\n-\n-\n-class LengthChangingSequence(Sequence):\n-    def __init__(self, shape, size=100, value=1.0):\n-        self.shape = shape\n-        self.inner = value\n-        self.size = size\n-\n-    def __getitem__(self, item):\n-        time.sleep(0.05)\n-        return np.ones(self.shape, dtype=np.uint32) * item * self.inner\n-\n-    def __len__(self):\n-        return self.size\n-\n-    def on_epoch_end(self):\n-        self.size = int(np.ceil(self.size / 2))\n-        self.inner *= 5.0\n-\n-\n-class FaultSequence(Sequence):\n-    def __getitem__(self, item):\n-        raise IndexError(item, 'is not present')\n-\n-    def __len__(self):\n-        return 100\n-\n-    def on_epoch_end(self):\n-        pass\n-\n-\n-class SlowSequence(Sequence):\n-    def __init__(self, shape, value=1.0):\n-        self.shape = shape\n-        self.inner = value\n-        self.wait = True\n-\n-    def __getitem__(self, item):\n-        if self.wait:\n-            self.wait = False\n-            time.sleep(40)\n-        return np.ones(self.shape, dtype=np.uint32) * item * self.inner\n-\n-    def __len__(self):\n-        return 10\n-\n-    def on_epoch_end(self):\n-        pass\n-\n-\n-@threadsafe_generator\n-def create_generator_from_sequence_threads(ds):\n-    for i in cycle(range(len(ds))):\n-        yield ds[i]\n-\n-\n-def create_generator_from_sequence_pcs(ds):\n-    for i in cycle(range(len(ds))):\n-        yield ds[i]\n-\n-\n-def test_generator_enqueuer_threads():\n-    enqueuer = GeneratorEnqueuer(create_generator_from_sequence_threads(\n-        DummySequence([3, 10, 10, 3])), use_multiprocessing=False)\n-    enqueuer.start(3, 10)\n-    gen_output = enqueuer.get()\n-    acc = []\n-    for i in range(100):\n-        acc.append(int(next(gen_output)[0, 0, 0, 0]))\n-\n-    \"\"\"\n-     Not comparing the order since it is not guaranteed.\n-     It may get ordered, but not a lot, one thread can take\n-     the GIL before he was supposed to.\n-    \"\"\"\n-    assert len(set(acc) - set(range(100))) == 0, \"Output is not the same\"\n-    enqueuer.stop()\n-\n-\n-@skip_generators\n-def DISABLED_test_generator_enqueuer_processes():\n-    enqueuer = GeneratorEnqueuer(create_generator_from_sequence_pcs(\n-        DummySequence([3, 10, 10, 3])), use_multiprocessing=True)\n-    enqueuer.start(3, 10)\n-    gen_output = enqueuer.get()\n-    acc = []\n-    for i in range(100):\n-        acc.append(int(next(gen_output)[0, 0, 0, 0]))\n-    assert acc != list(range(100)), ('Order was keep in GeneratorEnqueuer '\n-                                     'with processes')\n-    enqueuer.stop()\n-\n-\n-def test_generator_enqueuer_threadsafe():\n-    enqueuer = GeneratorEnqueuer(create_generator_from_sequence_pcs(\n-        DummySequence([3, 10, 10, 3])), use_multiprocessing=False)\n-    enqueuer.start(3, 10)\n-    gen_output = enqueuer.get()\n-    with pytest.raises(RuntimeError) as e:\n-        [next(gen_output) for _ in range(10)]\n-    assert 'thread-safe' in str(e.value)\n-    enqueuer.stop()\n-\n-\n-# TODO: resolve flakyness issue. Tracked with #11587\n-@flaky(rerun_filter=lambda err, *args: issubclass(err[0], StopIteration))\n-def test_generator_enqueuer_fail_threads():\n-    enqueuer = GeneratorEnqueuer(create_generator_from_sequence_threads(\n-        FaultSequence()), use_multiprocessing=False)\n-    enqueuer.start(3, 10)\n-    gen_output = enqueuer.get()\n-    with pytest.raises(IndexError):\n-        next(gen_output)\n-\n-\n-@skip_generators\n-def DISABLED_test_generator_enqueuer_fail_processes():\n-    enqueuer = GeneratorEnqueuer(create_generator_from_sequence_pcs(\n-        FaultSequence()), use_multiprocessing=True)\n-    enqueuer.start(3, 10)\n-    gen_output = enqueuer.get()\n-    with pytest.raises(IndexError):\n-        next(gen_output)\n-\n-\n-def test_ordered_enqueuer_threads():\n-    enqueuer = OrderedEnqueuer(DummySequence([3, 10, 10, 3]),\n-                               use_multiprocessing=False)\n-    enqueuer.start(3, 10)\n-    gen_output = enqueuer.get()\n-    acc = []\n-    for i in range(100):\n-        acc.append(next(gen_output)[0, 0, 0, 0])\n-    assert acc == list(range(100)), ('Order was not keep in GeneratorEnqueuer '\n-                                     'with threads')\n-    enqueuer.stop()\n-\n-\n-def test_ordered_enqueuer_threads_not_ordered():\n-    enqueuer = OrderedEnqueuer(DummySequence([3, 10, 10, 3]),\n-                               use_multiprocessing=False,\n-                               shuffle=True)\n-    enqueuer.start(3, 10)\n-    gen_output = enqueuer.get()\n-    acc = []\n-    for i in range(100):\n-        acc.append(next(gen_output)[0, 0, 0, 0])\n-    assert acc != list(range(100)), ('Order was not keep in GeneratorEnqueuer '\n-                                     'with threads')\n-    enqueuer.stop()\n-\n-\n-@use_spawn\n-def test_ordered_enqueuer_processes():\n-    enqueuer = OrderedEnqueuer(DummySequence([3, 10, 10, 3]),\n-                               use_multiprocessing=True)\n-    enqueuer.start(3, 10)\n-    gen_output = enqueuer.get()\n-    acc = []\n-    for i in range(100):\n-        acc.append(next(gen_output)[0, 0, 0, 0])\n-    assert acc == list(range(100)), ('Order was not keep in GeneratorEnqueuer '\n-                                     'with processes')\n-    enqueuer.stop()\n-\n-\n-def test_ordered_enqueuer_fail_threads():\n-    enqueuer = OrderedEnqueuer(FaultSequence(), use_multiprocessing=False)\n-    enqueuer.start(3, 10)\n-    gen_output = enqueuer.get()\n-    with pytest.raises(IndexError):\n-        next(gen_output)\n-\n-\n-def DISABLED_test_ordered_enqueuer_timeout_threads():\n-    enqueuer = OrderedEnqueuer(SlowSequence([3, 10, 10, 3]),\n-                               use_multiprocessing=False)\n-\n-    def handler(signum, frame):\n-        raise TimeoutError('Sequence deadlocked')\n-\n-    old = signal.signal(signal.SIGALRM, handler)\n-    signal.setitimer(signal.ITIMER_REAL, 60)\n-    with pytest.warns(UserWarning) as record:\n-        enqueuer.start(5, 10)\n-        gen_output = enqueuer.get()\n-        for epoch_num in range(2):\n-            acc = []\n-            for i in range(10):\n-                acc.append(next(gen_output)[0, 0, 0, 0])\n-            assert acc == list(range(10)), 'Order was not keep in ' \\\n-                                           'OrderedEnqueuer with threads'\n-        enqueuer.stop()\n-    assert len(record) == 1\n-    assert str(record[0].message) == 'The input 0 could not be retrieved. ' \\\n-                                     'It could be because a worker has died.'\n-    signal.setitimer(signal.ITIMER_REAL, 0)\n-    signal.signal(signal.SIGALRM, old)\n-\n-\n-@use_spawn\n-def test_on_epoch_end_processes():\n-    enqueuer = OrderedEnqueuer(DummySequence([3, 10, 10, 3]),\n-                               use_multiprocessing=True)\n-    enqueuer.start(3, 10)\n-    gen_output = enqueuer.get()\n-    acc = []\n-    for i in range(200):\n-        acc.append(next(gen_output)[0, 0, 0, 0])\n-    assert acc[100:] == list([k * 5 for k in range(100)]), (\n-        'Order was not keep in GeneratorEnqueuer with processes')\n-    enqueuer.stop()\n-\n-\n-@use_spawn\n-def test_context_switch():\n-    enqueuer = OrderedEnqueuer(DummySequence([3, 10, 10, 3]),\n-                               use_multiprocessing=True)\n-    enqueuer2 = OrderedEnqueuer(DummySequence([3, 10, 10, 3], value=15),\n-                                use_multiprocessing=True)\n-    enqueuer.start(3, 10)\n-    enqueuer2.start(3, 10)\n-    gen_output = enqueuer.get()\n-    gen_output2 = enqueuer2.get()\n-    acc = []\n-    for i in range(100):\n-        acc.append(next(gen_output)[0, 0, 0, 0])\n-    assert acc[-1] == 99\n-    # One epoch is completed so enqueuer will switch the Sequence\n-\n-    acc = []\n-    for i in range(100):\n-        acc.append(next(gen_output2)[0, 0, 0, 0])\n-    assert acc[-1] == 99 * 15\n-    # One epoch has been completed so enqueuer2 will switch\n-\n-    # Be sure that both Sequence were updated\n-    assert next(gen_output)[0, 0, 0, 0] == 0\n-    assert next(gen_output)[0, 0, 0, 0] == 5\n-    assert next(gen_output2)[0, 0, 0, 0] == 0\n-    assert next(gen_output2)[0, 0, 0, 0] == 15 * 5\n-\n-    # Tear down everything\n-    enqueuer.stop()\n-    enqueuer2.stop()\n-\n-\n-def DISABLED_test_on_epoch_end_threads():\n-    enqueuer = OrderedEnqueuer(DummySequence([3, 10, 10, 3]),\n-                               use_multiprocessing=False)\n-    enqueuer.start(3, 10)\n-    gen_output = enqueuer.get()\n-    acc = []\n-    for i in range(100):\n-        acc.append(next(gen_output)[0, 0, 0, 0])\n-    acc = []\n-    for i in range(100):\n-        acc.append(next(gen_output)[0, 0, 0, 0])\n-    assert acc == list([k * 5 for k in range(100)]), (\n-        'Order was not keep in GeneratorEnqueuer with processes')\n-    enqueuer.stop()\n-\n-\n-def DISABLED_test_on_epoch_end_threads_sequence_change_length():\n-    seq = LengthChangingSequence([3, 10, 10, 3])\n-    enqueuer = OrderedEnqueuer(seq,\n-                               use_multiprocessing=False)\n-    enqueuer.start(3, 10)\n-    gen_output = enqueuer.get()\n-    acc = []\n-    for i in range(100):\n-        acc.append(next(gen_output)[0, 0, 0, 0])\n-    assert acc == list(range(100)), ('Order was not keep in GeneratorEnqueuer '\n-                                     'with threads')\n-\n-    enqueuer.join_end_of_epoch()\n-    assert len(seq) == 50\n-    acc = []\n-    for i in range(50):\n-        acc.append(next(gen_output)[0, 0, 0, 0])\n-    assert acc == list([k * 5 for k in range(50)]), (\n-        'Order was not keep in GeneratorEnqueuer with processes')\n-    enqueuer.stop()\n-\n-\n-@use_spawn\n-def test_ordered_enqueuer_fail_processes():\n-    enqueuer = OrderedEnqueuer(FaultSequence(), use_multiprocessing=True)\n-    enqueuer.start(3, 10)\n-    gen_output = enqueuer.get()\n-    with pytest.raises(IndexError):\n-        next(gen_output)\n-\n-\n-@threadsafe_generator\n-def create_finite_generator_from_sequence_threads(ds):\n-    for i in range(len(ds)):\n-        yield ds[i]\n-\n-\n-def create_finite_generator_from_sequence_pcs(ds):\n-    for i in range(len(ds)):\n-        yield ds[i]\n-\n-\n-# TODO: resolve flakyness issue. Tracked with #11586\n-@flaky(rerun_filter=lambda err, *args: issubclass(err[0], AssertionError))\n-def test_finite_generator_enqueuer_threads():\n-    enqueuer = GeneratorEnqueuer(create_finite_generator_from_sequence_threads(\n-        DummySequence([3, 10, 10, 3])), use_multiprocessing=False)\n-    enqueuer.start(3, 10)\n-    gen_output = enqueuer.get()\n-    acc = []\n-    for output in gen_output:\n-        acc.append(int(output[0, 0, 0, 0]))\n-    assert set(acc) == set(range(100)), \"Output is not the same\"\n-    enqueuer.stop()\n-\n-\n-@skip_generators\n-def DISABLED_test_finite_generator_enqueuer_processes():\n-    enqueuer = GeneratorEnqueuer(create_finite_generator_from_sequence_pcs(\n-        DummySequence([3, 10, 10, 3])), use_multiprocessing=True)\n-    enqueuer.start(3, 10)\n-    gen_output = enqueuer.get()\n-    acc = []\n-    for output in gen_output:\n-        acc.append(int(output[0, 0, 0, 0]))\n-    assert acc != list(range(100)), ('Order was keep in GeneratorEnqueuer '\n-                                     'with processes')\n-    enqueuer.stop()\n-\n-\n-@pytest.mark.skipif('TRAVIS_PYTHON_VERSION' in os.environ,\n-                    reason='Takes 150s to run')\n-def DISABLED_test_missing_inputs():\n-    missing_idx = 10\n-\n-    class TimeOutSequence(DummySequence):\n-        def __getitem__(self, item):\n-            if item == missing_idx:\n-                time.sleep(120)\n-            return super(TimeOutSequence, self).__getitem__(item)\n-\n-    enqueuer = GeneratorEnqueuer(create_finite_generator_from_sequence_pcs(\n-        TimeOutSequence([3, 2, 2, 3])), use_multiprocessing=True)\n-    enqueuer.start(3, 10)\n-    gen_output = enqueuer.get()\n-    with pytest.warns(UserWarning, match='An input could not be retrieved.'):\n-        for _ in range(4 * missing_idx):\n-            next(gen_output)\n-\n-    enqueuer = OrderedEnqueuer(TimeOutSequence([3, 2, 2, 3]),\n-                               use_multiprocessing=True)\n-    enqueuer.start(3, 10)\n-    gen_output = enqueuer.get()\n-    warning_msg = \"The input {} could not be retrieved.\".format(missing_idx)\n-    with pytest.warns(UserWarning, match=warning_msg):\n-        for _ in range(11):\n-            next(gen_output)\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,83 +0,0 @@\n-import sys\n-import pytest\n-import numpy as np\n-import marshal\n-from keras.utils.generic_utils import custom_object_scope\n-from keras.utils.generic_utils import has_arg\n-from keras.utils.generic_utils import Progbar\n-from keras import activations\n-from keras import regularizers\n-\n-\n-def test_progbar():\n-    values_s = [None,\n-                [['key1', 1], ['key2', 1e-4]],\n-                [['key3', 1], ['key2', 1e-4]]]\n-\n-    for target in (len(values_s) - 1, None):\n-        for verbose in (0, 1, 2):\n-            bar = Progbar(target, width=30, verbose=verbose, interval=0.05)\n-            for current, values in enumerate(values_s):\n-                bar.update(current, values=values)\n-\n-\n-def test_custom_objects_scope():\n-\n-    def custom_fn():\n-        pass\n-\n-    class CustomClass(object):\n-        pass\n-\n-    with custom_object_scope({'CustomClass': CustomClass,\n-                              'custom_fn': custom_fn}):\n-        act = activations.get('custom_fn')\n-        assert act == custom_fn\n-        cl = regularizers.get('CustomClass')\n-        assert cl.__class__ == CustomClass\n-\n-\n-@pytest.mark.parametrize('fn, name, accept_all, expected', [\n-    ('f(x)', 'x', False, True),\n-    ('f(x)', 'y', False, False),\n-    ('f(x)', 'y', True, False),\n-    ('f(x, y)', 'y', False, True),\n-    ('f(x, y=1)', 'y', False, True),\n-    ('f(x, **kwargs)', 'x', False, True),\n-    ('f(x, **kwargs)', 'y', False, False),\n-    ('f(x, **kwargs)', 'y', True, True),\n-    ('f(x, y=1, **kwargs)', 'y', False, True),\n-    # Keyword-only arguments (Python 3 only)\n-    ('f(x, *args, y=1)', 'y', False, True),\n-    ('f(x, *args, y=1)', 'z', True, False),\n-    ('f(x, *, y=1)', 'x', False, True),\n-    ('f(x, *, y=1)', 'y', False, True),\n-    # lambda\n-    (lambda x: x, 'x', False, True),\n-    (lambda x: x, 'y', False, False),\n-    (lambda x: x, 'y', True, False),\n-])\n-def test_has_arg(fn, name, accept_all, expected):\n-    if isinstance(fn, str):\n-        context = dict()\n-        try:\n-            exec('def {}: pass'.format(fn), context)\n-        except SyntaxError:\n-            if sys.version_info >= (3,):\n-                raise\n-            pytest.skip('Function is not compatible with Python 2')\n-        # Sometimes exec adds builtins to the context\n-        context.pop('__builtins__', None)\n-        fn, = context.values()\n-\n-    assert has_arg(fn, name, accept_all) is expected\n-\n-\n-@pytest.mark.xfail(sys.version_info < (3, 3),\n-                   reason='inspect API does not reveal positional-only arguments')\n-def test_has_arg_positional_only():\n-    assert has_arg(pow, 'x') is False\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,64 +0,0 @@\n-import pytest\n-import numpy as np\n-from numpy.testing import assert_allclose\n-from keras import backend as K\n-from keras.layers import Conv2D\n-from keras.layers import Dense\n-from keras.layers import Flatten\n-from keras.models import Sequential\n-from keras.utils import layer_utils\n-\n-\n-def DISABLED_test_convert_weights():\n-    def get_model(shape, data_format):\n-        model = Sequential()\n-        model.add(Conv2D(filters=2,\n-                         kernel_size=(4, 3),\n-                         input_shape=shape,\n-                         data_format=data_format))\n-        model.add(Flatten())\n-        model.add(Dense(5))\n-        return model\n-\n-    for data_format in ['channels_first', 'channels_last']:\n-        if data_format == 'channels_first':\n-            shape = (3, 5, 5)\n-            target_shape = (5, 5, 3)\n-            prev_shape = (2, 3, 2)\n-            flip = lambda x: np.flip(np.flip(x, axis=2), axis=3)\n-            transpose = lambda x: np.transpose(x, (0, 2, 3, 1))\n-            target_data_format = 'channels_last'\n-        elif data_format == 'channels_last':\n-            shape = (5, 5, 3)\n-            target_shape = (3, 5, 5)\n-            prev_shape = (2, 2, 3)\n-            flip = lambda x: np.flip(np.flip(x, axis=1), axis=2)\n-            transpose = lambda x: np.transpose(x, (0, 3, 1, 2))\n-            target_data_format = 'channels_first'\n-\n-        model1 = get_model(shape, data_format)\n-        model2 = get_model(target_shape, target_data_format)\n-        conv = K.function([model1.input], [model1.layers[0].output])\n-\n-        x = np.random.random((1,) + shape)\n-\n-        # Test equivalence of convert_all_kernels_in_model\n-        convout1 = conv([x])[0]\n-        layer_utils.convert_all_kernels_in_model(model1)\n-        convout2 = flip(conv([flip(x)])[0])\n-\n-        assert_allclose(convout1, convout2, atol=1e-5)\n-\n-        # Test equivalence of convert_dense_weights_data_format\n-        out1 = model1.predict(x)\n-        layer_utils.convert_dense_weights_data_format(\n-            model1.layers[2], prev_shape, target_data_format)\n-        for (src, dst) in zip(model1.layers, model2.layers):\n-            dst.set_weights(src.get_weights())\n-        out2 = model2.predict(transpose(x))\n-\n-        assert_allclose(out1, out2, atol=1e-5)\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,33 +0,0 @@\n-\"\"\"Tests for functions in np_utils.py.\n-\"\"\"\n-import numpy as np\n-import pytest\n-from keras.utils import to_categorical\n-\n-\n-def test_to_categorical():\n-    num_classes = 5\n-    shapes = [(1,), (3,), (4, 3), (5, 4, 3), (3, 1), (3, 2, 1)]\n-    expected_shapes = [(1, num_classes),\n-                       (3, num_classes),\n-                       (4, 3, num_classes),\n-                       (5, 4, 3, num_classes),\n-                       (3, num_classes),\n-                       (3, 2, num_classes)]\n-    labels = [np.random.randint(0, num_classes, shape) for shape in shapes]\n-    one_hots = [to_categorical(label, num_classes) for label in labels]\n-    for label, one_hot, expected_shape in zip(labels,\n-                                              one_hots,\n-                                              expected_shapes):\n-        # Check shape\n-        assert one_hot.shape == expected_shape\n-        # Make sure there are only 0s and 1s\n-        assert np.array_equal(one_hot, one_hot.astype(bool))\n-        # Make sure there is exactly one 1 in a row\n-        assert np.all(one_hot.sum(axis=-1) == 1)\n-        # Get original labels back from one hots\n-        assert np.all(np.argmax(one_hot, -1).reshape(label.shape) == label)\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,56 +0,0 @@\n-import pytest\n-import os\n-import sys\n-import numpy as np\n-from keras import Input, Model\n-\n-from keras.layers import Conv2D, Bidirectional\n-from keras.layers import Dense\n-from keras.layers import Embedding\n-from keras.layers import Flatten\n-from keras.layers import LSTM\n-from keras.layers import TimeDistributed\n-from keras.models import Sequential\n-from keras.utils import vis_utils\n-\n-\n-def test_plot_model():\n-    model = Sequential()\n-    model.add(Conv2D(2, kernel_size=(2, 3), input_shape=(3, 5, 5), name='conv'))\n-    model.add(Flatten(name='flat'))\n-    model.add(Dense(5, name='dense1'))\n-    vis_utils.plot_model(model, to_file='model1.png', show_layer_names=False)\n-    os.remove('model1.png')\n-\n-    model = Sequential()\n-    model.add(LSTM(16, return_sequences=True, input_shape=(2, 3), name='lstm'))\n-    model.add(TimeDistributed(Dense(5, name='dense2')))\n-    vis_utils.plot_model(model, to_file='model2.png', show_shapes=True)\n-    os.remove('model2.png')\n-\n-    inner_input = Input(shape=(2, 3), dtype='float32', name='inner_input')\n-    inner_lstm = Bidirectional(LSTM(16, name='inner_lstm'), name='bd')(inner_input)\n-    encoder = Model(inner_input, inner_lstm, name='Encoder_Model')\n-    outer_input = Input(shape=(5, 2, 3), dtype='float32', name='input')\n-    inner_encoder = TimeDistributed(encoder, name='td_encoder')(outer_input)\n-    lstm = LSTM(16, name='outer_lstm')(inner_encoder)\n-    preds = Dense(5, activation='softmax', name='predictions')(lstm)\n-    model = Model(outer_input, preds)\n-    vis_utils.plot_model(model, to_file='model3.png', show_shapes=True,\n-                         expand_nested=True, dpi=300)\n-    os.remove('model3.png')\n-\n-\n-def test_plot_sequential_embedding():\n-    \"\"\"Fixes #11376\"\"\"\n-    model = Sequential()\n-    model.add(Embedding(10000, 256, input_length=400, name='embed'))\n-    vis_utils.plot_model(model,\n-                         to_file='model1.png',\n-                         show_shapes=True,\n-                         show_layer_names=True)\n-    os.remove('model1.png')\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,204 +0,0 @@\n-import pytest\n-import numpy as np\n-\n-from keras.utils.test_utils import get_test_data\n-\n-from keras.models import Sequential\n-from keras.layers.core import Dense, Activation\n-from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n-\n-input_dim = 5\n-hidden_dims = 5\n-num_train = 100\n-num_test = 50\n-num_classes = 3\n-batch_size = 32\n-epochs = 1\n-verbosity = 0\n-optim = 'adam'\n-loss = 'categorical_crossentropy'\n-\n-np.random.seed(42)\n-(X_train, y_train), (X_test, y_test) = get_test_data(\n-    num_train=num_train, num_test=num_test, input_shape=(input_dim,),\n-    classification=True, num_classes=num_classes)\n-\n-\n-def build_fn_clf(hidden_dims):\n-    model = Sequential()\n-    model.add(Dense(input_dim, input_shape=(input_dim,)))\n-    model.add(Activation('relu'))\n-    model.add(Dense(hidden_dims))\n-    model.add(Activation('relu'))\n-    model.add(Dense(num_classes))\n-    model.add(Activation('softmax'))\n-    model.compile(optimizer='sgd', loss='categorical_crossentropy',\n-                  metrics=['accuracy'])\n-    return model\n-\n-\n-def test_classify_build_fn():\n-    clf = KerasClassifier(\n-        build_fn=build_fn_clf, hidden_dims=hidden_dims,\n-        batch_size=batch_size, epochs=epochs)\n-\n-    assert_classification_works(clf)\n-    assert_string_classification_works(clf)\n-\n-\n-def test_classify_class_build_fn():\n-    class ClassBuildFnClf(object):\n-\n-        def __call__(self, hidden_dims):\n-            return build_fn_clf(hidden_dims)\n-\n-    clf = KerasClassifier(\n-        build_fn=ClassBuildFnClf(), hidden_dims=hidden_dims,\n-        batch_size=batch_size, epochs=epochs)\n-\n-    assert_classification_works(clf)\n-    assert_string_classification_works(clf)\n-\n-\n-def test_classify_inherit_class_build_fn():\n-    class InheritClassBuildFnClf(KerasClassifier):\n-\n-        def __call__(self, hidden_dims):\n-            return build_fn_clf(hidden_dims)\n-\n-    clf = InheritClassBuildFnClf(\n-        build_fn=None, hidden_dims=hidden_dims,\n-        batch_size=batch_size, epochs=epochs)\n-\n-    assert_classification_works(clf)\n-    assert_string_classification_works(clf)\n-\n-\n-def assert_classification_works(clf):\n-    clf.fit(X_train, y_train, sample_weight=np.ones(X_train.shape[0]),\n-            batch_size=batch_size, epochs=epochs)\n-\n-    score = clf.score(X_train, y_train, batch_size=batch_size)\n-    assert np.isscalar(score) and np.isfinite(score)\n-\n-    preds = clf.predict(X_test, batch_size=batch_size)\n-    assert preds.shape == (num_test, )\n-    for prediction in np.unique(preds):\n-        assert prediction in range(num_classes)\n-\n-    proba = clf.predict_proba(X_test, batch_size=batch_size)\n-    assert proba.shape == (num_test, num_classes)\n-    assert np.allclose(np.sum(proba, axis=1), np.ones(num_test))\n-\n-\n-def assert_string_classification_works(clf):\n-    string_classes = ['cls{}'.format(x) for x in range(num_classes)]\n-    str_y_train = np.array(string_classes)[y_train]\n-\n-    clf.fit(X_train, str_y_train, batch_size=batch_size, epochs=epochs)\n-\n-    score = clf.score(X_train, str_y_train, batch_size=batch_size)\n-    assert np.isscalar(score) and np.isfinite(score)\n-\n-    preds = clf.predict(X_test, batch_size=batch_size)\n-    assert preds.shape == (num_test, )\n-    for prediction in np.unique(preds):\n-        assert prediction in string_classes\n-\n-    proba = clf.predict_proba(X_test, batch_size=batch_size)\n-    assert proba.shape == (num_test, num_classes)\n-    assert np.allclose(np.sum(proba, axis=1), np.ones(num_test))\n-\n-\n-def build_fn_reg(hidden_dims=50):\n-    model = Sequential()\n-    model.add(Dense(input_dim, input_shape=(input_dim,)))\n-    model.add(Activation('relu'))\n-    model.add(Dense(hidden_dims))\n-    model.add(Activation('relu'))\n-    model.add(Dense(1))\n-    model.add(Activation('linear'))\n-    model.compile(optimizer='sgd', loss='mean_absolute_error',\n-                  metrics=['accuracy'])\n-    return model\n-\n-\n-def test_regression_build_fn():\n-    reg = KerasRegressor(\n-        build_fn=build_fn_reg, hidden_dims=hidden_dims,\n-        batch_size=batch_size, epochs=epochs)\n-\n-    assert_regression_works(reg)\n-\n-\n-def test_regression_class_build_fn():\n-    class ClassBuildFnReg(object):\n-\n-        def __call__(self, hidden_dims):\n-            return build_fn_reg(hidden_dims)\n-\n-    reg = KerasRegressor(\n-        build_fn=ClassBuildFnReg(), hidden_dims=hidden_dims,\n-        batch_size=batch_size, epochs=epochs)\n-\n-    assert_regression_works(reg)\n-\n-\n-def test_regression_inherit_class_build_fn():\n-    class InheritClassBuildFnReg(KerasRegressor):\n-\n-        def __call__(self, hidden_dims):\n-            return build_fn_reg(hidden_dims)\n-\n-    reg = InheritClassBuildFnReg(\n-        build_fn=None, hidden_dims=hidden_dims,\n-        batch_size=batch_size, epochs=epochs)\n-\n-    assert_regression_works(reg)\n-\n-\n-def assert_regression_works(reg):\n-    reg.fit(X_train, y_train, batch_size=batch_size, epochs=epochs)\n-\n-    score = reg.score(X_train, y_train, batch_size=batch_size)\n-    assert np.isscalar(score) and np.isfinite(score)\n-\n-    preds = reg.predict(X_test, batch_size=batch_size)\n-    assert preds.shape == (num_test, )\n-\n-\n-def DISABLED_test_regression_predict_shape_correct_num_test_0():\n-    assert_regression_predict_shape_correct(num_test=0)\n-\n-\n-def DISABLED_test_regression_predict_shape_correct_num_test_1():\n-    assert_regression_predict_shape_correct(num_test=1)\n-\n-\n-def assert_regression_predict_shape_correct(num_test):\n-    reg = KerasRegressor(\n-        build_fn=build_fn_reg, hidden_dims=hidden_dims,\n-        batch_size=batch_size, epochs=epochs)\n-    reg.fit(X_train, y_train, batch_size=batch_size, epochs=epochs)\n-\n-    preds = reg.predict(X_test[:num_test], batch_size=batch_size)\n-    assert preds.shape == (num_test, )\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n-\n-# Usage of sklearn's grid_search\n-# from sklearn import grid_search\n-# parameters = dict(hidden_dims = [20, 30], batch_size=[64, 128],\n-#                   epochs=[2], verbose=[0])\n-# classifier = Inherit_class_build_fn_clf()\n-# clf = grid_search.GridSearchCV(classifier, parameters)\n-# clf.fit(X_train, y_train)\n-# parameters = dict(hidden_dims = [20, 30], batch_size=[64, 128],\n-#                   epochs=[2], verbose=[0])\n-# regressor = Inherit_class_build_fn_reg()\n-# reg = grid_search.GridSearchCV(regressor, parameters,\n-#                                scoring='mean_squared_error',\n-#                                n_jobs=1, cv=2, verbose=2)\n-# reg.fit(X_train_reg, y_train_reg)\n\n@@ -1,32 +0,0 @@\n-import pytest\n-import pyux\n-import keras\n-import json\n-import os\n-\n-import keras.backend.tensorflow_backend\n-import keras.backend.theano_backend\n-import keras.backend.cntk_backend\n-import keras.backend.numpy_backend\n-import keras.utils.test_utils\n-\n-\n-def test_api():\n-    api_file = os.path.join(os.getcwd(), 'api.json')\n-    with open(api_file, 'r') as f:\n-        previous_api = json.load(f)\n-    current_api = pyux.sign(keras)\n-    diff = pyux.diff(current_api, previous_api)\n-\n-    exceptions = [\n-        pyux.ADDED_ARG_WITH_DEFAULT_IN_METHOD,\n-        pyux.ADDED_DEFAULT_IN_METHOD\n-    ]\n-\n-    diff = list(filter(lambda c: c[0] not in exceptions, diff))\n-    if diff:\n-        raise pyux.APIChangedException(diff)\n-\n-\n-if __name__ == '__main__':\n-    test_api()\n\n@@ -1,110 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import print_function\n-import pytest\n-\n-from keras.models import Model, Sequential\n-from keras.layers import Dense, Input\n-\n-\n-def test_layer_trainability_switch():\n-    # with constructor argument, in Sequential\n-    model = Sequential()\n-    model.add(Dense(2, trainable=False, input_dim=1))\n-    assert model.trainable_weights == []\n-\n-    # by setting the `trainable` argument, in Sequential\n-    model = Sequential()\n-    layer = Dense(2, input_dim=1)\n-    model.add(layer)\n-    assert model.trainable_weights == layer.trainable_weights\n-    layer.trainable = False\n-    assert model.trainable_weights == []\n-\n-    # with constructor argument, in Model\n-    x = Input(shape=(1,))\n-    y = Dense(2, trainable=False)(x)\n-    model = Model(x, y)\n-    assert model.trainable_weights == []\n-\n-    # by setting the `trainable` argument, in Model\n-    x = Input(shape=(1,))\n-    layer = Dense(2)\n-    y = layer(x)\n-    model = Model(x, y)\n-    assert model.trainable_weights == layer.trainable_weights\n-    layer.trainable = False\n-    assert model.trainable_weights == []\n-\n-\n-def test_model_trainability_switch():\n-    # a non-trainable model has no trainable weights\n-    x = Input(shape=(1,))\n-    y = Dense(2)(x)\n-    model = Model(x, y)\n-    model.trainable = False\n-    assert model.trainable_weights == []\n-\n-    # same for Sequential\n-    model = Sequential()\n-    model.add(Dense(2, input_dim=1))\n-    model.trainable = False\n-    assert model.trainable_weights == []\n-\n-\n-def test_nested_model_trainability():\n-    # a Sequential inside a Model\n-    inner_model = Sequential()\n-    inner_model.add(Dense(2, input_dim=1))\n-\n-    x = Input(shape=(1,))\n-    y = inner_model(x)\n-    outer_model = Model(x, y)\n-    assert outer_model.trainable_weights == inner_model.trainable_weights\n-    inner_model.trainable = False\n-    assert outer_model.trainable_weights == []\n-    inner_model.trainable = True\n-    inner_model.layers[-1].trainable = False\n-    assert outer_model.trainable_weights == []\n-\n-    # a Sequential inside a Sequential\n-    inner_model = Sequential()\n-    inner_model.add(Dense(2, input_dim=1))\n-    outer_model = Sequential()\n-    outer_model.add(inner_model)\n-    assert outer_model.trainable_weights == inner_model.trainable_weights\n-    inner_model.trainable = False\n-    assert outer_model.trainable_weights == []\n-    inner_model.trainable = True\n-    inner_model.layers[-1].trainable = False\n-    assert outer_model.trainable_weights == []\n-\n-    # a Model inside a Model\n-    x = Input(shape=(1,))\n-    y = Dense(2)(x)\n-    inner_model = Model(x, y)\n-    x = Input(shape=(1,))\n-    y = inner_model(x)\n-    outer_model = Model(x, y)\n-    assert outer_model.trainable_weights == inner_model.trainable_weights\n-    inner_model.trainable = False\n-    assert outer_model.trainable_weights == []\n-    inner_model.trainable = True\n-    inner_model.layers[-1].trainable = False\n-    assert outer_model.trainable_weights == []\n-\n-    # a Model inside a Sequential\n-    x = Input(shape=(1,))\n-    y = Dense(2)(x)\n-    inner_model = Model(x, y)\n-    outer_model = Sequential()\n-    outer_model.add(inner_model)\n-    assert outer_model.trainable_weights == inner_model.trainable_weights\n-    inner_model.trainable = False\n-    assert outer_model.trainable_weights == []\n-    inner_model.trainable = True\n-    inner_model.layers[-1].trainable = False\n-    assert outer_model.trainable_weights == []\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,37 +0,0 @@\n-import numpy as np\n-import pytest\n-\n-from keras.models import Sequential\n-from keras.layers import TimeDistributed, Masking, Dense\n-from keras import losses\n-from keras import backend as K\n-\n-\n-def create_masking_model():\n-    model = Sequential()\n-    model.add(Masking(mask_value=0, input_shape=(None, 1)))\n-    model.add(TimeDistributed(Dense(1, kernel_initializer='one')))\n-    model.compile(loss='mse', optimizer='sgd')\n-    return model\n-\n-\n-def test_masking():\n-    np.random.seed(1337)\n-    x = np.array([[[1], [1]],\n-                  [[0], [0]]])\n-    model = create_masking_model()\n-    y = np.array([[[1], [1]],\n-                  [[1], [1]]])\n-    loss = model.train_on_batch(x, y)\n-    assert loss == 0\n-\n-\n-def test_masking_is_all_zeros():\n-    x = y = np.array([[[0], [0]]])\n-    model = create_masking_model()\n-    loss = model.train_on_batch(x, y)\n-    assert loss == 0\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,174 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import print_function\n-import pytest\n-import numpy as np\n-\n-from keras import backend as K\n-from keras.utils.test_utils import get_test_data\n-from keras.models import Sequential, Model\n-from keras.layers import Dense, Activation, GRU, TimeDistributed, Input\n-from keras.utils import np_utils\n-from numpy.testing import assert_almost_equal, assert_array_almost_equal\n-\n-num_classes = 10\n-batch_size = 128\n-epochs = 15\n-weighted_class = 5\n-high_weight = 10\n-train_samples = 5000\n-test_samples = 1000\n-timesteps = 3\n-input_dim = 10\n-loss = 'mse'\n-standard_weight = 1\n-standard_score_sequential = 0.5\n-\n-decimal_precision = {\n-    'cntk': 2,\n-    'theano': 6,\n-    'tensorflow': 6\n-}\n-\n-\n-def _get_test_data():\n-    np.random.seed(1337)\n-    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=train_samples,\n-                                                         num_test=test_samples,\n-                                                         input_shape=(input_dim,),\n-                                                         classification=True,\n-                                                         num_classes=num_classes)\n-    int_y_test = y_test.copy()\n-    int_y_train = y_train.copy()\n-    # convert class vectors to binary class matrices\n-    y_train = np_utils.to_categorical(y_train, num_classes)\n-    y_test = np_utils.to_categorical(y_test, num_classes)\n-    test_ids = np.where(int_y_test == np.array(weighted_class))[0]\n-\n-    class_weight = dict([(i, standard_weight) for i in range(num_classes)])\n-    class_weight[weighted_class] = high_weight\n-\n-    sample_weight = np.ones((y_train.shape[0])) * standard_weight\n-    sample_weight[int_y_train == weighted_class] = high_weight\n-\n-    return ((x_train, y_train), (x_test, y_test),\n-            (sample_weight, class_weight, test_ids))\n-\n-\n-def create_sequential_model():\n-    model = Sequential()\n-    model.add(Dense(32, input_shape=(input_dim,)))\n-    model.add(Activation('relu'))\n-    model.add(Dense(num_classes))\n-    model.add(Activation('softmax'))\n-    return model\n-\n-\n-def create_temporal_sequential_model():\n-    model = Sequential()\n-    model.add(GRU(32, input_shape=(timesteps, input_dim), return_sequences=True))\n-    model.add(TimeDistributed(Dense(num_classes)))\n-    model.add(Activation('softmax'))\n-    return model\n-\n-\n-def test_sequential_class_weights():\n-    model = create_sequential_model()\n-    model.compile(loss=loss, optimizer='rmsprop')\n-\n-    ((x_train, y_train), (x_test, y_test),\n-     (sample_weight, class_weight, test_ids)) = _get_test_data()\n-\n-    model.fit(x_train, y_train, batch_size=batch_size,\n-              epochs=epochs // 3, verbose=0,\n-              class_weight=class_weight,\n-              validation_data=(x_train, y_train, sample_weight))\n-    model.fit(x_train, y_train, batch_size=batch_size,\n-              epochs=epochs // 2, verbose=0,\n-              class_weight=class_weight)\n-    model.fit(x_train, y_train, batch_size=batch_size,\n-              epochs=epochs // 2, verbose=0,\n-              class_weight=class_weight,\n-              validation_split=0.1)\n-\n-    model.train_on_batch(x_train[:32], y_train[:32],\n-                         class_weight=class_weight)\n-    score = model.evaluate(x_test[test_ids, :], y_test[test_ids, :], verbose=0)\n-    assert(score < standard_score_sequential)\n-\n-\n-def test_sequential_sample_weights():\n-    model = create_sequential_model()\n-    model.compile(loss=loss, optimizer='rmsprop')\n-\n-    ((x_train, y_train), (x_test, y_test),\n-     (sample_weight, class_weight, test_ids)) = _get_test_data()\n-\n-    model.fit(x_train, y_train, batch_size=batch_size,\n-              epochs=epochs // 3, verbose=0,\n-              sample_weight=sample_weight)\n-    model.fit(x_train, y_train, batch_size=batch_size,\n-              epochs=epochs // 3, verbose=0,\n-              sample_weight=sample_weight,\n-              validation_split=0.1)\n-\n-    model.train_on_batch(x_train[:32], y_train[:32],\n-                         sample_weight=sample_weight[:32])\n-    model.test_on_batch(x_train[:32], y_train[:32],\n-                        sample_weight=sample_weight[:32])\n-    score = model.evaluate(x_test[test_ids, :], y_test[test_ids, :], verbose=0)\n-    assert(score < standard_score_sequential)\n-\n-\n-def test_sequential_temporal_sample_weights():\n-    ((x_train, y_train), (x_test, y_test),\n-     (sample_weight, class_weight, test_ids)) = _get_test_data()\n-\n-    temporal_x_train = np.reshape(x_train, (len(x_train), 1, x_train.shape[1]))\n-    temporal_x_train = np.repeat(temporal_x_train, timesteps, axis=1)\n-    temporal_x_test = np.reshape(x_test, (len(x_test), 1, x_test.shape[1]))\n-    temporal_x_test = np.repeat(temporal_x_test, timesteps, axis=1)\n-\n-    temporal_y_train = np.reshape(y_train, (len(y_train), 1, y_train.shape[1]))\n-    temporal_y_train = np.repeat(temporal_y_train, timesteps, axis=1)\n-    temporal_y_test = np.reshape(y_test, (len(y_test), 1, y_test.shape[1]))\n-    temporal_y_test = np.repeat(temporal_y_test, timesteps, axis=1)\n-\n-    temporal_sample_weight = np.reshape(sample_weight, (len(sample_weight), 1))\n-    temporal_sample_weight = np.repeat(temporal_sample_weight, timesteps, axis=1)\n-\n-    model = create_temporal_sequential_model()\n-    model.compile(loss=loss, optimizer='rmsprop',\n-                  sample_weight_mode='temporal')\n-\n-    model.fit(temporal_x_train, temporal_y_train, batch_size=batch_size,\n-              epochs=epochs // 3, verbose=0,\n-              sample_weight=temporal_sample_weight)\n-    model.fit(temporal_x_train, temporal_y_train, batch_size=batch_size,\n-              epochs=epochs // 3, verbose=0,\n-              sample_weight=temporal_sample_weight,\n-              validation_split=0.1)\n-\n-    model.train_on_batch(temporal_x_train[:32], temporal_y_train[:32],\n-                         sample_weight=temporal_sample_weight[:32])\n-    model.test_on_batch(temporal_x_train[:32], temporal_y_train[:32],\n-                        sample_weight=temporal_sample_weight[:32])\n-    score = model.evaluate(temporal_x_test[test_ids], temporal_y_test[test_ids],\n-                           verbose=0)\n-    assert(score < standard_score_sequential)\n-\n-\n-def test_class_weight_wrong_classes():\n-    model = create_sequential_model()\n-    model.compile(loss=loss, optimizer='rmsprop')\n-\n-    ((x_train, y_train), (x_test, y_test),\n-     (sample_weight, class_weight, test_ids)) = _get_test_data()\n-\n-    del class_weight[1]\n-    with pytest.raises(ValueError):\n-        model.fit(x_train, y_train,\n-                  epochs=0, verbose=0, class_weight=class_weight)\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,675 +0,0 @@\n-import io\n-import pytest\n-import os\n-import h5py\n-import tempfile\n-import warnings\n-from contextlib import contextmanager\n-import numpy as np\n-from numpy.testing import assert_allclose\n-from numpy.testing import assert_raises\n-\n-from keras import backend as K\n-from keras.models import Model, Sequential\n-from keras.layers import Dense, Lambda, RepeatVector, TimeDistributed\n-from keras.layers import Bidirectional, GRU, LSTM\n-from keras.layers import Conv2D, Flatten, Activation\n-from keras.layers import Input, InputLayer\n-from keras.initializers import Constant\n-from keras import optimizers\n-from keras import losses\n-from keras import metrics\n-from keras.models import save_model, load_model\n-try:\n-    from unittest.mock import patch\n-except:\n-    from mock import patch\n-\n-\n-skipif_no_tf_gpu = True\n-\n-\n-def test_sequential_model_saving():\n-    model = Sequential()\n-    model.add(Dense(2, input_shape=(3,)))\n-    model.add(RepeatVector(3))\n-    model.add(TimeDistributed(Dense(3)))\n-    model.compile(loss=losses.MeanSquaredError(),\n-                  optimizer=optimizers.RMSprop(lr=0.0001),\n-                  metrics=[metrics.categorical_accuracy],\n-                  sample_weight_mode='temporal')\n-    x = np.random.random((1, 3))\n-    y = np.random.random((1, 3, 3))\n-    model.train_on_batch(x, y)\n-\n-    out = model.predict(x)\n-\n-    _, fname = tempfile.mkstemp('.h5')\n-    save_model(model, fname)\n-    new_model = load_model(fname)\n-    os.remove(fname)\n-\n-    x2 = np.random.random((1, 3))\n-    y2 = np.random.random((1, 3, 3))\n-    model.train_on_batch(x2, y2)\n-    out_2 = model.predict(x2)\n-\n-    new_out = new_model.predict(x)\n-    assert_allclose(out, new_out, atol=1e-05)\n-    # test that new updates are the same with both models\n-    new_model.train_on_batch(x2, y2)\n-    new_out_2 = new_model.predict(x2)\n-    assert_allclose(out_2, new_out_2, atol=1e-05)\n-\n-\n-def test_sequential_model_saving_2():\n-    # test with custom optimizer, loss\n-    custom_opt = optimizers.RMSprop\n-    custom_loss = losses.mse\n-    model = Sequential()\n-    model.add(Dense(2, input_shape=(3,)))\n-    model.add(Dense(3))\n-    model.compile(loss=custom_loss, optimizer=custom_opt(), metrics=['acc'])\n-\n-    x = np.random.random((1, 3))\n-    y = np.random.random((1, 3))\n-    model.train_on_batch(x, y)\n-    out = model.predict(x)\n-\n-    load_kwargs = {'custom_objects': {'custom_opt': custom_opt,\n-                                      'custom_loss': custom_loss}}\n-    _, fname = tempfile.mkstemp('.h5')\n-    save_model(model, fname)\n-    new_model = load_model(fname, **load_kwargs)\n-    os.remove(fname)\n-\n-    new_out = new_model.predict(x)\n-    assert_allclose(out, new_out, atol=1e-05)\n-\n-\n-def _get_sample_model_and_input():\n-    inputs = Input(shape=(3,))\n-    x = Dense(2)(inputs)\n-    outputs = Dense(3)(x)\n-\n-    model = Model(inputs, outputs)\n-    model.compile(loss=losses.MSE,\n-                  optimizer=optimizers.Adam(),\n-                  metrics=[metrics.categorical_accuracy])\n-    x = np.random.random((1, 3))\n-    y = np.random.random((1, 3))\n-    model.train_on_batch(x, y)\n-\n-    return model, x\n-\n-\n-def test_functional_model_saving():\n-    model, x = _get_sample_model_and_input()\n-    out = model.predict(x)\n-    _, fname = tempfile.mkstemp('.h5')\n-    save_model(model, fname)\n-    new_model = load_model(fname)\n-    os.remove(fname)\n-\n-    new_out = new_model.predict(x)\n-    assert_allclose(out, new_out, atol=1e-05)\n-\n-\n-def DISABLED_test_model_saving_to_pre_created_h5py_file():\n-    model, x = _get_sample_model_and_input()\n-\n-    out = model.predict(x)\n-    _, fname = tempfile.mkstemp('.h5')\n-    with h5py.File(fname, mode='r+') as h5file:\n-        save_model(model, h5file)\n-        loaded_model = load_model(h5file)\n-        out2 = loaded_model.predict(x)\n-    assert_allclose(out, out2, atol=1e-05)\n-\n-    # test non-default options in h5\n-    with h5py.File('does not matter', driver='core',\n-                   backing_store=False) as h5file:\n-        save_model(model, h5file)\n-        loaded_model = load_model(h5file)\n-        out2 = loaded_model.predict(x)\n-    assert_allclose(out, out2, atol=1e-05)\n-\n-    with h5py.File(fname, mode='r+') as h5file:\n-        g = h5file.create_group('model')\n-        save_model(model, g)\n-        loaded_model = load_model(g)\n-        out2 = loaded_model.predict(x)\n-    assert_allclose(out, out2, atol=1e-05)\n-\n-\n-@contextmanager\n-def temp_filename(filename):\n-    \"\"\"Context that returns a temporary filename and deletes the file on exit if\n-    it still exists (so that this is not forgotten).\n-    \"\"\"\n-    _, temp_fname = tempfile.mkstemp(filename)\n-    yield temp_fname\n-    if os.path.exists(temp_fname):\n-        os.remove(temp_fname)\n-\n-\n-def DISABLED_test_model_saving_to_binary_stream():\n-    model, x = _get_sample_model_and_input()\n-    out = model.predict(x)\n-\n-    with temp_filename('h5') as fname:\n-        # save directly to binary file\n-        with open(fname, 'wb') as raw_file:\n-            save_model(model, raw_file)\n-        # Load the data the usual way, and make sure the model is intact.\n-        with h5py.File(fname, mode='r') as h5file:\n-            loaded_model = load_model(h5file)\n-    out2 = loaded_model.predict(x)\n-    assert_allclose(out, out2, atol=1e-05)\n-\n-\n-def DISABLED_test_model_loading_from_binary_stream():\n-    model, x = _get_sample_model_and_input()\n-    out = model.predict(x)\n-\n-    with temp_filename('h5') as fname:\n-        # save the model the usual way\n-        with h5py.File(fname, mode='w') as h5file:\n-            save_model(model, h5file)\n-        # Load the data binary, and make sure the model is intact.\n-        with open(fname, 'rb') as raw_file:\n-            loaded_model = load_model(raw_file)\n-    out2 = loaded_model.predict(x)\n-    assert_allclose(out, out2, atol=1e-05)\n-\n-\n-def DISABLED_test_model_save_load_binary_in_memory():\n-    model, x = _get_sample_model_and_input()\n-    out = model.predict(x)\n-\n-    stream = io.BytesIO()\n-    save_model(model, stream)\n-    stream.seek(0)\n-    loaded_model = load_model(stream)\n-    out2 = loaded_model.predict(x)\n-    assert_allclose(out, out2, atol=1e-05)\n-\n-\n-def test_saving_multiple_metrics_outputs():\n-    inputs = Input(shape=(5,))\n-    x = Dense(5)(inputs)\n-    output1 = Dense(1, name='output1')(x)\n-    output2 = Dense(1, name='output2')(x)\n-\n-    model = Model(inputs=inputs, outputs=[output1, output2])\n-\n-    metrics = {'output1': ['mse', 'binary_accuracy'],\n-               'output2': ['mse', 'binary_accuracy']\n-               }\n-    loss = {'output1': 'mse', 'output2': 'mse'}\n-\n-    model.compile(loss=loss, optimizer='sgd', metrics=metrics)\n-\n-    # assure that model is working\n-    x = np.array([[1, 1, 1, 1, 1]])\n-    out = model.predict(x)\n-    _, fname = tempfile.mkstemp('.h5')\n-    save_model(model, fname)\n-\n-    model = load_model(fname)\n-    os.remove(fname)\n-\n-    out2 = model.predict(x)\n-    assert_allclose(out, out2, atol=1e-05)\n-\n-\n-def test_saving_without_compilation():\n-    \"\"\"Test saving model without compiling.\n-    \"\"\"\n-    model = Sequential()\n-    model.add(Dense(2, input_shape=(3,)))\n-    model.add(Dense(3))\n-\n-    _, fname = tempfile.mkstemp('.h5')\n-    save_model(model, fname)\n-    model = load_model(fname)\n-    os.remove(fname)\n-\n-\n-def test_saving_right_after_compilation():\n-    model = Sequential()\n-    model.add(Dense(2, input_shape=(3,)))\n-    model.add(Dense(3))\n-    model.compile(loss='mse', optimizer='sgd', metrics=['acc'])\n-\n-    _, fname = tempfile.mkstemp('.h5')\n-    save_model(model, fname)\n-    model = load_model(fname)\n-    os.remove(fname)\n-\n-\n-def test_saving_unused_layers_is_ok():\n-    a = Input(shape=(256, 512, 6))\n-    b = Input(shape=(256, 512, 1))\n-    c = Lambda(lambda x: x[:, :, :, :1])(a)\n-\n-    model = Model(inputs=[a, b], outputs=c)\n-\n-    _, fname = tempfile.mkstemp('.h5')\n-    save_model(model, fname)\n-    load_model(fname)\n-    os.remove(fname)\n-\n-\n-def test_loading_weights_by_name_2():\n-    \"\"\"\n-    test loading model weights by name on:\n-        - both sequential and functional api models\n-        - different architecture with shared names\n-    \"\"\"\n-    custom_loss = losses.mse\n-\n-    # sequential model\n-    model = Sequential()\n-    model.add(Dense(2, input_shape=(3,), name='rick'))\n-    model.add(Dense(3, name='morty'))\n-    model.compile(loss=custom_loss, optimizer='rmsprop', metrics=['acc'])\n-\n-    x = np.random.random((1, 3))\n-    y = np.random.random((1, 3))\n-    model.train_on_batch(x, y)\n-\n-    out = model.predict(x)\n-    old_weights = [layer.get_weights() for layer in model.layers]\n-    _, fname = tempfile.mkstemp('.h5')\n-\n-    model.save_weights(fname)\n-\n-    # delete and recreate model using Functional API\n-    del(model)\n-    data = Input(shape=(3,))\n-    rick = Dense(2, name='rick')(data)\n-    jerry = Dense(3, name='jerry')(rick)  # add 2 layers (but maintain shapes)\n-    jessica = Dense(2, name='jessica')(jerry)\n-    morty = Dense(3, name='morty')(jessica)\n-\n-    model = Model(inputs=[data], outputs=[morty])\n-    model.compile(loss=custom_loss, optimizer='rmsprop', metrics=['acc'])\n-\n-    # load weights from first model\n-    model.load_weights(fname, by_name=True)\n-    os.remove(fname)\n-\n-    out2 = model.predict(x)\n-    assert np.max(np.abs(out - out2)) > 1e-05\n-\n-    rick = model.layers[1].get_weights()\n-    jerry = model.layers[2].get_weights()\n-    jessica = model.layers[3].get_weights()\n-    morty = model.layers[4].get_weights()\n-\n-    assert_allclose(old_weights[0][0], rick[0], atol=1e-05)\n-    assert_allclose(old_weights[0][1], rick[1], atol=1e-05)\n-    assert_allclose(old_weights[1][0], morty[0], atol=1e-05)\n-    assert_allclose(old_weights[1][1], morty[1], atol=1e-05)\n-    assert_allclose(np.zeros_like(jerry[1]), jerry[1])  # biases init to 0\n-    assert_allclose(np.zeros_like(jessica[1]), jessica[1])  # biases init to 0\n-\n-\n-def test_loading_weights_by_name_skip_mismatch():\n-    \"\"\"\n-    test skipping layers while loading model weights by name on:\n-        - sequential model\n-    \"\"\"\n-\n-    # test with custom optimizer, loss\n-    custom_loss = losses.mse\n-\n-    # sequential model\n-    model = Sequential()\n-    model.add(Dense(2, input_shape=(3,), name='rick'))\n-    model.add(Dense(3, name='morty'))\n-    model.compile(loss=custom_loss, optimizer='rmsprop', metrics=['acc'])\n-\n-    x = np.random.random((1, 3))\n-    y = np.random.random((1, 3))\n-    model.train_on_batch(x, y)\n-\n-    out = model.predict(x)\n-    old_weights = [layer.get_weights() for layer in model.layers]\n-    _, fname = tempfile.mkstemp('.h5')\n-\n-    model.save_weights(fname)\n-\n-    # delete and recreate model\n-    del(model)\n-    model = Sequential()\n-    model.add(Dense(2, input_shape=(3,), name='rick'))\n-    model.add(Dense(4, name='morty'))  # different shape w.r.t. previous model\n-    model.compile(loss=custom_loss, optimizer='rmsprop', metrics=['acc'])\n-\n-    # load weights from first model\n-    model.load_weights(fname, by_name=True, skip_mismatch=True)\n-    os.remove(fname)\n-\n-    # assert layers 'rick' are equal\n-    for old, new in zip(old_weights[0], model.layers[0].get_weights()):\n-        assert_allclose(old, new, atol=1e-05)\n-\n-    # assert layers 'morty' are not equal, since we skipped loading this layer\n-    for old, new in zip(old_weights[1], model.layers[1].get_weights()):\n-        assert_raises(AssertionError, assert_allclose, old, new, atol=1e-05)\n-\n-\n-# a function to be called from the Lambda layer\n-def square_fn(x):\n-    return x * x\n-\n-\n-def test_saving_lambda_custom_objects():\n-    inputs = Input(shape=(3,))\n-    x = Lambda(lambda x: square_fn(x), output_shape=(3,))(inputs)\n-    outputs = Dense(3)(x)\n-\n-    model = Model(inputs, outputs)\n-    model.compile(loss=losses.MSE,\n-                  optimizer=optimizers.RMSprop(lr=0.0001),\n-                  metrics=[metrics.categorical_accuracy])\n-    x = np.random.random((1, 3))\n-    y = np.random.random((1, 3))\n-    model.train_on_batch(x, y)\n-\n-    out = model.predict(x)\n-    _, fname = tempfile.mkstemp('.h5')\n-    save_model(model, fname)\n-\n-    model = load_model(fname, custom_objects={'square_fn': square_fn})\n-    os.remove(fname)\n-\n-    out2 = model.predict(x)\n-    assert_allclose(out, out2, atol=1e-05)\n-\n-\n-def test_saving_lambda_numpy_array_arguments():\n-    mean = np.random.random((4, 2, 3))\n-    std = np.abs(np.random.random((4, 2, 3))) + 1e-5\n-    inputs = Input(shape=(4, 2, 3))\n-    outputs = Lambda(lambda image, mu, std: (image - mu) / std,\n-                     arguments={'mu': mean, 'std': std})(inputs)\n-    model = Model(inputs, outputs)\n-    model.compile(loss='mse', optimizer='sgd', metrics=['acc'])\n-\n-    _, fname = tempfile.mkstemp('.h5')\n-    save_model(model, fname)\n-\n-    model = load_model(fname)\n-    os.remove(fname)\n-\n-    assert_allclose(mean, model.layers[1].arguments['mu'])\n-    assert_allclose(std, model.layers[1].arguments['std'])\n-\n-\n-def test_saving_custom_activation_function():\n-    x = Input(shape=(3,))\n-    output = Dense(3, activation=K.cos)(x)\n-\n-    model = Model(x, output)\n-    model.compile(loss=losses.MSE,\n-                  optimizer=optimizers.RMSprop(lr=0.0001),\n-                  metrics=[metrics.categorical_accuracy])\n-    x = np.random.random((1, 3))\n-    y = np.random.random((1, 3))\n-    model.train_on_batch(x, y)\n-\n-    out = model.predict(x)\n-    _, fname = tempfile.mkstemp('.h5')\n-    save_model(model, fname)\n-\n-    model = load_model(fname, custom_objects={'cos': K.cos})\n-    os.remove(fname)\n-\n-    out2 = model.predict(x)\n-    assert_allclose(out, out2, atol=1e-05)\n-\n-\n-def test_saving_model_with_long_layer_names():\n-    # This layer name will make the `layers_name` HDF5 attribute blow\n-    # out of proportion. Note that it fits into the internal HDF5\n-    # attribute memory limit on its own but because h5py converts\n-    # the list of layer names into numpy array, which uses the same\n-    # amout of memory for every item, it increases the memory\n-    # requirements substantially.\n-    x = Input(shape=(2,), name='input_' + ('x' * (2**15)))\n-    f = x\n-    for i in range(4):\n-        f = Dense(2, name='dense_%d' % (i,))(f)\n-\n-    model = Model(inputs=[x], outputs=[f])\n-\n-    model.compile(loss='mse', optimizer='adam', metrics=['acc'])\n-\n-    x = np.random.random((1, 2))\n-    y = np.random.random((1, 2))\n-    model.train_on_batch(x, y)\n-\n-    out = model.predict(x)\n-\n-    _, fname = tempfile.mkstemp('.h5')\n-    save_model(model, fname)\n-\n-    model = load_model(fname)\n-\n-    # Check that the HDF5 files contains chunked array\n-    # of layer names.\n-    with h5py.File(fname, 'r') as h5file:\n-        n_layer_names_arrays = len([attr for attr in h5file['model_weights'].attrs\n-                                    if attr.startswith('layer_names')])\n-\n-    os.remove(fname)\n-\n-    # The chunking of layer names array should have happened.\n-    assert n_layer_names_arrays > 0\n-\n-    out2 = model.predict(x)\n-    assert_allclose(out, out2, atol=1e-05)\n-\n-\n-def test_saving_model_with_long_weights_names():\n-    x = Input(shape=(2,), name='nested_model_input')\n-    f = x\n-    for i in range(4):\n-        f = Dense(2, name='nested_model_dense_%d' % (i,))(f)\n-    f = Dense(2, name='nested_model_dense_4', trainable=False)(f)\n-    # This layer name will make the `weights_name`\n-    # HDF5 attribute blow out of proportion.\n-    f = Dense(2, name='nested_model_output' + ('x' * (2**15)))(f)\n-    nested_model = Model(inputs=[x], outputs=[f], name='nested_model')\n-\n-    x = Input(shape=(2,), name='outer_model_input')\n-    f = nested_model(x)\n-    f = Dense(2, name='outer_model_output')(f)\n-\n-    model = Model(inputs=[x], outputs=[f])\n-\n-    model.compile(loss='mse', optimizer='adam', metrics=['acc'])\n-\n-    x = np.random.random((1, 2))\n-    y = np.random.random((1, 2))\n-    model.train_on_batch(x, y)\n-\n-    out = model.predict(x)\n-\n-    _, fname = tempfile.mkstemp('.h5')\n-    save_model(model, fname)\n-\n-    model = load_model(fname)\n-\n-    # Check that the HDF5 files contains chunked array\n-    # of weight names.\n-    with h5py.File(fname, 'r') as h5file:\n-        attrs = [attr for attr in h5file['model_weights']['nested_model'].attrs\n-                 if attr.startswith('weight_names')]\n-        n_weight_names_arrays = len(attrs)\n-\n-    os.remove(fname)\n-\n-    # The chunking of layer names array should have happened.\n-    assert n_weight_names_arrays > 0\n-\n-    out2 = model.predict(x)\n-    assert_allclose(out, out2, atol=1e-05)\n-\n-\n-def test_saving_recurrent_layer_with_init_state():\n-    vector_size = 8\n-    input_length = 20\n-\n-    input_initial_state = Input(shape=(vector_size,))\n-    input_x = Input(shape=(input_length, vector_size))\n-\n-    lstm = LSTM(vector_size, return_sequences=True)(\n-        input_x, initial_state=[input_initial_state, input_initial_state])\n-\n-    model = Model(inputs=[input_x, input_initial_state], outputs=[lstm])\n-\n-    _, fname = tempfile.mkstemp('.h5')\n-    model.save(fname)\n-\n-    loaded_model = load_model(fname)\n-    os.remove(fname)\n-\n-\n-def test_saving_recurrent_layer_without_bias():\n-    vector_size = 8\n-    input_length = 20\n-\n-    input_x = Input(shape=(input_length, vector_size))\n-    lstm = LSTM(vector_size, use_bias=False)(input_x)\n-    model = Model(inputs=[input_x], outputs=[lstm])\n-\n-    _, fname = tempfile.mkstemp('.h5')\n-    model.save(fname)\n-\n-    loaded_model = load_model(fname)\n-    os.remove(fname)\n-\n-\n-def test_loop_model_saving():\n-    model = Sequential()\n-    model.add(Dense(2, input_shape=(3,)))\n-    model.compile(loss=losses.MSE,\n-                  optimizer=optimizers.RMSprop(lr=0.0001),\n-                  metrics=[metrics.categorical_accuracy])\n-\n-    x = np.random.random((1, 3))\n-    y = np.random.random((1, 2))\n-    _, fname = tempfile.mkstemp('.h5')\n-\n-    for _ in range(3):\n-        model.train_on_batch(x, y)\n-        save_model(model, fname, overwrite=True)\n-        out = model.predict(x)\n-\n-    new_model = load_model(fname)\n-    os.remove(fname)\n-\n-    out2 = new_model.predict(x)\n-    assert_allclose(out, out2, atol=1e-05)\n-\n-\n-def test_saving_constant_initializer_with_numpy():\n-    \"\"\"Test saving and loading model of constant initializer with numpy inputs.\n-    \"\"\"\n-    model = Sequential()\n-    model.add(Dense(2, input_shape=(3,),\n-                    kernel_initializer=Constant(np.ones((3, 2)))))\n-    model.add(Dense(3))\n-    model.compile(loss='mse', optimizer='sgd', metrics=['acc'])\n-\n-    _, fname = tempfile.mkstemp('.h5')\n-    save_model(model, fname)\n-    model = load_model(fname)\n-    os.remove(fname)\n-\n-\n-def test_saving_group_naming_h5py(tmpdir):\n-    \"\"\"Test saving model with layer which name is prefix to a previous layer\n-    name\n-    \"\"\"\n-\n-    input_layer = Input((None, None, 3), name='test_input')\n-    x = Conv2D(1, 1, name='conv1/conv')(input_layer)\n-    x = Activation('relu', name='conv1')(x)\n-\n-    model = Model(inputs=input_layer, outputs=x)\n-    p = tmpdir.mkdir(\"test\").join(\"test.h5\")\n-    model.save_weights(p)\n-    model.load_weights(p)\n-\n-\n-def _make_nested_model(input_shape, layer, level=1, model_type='func'):\n-    # example: make_nested_seq_model((1,), Dense(10), level=2).summary()\n-    def make_nested_seq_model(input_shape, layer, level=1):\n-        model = layer\n-        for i in range(1, level + 1):\n-            layers = [InputLayer(input_shape), model] if (i == 1) else [model]\n-            model = Sequential(layers)\n-        return model\n-\n-    # example: make_nested_func_model((1,), Dense(10), level=2).summary()\n-    def make_nested_func_model(input_shape, layer, level=1):\n-        input = Input(input_shape)\n-        model = layer\n-        for i in range(level):\n-            model = Model(input, model(input))\n-        return model\n-\n-    if model_type == 'func':\n-        return make_nested_func_model(input_shape, layer, level)\n-    elif model_type == 'seq':\n-        return make_nested_seq_model(input_shape, layer, level)\n-\n-\n-def _convert_model_weights(source_model, target_model):\n-    _, fname = tempfile.mkstemp('.h5')\n-    source_model.save_weights(fname)\n-    target_model.load_weights(fname)\n-    os.remove(fname)\n-\n-\n-def test_model_saving_with_rnn_initial_state_and_args():\n-    class CustomRNN(LSTM):\n-        def call(self, inputs, arg=1, mask=None, training=None, initial_state=None):\n-            if isinstance(inputs, list):\n-                inputs = inputs[:]\n-                shape = K.int_shape(inputs[0])\n-                inputs[0] *= arg\n-                inputs[0]._keras_shape = shape  # for theano backend\n-            else:\n-                shape = K.int_shape(inputs)\n-                inputs *= arg\n-                inputs._keras_shape = shape  # for theano backend\n-            return super(CustomRNN, self).call(inputs, mask, training, initial_state)\n-\n-    inp = Input((3, 2))\n-    rnn_out, h, c = CustomRNN(2, return_state=True, return_sequences=True)(inp)\n-    assert hasattr(rnn_out, '_keras_history')\n-    assert hasattr(h, '_keras_history')\n-    assert hasattr(c, '_keras_history')\n-    rnn2_out = CustomRNN(2)(rnn_out, arg=2, initial_state=[h, c])\n-    assert hasattr(rnn2_out, '_keras_history')\n-    model = Model(inputs=inp, outputs=rnn2_out)\n-    x = np.random.random((2, 3, 2))\n-    y1 = model.predict(x)\n-    _, fname = tempfile.mkstemp('.h5')\n-    with warnings.catch_warnings():\n-        warnings.filterwarnings('error')\n-        model.save(fname)\n-    model2 = load_model(fname, custom_objects={'CustomRNN': CustomRNN})\n-    y2 = model2.predict(x)\n-    assert_allclose(y1, y2, atol=1e-5)\n-    os.remove(fname)\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,1092 +0,0 @@\n-from __future__ import print_function\n-\n-import multiprocessing as mp\n-import os\n-import sys\n-import threading\n-import pytest\n-import numpy as np\n-import six\n-\n-from keras.models import Sequential\n-from keras.layers.core import Dense\n-from keras.utils import Sequence\n-from keras import backend as K\n-\n-pytestmark = pytest.mark.skipif(\n-    True,\n-    reason='Temporarily disabled until the use_multiprocessing problem is solved')\n-\n-skip_generators = pytest.mark.skipif(K.backend() in {'tensorflow', 'cntk'} and\n-                                     'TRAVIS_PYTHON_VERSION' in os.environ,\n-                                     reason='Generators do not work with `spawn`.')\n-\n-\n-def use_spawn(func):\n-    \"\"\"Decorator which uses `spawn` when possible.\n-    This is useful on Travis to avoid memory issues.\n-    \"\"\"\n-\n-    @six.wraps(func)\n-    def wrapper(*args, **kwargs):\n-        if sys.version_info > (3, 4) and os.name != 'nt':\n-            mp.set_start_method('spawn', force=True)\n-            out = func(*args, **kwargs)\n-            mp.set_start_method('fork', force=True)\n-        else:\n-            out = func(*args, **kwargs)\n-        return out\n-\n-    return wrapper\n-\n-\n-STEPS_PER_EPOCH = 100\n-STEPS = 100\n-WORKERS = 4 if K.backend() != 'tensorflow' else 2\n-\n-\n-class DummySequence(Sequence):\n-    def __getitem__(self, idx):\n-        return np.zeros([10, 2]), np.ones([10])\n-\n-    def __len__(self):\n-        return 10\n-\n-\n-class threadsafe_iter:\n-    \"\"\"Takes an iterator/generator and makes it thread-safe by\n-    serializing call to the `next` method of given iterator/generator.\n-    \"\"\"\n-\n-    def __init__(self, it):\n-        self.it = it\n-        self.lock = threading.Lock()\n-\n-    def __iter__(self):\n-        return self\n-\n-    def __next__(self):\n-        return self.next()\n-\n-    def next(self):\n-        with self.lock:\n-            return next(self.it)\n-\n-\n-def threadsafe_generator(f):\n-    \"\"\"A decorator that takes a generator function and makes it thread-safe.\n-    \"\"\"\n-\n-    def g(*a, **kw):\n-        return threadsafe_iter(f(*a, **kw))\n-\n-    return g\n-\n-\n-@pytest.fixture\n-def in_tmpdir(tmpdir):\n-    \"\"\"Runs a function in a temporary directory.\n-\n-    Checks that the directory is empty afterwards.\n-    \"\"\"\n-    with tmpdir.as_cwd():\n-        yield None\n-    assert not tmpdir.listdir()\n-\n-\n-@skip_generators\n-def test_multiprocessing_training():\n-    arr_data = np.random.randint(0, 256, (50, 2))\n-    arr_labels = np.random.randint(0, 2, 50)\n-    arr_weights = np.random.random(50)\n-\n-    @threadsafe_generator\n-    def custom_generator(use_weights=False):\n-        batch_size = 10\n-        n_samples = 50\n-\n-        while True:\n-            batch_index = np.random.randint(0, n_samples - batch_size)\n-            start = batch_index\n-            end = start + batch_size\n-            X = arr_data[start: end]\n-            y = arr_labels[start: end]\n-            if use_weights:\n-                w = arr_weights[start: end]\n-                yield X, y, w\n-            else:\n-                yield X, y\n-\n-    # Build a NN\n-    model = Sequential()\n-    model.add(Dense(1, input_shape=(2,)))\n-    model.compile(loss='mse', optimizer='adadelta')\n-\n-    # - Produce data on 4 worker processes, consume on main process:\n-    #   - Each worker process runs OWN copy of generator\n-    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n-    #     process boundaries -> make sure `fit_generator()` raises ValueError\n-    #     exception and does not attempt to run the generator.\n-    if os.name == 'nt':\n-        with pytest.raises(ValueError):\n-            model.fit_generator(custom_generator(),\n-                                steps_per_epoch=STEPS_PER_EPOCH,\n-                                epochs=1,\n-                                verbose=1,\n-                                validation_steps=None,\n-                                max_queue_size=10,\n-                                workers=WORKERS,\n-                                use_multiprocessing=True)\n-    else:\n-        model.fit_generator(custom_generator(),\n-                            steps_per_epoch=STEPS_PER_EPOCH,\n-                            epochs=1,\n-                            verbose=1,\n-                            validation_steps=None,\n-                            max_queue_size=10,\n-                            workers=WORKERS,\n-                            use_multiprocessing=True)\n-\n-    # - Produce data on 4 worker threads, consume on main thread:\n-    #   - All worker threads share the SAME generator\n-    model.fit_generator(custom_generator(),\n-                        steps_per_epoch=STEPS_PER_EPOCH,\n-                        epochs=1,\n-                        verbose=1,\n-                        validation_steps=None,\n-                        max_queue_size=10,\n-                        workers=WORKERS,\n-                        use_multiprocessing=False)\n-\n-    # - Produce data on 1 worker process, consume on main process:\n-    #   - Worker process runs generator\n-    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n-    #     process boundaries -> make sure `fit_generator()` raises ValueError\n-    #     exception and does not attempt to run the generator.\n-    if os.name == 'nt':\n-        with pytest.raises(ValueError):\n-            model.fit_generator(custom_generator(True),\n-                                steps_per_epoch=STEPS_PER_EPOCH,\n-                                validation_data=(arr_data[:10],\n-                                                 arr_labels[:10],\n-                                                 arr_weights[:10]),\n-                                validation_steps=1,\n-                                max_queue_size=10,\n-                                workers=1,\n-                                use_multiprocessing=True)\n-    else:\n-        model.fit_generator(custom_generator(True),\n-                            steps_per_epoch=STEPS_PER_EPOCH,\n-                            validation_data=(arr_data[:10],\n-                                             arr_labels[:10],\n-                                             arr_weights[:10]),\n-                            validation_steps=1,\n-                            max_queue_size=10,\n-                            workers=1,\n-                            use_multiprocessing=True)\n-\n-    # - Produce data on 1 worker thread, consume on main thread:\n-    #   - Worker thread is the only thread running the generator\n-    model.fit_generator(custom_generator(True),\n-                        steps_per_epoch=STEPS_PER_EPOCH,\n-                        validation_data=(arr_data[:10],\n-                                         arr_labels[:10],\n-                                         arr_weights[:10]),\n-                        validation_steps=1,\n-                        max_queue_size=10,\n-                        workers=1,\n-                        use_multiprocessing=False)\n-\n-    # - Produce data on 1 worker process, consume on main process:\n-    #   - Worker process runs generator\n-    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n-    #     process boundaries -> make sure `fit_generator()` raises ValueError\n-    #     exception and does not attempt to run the generator.\n-    if os.name == 'nt':\n-        with pytest.raises(ValueError):\n-            model.fit_generator(custom_generator(True),\n-                                steps_per_epoch=STEPS_PER_EPOCH,\n-                                validation_data=custom_generator(True),\n-                                validation_steps=1,\n-                                max_queue_size=10,\n-                                workers=1,\n-                                use_multiprocessing=True)\n-    else:\n-        model.fit_generator(custom_generator(True),\n-                            steps_per_epoch=STEPS_PER_EPOCH,\n-                            validation_data=custom_generator(True),\n-                            validation_steps=1,\n-                            max_queue_size=10,\n-                            workers=1,\n-                            use_multiprocessing=True)\n-\n-    # - Produce data on 1 worker thread AT A TIME, consume on main thread:\n-    #   - Worker threads for training and validation run generator SEQUENTIALLY\n-    model.fit_generator(custom_generator(True),\n-                        steps_per_epoch=STEPS_PER_EPOCH,\n-                        validation_data=custom_generator(True),\n-                        validation_steps=1,\n-                        max_queue_size=10,\n-                        workers=1,\n-                        use_multiprocessing=False)\n-\n-    # - Produce and consume data without a queue on main thread\n-    #   - Make sure the value of `use_multiprocessing` is ignored\n-    model.fit_generator(custom_generator(True),\n-                        steps_per_epoch=STEPS_PER_EPOCH,\n-                        validation_data=custom_generator(True),\n-                        validation_steps=1,\n-                        max_queue_size=10,\n-                        workers=0,\n-                        use_multiprocessing=True)\n-    model.fit_generator(custom_generator(True),\n-                        steps_per_epoch=STEPS_PER_EPOCH,\n-                        validation_data=custom_generator(True),\n-                        validation_steps=1,\n-                        max_queue_size=10,\n-                        workers=0,\n-                        use_multiprocessing=False)\n-\n-    # Test invalid use cases\n-    @threadsafe_generator\n-    def invalid_generator():\n-        while True:\n-            yield arr_data[:10], arr_data[:10], arr_labels[:10], arr_labels[:10]\n-\n-    # not specified `validation_steps`\n-    with pytest.raises(ValueError):\n-        model.fit_generator(custom_generator(),\n-                            steps_per_epoch=STEPS_PER_EPOCH,\n-                            validation_data=custom_generator(),\n-                            validation_steps=None,\n-                            max_queue_size=10,\n-                            workers=1,\n-                            use_multiprocessing=False)\n-\n-    # validation data is neither a tuple nor a triple.\n-    with pytest.raises(ValueError):\n-        model.fit_generator(custom_generator(),\n-                            steps_per_epoch=STEPS_PER_EPOCH,\n-                            validation_data=(arr_data[:10],\n-                                             arr_data[:10],\n-                                             arr_labels[:10],\n-                                             arr_weights[:10]),\n-                            validation_steps=1,\n-                            max_queue_size=10,\n-                            workers=1,\n-                            use_multiprocessing=False)\n-\n-    # validation generator is neither a tuple nor a triple.\n-    with pytest.raises(ValueError):\n-        model.fit_generator(custom_generator(),\n-                            steps_per_epoch=STEPS_PER_EPOCH,\n-                            validation_data=invalid_generator(),\n-                            validation_steps=1,\n-                            max_queue_size=10,\n-                            workers=1,\n-                            use_multiprocessing=False)\n-\n-    # - For Sequence\n-    model.fit_generator(DummySequence(),\n-                        steps_per_epoch=STEPS_PER_EPOCH,\n-                        validation_data=DummySequence(),\n-                        validation_steps=1,\n-                        max_queue_size=10,\n-                        workers=0,\n-                        use_multiprocessing=True)\n-    model.fit_generator(DummySequence(),\n-                        steps_per_epoch=STEPS_PER_EPOCH,\n-                        validation_data=DummySequence(),\n-                        validation_steps=1,\n-                        max_queue_size=10,\n-                        workers=0,\n-                        use_multiprocessing=False)\n-\n-\n-@skip_generators\n-def test_multiprocessing_training_from_file(in_tmpdir):\n-    arr_data = np.random.randint(0, 256, (50, 2))\n-    arr_labels = np.random.randint(0, 2, 50)\n-    np.savez('data.npz', **{'data': arr_data, 'labels': arr_labels})\n-\n-    @threadsafe_generator\n-    def custom_generator():\n-\n-        batch_size = 10\n-        n_samples = 50\n-\n-        with np.load('data.npz') as arr:\n-            while True:\n-                batch_index = np.random.randint(0, n_samples - batch_size)\n-                start = batch_index\n-                end = start + batch_size\n-                X = arr['data'][start: end]\n-                y = arr['labels'][start: end]\n-                yield X, y\n-\n-    # Build a NN\n-    model = Sequential()\n-    model.add(Dense(1, input_shape=(2,)))\n-    model.compile(loss='mse', optimizer='adadelta')\n-\n-    # - Produce data on 4 worker processes, consume on main process:\n-    #   - Each worker process runs OWN copy of generator\n-    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n-    #     process boundaries -> make sure `fit_generator()` raises ValueError\n-    #     exception and does not attempt to run the generator.\n-    if os.name == 'nt':\n-        with pytest.raises(ValueError):\n-            model.fit_generator(custom_generator(),\n-                                steps_per_epoch=STEPS_PER_EPOCH,\n-                                epochs=1,\n-                                verbose=1,\n-                                validation_steps=None,\n-                                max_queue_size=10,\n-                                workers=WORKERS,\n-                                use_multiprocessing=True)\n-    else:\n-        model.fit_generator(custom_generator(),\n-                            steps_per_epoch=STEPS_PER_EPOCH,\n-                            epochs=1,\n-                            verbose=1,\n-                            validation_steps=None,\n-                            max_queue_size=10,\n-                            workers=WORKERS,\n-                            use_multiprocessing=True)\n-\n-    # - Produce data on 1 worker process, consume on main process:\n-    #   - Worker process runs generator\n-    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n-    #     process boundaries -> make sure `fit_generator()` raises ValueError\n-    #     exception and does not attempt to run the generator.\n-    if os.name == 'nt':\n-        with pytest.raises(ValueError):\n-            model.fit_generator(custom_generator(),\n-                                steps_per_epoch=STEPS_PER_EPOCH,\n-                                epochs=1,\n-                                verbose=1,\n-                                validation_steps=None,\n-                                max_queue_size=10,\n-                                workers=1,\n-                                use_multiprocessing=True)\n-    else:\n-        model.fit_generator(custom_generator(),\n-                            steps_per_epoch=STEPS_PER_EPOCH,\n-                            epochs=1,\n-                            verbose=1,\n-                            validation_steps=None,\n-                            max_queue_size=10,\n-                            workers=1,\n-                            use_multiprocessing=True)\n-\n-    # - Produce data on 1 worker thread, consume on main thread:\n-    #   - Worker thread is the only thread running the generator\n-\n-    # - Produce and consume data without a queue on main thread\n-    #   - Make sure the value of `use_multiprocessing` is ignored\n-    model.fit_generator(custom_generator(),\n-                        steps_per_epoch=STEPS_PER_EPOCH,\n-                        epochs=1,\n-                        verbose=1,\n-                        validation_steps=None,\n-                        max_queue_size=10,\n-                        workers=0,\n-                        use_multiprocessing=True)\n-\n-    os.remove('data.npz')\n-\n-\n-def test_multithreading_from_file():\n-    arr_data = np.random.randint(0, 256, (50, 2))\n-    arr_labels = np.random.randint(0, 2, 50)\n-    np.savez('data_threads.npz', **{'data': arr_data, 'labels': arr_labels})\n-\n-    @threadsafe_generator\n-    def custom_generator():\n-        batch_size = 10\n-        n_samples = 50\n-\n-        with np.load('data_threads.npz') as arr:\n-            while True:\n-                batch_index = np.random.randint(0, n_samples - batch_size)\n-                start = batch_index\n-                end = start + batch_size\n-                X = arr['data'][start: end]\n-                y = arr['labels'][start: end]\n-                yield X, y\n-\n-    # Build a NN\n-    model = Sequential()\n-    model.add(Dense(1, input_shape=(2,)))\n-    model.compile(loss='mse', optimizer='adadelta')\n-\n-    # - Produce data on 4 worker threads, consume on main thread:\n-    #   - All worker threads share the SAME generator\n-    model.fit_generator(custom_generator(),\n-                        steps_per_epoch=STEPS_PER_EPOCH,\n-                        epochs=1,\n-                        verbose=1,\n-                        validation_steps=None,\n-                        max_queue_size=10,\n-                        workers=WORKERS,\n-                        use_multiprocessing=False)\n-\n-    # - Produce data on 1 worker thread, consume on main thread:\n-    #   - Worker thread is the only thread running the generator\n-    model.fit_generator(custom_generator(),\n-                        steps_per_epoch=STEPS_PER_EPOCH,\n-                        epochs=1,\n-                        verbose=1,\n-                        validation_steps=None,\n-                        max_queue_size=10,\n-                        workers=1,\n-                        use_multiprocessing=False)\n-\n-    # - Produce and consume data without a queue on main thread\n-    #   - Make sure the value of `use_multiprocessing` is ignored\n-    model.fit_generator(custom_generator(),\n-                        steps_per_epoch=STEPS_PER_EPOCH,\n-                        epochs=1,\n-                        verbose=1,\n-                        validation_steps=None,\n-                        max_queue_size=10,\n-                        workers=0,\n-                        use_multiprocessing=False)\n-\n-    os.remove('data_threads.npz')\n-\n-\n-@skip_generators\n-def test_multiprocessing_predicting():\n-    arr_data = np.random.randint(0, 256, (50, 2))\n-\n-    @threadsafe_generator\n-    def custom_generator():\n-        batch_size = 10\n-        n_samples = 50\n-\n-        while True:\n-            batch_index = np.random.randint(0, n_samples - batch_size)\n-            start = batch_index\n-            end = start + batch_size\n-            X = arr_data[start: end]\n-            yield X\n-\n-    # Build a NN\n-    model = Sequential()\n-    model.add(Dense(1, input_shape=(2,)))\n-    model.compile(loss='mse', optimizer='adadelta')\n-\n-    # - Produce data on 4 worker processes, consume on main process:\n-    #   - Each worker process runs OWN copy of generator\n-    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n-    #     process boundaries -> make sure `predict_generator()` raises ValueError\n-    #     exception and does not attempt to run the generator.\n-    if os.name == 'nt':\n-        with pytest.raises(ValueError):\n-            model.predict_generator(custom_generator(),\n-                                    steps=STEPS,\n-                                    max_queue_size=10,\n-                                    workers=WORKERS,\n-                                    use_multiprocessing=True)\n-    else:\n-        model.predict_generator(custom_generator(),\n-                                steps=STEPS,\n-                                max_queue_size=10,\n-                                workers=WORKERS,\n-                                use_multiprocessing=True)\n-\n-    # - Produce data on 1 worker process, consume on main process:\n-    #   - Worker process runs generator\n-    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n-    #     process boundaries -> make sure `predict_generator()` raises ValueError\n-    #     exception and does not attempt to run the generator.\n-    if os.name == 'nt':\n-        with pytest.raises(ValueError):\n-            model.predict_generator(custom_generator(),\n-                                    steps=STEPS,\n-                                    max_queue_size=10,\n-                                    workers=1,\n-                                    use_multiprocessing=True)\n-    else:\n-        model.predict_generator(custom_generator(),\n-                                steps=STEPS,\n-                                max_queue_size=10,\n-                                workers=1,\n-                                use_multiprocessing=True)\n-\n-    # - Main thread runs the generator without a queue\n-    #   - Make sure the value of `use_multiprocessing` is ignored\n-    model.predict_generator(custom_generator(),\n-                            steps=STEPS,\n-                            max_queue_size=10,\n-                            workers=0,\n-                            use_multiprocessing=True)\n-\n-\n-def test_multithreading_predicting():\n-    arr_data = np.random.randint(0, 256, (50, 2))\n-\n-    @threadsafe_generator\n-    def custom_generator():\n-        batch_size = 10\n-        n_samples = 50\n-\n-        while True:\n-            batch_index = np.random.randint(0, n_samples - batch_size)\n-            start = batch_index\n-            end = start + batch_size\n-            X = arr_data[start: end]\n-            yield X\n-\n-    # Build a NN\n-    model = Sequential()\n-    model.add(Dense(1, input_shape=(2,)))\n-    model.compile(loss='mse', optimizer='adadelta')\n-\n-    # - Produce data on 4 worker threads, consume on main thread:\n-    #   - All worker threads share the SAME generator\n-    model.predict_generator(custom_generator(),\n-                            steps=STEPS,\n-                            max_queue_size=10,\n-                            workers=WORKERS,\n-                            use_multiprocessing=False)\n-\n-    # - Produce data on 1 worker thread, consume on main thread:\n-    #   - Worker thread is the only thread running the generator\n-    model.predict_generator(custom_generator(),\n-                            steps=STEPS,\n-                            max_queue_size=10,\n-                            workers=1,\n-                            use_multiprocessing=False)\n-\n-    # - Main thread runs the generator without a queue\n-    #   - Make sure the value of `use_multiprocessing` is ignored\n-    model.predict_generator(custom_generator(),\n-                            steps=STEPS,\n-                            max_queue_size=10,\n-                            workers=0,\n-                            use_multiprocessing=False)\n-\n-\n-@skip_generators\n-def test_multiprocessing_evaluating():\n-    arr_data = np.random.randint(0, 256, (50, 2))\n-    arr_labels = np.random.randint(0, 2, 50)\n-\n-    @threadsafe_generator\n-    def custom_generator():\n-        batch_size = 10\n-        n_samples = 50\n-\n-        while True:\n-            batch_index = np.random.randint(0, n_samples - batch_size)\n-            start = batch_index\n-            end = start + batch_size\n-            X = arr_data[start: end]\n-            y = arr_labels[start: end]\n-            yield X, y\n-\n-    # Build a NN\n-    model = Sequential()\n-    model.add(Dense(1, input_shape=(2,)))\n-    model.compile(loss='mse', optimizer='adadelta')\n-\n-    # - Produce data on 4 worker processes, consume on main process:\n-    #   - Each worker process runs OWN copy of generator\n-    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n-    #     process boundaries\n-    #       -> make sure `evaluate_generator()` raises raises ValueError\n-    #          exception and does not attempt to run the generator.\n-    if os.name == 'nt':\n-        with pytest.raises(ValueError):\n-            model.evaluate_generator(custom_generator(),\n-                                     steps=STEPS,\n-                                     max_queue_size=10,\n-                                     workers=WORKERS,\n-                                     use_multiprocessing=True)\n-    else:\n-        model.evaluate_generator(custom_generator(),\n-                                 steps=STEPS,\n-                                 max_queue_size=10,\n-                                 workers=WORKERS,\n-                                 use_multiprocessing=True)\n-\n-    # - Produce data on 1 worker process, consume on main process:\n-    #   - Worker process runs generator\n-    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n-    #     process boundaries -> make sure `evaluate_generator()` raises ValueError\n-    #     exception and does not attempt to run the generator.\n-    if os.name == 'nt':\n-        with pytest.raises(ValueError):\n-            model.evaluate_generator(custom_generator(),\n-                                     steps=STEPS,\n-                                     max_queue_size=10,\n-                                     workers=1,\n-                                     use_multiprocessing=True)\n-    else:\n-        model.evaluate_generator(custom_generator(),\n-                                 steps=STEPS,\n-                                 max_queue_size=10,\n-                                 workers=1,\n-                                 use_multiprocessing=True)\n-\n-    # - Produce and consume data without a queue on main thread\n-    #   - Make sure the value of `use_multiprocessing` is ignored\n-    model.evaluate_generator(custom_generator(),\n-                             steps=STEPS,\n-                             max_queue_size=10,\n-                             workers=0,\n-                             use_multiprocessing=True)\n-\n-\n-def test_multithreading_evaluating():\n-    arr_data = np.random.randint(0, 256, (50, 2))\n-    arr_labels = np.random.randint(0, 2, 50)\n-\n-    @threadsafe_generator\n-    def custom_generator():\n-        batch_size = 10\n-        n_samples = 50\n-\n-        while True:\n-            batch_index = np.random.randint(0, n_samples - batch_size)\n-            start = batch_index\n-            end = start + batch_size\n-            X = arr_data[start: end]\n-            y = arr_labels[start: end]\n-            yield X, y\n-\n-    # Build a NN\n-    model = Sequential()\n-    model.add(Dense(1, input_shape=(2,)))\n-    model.compile(loss='mse', optimizer='adadelta')\n-\n-    # - Produce data on 4 worker threads, consume on main thread:\n-    #   - All worker threads share the SAME generator\n-    model.evaluate_generator(custom_generator(),\n-                             steps=STEPS,\n-                             max_queue_size=10,\n-                             workers=WORKERS,\n-                             use_multiprocessing=False)\n-\n-    # - Produce data on 1 worker thread, consume on main thread:\n-    #   - Worker thread is the only thread running the generator\n-    model.evaluate_generator(custom_generator(),\n-                             steps=STEPS,\n-                             max_queue_size=10,\n-                             workers=1,\n-                             use_multiprocessing=False)\n-\n-    # - Produce and consume data without a queue on main thread\n-    #   - Make sure the value of `use_multiprocessing` is ignored\n-    model.evaluate_generator(custom_generator(),\n-                             steps=STEPS,\n-                             max_queue_size=10,\n-                             workers=0,\n-                             use_multiprocessing=False)\n-\n-\n-@skip_generators\n-def test_multiprocessing_fit_error():\n-    arr_data = np.random.randint(0, 256, (50, 2))\n-    arr_labels = np.random.randint(0, 2, 50)\n-    batch_size = 10\n-    n_samples = 50\n-    good_batches = 3\n-\n-    @threadsafe_generator\n-    def custom_generator(use_weights=False):\n-        \"\"\"Raises an exception after a few good batches\"\"\"\n-        for i in range(good_batches):\n-            batch_index = np.random.randint(0, n_samples - batch_size)\n-            start = batch_index\n-            end = start + batch_size\n-            X = arr_data[start: end]\n-            y = arr_labels[start: end]\n-            yield X, y\n-        raise RuntimeError\n-\n-    model = Sequential()\n-    model.add(Dense(1, input_shape=(2,)))\n-    model.compile(loss='mse', optimizer='adadelta')\n-\n-    samples = batch_size * (good_batches + 1)\n-\n-    # - Produce data on 4 worker processes, consume on main process:\n-    #   - Each worker process runs OWN copy of generator\n-    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n-    #     process boundaries -> make sure `fit_generator()` raises ValueError\n-    #     exception and does not attempt to run the generator.\n-    #   - On other platforms, make sure `RuntimeError` exception bubbles up\n-    if os.name == 'nt':\n-        with pytest.raises(RuntimeError):\n-            model.fit_generator(custom_generator(),\n-                                steps_per_epoch=samples,\n-                                validation_steps=None,\n-                                max_queue_size=10,\n-                                workers=WORKERS,\n-                                use_multiprocessing=True)\n-    else:\n-        with pytest.raises(RuntimeError):\n-            model.fit_generator(custom_generator(),\n-                                steps_per_epoch=samples,\n-                                validation_steps=None,\n-                                max_queue_size=10,\n-                                workers=WORKERS,\n-                                use_multiprocessing=True)\n-\n-    # - Produce data on 1 worker process, consume on main process:\n-    #   - Worker process runs generator\n-    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n-    #     process boundaries -> make sure `fit_generator()` raises ValueError\n-    #     exception and does not attempt to run the generator.\n-    #   - On other platforms, make sure `RuntimeError` exception bubbles up\n-    if os.name == 'nt':\n-        with pytest.raises(RuntimeError):\n-            model.fit_generator(custom_generator(),\n-                                steps_per_epoch=samples,\n-                                validation_steps=None,\n-                                max_queue_size=10,\n-                                workers=1,\n-                                use_multiprocessing=True)\n-    else:\n-        with pytest.raises(RuntimeError):\n-            model.fit_generator(custom_generator(),\n-                                steps_per_epoch=samples,\n-                                validation_steps=None,\n-                                max_queue_size=10,\n-                                workers=1,\n-                                use_multiprocessing=True)\n-\n-    #   - Make sure the value of `use_multiprocessing` is ignored\n-    #   - Make sure `RuntimeError` exception bubbles up\n-    with pytest.raises(RuntimeError):\n-        model.fit_generator(custom_generator(),\n-                            steps_per_epoch=samples,\n-                            validation_steps=None,\n-                            max_queue_size=10,\n-                            workers=0,\n-                            use_multiprocessing=True)\n-\n-\n-def test_multithreading_fit_error():\n-    arr_data = np.random.randint(0, 256, (50, 2))\n-    arr_labels = np.random.randint(0, 2, 50)\n-    batch_size = 10\n-    n_samples = 50\n-    good_batches = 3\n-\n-    @threadsafe_generator\n-    def custom_generator():\n-        \"\"\"Raises an exception after a few good batches\"\"\"\n-        for i in range(good_batches):\n-            batch_index = np.random.randint(0, n_samples - batch_size)\n-            start = batch_index\n-            end = start + batch_size\n-            X = arr_data[start: end]\n-            y = arr_labels[start: end]\n-            yield X, y\n-        raise RuntimeError\n-\n-    model = Sequential()\n-    model.add(Dense(1, input_shape=(2,)))\n-    model.compile(loss='mse', optimizer='adadelta')\n-\n-    samples = batch_size * (good_batches + 1)\n-\n-    # - Produce data on 4 worker threads, consume on main thread:\n-    #   - All worker threads share the SAME generator\n-    #   - Make sure `RuntimeError` exception bubbles up\n-    with pytest.raises(RuntimeError):\n-        model.fit_generator(custom_generator(),\n-                            steps_per_epoch=samples,\n-                            validation_steps=None,\n-                            max_queue_size=10,\n-                            workers=WORKERS,\n-                            use_multiprocessing=False)\n-\n-    # - Produce data on 1 worker thread, consume on main thread:\n-    #   - Worker thread is the only thread running the generator\n-    #   - Make sure `RuntimeError` exception bubbles up\n-    with pytest.raises(RuntimeError):\n-        model.fit_generator(custom_generator(),\n-                            steps_per_epoch=samples,\n-                            validation_steps=None,\n-                            max_queue_size=10,\n-                            workers=1,\n-                            use_multiprocessing=False)\n-\n-    # - Produce and consume data without a queue on main thread\n-    #   - Make sure the value of `use_multiprocessing` is ignored\n-    #   - Make sure `RuntimeError` exception bubbles up\n-    with pytest.raises(RuntimeError):\n-        model.fit_generator(custom_generator(),\n-                            steps_per_epoch=samples,\n-                            validation_steps=None,\n-                            max_queue_size=10,\n-                            workers=0,\n-                            use_multiprocessing=False)\n-\n-\n-@skip_generators\n-def test_multiprocessing_evaluate_error():\n-    arr_data = np.random.randint(0, 256, (50, 2))\n-    arr_labels = np.random.randint(0, 2, 50)\n-    batch_size = 10\n-    n_samples = 50\n-    good_batches = 3\n-\n-    @threadsafe_generator\n-    def custom_generator():\n-        \"\"\"Raises an exception after a few good batches\"\"\"\n-        for i in range(good_batches):\n-            batch_index = np.random.randint(0, n_samples - batch_size)\n-            start = batch_index\n-            end = start + batch_size\n-            X = arr_data[start: end]\n-            y = arr_labels[start: end]\n-            yield X, y\n-        raise RuntimeError\n-\n-    model = Sequential()\n-    model.add(Dense(1, input_shape=(2,)))\n-    model.compile(loss='mse', optimizer='adadelta')\n-\n-    # - Produce data on 4 worker processes, consume on main process:\n-    #   - Each worker process runs OWN copy of generator\n-    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n-    #     process boundaries -> make sure `evaluate_generator()` raises ValueError\n-    #     exception and does not attempt to run the generator.\n-    #   - On other platforms, make sure `RuntimeError` exception bubbles up\n-    if os.name == 'nt':\n-        with pytest.raises(ValueError):\n-            model.evaluate_generator(custom_generator(),\n-                                     steps=good_batches * WORKERS + 1,\n-                                     max_queue_size=10,\n-                                     workers=WORKERS,\n-                                     use_multiprocessing=True)\n-    else:\n-        with pytest.raises(RuntimeError):\n-            model.evaluate_generator(custom_generator(),\n-                                     steps=good_batches * WORKERS + 1,\n-                                     max_queue_size=10,\n-                                     workers=WORKERS,\n-                                     use_multiprocessing=True)\n-\n-    # - Produce data on 1 worker process, consume on main process:\n-    #   - Worker process runs generator\n-    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n-    #     process boundaries -> make sure `evaluate_generator()` raises ValueError\n-    #     exception and does not attempt to run the generator.\n-    #   - On other platforms, make sure `RuntimeError` exception bubbles up\n-    if os.name == 'nt':\n-        with pytest.raises(RuntimeError):\n-            model.evaluate_generator(custom_generator(),\n-                                     steps=good_batches + 1,\n-                                     max_queue_size=10,\n-                                     workers=1,\n-                                     use_multiprocessing=True)\n-    else:\n-        with pytest.raises(RuntimeError):\n-            model.evaluate_generator(custom_generator(),\n-                                     steps=good_batches + 1,\n-                                     max_queue_size=10,\n-                                     workers=1,\n-                                     use_multiprocessing=True)\n-\n-    # - Produce and consume data without a queue on main thread\n-    #   - Make sure the value of `use_multiprocessing` is ignored\n-    #   - Make sure `RuntimeError` exception bubbles up\n-    with pytest.raises(RuntimeError):\n-        model.evaluate_generator(custom_generator(),\n-                                 steps=good_batches + 1,\n-                                 max_queue_size=10,\n-                                 workers=0,\n-                                 use_multiprocessing=True)\n-\n-\n-def test_multithreading_evaluate_error():\n-    arr_data = np.random.randint(0, 256, (50, 2))\n-    arr_labels = np.random.randint(0, 2, 50)\n-    batch_size = 10\n-    n_samples = 50\n-    good_batches = 3\n-\n-    @threadsafe_generator\n-    def custom_generator():\n-        \"\"\"Raises an exception after a few good batches\"\"\"\n-        for i in range(good_batches):\n-            batch_index = np.random.randint(0, n_samples - batch_size)\n-            start = batch_index\n-            end = start + batch_size\n-            X = arr_data[start: end]\n-            y = arr_labels[start: end]\n-            yield X, y\n-        raise RuntimeError\n-\n-    model = Sequential()\n-    model.add(Dense(1, input_shape=(2,)))\n-    model.compile(loss='mse', optimizer='adadelta')\n-\n-    # - Produce data on 4 worker threads, consume on main thread:\n-    #   - All worker threads share the SAME generator\n-    #   - Make sure `RuntimeError` exception bubbles up\n-    with pytest.raises(RuntimeError):\n-        model.evaluate_generator(custom_generator(),\n-                                 steps=good_batches * WORKERS + 1,\n-                                 max_queue_size=10,\n-                                 workers=WORKERS,\n-                                 use_multiprocessing=False)\n-\n-    # - Produce data on 1 worker thread, consume on main thread:\n-    #   - Worker thread is the only thread running the generator\n-    #   - Make sure `RuntimeError` exception bubbles up\n-    with pytest.raises(RuntimeError):\n-        model.evaluate_generator(custom_generator(),\n-                                 steps=good_batches + 1,\n-                                 max_queue_size=10,\n-                                 workers=1,\n-                                 use_multiprocessing=False)\n-\n-    # - Produce and consume data without a queue on main thread\n-    #   - Make sure the value of `use_multiprocessing` is ignored\n-    #   - Make sure `RuntimeError` exception bubbles up\n-    with pytest.raises(RuntimeError):\n-        model.evaluate_generator(custom_generator(),\n-                                 steps=good_batches + 1,\n-                                 max_queue_size=10,\n-                                 workers=0,\n-                                 use_multiprocessing=False)\n-\n-\n-@skip_generators\n-def test_multiprocessing_predict_error():\n-    arr_data = np.random.randint(0, 256, (50, 2))\n-    good_batches = 3\n-\n-    @threadsafe_generator\n-    def custom_generator():\n-        \"\"\"Raises an exception after a few good batches\"\"\"\n-        batch_size = 10\n-        n_samples = 50\n-\n-        for i in range(good_batches):\n-            batch_index = np.random.randint(0, n_samples - batch_size)\n-            start = batch_index\n-            end = start + batch_size\n-            X = arr_data[start: end]\n-            yield X\n-        raise RuntimeError\n-\n-    model = Sequential()\n-    model.add(Dense(1, input_shape=(2,)))\n-    model.compile(loss='mse', optimizer='adadelta')\n-\n-    # - Produce data on 4 worker processes, consume on main process:\n-    #   - Each worker process runs OWN copy of generator\n-    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n-    #     process boundaries -> make sure `predict_generator()` raises ValueError\n-    #     exception and does not attempt to run the generator.\n-    #   - On other platforms, make sure `RuntimeError` exception bubbles up\n-    if os.name == 'nt':\n-        with pytest.raises(StopIteration):\n-            model.predict_generator(custom_generator(),\n-                                    steps=good_batches * WORKERS + 1,\n-                                    max_queue_size=10,\n-                                    workers=WORKERS,\n-                                    use_multiprocessing=True)\n-    else:\n-        with pytest.raises(RuntimeError):\n-            model.predict_generator(custom_generator(),\n-                                    steps=good_batches * WORKERS + 1,\n-                                    max_queue_size=10,\n-                                    workers=WORKERS,\n-                                    use_multiprocessing=True)\n-\n-    # - Produce data on 1 worker process, consume on main process:\n-    #   - Worker process runs generator\n-    #   - BUT on Windows, `multiprocessing` won't marshall generators across\n-    #     process boundaries -> make sure `predict_generator()` raises ValueError\n-    #     exception and does not attempt to run the generator.\n-    #   - On other platforms, make sure `RuntimeError` exception bubbles up\n-    if os.name == 'nt':\n-        with pytest.raises(RuntimeError):\n-            model.predict_generator(custom_generator(),\n-                                    steps=good_batches + 1,\n-                                    max_queue_size=10,\n-                                    workers=1,\n-                                    use_multiprocessing=True)\n-    else:\n-        with pytest.raises(RuntimeError):\n-            model.predict_generator(custom_generator(),\n-                                    steps=good_batches + 1,\n-                                    max_queue_size=10,\n-                                    workers=1,\n-                                    use_multiprocessing=True)\n-\n-    # - Produce and consume data without a queue on main thread\n-    #   - Make sure the value of `use_multiprocessing` is ignored\n-    #   - Make sure `RuntimeError` exception bubbles up\n-    with pytest.raises(RuntimeError):\n-        model.predict_generator(custom_generator(),\n-                                steps=good_batches + 1,\n-                                max_queue_size=10,\n-                                workers=0,\n-                                use_multiprocessing=True)\n-\n-\n-def test_multithreading_predict_error():\n-    arr_data = np.random.randint(0, 256, (50, 2))\n-    good_batches = 3\n-\n-    @threadsafe_generator\n-    def custom_generator():\n-        \"\"\"Raises an exception after a few good batches\"\"\"\n-        batch_size = 10\n-        n_samples = 50\n-\n-        for i in range(good_batches):\n-            batch_index = np.random.randint(0, n_samples - batch_size)\n-            start = batch_index\n-            end = start + batch_size\n-            X = arr_data[start: end]\n-            yield X\n-        raise RuntimeError\n-\n-    model = Sequential()\n-    model.add(Dense(1, input_shape=(2,)))\n-    model.compile(loss='mse', optimizer='adadelta')\n-\n-    # - Produce data on 4 worker threads, consume on main thread:\n-    #   - All worker threads share the SAME generator\n-    #   - Make sure `RuntimeError` exception bubbles up\n-    with pytest.raises(RuntimeError):\n-        model.predict_generator(custom_generator(),\n-                                steps=good_batches * WORKERS + 1,\n-                                max_queue_size=10,\n-                                workers=WORKERS,\n-                                use_multiprocessing=False)\n-    # - Produce data on 1 worker thread, consume on main thread:\n-    #   - Worker thread is the only thread running the generator\n-    #   - Make sure `RuntimeError` exception bubbles up\n-    with pytest.raises(RuntimeError):\n-        model.predict_generator(custom_generator(),\n-                                steps=good_batches + 1,\n-                                max_queue_size=10,\n-                                workers=1,\n-                                use_multiprocessing=False)\n-\n-    # - Produce and consume data without a queue on main thread\n-    #   - Make sure the value of `use_multiprocessing` is ignored\n-    #   - Make sure `RuntimeError` exception bubbles up\n-    with pytest.raises(RuntimeError):\n-        model.predict_generator(custom_generator(),\n-                                steps=good_batches + 1,\n-                                max_queue_size=10,\n-                                workers=0,\n-                                use_multiprocessing=False)\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n@@ -1,15 +0,0 @@\n-import pyux\n-import keras\n-import json\n-\n-\n-import keras.backend.tensorflow_backend\n-import keras.backend.theano_backend\n-import keras.backend.cntk_backend\n-import keras.backend.numpy_backend\n-import keras.utils.test_utils\n-\n-sign = pyux.sign(keras)\n-\n-with open('api.json', 'w') as f:\n-    json.dump(sign, f)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
