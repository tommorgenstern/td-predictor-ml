{"custom_id": "keras#6ac9af0a5a38368926e1276a8162affd4e0cc0e3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 2197 | Contributors (this commit): 28 | Commits (past 90d): 14 | Contributors (cumulative): 28 | DMM Complexity: None\n\nDIFF:\n@@ -258,10 +258,12 @@ def collect_trainable_weights(layer):\n         weights += layer.trainable_weights\n     # dedupe weights\n     weights = list(set(weights))\n-    # TF variables have auto-generated the name, while Theano has auto-generated the auto_name variable. name in Theano is None\n+    # TF variables have auto-generated the name, while Theano has auto-generated the auto_name variable.\n+    # name in Theano is sometimes None.\n+    # However, to work save_model() and load_model() properly, weights must be sorted by names.\n     if weights:\n         if K.backend() == 'theano':\n-            weights.sort(key=lambda x: x.auto_name)\n+            weights.sort(key=lambda x: x.name if x.name else x.auto_name)\n         else:\n             weights.sort(key=lambda x: x.name)\n     return weights\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#94fba3d8f05c65799e21198f845051f82b30416d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 6824 | Contributors (this commit): 33 | Commits (past 90d): 33 | Contributors (cumulative): 43 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1668,6 +1668,7 @@ class Cropping1D(Layer):\n \n     def build(self, input_shape):\n         self.input_spec = [InputSpec(shape=input_shape)]\n+        self.built = True\n \n     def get_output_shape_for(self, input_shape):\n         length = input_shape[1] - self.cropping[0] - self.cropping[1] if input_shape[1] is not None else None\n@@ -1684,6 +1685,7 @@ class Cropping1D(Layer):\n         base_config = super(Cropping1D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n+\n class Cropping2D(Layer):\n     '''Cropping layer for 2D input (e.g. picture).\n     It crops along spatial dimensions, i.e. width and height.\n@@ -1736,6 +1738,7 @@ class Cropping2D(Layer):\n \n     def build(self, input_shape):\n         self.input_spec = [InputSpec(shape=input_shape)]\n+        self.built = True\n \n     def get_output_shape_for(self, input_shape):\n         if self.dim_ordering == 'th':\n@@ -1769,6 +1772,7 @@ class Cropping2D(Layer):\n         base_config = super(Cropping2D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n+\n class Cropping3D(Layer):\n     '''Cropping layer for 3D data (e.g. spatial or saptio-temporal).\n \n@@ -1808,6 +1812,7 @@ class Cropping3D(Layer):\n \n     def build(self, input_shape):\n         self.input_spec = [InputSpec(shape=input_shape)]\n+        self.built = True\n \n     def get_output_shape_for(self, input_shape):\n         if self.dim_ordering == 'th':\n\n@@ -451,8 +451,8 @@ def test_zero_padding_2d():\n \n     # correctness test\n     layer = convolutional.ZeroPadding2D(padding=(2, 2))\n-    output = layer(K.variable(input))\n     layer.build(input.shape)\n+    output = layer(K.variable(input))\n     np_output = K.eval(output)\n     if dim_ordering == 'tf':\n         for offset in [0, 1, -1, -2]:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#789a2be8d9338b4ad2e47bfba3d628afd8e84134", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 3954 | Contributors (this commit): 25 | Commits (past 90d): 19 | Contributors (cumulative): 25 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1853,11 +1853,13 @@ class Container(Layer):\n         # based on layer names, because names can potentially\n         # be changed at any point by the user\n         # without the container being notified of it.\n-        if index:\n+        if index is not None:\n             if len(self.layers) <= index:\n                 raise Exception('Was asked to retrieve layer at index ' +\n                                 str(index) + ' but model only has ' +\n                                 str(len(self.layers)) + ' layers.')\n+            else:\n+                return self.layers[index]\n         else:\n             assert name, 'Provide either a layer name or layer index.'\n         layer = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fdd150eb4d60e46b0c6716d5cb5b4e034203682f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 9 | Churn Cumulative: 4184 | Contributors (this commit): 31 | Commits (past 90d): 21 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -307,8 +307,7 @@ class Layer(object):\n                           'batch_input_shape',\n                           'input_dtype',\n                           'name',\n-                          'trainable',\n-                          'create_input_layer'}\n+                          'trainable'}\n         for kwarg in kwargs.keys():\n             assert kwarg in allowed_kwargs, 'Keyword argument not understood: ' + kwarg\n \n@@ -329,8 +328,6 @@ class Layer(object):\n             self.batch_input_shape = batch_input_shape\n             input_dtype = kwargs.get('input_dtype', K.floatx())\n             self.input_dtype = input_dtype\n-            if 'create_input_layer' in kwargs:\n-                self.create_input_layer(batch_input_shape, input_dtype)\n \n     @property\n     def trainable_weights(self):\n\n@@ -1,6 +1,7 @@\n from __future__ import absolute_import\n import numpy as np\n from . import backend as K\n+from .utils.generic_utils import get_from_module\n \n \n def get_fans(shape, dim_ordering='th'):\n@@ -20,7 +21,7 @@ def get_fans(shape, dim_ordering='th'):\n             fan_in = shape[-2] * receptive_field_size\n             fan_out = shape[-1] * receptive_field_size\n         else:\n-            raise Exception('Invalid dim_ordering: ' + dim_ordering)\n+            raise ValueError('Invalid dim_ordering: ' + dim_ordering)\n     else:\n         # no specific assumptions\n         fan_in = np.sqrt(np.prod(shape))\n@@ -101,7 +102,6 @@ def one(shape, name=None):\n     return K.ones(shape, name=name)\n \n \n-from .utils.generic_utils import get_from_module\n def get(identifier, **kwargs):\n     return get_from_module(identifier, globals(),\n                            'initialization', kwargs=kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#016d85c9e6d8a36fe7107e32752f6a9cd8d77c86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 121 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n from __future__ import absolute_import\n from . import backend as K\n+from .utils.generic_utils import get_from_module\n \n \n def softmax(x):\n@@ -11,13 +12,15 @@ def softmax(x):\n         s = K.sum(e, axis=-1, keepdims=True)\n         return e / s\n     else:\n-        raise Exception('Cannot apply softmax to a tensor that is not 2D or 3D. ' +\n+        raise ValueError('Cannot apply softmax to a tensor '\n+                         'that is not 2D or 3D. '\n                          'Here, ndim=' + str(ndim))\n \n \n def elu(x, alpha=1.0):\n     return K.elu(x, alpha)\n \n+\n def softplus(x):\n     return K.softplus(x)\n \n@@ -43,13 +46,9 @@ def hard_sigmoid(x):\n \n \n def linear(x):\n-    '''\n-    The function returns the variable that is passed in, so all types work.\n-    '''\n     return x\n \n \n-from .utils.generic_utils import get_from_module\n def get(identifier):\n     if identifier is None:\n         return linear\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c4c4fac1aed6ccc84d2f21028ebdfd03213709aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 19 | Files Changed: 4 | Hunks: 12 | Methods Changed: 10 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 66 | Churn Cumulative: 17101 | Contributors (this commit): 95 | Commits (past 90d): 46 | Contributors (cumulative): 123 | DMM Complexity: 0.7647058823529411\n\nDIFF:\n@@ -282,6 +282,9 @@ class Layer(object):\n         if not hasattr(self, 'uses_learning_phase'):\n             self.uses_learning_phase = False\n \n+        # Per-input updates.\n+        self._per_input_updates = {}\n+\n         # These lists will be filled via successive calls\n         # to self.add_inbound_node().\n         self.inbound_nodes = []\n@@ -799,6 +802,26 @@ class Layer(object):\n                             'ill-defined for the layer. ' +\n                             'Use `get_output_shape_at(node_index)` instead.')\n \n+    def add_updates(self, updates, inputs):\n+        # Update self.updates\n+        if not hasattr(self, 'updates'):\n+            self.updates = []\n+        self.updates += updates\n+        # Update self._per_input_updates\n+        inputs = to_list(inputs)\n+        updates = to_list(updates)\n+        inputs_hash = ', '.join([str(abs(id(x))) for x in inputs])\n+        if inputs_hash not in self._per_input_updates:\n+            self._per_input_updates[inputs_hash] = []\n+        self._per_input_updates[inputs_hash] += updates\n+\n+    def get_updates_for(self, inputs):\n+        inputs = to_list(inputs)\n+        inputs_hash = ', '.join([str(abs(id(x))) for x in inputs])\n+        if inputs_hash in self._per_input_updates:\n+            return self._per_input_updates[inputs_hash]\n+        return []\n+\n     @property\n     def weights(self):\n         return self.trainable_weights + self.non_trainable_weights\n@@ -1871,9 +1894,23 @@ class Container(Layer):\n         updates = []\n         for layer in self.layers:\n             if hasattr(layer, 'updates'):\n+                if len(layer.inbound_nodes) == 1:\n                     updates += layer.updates\n+                else:\n+                    for node_index, node in enumerate(layer.inbound_nodes):\n+                        node_key = layer.name + '_ib-' + str(node_index)\n+                        if node_key in self.container_nodes:\n+                            # The model owns this layer node.\n+                            inputs = node.input_tensors\n+                            updates += layer.get_updates_for(inputs)\n         return updates\n \n+    def get_updates_for(self, inputs):\n+        # In this case, returns model updates,\n+        # since a model cannot have inputs-specific updates\n+        # (only atomic layers can).\n+        return self.updates\n+\n     @property\n     def stateful(self):\n         return any([(hasattr(layer, 'stateful') and layer.stateful) for layer in self.layers])\n\n@@ -104,7 +104,6 @@ class BatchNormalization(Layer):\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n         self.built = True\n-        self.called_with = None\n \n     def call(self, x, mask=None):\n         if self.mode == 0 or self.mode == 2:\n@@ -122,23 +121,12 @@ class BatchNormalization(Layer):\n                     epsilon=self.epsilon)\n             else:\n                 # mode 0\n-                if self.called_with not in {None, x}:\n-                    raise Exception('You are attempting to share a '\n-                                    'same `BatchNormalization` layer across '\n-                                    'different data flows. '\n-                                    'This is not possible. '\n-                                    'You should use `mode=2` in '\n-                                    '`BatchNormalization`, which has '\n-                                    'a similar behavior but is shareable '\n-                                    '(see docs for a description of '\n-                                    'the behavior).')\n-                self.called_with = x\n                 x_normed, mean, std = K.normalize_batch_in_training(\n                     x, self.gamma, self.beta, reduction_axes,\n                     epsilon=self.epsilon)\n \n-                self.updates = [K.moving_average_update(self.running_mean, mean, self.momentum),\n-                                K.moving_average_update(self.running_std, std, self.momentum)]\n+                self.add_updates([K.moving_average_update(self.running_mean, mean, self.momentum),\n+                                  K.moving_average_update(self.running_std, std, self.momentum)], x)\n \n                 if K.backend() == 'tensorflow' and sorted(reduction_axes) == range(K.ndim(x))[:-1]:\n                     x_normed_running = K.batch_normalization(\n\n@@ -226,9 +226,10 @@ class Recurrent(Layer):\n                                              unroll=self.unroll,\n                                              input_length=input_shape[1])\n         if self.stateful:\n-            self.updates = []\n+            updates = []\n             for i in range(len(states)):\n-                self.updates.append((self.states[i], states[i]))\n+                updates.append((self.states[i], states[i]))\n+            self.add_updates(updates, x)\n \n         if self.return_sequences:\n             return outputs\n\n@@ -473,13 +473,15 @@ class Sequential(Model):\n \n     @property\n     def updates(self):\n-        # support for legacy behavior\n-        return self._gather_list_attr('updates')\n+        return self.model.updates\n \n     @property\n     def state_updates(self):\n         # support for legacy behavior\n-        return self._gather_list_attr('state_updates')\n+        return self.model.state_updates\n+\n+    def get_updates_for(self, inputs):\n+        return self.model.get_updates_for(inputs)\n \n     @property\n     def regularizers(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8d20bac7fa534383fde1054334110691a11735c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 99 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -54,7 +54,6 @@ model.add(LSTM(50,\n                return_sequences=True,\n                stateful=True))\n model.add(LSTM(50,\n-               batch_input_shape=(batch_size, tsteps, 1),\n                return_sequences=False,\n                stateful=True))\n model.add(Dense(1))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#771010f43b5b4f26e18c78449d6b42449e294269", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 52 | Churn Cumulative: 4247 | Contributors (this commit): 27 | Commits (past 90d): 23 | Contributors (cumulative): 28 | DMM Complexity: 0.30434782608695654\n\nDIFF:\n@@ -282,9 +282,6 @@ class Layer(object):\n         if not hasattr(self, 'uses_learning_phase'):\n             self.uses_learning_phase = False\n \n-        # Per-input updates.\n-        self._per_input_updates = {}\n-\n         # These lists will be filled via successive calls\n         # to self.add_inbound_node().\n         self.inbound_nodes = []\n@@ -806,8 +803,13 @@ class Layer(object):\n         # Update self.updates\n         if not hasattr(self, 'updates'):\n             self.updates = []\n+        try:\n             self.updates += updates\n+        except AttributeError:\n+            pass\n         # Update self._per_input_updates\n+        if not hasattr(self, '_per_input_updates'):\n+            self._per_input_updates = {}\n         inputs = to_list(inputs)\n         updates = to_list(updates)\n         inputs_hash = ', '.join([str(abs(id(x))) for x in inputs])\n@@ -816,6 +818,8 @@ class Layer(object):\n         self._per_input_updates[inputs_hash] += updates\n \n     def get_updates_for(self, inputs):\n+        if not hasattr(self, '_per_input_updates'):\n+            return []\n         inputs = to_list(inputs)\n         inputs_hash = ', '.join([str(abs(id(x))) for x in inputs])\n         if inputs_hash in self._per_input_updates:\n@@ -1905,12 +1909,6 @@ class Container(Layer):\n                             updates += layer.get_updates_for(inputs)\n         return updates\n \n-    def get_updates_for(self, inputs):\n-        # In this case, returns model updates,\n-        # since a model cannot have inputs-specific updates\n-        # (only atomic layers can).\n-        return self.updates\n-\n     @property\n     def stateful(self):\n         return any([(hasattr(layer, 'stateful') and layer.stateful) for layer in self.layers])\n@@ -2198,6 +2196,10 @@ class Container(Layer):\n                         output_tensors = to_list(layer.call(computed_tensors, computed_masks))\n                         output_masks = to_list(layer.compute_mask(computed_tensors, computed_masks))\n \n+                    # update model updates\n+                    layer_inputs = [x[0] for x in computed_data]\n+                    self.add_updates(layer.get_updates_for(layer_inputs), inputs)\n+\n                     # Update _keras_shape.\n                     if all([hasattr(x, '_keras_shape') for x in computed_tensors]):\n                         if len(computed_tensors) == 1:\n\n@@ -2,10 +2,10 @@ import pytest\n import numpy as np\n from numpy.testing import assert_allclose\n \n-from keras.layers.core import Dense, Activation\n+from keras.layers import Dense, Activation, Input\n from keras.utils.test_utils import layer_test, keras_test\n from keras.layers import normalization\n-from keras.models import Sequential\n+from keras.models import Sequential, Model\n from keras import backend as K\n \n input_1 = np.arange(10)\n@@ -78,5 +78,33 @@ def test_batchnorm_mode_1():\n             assert_allclose(K.eval(K.std(out)), 0.0, atol=1e-1)\n \n \n+@keras_test\n+def test_shared_batchnorm():\n+    '''Test that a BN layer can be shared\n+    across different data streams.\n+    '''\n+    # Test single layer reuse\n+    bn = normalization.BatchNormalization(input_shape=(10,), mode=0)\n+    x1 = Input(shape=(10,))\n+    bn(x1)\n+\n+    x2 = Input(shape=(10,))\n+    y2 = bn(x2)\n+\n+    x = np.random.normal(loc=5.0, scale=10.0, size=(2, 10))\n+    model = Model(x2, y2)\n+    assert len(model.updates) == 2\n+    model.compile('sgd', 'mse')\n+    model.train_on_batch(x, x)\n+\n+    # Test model-level reuse\n+    x3 = Input(shape=(10,))\n+    y3 = model(x3)\n+    new_model = Model(x3, y3)\n+    assert len(model.updates) == 2\n+    new_model.compile('sgd', 'mse')\n+    new_model.train_on_batch(x, x)\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#04ea01f3857f83e01590991f8df8475af03aa4a0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 17 | Churn Cumulative: 5041 | Contributors (this commit): 31 | Commits (past 90d): 18 | Contributors (cumulative): 40 | DMM Complexity: 1.0\n\nDIFF:\n@@ -242,7 +242,7 @@ class Recurrent(Layer):\n                   'stateful': self.stateful,\n                   'unroll': self.unroll,\n                   'consume_less': self.consume_less}\n-        if self.stateful:\n+        if self.stateful and self.input_spec[0].shape:\n             config['batch_input_shape'] = self.input_spec[0].shape\n         else:\n             config['input_dim'] = self.input_dim\n\n@@ -157,5 +157,13 @@ def test_masking_layer():\n     model.fit(I, V, nb_epoch=1, batch_size=100, verbose=1)\n \n \n+@rnn_test\n+def test_from_config(layer_class):\n+    for stateful in (False, True):\n+        l1 = layer_class(output_dim=1, stateful=stateful)\n+        l2 = layer_class.from_config(l1.get_config())\n+        assert l1.get_config() == l2.get_config()\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n@@ -115,6 +115,13 @@ def test_Bidirectional():\n         model.compile(loss='mse', optimizer='sgd')\n         model.fit(x, y, nb_epoch=1, batch_size=1)\n \n+        # Bidirectional and stateful\n+        input = Input(batch_shape=(1, timesteps, dim))\n+        output = wrappers.Bidirectional(rnn(output_dim, stateful=True), merge_mode=mode)(input)\n+        model = Model(input, output)\n+        model.compile(loss='mse', optimizer='sgd')\n+        model.fit(x, y, nb_epoch=1, batch_size=1)\n+\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6b04add93209f557e265bbd04ab34d5491d463f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 33 | Churn Cumulative: 2518 | Contributors (this commit): 29 | Commits (past 90d): 13 | Contributors (cumulative): 32 | DMM Complexity: 0.9285714285714286\n\nDIFF:\n@@ -186,13 +186,12 @@ def check_array_lengths(X, Y, W):\n \n \n def check_loss_and_target_compatibility(targets, losses, output_shapes):\n-    assert len(targets) == len(losses) == len(output_shapes)\n     key_losses = {'mean_square_error',\n                   'binary_crossentropy',\n                   'categorical_crossentropy'}\n     for y, loss, shape in zip(targets, losses, output_shapes):\n         if loss.__name__ == 'categorical_crossentropy':\n-            if y.shape[1] == 1:\n+            if y.shape[-1] == 1:\n                 raise Exception('You are passing a target array of shape ' + str(y.shape) +\n                                 ' while using as loss `categorical_crossentropy`. '\n                                 '`categorical_crossentropy` expects '\n@@ -208,7 +207,9 @@ def check_loss_and_target_compatibility(targets, losses, output_shapes):\n                                 'Alternatively, you can use the loss function '\n                                 '`sparse_categorical_crossentropy` instead, '\n                                 'which does expect integer targets.')\n-        if loss.__name__ in key_losses and shape[1] is not None and y.shape[1] != shape[1]:\n+        if loss.__name__ in key_losses:\n+            for target_dim, out_dim in zip(y.shape[1:], shape[1:]):\n+                if target_dim is not None and target_dim != out_dim:\n                     raise Exception('A target array with shape ' + str(y.shape) +\n                                     ' was passed for an output of shape ' + str(shape) +\n                                     ' while using as loss `' + loss.__name__ + '`. '\n\n@@ -4,7 +4,7 @@ from numpy.testing import assert_allclose\n \n from keras.layers import Dense, Dropout\n from keras.engine.topology import merge, Input\n-from keras.engine.training import Model\n+from keras.engine.training import Model, check_loss_and_target_compatibility\n from keras.models import Sequential\n from keras import backend as K\n from keras.utils.test_utils import keras_test\n@@ -202,5 +202,29 @@ def test_trainable_argument():\n     assert_allclose(out, out_2)\n \n \n+@keras_test\n+def test_check_not_last_is_one():\n+    a = np.random.random((2, 1, 3))\n+    check_loss_and_target_compatibility([a], [K.categorical_crossentropy], [a.shape])\n+\n+\n+@keras_test\n+def test_check_last_is_one():\n+    a = np.random.random((2, 3, 1))\n+    with pytest.raises(Exception) as exc:\n+        check_loss_and_target_compatibility([a], [K.categorical_crossentropy], [a.shape])\n+\n+    assert \"You are passing a target array\" in str(exc)\n+\n+\n+@keras_test\n+def test_check_bad_shape():\n+    a = np.random.random((2, 3, 5))\n+    with pytest.raises(Exception) as exc:\n+        check_loss_and_target_compatibility([a], [K.categorical_crossentropy], [(2, 3, 6)])\n+\n+    assert \"targets to have the same shape\" in str(exc)\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#97484ec9c13f44907642c12bd8c9d6091ae099f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 36 | Churn Cumulative: 12718 | Contributors (this commit): 63 | Commits (past 90d): 19 | Contributors (cumulative): 75 | DMM Complexity: 1.0\n\nDIFF:\n@@ -139,6 +139,7 @@ PAGES = [\n             core.Dense,\n             core.Activation,\n             core.Dropout,\n+            core.SpatialDropout1D,\n             core.SpatialDropout2D,\n             core.SpatialDropout3D,\n             core.Flatten,\n\n@@ -96,6 +96,37 @@ class Dropout(Layer):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n+class SpatialDropout1D(Dropout):\n+    '''This version performs the same function as Dropout, however it drops\n+    entire 1D feature maps instead of individual elements. If adjacent frames\n+    within feature maps are strongly correlated (as is normally the case in\n+    early convolution layers) then regular dropout will not regularize the\n+    activations and will otherwise just result in an effective learning rate\n+    decrease. In this case, SpatialDropout1D will help promote independence\n+    between feature maps and should be used instead.\n+\n+    # Arguments\n+        p: float between 0 and 1. Fraction of the input units to drop.\n+\n+    # Input shape\n+        3D tensor with shape:\n+        `(samples, timesteps, channels)`\n+\n+    # Output shape\n+        Same as input\n+\n+    # References\n+        - [Efficient Object Localization Using Convolutional Networks](https://arxiv.org/pdf/1411.4280.pdf)\n+    '''\n+    def __init__(self, p, **kwargs):\n+        super(SpatialDropout1D, self).__init__(p, **kwargs)\n+\n+    def _get_noise_shape(self, x):\n+        input_shape = K.shape(x)\n+        noise_shape = (input_shape[0], 1, input_shape[2])\n+        return noise_shape\n+    \n+    \n class SpatialDropout2D(Dropout):\n     '''This version performs the same function as Dropout, however it drops\n     entire 2D feature maps instead of individual elements. If adjacent pixels\n\n@@ -153,6 +153,10 @@ def test_dropout():\n                kwargs={'p': 0.5},\n                input_shape=(3, 2))\n \n+    layer_test(core.SpatialDropout1D,\n+               kwargs={'p': 0.5},\n+               input_shape=(2, 3, 4))\n+    \n     layer_test(core.SpatialDropout2D,\n                kwargs={'p': 0.5},\n                input_shape=(2, 3, 4, 5))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#06cc6d7fea7527e99e36c9fc766390c51e73ebba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 11 | Methods Changed: 9 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 43 | Churn Cumulative: 2561 | Contributors (this commit): 30 | Commits (past 90d): 15 | Contributors (cumulative): 34 | DMM Complexity: 0.9\n\nDIFF:\n@@ -760,7 +760,7 @@ class Model(Container):\n     def _fit_loop(self, f, ins, out_labels=[], batch_size=32,\n                   nb_epoch=100, verbose=1, callbacks=[],\n                   val_f=None, val_ins=None, shuffle=True,\n-                  callback_metrics=[]):\n+                  callback_metrics=[], initial_epoch=0):\n         '''Abstract fit function for f(ins).\n         Assume that f returns a list, labeled by out_labels.\n \n@@ -780,6 +780,8 @@ class Model(Container):\n                 passed to the callbacks. They should be the\n                 concatenation of list the display names of the outputs of\n                  `f` and the list of display names of the outputs of `f_val`.\n+            initial_epoch: epoch at which to start training\n+                (useful for resuming a previous training run)\n \n         # Returns\n             `History` object.\n@@ -820,7 +822,7 @@ class Model(Container):\n         callback_model.stop_training = False\n         self.validation_data = val_ins\n \n-        for epoch in range(nb_epoch):\n+        for epoch in range(initial_epoch, nb_epoch):\n             callbacks.on_epoch_begin(epoch)\n             if shuffle == 'batch':\n                 index_array = batch_shuffle(index_array, batch_size)\n@@ -1007,7 +1009,7 @@ class Model(Container):\n \n     def fit(self, x, y, batch_size=32, nb_epoch=10, verbose=1, callbacks=[],\n             validation_split=0., validation_data=None, shuffle=True,\n-            class_weight=None, sample_weight=None):\n+            class_weight=None, sample_weight=None, initial_epoch=0):\n         '''Trains the model for a fixed number of epochs (iterations on a dataset).\n \n         # Arguments\n@@ -1044,6 +1046,8 @@ class Model(Container):\n                 with shape (samples, sequence_length),\n                 to apply a different weight to every timestep of every sample.\n                 In this case you should make sure to specify sample_weight_mode=\"temporal\" in compile().\n+            initial_epoch: epoch at which to start training\n+                (useful for resuming a previous training run)\n \n \n         # Returns\n@@ -1127,7 +1131,8 @@ class Model(Container):\n                               batch_size=batch_size, nb_epoch=nb_epoch,\n                               verbose=verbose, callbacks=callbacks,\n                               val_f=val_f, val_ins=val_ins, shuffle=shuffle,\n-                              callback_metrics=callback_metrics)\n+                              callback_metrics=callback_metrics,\n+                              initial_epoch=initial_epoch)\n \n     def evaluate(self, x, y, batch_size=32, verbose=1, sample_weight=None):\n         '''Returns the loss value and metrics values for the model\n@@ -1303,7 +1308,8 @@ class Model(Container):\n     def fit_generator(self, generator, samples_per_epoch, nb_epoch,\n                       verbose=1, callbacks=[],\n                       validation_data=None, nb_val_samples=None,\n-                      class_weight={}, max_q_size=10, nb_worker=1, pickle_safe=False):\n+                      class_weight={}, max_q_size=10, nb_worker=1, pickle_safe=False,\n+                      initial_epoch=0):\n         '''Fits the model on data generated batch-by-batch by\n         a Python generator.\n         The generator is run in parallel to the model, for efficiency.\n@@ -1339,6 +1345,8 @@ class Model(Container):\n                 this implementation relies on multiprocessing, you should not pass\n                 non picklable arguments to the generator as they can't be passed\n                 easily to children processes.\n+            initial_epoch: epoch at which to start training\n+                (useful for resuming a previous training run)\n \n         # Returns\n             A `History` object.\n@@ -1361,7 +1369,7 @@ class Model(Container):\n         ```\n         '''\n         wait_time = 0.01  # in seconds\n-        epoch = 0\n+        epoch = initial_epoch\n \n         do_validation = bool(validation_data)\n         self._make_train_function()\n\n@@ -8,6 +8,7 @@ from keras.engine.training import Model, check_loss_and_target_compatibility\n from keras.models import Sequential\n from keras import backend as K\n from keras.utils.test_utils import keras_test\n+from keras.callbacks import LambdaCallback\n \n \n @keras_test\n@@ -146,6 +147,28 @@ def test_model_methods():\n                               [output_a_np, output_b_np])\n     assert len(out) == 4\n \n+    # test starting from non-zero initial epoch\n+    trained_epochs = []\n+\n+    def on_epoch_begin(epoch, logs):\n+        trained_epochs.append(epoch)\n+    tracker_cb = LambdaCallback(on_epoch_begin=on_epoch_begin)\n+    out = model.fit([input_a_np, input_b_np],\n+                    [output_a_np, output_b_np], nb_epoch=5, batch_size=4,\n+                    initial_epoch=2, callbacks=[tracker_cb])\n+    assert trained_epochs == [2, 3, 4]\n+\n+    # test starting from non-zero initial epoch for generator too\n+    trained_epochs = []\n+\n+    def gen_data(batch_sz):\n+        while True:\n+            yield ([np.random.random((batch_sz, 3)), np.random.random((batch_sz, 3))],\n+                   [np.random.random((batch_sz, 4)), np.random.random((batch_sz, 3))])\n+    out = model.fit_generator(gen_data(4), samples_per_epoch=10, nb_epoch=5,\n+                              initial_epoch=2, callbacks=[tracker_cb])\n+    assert trained_epochs == [2, 3, 4]\n+\n     # test with a custom metric function\n     mse = lambda y_true, y_pred: K.mean(K.pow(y_true - y_pred, 2))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#709bc5e15ab015c874adcaa362aee04a39c0667c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 8 | Churn Cumulative: 3352 | Contributors (this commit): 42 | Commits (past 90d): 25 | Contributors (cumulative): 42 | DMM Complexity: 0.0\n\nDIFF:\n@@ -184,7 +184,11 @@ def variable(value, dtype=_FLOATX, name=None):\n \n \n def _initialize_variables():\n+    if hasattr(tf, 'global_variables'):\n+        variables = tf.global_variables()\n+    else:\n         variables = tf.all_variables()\n+\n     uninitialized_variables = []\n     for v in variables:\n         if not hasattr(v, '_keras_initialized') or not v._keras_initialized:\n@@ -192,9 +196,11 @@ def _initialize_variables():\n             v._keras_initialized = True\n     if uninitialized_variables:\n         sess = get_session()\n+        if hasattr(tf, 'variables_initializer'):\n+            sess.run(tf.variables_initializer(uninitialized_variables))\n+        else:\n             sess.run(tf.initialize_variables(uninitialized_variables))\n \n-\n def placeholder(shape=None, ndim=None, dtype=_FLOATX, sparse=False, name=None):\n     '''Instantiates a placeholder.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dad7790ec363979a1ac989a41564d87498156dcd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 679 | Contributors (this commit): 15 | Commits (past 90d): 3 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -53,6 +53,8 @@ def print_summary(layers, relevant_nodes=None, line_length=100, positions=[.33,\n     def print_row(fields, positions):\n         line = ''\n         for i in range(len(fields)):\n+            if i > 0:\n+                line = line[:-1] + ' '\n             line += str(fields[i])\n             line = line[:positions[i]]\n             line += ' ' * (positions[i] - len(line))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2878f606342d1ee8167e316ae9b0d3e2870d3caf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 143 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 9 | Complexity Δ (Sum/Max): 14/6 | Churn Δ: 143 | Churn Cumulative: 7400 | Contributors (this commit): 61 | Commits (past 90d): 48 | Contributors (cumulative): 105 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1980,3 +1980,52 @@ def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n                      for st in decoded]\n \n     return (decoded_dense, log_prob)\n+\n+\n+# HIGH ORDER FUNCTIONS\n+\n+def map_fn(fn, elems, name=None):\n+    '''Map the function fn over the elements elems and return the outputs.\n+\n+    # Arguments\n+        fn: Callable that will be called upon each element in elems\n+        elems: tensor\n+        name: A string name for the map node in the graph\n+\n+    # Returns\n+        Tensor with first dimension equal to the elems and second depending on\n+        fn\n+    '''\n+    return tf.map_fn(fn, elems, name=name)\n+\n+\n+def foldl(fn, elems, initializer=None, name=None):\n+    '''Reduce elems using fn to combine them from left to right.\n+\n+    # Arguments\n+        fn: Callable that will be called upon each element in elems and an\n+            accumulator, for instance lambda acc, x: acc + x\n+        elems: tensor\n+        initializer: The first value used (elems[0] in case of None)\n+        name: A string name for the foldl node in the graph\n+\n+    # Returns\n+        Same type and shape as initializer\n+    '''\n+    return tf.foldl(fn, elems, initializer=initializer, name=name)\n+\n+\n+def foldr(fn, elems, initializer=None, name=None):\n+    '''Reduce elems using fn to combine them from right to left.\n+\n+    # Arguments\n+        fn: Callable that will be called upon each element in elems and an\n+            accumulator, for instance lambda acc, x: acc + x\n+        elems: tensor\n+        initializer: The first value used (elems[-1] in case of None)\n+        name: A string name for the foldr node in the graph\n+\n+    # Returns\n+        Same type and shape as initializer\n+    '''\n+    return tf.foldr(fn, elems, initializer=initializer, name=name)\n\n@@ -1851,3 +1851,68 @@ def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n \n     ret = ret.dimshuffle('x', 0)\n     return ret\n+\n+\n+# HIGH ORDER FUNCTIONS\n+\n+def map_fn(fn, elems, name=None):\n+    '''Map the function fn over the elements elems and return the outputs.\n+\n+    # Arguments\n+        fn: Callable that will be called upon each element in elems\n+        elems: tensor, at least 2 dimensional\n+        name: A string name for the map node in the graph\n+\n+    # Returns\n+        Tensor with first dimension equal to the elems and second depending on\n+        fn\n+    '''\n+    return theano.map(fn, elems, name=name)[0]\n+\n+\n+def foldl(fn, elems, initializer=None, name=None):\n+    '''Reduce elems using fn to combine them from left to right.\n+\n+    # Arguments\n+        fn: Callable that will be called upon each element in elems and an\n+            accumulator, for instance lambda acc, x: acc + x\n+        elems: tensor\n+        initializer: The first value used (elems[0] in case of None)\n+        name: A string name for the foldl node in the graph\n+\n+    # Returns\n+        Same type and shape as initializer\n+    '''\n+    if initializer is None:\n+        initializer = elems[0]\n+        elems = elems[1:]\n+\n+    # We need to change the order of the arguments because theano accepts x as\n+    # first parameter and accumulator as second\n+    fn2 = lambda x, acc: fn(acc, x)\n+\n+    return theano.foldl(fn2, elems, initializer, name=name)[0]\n+\n+\n+def foldr(fn, elems, initializer=None, name=None):\n+    '''Reduce elems using fn to combine them from right to left.\n+\n+    # Arguments\n+        fn: Callable that will be called upon each element in elems and an\n+            accumulator, for instance lambda acc, x: acc + x\n+        elems: tensor\n+        initializer: The first value used (elems[-1] in case of None)\n+        name: A string name for the foldr node in the graph\n+\n+    # Returns\n+        Same type and shape as initializer\n+    '''\n+    if initializer is None:\n+        initializer = elems[-1]\n+        elems = elems[:-1]\n+\n+    # We need to change the order of the arguments because theano accepts x as\n+    # first parameter and accumulator as second\n+    fn2 = lambda x, acc: fn(acc, x)\n+\n+    return theano.foldr(fn2, elems, initializer, name=name)[0]\n\n@@ -881,6 +881,35 @@ class TestBackend(object):\n             assert k_s_d.shape == k_d.shape\n             assert_allclose(k_s_d, k_d, atol=1e-05)\n \n+    def test_map(self):\n+        x = np.random.rand(10, 3).astype(np.float32)\n+        for K in [KTF, KTH]:\n+            kx = K.eval(K.map_fn(K.sum, x))\n+\n+            assert (10,) == kx.shape\n+            assert_allclose(x.sum(axis=1), kx, atol=1e-05)\n+\n+    def test_foldl(self):\n+        x = np.random.rand(10, 3).astype(np.float32)\n+        for K in [KTF, KTH]:\n+            kx = K.eval(K.foldl(lambda a, b: a+b, x))\n+\n+            assert (3,) == kx.shape\n+            assert_allclose(x.sum(axis=0), kx, atol=1e-05)\n+\n+    def test_foldr(self):\n+        # This test aims to make sure that we walk the array from right to left\n+        # and checks it in the following way: multiplying left to right 1e-40\n+        # cannot be held into a float32 so it causes an underflow while from\n+        # right to left we have no such problem and the result is larger\n+        x = np.array([1e-20, 1e-20, 10, 10, 10], dtype=np.float32)\n+        for K in [KTF, KTH]:\n+            p1 = K.eval(K.foldl(lambda a, b: a*b, x))\n+            p2 = K.eval(K.foldr(lambda a, b: a*b, x))\n+\n+            assert p1 < p2\n+            assert 9e-38 < p2 <= 1e-37\n+\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7bd5c862a271f125a76fa1ada7f0d9ae27159549", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 2566 | Contributors (this commit): 30 | Commits (past 90d): 17 | Contributors (cumulative): 34 | DMM Complexity: 1.0\n\nDIFF:\n@@ -209,7 +209,7 @@ def check_loss_and_target_compatibility(targets, losses, output_shapes):\n                                 'which does expect integer targets.')\n         if loss.__name__ in key_losses:\n             for target_dim, out_dim in zip(y.shape[1:], shape[1:]):\n-                if target_dim is not None and target_dim != out_dim:\n+                if out_dim is not None and target_dim != out_dim:\n                     raise Exception('A target array with shape ' + str(y.shape) +\n                                     ' was passed for an output of shape ' + str(shape) +\n                                     ' while using as loss `' + loss.__name__ + '`. '\n\n@@ -226,9 +226,10 @@ def test_trainable_argument():\n \n \n @keras_test\n-def test_check_not_last_is_one():\n+def test_check_not_failing():\n     a = np.random.random((2, 1, 3))\n     check_loss_and_target_compatibility([a], [K.categorical_crossentropy], [a.shape])\n+    check_loss_and_target_compatibility([a], [K.categorical_crossentropy], [(2, None, 3)])\n \n \n @keras_test\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#509d6d823565e094fd19a45003895f75ca4b8a3e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 113 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 18 | Methods Changed: 13 | Complexity Δ (Sum/Max): 10/5 | Churn Δ: 123 | Churn Cumulative: 5293 | Contributors (this commit): 31 | Commits (past 90d): 22 | Contributors (cumulative): 38 | DMM Complexity: 0.0\n\nDIFF:\n@@ -9,6 +9,7 @@ import types as python_types\n import warnings\n import copy\n import os\n+import inspect\n from six.moves import zip\n \n from .. import backend as K\n@@ -1129,7 +1130,8 @@ class Merge(Layer):\n     '''\n     def __init__(self, layers=None, mode='sum', concat_axis=-1,\n                  dot_axes=-1, output_shape=None, output_mask=None,\n-                 node_indices=None, tensor_indices=None, name=None):\n+                 arguments={}, node_indices=None, tensor_indices=None,\n+                 name=None):\n         self.layers = layers\n         self.mode = mode\n         self.concat_axis = concat_axis\n@@ -1137,6 +1139,7 @@ class Merge(Layer):\n         self._output_shape = output_shape\n         self.node_indices = node_indices\n         self._output_mask = output_mask\n+        self.arguments = arguments\n \n         # Layer parameters.\n         self.inbound_nodes = []\n@@ -1239,9 +1242,10 @@ class Merge(Layer):\n                             '(at least 2). Got: ' + str(inputs))\n         # Case: \"mode\" is a lambda or function.\n         if hasattr(self.mode, '__call__'):\n-            # TODO: consider making it possible to\n-            # pass custom arguments to lambda.\n-            arguments = {}\n+            arguments = self.arguments\n+            arg_spec = inspect.getargspec(self.mode)\n+            if 'mask' in arg_spec.args:\n+                arguments['mask'] = mask\n             return self.mode(inputs, **arguments)\n \n         if self.mode == 'sum' or self.mode == 'ave':\n@@ -1421,13 +1425,26 @@ class Merge(Layer):\n             output_shape = self._output_shape\n             output_shape_type = 'raw'\n \n+        if isinstance(self._output_mask, python_types.LambdaType):\n+            output_mask = func_dump(self._output_mask)\n+            output_mask_type = 'lambda'\n+        elif callable(self._output_mask):\n+            output_mask = self._output_mask.__name__\n+            output_mask_type = 'function'\n+        else:\n+            output_mask = self._output_mask\n+            output_mask_type = 'raw'\n+\n         return {'name': self.name,\n                 'mode': mode,\n                 'mode_type': mode_type,\n                 'concat_axis': self.concat_axis,\n                 'dot_axes': self.dot_axes,\n                 'output_shape': output_shape,\n-                'output_shape_type': output_shape_type}\n+                'output_shape_type': output_shape_type,\n+                'output_mask': output_mask,\n+                'output_mask_type': output_mask_type,\n+                'arguments': self.arguments}\n \n     @classmethod\n     def from_config(cls, config):\n@@ -1447,13 +1464,22 @@ class Merge(Layer):\n         else:\n             output_shape = config['output_shape']\n \n+        output_mask_type = config.pop('output_mask_type')\n+        if output_mask_type == 'function':\n+            output_mask = globals()[config['output_mask']]\n+        elif output_mask_type == 'lambda':\n+            output_mask = func_load(config['output_mask'], globs=globals())\n+        else:\n+            output_mask = config['output_mask']\n+\n         config['mode'] = mode\n         config['output_shape'] = output_shape\n+        config['output_mask'] = output_mask\n         return super(Merge, cls).from_config(config)\n \n \n def merge(inputs, mode='sum', concat_axis=-1,\n-          dot_axes=-1, output_shape=None, output_mask=None, name=None):\n+          dot_axes=-1, output_shape=None, output_mask=None, arguments={}, name=None):\n     '''Functional merge, to apply to Keras tensors (NOT layers).\n     Returns a Keras tensor.\n \n@@ -1504,6 +1530,7 @@ def merge(inputs, mode='sum', concat_axis=-1,\n                             dot_axes=dot_axes,\n                             output_shape=output_shape,\n                             output_mask=output_mask,\n+                            arguments=arguments,\n                             node_indices=node_indices,\n                             tensor_indices=tensor_indices,\n                             name=name)\n@@ -1514,6 +1541,7 @@ def merge(inputs, mode='sum', concat_axis=-1,\n                             dot_axes=dot_axes,\n                             output_shape=output_shape,\n                             output_mask=output_mask,\n+                            arguments=arguments,\n                             name=name)\n         return merge_layer(inputs)\n \n\n@@ -15,7 +15,7 @@ def test_masking():\n \n @keras_test\n def test_merge():\n-    from keras.layers import Input, merge, Merge\n+    from keras.layers import Input, merge, Merge, Masking\n     from keras.models import Model\n \n     # test modes: 'sum', 'mul', 'concat', 'ave', 'cos', 'dot'.\n@@ -53,7 +53,8 @@ def test_merge():\n     input_b = Input(shape=input_shapes[1][1:])\n     merged = merge([input_a, input_b],\n                    mode=lambda tup: K.concatenate([tup[0], tup[1]]),\n-                   output_shape=lambda tup: (tup[0][:-1],) + (tup[0][-1] + tup[1][-1],))\n+                   output_shape=lambda tup: tup[0][:-1] + (tup[0][-1] + tup[1][-1],))\n+    model = Model([input_a, input_b], merged)\n     expected_output_shape = model.get_output_shape_for(input_shapes)\n     actual_output_shape = model.predict(inputs).shape\n     assert expected_output_shape == actual_output_shape\n@@ -65,17 +66,18 @@ def test_merge():\n     # test function with output_shape function\n     def fn_mode(tup):\n         x, y = tup\n-        return K.concatenate([x, y])\n+        return K.concatenate([x, y], axis=1)\n \n     def fn_output_shape(tup):\n         s1, s2 = tup\n-        return (s1[:-1],) + (s1[-1] + s2[-1],)\n+        return (s1[0], s1[1] + s2[1]) + s1[2:]\n \n     input_a = Input(shape=input_shapes[0][1:])\n     input_b = Input(shape=input_shapes[1][1:])\n     merged = merge([input_a, input_b],\n                    mode=fn_mode,\n                    output_shape=fn_output_shape)\n+    model = Model([input_a, input_b], merged)\n     expected_output_shape = model.get_output_shape_for(input_shapes)\n     actual_output_shape = model.predict(inputs).shape\n     assert expected_output_shape == actual_output_shape\n@@ -84,6 +86,74 @@ def test_merge():\n     model = Model.from_config(config)\n     model.compile('rmsprop', 'mse')\n \n+    # test function with output_mask function\n+    # time dimension is required for masking\n+    input_shapes = [(4, 3, 2), (4, 3, 2)]\n+    inputs = [np.random.random(shape) for shape in input_shapes]\n+\n+    def fn_output_mask(tup):\n+        x_mask, y_mask = tup\n+        return K.concatenate([x_mask, y_mask])\n+\n+    input_a = Input(shape=input_shapes[0][1:])\n+    input_b = Input(shape=input_shapes[1][1:])\n+    a = Masking()(input_a)\n+    b = Masking()(input_b)\n+    merged = merge([a, b], mode=fn_mode, output_shape=fn_output_shape, output_mask=fn_output_mask)\n+    model = Model([input_a, input_b], merged)\n+    expected_output_shape = model.get_output_shape_for(input_shapes)\n+    actual_output_shape = model.predict(inputs).shape\n+    assert expected_output_shape == actual_output_shape\n+\n+    config = model.get_config()\n+    model = Model.from_config(config)\n+    model.compile('rmsprop', 'mse')\n+\n+    mask_inputs = (np.zeros(input_shapes[0][:-1]), np.ones(input_shapes[1][:-1]))\n+    expected_mask_output = np.concatenate(mask_inputs, axis=-1)\n+    mask_input_placeholders = [K.placeholder(shape=input_shape[:-1]) for input_shape in input_shapes]\n+    mask_output = model.layers[-1]._output_mask(mask_input_placeholders)\n+    assert np.all(K.function(mask_input_placeholders, [mask_output])(mask_inputs)[0] == expected_mask_output)\n+\n+    # test lambda with output_mask lambda\n+    input_a = Input(shape=input_shapes[0][1:])\n+    input_b = Input(shape=input_shapes[1][1:])\n+    a = Masking()(input_a)\n+    b = Masking()(input_b)\n+    merged = merge([a, b], mode=lambda tup: K.concatenate([tup[0], tup[1]], axis=1),\n+                   output_shape=lambda tup: (tup[0][0], tup[0][1] + tup[1][1]) + tup[0][2:],\n+                   output_mask=lambda tup: K.concatenate([tup[0], tup[1]]))\n+    model = Model([input_a, input_b], merged)\n+    expected_output_shape = model.get_output_shape_for(input_shapes)\n+    actual_output_shape = model.predict(inputs).shape\n+    assert expected_output_shape == actual_output_shape\n+\n+    config = model.get_config()\n+    model = Model.from_config(config)\n+    model.compile('rmsprop', 'mse')\n+\n+    mask_output = model.layers[-1]._output_mask(mask_input_placeholders)\n+    assert np.all(K.function(mask_input_placeholders, [mask_output])(mask_inputs)[0] == expected_mask_output)\n+\n+    # test with arguments\n+    input_shapes = [(3, 2), (3, 2)]\n+    inputs = [np.random.random(shape) for shape in input_shapes]\n+\n+    def fn_mode(tup, a, b):\n+        x, y = tup\n+        return x * a + y * b\n+\n+    input_a = Input(shape=input_shapes[0][1:])\n+    input_b = Input(shape=input_shapes[1][1:])\n+    merged = merge([input_a, input_b], mode=fn_mode, output_shape=lambda s: s[0], arguments={'a': 0.7, 'b': 0.3})\n+    model = Model([input_a, input_b], merged)\n+    output = model.predict(inputs)\n+\n+    config = model.get_config()\n+    model = Model.from_config(config)\n+\n+    assert np.all(model.predict(inputs) == output)\n+\n \n @keras_test\n def test_merge_mask_2d():\n@@ -216,6 +286,11 @@ def test_lambda():\n                kwargs={'function': lambda x: x + 1},\n                input_shape=(3, 2))\n \n+    layer_test(Lambda,\n+               kwargs={'function': lambda x, a, b: x * a + b,\n+                       'arguments': {'a': 0.6, 'b': 0.4}},\n+               input_shape=(3, 2))\n+\n     # test serialization with function\n     def f(x):\n         return x + 1\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
