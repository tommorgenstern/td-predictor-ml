{"custom_id": "scrapy#5c68e2b34b7c9792cea9adcd86016d7166a30354", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 456 | Contributors (this commit): 2 | Commits (past 90d): 15 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -67,7 +67,8 @@ class FSImagesStore(object):\n         return {'last_modified': last_modified, 'checksum': checksum}\n \n     def _get_filesystem_path(self, key):\n-        return os.path.join(self.basedir, key)\n+        path_comps = key.split('/')\n+        return os.path.join(self.basedir, *path_comps)\n \n     def _mkdir(self, dirname, domain=None):\n         seen = self.created_directories[domain] if domain else set()\n\n@@ -1,9 +1,9 @@\n+import os\n from twisted.trial import unittest\n from scrapy.conf import settings\n from tempfile import mkdtemp\n from shutil import rmtree\n \n-SETTINGS_DISABLED = settings.disabled\n \n class ImagesPipelineTestCase(unittest.TestCase):\n     def setUp(self):\n@@ -14,6 +14,7 @@ class ImagesPipelineTestCase(unittest.TestCase):\n \n         from scrapy.contrib.pipeline.images import ImagesPipeline\n         self.tempdir = mkdtemp()\n+        self.settings_disabled_before = settings.disabled\n         settings.disabled = False\n         settings.overrides['IMAGES_STORE'] = self.tempdir\n         self.pipeline = ImagesPipeline()\n@@ -21,7 +22,7 @@ class ImagesPipelineTestCase(unittest.TestCase):\n     def tearDown(self):\n         del self.pipeline\n         rmtree(self.tempdir)\n-        settings.disabled = SETTINGS_DISABLED\n+        settings.disabled = self.settings_disabled_before\n \n     def test_image_path(self):\n         image_path = self.pipeline.image_key\n@@ -50,5 +51,16 @@ class ImagesPipelineTestCase(unittest.TestCase):\n         self.assertEqual(thumbnail_name(\"/tmp/some.name/foo\", name),\n                          'thumbs/50/92dac2a6a2072c5695a5dff1f865b3cb70c657bb.jpg')\n \n+    def test_fs_store(self):\n+        from scrapy.contrib.pipeline.images import FSImagesStore\n+        assert isinstance(self.pipeline.store, FSImagesStore)\n+        self.assertEqual(self.pipeline.store.basedir, self.tempdir)\n+\n+        key = 'some/image/key.jpg'\n+        path = os.path.join(self.tempdir, 'some', 'image', 'key.jpg')\n+        self.assertEqual(self.pipeline.store._get_filesystem_path(key),\n+            path)\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#daf86144c815d8b0a56d398fe671129c3b6c9d41", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 429 | Contributors (this commit): 2 | Commits (past 90d): 14 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -195,6 +195,9 @@ class ImagesPipeline(MediaPipeline):\n         super(ImagesPipeline, self).__init__()\n \n     def _get_store(self, uri):\n+        if os.path.isabs(uri): # to support win32 paths like: C:\\\\some\\dir\n+            scheme = 'file'\n+        else:\n             scheme = urlparse.urlparse(uri).scheme\n         store_cls = self.STORE_SCHEMES[scheme]\n         return store_cls(uri)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d7024dcd33736e20fb547f0b0611e4c1d174ee99", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 88 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n import os\n-from unittest import TestCase\n+import sys\n+from twisted.trial.unittest import TestCase, SkipTest\n \n from scrapy.contrib.downloadermiddleware.httpproxy import HttpProxyMiddleware\n from scrapy.core.exceptions import NotConfigured\n@@ -11,6 +12,8 @@ spider = BaseSpider()\n \n class TestDefaultHeadersMiddleware(TestCase):\n \n+    failureException = AssertionError\n+\n     def setUp(self):\n         self._oldenv = os.environ.copy()\n \n@@ -61,6 +64,8 @@ class TestDefaultHeadersMiddleware(TestCase):\n \n \n     def test_no_proxy(self):\n+        if sys.version_info < (2, 6):\n+            raise SkipTest('no_proxy is not supported in python < 2.6')\n         os.environ['http_proxy'] = http_proxy = 'https://proxy.for.http:3128'\n         mw = HttpProxyMiddleware()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#af09029649637916d4df01592eba7f0aa1e8af48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 11 | Churn Cumulative: 13 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,9 +5,10 @@ garbage collection to libxml2 documents (xmlDoc).\n \n import weakref\n \n+from scrapy.utils.trackref import object_ref\n from .factories import xmlDoc_from_html\n \n-class Libxml2Document(object):\n+class Libxml2Document(object_ref):\n \n     cache = weakref.WeakKeyDictionary()\n     __slots__ = ['xmlDoc', 'xpathContext', '__weakref__']\n@@ -15,7 +16,7 @@ class Libxml2Document(object):\n     def __new__(cls, response, factory=xmlDoc_from_html):\n         cache = cls.cache.setdefault(response, {})\n         if factory not in cache:\n-            obj = object.__new__(cls)\n+            obj = object_ref.__new__(cls)\n             obj.xmlDoc = factory(response)\n             obj.xpathContext = obj.xmlDoc.xpathNewContext()\n             cache[factory] = obj\n@@ -29,12 +30,8 @@ class Libxml2Document(object):\n         # TypeError, so the try/except block silences them\n         try:\n             self.xmlDoc.freeDoc()\n-        except:\n-            pass\n-        try:\n+        finally:\n             self.xpathContext.xpathFreeContext()\n-        except:\n-            pass\n \n     def __str__(self):\n         return \"<Libxml2Document %s>\" % self.xmlDoc.name\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#51c4be78d781dfa8fa54b680fdd049f57a85fe87", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 7 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 44 | Churn Cumulative: 48 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,7 +6,8 @@ from scrapy import log\n from scrapy.utils.python import re_rsearch, str_to_unicode\n from scrapy.utils.response import body_or_str\n \n-def xmliter(obj, nodename):\n+\n+def _xmliter_regex(obj, nodename):\n     \"\"\"Return a iterator of XPathSelector's over all nodes of a XML document,\n        given tha name of the node to iterate. Useful for parsing XML feeds.\n \n@@ -29,6 +30,47 @@ def xmliter(obj, nodename):\n         nodetext = header_start + match.group() + header_end\n         yield XmlXPathSelector(text=nodetext).select('//' + nodename)[0]\n \n+\n+def _xmliter_lxml(obj, nodename, encoding='utf-8'):\n+    reader = _StreamReader(obj)\n+    iterable = etree.iterparse(reader, tag=nodename, encoding=reader.encoding)\n+    for _, node in iterable:\n+        nodetext = etree.tostring(node)\n+        node.clear()\n+        yield XmlXPathSelector(text=nodetext).select('//' + nodename)[0]\n+\n+class _StreamReader(object):\n+\n+    def __init__(self, obj):\n+        self._ptr = 0\n+        if isinstance(obj, Response):\n+            self._text, self.encoding = obj.body, obj.encoding\n+        else:\n+            self._text, self.encoding = obj, 'utf-8'\n+        self._is_unicode = isinstance(self._text, unicode)\n+\n+    def read(self, n=65535):\n+        self.read = self._read_unicode if self._is_unicode else self._read_string\n+        return self.read(n).lstrip()\n+\n+    def _read_string(self, n=65535):\n+        s, e = self._ptr, self._ptr + n\n+        self._ptr = e\n+        return self._text[s:e]\n+\n+    def _read_unicode(self, n=65535):\n+        s, e = self._ptr, self._ptr + n\n+        self._ptr = e\n+        return self._text[s:e].encode('utf-8')\n+\n+\n+try:\n+    from lxml import etree\n+    xmliter = _xmliter_lxml\n+except ImportError:\n+    xmliter = _xmliter_regex\n+\n+\n def csviter(obj, delimiter=None, headers=None, encoding=None):\n     \"\"\" Returns an iterator of dictionaries from the given csv object\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6db5a37cdb53bbbde6f9234e59f88c4461563a98", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 14 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 44 | Churn Cumulative: 161 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -1,15 +1,16 @@\n import os\n-import unittest\n import libxml2\n+from twisted.trial import unittest\n \n-from scrapy.utils.iterators import csviter, xmliter\n+from scrapy.utils.iterators import csviter, xmliter, xmliter_lxml, xmliter_regex\n from scrapy.http import XmlResponse, TextResponse\n from scrapy.tests import get_testdata\n \n-class UtilsIteratorsTestCase(unittest.TestCase):\n-    ### NOTE: Encoding issues have been found with BeautifulSoup for utf-16 files, utf-16 test removed ###\n-    # pablo: Tests shouldn't be removed, but commented with proper steps on how\n-    # to reproduce the missing functionality\n+\n+class XmliterTestCase(unittest.TestCase):\n+\n+    xmliter = staticmethod(xmliter)\n+\n     def test_xmliter(self):\n         body = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\\n             <products xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:noNamespaceSchemaLocation=\"someschmea.xsd\">\\\n@@ -25,7 +26,7 @@ class UtilsIteratorsTestCase(unittest.TestCase):\n \n         response = XmlResponse(url=\"http://example.com\", body=body)\n         attrs = []\n-        for x in xmliter(response, 'product'):\n+        for x in self.xmliter(response, 'product'):\n             attrs.append((x.select(\"@id\").extract(), x.select(\"name/text()\").extract(), x.select(\"./type/text()\").extract()))\n \n         self.assertEqual(attrs, \n@@ -34,7 +35,7 @@ class UtilsIteratorsTestCase(unittest.TestCase):\n     def test_xmliter_text(self):\n         body = u\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?><products><product>one</product><product>two</product></products>\"\"\"\n         \n-        self.assertEqual([x.select(\"text()\").extract() for x in xmliter(body, 'product')],\n+        self.assertEqual([x.select(\"text()\").extract() for x in self.xmliter(body, 'product')],\n                          [[u'one'], [u'two']])\n \n     def test_xmliter_namespaces(self):\n@@ -57,7 +58,7 @@ class UtilsIteratorsTestCase(unittest.TestCase):\n             </rss>\n         \"\"\"\n         response = XmlResponse(url='http://mydummycompany.com', body=body)\n-        my_iter = xmliter(response, 'item')\n+        my_iter = self.xmliter(response, 'item')\n \n         node = my_iter.next()\n         node.register_namespace('g', 'http://base.google.com/ns/1.0')\n@@ -74,7 +75,7 @@ class UtilsIteratorsTestCase(unittest.TestCase):\n     def test_xmliter_exception(self):\n         body = u\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?><products><product>one</product><product>two</product></products>\"\"\"\n         \n-        iter = xmliter(body, 'product')\n+        iter = self.xmliter(body, 'product')\n         iter.next()\n         iter.next()\n \n@@ -84,10 +85,23 @@ class UtilsIteratorsTestCase(unittest.TestCase):\n         body = '<?xml version=\"1.0\" encoding=\"ISO-8859-9\"?>\\n<xml>\\n    <item>Some Turkish Characters \\xd6\\xc7\\xde\\xdd\\xd0\\xdc \\xfc\\xf0\\xfd\\xfe\\xe7\\xf6</item>\\n</xml>\\n\\n'\n         response = XmlResponse('http://www.example.com', body=body)\n         self.assertEqual(\n-            xmliter(response, 'item').next().extract(),\n+            self.xmliter(response, 'item').next().extract(),\n             u'<item>Some Turkish Characters \\xd6\\xc7\\u015e\\u0130\\u011e\\xdc \\xfc\\u011f\\u0131\\u015f\\xe7\\xf6</item>'\n         )\n \n+\n+class RegexXmliterTestCase(XmliterTestCase):\n+    xmliter = staticmethod(xmliter_regex)\n+\n+\n+class LxmlXmliterTestCase(XmliterTestCase):\n+    xmliter = staticmethod(xmliter_lxml)\n+    try:\n+        import lxml\n+    except ImportError:\n+        skip = True\n+\n+\n class UtilsCsvTestCase(unittest.TestCase):\n     sample_feeds_dir = os.path.join(os.path.abspath(os.path.dirname(__file__)), 'sample_data', 'feeds')\n     sample_feed_path = os.path.join(sample_feeds_dir, 'feed-sample3.csv')\n\n@@ -7,7 +7,7 @@ from scrapy.utils.python import re_rsearch, str_to_unicode\n from scrapy.utils.response import body_or_str\n \n \n-def _xmliter_regex(obj, nodename):\n+def xmliter_regex(obj, nodename):\n     \"\"\"Return a iterator of XPathSelector's over all nodes of a XML document,\n        given tha name of the node to iterate. Useful for parsing XML feeds.\n \n@@ -31,7 +31,7 @@ def _xmliter_regex(obj, nodename):\n         yield XmlXPathSelector(text=nodetext).select('//' + nodename)[0]\n \n \n-def _xmliter_lxml(obj, nodename, encoding='utf-8'):\n+def xmliter_lxml(obj, nodename):\n     reader = _StreamReader(obj)\n     iterable = etree.iterparse(reader, tag=nodename, encoding=reader.encoding)\n     for _, node in iterable:\n@@ -66,9 +66,9 @@ class _StreamReader(object):\n \n try:\n     from lxml import etree\n-    xmliter = _xmliter_lxml\n+    xmliter = xmliter_lxml\n except ImportError:\n-    xmliter = _xmliter_regex\n+    xmliter = xmliter_regex\n \n \n def csviter(obj, delimiter=None, headers=None, encoding=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f7fbbfec29d2af6cca62aef158a3e187e2375a95", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 175 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,7 @@ import os\n import libxml2\n from twisted.trial import unittest\n \n-from scrapy.utils.iterators import csviter, xmliter, xmliter_lxml, xmliter_regex\n+from scrapy.utils.iterators import csviter, xmliter, _xmliter_lxml, _xmliter_regex\n from scrapy.http import XmlResponse, TextResponse\n from scrapy.tests import get_testdata\n \n@@ -91,11 +91,11 @@ class XmliterTestCase(unittest.TestCase):\n \n \n class RegexXmliterTestCase(XmliterTestCase):\n-    xmliter = staticmethod(xmliter_regex)\n+    xmliter = staticmethod(_xmliter_regex)\n \n \n class LxmlXmliterTestCase(XmliterTestCase):\n-    xmliter = staticmethod(xmliter_lxml)\n+    xmliter = staticmethod(_xmliter_lxml)\n     try:\n         import lxml\n     except ImportError:\n\n@@ -7,7 +7,7 @@ from scrapy.utils.python import re_rsearch, str_to_unicode\n from scrapy.utils.response import body_or_str\n \n \n-def xmliter_regex(obj, nodename):\n+def _xmliter_regex(obj, nodename):\n     \"\"\"Return a iterator of XPathSelector's over all nodes of a XML document,\n        given tha name of the node to iterate. Useful for parsing XML feeds.\n \n@@ -31,7 +31,7 @@ def xmliter_regex(obj, nodename):\n         yield XmlXPathSelector(text=nodetext).select('//' + nodename)[0]\n \n \n-def xmliter_lxml(obj, nodename):\n+def _xmliter_lxml(obj, nodename):\n     reader = _StreamReader(obj)\n     iterable = etree.iterparse(reader, tag=nodename, encoding=reader.encoding)\n     for _, node in iterable:\n@@ -66,9 +66,9 @@ class _StreamReader(object):\n \n try:\n     from lxml import etree\n-    xmliter = xmliter_lxml\n+    xmliter = _xmliter_lxml\n except ImportError:\n-    xmliter = xmliter_regex\n+    xmliter = _xmliter_regex\n \n \n def csviter(obj, delimiter=None, headers=None, encoding=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6bb84f07977ec2c47629a2fb892720b26a4753d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 74 | Lines Deleted: 72 | Files Changed: 2 | Hunks: 29 | Methods Changed: 22 | Complexity Δ (Sum/Max): -3/2 | Churn Δ: 146 | Churn Cumulative: 166 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,10 +2,10 @@\n and extract the potentially compressed responses that may arrive. \n \"\"\"\n \n+import bz2\n+import gzip\n import zipfile\n import tarfile\n-import gzip\n-import bz2\n from cStringIO import StringIO\n from tempfile import mktemp\n \n@@ -13,79 +13,69 @@ from scrapy import log\n from scrapy.http import Response\n from scrapy.core.downloader.responsetypes import responsetypes\n \n+\n class DecompressionMiddleware(object):\n     \"\"\" This middleware tries to recognise and extract the possibly compressed\n     responses that may arrive. \"\"\"\n \n     def __init__(self):\n-        self.decompressors = {\n-            'tar': self.is_tar,\n-            'zip': self.is_zip,\n-            'gz': self.is_gzip,\n-            'bz2': self.is_bzip2\n+        self._formats = {\n+            'tar': self._is_tar,\n+            'zip': self._is_zip,\n+            'gz': self._is_gzip,\n+            'bz2': self._is_bzip2\n         }\n \n-    def is_tar(self, response):\n+    def _is_tar(self, response):\n+        archive = StringIO(response.body)\n         try:\n-            tar_file = tarfile.open(name=mktemp(), fileobj=self.archive)\n+            tar_file = tarfile.open(name=mktemp(), fileobj=archive)\n         except tarfile.ReadError:\n-            return False\n-        if tar_file.members:\n-            body = body=tar_file.extractfile(tar_file.members[0]).read()\n+            return\n+\n+        body = tar_file.extractfile(tar_file.members[0]).read()\n         respcls = responsetypes.from_args(filename=tar_file.members[0].name, body=body)\n         return response.replace(body=body, cls=respcls)\n-        else:\n-            raise self.ArchiveIsEmpty\n \n-    def is_zip(self, response):\n+    def _is_zip(self, response):\n+        archive = StringIO(response.body)\n         try:\n-            zip_file = zipfile.ZipFile(self.archive)\n+            zip_file = zipfile.ZipFile(archive)\n         except zipfile.BadZipfile:\n-            return False\n+            return\n+\n         namelist = zip_file.namelist()\n-        if namelist:\n         body = zip_file.read(namelist[0])\n         respcls = responsetypes.from_args(filename=namelist[0], body=body)\n         return response.replace(body=body, cls=respcls)\n-        else:\n-            raise self.ArchiveIsEmpty\n \n-    def is_gzip(self, response):\n+    def _is_gzip(self, response):\n+        archive = StringIO(response.body)\n         try:\n-            gzip_file = gzip.GzipFile(fileobj=self.archive)\n-            decompressed_body = gzip_file.read()\n+            body = gzip.GzipFile(fileobj=archive).read()\n         except IOError:\n-            return False\n-        respcls = responsetypes.from_args(body=decompressed_body)\n-        return response.replace(body=decompressed_body, cls=respcls)\n+            return\n \n-    def is_bzip2(self, response):\n+        respcls = responsetypes.from_args(body=body)\n+        return response.replace(body=body, cls=respcls)\n+\n+    def _is_bzip2(self, response):\n         try:\n-            decompressed_body = bz2.decompress(self.body)\n+            body = bz2.decompress(response.body)\n         except IOError:\n-            return False\n-        respcls = responsetypes.from_args(body=decompressed_body)\n-        return response.replace(body=decompressed_body, cls=respcls)\n+            return\n \n-    def extract(self, response):\n-        \"\"\" This method tries to decompress the given response, if possible,\n-        and returns a tuple containing the resulting response, and the name\n-        of the used decompressor \"\"\"\n-\n-        self.body = response.body\n-        self.archive = StringIO()\n-        self.archive.write(self.body)\n-\n-        for decompressor in self.decompressors.keys():\n-            self.archive.seek(0)\n-            new_response = self.decompressors[decompressor](response)\n-            if new_response:\n-                return (new_response, decompressor)\n-        return (response, None)\n+        respcls = responsetypes.from_args(body=body)\n+        return response.replace(body=body, cls=respcls)\n \n     def process_response(self, request, response, spider):\n-        if isinstance(response, Response) and response.body:\n-            response, format = self.extract(response)\n-            if format:\n-                log.msg('Decompressed response with format: %s' % format, log.DEBUG, domain=spider.domain_name)\n+        if not response.body:\n+            return response\n+\n+        for fmt, func in self._formats.iteritems():\n+            new_response = func(response)\n+            if new_response:\n+                log.msg('Decompressed response with format: %s' % \\\n+                        fmt, log.DEBUG, domain=spider.domain_name)\n+                return new_response\n         return response\n\n@@ -1,10 +1,11 @@\n from unittest import TestCase, main\n-from scrapy.http import Response, XmlResponse\n+from scrapy.http import Response, XmlResponse, Request\n from scrapy.contrib_exp.downloadermiddleware.decompression import DecompressionMiddleware\n+from scrapy.spider import BaseSpider\n from scrapy.tests import get_testdata\n \n-def setUp():\n-    formats = ['tar', 'xml.bz2', 'xml.gz', 'zip']\n+\n+def _test_data(formats):\n     uncompressed_body = get_testdata('compressed', 'feed-sample1.xml')\n     test_responses = {}\n     for format in formats:\n@@ -12,29 +13,40 @@ def setUp():\n         test_responses[format] = Response('http://foo.com/bar', body=body)\n     return uncompressed_body, test_responses\n \n-class ScrapyDecompressionTest(TestCase):\n-    uncompressed_body, test_responses = setUp()\n-    middleware = DecompressionMiddleware()\n \n-    def test_tar(self):\n-        response, format = self.middleware.extract(self.test_responses['tar'])\n-        assert isinstance(response, XmlResponse)\n-        self.assertEqual(response.body, self.uncompressed_body)\n+class DecompressionMiddlewareTest(TestCase):\n     \n-    def test_zip(self):\n-        response, format = self.middleware.extract(self.test_responses['zip'])\n-        assert isinstance(response, XmlResponse)\n-        self.assertEqual(response.body, self.uncompressed_body)\n+    test_formats = ['tar', 'xml.bz2', 'xml.gz', 'zip']\n+    uncompressed_body, test_responses = _test_data(test_formats)\n \n-    def test_gz(self):\n-        response, format = self.middleware.extract(self.test_responses['xml.gz'])\n-        assert isinstance(response, XmlResponse)\n-        self.assertEqual(response.body, self.uncompressed_body)\n+    def setUp(self):\n+        self.mw = DecompressionMiddleware()\n+        self.spider = BaseSpider()\n+\n+    def test_known_compression_formats(self):\n+        for fmt in self.test_formats:\n+            rsp = self.test_responses[fmt]\n+            new = self.mw.process_response(None, rsp, self.spider)\n+            assert isinstance(new, XmlResponse), \\\n+                    'Failed %s, response type %s' % (fmt, type(new).__name__)\n+            self.assertEqual(new.body, self.uncompressed_body, fmt)\n+\n+    def test_plain_response(self):\n+        rsp = Response(url='http://test.com', body=self.uncompressed_body)\n+        new = self.mw.process_response(None, rsp, self.spider)\n+        assert new is rsp\n+        self.assertEqual(new.body, rsp.body)\n+\n+    def test_empty_response(self):\n+        rsp = Response(url='http://test.com', body='')\n+        new = self.mw.process_response(None, rsp, self.spider)\n+        assert new is rsp\n+        assert not rsp.body\n+        assert not new.body\n+\n+    def tearDown(self):\n+        del self.mw\n \n-    def test_bz2(self):\n-        response, format = self.middleware.extract(self.test_responses['xml.bz2'])\n-        assert isinstance(response, XmlResponse)\n-        self.assertEqual(response.body, self.uncompressed_body)\n \n if __name__ == '__main__':\n     main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#23c49bcb3f3a2b27730527aac684cb3100691d92", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 47 | Files Changed: 2 | Hunks: 5 | Methods Changed: 7 | Complexity Δ (Sum/Max): -8/0 | Churn Δ: 51 | Churn Cumulative: 226 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,7 +2,8 @@ import os\n import libxml2\n from twisted.trial import unittest\n \n-from scrapy.utils.iterators import csviter, xmliter, _xmliter_lxml, _xmliter_regex\n+from scrapy.utils.iterators import csviter, xmliter\n+from scrapy.contrib_exp.iterators import xmliter_lxml\n from scrapy.http import XmlResponse, TextResponse\n from scrapy.tests import get_testdata\n \n@@ -90,12 +91,8 @@ class XmliterTestCase(unittest.TestCase):\n         )\n \n \n-class RegexXmliterTestCase(XmliterTestCase):\n-    xmliter = staticmethod(_xmliter_regex)\n-\n-\n class LxmlXmliterTestCase(XmliterTestCase):\n-    xmliter = staticmethod(_xmliter_lxml)\n+    xmliter = staticmethod(xmliter_lxml)\n     try:\n         import lxml\n     except ImportError:\n\n@@ -7,7 +7,7 @@ from scrapy.utils.python import re_rsearch, str_to_unicode\n from scrapy.utils.response import body_or_str\n \n \n-def _xmliter_regex(obj, nodename):\n+def xmliter(obj, nodename):\n     \"\"\"Return a iterator of XPathSelector's over all nodes of a XML document,\n        given tha name of the node to iterate. Useful for parsing XML feeds.\n \n@@ -31,46 +31,6 @@ def _xmliter_regex(obj, nodename):\n         yield XmlXPathSelector(text=nodetext).select('//' + nodename)[0]\n \n \n-def _xmliter_lxml(obj, nodename):\n-    reader = _StreamReader(obj)\n-    iterable = etree.iterparse(reader, tag=nodename, encoding=reader.encoding)\n-    for _, node in iterable:\n-        nodetext = etree.tostring(node)\n-        node.clear()\n-        yield XmlXPathSelector(text=nodetext).select('//' + nodename)[0]\n-\n-class _StreamReader(object):\n-\n-    def __init__(self, obj):\n-        self._ptr = 0\n-        if isinstance(obj, Response):\n-            self._text, self.encoding = obj.body, obj.encoding\n-        else:\n-            self._text, self.encoding = obj, 'utf-8'\n-        self._is_unicode = isinstance(self._text, unicode)\n-\n-    def read(self, n=65535):\n-        self.read = self._read_unicode if self._is_unicode else self._read_string\n-        return self.read(n).lstrip()\n-\n-    def _read_string(self, n=65535):\n-        s, e = self._ptr, self._ptr + n\n-        self._ptr = e\n-        return self._text[s:e]\n-\n-    def _read_unicode(self, n=65535):\n-        s, e = self._ptr, self._ptr + n\n-        self._ptr = e\n-        return self._text[s:e].encode('utf-8')\n-\n-\n-try:\n-    from lxml import etree\n-    xmliter = _xmliter_lxml\n-except ImportError:\n-    xmliter = _xmliter_regex\n-\n-\n def csviter(obj, delimiter=None, headers=None, encoding=None):\n     \"\"\" Returns an iterator of dictionaries from the given csv object\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cc85c463aed1f4c8e7b60cdd198e363d084524cc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 5 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 37 | Churn Cumulative: 37 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,37 @@\n+from scrapy.http import Response\n+from scrapy.selector import XmlXPathSelector\n+\n+\n+def xmliter_lxml(obj, nodename):\n+    from lxml import etree\n+    reader = _StreamReader(obj)\n+    iterable = etree.iterparse(reader, tag=nodename, encoding=reader.encoding)\n+    for _, node in iterable:\n+        nodetext = etree.tostring(node)\n+        node.clear()\n+        yield XmlXPathSelector(text=nodetext).select('//' + nodename)[0]\n+\n+\n+class _StreamReader(object):\n+\n+    def __init__(self, obj):\n+        self._ptr = 0\n+        if isinstance(obj, Response):\n+            self._text, self.encoding = obj.body, obj.encoding\n+        else:\n+            self._text, self.encoding = obj, 'utf-8'\n+        self._is_unicode = isinstance(self._text, unicode)\n+\n+    def read(self, n=65535):\n+        self.read = self._read_unicode if self._is_unicode else self._read_string\n+        return self.read(n).lstrip()\n+\n+    def _read_string(self, n=65535):\n+        s, e = self._ptr, self._ptr + n\n+        self._ptr = e\n+        return self._text[s:e]\n+\n+    def _read_unicode(self, n=65535):\n+        s, e = self._ptr, self._ptr + n\n+        self._ptr = e\n+        return self._text[s:e].encode('utf-8')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d6735b395247d7b3a42131b5535ba4b9b79d3d6b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 20 | Churn Cumulative: 287 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -87,7 +87,7 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n         self.url = urldefrag(request.url)[0]\n         self.method = request.method\n         self.body = request.body or None\n-        self.headers = request.headers\n+        self.headers = Headers(request.headers)\n         self.response_headers = None\n         self.timeout = timeout\n         self.deferred = defer.Deferred().addCallback(self._build_response)\n\n@@ -84,11 +84,25 @@ class HttpTestCase(unittest.TestCase):\n         d = download_http(request, spider)\n         return self.assertFailure(d, defer.TimeoutError)\n \n-    def test_host_header(self):\n+    def test_host_header_not_in_request_headers(self):\n+        def _test(response):\n+            self.assertEquals(response.body, '127.0.0.1:%d' % self.portno)\n+            self.assertEquals(request.headers, {})\n+\n         request = Request(self.getURL('host'))\n+        return download_http(request, BaseSpider()).addCallback(_test)\n+\n+    def test_host_header_seted_in_request_headers(self):\n+        def _test(response):\n+            self.assertEquals(response.body, 'example.com')\n+            self.assertEquals(request.headers.get('Host'), 'example.com')\n+\n+        request = Request(self.getURL('host'), headers={'Host': 'example.com'})\n+        return download_http(request, BaseSpider()).addCallback(_test)\n+\n         d = download_http(request, BaseSpider())\n         d.addCallback(lambda r: r.body)\n-        d.addCallback(self.assertEquals, '127.0.0.1:%d' % self.portno)\n+        d.addCallback(self.assertEquals, 'example.com')\n         return d\n \n     def test_payload(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#789cba2bd892727f0494cca70b38eabc3ea7e2b4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 109 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,5 @@\n import re, csv\n+from cStringIO import StringIO\n \n from scrapy.http import Response\n from scrapy.selector import XmlXPathSelector\n@@ -48,7 +49,7 @@ def csviter(obj, delimiter=None, headers=None, encoding=None):\n     def _getrow(csv_r):\n         return [str_to_unicode(field, encoding) for field in csv_r.next()]\n \n-    lines = body_or_str(obj, unicode=False).splitlines(True)\n+    lines = StringIO(body_or_str(obj, unicode=False))\n     if delimiter:\n         csv_r = csv.reader(lines, delimiter=delimiter)\n     else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6abb3c17ee90c0c070b2abbdb6a427b1382a4993", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 43 | Churn Cumulative: 242 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -37,7 +37,7 @@ class FormRequest(Request):\n             self.headers['Content-Type'] = 'application/x-www-form-urlencoded'\n \n     @classmethod\n-    def from_response(cls, response, formnumber=0, formdata=None, **kwargs):\n+    def from_response(cls, response, formnumber=0, formdata=None, clickdata=None, **kwargs):\n         encoding = getattr(response, 'encoding', 'utf-8')\n         forms = ParseFile(StringIO(response.body), response.url,\n                           encoding=encoding, backwards_compat=False)\n@@ -51,11 +51,10 @@ class FormRequest(Request):\n             # remove all existing fields with the same name before, so that\n             # formdata fields properly can properly override existing ones,\n             # which is the desired behaviour\n-            form.controls = [c for c in form.controls if c.name not in formdata.keys()]\n+            form.controls = [c for c in form.controls if c.name not in formdata]\n             for k, v in formdata.iteritems():\n                 for v2 in v if hasattr(v, '__iter__') else [v]:\n                     form.new_control('text', k, {'value': v2})\n \n-        url, body, headers = form.click_request_data()       \n-        request = cls(url, method=form.method, body=body, headers=headers, **kwargs)\n-        return request\n+        url, body, headers = form.click_request_data(**(clickdata or {}))\n+        return cls(url, method=form.method, body=body, headers=headers, **kwargs)\n\n@@ -258,6 +258,40 @@ class FormRequestTest(RequestTest):\n         self.assertEqual(fs['one'].value, '1')\n         self.assertEqual(fs['two'].value, '2')\n \n+    def test_from_response_submit_first_clickeable(self):\n+        respbody = \"\"\"\n+<form action=\"get.php\" method=\"GET\">\n+<input type=\"submit\" name=\"clickeable1\" value=\"clicked1\">\n+<input type=\"hidden\" name=\"one\" value=\"1\">\n+<input type=\"hidden\" name=\"two\" value=\"3\">\n+<input type=\"submit\" name=\"clickeable2\" value=\"clicked2\">\n+</form>\n+        \"\"\"\n+        response = Response(\"http://www.example.com/this/list.html\", body=respbody)\n+        r1 = self.request_class.from_response(response, formdata={'two': '2'})\n+        urlargs = cgi.parse_qs(urlparse(r1.url).query)\n+        self.assertEqual(urlargs['clickeable1'], ['clicked1'])\n+        self.assertFalse('clickeable2' in urlargs, urlargs)\n+        self.assertEqual(urlargs['one'], ['1'])\n+        self.assertEqual(urlargs['two'], ['2'])\n+\n+    def test_from_response_submit_not_first_clickeable(self):\n+        respbody = \"\"\"\n+<form action=\"get.php\" method=\"GET\">\n+<input type=\"submit\" name=\"clickeable1\" value=\"clicked1\">\n+<input type=\"hidden\" name=\"one\" value=\"1\">\n+<input type=\"hidden\" name=\"two\" value=\"3\">\n+<input type=\"submit\" name=\"clickeable2\" value=\"clicked2\">\n+</form>\n+        \"\"\"\n+        response = Response(\"http://www.example.com/this/list.html\", body=respbody)\n+        r1 = self.request_class.from_response(response, formdata={'two': '2'}, clickdata={'name': 'clickeable2'})\n+        urlargs = cgi.parse_qs(urlparse(r1.url).query)\n+        self.assertEqual(urlargs['clickeable2'], ['clicked2'])\n+        self.assertFalse('clickeable1' in urlargs, urlargs)\n+        self.assertEqual(urlargs['one'], ['1'])\n+        self.assertEqual(urlargs['two'], ['2'])\n+\n     def test_from_response_errors_noform(self):\n         respbody = \"\"\"<html></html>\"\"\"\n         response = Response(\"http://www.example.com/lala.html\", body=respbody)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#939e302b6efc457cd761759421eabf2c55bb8251", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 13 | Files Changed: 3 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 70 | Churn Cumulative: 243 | Contributors (this commit): 3 | Commits (past 90d): 13 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,8 +14,6 @@ class RedirectMiddleware(object):\n         self.priority_adjust = settings.getint('REDIRECT_PRIORITY_ADJUST')\n \n     def process_response(self, request, response, spider):\n-        domain = spider.domain_name\n-\n         if response.status in [302, 303] and 'Location' in response.headers:\n             redirected_url = urljoin_rfc(request.url, response.headers['location'])\n             redirected = request.replace(url=redirected_url, method='GET', body='')\n@@ -29,8 +27,8 @@ class RedirectMiddleware(object):\n             return self._redirect(redirected, request, spider, response.status)\n \n         interval, url = get_meta_refresh(response)\n-        if url and int(interval) < self.max_metarefresh_delay:\n-            redirected = request.replace(url=urljoin_rfc(request.url, url))\n+        if url and interval < self.max_metarefresh_delay:\n+            redirected = request.replace(url=url)\n             return self._redirect(redirected, request, spider, 'meta refresh')\n \n         return response\n\n@@ -42,7 +42,7 @@ class ResponseUtilsTest(unittest.TestCase):\n             <body>blahablsdfsal&amp;</body>\n             </html>\"\"\"\n         response = Response(url='http://example.org', body=body)\n-        self.assertEqual(get_meta_refresh(response), ('5', 'http://example.org/newpage'))\n+        self.assertEqual(get_meta_refresh(response), (5, 'http://example.org/newpage'))\n \n         # refresh without url should return (None, None)\n         body = \"\"\"<meta http-equiv=\"refresh\" content=\"5\" />\"\"\"\n@@ -52,7 +52,7 @@ class ResponseUtilsTest(unittest.TestCase):\n         body = \"\"\"<meta http-equiv=\"refresh\" content=\"5;\n             url=http://example.org/newpage\" /></head>\"\"\"\n         response = Response(url='http://example.org', body=body)\n-        self.assertEqual(get_meta_refresh(response), ('5', 'http://example.org/newpage'))\n+        self.assertEqual(get_meta_refresh(response), (5, 'http://example.org/newpage'))\n \n         # meta refresh in multiple lines\n         body = \"\"\"<html><head>\n@@ -60,7 +60,38 @@ class ResponseUtilsTest(unittest.TestCase):\n                HTTP-EQUIV=\"Refresh\"\n                CONTENT=\"1; URL=http://example.org/newpage\">\"\"\"\n         response = Response(url='http://example.org', body=body)\n-        self.assertEqual(get_meta_refresh(response), ('1', 'http://example.org/newpage'))\n+        self.assertEqual(get_meta_refresh(response), (1, 'http://example.org/newpage'))\n+\n+        # entities in the redirect url\n+        body = \"\"\"<meta http-equiv=\"refresh\" content=\"3; url=&#39;http://www.example.com/other&#39;\">\"\"\"\n+        response = Response(url='http://example.com', body=body)\n+        self.assertEqual(get_meta_refresh(response), (3, 'http://www.example.com/other'))\n+\n+        # relative redirects\n+        body = \"\"\"<meta http-equiv=\"refresh\" content=\"3; url=other.html\">\"\"\"\n+        response = Response(url='http://example.com/page/this.html', body=body)\n+        self.assertEqual(get_meta_refresh(response), (3, 'http://example.com/page/other.html'))\n+\n+        # non-standard encodings (utf-16)\n+        body = \"\"\"<meta http-equiv=\"refresh\" content=\"3; url=http://example.com/redirect\">\"\"\"\n+        body = body.decode('ascii').encode('utf-16')\n+        response = TextResponse(url='http://example.com', body=body, encoding='utf-16')\n+        self.assertEqual(get_meta_refresh(response), (3, 'http://example.com/redirect'))\n+\n+        # non-ascii chars in the url (default encoding - utf8)\n+        body = \"\"\"<meta http-equiv=\"refresh\" content=\"3; url=http://example.com/to\\xc2\\xa3\">\"\"\"\n+        response = Response(url='http://example.com', body=body)\n+        self.assertEqual(get_meta_refresh(response), (3, 'http://example.com/to%C2%A3'))\n+\n+        # non-ascii chars in the url (custom encoding - latin1)\n+        body = \"\"\"<meta http-equiv=\"refresh\" content=\"3; url=http://example.com/to\\xa3\">\"\"\"\n+        response = TextResponse(url='http://example.com', body=body, encoding='latin1')\n+        self.assertEqual(get_meta_refresh(response), (3, 'http://example.com/to%C2%A3'))\n+\n+        # wrong encodings (possibly caused by truncated chunks)\n+        body = \"\"\"<meta http-equiv=\"refresh\" content=\"3; url=http://example.com/this\\xc2_THAT\">\"\"\"\n+        response = Response(url='http://example.com', body=body)\n+        self.assertEqual(get_meta_refresh(response), (3, 'http://example.com/thisTHAT'))\n \n     def test_response_httprepr(self):\n         r1 = Response(\"http://www.example.com\")\n\n@@ -13,6 +13,8 @@ from tempfile import NamedTemporaryFile\n from twisted.web import http\n from twisted.web.http import RESPONSES\n \n+from scrapy.utils.markup import remove_entities\n+from scrapy.utils.url import safe_url_string, urljoin_rfc\n from scrapy.xlib.BeautifulSoup import BeautifulSoup\n from scrapy.http import Response, HtmlResponse\n \n@@ -34,16 +36,29 @@ def get_base_url(response):\n         _baseurl_cache[response] = match.group(1) if match else response.url\n     return _baseurl_cache[response]\n \n-META_REFRESH_RE = re.compile(r'<meta[^>]*http-equiv[^>]*refresh[^>].*?(\\d+);\\s*url=([^\"\\']+)', re.DOTALL | re.IGNORECASE)\n+META_REFRESH_RE = re.compile(ur'<meta[^>]*http-equiv[^>]*refresh[^>]*content\\s*=\\s*(?P<quote>[\"\\'])(?P<int>\\d+)\\s*;\\s*url=(?P<url>.*?)(?P=quote)', re.DOTALL | re.IGNORECASE)\n _metaref_cache = weakref.WeakKeyDictionary()\n def get_meta_refresh(response):\n-    \"\"\" Return a tuple of two strings containing the interval and url included\n-    in the http-equiv parameter of the HTML meta element. If no url is included\n-    (None, None) is returned [instead of (interval, None)]\n+    \"\"\"Parse the http-equiv parameter of the HTML meta element from the given\n+    response and return a tuple (interval, url) where interval is an integer\n+    containing the delay in seconds (or zero if not present) and url is a\n+    string with the absolute url to redirect.\n+\n+    If no meta redirect is found, (None, None) is returned.\n     \"\"\"\n     if response not in _metaref_cache:\n-        match = META_REFRESH_RE.search(response.body[0:4096])\n-        _metaref_cache[response] = match.groups() if match else (None, None)\n+        encoding = getattr(response, 'encoding', 'utf-8')\n+        body_chunk = remove_entities(unicode(response.body[0:4096], encoding, \\\n+            errors='ignore'))\n+        match = META_REFRESH_RE.search(body_chunk)\n+        if match:\n+            interval = int(match.group('int'))\n+            url = safe_url_string(match.group('url').strip(' \"\\''))\n+            url = urljoin_rfc(response.url, url)\n+            _metaref_cache[response] = (interval, url)\n+        else:\n+            _metaref_cache[response] = (None, None)\n+        #_metaref_cache[response] = match.groups() if match else (None, None)\n     return _metaref_cache[response]\n \n _beautifulsoup_cache = weakref.WeakKeyDictionary()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0d637a67034ab860d0b802962b1651359f51b57b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 24 | Churn Cumulative: 138 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 0.9\n\nDIFF:\n@@ -28,7 +28,9 @@ class RedirectMiddleware(object):\n \n         interval, url = get_meta_refresh(response)\n         if url and interval < self.max_metarefresh_delay:\n-            redirected = request.replace(url=url)\n+            redirected = request.replace(url=url, method='GET', body='')\n+            redirected.headers.pop('Content-Type', None)\n+            redirected.headers.pop('Content-Length', None)\n             return self._redirect(redirected, request, spider, 'meta refresh')\n \n         return response\n\n@@ -64,6 +64,7 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         assert isinstance(req2, Request)\n         self.assertEqual(req2.url, 'http://example.org/newpage')\n \n+    def test_meta_refresh_with_high_interval(self):\n         # meta-refresh with high intervals don't trigger redirects\n         body = \"\"\"<html>\n             <head><meta http-equiv=\"refresh\" content=\"1000;url=http://example.org/newpage\" /></head>\n@@ -74,6 +75,25 @@ class RedirectMiddlewareTest(unittest.TestCase):\n \n         assert rsp is rsp2\n \n+    def test_meta_refresh_trough_posted_request(self):\n+        body = \"\"\"<html>\n+            <head><meta http-equiv=\"refresh\" content=\"5;url=http://example.org/newpage\" /></head>\n+            </html>\"\"\"\n+        req = Request(url='http://example.org', method='POST', body='test',\n+            headers={'Content-Type': 'text/plain', 'Content-length': '4'})\n+        rsp = Response(url='http://example.org', body=body)\n+        req2 = self.mw.process_response(req, rsp, self.spider)\n+\n+        assert isinstance(req2, Request)\n+        self.assertEqual(req2.url, 'http://example.org/newpage')\n+        self.assertEqual(req2.method, 'GET')\n+        assert 'Content-Type' not in req2.headers, \\\n+            \"Content-Type header must not be present in redirected request\"\n+        assert 'Content-Length' not in req2.headers, \\\n+            \"Content-Length header must not be present in redirected request\"\n+        assert not req2.body, \\\n+            \"Redirected body must be empty, not '%s'\" % req2.body\n+\n     def test_max_redirect_times(self):\n         self.mw.max_redirect_times = 1\n         req = Request('http://scrapytest.org/302')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5371d900a5fc86519d8c6cf95a4491d8cdb4e9f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 170 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -44,9 +44,9 @@ class Spiderctl(object):\n         s += '<table border=1\">\\n'\n         s += \"<tr><th>Idle (%d)</th><th>Scheduled (%d)</th><th>Running (%d/%d)</th><th>Finished (%d)</th></tr>\\n\" % \\\n                 (len(self.idle),\n+                 len(self.scheduled),\n                  len(self.running),\n                  settings['CONCURRENT_DOMAINS'],\n-                 len(self.scheduled),\n                  len(self.finished))\n         s += \"<tr>\\n\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
