{"custom_id": "scrapy#f79351556577536f9d09910481919ca5a11d99e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 12 | Churn Cumulative: 40 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,3 @@\n-import pprint\n-\n from w3lib.url import is_url\n \n from scrapy import log\n@@ -29,9 +27,16 @@ class Command(ScrapyCommand):\n         parser.add_option(\"--headers\", dest=\"headers\", action=\"store_true\", \\\n             help=\"print response HTTP headers instead of body\")\n \n+    def _print_headers(self, headers, prefix):\n+        for key, values in headers.items():\n+            for value in values:\n+                print '%s %s: %s' % (prefix, key, value)\n+\n     def _print_response(self, response, opts):\n         if opts.headers:\n-            pprint.pprint(response.headers)\n+            self._print_headers(response.request.headers, '>')\n+            print '>'\n+            self._print_headers(response.headers, '<')\n         else:\n             print response.body\n \n@@ -40,6 +45,7 @@ class Command(ScrapyCommand):\n             raise UsageError()\n         cb = lambda x: self._print_response(x, opts)\n         request = Request(args[0], callback=cb, dont_filter=True)\n+        request.meta['handle_httpstatus_all'] = True\n \n         spider = None\n         if opts.spider:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#48509b036a4cebe52e8043c48782e1e41cea86b0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 28 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -18,6 +18,5 @@ class FetchTest(ProcessTest, SiteTest, unittest.TestCase):\n     def test_headers(self):\n         _, out, _ = yield self.execute([self.url('/text'), '--headers'])\n         out = out.replace('\\r', '') # required on win32\n-        headers = eval(out)\n-        assert 'TwistedWeb' in headers['Server'][0]\n-        self.assertEqual(headers['Content-Type'], ['text/plain'])\n+        assert 'Server: TwistedWeb' in out\n+        assert 'Content-Type: text/plain' in out\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7643f14c88d51294e1eee99797dff9f68cc63a30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/4 | Churn Δ: 56 | Churn Cumulative: 70 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,6 @@\n import zlib\n-from gzip import GzipFile\n-from cStringIO import StringIO\n \n+from scrapy.utils.gz import gunzip\n from scrapy.http import Response, TextResponse\n from scrapy.core.downloader.responsetypes import responsetypes\n \n@@ -18,6 +17,8 @@ class HttpCompressionMiddleware(object):\n             content_encoding = response.headers.getlist('Content-Encoding')\n             if content_encoding:\n                 encoding = content_encoding.pop()\n+                with open('body', 'w') as f:\n+                    f.write(response.body)\n                 decoded_body = self._decode(response.body, encoding.lower())\n                 respcls = responsetypes.from_args(headers=response.headers, \\\n                     url=response.url)\n@@ -34,7 +35,7 @@ class HttpCompressionMiddleware(object):\n \n     def _decode(self, body, encoding):\n         if encoding == 'gzip':\n-            body = GzipFile(fileobj=StringIO(body)).read()\n+            body = gunzip(body)\n \n         if encoding == 'deflate':\n             try:\n\n@@ -0,0 +1,26 @@\n+from __future__ import with_statement\n+\n+import unittest\n+from os.path import join\n+\n+from scrapy.tests import tests_datadir\n+from scrapy.utils.gz import gunzip\n+\n+SAMPLEDIR = join(tests_datadir, 'compressed')\n+\n+class GzTest(unittest.TestCase):\n+\n+    def test_gunzip_basic(self):\n+        with open(join(SAMPLEDIR, 'feed-sample1.xml.gz'), 'rb') as f:\n+            text = gunzip(f.read())\n+            self.assertEqual(len(text), 9950)\n+\n+    def test_gunzip_truncated(self):\n+        with open(join(SAMPLEDIR, 'truncated-crc-error.gz'), 'rb') as f:\n+            text = gunzip(f.read())\n+            assert text.endswith('</html')\n+\n+    def test_gunzip_no_gzip_file_raises(self):\n+        with open(join(SAMPLEDIR, 'feed-sample1.xml'), 'rb') as f:\n+            self.assertRaises(IOError, gunzip, f.read())\n+\n\n@@ -0,0 +1,23 @@\n+from cStringIO import StringIO\n+from gzip import GzipFile\n+\n+def gunzip(data):\n+    \"\"\"Gunzip the given data and return as much data as possible.\n+\n+    This is resilient to CRC checksum errors.\n+    \"\"\"\n+    f = GzipFile(fileobj=StringIO(data))\n+    output = ''\n+    chunk = '.'\n+    while chunk:\n+        try:\n+            chunk = f.read(8196)\n+            output += chunk\n+        except IOError:\n+            # complete only if there is some data, otherwise re-raise\n+            if output:\n+                output += f.extrabuf\n+                break\n+            else:\n+                raise\n+    return output\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#07df0edf7472d97b2bbeb3b0acffdf9127c29d9a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 194 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,4 +1,3 @@\n-import cgi\n import traceback\n import uuid\n from cStringIO import StringIO\n@@ -36,12 +35,9 @@ class Schedule(WsResource):\n class AddVersion(WsResource):\n \n     def render_POST(self, txrequest):\n-        ct = txrequest.requestHeaders.getRawHeaders('Content-Type')[0]\n-        boundary = ct.split('boundary=', 1)[1]\n-        d = cgi.parse_multipart(txrequest.content, {'boundary': boundary})\n-        project = d['project'][0]\n-        version = d['version'][0]\n-        eggf = StringIO(d['egg'][0])\n+        project = txrequest.args['project'][0]\n+        version = txrequest.args['version'][0]\n+        eggf = StringIO(txrequest.args['egg'][0])\n         self.root.eggstorage.put(eggf, project, version)\n         spiders = get_spider_list(project)\n         self.root.update_projects()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#88e33ad0ad95d5f9049d8d8b1359819f4fbbf704", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 12 | Churn Cumulative: 338 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -88,10 +88,7 @@ class Request(object_ref):\n     def __str__(self):\n         return \"<%s %s>\" % (self.method, self.url)\n \n-    def __repr__(self):\n-        attrs = ['url', 'method', 'body', 'headers', 'cookies', 'meta']\n-        args = \", \".join([\"%s=%r\" % (a, getattr(self, a)) for a in attrs])\n-        return \"%s(%s)\" % (self.__class__.__name__, args)\n+    __repr__ = __str__\n \n     def copy(self):\n         \"\"\"Return a copy of this Request\"\"\"\n\n@@ -61,14 +61,11 @@ class Response(object_ref):\n \n     body = property(_get_body, deprecated_setter(_set_body, 'body'))\n \n-    def __repr__(self):\n-        attrs = ['url', 'status', 'body', 'headers', 'request', 'flags']\n-        args = \", \".join([\"%s=%r\" % (a, getattr(self, a)) for a in attrs])\n-        return \"%s(%s)\" % (self.__class__.__name__, args)\n-\n     def __str__(self):\n         return \"<%d %s>\" % (self.status, self.url)\n \n+    __repr__ = __str__\n+\n     def copy(self):\n         \"\"\"Return a copy of this Response\"\"\"\n         return self.replace()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c4a607fc787b2b1d61bf309c98dc9ced38d9d445", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 5 | Churn Cumulative: 533 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -60,6 +60,8 @@ class Request(object_ref):\n             self._url = safe_url_string(unicode_url, self.encoding)\n         else:\n             raise TypeError('Request url must be str or unicode, got %s:' % type(url).__name__)\n+        if ':' not in self._url:\n+            raise ValueError('Missing scheme in request url: %s' % self._url)\n \n     url = property(_get_url, deprecated_setter(_set_url, 'url'))\n \n\n@@ -42,6 +42,9 @@ class RequestTest(unittest.TestCase):\n         assert r.headers is not headers\n         self.assertEqual(r.headers[\"caca\"], \"coco\")\n \n+    def test_url_no_scheme(self):\n+        self.assertRaises(ValueError, self.request_class, 'foo')\n+\n     def test_headers(self):\n         # Different ways of setting headers attribute\n         url = 'http://www.scrapy.org'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#37830da1f6a7bef5bf9b54f29d18a3cdc6fa3795", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 22 | Files Changed: 3 | Hunks: 16 | Methods Changed: 13 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 44 | Churn Cumulative: 872 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -14,7 +14,7 @@ class TestDownloaderStats(TestCase):\n \n         stats.open_spider(self.spider)\n \n-        self.req = Request('scrapytest.org')\n+        self.req = Request('http://scrapytest.org')\n         self.res = Response('scrapytest.org', status=400)\n \n     def test_process_request(self):\n\n@@ -142,7 +142,7 @@ class RequestTest(unittest.TestCase):\n         class CustomRequest(self.request_class):\n             pass\n \n-        r1 = CustomRequest('example.com', 'http://www.example.com')\n+        r1 = CustomRequest('http://www.example.com')\n         r2 = r1.copy()\n \n         assert type(r2) is CustomRequest\n\n@@ -36,7 +36,7 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n         self.pipe.close_spider(self.spider)\n \n     def test_default_media_to_download(self):\n-        request = Request('url')\n+        request = Request('http://url')\n         assert self.pipe.media_to_download(request, self.info) is None\n \n     def test_default_get_media_requests(self):\n@@ -44,12 +44,12 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n         assert self.pipe.get_media_requests(item, self.info) is None\n \n     def test_default_media_downloaded(self):\n-        request = Request('url')\n-        response = Response('url', body='')\n+        request = Request('http://url')\n+        response = Response('http://url', body='')\n         assert self.pipe.media_downloaded(response, request, self.info) is response\n \n     def test_default_media_failed(self):\n-        request = Request('url')\n+        request = Request('http://url')\n         fail = Failure(Exception())\n         assert self.pipe.media_failed(fail, request, self.info) is fail\n \n@@ -132,8 +132,8 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n     def test_result_succeed(self):\n         cb = lambda _: self.pipe._mockcalled.append('request_callback') or _\n         eb = lambda _: self.pipe._mockcalled.append('request_errback') or _\n-        rsp = Response('url1')\n-        req = Request('url1', meta=dict(response=rsp), callback=cb, errback=eb)\n+        rsp = Response('http://url1')\n+        req = Request('http://url1', meta=dict(response=rsp), callback=cb, errback=eb)\n         item = dict(requests=req)\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item['results'], [(True, rsp)])\n@@ -147,7 +147,7 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n         cb = lambda _: self.pipe._mockcalled.append('request_callback') or _\n         eb = lambda _: self.pipe._mockcalled.append('request_errback') or _\n         fail = Failure(Exception())\n-        req = Request('url1', meta=dict(response=fail), callback=cb, errback=eb)\n+        req = Request('http://url1', meta=dict(response=fail), callback=cb, errback=eb)\n         item = dict(requests=req)\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item['results'], [(False, fail)])\n@@ -158,10 +158,10 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n     @inlineCallbacks\n     def test_mix_of_success_and_failure(self):\n         self.pipe.LOG_FAILED_RESULTS = False\n-        rsp1 = Response('url1')\n-        req1 = Request('url1', meta=dict(response=rsp1))\n+        rsp1 = Response('http://url1')\n+        req1 = Request('http://url1', meta=dict(response=rsp1))\n         fail = Failure(Exception())\n-        req2 = Request('url2', meta=dict(response=fail))\n+        req2 = Request('http://url2', meta=dict(response=fail))\n         item = dict(requests=[req1, req2])\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item['results'], [(True, rsp1), (False, fail)])\n@@ -180,15 +180,15 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n     @inlineCallbacks\n     def test_get_media_requests(self):\n         # returns single Request (without callback)\n-        req = Request('url')\n+        req = Request('http://url')\n         item = dict(requests=req) # pass a single item\n         new_item = yield self.pipe.process_item(item, self.spider)\n         assert new_item is item\n         assert request_fingerprint(req) in self.info.downloaded\n \n         # returns iterable of Requests\n-        req1 = Request('url1')\n-        req2 = Request('url2')\n+        req1 = Request('http://url1')\n+        req2 = Request('http://url2')\n         item = dict(requests=iter([req1, req2]))\n         new_item = yield self.pipe.process_item(item, self.spider)\n         assert new_item is item\n@@ -197,8 +197,8 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n \n     @inlineCallbacks\n     def test_results_are_cached_across_multiple_items(self):\n-        rsp1 = Response('url1')\n-        req1 = Request('url1', meta=dict(response=rsp1))\n+        rsp1 = Response('http://url1')\n+        req1 = Request('http://url1', meta=dict(response=rsp1))\n         item = dict(requests=req1)\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertTrue(new_item is item)\n@@ -214,8 +214,8 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n \n     @inlineCallbacks\n     def test_results_are_cached_for_requests_of_single_item(self):\n-        rsp1 = Response('url1')\n-        req1 = Request('url1', meta=dict(response=rsp1))\n+        rsp1 = Response('http://url1')\n+        req1 = Request('http://url1', meta=dict(response=rsp1))\n         req2 = Request(req1.url, meta=dict(response=Response('http://donot.download.me')))\n         item = dict(requests=[req1, req2])\n         new_item = yield self.pipe.process_item(item, self.spider)\n@@ -232,7 +232,7 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n             self.assertEqual(len(self.info.waiting[fp]), 2)\n             return response\n \n-        rsp1 = Response('url')\n+        rsp1 = Response('http://url')\n         def rsp1_func():\n             dfd = Deferred().addCallback(_check_downloading)\n             reactor.callLater(.1, dfd.callback, rsp1)\n@@ -241,7 +241,7 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n         def rsp2_func():\n             self.fail('it must cache rsp1 result and must not try to redownload')\n \n-        req1 = Request('url', meta=dict(response=rsp1_func))\n+        req1 = Request('http://url', meta=dict(response=rsp1_func))\n         req2 = Request(req1.url, meta=dict(response=rsp2_func))\n         item = dict(requests=[req1, req2])\n         new_item = yield self.pipe.process_item(item, self.spider)\n@@ -249,7 +249,7 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n \n     @inlineCallbacks\n     def test_use_media_to_download_result(self):\n-        req = Request('url', meta=dict(result='ITSME', response=self.fail))\n+        req = Request('http://url', meta=dict(result='ITSME', response=self.fail))\n         item = dict(requests=req)\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item['results'], [(True, 'ITSME')])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#05101c7bba2a472888693b6c82b982a58659a697", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 58 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -11,8 +11,7 @@ class DjangoItemMeta(ItemMeta):\n             cls._model_fields = []\n             cls._model_meta = cls.django_model._meta\n             for model_field in cls._model_meta.fields:\n-                # XXX: for now we're treating each PK as autogenerated field\n-                if model_field != cls._model_meta.pk:\n+                if model_field.auto_created == False:\n                     if model_field.name not in cls.fields:\n                         cls.fields[model_field.name] = Field()\n                     cls._model_fields.append(model_field.name)\n@@ -31,4 +30,3 @@ class DjangoItem(Item):\n         if commit:\n             model.save()\n         return model\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6873d5b9524a9f2dd3d444a65bb01d03bdf468a3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 22 | Churn Cumulative: 117 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,9 +11,10 @@ except ImportError:\n     django = None\n \n if django:\n-    from .models import Person\n+    from .models import Person, IdentifiedPerson\n else:\n     Person = None\n+    IdentifiedPerson = None\n \n \n class BasePersonItem(DjangoItem):\n@@ -28,6 +29,10 @@ class OverrideFieldPersonItem(BasePersonItem):\n     age = Field()\n \n \n+class IdentifiedPersonItem(DjangoItem):\n+    django_model = IdentifiedPerson\n+\n+\n class DjangoItemTest(unittest.TestCase):\n     \n     def setUp(self):\n@@ -46,6 +51,14 @@ class DjangoItemTest(unittest.TestCase):\n         i = OverrideFieldPersonItem()\n         self.assertEqual(i.fields.keys(), ['age', 'name'])\n \n+    def test_custom_primary_key_field(self):\n+        \"\"\"\n+        Test that if a custom primary key exists, it is\n+        in the field list.\n+        \"\"\"\n+        i = IdentifiedPersonItem()\n+        self.assertEqual(i.fields.keys(), ['age', 'identifier', 'name'])\n+\n     def test_save(self):\n         i = BasePersonItem()\n         self.assertEqual(i.fields.keys(), ['age', 'name'])\n\n@@ -8,3 +8,10 @@ class Person(models.Model):\n     class Meta:\n         app_label = 'test_djangoitem'\n \n+class IdentifiedPerson(models.Model):\n+    identifier = models.PositiveIntegerField(primary_key=True)\n+    name = models.CharField(max_length=255)\n+    age = models.IntegerField()\n+\n+    class Meta:\n+        app_label = 'test_djangoitem'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0d5399d0bfeb98b1fd0d380e50d9c004cdefc8f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 105 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,3 @@\n-import os\n-\n from scrapy.command import ScrapyCommand\n from scrapy.utils.misc import load_object\n from scrapy.conf import settings\n@@ -15,4 +13,5 @@ class Command(ScrapyCommand):\n     def run(self, args, opts):\n         spman_cls = load_object(settings['SPIDER_MANAGER_CLASS'])\n         spiders = spman_cls.from_settings(settings)\n-        print os.linesep.join(spiders.list())\n+        for s in spiders.list():\n+            print s\n\n@@ -29,10 +29,12 @@ class EggStorageTest(unittest.TestCase):\n         v, f = self.eggst.get('mybot')\n         self.assertEqual(v, \"03\")\n         self.assertEqual(f.read(), \"egg03\")\n+        f.close()\n \n         v, f = self.eggst.get('mybot', '02')\n         self.assertEqual(v, \"02\")\n         self.assertEqual(f.read(), \"egg02\")\n+        f.close()\n \n         self.eggst.delete('mybot', '02')\n         self.assertEqual(self.eggst.list('mybot'), ['01', '03'])\n\n@@ -31,5 +31,5 @@ class EnvironmentTest(unittest.TestCase):\n         self.assertEqual(env['SCRAPY_JOB'], 'ID')\n         self.assertEqual(env['SCRAPY_CONCURRENT_SPIDERS'], '1')\n         self.assert_(env['SCRAPY_SQLITE_DB'].endswith('mybot.db'))\n-        self.assert_(env['SCRAPY_LOG_FILE'].endswith('/mybot/myspider/ID.log'))\n+        self.assert_(env['SCRAPY_LOG_FILE'].endswith(os.path.join('mybot', 'myspider', 'ID2.log')))\n         self.failIf('SCRAPY_SETTINGS_MODULE' in env)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#80b557849a9a8c0b840e1727f59a9e5c8f39d153", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 43 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -31,5 +31,5 @@ class EnvironmentTest(unittest.TestCase):\n         self.assertEqual(env['SCRAPY_JOB'], 'ID')\n         self.assertEqual(env['SCRAPY_CONCURRENT_SPIDERS'], '1')\n         self.assert_(env['SCRAPY_SQLITE_DB'].endswith('mybot.db'))\n-        self.assert_(env['SCRAPY_LOG_FILE'].endswith(os.path.join('mybot', 'myspider', 'ID2.log')))\n+        self.assert_(env['SCRAPY_LOG_FILE'].endswith(os.path.join('mybot', 'myspider', 'ID.log')))\n         self.failIf('SCRAPY_SETTINGS_MODULE' in env)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#72cf5a97c3838c677be0e1ec019a9f5b2d2aff5a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 80 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.5\n\nDIFF:\n@@ -1,6 +1,6 @@\n+import os\n import shutil\n import string\n-from os import listdir\n from os.path import join, dirname, abspath, exists, splitext\n \n import scrapy\n@@ -38,6 +38,8 @@ class Command(ScrapyCommand):\n         ScrapyCommand.add_options(self, parser)\n         parser.add_option(\"-l\", \"--list\", dest=\"list\", action=\"store_true\",\n             help=\"List available templates\")\n+        parser.add_option(\"-e\", \"--edit\", dest=\"edit\", action=\"store_true\",\n+            help=\"Edit spider after creating it\")\n         parser.add_option(\"-d\", \"--dump\", dest=\"dump\", metavar=\"TEMPLATE\",\n             help=\"Dump template to standard output\")\n         parser.add_option(\"-t\", \"--template\", dest=\"template\", default=\"crawl\",\n@@ -72,6 +74,8 @@ class Command(ScrapyCommand):\n         template_file = self._find_template(opts.template)\n         if template_file:\n             self._genspider(module, name, domain, opts.template, template_file)\n+            if opts.edit:\n+                self.exitcode = os.system('scrapy edit \"%s\"' % name)\n \n     def _genspider(self, module, name, domain, template_name, template_file):\n         \"\"\"Generate the spider module, based on the given template\"\"\"\n@@ -102,7 +106,7 @@ class Command(ScrapyCommand):\n \n     def _list_templates(self):\n         print \"Available templates:\"\n-        for filename in sorted(listdir(self.templates_dir)):\n+        for filename in sorted(os.listdir(self.templates_dir)):\n             if filename.endswith('.tmpl'):\n                 print \"  %s\" % splitext(filename)[0]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5dea6be5132ea110d849c2e82e3da69232ada5e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 35 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,11 +4,13 @@ Extensions for debugging Scrapy\n See documentation in docs/topics/extensions.rst\n \"\"\"\n \n+import os\n import signal\n import traceback\n from pdb import Pdb\n \n-from scrapy.utils.engine import print_engine_status\n+from scrapy.utils.engine import format_engine_status\n+from scrapy import log\n \n \n class StackTraceDump(object):\n@@ -21,8 +23,11 @@ class StackTraceDump(object):\n             pass\n \n     def dump_stacktrace(self, signum, frame):\n-        traceback.print_stack(frame)\n-        print_engine_status()\n+        msg = \"Dumping stack trace and engine status\" + os.linesep\n+        msg += \"\".join(traceback.format_stack(frame))\n+        msg += os.linesep\n+        msg += format_engine_status()\n+        log.msg(msg)\n \n \n class Debugger(object):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#841e9913dbbc35735d3a269402ed0996ecc19c30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 103 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -18,7 +18,8 @@ class CloseSpider(object):\n \n     def __init__(self):\n         self.timeout = settings.getint('CLOSESPIDER_TIMEOUT')\n-        self.itempassed = settings.getint('CLOSESPIDER_ITEMPASSED')\n+        self.itemcount = settings.getint('CLOSESPIDER_ITEMCOUNT') or \\\n+            settings.getint('CLOSESPIDER_ITEMPASSED') # XXX: legacy support\n         self.pagecount = settings.getint('CLOSESPIDER_PAGECOUNT')\n         self.errorcount = settings.getint('CLOSESPIDER_ERRORCOUNT')\n \n@@ -33,7 +34,7 @@ class CloseSpider(object):\n             dispatcher.connect(self.page_count, signal=signals.response_received)\n         if self.timeout:\n             dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n-        if self.itempassed:\n+        if self.itemcount:\n             dispatcher.connect(self.item_scraped, signal=signals.item_scraped)\n         dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n \n@@ -57,8 +58,8 @@ class CloseSpider(object):\n \n     def item_scraped(self, item, spider):\n         self.counts[spider] += 1\n-        if self.counts[spider] == self.itempassed:\n-            crawler.engine.close_spider(spider, 'closespider_itempassed')\n+        if self.counts[spider] == self.itemcount:\n+            crawler.engine.close_spider(spider, 'closespider_itemcount')\n \n     def spider_closed(self, spider):\n         self.counts.pop(spider, None)\n\n@@ -21,7 +21,7 @@ BOT_VERSION = '1.0'\n \n CLOSESPIDER_TIMEOUT = 0\n CLOSESPIDER_PAGECOUNT = 0\n-CLOSESPIDER_ITEMPASSED = 0\n+CLOSESPIDER_ITEMCOUNT = 0\n \n COMMANDS_MODULE = ''\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#91dc46539fbcfd0df553d632676254d46d97f9e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 7 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 60 | Churn Cumulative: 93 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,57 @@\n+from collections import defaultdict\n+\n+from twisted.internet import task\n+\n+from scrapy.xlib.pydispatch import dispatcher\n+from scrapy.exceptions import NotConfigured\n+from scrapy.conf import settings\n+from scrapy import log, signals\n+\n+class Slot(object):\n+\n+    def __init__(self):\n+        self.items = 0\n+        self.itemsprev = 0\n+        self.pages = 0\n+        self.pagesprev = 0\n+\n+class LogStats(object):\n+    \"\"\"Log basic scraping stats periodically\"\"\"\n+\n+    def __init__(self):\n+        self.interval = settings.getfloat('LOGSTATS_INTERVAL')\n+        if not self.interval:\n+            raise NotConfigured\n+        self.slots = defaultdict(Slot)\n+        self.multiplier = 60.0 / self.interval\n+        dispatcher.connect(self.item_scraped, signal=signals.item_scraped)\n+        dispatcher.connect(self.response_received, signal=signals.response_received)\n+        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n+        dispatcher.connect(self.engine_started, signal=signals.engine_started)\n+        dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n+\n+    def item_scraped(self, spider):\n+        self.slots[spider].items += 1\n+\n+    def response_received(self, spider):\n+        self.slots[spider].pages += 1\n+\n+    def spider_closed(self, spider):\n+        del self.slots[spider]\n+\n+    def engine_started(self):\n+        self.tsk = task.LoopingCall(self.log)\n+        self.tsk.start(self.interval)\n+\n+    def log(self):\n+        for spider, slot in self.slots.items():\n+            irate = (slot.items - slot.itemsprev) * self.multiplier\n+            prate = (slot.pages - slot.pagesprev) * self.multiplier\n+            slot.pagesprev, slot.itemsprev = slot.pages, slot.items\n+            msg = \"Crawled %d pages (at %d pages/min), scraped %d items (at %d items/min)\" \\\n+                % (slot.pages, prate, slot.items, irate)\n+            log.msg(msg, spider=spider)\n+\n+    def engine_stopped(self):\n+        if self.tsk.running:\n+            self.tsk.stop()\n\n@@ -137,6 +137,7 @@ EXTENSIONS_BASE = {\n     'scrapy.contrib.feedexport.FeedExporter': 0,\n     'scrapy.contrib.spidercontext.SpiderContext': 0,\n     'scrapy.contrib.throttle.AutoThrottle': 0,\n+    'scrapy.contrib.logstats.LogStats': 0,\n }\n \n FEED_URI = None\n@@ -182,6 +183,8 @@ LOG_STDOUT = False\n LOG_LEVEL = 'DEBUG'\n LOG_FILE = None\n \n+LOGSTATS_INTERVAL = 60.0\n+\n MAIL_DEBUG = False\n MAIL_HOST = 'localhost'\n MAIL_PORT = 25\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#57c43fdce6809382b360e1dff61b1289d62ce679", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 142 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 4 | Methods Changed: 9 | Complexity Δ (Sum/Max): 25/15 | Churn Δ: 142 | Churn Cumulative: 142 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: 0.8333333333333334\n\nDIFF:\n@@ -1,2 +1,3 @@\n from scrapy.contrib.spiders.crawl import CrawlSpider, Rule\n from scrapy.contrib.spiders.feed import XMLFeedSpider, CSVFeedSpider\n+from scrapy.contrib.spiders.sitemap import SitemapSpider\n\n@@ -0,0 +1,41 @@\n+import re\n+\n+from scrapy.spider import BaseSpider\n+from scrapy.http import Request\n+from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n+\n+class SitemapSpider(BaseSpider):\n+\n+    sitemap_urls = ()\n+    sitemap_rules = [('', 'parse')]\n+\n+    def __init__(self, *a, **kw):\n+        super(SitemapSpider, self).__init__(*a, **kw)\n+        self._cbs = []\n+        for r, c in self.sitemap_rules:\n+            if isinstance(r, basestring):\n+                r = re.compile(r)\n+            if isinstance(c, basestring):\n+                c = getattr(self, c)\n+            self._cbs.append((r, c))\n+            print self._cbs\n+\n+    def start_requests(self):\n+        return [Request(x, callback=self._parse_sitemap) for x in self.sitemap_urls]\n+\n+    def _parse_sitemap(self, response):\n+        if response.url.endswith('/robots.txt'):\n+            for url in sitemap_urls_from_robots(response.body):\n+                yield Request(url, callback=self._parse_sitemap)\n+        else:\n+            s = Sitemap(response.body)\n+            if s.type == 'sitemapindex':\n+                for sitemap in s:\n+                    yield Request(sitemap['loc'], callback=self._parse_sitemap)\n+            elif s.type == 'urlset':\n+                for url in s:\n+                    loc = url['loc']\n+                    for r, c in self._cbs:\n+                        if r.search(loc):\n+                            yield Request(loc, callback=c)\n+                            break\n\n@@ -0,0 +1,65 @@\n+import unittest\n+\n+from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n+\n+class SitemapTest(unittest.TestCase):\n+\n+    def test_sitemap(self):\n+        s = Sitemap(\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<urlset xmlns=\"http://www.google.com/schemas/sitemap/0.84\">\n+  <url>\n+    <loc>http://www.example.com/</loc>\n+    <lastmod>2009-08-16</lastmod>\n+    <changefreq>daily</changefreq>\n+    <priority>1</priority>\n+  </url>\n+  <url>\n+    <loc>http://www.example.com/Special-Offers.html</loc>\n+    <lastmod>2009-08-16</lastmod>\n+    <changefreq>weekly</changefreq>\n+    <priority>0.8</priority>\n+  </url>\n+</urlset>\"\"\")\n+        assert s.type == 'urlset'\n+        self.assertEqual(list(s),\n+            [{'priority': '1', 'loc': 'http://www.example.com/', 'lastmod': '2009-08-16', 'changefreq': 'daily'}, {'priority': '0.8', 'loc': 'http://www.example.com/Special-Offers.html', 'lastmod': '2009-08-16', 'changefreq': 'weekly'}])\n+\n+    def test_sitemap_index(self):\n+        s = Sitemap(\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<sitemapindex xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n+   <sitemap>\n+      <loc>http://www.example.com/sitemap1.xml.gz</loc>\n+      <lastmod>2004-10-01T18:23:17+00:00</lastmod>\n+   </sitemap>\n+   <sitemap>\n+      <loc>http://www.example.com/sitemap2.xml.gz</loc>\n+      <lastmod>2005-01-01</lastmod>\n+   </sitemap>\n+</sitemapindex>\"\"\")\n+        assert s.type == 'sitemapindex'\n+        self.assertEqual(list(s), [{'loc': 'http://www.example.com/sitemap1.xml.gz', 'lastmod': '2004-10-01T18:23:17+00:00'}, {'loc': 'http://www.example.com/sitemap2.xml.gz', 'lastmod': '2005-01-01'}])\n+\n+class RobotsTest(unittest.TestCase):\n+\n+    def test_sitemap_urls_from_robots(self):\n+        robots = \"\"\"User-agent: *\n+Disallow: /aff/\n+Disallow: /wl/\n+\n+# Search and shopping refining\n+Disallow: /s*/*facet\n+Disallow: /s*/*tags\n+\n+# Sitemap files\n+Sitemap: http://example.com/sitemap.xml\n+Sitemap: http://example.com/sitemap-product-index.xml\n+\n+# Forums \n+Disallow: /forum/search/\n+Disallow: /forum/active/\n+\"\"\"\n+        self.assertEqual(list(sitemap_urls_from_robots(robots)), \n+             ['http://example.com/sitemap.xml', 'http://example.com/sitemap-product-index.xml'])\n+\n+if __name__ == '__main__':\n+    unittest.main()\n\n@@ -0,0 +1,35 @@\n+\"\"\"\n+Module for processing Sitemaps.\n+\n+Note: The main purpose of this module is to provide support for the\n+SitemapSpider, its API is subject to change without notice.\n+\"\"\"\n+\n+from cStringIO import StringIO\n+from xml.etree.cElementTree import ElementTree\n+\n+class Sitemap(object):\n+    \"\"\"Class to parse Sitemap (type=urlset) and Sitemap Index\n+    (type=sitemapindex) files\"\"\"\n+\n+    def __init__(self, xmltext):\n+        tree = ElementTree()\n+        tree.parse(StringIO(xmltext))\n+        self._root = tree.getroot()\n+        _, self.type = self._root.tag.split('}', 1)\n+\n+    def __iter__(self):\n+        for elem in self._root.getchildren():\n+            d = {}\n+            for el in elem.getchildren():\n+                _, name = el.tag.split('}', 1)\n+                d[name] = el.text\n+            yield d\n+\n+def sitemap_urls_from_robots(robots_text):\n+    \"\"\"Return an iterator over all sitemap urls contained in the given\n+    robots.txt file\n+    \"\"\"\n+    for line in robots_text.splitlines():\n+        if line.lstrip().startswith('Sitemap:'):\n+            yield line.split(':', 1)[1].strip()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cd52a7c83b8a5b0a31c27bc7fc99a1c7adc8dfa8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 42 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -18,7 +18,6 @@ class SitemapSpider(BaseSpider):\n             if isinstance(c, basestring):\n                 c = getattr(self, c)\n             self._cbs.append((r, c))\n-            print self._cbs\n \n     def start_requests(self):\n         return [Request(x, callback=self._parse_sitemap) for x in self.sitemap_urls]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#59acb129e589d32404c262689af79905afb5dbd1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 96 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -11,4 +11,4 @@ def activate_egg(eggpath):\n         raise ValueError(\"Unknown or corrupt egg\")\n     d.activate()\n     settings_module = d.get_entry_info('scrapy', 'settings').module_name\n-    os.environ['SCRAPY_SETTINGS_MODULE'] = settings_module\n+    os.environ.setdefault('SCRAPY_SETTINGS_MODULE', settings_module)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#25b0ca3125b056f1b0e3591e4a29c89043b31b1d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 180 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -6,8 +6,7 @@ from twisted.trial import unittest\n \n from scrapy.spider import BaseSpider\n from scrapy.contrib.spiders.init import InitSpider\n-from scrapy.contrib.spiders.crawl import CrawlSpider\n-from scrapy.contrib.spiders.feed import XMLFeedSpider, CSVFeedSpider\n+from scrapy.contrib.spiders import CrawlSpider, XMLFeedSpider, CSVFeedSpider\n \n \n class BaseSpiderTest(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cfc93ba9db1fc58c60d23fbe33774851a257c1b5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 186 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,7 @@ from twisted.trial import unittest\n \n from scrapy.spider import BaseSpider\n from scrapy.contrib.spiders.init import InitSpider\n-from scrapy.contrib.spiders import CrawlSpider, XMLFeedSpider, CSVFeedSpider\n+from scrapy.contrib.spiders import CrawlSpider, XMLFeedSpider, CSVFeedSpider, SitemapSpider\n \n \n class BaseSpiderTest(unittest.TestCase):\n@@ -51,6 +51,10 @@ class CrawlSpiderTest(BaseSpiderTest):\n \n     spider_class = CrawlSpider\n \n+class SitemapSpiderTest(BaseSpiderTest):\n+\n+    spider_class = SitemapSpider\n+\n \n if __name__ == '__main__':\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e575e015c159b0655b69eeb9027d23cbb702b98f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 65 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,3 @@\n-from collections import defaultdict\n-\n from twisted.internet import task\n \n from scrapy.xlib.pydispatch import dispatcher\n@@ -22,10 +20,11 @@ class LogStats(object):\n         self.interval = settings.getfloat('LOGSTATS_INTERVAL')\n         if not self.interval:\n             raise NotConfigured\n-        self.slots = defaultdict(Slot)\n+        self.slots = {}\n         self.multiplier = 60.0 / self.interval\n         dispatcher.connect(self.item_scraped, signal=signals.item_scraped)\n         dispatcher.connect(self.response_received, signal=signals.response_received)\n+        dispatcher.connect(self.spider_opened, signal=signals.spider_opened)\n         dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n         dispatcher.connect(self.engine_started, signal=signals.engine_started)\n         dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n@@ -36,6 +35,9 @@ class LogStats(object):\n     def response_received(self, spider):\n         self.slots[spider].pages += 1\n \n+    def spider_opened(self, spider):\n+        self.slots[spider] = Slot()\n+\n     def spider_closed(self, spider):\n         del self.slots[spider]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dd90e83eae857830e5a8d01a68132a70f822f97f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 142 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -43,7 +43,7 @@ def get_engine_status(engine=None):\n             status['global'][test] = eval(test)\n         except Exception, e:\n             status['global'][test] = \"%s (exception)\" % type(e).__name__\n-    for spider in engine.downloader.sites:\n+    for spider in engine.downloader.sites + engine.scraper.sites:\n         x = {}\n         for test in spider_tests:\n             try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7e5e00cea5a9a1e8665cdfc83ef7a20f3b902291", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 14 | Churn Cumulative: 1953 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,7 +3,7 @@ from twisted.internet.defer import Deferred, DeferredList\n \n from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy import log\n-from scrapy.utils.request import request_fingerprint, request_deferred\n+from scrapy.utils.request import request_fingerprint\n from scrapy.utils.misc import arg_to_iter\n \n class MediaPipeline(object):\n@@ -75,11 +75,10 @@ class MediaPipeline(object):\n                 errback=self.media_failed, errbackArgs=(request, info))\n         else:\n             request.meta['handle_httpstatus_all'] = True\n-            dfd = request_deferred(request)\n+            dfd = self.crawler.engine.download(request, info.spider)\n             dfd.addCallbacks(\n                 callback=self.media_downloaded, callbackArgs=(request, info),\n                 errback=self.media_failed, errbackArgs=(request, info))\n-            self.crawler.engine.crawl(request, info.spider)\n         return dfd\n \n     def _cache_result_and_execute_waiters(self, result, fp, info):\n\n@@ -97,7 +97,7 @@ class ExecutionEngine(object):\n         request = self.scheduler.next_request(spider)\n         if not request:\n             return\n-        d = self.download(request, spider)\n+        d = self._download(request, spider)\n         d.addBoth(self._handle_downloader_output, request, spider)\n         d.addBoth(lambda _: self.next_request(spider))\n         return d\n@@ -160,6 +160,13 @@ class ExecutionEngine(object):\n         return self.scheduler.enqueue_request(spider, request)\n \n     def download(self, request, spider):\n+        if isinstance(request, Response):\n+            return request\n+        d = self._download(request, spider)\n+        d.addCallback(self.download, spider)\n+        return d\n+\n+    def _download(self, request, spider):\n         def _on_success(response):\n             \"\"\"handle the result of a page download\"\"\"\n             assert isinstance(response, (Response, Request))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#841007b5c5cee6f9701fb8ab6921231ca1205c2c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 40 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,7 +16,9 @@ def build(suffix):\n         with open(ifn, 'w') as of:\n             of.write(s)\n \n-    check_call('debchange -m -D unstable --force-distribution -v $(python setup.py --version)-r$(hg tip --template \"{rev}\")+$(date +%s) \"Automatic build\"', shell=True)\n+    env={'SCRAPY_VERSION_FROM_HG': '1'}\n+    check_call('debchange -m -D unstable --force-distribution -v $(python setup.py --version)+$(date +%s) \"Automatic build\"', \\\n+        shell=True, env=env)\n     check_call('debuild -us -uc -b', shell=True)\n \n def clean(suffix):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
