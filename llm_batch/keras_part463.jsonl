{"custom_id": "keras#cefa783c3b017b7c012bb167d6d4034483e4676c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 93 | Lines Deleted: 100 | Files Changed: 4 | Hunks: 36 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 193 | Churn Cumulative: 3446 | Contributors (this commit): 11 | Commits (past 90d): 18 | Contributors (cumulative): 27 | DMM Complexity: None\n\nDIFF:\n@@ -67,7 +67,8 @@ def populate_deserializable_objects():\n     LOCAL.ALL_OBJECTS[\"ZerosV2\"] = initializers_v2.Zeros\n \n     # Out of an abundance of caution we also include these aliases that have\n-    # a non-zero probability of having been included in saved configs in the past.\n+    # a non-zero probability of having been included in saved configs in the\n+    # past.\n     LOCAL.ALL_OBJECTS[\"glorot_normalV2\"] = initializers_v2.GlorotNormal\n     LOCAL.ALL_OBJECTS[\"glorot_uniformV2\"] = initializers_v2.GlorotUniform\n     LOCAL.ALL_OBJECTS[\"he_normalV2\"] = initializers_v2.HeNormal\n@@ -150,16 +151,16 @@ def deserialize(config, custom_objects=None):\n def get(identifier):\n     \"\"\"Retrieve a Keras initializer by the identifier.\n \n-    The `identifier` may be the string name of a initializers function or class (\n-    case-sensitively).\n+    The `identifier` may be the string name of a initializers function or class\n+    (case-sensitively).\n \n     >>> identifier = 'Ones'\n     >>> tf.keras.initializers.deserialize(identifier)\n     <...keras.initializers.initializers_v2.Ones...>\n \n     You can also specify `config` of the initializer to this function by passing\n-    dict containing `class_name` and `config` as an identifier. Also note that the\n-    `class_name` must map to a `Initializer` class.\n+    dict containing `class_name` and `config` as an identifier. Also note that\n+    the `class_name` must map to a `Initializer` class.\n \n     >>> cfg = {'class_name': 'Ones', 'config': {}}\n     >>> tf.keras.initializers.deserialize(cfg)\n\n@@ -65,8 +65,8 @@ class KerasInitializersTest(tf.test.TestCase, parameterized.TestCase):\n         target_max=None,\n         target_min=None,\n     ):\n-        # The global seed is set so that we can get the same random streams between\n-        # eager and graph mode when stateful op is used.\n+        # The global seed is set so that we can get the same random streams\n+        # between eager and graph mode when stateful op is used.\n         tf.random.set_seed(1337)\n         variable = backend.variable(init(shape))\n         output = backend.get_value(variable)\n@@ -314,8 +314,9 @@ class KerasInitializersTest(tf.test.TestCase, parameterized.TestCase):\n             self.assertEqual(result.shape, (2, 2))\n \n             if hasattr(initializer, \"seed\"):\n-                # Make sure the result are different when the partition_shape is same,\n-                # but partition_offset is different, for random related initializers.\n+                # Make sure the result are different when the partition_shape is\n+                # same, but partition_offset is different, for random related\n+                # initializers.\n                 result_2 = initializer(\n                     shape=(4, 2),\n                     partition_shape=(2, 2),\n@@ -325,9 +326,11 @@ class KerasInitializersTest(tf.test.TestCase, parameterized.TestCase):\n \n                 # Make sure initializer produce same result when provide same\n                 # partition offset.\n-                # TODO(scottzhu): Enable this assert when initializer is fully stateless\n+                # TODO(scottzhu): Enable this assert when initializer is fully\n+                # stateless\n                 # result_3 = initializer(\n-                #     shape=(4, 2), partition_shape=(2, 2), partition_offset=(1, 0))\n+                #     shape=(4, 2), partition_shape=(2, 2), partition_offset=(1,\n+                #     0))\n                 # self.assertAllClose(result_2, result_3)\n \n     @parameterized.named_parameters(\n\n@@ -71,8 +71,8 @@ class RandomNormal(tf.compat.v1.random_normal_initializer):\n     Args:\n       mean: a python scalar or a scalar tensor. Mean of the random values to\n         generate.\n-      stddev: a python scalar or a scalar tensor. Standard deviation of the random\n-        values to generate.\n+      stddev: a python scalar or a scalar tensor. Standard deviation of the\n+        random values to generate.\n       seed: A Python integer. Used to create random seeds. See\n         `tf.compat.v1.set_random_seed` for behavior.\n       dtype: Default data type, used if no `dtype` argument is provided when\n\n@@ -13,7 +13,6 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras initializers for TF 2.\"\"\"\n-# pylint: disable=g-classes-have-attributes, missing-docstring, g-direct-tensorflow-import\n \n import math\n \n@@ -65,10 +64,10 @@ class Initializer:\n         return {\"mean\": self.mean, \"stddev\": self.stddev}\n     ```\n \n-    Note that we don't have to implement `from_config` in the example above since\n-    the constructor arguments of the class the keys in the config returned by\n-    `get_config` are the same. In this case, the default `from_config`\n-    works fine.\n+    Note that we don't have to implement `from_config` in the example above\n+    since the constructor arguments of the class the keys in the config returned\n+    by `get_config` are the same. In this case, the default `from_config` works\n+    fine.\n     \"\"\"\n \n     def __call__(self, shape, dtype=None, **kwargs):\n@@ -135,9 +134,9 @@ class Zeros(Initializer):\n \n         Args:\n           shape: Shape of the tensor.\n-          dtype: Optional dtype of the tensor. Only numeric or boolean dtypes are\n-           supported. If not specified, `tf.keras.backend.floatx()` is used,\n-           which default to `float32` unless you configured it otherwise\n+          dtype: Optional dtype of the tensor. Only numeric or boolean dtypes\n+            are supported. If not specified, `tf.keras.backend.floatx()` is\n+            used, which default to `float32` unless you configured it otherwise\n             (via `tf.keras.backend.set_floatx(float_dtype)`).\n           **kwargs: Additional keyword arguments.\n         \"\"\"\n@@ -177,9 +176,9 @@ class Ones(Initializer):\n \n         Args:\n           shape: Shape of the tensor.\n-          dtype: Optional dtype of the tensor. Only numeric or boolean dtypes are\n-           supported. If not specified, `tf.keras.backend.floatx()` is used,\n-           which default to `float32` unless you configured it otherwise\n+          dtype: Optional dtype of the tensor. Only numeric or boolean dtypes\n+            are supported. If not specified, `tf.keras.backend.floatx()` is\n+            used, which default to `float32` unless you configured it otherwise\n             (via `tf.keras.backend.set_floatx(float_dtype)`).\n           **kwargs: Additional keyword arguments.\n         \"\"\"\n@@ -279,10 +278,9 @@ class RandomUniform(Initializer):\n       maxval: A python scalar or a scalar tensor. Upper bound of the range of\n         random values to generate (exclusive).\n       seed: A Python integer. Used to make the behavior of the initializer\n-        deterministic. Note that a seeded\n-        initializer will not produce the same random values across multiple calls,\n-        but multiple initializers will produce the same sequence when constructed\n-        with the same seed value.\n+        deterministic. Note that a seeded initializer will not produce the same\n+        random values across multiple calls, but multiple initializers will\n+        produce the same sequence when constructed with the same seed value.\n     \"\"\"\n \n     def __init__(self, minval=-0.05, maxval=0.05, seed=None):\n@@ -356,13 +354,12 @@ class RandomNormal(Initializer):\n     Args:\n       mean: a python scalar or a scalar tensor. Mean of the random values to\n         generate.\n-      stddev: a python scalar or a scalar tensor. Standard deviation of the random\n-        values to generate.\n+      stddev: a python scalar or a scalar tensor. Standard deviation of the\n+        random values to generate.\n       seed: A Python integer. Used to make the behavior of the initializer\n-        deterministic. Note that a seeded\n-        initializer will not produce the same random values across multiple calls,\n-        but multiple initializers will produce the same sequence when constructed\n-        with the same seed value.\n+        deterministic. Note that a seeded initializer will not produce the same\n+        random values across multiple calls, but multiple initializers will\n+        produce the same sequence when constructed with the same seed value.\n     \"\"\"\n \n     def __init__(self, mean=0.0, stddev=0.05, seed=None):\n@@ -377,8 +374,8 @@ class RandomNormal(Initializer):\n         Args:\n           shape: Shape of the tensor.\n           dtype: Optional dtype of the tensor. Only floating point types are\n-            supported. If not specified, `tf.keras.backend.floatx()` is used, which\n-            default to `float32` unless you configured it otherwise (via\n+            supported. If not specified, `tf.keras.backend.floatx()` is used,\n+            which default to `float32` unless you configured it otherwise (via\n             `tf.keras.backend.set_floatx(float_dtype)`)\n           **kwargs: Additional keyword arguments.\n         \"\"\"\n@@ -443,10 +440,9 @@ class TruncatedNormal(Initializer):\n       stddev: a python scalar or a scalar tensor. Standard deviation of the\n         random values to generate before truncation.\n       seed: A Python integer. Used to make the behavior of the initializer\n-        deterministic. Note that a seeded\n-        initializer will not produce the same random values across multiple calls,\n-        but multiple initializers will produce the same sequence when constructed\n-        with the same seed value.\n+        deterministic. Note that a seeded initializer will not produce the same\n+        random values across multiple calls, but multiple initializers will\n+        produce the same sequence when constructed with the same seed value.\n     \"\"\"\n \n     def __init__(self, mean=0.0, stddev=0.05, seed=None):\n@@ -461,8 +457,8 @@ class TruncatedNormal(Initializer):\n         Args:\n           shape: Shape of the tensor.\n           dtype: Optional dtype of the tensor. Only floating point types are\n-            supported. If not specified, `tf.keras.backend.floatx()` is used, which\n-            default to `float32` unless you configured it otherwise (via\n+            supported. If not specified, `tf.keras.backend.floatx()` is used,\n+            which default to `float32` unless you configured it otherwise (via\n             `tf.keras.backend.set_floatx(float_dtype)`)\n           **kwargs: Additional keyword arguments.\n         \"\"\"\n@@ -507,9 +503,9 @@ class VarianceScaling(Initializer):\n     `tf.keras.initializers.variance_scaling`.\n \n     With `distribution=\"truncated_normal\" or \"untruncated_normal\"`, samples are\n-    drawn from a truncated/untruncated normal distribution with a mean of zero and\n-    a standard deviation (after truncation, if used) `stddev = sqrt(scale / n)`,\n-    where `n` is:\n+    drawn from a truncated/untruncated normal distribution with a mean of zero\n+    and a standard deviation (after truncation, if used) `stddev = sqrt(scale /\n+    n)`, where `n` is:\n \n     - number of input units in the weight tensor, if `mode=\"fan_in\"`\n     - number of output units, if `mode=\"fan_out\"`\n@@ -536,10 +532,9 @@ class VarianceScaling(Initializer):\n       distribution: Random distribution to use. One of \"truncated_normal\",\n         \"untruncated_normal\" and  \"uniform\".\n       seed: A Python integer. Used to make the behavior of the initializer\n-        deterministic. Note that a seeded\n-        initializer will not produce the same random values across multiple calls,\n-        but multiple initializers will produce the same sequence when constructed\n-        with the same seed value.\n+        deterministic. Note that a seeded initializer will not produce the same\n+        random values across multiple calls, but multiple initializers will\n+        produce the same sequence when constructed with the same seed value.\n     \"\"\"\n \n     def __init__(\n@@ -585,8 +580,8 @@ class VarianceScaling(Initializer):\n         Args:\n           shape: Shape of the tensor.\n           dtype: Optional dtype of the tensor. Only floating point types are\n-            supported. If not specified, `tf.keras.backend.floatx()` is used, which\n-            default to `float32` unless you configured it otherwise (via\n+            supported. If not specified, `tf.keras.backend.floatx()` is used,\n+            which default to `float32` unless you configured it otherwise (via\n             `tf.keras.backend.set_floatx(float_dtype)`)\n           **kwargs: Additional keyword arguments.\n         \"\"\"\n@@ -621,7 +616,8 @@ class VarianceScaling(Initializer):\n         else:\n             scale /= max(1.0, (fan_in + fan_out) / 2.0)\n         if self.distribution == \"truncated_normal\":\n-            # constant from scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\n+            # constant from scipy.stats.truncnorm.std(a=-2, b=2, loc=0.,\n+            # scale=1.)\n             stddev = math.sqrt(scale) / 0.87962566103423978\n             return self._random_generator.truncated_normal(\n                 shape, 0.0, stddev, dtype, nonce\n@@ -654,11 +650,11 @@ class Orthogonal(Initializer):\n \n     Also available via the shortcut function `tf.keras.initializers.orthogonal`.\n \n-    If the shape of the tensor to initialize is two-dimensional, it is initialized\n-    with an orthogonal matrix obtained from the QR decomposition of a matrix of\n-    random numbers drawn from a normal distribution.\n-    If the matrix has fewer rows than columns then the output will have orthogonal\n-    rows. Otherwise, the output will have orthogonal columns.\n+    If the shape of the tensor to initialize is two-dimensional, it is\n+    initialized with an orthogonal matrix obtained from the QR decomposition of\n+    a matrix of random numbers drawn from a normal distribution. If the matrix\n+    has fewer rows than columns then the output will have orthogonal rows.\n+    Otherwise, the output will have orthogonal columns.\n \n     If the shape of the tensor to initialize is more than two-dimensional,\n     a matrix of shape `(shape[0] * ... * shape[n - 2], shape[n - 1])`\n@@ -678,10 +674,9 @@ class Orthogonal(Initializer):\n     Args:\n       gain: multiplicative factor to apply to the orthogonal matrix\n       seed: A Python integer. Used to make the behavior of the initializer\n-        deterministic. Note that a seeded\n-        initializer will not produce the same random values across multiple calls,\n-        but multiple initializers will produce the same sequence when constructed\n-        with the same seed value.\n+        deterministic. Note that a seeded initializer will not produce the same\n+        random values across multiple calls, but multiple initializers will\n+        produce the same sequence when constructed with the same seed value.\n \n     References:\n       - [Saxe et al., 2014](https://openreview.net/forum?id=_wzZwKpTDF_9C)\n@@ -823,8 +818,8 @@ class GlorotUniform(VarianceScaling):\n     `tf.keras.initializers.glorot_uniform`.\n \n     Draws samples from a uniform distribution within `[-limit, limit]`, where\n-    `limit = sqrt(6 / (fan_in + fan_out))` (`fan_in` is the number of input units\n-    in the weight tensor and `fan_out` is the number of output units).\n+    `limit = sqrt(6 / (fan_in + fan_out))` (`fan_in` is the number of input\n+    units in the weight tensor and `fan_out` is the number of output units).\n \n     Examples:\n \n@@ -838,10 +833,9 @@ class GlorotUniform(VarianceScaling):\n \n     Args:\n       seed: A Python integer. Used to make the behavior of the initializer\n-        deterministic. Note that a seeded\n-        initializer will not produce the same random values across multiple calls,\n-        but multiple initializers will produce the same sequence when constructed\n-        with the same seed value.\n+        deterministic. Note that a seeded initializer will not produce the same\n+        random values across multiple calls, but multiple initializers will\n+        produce the same sequence when constructed with the same seed value.\n \n     References:\n       - [Glorot et al., 2010](http://proceedings.mlr.press/v9/glorot10a.html)\n@@ -865,10 +859,10 @@ class GlorotNormal(VarianceScaling):\n     Also available via the shortcut function\n     `tf.keras.initializers.glorot_normal`.\n \n-    Draws samples from a truncated normal distribution centered on 0 with `stddev\n-    = sqrt(2 / (fan_in + fan_out))` where `fan_in` is the number of input units in\n-    the weight tensor and `fan_out` is the number of output units in the weight\n-    tensor.\n+    Draws samples from a truncated normal distribution centered on 0 with\n+    `stddev = sqrt(2 / (fan_in + fan_out))` where `fan_in` is the number of\n+    input units in the weight tensor and `fan_out` is the number of output units\n+    in the weight tensor.\n \n     Examples:\n \n@@ -882,10 +876,9 @@ class GlorotNormal(VarianceScaling):\n \n     Args:\n       seed: A Python integer. Used to make the behavior of the initializer\n-        deterministic. Note that a seeded\n-        initializer will not produce the same random values across multiple calls,\n-        but multiple initializers will produce the same sequence when constructed\n-        with the same seed value.\n+        deterministic. Note that a seeded initializer will not produce the same\n+        random values across multiple calls, but multiple initializers will\n+        produce the same sequence when constructed with the same seed value.\n \n     References:\n       - [Glorot et al., 2010](http://proceedings.mlr.press/v9/glorot10a.html)\n@@ -916,9 +909,9 @@ class LecunNormal(VarianceScaling):\n     the Initializer object, without knowing the shape and dtype of the variable\n     being initialized.\n \n-    Draws samples from a truncated normal distribution centered on 0 with `stddev\n-    = sqrt(1 / fan_in)` where `fan_in` is the number of input units in the weight\n-    tensor.\n+    Draws samples from a truncated normal distribution centered on 0 with\n+    `stddev = sqrt(1 / fan_in)` where `fan_in` is the number of input units in\n+    the weight tensor.\n \n     Examples:\n \n@@ -932,10 +925,9 @@ class LecunNormal(VarianceScaling):\n \n     Args:\n       seed: A Python integer. Used to make the behavior of the initializer\n-        deterministic. Note that a seeded\n-        initializer will not produce the same random values across multiple calls,\n-        but multiple initializers will produce the same sequence when constructed\n-        with the same seed value.\n+        deterministic. Note that a seeded initializer will not produce the same\n+        random values across multiple calls, but multiple initializers will\n+        produce the same sequence when constructed with the same seed value.\n \n     References:\n       - [Klambauer et al., 2017](https://arxiv.org/abs/1706.02515)\n@@ -959,8 +951,8 @@ class LecunUniform(VarianceScaling):\n      Also available via the shortcut function\n     `tf.keras.initializers.lecun_uniform`.\n \n-    Draws samples from a uniform distribution within `[-limit, limit]`,\n-    where `limit = sqrt(3 / fan_in)` (`fan_in` is the number of input units in the\n+    Draws samples from a uniform distribution within `[-limit, limit]`, where\n+    `limit = sqrt(3 / fan_in)` (`fan_in` is the number of input units in the\n     weight tensor).\n \n     Examples:\n@@ -975,10 +967,9 @@ class LecunUniform(VarianceScaling):\n \n     Args:\n       seed: A Python integer. Used to make the behavior of the initializer\n-        deterministic. Note that a seeded\n-        initializer will not produce the same random values across multiple calls,\n-        but multiple initializers will produce the same sequence when constructed\n-        with the same seed value.\n+        deterministic. Note that a seeded initializer will not produce the same\n+        random values across multiple calls, but multiple initializers will\n+        produce the same sequence when constructed with the same seed value.\n \n     References:\n       - [Klambauer et al., 2017](https://arxiv.org/abs/1706.02515)\n@@ -1003,8 +994,8 @@ class HeNormal(VarianceScaling):\n     `tf.keras.initializers.he_normal`.\n \n     It draws samples from a truncated normal distribution centered on 0 with\n-    `stddev = sqrt(2 / fan_in)` where `fan_in` is the number of input units in the\n-    weight tensor.\n+    `stddev = sqrt(2 / fan_in)` where `fan_in` is the number of input units in\n+    the weight tensor.\n \n     Examples:\n \n@@ -1018,10 +1009,9 @@ class HeNormal(VarianceScaling):\n \n     Args:\n       seed: A Python integer. Used to make the behavior of the initializer\n-        deterministic. Note that a seeded\n-        initializer will not produce the same random values across multiple calls,\n-        but multiple initializers will produce the same sequence when constructed\n-        with the same seed value.\n+        deterministic. Note that a seeded initializer will not produce the same\n+        random values across multiple calls, but multiple initializers will\n+        produce the same sequence when constructed with the same seed value.\n \n     References:\n       - [He et al., 2015](https://arxiv.org/abs/1502.01852)\n@@ -1061,10 +1051,9 @@ class HeUniform(VarianceScaling):\n \n     Args:\n       seed: A Python integer. Used to make the behavior of the initializer\n-        deterministic. Note that a seeded\n-        initializer will not produce the same random values across multiple calls,\n-        but multiple initializers will produce the same sequence when constructed\n-        with the same seed value.\n+        deterministic. Note that a seeded initializer will not produce the same\n+        random values across multiple calls, but multiple initializers will\n+        produce the same sequence when constructed with the same seed value.\n \n     References:\n       - [He et al., 2015](https://arxiv.org/abs/1502.01852)\n@@ -1152,8 +1141,8 @@ def _ensure_keras_seeded():\n     \"\"\"Make sure the keras.backend global seed generator is set.\n \n     This is important for DTensor use case to ensure that each client are\n-    initialized with same seed for tf.random.Generator, so that the value created\n-    are in sync among all the clients.\n+    initialized with same seed for tf.random.Generator, so that the value\n+    created are in sync among all the clients.\n     \"\"\"\n     if not getattr(\n         backend._SEED_GENERATOR, \"generator\", None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4f1d333ded256b0315cf02eee067d6fa902b748d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 54 | Lines Deleted: 42 | Files Changed: 10 | Hunks: 23 | Methods Changed: 22 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 96 | Churn Cumulative: 3331 | Contributors (this commit): 11 | Commits (past 90d): 35 | Contributors (cumulative): 51 | DMM Complexity: None\n\nDIFF:\n@@ -32,7 +32,8 @@ from keras.utils import generic_utils\n # `tf.print` message is only available in stderr in TF2, which this test checks.\n @test_utils.run_v2_only\n class CustomObjectSavingTest(tf.test.TestCase, parameterized.TestCase):\n-    \"\"\"Test for custom Keras object saving with `register_keras_serializable`.\"\"\"\n+    \"\"\"Test for custom Keras object saving with\n+    `register_keras_serializable`.\"\"\"\n \n     def setUp(self):\n         super().setUp()\n\n@@ -49,11 +49,12 @@ class DistributedTrainingTest(tf.test.TestCase):\n             strategy, tf.distribute.experimental.ParameterServerStrategy\n         ):\n             self.skipTest(\n-                \"Parameter Server strategy with dataset creator need to be run when \"\n-                \"eager execution is enabled.\"\n+                \"Parameter Server strategy with dataset creator need to be run \"\n+                \"when eager execution is enabled.\"\n             )\n \n-        # A `dataset_fn` is required for `Model.fit` to work across all strategies.\n+        # A `dataset_fn` is required for `Model.fit` to work across all\n+        # strategies.\n         def dataset_fn(input_context):\n             batch_size = input_context.get_per_replica_batch_size(\n                 global_batch_size=64\n\n@@ -113,11 +113,11 @@ def _forward_over_back_hessian(f, params, use_pfor, dtype=None):\n         (e.g. `tf.float32`) matching the structure of `f`'s returns.\n \n     Returns:\n-      A possibly nested structure of matrix slices corresponding to `params`. Each\n-      slice has shape [P, p_s] where `p_s` is the number of parameters (`tf.size`)\n-      in the corresponding element of `params` and `P` is the total number of\n-      parameters (`sum_s(p_s)`). The full matrix can be obtained by concatenating\n-      along the second axis.\n+      A possibly nested structure of matrix slices corresponding to `params`.\n+      Each slice has shape [P, p_s] where `p_s` is the number of parameters\n+      (`tf.size`) in the corresponding element of `params` and `P` is the total\n+      number of parameters (`sum_s(p_s)`). The full matrix can be obtained by\n+      concatenating along the second axis.\n     \"\"\"\n     return _vectorize_parameters(\n         functools.partial(_hvp, f, params),\n@@ -130,7 +130,8 @@ def _forward_over_back_hessian(f, params, use_pfor, dtype=None):\n def _test_gradients(\n     testcase, f, primals, order, delta=1e-3, rtol=1e-2, atol=1e-6\n ):\n-    \"\"\"Tests forward/backward jacobians of `f`'s [0, `order`)-order gradients.\"\"\"\n+    \"\"\"Tests forward/backward jacobians of `f`'s [0, `order`)-order\n+    gradients.\"\"\"\n     if order < 1:\n         raise ValueError(\n             \"`order` should be a positive integer, got '{}'.\".format(order)\n@@ -325,9 +326,9 @@ class ForwardpropTest(tf.test.TestCase, parameterized.TestCase):\n         parameters = model.embed.variables\n         tangents = [tf.ones_like(v) for v in parameters]\n         with tf.autodiff.ForwardAccumulator(parameters, tangents):\n-            # Note that forwardprop runs alongside the original computation. This test\n-            # is just checking that it doesn't crash; correctness is tested in core\n-            # TF.\n+            # Note that forwardprop runs alongside the original computation.\n+            # This test is just checking that it doesn't crash; correctness is\n+            # tested in core TF.\n             model(\n                 tf.zeros([3, 3], dtype=tf.int32)\n             )  # pylint: disable=not-callable\n\n@@ -75,8 +75,9 @@ class FunctionTest(tf.test.TestCase):\n             self.assertNotIn(\"ValueError\", printed.contents())\n \n         # Shape relaxation passes TensorShape([None, None]), which causes layer\n-        # matmul to fail, due to incompatible dims.  What would have been a graph\n-        # build time error (layer would complain about the inner dim being 4).\n+        # matmul to fail, due to incompatible dims.  What would have been a\n+        # graph build time error (layer would complain about the inner dim being\n+        # 4).\n         with self.captureWritesToStream(sys.stderr) as printed:\n             with self.assertRaisesRegex(\n                 tf.errors.InvalidArgumentError, r\"Matrix size-incompatible\"\n@@ -153,8 +154,8 @@ class FunctionTest(tf.test.TestCase):\n \n         # Verifying if the variables are only referenced from variable_refs.\n         # We expect the reference counter to be 1, but `sys.getrefcount` reports\n-        # one higher reference counter because a temporary is created when we call\n-        # sys.getrefcount().  Hence check if the number returned is 2.\n+        # one higher reference counter because a temporary is created when we\n+        # call sys.getrefcount().  Hence check if the number returned is 2.\n         # https://docs.python.org/3/library/sys.html#sys.getrefcount\n         self.assertEqual(sys.getrefcount(variable_refs[0].deref()), 2)\n         self.assertEqual(sys.getrefcount(variable_refs[1].deref()), 2)\n@@ -226,20 +227,21 @@ class FunctionTest(tf.test.TestCase):\n \n class AutomaticControlDependenciesTest(tf.test.TestCase):\n     def testVariableInitializersCanBeLifted(self):\n-        # The initializer is a stateful op, but using it inside a function should\n-        # *not* create additional dependencies.  That's what we're testing.\n+        # The initializer is a stateful op, but using it inside a function\n+        # should *not* create additional dependencies.  That's what we're\n+        # testing.\n         layer = tf.keras.layers.Dense(1, kernel_initializer=\"glorot_uniform\")\n \n         @tf.function\n         def fn(x):\n             # Stateful operation\n             tf.debugging.Assert(x, [\"Error\"])\n-            # Variable initialization should be lifted.  Prior to the change that\n-            # added this test, the lifting would crash because of an auto control dep\n-            # added on `x`.  Note, the error did not happen if we\n+            # Variable initialization should be lifted.  Prior to the change\n+            # that added this test, the lifting would crash because of an auto\n+            # control dep added on `x`. Note, the error did not happen if we\n             # manually created a tf.Variable outside of function and used it\n-            # here.  Alternatively, creating a tf.Variable inside fn() causes\n-            # a different sort of error that is out of scope for this test.\n+            # here.  Alternatively, creating a tf.Variable inside fn() causes a\n+            # different sort of error that is out of scope for this test.\n             return layer(tf.convert_to_tensor([[1.0, 1.0]]))\n \n         true = tf.convert_to_tensor(True)\n\n@@ -28,7 +28,8 @@ optimizers = tf.keras.optimizers\n def _get_big_cnn_model(\n     img_dim, n_channels, num_partitions, blocks_per_partition\n ):\n-    \"\"\"Creates a test model whose activations are significantly larger than model size.\"\"\"\n+    \"\"\"Creates a test model whose activations are significantly larger than\n+    model size.\"\"\"\n     model = tf.keras.Sequential()\n     model.add(layers.Input(shape=(img_dim, img_dim, n_channels)))\n     for _ in range(num_partitions):\n@@ -54,7 +55,8 @@ def _get_big_cnn_model(\n def _get_split_cnn_model(\n     img_dim, n_channels, num_partitions, blocks_per_partition\n ):\n-    \"\"\"Creates a test model that is split into `num_partitions` smaller models.\"\"\"\n+    \"\"\"Creates a test model that is split into `num_partitions` smaller\n+    models.\"\"\"\n     models = [tf.keras.Sequential() for _ in range(num_partitions)]\n     models[0].add(layers.Input(shape=(img_dim, img_dim, n_channels)))\n     for i in range(num_partitions):\n@@ -133,7 +135,8 @@ def _train_no_recompute(n_steps):\n \n \n def _train_with_recompute(n_steps):\n-    \"\"\"Trains a single large model with gradient checkpointing using tf.recompute_grad.\"\"\"\n+    \"\"\"Trains a single large model with gradient checkpointing using\n+    tf.recompute_grad.\"\"\"\n     img_dim, n_channels, batch_size = 256, 1, 4\n     x, y = _get_dummy_data(img_dim, n_channels, batch_size)\n     # This model is the same model as _get_big_cnn_model but split into 3 parts.\n@@ -195,7 +198,8 @@ class GradientCheckpointTest(tf.test.TestCase):\n     def tearDown(self):\n         super().tearDown()\n         # Make sure all the models created in keras has been deleted and cleared\n-        # from the global keras grpah, also do a force GC to recycle the GPU memory.\n+        # from the global keras grpah, also do a force GC to recycle the GPU\n+        # memory.\n         tf.keras.backend.clear_session()\n         gc.collect()\n \n\n@@ -295,9 +295,9 @@ class LegacyRNNTest(tf.test.TestCase):\n         )\n         fix_weights_generator = tf.keras.layers.SimpleRNNCell(output_shape)\n         fix_weights_generator.build((None, input_shape))\n-        # The SimpleRNNCell contains 3 weights: kernel, recurrent_kernel, and bias\n-        # The BasicRNNCell contains 2 weight: kernel and bias, where kernel is\n-        # zipped [kernel, recurrent_kernel] in SimpleRNNCell.\n+        # The SimpleRNNCell contains 3 weights: kernel, recurrent_kernel, and\n+        # bias The BasicRNNCell contains 2 weight: kernel and bias, where kernel\n+        # is zipped [kernel, recurrent_kernel] in SimpleRNNCell.\n         keras_weights = fix_weights_generator.get_weights()\n         kernel, recurrent_kernel, bias = keras_weights\n         tf_weights = [np.concatenate((kernel, recurrent_kernel)), bias]\n@@ -344,8 +344,8 @@ class LegacyRNNTest(tf.test.TestCase):\n                 weights = model.get_weights()\n                 config = layer.get_config()\n                 # The custom_objects is important here since rnn_cell_impl is\n-                # not visible as a Keras layer, and also has a name conflict with\n-                # keras.LSTMCell and GRUCell.\n+                # not visible as a Keras layer, and also has a name conflict\n+                # with keras.LSTMCell and GRUCell.\n                 layer = tf.keras.layers.RNN.from_config(\n                     config,\n                     custom_objects={\n@@ -380,7 +380,8 @@ class LegacyRNNTest(tf.test.TestCase):\n             kn2_new = KerasNetworkKerasRNNs(name=\"kn2_new\")\n \n         kn2_new(z)  # pylint:disable=not-callable\n-        # Most importantly, this doesn't fail due to variable scope reuse issues.\n+        # Most importantly, this doesn't fail due to variable scope reuse\n+        # issues.\n         kn1_new(z)  # pylint:disable=not-callable\n \n         self.assertTrue(\n\n@@ -71,8 +71,8 @@ class MultiWorkerTutorialTest(parameterized.TestCase, tf.test.TestCase):\n         try:\n             yield\n         except zipfile.BadZipfile as e:\n-            # There can be a race when multiple processes are downloading the data.\n-            # Skip the test if that results in loading errors.\n+            # There can be a race when multiple processes are downloading the\n+            # data.  Skip the test if that results in loading errors.\n             self.skipTest(\n                 \"Data loading error: Bad magic number for file header.\"\n             )\n@@ -285,7 +285,8 @@ class MultiWorkerTutorialTest(parameterized.TestCase, tf.test.TestCase):\n         )\n     )\n     def testMwmsWithCtl(self, mode):\n-        \"\"\"Test multi-worker CTL training flow demo'ed in a to-be-added tutorial.\"\"\"\n+        \"\"\"Test multi-worker CTL training flow demo'ed in a to-be-added\n+        tutorial.\"\"\"\n \n         def proc_func(checkpoint_dir):\n             global_batch_size = PER_WORKER_BATCH_SIZE * NUM_WORKERS\n\n@@ -33,7 +33,8 @@ class MwmsMultiProcessRunnerTest(tf.test.TestCase):\n     def testMwmsWithModelFit(self):\n         def worker_fn():\n             def dataset_fn(input_context):\n-                del input_context  # User should shard data accordingly. Omitted here.\n+                # User should shard data accordingly. Omitted here.\n+                del input_context\n                 return tf.data.Dataset.from_tensor_slices(\n                     (tf.random.uniform((6, 10)), tf.random.uniform((6, 10)))\n                 ).batch(2)\n\n@@ -52,8 +52,8 @@ class PreprocessingAppliedInDatasetCreatorTest(tf.test.TestCase):\n             strategy, tf.distribute.experimental.ParameterServerStrategy\n         ):\n             self.skipTest(\n-                \"Parameter Server strategy with dataset creator need to be run when \"\n-                \"eager execution is enabled.\"\n+                \"Parameter Server strategy with dataset creator need to be run \"\n+                \"when eager execution is enabled.\"\n             )\n         with strategy.scope():\n             preprocessing_model = utils.make_preprocessing_model(\n\n@@ -53,8 +53,8 @@ class PreprocessingAppliedInModelTest(tf.test.TestCase):\n             strategy, tf.distribute.experimental.ParameterServerStrategy\n         ):\n             self.skipTest(\n-                \"Parameter Server strategy with dataset creator need to be run when \"\n-                \"eager execution is enabled.\"\n+                \"Parameter Server strategy with dataset creator need to be run \"\n+                \"when eager execution is enabled.\"\n             )\n         with strategy.scope():\n             preprocessing_model = utils.make_preprocessing_model(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#901f86ac1afee1d1a585037ebc4ac2b1e7e44a1c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 441 | Lines Deleted: 385 | Files Changed: 13 | Hunks: 164 | Methods Changed: 57 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 826 | Churn Cumulative: 19652 | Contributors (this commit): 11 | Commits (past 90d): 53 | Contributors (cumulative): 90 | DMM Complexity: 1.0\n\nDIFF:\n@@ -79,8 +79,8 @@ def keras_style_scope():\n     output_2, next_state_2 = model_2(input, state)\n     ```\n \n-    The solution is to wrap the model construction and execution in a keras-style\n-    scope:\n+    The solution is to wrap the model construction and execution in a\n+    keras-style scope:\n \n     ```python\n     with keras_style_scope():\n@@ -165,13 +165,13 @@ class Layer(base_layer.Layer):\n     Args:\n       trainable: Boolean, whether the layer's variables should be trainable.\n       name: String name of the layer.\n-      dtype: Default dtype of the layer's weights (default of `None` means use the\n-        type of the first input).\n+      dtype: Default dtype of the layer's weights (default of `None` means use\n+        the type of the first input).\n \n     Read-only properties:\n       name: The name of the layer (string).\n-      dtype: Default dtype of the layer's weights (default of `None` means use the\n-        type of the first input).\n+      dtype: Default dtype of the layer's weights (default of `None` means use\n+        the type of the first input).\n       trainable_variables: List of trainable variables.\n       non_trainable_variables: List of non-trainable variables.\n       variables: List of all variables of this layer, trainable and\n@@ -191,8 +191,8 @@ class Layer(base_layer.Layer):\n     \"\"\"\n \n     def __init__(self, trainable=True, name=None, dtype=None, **kwargs):\n-        # For backwards compatibility, legacy layers do not use `ResourceVariable`\n-        # by default.\n+        # For backwards compatibility, legacy layers do not use\n+        # `ResourceVariable` by default.\n         self._use_resource_variables = False\n         scope = kwargs.pop(\"_scope\", None)\n         self._reuse = kwargs.pop(\"_reuse\", None)\n@@ -202,9 +202,9 @@ class Layer(base_layer.Layer):\n         self.built = False\n \n         if dtype is None:\n-            # Indicates to infer dtype from inputs. When the V2 dtype behavior is\n-            # enabled, Keras layers default their dtype to floatx instead, so we pass\n-            # an \"_infer\" policy to keep the old V1 behavior.\n+            # Indicates to infer dtype from inputs. When the V2 dtype behavior\n+            # is enabled, Keras layers default their dtype to floatx instead, so\n+            # we pass an \"_infer\" policy to keep the old V1 behavior.\n             dtype = policy.Policy(\"_infer\")\n \n         if \"autocast\" not in kwargs:\n@@ -218,13 +218,13 @@ class Layer(base_layer.Layer):\n         if _is_in_keras_style_scope():\n             if scope is not None:\n                 raise ValueError(\n-                    \"scope argument not allowed when keras style layers are enabled, \"\n-                    \"but saw: {}\".format(scope)\n+                    \"scope argument not allowed when keras style layers are \"\n+                    \"enabled, but saw: {}\".format(scope)\n                 )\n             if self._reuse is not None:\n                 raise ValueError(\n-                    \"reuse argument not allowed when keras style layers are enabled, \"\n-                    \"but saw: {}\".format(self._reuse)\n+                    \"reuse argument not allowed when keras style layers are \"\n+                    \"enabled, but saw: {}\".format(self._reuse)\n                 )\n             self._keras_style = True\n         else:\n@@ -241,8 +241,8 @@ class Layer(base_layer.Layer):\n     def apply(self, *args, **kwargs):\n         return self(*args, **kwargs)\n \n-    # We no longer track graph in tf.layers layers. This property is only kept to\n-    # maintain API backward compatibility.\n+    # We no longer track graph in tf.layers layers. This property is only kept\n+    # to maintain API backward compatibility.\n     @property\n     def graph(self):\n         warnings.warn(\n@@ -353,12 +353,13 @@ class Layer(base_layer.Layer):\n         partitioner=None,\n         **kwargs\n     ):\n-        \"\"\"Adds a new variable to the layer, or gets an existing one; returns it.\n+        \"\"\"Adds a new variable to the layer, or gets an existing one; returns it\n \n         Args:\n           name: variable name.\n           shape: variable shape.\n-          dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n+          dtype: The type of the variable. Defaults to `self.dtype` or\n+            `float32`.\n           initializer: initializer instance (callable).\n           regularizer: regularizer instance (callable).\n           trainable: whether the variable should be part of the layer's\n@@ -372,10 +373,10 @@ class Layer(base_layer.Layer):\n           use_resource: Whether to use `ResourceVariable`.\n           synchronization: Indicates when a distributed a variable will be\n             aggregated. Accepted values are constants defined in the class\n-            `tf.VariableSynchronization`. By default the synchronization is set to\n-            `AUTO` and the current `DistributionStrategy` chooses\n-            when to synchronize. If `synchronization` is set to `ON_READ`,\n-            `trainable` must not be set to `True`.\n+            `tf.VariableSynchronization`. By default the synchronization is set\n+            to `AUTO` and the current `DistributionStrategy` chooses when to\n+            synchronize. If `synchronization` is set to `ON_READ`, `trainable`\n+            must not be set to `True`.\n           aggregation: Indicates how a distributed variable will be aggregated.\n             Accepted values are constants defined in the class\n             `tf.VariableAggregation`.\n@@ -384,15 +385,15 @@ class Layer(base_layer.Layer):\n             into multiple partitions according to `partitioner`.  In this case,\n             an instance of `PartitionedVariable` is returned.  Available\n             partitioners include `tf.compat.v1.fixed_size_partitioner` and\n-            `tf.compat.v1.variable_axis_size_partitioner`.  For more details, see\n-            the documentation of `tf.compat.v1.get_variable` and the  \"Variable\n-            Partitioners and Sharding\" section of the API guide.\n+            `tf.compat.v1.variable_axis_size_partitioner`.  For more details,\n+            see the documentation of `tf.compat.v1.get_variable` and the\n+            \"Variable Partitioners and Sharding\" section of the API guide.\n           **kwargs: Additional keyword arguments.\n \n         Returns:\n-          The created variable.  Usually either a `Variable` or `ResourceVariable`\n-          instance.  If `partitioner` is not `None`, a `PartitionedVariable`\n-          instance is returned.\n+          The created variable.  Usually either a `Variable` or\n+          `ResourceVariable` instance.  If `partitioner` is not `None`, a\n+          `PartitionedVariable` instance is returned.\n \n         Raises:\n           RuntimeError: If called with partitioned variable regularization and\n@@ -423,12 +424,13 @@ class Layer(base_layer.Layer):\n             if trainable:\n                 raise ValueError(\n                     \"Synchronization value can be set to \"\n-                    \"VariableSynchronization.ON_READ only for non-trainable variables. \"\n-                    \"You have specified trainable=True and \"\n+                    \"VariableSynchronization.ON_READ only for non-trainable \"\n+                    \"variables. You have specified trainable=True and \"\n                     \"synchronization=VariableSynchronization.ON_READ.\"\n                 )\n             else:\n-                # Set trainable to be false when variable is to be synced on read.\n+                # Set trainable to be false when variable is to be synced on\n+                # read.\n                 trainable = False\n         elif trainable is None:\n             trainable = True\n@@ -449,15 +451,17 @@ class Layer(base_layer.Layer):\n                 with tf.init_scope():\n                     # Retrieve the variables from the graph into which variables\n                     # will be lifted; if initialization ops will be lifted into\n-                    # the eager context, then there is nothing to retrieve, since variable\n-                    # collections are not supported when eager execution is enabled.\n+                    # the eager context, then there is nothing to retrieve,\n+                    # since variable collections are not supported when eager\n+                    # execution is enabled.\n                     if not tf.executing_eagerly():\n                         init_graph = tf.compat.v1.get_default_graph()\n                         existing_variables = set(\n                             tf.compat.v1.global_variables()\n                         )\n             else:\n-                # Initialization ops will not be lifted out of the default graph.\n+                # Initialization ops will not be lifted out of the default\n+                # graph.\n                 init_graph = default_graph\n                 existing_variables = set(tf.compat.v1.global_variables())\n \n@@ -507,16 +511,17 @@ class Layer(base_layer.Layer):\n                         var_store = (\n                             vs._get_default_variable_store()\n                         )  # pylint: disable=protected-access\n-                        # When the shim to get variable scope working in TF2 is used,\n-                        # We need to explicitly make the shim track the regularization\n-                        # losses as the collections will not be accessible.\n+                        # When the shim to get variable scope working in TF2 is\n+                        # used, We need to explicitly make the shim track the\n+                        # regularization losses as the collections will not be\n+                        # accessible.\n                         if hasattr(var_store, \"add_regularizer\"):\n                             var_store.add_regularizer(variable, regularizer)\n \n                 if init_graph is not None:\n-                    # Handle edge case where a custom getter has overridden `trainable`.\n-                    # There is one known occurrence of this, in unit test\n-                    # testBasicRNNCellNotTrainable in\n+                    # Handle edge case where a custom getter has overridden\n+                    # `trainable`.  There is one known occurrence of this, in\n+                    # unit test testBasicRNNCellNotTrainable in\n                     # contrib.rnn.python.kernel_tests.core_rnn_cell_test\n                     with init_graph.as_default():\n                         trainable_variables = tf.compat.v1.trainable_variables()\n@@ -525,7 +530,8 @@ class Layer(base_layer.Layer):\n                         and self.trainable\n                         and variable not in trainable_variables\n                     ):\n-                        # A custom getter / variable scope overrode the trainable flag.\n+                        # A custom getter / variable scope overrode the\n+                        # trainable flag.\n                         extra_trainable_vars = self._trainable_weights[\n                             prev_len_trainable:\n                         ]\n@@ -548,8 +554,8 @@ class Layer(base_layer.Layer):\n           Output tensor(s).\n \n         Note:\n-          - If the layer's `call` method takes a `scope` keyword argument,\n-            this argument will be automatically set to the current variable scope.\n+          - If the layer's `call` method takes a `scope` keyword argument, this\n+            argument will be automatically set to the current variable scope.\n           - If the layer's `call` method takes a `mask` argument (as some Keras\n             layers do), its default value will be set to the mask generated\n             for `inputs` by the previous layer (if `input` did come from\n@@ -557,15 +563,16 @@ class Layer(base_layer.Layer):\n             a Keras layer with masking support.\n \n         Raises:\n-          ValueError: if the layer's `call` method returns None (an invalid value).\n+          ValueError: if the layer's `call` method returns None (an invalid\n+            value).\n         \"\"\"\n         scope = kwargs.pop(\"scope\", None)\n \n         if self._keras_style:\n             if scope is not None:\n                 raise ValueError(\n-                    \"scope argument not allowed when keras style layers are enabled, \"\n-                    \"but saw: {}\".format(scope)\n+                    \"scope argument not allowed when keras style layers are \"\n+                    \"enabled, but saw: {}\".format(scope)\n                 )\n             return super().__call__(inputs, *args, **kwargs)\n \n@@ -573,8 +580,9 @@ class Layer(base_layer.Layer):\n \n         if self.built:\n             try:\n-                # Some classes which inherit from Layer do not use its constructor, so\n-                # rather than initializing to None we check for an AttributeError.\n+                # Some classes which inherit from Layer do not use its\n+                # constructor, so rather than initializing to None we check for\n+                # an AttributeError.\n                 scope_context_manager = (\n                     self._always_reuse_variable_scope\n                 )  # pylint: disable=access-member-before-definition\n@@ -582,16 +590,17 @@ class Layer(base_layer.Layer):\n                 scope_context_manager = None\n \n             if scope_context_manager is None:\n-                # From this point we will always set reuse=True, so create a \"final\"\n-                # variable scope with this setting. We avoid re-creating variable scopes\n-                # after this point as an optimization.\n+                # From this point we will always set reuse=True, so create a\n+                # \"final\" variable scope with this setting. We avoid re-creating\n+                # variable scopes after this point as an optimization.\n                 scope_context_manager = tf.compat.v1.variable_scope(\n                     self._scope, reuse=True, auxiliary_name_scope=False\n                 )\n \n-                # Do not cache variable scopes if Eager mode is enabled. If Eager mode\n-                # is enabled then we don't want to reuse scopes because the cached scope\n-                # might be from a FuncGraph or Eager scope we are no longer in.\n+                # Do not cache variable scopes if Eager mode is enabled. If\n+                # Eager mode is enabled then we don't want to reuse scopes\n+                # because the cached scope might be from a FuncGraph or Eager\n+                # scope we are no longer in.\n                 if not tf.compat.v1.executing_eagerly_outside_functions():\n                     self._always_reuse_variable_scope = scope_context_manager\n         else:\n@@ -641,14 +650,16 @@ class Layer(base_layer.Layer):\n         return result\n \n     def __setattr__(self, value, name):\n-        # By-pass the automatic dependency tracking performed by the parent Layer.\n+        # By-pass the automatic dependency tracking performed by the parent\n+        # Layer.\n         super(tf.__internal__.tracking.Trackable, self).__setattr__(\n             value, name\n         )  # pylint: disable=bad-super-call\n \n     @property\n     def _is_legacy_layer(self):\n-        \"\"\"Used by keras to check compatibility. This should not be overridden.\"\"\"\n+        \"\"\"Used by keras to check compatibility. This should not be\n+        overridden.\"\"\"\n         return True\n \n \n\n@@ -326,8 +326,8 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n         with self.assertRaisesRegex(ValueError, r\"expected ndim=2\"):\n             layer(tf.constant([1]))\n \n-        # Note that we re-create the layer since in Eager mode, input spec checks\n-        # only happen on first call.\n+        # Note that we re-create the layer since in Eager mode, input spec\n+        # checks only happen on first call.\n         # Works\n         layer = CustomerLayer()\n         layer(tf.constant([[1], [2]]))\n@@ -576,8 +576,9 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n         with outer_graph.as_default():\n             with function_building_graph.as_default():\n                 layer = MyLayer()\n-                # Create a variable by invoking build through __call__ and assert that\n-                # it is both tracked and lifted into the outer graph.\n+                # Create a variable by invoking build through __call__ and\n+                # assert that it is both tracked and lifted into the outer\n+                # graph.\n                 inputs = tf.compat.v1.placeholder(tf.float32, (), \"inputs\")\n                 layer(inputs)\n                 self.assertEqual(len(layer.variables), 1)\n\n@@ -52,8 +52,8 @@ class Conv1D(keras_layers.Conv1D, base.Layer):\n         `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n         the left/right or up/down of the input such that output has the same\n         height/width dimension as the input.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`. The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch, length, channels)` while `channels_first` corresponds to\n         inputs with shape `(batch, channels, length)`.\n@@ -200,8 +200,8 @@ def conv1d(\n         `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n         the left/right or up/down of the input such that output has the same\n         height/width dimension as the input.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`. The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch, length, channels)` while `channels_first` corresponds to\n         inputs with shape `(batch, channels, length)`.\n@@ -331,8 +331,8 @@ class Conv2D(keras_layers.Conv2D, base.Layer):\n         `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n         the left/right or up/down of the input such that output has the same\n         height/width dimension as the input.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`. The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch, height, width, channels)` while `channels_first` corresponds to\n         inputs with shape `(batch, channels, height, width)`.\n@@ -487,8 +487,8 @@ def conv2d(\n         `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n         the left/right or up/down of the input such that output has the same\n         height/width dimension as the input.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`. The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch, height, width, channels)` while `channels_first` corresponds to\n         inputs with shape `(batch, channels, height, width)`.\n@@ -622,8 +622,8 @@ class Conv3D(keras_layers.Conv3D, base.Layer):\n         `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n         the left/right or up/down of the input such that output has the same\n         height/width dimension as the input.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`. The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch, depth, height, width, channels)` while `channels_first`\n         corresponds to inputs with shape\n@@ -779,8 +779,8 @@ def conv3d(\n         `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n         the left/right or up/down of the input such that output has the same\n         height/width dimension as the input.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`. The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch, depth, height, width, channels)` while `channels_first`\n         corresponds to inputs with shape\n@@ -894,7 +894,8 @@ class SeparableConv1D(keras_layers.SeparableConv1D, base.Layer):\n     channels, followed by a pointwise convolution that mixes channels.\n     If `use_bias` is True and a bias initializer is provided,\n     it adds a bias vector to the output.\n-    It then optionally applies an activation function to produce the final output.\n+    It then optionally applies an activation function to produce the final\n+    output.\n \n     Args:\n       filters: Integer, the dimensionality of the output space (i.e. the number\n@@ -909,8 +910,8 @@ class SeparableConv1D(keras_layers.SeparableConv1D, base.Layer):\n         `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n         the left/right or up/down of the input such that output has the same\n         height/width dimension as the input.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`. The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch, length, channels)` while `channels_first` corresponds to\n         inputs with shape `(batch, channels, length)`.\n@@ -924,8 +925,10 @@ class SeparableConv1D(keras_layers.SeparableConv1D, base.Layer):\n       activation: Activation function. Set it to None to maintain a\n         linear activation.\n       use_bias: Boolean, whether the layer uses a bias.\n-      depthwise_initializer: An initializer for the depthwise convolution kernel.\n-      pointwise_initializer: An initializer for the pointwise convolution kernel.\n+      depthwise_initializer: An initializer for the depthwise convolution\n+        kernel.\n+      pointwise_initializer: An initializer for the pointwise convolution\n+        kernel.\n       bias_initializer: An initializer for the bias vector. If None, the default\n         initializer will be used.\n       depthwise_regularizer: Optional regularizer for the depthwise\n@@ -1039,8 +1042,8 @@ class SeparableConv2D(keras_layers.SeparableConv2D, base.Layer):\n     This layer performs a depthwise convolution that acts separately on\n     channels, followed by a pointwise convolution that mixes channels.\n     If `use_bias` is True and a bias initializer is provided,\n-    it adds a bias vector to the output.\n-    It then optionally applies an activation function to produce the final output.\n+    it adds a bias vector to the output. It then optionally applies an\n+    activation function to produce the final output.\n \n     Args:\n       filters: Integer, the dimensionality of the output space (i.e. the number\n@@ -1049,16 +1052,16 @@ class SeparableConv2D(keras_layers.SeparableConv2D, base.Layer):\n         dimensions of the filters. Can be a single integer to specify the same\n         value for all spatial dimensions.\n       strides: A tuple or list of 2 positive integers specifying the strides\n-        of the convolution. Can be a single integer to specify the same value for\n-        all spatial dimensions.\n+        of the convolution. Can be a single integer to specify the same value\n+        for all spatial dimensions.\n         Specifying any `stride` value != 1 is incompatible with specifying\n         any `dilation_rate` value != 1.\n       padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n         `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n         the left/right or up/down of the input such that output has the same\n         height/width dimension as the input.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`. The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch, height, width, channels)` while `channels_first` corresponds to\n         inputs with shape `(batch, channels, height, width)`.\n@@ -1075,8 +1078,10 @@ class SeparableConv2D(keras_layers.SeparableConv2D, base.Layer):\n       activation: Activation function. Set it to None to maintain a\n         linear activation.\n       use_bias: Boolean, whether the layer uses a bias.\n-      depthwise_initializer: An initializer for the depthwise convolution kernel.\n-      pointwise_initializer: An initializer for the pointwise convolution kernel.\n+      depthwise_initializer: An initializer for the depthwise convolution\n+        kernel.\n+      pointwise_initializer: An initializer for the pointwise convolution\n+        kernel.\n       bias_initializer: An initializer for the bias vector. If None, the default\n         initializer will be used.\n       depthwise_regularizer: Optional regularizer for the depthwise\n@@ -1214,8 +1219,8 @@ def separable_conv1d(\n     This layer performs a depthwise convolution that acts separately on\n     channels, followed by a pointwise convolution that mixes channels.\n     If `use_bias` is True and a bias initializer is provided,\n-    it adds a bias vector to the output.\n-    It then optionally applies an activation function to produce the final output.\n+    it adds a bias vector to the output. It then optionally applies an\n+    activation function to produce the final output.\n \n     Args:\n       inputs: Input tensor.\n@@ -1231,8 +1236,8 @@ def separable_conv1d(\n         `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n         the left/right or up/down of the input such that output has the same\n         height/width dimension as the input.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`. The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch, length, channels)` while `channels_first` corresponds to\n         inputs with shape `(batch, channels, length)`.\n@@ -1246,8 +1251,10 @@ def separable_conv1d(\n       activation: Activation function. Set it to None to maintain a\n         linear activation.\n       use_bias: Boolean, whether the layer uses a bias.\n-      depthwise_initializer: An initializer for the depthwise convolution kernel.\n-      pointwise_initializer: An initializer for the pointwise convolution kernel.\n+      depthwise_initializer: An initializer for the depthwise convolution\n+        kernel.\n+      pointwise_initializer: An initializer for the pointwise convolution\n+        kernel.\n       bias_initializer: An initializer for the bias vector. If None, the default\n         initializer will be used.\n       depthwise_regularizer: Optional regularizer for the depthwise\n@@ -1380,8 +1387,8 @@ def separable_conv2d(\n     This layer performs a depthwise convolution that acts separately on\n     channels, followed by a pointwise convolution that mixes channels.\n     If `use_bias` is True and a bias initializer is provided,\n-    it adds a bias vector to the output.\n-    It then optionally applies an activation function to produce the final output.\n+    it adds a bias vector to the output. It then optionally applies an\n+    activation function to produce the final output.\n \n     Args:\n       inputs: Input tensor.\n@@ -1391,16 +1398,15 @@ def separable_conv2d(\n         dimensions of the filters. Can be a single integer to specify the same\n         value for all spatial dimensions.\n       strides: A tuple or list of 2 positive integers specifying the strides\n-        of the convolution. Can be a single integer to specify the same value for\n-        all spatial dimensions.\n-        Specifying any `stride` value != 1 is incompatible with specifying\n-        any `dilation_rate` value != 1.\n+        of the convolution. Can be a single integer to specify the same value\n+        for all spatial dimensions. Specifying any `stride` value != 1 is\n+        incompatible with specifying any `dilation_rate` value != 1.\n       padding: One of `\"valid\"` or `\"same\"` (case-insensitive).\n         `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n         the left/right or up/down of the input such that output has the same\n         height/width dimension as the input.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`. The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch, height, width, channels)` while `channels_first` corresponds to\n         inputs with shape `(batch, channels, height, width)`.\n@@ -1417,8 +1423,10 @@ def separable_conv2d(\n       activation: Activation function. Set it to None to maintain a\n         linear activation.\n       use_bias: Boolean, whether the layer uses a bias.\n-      depthwise_initializer: An initializer for the depthwise convolution kernel.\n-      pointwise_initializer: An initializer for the pointwise convolution kernel.\n+      depthwise_initializer: An initializer for the depthwise convolution\n+        kernel.\n+      pointwise_initializer: An initializer for the pointwise convolution\n+        kernel.\n       bias_initializer: An initializer for the bias vector. If None, the default\n         initializer will be used.\n       depthwise_regularizer: Optional regularizer for the depthwise\n@@ -1538,14 +1546,14 @@ class Conv2DTranspose(keras_layers.Conv2DTranspose, base.Layer):\n         dimensions of the filters. Can be a single integer to specify the same\n         value for all spatial dimensions.\n       strides: A tuple or list of 2 positive integers specifying the strides\n-        of the convolution. Can be a single integer to specify the same value for\n-        all spatial dimensions.\n+        of the convolution. Can be a single integer to specify the same value\n+        for all spatial dimensions.\n       padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n         `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n         the left/right or up/down of the input such that output has the same\n         height/width dimension as the input.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`. The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch, height, width, channels)` while `channels_first` corresponds to\n         inputs with shape `(batch, channels, height, width)`.\n@@ -1682,14 +1690,14 @@ def conv2d_transpose(\n         dimensions of the filters. Can be a single integer to specify the same\n         value for all spatial dimensions.\n       strides: A tuple or list of 2 positive integers specifying the strides\n-        of the convolution. Can be a single integer to specify the same value for\n-        all spatial dimensions.\n+        of the convolution. Can be a single integer to specify the same value\n+        for all spatial dimensions.\n       padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n         `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n         the left/right or up/down of the input such that output has the same\n         height/width dimension as the input.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`. The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch, height, width, channels)` while `channels_first` corresponds to\n         inputs with shape `(batch, channels, height, width)`.\n@@ -1697,8 +1705,8 @@ def conv2d_transpose(\n         linear activation.\n       use_bias: Boolean, whether the layer uses a bias.\n       kernel_initializer: An initializer for the convolution kernel.\n-      bias_initializer: An initializer for the bias vector. If `None`, the default\n-        initializer will be used.\n+      bias_initializer: An initializer for the bias vector. If `None`, the\n+        default initializer will be used.\n       kernel_regularizer: Optional regularizer for the convolution kernel.\n       bias_regularizer: Optional regularizer for the bias vector.\n       activity_regularizer: Optional regularizer function for the output.\n@@ -1807,8 +1815,8 @@ class Conv3DTranspose(keras_layers.Conv3DTranspose, base.Layer):\n         `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n         the left/right or up/down of the input such that output has the same\n         height/width dimension as the input.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`. The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch, depth, height, width, channels)` while `channels_first`\n         corresponds to inputs with shape\n@@ -1817,8 +1825,8 @@ class Conv3DTranspose(keras_layers.Conv3DTranspose, base.Layer):\n         linear activation.\n       use_bias: Boolean, whether the layer uses a bias.\n       kernel_initializer: An initializer for the convolution kernel.\n-      bias_initializer: An initializer for the bias vector. If `None`, the default\n-        initializer will be used.\n+      bias_initializer: An initializer for the bias vector. If `None`, the\n+        default initializer will be used.\n       kernel_regularizer: Optional regularizer for the convolution kernel.\n       bias_regularizer: Optional regularizer for the bias vector.\n       activity_regularizer: Optional regularizer function for the output.\n@@ -1939,14 +1947,14 @@ def conv3d_transpose(\n         dimensions of the filters. Can be a single integer to specify the same\n         value for all spatial dimensions.\n       strides: A tuple or list of 3 positive integers specifying the strides\n-        of the convolution. Can be a single integer to specify the same value for\n-        all spatial dimensions.\n+        of the convolution. Can be a single integer to specify the same value\n+        for all spatial dimensions.\n       padding: one of `\"valid\"` or `\"same\"` (case-insensitive).\n         `\"valid\"` means no padding. `\"same\"` results in padding evenly to\n         the left/right or up/down of the input such that output has the same\n         height/width dimension as the input.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`. The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch, depth, height, width, channels)` while `channels_first`\n         corresponds to inputs with shape\n\n@@ -321,7 +321,8 @@ class ConvTest(tf.test.TestCase):\n                 self.assertTrue(\"bias\" in weights[1].name)\n                 self.evaluate(tf.compat.v1.global_variables_initializer())\n                 weights = self.evaluate(weights)\n-                # Check that the kernel weights got initialized to ones (from scope)\n+                # Check that the kernel weights got initialized to ones (from\n+                # scope)\n                 self.assertAllClose(weights[0], np.ones((3, 3, 3, 32)))\n                 # Check that the bias still got initialized to zeros.\n                 self.assertAllClose(weights[1], np.zeros((32)))\n@@ -806,7 +807,8 @@ class SeparableConv2DTest(tf.test.TestCase):\n                 self.assertTrue(\"bias\" in weights[2].name)\n                 self.evaluate(tf.compat.v1.global_variables_initializer())\n                 weights = self.evaluate(weights)\n-                # Check that the kernel weights got initialized to ones (from scope)\n+                # Check that the kernel weights got initialized to ones (from\n+                # scope)\n                 self.assertAllClose(weights[0], np.ones((3, 3, 3, 1)))\n                 self.assertAllClose(weights[1], np.ones((1, 1, 3, 32)))\n                 # Check that the bias still got initialized to zeros.\n@@ -1115,7 +1117,8 @@ class Conv2DTransposeTest(tf.test.TestCase):\n                 self.assertTrue(\"bias\" in weights[1].name)\n                 self.evaluate(tf.compat.v1.global_variables_initializer())\n                 weights = self.evaluate(weights)\n-                # Check that the kernel weights got initialized to ones (from scope)\n+                # Check that the kernel weights got initialized to ones (from\n+                # scope)\n                 self.assertAllClose(weights[0], np.ones((3, 3, 32, 3)))\n                 # Check that the bias still got initialized to zeros.\n                 self.assertAllClose(weights[1], np.zeros((32)))\n@@ -1356,7 +1359,8 @@ class Conv3DTransposeTest(tf.test.TestCase):\n                 self.assertTrue(\"bias\" in weights[1].name)\n                 self.evaluate(tf.compat.v1.global_variables_initializer())\n                 weights = self.evaluate(weights)\n-                # Check that the kernel weights got initialized to ones (from scope)\n+                # Check that the kernel weights got initialized to ones (from\n+                # scope)\n                 self.assertAllClose(weights[0], np.ones((3, 3, 3, 4, 32)))\n                 # Check that the bias still got initialized to zeros.\n                 self.assertAllClose(weights[1], np.zeros((4)))\n\n@@ -66,7 +66,8 @@ class Dense(keras_layers.Dense, base.Layer):\n       trainable: Boolean, if `True` also add variables to the graph collection\n         `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`).\n       name: String, the name of the layer. Layers with the same name will\n-        share weights, but to avoid mistakes we require reuse=True in such cases.\n+        share weights, but to avoid mistakes we require reuse=True in such\n+        cases.\n       _reuse: Boolean, whether to reuse the weights of a previous layer\n         by the same name.\n \n@@ -423,8 +424,8 @@ class Flatten(keras_layers.Flatten, base.Layer):\n     \"\"\"Flattens an input tensor while preserving the batch axis (axis 0).\n \n     Args:\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`. The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch, ..., channels)` while `channels_first` corresponds to\n         inputs with shape `(batch, channels, ...)`.\n@@ -482,8 +483,8 @@ def flatten(inputs, name=None, data_format=\"channels_last\"):\n     Args:\n       inputs: Tensor input.\n       name: The name of the layer (string).\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`. The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch, height, width, channels)` while `channels_first` corresponds to\n         inputs with shape `(batch, channels, height, width)`.\n\n@@ -350,7 +350,8 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n             self.evaluate(tf.compat.v1.global_variables_initializer())\n             weights = _get_variable_dict_from_varstore()\n             self.assertEqual(len(weights), 2)\n-            # Check that the matrix weights got initialized to ones (from scope).\n+            # Check that the matrix weights got initialized to ones (from\n+            # scope).\n             self.assertAllClose(\n                 weights[\"scope/dense/kernel\"].read_value(), np.ones((3, 2))\n             )\n\n@@ -15,27 +15,28 @@ from tensorflow.python.util.tf_export import keras_export\n class DeterministicRandomTestTool(object):\n     \"\"\"DeterministicRandomTestTool is a testing tool.\n \n-    This tool is used to validate random number generation semantics match between\n-    TF1.x graphs/sessions and eager execution.\n+    This tool is used to validate random number generation semantics match\n+    between TF1.x graphs/sessions and eager execution.\n \n-    This is useful when you are migrating from TF 1.x to TF2 and need to make sure\n-    your computation is still happening correctly along the way. See the\n+    This is useful when you are migrating from TF 1.x to TF2 and need to make\n+    sure your computation is still happening correctly along the way. See the\n     validating correctness migration guide for more info:\n     https://www.tensorflow.org/guide/migrate/validate_correctness\n \n     The following DeterministicRandomTestTool object provides a context manager\n-    scope() that can make stateful random operations use the same seed across both\n-    TF1 graphs/sessions and eager execution,The tool provides two testing modes:\n+    scope() that can make stateful random operations use the same seed across\n+    both TF1 graphs/sessions and eager execution,The tool provides two testing\n+    modes:\n     - constant which uses the same seed for every single operation no matter how\n     many times it has been called and,\n-    - num_random_ops which uses the number of previously-observed stateful random\n-    operations as the operation seed.\n+    - num_random_ops which uses the number of previously-observed stateful\n+    random operations as the operation seed.\n     The num_random_ops mode serves as a more sensitive validation check than the\n-    constant mode. It ensures that the random numbers initialization does not get\n-    accidentaly reused.(for example if several weights take on the same\n+    constant mode. It ensures that the random numbers initialization does not\n+    get accidentaly reused.(for example if several weights take on the same\n     initializations), you can use the num_random_ops mode to avoid this. In the\n-    num_random_ops mode, the generated random numbers will depend on the ordering\n-    of random ops in the program.\n+    num_random_ops mode, the generated random numbers will depend on the\n+    ordering of random ops in the program.\n \n     This applies both to the stateful random operations used for creating and\n     initializing variables, and to the stateful random operations used in\n@@ -43,7 +44,8 @@ class DeterministicRandomTestTool(object):\n     \"\"\"\n \n     def __init__(self, seed: int = 42, mode=\"constant\"):\n-        \"\"\"Set mode to 'constant' or 'num_random_ops'. Defaults to 'constant'.\"\"\"\n+        \"\"\"Set mode to 'constant' or 'num_random_ops'. Defaults to\n+        'constant'.\"\"\"\n         if mode not in {\"constant\", \"num_random_ops\"}:\n             raise ValueError(\n                 \"Mode arg must be 'constant' or 'num_random_ops'. \"\n@@ -71,10 +73,10 @@ class DeterministicRandomTestTool(object):\n         def _get_seed(_):\n             \"\"\"Wraps TF get_seed to make deterministic random generation easier.\n \n-            This makes a variable's initialization (and calls that involve random\n-            number generation) depend only on how many random number generations\n-            were used in the scope so far, rather than on how many unrelated\n-            operations the graph contains.\n+            This makes a variable's initialization (and calls that involve\n+            random number generation) depend only on how many random number\n+            generations were used in the scope so far, rather than on how many\n+            unrelated operations the graph contains.\n \n             Returns:\n               Random seed tuple.\n@@ -85,14 +87,15 @@ class DeterministicRandomTestTool(object):\n             else:\n                 if op_seed in self._observed_seeds:\n                     raise ValueError(\n-                        \"This `DeterministicRandomTestTool` object is trying to re-use the \"\n+                        \"This `DeterministicRandomTestTool` \"\n+                        \"object is trying to re-use the \"\n                         + \"already-used operation seed {}. \".format(op_seed)\n-                        + \"It cannot guarantee random numbers will match between eager \"\n-                        + \"and sessions when an operation seed is reused. \"\n-                        + \"You most likely set \"\n-                        + \"`operation_seed` explicitly but used a value that caused the \"\n-                        + \"naturally-incrementing operation seed sequences to overlap \"\n-                        + \"with an already-used seed.\"\n+                        + \"It cannot guarantee random numbers will match \"\n+                        + \"between eager and sessions when an operation seed \"\n+                        + \"is reused. You most likely set \"\n+                        + \"`operation_seed` explicitly but used a value that \"\n+                        + \"caused the naturally-incrementing operation seed \"\n+                        + \"sequences to overlap with an already-used seed.\"\n                     )\n \n                 self._observed_seeds.add(op_seed)\n@@ -100,8 +103,8 @@ class DeterministicRandomTestTool(object):\n \n             return (self._seed, op_seed)\n \n-        # mock.patch internal symbols to modify the behavior of TF APIs relying on\n-        # them\n+        # mock.patch internal symbols to modify the behavior of TF APIs relying\n+        # on them\n \n         return tf.compat.v1.test.mock.patch.object(\n             self.seed_implementation, \"get_seed\", wraps=_get_seed\n\n@@ -22,8 +22,8 @@ class DeterministicRandomTestToolTest(tf.test.TestCase):\n             graph = tf.Graph()\n             with graph.as_default(), tf.compat.v1.Session(graph=graph) as sess:\n                 a = tf.compat.v1.random.uniform(shape=(3, 1))\n-                # adding additional computation/ops to the graph and ensuring consistant\n-                # random number generation\n+                # adding additional computation/ops to the graph and ensuring\n+                # consistant random number generation\n                 a = a * 3\n                 b = tf.compat.v1.random.uniform(shape=(3, 3))\n                 b = b * 3\n@@ -46,9 +46,9 @@ class DeterministicRandomTestToolTest(tf.test.TestCase):\n         self.assertAllClose(graph_b, b)\n         self.assertAllClose(graph_c, c)\n         self.assertAllClose(graph_d, d)\n-        # In constant mode, because b and c were generated with the same seed within\n-        # the same scope and have the same shape, they will have exactly the same\n-        # values.\n+        # In constant mode, because b and c were generated with the same seed\n+        # within the same scope and have the same shape, they will have exactly\n+        # the same values.\n         # validate that b and c are the same, also graph_b and graph_c\n         self.assertAllClose(b, c)\n         self.assertAllClose(graph_b, graph_c)\n@@ -63,8 +63,8 @@ class DeterministicRandomTestToolTest(tf.test.TestCase):\n         with random_tool.scope():\n             graph = tf.Graph()\n             with graph.as_default(), tf.compat.v1.Session(graph=graph) as sess:\n-                # adding additional computation/ops to the graph and ensuring consistant\n-                # random number generation\n+                # adding additional computation/ops to the graph and ensuring\n+                # consistant random number generation\n                 a = tf.compat.v1.random.uniform(shape=(3, 1), seed=1234)\n                 a = a * 3\n                 b = tf.compat.v1.random.uniform(shape=(3, 3), seed=1234)\n@@ -97,8 +97,8 @@ class DeterministicRandomTestToolTest(tf.test.TestCase):\n         with random_tool.scope():\n             graph = tf.Graph()\n             with graph.as_default(), tf.compat.v1.Session(graph=graph) as sess:\n-                # adding additional computation/ops to the graph and ensuring consistant\n-                # random number generation\n+                # adding additional computation/ops to the graph and ensuring\n+                # consistant random number generation\n                 a = tf.compat.v1.random.uniform(shape=(3, 1))\n                 a = a * 3\n                 b = tf.compat.v1.random.uniform(shape=(3, 3))\n@@ -133,16 +133,16 @@ class DeterministicRandomTestToolTest(tf.test.TestCase):\n     def test_num_rand_ops_program_order(self):\n         \"\"\"Test random tensor generation consistancy in num_random_ops mode.\n \n-        validate that in this mode random number generation is sensitive to program\n-        order, so the generated random tesnors should not match.\n+        validate that in this mode random number generation is sensitive to\n+        program order, so the generated random tesnors should not match.\n         \"\"\"\n         random_tool = migration_utils.DeterministicRandomTestTool(\n             mode=\"num_random_ops\"\n         )\n         with random_tool.scope():\n             a = tf.random.uniform(shape=(3, 1))\n-            # adding additional computation/ops to the graph and ensuring consistant\n-            # random number generation\n+            # adding additional computation/ops to the graph and ensuring\n+            # consistant random number generation\n             a = a * 3\n             b = tf.random.uniform(shape=(3, 3))\n             b = b * 3\n@@ -152,8 +152,8 @@ class DeterministicRandomTestToolTest(tf.test.TestCase):\n         )\n         with random_tool.scope():\n             b_prime = tf.random.uniform(shape=(3, 3))\n-            # adding additional computation/ops to the graph and ensuring consistant\n-            # random number generation\n+            # adding additional computation/ops to the graph and ensuring\n+            # consistant random number generation\n             b_prime = b_prime * 3\n             a_prime = tf.random.uniform(shape=(3, 1))\n             a_prime = a_prime * 3\n\n@@ -34,9 +34,10 @@ class BatchNormalization(batch_normalization_v1.BatchNormalization, base.Layer):\n     \"\"\"Batch Normalization layer from (Ioffe et al., 2015).\n \n     Keras APIs handle BatchNormalization updates to the moving_mean and\n-    moving_variance as part of their `fit()` and `evaluate()` loops. However, if a\n-    custom training loop is used with an instance of `Model`, these updates need\n-    to be explicitly included.  Here's a simple example of how it can be done:\n+    moving_variance as part of their `fit()` and `evaluate()` loops. However, if\n+    a custom training loop is used with an instance of `Model`, these updates\n+    need to be explicitly included.  Here's a simple example of how it can be\n+    done:\n \n     ```python\n       # model is an instance of Model that contains BatchNormalization layer.\n@@ -46,38 +47,38 @@ class BatchNormalization(batch_normalization_v1.BatchNormalization, base.Layer):\n     ```\n \n     Args:\n-      axis: An `int` or list of `int`, the axis or axes that should be normalized,\n-        typically the features axis/axes. For instance, after a `Conv2D` layer\n-        with `data_format=\"channels_first\"`, set `axis=1`. If a list of axes is\n-        provided, each axis in `axis` will be normalized\n+      axis: An `int` or list of `int`, the axis or axes that should be\n+        normalized, typically the features axis/axes. For instance, after a\n+        `Conv2D` layer with `data_format=\"channels_first\"`, set `axis=1`. If a\n+        list of axes is provided, each axis in `axis` will be normalized\n         simultaneously. Default is `-1` which uses the last axis. Note: when\n         using multi-axis batch norm, the `beta`, `gamma`, `moving_mean`, and\n-            `moving_variance` variables are the same rank as the input Tensor,\n-            with dimension size 1 in all reduced (non-axis) dimensions).\n+        `moving_variance` variables are the same rank as the input Tensor, with\n+        dimension size 1 in all reduced (non-axis) dimensions).\n       momentum: Momentum for the moving average.\n       epsilon: Small float added to variance to avoid dividing by zero.\n-      center: If True, add offset of `beta` to normalized tensor. If False, `beta`\n-        is ignored.\n-      scale: If True, multiply by `gamma`. If False, `gamma` is not used. When the\n-        next layer is linear (also e.g. `nn.relu`), this can be disabled since the\n-        scaling can be done by the next layer.\n+      center: If True, add offset of `beta` to normalized tensor. If False,\n+        `beta` is ignored.\n+      scale: If True, multiply by `gamma`. If False, `gamma` is not used. When\n+        the next layer is linear (also e.g. `nn.relu`), this can be disabled\n+        since the scaling can be done by the next layer.\n       beta_initializer: Initializer for the beta weight.\n       gamma_initializer: Initializer for the gamma weight.\n       moving_mean_initializer: Initializer for the moving mean.\n       moving_variance_initializer: Initializer for the moving variance.\n       beta_regularizer: Optional regularizer for the beta weight.\n       gamma_regularizer: Optional regularizer for the gamma weight.\n-      beta_constraint: An optional projection function to be applied to the `beta`\n-        weight after being updated by an `Optimizer` (e.g. used to implement norm\n-        constraints or value constraints for layer weights). The function must\n-        take as input the unprojected variable and must return the projected\n-        variable (which must have the same shape). Constraints are not safe to use\n-        when doing asynchronous distributed training.\n+      beta_constraint: An optional projection function to be applied to the\n+        `beta` weight after being updated by an `Optimizer` (e.g. used to\n+        implement norm constraints or value constraints for layer weights). The\n+        function must take as input the unprojected variable and must return the\n+        projected variable (which must have the same shape). Constraints are not\n+        safe to use when doing asynchronous distributed training.\n       gamma_constraint: An optional projection function to be applied to the\n         `gamma` weight after being updated by an `Optimizer`.\n-      renorm: Whether to use Batch Renormalization (Ioffe, 2017). This adds extra\n-        variables during training. The inference is the same for either value of\n-        this parameter.\n+      renorm: Whether to use Batch Renormalization (Ioffe, 2017). This adds\n+        extra variables during training. The inference is the same for either\n+        value of this parameter.\n       renorm_clipping: A dictionary that may map keys 'rmax', 'rmin', 'dmax' to\n         scalar `Tensors` used to clip the renorm correction. The correction `(r,\n         d)` is used as `corrected_value = normalized_value * r + d`, with `r`\n@@ -86,20 +87,20 @@ class BatchNormalization(batch_normalization_v1.BatchNormalization, base.Layer):\n       renorm_momentum: Momentum used to update the moving means and standard\n         deviations with renorm. Unlike `momentum`, this affects training and\n         should be neither too small (which would add noise) nor too large (which\n-        would give stale estimates). Note that `momentum` is still applied to get\n-        the means and variances for inference.\n-      fused: if `None` or `True`, use a faster, fused implementation if possible.\n-        If `False`, use the system recommended implementation.\n+        would give stale estimates). Note that `momentum` is still applied to\n+        get the means and variances for inference.\n+      fused: if `None` or `True`, use a faster, fused implementation if\n+        possible. If `False`, use the system recommended implementation.\n       trainable: Boolean, if `True` also add variables to the graph collection\n         `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n       virtual_batch_size: An `int`. By default, `virtual_batch_size` is `None`,\n-        which means batch normalization is performed across the whole batch. When\n-        `virtual_batch_size` is not `None`, instead perform \"Ghost Batch\n+        which means batch normalization is performed across the whole batch.\n+        When `virtual_batch_size` is not `None`, instead perform \"Ghost Batch\n         Normalization\", which creates virtual sub-batches which are each\n         normalized separately (with shared gamma, beta, and moving statistics).\n         Must divide the actual batch size during execution.\n-      adjustment: A function taking the `Tensor` containing the (dynamic) shape of\n-        the input tensor and returning a pair (scale, bias) to apply to the\n+      adjustment: A function taking the `Tensor` containing the (dynamic) shape\n+        of the input tensor and returning a pair (scale, bias) to apply to the\n         normalized values (before gamma and beta), only during training. For\n         example, if axis==-1,\n           `adjustment = lambda shape: (\n@@ -262,7 +263,8 @@ def batch_normalization(\n     virtual_batch_size=None,\n     adjustment=None,\n ):\n-    \"\"\"Functional interface for the batch normalization layer from_config(Ioffe et al., 2015).\n+    \"\"\"Functional interface for the batch normalization layer from_config(Ioffe\n+    et al., 2015).\n \n     Note: when training, the moving_mean and moving_variance need to be updated.\n     By default the update ops are placed in `tf.GraphKeys.UPDATE_OPS`, so they\n@@ -288,23 +290,23 @@ def batch_normalization(\n         `data_format=\"channels_first\"`, set `axis=1` in `BatchNormalization`.\n       momentum: Momentum for the moving average.\n       epsilon: Small float added to variance to avoid dividing by zero.\n-      center: If True, add offset of `beta` to normalized tensor. If False, `beta`\n-        is ignored.\n-      scale: If True, multiply by `gamma`. If False, `gamma` is not used. When the\n-        next layer is linear (also e.g. `nn.relu`), this can be disabled since the\n-        scaling can be done by the next layer.\n+      center: If True, add offset of `beta` to normalized tensor. If False,\n+        `beta` is ignored.\n+      scale: If True, multiply by `gamma`. If False, `gamma` is not used. When\n+        the next layer is linear (also e.g. `nn.relu`), this can be disabled\n+        since the scaling can be done by the next layer.\n       beta_initializer: Initializer for the beta weight.\n       gamma_initializer: Initializer for the gamma weight.\n       moving_mean_initializer: Initializer for the moving mean.\n       moving_variance_initializer: Initializer for the moving variance.\n       beta_regularizer: Optional regularizer for the beta weight.\n       gamma_regularizer: Optional regularizer for the gamma weight.\n-      beta_constraint: An optional projection function to be applied to the `beta`\n-        weight after being updated by an `Optimizer` (e.g. used to implement norm\n-        constraints or value constraints for layer weights). The function must\n-        take as input the unprojected variable and must return the projected\n-        variable (which must have the same shape). Constraints are not safe to use\n-        when doing asynchronous distributed training.\n+      beta_constraint: An optional projection function to be applied to the\n+        `beta` weight after being updated by an `Optimizer` (e.g. used to\n+        implement norm constraints or value constraints for layer weights). The\n+        function must take as input the unprojected variable and must return the\n+        projected variable (which must have the same shape). Constraints are not\n+        safe to use when doing asynchronous distributed training.\n       gamma_constraint: An optional projection function to be applied to the\n         `gamma` weight after being updated by an `Optimizer`.\n       training: Either a Python boolean, or a TensorFlow boolean scalar tensor\n@@ -316,11 +318,11 @@ def batch_normalization(\n       trainable: Boolean, if `True` also add variables to the graph collection\n         `GraphKeys.TRAINABLE_VARIABLES` (see tf.Variable).\n       name: String, the name of the layer.\n-      reuse: Boolean, whether to reuse the weights of a previous layer by the same\n-        name.\n-      renorm: Whether to use Batch Renormalization (Ioffe, 2017). This adds extra\n-        variables during training. The inference is the same for either value of\n-        this parameter.\n+      reuse: Boolean, whether to reuse the weights of a previous layer by the\n+        same name.\n+      renorm: Whether to use Batch Renormalization (Ioffe, 2017). This adds\n+        extra variables during training. The inference is the same for either\n+        value of this parameter.\n       renorm_clipping: A dictionary that may map keys 'rmax', 'rmin', 'dmax' to\n         scalar `Tensors` used to clip the renorm correction. The correction `(r,\n         d)` is used as `corrected_value = normalized_value * r + d`, with `r`\n@@ -329,18 +331,18 @@ def batch_normalization(\n       renorm_momentum: Momentum used to update the moving means and standard\n         deviations with renorm. Unlike `momentum`, this affects training and\n         should be neither too small (which would add noise) nor too large (which\n-        would give stale estimates). Note that `momentum` is still applied to get\n-        the means and variances for inference.\n-      fused: if `None` or `True`, use a faster, fused implementation if possible.\n-        If `False`, use the system recommended implementation.\n+        would give stale estimates). Note that `momentum` is still applied to\n+        get the means and variances for inference.\n+      fused: if `None` or `True`, use a faster, fused implementation if\n+        possible.  If `False`, use the system recommended implementation.\n       virtual_batch_size: An `int`. By default, `virtual_batch_size` is `None`,\n-        which means batch normalization is performed across the whole batch. When\n-        `virtual_batch_size` is not `None`, instead perform \"Ghost Batch\n+        which means batch normalization is performed across the whole batch.\n+        When `virtual_batch_size` is not `None`, instead perform \"Ghost Batch\n         Normalization\", which creates virtual sub-batches which are each\n         normalized separately (with shared gamma, beta, and moving statistics).\n         Must divide the actual batch size during execution.\n-      adjustment: A function taking the `Tensor` containing the (dynamic) shape of\n-        the input tensor and returning a pair (scale, bias) to apply to the\n+      adjustment: A function taking the `Tensor` containing the (dynamic) shape\n+        of the input tensor and returning a pair (scale, bias) to apply to the\n         normalized values (before gamma and beta), only during training. For\n         example, if axis==-1,\n           `adjustment = lambda shape: (\n\n@@ -1103,8 +1103,8 @@ class BNTest(tf.test.TestCase):\n                 moving_mean += (mean - moving_mean) * (1.0 - momentum)\n                 moving_stddev += (stddev - moving_stddev) * (1.0 - momentum)\n \n-                # Compute test values first, before the train mode updates the moving\n-                # averages.\n+                # Compute test values first, before the train mode updates the\n+                # moving averages.\n                 yt_val_test, _, _ = sess.run(\n                     [yt] + bn.updates, feed_dict={xt: x, training: False}\n                 )\n@@ -1112,9 +1112,10 @@ class BNTest(tf.test.TestCase):\n                     [yt] + bn.updates, feed_dict={xt: x, training: True}\n                 )\n \n-                # Due to initialization inconsistencies, values may not be identical\n-                # on the first iteration (but shouldn't be different by much more than\n-                # epsilon). After the first iteration they should be identical.\n+                # Due to initialization inconsistencies, values may not be\n+                # identical on the first iteration (but shouldn't be different\n+                # by much more than epsilon). After the first iteration they\n+                # should be identical.\n                 atol = epsilon * 1.5 if step == 0 else 1e-5\n                 self.assertAllClose(y_train, yt_val_train, atol=atol)\n                 self.assertAllClose(y_test, yt_val_test, atol=atol)\n\n@@ -39,8 +39,8 @@ class AveragePooling1D(keras_layers.AveragePooling1D, base.Layer):\n         strides of the pooling operation.\n       padding: A string. The padding method, either 'valid' or 'same'.\n         Case-insensitive.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`. The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch, length, channels)` while `channels_first` corresponds to\n         inputs with shape `(batch, channels, length)`.\n@@ -119,8 +119,8 @@ def average_pooling1d(\n         strides of the pooling operation.\n       padding: A string. The padding method, either 'valid' or 'same'.\n         Case-insensitive.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`. The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch, length, channels)` while `channels_first` corresponds to\n         inputs with shape `(batch, channels, length)`.\n@@ -196,8 +196,8 @@ class MaxPooling1D(keras_layers.MaxPooling1D, base.Layer):\n         strides of the pooling operation.\n       padding: A string. The padding method, either 'valid' or 'same'.\n         Case-insensitive.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`. The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch, length, channels)` while `channels_first` corresponds to\n         inputs with shape `(batch, channels, length)`.\n@@ -276,8 +276,8 @@ def max_pooling1d(\n         strides of the pooling operation.\n       padding: A string. The padding method, either 'valid' or 'same'.\n         Case-insensitive.\n-      data_format: A string, one of `channels_last` (default) or `channels_first`.\n-        The ordering of the dimensions in the inputs.\n+      data_format: A string, one of `channels_last` (default) or\n+        `channels_first`. The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape\n         `(batch, length, channels)` while `channels_first` corresponds to\n         inputs with shape `(batch, channels, length)`.\n@@ -347,8 +347,8 @@ class AveragePooling2D(keras_layers.AveragePooling2D, base.Layer):\n     \"\"\"Average pooling layer for 2D inputs (e.g. images).\n \n     Args:\n-      pool_size: An integer or tuple/list of 2 integers: (pool_height, pool_width)\n-        specifying the size of the pooling window.\n+      pool_size: An integer or tuple/list of 2 integers: (pool_height,\n+        pool_width) specifying the size of the pooling window.\n         Can be a single integer to specify the same value for\n         all spatial dimensions.\n       strides: An integer or tuple/list of 2 integers,\n@@ -431,8 +431,8 @@ def average_pooling2d(\n \n     Args:\n       inputs: The tensor over which to pool. Must have rank 4.\n-      pool_size: An integer or tuple/list of 2 integers: (pool_height, pool_width)\n-        specifying the size of the pooling window.\n+      pool_size: An integer or tuple/list of 2 integers: (pool_height,\n+        pool_width) specifying the size of the pooling window.\n         Can be a single integer to specify the same value for\n         all spatial dimensions.\n       strides: An integer or tuple/list of 2 integers,\n@@ -512,8 +512,8 @@ class MaxPooling2D(keras_layers.MaxPooling2D, base.Layer):\n     \"\"\"Max pooling layer for 2D inputs (e.g. images).\n \n     Args:\n-      pool_size: An integer or tuple/list of 2 integers: (pool_height, pool_width)\n-        specifying the size of the pooling window.\n+      pool_size: An integer or tuple/list of 2 integers: (pool_height,\n+        pool_width) specifying the size of the pooling window.\n         Can be a single integer to specify the same value for\n         all spatial dimensions.\n       strides: An integer or tuple/list of 2 integers,\n@@ -596,8 +596,8 @@ def max_pooling2d(\n \n     Args:\n       inputs: The tensor over which to pool. Must have rank 4.\n-      pool_size: An integer or tuple/list of 2 integers: (pool_height, pool_width)\n-        specifying the size of the pooling window.\n+      pool_size: An integer or tuple/list of 2 integers: (pool_height,\n+        pool_width) specifying the size of the pooling window.\n         Can be a single integer to specify the same value for\n         all spatial dimensions.\n       strides: An integer or tuple/list of 2 integers,\n@@ -934,9 +934,10 @@ def max_pooling3d(\n \n     Args:\n       inputs: The tensor over which to pool. Must have rank 5.\n-      pool_size: An integer or tuple/list of 3 integers: (pool_depth, pool_height,\n-        pool_width) specifying the size of the pooling window. Can be a single\n-        integer to specify the same value for all spatial dimensions.\n+      pool_size: An integer or tuple/list of 3 integers: (pool_depth,\n+        pool_height, pool_width) specifying the size of the pooling window. Can\n+        be a single integer to specify the same value for all spatial\n+        dimensions.\n       strides: An integer or tuple/list of 3 integers, specifying the strides of\n         the pooling operation. Can be a single integer to specify the same value\n         for all spatial dimensions.\n@@ -945,8 +946,8 @@ def max_pooling3d(\n       data_format: A string. The ordering of the dimensions in the inputs.\n         `channels_last` (default) and `channels_first` are supported.\n         `channels_last` corresponds to inputs with shape `(batch, depth, height,\n-        width, channels)` while `channels_first` corresponds to inputs with shape\n-        `(batch, channels, depth, height, width)`.\n+        width, channels)` while `channels_first` corresponds to inputs with\n+        shape `(batch, channels, depth, height, width)`.\n       name: A string, the name of the layer.\n \n     Returns:\n\n@@ -47,7 +47,8 @@ def _has_kwargs(fn):\n     \"\"\"Returns whether the passed callable has **kwargs in its signature.\n \n     Args:\n-      fn: Function, or function-like object (e.g., result of `functools.partial`).\n+      fn: Function, or function-like object (e.g., result of\n+        `functools.partial`).\n \n     Returns:\n       `bool`: if `fn` has **kwargs in its signature.\n@@ -72,7 +73,8 @@ def fn_args(fn):\n     \"\"\"Get argument names for function-like object.\n \n     Args:\n-      fn: Function, or function-like object (e.g., result of `functools.partial`).\n+      fn: Function, or function-like object (e.g., result of\n+        `functools.partial`).\n \n     Returns:\n       `tuple` of string argument names.\n@@ -115,9 +117,8 @@ def validate_synchronization_aggregation_trainable(\n                 aggregation = tf.VariableAggregation(aggregation)\n             except ValueError:\n                 raise ValueError(\n-                    \"Invalid variable aggregation mode: {} for variable: {}\".format(\n-                        aggregation, name\n-                    )\n+                    \"Invalid variable aggregation mode: {} \"\n+                    \"for variable: {}\".format(aggregation, name)\n                 )\n     if synchronization is None:\n         synchronization = tf.VariableSynchronization.AUTO\n@@ -126,9 +127,8 @@ def validate_synchronization_aggregation_trainable(\n             synchronization = tf.VariableSynchronization(synchronization)\n         except ValueError:\n             raise ValueError(\n-                \"Invalid variable synchronization mode: {} for variable: {}\".format(\n-                    synchronization, name\n-                )\n+                \"Invalid variable synchronization mode: {} \"\n+                \"for variable: {}\".format(synchronization, name)\n             )\n     if trainable is None:\n         trainable = synchronization != tf.VariableSynchronization.ON_READ\n@@ -149,8 +149,8 @@ class _EagerVariableStore(tf.Module):\n     tf.compat.v1.AUTO_REUSE\n \n     Attributes:\n-      vars: a dictionary with string names (same as passed in GetVar) as keys and\n-        the corresponding TensorFlow Variables as values.\n+      vars: a dictionary with string names (same as passed in GetVar) as keys\n+        and the corresponding TensorFlow Variables as values.\n       regularizers: a dictionary with string names as keys and the corresponding\n         callables that return losses as values.\n       layers: a dictionary with string names as keys and the corresponding\n@@ -192,22 +192,22 @@ class _EagerVariableStore(tf.Module):\n     ):\n         \"\"\"Gets an existing variable with these parameters or create a new one.\n \n-        If a variable with the given name is already stored, we return the stored\n-        variable. Otherwise, we create a new one.\n+        If a variable with the given name is already stored, we return the\n+        stored variable. Otherwise, we create a new one.\n \n         Set `reuse` to `True` when you only want to reuse existing Variables.\n-        Set `reuse` to None (the default) or tf.compat.v1.AUTO_REUSE when you want\n-        variables to be created if they don't exist or returned if they do.\n+        Set `reuse` to None (the default) or tf.compat.v1.AUTO_REUSE when you\n+        want variables to be created if they don't exist or returned if they do.\n         In this shim, `reuse` of `False` will be treated as auto-reuse.\n \n-        If initializer is `None` (the default), the default initializer passed in\n-        the constructor is used. If that one is `None` too, we use a new\n-        `glorot_uniform_initializer`. If initializer is a Tensor, we use\n-        it as a value and derive the shape from the initializer.\n+        If initializer is `None` (the default), the default initializer passed\n+        in the constructor is used. If that one is `None` too, we use a new\n+        `glorot_uniform_initializer`. If initializer is a Tensor, we use it as a\n+        value and derive the shape from the initializer.\n \n         If a partitioner is provided, a `PartitionedVariable` is returned.\n-        Accessing this object as a `Tensor` returns the shards concatenated along\n-        the partition axis.\n+        Accessing this object as a `Tensor` returns the shards concatenated\n+        along the partition axis.\n \n         Some useful partitioners are available.  See, e.g.,\n         `variable_axis_size_partitioner` and `min_max_variable_partitioner`.\n@@ -217,55 +217,60 @@ class _EagerVariableStore(tf.Module):\n           shape: Shape of the new or existing variable.\n           dtype: Type of the new or existing variable (defaults to `DT_FLOAT`).\n           initializer: Initializer for the variable.\n-          regularizer: A (Tensor -> Tensor or None) function; the result of applying\n-            it on a newly created variable will be added to the collection\n-            GraphKeys.REGULARIZATION_LOSSES and can be used for regularization.\n-          reuse: a Boolean, None, or tf.AUTO_REUSE. Controls reuse or creation of\n-            variables. When eager execution is enabled  this argument is always\n-            forced to be False.\n+          regularizer: A (Tensor -> Tensor or None) function; the result of\n+            applying it on a newly created variable will be added to the\n+            collection GraphKeys.REGULARIZATION_LOSSES and can be used for\n+            regularization.\n+          reuse: a Boolean, None, or tf.AUTO_REUSE. Controls reuse or creation\n+            of variables. When eager execution is enabled  this argument is\n+            always forced to be False.\n           trainable: If `True` also add the variable to the graph collection\n             `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`). `trainable`\n             defaults to `True`, unless `synchronization` is set to `ON_READ`, in\n             which case it defaults to `False`.\n           collections: List of graph collections keys to add the `Variable` to.\n             Defaults to `[GraphKeys.GLOBAL_VARIABLES]` (see `tf.Variable`).\n-          caching_device: Optional device string or function describing where the\n-            Variable should be cached for reading.  Defaults to the Variable's\n-            device.  If not `None`, caches on another device.  Typical use is to\n-            cache on the device where the Ops using the `Variable` reside, to\n-            deduplicate copying through `Switch` and other conditional statements.\n-          partitioner: Optional callable that accepts a fully defined `TensorShape`\n-            and dtype of the `Variable` to be created, and returns a list of\n-            partitions for each axis (currently only one axis can be partitioned).\n+          caching_device: Optional device string or function describing where\n+            the Variable should be cached for reading.  Defaults to the\n+            Variable's device.  If not `None`, caches on another device.\n+            Typical use is to cache on the device where the Ops using the\n+            `Variable` reside, to deduplicate copying through `Switch` and other\n+            conditional statements.\n+          partitioner: Optional callable that accepts a fully defined\n+            `TensorShape` and dtype of the `Variable` to be created, and returns\n+            a list of partitions for each axis (currently only one axis can be\n+            partitioned).\n           validate_shape: If False, allows the variable to be initialized with a\n-            value of unknown shape. If True, the default, the shape of initial_value\n-            must be known.\n+            value of unknown shape. If True, the default, the shape of\n+            initial_value must be known.\n           use_resource: If False, creates a regular Variable. If True, creates\n             instead an experimental ResourceVariable which has well-defined\n             semantics. Defaults to False (will later change to True). When eager\n             execution is enabled this argument is always forced to be true.\n-          custom_getter: Callable that takes as a first argument the true getter,\n-            and allows overwriting the internal get_variable method. The signature\n-            of `custom_getter` should match that of this method,\n-            but the most future-proof version will allow for changes: `def\n-              custom_getter(getter, *args, **kwargs)`.  Direct access to\n-            all `get_variable` parameters is also allowed: `def\n-              custom_getter(getter, name, *args, **kwargs)`.  A simple identity\n-            custom getter that simply creates variables with modified names is:\n+          custom_getter: Callable that takes as a first argument the true\n+            getter, and allows overwriting the internal get_variable method. The\n+            signature of `custom_getter` should match that of this method, but\n+            the most future-proof version will allow for changes:\n+            `def custom_getter(getter, *args, **kwargs)`.\n+            Direct access to all `get_variable` parameters is also allowed:\n+            `def custom_getter(getter, name, *args, **kwargs)`.\n+            A simple identity custom getter that simply creates variables with\n+            modified names is:\n             ```python\n-            def custom_getter(getter, name, *args, **kwargs): return getter(name +\n-              '_suffix', *args, **kwargs) ```\n-          constraint: An optional projection function to be applied to the variable\n-            after being updated by an `Optimizer` (e.g. used to implement norm\n-            constraints or value constraints for layer weights). The function must\n-            take as input the unprojected Tensor representing the value of the\n-            variable and return the Tensor for the projected value (which must have\n-            the same shape). Constraints are not safe to use when doing asynchronous\n-            distributed training.\n+            def custom_getter(getter, name, *args, **kwargs):\n+              return getter(name + '_suffix', *args, **kwargs)\n+            ```\n+          constraint: An optional projection function to be applied to the\n+            variable after being updated by an `Optimizer` (e.g. used to\n+            implement norm constraints or value constraints for layer weights).\n+            The function must take as input the unprojected Tensor representing\n+            the value of the variable and return the Tensor for the projected\n+            value (which must have the same shape). Constraints are not safe to\n+            use when doing asynchronous distributed training.\n           synchronization: Indicates when a distributed a variable will be\n             aggregated. Accepted values are constants defined in the class\n-            `tf.VariableSynchronization`. By default the synchronization is set to\n-            `AUTO` and the current `DistributionStrategy` chooses when to\n+            `tf.VariableSynchronization`. By default the synchronization is set\n+            to `AUTO` and the current `DistributionStrategy` chooses when to\n             synchronize.\n           aggregation: Indicates how a distributed variable will be aggregated.\n             Accepted values are constants defined in the class\n@@ -290,27 +295,28 @@ class _EagerVariableStore(tf.Module):\n \n         with tf.init_scope():\n             if tf.executing_eagerly():\n-                # Variable creation and initialization takes place in `init_scope`s;\n-                # as such, if an `init_scope` lifts us into the eager context, then we\n-                # need to use `ResourceVariable`s.\n+                # Variable creation and initialization takes place in\n+                # `init_scope`s; as such, if an `init_scope` lifts us into the\n+                # eager context, then we need to use `ResourceVariable`s.\n                 use_resource = True\n \n         # Note that it's fine to reuse eager variables whose initialization was\n-        # lifted from a function-building graph into the eager context (that's why\n-        # the following clause is not wrapped in an `init_scope`); lifted variables\n-        # are tracked by the graph's `VariableStore`.\n+        # lifted from a function-building graph into the eager context (that's\n+        # why the following clause is not wrapped in an `init_scope`); lifted\n+        # variables are tracked by the graph's `VariableStore`.\n         if not reuse:\n             reuse = tf.compat.v1.AUTO_REUSE\n \n-        # If a *_ref type is passed in an error would be triggered further down the\n-        # stack. We prevent this using base_dtype to get a non-ref version of the\n-        # type, before doing anything else. When _ref types are removed in favor of\n-        # resources, this line can be removed.\n+        # If a *_ref type is passed in an error would be triggered further down\n+        # the stack. We prevent this using base_dtype to get a non-ref version\n+        # of the type, before doing anything else. When _ref types are removed\n+        # in favor of resources, this line can be removed.\n         try:\n             dtype = dtype.base_dtype\n         except AttributeError:\n-            # .base_dtype not existing means that we will try and use the raw dtype\n-            # which was passed in - this might be a NumPy type which is valid.\n+            # .base_dtype not existing means that we will try and use the raw\n+            # dtype which was passed in - this might be a NumPy type which is\n+            # valid.\n             pass\n \n         # This is the main logic of get_variable.  However, custom_getter\n@@ -338,16 +344,18 @@ class _EagerVariableStore(tf.Module):\n             # Partitioned variable currently unsupported w/ the shim\n             if partitioner is not None:\n                 raise ValueError(\n-                    \"`partitioner` arg for `get_variable` is unsupported in TF2.\"\n-                    \"File a bug if you need help. You passed %s\" % partitioner\n+                    \"`partitioner` arg for `get_variable` is unsupported in \"\n+                    \"TF2. File a bug if you need help. \"\n+                    \"You passed %s\" % partitioner\n                 )\n \n             # Single variable case\n             if \"%s/part_0\" % name in self._vars:\n                 raise ValueError(\n-                    \"No partitioner was provided, but a partitioned version of the \"\n-                    \"variable was found: %s/part_0. Perhaps a variable of the same \"\n-                    \"name was already created with partitioning?\" % name\n+                    \"No partitioner was provided, but a partitioned version of \"\n+                    \"the variable was found: %s/part_0. Perhaps a variable of \"\n+                    \"the same name was already created with \"\n+                    \"partitioning?\" % name\n                 )\n \n             return self._get_single_variable(\n@@ -374,8 +382,8 @@ class _EagerVariableStore(tf.Module):\n         )\n \n         if custom_getter is not None:\n-            # Handle backwards compatibility with getter arguments that were added\n-            # to the API after users started writing custom getters.\n+            # Handle backwards compatibility with getter arguments that were\n+            # added to the API after users started writing custom getters.\n             custom_getter_kwargs = {\n                 \"getter\": _true_getter,\n                 \"name\": name,\n@@ -393,8 +401,8 @@ class _EagerVariableStore(tf.Module):\n                 \"synchronization\": synchronization,\n                 \"aggregation\": aggregation,\n             }\n-            # `fn_args` and `has_kwargs` can handle functions, `functools.partial`,\n-            # `lambda`.\n+            # `fn_args` and `has_kwargs` can handle functions,\n+            # `functools.partial`, `lambda`.\n             if \"constraint\" in fn_args(custom_getter) or _has_kwargs(\n                 custom_getter\n             ):\n@@ -435,12 +443,10 @@ class _EagerVariableStore(tf.Module):\n         synchronization=tf.VariableSynchronization.AUTO,\n         aggregation=tf.compat.v1.VariableAggregation.NONE,\n     ):\n-        \"\"\"Get or create a single Variable (e.g.\n+        \"\"\"Get or create a single Variable (e.g. a shard or entire variable).\n \n-        a shard or entire variable).\n-\n-        See the documentation of get_variable above (ignore partitioning components)\n-        for details.\n+        See the documentation of get_variable above (ignore partitioning\n+        components) for details.\n \n         Args:\n           name: see get_variable.\n@@ -515,7 +521,8 @@ class _EagerVariableStore(tf.Module):\n                 init_val = initializer\n                 variable_dtype = None\n             else:\n-                # Instantiate initializer if provided initializer is a type object.\n+                # Instantiate initializer if provided initializer is a type\n+                # object.\n                 if tf_inspect.isclass(initializer):\n                     initializer = initializer()\n                 if shape.is_fully_defined():\n@@ -611,7 +618,8 @@ class _EagerVariableStore(tf.Module):\n         ):\n             initializer = tf.compat.v1.zeros_initializer()\n             initializing_from_value = False\n-        # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here?\n+        # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX\n+        # here?\n         else:\n             raise ValueError(\n                 \"An initializer for variable %s of %s is required\"\n@@ -623,7 +631,8 @@ class _EagerVariableStore(tf.Module):\n \n @keras_export(v1=[\"keras.utils.track_tf1_style_variables\"])\n def track_tf1_style_variables(method):\n-    \"\"\"Wrap layer & module methods in this decorator to capture tf1-style weights.\n+    \"\"\"Wrap layer & module methods in this decorator to capture tf1-style\n+    weights.\n \n     Decorating a `tf.keras.Layer`'s  or `tf.Module`'s methods with this\n     decorator will cause the layer/module to track weights created/used\n@@ -637,9 +646,11 @@ def track_tf1_style_variables(method):\n     tracked by the layer under the standard `layer.losses` property.\n \n     This tracking enables using large classes of TF1-style model-forward-pass\n-    code inside of Keras layers or `tf.Modules` in TF2 with TF2 behaviors enabled.\n+    code inside of Keras layers or `tf.Modules` in TF2 with TF2 behaviors\n+    enabled.\n \n-    Example of capturing tf.compat.v1.layer-based modeling code as a Keras layer:\n+    Example of capturing tf.compat.v1.layer-based modeling code as a Keras\n+    layer:\n \n     ```python\n     class WrappedDoubleDenseLayer(tf.keras.layers.Layer):\n@@ -734,10 +745,10 @@ def track_tf1_style_variables(method):\n     ```\n \n     Regularization losses:\n-      Any regularizers specified in the `get_variable` calls or `compat.v1.layer`\n-      creations will get captured if they occur in your decorated method\n-      and the method belongs to a `tf.keras.Layer`/`tf.keras.Module`.\n-      Regularization losses\n+      Any regularizers specified in the `get_variable` calls or\n+      `compat.v1.layer` creations will get captured if they occur in your\n+      decorated method and the method belongs to a\n+      `tf.keras.Layer`/`tf.keras.Module`. Regularization losses\n       are accessible in `layer.losses` after a call just like in a standard\n       Keras layer, and will be captured by any model that includes this layer.\n       Regularization losses attached to Keras layers/models set as attributes\n@@ -786,10 +797,10 @@ def track_tf1_style_variables(method):\n         assign them as attributes of your layer so that Keras/Module's standard\n         object-oriented weights (and loss tracking for layers) will kick in.\n         See the intro to modules, layers, and models\n-        [guide](https://www.tensorflow.org/guide/intro_to_modules) for more info.\n-        As a backup, the `compat.v1.keras.utils.get_or_create_layer` method will\n-        ease tracking nested keras model weights and losses for existing TF1 code,\n-        but new code should use explicit tracking.\n+        [guide](https://www.tensorflow.org/guide/intro_to_modules) for more\n+        info.  As a backup, the `compat.v1.keras.utils.get_or_create_layer`\n+        method will ease tracking nested keras model weights and losses for\n+        existing TF1 code, but new code should use explicit tracking.\n \n     Args:\n       method: The method to decorate. This should belong to a custom tf.Module,\n@@ -806,12 +817,12 @@ def track_tf1_style_variables(method):\n                 # Raise an error if you incorrectly decorate a method\n                 # that is not a method of a Module, Layer, or Model:\n                 raise ValueError(\n-                    \"`@tf.compat.v1.keras.utils.track_tf1_layers_and_variables` must \"\n-                    \"be applied to a method of a subclassed `tf.Module`, \"\n-                    \"`tf.keras.layers.Layer`, or `tf.keras.Model` and which takes \"\n-                    \"`self` as the first argument. But, the first argument passed \"\n-                    \"to the decorated method was {}, which does not \"\n-                    \"extend Module, Layer, or Model.\".format(self)\n+                    \"`@tf.compat.v1.keras.utils.track_tf1_layers_and_variables`\"\n+                    \" must be applied to a method of a subclassed `tf.Module`, \"\n+                    \"`tf.keras.layers.Layer`, or `tf.keras.Model` and which \"\n+                    \"takes `self` as the first argument. But, the first \"\n+                    \"argument passed to the decorated method was {}, which \"\n+                    \"does not extend Module, Layer, or Model.\".format(self)\n                 )\n             var_store = _EagerVariableStore()\n             self._tf1_style_var_store = (\n@@ -856,7 +867,8 @@ class VariableScopeLayer(base_layer.Layer):\n     Below are some examples, and then more details on the functionality of this\n     shim layer to wrap TF1 model forward passes.\n \n-    Example of capturing tf.compat.v1.layer-based modeling code as a Keras layer:\n+    Example of capturing tf.compat.v1.layer-based modeling code as a Keras\n+    layer:\n \n     ```python\n     class WrappedDoubleDenseLayer(variable_scope_shim.VariableScopeLayer):\n@@ -949,13 +961,13 @@ class VariableScopeLayer(base_layer.Layer):\n     ```\n \n     Regularization losses:\n-      Any regularizers specified in the `get_variable` calls or `compat.v1.layer`\n-      creations will get captured by this wrapper layer. Regularization losses\n-      are accessible in `layer.losses` after a call just like in a standard\n-      Keras layer, and will be captured by any model that includes this layer.\n-      Regularization losses attached to Keras layers/models set as attributes\n-      of your layer will also get captured in the standard Keras regularization\n-      loss tracking.\n+      Any regularizers specified in the `get_variable` calls or\n+      `compat.v1.layer` creations will get captured by this wrapper layer.\n+      Regularization losses are accessible in `layer.losses` after a call just\n+      like in a standard Keras layer, and will be captured by any model that\n+      includes this layer.  Regularization losses attached to Keras\n+      layers/models set as attributes of your layer will also get captured in\n+      the standard Keras regularization loss tracking.\n \n     Variable scope / variable reuse:\n       variable-scope based reuse in the `forward_pass` will be respected,\n@@ -1022,11 +1034,12 @@ def get_or_create_layer(name, create_layer_method):\n \n     This method can be used within a `tf.keras.Layer`'s methods decorated by\n     the`track_tf1_style_variables` shim, to additionally track inner keras Model\n-    objects created within the same method. The inner model's variables and losses\n-    will be accessible via the outer model's `variables` and `losses` attributes.\n+    objects created within the same method. The inner model's variables and\n+    losses will be accessible via the outer model's `variables` and `losses`\n+    attributes.\n \n-    This enables tracking of inner keras models using TF2 behaviors, with minimal\n-    changes to existing TF1-style code.\n+    This enables tracking of inner keras models using TF2 behaviors, with\n+    minimal changes to existing TF1-style code.\n \n     Example:\n \n@@ -1052,8 +1065,8 @@ def get_or_create_layer(name, create_layer_method):\n         return model(inputs)\n     ```\n     The inner model creation should be confined to its own zero-arg function,\n-    which should be passed into this method. In TF1, this method will immediately\n-    create and return the desired model, without any tracking.\n+    which should be passed into this method. In TF1, this method will\n+    immediately create and return the desired model, without any tracking.\n \n     Args:\n       name: A name to give the nested layer to track.\n@@ -1070,8 +1083,9 @@ def get_or_create_layer(name, create_layer_method):\n             return create_layer_method()\n         else:\n             raise ValueError(\n-                \"Tried to call get_or_create_layer in eager mode from a method not\"\n-                \"decorated with @tf.compat.v1.keras.utils.track_tf1_style_variables.\"\n+                \"Tried to call get_or_create_layer in eager mode from a method \"\n+                \"notdecorated with \"\n+                \"@tf.compat.v1.keras.utils.track_tf1_style_variables.\"\n             )\n     vs_name = tf.compat.v1.get_variable_scope().name\n     name = f\"{vs_name}/{name}\"\n\n@@ -43,9 +43,9 @@ from keras.testing_infra import test_combinations\n def run_inside_wrap_function_in_eager_mode(graph_function):\n     \"\"\"Decorator to execute the same graph code in eager and graph modes.\n \n-    In graph mode, we just execute the graph_function passed as argument. In eager\n-    mode, we wrap the function using wrap_function and then execute the wrapped\n-    result.\n+    In graph mode, we just execute the graph_function passed as argument. In\n+    eager mode, we wrap the function using wrap_function and then execute the\n+    wrapped result.\n \n     Args:\n       graph_function: python function containing graph code to be wrapped\n@@ -209,7 +209,8 @@ class VariableScopeTest(tf.test.TestCase):\n                 tf.compat.v1.get_variable(\"u\", [])\n                 vs.set_regularizer(regularizer2)\n                 tf.compat.v1.get_variable(\"w\", [])\n-                # Next 3 variable not regularized to test disabling regularization.\n+                # Next 3 variable not regularized to test disabling\n+                # regularization.\n                 tf.compat.v1.get_variable(\n                     \"x\", [], regularizer=tf.compat.v1.no_regularizer\n                 )\n@@ -229,7 +230,8 @@ class VariableScopeTest(tf.test.TestCase):\n         self.assertAllClose(self.evaluate(w.value()), 0.1)\n \n         with self.assertRaisesRegex(ValueError, \"shape\"):\n-            # We disallow explicit shape specification when initializer is constant.\n+            # We disallow explicit shape specification when initializer is\n+            # constant.\n             tf.compat.v1.get_variable(\"u\", [1], initializer=init)\n \n         with tf.compat.v1.variable_scope(\"foo\", initializer=init):\n@@ -285,8 +287,8 @@ class VariableScopeTest(tf.test.TestCase):\n                     _ = tf.compat.v1.assign(\n                         tf.compat.v1.get_variable(\"var\", []), x\n                     )\n-                # We need to ignore reuse=False in the shim, because the\n-                # code is expected to get rerun each time the user calls the shim.\n+                # We need to ignore reuse=False in the shim, because the code is\n+                # expected to get rerun each time the user calls the shim.\n                 with tf.compat.v1.variable_scope(\n                     \"testVarScopeGetOrCreateReuse_bar\", reuse=False\n                 ):\n@@ -905,7 +907,8 @@ class VariableScopeMultithreadedTest(tf.test.TestCase):\n                         v = tf.compat.v1.get_variable(\"v\", [])\n                         self.assertEqual(\"main/foo/v:0\", v.name)\n \n-                # Variable created outside main scope will not have prefix \"main\".\n+                # Variable created outside main scope will not have prefix\n+                # \"main\".\n                 with tf.compat.v1.variable_scope(\"bar\"):\n                     v = tf.compat.v1.get_variable(\"v\", [])\n                     self.assertEqual(\"bar/v:0\", v.name)\n@@ -951,7 +954,8 @@ class VariableScopeModule(tf.Module):\n             return self.forward_pass(*args, **kwargs)\n \n     def get_compat_v1_regularization_losses(self):\n-        \"\"\"Dict w/ regularization losses from `get_variable`&`compat.v1.layers`.\"\"\"\n+        \"\"\"Dict w/ regularization losses from\n+        `get_variable`&`compat.v1.layers`.\"\"\"\n         return {\n             name: regularizer()\n             for name, regularizer in self._tf1_style_var_store._regularizers.items()\n@@ -1012,7 +1016,8 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n         out = layer(tf.ones(shape=(5, 5)))\n         weights = {x.name: x for x in layer.variables}\n \n-        # Verify the correct output, regularization losses, + variables were made\n+        # Verify the correct output, regularization losses, + variables were\n+        # made\n         self.assertEqual(\n             weights.keys(),\n             {\n@@ -1130,7 +1135,8 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n         tf.saved_model.save(model, tmp_dir)\n \n     def test_variable_store_scope_get_variable(self):\n-        # Test the module shim when using `get_variable` (and regularizers) directly\n+        # Test the module shim when using `get_variable` (and regularizers)\n+        # directly\n \n         class WrappedDenseLayer(tf.Module):\n             def __init__(self, units, *args, **kwargs):\n@@ -1189,7 +1195,8 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n         out = layer(tf.ones(shape=(5, 5)))\n         weights = {x.name: x for x in layer.variables}\n \n-        # Verify the correct output, regularization losses, + variables were made\n+        # Verify the correct output, regularization losses, + variables were\n+        # made\n         self.assertEqual(\n             weights.keys(),\n             {\n@@ -1216,7 +1223,8 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n         )\n \n     def test_module_get_variable(self):\n-        # Test the module shim when using `get_variable` (and regularizers) directly\n+        # Test the module shim when using `get_variable` (and regularizers)\n+        # directly\n \n         class WrappedDenseLayer(VariableScopeModule):\n             def __init__(self, units, *args, **kwargs):\n@@ -1266,7 +1274,8 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n         out = layer(tf.ones(shape=(5, 5)))\n         weights = {x.name: x for x in layer.variables}\n \n-        # Verify the correct output, regularization losses, + variables were made\n+        # Verify the correct output, regularization losses, + variables were\n+        # made\n         self.assertEqual(\n             weights.keys(),\n             {\n@@ -1388,8 +1397,8 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n \n             @variable_scope_shim.track_tf1_style_variables\n             def call(self, inputs):\n-                # Only create the nested tf.variable/module/layer/model if it has not\n-                # already been created!\n+                # Only create the nested tf.variable/module/layer/model if it\n+                # has not already been created!\n                 if not self.dense_layer_a:\n                     self.dense_layer_a = NestedLayer(\n                         self.units * 2, \"dense_one\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
