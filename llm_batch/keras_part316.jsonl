{"custom_id": "keras#ad9268d67014273e35faac4ff21cbfe929bf1d2b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 76 | Lines Deleted: 538 | Files Changed: 21 | Hunks: 66 | Methods Changed: 63 | Complexity Δ (Sum/Max): -50/2 | Churn Δ: 614 | Churn Cumulative: 66919 | Contributors (this commit): 248 | Commits (past 90d): 95 | Contributors (cumulative): 374 | DMM Complexity: 0.10619469026548672\n\nDIFF:\n@@ -37,7 +37,7 @@ def _get_metadata(name):\n def _get_layer_args(layer_cls, layer_args):\n   # To make benchmark parameters be compatible with GPU platform.\n   if layer_cls is tf.keras.layers.Bidirectional:\n-    return layer_args.update({\"layer\": tf.keras.layers.LSTM(1)})\n+    return {\"layer\": tf.keras.layers.LSTM(1)}\n   return layer_args\n \n \n\n@@ -2379,7 +2379,7 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n     if self.write_steps_per_second:\n       batch_run_time = time.time() - self._batch_start_time\n       self._train_accumulated_time += batch_run_time\n-      summary_ops_v2.scalar('batch_steps_per_second', 1. / batch_run_time)\n+      tf.summary.scalar('batch_steps_per_second', 1. / batch_run_time)\n     if not self._should_trace:\n       return\n \n@@ -2451,12 +2451,12 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n       if train_logs:\n         with self._train_writer.as_default():\n           for name, value in train_logs.items():\n-            summary_ops_v2.scalar('epoch_' + name, value, step=epoch)\n+            tf.summary.scalar('epoch_' + name, value, step=epoch)\n       if val_logs:\n         with self._val_writer.as_default():\n           for name, value in val_logs.items():\n             name = name[4:]  # Remove 'val_' prefix.\n-            summary_ops_v2.scalar('epoch_' + name, value, step=epoch)\n+            tf.summary.scalar('epoch_' + name, value, step=epoch)\n \n   def _log_weights(self, epoch):\n     \"\"\"Logs the weights of the Model to TensorBoard.\"\"\"\n@@ -2465,7 +2465,7 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n         for layer in self.model.layers:\n           for weight in layer.weights:\n             weight_name = weight.name.replace(':', '_')\n-            summary_ops_v2.histogram(weight_name, weight, step=epoch)\n+            tf.summary.histogram(weight_name, weight, step=epoch)\n             if self.write_images:\n               self._log_weight_as_image(weight, weight_name, epoch)\n         self._train_writer.flush()\n@@ -2492,7 +2492,7 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n     shape = K.int_shape(w_img)\n     # Not possible to handle 3D convnets etc.\n     if len(shape) == 4 and shape[-1] in [1, 3, 4]:\n-      summary_ops_v2.image(weight_name, w_img, step=epoch)\n+      tf.summary.image(weight_name, w_img, step=epoch)\n \n   def _log_embeddings(self, epoch):\n     embeddings_ckpt = os.path.join(self._log_write_dir, 'train',\n\n@@ -1821,6 +1821,7 @@ class _SummaryFile(object):\n     self.histograms = set()\n     self.tensors = set()\n     self.graph_defs = []\n+    self.convert_from_v2_summary_proto = False\n \n \n def list_summaries(logdir):\n@@ -1868,11 +1869,17 @@ def list_summaries(logdir):\n                 'Unexpected summary kind %r in event file %s:\\n%r'\n                 % (kind, path, event))\n           elif kind == 'tensor' and tag != 'keras':\n-            # Check for V2 scalar summaries, which have a different PB\n-            # structure.\n-            if event.summary.value[\n-                0].metadata.plugin_data.plugin_name == 'scalars':\n-              container = result.scalars\n+            # Convert the tf2 summary proto to old style for type checking.\n+            plugin_name = value.metadata.plugin_data.plugin_name\n+            container = {\n+                'images': result.images,\n+                'histograms': result.histograms,\n+                'scalars': result.scalars,\n+            }.get(plugin_name)\n+            if container is not None:\n+              result.convert_from_v2_summary_proto = True\n+            else:\n+              container = result.tensors\n           container.add(_ObservedSummary(logdir=dirpath, tag=tag))\n   return result\n \n@@ -2128,14 +2135,21 @@ class TestTensorBoardV2(keras_parameterized.TestCase):\n             _ObservedSummary(logdir=self.train_dir, tag='kernel_0'),\n         },\n     )\n-    self.assertEqual(\n-        self._strip_layer_names(summary_file.images, model_type),\n-        {\n+    if summary_file.convert_from_v2_summary_proto:\n+      expected = {\n+          _ObservedSummary(logdir=self.train_dir, tag='bias_0'),\n+          _ObservedSummary(logdir=self.train_dir, tag='kernel_0'),\n+      }\n+    else:\n+      expected = {\n           _ObservedSummary(logdir=self.train_dir, tag='bias_0/image/0'),\n           _ObservedSummary(logdir=self.train_dir, tag='kernel_0/image/0'),\n           _ObservedSummary(logdir=self.train_dir, tag='kernel_0/image/1'),\n           _ObservedSummary(logdir=self.train_dir, tag='kernel_0/image/2'),\n-        },\n+      }\n+    self.assertEqual(\n+        self._strip_layer_names(summary_file.images, model_type),\n+        expected\n     )\n \n   def test_TensorBoard_projector_callback(self):\n\n@@ -26,7 +26,6 @@ import os\n import numpy as np\n from keras import backend as K\n from keras import callbacks\n-from tensorflow.python.ops import summary_ops_v2\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n@@ -324,7 +323,7 @@ class TensorBoard(callbacks.TensorBoard):\n         for name, value in logs.items():\n           if isinstance(value, np.ndarray):\n             value = value.item()\n-          summary_ops_v2.scalar(name, value, step=step)\n+          tf.summary.scalar(name, value, step=step)\n     else:\n       # use FileWriter from v1 summary\n       for name, value in logs.items():\n\n@@ -22,7 +22,6 @@ from __future__ import print_function\n import tensorflow as tf\n \n import re\n-from tensorflow.python.ops import summary_ops_v2\n from tensorflow.python.platform import tf_logging as logging\n \n _PRINT_EVAL_STEP_EVERY_SEC = 60.0\n@@ -212,10 +211,9 @@ class SidecarEvaluator(object):\n       ])\n \n       if self._summary_writer:\n-        with summary_ops_v2.always_record_summaries(\n-        ), self._summary_writer.as_default():\n+        with tf.summary.record_if(True), self._summary_writer.as_default():\n           for metric in self.model.compiled_metrics.metrics:\n-            summary_ops_v2.scalar(\n+            tf.summary.scalar(\n                 metric.name,\n                 metric.result(),\n                 step=self._iterations.read_value())\n\n@@ -284,6 +284,13 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n   # not available to the restoration code).\n   _must_restore_from_config = False\n \n+  def _get_cell_name(self):\n+    canonical_name = get_canonical_name_for_symbol(\n+        self.__class__, api_name='keras', add_prefix_to_v1_names=True)\n+    if canonical_name is not None:\n+      return 'tf.{}'.format(canonical_name)\n+    return self.__class__.__module__ + '.' + self.__class__.__name__\n+\n   def _instrument_layer_creation(self):\n     self._instrumented_keras_api = False\n     self._instrumented_keras_layer_class = False\n@@ -292,10 +299,10 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n       keras_api_gauge.get_cell('layer').set(True)\n       self._instrumented_keras_api = True\n       if getattr(self, '_is_model_for_instrumentation', False):\n-        keras_models_gauge.get_cell(self.__class__.__name__).set(True)\n+        keras_models_gauge.get_cell(self._get_cell_name()).set(True)\n         self._instrumented_keras_model_class = True\n       else:\n-        keras_layers_gauge.get_cell(self.__class__.__name__).set(True)\n+        keras_layers_gauge.get_cell(self._get_cell_name()).set(True)\n         self._instrumented_keras_layer_class = True\n \n   @trackable.no_automatic_dependency_tracking\n\n@@ -39,7 +39,6 @@ from keras.engine import training as training_lib\n from keras.legacy_tf_layers import core as legacy_core\n from keras.optimizer_v2 import rmsprop\n from keras.utils import control_flow_util\n-from tensorflow.python.ops import summary_ops_v2\n \n \n class DynamicLayer(base_layer.Layer):\n@@ -66,12 +65,14 @@ class InvalidLayer(base_layer.Layer):\n \n class BaseLayerTest(keras_parameterized.TestCase):\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @combinations.generate(combinations.keras_mode_combinations())\n   def test_layer_instrumentation(self):\n     layer = layers.Add()\n     self.assertTrue(layer._instrumented_keras_api)\n     self.assertTrue(layer._instrumented_keras_layer_class)\n     self.assertFalse(layer._instrumented_keras_model_class)\n+    self.assertTrue(base_layer.keras_api_gauge.get_cell('tf.keras.layers.Add'))\n+    base_layer.keras_api_gauge.get_cell('tf.keras.layers.Add').set(False)\n \n   @combinations.generate(combinations.keras_model_type_combinations())\n   def test_dynamic_layer(self):\n@@ -947,12 +948,12 @@ class SymbolicSupportTest(keras_parameterized.TestCase):\n     class MyLayer(base_layer.Layer):\n \n       def call(self, inputs):\n-        summary_ops_v2.scalar('mean', tf.reduce_mean(inputs))\n+        tf.summary.scalar('mean', tf.reduce_mean(inputs))\n         return inputs\n \n     tmp_dir = self.get_temp_dir()\n     writer = tf.summary.create_file_writer(tmp_dir)\n-    with writer.as_default(), tf.summary.record_if(True):\n+    with writer.as_default(step=1), tf.summary.record_if(True):\n       my_layer = MyLayer()\n       x = tf.ones((10, 10))\n \n\n@@ -666,7 +666,6 @@ class Functional(training_lib.Model):\n     Raises:\n         ValueError: In case of improperly formatted config dict.\n     \"\"\"\n-    with generic_utils.SharedObjectLoadingScope():\n     input_tensors, output_tensors, created_layers = reconstruct_from_config(\n         config, custom_objects)\n     model = cls(inputs=input_tensors, outputs=output_tensors,\n@@ -1342,8 +1341,6 @@ def get_network_config(network, serialize_layer_fn=None):\n         node_conversion_map[node_key] = kept_nodes\n         kept_nodes += 1\n   layer_configs = []\n-\n-  with generic_utils.SharedObjectSavingScope():\n   for layer in network.layers:  # From the earliest layers on.\n     filtered_inbound_nodes = []\n     for original_node_index, node in enumerate(layer._inbound_nodes):\n\n@@ -53,7 +53,6 @@ from keras.utils import version_utils\n from keras.utils.io_utils import ask_to_proceed_with_overwrite\n from keras.utils.io_utils import path_to_string\n from keras.utils.mode_keys import ModeKeys\n-from tensorflow.python.ops import summary_ops_v2\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.training import checkpoint_management\n from tensorflow.python.training.tracking import base as trackable\n@@ -2749,11 +2748,7 @@ def _collective_all_reduce_multi_worker(strategy):\n # for all strategies\n def _multi_worker_concat(v, strategy):\n   \"\"\"Order PerReplica objects for CollectiveAllReduceStrategy and concat.\"\"\"\n-  replicas = strategy.gather(v, axis=0)  # pylint: disable=protected-access\n-  # TODO(b/170435030): We now need to make sure these run after the iterator\n-  # GetNext, so that we don't trigger aborting collective ops in the case of\n-  # EOF. Remove after the issue is fixed.\n-  with tf.control_dependencies([replicas]):\n+  replicas = strategy.gather(v, axis=0)\n   # v might not have the same shape on different replicas\n   if isinstance(v, ds_values.PerReplica):\n     shapes = tf.concat([\n@@ -2765,8 +2760,7 @@ def _multi_worker_concat(v, strategy):\n   else:\n     # v is a tensor. This may happen when, say, we have 2x1 multi-worker.\n     all_shapes = strategy.gather(\n-          tf.expand_dims(tf.compat.v1.shape(v)[0], axis=0),\n-          axis=0)\n+        tf.expand_dims(tf.compat.v1.shape(v)[0], axis=0), axis=0)\n \n   replicas = tf.split(\n       replicas,\n@@ -2786,7 +2780,7 @@ def _is_scalar(x):\n def write_scalar_summaries(logs, step):\n   for name, value in logs.items():\n     if _is_scalar(value):\n-      summary_ops_v2.scalar('batch_' + name, value, step=step)\n+      tf.summary.scalar('batch_' + name, value, step=step)\n \n \n def _minimum_control_deps(outputs):\n\n@@ -26,7 +26,6 @@ from keras.engine import base_preprocessing_layer\n from keras.engine.base_preprocessing_layer import PreprocessingLayer\n from keras.engine.input_spec import InputSpec\n from keras.utils import control_flow_util\n-from tensorflow.python.ops import control_flow_ops\n from tensorflow.python.ops import stateless_random_ops\n from tensorflow.python.util.tf_export import keras_export\n \n@@ -224,7 +223,7 @@ class RandomCrop(PreprocessingLayer):\n       check = tf.Assert(\n           tf.reduce_all(input_shape >= crop_size),\n           [self.height, self.width])\n-      input_shape = control_flow_ops.with_dependencies([check], input_shape)\n+      with tf.control_dependencies([check]):\n         limit = input_shape - crop_size + 1\n         offset = stateless_random_ops.stateless_random_uniform(\n             tf.compat.v1.shape(input_shape),\n\n@@ -57,7 +57,6 @@ from keras.utils.generic_utils import deserialize_keras_object\n from keras.utils.generic_utils import serialize_keras_object\n from keras.utils.generic_utils import to_list\n from keras.utils.tf_utils import is_tensor_or_variable\n-from tensorflow.python.ops import control_flow_ops\n from tensorflow.python.util.tf_export import keras_export\n from tensorflow.tools.docs import doc_controls\n \n@@ -2006,8 +2005,8 @@ class AUC(Metric):\n               label_weights,\n               message='All values of `label_weights` must be non-negative.')\n       ]\n-      self.label_weights = control_flow_ops.with_dependencies(\n-          checks, label_weights)\n+      with tf.control_dependencies(checks):\n+        self.label_weights = label_weights\n \n     else:\n       self.label_weights = None\n\n@@ -392,10 +392,6 @@ def clone_model(model, input_tensors=None, clone_function=None):\n   except that it creates new layers (and thus new weights) instead\n   of sharing the weights of the existing layers.\n \n-  `clone_model` will not preserve the uniqueness of shared objects within the\n-  model (e.g. a single variable attached to two distinct layers will be\n-  restored as two separate variables).\n-\n   Args:\n       model: Instance of `Model`\n           (could be a functional model or a Sequential model).\n@@ -423,7 +419,6 @@ def clone_model(model, input_tensors=None, clone_function=None):\n   Raises:\n       ValueError: in case of invalid `model` argument value.\n   \"\"\"\n-  with generic_utils.DisableSharedObjectScope():\n   if clone_function is None:\n     clone_function = _clone_layer\n \n\n@@ -239,28 +239,6 @@ class TestModelCloning(keras_parameterized.TestCase):\n     loss = model.train_on_batch(x, y)\n     self.assertEqual(float(loss), 0.)\n \n-  def test_clone_rnn(self):\n-    # Test cloning a model with multiple cells in an RNN.  This exercises a\n-    # few \"fancier\" features such as the `Bidrectional` wrapper and\n-    # `StackedRNNCells` under the hood.\n-    inputs = keras.Input(shape=(3, 3))\n-    cells = [\n-        keras.layers.LSTMCell(\n-            units=32,\n-            enable_caching_device=True,\n-            implementation=2,\n-            activation='relu')]\n-    rnn = keras.layers.RNN(cells, return_sequences=True)\n-    outputs = keras.layers.Bidirectional(rnn)(inputs)\n-    outputs = keras.layers.Dense(\n-        12, activation='softmax', name='scores')(outputs)\n-    model = keras.Model(inputs=inputs, outputs=outputs)\n-    model.compile(\n-        loss=keras.losses.CategoricalCrossentropy(),\n-        optimizer=keras.optimizer_v2.rmsprop.RMSprop(lr=0.01),\n-        metrics=['accuracy'])\n-    keras.models.clone_model(model)\n-\n   def test_model_cloning_invalid_use_cases(self):\n     seq_model = keras.models.Sequential()\n     seq_model.add(keras.layers.Dense(4, input_shape=(4,)))\n\n@@ -148,7 +148,6 @@ def save_model(model,\n     hdf5_format.save_model_to_hdf5(\n         model, filepath, overwrite, include_optimizer)\n   else:\n-    with generic_utils.SharedObjectSavingScope():\n     saved_model_save.save(model, filepath, overwrite, include_optimizer,\n                           signatures, options, save_traces)\n \n@@ -195,7 +194,6 @@ def load_model(filepath, custom_objects=None, compile=True, options=None):  # py\n       ImportError: if loading from an hdf5 file and h5py is not available.\n       IOError: In case of an invalid savefile.\n   \"\"\"\n-  with generic_utils.SharedObjectLoadingScope():\n   with generic_utils.CustomObjectScope(custom_objects or {}):\n     with load_context.load_context(options):\n       if (h5py is not None and\n\n@@ -20,7 +20,6 @@ from __future__ import print_function\n \n import tensorflow as tf\n \n-import collections\n import os\n import shutil\n import sys\n@@ -28,7 +27,6 @@ import tempfile\n \n from absl.testing import parameterized\n import numpy as np\n-from six import string_types\n \n import keras\n from keras import combinations\n@@ -852,125 +850,6 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n     self.assertAllEqual(loaded_model.predict(args, batch_size=batch_size),\n                         expected)\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n-  def test_shared_objects(self):\n-    class OuterLayer(keras.layers.Layer):\n-\n-      def __init__(self, inner_layer):\n-        super(OuterLayer, self).__init__()\n-        self.inner_layer = inner_layer\n-\n-      def call(self, inputs):\n-        return self.inner_layer(inputs)\n-\n-      def get_config(self):\n-        return {\n-            'inner_layer': generic_utils.serialize_keras_object(\n-                self.inner_layer)\n-        }\n-\n-      @classmethod\n-      def from_config(cls, config):\n-        return cls(generic_utils.deserialize_keras_object(\n-            config['inner_layer']))\n-\n-    class InnerLayer(keras.layers.Layer):\n-\n-      def __init__(self):\n-        super(InnerLayer, self).__init__()\n-        self.v = self.add_weight(name='v', shape=[], dtype=tf.float32)\n-\n-      def call(self, inputs):\n-        return self.v + inputs\n-\n-      @classmethod\n-      def from_config(cls, config):\n-        return cls()\n-\n-    # Create a model with 2 output layers that share the same inner layer.\n-    inner_layer = InnerLayer()\n-    outer_layer_1 = OuterLayer(inner_layer)\n-    outer_layer_2 = OuterLayer(inner_layer)\n-    input_ = keras.Input(shape=(1,))\n-    model = keras.Model(\n-        inputs=input_, outputs=[outer_layer_1(input_), outer_layer_2(input_)])\n-\n-    # Changes to the shared layer should affect both outputs.\n-    model.layers[1].inner_layer.v.assign(5)\n-    self.assertAllEqual(model(1), [6.0, 6.0])\n-    model.layers[1].inner_layer.v.assign(3)\n-    self.assertAllEqual(model(1), [4.0, 4.0])\n-\n-    # After loading, changes to the shared layer should still affect both\n-    # outputs.\n-    def _do_assertions(loaded):\n-      loaded.layers[1].inner_layer.v.assign(5)\n-      self.assertAllEqual(loaded(1), [6.0, 6.0])\n-      loaded.layers[1].inner_layer.v.assign(3)\n-      self.assertAllEqual(loaded(1), [4.0, 4.0])\n-      loaded.layers[2].inner_layer.v.assign(5)\n-      self.assertAllEqual(loaded(1), [6.0, 6.0])\n-      loaded.layers[2].inner_layer.v.assign(3)\n-      self.assertAllEqual(loaded(1), [4.0, 4.0])\n-\n-    # We'd like to make sure we only attach shared object IDs when strictly\n-    # necessary, so we'll recursively traverse the generated config to count\n-    # whether we have the exact number we expect.\n-    def _get_all_keys_recursive(dict_or_iterable):\n-      if isinstance(dict_or_iterable, dict):\n-        for key in dict_or_iterable.keys():\n-          yield key\n-        for key in _get_all_keys_recursive(dict_or_iterable.values()):\n-          yield key\n-      elif isinstance(dict_or_iterable, string_types):\n-        return\n-      else:\n-        try:\n-          for item in dict_or_iterable:\n-            for key in _get_all_keys_recursive(item):\n-              yield key\n-        # Not an iterable or dictionary\n-        except TypeError:\n-          return\n-\n-    with generic_utils.CustomObjectScope({\n-        'OuterLayer': OuterLayer, 'InnerLayer': InnerLayer}):\n-\n-      # Test saving and loading to disk\n-      save_format = testing_utils.get_save_format()\n-      saved_model_dir = self._save_model_dir()\n-      keras.models.save_model(model, saved_model_dir, save_format=save_format)\n-      loaded = keras.models.load_model(saved_model_dir)\n-      _do_assertions(loaded)\n-\n-      # Test recreating directly from config\n-      config = model.get_config()\n-      key_count = collections.Counter(_get_all_keys_recursive(config))\n-      self.assertEqual(key_count[generic_utils.SHARED_OBJECT_KEY], 2)\n-      loaded = keras.Model.from_config(config)\n-      _do_assertions(loaded)\n-\n-  @combinations.generate(combinations.combine(mode=['eager']))\n-  def test_shared_objects_wrapper(self):\n-    \"\"\"Tests that shared layers wrapped with `Wrapper` restore correctly.\"\"\"\n-    input_ = keras.Input(shape=(1,))\n-    unwrapped = keras.layers.Layer(name='unwrapped')\n-    wrapped = keras.layers.Wrapper(unwrapped, name='wrapped')\n-    model = keras.Model(inputs=input_,\n-                        outputs=[unwrapped(input_), wrapped(input_)])\n-\n-    # Test recreating directly from config\n-    config = model.get_config()\n-    loaded = keras.Model.from_config(config)\n-    self.assertIs(loaded.layers[1], loaded.layers[2].layer)\n-\n-    # Test saving and loading to disk\n-    save_format = testing_utils.get_save_format()\n-    saved_model_dir = self._save_model_dir()\n-    keras.models.save_model(model, saved_model_dir, save_format=save_format)\n-    loaded = keras.models.load_model(saved_model_dir)\n-    self.assertIs(loaded.layers[1], loaded.layers[2].layer)\n-\n \n # Factory functions to create models that will be serialized inside a Network.\n def _make_graph_network(input_size, output_size):\n\n@@ -47,6 +47,7 @@ class LayerSavedModelSaver(base_serialization.SavedModelSaver):\n     # TODO(kathywu): Synchronize with the keras spec (go/keras-json-spec) once\n     # the python config serialization has caught up.\n     metadata = dict(\n+        class_name=generic_utils.get_registered_name(type(self.obj)),\n         name=self.obj.name,\n         trainable=self.obj.trainable,\n         expects_training_arg=self.obj._expects_training_arg,  # pylint: disable=protected-access\n@@ -56,7 +57,7 @@ class LayerSavedModelSaver(base_serialization.SavedModelSaver):\n         must_restore_from_config=self.obj._must_restore_from_config,  # pylint: disable=protected-access\n     )\n \n-    metadata.update(get_serialized(self.obj))\n+    metadata.update(get_config(self.obj))\n     if self.obj.input_spec is not None:\n       # Layer's input_spec has already been type-checked in the property setter.\n       metadata['input_spec'] = tf.nest.map_structure(\n@@ -110,12 +111,16 @@ class LayerSavedModelSaver(base_serialization.SavedModelSaver):\n \n # TODO(kathywu): Move serialization utils (and related utils from\n # generic_utils.py) to a separate file.\n-def get_serialized(obj):\n+def get_config(obj):\n   with generic_utils.skip_failed_serialization():\n     # Store the config dictionary, which may be used when reviving the object.\n     # When loading, the program will attempt to revive the object from config,\n     # and if that fails, the object will be revived from the SavedModel.\n-    return generic_utils.serialize_keras_object(obj)\n+    config = generic_utils.serialize_keras_object(obj)['config']\n+\n+  if config is not None:\n+    return {'config': config}\n+  return {}\n \n \n class InputLayerSavedModelSaver(base_serialization.SavedModelSaver):\n\n@@ -485,15 +485,13 @@ class KerasObjectLoader(object):\n     #       found.\n     class_name = metadata.get('class_name')\n     config = metadata.get('config')\n-    shared_object_id = metadata.get('shared_object_id')\n     must_restore_from_config = metadata.get('must_restore_from_config')\n     if not generic_utils.validate_config(config):\n       return None\n \n     try:\n       obj = layers_module.deserialize(\n-          generic_utils.serialize_keras_class_and_config(\n-              class_name, config, shared_object_id=shared_object_id))\n+          generic_utils.serialize_keras_class_and_config(class_name, config))\n     except ValueError:\n       if must_restore_from_config:\n         raise RuntimeError(\n\n@@ -36,7 +36,7 @@ class MetricSavedModelSaver(layer_serialization.LayerSavedModelSaver):\n         class_name=generic_utils.get_registered_name(type(self.obj)),\n         name=self.obj.name,\n         dtype=self.obj.dtype)\n-    metadata.update(layer_serialization.get_serialized(self.obj))\n+    metadata.update(layer_serialization.get_config(self.obj))\n     if self.obj._build_input_shape is not None:  # pylint: disable=protected-access\n       metadata['build_input_shape'] = self.obj._build_input_shape  # pylint: disable=protected-access\n     return metadata\n\n@@ -26,10 +26,8 @@ import marshal\n import os\n import re\n import sys\n-import threading\n import time\n import types as python_types\n-import weakref\n \n import numpy as np\n import six\n@@ -112,223 +110,9 @@ def get_custom_objects():\n   return _GLOBAL_CUSTOM_OBJECTS\n \n \n-# Store a unique, per-object ID for shared objects.\n-#\n-# We store a unique ID for each object so that we may, at loading time,\n-# re-create the network properly.  Without this ID, we would have no way of\n-# determining whether a config is a description of a new object that\n-# should be created or is merely a reference to an already-created object.\n-SHARED_OBJECT_KEY = 'shared_object_id'\n-\n-\n-SHARED_OBJECT_DISABLED = threading.local()\n-SHARED_OBJECT_LOADING = threading.local()\n-SHARED_OBJECT_SAVING = threading.local()\n-\n-\n-# Attributes on the threadlocal variable must be set per-thread, thus we\n-# cannot initialize these globally. Instead, we have accessor functions with\n-# default values.\n-def _shared_object_disabled():\n-  \"\"\"Get whether shared object handling is disabled in a threadsafe manner.\"\"\"\n-  return getattr(SHARED_OBJECT_DISABLED, 'disabled', False)\n-\n-\n-def _shared_object_loading_scope():\n-  \"\"\"Get the current shared object saving scope in a threadsafe manner.\"\"\"\n-  return getattr(SHARED_OBJECT_LOADING, 'scope', NoopLoadingScope())\n-\n-\n-def _shared_object_saving_scope():\n-  \"\"\"Get the current shared object saving scope in a threadsafe manner.\"\"\"\n-  return getattr(SHARED_OBJECT_SAVING, 'scope', None)\n-\n-\n-class DisableSharedObjectScope(object):\n-  \"\"\"A context manager for disabling handling of shared objects.\n-\n-  Disables shared object handling for both saving and loading.\n-\n-  Created primarily for use with `clone_model`, which does extra surgery that\n-  is incompatible with shared objects.\n-  \"\"\"\n-\n-  def __enter__(self):\n-    SHARED_OBJECT_DISABLED.disabled = True\n-    self._orig_loading_scope = _shared_object_loading_scope()\n-    self._orig_saving_scope = _shared_object_saving_scope()\n-\n-  def __exit__(self, *args, **kwargs):\n-    SHARED_OBJECT_DISABLED.disabled = False\n-    SHARED_OBJECT_LOADING.scope = self._orig_loading_scope\n-    SHARED_OBJECT_SAVING.scope = self._orig_saving_scope\n-\n-\n-class NoopLoadingScope(object):\n-  \"\"\"The default shared object loading scope. It does nothing.\n-\n-  Created to simplify serialization code that doesn't care about shared objects\n-  (e.g. when serializing a single object).\n-  \"\"\"\n-\n-  def get(self, unused_object_id):\n-    return None\n-\n-  def set(self, object_id, obj):\n-    pass\n-\n-\n-class SharedObjectLoadingScope(object):\n-  \"\"\"A context manager for keeping track of loaded objects.\n-\n-  During the deserialization process, we may come across objects that are\n-  shared across multiple layers. In order to accurately restore the network\n-  structure to its original state, `SharedObjectLoadingScope` allows us to\n-  re-use shared objects rather than cloning them.\n-  \"\"\"\n-\n-  def __enter__(self):\n-    if _shared_object_disabled():\n-      return NoopLoadingScope()\n-\n-    global SHARED_OBJECT_LOADING\n-    SHARED_OBJECT_LOADING.scope = self\n-    self._obj_ids_to_obj = {}\n-    return self\n-\n-  def get(self, object_id):\n-    \"\"\"Given a shared object ID, returns a previously instantiated object.\n-\n-    Args:\n-      object_id: shared object ID to use when attempting to find already-loaded\n-        object.\n-\n-    Returns:\n-      The object, if we've seen this ID before. Else, `None`.\n-    \"\"\"\n-    # Explicitly check for `None` internally to make external calling code a\n-    # bit cleaner.\n-    if object_id is None:\n-      return\n-    return self._obj_ids_to_obj.get(object_id)\n-\n-  def set(self, object_id, obj):\n-    \"\"\"Stores an instantiated object for future lookup and sharing.\"\"\"\n-    if object_id is None:\n-      return\n-    self._obj_ids_to_obj[object_id] = obj\n-\n-  def __exit__(self, *args, **kwargs):\n-    global SHARED_OBJECT_LOADING\n-    SHARED_OBJECT_LOADING.scope = NoopLoadingScope()\n-\n-\n-class SharedObjectConfig(dict):\n-  \"\"\"A configuration container that keeps track of references.\n-\n-  `SharedObjectConfig` will automatically attach a shared object ID to any\n-  configs which are referenced more than once, allowing for proper shared\n-  object reconstruction at load time.\n-\n-  In most cases, it would be more proper to subclass something like\n-  `collections.UserDict` or `collections.Mapping` rather than `dict` directly.\n-  Unfortunately, python's json encoder does not support `Mapping`s. This is\n-  important functionality to retain, since we are dealing with serialization.\n-\n-  We should be safe to subclass `dict` here, since we aren't actually\n-  overriding any core methods, only augmenting with a new one for reference\n-  counting.\n-  \"\"\"\n-\n-  def __init__(self, base_config, object_id, **kwargs):\n-    self.ref_count = 1\n-    self.object_id = object_id\n-    super(SharedObjectConfig, self).__init__(base_config, **kwargs)\n-\n-  def increment_ref_count(self):\n-    # As soon as we've seen the object more than once, we want to attach the\n-    # shared object ID. This allows us to only attach the shared object ID when\n-    # it's strictly necessary, making backwards compatibility breakage less\n-    # likely.\n-    if self.ref_count == 1:\n-      self[SHARED_OBJECT_KEY] = self.object_id\n-    self.ref_count += 1\n-\n-\n-class SharedObjectSavingScope(object):\n-  \"\"\"Keeps track of shared object configs when serializing.\"\"\"\n-\n-  def __enter__(self):\n-    if _shared_object_disabled():\n-      return None\n-\n-    global SHARED_OBJECT_SAVING\n-\n-    # Serialization can happen at a number of layers for a number of reasons.\n-    # We may end up with a case where we're opening a saving scope within\n-    # another saving scope. In that case, we'd like to use the outermost scope\n-    # available and ignore inner scopes, since there is not (yet) a reasonable\n-    # use case for having these nested and distinct.\n-    if _shared_object_saving_scope() is not None:\n-      self._passthrough = True\n-      return _shared_object_saving_scope()\n-    else:\n-      self._passthrough = False\n-\n-    SHARED_OBJECT_SAVING.scope = self\n-    self._shared_objects_config = weakref.WeakKeyDictionary()\n-    self._next_id = 0\n-    return self\n-\n-  def get_config(self, obj):\n-    \"\"\"Gets a `SharedObjectConfig` if one has already been seen for `obj`.\n-\n-    Args:\n-      obj: The object for which to retrieve the `SharedObjectConfig`.\n-\n-    Returns:\n-      The SharedObjectConfig for a given object, if already seen. Else,\n-        `None`.\n-    \"\"\"\n-    if obj in self._shared_objects_config:\n-      shared_object_config = self._shared_objects_config[obj]\n-      shared_object_config.increment_ref_count()\n-      return shared_object_config\n-\n-  def create_config(self, base_config, obj):\n-    shared_object_config = SharedObjectConfig(base_config, self._next_id)\n-    self._next_id += 1\n-    self._shared_objects_config[obj] = shared_object_config\n-    return shared_object_config\n-\n-  def __exit__(self, *args, **kwargs):\n-    if not getattr(self, '_passthrough', False):\n-      global SHARED_OBJECT_SAVING\n-      SHARED_OBJECT_SAVING.scope = None\n-\n-\n-def serialize_keras_class_and_config(\n-    cls_name, cls_config, obj=None, shared_object_id=None):\n+def serialize_keras_class_and_config(cls_name, cls_config):\n   \"\"\"Returns the serialization of the class with the given config.\"\"\"\n-  base_config = {'class_name': cls_name, 'config': cls_config}\n-\n-  # We call `serialize_keras_class_and_config` for some branches of the load\n-  # path. In that case, we may already have a shared object ID we'd like to\n-  # retain.\n-  if shared_object_id is not None:\n-    base_config[SHARED_OBJECT_KEY] = shared_object_id\n-\n-  # If we have an active `SharedObjectSavingScope`, check whether we've already\n-  # serialized this config. If so, just use that config. This will store an\n-  # extra ID field in the config, allowing us to re-create the shared object\n-  # relationship at load time.\n-  if _shared_object_saving_scope() is not None and obj is not None:\n-    shared_object_config = _shared_object_saving_scope().get_config(obj)\n-    if shared_object_config is None:\n-      return _shared_object_saving_scope().create_config(base_config, obj)\n-    return shared_object_config\n-\n-  return base_config\n+  return {'class_name': cls_name, 'config': cls_config}\n \n \n @keras_export('keras.utils.register_keras_serializable')\n@@ -450,19 +234,7 @@ def get_registered_object(name, custom_objects=None, module_objects=None):\n \n @keras_export('keras.utils.serialize_keras_object')\n def serialize_keras_object(instance):\n-  \"\"\"Serialize a Keras object into a JSON-compatible representation.\n-\n-  Calls to `serialize_keras_object` while underneath the\n-  `SharedObjectSavingScope` context manager will cause any objects re-used\n-  across multiple layers to be saved with a special shared object ID. This\n-  allows the network to be re-created properly during deserialization.\n-\n-  Args:\n-    instance: The object to serialize.\n-\n-  Returns:\n-    A dict-like, JSON-compatible representation of the object's config.\n-  \"\"\"\n+  \"\"\"Serialize a Keras object into a JSON-compatible representation.\"\"\"\n   _, instance = tf.__internal__.decorator.unwrap(instance)\n   if instance is None:\n     return None\n@@ -493,8 +265,7 @@ def serialize_keras_object(instance):\n         serialization_config[key] = item\n \n     name = get_registered_name(instance.__class__)\n-    return serialize_keras_class_and_config(\n-        name, serialization_config, instance)\n+    return serialize_keras_class_and_config(name, serialization_config)\n   if hasattr(instance, '__name__'):\n     return get_registered_name(instance)\n   raise ValueError('Cannot serialize', instance)\n@@ -515,9 +286,8 @@ def class_and_config_for_serialized_keras_object(\n     custom_objects=None,\n     printable_module_name='object'):\n   \"\"\"Returns the class name and config for a serialized keras object.\"\"\"\n-  if (not isinstance(config, dict)\n-      or 'class_name' not in config\n-      or 'config' not in config):\n+  if (not isinstance(config, dict) or 'class_name' not in config or\n+      'config' not in config):\n     raise ValueError('Improper config format: ' + str(config))\n \n   class_name = config['class_name']\n@@ -571,24 +341,7 @@ def deserialize_keras_object(identifier,\n                              module_objects=None,\n                              custom_objects=None,\n                              printable_module_name='object'):\n-  \"\"\"Turns the serialized form of a Keras object back into an actual object.\n-\n-  Calls to `deserialize_keras_object` while underneath the\n-  `SharedObjectLoadingScope` context manager will cause any already-seen shared\n-  objects to be returned as-is rather than creating a new object.\n-\n-  Args:\n-    identifier: the serialized form of the object.\n-    module_objects: A dictionary of custom objects to look the name up in.\n-      Generally, module_objects is provided by midlevel library implementers.\n-    custom_objects: A dictionary of custom objects to look the name up in.\n-      Generally, custom_objects is provided by the user.\n-    printable_module_name: A human-readable string representing the type of the\n-      object. Printed in case of exception.\n-\n-  Returns:\n-    The deserialized object.\n-  \"\"\"\n+  \"\"\"Turns the serialized form of a Keras object back into an actual object.\"\"\"\n   if identifier is None:\n     return None\n \n@@ -598,39 +351,25 @@ def deserialize_keras_object(identifier,\n     (cls, cls_config) = class_and_config_for_serialized_keras_object(\n         config, module_objects, custom_objects, printable_module_name)\n \n-    # If this object has already been loaded (i.e. it's shared between multiple\n-    # objects), return the already-loaded object.\n-    shared_object_id = config.get(SHARED_OBJECT_KEY)\n-    shared_object = _shared_object_loading_scope().get(shared_object_id)  # pylint: disable=assignment-from-none\n-    if shared_object is not None:\n-      return shared_object\n-\n     if hasattr(cls, 'from_config'):\n       arg_spec = tf_inspect.getfullargspec(cls.from_config)\n       custom_objects = custom_objects or {}\n \n       if 'custom_objects' in arg_spec.args:\n-        deserialized_obj = cls.from_config(\n+        return cls.from_config(\n             cls_config,\n             custom_objects=dict(\n                 list(_GLOBAL_CUSTOM_OBJECTS.items()) +\n                 list(custom_objects.items())))\n-      else:\n       with CustomObjectScope(custom_objects):\n-          deserialized_obj = cls.from_config(cls_config)\n+        return cls.from_config(cls_config)\n     else:\n       # Then `cls` may be a function returning a class.\n       # in this case by convention `config` holds\n       # the kwargs of the function.\n       custom_objects = custom_objects or {}\n       with CustomObjectScope(custom_objects):\n-        deserialized_obj = cls(**cls_config)\n-\n-    # Add object to shared objects, in case we find it referenced again.\n-    _shared_object_loading_scope().set(shared_object_id, deserialized_obj)\n-\n-    return deserialized_obj\n-\n+        return cls(**cls_config)\n   elif isinstance(identifier, six.string_types):\n     object_name = identifier\n     if custom_objects and object_name in custom_objects:\n\n@@ -25,7 +25,6 @@ from functools import partial\n import numpy as np\n \n import keras\n-from keras.utils import generic_utils\n \n \n class HasArgTest(tf.test.TestCase):\n@@ -386,63 +385,5 @@ class SliceArraysTest(tf.test.TestCase):\n         [None, None, None])\n \n \n-# object() alone isn't compatible with WeakKeyDictionary, which we use to\n-# track shared configs.\n-class MaybeSharedObject(object):\n-  pass\n-\n-\n-class SharedObjectScopeTest(tf.test.TestCase):\n-\n-  def test_shared_object_saving_scope_single_object_doesnt_export_id(self):\n-    with generic_utils.SharedObjectSavingScope() as scope:\n-      single_object = MaybeSharedObject()\n-      self.assertIsNone(scope.get_config(single_object))\n-      single_object_config = scope.create_config({}, single_object)\n-      self.assertIsNotNone(single_object_config)\n-      self.assertNotIn(generic_utils.SHARED_OBJECT_KEY,\n-                       single_object_config)\n-\n-  def test_shared_object_saving_scope_shared_object_exports_id(self):\n-    with generic_utils.SharedObjectSavingScope() as scope:\n-      shared_object = MaybeSharedObject()\n-      self.assertIsNone(scope.get_config(shared_object))\n-      scope.create_config({}, shared_object)\n-      first_object_config = scope.get_config(shared_object)\n-      second_object_config = scope.get_config(shared_object)\n-      self.assertIn(generic_utils.SHARED_OBJECT_KEY,\n-                    first_object_config)\n-      self.assertIn(generic_utils.SHARED_OBJECT_KEY,\n-                    second_object_config)\n-      self.assertIs(first_object_config, second_object_config)\n-\n-  def test_shared_object_loading_scope_noop(self):\n-    # Test that, without a context manager scope, adding configs will do\n-    # nothing.\n-    obj_id = 1\n-    obj = MaybeSharedObject()\n-    generic_utils._shared_object_loading_scope().set(obj_id, obj)\n-    self.assertIsNone(generic_utils._shared_object_loading_scope().get(obj_id))\n-\n-  def test_shared_object_loading_scope_returns_shared_obj(self):\n-    obj_id = 1\n-    obj = MaybeSharedObject()\n-    with generic_utils.SharedObjectLoadingScope() as scope:\n-      scope.set(obj_id, obj)\n-      self.assertIs(scope.get(obj_id), obj)\n-\n-  def test_nested_shared_object_saving_scopes(self):\n-    my_obj = MaybeSharedObject()\n-    with generic_utils.SharedObjectSavingScope() as scope_1:\n-      scope_1.create_config({}, my_obj)\n-      with generic_utils.SharedObjectSavingScope() as scope_2:\n-        # Nesting saving scopes should return the original scope and should\n-        # not clear any objects we're tracking.\n-        self.assertIs(scope_1, scope_2)\n-        self.assertIsNotNone(scope_2.get_config(my_obj))\n-      self.assertIsNotNone(scope_1.get_config(my_obj))\n-    self.assertIsNone(generic_utils._shared_object_saving_scope())\n-\n-\n if __name__ == '__main__':\n   tf.test.main()\n\n@@ -29,7 +29,6 @@ from keras import backend\n from keras.utils import losses_utils\n from keras.utils import tf_utils\n from keras.utils.generic_utils import to_list\n-from tensorflow.python.ops import control_flow_ops\n \n NEG_INF = -1e10\n \n@@ -508,16 +507,14 @@ def ragged_assert_compatible_and_get_flat_values(values, mask=None):\n     if isinstance(mask, tf.RaggedTensor):\n       assertion_list_for_mask = _assert_splits_match(\n           [nested_row_split_list[0], mask.nested_row_splits])\n-      tmp = control_flow_ops.with_dependencies(assertion_list_for_mask,\n-                                               mask.flat_values)\n-      mask = tf.compat.v1.expand_dims(tmp, -1)\n+      with tf.control_dependencies(assertion_list_for_mask):\n+        mask = tf.compat.v1.expand_dims(mask.flat_values, -1)\n \n     # values has at least 1 element.\n     flat_values = []\n     for value in values:\n-      tmp = control_flow_ops.with_dependencies(assertion_list,\n-                                               value.flat_values)\n-      flat_values.append(tf.compat.v1.expand_dims(tmp, -1))\n+      with tf.control_dependencies(assertion_list):\n+        flat_values.append(tf.compat.v1.expand_dims(value.flat_values, -1))\n \n     values = flat_values[0] if to_be_stripped else flat_values\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#56e940e9634764e15a211e5642b3e9a838d075ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 331 | Lines Deleted: 41 | Files Changed: 13 | Hunks: 33 | Methods Changed: 26 | Complexity Δ (Sum/Max): 18/6 | Churn Δ: 372 | Churn Cumulative: 35600 | Contributors (this commit): 142 | Commits (past 90d): 44 | Contributors (cumulative): 180 | DMM Complexity: 0.9279279279279279\n\nDIFF:\n@@ -1491,6 +1491,19 @@ def dtype(x):\n   return x.dtype.base_dtype.name\n \n \n+@doc_controls.do_not_generate_docs\n+def dtype_numpy(x):\n+  \"\"\"Returns the numpy dtype of a Keras tensor or variable.\n+\n+  Args:\n+      x: Tensor or variable.\n+\n+  Returns:\n+      numpy.dtype, dtype of `x`.\n+  \"\"\"\n+  return tf.as_dtype(x.dtype).as_numpy_dtype\n+\n+\n @keras_export('keras.backend.eval')\n @doc_controls.do_not_generate_docs\n def eval(x):\n@@ -3774,7 +3787,7 @@ def batch_set_value(tuples):\n   \"\"\"\n   if tf.compat.v1.executing_eagerly_outside_functions():\n     for x, value in tuples:\n-      x.assign(np.asarray(value, dtype=dtype(x)))\n+      x.assign(np.asarray(value, dtype=dtype_numpy(x)))\n   else:\n     with get_graph().as_default():\n       if tuples:\n\n@@ -29,6 +29,7 @@ import io\n import json\n import os\n import re\n+import sys\n import time\n \n import numpy as np\n@@ -44,7 +45,6 @@ from keras.utils.data_utils import Sequence\n from keras.utils.generic_utils import Progbar\n from keras.utils.io_utils import path_to_string\n from keras.utils.mode_keys import ModeKeys\n-from tensorflow.python.ops import summary_ops_v2\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n from tensorflow.tools.docs import doc_controls\n@@ -2255,35 +2255,24 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n     if self.update_freq == 'epoch':\n       return\n \n-    summary_state = summary_ops_v2._summary_state  # pylint: disable=protected-access\n-    self._prev_summary_state.append({\n-        'is_recording': summary_state.is_recording,\n-        'writer': summary_state.writer,\n-        'step': summary_state.step\n-    })\n-\n-    if self.update_freq == 'epoch':\n-      should_record = False\n-      writer = None\n-    else:\n     should_record = lambda: tf.equal(step % self.update_freq, 0)\n-\n-    summary_state.is_recording = should_record\n-    summary_state.writer = writer\n     # TODO(b/151339474): Fix deadlock when not using .value() here.\n-    tf.summary.experimental.set_step(step.value())\n+    summary_context = (writer.as_default(step.value()),\n+                       tf.summary.record_if(should_record))\n+    self._prev_summary_state.append(summary_context)\n+    summary_context[0].__enter__()\n+    summary_context[1].__enter__()\n \n   def _pop_writer(self):\n     \"\"\"Pops the current writer.\"\"\"\n     if self.update_freq == 'epoch':\n       return\n \n-    prev_state = self._prev_summary_state.pop()\n-\n-    summary_state = summary_ops_v2._summary_state  # pylint: disable=protected-access\n-    summary_state.is_recording = prev_state['is_recording']\n-    summary_state.writer = prev_state['writer']\n-    tf.summary.experimental.set_step(prev_state['step'])\n+    # See _push_writer for the content of the previous_context, which is pair\n+    # of context.\n+    previous_context = self._prev_summary_state.pop()\n+    previous_context[1].__exit__(*sys.exc_info())\n+    previous_context[0].__exit__(*sys.exc_info())\n \n   def _close_writers(self):\n     for writer in self._writers.values():\n\n@@ -205,14 +205,16 @@ class SidecarEvaluator(object):\n       # TODO(rchao): Support arbitrary callback for extensibility.\n       self.model.evaluate(self.data, steps=self.steps)\n \n-      logging.info('End of evaluation. Accuracy: %r', [\n-          metric.result().numpy()\n-          for metric in self.model.compiled_metrics.metrics\n-      ])\n+      logging.info(\n+          'End of evaluation. Metrics: %s', ' '.join([\n+              '{}={}'.format(metric.name,\n+                             metric.result().numpy())\n+              for metric in self.model.metrics\n+          ]))\n \n       if self._summary_writer:\n         with tf.summary.record_if(True), self._summary_writer.as_default():\n-          for metric in self.model.compiled_metrics.metrics:\n+          for metric in self.model.metrics:\n             tf.summary.scalar(\n                 metric.name,\n                 metric.result(),\n\n@@ -53,12 +53,14 @@ class SidecarEvaluatorTest(tf.test.TestCase):\n \n     # Asserts the content of the summary file.\n     event_pb_written = False\n+    event_tags = []\n     for event_pb in tf.compat.v1.train.summary_iterator(\n         os.path.join(log_dir, summary_files[0])):\n       if event_pb.step > 0:\n         self.assertEqual(event_pb.step, 32)\n-        self.assertEqual(event_pb.summary.value[0].tag, 'categorical_accuracy')\n+        event_tags.append(event_pb.summary.value[0].tag)\n         event_pb_written = True\n+    self.assertCountEqual(event_tags, ['categorical_accuracy', 'loss'])\n \n     # Verifying at least one non-zeroth step is written to summary.\n     self.assertTrue(event_pb_written)\n\n@@ -34,8 +34,14 @@ NUM_STEPS_PER_EPOCH = 50\n \n \n def _is_chief(task_type, task_id):\n-  return task_type is None or task_type == 'chief' or (task_type == 'worker' and\n-                                                       task_id == 0)\n+  # Note: there are two possible `TF_CONFIG` configuration.\n+  #   1) In addition to `worker` tasks, a `chief` task type is use;\n+  #      in this case, this function should be modified to\n+  #      `return task_type == 'chief'`.\n+  #   2) Only `worker` task type is used; in this case, worker 0 is\n+  #      regarded as the chief. The implementation demonstrated here\n+  #      is for this case.\n+  return task_type == 'worker' and task_id == 0\n \n \n def _get_temp_dir(dirpath, task_id):\n\n@@ -238,6 +238,8 @@ class Conv(Layer):\n     self.built = True\n \n   def call(self, inputs):\n+    input_shape = inputs.shape\n+\n     if self._is_causal:  # Apply causal padding to inputs for Conv1D.\n       inputs = tf.compat.v1.pad(inputs, self._compute_causal_padding(inputs))\n \n@@ -262,6 +264,11 @@ class Conv(Layer):\n           outputs = tf.nn.bias_add(\n               outputs, self.bias, data_format=self._tf_data_format)\n \n+    if not tf.executing_eagerly():\n+      # Infer the static output shape:\n+      out_shape = self.compute_output_shape(input_shape)\n+      outputs.set_shape(out_shape)\n+\n     if self.activation is not None:\n       return self.activation(outputs)\n     return outputs\n\n@@ -169,24 +169,36 @@ class Conv1DTest(keras_parameterized.TestCase):\n @keras_parameterized.run_all_keras_modes\n class Conv2DTest(keras_parameterized.TestCase):\n \n-  def _run_test(self, kwargs, expected_output_shape):\n+  def _run_test(self, kwargs, expected_output_shape, spatial_shape=(7, 6)):\n     num_samples = 2\n     stack_size = 3\n-    num_row = 7\n-    num_col = 6\n+    num_row, num_col = spatial_shape\n+    input_data = None\n+    # Generate valid input data.\n+    if None in spatial_shape:\n+      input_data_shape = (num_samples, num_row or 7, num_col or 6, stack_size)\n+      input_data = 10 * np.random.random(input_data_shape).astype(np.float32)\n \n     with self.cached_session(use_gpu=True):\n       testing_utils.layer_test(\n           keras.layers.Conv2D,\n           kwargs=kwargs,\n           input_shape=(num_samples, num_row, num_col, stack_size),\n+          input_data=input_data,\n           expected_output_shape=expected_output_shape)\n \n-  def _run_test_extra_batch_dim(self, kwargs, expected_output_shape):\n+  def _run_test_extra_batch_dim(self,\n+                                kwargs,\n+                                expected_output_shape,\n+                                spatial_shape=(7, 6)):\n     batch_shape = (2, 11)\n     stack_size = 3\n-    num_row = 7\n-    num_col = 6\n+    num_row, num_col = spatial_shape\n+    input_data = None\n+    # Generate valid input data.\n+    if None in spatial_shape:\n+      input_data_shape = batch_shape + (num_row or 7, num_col or 6, stack_size)\n+      input_data = 10 * np.random.random(input_data_shape).astype(np.float32)\n \n     with self.cached_session(use_gpu=True):\n       if expected_output_shape is not None:\n@@ -195,6 +207,7 @@ class Conv2DTest(keras_parameterized.TestCase):\n           keras.layers.Conv2D,\n           kwargs=kwargs,\n           input_shape=batch_shape + (num_row, num_col, stack_size),\n+          input_data=input_data,\n           expected_output_shape=expected_output_shape)\n \n   @parameterized.named_parameters(\n@@ -225,13 +238,24 @@ class Conv2DTest(keras_parameterized.TestCase):\n           'groups': 3,\n           'filters': 6\n       }, (None, 5, 4, 6), True),\n+      ('dilation_2_unknown_width', {\n+          'dilation_rate': (2, 2)\n+      }, (None, None, 2, 2), False, (None, 6)),\n+      ('dilation_2_unknown_height', {\n+          'dilation_rate': (2, 2)\n+      }, (None, 3, None, 2), False, (7, None)),\n   )\n-  def test_conv2d(self, kwargs, expected_output_shape=None, requires_gpu=False):\n+  def test_conv2d(self,\n+                  kwargs,\n+                  expected_output_shape=None,\n+                  requires_gpu=False,\n+                  spatial_shape=(7, 6)):\n     kwargs['filters'] = kwargs.get('filters', 2)\n     kwargs['kernel_size'] = (3, 3)\n     if not requires_gpu or tf.test.is_gpu_available(cuda_only=True):\n-      self._run_test(kwargs, expected_output_shape)\n-      self._run_test_extra_batch_dim(kwargs, expected_output_shape)\n+      self._run_test(kwargs, expected_output_shape, spatial_shape)\n+      self._run_test_extra_batch_dim(kwargs, expected_output_shape,\n+                                     spatial_shape)\n \n   def test_conv2d_regularizers(self):\n     kwargs = {\n\n@@ -28,6 +28,7 @@ from __future__ import print_function\n import tensorflow as tf\n \n import collections.abc as collections_abc\n+import enum\n import json\n import numpy as np\n import wrapt\n@@ -139,4 +140,7 @@ def get_json_type(obj):\n                        'class {} has not been registered.'\n                        .format(obj, type(obj)))\n \n+  if isinstance(obj, enum.Enum):\n+    return obj.value\n+\n   raise TypeError('Not JSON Serializable:', obj)\n\n@@ -20,6 +20,8 @@ from __future__ import division\n from __future__ import print_function\n \n import tensorflow as tf\n+\n+import enum\n from keras.saving.saved_model import json_utils\n \n \n@@ -61,6 +63,14 @@ class JsonUtilsTest(tf.test.TestCase):\n     with self.assertRaisesRegexp(ValueError, 'No TypeSpec has been registered'):\n       loaded = json_utils.decode(string)\n \n+  def test_encode_decode_enum(self):\n+    class Enum(enum.Enum):\n+      CLASS_A = 'a'\n+      CLASS_B = 'b'\n+    config = {'key': Enum.CLASS_A, 'key2': Enum.CLASS_B}\n+    string = json_utils.Encoder().encode(config)\n+    loaded = json_utils.decode(string)\n+    self.assertAllEqual({'key': 'a', 'key2': 'b'}, loaded)\n \n if __name__ == '__main__':\n   tf.test.main()\n\n@@ -126,7 +126,7 @@ def load(path, compile=True, options=None):  # pylint: disable=redefined-builtin\n       raise IOError('Cannot parse keras metadata {}: {}.'\n                     .format(path_to_metadata_pb, str(e)))\n   else:\n-    logging.warning('SavedModel saved prior to TF 2.4 detected when loading '\n+    logging.warning('SavedModel saved prior to TF 2.5 detected when loading '\n                     'Keras model. Please ensure that you are saving the model '\n                     'with model.save() or tf.keras.models.save_model(), *NOT* '\n                     'tf.saved_model.save(). To confirm, there should be a file '\n\n@@ -0,0 +1,68 @@\n+# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+# pylint: disable=g-classes-have-attributes\n+\"\"\"Input dataset creator for `model.fit`.\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import tensorflow as tf\n+\n+\n+class DatasetCreator(object):\n+  \"\"\"Object that returns a `tf.data.Dataset` upon invoking.\n+\n+  `DatasetCreator` is designated as a supported type for `x`, or the input, in\n+  `tf.keras.Model.fit`. Pass an instance of this class to `fit` when using a\n+  callable (with a `input_context` argument) that returns a `tf.data.Dataset`.\n+\n+  ```python\n+  model = tf.keras.Sequential([tf.keras.layers.Dense(10)])\n+  model.compile(tf.keras.optimizers.SGD(), loss=\"mse\")\n+\n+  def dataset_fn(input_context):\n+    global_batch_size = 64\n+    batch_size = input_context.get_per_replica_batch_size(global_batch_size)\n+    dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat()\n+    dataset = dataset.shard(\n+        input_context.num_input_pipelines, input_context.input_pipeline_id)\n+    dataset = dataset.batch(batch_size)\n+    dataset = dataset.prefetch(2)\n+    return dataset\n+\n+  model.fit(DatasetCreator(dataset_fn), epochs=10, steps_per_epoch=10)\n+  ```\n+\n+  Args:\n+    dataset_fn: A callable that takes a single argument of type\n+      `tf.distribute.InputContext`, which is used for batch size calculation and\n+      cross-worker input pipeline sharding (if neither is needed, the\n+      `InputContext` parameter can be ignored in the `dataset_fn`), and returns\n+      a `tf.data.Dataset`.\n+  \"\"\"\n+\n+  def __init__(self, dataset_fn):\n+    if not callable(dataset_fn):\n+      raise TypeError('`dataset_fn` for `DatasetCreator` must be a `callable`.')\n+    self.dataset_fn = dataset_fn\n+\n+  def __call__(self, *args, **kwargs):\n+    # When a `DatasetCreator` is invoked, it forwards args/kwargs straight to\n+    # the callable.\n+    dataset = self.dataset_fn(*args, **kwargs)\n+    if not isinstance(dataset, tf.data.Dataset):\n+      raise TypeError('The `callable` provided to `DatasetCreator` must return '\n+                      'a Dataset.')\n+    return dataset\n\n@@ -0,0 +1,84 @@\n+# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for dataset_creator.\"\"\"\n+\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import tensorflow as tf\n+from tensorflow.python.distribute import multi_worker_test_base\n+from tensorflow.python.distribute.cluster_resolver import SimpleClusterResolver\n+from keras.engine import sequential\n+from keras.layers import core as core_layers\n+from keras.optimizer_v2 import gradient_descent\n+from keras.utils import dataset_creator\n+from tensorflow.python.training.server_lib import ClusterSpec\n+\n+\n+class DatasetCreatorTest(tf.test.TestCase):\n+\n+  def test_dataset_creator(self):\n+    with self.assertRaisesRegex(\n+        TypeError, \"`dataset_fn` for `DatasetCreator` must be a `callable`.\"):\n+      dataset_creator.DatasetCreator(2)\n+\n+    dataset_fn = lambda: 3\n+    with self.assertRaisesRegex(\n+        TypeError, \"The `callable` provided to `DatasetCreator` must return \"\n+        \"a Dataset.\"):\n+      dataset_creator.DatasetCreator(dataset_fn)()\n+\n+    dataset_fn = lambda: tf.data.Dataset.from_tensor_slices([1, 1])\n+    got = dataset_creator.DatasetCreator(dataset_fn)()\n+    self.assertEqual(\n+        next(iter(got)),\n+        next(iter(tf.data.Dataset.from_tensor_slices([1, 1]))))\n+\n+  def test_dataset_creator_usage_in_parameter_server_model_fit(self):\n+    self.skipTest(\"TODO(rchao): Enable this test once training API changes for \"\n+                  \"DatasetFactory is submitted.\")\n+    cluster_def = multi_worker_test_base.create_in_process_cluster(\n+        num_workers=2, num_ps=1, rpc_layer=\"grpc\")\n+    cluster_def[\"chief\"] = [\n+        \"localhost:%d\" % multi_worker_test_base.pick_unused_port()\n+    ]\n+    strategy = tf.distribute.experimental.ParameterServerStrategy(\n+        SimpleClusterResolver(ClusterSpec(cluster_def), rpc_layer=\"grpc\"))\n+    with strategy.scope():\n+      model = sequential.Sequential([core_layers.Dense(10)])\n+    model.compile(gradient_descent.SGD(), loss=\"mse\")\n+\n+    def dataset_fn(input_context):\n+      global_batch_size = 64\n+      batch_size = input_context.get_per_replica_batch_size(global_batch_size)\n+      dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat()\n+      dataset = dataset.shard(input_context.num_input_pipelines,\n+                              input_context.input_pipeline_id)\n+      dataset = dataset.batch(batch_size)\n+      dataset = dataset.prefetch(2)\n+      return dataset\n+\n+    history = model.fit(\n+        dataset_creator.DatasetCreator(dataset_fn),\n+        epochs=10,\n+        steps_per_epoch=10,\n+        verbose=0)\n+    self.assertLen(history.history[\"loss\"], 10)\n+\n+\n+if __name__ == \"__main__\":\n+  tf.compat.v1.enable_v2_behavior()\n+  tf.test.main()\n\n@@ -0,0 +1,81 @@\n+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for losses_utils.\"\"\"\n+\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import tensorflow as tf\n+from keras import combinations\n+from keras.utils import losses_utils\n+\n+\n+@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+class RemoveSqueezableTest(tf.test.TestCase):\n+  \"\"\"Test remove_squeezable_dimensions\"\"\"\n+\n+  def test_ragged_3d_same_shape(self):\n+    \"\"\" shape (2, (sequence={1, 2}), 3)\"\"\"\n+    x = tf.ragged.constant([[[1, 2, 3]], [[4, 5, 6], [7, 8, 9]]])\n+    rank = x.shape.ndims\n+    x_p, _ = losses_utils.remove_squeezable_dimensions(x, x)\n+    self.assertEqual(x_p.shape.ndims, rank)\n+\n+  def test_ragged_3d_4d_squeezable(self):\n+    \"\"\" shapes:\n+\n+        x: (2, (sequence={1, 2}), 3)\n+        y: (2, (sequence={1, 2}), 3, 1)\n+    \"\"\"\n+    x = tf.ragged.constant([[[1, 2, 3]], [[4, 5, 6], [7, 8, 9]]])\n+    y = tf.compat.v1.expand_dims(x, axis=-1)\n+    self.assertEqual(x.shape.ndims, 3)\n+    self.assertEqual(y.shape.ndims, 4)\n+    _, y_p = losses_utils.remove_squeezable_dimensions(x, y)\n+    y_p.shape.assert_is_compatible_with(x.shape)\n+    self.assertEqual(y_p.shape.ndims, 3)\n+\n+    x_p, _ = losses_utils.remove_squeezable_dimensions(y, x)\n+    x_p.shape.assert_is_compatible_with(x.shape)\n+    self.assertEqual(x_p.shape.ndims, 3)\n+\n+  def test_dense_2d_3d_squeezable(self):\n+    x = tf.constant([[1, 2], [3, 4]])\n+    y = tf.constant([[[1], [2]], [[3], [4]]])\n+    _, y_p = losses_utils.remove_squeezable_dimensions(x, y)\n+    y_p.shape.assert_is_compatible_with(x.shape)\n+    self.assertEqual(y_p.shape.ndims, x.shape.ndims)\n+    x_p, _ = losses_utils.remove_squeezable_dimensions(y, x)\n+    x_p.shape.assert_is_compatible_with(x.shape)\n+\n+\n+class RemoveSqueezableTestGraphOnly(tf.test.TestCase):\n+  \"\"\"Test remove_squeezable_dimensions (graph-mode only).\"\"\"\n+\n+  def test_placeholder(self):\n+    \"\"\"Test dynamic rank tensors.\"\"\"\n+    with tf.Graph().as_default():\n+      x = tf.compat.v1.placeholder_with_default([1., 2., 3.], shape=None)\n+      y = tf.compat.v1.placeholder_with_default([[1.], [2.], [3.]], shape=None)\n+      _, y_p = losses_utils.remove_squeezable_dimensions(x, y)\n+      y_p.shape.assert_is_compatible_with(x.shape)\n+      self.assertAllEqual(tf.compat.v1.shape(x), tf.compat.v1.shape(y_p))\n+      x_p, _ = losses_utils.remove_squeezable_dimensions(y, x)\n+      x_p.shape.assert_is_compatible_with(x.shape)\n+\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
