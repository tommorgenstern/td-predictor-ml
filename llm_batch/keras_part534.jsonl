{"custom_id": "keras#8fd9ad368b0c2e1bd16995c2b7d68fa58f974873", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 12/6 | Churn Δ: 12 | Churn Cumulative: 396 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -41,10 +41,10 @@ class SpatialDropout2D(Dropout):\n       rate: Float between 0 and 1. Fraction of the input units to drop.\n       data_format: 'channels_first' or 'channels_last'. In 'channels_first'\n         mode, the channels dimension (the depth) is at index 1, in\n-        'channels_last' mode is it at index 3. It defaults to the\n+        'channels_last' mode is it at index 3. When unspecified, uses\n         `image_data_format` value found in your Keras config file at\n-        `~/.keras/keras.json`. If you never set it, then it will be\n-        \"channels_last\".\n+        `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n     Call arguments:\n       inputs: A 4D tensor.\n       training: Python boolean indicating whether the layer should behave in\n\n@@ -41,10 +41,10 @@ class SpatialDropout3D(Dropout):\n       rate: Float between 0 and 1. Fraction of the input units to drop.\n       data_format: 'channels_first' or 'channels_last'. In 'channels_first'\n         mode, the channels dimension (the depth) is at index 1, in\n-        'channels_last' mode is it at index 4. It defaults to the\n+        'channels_last' mode is it at index 4. When unspecified, uses\n         `image_data_format` value found in your Keras config file at\n-        `~/.keras/keras.json`. If you never set it, then it will be\n-        \"channels_last\".\n+        `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n     Call arguments:\n       inputs: A 5D tensor.\n       training: Python boolean indicating whether the layer should behave in\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6937ac764c731ebd0e1851d3b50df673378776bb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 21 | Files Changed: 7 | Hunks: 7 | Methods Changed: 0 | Complexity Δ (Sum/Max): 119/30 | Churn Δ: 49 | Churn Cumulative: 3098 | Contributors (this commit): 9 | Commits (past 90d): 15 | Contributors (cumulative): 45 | DMM Complexity: None\n\nDIFF:\n@@ -57,9 +57,10 @@ class Cropping2D(Layer):\n         `(batch_size, height, width, channels)` while `channels_first`\n         corresponds to inputs with shape\n         `(batch_size, channels, height, width)`.\n-        It defaults to the `image_data_format` value found in your\n-        Keras config file at `~/.keras/keras.json`.\n-        If you never set it, then it will be \"channels_last\".\n+        When unspecified, uses\n+        `image_data_format` value found in your Keras config file at\n+         `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n \n     Input shape:\n       4D tensor with shape:\n\n@@ -54,9 +54,10 @@ class Cropping3D(Layer):\n         `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n         while `channels_first` corresponds to inputs with shape\n         `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n-        It defaults to the `image_data_format` value found in your\n-        Keras config file at `~/.keras/keras.json`.\n-        If you never set it, then it will be \"channels_last\".\n+        When unspecified, uses\n+        `image_data_format` value found in your Keras config file at\n+         `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n \n     Input shape:\n       5D tensor with shape:\n\n@@ -43,9 +43,10 @@ class Flatten(Layer):\n         `channels_last` corresponds to inputs with shape\n         `(batch, ..., channels)` while `channels_first` corresponds to\n         inputs with shape `(batch, channels, ...)`.\n-        It defaults to the `image_data_format` value found in your\n-        Keras config file at `~/.keras/keras.json`.\n-        If you never set it, then it will be \"channels_last\".\n+        When unspecified, uses\n+        `image_data_format` value found in your Keras config file at\n+         `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n \n     Example:\n \n\n@@ -64,9 +64,10 @@ class UpSampling2D(Layer):\n         `(batch_size, height, width, channels)` while `channels_first`\n         corresponds to inputs with shape\n         `(batch_size, channels, height, width)`.\n-        It defaults to the `image_data_format` value found in your\n-        Keras config file at `~/.keras/keras.json`.\n-        If you never set it, then it will be \"channels_last\".\n+        When unspecified, uses\n+        `image_data_format` value found in your Keras config file at\n+         `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n       interpolation: A string, one of `\"area\"`, `\"bicubic\"`, `\"bilinear\"`,\n         `\"gaussian\"`, `\"lanczos3\"`, `\"lanczos5\"`, `\"mitchellcubic\"`,\n         `\"nearest\"`.\n\n@@ -51,9 +51,10 @@ class UpSampling3D(Layer):\n         `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n         while `channels_first` corresponds to inputs with shape\n         `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n-        It defaults to the `image_data_format` value found in your\n-        Keras config file at `~/.keras/keras.json`.\n-        If you never set it, then it will be \"channels_last\".\n+        When unspecified, uses\n+        `image_data_format` value found in your Keras config file at\n+         `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n \n     Input shape:\n       5D tensor with shape:\n\n@@ -74,9 +74,10 @@ class ZeroPadding2D(Layer):\n         `(batch_size, height, width, channels)` while `channels_first`\n         corresponds to inputs with shape\n         `(batch_size, channels, height, width)`.\n-        It defaults to the `image_data_format` value found in your\n-        Keras config file at `~/.keras/keras.json`.\n-        If you never set it, then it will be \"channels_last\".\n+        When unspecified, uses\n+        `image_data_format` value found in your Keras config file at\n+         `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n \n     Input shape:\n       4D tensor with shape:\n\n@@ -57,9 +57,10 @@ class ZeroPadding3D(Layer):\n         `(batch_size, spatial_dim1, spatial_dim2, spatial_dim3, channels)`\n         while `channels_first` corresponds to inputs with shape\n         `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`.\n-        It defaults to the `image_data_format` value found in your\n-        Keras config file at `~/.keras/keras.json`.\n-        If you never set it, then it will be \"channels_last\".\n+        When unspecified, uses\n+        `image_data_format` value found in your Keras config file at\n+         `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n \n     Input shape:\n       5D tensor with shape:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#00a55ceb22e48d7ef5406acb11b4fc327c5f0ad2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 26 | Files Changed: 7 | Hunks: 14 | Methods Changed: 1 | Complexity Δ (Sum/Max): 415/165 | Churn Δ: 58 | Churn Cumulative: 12197 | Contributors (this commit): 17 | Commits (past 90d): 21 | Contributors (cumulative): 62 | DMM Complexity: None\n\nDIFF:\n@@ -45,9 +45,10 @@ class ConvLSTMCell(DropoutRNNCellMixin, base_layer.BaseRandomLayer):\n         up/down of the input such that output has the same height/width\n         dimension as the input.\n       data_format: A string, one of `channels_last` (default) or\n-        `channels_first`.  It defaults to the `image_data_format` value found in\n-        your Keras config file at `~/.keras/keras.json`. If you never set it,\n-        then it will be \"channels_last\".\n+        `channels_first`. When unspecified, uses\n+        `image_data_format` value found in your Keras config file at\n+        `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n       dilation_rate: An integer or tuple/list of n integers, specifying the\n         dilation rate to use for dilated convolution. Currently, specifying any\n         `dilation_rate` value != 1 is incompatible with specifying any `strides`\n@@ -383,9 +384,10 @@ class ConvLSTM(ConvRNN):\n         `(batch, time, ..., channels)`\n         while `channels_first` corresponds to\n         inputs with shape `(batch, time, channels, ...)`.\n-        It defaults to the `image_data_format` value found in your\n-        Keras config file at `~/.keras/keras.json`.\n-        If you never set it, then it will be \"channels_last\".\n+        When unspecified, uses\n+        `image_data_format` value found in your Keras config file at\n+         `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n       dilation_rate: An integer or tuple/list of n integers, specifying\n         the dilation rate to use for dilated convolution.\n         Currently, specifying any `dilation_rate` value != 1 is\n\n@@ -44,9 +44,10 @@ class ConvLSTM1D(ConvLSTM):\n         `channels_first`.  The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape `(batch, time, ...,\n         channels)` while `channels_first` corresponds to inputs with shape\n-        `(batch, time, channels, ...)`. It defaults to the `image_data_format`\n-        value found in your Keras config file at `~/.keras/keras.json`. If you\n-        never set it, then it will be \"channels_last\".\n+        `(batch, time, channels, ...)`. When unspecified, uses\n+        `image_data_format` value found in your Keras config file at\n+        `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n       dilation_rate: An integer or tuple/list of n integers, specifying the\n         dilation rate to use for dilated convolution. Currently, specifying any\n         `dilation_rate` value != 1 is incompatible with specifying any `strides`\n\n@@ -44,9 +44,10 @@ class ConvLSTM2D(ConvLSTM):\n         `channels_first`.  The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape `(batch, time, ...,\n         channels)` while `channels_first` corresponds to inputs with shape\n-        `(batch, time, channels, ...)`. It defaults to the `image_data_format`\n-        value found in your Keras config file at `~/.keras/keras.json`. If you\n-        never set it, then it will be \"channels_last\".\n+        `(batch, time, channels, ...)`. When unspecified, uses\n+        `image_data_format` value found in your Keras config file at\n+        `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n       dilation_rate: An integer or tuple/list of n integers, specifying the\n         dilation rate to use for dilated convolution. Currently, specifying any\n         `dilation_rate` value != 1 is incompatible with specifying any `strides`\n\n@@ -44,9 +44,10 @@ class ConvLSTM3D(ConvLSTM):\n         `channels_first`. The ordering of the dimensions in the inputs.\n         `channels_last` corresponds to inputs with shape `(batch, time, ...,\n         channels)` while `channels_first` corresponds to inputs with shape\n-        `(batch, time, channels, ...)`. It defaults to the `image_data_format`\n-        value found in your Keras config file at `~/.keras/keras.json`. If you\n-        never set it, then it will be \"channels_last\".\n+        `(batch, time, channels, ...)`. When unspecified, uses\n+        `image_data_format` value found in your Keras config file at\n+        `~/.keras/keras.json` (if exists) else 'channels_last'.\n+        Defaults to 'channels_last'.\n       dilation_rate: An integer or tuple/list of n integers, specifying the\n         dilation rate to use for dilated convolution. Currently, specifying any\n         `dilation_rate` value != 1 is incompatible with specifying any `strides`\n\n@@ -507,17 +507,17 @@ class GRU(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):\n     Call arguments:\n       inputs: A 3D tensor, with shape `[batch, timesteps, feature]`.\n       mask: Binary tensor of shape `[samples, timesteps]` indicating whether\n-        a given timestep should be masked  (optional, defaults to `None`).\n+        a given timestep should be masked  (optional).\n         An individual `True` entry indicates that the corresponding timestep\n         should be utilized, while a `False` entry indicates that the\n-        corresponding timestep should be ignored.\n+        corresponding timestep should be ignored. Defaults to `None`.\n       training: Python boolean indicating whether the layer should behave in\n         training mode or in inference mode. This argument is passed to the cell\n         when calling it. This is only relevant if `dropout` or\n-        `recurrent_dropout` is used  (optional, defaults to `None`).\n+        `recurrent_dropout` is used  (optional). Defaults to `None`.\n       initial_state: List of initial state tensors to be passed to the first\n-        call of the cell  (optional, defaults to `None` which causes creation\n-        of zero-filled initial state tensors).\n+        call of the cell  (optional, `None` causes creation\n+        of zero-filled initial state tensors). Defaults to `None`.\n     \"\"\"\n \n     def __init__(\n\n@@ -186,7 +186,8 @@ class RNNCell(base_layer.Layer):\n             `2-D Tensor` with shape `[batch_size, self.state_size]`. Otherwise,\n             if `self.state_size` is a tuple of integers, this should be a tuple\n             with shapes `[batch_size, s] for s in self.state_size`.\n-          scope: VariableScope for the created subgraph; defaults to class name.\n+          scope: VariableScope for the created subgraph; None uses class name.\n+            Defaults to `None`.\n \n         Returns:\n           A pair containing:\n\n@@ -480,17 +480,17 @@ class LSTM(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):\n     Call arguments:\n       inputs: A 3D tensor with shape `[batch, timesteps, feature]`.\n       mask: Binary tensor of shape `[batch, timesteps]` indicating whether\n-        a given timestep should be masked (optional, defaults to `None`).\n+        a given timestep should be masked (optional).\n         An individual `True` entry indicates that the corresponding timestep\n         should be utilized, while a `False` entry indicates that the\n-        corresponding timestep should be ignored.\n+        corresponding timestep should be ignored. Defaults to `None`.\n       training: Python boolean indicating whether the layer should behave in\n         training mode or in inference mode. This argument is passed to the cell\n         when calling it. This is only relevant if `dropout` or\n-        `recurrent_dropout` is used (optional, defaults to `None`).\n+        `recurrent_dropout` is used (optional). Defaults to `None`.\n       initial_state: List of initial state tensors to be passed to the first\n-        call of the cell (optional, defaults to `None` which causes creation\n-        of zero-filled initial state tensors).\n+        call of the cell (optional, `None` causes creation\n+        of zero-filled initial state tensors). Defaults to `None`.\n     \"\"\"\n \n     def __init__(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5331dac9283c92091517b04f6a42342d8e123722", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 11 | Files Changed: 3 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 169/83 | Churn Δ: 26 | Churn Cumulative: 5330 | Contributors (this commit): 13 | Commits (past 90d): 7 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -365,8 +365,8 @@ class Layer(base_layer.Layer):\n             or \"non_trainable_variables\" (e.g. BatchNorm mean, stddev).\n             Note, if the current variable scope is marked as non-trainable\n             then this parameter is ignored and any added variables are also\n-            marked as non-trainable. `trainable` defaults to `True` unless\n-            `synchronization` is set to `ON_READ`.\n+            marked as non-trainable. `trainable` becomes `True` unless\n+            `synchronization` is set to `ON_READ`. Defaults to `True`.\n           constraint: constraint instance (callable).\n           use_resource: Whether to use `ResourceVariable`.\n           synchronization: Indicates when a distributed a variable will be\n\n@@ -46,8 +46,11 @@ class DeterministicRandomTestTool(object):\n     \"\"\"\n \n     def __init__(self, seed: int = 42, mode=\"constant\"):\n-        \"\"\"Set mode to 'constant' or 'num_random_ops'. Defaults to\n-        'constant'.\"\"\"\n+        \"\"\"\n+        Args:\n+          mode: Set mode to 'constant' or 'num_random_ops'. Defaults to\n+        'constant'.\n+        \"\"\"\n         if mode not in {\"constant\", \"num_random_ops\"}:\n             raise ValueError(\n                 \"Mode arg must be 'constant' or 'num_random_ops'. \"\n\n@@ -215,7 +215,7 @@ class _EagerVariableStore(tf.Module):\n         Args:\n           name: The name of the new or existing variable.\n           shape: Shape of the new or existing variable.\n-          dtype: Type of the new or existing variable (defaults to `DT_FLOAT`).\n+          dtype: Type of the new or existing variable. Defaults to `DT_FLOAT`.\n           initializer: Initializer for the variable.\n           regularizer: A (Tensor -> Tensor or None) function; the result of\n             applying it on a newly created variable will be added to the\n@@ -226,16 +226,16 @@ class _EagerVariableStore(tf.Module):\n             always forced to be False.\n           trainable: If `True` also add the variable to the graph collection\n             `GraphKeys.TRAINABLE_VARIABLES` (see `tf.Variable`). `trainable`\n-            defaults to `True`, unless `synchronization` is set to `ON_READ`, in\n-            which case it defaults to `False`.\n+            becomes `True`, unless `synchronization` is set to `ON_READ`, in\n+            which case it becomes `False`. Defaults to `True`.\n           collections: List of graph collections keys to add the `Variable` to.\n             Defaults to `[GraphKeys.GLOBAL_VARIABLES]` (see `tf.Variable`).\n           caching_device: Optional device string or function describing where\n-            the Variable should be cached for reading.  Defaults to the\n+            the Variable should be cached for reading. `None` to use the\n             Variable's device.  If not `None`, caches on another device.\n             Typical use is to cache on the device where the Ops using the\n             `Variable` reside, to deduplicate copying through `Switch` and other\n-            conditional statements.\n+            conditional statements. Defaults to `None`.\n           partitioner: Optional callable that accepts a fully defined\n             `TensorShape` and dtype of the `Variable` to be created, and returns\n             a list of partitions for each axis (currently only one axis can be\n@@ -245,8 +245,9 @@ class _EagerVariableStore(tf.Module):\n             initial_value must be known.\n           use_resource: If False, creates a regular Variable. If True, creates\n             instead an experimental ResourceVariable which has well-defined\n-            semantics. Defaults to False (will later change to True). When eager\n-            execution is enabled this argument is always forced to be true.\n+            semantics. When starting off as False it will later change to True.\n+            When eager execution is enabled this argument always True.\n+            Defaults to `False`.\n           custom_getter: Callable that takes as a first argument the true\n             getter, and allows overwriting the internal get_variable method. The\n             signature of `custom_getter` should match that of this method, but\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b31965e9d8bb21b1dc9776c4f606844922c4d420", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 63 | Lines Deleted: 59 | Files Changed: 6 | Hunks: 50 | Methods Changed: 7 | Complexity Δ (Sum/Max): 285/103 | Churn Δ: 122 | Churn Cumulative: 10504 | Contributors (this commit): 13 | Commits (past 90d): 22 | Contributors (cumulative): 28 | DMM Complexity: None\n\nDIFF:\n@@ -261,7 +261,7 @@ class TopKCategoricalAccuracy(base_metric.MeanMetricWrapper):\n \n     Args:\n       k: (Optional) Number of top elements to look at for computing accuracy.\n-        Defaults to 5.\n+        Defaults to `5`.\n       name: (Optional) string name of the metric instance.\n       dtype: (Optional) data type of the metric result.\n \n@@ -307,7 +307,7 @@ class SparseTopKCategoricalAccuracy(base_metric.MeanMetricWrapper):\n \n     Args:\n       k: (Optional) Number of top elements to look at for computing accuracy.\n-        Defaults to 5.\n+        Defaults to `5`.\n       name: (Optional) string name of the metric instance.\n       dtype: (Optional) data type of the metric result.\n \n@@ -482,7 +482,7 @@ def top_k_categorical_accuracy(y_true, y_pred, k=5):\n       y_true: The ground truth values.\n       y_pred: The prediction values.\n       k: (Optional) Number of top elements to look at for computing accuracy.\n-        Defaults to 5.\n+        Defaults to `5`.\n \n     Returns:\n       Top K categorical accuracy value.\n@@ -514,7 +514,7 @@ def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n       y_true: tensor of true targets.\n       y_pred: tensor of predicted targets.\n       k: (Optional) Number of top elements to look at for computing accuracy.\n-        Defaults to 5.\n+        Defaults to `5`.\n \n     Returns:\n       Sparse top K categorical accuracy value.\n\n@@ -471,7 +471,7 @@ class Reduce(Metric):\n \n         Args:\n           values: Per-example value.\n-          sample_weight: Optional weighting of each example. Defaults to 1.\n+          sample_weight: Optional weighting of each example. Defaults to `1`.\n \n         Returns:\n           Update op.\n@@ -828,7 +828,7 @@ class MeanTensor(Metric):\n \n         Args:\n           values: Per-example value.\n-          sample_weight: Optional weighting of each example. Defaults to 1.\n+          sample_weight: Optional weighting of each example. Defaults to `1`.\n \n         Returns:\n           Update op.\n\n@@ -36,11 +36,11 @@ class _ConfusionMatrixConditionCount(base_metric.Metric):\n \n     Args:\n       confusion_matrix_cond: One of `metrics_utils.ConfusionMatrix` conditions.\n-      thresholds: (Optional) Defaults to 0.5. A float value or a python\n-        list/tuple of float threshold values in [0, 1]. A threshold is compared\n-        with prediction values to determine the truth value of predictions\n+      thresholds: (Optional) A float value or a python list/tuple of float\n+        threshold values in [0, 1]. A threshold is compared with prediction\n+        values to determine the truth value of predictions\n         (i.e., above the threshold is `true`, below is `false`). One metric\n-        value is generated for each threshold value.\n+        value is generated for each threshold value. Defaults to 0.5.\n       name: (Optional) string name of the metric instance.\n       dtype: (Optional) data type of the metric result.\n     \"\"\"\n@@ -67,9 +67,9 @@ class _ConfusionMatrixConditionCount(base_metric.Metric):\n         Args:\n           y_true: The ground truth values.\n           y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+          sample_weight: Optional weighting of each example. Can\n             be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n-            and must be broadcastable to `y_true`.\n+            and must be broadcastable to `y_true`. Defaults to `1`.\n \n         Returns:\n           Update op.\n@@ -113,13 +113,13 @@ class FalsePositives(_ConfusionMatrixConditionCount):\n     Use `sample_weight` of 0 to mask values.\n \n     Args:\n-      thresholds: (Optional) Defaults to 0.5. A float value, or a Python\n+      thresholds: (Optional) A float value, or a Python\n         list/tuple of float threshold values in [0, 1]. A threshold is compared\n         with prediction values to determine the truth value of predictions\n         (i.e., above the threshold is `true`, below is `false`). If used with a\n         loss function that sets `from_logits=True` (i.e. no sigmoid applied to\n         predictions), `thresholds` should be set to 0. One metric value is\n-        generated for each threshold value.\n+        generated for each threshold value. Defaults to `0.5`.\n       name: (Optional) string name of the metric instance.\n       dtype: (Optional) data type of the metric result.\n \n@@ -174,13 +174,13 @@ class FalseNegatives(_ConfusionMatrixConditionCount):\n     Use `sample_weight` of 0 to mask values.\n \n     Args:\n-      thresholds: (Optional) Defaults to 0.5. A float value, or a Python\n+      thresholds: (Optional) A float value, or a Python\n         list/tuple of float threshold values in [0, 1]. A threshold is compared\n         with prediction values to determine the truth value of predictions\n         (i.e., above the threshold is `true`, below is `false`). If used with a\n         loss function that sets `from_logits=True` (i.e. no sigmoid applied to\n         predictions), `thresholds` should be set to 0. One metric value is\n-        generated for each threshold value.\n+        generated for each threshold value. Defaults to `0.5`.\n       name: (Optional) string name of the metric instance.\n       dtype: (Optional) data type of the metric result.\n \n@@ -235,13 +235,13 @@ class TrueNegatives(_ConfusionMatrixConditionCount):\n     Use `sample_weight` of 0 to mask values.\n \n     Args:\n-      thresholds: (Optional) Defaults to 0.5. A float value, or a Python\n+      thresholds: (Optional) A float value, or a Python\n         list/tuple of float threshold values in [0, 1]. A threshold is compared\n         with prediction values to determine the truth value of predictions\n         (i.e., above the threshold is `true`, below is `false`). If used with a\n         loss function that sets `from_logits=True` (i.e. no sigmoid applied to\n         predictions), `thresholds` should be set to 0. One metric value is\n-        generated for each threshold value.\n+        generated for each threshold value. Defaults to `0.5`.\n       name: (Optional) string name of the metric instance.\n       dtype: (Optional) data type of the metric result.\n \n@@ -296,13 +296,13 @@ class TruePositives(_ConfusionMatrixConditionCount):\n     Use `sample_weight` of 0 to mask values.\n \n     Args:\n-      thresholds: (Optional) Defaults to 0.5. A float value, or a Python\n+      thresholds: (Optional) A float value, or a Python\n         list/tuple of float threshold values in [0, 1]. A threshold is compared\n         with prediction values to determine the truth value of predictions\n         (i.e., above the threshold is `true`, below is `false`). If used with a\n         loss function that sets `from_logits=True` (i.e. no sigmoid applied to\n         predictions), `thresholds` should be set to 0. One metric value is\n-        generated for each threshold value.\n+        generated for each threshold value. Defaults to `0.5`.\n       name: (Optional) string name of the metric instance.\n       dtype: (Optional) data type of the metric result.\n \n@@ -460,9 +460,9 @@ class Precision(base_metric.Metric):\n             Will be cast to `bool`.\n           y_pred: The predicted values. Each element must be in the range\n             `[0, 1]`.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+          sample_weight: Optional weighting of each example. Can\n             be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n-            and must be broadcastable to `y_true`.\n+            and must be broadcastable to `y_true`. Defaults to `1`.\n \n         Returns:\n           Update op.\n@@ -606,9 +606,9 @@ class Recall(base_metric.Metric):\n             Will be cast to `bool`.\n           y_pred: The predicted values. Each element must be in the range\n             `[0, 1]`.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+          sample_weight: Optional weighting of each example. Can\n             be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n-            and must be broadcastable to `y_true`.\n+            and must be broadcastable to `y_true`. Defaults to `1`.\n \n         Returns:\n           Update op.\n@@ -702,9 +702,9 @@ class SensitivitySpecificityBase(base_metric.Metric, metaclass=abc.ABCMeta):\n         Args:\n           y_true: The ground truth values.\n           y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+          sample_weight: Optional weighting of each example. Can\n             be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n-            and must be broadcastable to `y_true`.\n+            and must be broadcastable to `y_true`. Defaults to `1`.\n \n         Returns:\n           Update op.\n@@ -798,8 +798,8 @@ class SensitivityAtSpecificity(SensitivitySpecificityBase):\n \n     Args:\n       specificity: A scalar value in range `[0, 1]`.\n-      num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n-        use for matching the given specificity.\n+      num_thresholds: (Optional) The number of thresholds to\n+        use for matching the given specificity. Defaults to `200`.\n       class_id: (Optional) Integer class ID for which we want binary metrics.\n         This must be in the half-open interval `[0, num_classes)`, where\n         `num_classes` is the last dimension of predictions.\n@@ -903,8 +903,8 @@ class SpecificityAtSensitivity(SensitivitySpecificityBase):\n \n     Args:\n       sensitivity: A scalar value in range `[0, 1]`.\n-      num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n-        use for matching the given sensitivity.\n+      num_thresholds: (Optional) The number of thresholds to\n+        use for matching the given sensitivity. Defaults to `200`.\n       class_id: (Optional) Integer class ID for which we want binary metrics.\n         This must be in the half-open interval `[0, num_classes)`, where\n         `num_classes` is the last dimension of predictions.\n@@ -999,8 +999,8 @@ class PrecisionAtRecall(SensitivitySpecificityBase):\n \n     Args:\n       recall: A scalar value in range `[0, 1]`.\n-      num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n-        use for matching the given recall.\n+      num_thresholds: (Optional) The number of thresholds to\n+        use for matching the given recall. Defaults to `200`.\n       class_id: (Optional) Integer class ID for which we want binary metrics.\n         This must be in the half-open interval `[0, num_classes)`, where\n         `num_classes` is the last dimension of predictions.\n@@ -1090,8 +1090,8 @@ class RecallAtPrecision(SensitivitySpecificityBase):\n \n     Args:\n       precision: A scalar value in range `[0, 1]`.\n-      num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n-        use for matching the given precision.\n+      num_thresholds: (Optional) The number of thresholds to\n+        use for matching the given precision. Defaults to `200`.\n       class_id: (Optional) Integer class ID for which we want binary metrics.\n         This must be in the half-open interval `[0, num_classes)`, where\n         `num_classes` is the last dimension of predictions.\n@@ -1209,8 +1209,9 @@ class AUC(base_metric.Metric):\n     Use `sample_weight` of 0 to mask values.\n \n     Args:\n-      num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n+      num_thresholds: (Optional) The number of thresholds to\n         use when discretizing the roc curve. Values must be > 1.\n+        Defaults to `200`.\n       curve: (Optional) Specifies the name of the curve to be computed, 'ROC'\n         [default] or 'PR' for the Precision-Recall-curve.\n       summation_method: (Optional) Specifies the [Riemann summation method](\n@@ -1442,9 +1443,9 @@ class AUC(base_metric.Metric):\n         Args:\n           y_true: The ground truth values.\n           y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+          sample_weight: Optional weighting of each example. Can\n             be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n-            and must be broadcastable to `y_true`.\n+            and must be broadcastable to `y_true`. Defaults to `1`.\n \n         Returns:\n           Update op.\n\n@@ -67,7 +67,8 @@ class _IoUBase(base_metric.Metric):\n       sparse_y_pred: Whether predictions are encoded using integers or\n         dense floating point vectors. If `False`, the `tf.argmax` function\n         will be used to determine each sample's most likely associated label.\n-      axis: (Optional) Defaults to -1. The dimension containing the logits.\n+      axis: (Optional) -1 is the dimension containing the logits.\n+        Defaults to `-1`.\n     \"\"\"\n \n     def __init__(\n@@ -100,9 +101,9 @@ class _IoUBase(base_metric.Metric):\n         Args:\n           y_true: The ground truth values.\n           y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+          sample_weight: Optional weighting of each example. Can\n             be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n-            and must be broadcastable to `y_true`.\n+            and must be broadcastable to `y_true`. Defaults to `1`.\n \n         Returns:\n           Update op.\n@@ -197,7 +198,8 @@ class IoU(_IoUBase):\n       sparse_y_pred: Whether predictions are encoded using integers or\n         dense floating point vectors. If `False`, the `tf.argmax` function\n         will be used to determine each sample's most likely associated label.\n-      axis: (Optional) Defaults to -1. The dimension containing the logits.\n+      axis: (Optional) -1 is the dimension containing the logits.\n+        Defaults to `-1`.\n \n     Standalone usage:\n \n@@ -405,9 +407,9 @@ class BinaryIoU(IoU):\n         Args:\n           y_true: The ground truth values.\n           y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+          sample_weight: Optional weighting of each example. Can\n             be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n-            and must be broadcastable to `y_true`.\n+            and must be broadcastable to `y_true`. Defaults to `1`.\n \n         Returns:\n           Update op.\n@@ -465,7 +467,7 @@ class MeanIoU(IoU):\n       sparse_y_pred: Whether predictions are encoded using integers or\n         dense floating point vectors. If `False`, the `tf.argmax` function\n         will be used to determine each sample's most likely associated label.\n-      axis: (Optional) Defaults to -1. The dimension containing the logits.\n+      axis: (Optional) -1 dimension contains the logits. Defaults to `-1`.\n \n     Standalone usage:\n \n@@ -581,7 +583,7 @@ class OneHotIoU(IoU):\n       sparse_y_pred: Whether predictions are encoded using natural numbers or\n         probability distribution vectors. If `False`, the `tf.argmax` function\n         will be used to determine each sample's most likely associated label.\n-      axis: (Optional) Defaults to -1. The dimension containing the logits.\n+      axis: (Optional) -1 dimension contains the logits. Defaults to `-1`.\n \n     Standalone usage:\n \n@@ -695,7 +697,7 @@ class OneHotMeanIoU(MeanIoU):\n       sparse_y_pred: Whether predictions are encoded using natural numbers or\n         probability distribution vectors. If `False`, the `tf.argmax` function\n         will be used to determine each sample's most likely associated label.\n-      axis: (Optional) Defaults to -1. The dimension containing the logits.\n+      axis: (Optional) -1 dimension contains the logits. Defaults to `-1`.\n \n     Standalone usage:\n \n\n@@ -183,8 +183,8 @@ class CategoricalCrossentropy(base_metric.MeanMetricWrapper):\n         smoothed, meaning the confidence on label values are relaxed. e.g.\n         `label_smoothing=0.2` means that we will use a value of `0.1` for label\n         `0` and `0.9` for label `1`\"\n-      axis: (Optional) Defaults to -1. The dimension along which entropy is\n-        computed.\n+      axis: (Optional) -1 is the dimension along which entropy is\n+        computed. Defaults to `-1`.\n \n     Standalone usage:\n \n@@ -261,8 +261,8 @@ class SparseCategoricalCrossentropy(base_metric.MeanMetricWrapper):\n         metric computation. This is useful, for example, in segmentation\n         problems featuring a \"void\" class (commonly -1 or 255) in segmentation\n         maps. By default (`ignore_class=None`), all classes are considered.\n-      axis: (Optional) Defaults to -1. The dimension along which entropy is\n-        computed.\n+      axis: (Optional) The dimension along which entropy is\n+        computed. Defaults to `-1`.\n \n     Standalone usage:\n \n\n@@ -84,9 +84,9 @@ class MeanRelativeError(base_metric.Mean):\n         Args:\n           y_true: The ground truth values.\n           y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+          sample_weight: Optional weighting of each example. Can\n             be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n-            and must be broadcastable to `y_true`.\n+            and must be broadcastable to `y_true`. Defaults to `1`.\n \n         Returns:\n           Update op.\n@@ -138,8 +138,8 @@ class CosineSimilarity(base_metric.MeanMetricWrapper):\n     Args:\n       name: (Optional) string name of the metric instance.\n       dtype: (Optional) data type of the metric result.\n-      axis: (Optional) Defaults to -1. The dimension along which the cosine\n-        similarity is computed.\n+      axis: (Optional) The dimension along which the cosine\n+        similarity is computed. Defaults to `-1`.\n \n     Standalone usage:\n \n@@ -357,9 +357,9 @@ class RootMeanSquaredError(base_metric.Mean):\n         Args:\n           y_true: The ground truth values.\n           y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+          sample_weight: Optional weighting of each example. Can\n             be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n-            and must be broadcastable to `y_true`.\n+            and must be broadcastable to `y_true`. Defaults to `1`.\n \n         Returns:\n           Update op.\n@@ -443,7 +443,8 @@ class R2Score(base_metric.Metric):\n             `None` (no aggregation), `\"uniform_average\"`,\n             `\"variance_weighted_average\"`.\n         num_regressors: Number of independent regressors used\n-            (\"Adjusted R2\" score). Defaults to 0 (standard R2 score).\n+            (\"Adjusted R2\" score). 0 is the standard R2 score.\n+            Defaults to `0`.\n         name: Optional. string name of the metric instance.\n         dtype: Optional. data type of the metric result.\n \n@@ -614,8 +615,8 @@ def cosine_similarity(y_true, y_pred, axis=-1):\n     Args:\n       y_true: The ground truth values.\n       y_pred: The prediction values.\n-      axis: (Optional) Defaults to -1. The dimension along which the cosine\n-        similarity is computed.\n+      axis: (Optional) -1 is the dimension along which the cosine\n+        similarity is computed. Defaults to `-1`.\n \n     Returns:\n       Cosine similarity value.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b43b215af5d123ca907d22f8a9bfc27c03cba5e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 217/217 | Churn Δ: 16 | Churn Cumulative: 5510 | Contributors (this commit): 20 | Commits (past 90d): 6 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -406,14 +406,14 @@ class BaseLossScaleOptimizer(metaclass=LossScaleOptimizerMetaclass):\n     Args:\n       inner_optimizer: The `tf.keras.optimizers.Optimizer` or\n         `tf.keras.optimizers.experimental.Optimizer` instance to wrap.\n-      dynamic: Bool indicating whether dynamic loss scaling is used. Defaults to\n-        True. If True, the loss scale will be dynamically updated over time\n-        using an algorithm that keeps the loss scale at approximately its\n-        optimal value.  If False, a single fixed loss scale is used and\n-        `initial_scale` must be specified, which is used as the loss scale.\n+      dynamic: Bool indicating whether dynamic loss scaling is used.  If True,\n+        the loss scale will be dynamically updated over time using an algorithm\n+        that keeps the loss scale at approximately its optimal value. If False,\n+        a single fixed loss scale is used and  `initial_scale` must be\n+        specified, which is used as the loss scale.\n         Recommended to keep as True, as choosing a fixed loss scale can be\n         tricky. Currently, there is a small performance overhead to dynamic loss\n-        scaling compared to fixed loss scaling.\n+        scaling compared to fixed loss scaling. Defaults to `True`.\n       initial_scale: The initial loss scale. If `dynamic` is True, this defaults\n         to `2 ** 15`. If `dynamic` is False, this must be specified and acts as\n         the sole loss scale, as the loss scale does not change over time. When\n@@ -422,11 +422,11 @@ class BaseLossScaleOptimizer(metaclass=LossScaleOptimizerMetaclass):\n         quickly than a loss scale that is too low gets raised.\n       dynamic_growth_steps: With dynamic loss scaling, every\n         `dynamic_growth_steps` steps with finite gradients, the loss scale is\n-        doubled. Defaults to 2000. If a nonfinite gradient is encountered, the\n+        doubled. If a nonfinite gradient is encountered, the\n         count is reset back to zero, gradients are skipped that step, and the\n         loss scale is halved. The count can be queried with\n         `LossScaleOptimizer.dynamic_counter`. This argument can only be\n-        specified if `dynamic` is True.\n+        specified if `dynamic` is True. Defaults to `2000`.\n \n     `LossScaleOptimizer` will occasionally skip applying gradients to the\n     variables, in which case the trainable variables will not change that step.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b96f2bcebac884a0654e74ad62235db4efbb56d9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 144/124 | Churn Δ: 11 | Churn Cumulative: 1929 | Contributors (this commit): 10 | Commits (past 90d): 5 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -474,12 +474,13 @@ def clone_model(model, input_tensors=None, clone_function=None):\n             model (except `InputLayer` instances). It takes as argument the\n             layer instance to be cloned, and returns the corresponding layer\n             instance to be used in the model copy. If unspecified, this callable\n-            defaults to the following serialization/deserialization function:\n+            becomes the following serialization/deserialization function:\n             `lambda layer: layer.__class__.from_config(layer.get_config())`.\n             By passing a custom callable, you can customize your copy of the\n             model, e.g. by wrapping certain layers of interest (you might want\n             to replace all `LSTM` instances with equivalent\n             `Bidirectional(LSTM(...))` instances, for example).\n+            Defaults to `None`.\n \n     Returns:\n       An instance of `Model` reproducing the behavior\n\n@@ -41,11 +41,11 @@ class SharpnessAwareMinimization(Model):\n     Args:\n       model: `tf.keras.Model` instance. The inner model that does the\n         forward-backward pass.\n-      rho: float, defaults to 0.05. The gradients scaling factor.\n-      num_batch_splits: int, defaults to None. The number of mini batches to\n+      rho: float. The gradients scaling factor. Defaults to `0.05`.\n+      num_batch_splits: int. The number of mini batches to\n         split into from each data batch. If None, batches are not split into\n-        sub-batches.\n-      name: string, defaults to None. The name of the SAM model.\n+        sub-batches. Defaults to None.\n+      name: string. The name of the SAM model. Defaults to None.\n \n     Reference:\n       [Pierre Foret et al., 2020](https://arxiv.org/abs/2010.01412)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1a57052c4e234a32e05cf7fea0ed4956f1ae8439", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 27 | Files Changed: 8 | Hunks: 23 | Methods Changed: 4 | Complexity Δ (Sum/Max): 379/212 | Churn Δ: 57 | Churn Cumulative: 4984 | Contributors (this commit): 9 | Commits (past 90d): 20 | Contributors (cumulative): 39 | DMM Complexity: None\n\nDIFF:\n@@ -48,10 +48,10 @@ class Adadelta(optimizer_v2.OptimizerV2):\n       learning_rate: Initial value for the learning rate:\n         either a floating point value,\n         or a `tf.keras.optimizers.schedules.LearningRateSchedule` instance.\n-        Defaults to 0.001.\n         Note that `Adadelta` tends to benefit from higher initial learning rate\n         values compared to other optimizers.\n         To match the exact form in the original paper, use 1.0.\n+        Defaults to `0.001`.\n       rho: A `Tensor` or a floating point value. The decay rate.\n       epsilon: Small floating point value used to maintain numerical stability.\n       name: Optional name prefix for the operations created when applying\n\n@@ -40,10 +40,10 @@ class Adagrad(optimizer_v2.OptimizerV2):\n       learning_rate: Initial value for the learning rate:\n         either a floating point value,\n         or a `tf.keras.optimizers.schedules.LearningRateSchedule` instance.\n-        Defaults to 0.001.\n         Note that `Adagrad` tends to benefit from higher initial learning rate\n         values compared to other optimizers.\n         To match the exact form in the original paper, use 1.0.\n+        Defaults to `0.001`.\n       initial_accumulator_value: Floating point value.\n         Starting value for the accumulators (per-parameter momentum values).\n         Must be non-negative.\n\n@@ -44,17 +44,18 @@ class Adam(optimizer_v2.OptimizerV2):\n       learning_rate: A `Tensor`, floating point value, or a schedule that is a\n         `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n         that takes no arguments and returns the actual value to use, The\n-        learning rate. Defaults to 0.001.\n+        learning rate. Defaults to `0.001`.\n       beta_1: A float value or a constant float tensor, or a callable\n         that takes no arguments and returns the actual value to use. The\n-        exponential decay rate for the 1st moment estimates. Defaults to 0.9.\n+        exponential decay rate for the 1st moment estimates. Defaults to `0.9`.\n       beta_2: A float value or a constant float tensor, or a callable\n         that takes no arguments and returns the actual value to use, The\n-        exponential decay rate for the 2nd moment estimates. Defaults to 0.999.\n+        exponential decay rate for the 2nd moment estimates. Defaults to\n+        `0.999`.\n       epsilon: A small constant for numerical stability. This epsilon is\n         \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n         Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n-        1e-7.\n+        `1e-7`.\n       amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm from\n         the paper \"On the Convergence of Adam and beyond\". Defaults to `False`.\n       name: Optional name for the operations created when applying gradients.\n@@ -364,19 +365,19 @@ class NonFusedAdam(optimizer_v2.OptimizerV2):\n           learning_rate: A `Tensor`, floating point value, or a schedule that is\n             a `tf.keras.optimizers.schedules.LearningRateSchedule`, or a\n             callable that takes no arguments and returns the actual value to\n-            use, The learning rate. Defaults to 0.001.\n+            use, The learning rate. Defaults to `0.001`.\n           beta_1: A float value or a constant float tensor, or a callable that\n             takes no arguments and returns the actual value to use. The\n             exponential decay rate for the 1st moment estimates. Defaults to\n-            0.9.\n+            `0.9`.\n           beta_2: A float value or a constant float tensor, or a callable that\n             takes no arguments and returns the actual value to use, The\n             exponential decay rate for the 2nd moment estimates. Defaults to\n-            0.999.\n+            `0.999`.\n           epsilon: A small constant for numerical stability. This epsilon is\n             \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n             Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults\n-            to 1e-7.\n+            to `1e-7`.\n           amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm\n             from the paper \"On the Convergence of Adam and beyond\". Defaults to\n             `False`.\n\n@@ -81,9 +81,9 @@ class Ftrl(optimizer_v2.OptimizerV2):\n       initial_accumulator_value: The starting value for accumulators.\n         Only zero or positive values are allowed.\n       l1_regularization_strength: A float value, must be greater than or\n-        equal to zero. Defaults to 0.0.\n+        equal to zero. Defaults to `0.0`.\n       l2_regularization_strength: A float value, must be greater than or\n-        equal to zero. Defaults to 0.0.\n+        equal to zero. Defaults to `0.0`.\n       name: Optional name prefix for the operations created when applying\n         gradients.  Defaults to `\"Ftrl\"`.\n       l2_shrinkage_regularization_strength: A float value, must be greater than\n@@ -91,7 +91,7 @@ class Ftrl(optimizer_v2.OptimizerV2):\n         stabilization penalty, whereas this L2 shrinkage is a magnitude penalty.\n         When input is sparse shrinkage will only happen on the active weights.\n       beta: A float value, representing the beta value from the paper.\n-        Defaults to 0.0.\n+        Defaults to `0.0`.\n       **kwargs: keyword arguments. Allowed arguments are `clipvalue`,\n         `clipnorm`, `global_clipnorm`.\n         If `clipvalue` (float) is set, the gradient of each weight\n\n@@ -54,10 +54,10 @@ class SGD(optimizer_v2.OptimizerV2):\n       learning_rate: A `Tensor`, floating point value, or a schedule that is a\n         `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n         that takes no arguments and returns the actual value to use. The\n-        learning rate. Defaults to 0.01.\n+        learning rate. Defaults to `0.01`.\n       momentum: float hyperparameter >= 0 that accelerates gradient descent in\n-        the relevant direction and dampens oscillations. Defaults to 0, i.e.,\n-        vanilla gradient descent.\n+        the relevant direction and dampens oscillations. Vanilla gradient\n+        descent means no momentum. Defaults to `0.`.\n       nesterov: boolean. Whether to apply Nesterov momentum.\n         Defaults to `False`.\n       name: Optional name prefix for the operations created when applying\n\n@@ -692,8 +692,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n \n         Args:\n           grads_and_vars: List of (gradient, variable) pairs.\n-          name: Optional name for the returned operation. Default to the name\n-            passed to the `Optimizer` constructor.\n+          name: Optional name for the returned operation. When None, uses the\n+            name passed to the `Optimizer` constructor. Defaults to `None`.\n           experimental_aggregate_gradients: Whether to sum gradients from\n             different replicas in the presence of `tf.distribute.Strategy`. If\n             False, it's user responsibility to aggregate the gradients. Default\n\n@@ -45,13 +45,14 @@ class RMSprop(optimizer_v2.OptimizerV2):\n       learning_rate: A `Tensor`, floating point value, or a schedule that is a\n         `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n         that takes no arguments and returns the actual value to use. The\n-        learning rate. Defaults to 0.001.\n-      rho: Discounting factor for the history/coming gradient. Defaults to 0.9.\n-      momentum: A scalar or a scalar `Tensor`. Defaults to 0.0.\n+        learning rate. Defaults to `0.001`.\n+      rho: Discounting factor for the history/coming gradient. Defaults to\n+        `0.9`.\n+      momentum: A scalar or a scalar `Tensor`. Defaults to `0.0`.\n       epsilon: A small constant for numerical stability. This epsilon is\n         \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n         Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n-        1e-7.\n+        `1e-7`.\n       centered: Boolean. If `True`, gradients are normalized by the estimated\n         variance of the gradient; if False, by the uncentered second moment.\n         Setting this to `True` may help with training, but is slightly more\n@@ -111,10 +112,10 @@ class RMSprop(optimizer_v2.OptimizerV2):\n           learning_rate: A `Tensor`, floating point value, or a schedule that is\n             a `tf.keras.optimizers.schedules.LearningRateSchedule`, or a\n             callable that takes no arguments and returns the actual value to\n-            use. The learning rate. Defaults to 0.001.\n+            use. The learning rate. Defaults to `0.001`.\n           rho: Discounting factor for the history/coming gradient. Defaults to\n-            0.9.\n-          momentum: A scalar or a scalar `Tensor`. Defaults to 0.0.\n+            `0.9`.\n+          momentum: A scalar or a scalar `Tensor`. Defaults to `0.0`.\n           epsilon: A small constant for numerical stability. This epsilon is\n             \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n             Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults\n\n@@ -264,8 +264,9 @@ def polynomial_decay(\n       end_learning_rate: A scalar `float32` or `float64` `Tensor` or a Python\n         number.  The minimal end learning rate.\n       power: A scalar `float32` or `float64` `Tensor` or a Python number.  The\n-        power of the polynomial. Defaults to linear, 1.0.\n-      cycle: A boolean, whether or not it should cycle beyond decay_steps.\n+        power of the polynomial. Linear is default. Defaults to `1.0`.\n+      cycle: A boolean, whether it should cycle beyond decay_steps. Defaults to\n+        `False`.\n       name: String. Optional name of the operation. Defaults to\n         'PolynomialDecay'.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
