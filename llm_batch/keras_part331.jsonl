{"custom_id": "keras#36b4cd92b76a3ceec7bff226f39fef0743d1f255", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 94 | Lines Deleted: 40 | Files Changed: 16 | Hunks: 45 | Methods Changed: 20 | Complexity Δ (Sum/Max): 10/4 | Churn Δ: 134 | Churn Cumulative: 49054 | Contributors (this commit): 113 | Commits (past 90d): 132 | Contributors (cumulative): 160 | DMM Complexity: 0.7727272727272727\n\nDIFF:\n@@ -1065,6 +1065,7 @@ def track_tf_optimizer(tf_optimizer):\n   optimizers.add(tf_optimizer)\n \n \n+@keras_export('keras.__internal__.backend.track_variable', v1=[])\n def track_variable(v):\n   \"\"\"Tracks the given variable for initialization.\"\"\"\n   if tf.executing_eagerly():\n@@ -1143,6 +1144,7 @@ def _get_variables(graph=None):\n   return variables\n \n \n+@keras_export('keras.__internal__.backend.initialize_variables', v1=[])\n def _initialize_variables(session):\n   \"\"\"Utility to initialize uninitialized variables on the fly.\"\"\"\n   variables = _get_variables(get_graph())\n@@ -3660,7 +3662,7 @@ _VALUE_SET_CODE_STRING = \"\"\"\n def get_value(x):\n   \"\"\"Returns the value of a variable.\n \n-  `backend.get_value` is the compliment of `backend.set_value`, and provides\n+  `backend.get_value` is the complement of `backend.set_value`, and provides\n   a generic interface for reading from variables while abstracting away the\n   differences between TensorFlow 1.x and 2.x semantics.\n \n@@ -3721,7 +3723,7 @@ def batch_get_value(tensors):\n def set_value(x, value):\n   \"\"\"Sets the value of a variable, from a Numpy array.\n \n-  `backend.set_value` is the compliment of `backend.get_value`, and provides\n+  `backend.set_value` is the complement of `backend.get_value`, and provides\n   a generic interface for assigning to variables while abstracting away the\n   differences between TensorFlow 1.x and 2.x semantics.\n \n@@ -3732,7 +3734,7 @@ def set_value(x, value):\n       value: Value to set the tensor to, as a Numpy array\n           (of the same shape).\n   \"\"\"\n-  value = np.asarray(value, dtype=dtype(x))\n+  value = np.asarray(value, dtype=dtype_numpy(x))\n   if tf.compat.v1.executing_eagerly_outside_functions():\n     x.assign(value)\n   else:\n@@ -3774,7 +3776,7 @@ def batch_set_value(tuples):\n         assign_ops = []\n         feed_dict = {}\n         for x, value in tuples:\n-          value = np.asarray(value, dtype=dtype(x))\n+          value = np.asarray(value, dtype=dtype_numpy(x))\n           tf_dtype = tf.as_dtype(x.dtype.name.split('_')[0])\n           if hasattr(x, '_assign_placeholder'):\n             assign_placeholder = x._assign_placeholder\n@@ -4014,7 +4016,7 @@ class GraphExecutionFunction:\n     if self.feed_dict:\n       for key in sorted(self.feed_dict.keys()):\n         array_vals.append(\n-            np.asarray(self.feed_dict[key], dtype=key.dtype.base_dtype.name))\n+            np.asarray(self.feed_dict[key], dtype=key.dtype.as_numpy_dtype))\n \n     # Refresh callable if anything has changed.\n     if (self._callable_fn is None or feed_arrays != self._feed_arrays or\n\n@@ -231,11 +231,18 @@ class SidecarEvaluator(object):\n       self.model.evaluate(\n           self.data, steps=self.steps, callbacks=self.callbacks, verbose=2)\n \n+      return_metrics = {}\n+      for metric in self.model.metrics:\n+        result = metric.result()\n+        if isinstance(result, dict):\n+          return_metrics.update(result)\n+        else:\n+          return_metrics[metric.name] = result\n+\n       logging.info(\n           'End of evaluation. Metrics: %s', ' '.join([\n-              '{}={}'.format(metric.name,\n-                             metric.result().numpy())\n-              for metric in self.model.metrics\n+              '{}={}'.format(name, value.numpy())\n+              for name, value in return_metrics.items()\n           ]))\n \n       # TODO(rchao): Make the max evaluation robust in case users save the\n\n@@ -29,6 +29,13 @@ from keras.optimizer_v2 import gradient_descent\n _BATCH_SIZE = 32\n \n \n+class DictMetric(keras.metrics.MeanSquaredError):\n+\n+  def result(self):\n+    res = super().result()\n+    return {'mean_squared_error_1': res, 'mean_squared_error_2': res}\n+\n+\n class SidecarEvaluatorTest(tf.test.TestCase):\n \n   def createTestModel(self, compile_model):\n@@ -37,7 +44,8 @@ class SidecarEvaluatorTest(tf.test.TestCase):\n       model.compile(\n           gradient_descent.SGD(),\n           loss='mse',\n-          metrics=keras.metrics.CategoricalAccuracy())\n+          metrics=[keras.metrics.CategoricalAccuracy(),\n+                   DictMetric()])\n     return model\n \n   def assertSummaryEventsWritten(self, log_dir):\n@@ -59,7 +67,9 @@ class SidecarEvaluatorTest(tf.test.TestCase):\n           event_pb_written = True\n     self.assertCountEqual(event_tags, [\n         'evaluation_categorical_accuracy_vs_iterations',\n-        'evaluation_loss_vs_iterations'\n+        'evaluation_loss_vs_iterations',\n+        'evaluation_mean_squared_error_1_vs_iterations',\n+        'evaluation_mean_squared_error_2_vs_iterations',\n     ])\n \n     # Verifying at least one non-zeroth step is written to summary.\n@@ -114,13 +124,14 @@ class SidecarEvaluatorTest(tf.test.TestCase):\n \n     # Create a new model used for evaluation.\n     eval_model = self.createTestModel(compile_model=True)\n-    # Have an sidecar_evaluator evaluate once.\n-    sidecar_evaluator_lib.SidecarEvaluator(\n+    # Have a sidecar_evaluator evaluate once.\n+    sidecar_evaluator = sidecar_evaluator_lib.SidecarEvaluator(\n         eval_model,\n         data=dataset,\n         checkpoint_dir=checkpoint_dir,\n         max_evaluations=1,\n-        callbacks=[keras.callbacks.TensorBoard(log_dir=log_dir)]).start()\n+        callbacks=[keras.callbacks.TensorBoard(log_dir=log_dir)])\n+    sidecar_evaluator.start()\n     # Eval model has been restored to the same state as the original model, so\n     # their weights should match. If not, restoration of the model didn't\n     # work.\n@@ -148,15 +159,27 @@ class SidecarEvaluatorTest(tf.test.TestCase):\n \n     # Create a new model used for evaluation.\n     eval_model = self.createTestModel(compile_model=True)\n-    # Have an sidecar_evaluator evaluate once.\n+    # Have a sidecar_evaluator evaluate once.\n     sidecar_evaluator = sidecar_evaluator_lib.SidecarEvaluator(\n         eval_model,\n         data=dataset,\n         checkpoint_dir=checkpoint_dir,\n         max_evaluations=1,\n         callbacks=[keras.callbacks.TensorBoard(log_dir=log_dir)])\n+    with self.assertLogs() as cm:\n       sidecar_evaluator.start()\n \n+    metrics_logging = [\n+        line for line in cm.output if 'End of evaluation' in line\n+    ]\n+    self.assertLen(metrics_logging, 1)\n+    expected_logged_metrics = [\n+        'loss', 'categorical_accuracy', 'mean_squared_error_1',\n+        'mean_squared_error_2'\n+    ]\n+    for metric_name in expected_logged_metrics:\n+      self.assertRegex(metrics_logging[0], f'{metric_name}=')\n+\n     # Eval model has been restored to the same state as the original model, so\n     # their weights should match. If not, restoration of the model didn't\n     # work.\n\n@@ -1751,11 +1751,12 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n         weight_index += num_tensors\n       else:\n         weight = weights[weight_index]\n+        weight_shape = weight.shape if hasattr(weight, 'shape') else ()\n         ref_shape = param.shape\n-        if not ref_shape.is_compatible_with(weight.shape):\n+        if not ref_shape.is_compatible_with(weight_shape):\n           raise ValueError(\n               'Layer weight shape %s not compatible with provided weight '\n-              'shape %s' % (ref_shape, weight.shape))\n+              'shape %s' % (ref_shape, weight_shape))\n         weight_value_tuples.append((param, weight))\n         weight_index += 1\n \n\n@@ -558,6 +558,17 @@ class BaseLayerTest(keras_parameterized.TestCase):\n                                 'not compatible with provided weight shape'):\n       layer.set_weights([kernel.T, bias])\n \n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_set_weights_accepts_output_of_get_weights(self):\n+    layer = layers.Layer()\n+    layer.add_weight(name='scalar_float', shape=(), dtype=tf.float32)\n+    layer.add_weight(name='scalar_string', shape=(), dtype=tf.string,\n+                     initializer=lambda *a, **k: 'abc')\n+    layer.add_weight(name='vector_float', shape=(3,), dtype=tf.float32)\n+    layer.add_weight(name='vector_string', shape=(2,), dtype=tf.string,\n+                     initializer=lambda *a, **k: 2 * ['abc'])\n+    layer.set_weights(layer.get_weights())\n+\n   def test_get_config_error(self):\n \n     class MyLayer(base_layer.Layer):\n\n@@ -22,7 +22,6 @@ from keras import backend\n from keras import combinations\n from keras import keras_parameterized\n from keras.engine import base_layer_utils\n-from tensorflow.python.ops import lookup_ops\n \n \n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n@@ -33,7 +32,7 @@ class TrackableWeightHandlerTest(keras_parameterized.TestCase):\n     # does not play nicely with a separate setUp() call (causing errors related\n     # to graph building), so we have to use a called setup instead of a setUp()\n     # call.\n-    table = lookup_ops.MutableHashTable(\n+    table = tf.lookup.experimental.MutableHashTable(\n         key_dtype=tf.string, value_dtype=tf.int32, default_value=0)\n     return base_layer_utils.TrackableWeightHandler(table)\n \n\n@@ -1273,11 +1273,12 @@ class Layer(base_layer.Layer):\n         weight_index += num_tensors\n       else:\n         weight = weights[weight_index]\n+        weight_shape = weight.shape if hasattr(weight, 'shape') else ()\n         ref_shape = param.shape\n-        if not ref_shape.is_compatible_with(weight.shape):\n+        if not ref_shape.is_compatible_with(weight_shape):\n           raise ValueError(\n               'Layer weight shape %s not compatible with provided weight '\n-              'shape %s' % (ref_shape, weight.shape))\n+              'shape %s' % (ref_shape, weight_shape))\n         weight_value_tuples.append((param, weight))\n         weight_index += 1\n \n\n@@ -733,7 +733,7 @@ def local_conv_matmul(inputs, kernel, kernel_mask, output_shape):\n   kernel = kernel_mask * kernel\n   kernel = make_2d(kernel, split_dim=backend.ndim(kernel) // 2)\n \n-  output_flat = tf.compat.v1.sparse_matmul(inputs_flat, kernel, b_is_sparse=True)\n+  output_flat = tf.matmul(inputs_flat, kernel, b_is_sparse=True)\n   output = backend.reshape(output_flat, [\n       backend.shape(output_flat)[0],\n   ] + output_shape.as_list()[1:])\n\n@@ -139,7 +139,8 @@ class SyncBatchNormalization(normalization.BatchNormalizationBase):\n         local_sum = tf.reduce_sum(y, axis=axes, keepdims=True)\n         local_squared_sum = tf.reduce_sum(tf.square(y), axis=axes,\n                                                 keepdims=True)\n-        batch_size = tf.cast(tf.shape(y)[0], tf.float32)\n+        batch_size = tf.cast(tf.shape(y)[axes[0]],\n+                                   tf.float32)\n         # TODO(b/163099951): batch the all-reduces once we sort out the ordering\n         # issue for NCCL. We don't have a mechanism to launch NCCL in the same\n         # order in each replica nowadays, so we limit NCCL to batch all-reduces.\n@@ -149,7 +150,8 @@ class SyncBatchNormalization(normalization.BatchNormalizationBase):\n         global_batch_size = replica_ctx.all_reduce(tf.distribute.ReduceOp.SUM,\n                                                    batch_size)\n \n-        axes_vals = [(tf.shape(y))[i] for i in range(1, len(axes))]\n+        axes_vals = [(tf.shape(y))[axes[i]]\n+                     for i in range(1, len(axes))]\n         multiplier = tf.cast(tf.reduce_prod(axes_vals),\n                                    tf.float32)\n         multiplier = multiplier * global_batch_size\n\n@@ -290,7 +290,6 @@ class CategoryEncodingInputTest(keras_parameterized.TestCase,\n     self.assertAllEqual(expected_output, output_dataset)\n \n \n-@keras_parameterized.run_all_keras_modes\n @keras_parameterized.run_all_keras_modes\n class CategoryEncodingOutputTest(keras_parameterized.TestCase,\n                                  preprocessing_test_utils.PreprocessingLayerTest\n\n@@ -28,7 +28,6 @@ from keras.layers.preprocessing import category_encoding\n from keras.layers.preprocessing import table_utils\n from keras.saving.saved_model import layer_serialization\n from keras.utils import layer_utils\n-from tensorflow.python.ops import lookup_ops\n from tensorflow.python.platform import tf_logging as logging\n \n INT = \"int\"\n@@ -285,7 +284,7 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n \n     else:\n       self._has_static_table = False\n-      self._table = lookup_ops.MutableHashTable(\n+      self._table = tf.lookup.experimental.MutableHashTable(\n           key_dtype=self._key_dtype,\n           value_dtype=self._value_dtype,\n           default_value=default_value,\n\n@@ -20,7 +20,6 @@ import collections\n import os\n import numpy as np\n from keras.utils import tf_utils\n-from tensorflow.python.ops import lookup_ops\n \n \n class TableHandler(object):\n@@ -32,7 +31,7 @@ class TableHandler(object):\n                mask_token=None,\n                mask_value=0):\n     self.table = table\n-    self.mutable = isinstance(table, lookup_ops.MutableHashTable)\n+    self.mutable = isinstance(table, tf.lookup.experimental.MutableHashTable)\n     self.mask_token = mask_token\n     self.mask_value = mask_value\n \n\n@@ -23,11 +23,10 @@ import numpy as np\n from keras import keras_parameterized\n from keras.layers.preprocessing import preprocessing_test_utils\n from keras.layers.preprocessing import table_utils\n-from tensorflow.python.ops import lookup_ops\n \n \n def get_table(dtype=tf.string, oov_tokens=None):\n-  table = lookup_ops.MutableHashTable(\n+  table = tf.lookup.experimental.MutableHashTable(\n       key_dtype=dtype,\n       value_dtype=tf.int64,\n       default_value=-7,\n\n@@ -1319,8 +1319,9 @@ class Precision(Metric):\n \n   def reset_state(self):\n     num_thresholds = len(to_list(self.thresholds))\n-    backend.batch_set_value(\n-        [(v, np.zeros((num_thresholds,))) for v in self.variables])\n+    backend.batch_set_value([(v, np.zeros((num_thresholds,)))\n+                             for v in (self.true_positives,\n+                                       self.false_positives)])\n \n   def get_config(self):\n     config = {\n@@ -1444,8 +1445,9 @@ class Recall(Metric):\n \n   def reset_state(self):\n     num_thresholds = len(to_list(self.thresholds))\n-    backend.batch_set_value(\n-        [(v, np.zeros((num_thresholds,))) for v in self.variables])\n+    backend.batch_set_value([(v, np.zeros((num_thresholds,)))\n+                             for v in (self.true_positives,\n+                                       self.false_negatives)])\n \n   def get_config(self):\n     config = {\n@@ -1528,8 +1530,11 @@ class SensitivitySpecificityBase(Metric, metaclass=abc.ABCMeta):\n \n   def reset_state(self):\n     num_thresholds = len(self.thresholds)\n-    backend.batch_set_value(\n-        [(v, np.zeros((num_thresholds,))) for v in self.variables])\n+    confusion_matrix_variables = (self.true_positives, self.true_negatives,\n+                                  self.false_positives, self.false_negatives)\n+    backend.batch_set_value([\n+        (v, np.zeros((num_thresholds,))) for v in confusion_matrix_variables\n+    ])\n \n   def get_config(self):\n     config = {'class_id': self.class_id}\n@@ -2355,14 +2360,16 @@ class AUC(Metric):\n           name=self.name)\n \n   def reset_state(self):\n+    if self._built:\n+      confusion_matrix_variables = (self.true_positives, self.true_negatives,\n+                                    self.false_positives, self.false_negatives)\n       if self.multi_label:\n         backend.batch_set_value(\n             [(v, np.zeros((self.num_thresholds, self._num_labels)))\n-           for v in self.variables])\n+             for v in confusion_matrix_variables])\n       else:\n-      backend.batch_set_value([\n-          (v, np.zeros((self.num_thresholds,))) for v in self.variables\n-      ])\n+        backend.batch_set_value([(v, np.zeros((self.num_thresholds,)))\n+                                 for v in confusion_matrix_variables])\n \n   def get_config(self):\n     if is_tensor_or_variable(self.label_weights):\n\n@@ -577,6 +577,9 @@ def _reset_build_compile_trackers(model):\n   model.optimizer = None\n \n \n+@keras_export(\n+    'keras.__internal__.models.in_place_subclassed_model_state_restoration',\n+    v1=[])\n def in_place_subclassed_model_state_restoration(model):\n   \"\"\"Restores the original state of a model after it was \"reset\".\n \n@@ -609,6 +612,7 @@ def in_place_subclassed_model_state_restoration(model):\n     _reset_build_compile_trackers(model)\n \n \n+@keras_export('keras.__internal__.models.clone_and_build_model', v1=[])\n def clone_and_build_model(\n     model, input_tensors=None, target_tensors=None, custom_objects=None,\n     compile_clone=True, in_place_reset=False, optimizer_iterations=None,\n\n@@ -257,6 +257,7 @@ def reduce_weighted_loss(weighted_losses,\n   return loss\n \n \n+@keras_export('keras.__internal__.losses.compute_weighted_loss', v1=[])\n def compute_weighted_loss(losses,\n                           sample_weight=None,\n                           reduction=ReductionV2.SUM_OVER_BATCH_SIZE,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5640f0cfb4ca31974e71c9689966f282a3a85ba3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 67 | Lines Deleted: 28 | Files Changed: 11 | Hunks: 35 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 95 | Churn Cumulative: 25544 | Contributors (this commit): 94 | Commits (past 90d): 64 | Contributors (cumulative): 118 | DMM Complexity: 1.0\n\nDIFF:\n@@ -93,7 +93,7 @@ class ApplicationsTest(tf.test.TestCase, parameterized.TestCase):\n \n   @parameterized.parameters(*MODEL_LIST)\n   def test_application_notop(self, app, last_dim):\n-    if 'NASNet' in app.__name__:\n+    if 'NASNet' or 'MobileNetV3' in app.__name__:\n       only_check_last_dim = True\n     else:\n       only_check_last_dim = False\n@@ -119,6 +119,9 @@ class ApplicationsTest(tf.test.TestCase, parameterized.TestCase):\n       input_shape = (None, None, 1)\n     output_shape = _get_output_shape(\n         lambda: app(weights=None, include_top=False, input_shape=input_shape))\n+    if 'MobileNetV3' in app.__name__:\n+      self.assertShapeEqual(output_shape, (None, 1, 1, last_dim))\n+    else:\n       self.assertShapeEqual(output_shape, (None, None, None, last_dim))\n     backend.clear_session()\n \n@@ -128,6 +131,9 @@ class ApplicationsTest(tf.test.TestCase, parameterized.TestCase):\n       input_shape = (None, None, 4)\n     output_shape = _get_output_shape(\n         lambda: app(weights=None, include_top=False, input_shape=input_shape))\n+    if 'MobileNetV3' in app.__name__:\n+      self.assertShapeEqual(output_shape, (None, 1, 1, last_dim))\n+    else:\n       self.assertShapeEqual(output_shape, (None, None, None, last_dim))\n     backend.clear_session()\n \n\n@@ -293,6 +293,11 @@ def MobileNetV3(stack_fn,\n       axis=channel_axis, epsilon=1e-3,\n       momentum=0.999, name='Conv_1/BatchNorm')(x)\n   x = activation(x)\n+  x = layers.GlobalAveragePooling2D()(x)\n+  if channel_axis == 1:\n+    x = layers.Reshape((last_conv_ch, 1, 1))(x)\n+  else:\n+    x = layers.Reshape((1, 1, last_conv_ch))(x)\n   x = layers.Conv2D(\n       last_point_ch,\n       kernel_size=1,\n@@ -302,11 +307,6 @@ def MobileNetV3(stack_fn,\n   x = activation(x)\n \n   if include_top:\n-    x = layers.GlobalAveragePooling2D()(x)\n-    if channel_axis == 1:\n-      x = layers.Reshape((last_point_ch, 1, 1))(x)\n-    else:\n-      x = layers.Reshape((1, 1, last_point_ch))(x)\n     if dropout_rate > 0:\n       x = layers.Dropout(dropout_rate)(x)\n     x = layers.Conv2D(classes, kernel_size=1, padding='same', name='Logits')(x)\n\n@@ -452,6 +452,10 @@ class RaggedKerasTensor(KerasTensor):\n   def ragged_rank(self):\n     return self.type_spec.ragged_rank\n \n+# Overload slicing\n+RaggedKerasTensor._overload_operator(tf.RaggedTensor, '__getitem__')  # pylint: disable=protected-access\n+\n+# Overload math ops\n RaggedKerasTensor._overload_operator(tf.RaggedTensor, '__add__')  # pylint: disable=protected-access\n RaggedKerasTensor._overload_operator(tf.RaggedTensor, '__radd__')  # pylint: disable=protected-access\n RaggedKerasTensor._overload_operator(tf.RaggedTensor, '__mul__')  # pylint: disable=protected-access\n\n@@ -17,6 +17,7 @@\n import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n+import numpy as np\n from keras import keras_parameterized\n from keras import layers\n from keras.engine import training\n@@ -79,6 +80,24 @@ class RaggedKerasTensorTest(keras_parameterized.TestCase):\n     x = tf.ragged.constant([[3, 4], [1, 2], [3, 5]])\n     self.assertAllEqual(model(x), x / x)\n \n+  def test_getitem(self):\n+    # Test slicing / getitem\n+    inp = layers.Input(shape=(None, 2), ragged=True)\n+    out = inp[:, :2]\n+    model = training.Model(inp, out)\n+\n+    x = tf.RaggedTensor.from_row_lengths(\n+        tf.cast(np.random.randn(6, 2), dtype=tf.float32), [3, 1, 2])\n+    expected = x[:, :2]\n+\n+    self.assertAllEqual(model(x), expected)\n+\n+    # Test that models w/ slicing are correctly serialized/deserialized\n+    config = model.get_config()\n+    model = training.Model.from_config(config)\n+\n+    self.assertAllEqual(model(x), expected)\n+\n   @parameterized.parameters(\n       {'property_name': 'values'},\n       {'property_name': 'flat_values'},\n\n@@ -1554,9 +1554,12 @@ class TFSlicingOpDispatcher(tf.__internal__.dispatch.OpDispatcher):\n     else:\n       return self.NOT_SUPPORTED\n \n-for slicing_op in [tf.__operators__.getitem,  # pylint: disable=protected-access\n+for slicing_op in [\n+    tf.__operators__.getitem,  # pylint: disable=protected-access\n     tf.compat.v1.boolean_mask,\n-                   tf.boolean_mask]:\n+    tf.boolean_mask,\n+    tf.__operators__.ragged_getitem\n+]:\n   TFSlicingOpDispatcher(slicing_op).register(slicing_op)\n \n \n\n@@ -19,13 +19,14 @@ import tensorflow.compat.v2 as tf\n \n import itertools\n import numpy as np\n+from keras.engine import base_layer\n from keras.engine import base_preprocessing_layer\n from keras.utils import tf_utils\n from tensorflow.python.util.tf_export import keras_export\n \n \n @keras_export('keras.layers.experimental.preprocessing.CategoryCrossing')\n-class CategoryCrossing(base_preprocessing_layer.PreprocessingLayer):\n+class CategoryCrossing(base_layer.Layer):\n   \"\"\"Category crossing layer.\n \n   This layer concatenates multiple categorical inputs into a single categorical\n\n@@ -19,6 +19,7 @@ import tensorflow.compat.v2 as tf\n \n import numpy as np\n from keras import backend\n+from keras.engine import base_layer\n from keras.engine import base_preprocessing_layer\n from keras.utils import layer_utils\n from tensorflow.python.platform import tf_logging as logging\n@@ -30,7 +31,7 @@ COUNT = \"count\"\n \n \n @keras_export(\"keras.layers.experimental.preprocessing.CategoryEncoding\")\n-class CategoryEncoding(base_preprocessing_layer.PreprocessingLayer):\n+class CategoryEncoding(base_layer.Layer):\n   \"\"\"Category encoding layer.\n \n   This layer provides options for condensing data into a categorical encoding\n@@ -101,6 +102,8 @@ class CategoryEncoding(base_preprocessing_layer.PreprocessingLayer):\n       del kwargs[\"max_tokens\"]\n \n     super(CategoryEncoding, self).__init__(**kwargs)\n+    base_preprocessing_layer.keras_kpl_gauge.get_cell(\"CategoryEncoding\").set(\n+        True)\n \n     # 'output_mode' must be one of (COUNT, BINARY)\n     layer_utils.validate_string_arg(\n\n@@ -19,6 +19,7 @@ import tensorflow.compat.v2 as tf\n \n import functools\n import numpy as np\n+from keras.engine import base_layer\n from keras.engine import base_preprocessing_layer\n from tensorflow.python.util.tf_export import keras_export\n \n@@ -27,7 +28,7 @@ _DEFAULT_SALT_KEY = [0xDECAFCAFFE, 0xDECAFCAFFE]\n \n \n @keras_export('keras.layers.experimental.preprocessing.Hashing')\n-class Hashing(base_preprocessing_layer.PreprocessingLayer):\n+class Hashing(base_layer.Layer):\n   \"\"\"Implements categorical feature hashing, also known as \"hashing trick\".\n \n   This layer transforms single or multiple categorical inputs to hashed output.\n\n@@ -19,8 +19,8 @@ import tensorflow.compat.v2 as tf\n \n import numpy as np\n from keras import backend\n+from keras.engine import base_layer\n from keras.engine import base_preprocessing_layer\n-from keras.engine.base_preprocessing_layer import PreprocessingLayer\n from keras.engine.input_spec import InputSpec\n from keras.utils import control_flow_util\n from tensorflow.python.ops import stateless_random_ops\n@@ -54,7 +54,7 @@ def check_fill_mode_and_interpolation(fill_mode, interpolation):\n \n \n @keras_export('keras.layers.experimental.preprocessing.Resizing')\n-class Resizing(PreprocessingLayer):\n+class Resizing(base_layer.Layer):\n   \"\"\"Image resizing layer.\n \n   Resize the batched image input to target height and width. The input should\n@@ -104,7 +104,7 @@ class Resizing(PreprocessingLayer):\n \n \n @keras_export('keras.layers.experimental.preprocessing.CenterCrop')\n-class CenterCrop(PreprocessingLayer):\n+class CenterCrop(base_layer.Layer):\n   \"\"\"Crop the central portion of the images to target height and width.\n \n   Input shape:\n@@ -171,7 +171,7 @@ class CenterCrop(PreprocessingLayer):\n \n \n @keras_export('keras.layers.experimental.preprocessing.RandomCrop')\n-class RandomCrop(PreprocessingLayer):\n+class RandomCrop(base_layer.Layer):\n   \"\"\"Randomly crop the images to target height and width.\n \n   This layer will crop all the images in the same batch to the same cropping\n@@ -278,7 +278,7 @@ class RandomCrop(PreprocessingLayer):\n \n \n @keras_export('keras.layers.experimental.preprocessing.Rescaling')\n-class Rescaling(PreprocessingLayer):\n+class Rescaling(base_layer.Layer):\n   \"\"\"Multiply inputs by `scale` and adds `offset`.\n \n   For instance:\n@@ -332,7 +332,7 @@ HORIZONTAL_AND_VERTICAL = 'horizontal_and_vertical'\n \n \n @keras_export('keras.layers.experimental.preprocessing.RandomFlip')\n-class RandomFlip(PreprocessingLayer):\n+class RandomFlip(base_layer.Layer):\n   \"\"\"Randomly flip each image horizontally and vertically.\n \n   This layer will flip the images based on the `mode` attribute.\n@@ -411,7 +411,7 @@ class RandomFlip(PreprocessingLayer):\n \n # TODO(tanzheny): Add examples, here and everywhere.\n @keras_export('keras.layers.experimental.preprocessing.RandomTranslation')\n-class RandomTranslation(PreprocessingLayer):\n+class RandomTranslation(base_layer.Layer):\n   \"\"\"Randomly translate each image during training.\n \n   Args:\n@@ -720,7 +720,7 @@ def get_rotation_matrix(angles, image_height, image_width, name=None):\n \n \n @keras_export('keras.layers.experimental.preprocessing.RandomRotation')\n-class RandomRotation(PreprocessingLayer):\n+class RandomRotation(base_layer.Layer):\n   \"\"\"Randomly rotate each image.\n \n   By default, random rotations are only applied during training.\n@@ -833,7 +833,7 @@ class RandomRotation(PreprocessingLayer):\n \n \n @keras_export('keras.layers.experimental.preprocessing.RandomZoom')\n-class RandomZoom(PreprocessingLayer):\n+class RandomZoom(base_layer.Layer):\n   \"\"\"Randomly zoom each image during training.\n \n   Args:\n@@ -1018,7 +1018,7 @@ def get_zoom_matrix(zooms, image_height, image_width, name=None):\n \n \n @keras_export('keras.layers.experimental.preprocessing.RandomContrast')\n-class RandomContrast(PreprocessingLayer):\n+class RandomContrast(base_layer.Layer):\n   \"\"\"Adjust the contrast of an image or images by a random factor.\n \n   Contrast is adjusted independently for each channel of each image during\n@@ -1089,7 +1089,7 @@ class RandomContrast(PreprocessingLayer):\n \n \n @keras_export('keras.layers.experimental.preprocessing.RandomHeight')\n-class RandomHeight(PreprocessingLayer):\n+class RandomHeight(base_layer.Layer):\n   \"\"\"Randomly vary the height of a batch of images during training.\n \n   Adjusts the height of a batch of images by a random factor. The input\n@@ -1185,7 +1185,7 @@ class RandomHeight(PreprocessingLayer):\n \n \n @keras_export('keras.layers.experimental.preprocessing.RandomWidth')\n-class RandomWidth(PreprocessingLayer):\n+class RandomWidth(base_layer.Layer):\n   \"\"\"Randomly vary the width of a batch of images during training.\n \n   Adjusts the width of a batch of images by a random factor. The input\n\n@@ -1962,8 +1962,8 @@ class CosineSimilarity(LossFunctionWrapper):\n   >>> y_pred = [[1., 0.], [1., 1.]]\n   >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n   >>> cosine_loss = tf.keras.losses.CosineSimilarity(axis=1)\n-  >>> # l2_norm(y_true) = [[0., 1.], [1./1.414], 1./1.414]]]\n-  >>> # l2_norm(y_pred) = [[1., 0.], [1./1.414], 1./1.414]]]\n+  >>> # l2_norm(y_true) = [[0., 1.], [1./1.414, 1./1.414]]\n+  >>> # l2_norm(y_pred) = [[1., 0.], [1./1.414, 1./1.414]]\n   >>> # l2_norm(y_true) . l2_norm(y_pred) = [[0., 0.], [0.5, 0.5]]\n   >>> # loss = mean(sum(l2_norm(y_true) . l2_norm(y_pred), axis=1))\n   >>> #       = -((0. + 0.) +  (0.5 + 0.5)) / 2\n\n@@ -24,6 +24,7 @@ from keras import backend as K\n from keras.engine import keras_tensor\n from keras.utils import object_identity\n from keras.utils import tf_contextlib\n+from tensorflow.python.util.tf_export import keras_export\n \n \n def is_tensor_or_tensor_list(v):\n@@ -320,12 +321,13 @@ def is_symbolic_tensor(tensor):\n     return False\n \n \n+@keras_export('keras.__internal__.utils.register_symbolic_tensor_type', v1=[])\n def register_symbolic_tensor_type(cls):\n   \"\"\"Allows users to specify types regarded as symbolic `Tensor`s.\n \n   Used in conjunction with `tf.register_tensor_conversion_function`, calling\n-  `tf.keras.utils.register_symbolic_tensor_type(cls)` allows non-`Tensor`\n-  objects to be plumbed through Keras layers.\n+  `tf.keras.__internal__.utils.register_symbolic_tensor_type(cls)`\n+  allows non-`Tensor` objects to be plumbed through Keras layers.\n \n   Example:\n \n@@ -340,7 +342,7 @@ def register_symbolic_tensor_type(cls):\n   tf.register_tensor_conversion_function(\n       Foo, lambda x, *args, **kwargs: x.value())\n \n-  tf.keras.utils.register_symbolic_tensor_type(Foo)\n+  tf.keras.__internal__.utils.register_symbolic_tensor_type(Foo)\n \n   # User-land.\n   layer = tf.keras.layers.Lambda(lambda input_: Foo(input_))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#880d955ea5f94a9c5f85db95be907880d818a4a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 196 | Lines Deleted: 27 | Files Changed: 5 | Hunks: 50 | Methods Changed: 15 | Complexity Δ (Sum/Max): 8/6 | Churn Δ: 223 | Churn Cumulative: 7483 | Contributors (this commit): 36 | Commits (past 90d): 31 | Contributors (cumulative): 50 | DMM Complexity: 1.0\n\nDIFF:\n@@ -433,6 +433,8 @@ def hard_sigmoid(x):\n   \"\"\"Hard sigmoid activation function.\n \n   A faster approximation of the sigmoid activation. \n+  Piecewise linear approximation of the sigmoid function.\n+  Ref: 'https://en.wikipedia.org/wiki/Hard_sigmoid'\n \n   For example:\n \n\n@@ -882,15 +882,22 @@ class AveragePooling3D(Pooling3D):\n class GlobalPooling1D(Layer):\n   \"\"\"Abstract class for different global pooling 1D layers.\"\"\"\n \n-  def __init__(self, data_format='channels_last', **kwargs):\n+  def __init__(self, data_format='channels_last', keepdims=False, **kwargs):\n     super(GlobalPooling1D, self).__init__(**kwargs)\n     self.input_spec = InputSpec(ndim=3)\n     self.data_format = conv_utils.normalize_data_format(data_format)\n+    self.keepdims = keepdims\n \n   def compute_output_shape(self, input_shape):\n     input_shape = tf.TensorShape(input_shape).as_list()\n     if self.data_format == 'channels_first':\n+      if self.keepdims:\n+        return tf.TensorShape([input_shape[0], input_shape[1], 1])\n+      else:\n         return tf.TensorShape([input_shape[0], input_shape[1]])\n+    else:\n+      if self.keepdims:\n+        return tf.TensorShape([input_shape[0], 1, input_shape[2]])\n       else:\n         return tf.TensorShape([input_shape[0], input_shape[2]])\n \n@@ -898,7 +905,7 @@ class GlobalPooling1D(Layer):\n     raise NotImplementedError\n \n   def get_config(self):\n-    config = {'data_format': self.data_format}\n+    config = {'data_format': self.data_format, 'keepdims': self.keepdims}\n     base_config = super(GlobalPooling1D, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n@@ -924,6 +931,12 @@ class GlobalAveragePooling1D(GlobalPooling1D):\n       `(batch, steps, features)` while `channels_first`\n       corresponds to inputs with shape\n       `(batch, features, steps)`.\n+    keepdims: A boolean, whether to keep the temporal dimension or not.\n+      If `keepdims` is `False` (default), the rank of the tensor is reduced\n+      for spatial dimensions.\n+      If `keepdims` is `True`, the temporal dimension are retained with\n+      length 1.\n+      The behavior is the same as for `tf.reduce_mean` or `np.mean`.\n \n   Call arguments:\n     inputs: A 3D tensor.\n@@ -939,7 +952,13 @@ class GlobalAveragePooling1D(GlobalPooling1D):\n       `(batch_size, features, steps)`\n \n   Output shape:\n+    - If `keepdims`=False:\n       2D tensor with shape `(batch_size, features)`.\n+    - If `keepdims`=True:\n+      - If `data_format='channels_last'`:\n+        3D tensor with shape `(batch_size, 1, features)`\n+      - If `data_format='channels_first'`:\n+        3D tensor with shape `(batch_size, features, 1)`\n   \"\"\"\n \n   def __init__(self, data_format='channels_last', **kwargs):\n@@ -954,10 +973,12 @@ class GlobalAveragePooling1D(GlobalPooling1D):\n       mask = tf.compat.v1.expand_dims(\n           mask, 2 if self.data_format == 'channels_last' else 1)\n       inputs *= mask\n-      return backend.sum(inputs, axis=steps_axis) / tf.reduce_sum(\n-          mask, axis=steps_axis)\n+      return backend.sum(\n+          inputs, axis=steps_axis,\n+          keepdims=self.keepdims) / tf.reduce_sum(\n+              mask, axis=steps_axis, keepdims=self.keepdims)\n     else:\n-      return backend.mean(inputs, axis=steps_axis)\n+      return backend.mean(inputs, axis=steps_axis, keepdims=self.keepdims)\n \n   def compute_mask(self, inputs, mask=None):\n     return None\n@@ -994,6 +1015,12 @@ class GlobalMaxPooling1D(GlobalPooling1D):\n       `(batch, steps, features)` while `channels_first`\n       corresponds to inputs with shape\n       `(batch, features, steps)`.\n+    keepdims: A boolean, whether to keep the temporal dimension or not.\n+      If `keepdims` is `False` (default), the rank of the tensor is reduced\n+      for spatial dimensions.\n+      If `keepdims` is `True`, the temporal dimension are retained with\n+      length 1.\n+      The behavior is the same as for `tf.reduce_max` or `np.max`.\n \n   Input shape:\n     - If `data_format='channels_last'`:\n@@ -1004,27 +1031,40 @@ class GlobalMaxPooling1D(GlobalPooling1D):\n       `(batch_size, features, steps)`\n \n   Output shape:\n+    - If `keepdims`=False:\n       2D tensor with shape `(batch_size, features)`.\n+    - If `keepdims`=True:\n+      - If `data_format='channels_last'`:\n+        3D tensor with shape `(batch_size, 1, features)`\n+      - If `data_format='channels_first'`:\n+        3D tensor with shape `(batch_size, features, 1)`\n   \"\"\"\n \n   def call(self, inputs):\n     steps_axis = 1 if self.data_format == 'channels_last' else 2\n-    return backend.max(inputs, axis=steps_axis)\n+    return backend.max(inputs, axis=steps_axis, keepdims=self.keepdims)\n \n \n class GlobalPooling2D(Layer):\n   \"\"\"Abstract class for different global pooling 2D layers.\n   \"\"\"\n \n-  def __init__(self, data_format=None, **kwargs):\n+  def __init__(self, data_format=None, keepdims=False, **kwargs):\n     super(GlobalPooling2D, self).__init__(**kwargs)\n     self.data_format = conv_utils.normalize_data_format(data_format)\n     self.input_spec = InputSpec(ndim=4)\n+    self.keepdims = keepdims\n \n   def compute_output_shape(self, input_shape):\n     input_shape = tf.TensorShape(input_shape).as_list()\n     if self.data_format == 'channels_last':\n+      if self.keepdims:\n+        return tf.TensorShape([input_shape[0], 1, 1, input_shape[3]])\n+      else:\n         return tf.TensorShape([input_shape[0], input_shape[3]])\n+    else:\n+      if self.keepdims:\n+        return tf.TensorShape([input_shape[0], input_shape[1], 1, 1])\n       else:\n         return tf.TensorShape([input_shape[0], input_shape[1]])\n \n@@ -1032,7 +1072,7 @@ class GlobalPooling2D(Layer):\n     raise NotImplementedError\n \n   def get_config(self):\n-    config = {'data_format': self.data_format}\n+    config = {'data_format': self.data_format, 'keepdims': self.keepdims}\n     base_config = super(GlobalPooling2D, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n@@ -1061,6 +1101,12 @@ class GlobalAveragePooling2D(GlobalPooling2D):\n         It defaults to the `image_data_format` value found in your\n         Keras config file at `~/.keras/keras.json`.\n         If you never set it, then it will be \"channels_last\".\n+      keepdims: A boolean, whether to keep the spatial dimensions or not.\n+        If `keepdims` is `False` (default), the rank of the tensor is reduced\n+        for spatial dimensions.\n+        If `keepdims` is `True`, the spatial dimensions are retained with\n+        length 1.\n+        The behavior is the same as for `tf.reduce_mean` or `np.mean`.\n \n   Input shape:\n     - If `data_format='channels_last'`:\n@@ -1069,14 +1115,20 @@ class GlobalAveragePooling2D(GlobalPooling2D):\n       4D tensor with shape `(batch_size, channels, rows, cols)`.\n \n   Output shape:\n+    - If `keepdims`=False:\n       2D tensor with shape `(batch_size, channels)`.\n+    - If `keepdims`=True:\n+      - If `data_format='channels_last'`:\n+        4D tensor with shape `(batch_size, 1, 1, channels)`\n+      - If `data_format='channels_first'`:\n+        4D tensor with shape `(batch_size, channels, 1, 1)`\n   \"\"\"\n \n   def call(self, inputs):\n     if self.data_format == 'channels_last':\n-      return backend.mean(inputs, axis=[1, 2])\n+      return backend.mean(inputs, axis=[1, 2], keepdims=self.keepdims)\n     else:\n-      return backend.mean(inputs, axis=[2, 3])\n+      return backend.mean(inputs, axis=[2, 3], keepdims=self.keepdims)\n \n \n @keras_export('keras.layers.GlobalMaxPool2D', 'keras.layers.GlobalMaxPooling2D')\n@@ -1102,6 +1154,12 @@ class GlobalMaxPooling2D(GlobalPooling2D):\n       It defaults to the `image_data_format` value found in your\n       Keras config file at `~/.keras/keras.json`.\n       If you never set it, then it will be \"channels_last\".\n+    keepdims: A boolean, whether to keep the spatial dimensions or not.\n+      If `keepdims` is `False` (default), the rank of the tensor is reduced\n+      for spatial dimensions.\n+      If `keepdims` is `True`, the spatial dimensions are retained with\n+      length 1.\n+      The behavior is the same as for `tf.reduce_max` or `np.max`.\n \n   Input shape:\n     - If `data_format='channels_last'`:\n@@ -1110,28 +1168,43 @@ class GlobalMaxPooling2D(GlobalPooling2D):\n       4D tensor with shape `(batch_size, channels, rows, cols)`.\n \n   Output shape:\n+    - If `keepdims`=False:\n       2D tensor with shape `(batch_size, channels)`.\n+    - If `keepdims`=True:\n+      - If `data_format='channels_last'`:\n+        4D tensor with shape `(batch_size, 1, 1, channels)`\n+      - If `data_format='channels_first'`:\n+        4D tensor with shape `(batch_size, channels, 1, 1)`\n   \"\"\"\n \n   def call(self, inputs):\n     if self.data_format == 'channels_last':\n-      return backend.max(inputs, axis=[1, 2])\n+      return backend.max(inputs, axis=[1, 2], keepdims=self.keepdims)\n     else:\n-      return backend.max(inputs, axis=[2, 3])\n+      return backend.max(inputs, axis=[2, 3], keepdims=self.keepdims)\n \n \n class GlobalPooling3D(Layer):\n   \"\"\"Abstract class for different global pooling 3D layers.\"\"\"\n \n-  def __init__(self, data_format=None, **kwargs):\n+  def __init__(self, data_format=None, keepdims=False, **kwargs):\n     super(GlobalPooling3D, self).__init__(**kwargs)\n     self.data_format = conv_utils.normalize_data_format(data_format)\n     self.input_spec = InputSpec(ndim=5)\n+    self.keepdims = keepdims\n \n   def compute_output_shape(self, input_shape):\n     input_shape = tf.TensorShape(input_shape).as_list()\n     if self.data_format == 'channels_last':\n+      if self.keepdims:\n+        return tf.TensorShape(\n+            [input_shape[0], 1, 1, 1, input_shape[4]])\n+      else:\n         return tf.TensorShape([input_shape[0], input_shape[4]])\n+    else:\n+      if self.keepdims:\n+        return tf.TensorShape(\n+            [input_shape[0], input_shape[1], 1, 1, 1])\n       else:\n         return tf.TensorShape([input_shape[0], input_shape[1]])\n \n@@ -1139,7 +1212,7 @@ class GlobalPooling3D(Layer):\n     raise NotImplementedError\n \n   def get_config(self):\n-    config = {'data_format': self.data_format}\n+    config = {'data_format': self.data_format, 'keepdims': self.keepdims}\n     base_config = super(GlobalPooling3D, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n \n@@ -1160,6 +1233,12 @@ class GlobalAveragePooling3D(GlobalPooling3D):\n       It defaults to the `image_data_format` value found in your\n       Keras config file at `~/.keras/keras.json`.\n       If you never set it, then it will be \"channels_last\".\n+    keepdims: A boolean, whether to keep the spatial dimensions or not.\n+      If `keepdims` is `False` (default), the rank of the tensor is reduced\n+      for spatial dimensions.\n+      If `keepdims` is `True`, the spatial dimensions are retained with\n+      length 1.\n+      The behavior is the same as for `tf.reduce_mean` or `np.mean`.\n \n   Input shape:\n     - If `data_format='channels_last'`:\n@@ -1170,14 +1249,20 @@ class GlobalAveragePooling3D(GlobalPooling3D):\n       `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n \n   Output shape:\n+    - If `keepdims`=False:\n       2D tensor with shape `(batch_size, channels)`.\n+    - If `keepdims`=True:\n+      - If `data_format='channels_last'`:\n+        5D tensor with shape `(batch_size, 1, 1, 1, channels)`\n+      - If `data_format='channels_first'`:\n+        5D tensor with shape `(batch_size, channels, 1, 1, 1)`\n   \"\"\"\n \n   def call(self, inputs):\n     if self.data_format == 'channels_last':\n-      return backend.mean(inputs, axis=[1, 2, 3])\n+      return backend.mean(inputs, axis=[1, 2, 3], keepdims=self.keepdims)\n     else:\n-      return backend.mean(inputs, axis=[2, 3, 4])\n+      return backend.mean(inputs, axis=[2, 3, 4], keepdims=self.keepdims)\n \n \n @keras_export('keras.layers.GlobalMaxPool3D', 'keras.layers.GlobalMaxPooling3D')\n@@ -1195,6 +1280,12 @@ class GlobalMaxPooling3D(GlobalPooling3D):\n       It defaults to the `image_data_format` value found in your\n       Keras config file at `~/.keras/keras.json`.\n       If you never set it, then it will be \"channels_last\".\n+    keepdims: A boolean, whether to keep the spatial dimensions or not.\n+      If `keepdims` is `False` (default), the rank of the tensor is reduced\n+      for spatial dimensions.\n+      If `keepdims` is `True`, the spatial dimensions are retained with\n+      length 1.\n+      The behavior is the same as for `tf.reduce_max` or `np.max`.\n \n   Input shape:\n     - If `data_format='channels_last'`:\n@@ -1205,14 +1296,20 @@ class GlobalMaxPooling3D(GlobalPooling3D):\n       `(batch_size, channels, spatial_dim1, spatial_dim2, spatial_dim3)`\n \n   Output shape:\n+    - If `keepdims`=False:\n       2D tensor with shape `(batch_size, channels)`.\n+    - If `keepdims`=True:\n+      - If `data_format='channels_last'`:\n+        5D tensor with shape `(batch_size, 1, 1, 1, channels)`\n+      - If `data_format='channels_first'`:\n+        5D tensor with shape `(batch_size, channels, 1, 1, 1)`\n   \"\"\"\n \n   def call(self, inputs):\n     if self.data_format == 'channels_last':\n-      return backend.max(inputs, axis=[1, 2, 3])\n+      return backend.max(inputs, axis=[1, 2, 3], keepdims=self.keepdims)\n     else:\n-      return backend.max(inputs, axis=[2, 3, 4])\n+      return backend.max(inputs, axis=[2, 3, 4], keepdims=self.keepdims)\n \n \n # Aliases\n\n@@ -141,6 +141,84 @@ class GlobalPoolingTest(tf.test.TestCase, parameterized.TestCase):\n         kwargs={'data_format': 'channels_last'},\n         input_shape=(3, 4, 3, 4, 3))\n \n+  def test_globalpooling_1d_keepdims(self):\n+    testing_utils.layer_test(\n+        keras.layers.pooling.GlobalMaxPooling1D,\n+        kwargs={'keepdims': True},\n+        input_shape=(3, 4, 5),\n+        expected_output_shape=(None, 1, 5))\n+    testing_utils.layer_test(\n+        keras.layers.pooling.GlobalMaxPooling1D,\n+        kwargs={'data_format': 'channels_first', 'keepdims': True},\n+        input_shape=(3, 4, 5),\n+        expected_output_shape=(None, 4, 1))\n+    testing_utils.layer_test(\n+        keras.layers.pooling.GlobalAveragePooling1D,\n+        kwargs={'keepdims': True},\n+        input_shape=(3, 4, 5),\n+        expected_output_shape=(None, 1, 5))\n+    testing_utils.layer_test(\n+        keras.layers.pooling.GlobalAveragePooling1D,\n+        kwargs={'data_format': 'channels_first', 'keepdims': True},\n+        input_shape=(3, 4, 5),\n+        expected_output_shape=(None, 4, 1))\n+\n+  def test_globalpooling_2d_keepdims(self):\n+    testing_utils.layer_test(\n+        keras.layers.pooling.GlobalMaxPooling2D,\n+        kwargs={'data_format': 'channels_first', 'keepdims': True},\n+        input_shape=(3, 4, 5, 6),\n+        expected_output_shape=(None, 4, 1, 1))\n+    testing_utils.layer_test(\n+        keras.layers.pooling.GlobalMaxPooling2D,\n+        kwargs={'data_format': 'channels_last', 'keepdims': True},\n+        input_shape=(3, 4, 5, 6),\n+        expected_output_shape=(None, 1, 1, 6))\n+    testing_utils.layer_test(\n+        keras.layers.pooling.GlobalAveragePooling2D,\n+        kwargs={'data_format': 'channels_first', 'keepdims': True},\n+        input_shape=(3, 4, 5, 6),\n+        expected_output_shape=(None, 4, 1, 1))\n+    testing_utils.layer_test(\n+        keras.layers.pooling.GlobalAveragePooling2D,\n+        kwargs={'data_format': 'channels_last', 'keepdims': True},\n+        input_shape=(3, 4, 5, 6),\n+        expected_output_shape=(None, 1, 1, 6))\n+\n+  def test_globalpooling_3d_keepdims(self):\n+    testing_utils.layer_test(\n+        keras.layers.pooling.GlobalMaxPooling3D,\n+        kwargs={'data_format': 'channels_first', 'keepdims': True},\n+        input_shape=(3, 4, 3, 4, 3),\n+        expected_output_shape=(None, 4, 1, 1, 1))\n+    testing_utils.layer_test(\n+        keras.layers.pooling.GlobalMaxPooling3D,\n+        kwargs={'data_format': 'channels_last', 'keepdims': True},\n+        input_shape=(3, 4, 3, 4, 3),\n+        expected_output_shape=(None, 1, 1, 1, 3))\n+    testing_utils.layer_test(\n+        keras.layers.pooling.GlobalAveragePooling3D,\n+        kwargs={'data_format': 'channels_first', 'keepdims': True},\n+        input_shape=(3, 4, 3, 4, 3),\n+        expected_output_shape=(None, 4, 1, 1, 1))\n+    testing_utils.layer_test(\n+        keras.layers.pooling.GlobalAveragePooling3D,\n+        kwargs={'data_format': 'channels_last', 'keepdims': True},\n+        input_shape=(3, 4, 3, 4, 3),\n+        expected_output_shape=(None, 1, 1, 1, 3))\n+\n+  def test_globalpooling_1d_keepdims_masking_support(self):\n+    model = keras.Sequential()\n+    model.add(keras.layers.Masking(mask_value=0., input_shape=(None, 4)))\n+    model.add(keras.layers.GlobalAveragePooling1D(keepdims=True))\n+    model.compile(loss='mae', optimizer='rmsprop')\n+\n+    model_input = np.random.random((2, 3, 4))\n+    model_input[0, 1:, :] = 0\n+    output = model.predict(model_input)\n+    self.assertAllEqual((2, 1, 4), output.shape)\n+    self.assertAllClose(output[0, 0], model_input[0, 0, :])\n+\n \n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n class Pooling2DTest(tf.test.TestCase, parameterized.TestCase):\n\n@@ -156,7 +156,6 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n   >>> text_dataset = tf.data.Dataset.from_tensor_slices([\"foo\", \"bar\", \"baz\"])\n   >>> max_features = 5000  # Maximum vocab size.\n   >>> max_len = 4  # Sequence length to pad the outputs to.\n-  >>> embedding_dims = 2\n   >>>\n   >>> # Create the layer.\n   >>> vectorize_layer = TextVectorization(\n\n@@ -17,7 +17,6 @@\n import tensorflow.compat.v2 as tf\n \n import threading\n-from tensorflow.python.distribute import ps_values as ps_distribute_values\n from keras.distribute import distributed_training_utils\n \n \n@@ -505,8 +504,7 @@ def create_autocast_variable(variable):\n   Returns:\n     An AutoCastVariable that wraps the variable.\n   \"\"\"\n-  if ((not distributed_training_utils.is_distributed_variable(variable)) and\n-      not isinstance(variable, ps_distribute_values.AggregatingVariable)):\n+  if not distributed_training_utils.is_distributed_variable(variable):\n     return AutoCastVariable(variable)\n \n   class AutoCastDistributedVariable(AutoCastVariable, variable.__class__):\n@@ -517,11 +515,6 @@ def create_autocast_variable(variable):\n     \"\"\"\n \n     def __repr__(self):\n-      if issubclass(ps_distribute_values.AggregatingVariable,\n-                    variable.__class__):\n-        # AggregatingVariable's __repr__ simply calls super.__repr__. So we do\n-        # the same here for consistency, which calls AutoCastVariable.__repr__.\n-        return super(AutoCastDistributedVariable, self).__repr__()\n \n       # pylint: disable=missing-format-attribute\n       return ('<AutoCastDistributedVariable dtype={v.dtype.name} '\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
