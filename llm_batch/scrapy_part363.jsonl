{"custom_id": "scrapy#3db438127cb4e13f02d16322b3a88a84ca4655db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 31 | Files Changed: 2 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 34 | Churn Cumulative: 869 | Contributors (this commit): 21 | Commits (past 90d): 5 | Contributors (cumulative): 32 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,11 +1,9 @@\n import io\n-import warnings\n import zlib\n \n from scrapy.exceptions import NotConfigured\n from scrapy.http import Response, TextResponse\n from scrapy.responsetypes import responsetypes\n-from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.gz import gunzip\n \n ACCEPTED_ENCODINGS = [b\"gzip\", b\"deflate\"]\n@@ -36,18 +34,7 @@ class HttpCompressionMiddleware:\n     def from_crawler(cls, crawler):\n         if not crawler.settings.getbool(\"COMPRESSION_ENABLED\"):\n             raise NotConfigured\n-        try:\n         return cls(stats=crawler.stats)\n-        except TypeError:\n-            warnings.warn(\n-                \"HttpCompressionMiddleware subclasses must either modify \"\n-                \"their '__init__' method to support a 'stats' parameter or \"\n-                \"reimplement the 'from_crawler' method.\",\n-                ScrapyDeprecationWarning,\n-            )\n-            result = cls()\n-            result.stats = crawler.stats\n-            return result\n \n     def process_request(self, request, spider):\n         request.headers.setdefault(\"Accept-Encoding\", b\", \".join(ACCEPTED_ENCODINGS))\n\n@@ -2,7 +2,6 @@ from gzip import GzipFile\n from io import BytesIO\n from pathlib import Path\n from unittest import SkipTest, TestCase\n-from warnings import catch_warnings\n \n from w3lib.encoding import resolve_encoding\n \n@@ -10,7 +9,7 @@ from scrapy.downloadermiddlewares.httpcompression import (\n     ACCEPTED_ENCODINGS,\n     HttpCompressionMiddleware,\n )\n-from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n+from scrapy.exceptions import NotConfigured\n from scrapy.http import HtmlResponse, Request, Response\n from scrapy.responsetypes import responsetypes\n from scrapy.spiders import Spider\n@@ -381,20 +380,6 @@ class HttpCompressionSubclassTest(TestCase):\n                 super().__init__()\n \n         crawler = get_crawler(Spider)\n-        with catch_warnings(record=True) as caught_warnings:\n+\n+        with self.assertRaises(TypeError):\n             HttpCompressionMiddlewareSubclass.from_crawler(crawler)\n-        messages = tuple(\n-            str(warning.message)\n-            for warning in caught_warnings\n-            if warning.category is ScrapyDeprecationWarning\n-        )\n-        self.assertEqual(\n-            messages,\n-            (\n-                (\n-                    \"HttpCompressionMiddleware subclasses must either modify \"\n-                    \"their '__init__' method to support a 'stats' parameter \"\n-                    \"or reimplement the 'from_crawler' method.\"\n-                ),\n-            ),\n-        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#644ab3af48a16f746f843a8390ff9e1593dc5ce0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 116 | Files Changed: 2 | Hunks: 12 | Methods Changed: 4 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 125 | Churn Cumulative: 9256 | Contributors (this commit): 51 | Commits (past 90d): 16 | Contributors (cumulative): 88 | DMM Complexity: 0.0\n\nDIFF:\n@@ -29,7 +29,7 @@ from scrapy.utils.deprecate import create_deprecated_class\n from scrapy.utils.ftp import ftp_store_file\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import create_instance, load_object\n-from scrapy.utils.python import get_func_args, without_none_values\n+from scrapy.utils.python import without_none_values\n \n logger = logging.getLogger(__name__)\n \n@@ -42,17 +42,7 @@ except ImportError:\n \n \n def build_storage(builder, uri, *args, feed_options=None, preargs=(), **kwargs):\n-    argument_names = get_func_args(builder)\n-    if \"feed_options\" in argument_names:\n     kwargs[\"feed_options\"] = feed_options\n-    else:\n-        warnings.warn(\n-            f\"{builder.__qualname__} does not support the 'feed_options' keyword argument. Add a \"\n-            \"'feed_options' parameter to its signature to remove this \"\n-            \"warning. This parameter will become mandatory in a future \"\n-            \"version of Scrapy.\",\n-            category=ScrapyDeprecationWarning,\n-        )\n     return builder(*preargs, uri, *args, **kwargs)\n \n \n\n@@ -2850,20 +2850,8 @@ class StdoutFeedStoragePreFeedOptionsTest(unittest.TestCase):\n             \"FEED_URI\": \"file:///tmp/foobar\",\n             \"FEED_STORAGES\": {\"file\": StdoutFeedStorageWithoutFeedOptions},\n         }\n-        with pytest.warns(\n-            ScrapyDeprecationWarning,\n-            match=\"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated\",\n-        ):\n-            crawler = get_crawler(settings_dict=settings_dict)\n-            feed_exporter = FeedExporter.from_crawler(crawler)\n-\n-        spider = scrapy.Spider(\"default\")\n-        with pytest.warns(\n-            ScrapyDeprecationWarning,\n-            match=\"StdoutFeedStorageWithoutFeedOptions does not support \"\n-            \"the 'feed_options' keyword argument.\",\n-        ):\n-            feed_exporter.open_spider(spider)\n+        with pytest.raises(TypeError):\n+            get_crawler(settings_dict=settings_dict)\n \n \n class FileFeedStorageWithoutFeedOptions(FileFeedStorage):\n@@ -2872,10 +2860,6 @@ class FileFeedStorageWithoutFeedOptions(FileFeedStorage):\n \n \n class FileFeedStoragePreFeedOptionsTest(unittest.TestCase):\n-    \"\"\"Make sure that any feed exporter created by users before the\n-    introduction of the ``feed_options`` parameter continues to work as\n-    expected, and simply issues a warning.\"\"\"\n-\n     maxDiff = None\n \n     def test_init(self):\n@@ -2884,20 +2868,8 @@ class FileFeedStoragePreFeedOptionsTest(unittest.TestCase):\n                 \"FEED_URI\": f\"file:///{temp.name}\",\n                 \"FEED_STORAGES\": {\"file\": FileFeedStorageWithoutFeedOptions},\n             }\n-            with pytest.warns(\n-                ScrapyDeprecationWarning,\n-                match=\"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated\",\n-            ):\n-                crawler = get_crawler(settings_dict=settings_dict)\n-                feed_exporter = FeedExporter.from_crawler(crawler)\n-        spider = scrapy.Spider(\"default\")\n-\n-        with pytest.warns(\n-            ScrapyDeprecationWarning,\n-            match=\"FileFeedStorageWithoutFeedOptions does not support \"\n-            \"the 'feed_options' keyword argument.\",\n-        ):\n-            feed_exporter.open_spider(spider)\n+            with self.assertRaises(TypeError):\n+                get_crawler(settings_dict=settings_dict)\n \n \n class S3FeedStorageWithoutFeedOptions(S3FeedStorage):\n@@ -2912,10 +2884,6 @@ class S3FeedStorageWithoutFeedOptionsWithFromCrawler(S3FeedStorage):\n \n \n class S3FeedStoragePreFeedOptionsTest(unittest.TestCase):\n-    \"\"\"Make sure that any feed exporter created by users before the\n-    introduction of the ``feed_options`` parameter continues to work as\n-    expected, and simply issues a warning.\"\"\"\n-\n     maxDiff = None\n \n     def setUp(self):\n@@ -2936,11 +2904,6 @@ class S3FeedStoragePreFeedOptionsTest(unittest.TestCase):\n         spider = scrapy.Spider(\"default\")\n         spider.crawler = crawler\n \n-        with pytest.warns(\n-            ScrapyDeprecationWarning,\n-            match=\"S3FeedStorageWithoutFeedOptions does not support \"\n-            \"the 'feed_options' keyword argument.\",\n-        ):\n         feed_exporter.open_spider(spider)\n \n     def test_from_crawler(self):\n@@ -2948,22 +2911,8 @@ class S3FeedStoragePreFeedOptionsTest(unittest.TestCase):\n             \"FEED_URI\": \"file:///tmp/foobar\",\n             \"FEED_STORAGES\": {\"file\": S3FeedStorageWithoutFeedOptionsWithFromCrawler},\n         }\n-        with pytest.warns(\n-            ScrapyDeprecationWarning,\n-            match=\"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated\",\n-        ):\n-            crawler = get_crawler(settings_dict=settings_dict)\n-            feed_exporter = FeedExporter.from_crawler(crawler)\n-\n-        spider = scrapy.Spider(\"default\")\n-        spider.crawler = crawler\n-\n-        with pytest.warns(\n-            ScrapyDeprecationWarning,\n-            match=\"S3FeedStorageWithoutFeedOptionsWithFromCrawler.from_crawler does not support \"\n-            \"the 'feed_options' keyword argument.\",\n-        ):\n-            feed_exporter.open_spider(spider)\n+        with pytest.raises(TypeError):\n+            get_crawler(settings_dict=settings_dict)\n \n \n class FTPFeedStorageWithoutFeedOptions(FTPFeedStorage):\n@@ -2971,17 +2920,7 @@ class FTPFeedStorageWithoutFeedOptions(FTPFeedStorage):\n         super().__init__(uri)\n \n \n-class FTPFeedStorageWithoutFeedOptionsWithFromCrawler(FTPFeedStorage):\n-    @classmethod\n-    def from_crawler(cls, crawler, uri):\n-        return super().from_crawler(crawler, uri)\n-\n-\n class FTPFeedStoragePreFeedOptionsTest(unittest.TestCase):\n-    \"\"\"Make sure that any feed exporter created by users before the\n-    introduction of the ``feed_options`` parameter continues to work as\n-    expected, and simply issues a warning.\"\"\"\n-\n     maxDiff = None\n \n     def test_init(self):\n@@ -2989,44 +2928,8 @@ class FTPFeedStoragePreFeedOptionsTest(unittest.TestCase):\n             \"FEED_URI\": \"ftp://localhost/foo\",\n             \"FEED_STORAGES\": {\"ftp\": FTPFeedStorageWithoutFeedOptions},\n         }\n-        with pytest.warns(\n-            ScrapyDeprecationWarning,\n-            match=\"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated\",\n-        ):\n-            crawler = get_crawler(settings_dict=settings_dict)\n-            feed_exporter = FeedExporter.from_crawler(crawler)\n-\n-        spider = scrapy.Spider(\"default\")\n-        spider.crawler = crawler\n-\n-        with pytest.warns(\n-            ScrapyDeprecationWarning,\n-            match=\"FTPFeedStorageWithoutFeedOptions does not support \"\n-            \"the 'feed_options' keyword argument.\",\n-        ):\n-            feed_exporter.open_spider(spider)\n-\n-    def test_from_crawler(self):\n-        settings_dict = {\n-            \"FEED_URI\": \"ftp://localhost/foo\",\n-            \"FEED_STORAGES\": {\"ftp\": FTPFeedStorageWithoutFeedOptionsWithFromCrawler},\n-        }\n-        with pytest.warns(\n-            ScrapyDeprecationWarning,\n-            match=\"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated\",\n-        ):\n-            crawler = get_crawler(settings_dict=settings_dict)\n-            feed_exporter = FeedExporter.from_crawler(crawler)\n-\n-        spider = scrapy.Spider(\"default\")\n-        spider.crawler = crawler\n-\n-        with pytest.warns(\n-            ScrapyDeprecationWarning,\n-            match=\"FTPFeedStorageWithoutFeedOptionsWithFromCrawler.from_crawler does not support \"\n-            \"the 'feed_options' keyword argument.\",\n-        ):\n-            feed_exporter.open_spider(spider)\n+        with pytest.raises(TypeError):\n+            get_crawler(settings_dict=settings_dict)\n \n \n class URIParamsTest:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0956b764657b9f1d414e196a339988d3951c49dc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 25 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 25 | Churn Cumulative: 215 | Contributors (this commit): 14 | Commits (past 90d): 4 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3,14 +3,12 @@ from __future__ import annotations\n import logging\n from pathlib import Path\n from typing import TYPE_CHECKING, Optional, Set\n-from warnings import warn\n \n from twisted.internet.defer import Deferred\n \n from scrapy.http.request import Request\n from scrapy.settings import BaseSettings\n from scrapy.spiders import Spider\n-from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.job import job_dir\n from scrapy.utils.request import (\n     RequestFingerprinter,\n@@ -75,38 +73,15 @@ class RFPDupeFilter(BaseDupeFilter):\n         fingerprinter: Optional[RequestFingerprinterProtocol] = None,\n     ) -> Self:\n         debug = settings.getbool(\"DUPEFILTER_DEBUG\")\n-        try:\n         return cls(job_dir(settings), debug, fingerprinter=fingerprinter)\n-        except TypeError:\n-            warn(\n-                \"RFPDupeFilter subclasses must either modify their '__init__' \"\n-                \"method to support a 'fingerprinter' parameter or reimplement \"\n-                \"the 'from_settings' class method.\",\n-                ScrapyDeprecationWarning,\n-            )\n-            result = cls(job_dir(settings), debug)\n-            result.fingerprinter = fingerprinter or RequestFingerprinter()\n-            return result\n \n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n         assert crawler.request_fingerprinter\n-        try:\n         return cls.from_settings(\n             crawler.settings,\n             fingerprinter=crawler.request_fingerprinter,\n         )\n-        except TypeError:\n-            warn(\n-                \"RFPDupeFilter subclasses must either modify their overridden \"\n-                \"'__init__' method and 'from_settings' class method to \"\n-                \"support a 'fingerprinter' parameter, or reimplement the \"\n-                \"'from_crawler' class method.\",\n-                ScrapyDeprecationWarning,\n-            )\n-            result = cls.from_settings(crawler.settings)\n-            result.fingerprinter = crawler.request_fingerprinter\n-            return result\n \n     def request_seen(self, request: Request) -> bool:\n         fp = self.request_fingerprint(request)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#39ee8d1ee2decce995c4dafc5da5a6fb55f478c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 231 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,9 @@\n import logging\n import sys\n from abc import ABCMeta, abstractmethod\n+from warnings import warn\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.python import to_unicode\n \n logger = logging.getLogger(__name__)\n@@ -79,6 +81,7 @@ class PythonRobotParser(RobotParser):\n \n class ReppyRobotParser(RobotParser):\n     def __init__(self, robotstxt_body, spider):\n+        warn(\"ReppyRobotParser is deprecated.\", ScrapyDeprecationWarning, stacklevel=2)\n         from reppy.robots import Robots\n \n         self.spider = spider\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#38dbd4399361d6e3ac14cda40f4aee8d04204346", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 109 | Files Changed: 2 | Hunks: 2 | Methods Changed: 9 | Complexity Δ (Sum/Max): -13/0 | Churn Δ: 109 | Churn Cumulative: 8264 | Contributors (this commit): 51 | Commits (past 90d): 14 | Contributors (cumulative): 63 | DMM Complexity: 0.0\n\nDIFF:\n@@ -371,15 +371,3 @@ class HttpCompressionTest(TestCase):\n         self.assertEqual(response.body, b\"\")\n         self.assertStatsEqual(\"httpcompression/response_count\", None)\n         self.assertStatsEqual(\"httpcompression/response_bytes\", None)\n-\n-\n-class HttpCompressionSubclassTest(TestCase):\n-    def test_init_missing_stats(self):\n-        class HttpCompressionMiddlewareSubclass(HttpCompressionMiddleware):\n-            def __init__(self):\n-                super().__init__()\n-\n-        crawler = get_crawler(Spider)\n-\n-        with self.assertRaises(TypeError):\n-            HttpCompressionMiddlewareSubclass.from_crawler(crawler)\n\n@@ -2835,103 +2835,6 @@ class FeedExportInitTest(unittest.TestCase):\n         self.assertIsInstance(exporter, FeedExporter)\n \n \n-class StdoutFeedStorageWithoutFeedOptions(StdoutFeedStorage):\n-    def __init__(self, uri):\n-        super().__init__(uri)\n-\n-\n-class StdoutFeedStoragePreFeedOptionsTest(unittest.TestCase):\n-    \"\"\"Make sure that any feed exporter created by users before the\n-    introduction of the ``feed_options`` parameter continues to work as\n-    expected, and simply issues a warning.\"\"\"\n-\n-    def test_init(self):\n-        settings_dict = {\n-            \"FEED_URI\": \"file:///tmp/foobar\",\n-            \"FEED_STORAGES\": {\"file\": StdoutFeedStorageWithoutFeedOptions},\n-        }\n-        with pytest.raises(TypeError):\n-            get_crawler(settings_dict=settings_dict)\n-\n-\n-class FileFeedStorageWithoutFeedOptions(FileFeedStorage):\n-    def __init__(self, uri):\n-        super().__init__(uri)\n-\n-\n-class FileFeedStoragePreFeedOptionsTest(unittest.TestCase):\n-    maxDiff = None\n-\n-    def test_init(self):\n-        with tempfile.NamedTemporaryFile() as temp:\n-            settings_dict = {\n-                \"FEED_URI\": f\"file:///{temp.name}\",\n-                \"FEED_STORAGES\": {\"file\": FileFeedStorageWithoutFeedOptions},\n-            }\n-            with self.assertRaises(TypeError):\n-                get_crawler(settings_dict=settings_dict)\n-\n-\n-class S3FeedStorageWithoutFeedOptions(S3FeedStorage):\n-    def __init__(self, uri, access_key, secret_key, acl, endpoint_url, **kwargs):\n-        super().__init__(uri, access_key, secret_key, acl, endpoint_url, **kwargs)\n-\n-\n-class S3FeedStorageWithoutFeedOptionsWithFromCrawler(S3FeedStorage):\n-    @classmethod\n-    def from_crawler(cls, crawler, uri):\n-        return super().from_crawler(crawler, uri)\n-\n-\n-class S3FeedStoragePreFeedOptionsTest(unittest.TestCase):\n-    maxDiff = None\n-\n-    def setUp(self):\n-        skip_if_no_boto()\n-\n-    def test_init(self):\n-        settings_dict = {\n-            \"FEED_URI\": \"file:///tmp/foobar\",\n-            \"FEED_STORAGES\": {\"file\": S3FeedStorageWithoutFeedOptions},\n-        }\n-        with pytest.warns(\n-            ScrapyDeprecationWarning,\n-            match=\"The `FEED_URI` and `FEED_FORMAT` settings have been deprecated\",\n-        ):\n-            crawler = get_crawler(settings_dict=settings_dict)\n-            feed_exporter = FeedExporter.from_crawler(crawler)\n-\n-        spider = scrapy.Spider(\"default\")\n-        spider.crawler = crawler\n-\n-        feed_exporter.open_spider(spider)\n-\n-    def test_from_crawler(self):\n-        settings_dict = {\n-            \"FEED_URI\": \"file:///tmp/foobar\",\n-            \"FEED_STORAGES\": {\"file\": S3FeedStorageWithoutFeedOptionsWithFromCrawler},\n-        }\n-        with pytest.raises(TypeError):\n-            get_crawler(settings_dict=settings_dict)\n-\n-\n-class FTPFeedStorageWithoutFeedOptions(FTPFeedStorage):\n-    def __init__(self, uri, use_active_mode=False):\n-        super().__init__(uri)\n-\n-\n-class FTPFeedStoragePreFeedOptionsTest(unittest.TestCase):\n-    maxDiff = None\n-\n-    def test_init(self):\n-        settings_dict = {\n-            \"FEED_URI\": \"ftp://localhost/foo\",\n-            \"FEED_STORAGES\": {\"ftp\": FTPFeedStorageWithoutFeedOptions},\n-        }\n-        with pytest.raises(TypeError):\n-            get_crawler(settings_dict=settings_dict)\n-\n-\n class URIParamsTest:\n     spider_name = \"uri_params_spider\"\n     deprecated_options = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cc9c415bf3312e2c23288dfeb5f977a00874d0a2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 791 | Contributors (this commit): 12 | Commits (past 90d): 4 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import os\n import sys\n from io import BytesIO\n from pathlib import Path\n@@ -147,8 +148,10 @@ class InteractiveShellTest(unittest.TestCase):\n             \"scrapy.cmdline\",\n             \"shell\",\n         )\n+        env = os.environ.copy()\n+        env[\"SCRAPY_PYTHON_SHELL\"] = \"python\"\n         logfile = BytesIO()\n-        p = PopenSpawn(args, timeout=5)\n+        p = PopenSpawn(args, env=env, timeout=5)\n         p.logfile_read = logfile\n         p.expect_exact(\"Available Scrapy objects\")\n         with MockServer() as mockserver:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6b0c18e921c046d1c14106c629243e96d6cd60f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 1759 | Contributors (this commit): 22 | Commits (past 90d): 15 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -534,7 +534,10 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         p.expect_exact(\"Spider closed (shutdown)\")\n         p.wait()\n \n+    @defer.inlineCallbacks\n     def test_shutdown_forced(self):\n+        from twisted.internet import reactor\n+\n         sig = signal.SIGINT if sys.platform != \"win32\" else signal.SIGBREAK\n         args = self.get_script_args(\"sleeping.py\", \"-a\", \"sleep=10\")\n         p = PopenSpawn(args, timeout=5)\n@@ -542,6 +545,10 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         p.expect_exact(\"Crawled (200)\")\n         p.kill(sig)\n         p.expect_exact(\"shutting down gracefully\")\n+        # sending the second signal too fast often causes problems\n+        d = defer.Deferred()\n+        reactor.callLater(0.1, d.callback, None)\n+        yield d\n         p.kill(sig)\n         p.expect_exact(\"forcing unclean shutdown\")\n         p.wait()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1f797d0fdb2615af31a125e343ebbae664d4b238", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 87 | Files Changed: 4 | Hunks: 14 | Methods Changed: 4 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 88 | Churn Cumulative: 1629 | Contributors (this commit): 30 | Commits (past 90d): 9 | Contributors (cumulative): 55 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2,7 +2,6 @@ from __future__ import annotations\n \n import logging\n import sys\n-import warnings\n from logging.config import dictConfig\n from types import TracebackType\n from typing import TYPE_CHECKING, Any, List, Optional, Tuple, Type, Union, cast\n@@ -11,7 +10,6 @@ from twisted.python import log as twisted_log\n from twisted.python.failure import Failure\n \n import scrapy\n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.settings import Settings\n from scrapy.utils.versions import scrapy_components_versions\n \n@@ -232,18 +230,9 @@ def logformatter_adapter(logkws: dict) -> Tuple[int, str, dict]:\n     and adapts it into a tuple of positional arguments for logger.log calls,\n     handling backward compatibility as well.\n     \"\"\"\n-    if not {\"level\", \"msg\", \"args\"} <= set(logkws):\n-        warnings.warn(\"Missing keys in LogFormatter method\", ScrapyDeprecationWarning)\n-\n-    if \"format\" in logkws:\n-        warnings.warn(\n-            \"`format` key in LogFormatter methods has been \"\n-            \"deprecated, use `msg` instead\",\n-            ScrapyDeprecationWarning,\n-        )\n \n     level = logkws.get(\"level\", logging.INFO)\n-    message = logkws.get(\"format\", logkws.get(\"msg\"))\n+    message = logkws.get(\"msg\") or \"\"\n     # NOTE: This also handles 'args' being an empty dict, that case doesn't\n     # play well in logger.log calls\n     args = logkws if not logkws.get(\"args\") else logkws[\"args\"]\n\n@@ -14,7 +14,6 @@ from w3lib import html\n \n import scrapy\n from scrapy.http.response import Response\n-from scrapy.utils.decorators import deprecated\n from scrapy.utils.python import to_bytes, to_unicode\n \n _baseurl_cache: \"WeakKeyDictionary[Response, str]\" = WeakKeyDictionary()\n@@ -55,25 +54,6 @@ def response_status_message(status: Union[bytes, float, int, str]) -> str:\n     return f\"{status_int} {to_unicode(message)}\"\n \n \n-@deprecated\n-def response_httprepr(response: Response) -> bytes:\n-    \"\"\"Return raw HTTP representation (as bytes) of the given response. This\n-    is provided only for reference, since it's not the exact stream of bytes\n-    that was received (that's not exposed by Twisted).\n-    \"\"\"\n-    values = [\n-        b\"HTTP/1.1 \",\n-        to_bytes(str(response.status)),\n-        b\" \",\n-        to_bytes(http.RESPONSES.get(response.status, b\"\")),\n-        b\"\\r\\n\",\n-    ]\n-    if response.headers:\n-        values.extend([response.headers.to_string(), b\"\\r\\n\"])\n-    values.extend([b\"\\r\\n\", response.body])\n-    return b\"\".join(values)\n-\n-\n def open_in_browser(\n     response: Union[\n         \"scrapy.http.response.html.HtmlResponse\",\n\n@@ -1,12 +1,8 @@\n-import warnings\n-from itertools import product\n from unittest import TestCase\n \n from scrapy.downloadermiddlewares.stats import DownloaderStats\n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request, Response\n from scrapy.spiders import Spider\n-from scrapy.utils.response import response_httprepr\n from scrapy.utils.test import get_crawler\n \n \n@@ -40,25 +36,6 @@ class TestDownloaderStats(TestCase):\n         self.mw.process_response(self.req, self.res, self.spider)\n         self.assertStatsEqual(\"downloader/response_count\", 1)\n \n-    def test_response_len(self):\n-        body = (b\"\", b\"not_empty\")  # empty/notempty body\n-        headers = (\n-            {},\n-            {\"lang\": \"en\"},\n-            {\"lang\": \"en\", \"User-Agent\": \"scrapy\"},\n-        )  # 0 headers, 1h and 2h\n-        test_responses = [  # form test responses with all combinations of body/headers\n-            Response(url=\"scrapytest.org\", status=200, body=r[0], headers=r[1])\n-            for r in product(body, headers)\n-        ]\n-        for test_response in test_responses:\n-            self.crawler.stats.set_value(\"downloader/response_bytes\", 0)\n-            self.mw.process_response(self.req, test_response, self.spider)\n-            with warnings.catch_warnings():\n-                warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n-                resp_size = len(response_httprepr(test_response))\n-            self.assertStatsEqual(\"downloader/response_bytes\", resp_size)\n-\n     def test_process_exception(self):\n         self.mw.process_exception(self.req, MyException(), self.spider)\n         self.assertStatsEqual(\"downloader/exception_count\", 1)\n\n@@ -1,16 +1,13 @@\n import unittest\n-import warnings\n from pathlib import Path\n from urllib.parse import urlparse\n \n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import HtmlResponse, Response, TextResponse\n from scrapy.utils.python import to_bytes\n from scrapy.utils.response import (\n     get_base_url,\n     get_meta_refresh,\n     open_in_browser,\n-    response_httprepr,\n     response_status_message,\n )\n \n@@ -20,35 +17,6 @@ __doctests__ = [\"scrapy.utils.response\"]\n class ResponseUtilsTest(unittest.TestCase):\n     dummy_response = TextResponse(url=\"http://example.org/\", body=b\"dummy_response\")\n \n-    def test_response_httprepr(self):\n-        with warnings.catch_warnings():\n-            warnings.simplefilter(\"ignore\", ScrapyDeprecationWarning)\n-\n-            r1 = Response(\"http://www.example.com\")\n-            self.assertEqual(response_httprepr(r1), b\"HTTP/1.1 200 OK\\r\\n\\r\\n\")\n-\n-            r1 = Response(\n-                \"http://www.example.com\",\n-                status=404,\n-                headers={\"Content-type\": \"text/html\"},\n-                body=b\"Some body\",\n-            )\n-            self.assertEqual(\n-                response_httprepr(r1),\n-                b\"HTTP/1.1 404 Not Found\\r\\nContent-Type: text/html\\r\\n\\r\\nSome body\",\n-            )\n-\n-            r1 = Response(\n-                \"http://www.example.com\",\n-                status=6666,\n-                headers={\"Content-type\": \"text/html\"},\n-                body=b\"Some body\",\n-            )\n-            self.assertEqual(\n-                response_httprepr(r1),\n-                b\"HTTP/1.1 6666 \\r\\nContent-Type: text/html\\r\\n\\r\\nSome body\",\n-            )\n-\n     def test_open_in_browser(self):\n         url = \"http:///www.example.com/some/page.html\"\n         body = b\"<html> <head> <title>test page</title> </head> <body>test body</body> </html>\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8dff9633d0b80aff0c6f481e395755f3c8985f27", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 324 | Contributors (this commit): 20 | Commits (past 90d): 4 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -96,6 +96,16 @@ IGNORED_EXTENSIONS = [\n     \"dmg\",\n     \"iso\",\n     \"apk\",\n+    \"jar\",\n+    \"sh\",\n+    \"rb\",\n+    \"js\",\n+    \"hta\",\n+    \"bat\",\n+    \"cpl\",\n+    \"msi\",\n+    \"msp\",\n+    \"py\",\n ]\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#04024f1e796f99e77dda544396722fb9203f1d49", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 1934 | Contributors (this commit): 71 | Commits (past 90d): 5 | Contributors (cumulative): 92 | DMM Complexity: 1.0\n\nDIFF:\n@@ -352,7 +352,7 @@ class Scheduler(BaseScheduler):\n \n     def _dqdir(self, jobdir: Optional[str]) -> Optional[str]:\n         \"\"\"Return a folder name to keep disk queue state at\"\"\"\n-        if jobdir is not None:\n+        if jobdir:\n             dqdir = Path(jobdir, \"requests.queue\")\n             if not dqdir.exists():\n                 dqdir.mkdir(parents=True)\n\n@@ -206,6 +206,8 @@ ITEM_PROCESSOR = \"scrapy.pipelines.ItemPipelineManager\"\n ITEM_PIPELINES = {}\n ITEM_PIPELINES_BASE = {}\n \n+JOBDIR = None\n+\n LOG_ENABLED = True\n LOG_ENCODING = \"utf-8\"\n LOG_FORMATTER = \"scrapy.logformatter.LogFormatter\"\n\n@@ -5,7 +5,9 @@ from scrapy.settings import BaseSettings\n \n \n def job_dir(settings: BaseSettings) -> Optional[str]:\n-    path: str = settings[\"JOBDIR\"]\n-    if path and not Path(path).exists():\n+    path: Optional[str] = settings[\"JOBDIR\"]\n+    if not path:\n+        return None\n+    if not Path(path).exists():\n         Path(path).mkdir(parents=True)\n     return path\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#732557e6988dd3d1df26f973e3ae651f7bcbdc5b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 55 | Lines Deleted: 38 | Files Changed: 5 | Hunks: 33 | Methods Changed: 28 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 93 | Churn Cumulative: 2302 | Contributors (this commit): 46 | Commits (past 90d): 8 | Contributors (cumulative): 76 | DMM Complexity: 0.7391304347826086\n\nDIFF:\n@@ -1,5 +1,8 @@\n-def obsolete_setter(setter, attrname):\n-    def newsetter(self, value):\n+from typing import Any, Callable, NoReturn\n+\n+\n+def obsolete_setter(setter: Callable, attrname: str) -> Callable[[Any, Any], NoReturn]:\n+    def newsetter(self: Any, value: Any) -> NoReturn:\n         c = self.__class__.__name__\n         msg = f\"{c}.{attrname} is not modifiable, use {c}.replace() instead\"\n         raise AttributeError(msg)\n\n@@ -9,14 +9,17 @@ from typing import (\n     Any,\n     AnyStr,\n     Callable,\n+    Dict,\n     Iterable,\n     List,\n     Mapping,\n+    NoReturn,\n     Optional,\n     Tuple,\n     Type,\n     TypeVar,\n     Union,\n+    cast,\n )\n \n from w3lib.url import safe_url_string\n@@ -32,7 +35,7 @@ from scrapy.utils.url import escape_ajax\n RequestTypeVar = TypeVar(\"RequestTypeVar\", bound=\"Request\")\n \n \n-def NO_CALLBACK(*args, **kwargs):\n+def NO_CALLBACK(*args: Any, **kwargs: Any) -> NoReturn:\n     \"\"\"When assigned to the ``callback`` parameter of\n     :class:`~scrapy.http.Request`, it indicates that the request is not meant\n     to have a spider callback at all.\n@@ -92,21 +95,21 @@ class Request(object_ref):\n         headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n         body: Optional[Union[bytes, str]] = None,\n         cookies: Optional[Union[dict, List[dict]]] = None,\n-        meta: Optional[dict] = None,\n+        meta: Optional[Dict[str, Any]] = None,\n         encoding: str = \"utf-8\",\n         priority: int = 0,\n         dont_filter: bool = False,\n         errback: Optional[Callable] = None,\n         flags: Optional[List[str]] = None,\n-        cb_kwargs: Optional[dict] = None,\n+        cb_kwargs: Optional[Dict[str, Any]] = None,\n     ) -> None:\n-        self._encoding = encoding  # this one has to be set first\n-        self.method = str(method).upper()\n+        self._encoding: str = encoding  # this one has to be set first\n+        self.method: str = str(method).upper()\n         self._set_url(url)\n         self._set_body(body)\n         if not isinstance(priority, int):\n             raise TypeError(f\"Request priority not an integer: {priority!r}\")\n-        self.priority = priority\n+        self.priority: int = priority\n \n         if not (callable(callback) or callback is None):\n             raise TypeError(\n@@ -114,25 +117,27 @@ class Request(object_ref):\n             )\n         if not (callable(errback) or errback is None):\n             raise TypeError(f\"errback must be a callable, got {type(errback).__name__}\")\n-        self.callback = callback\n-        self.errback = errback\n+        self.callback: Optional[Callable] = callback\n+        self.errback: Optional[Callable] = errback\n \n-        self.cookies = cookies or {}\n-        self.headers = Headers(headers or {}, encoding=encoding)\n-        self.dont_filter = dont_filter\n+        self.cookies: Union[dict, List[dict]] = cookies or {}\n+        self.headers: Headers = Headers(headers or {}, encoding=encoding)\n+        self.dont_filter: bool = dont_filter\n \n-        self._meta = dict(meta) if meta else None\n-        self._cb_kwargs = dict(cb_kwargs) if cb_kwargs else None\n-        self.flags = [] if flags is None else list(flags)\n+        self._meta: Optional[Dict[str, Any]] = dict(meta) if meta else None\n+        self._cb_kwargs: Optional[Dict[str, Any]] = (\n+            dict(cb_kwargs) if cb_kwargs else None\n+        )\n+        self.flags: List[str] = [] if flags is None else list(flags)\n \n     @property\n-    def cb_kwargs(self) -> dict:\n+    def cb_kwargs(self) -> Dict[str, Any]:\n         if self._cb_kwargs is None:\n             self._cb_kwargs = {}\n         return self._cb_kwargs\n \n     @property\n-    def meta(self) -> dict:\n+    def meta(self) -> Dict[str, Any]:\n         if self._meta is None:\n             self._meta = {}\n         return self._meta\n@@ -174,19 +179,19 @@ class Request(object_ref):\n     def copy(self) -> \"Request\":\n         return self.replace()\n \n-    def replace(self, *args, **kwargs) -> \"Request\":\n+    def replace(self, *args: Any, **kwargs: Any) -> \"Request\":\n         \"\"\"Create a new Request with the same attributes except for those given new values\"\"\"\n         for x in self.attributes:\n             kwargs.setdefault(x, getattr(self, x))\n         cls = kwargs.pop(\"cls\", self.__class__)\n-        return cls(*args, **kwargs)\n+        return cast(Request, cls(*args, **kwargs))\n \n     @classmethod\n     def from_curl(\n         cls: Type[RequestTypeVar],\n         curl_command: str,\n         ignore_unknown_options: bool = True,\n-        **kwargs,\n+        **kwargs: Any,\n     ) -> RequestTypeVar:\n         \"\"\"Create a Request object from a string containing a `cURL\n         <https://curl.haxx.se/>`_ command. It populates the HTTP method, the\n@@ -219,7 +224,7 @@ class Request(object_ref):\n         request_kwargs.update(kwargs)\n         return cls(**request_kwargs)\n \n-    def to_dict(self, *, spider: Optional[\"scrapy.Spider\"] = None) -> dict:\n+    def to_dict(self, *, spider: Optional[\"scrapy.Spider\"] = None) -> Dict[str, Any]:\n         \"\"\"Return a dictionary containing the Request's data.\n \n         Use :func:`~scrapy.utils.request.request_from_dict` to convert back into a :class:`~scrapy.Request` object.\n@@ -244,7 +249,7 @@ class Request(object_ref):\n         return d\n \n \n-def _find_method(obj, func):\n+def _find_method(obj: Any, func: Callable) -> str:\n     \"\"\"Helper function for Request.to_dict\"\"\"\n     # Only instance methods contain ``__func__``\n     if obj and hasattr(func, \"__func__\"):\n\n@@ -5,7 +5,9 @@ This module implements the FormRequest class which is a more convenient class\n See documentation in docs/topics/request-response.rst\n \"\"\"\n \n-from typing import Iterable, List, Optional, Tuple, Type, TypeVar, Union, cast\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, Any, Iterable, List, Optional, Tuple, Union, cast\n from urllib.parse import urlencode, urljoin, urlsplit, urlunsplit\n \n from lxml.html import (\n@@ -24,7 +26,10 @@ from scrapy.http.response.text import TextResponse\n from scrapy.utils.python import is_listlike, to_bytes\n from scrapy.utils.response import get_base_url\n \n-FormRequestTypeVar = TypeVar(\"FormRequestTypeVar\", bound=\"FormRequest\")\n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n \n FormdataKVType = Tuple[str, Union[str, Iterable[str]]]\n FormdataType = Optional[Union[dict, List[FormdataKVType]]]\n@@ -33,7 +38,9 @@ FormdataType = Optional[Union[dict, List[FormdataKVType]]]\n class FormRequest(Request):\n     valid_form_methods = [\"GET\", \"POST\"]\n \n-    def __init__(self, *args, formdata: FormdataType = None, **kwargs) -> None:\n+    def __init__(\n+        self, *args: Any, formdata: FormdataType = None, **kwargs: Any\n+    ) -> None:\n         if formdata and kwargs.get(\"method\") is None:\n             kwargs[\"method\"] = \"POST\"\n \n@@ -54,7 +61,7 @@ class FormRequest(Request):\n \n     @classmethod\n     def from_response(\n-        cls: Type[FormRequestTypeVar],\n+        cls,\n         response: TextResponse,\n         formname: Optional[str] = None,\n         formid: Optional[str] = None,\n@@ -64,8 +71,8 @@ class FormRequest(Request):\n         dont_click: bool = False,\n         formxpath: Optional[str] = None,\n         formcss: Optional[str] = None,\n-        **kwargs,\n-    ) -> FormRequestTypeVar:\n+        **kwargs: Any,\n+    ) -> Self:\n         kwargs.setdefault(\"encoding\", response.encoding)\n \n         if formcss is not None:\n@@ -121,12 +128,12 @@ def _get_form(\n     if formname is not None:\n         f = root.xpath(f'//form[@name=\"{formname}\"]')\n         if f:\n-            return f[0]\n+            return cast(FormElement, f[0])\n \n     if formid is not None:\n         f = root.xpath(f'//form[@id=\"{formid}\"]')\n         if f:\n-            return f[0]\n+            return cast(FormElement, f[0])\n \n     # Get form element from xpath, if not found, go up\n     if formxpath is not None:\n@@ -135,7 +142,7 @@ def _get_form(\n             el = nodes[0]\n             while True:\n                 if el.tag == \"form\":\n-                    return el\n+                    return cast(FormElement, el)\n                 el = el.getparent()\n                 if el is None:\n                     break\n@@ -147,7 +154,7 @@ def _get_form(\n     except IndexError:\n         raise IndexError(f\"Form number {formnumber} not found in {response}\")\n     else:\n-        return form\n+        return cast(FormElement, form)\n \n \n def _get_inputs(\n\n@@ -8,7 +8,7 @@ See documentation in docs/topics/request-response.rst\n import copy\n import json\n import warnings\n-from typing import Optional, Tuple\n+from typing import Any, Optional, Tuple\n \n from scrapy.http.request import Request\n \n@@ -16,7 +16,9 @@ from scrapy.http.request import Request\n class JsonRequest(Request):\n     attributes: Tuple[str, ...] = Request.attributes + (\"dumps_kwargs\",)\n \n-    def __init__(self, *args, dumps_kwargs: Optional[dict] = None, **kwargs) -> None:\n+    def __init__(\n+        self, *args: Any, dumps_kwargs: Optional[dict] = None, **kwargs: Any\n+    ) -> None:\n         dumps_kwargs = copy.deepcopy(dumps_kwargs) if dumps_kwargs is not None else {}\n         dumps_kwargs.setdefault(\"sort_keys\", True)\n         self._dumps_kwargs = dumps_kwargs\n@@ -42,7 +44,7 @@ class JsonRequest(Request):\n     def dumps_kwargs(self) -> dict:\n         return self._dumps_kwargs\n \n-    def replace(self, *args, **kwargs) -> Request:\n+    def replace(self, *args: Any, **kwargs: Any) -> Request:\n         body_passed = kwargs.get(\"body\", None) is not None\n         data = kwargs.pop(\"data\", None)\n         data_passed = data is not None\n\n@@ -5,7 +5,7 @@ This module implements the XmlRpcRequest class which is a more convenient class\n See documentation in docs/topics/request-response.rst\n \"\"\"\n import xmlrpc.client as xmlrpclib\n-from typing import Optional\n+from typing import Any, Optional\n \n from scrapy.http.request import Request\n from scrapy.utils.python import get_func_args\n@@ -14,7 +14,7 @@ DUMPS_ARGS = get_func_args(xmlrpclib.dumps)\n \n \n class XmlRpcRequest(Request):\n-    def __init__(self, *args, encoding: Optional[str] = None, **kwargs):\n+    def __init__(self, *args: Any, encoding: Optional[str] = None, **kwargs: Any):\n         if \"body\" not in kwargs and \"params\" in kwargs:\n             kw = dict((k, kwargs.pop(k)) for k in DUMPS_ARGS if k in kwargs)\n             kwargs[\"body\"] = xmlrpclib.dumps(**kw)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4cb2fc2c3b9deba49ae3a32400819ed95ea262c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 185 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,6 @@\n from typing import Any, Optional\n \n-import OpenSSL._util as pyOpenSSLutil\n+import OpenSSL._util as pyOpenSSLutil  # type: ignore[import-untyped]\n import OpenSSL.SSL\n import OpenSSL.version\n from OpenSSL.crypto import X509Name\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#01d9d28324cbd5c7edff0d25ba00d8491400698a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 98 | Lines Deleted: 71 | Files Changed: 5 | Hunks: 40 | Methods Changed: 38 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 169 | Churn Cumulative: 4718 | Contributors (this commit): 42 | Commits (past 90d): 13 | Contributors (cumulative): 80 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,9 @@\n+from typing import Any, Dict\n+\n from w3lib.url import parse_data_uri\n \n-from scrapy.http import TextResponse\n+from scrapy import Request, Spider\n+from scrapy.http import Response, TextResponse\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.decorators import defers\n \n@@ -9,11 +12,11 @@ class DataURIDownloadHandler:\n     lazy = False\n \n     @defers\n-    def download_request(self, request, spider):\n+    def download_request(self, request: Request, spider: Spider) -> Response:\n         uri = parse_data_uri(request.url)\n         respcls = responsetypes.from_mimetype(uri.media_type)\n \n-        resp_kwargs = {}\n+        resp_kwargs: Dict[str, Any] = {}\n         if issubclass(respcls, TextResponse) and uri.media_type.split(\"/\")[0] == \"text\":\n             charset = uri.media_type_parameters.get(\"charset\")\n             resp_kwargs[\"encoding\"] = charset\n\n@@ -206,6 +206,7 @@ class Scraper:\n         if isinstance(result, Response):\n             if getattr(result, \"request\", None) is None:\n                 result.request = request\n+            assert result.request\n             callback = result.request.callback or spider._parse\n             warn_on_generator_with_return_value(spider, callback)\n             dfd = defer_succeed(result)\n\n@@ -4,9 +4,28 @@ responses in Scrapy.\n \n See documentation in docs/topics/request-response.rst\n \"\"\"\n-from typing import Any, AnyStr, Generator, Iterable, Mapping, Tuple, Union\n+from __future__ import annotations\n+\n+from ipaddress import IPv4Address, IPv6Address\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    AnyStr,\n+    Callable,\n+    Dict,\n+    Generator,\n+    Iterable,\n+    List,\n+    Mapping,\n+    Optional,\n+    Tuple,\n+    Union,\n+    cast,\n+)\n from urllib.parse import urljoin\n \n+from twisted.internet.ssl import Certificate\n+\n from scrapy.exceptions import NotSupported\n from scrapy.http.common import obsolete_setter\n from scrapy.http.headers import Headers\n@@ -14,6 +33,9 @@ from scrapy.http.request import Request\n from scrapy.link import Link\n from scrapy.utils.trackref import object_ref\n \n+if TYPE_CHECKING:\n+    from scrapy.selector import SelectorList\n+\n \n class Response(object_ref):\n     \"\"\"An object that represents an HTTP response, which is usually\n@@ -41,29 +63,29 @@ class Response(object_ref):\n     def __init__(\n         self,\n         url: str,\n-        status=200,\n+        status: int = 200,\n         headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n-        body=b\"\",\n-        flags=None,\n-        request=None,\n-        certificate=None,\n-        ip_address=None,\n-        protocol=None,\n+        body: bytes = b\"\",\n+        flags: Optional[List[str]] = None,\n+        request: Optional[Request] = None,\n+        certificate: Optional[Certificate] = None,\n+        ip_address: Union[IPv4Address, IPv6Address, None] = None,\n+        protocol: Optional[str] = None,\n     ):\n-        self.headers = Headers(headers or {})\n-        self.status = int(status)\n+        self.headers: Headers = Headers(headers or {})\n+        self.status: int = int(status)\n         self._set_body(body)\n         self._set_url(url)\n-        self.request = request\n-        self.flags = [] if flags is None else list(flags)\n-        self.certificate = certificate\n-        self.ip_address = ip_address\n-        self.protocol = protocol\n+        self.request: Optional[Request] = request\n+        self.flags: List[str] = [] if flags is None else list(flags)\n+        self.certificate: Optional[Certificate] = certificate\n+        self.ip_address: Union[IPv4Address, IPv6Address, None] = ip_address\n+        self.protocol: Optional[str] = protocol\n \n     @property\n-    def cb_kwargs(self):\n+    def cb_kwargs(self) -> Dict[str, Any]:\n         try:\n-            return self.request.cb_kwargs\n+            return self.request.cb_kwargs  # type: ignore[union-attr]\n         except AttributeError:\n             raise AttributeError(\n                 \"Response.cb_kwargs not available, this response \"\n@@ -71,21 +93,21 @@ class Response(object_ref):\n             )\n \n     @property\n-    def meta(self):\n+    def meta(self) -> Dict[str, Any]:\n         try:\n-            return self.request.meta\n+            return self.request.meta  # type: ignore[union-attr]\n         except AttributeError:\n             raise AttributeError(\n                 \"Response.meta not available, this response \"\n                 \"is not tied to any request\"\n             )\n \n-    def _get_url(self):\n+    def _get_url(self) -> str:\n         return self._url\n \n-    def _set_url(self, url: str):\n+    def _set_url(self, url: str) -> None:\n         if isinstance(url, str):\n-            self._url = url\n+            self._url: str = url\n         else:\n             raise TypeError(\n                 f\"{type(self).__name__} url must be str, \" f\"got {type(url).__name__}\"\n@@ -93,10 +115,10 @@ class Response(object_ref):\n \n     url = property(_get_url, obsolete_setter(_set_url, \"url\"))\n \n-    def _get_body(self):\n+    def _get_body(self) -> bytes:\n         return self._body\n \n-    def _set_body(self, body):\n+    def _set_body(self, body: Optional[bytes]) -> None:\n         if body is None:\n             self._body = b\"\"\n         elif not isinstance(body, bytes):\n@@ -110,45 +132,45 @@ class Response(object_ref):\n \n     body = property(_get_body, obsolete_setter(_set_body, \"body\"))\n \n-    def __repr__(self):\n+    def __repr__(self) -> str:\n         return f\"<{self.status} {self.url}>\"\n \n-    def copy(self):\n+    def copy(self) -> Response:\n         \"\"\"Return a copy of this Response\"\"\"\n         return self.replace()\n \n-    def replace(self, *args, **kwargs):\n+    def replace(self, *args: Any, **kwargs: Any) -> Response:\n         \"\"\"Create a new Response with the same attributes except for those given new values\"\"\"\n         for x in self.attributes:\n             kwargs.setdefault(x, getattr(self, x))\n         cls = kwargs.pop(\"cls\", self.__class__)\n-        return cls(*args, **kwargs)\n+        return cast(Response, cls(*args, **kwargs))\n \n-    def urljoin(self, url):\n+    def urljoin(self, url: str) -> str:\n         \"\"\"Join this Response's url with a possible relative url to form an\n         absolute interpretation of the latter.\"\"\"\n-        return urljoin(self.url, url)\n+        return urljoin(cast(str, self.url), url)\n \n     @property\n-    def text(self):\n+    def text(self) -> str:\n         \"\"\"For subclasses of TextResponse, this will return the body\n         as str\n         \"\"\"\n         raise AttributeError(\"Response content isn't text\")\n \n-    def css(self, *a, **kw):\n+    def css(self, *a: Any, **kw: Any) -> SelectorList:\n         \"\"\"Shortcut method implemented only by responses whose content\n         is text (subclasses of TextResponse).\n         \"\"\"\n         raise NotSupported(\"Response content isn't text\")\n \n-    def jmespath(self, *a, **kw):\n+    def jmespath(self, *a: Any, **kw: Any) -> SelectorList:\n         \"\"\"Shortcut method implemented only by responses whose content\n         is text (subclasses of TextResponse).\n         \"\"\"\n         raise NotSupported(\"Response content isn't text\")\n \n-    def xpath(self, *a, **kw):\n+    def xpath(self, *a: Any, **kw: Any) -> SelectorList:\n         \"\"\"Shortcut method implemented only by responses whose content\n         is text (subclasses of TextResponse).\n         \"\"\"\n@@ -156,19 +178,19 @@ class Response(object_ref):\n \n     def follow(\n         self,\n-        url,\n-        callback=None,\n-        method=\"GET\",\n-        headers=None,\n-        body=None,\n-        cookies=None,\n-        meta=None,\n-        encoding=\"utf-8\",\n-        priority=0,\n-        dont_filter=False,\n-        errback=None,\n-        cb_kwargs=None,\n-        flags=None,\n+        url: Union[str, Link],\n+        callback: Optional[Callable] = None,\n+        method: str = \"GET\",\n+        headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n+        body: Optional[Union[bytes, str]] = None,\n+        cookies: Optional[Union[dict, List[dict]]] = None,\n+        meta: Optional[Dict[str, Any]] = None,\n+        encoding: str = \"utf-8\",\n+        priority: int = 0,\n+        dont_filter: bool = False,\n+        errback: Optional[Callable] = None,\n+        cb_kwargs: Optional[Dict[str, Any]] = None,\n+        flags: Optional[List[str]] = None,\n     ) -> Request:\n         \"\"\"\n         Return a :class:`~.Request` instance to follow a link ``url``.\n@@ -207,19 +229,19 @@ class Response(object_ref):\n \n     def follow_all(\n         self,\n-        urls,\n-        callback=None,\n-        method=\"GET\",\n-        headers=None,\n-        body=None,\n-        cookies=None,\n-        meta=None,\n-        encoding=\"utf-8\",\n-        priority=0,\n-        dont_filter=False,\n-        errback=None,\n-        cb_kwargs=None,\n-        flags=None,\n+        urls: Iterable[Union[str, Link]],\n+        callback: Optional[Callable] = None,\n+        method: str = \"GET\",\n+        headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n+        body: Optional[Union[bytes, str]] = None,\n+        cookies: Optional[Union[dict, List[dict]]] = None,\n+        meta: Optional[Dict[str, Any]] = None,\n+        encoding: str = \"utf-8\",\n+        priority: int = 0,\n+        dont_filter: bool = False,\n+        errback: Optional[Callable] = None,\n+        cb_kwargs: Optional[Dict[str, Any]] = None,\n+        flags: Optional[List[str]] = None,\n     ) -> Generator[Request, None, None]:\n         \"\"\"\n         .. versionadded:: 2.0\n\n@@ -27,7 +27,7 @@ from scrapy.utils.python import memoizemethod_noargs, to_unicode\n from scrapy.utils.response import get_base_url\n \n if TYPE_CHECKING:\n-    from scrapy.selector import Selector\n+    from scrapy.selector import Selector, SelectorList\n \n _NONE = object()\n \n@@ -138,26 +138,26 @@ class TextResponse(Response):\n         return read_bom(self.body)[0]\n \n     @property\n-    def selector(self):\n+    def selector(self) -> Selector:\n         from scrapy.selector import Selector\n \n         if self._cached_selector is None:\n             self._cached_selector = Selector(self)\n         return self._cached_selector\n \n-    def jmespath(self, query, **kwargs):\n+    def jmespath(self, query: str, **kwargs: Any) -> SelectorList:\n         if not hasattr(self.selector, \"jmespath\"):  # type: ignore[attr-defined]\n             raise AttributeError(\n                 \"Please install parsel >= 1.8.1 to get jmespath support\"\n             )\n \n-        return self.selector.jmespath(query, **kwargs)  # type: ignore[attr-defined]\n+        return cast(SelectorList, self.selector.jmespath(query, **kwargs))  # type: ignore[attr-defined]\n \n-    def xpath(self, query, **kwargs):\n-        return self.selector.xpath(query, **kwargs)\n+    def xpath(self, query: str, **kwargs: Any) -> SelectorList:\n+        return cast(SelectorList, self.selector.xpath(query, **kwargs))\n \n-    def css(self, query):\n-        return self.selector.css(query)\n+    def css(self, query: str) -> SelectorList:\n+        return cast(SelectorList, self.selector.css(query))\n \n     def follow(\n         self,\n\n@@ -600,6 +600,7 @@ class Https2ClientProtocolTestCase(TestCase):\n         def assert_metadata(response: Response):\n             self.assertEqual(response.request, request)\n             self.assertIsInstance(response.certificate, Certificate)\n+            assert response.certificate  # typing\n             self.assertIsNotNone(response.certificate.original)\n             self.assertEqual(\n                 response.certificate.getIssuer(), self.client_certificate.getIssuer()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
