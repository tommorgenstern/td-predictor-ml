{"custom_id": "keras#0a43b8818293e4b458a645b86f70129fa12b6062", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 524 | Contributors (this commit): 4 | Commits (past 90d): 26 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -173,7 +173,7 @@ class AdamW(optimizer.Optimizer):\n     # Apply step weight decay\n     if self.weight_decay != 0 and variable not in self._exclude_from_weight_decay:\n       wd = tf.cast(self.weight_decay, variable.dtype)\n-      variable.assign_sub(variable * (1 - lr * wd))\n+      variable.assign_sub(variable * wd)\n \n     if isinstance(gradient, tf.IndexedSlices):\n       # Sparse gradients.\n\n@@ -124,7 +124,7 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n     optimizer_3.build([var3], exclude_from_weight_decay=[var3])\n     optimizer_3.apply_gradients(zip([grads], [var3]))\n \n-    self.assertAllClose(var1, 8e-6)\n+    self.assertEqual(var1, 1.992)\n     self.assertEqual(var2, 2.0)\n     self.assertEqual(var3, 2.0)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#258bf90b10d1bebb36ba10540fad8772aed6a930", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 2052 | Contributors (this commit): 15 | Commits (past 90d): 7 | Contributors (cumulative): 15 | DMM Complexity: 0.0\n\nDIFF:\n@@ -918,7 +918,6 @@ class KerasSequenceAdapter(GeneratorDataAdapter):\n       raise ValueError(\"`sample_weight` argument is not supported when using \"\n                        \"`keras.utils.Sequence` as input.\")\n \n-    self._size = len(x)\n     self._shuffle_sequence = shuffle\n     self._keras_sequence = x\n     self._enqueuer = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#47c7983dbf0264a0c794a5c57f17687d8dcd0b6c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 9 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 33 | Churn Cumulative: 778 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -13,16 +13,17 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Keras-based einsum dense layer.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-# pylint: disable=g-classes-have-attributes\n+# pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n \n import re\n+\n from keras import activations\n from keras import constraints\n from keras import initializers\n from keras import regularizers\n from keras.engine.base_layer import Layer\n+import tensorflow.compat.v2 as tf\n+\n from tensorflow.python.util.tf_export import keras_export\n \n \n@@ -263,8 +264,8 @@ def _analyze_split_string(split_string,\n     input_dim_map = {dim: i for i, dim in enumerate(input_spec)}\n     output_dim_map = {dim: i for i, dim in enumerate(output_spec)}\n \n-  for i, dim in enumerate(input_spec):\n-    input_shape_at_dim = input_shape[i]\n+  for dim in input_spec:\n+    input_shape_at_dim = input_shape[input_dim_map[dim]]\n     if dim in output_dim_map:\n       output_shape_at_dim = output_shape[output_dim_map[dim]]\n       if (output_shape_at_dim is not None and\n\n@@ -13,18 +13,15 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Tests for Keras-based einsum dense layer.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n+# pylint: disable=g-direct-tensorflow-import\n \n from absl.testing import parameterized\n-\n-import numpy as np\n-\n import keras\n-\n-from keras.testing_infra import test_combinations  # pylint: disable=g-direct-tensorflow-import\n-from keras.testing_infra import test_utils\n from keras.layers import einsum_dense\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n \n \n @test_combinations.run_all_keras_modes\n@@ -128,6 +125,15 @@ from keras.layers import einsum_dense\n         \"expected_weight_shape\": [32, 64],\n         \"expected_bias_shape\": None,\n         \"expected_output_shape\": (None, 64)\n+    }, {\n+        \"testcase_name\": \"_2d_precast_elided_input_used_in_output\",\n+        \"equation\": \"...bc,bc->...b\",\n+        \"bias_axes\": None,\n+        \"input_shape\": (None, 32, 64),\n+        \"output_shape\": (32),\n+        \"expected_weight_shape\": [32, 64],\n+        \"expected_bias_shape\": None,\n+        \"expected_output_shape\": (None, 32)\n     }, {\n         \"testcase_name\": \"_2d_precast_multiple_elided_dims\",\n         \"equation\": \"...b,bc->...c\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#261c08f01dec10b40b09eebc734ebfb4e62e5b6f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 740 | Files Changed: 4 | Hunks: 7 | Methods Changed: 36 | Complexity Δ (Sum/Max): -71/1 | Churn Δ: 754 | Churn Cumulative: 3667 | Contributors (this commit): 6 | Commits (past 90d): 8 | Contributors (cumulative): 14 | DMM Complexity: 0.18115942028985507\n\nDIFF:\n@@ -1,326 +0,0 @@\n-# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Denormalization preprocessing layer.\"\"\"\n-\n-# pylint: disable=g-classes-have-attributes\n-# pylint: disable=g-direct-tensorflow-import\n-\n-from keras import backend\n-from keras.engine import base_preprocessing_layer\n-from keras.layers.preprocessing import preprocessing_utils as utils\n-import numpy as np\n-import tensorflow.compat.v2 as tf\n-from tensorflow.python.util.tf_export import keras_export\n-\n-\n-@keras_export('keras.layers.Denormalization',\n-              'keras.layers.experimental.preprocessing.Denormalization')\n-class Denormalization(base_preprocessing_layer.PreprocessingLayer):\n-  \"\"\"A preprocessing layer which denormalizes continuous features.\n-\n-  This layer will shift and scale inputs into a distribution of training data. It accomplishes this by precomputing the mean and\n-  variance of the data, and calling `(input + mean) * sqrt(var)` at runtime.\n-\n-  The mean and variance values for the layer must be either supplied on\n-  construction or learned via `adapt()`. `adapt()` will compute the mean and\n-  variance of the data and store them as the layer's weights. `adapt()` should\n-  be called before `fit()`, `evaluate()`, or `predict()`.\n-\n-  For an overview and full list of preprocessing layers, see the preprocessing\n-  [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n-\n-  Args:\n-      axis: Integer, tuple of integers, or None. The axis or axes that should\n-        have a separate mean and variance for each index in the shape. For\n-        example, if shape is `(None, 5)` and `axis=1`, the layer will track 5\n-        separate mean and variance values for the last axis. If `axis` is set to\n-        `None`, the layer will normalize all elements in the input by a scalar\n-        mean and variance. Defaults to -1, where the last axis of the input is\n-        assumed to be a feature dimension and is normalized per index. Note that\n-        in the specific case of batched scalar inputs where the only axis is the\n-        batch axis, the default will normalize each index in the batch\n-        separately. In this case, consider passing `axis=None`.\n-      mean: The mean value(s) to use during denormalization. The passed value(s)\n-        will be broadcast to the shape of the kept axes above; if the value(s)\n-        cannot be broadcast, an error will be raised when this layer's `build()`\n-        method is called.\n-      variance: The variance value(s) to use during denormalization. The passed\n-        value(s) will be broadcast to the shape of the kept axes above; if the\n-        value(s) cannot be broadcast, an error will be raised when this layer's\n-        `build()` method is called.\n-\n-  Examples:\n-\n-  Calculate a global mean and variance by analyzing the dataset in `adapt()`.\n-\n-  >>> adapt_data = np.array([1., 2., 3., 4., 5.], dtype='float32')\n-  >>> input_data = np.array([1., 2., 3.], dtype='float32')\n-  >>> layer = tf.keras.layers.Denormalization(axis=None)\n-  >>> layer.adapt(adapt_data)\n-  >>> layer(input_data)\n-  <tf.Tensor: shape=(3,), dtype=float32, numpy=\n-  array([-1.4142135, -0.70710677, 0.], dtype=float32)>\n-\n-  Calculate a mean and variance for each index on the last axis.\n-\n-  >>> adapt_data = np.array([[0., 7., 4.],\n-  ...                        [2., 9., 6.],\n-  ...                        [0., 7., 4.],\n-  ...                        [2., 9., 6.]], dtype='float32')\n-  >>> input_data = np.array([[0., 7., 4.]], dtype='float32')\n-  >>> layer = tf.keras.layers.Denormalization(axis=-1)\n-  >>> layer.adapt(adapt_data)\n-  >>> layer(input_data)\n-  <tf.Tensor: shape=(1, 3), dtype=float32, numpy=\n-  array([-1., -1., -1.], dtype=float32)>\n-\n-  Pass the mean and variance directly.\n-\n-  >>> input_data = np.array([[1.], [2.], [3.]], dtype='float32')\n-  >>> layer = tf.keras.layers.Denormalization(mean=3., variance=2.)\n-  >>> layer(input_data)\n-  <tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n-  array([[-1.4142135 ],\n-         [-0.70710677],\n-         [ 0.        ]], dtype=float32)>\n-  \"\"\"\n-\n-  def __init__(self, axis=-1, mean=None, variance=None, **kwargs):\n-    super().__init__(**kwargs)\n-    base_preprocessing_layer.keras_kpl_gauge.get_cell('Denormalization').set(True)\n-\n-    # Standardize `axis` to a tuple.\n-    if axis is None:\n-      axis = ()\n-    elif isinstance(axis, int):\n-      axis = (axis,)\n-    else:\n-      axis = tuple(axis)\n-    self.axis = axis\n-\n-    # Set `mean` and `variance` if passed.\n-    if isinstance(mean, tf.Variable):\n-      raise ValueError('Denormalization does not support passing a Variable '\n-                       'for the `mean` init arg.')\n-    if isinstance(variance, tf.Variable):\n-      raise ValueError('Denormalization does not support passing a Variable '\n-                       'for the `variance` init arg.')\n-    if (mean is not None) != (variance is not None):\n-      raise ValueError(\n-          'When setting values directly, both `mean` and `variance` '\n-          'must be set. Got mean: {} and variance: {}'.format(mean, variance))\n-    self.input_mean = mean\n-    self.input_variance = variance\n-\n-  def build(self, input_shape):\n-    super().build(input_shape)\n-\n-    if (isinstance(input_shape, (list, tuple)) and\n-        all(isinstance(shape, tf.TensorShape) for shape in input_shape)):\n-      raise ValueError('Denormalization only accepts a single input. If you are '\n-                       'passing a python list or tuple as a single input, '\n-                       'please convert to a numpy array or `tf.Tensor`.')\n-\n-    input_shape = tf.TensorShape(input_shape).as_list()\n-    ndim = len(input_shape)\n-\n-    if any(a < -ndim or a >= ndim for a in self.axis):\n-      raise ValueError('All `axis` values must be in the range [-ndim, ndim). '\n-                       'Found ndim: `{}`, axis: {}'.format(ndim, self.axis))\n-\n-    # Axes to be kept, replacing negative values with positive equivalents.\n-    # Sorted to avoid transposing axes.\n-    self._keep_axis = sorted([d if d >= 0 else d + ndim for d in self.axis])\n-    # All axes to be kept should have known shape.\n-    for d in self._keep_axis:\n-      if input_shape[d] is None:\n-        raise ValueError(\n-            'All `axis` values to be kept must have known shape. Got axis: {}, '\n-            'input shape: {}, with unknown axis at index: {}'.format(\n-                self.axis, input_shape, d))\n-    # Axes to be reduced.\n-    self._reduce_axis = [d for d in range(ndim) if d not in self._keep_axis]\n-    # 1 if an axis should be reduced, 0 otherwise.\n-    self._reduce_axis_mask = [\n-        0 if d in self._keep_axis else 1 for d in range(ndim)\n-    ]\n-    # Broadcast any reduced axes.\n-    self._broadcast_shape = [\n-        input_shape[d] if d in self._keep_axis else 1 for d in range(ndim)\n-    ]\n-    mean_and_var_shape = tuple(input_shape[d] for d in self._keep_axis)\n-\n-    if self.input_mean is None:\n-      self.adapt_mean = self.add_weight(\n-          name='mean',\n-          shape=mean_and_var_shape,\n-          dtype=self.compute_dtype,\n-          initializer='zeros',\n-          trainable=False)\n-      self.adapt_variance = self.add_weight(\n-          name='variance',\n-          shape=mean_and_var_shape,\n-          dtype=self.compute_dtype,\n-          initializer='ones',\n-          trainable=False)\n-      self.count = self.add_weight(\n-          name='count',\n-          shape=(),\n-          dtype=tf.int64,\n-          initializer='zeros',\n-          trainable=False)\n-      self.finalize_state()\n-    else:\n-      # In the no adapt case, make constant tensors for mean and variance with\n-      # proper broadcast shape for use during call.\n-      mean = self.input_mean * np.ones(mean_and_var_shape)\n-      variance = self.input_variance * np.ones(mean_and_var_shape)\n-      mean = tf.reshape(mean, self._broadcast_shape)\n-      variance = tf.reshape(variance, self._broadcast_shape)\n-      self.mean = tf.cast(mean, self.compute_dtype)\n-      self.variance = tf.cast(variance, self.compute_dtype)\n-\n-  # We override this method solely to generate a docstring.\n-  def adapt(self, data, batch_size=None, steps=None):\n-    \"\"\"Computes the mean and variance of values in a dataset.\n-\n-    Calling `adapt()` on a `Denormalization` layer is an alternative to passing in\n-    `mean` and `variance` arguments during layer construction. A `Denormalization`\n-    layer should always either be adapted over a dataset or passed `mean` and\n-    `variance`.\n-\n-    During `adapt()`, the layer will compute a `mean` and `variance` separately\n-    for each position in each axis specified by the `axis` argument. To\n-    calculate a single `mean` and `variance` over the input data, simply pass\n-    `axis=None`.\n-\n-    In order to make `Denormalization` efficient in any distribution context, the\n-    computed mean and variance are kept static with respect to any compiled\n-    `tf.Graph`s that call the layer. As a consequence, if the layer is adapted a\n-    second time, any models using the layer should be re-compiled. For more\n-    information see\n-    `tf.keras.layers.experimental.preprocessing.PreprocessingLayer.adapt`.\n-\n-    `adapt()` is meant only as a single machine utility to compute layer state.\n-    To analyze a dataset that cannot fit on a single machine, see\n-    [Tensorflow Transform](https://www.tensorflow.org/tfx/transform/get_started)\n-    for a multi-machine, map-reduce solution.\n-\n-    Arguments:\n-      data: The data to train on. It can be passed either as a\n-          `tf.data.Dataset`, or as a numpy array.\n-      batch_size: Integer or `None`.\n-          Number of samples per state update.\n-          If unspecified, `batch_size` will default to 32.\n-          Do not specify the `batch_size` if your data is in the\n-          form of datasets, generators, or `keras.utils.Sequence` instances\n-          (since they generate batches).\n-      steps: Integer or `None`.\n-          Total number of steps (batches of samples)\n-          When training with input tensors such as\n-          TensorFlow data tensors, the default `None` is equal to\n-          the number of samples in your dataset divided by\n-          the batch size, or 1 if that cannot be determined. If x is a\n-          `tf.data` dataset, and 'steps' is None, the epoch will run until\n-          the input dataset is exhausted. When passing an infinitely\n-          repeating dataset, you must specify the `steps` argument. This\n-          argument is not supported with array inputs.\n-    \"\"\"\n-    super().adapt(data, batch_size=batch_size, steps=steps)\n-\n-  def update_state(self, data):\n-    if self.input_mean is not None:\n-      raise ValueError(\n-          'Cannot `adapt` a Denormalization layer that is initialized with '\n-          'static `mean` and `variance`, you passed mean {} and variance {}.'\n-          .format(self.input_mean, self.input_variance))\n-\n-    if not self.built:\n-      raise RuntimeError('`build` must be called before `update_state`.')\n-\n-    data = self._standardize_inputs(data)\n-    data = tf.cast(data, self.adapt_mean.dtype)\n-    batch_mean, batch_variance = tf.nn.moments(data, axes=self._reduce_axis)\n-    batch_shape = tf.shape(data, out_type=self.count.dtype)\n-    if self._reduce_axis:\n-      batch_reduce_shape = tf.gather(batch_shape, self._reduce_axis)\n-      batch_count = tf.reduce_prod(batch_reduce_shape)\n-    else:\n-      batch_count = 1\n-\n-    total_count = batch_count + self.count\n-    batch_weight = (\n-        tf.cast(batch_count, dtype=self.compute_dtype) /\n-        tf.cast(total_count, dtype=self.compute_dtype))\n-    existing_weight = 1. - batch_weight\n-\n-    total_mean = self.adapt_mean * existing_weight + batch_mean * batch_weight\n-    # The variance is computed using the lack-of-fit sum of squares\n-    # formula (see https://en.wikipedia.org/wiki/Lack-of-fit_sum_of_squares).\n-    total_variance = ((self.adapt_variance +\n-                       (self.adapt_mean - total_mean)**2) * existing_weight +\n-                      (batch_variance +\n-                       (batch_mean - total_mean)**2) * batch_weight)\n-    self.adapt_mean.assign(total_mean)\n-    self.adapt_variance.assign(total_variance)\n-    self.count.assign(total_count)\n-\n-  def reset_state(self):  # pylint: disable=method-hidden\n-    if self.input_mean is not None or not self.built:\n-      return\n-\n-    self.adapt_mean.assign(tf.zeros_like(self.adapt_mean))\n-    self.adapt_variance.assign(tf.ones_like(self.adapt_variance))\n-    self.count.assign(tf.zeros_like(self.count))\n-\n-  def finalize_state(self):\n-    if self.input_mean is not None or not self.built:\n-      return\n-\n-    # In the adapt case, we make constant tensors for mean and variance with\n-    # proper broadcast shape and dtype each time `finalize_state` is called.\n-    self.mean = tf.reshape(self.adapt_mean, self._broadcast_shape)\n-    self.mean = tf.cast(self.mean, self.compute_dtype)\n-    self.variance = tf.reshape(self.adapt_variance, self._broadcast_shape)\n-    self.variance = tf.cast(self.variance, self.compute_dtype)\n-\n-  def call(self, inputs):\n-    inputs = self._standardize_inputs(inputs)\n-    # The base layer automatically casts floating-point inputs, but we\n-    # explicitly cast here to also allow integer inputs to be passed\n-    inputs = tf.cast(inputs, self.compute_dtype)\n-    return ((inputs + self.mean) *\n-            tf.maximum(tf.sqrt(self.variance), backend.epsilon()))\n-\n-  def compute_output_shape(self, input_shape):\n-    return input_shape\n-\n-  def compute_output_signature(self, input_spec):\n-    return input_spec\n-\n-  def get_config(self):\n-    config = super().get_config()\n-    config.update({\n-        'axis': self.axis,\n-        'mean': utils.listify_tensors(self.input_mean),\n-        'variance': utils.listify_tensors(self.input_variance),\n-    })\n-    return config\n-\n-  def _standardize_inputs(self, inputs):\n-    inputs = tf.convert_to_tensor(inputs)\n-    if inputs.dtype != self.compute_dtype:\n-      inputs = tf.cast(inputs, self.compute_dtype)\n-    return inputs\n\n@@ -1,413 +0,0 @@\n-# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Tests for keras.layers.preprocessing.denormalization.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-import os\n-\n-from absl.testing import parameterized\n-\n-import numpy as np\n-\n-import keras\n-from keras.testing_infra import test_combinations\n-from keras.testing_infra import test_utils\n-from keras.layers.preprocessing import denormalization\n-from keras.layers.preprocessing import preprocessing_test_utils\n-from keras.mixed_precision import policy\n-\n-\n-def _get_layer_computation_test_cases():\n-  test_cases = ({\n-      \"adapt_data\": np.array([[1.], [2.], [3.], [4.], [5.]], dtype=np.float32),\n-      \"axis\": -1,\n-      \"test_data\": np.array([[1.], [2.], [3.]], np.float32),\n-      \"expected\": np.array([[-1.414214], [-.707107], [0]], np.float32),\n-      \"testcase_name\": \"2d_single_element\"\n-  }, {\n-      \"adapt_data\": np.array([[1], [2], [3], [4], [5]], dtype=np.int32),\n-      \"axis\": -1,\n-      \"test_data\": np.array([[1], [2], [3]], np.int32),\n-      \"expected\": np.array([[-1.414214], [-.707107], [0]], np.float32),\n-      \"testcase_name\": \"2d_int_data\"\n-  }, {\n-      \"adapt_data\": np.array([[1.], [2.], [3.], [4.], [5.]], dtype=np.float32),\n-      \"axis\": None,\n-      \"test_data\": np.array([[1.], [2.], [3.]], np.float32),\n-      \"expected\": np.array([[-1.414214], [-.707107], [0]], np.float32),\n-      \"testcase_name\": \"2d_single_element_none_axis\"\n-  }, {\n-      \"adapt_data\": np.array([[1., 2., 3., 4., 5.]], dtype=np.float32),\n-      \"axis\": None,\n-      \"test_data\": np.array([[1.], [2.], [3.]], np.float32),\n-      \"expected\": np.array([[-1.414214], [-.707107], [0]], np.float32),\n-      \"testcase_name\": \"2d_single_element_none_axis_flat_data\"\n-  }, {\n-      \"adapt_data\":\n-          np.array([[[1., 2., 3.], [2., 3., 4.]], [[3., 4., 5.], [4., 5., 6.]]],\n-                   np.float32),\n-      \"axis\":\n-          1,\n-      \"test_data\":\n-          np.array([[[1., 2., 3.], [2., 3., 4.]], [[3., 4., 5.], [4., 5., 6.]]],\n-                   np.float32),\n-      \"expected\":\n-          np.array([[[-1.549193, -0.774597, 0.], [-1.549193, -0.774597, 0.]],\n-                    [[0., 0.774597, 1.549193], [0., 0.774597, 1.549193]]],\n-                   np.float32),\n-      \"testcase_name\":\n-          \"3d_internal_axis\"\n-  }, {\n-      \"adapt_data\":\n-          np.array(\n-              [[[1., 0., 3.], [2., 3., 4.]], [[3., -1., 5.], [4., 5., 8.]]],\n-              np.float32),\n-      \"axis\": (1, 2),\n-      \"test_data\":\n-          np.array(\n-              [[[3., 1., -1.], [2., 5., 4.]], [[3., 0., 5.], [2., 5., 8.]]],\n-              np.float32),\n-      \"expected\":\n-          np.array(\n-              [[[1., 3., -5.], [-1., 1., -1.]], [[1., 1., 1.], [-1., 1., 1.]]],\n-              np.float32),\n-      \"testcase_name\":\n-          \"3d_multiple_axis\"\n-  }, {\n-      \"adapt_data\":\n-          np.zeros((3, 4)),\n-      \"axis\": -1,\n-      \"test_data\":\n-          np.zeros((3, 4)),\n-      \"expected\":\n-          np.zeros((3, 4)),\n-      \"testcase_name\":\n-          \"zero_variance\"\n-  })\n-\n-  crossed_test_cases = []\n-  # Cross above test cases with use_dataset in (True, False)\n-  for use_dataset in (True, False):\n-    for case in test_cases:\n-      case = case.copy()\n-      if use_dataset:\n-        case[\"testcase_name\"] = case[\"testcase_name\"] + \"_with_dataset\"\n-      case[\"use_dataset\"] = use_dataset\n-      crossed_test_cases.append(case)\n-\n-  return crossed_test_cases\n-\n-\n-@test_combinations.run_all_keras_modes\n-class NormalizationTest(test_combinations.TestCase,\n-                        preprocessing_test_utils.PreprocessingLayerTest):\n-\n-  def test_broadcasting_during_direct_setting(self):\n-    layer = denormalization.Denormalization(axis=-1, mean=[1.0], variance=[2.0])\n-    output = layer(np.array([[1., 2.]]))\n-    expected_output = [[4., 6.]]\n-    self.assertAllClose(output, expected_output)\n-    self.assertAllClose(layer.get_weights(), [])\n-\n-  def test_broadcasting_during_direct_setting_with_tensors(self):\n-    if not tf.executing_eagerly():\n-      self.skipTest(\"Only supported in TF2.\")\n-\n-    layer = denormalization.Denormalization(\n-        axis=-1,\n-        mean=tf.constant([1.0]),\n-        variance=tf.constant([2.0]))\n-    output = layer(np.array([[1., 2.]]))\n-    expected_output = [[4., 6.]]\n-    self.assertAllClose(output, expected_output)\n-    self.assertAllClose(layer.get_weights(), [])\n-\n-  def test_1d_data(self):\n-    data = np.array([0., 2., 0., 2.])\n-    layer = denormalization.Denormalization(mean=1.0, variance=2.0)\n-    output = layer(data)\n-    self.assertListEqual(output.shape.as_list(), [4])\n-    self.assertAllClose(output, [2, 6, -2, 6])\n-\n-  def test_0d_data(self):\n-    layer = denormalization.Denormalization(axis=None, mean=1.0, variance=2.0)\n-    output = layer(0.)\n-    self.assertListEqual(output.shape.as_list(), [])\n-    self.assertAllClose(output, 2)\n-\n-  def test_broadcasting_during_direct_setting_with_variables_fails(self):\n-    with self.assertRaisesRegex(ValueError, \"passing a Variable\"):\n-      _ = denormalization.Denormalization(\n-          axis=-1,\n-          mean=tf.Variable([1.0]),\n-          variance=tf.Variable([2.0]))\n-\n-  def test_keeping_an_unknown_axis_fails(self):\n-    layer = denormalization.Denormalization(axis=-1)\n-    with self.assertRaisesRegex(ValueError, \"axis.*must have known shape\"):\n-      layer.build([None])\n-\n-  @parameterized.parameters(\n-      # Out of bounds\n-      {\"axis\": 3},\n-      {\"axis\": -4},\n-      # In a tuple\n-      {\"axis\": (1, 3)},\n-      {\"axis\": (1, -4)},\n-  )\n-  def test_bad_axis_fail_build(self, axis):\n-    layer = denormalization.Denormalization(axis=axis)\n-    with self.assertRaisesRegex(ValueError, \"in the range\"):\n-      layer.build([None, 2, 3])\n-\n-  def test_list_input(self):\n-    with self.assertRaisesRegex(\n-        ValueError, (\"Normalization only accepts a single input. If you are \"\n-                     \"passing a python list or tuple as a single input, \"\n-                     \"please convert to a numpy array or `tf.Tensor`.\")):\n-      denormalization.Denormalization()([1, 2, 3])\n-\n-  def test_scalar_input(self):\n-    with self.assertRaisesRegex(ValueError,\n-                                \"axis.*values must be in the range\"):\n-      denormalization.Denormalization()(1)\n-\n-  def test_output_dtype(self):\n-    if not tf.__internal__.tf2.enabled():\n-      self.skipTest(\"set_global_policy only supported in TF2.\")\n-    # Output should respect an explicit dtype, and default to the global policy.\n-    policy.set_global_policy(\"float64\")\n-    input_data = keras.Input(batch_size=16, shape=(1,))\n-    layer = denormalization.Denormalization(mean=1.0, variance=2.0, dtype=\"float16\")\n-    output = layer(input_data)\n-    self.assertAllEqual(output.dtype, tf.float16)\n-    layer = denormalization.Denormalization(mean=1.0, variance=2.0)\n-    output = layer(input_data)\n-    self.assertAllEqual(output.dtype, tf.float64)\n-\n-\n-@test_combinations.run_all_keras_modes(always_skip_v1=True)\n-class NormalizationAdaptTest(test_combinations.TestCase,\n-                             preprocessing_test_utils.PreprocessingLayerTest):\n-\n-  def test_layer_api_compatibility(self):\n-    cls = denormalization.Denormalization\n-    output_data = test_utils.layer_test(\n-        cls,\n-        kwargs={\"axis\": -1},\n-        input_shape=(None, 3),\n-        input_data=np.array([[3, 1, 2], [6, 5, 4]], dtype=np.float32),\n-        validate_training=False,\n-        adapt_data=np.array([[1, 2, 1], [2, 3, 4], [1, 2, 1], [2, 3, 4]]))\n-    expected = np.array([[3., -3., -0.33333333], [9., 5., 1.]])\n-    self.assertAllClose(expected, output_data)\n-\n-  @parameterized.named_parameters(*_get_layer_computation_test_cases())\n-  def test_layer_computation(self, adapt_data, axis, test_data, use_dataset,\n-                             expected):\n-    input_shape = tuple([test_data.shape[i] for i in range(1, test_data.ndim)])\n-    if use_dataset:\n-      # Keras APIs expect batched datasets\n-      adapt_data = tf.data.Dataset.from_tensor_slices(adapt_data).batch(\n-          test_data.shape[0] // 2)\n-      test_data = tf.data.Dataset.from_tensor_slices(test_data).batch(\n-          test_data.shape[0] // 2)\n-\n-    layer = denormalization.Denormalization(axis=axis)\n-    layer.adapt(adapt_data)\n-\n-    input_data = keras.Input(shape=input_shape)\n-    output = layer(input_data)\n-    model = keras.Model(input_data, output)\n-    model._run_eagerly = test_utils.should_run_eagerly()\n-    output_data = model.predict(test_data)\n-    self.assertAllClose(expected, output_data)\n-\n-  def test_1d_unbatched_adapt(self):\n-    ds = tf.data.Dataset.from_tensor_slices([\n-        [2., 0., 2., 0.],\n-        [0., 2., 0., 2.],\n-    ])\n-    layer = denormalization.Denormalization(axis=-1)\n-    layer.adapt(ds)\n-    output_ds = ds.map(layer)\n-    self.assertAllClose(\n-        list(output_ds.as_numpy_iterator()), [\n-            [1., -1., 1., -1.],\n-            [-1., 1., -1., 1.],\n-        ])\n-\n-  def test_0d_unbatched_adapt(self):\n-    ds = tf.data.Dataset.from_tensor_slices([2., 0., 2., 0.])\n-    layer = denormalization.Denormalization(axis=None)\n-    layer.adapt(ds)\n-    output_ds = ds.map(layer)\n-    self.assertAllClose(list(output_ds.as_numpy_iterator()), [1., -1., 1., -1.])\n-\n-  @parameterized.parameters(\n-      # Results should be identical no matter how the axes are specified (3d).\n-      {\"axis\": (1, 2)},\n-      {\"axis\": (2, 1)},\n-      {\"axis\": (1, -1)},\n-      {\"axis\": (-1, 1)},\n-  )\n-  def test_axis_permutations(self, axis):\n-    layer = denormalization.Denormalization(axis=axis)\n-    # data.shape = [2, 2, 3]\n-    data = np.array([[[0., 1., 2.], [0., 2., 6.]],\n-                     [[2., 3., 4.], [3., 6., 10.]]])\n-    expect = np.array([[[-1., -1., -1.], [-1., -1., -1.]],\n-                       [[1., 1., 1.], [1., 1., 1.]]])\n-    layer.adapt(data)\n-    self.assertAllClose(expect, layer(data))\n-\n-  def test_model_summary_after_layer_adapt(self):\n-    data = np.array([[[0., 1., 2.], [0., 2., 6.]],\n-                     [[2., 3., 4.], [3., 6., 10.]]])\n-    layer = denormalization.Denormalization(axis=-1)\n-    layer.adapt(data)\n-    model = keras.Sequential(\n-        [layer,\n-         keras.layers.Dense(64, activation=\"relu\"),\n-         keras.layers.Dense(1)])\n-    model.summary()\n-\n-  def test_multiple_adapts(self):\n-    first_adapt = [[0], [2], [0], [2]]\n-    second_adapt = [[2], [4], [2], [4]]\n-    predict_input = [[2], [2]]\n-    expected_first_output = [[1], [1]]\n-    expected_second_output = [[-1], [-1]]\n-\n-    inputs = keras.Input(shape=(1,), dtype=tf.int32)\n-    layer = denormalization.Denormalization(axis=-1)\n-    layer.adapt(first_adapt)\n-    outputs = layer(inputs)\n-    model = keras.Model(inputs=inputs, outputs=outputs)\n-\n-    actual_output = model.predict(predict_input)\n-    self.assertAllClose(actual_output, expected_first_output)\n-\n-    # Re-adapt the layer on new inputs.\n-    layer.adapt(second_adapt)\n-    # Re-compile the model.\n-    model.compile()\n-    # `predict` should now use the new model state.\n-    actual_output = model.predict(predict_input)\n-    self.assertAllClose(actual_output, expected_second_output)\n-\n-  @parameterized.parameters(\n-      {\"adapted\": True},\n-      {\"adapted\": False},\n-  )\n-  def test_saved_model_tf(self, adapted):\n-    input_data = [[0.], [2.], [0.], [2.]]\n-    expected_output = [[-1.], [1.], [-1.], [1.]]\n-\n-    inputs = keras.Input(shape=(1,), dtype=tf.float32)\n-    if adapted:\n-      layer = denormalization.Normalization(axis=-1)\n-      layer.adapt(input_data)\n-    else:\n-      layer = denormalization.Normalization(mean=1., variance=2.)\n-    outputs = layer(inputs)\n-    model = keras.Model(inputs=inputs, outputs=outputs)\n-\n-    output_data = model.predict(input_data)\n-    self.assertAllClose(output_data, expected_output)\n-\n-    # Save the model to disk.\n-    output_path = os.path.join(self.get_temp_dir(), \"tf_saved_model\")\n-    tf.saved_model.save(model, output_path)\n-    loaded_model = tf.saved_model.load(output_path)\n-    f = loaded_model.signatures[\"serving_default\"]\n-\n-    # Ensure that the loaded model is unique (so that the save/load is real)\n-    self.assertIsNot(model, loaded_model)\n-\n-    # Validate correctness of the new model.\n-    new_output_data = f(tf.constant(input_data))[\"normalization\"]\n-    self.assertAllClose(new_output_data, expected_output)\n-\n-  @parameterized.product(\n-      save_format=[\"tf\", \"h5\"],\n-      adapt=[True, False],\n-  )\n-  def test_saved_model_keras(self, save_format, adapt):\n-    input_data = [[0.], [2.], [0.], [2.]]\n-    expected_output = [[-1.], [1.], [-1.], [1.]]\n-\n-    cls = denormalization.Denormalization\n-    inputs = keras.Input(shape=(1,), dtype=tf.float32)\n-    if adapt:\n-      layer = cls(axis=-1)\n-      layer.adapt(input_data)\n-    else:\n-      layer = cls(mean=1., variance=2.)\n-    outputs = layer(inputs)\n-    model = keras.Model(inputs=inputs, outputs=outputs)\n-\n-    output_data = model.predict(input_data)\n-    self.assertAllClose(output_data, expected_output)\n-\n-    # Save the model to disk.\n-    output_path = os.path.join(self.get_temp_dir(), \"tf_keras_saved_model\")\n-    model.save(output_path, save_format=format)\n-    loaded_model = keras.models.load_model(\n-        output_path, custom_objects={\"Normalization\": cls})\n-\n-    # Ensure that the loaded model is unique (so that the save/load is real)\n-    self.assertIsNot(model, loaded_model)\n-\n-    # Validate correctness of the new model.\n-    new_output_data = loaded_model.predict(input_data)\n-    self.assertAllClose(new_output_data, expected_output)\n-\n-  @parameterized.parameters(\n-      {\"adapted\": True},\n-      {\"adapted\": False},\n-  )\n-  def test_saved_weights_keras(self, adapted):\n-    input_data = [[0.], [2.], [0.], [2.]]\n-    expected_output = [[-1.], [1.], [-1.], [1.]]\n-\n-    cls = denormalization.Denormalization\n-    inputs = keras.Input(shape=(1,), dtype=tf.float32)\n-    if adapted:\n-      layer = cls(axis=-1)\n-      layer.adapt(input_data)\n-    else:\n-      layer = cls(mean=1., variance=2.)\n-    outputs = layer(inputs)\n-    model = keras.Model(inputs=inputs, outputs=outputs)\n-\n-    output_data = model.predict(input_data)\n-    self.assertAllClose(output_data, expected_output)\n-\n-    # Save the model to disk.\n-    output_path = os.path.join(self.get_temp_dir(), \"tf_keras_saved_weights\")\n-    model.save_weights(output_path, save_format=\"tf\")\n-    new_model = keras.Model.from_config(\n-        model.get_config(), custom_objects={\"Normalization\": cls})\n-    new_model.load_weights(output_path)\n-\n-    # Validate correctness of the new model.\n-    new_output_data = new_model.predict(input_data)\n-    self.assertAllClose(new_output_data, expected_output)\n-\n-\n-if __name__ == \"__main__\":\n-  tf.test.main()\n\n@@ -61,6 +61,7 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n         value(s) will be broadcast to the shape of the kept axes above; if the\n         value(s) cannot be broadcast, an error will be raised when this layer's\n         `build()` method is called.\n+      invert: If True, this layer will return the denormalized values of inputs. Default to False.\n \n   Examples:\n \n@@ -98,7 +99,7 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n          [ 0.        ]], dtype=float32)>\n   \"\"\"\n \n-  def __init__(self, axis=-1, mean=None, variance=None, **kwargs):\n+  def __init__(self, axis=-1, mean=None, variance=None, invert=False, **kwargs):\n     super().__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('Normalization').set(True)\n \n@@ -124,6 +125,7 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n           'must be set. Got mean: {} and variance: {}'.format(mean, variance))\n     self.input_mean = mean\n     self.input_variance = variance\n+    self.invert = invert\n \n   def build(self, input_shape):\n     super().build(input_shape)\n@@ -302,6 +304,10 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n     # The base layer automatically casts floating-point inputs, but we\n     # explicitly cast here to also allow integer inputs to be passed\n     inputs = tf.cast(inputs, self.compute_dtype)\n+    if self.invert:\n+      return ((inputs + self.mean) *\n+            tf.maximum(tf.sqrt(self.variance), backend.epsilon()))\n+    else:\n       return ((inputs - self.mean) /\n             tf.maximum(tf.sqrt(self.variance), backend.epsilon()))\n \n\n@@ -198,6 +198,13 @@ class NormalizationTest(test_combinations.TestCase,\n     output = layer(input_data)\n     self.assertAllEqual(output.dtype, tf.float64)\n \n+  def test_invert(self):\n+    data = np.array([0., 2., 0., 2.])\n+    layer = normalization.Normalization(mean=2.0, variance=3.0, invert=True)\n+    output = layer(data)\n+    self.assertListEqual(output.shape.as_list(), [4])\n+    self.assertAllClose(output, [6, 12, 6, 12])\n+\n \n @test_combinations.run_all_keras_modes(always_skip_v1=True)\n class NormalizationAdaptTest(test_combinations.TestCase,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6d22887761f2cd6d695b4eac9b0bf6b0256087c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1032 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -200,10 +200,12 @@ class NormalizationTest(test_combinations.TestCase,\n \n   def test_invert(self):\n     data = np.array([0., 2., 0., 2.])\n-    layer = normalization.Normalization(mean=2.0, variance=3.0, invert=True)\n+    layer = normalization.Normalization(mean=1.0, variance=1.0)\n+    layer2 = normalization.Normalization(mean=1.0, variance=1.0, invert=True)\n     output = layer(data)\n-    self.assertListEqual(output.shape.as_list(), [4])\n-    self.assertAllClose(output, [6, 12, 6, 12])\n+    output2 = layer2(output)\n+    self.assertListEqual(output2.shape.as_list(), [4])\n+    self.assertAllClose(output2, [0., 2., 0., 2.])\n \n \n @test_combinations.run_all_keras_modes(always_skip_v1=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5cc435810de326ee549c53e10ac014d676e37447", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 29 | Churn Cumulative: 1778 | Contributors (this commit): 8 | Commits (past 90d): 5 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -107,7 +107,7 @@ class LossesContainer(Container):\n     self._user_losses = losses\n     self._user_loss_weights = loss_weights\n \n-    self._losses = losses\n+    self._losses = copy.copy(losses)\n     self._loss_weights = loss_weights\n     self._per_output_metrics = None  # Per-output losses become metrics.\n     self._loss_metric = metrics_mod.Mean(name='loss')  # Total loss.\n@@ -309,7 +309,7 @@ class MetricsContainer(Container):\n     self._user_metrics = metrics\n     self._user_weighted_metrics = weighted_metrics\n \n-    self._metrics = metrics\n+    self._metrics = copy.copy(metrics)\n     self._weighted_metrics = weighted_metrics\n     self._built = False\n \n\n@@ -195,6 +195,19 @@ class LossesContainerTest(test_combinations.TestCase):\n     self.assertEqual(b_1_metric.name, 'b_1_loss')\n     self.assertEqual(b_1_metric.result().numpy(), 0.5)\n \n+  def test_no_input_mutation(self):\n+    loss = {'a': 'mae'}\n+    loss_container = compile_utils.LossesContainer(loss)\n+    \n+    y_t = {'a': tf.zeros((10, 1))}\n+    y_p = {'a': tf.ones((10, 1)), 'b': tf.zeros((10, 1))}\n+    sw = tf.convert_to_tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n+\n+    total_loss = loss_container(y_t, y_p, sample_weight=sw)\n+    self.assertIsInstance(total_loss, tf.Tensor)\n+    self.assertEqual(total_loss.numpy(), 0.5)\n+    self.assertLen(loss, 1)\n+    \n   def test_broadcast_single_loss(self):\n     loss_container = compile_utils.LossesContainer('mse')\n \n@@ -585,6 +598,18 @@ class MetricsContainerTest(test_combinations.TestCase):\n     self.assertEqual(b_1_mse_metric.name, 'b_1_mse')\n     self.assertEqual(b_1_mse_metric.result().numpy(), 4.)\n \n+  def test_no_input_mutation(self):\n+    metric = {'a': 'mae'}\n+    metric_container = compile_utils.MetricsContainer(metric)\n+    \n+    y_t = {'a': tf.zeros((10, 1))}\n+    y_p = {'a': tf.ones((10, 1)), 'b': tf.zeros((10, 1))}\n+\n+    metric_container.update_state(y_t, y_p)\n+    self.assertLen(metric, 1)\n+    mae_metric = metric_container.metrics[0]\n+    self.assertEqual(mae_metric.result().numpy(), 1.)\n+\n   def test_crossentropy(self):\n     metric_container = compile_utils.MetricsContainer('crossentropy')\n     y_t, y_p = tf.ones((10, 1)), tf.ones((10, 1))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3374be7d09a1e946e0f3b52bad502dae4988b29a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 17 | Churn Cumulative: 1049 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -200,13 +200,22 @@ class NormalizationTest(test_combinations.TestCase,\n \n   def test_invert(self):\n     data = np.array([0., 2., 0., 2.])\n-    layer = normalization.Normalization(mean=1.0, variance=1.0)\n-    layer2 = normalization.Normalization(mean=1.0, variance=1.0, invert=True)\n-    output = layer(data)\n-    output2 = layer2(output)\n+    norm = normalization.Normalization(mean=1.0, variance=1.0)\n+    inv_norm = normalization.Normalization(mean=1.0, variance=1.0, invert=True)\n+    output = norm(data)\n+    output2 = inv_norm(output)\n     self.assertListEqual(output2.shape.as_list(), [4])\n     self.assertAllClose(output2, [0., 2., 0., 2.])\n   \n+  def test_invert_adapt(self):\n+    input_data = [[0.], [2.], [0.], [2.]]\n+    norm = keras.layers.Normalization(axis=-1)\n+    norm.adapt(input_data)\n+    inv_norm = keras.layers.Normalization(axis=-1, invert=True)\n+    inv_norm.adapt(input_data)\n+    output = norm(input_data)\n+    output2 = inv_norm(output)\n+    self.assertAllClose(input_data, output2)\n \n @test_combinations.run_all_keras_modes(always_skip_v1=True)\n class NormalizationAdaptTest(test_combinations.TestCase,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fb507741c88d5614de680920aa97e3f71944a090", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 42/42 | Churn Δ: 2 | Churn Cumulative: 1300 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -419,7 +419,7 @@ class LSTM(DropoutRNNCellMixin, RNN, base_layer.BaseRandomLayer):\n       transformation of the inputs. Default: 0.\n     recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for\n       the linear transformation of the recurrent state. Default: 0.\n-    return_sequences: Boolean. Whether to return the last output. in the output\n+    return_sequences: Boolean. Whether to return the last output in the output\n       sequence, or the full sequence. Default: `False`.\n     return_state: Boolean. Whether to return the last state in addition to the\n       output. Default: `False`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3a6ebd0ddc07e9ce4cd87c896a9321edd9ea67f6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 497 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -154,7 +154,6 @@ class ShardedVariableTest(tf.test.TestCase):\n     self.assertAllClose(got, expect)\n     self.assertGreater(len(model.variables), len(loaded_model.variables))\n \n-    with self.assertRaises(ValueError):\n     with self.strategy.scope():\n       keras.models.load_model(saved_dir)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ad272f68b99a9fa9a474cce1fdd5a21510c63e92", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 23 | Files Changed: 6 | Hunks: 20 | Methods Changed: 2 | Complexity Δ (Sum/Max): 54/46 | Churn Δ: 49 | Churn Cumulative: 1815 | Contributors (this commit): 18 | Commits (past 90d): 35 | Contributors (cumulative): 33 | DMM Complexity: None\n\nDIFF:\n@@ -112,6 +112,7 @@ from keras.layers.reshaping.zero_padding3d import ZeroPadding3D\n # Core layers.\n from keras.layers.core.activation import Activation\n from keras.layers.core.dense import Dense\n+from keras.layers.core.einsum_dense import EinsumDense\n from keras.layers.core.embedding import Embedding\n from keras.layers.core.lambda_layer import Lambda\n from keras.layers.core.masking import Masking\n@@ -121,9 +122,6 @@ from keras.layers.core.tf_op_layer import InstanceProperty\n from keras.layers.core.tf_op_layer import SlicingOpLambda\n from keras.layers.core.tf_op_layer import TFOpLambda\n \n-# Einsum-based dense layer/\n-from keras.layers.einsum_dense import EinsumDense\n-\n # Locally-connected layers.\n from keras.layers.locally_connected.locally_connected1d import LocallyConnected1D\n from keras.layers.locally_connected.locally_connected2d import LocallyConnected2D\n\n@@ -25,7 +25,7 @@ from keras import initializers\n from keras import regularizers\n from keras.engine.base_layer import Layer\n from keras.layers import activation\n-from keras.layers import einsum_dense\n+from keras.layers import core\n from keras.layers import regularization\n from keras.utils import tf_utils\n import numpy as np\n@@ -344,7 +344,7 @@ class MultiHeadAttention(Layer):\n       free_dims = self._query_shape.rank - 1\n       einsum_equation, bias_axes, output_rank = _build_proj_equation(\n           free_dims, bound_dims=1, output_dims=2)\n-      self._query_dense = einsum_dense.EinsumDense(\n+      self._query_dense = core.EinsumDense(\n           einsum_equation,\n           output_shape=_get_output_shape(output_rank - 1,\n                                          [self._num_heads, self._key_dim]),\n@@ -353,7 +353,7 @@ class MultiHeadAttention(Layer):\n           **common_kwargs)\n       einsum_equation, bias_axes, output_rank = _build_proj_equation(\n           self._key_shape.rank - 1, bound_dims=1, output_dims=2)\n-      self._key_dense = einsum_dense.EinsumDense(\n+      self._key_dense = core.EinsumDense(\n           einsum_equation,\n           output_shape=_get_output_shape(output_rank - 1,\n                                          [self._num_heads, self._key_dim]),\n@@ -362,7 +362,7 @@ class MultiHeadAttention(Layer):\n           **common_kwargs)\n       einsum_equation, bias_axes, output_rank = _build_proj_equation(\n           self._value_shape.rank - 1, bound_dims=1, output_dims=2)\n-      self._value_dense = einsum_dense.EinsumDense(\n+      self._value_dense = core.EinsumDense(\n           einsum_equation,\n           output_shape=_get_output_shape(output_rank - 1,\n                                          [self._num_heads, self._value_dim]),\n@@ -397,7 +397,7 @@ class MultiHeadAttention(Layer):\n       output_shape = [self._query_shape[-1]]\n     einsum_equation, bias_axes, output_rank = _build_proj_equation(\n         free_dims, bound_dims=2, output_dims=len(output_shape))\n-    return einsum_dense.EinsumDense(\n+    return core.EinsumDense(\n         einsum_equation,\n         output_shape=_get_output_shape(output_rank - 1, output_shape),\n         bias_axes=bias_axes if self._use_bias else None,\n\n@@ -16,6 +16,7 @@\n \n from keras.layers.core.activation import Activation\n from keras.layers.core.dense import Dense\n+from keras.layers.core.einsum_dense import EinsumDense\n from keras.layers.core.embedding import Embedding\n from keras.layers.core.lambda_layer import Lambda\n from keras.layers.core.masking import Masking\n\n@@ -27,9 +27,10 @@ import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n \n-@keras_export(\"keras.layers.experimental.EinsumDense\")\n+@keras_export(\"keras.layers.EinsumDense\",\n+              \"keras.layers.experimental.EinsumDense\")\n class EinsumDense(Layer):\n-  \"\"\"A layer that uses tf.einsum as the backing computation.\n+  \"\"\"A layer that uses `tf.einsum` as the backing computation.\n \n   This layer can perform einsum calculations of arbitrary dimensionality.\n \n@@ -53,7 +54,7 @@ class EinsumDense(Layer):\n       matrix.\n     bias_regularizer: Regularizer function applied to the bias vector.\n     activity_regularizer: Regularizer function applied to the output of the\n-      layer (its \"activation\")..\n+      layer (its \"activation\").\n     kernel_constraint: Constraint function applied to the `kernel` weights\n       matrix.\n     bias_constraint: Constraint function applied to the bias vector.\n@@ -66,7 +67,9 @@ class EinsumDense(Layer):\n   einsum operations. This example is equivalent to\n   `tf.keras.layers.Dense(64, use_bias=True)`.\n \n-  >>> layer = EinsumDense(\"ab,bc->ac\", output_shape=64, bias_axes=\"c\")\n+  >>> layer = tf.keras.layers.EinsumDense(\"ab,bc->ac\",\n+  ...                                     output_shape=64,\n+  ...                                     bias_axes=\"c\")\n   >>> input_tensor = tf.keras.Input(shape=[32])\n   >>> output_tensor = layer(input_tensor)\n   >>> output_tensor\n@@ -75,12 +78,12 @@ class EinsumDense(Layer):\n   **Applying a dense layer to a sequence**\n \n   This example shows how to instantiate a layer that applies the same dense\n-  operation to every element in a sequence. Here, the 'output_shape' has two\n+  operation to every element in a sequence. Here, the `output_shape` has two\n   values (since there are two non-batch dimensions in the output); the first\n-  dimension in the output_shape is `None`, because the sequence dimension `b`\n+  dimension in the `output_shape` is `None`, because the sequence dimension `b`\n   has an unknown shape.\n \n-  >>> layer = EinsumDense(\"abc,cd->abd\",\n+  >>> layer = tf.keras.layers.EinsumDense(\"abc,cd->abd\",\n   ...                                     output_shape=(None, 64),\n   ...                                     bias_axes=\"d\")\n   >>> input_tensor = tf.keras.Input(shape=[32, 128])\n@@ -95,11 +98,13 @@ class EinsumDense(Layer):\n   instead of specifying the batch and sequence dimensions.\n \n   Because we are using ellipsis notation and have specified only one axis, the\n-  output_shape arg is a single value. When instantiated in this way, the layer\n+  `output_shape` arg is a single value. When instantiated in this way, the layer\n   can handle any number of sequence dimensions - including the case where no\n   sequence dimension exists.\n \n-  >>> layer = EinsumDense(\"...x,xy->...y\", output_shape=64, bias_axes=\"y\")\n+  >>> layer = tf.keras.layers.EinsumDense(\"...x,xy->...y\",\n+  ...                                     output_shape=64,\n+  ...                                     bias_axes=\"y\")\n   >>> input_tensor = tf.keras.Input(shape=[32, 128])\n   >>> output_tensor = layer(input_tensor)\n   >>> output_tensor\n\n@@ -17,7 +17,7 @@\n \n from absl.testing import parameterized\n import keras\n-from keras.layers import einsum_dense\n+from keras.layers.core import einsum_dense\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n import numpy as np\n\n@@ -25,7 +25,6 @@ from keras.layers import activation\n from keras.layers import attention\n from keras.layers import convolutional\n from keras.layers import core\n-from keras.layers import einsum_dense\n from keras.layers import locally_connected\n from keras.layers import merging\n from keras.layers import noise\n@@ -55,10 +54,10 @@ from keras.utils import tf_inspect as inspect\n from tensorflow.python.util.tf_export import keras_export\n \n ALL_MODULES = (base_layer, input_layer, activation, attention, convolutional,\n-               core, einsum_dense, locally_connected, merging,\n-               batch_normalization_v1, layer_normalization, unit_normalization,\n-               pooling, image_preprocessing, regularization, reshaping, rnn,\n-               hashing, hashed_crossing, category_encoding, discretization,\n+               core, locally_connected, merging, batch_normalization_v1,\n+               layer_normalization, unit_normalization, pooling,\n+               image_preprocessing, regularization, reshaping, rnn, hashing,\n+               hashed_crossing, category_encoding, discretization,\n                integer_lookup, preprocessing_normalization, string_lookup,\n                text_vectorization)\n ALL_V2_MODULES = (batch_normalization, layer_normalization, cell_wrappers, gru,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
