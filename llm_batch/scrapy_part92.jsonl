{"custom_id": "scrapy#35a7059636182d40de9f8b0080e19aa00fd5a19f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 113 | Lines Deleted: 91 | Files Changed: 6 | Hunks: 33 | Methods Changed: 11 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 204 | Churn Cumulative: 585 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 16 | DMM Complexity: 0.47368421052631576\n\nDIFF:\n@@ -20,6 +20,8 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n+        parser.add_option(\"--spider\", dest=\"spider\", default=None, \\\n+            help=\"always use this spider when arguments are urls\")\n         parser.add_option(\"-n\", \"--nofollow\", dest=\"nofollow\", action=\"store_true\", \\\n             help=\"don't follow links (for use with URLs only)\")\n \n@@ -29,34 +31,30 @@ class Command(ScrapyCommand):\n             settings.overrides['CRAWLSPIDER_FOLLOW_LINKS'] = False\n \n     def run(self, args, opts):\n-        if opts.spider:\n-            spider = spiders.create(opts.spider)\n-        else:\n-            spider = None\n \n-        # aggregate urls and domains\n-        urls = []\n-        domains = []\n-        for arg in args:\n-            if is_url(arg):\n-                urls.append(arg)\n-            else:\n-                domains.append(arg)\n-\n-        # schedule first domains\n+        urls, domains = self._split_urls_and_domains(args)\n         for dom in domains:\n             scrapymanager.crawl_domain(dom)\n \n-        # if forced spider schedule urls directly\n-        if spider:\n+        if opts.spider:\n+            try:\n+                spider = spiders.create(opts.spider)\n                 for url in urls:\n                     scrapymanager.crawl_url(url, spider)\n+            except KeyError:\n+                log.msg('Could not find spider: %s' % opts.spider, log.ERROR)\n         else:\n-            # group urls by spider\n-            spider_urls = defaultdict(list)\n-            find_by_url = lambda url: spiders.find_by_request(Request(url))\n+            for name, urls in self._group_urls_by_spider(urls):\n+                spider = spiders.create(name)\n                 for url in urls:\n-                spider_names = find_by_url(url)\n+                    scrapymanager.crawl_url(url, spider)\n+\n+        scrapymanager.start()\n+\n+    def _group_urls_by_spider(self, urls):\n+        spider_urls = defaultdict(list)\n+        for url in urls:\n+            spider_names = spiders.find_by_request(Request(url))\n             if not spider_names:\n                 log.msg('Could not find spider for url: %s' % url,\n                         log.ERROR)\n@@ -65,13 +63,14 @@ class Command(ScrapyCommand):\n                         log.ERROR)\n             else:\n                 spider_urls[spider_names[0]].append(url)\n+        return spider_urls.items()\n \n-            # schedule grouped urls with same spider\n-            for name, urls in spider_urls.iteritems():\n-                # instance spider for each url-list\n-                spider = spiders.create(name)\n-                for url in urls:\n-                    scrapymanager.crawl_url(url, spider)\n-\n-        # crawl just scheduled arguments without keeping idle\n-        scrapymanager.start()\n+    def _split_urls_and_domains(self, args):\n+        urls = []\n+        domains = []\n+        for arg in args:\n+            if is_url(arg):\n+                urls.append(arg)\n+            else:\n+                domains.append(arg)\n+        return urls, domains\n\n@@ -1,7 +1,11 @@\n import pprint\n \n+from scrapy import log\n from scrapy.command import ScrapyCommand\n-from scrapy.utils.fetch import fetch\n+from scrapy.core.manager import scrapymanager\n+from scrapy.http import Request\n+from scrapy.spider import BaseSpider, spiders\n+from scrapy.utils.url import is_url\n \n class Command(ScrapyCommand):\n \n@@ -19,17 +23,33 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n+        parser.add_option(\"--spider\", dest=\"spider\",\n+            help=\"use this spider\")\n         parser.add_option(\"--headers\", dest=\"headers\", action=\"store_true\", \\\n             help=\"print response HTTP headers instead of body\")\n \n     def run(self, args, opts):\n-        if len(args) != 1:\n-            print \"One URL is required\"\n-            return\n+        if len(args) != 1 or not is_url(args[0]):\n+            return False\n+        responses = [] # to collect downloaded responses\n+        request = Request(args[0], callback=responses.append, dont_filter=True)\n \n-        responses = fetch(args)\n+        if opts.spider:\n+            try:\n+                spider = spiders.create(opts.spider)\n+            except KeyError:\n+                log.msg(\"Could not find spider: %s\" % opts.spider, log.ERROR)\n+        else:\n+            spider = scrapymanager._create_spider_for_request(request, \\\n+                BaseSpider('default'))\n+\n+        scrapymanager.crawl_request(request, spider)\n+        scrapymanager.start()\n+\n+        # display response\n         if responses:\n             if opts.headers:\n                 pprint.pprint(responses[0].headers)\n             else:\n                 print responses[0].body\n+\n\n@@ -1,11 +1,15 @@\n from scrapy.command import ScrapyCommand\n-from scrapy.utils.fetch import fetch\n+from scrapy.core.manager import scrapymanager\n from scrapy.http import Request\n from scrapy.item import BaseItem\n from scrapy.spider import spiders\n from scrapy.utils import display\n+from scrapy.utils.spider import iterate_spider_output\n+from scrapy.utils.url import is_url\n from scrapy import log\n \n+from collections import defaultdict\n+\n class Command(ScrapyCommand):\n \n     requires_project = True\n@@ -18,6 +22,8 @@ class Command(ScrapyCommand):\n \n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n+        parser.add_option(\"--spider\", dest=\"spider\", default=None, \\\n+            help=\"always use this spider\")\n         parser.add_option(\"--nolinks\", dest=\"nolinks\", action=\"store_true\", \\\n             help=\"don't show extracted links\")\n         parser.add_option(\"--noitems\", dest=\"noitems\", action=\"store_true\", \\\n@@ -37,25 +43,13 @@ class Command(ScrapyCommand):\n         return item\n \n     def run_callback(self, spider, response, callback, args, opts):\n-        spider_names = spiders.find_by_request(response.request)\n-        if not spider_names:\n-            log.msg('Cannot find spider for url: %s' % response.url,\n-                    level=log.ERROR)\n-            return (), ()\n-        elif len(spider_names) > 1:\n-            log.msg('More than one spider found for url: %s' % response.url,\n-                    level=log.ERROR)\n-            return (), ()\n-        else:\n-            spider = spiders.create(spider_names[0])\n-\n         if callback:\n             callback_fcn = callback if callable(callback) else getattr(spider, callback, None)\n             if not callback_fcn:\n                 log.msg('Cannot find callback %s in %s spider' % (callback, spider.domain_name))\n                 return (), ()\n \n-            result = callback_fcn(response)\n+            result = iterate_spider_output(callback_fcn(response))\n             links = [i for i in result if isinstance(i, Request)]\n             items = [self.pipeline_process(i, spider, opts) for i in result if \\\n                      isinstance(i, BaseItem)]\n@@ -78,36 +72,68 @@ class Command(ScrapyCommand):\n             display.pprint(list(links))\n \n     def run(self, args, opts):\n-        if not args:\n-            print \"An URL is required\"\n+        if not len(args) == 1 or not is_url(args[0]):\n+            return False\n+\n+        request = Request(args[0])\n+\n+        if opts.spider:\n+            try:\n+                spider = spiders.create(opts.spider)\n+            except KeyError:\n+                log.msg('Could not find spider: %s' % opts.spider, log.ERROR)\n+                return\n+        else:\n+            spider = scrapymanager._create_spider_for_request(request, \\\n+                log_none=True, log_multiple=True)\n+\n+        if not spider:\n             return\n \n-        for response in fetch(args):\n-            spider = spiders.fromurl(response.url)\n-            if not spider:\n-                log.msg('Cannot find spider for \"%s\"' % response.url)\n-                continue\n+        responses = [] # to collect downloaded responses\n+        request = request.replace(callback=responses.append)\n+\n+        scrapymanager.crawl_request(request, spider)\n+        scrapymanager.start()\n+\n+        if not responses:\n+            log.msg('No response returned', log.ERROR, spider=spider)\n+            return\n+\n+        # now process response\n+        #   - if callbacks defined then call each one print results\n+        #   - if --rules option given search for matching spider's rule\n+        #   - default print result using default 'parse' spider's callback\n+        response = responses[0]\n \n         if self.callbacks:\n+            # apply each callback\n             for callback in self.callbacks:\n-                    items, links = self.run_callback(spider, response, callback, args, opts)\n+                items, links = self.run_callback(spider, response,\n+                                                    callback, args, opts)\n                 self.print_results(items, links, callback, opts)\n-\n         elif opts.rules:\n-                rules = getattr(spider, 'rules', None)\n-                if rules:\n+            # search for matching spider's rule\n+            if hasattr(spider, 'rules') and spider.rules:\n                 items, links = [], []\n-                    for rule in rules:\n-                        if rule.callback and rule.link_extractor.matches(response.url):\n-                            items, links = self.run_callback(spider, response, rule.callback, args, opts)\n-                            self.print_results(items, links, rule.callback, opts)\n+                for rule in spider.rules:\n+                    if rule.link_extractor.matches(response.url) \\\n+                        and rule.callback:\n+                    \n+                        items, links = self.run_callback(spider,\n+                                            response, rule.callback,\n+                                            args, opts)\n+                        self.print_results(items, links,\n+                                            rule.callback, opts)\n+                        # first-match rule breaks rules loop\n                         break\n             else:\n-                    log.msg('No rules found for spider \"%s\", please specify a callback for parsing' \\\n-                        % spider.domain_name)\n-                    continue\n-\n+                log.msg('No rules found for spider \"%s\", ' \\\n+                        'please specify a callback for parsing' \\\n+                        % spider.domain_name, log.ERROR)\n         else:\n-                items, links = self.run_callback(spider, response, 'parse', args, opts)\n+            # default callback 'parse'\n+            items, links = self.run_callback(spider, response,\n+                                                'parse', args, opts)\n             self.print_results(items, links, 'parse', opts)\n \n\n@@ -57,8 +57,6 @@ class ScrapyCommand(object):\n             help=\"log level (default: %s)\" % settings['LOGLEVEL'])\n         group.add_option(\"--nolog\", action=\"store_true\", dest=\"nolog\", \\\n             help=\"disable logging completely\")\n-        group.add_option(\"--spider\", dest=\"spider\", default=None, \\\n-            help=\"always use this spider when arguments are urls\")\n         group.add_option(\"--profile\", dest=\"profile\", metavar=\"FILE\", default=None, \\\n             help=\"write python cProfile stats to FILE\")\n         group.add_option(\"--lsprof\", dest=\"lsprof\", metavar=\"FILE\", default=None, \\\n\n@@ -1,20 +0,0 @@\n-from scrapy.http import Request\n-from scrapy.core.manager import scrapymanager\n-\n-def fetch(urls):\n-    \"\"\"Fetch a list of urls and return a list of the downloaded Scrapy\n-    Responses.\n-\n-    This is a blocking function not suitable for calling from spiders. Instead,\n-    it is indended to be called from outside the framework such as Scrapy\n-    commands or standalone scripts.\n-    \"\"\"\n-    responses = []\n-    for url in urls:\n-        req = Request(url, callback=responses.append, dont_filter=True)\n-        # @@@ request will require a suitable spider.\n-        #     If not will not be schedule\n-        scrapymanager.crawl_request(req)\n-    scrapymanager.start()\n-    return responses\n-\n\n@@ -4,4 +4,3 @@ from scrapy.utils.misc import  arg_to_iter\n \n def iterate_spider_output(result):\n     return [result] if isinstance(result, BaseItem) else arg_to_iter(result)\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#77a4d9aba9dcd137fc772be89bcb3ecab05e95ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 528 | Contributors (this commit): 3 | Commits (past 90d): 11 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -41,7 +41,7 @@ class Command(ScrapyCommand):\n                 log.msg(\"Could not find spider: %s\" % opts.spider, log.ERROR)\n         else:\n             spider = scrapymanager._create_spider_for_request(request, \\\n-                BaseSpider('default'))\n+                BaseSpider())\n \n         scrapymanager.crawl_request(request, spider)\n         scrapymanager.start()\n\n@@ -55,7 +55,7 @@ class Shell(object):\n             request = Request(url)\n \n         spider = scrapymanager._create_spider_for_request(request, \\\n-            BaseSpider('default'), log_multiple=True)\n+            BaseSpider(), log_multiple=True)\n \n         print \"Fetching %s...\" % request\n         response = threads.blockingCallFromThread(reactor, scrapyengine.schedule, \\\n\n@@ -55,7 +55,7 @@ class BaseSpider(object_ref):\n         if not getattr(self, 'extra_domain_names', None):\n             self.extra_domain_names = self.allowed_domains\n         if not self.name:\n-            raise ValueError(\"%s must have a name\" % type(self).__name__)\n+            self.name = 'default'\n \n     def log(self, message, level=log.DEBUG):\n         \"\"\"Log the given messages at the given log level. Always use this\n\n@@ -66,9 +66,10 @@ class BaseSpiderTest(unittest.TestCase):\n \n     def test_spider_without_name(self):\n         \"\"\"Constructor arguments are assigned to spider attributes\"\"\"\n-        spider = self.spider_class('example.com')\n-        self.assertRaises(ValueError, self.spider_class)\n-        self.assertRaises(ValueError, self.spider_class, somearg='foo')\n+        spider = self.spider_class()\n+        self.assertEqual(spider.name, 'default')\n+        spider = self.spider_class(foo='bar')\n+        self.assertEqual(spider.foo, 'bar')\n \n \n class InitSpiderTest(BaseSpiderTest):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0dfec04439334b4c2da36b154b9df4703913faaa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 47 | Files Changed: 25 | Hunks: 44 | Methods Changed: 39 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 91 | Churn Cumulative: 2225 | Contributors (this commit): 6 | Commits (past 90d): 43 | Contributors (cumulative): 56 | DMM Complexity: 0.0\n\nDIFF:\n@@ -41,7 +41,7 @@ class Command(ScrapyCommand):\n                 log.msg(\"Could not find spider: %s\" % opts.spider, log.ERROR)\n         else:\n             spider = scrapymanager._create_spider_for_request(request, \\\n-                BaseSpider())\n+                BaseSpider('default'))\n \n         scrapymanager.crawl_request(request, spider)\n         scrapymanager.start()\n\n@@ -15,9 +15,9 @@ class CrawlSpider(BaseSpider):\n     request_processors = None\n     rules = []\n \n-    def __init__(self):\n+    def __init__(self, *a, **kw):\n         \"\"\"Initialize dispatcher\"\"\"\n-        super(CrawlSpider, self).__init__()\n+        super(CrawlSpider, self).__init__(*a, **kw)\n \n         # auto follow start urls\n         if self.start_urls:\n\n@@ -55,7 +55,7 @@ class Shell(object):\n             request = Request(url)\n \n         spider = scrapymanager._create_spider_for_request(request, \\\n-            BaseSpider(), log_multiple=True)\n+            BaseSpider('default'), log_multiple=True)\n \n         print \"Fetching %s...\" % request\n         response = threads.blockingCallFromThread(reactor, scrapyengine.schedule, \\\n\n@@ -52,7 +52,7 @@ class BaseSpider(object_ref):\n         if not self.allowed_domains:\n             self.allowed_domains = []\n         if not self.name:\n-            self.name = 'default'\n+            raise ValueError(\"%s must have a name\" % type(self).__name__)\n \n         # XXX: SEP-12 forward compatibility (remove for 0.10)\n         self.domain_name = self.name\n\n@@ -109,7 +109,7 @@ class CompiledRuleInitializationTest(unittest.TestCase):\n \n class RulesTest(unittest.TestCase):\n     def test_rules_manager_basic(self):\n-        spider = BaseSpider()\n+        spider = BaseSpider('foo')\n         response1 = HtmlResponse('http://example.org')\n         response2 = HtmlResponse('http://othersite.org')\n         rulesman = RulesManager([], spider)\n@@ -134,7 +134,7 @@ class RulesTest(unittest.TestCase):\n         self.failUnlessEqual(rule1.follow, True)\n \n     def test_rules_manager_empty_rule(self):\n-        spider = BaseSpider()\n+        spider = BaseSpider('foo')\n         response = HtmlResponse('http://example.org')\n \n         rulesman = RulesManager([Rule(follow=True)], spider)\n@@ -144,7 +144,7 @@ class RulesTest(unittest.TestCase):\n         self.failUnless(isinstance(rule.matcher, BaseMatcher))\n \n     def test_rules_manager_default_matcher(self):\n-        spider = BaseSpider()\n+        spider = BaseSpider('foo')\n         response = HtmlResponse('http://example.org')\n         callback = lambda x: None\n \n@@ -156,7 +156,7 @@ class RulesTest(unittest.TestCase):\n         self.failUnless(isinstance(rule.matcher, UrlMatcher))\n \n     def test_rules_manager_matchers(self):\n-        spider = BaseSpider()\n+        spider = BaseSpider('foo')\n         response1 = HtmlResponse('http://example.org')\n         response2 = HtmlResponse('http://othersite.org')\n \n@@ -200,7 +200,7 @@ class RulesTest(unittest.TestCase):\n     def test_rules_manager_callbacks(self):\n         mycallback = lambda: True\n \n-        spider = BaseSpider()\n+        spider = BaseSpider('foo')\n         spider.parse_item = lambda: True\n \n         response1 = HtmlResponse('http://example.org')\n@@ -229,7 +229,7 @@ class RulesTest(unittest.TestCase):\n \n \n     def test_rules_manager_callback_with_arguments(self):\n-        spider = BaseSpider()\n+        spider = BaseSpider('foo')\n         response = HtmlResponse('http://example.org')\n \n         kwargs = {'a': 1}\n\n@@ -60,7 +60,7 @@ class CrawlSpiderTest(unittest.TestCase):\n         Spider.request_extractors = extractors\n         Spider.request_processors = processors\n \n-        return Spider()\n+        return Spider('foo')\n \n     def test_start_url_auto_rule(self):\n         spider = self.spider_factory()\n\n@@ -32,11 +32,11 @@ class FileTestCase(unittest.TestCase):\n \n         request = Request('file://%s' % self.tmpname + '^')\n         assert request.url.upper().endswith('%5E')\n-        return download_file(request, BaseSpider()).addCallback(_test)\n+        return download_file(request, BaseSpider('foo')).addCallback(_test)\n \n     def test_non_existent(self):\n         request = Request('file://%s' % self.mktemp())\n-        d = download_file(request, BaseSpider())\n+        d = download_file(request, BaseSpider('foo'))\n         return self.assertFailure(d, IOError)\n \n \n@@ -66,20 +66,20 @@ class HttpTestCase(unittest.TestCase):\n \n     def test_download(self):\n         request = Request(self.getURL('file'))\n-        d = download_http(request, BaseSpider())\n+        d = download_http(request, BaseSpider('foo'))\n         d.addCallback(lambda r: r.body)\n         d.addCallback(self.assertEquals, \"0123456789\")\n         return d\n \n     def test_redirect_status(self):\n         request = Request(self.getURL('redirect'))\n-        d = download_http(request, BaseSpider())\n+        d = download_http(request, BaseSpider('foo'))\n         d.addCallback(lambda r: r.status)\n         d.addCallback(self.assertEquals, 302)\n         return d\n \n     def test_timeout_download_from_spider(self):\n-        spider = BaseSpider()\n+        spider = BaseSpider('foo')\n         spider.download_timeout = 0.000001\n         request = Request(self.getURL('wait'))\n         d = download_http(request, spider)\n@@ -91,7 +91,7 @@ class HttpTestCase(unittest.TestCase):\n             self.assertEquals(request.headers, {})\n \n         request = Request(self.getURL('host'))\n-        return download_http(request, BaseSpider()).addCallback(_test)\n+        return download_http(request, BaseSpider('foo')).addCallback(_test)\n \n     def test_host_header_seted_in_request_headers(self):\n         def _test(response):\n@@ -99,9 +99,9 @@ class HttpTestCase(unittest.TestCase):\n             self.assertEquals(request.headers.get('Host'), 'example.com')\n \n         request = Request(self.getURL('host'), headers={'Host': 'example.com'})\n-        return download_http(request, BaseSpider()).addCallback(_test)\n+        return download_http(request, BaseSpider('foo')).addCallback(_test)\n \n-        d = download_http(request, BaseSpider())\n+        d = download_http(request, BaseSpider('foo'))\n         d.addCallback(lambda r: r.body)\n         d.addCallback(self.assertEquals, 'example.com')\n         return d\n@@ -109,14 +109,14 @@ class HttpTestCase(unittest.TestCase):\n     def test_payload(self):\n         body = '1'*100 # PayloadResource requires body length to be 100\n         request = Request(self.getURL('payload'), method='POST', body=body)\n-        d = download_http(request, BaseSpider())\n+        d = download_http(request, BaseSpider('foo'))\n         d.addCallback(lambda r: r.body)\n         d.addCallback(self.assertEquals, body)\n         return d\n \n     def test_broken_download(self):\n         request = Request(self.getURL('broken'))\n-        d = download_http(request, BaseSpider())\n+        d = download_http(request, BaseSpider('foo'))\n         return self.assertFailure(d, PartialDownloadError)\n \n \n@@ -152,7 +152,7 @@ class HttpProxyTestCase(unittest.TestCase):\n \n         http_proxy = self.getURL('')\n         request = Request('https://example.com', meta={'proxy': http_proxy})\n-        return download_http(request, BaseSpider()).addCallback(_test)\n+        return download_http(request, BaseSpider('foo')).addCallback(_test)\n \n     def test_download_without_proxy(self):\n         def _test(response):\n@@ -161,4 +161,4 @@ class HttpProxyTestCase(unittest.TestCase):\n             self.assertEquals(response.body, '/path/to/resource')\n \n         request = Request(self.getURL('path/to/resource'))\n-        return download_http(request, BaseSpider()).addCallback(_test)\n+        return download_http(request, BaseSpider('foo')).addCallback(_test)\n\n@@ -10,7 +10,7 @@ from scrapy.contrib.downloadermiddleware.cookies import CookiesMiddleware\n class CookiesMiddlewareTest(TestCase):\n \n     def setUp(self):\n-        self.spider = BaseSpider()\n+        self.spider = BaseSpider('foo')\n         self.mw = CookiesMiddleware()\n \n     def tearDown(self):\n\n@@ -21,7 +21,7 @@ class DecompressionMiddlewareTest(TestCase):\n \n     def setUp(self):\n         self.mw = DecompressionMiddleware()\n-        self.spider = BaseSpider()\n+        self.spider = BaseSpider('foo')\n \n     def test_known_compression_formats(self):\n         for fmt in self.test_formats:\n\n@@ -9,7 +9,7 @@ from scrapy.spider import BaseSpider\n class TestDefaultHeadersMiddleware(TestCase):\n \n     def setUp(self):\n-        self.spider = BaseSpider()\n+        self.spider = BaseSpider('foo')\n         self.mw = DefaultHeadersMiddleware()\n         self.default_headers = dict([(k, [v]) for k, v in \\\n             settings.get('DEFAULT_REQUEST_HEADERS').iteritems()])\n\n@@ -18,7 +18,7 @@ class HttpAuthMiddlewareTest(unittest.TestCase):\n \n     def test_auth(self):\n         self.mw.default_useragent = 'default_useragent'\n-        spider = TestSpider()\n+        spider = TestSpider('foo')\n         req = Request('http://scrapytest.org/')\n         assert self.mw.process_request(req, spider) is None\n         self.assertEquals(req.headers['Authorization'], 'Basic Zm9vOmJhcg==')\n\n@@ -20,7 +20,7 @@ FORMAT = {\n class HttpCompressionTest(TestCase):\n \n     def setUp(self):\n-        self.spider = BaseSpider()\n+        self.spider = BaseSpider('foo')\n         self.mw = HttpCompressionMiddleware()\n \n     def _getresponse(self, coding):\n\n@@ -8,7 +8,7 @@ from scrapy.http import Response, Request\n from scrapy.spider import BaseSpider\n from scrapy.conf import settings\n \n-spider = BaseSpider()\n+spider = BaseSpider('foo')\n \n class TestDefaultHeadersMiddleware(TestCase):\n \n\n@@ -8,7 +8,7 @@ from scrapy.http import Request, Response, HtmlResponse, Headers\n class RedirectMiddlewareTest(unittest.TestCase):\n \n     def setUp(self):\n-        self.spider = BaseSpider()\n+        self.spider = BaseSpider('foo')\n         self.mw = RedirectMiddleware()\n \n     def test_priority_adjust(self):\n\n@@ -10,7 +10,7 @@ from scrapy.http import Request, Response\n \n class RetryTest(unittest.TestCase):\n     def setUp(self):\n-        self.spider = BaseSpider()\n+        self.spider = BaseSpider('foo')\n         self.mw = RetryMiddleware()\n         self.mw.max_retry_times = 2\n \n\n@@ -9,7 +9,7 @@ from scrapy.conf import settings\n class UserAgentMiddlewareTest(TestCase):\n \n     def setUp(self):\n-        self.spider = BaseSpider()\n+        self.spider = BaseSpider('foo')\n         self.mw = UserAgentMiddleware()\n \n     def tearDown(self):\n\n@@ -8,7 +8,7 @@ from scrapy.contrib.dupefilter import RequestFingerprintDupeFilter, NullDupeFilt\n class RequestFingerprintDupeFilterTest(unittest.TestCase):\n \n     def test_filter(self):\n-        spider = BaseSpider()\n+        spider = BaseSpider('foo')\n         filter = RequestFingerprintDupeFilter()\n         filter.open_spider(spider)\n \n@@ -28,7 +28,7 @@ class RequestFingerprintDupeFilterTest(unittest.TestCase):\n class NullDupeFilterTest(unittest.TestCase):\n \n     def test_filter(self):\n-        spider = BaseSpider()\n+        spider = BaseSpider('foo')\n         filter = NullDupeFilter()\n         filter.open_spider(spider)\n \n\n@@ -10,7 +10,7 @@ class DuplicatesFilterMiddlewareTest(unittest.TestCase):\n \n     def setUp(self):\n         self.mw = DuplicatesFilterMiddleware()\n-        self.spider = BaseSpider()\n+        self.spider = BaseSpider('foo')\n         self.mw.open_spider(self.spider)\n \n     def tearDown(self):\n\n@@ -9,7 +9,6 @@ from scrapy.spider import BaseSpider\n from scrapy.contrib.spiders.init import InitSpider\n from scrapy.contrib.spiders.crawl import CrawlSpider\n from scrapy.contrib.spiders.feed import XMLFeedSpider, CSVFeedSpider\n-from scrapy.contrib.dupefilter import RequestFingerprintDupeFilter, NullDupeFilter\n \n \n class BaseSpiderTest(unittest.TestCase):\n@@ -77,10 +76,8 @@ class BaseSpiderTest(unittest.TestCase):\n \n     def test_spider_without_name(self):\n         \"\"\"Constructor arguments are assigned to spider attributes\"\"\"\n-        spider = self.spider_class()\n-        self.assertEqual(spider.name, 'default')\n-        spider = self.spider_class(foo='bar')\n-        self.assertEqual(spider.foo, 'bar')\n+        self.assertRaises(ValueError, self.spider_class)\n+        self.assertRaises(ValueError, self.spider_class, somearg='foo')\n \n \n class InitSpiderTest(BaseSpiderTest):\n\n@@ -8,7 +8,7 @@ from scrapy.contrib.spidermiddleware.httperror import HttpErrorMiddleware\n class TestHttpErrorMiddleware(TestCase):\n \n     def setUp(self):\n-        self.spider = BaseSpider()\n+        self.spider = BaseSpider('foo')\n         self.mw = HttpErrorMiddleware()\n         self.req = Request('http://scrapytest.org')\n \n\n@@ -8,7 +8,7 @@ from scrapy.contrib.spidermiddleware.offsite import OffsiteMiddleware\n class TestOffsiteMiddleware(TestCase):\n \n     def setUp(self):\n-        self.spider = BaseSpider()\n+        self.spider = BaseSpider('foo')\n         self.spider.name = 'scrapytest.org'\n         self.spider.allowed_domains = ['scrapytest.org', 'scrapy.org']\n \n\n@@ -8,7 +8,7 @@ from scrapy.contrib.spidermiddleware.referer import RefererMiddleware\n class TestRefererMiddleware(TestCase):\n \n     def setUp(self):\n-        self.spider = BaseSpider()\n+        self.spider = BaseSpider('foo')\n         self.mw = RefererMiddleware()\n \n     def test_process_spider_output(self):\n\n@@ -9,7 +9,7 @@ from scrapy.utils.url import canonicalize_url\n class TestUrlFilterMiddleware(TestCase):\n \n     def setUp(self):\n-        self.spider = BaseSpider()\n+        self.spider = BaseSpider('foo')\n         self.mw = UrlFilterMiddleware()\n \n     def test_process_spider_output(self):\n\n@@ -12,7 +12,7 @@ class TestUrlLengthMiddleware(TestCase):\n         settings.disabled = False\n         settings.overrides['URLLENGTH_LIMIT'] = 25\n \n-        self.spider = BaseSpider()\n+        self.spider = BaseSpider('foo')\n         self.mw = UrlLengthMiddleware()\n \n     def test_process_spider_output(self):\n\n@@ -9,7 +9,7 @@ from scrapy.stats.signals import stats_spider_opened, stats_spider_closing, \\\n class StatsCollectorTest(unittest.TestCase):\n \n     def setUp(self):\n-        self.spider = BaseSpider()\n+        self.spider = BaseSpider('foo')\n \n     def test_collector(self):\n         stats = StatsCollector()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0b07742adb77e34982ebd9a5c9f32b8e8aad737d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 31 | Churn Cumulative: 601 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -74,18 +74,35 @@ DUPEFILTER_CLASS = 'scrapy.contrib.dupefilter.RequestFingerprintDupeFilter'\n ENCODING_ALIASES = {}\n \n ENCODING_ALIASES_BASE = {\n-    'zh-cn': 'gb2312',\n-    'win-1251': 'cp1251',\n-    'macintosh' : 'mac-roman',\n-    'x-sjis': 'shift-jis',\n+    # gb2312 is superseded by gb18030\n+    'gb2312': 'gb18030',\n+    'chinese': 'gb18030',\n+    'csiso58gb231280': 'gb18030',\n+    'euc- cn': 'gb18030',\n+    'euccn': 'gb18030',\n+    'eucgb2312-cn': 'gb18030',\n+    'gb2312-1980': 'gb18030',\n+    'gb2312-80': 'gb18030',\n+    'iso- ir-58': 'gb18030',\n+    # gbk is superseded by gb18030\n+    'gbk': 'gb18030',\n+    '936': 'gb18030',\n+    'cp936': 'gb18030',\n+    'ms936': 'gb18030',\n+    # latin_1 is a subset of cp1252\n+    'latin_1': 'cp1252',\n     'iso-8859-1': 'cp1252',\n     'iso8859-1': 'cp1252',\n     '8859': 'cp1252',\n     'cp819': 'cp1252',\n     'latin': 'cp1252',\n     'latin1': 'cp1252',\n-    'latin_1': 'cp1252',\n     'l1': 'cp1252',\n+    # others\n+    'zh-cn': 'gb18030',\n+    'win-1251': 'cp1251',\n+    'macintosh' : 'mac_roman',\n+    'x-sjis': 'shift_jis',\n }\n \n EXTENSIONS = {}\n\n@@ -179,6 +179,8 @@ class TextResponseTest(BaseResponseTest):\n         r3 = self.response_class(\"http://www.example.com\", headers={\"Content-type\": [\"text/html; charset=iso-8859-1\"]}, body=\"\\xa3\")\n         r4 = self.response_class(\"http://www.example.com\", body=\"\\xa2\\xa3\")\n         r5 = self.response_class(\"http://www.example.com\", headers={\"Content-type\": [\"text/html; charset=None\"]}, body=\"\\xc2\\xa3\")\n+        r6 = self.response_class(\"http://www.example.com\", headers={\"Content-type\": [\"text/html; charset=gb2312\"]}, body=\"\\xa8D\")\n+        r7 = self.response_class(\"http://www.example.com\", headers={\"Content-type\": [\"text/html; charset=gbk\"]}, body=\"\\xa8D\")\n \n         self.assertEqual(r1._headers_encoding(), \"utf-8\")\n         self.assertEqual(r2._headers_encoding(), None)\n@@ -193,6 +195,8 @@ class TextResponseTest(BaseResponseTest):\n         self._assert_response_values(r1, 'utf-8', u\"\\xa3\")\n         self._assert_response_values(r2, 'utf-8', u\"\\xa3\")\n         self._assert_response_values(r3, 'iso-8859-1', u\"\\xa3\")\n+        self._assert_response_values(r6, 'gb18030', u\"\\u2015\")\n+        self._assert_response_values(r7, 'gb18030', u\"\\u2015\")\n \n         # TextResponse (and subclasses) must be passed a encoding when instantiating with unicode bodies\n         self.assertRaises(TypeError, self.response_class, \"http://www.example.com\", body=u\"\\xa3\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3fcd69c3476e88e0eed2285327ce89f5f5c9a412", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 23 | Churn Cumulative: 62 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import weakref\n import unittest\n \n # just a hack to avoid cyclic imports of scrapy.spider when running this test\n@@ -25,6 +26,14 @@ class TwistedPluginSpiderManagerTest(unittest.TestCase):\n         self.assertEqual(spider2.__class__.__name__, 'Spider2')\n         self.assertEqual(spider2.foo, 'bar')\n \n+    def test_create_uses_cache(self):\n+        # TwistedPluginSpiderManager uses an internal cache which is\n+        # invalidated in close_spider() but this isn't necessarily the best\n+        # thing to do in all cases.\n+        spider1 = self.spiderman.create(\"spider1\")\n+        spider2 = self.spiderman.create(\"spider1\")\n+        assert spider1 is spider2\n+\n     def test_find_by_request(self):\n         self.assertEqual(self.spiderman.find_by_request(Request('http://scrapy1.org/test')),\n             ['spider1'])\n@@ -35,5 +44,19 @@ class TwistedPluginSpiderManagerTest(unittest.TestCase):\n         self.assertEqual(self.spiderman.find_by_request(Request('http://scrapy999.org/test')),\n             [])\n \n+    def test_close_spider_remove_refs(self):\n+        spider = self.spiderman.create(\"spider1\")\n+        wref = weakref.ref(spider)\n+        assert wref()\n+        self.spiderman.close_spider(spider)\n+        del spider\n+        assert not wref()\n+\n+    def test_close_spider_invalidates_cache(self):\n+        spider1 = self.spiderman.create(\"spider1\")\n+        self.spiderman.close_spider(spider1)\n+        spider2 = self.spiderman.create(\"spider1\")\n+        assert spider1 is not spider2\n+\n if __name__ == '__main__':\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8b86e1d008b84eee6fe4d2b73a39bf904b79b68a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 490 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,14 +6,19 @@ See documentation in docs/topics/request-response.rst\n \"\"\"\n \n import re\n-\n+import codecs\n from scrapy.xlib.BeautifulSoup import UnicodeDammit\n-\n from scrapy.http.response import Response\n from scrapy.utils.python import memoizemethod_noargs\n from scrapy.utils.encoding import encoding_exists, resolve_encoding\n from scrapy.conf import settings\n \n+\n+# Python decoder doesn't follow unicode standard when handling\n+# bad utf-8 encoded strings. see http://bugs.python.org/issue8271\n+codecs.register_error('scrapy_replace', lambda exc: (u'\\ufffd', exc.start+1))\n+\n+\n class TextResponse(Response):\n \n     _DEFAULT_ENCODING = settings['DEFAULT_RESPONSE_ENCODING']\n@@ -77,7 +82,7 @@ class TextResponse(Response):\n     def body_as_unicode(self):\n         \"\"\"Return body as unicode\"\"\"\n         if self._cached_ubody is None:\n-            self._cached_ubody = self.body.decode(self.encoding, 'replace')\n+            self._cached_ubody = self.body.decode(self.encoding, 'scrapy_replace')\n         return self._cached_ubody\n \n     @memoizemethod_noargs\n\n@@ -215,7 +215,7 @@ class TextResponseTest(BaseResponseTest):\n     def test_invalid_utf8_encoded_body_with_valid_utf8_BOM(self):\n         r6 = self.response_class(\"http://www.example.com\", headers={\"Content-type\": [\"text/html; charset=utf-8\"]}, body=\"\\xef\\xbb\\xbfWORD\\xe3\\xab\")\n         self.assertEqual(r6.encoding, 'utf-8')\n-        self.assertEqual(r6.body_as_unicode(), u'\\ufeffWORD\\ufffd')\n+        self.assertEqual(r6.body_as_unicode(), u'\\ufeffWORD\\ufffd\\ufffd')\n \n     def test_replace_wrong_encoding(self):\n         \"\"\"Test invalid chars are replaced properly\"\"\"\n@@ -223,6 +223,13 @@ class TextResponseTest(BaseResponseTest):\n         # XXX: Policy for replacing invalid chars may suffer minor variations\n         # but it should always contain the unicode replacement char (u'\\ufffd')\n         assert u'\\ufffd' in r.body_as_unicode(), repr(r.body_as_unicode())\n+        assert u'PREFIX' in r.body_as_unicode(), repr(r.body_as_unicode())\n+        assert u'SUFFIX' in r.body_as_unicode(), repr(r.body_as_unicode())\n+\n+        # Do not destroy html tags due to encoding bugs\n+        r = self.response_class(\"http://example.com\", encoding='utf-8', \\\n+                body='\\xf0<span>value</span>')\n+        assert u'<span>value</span>' in r.body_as_unicode(), repr(r.body_as_unicode())\n \n         # FIXME: This test should pass once we stop using BeautifulSoup's UnicodeDammit in TextResponse\n         #r = self.response_class(\"http://www.example.com\", body='PREFIX\\xe3\\xabSUFFIX')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0dbb5d44ae8f841d60a34ce507ca54ca8cf49a23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 15 | Churn Cumulative: 584 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,13 +5,13 @@ because Amazon Web Service use timestamps for authentication.\n \"\"\"\n \n import os\n-import time\n-\n-from scrapy.utils.httpobj import urlparse_cached\n+from time import strftime, gmtime\n from scrapy.utils.aws import sign_request\n from scrapy.conf import settings\n \n+\n class AWSMiddleware(object):\n+\n     def __init__(self):\n         self.access_key = settings['AWS_ACCESS_KEY_ID'] or \\\n             os.environ.get('AWS_ACCESS_KEY_ID')\n@@ -19,9 +19,6 @@ class AWSMiddleware(object):\n             os.environ.get('AWS_SECRET_ACCESS_KEY')\n \n     def process_request(self, request, spider):\n-        hostname = urlparse_cached(request).hostname\n-        if spider.name == 's3.amazonaws.com' \\\n-                or (hostname and hostname.endswith('s3.amazonaws.com')):\n-            request.headers['Date'] = time.strftime(\"%a, %d %b %Y %H:%M:%S GMT\", \\\n-                time.gmtime())\n+        if request.meta.get('sign_s3_request'):\n+            request.headers['Date'] = strftime(\"%a, %d %b %Y %H:%M:%S GMT\", gmtime())\n             sign_request(request, self.access_key, self.secret_key)\n\n@@ -143,7 +143,7 @@ class S3ImagesStore(object):\n     def _build_request(self, key, method, body=None, headers=None):\n         url = 'http://%s.s3.amazonaws.com/%s%s' % (self.bucket, self.prefix, key)\n         return Request(url, method=method, body=body, headers=headers, \\\n-                priority=self.request_priority)\n+                meta={'sign_s3_request': True}, priority=self.request_priority)\n \n     def _download_request(self, request, info):\n         \"\"\"This method is used for HEAD and PUT requests sent to amazon S3\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#be45acd4576f75dbc063ec5058f071f1384dd00a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 97 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 8 | Complexity Δ (Sum/Max): 18/10 | Churn Δ: 97 | Churn Cumulative: 97 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.6226415094339622\n\nDIFF:\n@@ -0,0 +1,60 @@\n+import sys, os\n+\n+from twisted.python import log\n+from twisted.internet import reactor, protocol, error\n+from twisted.application.service import Service\n+\n+from scrapy.utils.py26 import cpu_count\n+from scrapy.conf import settings\n+\n+\n+class ScrapyService(Service):\n+\n+    def startService(self):\n+        reactor.callWhenRunning(self.start_processes)\n+\n+    def start_processes(self):\n+        for i in range(cpu_count()):\n+            self.start_process(i+1)\n+\n+    def start_process(self, id):\n+        args = [sys.executable, '-m', 'scrapy.service']\n+        env = os.environ.copy()\n+        self.set_log_file(env, id)\n+        pp = ScrapyProcessProtocol(self, id, env.get('SCRAPY_LOG_FILE'))\n+        reactor.spawnProcess(pp, sys.executable, args=args, env=env)\n+\n+    def set_log_file(self, env, suffix):\n+        logfile = settings['LOG_FILE']\n+        if logfile:\n+            file, ext = os.path.splitext(logfile)\n+            env['SCRAPY_LOG_FILE'] = \"%s-%s%s\" % (file, suffix, ext)\n+\n+\n+class ScrapyProcessProtocol(protocol.ProcessProtocol):\n+\n+    def __init__(self, service, id, logfile):\n+        self.service = service\n+        self.id = id\n+        self.logfile = logfile\n+        self.pid = None\n+\n+    def connectionMade(self):\n+        self.pid = self.transport.pid\n+        log.msg(\"Process %r started: pid=%r logfile=%r\" % (self.id, self.pid, \\\n+            self.logfile))\n+\n+    def processEnded(self, status):\n+        if isinstance(status.value, error.ProcessDone):\n+            log.msg(\"Process %r finished: pid=%r logfile=%r\" % (self.id, \\\n+                self.pid, self.logfile))\n+        else:\n+            log.msg(\"Process %r died: exitstatus=%r pid=%r logfile=%r\" % \\\n+                (self.id, status.value.exitCode, self.pid, self.logfile))\n+        reactor.callLater(5, self.service.start_process, self.id)\n+\n+\n+if __name__ == '__main__':\n+    from scrapy.core.manager import scrapymanager\n+    scrapymanager.configure()\n+    scrapymanager.start(keep_alive=True)\n\n@@ -0,0 +1,37 @@\n+\"\"\"\n+This module provides functions added in Python 2.6, which weren't yet available\n+in Python 2.5. The Python 2.6 function is used when available.\n+\"\"\"\n+\n+import sys, os\n+\n+try:\n+    import multiprocessing\n+    cpu_count = multiprocessing.cpu_count\n+except ImportError:\n+    def cpu_count():\n+        '''\n+        Returns the number of CPUs in the system\n+        '''\n+        if sys.platform == 'win32':\n+            try:\n+                num = int(os.environ['NUMBER_OF_PROCESSORS'])\n+            except (ValueError, KeyError):\n+                num = 0\n+        elif 'bsd' in sys.platform or sys.platform == 'darwin':\n+            try:\n+                num = int(os.popen('sysctl -n hw.ncpu').read())\n+            except ValueError:\n+                num = 0\n+        else:\n+            try:\n+                num = os.sysconf('SC_NPROCESSORS_ONLN')\n+            except (ValueError, OSError, AttributeError):\n+                num = 0\n+\n+        if num >= 1:\n+            return num\n+        else:\n+            raise NotImplementedError('cannot determine number of cpus')\n+\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#650d1c4fbe68440e38d9387f0563f02c27f81b65", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 61 | Lines Deleted: 67 | Files Changed: 2 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/16 | Churn Δ: 128 | Churn Cumulative: 254 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -3,7 +3,12 @@ This module provides functions added in Python 2.6, which weren't yet available\n in Python 2.5. The Python 2.6 function is used when available.\n \"\"\"\n \n-import sys, os\n+import sys\n+import os\n+import fnmatch\n+from shutil import copytree, ignore_patterns, copy2, copystat\n+\n+__all__ = ['cpu_count', 'copytree', 'ignore_patterns']\n \n try:\n     import multiprocessing\n@@ -34,4 +39,59 @@ except ImportError:\n         else:\n             raise NotImplementedError('cannot determine number of cpus')\n \n+if sys.version_info < (2, 6):\n+    try:\n+        WindowsError\n+    except NameError:\n+        WindowsError = None\n \n+    class Error(EnvironmentError):\n+        pass\n+\n+    def ignore_patterns(*patterns):\n+        def _ignore_patterns(path, names):\n+            ignored_names = []\n+            for pattern in patterns:\n+                ignored_names.extend(fnmatch.filter(names, pattern))\n+            return set(ignored_names)\n+        return _ignore_patterns\n+\n+    def copytree(src, dst, symlinks=False, ignore=None):\n+        names = os.listdir(src)\n+        if ignore is not None:\n+            ignored_names = ignore(src, names)\n+        else:\n+            ignored_names = set()\n+\n+        os.makedirs(dst)\n+        errors = []\n+        for name in names:\n+            if name in ignored_names:\n+                continue\n+            srcname = os.path.join(src, name)\n+            dstname = os.path.join(dst, name)\n+            try:\n+                if symlinks and os.path.islink(srcname):\n+                    linkto = os.readlink(srcname)\n+                    os.symlink(linkto, dstname)\n+                elif os.path.isdir(srcname):\n+                    copytree(srcname, dstname, symlinks, ignore)\n+                else:\n+                    copy2(srcname, dstname)\n+                # XXX What about devices, sockets etc.?\n+            except (IOError, os.error), why:\n+                errors.append((srcname, dstname, str(why)))\n+            # catch the Error from the recursive copytree so that we can\n+            # continue with other files\n+            except Error, err:\n+                errors.extend(err.args[0])\n+        try:\n+            copystat(src, dst)\n+        except OSError, why:\n+            if WindowsError is not None and isinstance(why, WindowsError):\n+                # Copying file access times may fail on Windows\n+                pass\n+            else:\n+                errors.extend((src, dst, str(why)))\n+        if errors:\n+            raise Error, errors\n\n@@ -6,12 +6,9 @@ higher than 2.5 which is the lowest version supported by Scrapy.\n \n \"\"\"\n import re\n-import os\n-import fnmatch\n import inspect\n import weakref\n from functools import wraps\n-from shutil import copy2, copystat\n from sgmllib import SGMLParser\n \n class FixedSGMLParser(SGMLParser):\n@@ -146,68 +143,6 @@ def isbinarytext(text):\n     assert isinstance(text, str), \"text must be str, got '%s'\" % type(text).__name__\n     return any(c in _BINARYCHARS for c in text)\n \n-\n-# ----- shutil.copytree function from Python 2.6 adds ignore argument ---- #\n-\n-try:\n-    WindowsError\n-except NameError:\n-    WindowsError = None\n-\n-class Error(EnvironmentError):\n-    pass\n-\n-def ignore_patterns(*patterns):\n-    def _ignore_patterns(path, names):\n-        ignored_names = []\n-        for pattern in patterns:\n-            ignored_names.extend(fnmatch.filter(names, pattern))\n-        return set(ignored_names)\n-    return _ignore_patterns\n-\n-def copytree(src, dst, symlinks=False, ignore=None):\n-    names = os.listdir(src)\n-    if ignore is not None:\n-        ignored_names = ignore(src, names)\n-    else:\n-        ignored_names = set()\n-\n-    os.makedirs(dst)\n-    errors = []\n-    for name in names:\n-        if name in ignored_names:\n-            continue\n-        srcname = os.path.join(src, name)\n-        dstname = os.path.join(dst, name)\n-        try:\n-            if symlinks and os.path.islink(srcname):\n-                linkto = os.readlink(srcname)\n-                os.symlink(linkto, dstname)\n-            elif os.path.isdir(srcname):\n-                copytree(srcname, dstname, symlinks, ignore)\n-            else:\n-                copy2(srcname, dstname)\n-            # XXX What about devices, sockets etc.?\n-        except (IOError, os.error), why:\n-            errors.append((srcname, dstname, str(why)))\n-        # catch the Error from the recursive copytree so that we can\n-        # continue with other files\n-        except Error, err:\n-            errors.extend(err.args[0])\n-    try:\n-        copystat(src, dst)\n-    except OSError, why:\n-        if WindowsError is not None and isinstance(why, WindowsError):\n-            # Copying file access times may fail on Windows\n-            pass\n-        else:\n-            errors.extend((src, dst, str(why)))\n-    if errors:\n-        raise Error, errors\n-\n-# ----- end of shutil.copytree function from Python 2.6 ---- #\n-\n-\n def get_func_args(func):\n     \"\"\"Return the argument name list of a callable\"\"\"\n     if inspect.isfunction(func):\n@@ -221,7 +156,6 @@ def get_func_args(func):\n         raise TypeError('%s is not callable' % type(func))\n     return func_args\n \n-\n def equal_attributes(obj1, obj2, attributes):\n     \"\"\"Compare two objects attributes\"\"\"\n     # not attributes given return False by default\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
