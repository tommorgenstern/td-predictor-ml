{"custom_id": "scrapy#5bacc4822e80a78a2a287a5d1e417c0d3b7ddbdf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 15 | Files Changed: 5 | Hunks: 10 | Methods Changed: 10 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 20 | Churn Cumulative: 3531 | Contributors (this commit): 29 | Commits (past 90d): 10 | Contributors (cumulative): 61 | DMM Complexity: None\n\nDIFF:\n@@ -151,11 +151,9 @@ class Stream:\n \n         self._deferred_response = Deferred(_cancel)\n     \n-    def __str__(self) -> str:\n+    def __repr__(self):\n         return f'Stream(id={self.stream_id!r})'\n \n-    __repr__ = __str__\n-\n     @property\n     def _log_warnsize(self) -> bool:\n         \"\"\"Checks if we have received data which exceeds the download warnsize\n\n@@ -109,11 +109,9 @@ class Request(object_ref):\n     def encoding(self):\n         return self._encoding\n \n-    def __str__(self):\n+    def __repr__(self):\n         return f\"<{self.method} {self.url}>\"\n \n-    __repr__ = __str__\n-\n     def copy(self):\n         \"\"\"Return a copy of this Request\"\"\"\n         return self.replace()\n\n@@ -87,11 +87,9 @@ class Response(object_ref):\n \n     body = property(_get_body, obsolete_setter(_set_body, 'body'))\n \n-    def __str__(self):\n+    def __repr__(self):\n         return f\"<{self.status} {self.url}>\"\n \n-    __repr__ = __str__\n-\n     def copy(self):\n         \"\"\"Return a copy of this Response\"\"\"\n         return self.replace()\n\n@@ -51,11 +51,9 @@ class SettingsAttribute:\n             self.value = value\n             self.priority = priority\n \n-    def __str__(self):\n+    def __repr__(self):\n         return f\"<SettingsAttribute value={self.value!r} priority={self.priority}>\"\n \n-    __repr__ = __str__\n-\n \n class BaseSettings(MutableMapping):\n     \"\"\"\n\n@@ -106,11 +106,9 @@ class Spider(object_ref):\n         if callable(closed):\n             return closed(reason)\n \n-    def __str__(self):\n+    def __repr__(self):\n         return f\"<{type(self).__name__} {self.name!r} at 0x{id(self):0x}>\"\n \n-    __repr__ = __str__\n-\n \n # Top-level imports\n from scrapy.spiders.crawl import CrawlSpider, Rule\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#23cfdb058e80a18ee2b66e1b966355f3aca426d0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 7 | Files Changed: 7 | Hunks: 16 | Methods Changed: 9 | Complexity Δ (Sum/Max): -12/0 | Churn Δ: 30 | Churn Cumulative: 6491 | Contributors (this commit): 44 | Commits (past 90d): 8 | Contributors (cumulative): 103 | DMM Complexity: 1.0\n\nDIFF:\n@@ -21,7 +21,8 @@ collect_ignore = [\n     *_py_files(\"tests/CrawlerRunner\"),\n ]\n \n-for line in open('tests/ignores.txt'):\n+with open('tests/ignores.txt') as reader:\n+    for line in reader:\n         file_path = line.strip()\n         if file_path and file_path[0] != '#':\n             collect_ignore.append(file_path)\n\n@@ -6,12 +6,14 @@ import tempfile\n import unittest\n from io import BytesIO\n from datetime import datetime\n+from warnings import catch_warnings, filterwarnings\n \n import lxml.etree\n from itemadapter import ItemAdapter\n \n from scrapy.item import Item, Field\n from scrapy.utils.python import to_unicode\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.exporters import (\n     BaseItemExporter, PprintItemExporter, PickleItemExporter, CsvItemExporter,\n     XmlItemExporter, JsonLinesItemExporter, JsonItemExporter,\n@@ -172,6 +174,8 @@ class PythonItemExporterTest(BaseItemExporterTest):\n         self.assertEqual(type(exported['age'][0]['age'][0]), dict)\n \n     def test_export_binary(self):\n+        with catch_warnings():\n+            filterwarnings('ignore', category=ScrapyDeprecationWarning)\n             exporter = PythonItemExporter(binary=True)\n             value = self.item_class(name='John\\xa3', age='22')\n             expected = {b'name': b'John\\xc2\\xa3', b'age': b'22'}\n\n@@ -515,7 +515,7 @@ class FromCrawlerFileFeedStorage(FileFeedStorage, FromCrawlerMixin):\n \n class DummyBlockingFeedStorage(BlockingFeedStorage):\n \n-    def __init__(self, uri):\n+    def __init__(self, uri, *args, feed_options=None):\n         self.path = file_uri_to_path(uri)\n \n     def _store_in_thread(self, file):\n@@ -541,7 +541,7 @@ class LogOnStoreFileStorage:\n     It can be used to make sure `store` method is invoked.\n     \"\"\"\n \n-    def __init__(self, uri):\n+    def __init__(self, uri, feed_options=None):\n         self.path = file_uri_to_path(uri)\n         self.logger = getLogger()\n \n\n@@ -1,6 +1,6 @@\n import unittest\n from unittest import mock\n-from warnings import catch_warnings\n+from warnings import catch_warnings, filterwarnings\n \n from w3lib.encoding import resolve_encoding\n \n@@ -134,6 +134,8 @@ class BaseResponseTest(unittest.TestCase):\n         assert isinstance(response.text, str)\n         self._assert_response_encoding(response, encoding)\n         self.assertEqual(response.body, body_bytes)\n+        with catch_warnings():\n+            filterwarnings(\"ignore\", category=ScrapyDeprecationWarning)\n             self.assertEqual(response.body_as_unicode(), body_unicode)\n         self.assertEqual(response.text, body_unicode)\n \n@@ -345,6 +347,8 @@ class TextResponseTest(BaseResponseTest):\n         r1 = self.response_class('http://www.example.com', body=original_string, encoding='cp1251')\n \n         # check body_as_unicode\n+        with catch_warnings():\n+            filterwarnings(\"ignore\", category=ScrapyDeprecationWarning)\n             self.assertTrue(isinstance(r1.body_as_unicode(), str))\n             self.assertEqual(r1.body_as_unicode(), unicode_string)\n \n\n@@ -1,6 +1,6 @@\n import unittest\n from unittest import mock\n-from warnings import catch_warnings\n+from warnings import catch_warnings, filterwarnings\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.item import ABCMeta, _BaseItem, BaseItem, DictItem, Field, Item, ItemMeta\n@@ -328,6 +328,8 @@ class BaseItemTest(unittest.TestCase):\n         class SubclassedItem(Item):\n             pass\n \n+        with catch_warnings():\n+            filterwarnings(\"ignore\", category=ScrapyDeprecationWarning)\n             self.assertTrue(isinstance(BaseItem(), BaseItem))\n             self.assertTrue(isinstance(SubclassedBaseItem(), BaseItem))\n             self.assertTrue(isinstance(Item(), BaseItem))\n\n@@ -108,7 +108,7 @@ class WarnWhenSubclassedTest(unittest.TestCase):\n \n         # ignore subclassing warnings\n         with warnings.catch_warnings():\n-            warnings.simplefilter('ignore', ScrapyDeprecationWarning)\n+            warnings.simplefilter('ignore', MyWarning)\n \n             class UserClass(Deprecated):\n                 pass\n\n@@ -5,8 +5,9 @@ import platform\n import unittest\n from datetime import datetime\n from itertools import count\n-from warnings import catch_warnings\n+from warnings import catch_warnings, filterwarnings\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.python import (\n     memoizemethod_noargs, binary_is_text, equal_attributes,\n     WeakKeyCache, get_func_args, to_bytes, to_unicode,\n@@ -160,7 +161,11 @@ class UtilsPythonTestCase(unittest.TestCase):\n             pass\n \n         _values = count()\n+\n+        with catch_warnings():\n+            filterwarnings(\"ignore\", category=ScrapyDeprecationWarning)\n             wk = WeakKeyCache(lambda k: next(_values))\n+\n         k = _Weakme()\n         v = wk[k]\n         self.assertEqual(v, wk[k])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#216dd37953112e360348e19eaf8cae45a52fe87a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 1064 | Contributors (this commit): 33 | Commits (past 90d): 2 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,6 +5,7 @@ This module implements the FormRequest class which is a more convenient class\n See documentation in docs/topics/request-response.rst\n \"\"\"\n \n+from typing import Optional, Type, TypeVar\n from urllib.parse import urljoin, urlencode\n \n import lxml.html\n@@ -12,15 +13,18 @@ from parsel.selector import create_root_node\n from w3lib.html import strip_html5_whitespace\n \n from scrapy.http.request import Request\n+from scrapy.http.response.text import TextResponse\n from scrapy.utils.python import to_bytes, is_listlike\n from scrapy.utils.response import get_base_url\n \n \n+FormRequestTypeVar = TypeVar(\"FormRequestTypeVar\", bound=\"FormRequest\")\n+\n+\n class FormRequest(Request):\n     valid_form_methods = ['GET', 'POST']\n \n-    def __init__(self, *args, **kwargs):\n-        formdata = kwargs.pop('formdata', None)\n+    def __init__(self, *args, formdata: Optional[dict] = None, **kwargs) -> None:\n         if formdata and kwargs.get('method') is None:\n             kwargs['method'] = 'POST'\n \n@@ -36,9 +40,19 @@ class FormRequest(Request):\n                 self._set_url(self.url + ('&' if '?' in self.url else '?') + querystr)\n \n     @classmethod\n-    def from_response(cls, response, formname=None, formid=None, formnumber=0, formdata=None,\n-                      clickdata=None, dont_click=False, formxpath=None, formcss=None, **kwargs):\n-\n+    def from_response(\n+        cls: Type[FormRequestTypeVar],\n+        response: TextResponse,\n+        formname: Optional[str] = None,\n+        formid: Optional[str] = None,\n+        formnumber: Optional[int] = 0,\n+        formdata: Optional[dict] = None,\n+        clickdata: Optional[dict] = None,\n+        dont_click: bool = False,\n+        formxpath: Optional[str] = None,\n+        formcss: Optional[str] = None,\n+        **kwargs,\n+    ) -> FormRequestTypeVar:\n         kwargs.setdefault('encoding', response.encoding)\n \n         if formcss is not None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c594017e518af31edc24619bfc590907ba6280c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 1084 | Contributors (this commit): 33 | Commits (past 90d): 3 | Contributors (cumulative): 33 | DMM Complexity: 0.0\n\nDIFF:\n@@ -8,7 +8,7 @@ See documentation in docs/topics/request-response.rst\n from typing import Optional, Type, TypeVar\n from urllib.parse import urljoin, urlencode\n \n-import lxml.html\n+from lxml.html import HTMLParser, FormElement\n from parsel.selector import create_root_node\n from w3lib.html import strip_html5_whitespace\n \n@@ -72,7 +72,7 @@ class FormRequest(Request):\n         return cls(url=url, method=method, formdata=formdata, **kwargs)\n \n \n-def _get_form_url(form, url):\n+def _get_form_url(form: FormElement, url: Optional[str]) -> str:\n     if url is None:\n         action = form.get('action')\n         if action is None:\n@@ -88,10 +88,15 @@ def _urlencode(seq, enc):\n     return urlencode(values, doseq=True)\n \n \n-def _get_form(response, formname, formid, formnumber, formxpath):\n-    \"\"\"Find the form element \"\"\"\n-    root = create_root_node(response.text, lxml.html.HTMLParser,\n-                            base_url=get_base_url(response))\n+def _get_form(\n+    response: TextResponse,\n+    formname: Optional[str],\n+    formid: Optional[str],\n+    formnumber: Optional[int],\n+    formxpath: Optional[str],\n+) -> FormElement:\n+    \"\"\"Find the wanted form element within the given response.\"\"\"\n+    root = create_root_node(response.text, HTMLParser, base_url=get_base_url(response))\n     forms = root.xpath('//form')\n     if not forms:\n         raise ValueError(f\"No <form> element found in {response}\")\n@@ -119,8 +124,7 @@ def _get_form(response, formname, formid, formnumber, formxpath):\n                     break\n         raise ValueError(f'No <form> element found with {formxpath}')\n \n-    # If we get here, it means that either formname was None\n-    # or invalid\n+    # If we get here, it means that either formname was None or invalid\n     if formnumber is not None:\n         try:\n             form = forms[formnumber]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#85f88a5710e51a3137ded72f5fdb1c3465480355", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 29 | Churn Cumulative: 1113 | Contributors (this commit): 33 | Commits (past 90d): 4 | Contributors (cumulative): 33 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,7 +5,7 @@ This module implements the FormRequest class which is a more convenient class\n See documentation in docs/topics/request-response.rst\n \"\"\"\n \n-from typing import Optional, Type, TypeVar\n+from typing import List, Optional, Tuple, Type, TypeVar, Union\n from urllib.parse import urljoin, urlencode\n \n from lxml.html import HTMLParser, FormElement\n@@ -20,11 +20,13 @@ from scrapy.utils.response import get_base_url\n \n FormRequestTypeVar = TypeVar(\"FormRequestTypeVar\", bound=\"FormRequest\")\n \n+FormdataType = Optional[Union[dict, List[Tuple[str, str]]]]\n+\n \n class FormRequest(Request):\n     valid_form_methods = ['GET', 'POST']\n \n-    def __init__(self, *args, formdata: Optional[dict] = None, **kwargs) -> None:\n+    def __init__(self, *args, formdata: FormdataType = None, **kwargs) -> None:\n         if formdata and kwargs.get('method') is None:\n             kwargs['method'] = 'POST'\n \n@@ -46,7 +48,7 @@ class FormRequest(Request):\n         formname: Optional[str] = None,\n         formid: Optional[str] = None,\n         formnumber: Optional[int] = 0,\n-        formdata: Optional[dict] = None,\n+        formdata: FormdataType = None,\n         clickdata: Optional[dict] = None,\n         dont_click: bool = False,\n         formxpath: Optional[str] = None,\n@@ -60,7 +62,7 @@ class FormRequest(Request):\n             formxpath = HTMLTranslator().css_to_xpath(formcss)\n \n         form = _get_form(response, formname, formid, formnumber, formxpath)\n-        formdata = _get_inputs(form, formdata, dont_click, clickdata, response)\n+        formdata = _get_inputs(form, formdata, dont_click, clickdata)\n         url = _get_form_url(form, kwargs.pop('url', None))\n \n         method = kwargs.pop('method', form.method)\n@@ -134,22 +136,27 @@ def _get_form(\n             return form\n \n \n-def _get_inputs(form, formdata, dont_click, clickdata, response):\n+def _get_inputs(\n+    form: FormElement,\n+    formdata: FormdataType,\n+    dont_click: bool,\n+    clickdata: Optional[dict],\n+) -> List[Tuple[str, str]]:\n+    \"\"\"Return a list of key-value pairs for the inputs found in the given form.\"\"\"\n     try:\n         formdata_keys = dict(formdata or ()).keys()\n     except (ValueError, TypeError):\n         raise ValueError('formdata should be a dict or iterable of tuples')\n \n     if not formdata:\n-        formdata = ()\n+        formdata = []\n     inputs = form.xpath('descendant::textarea'\n                         '|descendant::select'\n                         '|descendant::input[not(@type) or @type['\n                         ' not(re:test(., \"^(?:submit|image|reset)$\", \"i\"))'\n                         ' and (../@checked or'\n                         '  not(re:test(., \"^(?:checkbox|radio)$\", \"i\")))]]',\n-                        namespaces={\n-                            \"re\": \"http://exslt.org/regular-expressions\"})\n+                        namespaces={\"re\": \"http://exslt.org/regular-expressions\"})\n     values = [(k, '' if v is None else v)\n               for k, v in (_value(e) for e in inputs)\n               if k and k not in formdata_keys]\n@@ -160,7 +167,7 @@ def _get_inputs(form, formdata, dont_click, clickdata, response):\n             values.append(clickable)\n \n     if isinstance(formdata, dict):\n-        formdata = formdata.items()\n+        formdata = formdata.items()  # type: ignore[assignment]\n \n     values.extend((k, v) for k, v in formdata if v is not None)\n     return values\n@@ -189,7 +196,7 @@ def _select_value(ele, n, v):\n     return n, v\n \n \n-def _get_clickable(clickdata, form):\n+def _get_clickable(clickdata: Optional[dict], form: FormElement) -> Optional[Tuple[str, str]]:\n     \"\"\"\n     Returns the clickable element specified in clickdata,\n     if the latter is given. If not, it returns the first\n@@ -201,7 +208,7 @@ def _get_clickable(clickdata, form):\n         namespaces={\"re\": \"http://exslt.org/regular-expressions\"}\n     ))\n     if not clickables:\n-        return\n+        return None\n \n     # If we don't have clickdata, we just use the first clickable element\n     if clickdata is None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c9fecca010a3ddddd1145f65a78dde30b5c71a72", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 8 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 1132 | Contributors (this commit): 33 | Commits (past 90d): 5 | Contributors (cumulative): 33 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,10 +5,10 @@ This module implements the FormRequest class which is a more convenient class\n See documentation in docs/topics/request-response.rst\n \"\"\"\n \n-from typing import List, Optional, Tuple, Type, TypeVar, Union\n+from typing import Iterable, List, Optional, Tuple, Type, TypeVar, Union\n from urllib.parse import urljoin, urlencode\n \n-from lxml.html import HTMLParser, FormElement\n+from lxml.html import FormElement, HtmlElement, HTMLParser, SelectElement\n from parsel.selector import create_root_node\n from w3lib.html import strip_html5_whitespace\n \n@@ -83,7 +83,7 @@ def _get_form_url(form: FormElement, url: Optional[str]) -> str:\n     return urljoin(form.base_url, url)\n \n \n-def _urlencode(seq, enc):\n+def _urlencode(seq: Iterable, enc: str) -> str:\n     values = [(to_bytes(k, enc), to_bytes(v, enc))\n               for k, vs in seq\n               for v in (vs if is_listlike(vs) else [vs])]\n@@ -157,9 +157,11 @@ def _get_inputs(\n                         ' and (../@checked or'\n                         '  not(re:test(., \"^(?:checkbox|radio)$\", \"i\")))]]',\n                         namespaces={\"re\": \"http://exslt.org/regular-expressions\"})\n-    values = [(k, '' if v is None else v)\n+    values = [\n+        (k, '' if v is None else v)\n         for k, v in (_value(e) for e in inputs)\n-              if k and k not in formdata_keys]\n+        if k and k not in formdata_keys\n+    ]\n \n     if not dont_click:\n         clickable = _get_clickable(clickdata, form)\n@@ -173,7 +175,7 @@ def _get_inputs(\n     return values\n \n \n-def _value(ele):\n+def _value(ele: HtmlElement):\n     n = ele.name\n     v = ele.value\n     if ele.tag == 'select':\n@@ -181,7 +183,7 @@ def _value(ele):\n     return n, v\n \n \n-def _select_value(ele, n, v):\n+def _select_value(ele: SelectElement, n: str, v: str):\n     multiple = ele.multiple\n     if v is None and not multiple:\n         # Match browser behaviour on simple select tag without options selected\n@@ -192,7 +194,8 @@ def _select_value(ele, n, v):\n         # This is a workround to bug in lxml fixed 2.3.1\n         # fix https://github.com/lxml/lxml/commit/57f49eed82068a20da3db8f1b18ae00c1bab8b12#L1L1139\n         selected_options = ele.xpath('.//option[@selected]')\n-        v = [(o.get('value') or o.text or '').strip() for o in selected_options]\n+        values = [(o.get('value') or o.text or '').strip() for o in selected_options]\n+        return n, values\n     return n, v\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#479260dca012b3d03221f2e6322008448d0fb8b5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 21 | Churn Cumulative: 177 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,9 +8,9 @@ See documentation in docs/topics/request-response.rst\n import copy\n import json\n import warnings\n-from typing import Tuple\n+from typing import Optional, Tuple\n \n-from scrapy.http.request import Request\n+from scrapy.http.request import Request, RequestTypeVar\n from scrapy.utils.deprecate import create_deprecated_class\n \n \n@@ -18,8 +18,8 @@ class JsonRequest(Request):\n \n     attributes: Tuple[str, ...] = Request.attributes + (\"dumps_kwargs\",)\n \n-    def __init__(self, *args, **kwargs):\n-        dumps_kwargs = copy.deepcopy(kwargs.pop('dumps_kwargs', {}))\n+    def __init__(self, *args, dumps_kwargs: Optional[dict] = None, **kwargs) -> None:\n+        dumps_kwargs = copy.deepcopy(dumps_kwargs) if dumps_kwargs is not None else {}\n         dumps_kwargs.setdefault('sort_keys', True)\n         self._dumps_kwargs = dumps_kwargs\n \n@@ -29,10 +29,8 @@ class JsonRequest(Request):\n \n         if body_passed and data_passed:\n             warnings.warn('Both body and data passed. data will be ignored')\n-\n         elif not body_passed and data_passed:\n             kwargs['body'] = self._dumps(data)\n-\n             if 'method' not in kwargs:\n                 kwargs['method'] = 'POST'\n \n@@ -41,23 +39,22 @@ class JsonRequest(Request):\n         self.headers.setdefault('Accept', 'application/json, text/javascript, */*; q=0.01')\n \n     @property\n-    def dumps_kwargs(self):\n+    def dumps_kwargs(self) -> dict:\n         return self._dumps_kwargs\n \n-    def replace(self, *args, **kwargs):\n+    def replace(self, *args, **kwargs) -> RequestTypeVar:\n         body_passed = kwargs.get('body', None) is not None\n         data = kwargs.pop('data', None)\n         data_passed = data is not None\n \n         if body_passed and data_passed:\n             warnings.warn('Both body and data passed. data will be ignored')\n-\n         elif not body_passed and data_passed:\n             kwargs['body'] = self._dumps(data)\n \n         return super().replace(*args, **kwargs)\n \n-    def _dumps(self, data):\n+    def _dumps(self, data: dict) -> str:\n         \"\"\"Convert to JSON \"\"\"\n         return json.dumps(data, **self._dumps_kwargs)\n \n\n@@ -5,6 +5,7 @@ This module implements the XmlRpcRequest class which is a more convenient class\n See documentation in docs/topics/request-response.rst\n \"\"\"\n import xmlrpc.client as xmlrpclib\n+from typing import Optional\n \n from scrapy.http.request import Request\n from scrapy.utils.python import get_func_args\n@@ -15,8 +16,7 @@ DUMPS_ARGS = get_func_args(xmlrpclib.dumps)\n \n class XmlRpcRequest(Request):\n \n-    def __init__(self, *args, **kwargs):\n-        encoding = kwargs.get('encoding', None)\n+    def __init__(self, *args, encoding: Optional[str] = None, **kwargs):\n         if 'body' not in kwargs and 'params' in kwargs:\n             kw = dict((k, kwargs.pop(k)) for k in DUMPS_ARGS if k in kwargs)\n             kwargs['body'] = xmlrpclib.dumps(**kw)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ce6447731a91e32faf17564d21ddc3b88e582f50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 624 | Contributors (this commit): 15 | Commits (past 90d): 7 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -126,10 +126,10 @@ class Request(object_ref):\n \n     __repr__ = __str__\n \n-    def copy(self) -> RequestTypeVar:\n+    def copy(self) -> \"Request\":\n         return self.replace()\n \n-    def replace(self, *args, **kwargs) -> RequestTypeVar:\n+    def replace(self, *args, **kwargs) -> \"Request\":\n         \"\"\"Create a new Request with the same attributes except for those given new values\"\"\"\n         for x in self.attributes:\n             kwargs.setdefault(x, getattr(self, x))\n\n@@ -10,7 +10,7 @@ import json\n import warnings\n from typing import Optional, Tuple\n \n-from scrapy.http.request import Request, RequestTypeVar\n+from scrapy.http.request import Request\n from scrapy.utils.deprecate import create_deprecated_class\n \n \n@@ -42,7 +42,7 @@ class JsonRequest(Request):\n     def dumps_kwargs(self) -> dict:\n         return self._dumps_kwargs\n \n-    def replace(self, *args, **kwargs) -> RequestTypeVar:\n+    def replace(self, *args, **kwargs) -> Request:\n         body_passed = kwargs.get('body', None) is not None\n         data = kwargs.pop('data', None)\n         data_passed = data is not None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e876d8e38780a88e4c1fc060e3ead779222be83a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 2644 | Contributors (this commit): 32 | Commits (past 90d): 5 | Contributors (cumulative): 35 | DMM Complexity: 0.0\n\nDIFF:\n@@ -283,9 +283,9 @@ class ScrapyAgent:\n             if omitConnectTunnel:\n                 warnings.warn(\n                     \"Using HTTPS proxies in the noconnect mode is deprecated. \"\n-                    \"If you use Zyte Smart Proxy Manager (formerly Crawlera), \"\n-                    \"it doesn't require this mode anymore, so you should \"\n-                    \"update scrapy-crawlera to 1.3.0+ and remove '?noconnect' \"\n+                    \"If you use Zyte Smart Proxy Manager, it doesn't require \"\n+                    \"this mode anymore, so you should update scrapy-crawlera \"\n+                    \"to scrapy-zyte-smartproxy and remove '?noconnect' \"\n                     \"from the Zyte Smart Proxy Manager URL.\",\n                     ScrapyDeprecationWarning,\n                 )\n\n@@ -72,10 +72,14 @@ class ScrapyH2Agent:\n             proxy_host = proxy_host.decode()\n             omit_connect_tunnel = b'noconnect' in proxy_params\n             if omit_connect_tunnel:\n-                warnings.warn(\"Using HTTPS proxies in the noconnect mode is not supported by the \"\n-                              \"downloader handler. If you use Crawlera, it doesn't require this \"\n-                              \"mode anymore, so you should update scrapy-crawlera to 1.3.0+ \"\n-                              \"and remove '?noconnect' from the Crawlera URL.\")\n+                warnings.warn(\n+                    \"Using HTTPS proxies in the noconnect mode is not \"\n+                    \"supported by the downloader handler. If you use Zyte \"\n+                    \"Smart Proxy Manager, it doesn't require this mode \"\n+                    \"anymore, so you should update scrapy-crawlera to \"\n+                    \"scrapy-zyte-smartproxy and remove '?noconnect' from the \"\n+                    \"Zyte Smart Proxy Manager URL.\"\n+                )\n \n             if scheme == b'https' and not omit_connect_tunnel:\n                 # ToDo\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#66e200423943a921326334a37777720c706ee2b5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 1514 | Contributors (this commit): 24 | Commits (past 90d): 1 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -381,7 +381,8 @@ class FormRequestTest(RequestTest):\n \n     def test_formdata_overrides_querystring(self):\n         data = (('a', 'one'), ('a', 'two'), ('b', '2'))\n-        url = self.request_class('http://www.example.com/?a=0&b=1&c=3#fragment', method='GET', formdata=data).url.split('#')[0]\n+        url = self.request_class('http://www.example.com/?a=0&b=1&c=3#fragment',\n+                                 method='GET', formdata=data).url.split('#')[0]\n         fs = _qs(self.request_class(url, method='GET', formdata=data))\n         self.assertEqual(set(fs[b'a']), {b'one', b'two'})\n         self.assertEqual(fs[b'b'], [b'2'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5044549c550876cc31fb2e15aaa5013e8fc333c3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 2 | Churn Cumulative: 2295 | Contributors (this commit): 32 | Commits (past 90d): 5 | Contributors (cumulative): 32 | DMM Complexity: None\n\nDIFF:\n@@ -319,7 +319,7 @@ class ScrapyAgent:\n                     pool=self._pool,\n                 )\n             else:\n-                proxyScheme = b'http' if not proxyScheme else proxyScheme\n+                proxyScheme = proxyScheme or b'http'\n                 proxyHost = to_bytes(proxyHost, encoding='ascii')\n                 proxyPort = to_bytes(str(proxyPort), encoding='ascii')\n                 proxyURI = urlunparse((proxyScheme, proxyNetloc, proxyParams, '', '', ''))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7d653288e3d9369e4c007d223327f919b99b7737", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 2 | Churn Cumulative: 2046 | Contributors (this commit): 26 | Commits (past 90d): 2 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -768,7 +768,7 @@ class Http11ProxyTestCase(HttpProxyTestCase):\n         def _test(response):\n             self.assertEqual(response.status, 200)\n             self.assertEqual(response.url, request.url)\n-            self.assertEqual(response.body, b'http://example.com')\n+            self.assertEqual(response.body, self.expected_http_proxy_request_body)\n \n         http_proxy = self.getURL('').replace('http://', '')\n         request = Request('http://example.com', meta={'proxy': http_proxy})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#812b4bb51855605a36bcfb166f66a121cc67f97c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 26 | Churn Cumulative: 2945 | Contributors (this commit): 26 | Commits (past 90d): 5 | Contributors (cumulative): 31 | DMM Complexity: 0.0\n\nDIFF:\n@@ -15,7 +15,8 @@ from twisted.python.failure import Failure\n \n from scrapy import signals\n from scrapy.core.scraper import Scraper\n-from scrapy.exceptions import DontCloseSpider, ScrapyDeprecationWarning\n+from scrapy.exceptions import DontCloseSpider, ScrapyDeprecationWarning, \\\n+    CloseSpider\n from scrapy.http import Response, Request\n from scrapy.settings import BaseSettings\n from scrapy.spiders import Spider\n@@ -325,14 +326,19 @@ class ExecutionEngine:\n         Called when a spider gets idle, i.e. when there are no remaining requests to download or schedule.\n         It can be called multiple times. If a handler for the spider_idle signal raises a DontCloseSpider\n         exception, the spider is not closed until the next loop and this function is guaranteed to be called\n-        (at least) once again.\n+        (at least) once again. A handler can raise CloseSpider to provide a custom closing reason\n         \"\"\"\n         assert self.spider is not None  # typing\n-        res = self.signals.send_catch_log(signals.spider_idle, spider=self.spider, dont_log=DontCloseSpider)\n-        if any(isinstance(x, Failure) and isinstance(x.value, DontCloseSpider) for _, x in res):\n+        expected_ex = (DontCloseSpider, CloseSpider)\n+        res = self.signals.send_catch_log(signals.spider_idle, spider=self.spider, dont_log=expected_ex)\n+        detected_ex = {ex: x.value\n+                       for _, x in res for ex in expected_ex\n+                       if isinstance(x, Failure) and isinstance(x.value, ex)}\n+        if DontCloseSpider in detected_ex:\n             return None\n         if self.spider_is_idle():\n-            self.close_spider(self.spider, reason='finished')\n+            reason = detected_ex[CloseSpider].reason if CloseSpider in detected_ex else 'finished'\n+            self.close_spider(self.spider, reason=reason)\n \n     def close_spider(self, spider: Spider, reason: str = \"cancelled\") -> Deferred:\n         \"\"\"Close (cancel) spider and clear all its outstanding requests\"\"\"\n\n@@ -1,5 +1,5 @@\n \"\"\"Helper functions for working with signals\"\"\"\n-\n+import collections\n import logging\n \n from twisted.internet.defer import DeferredList, Deferred\n@@ -16,15 +16,13 @@ from scrapy.utils.log import failure_to_exc_info\n logger = logging.getLogger(__name__)\n \n \n-class _IgnoredException(Exception):\n-    pass\n-\n-\n def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):\n     \"\"\"Like pydispatcher.robust.sendRobust but it also logs errors and returns\n     Failures instead of exceptions.\n     \"\"\"\n-    dont_log = (named.pop('dont_log', _IgnoredException), StopDownload)\n+    dont_log = named.pop('dont_log', ())\n+    dont_log = tuple(dont_log) if isinstance(dont_log, collections.Sequence) else (dont_log,)\n+    dont_log += (StopDownload, )\n     spider = named.get('spider', None)\n     responses = []\n     for receiver in liveReceivers(getAllReceivers(sender, signal)):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ce445f20462463e49bd9dfd417aa7397d4c17f1a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 5 | Churn Cumulative: 2764 | Contributors (this commit): 25 | Commits (past 90d): 5 | Contributors (cumulative): 25 | DMM Complexity: 0.0\n\nDIFF:\n@@ -337,8 +337,9 @@ class ExecutionEngine:\n         if DontCloseSpider in detected_ex:\n             return None\n         if self.spider_is_idle():\n-            reason = detected_ex[CloseSpider].reason if CloseSpider in detected_ex else 'finished'\n-            self.close_spider(self.spider, reason=reason)\n+            ex = detected_ex.get(CloseSpider, CloseSpider(reason='finished'))\n+            assert isinstance(ex, CloseSpider)  # typing\n+            self.close_spider(self.spider, reason=ex.reason)\n \n     def close_spider(self, spider: Spider, reason: str = \"cancelled\") -> Deferred:\n         \"\"\"Close (cancel) spider and clear all its outstanding requests\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f35970778b93033ee36379e0b8b55035aacaf918", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 21 | Churn Cumulative: 684 | Contributors (this commit): 16 | Commits (past 90d): 4 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,7 +26,7 @@ from twisted.web import server, static, util\n \n from scrapy import signals\n from scrapy.core.engine import ExecutionEngine\n-from scrapy.exceptions import ScrapyDeprecationWarning\n+from scrapy.exceptions import ScrapyDeprecationWarning, CloseSpider\n from scrapy.http import Request\n from scrapy.item import Item, Field\n from scrapy.linkextractors import LinkExtractor\n@@ -113,6 +113,18 @@ class ItemZeroDivisionErrorSpider(TestSpider):\n     }\n \n \n+class ChangeCloseReasonSpider(TestSpider):\n+    @classmethod\n+    def from_crawler(cls, crawler, *args, **kwargs):\n+        spider = cls(*args, **kwargs)\n+        spider._set_crawler(crawler)\n+        crawler.signals.connect(spider.spider_idle, signals.spider_idle)\n+        return spider\n+\n+    def spider_idle(self):\n+        raise CloseSpider(reason=\"custom_reason\")\n+\n+\n def start_test_site(debug=False):\n     root_dir = os.path.join(tests_datadir, \"test_site\")\n     r = static.File(root_dir)\n@@ -251,6 +263,13 @@ class EngineTest(unittest.TestCase):\n         yield self.run.run()\n         self._assert_items_error()\n \n+    @defer.inlineCallbacks\n+    def test_crawler_change_close_reason_on_idle(self):\n+        self.run = CrawlerRun(ChangeCloseReasonSpider)\n+        yield self.run.run()\n+        self.assertEqual({'spider': self.run.spider, 'reason': 'custom_reason'},\n+                         self.run.signals_caught[signals.spider_closed])\n+\n     def _assert_visited_urls(self):\n         must_be_visited = [\"/\", \"/redirect\", \"/redirected\",\n                            \"/item1.html\", \"/item2.html\", \"/item999.html\"]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e94d3ac173d2fcd269bdd8f06b6b1633953a225e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2766 | Contributors (this commit): 25 | Commits (past 90d): 6 | Contributors (cumulative): 25 | DMM Complexity: None\n\nDIFF:\n@@ -326,7 +326,7 @@ class ExecutionEngine:\n         Called when a spider gets idle, i.e. when there are no remaining requests to download or schedule.\n         It can be called multiple times. If a handler for the spider_idle signal raises a DontCloseSpider\n         exception, the spider is not closed until the next loop and this function is guaranteed to be called\n-        (at least) once again. A handler can raise CloseSpider to provide a custom closing reason\n+        (at least) once again. A handler can raise CloseSpider to provide a custom closing reason.\n         \"\"\"\n         assert self.spider is not None  # typing\n         expected_ex = (DontCloseSpider, CloseSpider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7597d860c86e0ddfaf8e3ce57492106c907245f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 2773 | Contributors (this commit): 26 | Commits (past 90d): 7 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -15,8 +15,11 @@ from twisted.python.failure import Failure\n \n from scrapy import signals\n from scrapy.core.scraper import Scraper\n-from scrapy.exceptions import DontCloseSpider, ScrapyDeprecationWarning, \\\n-    CloseSpider\n+from scrapy.exceptions import (\n+    CloseSpider,\n+    DontCloseSpider, \n+    ScrapyDeprecationWarning,\n+)   \n from scrapy.http import Response, Request\n from scrapy.settings import BaseSettings\n from scrapy.spiders import Spider\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6b8f694653b0eec95055d50fafde0ef2e0ec8bc9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 2782 | Contributors (this commit): 26 | Commits (past 90d): 8 | Contributors (cumulative): 26 | DMM Complexity: 0.0\n\nDIFF:\n@@ -334,9 +334,12 @@ class ExecutionEngine:\n         assert self.spider is not None  # typing\n         expected_ex = (DontCloseSpider, CloseSpider)\n         res = self.signals.send_catch_log(signals.spider_idle, spider=self.spider, dont_log=expected_ex)\n-        detected_ex = {ex: x.value\n-                       for _, x in res for ex in expected_ex\n-                       if isinstance(x, Failure) and isinstance(x.value, ex)}\n+        detected_ex = {\n+            ex: x.value\n+            for _, x in res\n+            for ex in expected_ex\n+            if isinstance(x, Failure) and isinstance(x.value, ex)\n+        }\n         if DontCloseSpider in detected_ex:\n             return None\n         if self.spider_is_idle():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eca641aa3da335781c373e3d50deb78837044c16", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 686 | Contributors (this commit): 17 | Commits (past 90d): 5 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -26,7 +26,7 @@ from twisted.web import server, static, util\n \n from scrapy import signals\n from scrapy.core.engine import ExecutionEngine\n-from scrapy.exceptions import ScrapyDeprecationWarning, CloseSpider\n+from scrapy.exceptions import CloseSpider, ScrapyDeprecationWarning\n from scrapy.http import Request\n from scrapy.item import Item, Field\n from scrapy.linkextractors import LinkExtractor\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4ddc9d6b55fa708becfd70812ea44eb0c6837638", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 160 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 13 | Methods Changed: 13 | Complexity Δ (Sum/Max): 18/11 | Churn Δ: 166 | Churn Cumulative: 4649 | Contributors (this commit): 33 | Commits (past 90d): 3 | Contributors (cumulative): 55 | DMM Complexity: 0.9739130434782609\n\nDIFF:\n@@ -46,6 +46,38 @@ def build_storage(builder, uri, *args, feed_options=None, preargs=(), **kwargs):\n     return builder(*preargs, uri, *args, **kwargs)\n \n \n+class ItemFilter:\n+    \"\"\"\n+    This will be used by FeedExporter to decide if an item should be allowed\n+    to be exported to a particular feed.\n+\n+    :param feed_options: feed specific options passed from FeedExporter\n+    :type feed_options: dict\n+    \"\"\"\n+\n+    def __init__(self, feed_options):\n+        self.feed_options = feed_options\n+        self.item_classes = set()\n+\n+        if 'item_classes' in self.feed_options:\n+            for item_class in self.feed_options['item_classes']:\n+                self.item_classes.add(load_object(item_class))\n+\n+    def accepts(self, item):\n+        \"\"\"\n+        Return ``True`` if `item` should be exported or ``False`` otherwise.\n+\n+        :param item: scraped item which user wants to check if is acceptable\n+        :type item: :ref:`Scrapy items <topics-items>`\n+        :return: `True` if accepted, `False` otherwise\n+        :rtype: bool\n+        \"\"\"\n+        if self.item_classes:\n+            return isinstance(item, tuple(self.item_classes))\n+\n+        return True    # accept all items if none declared in item_classes\n+\n+\n class IFeedStorage(Interface):\n     \"\"\"Interface that all Feed Storages must implement\"\"\"\n \n@@ -215,7 +247,7 @@ class FTPFeedStorage(BlockingFeedStorage):\n \n \n class _FeedSlot:\n-    def __init__(self, file, exporter, storage, uri, format, store_empty, batch_id, uri_template):\n+    def __init__(self, file, exporter, storage, uri, format, store_empty, batch_id, uri_template, filter):\n         self.file = file\n         self.exporter = exporter\n         self.storage = storage\n@@ -225,6 +257,7 @@ class _FeedSlot:\n         self.store_empty = store_empty\n         self.uri_template = uri_template\n         self.uri = uri\n+        self.filter = filter\n         # flags\n         self.itemcount = 0\n         self._exporting = False\n@@ -255,6 +288,7 @@ class FeedExporter:\n         self.settings = crawler.settings\n         self.feeds = {}\n         self.slots = []\n+        self.filters = {}\n \n         if not self.settings['FEEDS'] and not self.settings['FEED_URI']:\n             raise NotConfigured\n@@ -269,12 +303,14 @@ class FeedExporter:\n             uri = str(self.settings['FEED_URI'])  # handle pathlib.Path objects\n             feed_options = {'format': self.settings.get('FEED_FORMAT', 'jsonlines')}\n             self.feeds[uri] = feed_complete_default_values_from_settings(feed_options, self.settings)\n+            self.filters[uri] = self._load_filter(feed_options)\n         # End: Backward compatibility for FEED_URI and FEED_FORMAT settings\n \n         # 'FEEDS' setting takes precedence over 'FEED_URI'\n         for uri, feed_options in self.settings.getdict('FEEDS').items():\n             uri = str(uri)  # handle pathlib.Path objects\n             self.feeds[uri] = feed_complete_default_values_from_settings(feed_options, self.settings)\n+            self.filters[uri] = self._load_filter(feed_options)\n \n         self.storages = self._load_components('FEED_STORAGES')\n         self.exporters = self._load_components('FEED_EXPORTERS')\n@@ -368,6 +404,7 @@ class FeedExporter:\n             store_empty=feed_options['store_empty'],\n             batch_id=batch_id,\n             uri_template=uri_template,\n+            filter=self.filters[uri_template]\n         )\n         if slot.store_empty:\n             slot.start_exporting()\n@@ -376,6 +413,10 @@ class FeedExporter:\n     def item_scraped(self, item, spider):\n         slots = []\n         for slot in self.slots:\n+            if not slot.filter.accepts(item):\n+                slots.append(slot)    # if slot doesn't accept item, continue with next slot\n+                continue\n+\n             slot.start_exporting()\n             slot.exporter.export_item(item)\n             slot.itemcount += 1\n@@ -486,3 +527,8 @@ class FeedExporter:\n         uripar_function = load_object(uri_params) if uri_params else lambda x, y: None\n         uripar_function(params, spider)\n         return params\n+\n+    def _load_filter(self, feed_options):\n+        # load the item filter if declared else load the default filter class\n+        item_filter_class = load_object(feed_options.get(\"item_filter\", ItemFilter))\n+        return item_filter_class(feed_options)\n\n@@ -561,6 +561,10 @@ class FeedExportTestBase(ABC, unittest.TestCase):\n         egg = scrapy.Field()\n         baz = scrapy.Field()\n \n+    class MyItem2(scrapy.Item):\n+        foo = scrapy.Field()\n+        hello = scrapy.Field()\n+\n     def _random_temp_filename(self, inter_dir=''):\n         chars = [random.choice(ascii_letters + digits) for _ in range(15)]\n         filename = ''.join(chars)\n@@ -888,13 +892,9 @@ class FeedExportTest(FeedExportTestBase):\n     @defer.inlineCallbacks\n     def test_export_multiple_item_classes(self):\n \n-        class MyItem2(scrapy.Item):\n-            foo = scrapy.Field()\n-            hello = scrapy.Field()\n-\n         items = [\n             self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n-            MyItem2({'hello': 'world2', 'foo': 'bar2'}),\n+            self.MyItem2({'hello': 'world2', 'foo': 'bar2'}),\n             self.MyItem({'foo': 'bar3', 'egg': 'spam3', 'baz': 'quux3'}),\n             {'hello': 'world4', 'egg': 'spam4'},\n         ]\n@@ -929,6 +929,114 @@ class FeedExportTest(FeedExportTestBase):\n         yield self.assertExported(items, header, rows,\n                                   settings=settings, ordered=True)\n \n+    @defer.inlineCallbacks\n+    def test_export_based_on_item_classes(self):\n+        items = [\n+            self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n+            self.MyItem2({'hello': 'world2', 'foo': 'bar2'}),\n+            {'hello': 'world3', 'egg': 'spam3'},\n+        ]\n+\n+        formats = {\n+            'csv': b'baz,egg,foo\\r\\n,spam1,bar1\\r\\n',\n+            'json': b'[\\n{\"hello\": \"world2\", \"foo\": \"bar2\"}\\n]',\n+            'jsonlines': (\n+                b'{\"foo\": \"bar1\", \"egg\": \"spam1\"}\\n'\n+                b'{\"hello\": \"world2\", \"foo\": \"bar2\"}\\n'\n+            ),\n+            'xml': (\n+                b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items>\\n<item>'\n+                b'<foo>bar1</foo><egg>spam1</egg></item>\\n<item><hello>'\n+                b'world2</hello><foo>bar2</foo></item>\\n<item><hello>world3'\n+                b'</hello><egg>spam3</egg></item>\\n</items>'\n+            ),\n+        }\n+\n+        settings = {\n+            'FEEDS': {\n+                self._random_temp_filename(): {\n+                    'format': 'csv',\n+                    'item_classes': [self.MyItem],\n+                },\n+                self._random_temp_filename(): {\n+                    'format': 'json',\n+                    'item_classes': [self.MyItem2],\n+                },\n+                self._random_temp_filename(): {\n+                    'format': 'jsonlines',\n+                    'item_classes': [self.MyItem, self.MyItem2],\n+                },\n+                self._random_temp_filename(): {\n+                    'format': 'xml',\n+                },\n+            },\n+        }\n+\n+        data = yield self.exported_data(items, settings)\n+        for fmt, expected in formats.items():\n+            self.assertEqual(expected, data[fmt])\n+\n+    @defer.inlineCallbacks\n+    def test_export_based_on_custom_filters(self):\n+        items = [\n+            self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n+            self.MyItem2({'hello': 'world2', 'foo': 'bar2'}),\n+            {'hello': 'world3', 'egg': 'spam3'},\n+        ]\n+\n+        MyItem = self.MyItem\n+\n+        class CustomFilter1:\n+            def __init__(self, feed_options):\n+                pass\n+\n+            def accepts(self, item):\n+                return isinstance(item, MyItem)\n+\n+        class CustomFilter2(scrapy.extensions.feedexport.ItemFilter):\n+            def accepts(self, item):\n+                if 'foo' not in item.fields:\n+                    return False\n+                return True\n+\n+        class CustomFilter3(scrapy.extensions.feedexport.ItemFilter):\n+            def accepts(self, item):\n+                if isinstance(item, tuple(self.item_classes)) and item['foo'] == \"bar1\":\n+                    return True\n+                return False\n+\n+        formats = {\n+            'json': b'[\\n{\"foo\": \"bar1\", \"egg\": \"spam1\"}\\n]',\n+            'xml': (\n+                b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items>\\n<item>'\n+                b'<foo>bar1</foo><egg>spam1</egg></item>\\n<item><hello>'\n+                b'world2</hello><foo>bar2</foo></item>\\n</items>'\n+            ),\n+            'jsonlines': b'{\"foo\": \"bar1\", \"egg\": \"spam1\"}\\n',\n+        }\n+\n+        settings = {\n+            'FEEDS': {\n+                self._random_temp_filename(): {\n+                    'format': 'json',\n+                    'item_filter': CustomFilter1,\n+                },\n+                self._random_temp_filename(): {\n+                    'format': 'xml',\n+                    'item_filter': CustomFilter2,\n+                },\n+                self._random_temp_filename(): {\n+                    'format': 'jsonlines',\n+                    'item_classes': [self.MyItem, self.MyItem2],\n+                    'item_filter': CustomFilter3,\n+                },\n+            },\n+        }\n+\n+        data = yield self.exported_data(items, settings)\n+        for fmt, expected in formats.items():\n+            self.assertEqual(expected, data[fmt])\n+\n     @defer.inlineCallbacks\n     def test_export_dicts(self):\n         # When dicts are used, only keys from the first row are used as\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
