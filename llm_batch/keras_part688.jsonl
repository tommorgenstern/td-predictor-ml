{"custom_id": "keras#8266d6644d7732572fcd09eb0a9201b20f76c4bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 3059 | Contributors (this commit): 13 | Commits (past 90d): 56 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -99,20 +99,20 @@ class RandomCrop(TFDataLayer):\n                     (),\n                     0,\n                     maxval=float(input_height - self.height + 1),\n-                    dtype=inputs.dtype,\n+                    dtype=\"int\",\n                     seed=seed_generator,\n                 ),\n-                \"int32\",\n+                \"int\",\n             )\n             w_start = self.backend.cast(\n                 ops.random.uniform(\n                     (),\n                     0,\n                     maxval=float(input_width - self.width + 1),\n-                    dtype=inputs.dtype,\n+                    dtype=\"int\",\n                     seed=seed_generator,\n                 ),\n-                \"int32\",\n+                \"int\",\n             )\n             if self.data_format == \"channels_last\":\n                 return inputs[\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b11396fdc040caa68633c7996aa2e0a89b1ade55", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): 252/138 | Churn Δ: 25 | Churn Cumulative: 5886 | Contributors (this commit): 11 | Commits (past 90d): 2 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -906,7 +906,7 @@ class RNN(base_layer.Layer):\n                     )\n                 )\n             flat_states_variables = tf.nest.map_structure(\n-                backend.variable, flat_init_state_values\n+                lambda v: backend.variable(v, v.dtype), flat_init_state_values\n             )\n             self.states = tf.nest.pack_sequence_as(\n                 self.cell.state_size, flat_states_variables\n\n@@ -1478,7 +1478,6 @@ class RNNTest(test_combinations.TestCase):\n         self.assertAllClose(y_np, x_np[:, 1, :])\n \n     def test_zero_output_for_masking(self):\n-\n         for unroll in [True, False]:\n             cell = keras.layers.SimpleRNNCell(5)\n             x = keras.Input((5, 5))\n@@ -1682,7 +1681,6 @@ class RNNTest(test_combinations.TestCase):\n \n     def test_stateful_rnn_with_customized_get_initial_state(self):\n         class TestCell(keras.layers.AbstractRNNCell):\n-\n             state_size = 1\n             output_size = 2\n \n@@ -1702,6 +1700,27 @@ class RNNTest(test_combinations.TestCase):\n         self.assertAllClose(output, np.ones((4, 2)))\n         self.assertAllClose(state, np.ones((4, 1)))\n \n+    def test_stateful_rnn_with_customized_dtype(self):\n+        class TestCell(keras.layers.AbstractRNNCell):\n+            state_size = 1\n+            output_size = 2\n+\n+            def get_initial_state(\n+                self, inputs=None, batch_size=None, dtype=None\n+            ):\n+                return np.ones((batch_size, 1), dtype=np.float16)\n+\n+            def call(self, inputs, states):\n+                return inputs, states\n+\n+        layer = keras.layers.RNN(TestCell(), stateful=True, return_state=True)\n+        inputs = keras.Input(shape=(10, 2), batch_size=4)\n+        model = keras.Model(inputs, layer(inputs))\n+        x = np.ones((4, 10, 2), dtype=np.float16)\n+        output, state = model.predict(x)\n+        self.assertAllClose(output, np.ones((4, 2), dtype=np.float16))\n+        self.assertAllClose(state, np.ones((4, 1), dtype=np.float16))\n+\n     def test_input_dim_length(self):\n         simple_rnn = keras.layers.SimpleRNN(5, input_length=10, input_dim=8)\n         self.assertEqual(simple_rnn._batch_input_shape, (None, 10, 8))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6e4bae119bda9c1f2ff42328888e633bd27c9d92", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 17 | Churn Cumulative: 30756 | Contributors (this commit): 150 | Commits (past 90d): 12 | Contributors (cumulative): 169 | DMM Complexity: 1.0\n\nDIFF:\n@@ -130,7 +130,6 @@ class Functional(training_lib.Model):\n         itertools.chain(\n             (\n                 \"_layer_call_argspecs\",\n-                \"_compiled_trainable_state\",\n                 \"_output_mask_cache\",\n                 \"_output_tensor_cache\",\n                 \"_output_shape_cache\",\n\n@@ -181,6 +181,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                 \"_test_counter\",\n                 \"_predict_counter\",\n                 \"_steps_per_execution\",\n+                \"_compiled_trainable_state\",\n             ),\n             base_layer.Layer._TF_MODULE_IGNORED_PROPERTIES,\n         )\n\n@@ -56,6 +56,21 @@ class ModuleTest(tf.test.TestCase):\n         self.assertNotEmpty(m.submodules)\n         self.assertLen(m.variables, 2)\n \n+    def test_subclass_model(self):\n+        class Model(tf.keras.Model):\n+            def __init__(self):\n+                super().__init__()\n+                self.dense = tf.keras.layers.Dense(units=1)\n+\n+            def call(self, inputs, training=None, mask=None):\n+                return self.dense(inputs)\n+\n+        model = Model()\n+        self.assertLen(model.submodules, 1)  # For the dense layer\n+        model.compile(loss=\"mse\", optimizer=\"sgd\")\n+        # Make sure the compiled metric doesn't break tf.module\n+        self.assertLen(model.submodules, 1)\n+\n \n if __name__ == \"__main__\":\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9d8ec7448ee47ae883c4531469b252138b59bd80", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 387 | Lines Deleted: 112 | Files Changed: 4 | Hunks: 16 | Methods Changed: 20 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 499 | Churn Cumulative: 7085 | Contributors (this commit): 16 | Commits (past 90d): 111 | Contributors (cumulative): 33 | DMM Complexity: 0.3103448275862069\n\nDIFF:\n@@ -0,0 +1,326 @@\n+\"\"\"\n+Title: Image classification with EANet (External Attention Transformer)\n+Author: [ZhiYong Chang](https://github.com/czy00000)\n+Converted to Keras Core: [Muhammad Anas Raza](https://anasrz.com)\n+Date created: 2021/10/19\n+Last modified: 2023/07/18\n+Description: Image classification with a Transformer that leverages external attention.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Introduction\n+\n+This example implements the [EANet](https://arxiv.org/abs/2105.02358)\n+model for image classification, and demonstrates it on the CIFAR-100 dataset.\n+EANet introduces a novel attention mechanism\n+named ***external attention***, based on two external, small, learnable, and\n+shared memories, which can be implemented easily by simply using two cascaded\n+linear layers and two normalization layers. It conveniently replaces self-attention\n+as used in existing architectures. External attention has linear complexity, as it only\n+implicitly considers the correlations between all samples.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import keras_core as keras\n+from keras_core import layers\n+from keras_core import ops\n+\n+import matplotlib.pyplot as plt\n+\n+\n+\"\"\"\n+## Prepare the data\n+\"\"\"\n+\n+num_classes = 100\n+input_shape = (32, 32, 3)\n+\n+(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n+y_train = keras.utils.to_categorical(y_train, num_classes)\n+y_test = keras.utils.to_categorical(y_test, num_classes)\n+print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n+print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n+\n+\"\"\"\n+## Configure the hyperparameters\n+\"\"\"\n+\n+weight_decay = 0.0001\n+learning_rate = 0.001\n+label_smoothing = 0.1\n+validation_split = 0.2\n+batch_size = 128\n+num_epochs = 50\n+patch_size = 2  # Size of the patches to be extracted from the input images.\n+num_patches = (input_shape[0] // patch_size) ** 2  # Number of patch\n+embedding_dim = 64  # Number of hidden units.\n+mlp_dim = 64\n+dim_coefficient = 4\n+num_heads = 4\n+attention_dropout = 0.2\n+projection_dropout = 0.2\n+num_transformer_blocks = 8  # Number of repetitions of the transformer layer\n+\n+print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n+print(f\"Patches per image: {num_patches}\")\n+\n+\n+\"\"\"\n+## Use data augmentation\n+\"\"\"\n+\n+data_augmentation = keras.Sequential(\n+    [\n+        layers.Normalization(),\n+        layers.RandomFlip(\"horizontal\"),\n+        layers.RandomRotation(factor=0.1),\n+        layers.RandomContrast(factor=0.1),\n+        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n+    ],\n+    name=\"data_augmentation\",\n+)\n+# Compute the mean and the variance of the training data for normalization.\n+data_augmentation.layers[0].adapt(x_train)\n+\n+\"\"\"\n+## Implement the patch extraction and encoding layer\n+\"\"\"\n+\n+\n+class PatchExtract(layers.Layer):\n+    def __init__(self, patch_size, **kwargs):\n+        super().__init__(**kwargs)\n+        self.patch_size = patch_size\n+\n+    def call(self, x):\n+        B, C = ops.shape(x)[0], ops.shape(x)[-1]\n+        x = ops.image.extract_patches(x, self.patch_size)\n+        x = ops.reshape(x, (B, -1, self.patch_size * self.patch_size * C))\n+        return x\n+\n+\n+class PatchEmbedding(layers.Layer):\n+    def __init__(self, num_patch, embed_dim, **kwargs):\n+        super().__init__(**kwargs)\n+        self.num_patch = num_patch\n+        self.proj = layers.Dense(embed_dim)\n+        self.pos_embed = layers.Embedding(\n+            input_dim=num_patch, output_dim=embed_dim\n+        )\n+\n+    def call(self, patch):\n+        pos = ops.arange(start=0, stop=self.num_patch, step=1)\n+        return self.proj(patch) + self.pos_embed(pos)\n+\n+\n+\"\"\"\n+## Implement the external attention block\n+\"\"\"\n+\n+\n+def external_attention(\n+    x,\n+    dim,\n+    num_heads,\n+    dim_coefficient=4,\n+    attention_dropout=0,\n+    projection_dropout=0,\n+):\n+    _, num_patch, channel = x.shape\n+    assert dim % num_heads == 0\n+    num_heads = num_heads * dim_coefficient\n+\n+    x = layers.Dense(dim * dim_coefficient)(x)\n+    # create tensor [batch_size, num_patches, num_heads, dim*dim_coefficient//num_heads]\n+    x = ops.reshape(\n+        x, (-1, num_patch, num_heads, dim * dim_coefficient // num_heads)\n+    )\n+    x = ops.transpose(x, axes=[0, 2, 1, 3])\n+    # a linear layer M_k\n+    attn = layers.Dense(dim // dim_coefficient)(x)\n+    # normalize attention map\n+    attn = layers.Softmax(axis=2)(attn)\n+    # dobule-normalization\n+    attn = layers.Lambda(\n+        lambda attn: ops.divide(\n+            attn,\n+            ops.convert_to_tensor(1e-9) + ops.sum(attn, axis=-1, keepdims=True),\n+        )\n+    )(attn)\n+    attn = layers.Dropout(attention_dropout)(attn)\n+    # a linear layer M_v\n+    x = layers.Dense(dim * dim_coefficient // num_heads)(attn)\n+    x = ops.transpose(x, axes=[0, 2, 1, 3])\n+    x = ops.reshape(x, [-1, num_patch, dim * dim_coefficient])\n+    # a linear layer to project original dim\n+    x = layers.Dense(dim)(x)\n+    x = layers.Dropout(projection_dropout)(x)\n+    return x\n+\n+\n+\"\"\"\n+## Implement the MLP block\n+\"\"\"\n+\n+\n+def mlp(x, embedding_dim, mlp_dim, drop_rate=0.2):\n+    x = layers.Dense(mlp_dim, activation=ops.gelu)(x)\n+    x = layers.Dropout(drop_rate)(x)\n+    x = layers.Dense(embedding_dim)(x)\n+    x = layers.Dropout(drop_rate)(x)\n+    return x\n+\n+\n+\"\"\"\n+## Implement the Transformer block\n+\"\"\"\n+\n+\n+def transformer_encoder(\n+    x,\n+    embedding_dim,\n+    mlp_dim,\n+    num_heads,\n+    dim_coefficient,\n+    attention_dropout,\n+    projection_dropout,\n+    attention_type=\"external_attention\",\n+):\n+    residual_1 = x\n+    x = layers.LayerNormalization(epsilon=1e-5)(x)\n+    if attention_type == \"external_attention\":\n+        x = external_attention(\n+            x,\n+            embedding_dim,\n+            num_heads,\n+            dim_coefficient,\n+            attention_dropout,\n+            projection_dropout,\n+        )\n+    elif attention_type == \"self_attention\":\n+        x = layers.MultiHeadAttention(\n+            num_heads=num_heads,\n+            key_dim=embedding_dim,\n+            dropout=attention_dropout,\n+        )(x, x)\n+    x = layers.add([x, residual_1])\n+    residual_2 = x\n+    x = layers.LayerNormalization(epsilon=1e-5)(x)\n+    x = mlp(x, embedding_dim, mlp_dim)\n+    x = layers.add([x, residual_2])\n+    return x\n+\n+\n+\"\"\"\n+## Implement the EANet model\n+\"\"\"\n+\n+\"\"\"\n+The EANet model leverages external attention.\n+The computational complexity of traditional self attention is `O(d * N ** 2)`,\n+where `d` is the embedding size, and `N` is the number of patch.\n+the authors find that most pixels are closely related to just a few other\n+pixels, and an `N`-to-`N` attention matrix may be redundant.\n+So, they propose as an alternative an external\n+attention module where the computational complexity of external attention is `O(d * S * N)`.\n+As `d` and `S` are hyper-parameters,\n+the proposed algorithm is linear in the number of pixels. In fact, this is equivalent\n+to a drop patch operation, because a lot of information contained in a patch\n+in an image is redundant and unimportant.\n+\"\"\"\n+\n+\n+def get_model(attention_type=\"external_attention\"):\n+    inputs = layers.Input(shape=input_shape)\n+    # Image augment\n+    x = data_augmentation(inputs)\n+    # Extract patches.\n+    x = PatchExtract(patch_size)(x)\n+    # Create patch embedding.\n+    x = PatchEmbedding(num_patches, embedding_dim)(x)\n+    # Create Transformer block.\n+    for _ in range(num_transformer_blocks):\n+        x = transformer_encoder(\n+            x,\n+            embedding_dim,\n+            mlp_dim,\n+            num_heads,\n+            dim_coefficient,\n+            attention_dropout,\n+            projection_dropout,\n+            attention_type,\n+        )\n+\n+    x = layers.GlobalAveragePooling1D()(x)\n+    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n+    model = keras.Model(inputs=inputs, outputs=outputs)\n+    return model\n+\n+\n+\"\"\"\n+## Train on CIFAR-100\n+\n+\"\"\"\n+\n+\n+model = get_model(attention_type=\"external_attention\")\n+\n+model.compile(\n+    loss=keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing),\n+    optimizer=keras.optimizers.AdamW(\n+        learning_rate=learning_rate, weight_decay=weight_decay\n+    ),\n+    metrics=[\n+        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n+        keras.metrics.TopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n+    ],\n+)\n+\n+history = model.fit(\n+    x_train,\n+    y_train,\n+    batch_size=batch_size,\n+    epochs=num_epochs,\n+    validation_split=validation_split,\n+)\n+\n+\"\"\"\n+### Let's visualize the training progress of the model.\n+\n+\"\"\"\n+\n+plt.plot(history.history[\"loss\"], label=\"train_loss\")\n+plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n+plt.xlabel(\"Epochs\")\n+plt.ylabel(\"Loss\")\n+plt.title(\"Train and Validation Losses Over Epochs\", fontsize=14)\n+plt.legend()\n+plt.grid()\n+plt.show()\n+\n+\"\"\"\n+### Let's display the final results of the test on CIFAR-100.\n+\n+\"\"\"\n+\n+loss, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n+print(f\"Test loss: {round(loss, 2)}\")\n+print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n+print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n+\n+\"\"\"\n+EANet just replaces self attention in Vit with external attention.\n+The traditional Vit achieved a ~73% test top-5 accuracy and ~41 top-1 accuracy after\n+training 50 epochs, but with 0.6M parameters. Under the same experimental environment\n+and the same hyperparameters, The EANet model we just trained has just 0.3M parameters,\n+and it gets us to ~73% test top-5 accuracy and ~43% top-1 accuracy. This fully demonstrates the\n+effectiveness of external attention.\n+\n+We only show the training\n+process of EANet, you can train Vit under the same experimental conditions and observe\n+the test results.\n+\"\"\"\n\n@@ -55,7 +55,13 @@ def mean(x, axis=None, keepdims=False):\n def max(x, axis=None, keepdims=False, initial=None):\n     x = convert_to_tensor(x)\n     if 0 in x.shape:\n-        return 0\n+        if initial is None:\n+            raise ValueError(\"Cannot compute the max of an empty tensor.\")\n+        elif keepdims:\n+            return torch.full((1,) * len(x.shape), initial)\n+        else:\n+            return torch.tensor(initial)\n+\n     if axis is None:\n         result = torch.max(x)\n     else:\n@@ -550,6 +556,14 @@ def meshgrid(*x, indexing=\"xy\"):\n \n def min(x, axis=None, keepdims=False, initial=None):\n     x = convert_to_tensor(x)\n+    if 0 in x.shape:\n+        if initial is None:\n+            raise ValueError(\"Cannot compute the min of an empty tensor.\")\n+        elif keepdims:\n+            return torch.full((1,) * len(x.shape), initial)\n+        else:\n+            return torch.tensor(initial)\n+\n     if axis is None:\n         result = torch.min(x)\n     else:\n\n@@ -1,14 +1,14 @@\n+import numpy as np\n+\n from keras_core import backend\n-from keras_core import ops\n from keras_core.api_export import keras_core_export\n-from keras_core.layers.preprocessing.tf_data_layer import TFDataLayer\n-from keras_core.random.seed_generator import SeedGenerator\n+from keras_core.layers.layer import Layer\n from keras_core.utils import backend_utils\n-from keras_core.utils import image_utils\n+from keras_core.utils.module_utils import tensorflow as tf\n \n \n @keras_core_export(\"keras_core.layers.RandomCrop\")\n-class RandomCrop(TFDataLayer):\n+class RandomCrop(Layer):\n     \"\"\"A preprocessing layer which randomly crops images during training.\n \n     During training, this layer will randomly choose a location to crop images\n@@ -52,122 +52,41 @@ class RandomCrop(TFDataLayer):\n             `name` and `dtype`.\n     \"\"\"\n \n-    def __init__(\n-        self, height, width, seed=None, data_format=None, name=None, **kwargs\n-    ):\n+    def __init__(self, height, width, seed=None, name=None, **kwargs):\n+        if not tf.available:\n+            raise ImportError(\n+                \"Layer RandomCrop requires TensorFlow. \"\n+                \"Install it via `pip install tensorflow`.\"\n+            )\n+\n         super().__init__(name=name, **kwargs)\n-        self.height = height\n-        self.width = width\n         self.seed = seed or backend.random.make_default_seed()\n-        self.generator = SeedGenerator(seed)\n-        self.data_format = backend.standardize_data_format(data_format)\n-\n-        if self.data_format == \"channels_first\":\n-            self.heigh_axis = -2\n-            self.width_axis = -1\n-        elif self.data_format == \"channels_last\":\n-            self.height_axis = -3\n-            self.width_axis = -2\n-\n+        self.layer = tf.keras.layers.RandomCrop(\n+            height=height,\n+            width=width,\n+            seed=self.seed,\n+            name=name,\n+        )\n         self.supports_masking = False\n         self.supports_jit = False\n         self._convert_input_args = False\n         self._allow_non_tensor_positional_args = True\n \n     def call(self, inputs, training=True):\n-        inputs = self.backend.cast(inputs, self.compute_dtype)\n-        input_shape = self.backend.shape(inputs)\n-        is_batched = len(input_shape) > 3\n-        inputs = (\n-            self.backend.numpy.expand_dims(inputs, axis=0)\n-            if not is_batched\n-            else inputs\n-        )\n-\n-        h_diff = input_shape[self.height_axis] - self.height\n-        w_diff = input_shape[self.width_axis] - self.width\n-\n-        def random_crop():\n-            input_height, input_width = (\n-                input_shape[self.height_axis],\n-                input_shape[self.width_axis],\n-            )\n-\n-            seed_generator = self._get_seed_generator(self.backend._backend)\n-            h_start = self.backend.cast(\n-                ops.random.uniform(\n-                    (),\n-                    0,\n-                    maxval=float(input_height - self.height + 1),\n-                    dtype=\"int\",\n-                    seed=seed_generator,\n-                ),\n-                \"int\",\n-            )\n-            w_start = self.backend.cast(\n-                ops.random.uniform(\n-                    (),\n-                    0,\n-                    maxval=float(input_width - self.width + 1),\n-                    dtype=\"int\",\n-                    seed=seed_generator,\n-                ),\n-                \"int\",\n-            )\n-            if self.data_format == \"channels_last\":\n-                return inputs[\n-                    :,\n-                    h_start : h_start + self.height,\n-                    w_start : w_start + self.width,\n-                ]\n-            else:\n-                return inputs[\n-                    :,\n-                    :,\n-                    h_start : h_start + self.height,\n-                    w_start : w_start + self.width,\n-                ]\n-\n-        def resize():\n-            outputs = image_utils.smart_resize(\n-                inputs,\n-                [self.height, self.width],\n-                data_format=self.data_format,\n-                backend_module=self.backend,\n-            )\n-            # smart_resize will always output float32, so we need to re-cast.\n-            return self.backend.cast(outputs, self.compute_dtype)\n-\n-        outputs = self.backend.cond(\n-            self.backend.numpy.all((training, h_diff >= 0, w_diff >= 0)),\n-            random_crop,\n-            resize,\n-        )\n-\n-        outputs = (\n-            self.backend.numpy.squeeze(outputs, axis=0)\n-            if not is_batched\n-            else outputs\n-        )\n-\n-        if self.backend != \"tensorflow\" and not backend_utils.in_tf_graph():\n-            outputs = self.backend.convert_to_tensor(outputs)\n+        if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n+            inputs = tf.convert_to_tensor(backend.convert_to_numpy(inputs))\n+        outputs = self.layer.call(inputs, training=training)\n+        if (\n+            backend.backend() != \"tensorflow\"\n+            and not backend_utils.in_tf_graph()\n+        ):\n+            outputs = backend.convert_to_tensor(outputs)\n         return outputs\n \n-    def compute_output_shape(self, input_shape, *args, **kwargs):\n-        input_shape = list(input_shape)\n-        input_shape[self.height_axis] = self.height\n-        input_shape[self.width_axis] = self.width\n-        return tuple(input_shape)\n+    def compute_output_shape(self, input_shape):\n+        return tuple(self.layer.compute_output_shape(input_shape))\n \n     def get_config(self):\n-        config = super().get_config()\n-        config.update(\n-            {\n-                \"height\": self.height,\n-                \"width\": self.width,\n-                \"seed\": self.seed,\n-                \"data_format\": self.data_format,\n-            }\n-        )\n+        config = self.layer.get_config()\n+        config.update({\"seed\": self.seed})\n         return config\n\n@@ -2652,6 +2652,14 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         # test max with initial\n         self.assertAllClose(knp.max(x, initial=4), 4)\n \n+        # test empty tensor\n+        x = np.array([[]])\n+        self.assertAllClose(knp.max(x, initial=1), np.max(x, initial=1))\n+        self.assertAllClose(\n+            knp.max(x, initial=1, keepdims=True),\n+            np.max(x, initial=1, keepdims=True),\n+        )\n+\n     def test_min(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.min(x), np.min(x))\n@@ -2666,6 +2674,14 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         # test min with initial\n         self.assertAllClose(knp.min(x, initial=0), 0)\n \n+        # test empty tensor\n+        x = np.array([[]])\n+        self.assertAllClose(knp.min(x, initial=1), np.min(x, initial=1))\n+        self.assertAllClose(\n+            knp.min(x, initial=1, keepdims=True),\n+            np.min(x, initial=1, keepdims=True),\n+        )\n+\n     def test_meshgrid(self):\n         x = np.array([1, 2, 3])\n         y = np.array([4, 5, 6])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3825be015f35f38882700bc21b83eba8a1fc1d45", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 68 | Lines Deleted: 22 | Files Changed: 3 | Hunks: 33 | Methods Changed: 19 | Complexity Δ (Sum/Max): 15/13 | Churn Δ: 90 | Churn Cumulative: 3673 | Contributors (this commit): 10 | Commits (past 90d): 76 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -134,7 +134,12 @@ def array(x, dtype=None):\n \n \n def average(x, axis=None, weights=None):\n-    return tfnp.average(x, weights=weights, axis=axis)\n+    if not isinstance(axis, (list, tuple)):\n+        axis = (axis,)\n+    for a in axis:\n+        # `tfnp.average` does not handle multiple axes.\n+        x = tfnp.average(x, weights=weights, axis=a)\n+    return x\n \n \n def broadcast_to(x, shape):\n\n@@ -106,38 +106,46 @@ def abs(x):\n \n def all(x, axis=None, keepdims=False):\n     x = convert_to_tensor(x)\n-    if axis is not None:\n-        if isinstance(axis, list):\n-            axis = axis[-1]\n-        return torch.all(x, dim=axis, keepdim=keepdims)\n-    else:\n+    if axis is None:\n         return torch.all(x)\n+    if not isinstance(axis, (list, tuple)):\n+        axis = (axis,)\n+    for a in axis:\n+        # `torch.all` does not handle multiple axes.\n+        x = torch.all(x, dim=a, keepdim=keepdims)\n+    return x\n \n \n def any(x, axis=None, keepdims=False):\n     x = convert_to_tensor(x)\n-    if axis is not None:\n-        if isinstance(axis, list):\n-            axis = axis[-1]\n-        return torch.any(x, dim=axis, keepdim=keepdims)\n-    else:\n+    if axis is None:\n         return torch.any(x)\n+    if not isinstance(axis, (list, tuple)):\n+        axis = (axis,)\n+    for a in axis:\n+        # `torch.any` does not handle multiple axes.\n+        x = torch.any(x, dim=a, keepdim=keepdims)\n+    return x\n \n \n def amax(x, axis=None, keepdims=False):\n     x = convert_to_tensor(x)\n-    if axis is not None:\n-        return torch.amax(x, dim=axis, keepdim=keepdims)\n-    else:\n+    if axis is None:\n         return torch.amax(x)\n+    if axis == () or axis == []:\n+        # Torch handles the empty axis case differently from numpy.\n+        return x\n+    return torch.amax(x, dim=axis, keepdim=keepdims)\n \n \n def amin(x, axis=None, keepdims=False):\n     x = convert_to_tensor(x)\n-    if axis is not None:\n-        return torch.amin(x, dim=axis, keepdim=keepdims)\n-    else:\n+    if axis is None:\n         return torch.amin(x)\n+    if axis == () or axis == []:\n+        # Torch handles the empty axis case differently from numpy.\n+        return x\n+    return torch.amin(x, dim=axis, keepdim=keepdims)\n \n \n def append(\n@@ -209,6 +217,9 @@ def average(x, axis=None, weights=None):\n     x = convert_to_tensor(x)\n     # Conversion to float necessary for `torch.mean`\n     x = cast(x, \"float32\") if x.dtype in TORCH_INT_TYPES else x\n+    if axis == () or axis == []:\n+        # Torch handles the empty axis case differently from numpy.\n+        return x\n     if weights is not None:\n         weights = convert_to_tensor(weights)\n         return torch.sum(torch.mul(x, weights), dim=axis) / torch.sum(\n@@ -268,6 +279,9 @@ def cos(x):\n \n def count_nonzero(x, axis=None):\n     x = convert_to_tensor(x)\n+    if axis == () or axis == []:\n+        # Torch handles the empty axis case differently from numpy.\n+        return cast(torch.ne(x, 0), \"int32\")\n     return torch.count_nonzero(x, dim=axis).T\n \n \n@@ -647,9 +661,12 @@ def prod(x, axis=None, keepdims=False, dtype=None):\n     dtype = to_torch_dtype(dtype)\n     if axis is None:\n         return torch.prod(x, dtype=dtype)\n-    elif isinstance(axis, list):\n-        axis = axis[-1]\n-    return torch.prod(x, dim=axis, keepdim=keepdims, dtype=dtype)\n+    if not isinstance(axis, (list, tuple)):\n+        axis = (axis,)\n+    for a in axis:\n+        # `torch.prod` does not handle multiple axes.\n+        x = torch.prod(x, dim=a, keepdim=keepdims, dtype=dtype)\n+    return x\n \n \n def ravel(x):\n@@ -895,6 +912,9 @@ def var(x, axis=None, keepdims=False):\n     x = convert_to_tensor(x, dtype=\"float32\")\n     # Conversion to float necessary for `torch.var`\n     x = cast(x, \"float32\") if x.dtype in TORCH_INT_TYPES else x\n+    if axis == [] or axis == ():\n+        # Torch handles the empty axis case differently from numpy.\n+        return zeros_like(x)\n     # Bessel correction removed for numpy compatibility\n     return torch.var(x, dim=axis, keepdim=keepdims, correction=0)\n \n\n@@ -2097,8 +2097,8 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n     def test_mean(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.mean(x), np.mean(x))\n-        self.assertAllClose(knp.mean(x, axis=1), np.mean(x, axis=1))\n         self.assertAllClose(knp.mean(x, axis=()), np.mean(x, axis=()))\n+        self.assertAllClose(knp.mean(x, axis=1), np.mean(x, axis=1))\n         self.assertAllClose(knp.mean(x, axis=(1,)), np.mean(x, axis=(1,)))\n         self.assertAllClose(\n             knp.mean(x, axis=1, keepdims=True),\n@@ -2115,7 +2115,9 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n     def test_all(self):\n         x = np.array([[True, False, True], [True, True, True]])\n         self.assertAllClose(knp.all(x), np.all(x))\n+        self.assertAllClose(knp.all(x, axis=()), np.all(x, axis=()))\n         self.assertAllClose(knp.all(x, axis=1), np.all(x, axis=1))\n+        self.assertAllClose(knp.all(x, axis=(1,)), np.all(x, axis=(1,)))\n         self.assertAllClose(\n             knp.all(x, axis=1, keepdims=True),\n             np.all(x, axis=1, keepdims=True),\n@@ -2131,7 +2133,9 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n     def test_any(self):\n         x = np.array([[True, False, True], [True, True, True]])\n         self.assertAllClose(knp.any(x), np.any(x))\n+        self.assertAllClose(knp.any(x, axis=()), np.any(x, axis=()))\n         self.assertAllClose(knp.any(x, axis=1), np.any(x, axis=1))\n+        self.assertAllClose(knp.any(x, axis=(1,)), np.any(x, axis=(1,)))\n         self.assertAllClose(\n             knp.any(x, axis=1, keepdims=True),\n             np.any(x, axis=1, keepdims=True),\n@@ -2147,7 +2151,9 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n     def test_var(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.var(x), np.var(x))\n+        self.assertAllClose(knp.var(x, axis=()), np.var(x, axis=()))\n         self.assertAllClose(knp.var(x, axis=1), np.var(x, axis=1))\n+        self.assertAllClose(knp.var(x, axis=(1,)), np.var(x, axis=(1,)))\n         self.assertAllClose(\n             knp.var(x, axis=1, keepdims=True),\n             np.var(x, axis=1, keepdims=True),\n@@ -2163,9 +2169,9 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n     def test_sum(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.sum(x), np.sum(x))\n+        self.assertAllClose(knp.sum(x, axis=()), np.sum(x, axis=()))\n         self.assertAllClose(knp.sum(x, axis=1), np.sum(x, axis=1))\n         self.assertAllClose(knp.sum(x, axis=(1,)), np.sum(x, axis=(1,)))\n-        self.assertAllClose(knp.sum(x, axis=()), np.sum(x, axis=()))\n         self.assertAllClose(\n             knp.sum(x, axis=1, keepdims=True),\n             np.sum(x, axis=1, keepdims=True),\n@@ -2181,7 +2187,9 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n     def test_amax(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.amax(x), np.amax(x))\n+        self.assertAllClose(knp.amax(x, axis=()), np.amax(x, axis=()))\n         self.assertAllClose(knp.amax(x, axis=1), np.amax(x, axis=1))\n+        self.assertAllClose(knp.amax(x, axis=(1,)), np.amax(x, axis=(1,)))\n         self.assertAllClose(\n             knp.amax(x, axis=1, keepdims=True),\n             np.amax(x, axis=1, keepdims=True),\n@@ -2197,7 +2205,9 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n     def test_amin(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.amin(x), np.amin(x))\n+        self.assertAllClose(knp.amin(x, axis=()), np.amin(x, axis=()))\n         self.assertAllClose(knp.amin(x, axis=1), np.amin(x, axis=1))\n+        self.assertAllClose(knp.amin(x, axis=(1,)), np.amin(x, axis=(1,)))\n         self.assertAllClose(\n             knp.amin(x, axis=1, keepdims=True),\n             np.amin(x, axis=1, keepdims=True),\n@@ -2312,7 +2322,9 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         weights = np.ones([2, 3])\n         weights_1d = np.ones([3])\n         self.assertAllClose(knp.average(x), np.average(x))\n+        self.assertAllClose(knp.average(x, axis=()), np.average(x, axis=()))\n         self.assertAllClose(knp.average(x, axis=1), np.average(x, axis=1))\n+        self.assertAllClose(knp.average(x, axis=(1,)), np.average(x, axis=(1,)))\n         self.assertAllClose(\n             knp.average(x, axis=1, weights=weights),\n             np.average(x, axis=1, weights=weights),\n@@ -2424,10 +2436,17 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n     def test_count_nonzero(self):\n         x = np.array([[0, 2, 3], [3, 2, 0]])\n         self.assertAllClose(knp.count_nonzero(x), np.count_nonzero(x))\n+        self.assertAllClose(\n+            knp.count_nonzero(x, axis=()), np.count_nonzero(x, axis=())\n+        )\n         self.assertAllClose(\n             knp.count_nonzero(x, axis=1),\n             np.count_nonzero(x, axis=1),\n         )\n+        self.assertAllClose(\n+            knp.count_nonzero(x, axis=(1,)),\n+            np.count_nonzero(x, axis=(1,)),\n+        )\n \n         self.assertAllClose(\n             knp.CountNonzero()(x),\n@@ -2806,7 +2825,9 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n     def test_prod(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n         self.assertAllClose(knp.prod(x), np.prod(x))\n+        self.assertAllClose(knp.prod(x, axis=()), np.prod(x, axis=()))\n         self.assertAllClose(knp.prod(x, axis=1), np.prod(x, axis=1))\n+        self.assertAllClose(knp.prod(x, axis=(1,)), np.prod(x, axis=(1,)))\n         self.assertAllClose(\n             knp.prod(x, axis=1, keepdims=True),\n             np.prod(x, axis=1, keepdims=True),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7c723f242c6b053ebd42aad7034230ff89cb86a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 59 | Lines Deleted: 21 | Files Changed: 1 | Hunks: 14 | Methods Changed: 6 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 80 | Churn Cumulative: 80 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n \"\"\"\n Title: Image classification with Vision Transformer\n Author: [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)\n+Converted to Keras Core by: [divyasreepat](https://github.com/divyashreepathihalli), [Soumik Rakshit](http://github.com/soumik12345)\n Date created: 2021/01/18\n Last modified: 2021/01/18\n Description: Implementing the Vision Transformer (ViT) model for image classification.\n@@ -22,10 +23,16 @@ image patches, without using convolution layers.\n ## Setup\n \"\"\"\n \n-import numpy as np\n-import tensorflow as tf\n+import os\n+\n+os.environ[\"KERAS_BACKEND\"] = \"jax\"  # @param [\"tensorflow\", \"jax\", \"torch\"]\n+\n import keras_core as keras\n from keras_core import layers\n+from keras_core import ops\n+\n+import numpy as np\n+import matplotlib.pyplot as plt\n \n \"\"\"\n ## Prepare the data\n@@ -89,7 +96,7 @@ data_augmentation.layers[0].adapt(x_train)\n \n def mlp(x, hidden_units, dropout_rate):\n     for units in hidden_units:\n-        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n+        x = layers.Dense(units, activation=keras.activations.gelu)(x)\n         x = layers.Dropout(dropout_rate)(x)\n     return x\n \n@@ -105,32 +112,40 @@ class Patches(layers.Layer):\n         self.patch_size = patch_size\n \n     def call(self, images):\n-        batch_size = tf.shape(images)[0]\n-        patches = tf.image.extract_patches(\n-            images=images,\n-            sizes=[1, self.patch_size, self.patch_size, 1],\n-            strides=[1, self.patch_size, self.patch_size, 1],\n-            rates=[1, 1, 1, 1],\n-            padding=\"VALID\",\n+        input_shape = ops.backend.shape(images)\n+        batch_size = input_shape[0]\n+        height = input_shape[1]\n+        width = input_shape[2]\n+        channels = input_shape[3]\n+        num_patches_h = height // self.patch_size\n+        num_patches_w = width // self.patch_size\n+        patches = keras.ops.image.extract_patches(images, size=self.patch_size)\n+        patches = ops.reshape(\n+            patches, (\n+                batch_size,\n+                num_patches_h * num_patches_w,\n+                self.patch_size * self.patch_size * channels,\n+            )\n         )\n-        patch_dims = patches.shape[-1]\n-        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n         return patches\n \n+    def get_config(self):\n+        config = super().get_config()\n+        config.update({\"patch_size\": self.patch_size})\n+        return config\n+\n \n \"\"\"\n Let's display patches for a sample image\n \"\"\"\n \n-import matplotlib.pyplot as plt\n-\n plt.figure(figsize=(4, 4))\n image = x_train[np.random.choice(range(x_train.shape[0]))]\n plt.imshow(image.astype(\"uint8\"))\n plt.axis(\"off\")\n \n-resized_image = tf.image.resize(\n-    tf.convert_to_tensor([image]), size=(image_size, image_size)\n+resized_image = ops.image.resize(\n+    ops.convert_to_tensor([image]), size=(image_size, image_size)\n )\n patches = Patches(patch_size)(resized_image)\n print(f\"Image size: {image_size} X {image_size}\")\n@@ -142,8 +157,8 @@ n = int(np.sqrt(patches.shape[1]))\n plt.figure(figsize=(4, 4))\n for i, patch in enumerate(patches[0]):\n     ax = plt.subplot(n, n, i + 1)\n-    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n-    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n+    patch_img = ops.reshape(patch, (patch_size, patch_size, 3))\n+    plt.imshow(ops.convert_to_numpy(patch_img).astype(\"uint8\"))\n     plt.axis(\"off\")\n \n \"\"\"\n@@ -165,10 +180,18 @@ class PatchEncoder(layers.Layer):\n         )\n \n     def call(self, patch):\n-        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n-        encoded = self.projection(patch) + self.position_embedding(positions)\n+        positions = ops.expand_dims(\n+            ops.arange(start=0, stop=self.num_patches, step=1), axis=0\n+        )\n+        projected_patches = self.projection(patch)\n+        encoded = projected_patches + self.position_embedding(positions)\n         return encoded\n \n+    def get_config(self):\n+        config = super().get_config()\n+        config.update({\"num_patches\": self.num_patches})\n+        return config\n+\n \n \"\"\"\n ## Build the ViT model\n@@ -191,7 +214,7 @@ especially when the number of patches and the projection dimensions are large.\n \n \n def create_vit_classifier():\n-    inputs = layers.Input(shape=input_shape)\n+    inputs = keras.Input(shape=input_shape)\n     # Augment data.\n     augmented = data_augmentation(inputs)\n     # Create patches.\n@@ -281,6 +304,21 @@ vit_classifier = create_vit_classifier()\n history = run_experiment(vit_classifier)\n \n \n+def plot_history(item):\n+    plt.plot(history.history[item], label=item)\n+    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n+    plt.xlabel(\"Epochs\")\n+    plt.ylabel(item)\n+    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n+    plt.legend()\n+    plt.grid()\n+    plt.show()\n+\n+\n+plot_history(\"loss\")\n+plot_history(\"top-5-accuracy\")\n+\n+\n \"\"\"\n After 100 epochs, the ViT model achieves around 55% accuracy and\n 82% top-5 accuracy on the test data. These are not competitive results on the CIFAR-100 dataset,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fa22d8bf66979073ca625dc719abc9b03ee59747", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 49 | Files Changed: 2 | Hunks: 38 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 109 | Churn Cumulative: 1220 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,7 +3,7 @@ Title: CutMix data augmentation for image classification\n Author: [Sayan Nath](https://twitter.com/sayannath2350)\n Converted to Keras Core By: [Piyush Thakur](https://github.com/cosmo3769)\n Date created: 2021/06/08\n-Last modified: 2023/07/18\n+Last modified: 2023/07/24\n Description: Data augmentation with CutMix for image classification on CIFAR-10.\n Accelerator: GPU\n \"\"\"\n@@ -48,10 +48,16 @@ where `rx, ry` are randomly drawn from a uniform distribution with upper bound.\n \n import numpy as np\n import pandas as pd\n-import matplotlib.pyplot as plt\n-import tensorflow as tf\n-from keras_core import layers\n import keras_core as keras\n+import matplotlib.pyplot as plt\n+\n+from keras_core import layers\n+\n+# TF imports related to tf.data preprocessing \n+from tensorflow import clip_by_value\n+from tensorflow import data as tf_data\n+from tensorflow import image as tf_image\n+from tensorflow.random import gamma as tf_random_gamma\n \n keras.utils.set_random_seed(42)\n \n@@ -88,7 +94,7 @@ class_names = [\n ## Define hyperparameters\n \"\"\"\n \n-AUTO = tf.data.AUTOTUNE\n+AUTO = tf_data.AUTOTUNE\n BATCH_SIZE = 32\n IMG_SIZE = 32\n \n@@ -98,9 +104,9 @@ IMG_SIZE = 32\n \n \n def preprocess_image(image, label):\n-    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n-    image = tf.image.convert_image_dtype(image, tf.float32) / 255.0\n-    label = tf.cast(label, tf.float32)\n+    image = tf_image.resize(image, (IMG_SIZE, IMG_SIZE))\n+    image = tf_image.convert_image_dtype(image, \"float32\") / 255.0\n+    label = keras.backend.cast(label, dtype=\"float32\")\n     return image, label\n \n \n@@ -109,19 +115,19 @@ def preprocess_image(image, label):\n \"\"\"\n \n train_ds_one = (\n-    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n+    tf_data.Dataset.from_tensor_slices((x_train, y_train))\n     .shuffle(1024)\n     .map(preprocess_image, num_parallel_calls=AUTO)\n )\n train_ds_two = (\n-    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n+    tf_data.Dataset.from_tensor_slices((x_train, y_train))\n     .shuffle(1024)\n     .map(preprocess_image, num_parallel_calls=AUTO)\n )\n \n-train_ds_simple = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n+train_ds_simple = tf_data.Dataset.from_tensor_slices((x_train, y_train))\n \n-test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n+test_ds = tf_data.Dataset.from_tensor_slices((x_test, y_test))\n \n train_ds_simple = (\n     train_ds_simple.map(preprocess_image, num_parallel_calls=AUTO)\n@@ -130,7 +136,7 @@ train_ds_simple = (\n )\n \n # Combine two shuffled datasets from the same training data.\n-train_ds = tf.data.Dataset.zip((train_ds_one, train_ds_two))\n+train_ds = tf_data.Dataset.zip((train_ds_one, train_ds_two))\n \n test_ds = (\n     test_ds.map(preprocess_image, num_parallel_calls=AUTO)\n@@ -146,28 +152,29 @@ The CutMix function takes two `image` and `label` pairs to perform the augmentat\n \n \n def sample_beta_distribution(size, concentration_0=0.2, concentration_1=0.2):\n-    gamma_1_sample = tf.random.gamma(shape=[size], alpha=concentration_1)\n-    gamma_2_sample = tf.random.gamma(shape=[size], alpha=concentration_0)\n+    gamma_1_sample = tf_random_gamma(shape=[size], alpha=concentration_1)\n+    gamma_2_sample = tf_random_gamma(shape=[size], alpha=concentration_0)\n     return gamma_1_sample / (gamma_1_sample + gamma_2_sample)\n \n \n-@tf.function\n def get_box(lambda_value):\n-    cut_rat = tf.math.sqrt(1.0 - lambda_value)\n+    cut_rat = keras.ops.sqrt(1.0 - lambda_value)\n \n     cut_w = IMG_SIZE * cut_rat  # rw\n-    cut_w = tf.cast(cut_w, tf.int32)\n+    cut_w = keras.backend.cast(cut_w, \"int32\")\n \n     cut_h = IMG_SIZE * cut_rat  # rh\n-    cut_h = tf.cast(cut_h, tf.int32)\n+    cut_h = keras.backend.cast(cut_h, \"int32\")\n \n-    cut_x = tf.random.uniform((1,), minval=0, maxval=IMG_SIZE, dtype=tf.int32)  # rx\n-    cut_y = tf.random.uniform((1,), minval=0, maxval=IMG_SIZE, dtype=tf.int32)  # ry\n+    cut_x = keras.random.random.uniform((1,), minval=0, maxval=IMG_SIZE)  # rx\n+    cut_x = keras.backend.cast(cut_x, \"int32\")\n+    cut_y = keras.random.random.uniform((1,), minval=0, maxval=IMG_SIZE)  # ry\n+    cut_y = keras.backend.cast(cut_y, \"int32\")\n \n-    boundaryx1 = tf.clip_by_value(cut_x[0] - cut_w // 2, 0, IMG_SIZE)\n-    boundaryy1 = tf.clip_by_value(cut_y[0] - cut_h // 2, 0, IMG_SIZE)\n-    bbx2 = tf.clip_by_value(cut_x[0] + cut_w // 2, 0, IMG_SIZE)\n-    bby2 = tf.clip_by_value(cut_y[0] + cut_h // 2, 0, IMG_SIZE)\n+    boundaryx1 = clip_by_value(cut_x[0] - cut_w // 2, 0, IMG_SIZE)\n+    boundaryy1 = clip_by_value(cut_y[0] - cut_h // 2, 0, IMG_SIZE)\n+    bbx2 = clip_by_value(cut_x[0] + cut_w // 2, 0, IMG_SIZE)\n+    bby2 = clip_by_value(cut_y[0] + cut_h // 2, 0, IMG_SIZE)\n \n     target_h = bby2 - boundaryy1\n     if target_h == 0:\n@@ -180,7 +187,6 @@ def get_box(lambda_value):\n     return boundaryx1, boundaryy1, target_h, target_w\n \n \n-@tf.function\n def cutmix(train_ds_one, train_ds_two):\n     (image1, label1), (image2, label2) = train_ds_one, train_ds_two\n \n@@ -197,19 +203,19 @@ def cutmix(train_ds_one, train_ds_two):\n     boundaryx1, boundaryy1, target_h, target_w = get_box(lambda_value)\n \n     # Get a patch from the second image (`image2`)\n-    crop2 = tf.image.crop_to_bounding_box(\n+    crop2 = tf_image.crop_to_bounding_box(\n         image2, boundaryy1, boundaryx1, target_h, target_w\n     )\n     # Pad the `image2` patch (`crop2`) with the same offset\n-    image2 = tf.image.pad_to_bounding_box(\n+    image2 = tf_image.pad_to_bounding_box(\n         crop2, boundaryy1, boundaryx1, IMG_SIZE, IMG_SIZE\n     )\n     # Get a patch from the first image (`image1`)\n-    crop1 = tf.image.crop_to_bounding_box(\n+    crop1 = tf_image.crop_to_bounding_box(\n         image1, boundaryy1, boundaryx1, target_h, target_w\n     )\n     # Pad the `image1` patch (`crop1`) with the same offset\n-    img1 = tf.image.pad_to_bounding_box(\n+    img1 = tf_image.pad_to_bounding_box(\n         crop1, boundaryy1, boundaryx1, IMG_SIZE, IMG_SIZE\n     )\n \n@@ -221,7 +227,7 @@ def cutmix(train_ds_one, train_ds_two):\n \n     # Adjust Lambda in accordance to the pixel ration\n     lambda_value = 1 - (target_w * target_h) / (IMG_SIZE * IMG_SIZE)\n-    lambda_value = tf.cast(lambda_value, tf.float32)\n+    lambda_value = keras.backend.cast(lambda_value, \"float32\")\n \n     # Combine the labels of both images\n     label = lambda_value * label1 + (1 - lambda_value) * label2\n@@ -371,7 +377,7 @@ print(\"Test accuracy: {:.2f}%\".format(test_accuracy * 100))\n \n In this example, we trained our model for 15 epochs.\n In our experiment, the model with CutMix achieves a better accuracy on the CIFAR-10 dataset\n-(76.92% in our experiment) compared to the model that doesn't use the augmentation (72.23%).\n+(77.34% in our experiment) compared to the model that doesn't use the augmentation (66.90%).\n You may notice it takes less time to train the model with the CutMix augmentation.\n \n You can experiment further with the CutMix technique by following the\n\n@@ -2,7 +2,7 @@\n Title: MixUp augmentation for image classification\n Author: [Sayak Paul](https://twitter.com/RisingSayak)\n Date created: 2021/03/06\n-Last modified: 2021/03/06\n+Last modified: 2023/07/24\n Description: Data augmentation using the mixup technique for image classification.\n Accelerator: GPU\n \"\"\"\n@@ -37,10 +37,15 @@ processing, speech, and so on.\n \"\"\"\n \n import numpy as np\n-import tensorflow as tf\n-import matplotlib.pyplot as plt\n-from keras_core import layers\n import keras_core as keras\n+import matplotlib.pyplot as plt\n+\n+from keras_core import layers\n+\n+# TF imports related to tf.data preprocessing \n+from tensorflow import data as tf_data\n+from tensorflow import image as tf_image\n+from tensorflow.random import gamma as tf_random_gamma\n \n \"\"\"\n ## Prepare the dataset\n@@ -53,17 +58,17 @@ be used for other classification datasets as well.\n \n x_train = x_train.astype(\"float32\") / 255.0\n x_train = np.reshape(x_train, (-1, 28, 28, 1))\n-y_train = tf.one_hot(y_train, 10)\n+y_train = keras.ops.one_hot(y_train, 10)\n \n x_test = x_test.astype(\"float32\") / 255.0\n x_test = np.reshape(x_test, (-1, 28, 28, 1))\n-y_test = tf.one_hot(y_test, 10)\n+y_test = keras.ops.one_hot(y_test, 10)\n \n \"\"\"\n ## Define hyperparameters\n \"\"\"\n \n-AUTO = tf.data.AUTOTUNE\n+AUTO = tf_data.AUTOTUNE\n BATCH_SIZE = 64\n EPOCHS = 10\n \n@@ -77,22 +82,22 @@ x_val, y_val = x_train[:val_samples], y_train[:val_samples]\n new_x_train, new_y_train = x_train[val_samples:], y_train[val_samples:]\n \n train_ds_one = (\n-    tf.data.Dataset.from_tensor_slices((new_x_train, new_y_train))\n+    tf_data.Dataset.from_tensor_slices((new_x_train, new_y_train))\n     .shuffle(BATCH_SIZE * 100)\n     .batch(BATCH_SIZE)\n )\n train_ds_two = (\n-    tf.data.Dataset.from_tensor_slices((new_x_train, new_y_train))\n+    tf_data.Dataset.from_tensor_slices((new_x_train, new_y_train))\n     .shuffle(BATCH_SIZE * 100)\n     .batch(BATCH_SIZE)\n )\n # Because we will be mixing up the images and their corresponding labels, we will be\n # combining two shuffled datasets from the same training data.\n-train_ds = tf.data.Dataset.zip((train_ds_one, train_ds_two))\n+train_ds = tf_data.Dataset.zip((train_ds_one, train_ds_two))\n \n-val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(BATCH_SIZE)\n+val_ds = tf_data.Dataset.from_tensor_slices((x_val, y_val)).batch(BATCH_SIZE)\n \n-test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)\n+test_ds = tf_data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)\n \n \"\"\"\n ## Define the mixup technique function\n@@ -105,8 +110,8 @@ the same dataset, and apply a lambda value within the [0, 1] range sampled from\n \n \n def sample_beta_distribution(size, concentration_0=0.2, concentration_1=0.2):\n-    gamma_1_sample = tf.random.gamma(shape=[size], alpha=concentration_1)\n-    gamma_2_sample = tf.random.gamma(shape=[size], alpha=concentration_0)\n+    gamma_1_sample = tf_random_gamma(shape=[size], alpha=concentration_1)\n+    gamma_2_sample = tf_random_gamma(shape=[size], alpha=concentration_0)\n     return gamma_1_sample / (gamma_1_sample + gamma_2_sample)\n \n \n@@ -114,12 +119,12 @@ def mix_up(ds_one, ds_two, alpha=0.2):\n     # Unpack two datasets\n     images_one, labels_one = ds_one\n     images_two, labels_two = ds_two\n-    batch_size = tf.shape(images_one)[0]\n+    batch_size = keras.backend.shape(images_one)[0]\n \n     # Sample lambda and reshape it to do the mixup\n     l = sample_beta_distribution(batch_size, alpha, alpha)\n-    x_l = tf.reshape(l, (batch_size, 1, 1, 1))\n-    y_l = tf.reshape(l, (batch_size, 1))\n+    x_l = keras.ops.reshape(l, (batch_size, 1, 1, 1))\n+    y_l = keras.ops.reshape(l, (batch_size, 1))\n \n     # Perform mixup on both images and labels by combining a pair of images/labels\n     # (one from each dataset) into one image/label\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
