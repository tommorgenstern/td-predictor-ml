{"custom_id": "scrapy#e6bf2821ef8e106863bae86470d7216f9fa0eb96", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 23 | Churn Cumulative: 149 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -73,10 +73,9 @@ class Downloader(object):\n         Response object, then request never reach downloader queue, and it will\n         not be downloaded from site.\n         \"\"\"\n-        domain = spider.domain_name\n-        site = self.sites[domain]\n+        site = self.sites[spider.domain_name]\n         if site.closed:\n-            raise IgnoreRequest('Can\\'t fetch on a closed domain: %s' + request)\n+            raise IgnoreRequest('Can\\'t fetch on a closed domain')\n \n         site.active.add(request)\n         def _deactivate(_):\n@@ -101,7 +100,7 @@ class Downloader(object):\n         if not site:\n             return\n \n-        # download delay handling\n+        # Delay queue processing if a download_delay is configured\n         now = datetime.datetime.now()\n         if site.download_delay and site.lastseen:\n             delta = now - site.lastseen\n@@ -111,23 +110,23 @@ class Downloader(object):\n                 return\n         site.lastseen = now\n \n+        # Process requests in queue if there are free slots to transfer for this site\n         while site.queue and site.capacity() > 0:\n             request, deferred = site.queue.pop(0)\n-            self._download(site, request, spider, deferred)\n+            self._download(site, request, spider).chainDeferred(deferred)\n \n+        # Free site resources if domain was asked to be closed and it is idle.\n         if site.closed and site.is_idle():\n             del self.sites[domain]\n-            self.engine.closed_domain(domain)\n+            self.engine.closed_domain(domain) # notify engine.\n \n-    def _download(self, site, request, spider, deferred):\n+    def _download(self, site, request, spider):\n         site.transferring.add(request)\n-        def _finish(_):\n+        def _transferred(_):\n             site.transferring.remove(request)\n             self.process_queue(spider)\n-\n-        dwld = mustbe_deferred(download_any, request, spider)\n-        chain_deferred(dwld, deferred)\n-        return dwld.addBoth(_finish)\n+            return _\n+        return mustbe_deferred(download_any, request, spider).addBoth(_transferred)\n \n     def open_domain(self, domain):\n         \"\"\"Allocate resources to begin processing a domain\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b488e3b4a90721debfff25e51011ba4886b81a7f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 6 | Churn Cumulative: 155 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -93,6 +93,12 @@ class Downloader(object):\n         return deferred\n \n     def process_queue(self, spider):\n+        try:\n+            self._process_queue(spider)\n+        except:\n+            log.exc('Downloader process queue bug')\n+\n+    def _process_queue(self, spider):\n         \"\"\" Effective download requests from site queue\n         \"\"\"\n         domain = spider.domain_name\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e8543ca1f9778317c5ad3e3b739063910d1f06eb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 328 | Contributors (this commit): 3 | Commits (past 90d): 15 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -372,7 +372,7 @@ class ExecutionEngine(object):\n             log.exc(\"Exception catched on domain_idle signal dispatch\")\n \n         if self.domain_is_idle(domain):\n-            self._close_domain(domain)\n+            self.close_domain(domain, reason='finished')\n \n     def _stop_if_idle(self):\n         \"\"\"Call the stop method if the system has no outstanding tasks. \"\"\"\n@@ -384,9 +384,6 @@ class ExecutionEngine(object):\n         if domain not in self.closing:\n             log.msg(\"Closing domain (%s)\" % reason, domain=domain)\n             self.closing[domain] = reason\n-            self._close_domain(domain)\n-\n-    def _close_domain(self, domain):\n             self.downloader.close_domain(domain)\n \n     def closed_domain(self, domain):\n@@ -400,7 +397,7 @@ class ExecutionEngine(object):\n         self.scheduler.close_domain(domain)\n         self.pipeline.close_domain(domain)\n         del self._scraping[domain]\n-        reason = self.closing.get(domain, 'finished')\n+        reason = self.closing[domain]\n         signals.send_catch_log(signal=signals.domain_closed, sender=self.__class__, domain=domain, spider=spider, reason=reason)\n         log.msg(\"Domain closed (%s)\" % reason, domain=domain) \n         self.closing.pop(domain, None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fb8e24acb5114d84c63e6ae287696b5710615d7e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 48 | Files Changed: 2 | Hunks: 20 | Methods Changed: 21 | Complexity Δ (Sum/Max): -4/2 | Churn Δ: 79 | Churn Cumulative: 562 | Contributors (this commit): 3 | Commits (past 90d): 27 | Contributors (cumulative): 6 | DMM Complexity: 0.16666666666666666\n\nDIFF:\n@@ -11,7 +11,8 @@ from scrapy.spider import spiders\n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n from scrapy.core.downloader.handlers import download_any\n from scrapy.conf import settings\n-from scrapy.utils.defer import chain_deferred, mustbe_deferred\n+from scrapy.utils.defer import mustbe_deferred\n+from scrapy import log\n \n \n class SiteInfo(object):\n@@ -33,20 +34,14 @@ class SiteInfo(object):\n         self.queue = []\n         self.active = set()\n         self.transferring = set()\n-        self.closed = False\n+        self.closing = False\n         self.lastseen = None\n \n-    def is_idle(self):\n-        return not (self.active or self.transferring)\n-\n-    def capacity(self):\n+    def free_transfer_slots(self):\n         return self.max_concurrent_requests - len(self.transferring)\n \n-    def outstanding(self):\n-        return len(self.active) + len(self.queue)\n-\n     def needs_backout(self):\n-        return self.outstanding() > (2 * self.max_concurrent_requests)\n+        return len(self.queue) > 2 * self.max_concurrent_requests\n \n \n class Downloader(object):\n@@ -74,12 +69,13 @@ class Downloader(object):\n         not be downloaded from site.\n         \"\"\"\n         site = self.sites[spider.domain_name]\n-        if site.closed:\n-            raise IgnoreRequest('Can\\'t fetch on a closed domain')\n+        if site.closing:\n+            raise IgnoreRequest('Can\\'t fetch on a closing domain')\n \n         site.active.add(request)\n         def _deactivate(_):\n             site.active.remove(request)\n+            self._close_if_idle(spider.domain_name)\n             return _\n \n         return self.middleware.download(self.enqueue, request, spider).addBoth(_deactivate)\n@@ -117,12 +113,15 @@ class Downloader(object):\n         site.lastseen = now\n \n         # Process requests in queue if there are free slots to transfer for this site\n-        while site.queue and site.capacity() > 0:\n+        while site.queue and site.free_transfer_slots() > 0:\n             request, deferred = site.queue.pop(0)\n             self._download(site, request, spider).chainDeferred(deferred)\n \n-        # Free site resources if domain was asked to be closed and it is idle.\n-        if site.closed and site.is_idle():\n+        self._close_if_idle(domain)\n+\n+    def _close_if_idle(self, domain):\n+        site = self.sites.get(domain)\n+        if site and site.closing and not site.active:\n             del self.sites[domain]\n             self.engine.closed_domain(domain) # notify engine.\n \n@@ -148,29 +147,13 @@ class Downloader(object):\n     def close_domain(self, domain):\n         \"\"\"Free any resources associated with the given domain\"\"\"\n         site = self.sites.get(domain)\n-        if not site or site.closed:\n+        if not site or site.closing:\n             raise RuntimeError('Downloader domain already closed: %s' % domain)\n \n-        site.closed = True\n+        site.closing = True\n         spider = spiders.fromdomain(domain)\n         self.process_queue(spider)\n \n-    def needs_backout(self, domain):\n-        site = self.sites.get(domain)\n-        return (site.needs_backout() if site else True)\n-\n-    # Most of the following functions must be reviewed to decide if are really needed\n-    def domain_is_open(self, domain):\n-        return domain in self.sites\n-\n-    def outstanding(self, domain):\n-        \"\"\"The number of outstanding requests for a domain\n-        This includes both active requests and pending requests.\n-        \"\"\"\n-        site = self.sites.get(domain)\n-        if site:\n-            return site.outstanding()\n-\n     def has_capacity(self):\n         \"\"\"Does the downloader have capacity to handle more domains\"\"\"\n         return len(self.sites) < self.concurrent_domains\n\n@@ -164,7 +164,7 @@ class ExecutionEngine(object):\n             self.open_domain(domain, spider)\n         return domain\n \n-    def next_request(self, spider, breakloop=True):\n+    def next_request(self, spider, now=False):\n         \"\"\"Scrape the next request for the domain passed.\n \n         The next request to be scraped is retrieved from the scheduler and\n@@ -175,18 +175,16 @@ class ExecutionEngine(object):\n         if self.paused:\n             return reactor.callLater(5, self.next_request, spider)\n \n-        if breakloop:\n-            # delaying make reentrant call to next_request safe\n-            return reactor.callLater(0, self.next_request, spider, breakloop=False)\n+        # call next_request in next reactor loop by default\n+        if not now:\n+            return reactor.callLater(0, self.next_request, spider, now=True)\n \n         domain = spider.domain_name\n \n-        # check that the engine is still running and domain is open\n-        if not self.running:\n-            return\n-\n-        # backout enqueing downloads if domain needs it\n-        if domain in self.closing or self.downloader.needs_backout(domain):\n+        if not self.running or \\\n+                domain in self.closing or \\\n+                self.domain_is_closed(domain) or \\\n+                self.downloader.sites[domain].needs_backout():\n             return\n \n         # Next pending request from scheduler\n@@ -208,12 +206,12 @@ class ExecutionEngine(object):\n     def domain_is_idle(self, domain):\n         scraping = self._scraping.get(domain)\n         pending = self.scheduler.domain_has_pending_requests(domain)\n-        downloading = domain in self.downloader.sites and self.downloader.sites[domain].outstanding() > 0\n+        downloading = domain in self.downloader.sites and self.downloader.sites[domain].active\n         haspipe = not self.pipeline.domain_is_idle(domain)\n         return not (pending or downloading or haspipe or scraping)\n \n-    def domain_is_open(self, domain):\n-        return domain in self.downloader.sites\n+    def domain_is_closed(self, domain):\n+        return domain not in self.downloader.sites\n \n     @property\n     def open_domains(self):\n@@ -283,9 +281,11 @@ class ExecutionEngine(object):\n \n     def schedule(self, request, spider):\n         domain = spider.domain_name\n+        if domain in self.closing:\n+            raise IgnoreRequest()\n         if not self.scheduler.domain_is_open(domain):\n             self.scheduler.open_domain(domain)\n-            if not self.downloader.domain_is_open(domain):\n+            if self.domain_is_closed(domain): # scheduler auto-open\n                 self.domain_scheduler.add_domain(domain)\n         self.next_request(spider)\n         return self.scheduler.enqueue_request(domain, request)\n@@ -429,7 +429,7 @@ class ExecutionEngine(object):\n             \"len(self.downloader.sites[domain].queue)\",\n             \"len(self.downloader.sites[domain].active)\",\n             \"len(self.downloader.sites[domain].transferring)\",\n-            \"self.downloader.sites[domain].closed\",\n+            \"self.downloader.sites[domain].closing\",\n             \"self.downloader.sites[domain].lastseen\",\n             \"self.pipeline.domain_is_idle(domain)\",\n             \"len(self.pipeline.domaininfo[domain])\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b037ef6a27d01b6e29ebaaf9b53030511a64eae7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 9 | Churn Cumulative: 179 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,8 +3,10 @@ The Scrapy Scheduler\n \"\"\"\n \n from twisted.internet import defer\n+from twisted.python.failure import Failure\n \n from scrapy.utils.datatypes import PriorityQueue, PriorityStack\n+from scrapy.core.exceptions import IgnoreRequest\n from scrapy.conf import settings\n from .middleware import SchedulerMiddlewareManager\n \n@@ -57,6 +59,13 @@ class Scheduler(object):\n         self.pending_requests[domain].push((request, dfd), -request.priority)\n         return dfd\n \n+    def clear_pending_requests(self, domain):\n+        \"\"\"Remove all pending requests for the given domain\"\"\"\n+        q = self.pending_requests[domain]\n+        while q:\n+            _, dfd = q.pop()[0]\n+            dfd.errback(Failure(IgnoreRequest()))\n+\n     def next_request(self, domain):\n         \"\"\"Return the next available request to be downloaded for a domain.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ab0950b67e844e85f82855d2a7168b61572bbaaa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 361 | Contributors (this commit): 3 | Commits (past 90d): 17 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -437,7 +437,10 @@ class ExecutionEngine(object):\n             ]\n \n         for test in global_tests:\n+            try:\n                 s += \"%-47s : %s\\n\" % (test, eval(test))\n+            except Exception, e:\n+                s += \"%-47s : %s (exception)\\n\" % (test, type(e).__name__)\n         s += \"\\n\"\n         for domain in self.downloader.sites:\n             s += \"%s\\n\" % domain\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b3a624ed6b70b3dc071de8dba1aba30f41268353", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 15 | Churn Cumulative: 376 | Contributors (this commit): 3 | Commits (past 90d): 18 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,6 +26,7 @@ from scrapy.spider.middleware import SpiderMiddlewareManager\n from scrapy.utils.defer import chain_deferred, deferred_imap\n from scrapy.utils.request import request_info\n from scrapy.utils.misc import load_object\n+from scrapy.utils.defer import mustbe_deferred\n \n class ExecutionEngine(object):\n     \"\"\"\n@@ -182,7 +183,6 @@ class ExecutionEngine(object):\n         domain = spider.domain_name\n \n         if not self.running or \\\n-                domain in self.closing or \\\n                 self.domain_is_closed(domain) or \\\n                 self.downloader.sites[domain].needs_backout():\n             return\n@@ -190,17 +190,8 @@ class ExecutionEngine(object):\n         # Next pending request from scheduler\n         request, deferred = self.scheduler.next_request(domain)\n         if request:\n-            try:\n-                dwld = self.download(request, spider)\n-            except IgnoreRequest, ex:\n-                log.msg(ex.message, log.WARNING, domain=domain)\n-            except Exception, ex:\n-                log.exc(\"Bug in download code: %s\" % request, domain=domain)\n-                self._domain_idle(domain)\n-            else:\n-                chain_deferred(dwld, deferred)\n-        else:\n-            if self.domain_is_idle(domain):\n+            mustbe_deferred(self.download, request, spider).chainDeferred(deferred)\n+        elif self.domain_is_idle(domain):\n             self._domain_idle(domain)\n \n     def domain_is_idle(self, domain):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5271d1f185b4fd70f9ff6f561061e077fda3bc86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 378 | Contributors (this commit): 3 | Commits (past 90d): 19 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -151,7 +151,7 @@ class ExecutionEngine(object):\n         \"\"\"Pause the execution engine\"\"\"\n         self.paused = True\n \n-    def resume(self):\n+    def unpause(self):\n         \"\"\"Resume the execution engine\"\"\"\n         self.paused = False\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a8a44aa035d73f38350a847cbe35fe98aacf0d7f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 18 | Files Changed: 2 | Hunks: 7 | Methods Changed: 9 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 32 | Churn Cumulative: 614 | Contributors (this commit): 3 | Commits (past 90d): 32 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -50,13 +50,7 @@ class Downloader(object):\n     parallel.\n     \"\"\"\n \n-    def __init__(self, engine):\n-        \"\"\"Create the downlaoder. \n-        \n-        ``engine`` is the scrapy engine controlling this downloader\n-        \"\"\"\n-\n-        self.engine = engine\n+    def __init__(self):\n         self.sites = {}\n         self.middleware = DownloaderMiddlewareManager()\n         self.concurrent_domains = settings.getint('CONCURRENT_DOMAINS')\n@@ -123,7 +117,6 @@ class Downloader(object):\n         site = self.sites.get(domain)\n         if site and site.closing and not site.active:\n             del self.sites[domain]\n-            self.engine.closed_domain(domain) # notify engine.\n \n     def _download(self, site, request, spider):\n         site.transferring.add(request)\n\n@@ -62,7 +62,7 @@ class ExecutionEngine(object):\n         \"\"\"\n         self.scheduler = scheduler or Scheduler()\n         self.domain_scheduler = load_object(settings['DOMAIN_SCHEDULER'])()\n-        self.downloader = downloader or Downloader(self)\n+        self.downloader = downloader or Downloader()\n         self.spidermiddleware = SpiderMiddlewareManager()\n         self._scraping = {}\n         self.pipeline = ItemPipelineManager()\n@@ -361,7 +361,6 @@ class ExecutionEngine(object):\n             return\n         except:\n             log.exc(\"Exception catched on domain_idle signal dispatch\")\n-\n         if self.domain_is_idle(domain):\n             self.close_domain(domain, reason='finished')\n \n@@ -376,19 +375,23 @@ class ExecutionEngine(object):\n             log.msg(\"Closing domain (%s)\" % reason, domain=domain)\n             self.closing[domain] = reason\n             self.downloader.close_domain(domain)\n+            self.scheduler.clear_pending_requests(domain)\n+            self._finish_closing_domain_if_idle(domain)\n \n-    def closed_domain(self, domain):\n-        \"\"\"\n-        This function is called after the domain has been closed, and throws\n-        the domain_closed signal which is meant to be used for cleaning up\n-        purposes. In contrast to domain_idle, this function is called only\n-        ONCE for each domain run.\n-        \"\"\" \n+    def _finish_closing_domain_if_idle(self, domain):\n+        \"\"\"Call _finish_closing_domain if domain is idle\"\"\"\n+        if self.domain_is_idle(domain):\n+            self._finish_closing_domain(domain)\n+        else:\n+            reactor.callLater(5, self._finish_closing_domain_if_idle, domain)\n+\n+    def _finish_closing_domain(self, domain):\n+        \"\"\"This function is called after the domain has been closed\"\"\"\n         spider = spiders.fromdomain(domain) \n         self.scheduler.close_domain(domain)\n         self.pipeline.close_domain(domain)\n         del self._scraping[domain]\n-        reason = self.closing[domain]\n+        reason = self.closing.get(domain, 'finished')\n         signals.send_catch_log(signal=signals.domain_closed, sender=self.__class__, domain=domain, spider=spider, reason=reason)\n         log.msg(\"Domain closed (%s)\" % reason, domain=domain) \n         self.closing.pop(domain, None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a1cca0da506bac7196dec5f5a91f721d834df5fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,2 +0,0 @@\n-LOG_ENABLED = False\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a8fd107ad430fe776d6a1c4650087a9cdb0d5e2d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 11 | Methods Changed: 6 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 22 | Churn Cumulative: 528 | Contributors (this commit): 3 | Commits (past 90d): 35 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -84,8 +84,6 @@ DUPEFILTER_CLASS = 'scrapy.contrib.dupefilter.RequestFingerprintDupeFilter'\n \n ENABLED_SPIDERS_FILE = ''\n \n-ENGINE_DEBUG = False\n-\n EXTENSIONS = [\n     'scrapy.stats.corestats.CoreStats',\n     'scrapy.xpath.extension.ResponseLibxml2',\n\n@@ -24,7 +24,6 @@ from scrapy.item.pipeline import ItemPipelineManager\n from scrapy.spider import spiders\n from scrapy.spider.middleware import SpiderMiddlewareManager\n from scrapy.utils.defer import chain_deferred, deferred_imap\n-from scrapy.utils.request import request_info\n from scrapy.utils.misc import load_object\n from scrapy.utils.defer import mustbe_deferred\n \n@@ -48,8 +47,7 @@ class ExecutionEngine(object):\n     def __init__(self):\n         self.configured = False\n         self.keep_alive = False\n-        self.closing = {} # domains being closed\n-        self.debug_mode = settings.getbool('ENGINE_DEBUG')\n+        self.closing = {} # dict (domain -> reason) of spiders being closed\n         self.tasks = []\n         self.ports = []\n         self.running = False\n@@ -161,8 +159,7 @@ class ExecutionEngine(object):\n     def next_domain(self):\n         domain = self.domain_scheduler.next_domain()\n         if domain:\n-            spider = spiders.fromdomain(domain)\n-            self.open_domain(domain, spider)\n+            self.open_domain(domain)\n         return domain\n \n     def next_request(self, spider, now=False):\n@@ -295,22 +292,18 @@ class ExecutionEngine(object):\n                 return self._stop_if_idle()\n \n     def download(self, request, spider):\n-        if self.debug_mode:\n-            log.msg('Downloading %s' % request_info(request), log.DEBUG)\n         domain = spider.domain_name\n         referer = request.headers.get('Referer', None)\n \n         def _on_success(response):\n             \"\"\"handle the result of a page download\"\"\"\n             assert isinstance(response, (Response, Request))\n-            if self.debug_mode:\n-                log.msg(\"Requested %s\" % request_info(request), level=log.DEBUG, domain=domain)\n             if isinstance(response, Response):\n-                response.request = request # tie request to obtained response\n+                response.request = request # tie request to response received\n                 log.msg(\"Crawled %s from <%s>\" % (response, referer), level=log.DEBUG, domain=domain)\n                 return response\n             elif isinstance(response, Request):\n-                newrequest = response # proper alias\n+                newrequest = response\n                 schd = self.schedule(newrequest, spider)\n                 chain_deferred(schd, newrequest.deferred)\n                 return schd\n@@ -332,12 +325,11 @@ class ExecutionEngine(object):\n         dwld.addBoth(_on_complete)\n         return deferred\n \n-    def open_domain(self, domain, spider=None):\n+    def open_domain(self, domain):\n         log.msg(\"Domain opened\", domain=domain)\n-        spider = spider or spiders.fromdomain(domain)\n+        spider = spiders.fromdomain(domain)\n         self.next_request(spider)\n \n-        self.closing.pop(domain, None)\n         self.downloader.open_domain(domain)\n         self.pipeline.open_domain(domain)\n         self._scraping[domain] = set()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#38f184e42ad5f0e6e21780ba30581341d54caba0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 12 | Methods Changed: 8 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 25 | Churn Cumulative: 446 | Contributors (this commit): 4 | Commits (past 90d): 22 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -23,7 +23,6 @@ from scrapy.item import ScrapedItem\n from scrapy.item.pipeline import ItemPipelineManager\n from scrapy.spider import spiders\n from scrapy.spider.middleware import SpiderMiddlewareManager\n-from scrapy.utils.defer import chain_deferred, deferred_imap\n from scrapy.utils.request import request_info\n from scrapy.utils.misc import load_object\n from scrapy.utils.defer import mustbe_deferred\n@@ -222,7 +221,6 @@ class ExecutionEngine(object):\n                             signals.send_catch_log(signal=signals.item_dropped, sender=self.__class__, item=item, spider=spider, response=response, exception=pipe_result.value)\n                         else:\n                             signals.send_catch_log(signal=signals.item_passed, sender=self.__class__, item=item, spider=spider, response=response, pipe_output=pipe_result)\n-                        self.next_request(spider)\n \n                     if domain in self.closing:\n                         return\n@@ -237,9 +235,10 @@ class ExecutionEngine(object):\n                     elif output is None:\n                         pass # may be next time.\n                     else:\n-                        log.msg(\"Spider must return Request, ScrapedItem or None, got '%s' while processing %s\" % (type(output).__name__, request), log.WARNING, domain=domain)\n+                        log.msg(\"Spider must return Request, ScrapedItem or None, got '%s' while processing %s\" \\\n+                                % (type(output).__name__, request), log.WARNING, domain=domain)\n \n-                return deferred_imap(cb_spider_output, spmw_result)\n+                return task.coiterate(cb_spider_output, spmw_result)\n \n             def eb_user(_failure):\n                 if not isinstance(_failure.value, IgnoreRequest):\n@@ -251,7 +250,7 @@ class ExecutionEngine(object):\n                 log.msg('FRAMEWORK BUG processing %s: %s' % (request, _failure), log.ERROR, domain=domain)\n \n \n-            scd = self.spidermiddleware.scrape(request, response, spider)\n+            scd = mustbe_deferred(self.spidermiddleware.scrape, request, response, spider)\n             scd.addCallbacks(cb_spidermiddleware_output, eb_user)\n             scd.addErrback(eb_framework)\n \n@@ -266,7 +265,7 @@ class ExecutionEngine(object):\n             request.deferred.addErrback(lambda _:None)\n             request.deferred.errback(_failure) # TODO: merge into spider middleware.\n \n-        schd = self.schedule(request, spider)\n+        schd = mustbe_deferred(self.schedule, request, spider)\n         schd.addCallbacks(_process_response, _cleanfailure)\n         return schd\n \n@@ -305,15 +304,16 @@ class ExecutionEngine(object):\n             assert isinstance(response, (Response, Request))\n             if self.debug_mode:\n                 log.msg(\"Requested %s\" % request_info(request), level=log.DEBUG, domain=domain)\n+\n             if isinstance(response, Response):\n                 response.request = request # tie request to obtained response\n                 log.msg(\"Crawled %s from <%s>\" % (response, referer), level=log.DEBUG, domain=domain)\n                 return response\n             elif isinstance(response, Request):\n                 newrequest = response # proper alias\n-                schd = self.schedule(newrequest, spider)\n-                chain_deferred(schd, newrequest.deferred)\n-                return schd\n+                schd = mustbe_deferred(self.schedule, newrequest, spider)\n+                schd.chainDeferred(newrequest.deferred)\n+                return newrequest.deferred\n \n         def _on_error(_failure):\n             \"\"\"handle an error processing a page\"\"\"\n@@ -324,13 +324,12 @@ class ExecutionEngine(object):\n \n         def _on_complete(_):\n             self.next_request(spider)\n+            return _\n \n-        dwld = self.downloader.fetch(request, spider)\n+        dwld = mustbe_deferred(self.downloader.fetch, request, spider)\n         dwld.addCallbacks(_on_success, _on_error)\n-        deferred = defer.Deferred()\n-        chain_deferred(dwld, deferred)\n         dwld.addBoth(_on_complete)\n-        return deferred\n+        return dwld\n \n     def open_domain(self, domain, spider=None):\n         log.msg(\"Domain opened\", domain=domain)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#21ed9a24a3e1d437831010ecbd6bdb615913da79", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 4 | Churn Cumulative: 450 | Contributors (this commit): 4 | Commits (past 90d): 23 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,6 +5,7 @@ For more information see docs/topics/architecture.rst\n \n \"\"\"\n from datetime import datetime\n+from itertools import imap\n \n from twisted.internet import defer, reactor, task\n from twisted.internet.error import CannotListenError\n@@ -235,7 +236,7 @@ class ExecutionEngine(object):\n                         log.msg(\"Spider must return Request, ScrapedItem or None, got '%s' while processing %s\" \\\n                                 % (type(output).__name__, request), log.WARNING, domain=domain)\n \n-                return task.coiterate(cb_spider_output, spmw_result)\n+                return task.coiterate(imap(cb_spider_output, spmw_result))\n \n             def eb_user(_failure):\n                 if not isinstance(_failure.value, IgnoreRequest):\n@@ -253,6 +254,7 @@ class ExecutionEngine(object):\n \n             self._scraping[domain].add(response)\n             scd.addBoth(lambda _: self._scraping[domain].remove(response))\n+            scd.addBoth(lambda _: self.next_request(spider))\n             return scd\n \n         def _cleanfailure(_failure):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#680937581513bd7f22aa9f475f65f3bdb48bcca6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 451 | Contributors (this commit): 4 | Commits (past 90d): 24 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -219,6 +219,7 @@ class ExecutionEngine(object):\n                             signals.send_catch_log(signal=signals.item_dropped, sender=self.__class__, item=item, spider=spider, response=response, exception=pipe_result.value)\n                         else:\n                             signals.send_catch_log(signal=signals.item_passed, sender=self.__class__, item=item, spider=spider, response=response, pipe_output=pipe_result)\n+                        self.next_request(spider)\n \n                     if domain in self.closing:\n                         return\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e9c724e5b7c335038f758d58108b629f249d7f50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 458 | Contributors (this commit): 4 | Commits (past 90d): 25 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -254,9 +254,10 @@ class ExecutionEngine(object):\n             scd.addErrback(eb_framework)\n \n             self._scraping[domain].add(response)\n-            scd.addBoth(lambda _: self._scraping[domain].remove(response))\n-            scd.addBoth(lambda _: self.next_request(spider))\n-            return scd\n+            def _remove(_):\n+                self._scraping[domain].remove(response)\n+                self.next_request(spider)\n+            return scd.addBoth(_remove)\n \n         def _cleanfailure(_failure):\n             ex = _failure.value\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9b78f929def2f9b40353d832ecc197e0faf6da4a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 73 | Files Changed: 2 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 73 | Churn Cumulative: 101 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,37 +0,0 @@\n-from itertools import imap\n-from twisted.trial import unittest\n-\n-from scrapy.utils.defer import deferred_imap\n-\n-\n-class DeferTest(unittest.TestCase):\n-    def test_deferred_imap_1(self):\n-        \"\"\"deferred_imap storing results\"\"\"\n-        seq = [1, 2, 3]\n-        output = list(imap(None, seq))\n-\n-        dfd = deferred_imap(None, seq)\n-        dfd.addCallback(self.assertEqual, output)\n-        return dfd\n-\n-    def test_deferred_imap_2(self):\n-        \"\"\"deferred_imap not storing results\"\"\"\n-        seq = [1, 2, 3]\n-        output = list(imap(None, seq, seq))\n-\n-        dfd = deferred_imap(None, seq, seq)\n-        dfd.addCallback(self.assertEqual, output)\n-        return dfd\n-\n-    def test_deferred_imap_3(self):\n-        \"\"\"deferred_imap not storing results\"\"\"\n-        seq = [1, 2, 3]\n-        output = []\n-        function = lambda v: output.append(v)\n-\n-        dfd = deferred_imap(function, seq, store_results=False)\n-        dfd.addCallback(self.assertEqual, [])\n-        dfd.addCallback(lambda _: self.assertEqual(output, seq))\n-        return dfd\n-\n-\n\n@@ -2,7 +2,6 @@\n Helper functions for dealing with Twisted deferreds\n \"\"\"\n \n-from itertools import imap\n from twisted.internet import defer, reactor\n from twisted.python import failure\n \n@@ -38,38 +37,3 @@ def mustbe_deferred(f, *args, **kw):\n def chain_deferred(d1, d2):\n     return d1.chainDeferred(d2).addBoth(lambda _:d2)\n \n-def deferred_imap(function, *sequences, **kwargs):\n-    \"\"\"Analog to itertools.imap python function but friendly iterable evaluation\n-    taking in count cooperative multitasking.\n-\n-    It returns a Deferred object that is fired when StopIteration is reached or\n-    when any exception is raised when calling function.\n-\n-    By default the output of the evaluation is collected into a list and\n-    returned as deferred result when iterable finished. But it can be disabled\n-    (to save memory) using `store_results` parameter.\n-\n-    \"\"\"\n-\n-    next_delay = kwargs.pop('next_delay', 0)\n-    store_results = kwargs.pop('store_results', True)\n-\n-    deferred = defer.Deferred()\n-    container = []\n-\n-    iterator = imap(function, *sequences)\n-\n-    def _next():\n-        try:\n-            value = iterator.next()\n-            if store_results:\n-                container.append(value)\n-        except StopIteration:\n-            reactor.callLater(0, deferred.callback, container)\n-        except:\n-            reactor.callLater(0, deferred.errback, failure.Failure())\n-        else:\n-            reactor.callLater(next_delay, _next)\n-\n-    _next()\n-    return deferred\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#32bae8040d3ca967f1ca50860a87b535eddaa57d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 33 | Churn Cumulative: 70 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,33 @@\n+from itertools import imap\n+from twisted.trial import unittest\n+\n+from twisted.internet import reactor\n+from twisted.internet.defer import Deferred\n+from scrapy.utils.defer import mustbe_deferred\n+\n+\n+class MustbeDeferredTest(unittest.TestCase):\n+    def test_success_function(self):\n+        steps = []\n+        def _append(v):\n+            steps.append(v)\n+            return steps\n+\n+        dfd = mustbe_deferred(_append, 1)\n+        dfd.addCallback(self.assertEqual, [1,2]) # it is [1] with maybeDeferred\n+        steps.append(2) # add another value, that should be catched by assertEqual\n+        return dfd\n+\n+    def test_unfired_deferred(self):\n+        steps = []\n+        def _append(v):\n+            steps.append(v)\n+            dfd = Deferred()\n+            reactor.callLater(0, dfd.callback, steps)\n+            return dfd\n+\n+        dfd = mustbe_deferred(_append, 1)\n+        dfd.addCallback(self.assertEqual, [1,2]) # it is [1] with maybeDeferred\n+        steps.append(2) # add another value, that should be catched by assertEqual\n+        return dfd\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#834ac9fca08ef4850405f98c51e61b7b5d7c8320", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 29 | Churn Cumulative: 136 | Contributors (this commit): 3 | Commits (past 90d): 17 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -197,7 +197,7 @@ URLLENGTH_LIMIT = 2083\n USER_AGENT = '%s/%s' % (BOT_NAME, BOT_VERSION)\n \n TELNETCONSOLE_ENABLED = 1\n-#TELNETCONSOLE_PORT = 2020  # if not set uses a dynamic port\n+TELNETCONSOLE_PORT = 6023  # if None, uses a dynamic port\n \n WEBCONSOLE_ENABLED = True\n WEBCONSOLE_PORT = None\n\n@@ -1,6 +1,11 @@\n+\"\"\"\n+Telnet Console extension \n+\n+See documentation in docs/ref/telnetconsole.rst\n+\"\"\"\n+\n import pprint\n \n-from twisted.internet import reactor\n from twisted.conch import manhole, telnet\n from twisted.conch.insults import insults\n from twisted.internet import protocol\n@@ -8,6 +13,7 @@ from twisted.internet import protocol\n from scrapy.extension import extensions\n from scrapy.core.manager import scrapymanager\n from scrapy.core.engine import scrapyengine\n+from scrapy.spider import spiders\n from scrapy.stats import stats\n from scrapy.conf import settings\n \n@@ -17,12 +23,17 @@ try:\n except ImportError:\n     hpy = None\n \n-telnet_namespace = {'ee': scrapyengine,\n-                    'em': scrapymanager,\n-                    'ex': extensions.enabled,\n-                    'st': stats,\n-                    'h': hpy,\n-                    'p': pprint.pprint}  # useful shortcut for debugging\n+# if you add entries here also update topics/telnetconsole.rst\n+telnet_namespace = {\n+    'engine': scrapyengine,\n+    'manager': scrapymanager,\n+    'extensions': extensions,\n+    'stats': stats,\n+    'spiders': spiders,\n+    'settings': settings,\n+    'p': pprint.pprint,\n+    'hpy': hpy,\n+}\n \n def makeProtocol():\n     return telnet.TelnetTransport(telnet.TelnetBootstrapProtocol,\n@@ -34,8 +45,6 @@ class TelnetConsole(protocol.ServerFactory):\n     def __init__(self):\n         if not settings.getbool('TELNETCONSOLE_ENABLED'):\n             return\n-\n-        #protocol.ServerFactory.__init__(self)\n         self.protocol = makeProtocol\n         self.noisy = False\n         port = settings.getint('TELNETCONSOLE_PORT')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#93fcf6e314a9a9b2bad865ff8a6495fe49df8d51", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 11 | Churn Cumulative: 38 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n \"\"\"\n-Telnet Console extension \n+Scrapy Telnet Console extension\n \n-See documentation in docs/ref/telnetconsole.rst\n+See documentation in docs/topics/telnetconsole.rst\n \"\"\"\n \n import pprint\n\n@@ -1,8 +1,13 @@\n+\"\"\"\n+Scrapy Web Console extension\n+\n+See docs/topics/webconsole.rst\n+\"\"\"\n+\n import re\n import socket\n from datetime import datetime\n \n-from twisted.internet import reactor\n from twisted.web import server, resource\n from pydispatch import dispatcher\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#73b60788c17f3e56e2b2b3d6953bfeb891a942b9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 13 | Churn Cumulative: 471 | Contributors (this commit): 4 | Commits (past 90d): 26 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -245,19 +245,18 @@ class ExecutionEngine(object):\n                     log.msg(\"Error while spider was processing <%s> from <%s>: %s\" % (request.url, referer, _failure), log.ERROR, domain=domain)\n                     stats.incpath(\"%s/spider_exceptions/%s\" % (domain, _failure.value.__class__.__name__))\n \n-            def eb_framework(_failure):\n-                log.msg('FRAMEWORK BUG processing %s: %s' % (request, _failure), log.ERROR, domain=domain)\n-\n-\n             scd = mustbe_deferred(self.spidermiddleware.scrape, request, response, spider)\n             scd.addCallbacks(cb_spidermiddleware_output, eb_user)\n-            scd.addErrback(eb_framework)\n \n             self._scraping[domain].add(response)\n             def _remove(_):\n                 self._scraping[domain].remove(response)\n                 self.next_request(spider)\n-            return scd.addBoth(_remove)\n+                return _\n+\n+            scd.addBoth(_remove)\n+            scd.addErrback(log.err, 'FRAMEWORK BUG processing %s' % request, domain=domain)\n+            return scd\n \n         def _cleanfailure(_failure):\n             ex = _failure.value\n@@ -268,7 +267,7 @@ class ExecutionEngine(object):\n \n         schd = mustbe_deferred(self.schedule, request, spider)\n         schd.addCallbacks(_process_response, _cleanfailure)\n-        return schd\n+        return schd.addErrback(log.err)\n \n     def schedule(self, request, spider):\n         domain = spider.domain_name\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2b65f20c2616f4211a9c82b5a094931114e1e245", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 476 | Contributors (this commit): 4 | Commits (past 90d): 27 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -7,7 +7,7 @@ For more information see docs/topics/architecture.rst\n from datetime import datetime\n from itertools import imap\n \n-from twisted.internet import defer, reactor, task\n+from twisted.internet import reactor, task\n from twisted.internet.error import CannotListenError\n from twisted.python.failure import Failure\n from pydispatch import dispatcher\n@@ -384,10 +384,9 @@ class ExecutionEngine(object):\n         self.scheduler.close_domain(domain)\n         self.pipeline.close_domain(domain)\n         del self._scraping[domain]\n-        reason = self.closing.get(domain, 'finished')\n+        reason = self.closing.pop(domain, 'finished')\n         signals.send_catch_log(signal=signals.domain_closed, sender=self.__class__, domain=domain, spider=spider, reason=reason)\n         log.msg(\"Domain closed (%s)\" % reason, domain=domain) \n-        self.closing.pop(domain, None)\n         self._mainloop()\n \n     def getstatus(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d7d18d27dfc40d245b615dc0850b656ae66f10d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 12 | Methods Changed: 14 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 29 | Churn Cumulative: 505 | Contributors (this commit): 4 | Commits (past 90d): 28 | Contributors (cumulative): 4 | DMM Complexity: 0.14285714285714285\n\nDIFF:\n@@ -53,6 +53,7 @@ class ExecutionEngine(object):\n         self.running = False\n         self.paused = False\n         self.control_reactor = True\n+        self._next_request_called = set()\n \n     def configure(self, scheduler=None, downloader=None):\n         \"\"\"\n@@ -162,7 +163,7 @@ class ExecutionEngine(object):\n             self.open_domain(domain)\n         return domain\n \n-    def next_request(self, spider, now=False):\n+    def next_request(self, domain, now=False):\n         \"\"\"Scrape the next request for the domain passed.\n \n         The next request to be scraped is retrieved from the scheduler and\n@@ -170,14 +171,19 @@ class ExecutionEngine(object):\n \n         The domain is closed if there are no more pages to scrape.\n         \"\"\"\n+        if now:\n+            self._next_request_called.discard(domain)\n+        elif domain not in self._next_request_called:\n+            self._next_request_called.add(domain)\n+        else:\n+            return\n+\n         if self.paused:\n-            return reactor.callLater(5, self.next_request, spider)\n+            return reactor.callLater(5, self.next_request, domain)\n \n         # call next_request in next reactor loop by default\n         if not now:\n-            return reactor.callLater(0, self.next_request, spider, now=True)\n-\n-        domain = spider.domain_name\n+            return reactor.callLater(0, self.next_request, domain, now=True)\n \n         if not self.running or \\\n                 self.domain_is_closed(domain) or \\\n@@ -187,6 +193,7 @@ class ExecutionEngine(object):\n         # Next pending request from scheduler\n         request, deferred = self.scheduler.next_request(domain)\n         if request:\n+            spider = spiders.fromdomain(domain)\n             mustbe_deferred(self.download, request, spider).chainDeferred(deferred)\n         elif self.domain_is_idle(domain):\n             self._domain_idle(domain)\n@@ -219,7 +226,7 @@ class ExecutionEngine(object):\n                             signals.send_catch_log(signal=signals.item_dropped, sender=self.__class__, item=item, spider=spider, response=response, exception=pipe_result.value)\n                         else:\n                             signals.send_catch_log(signal=signals.item_passed, sender=self.__class__, item=item, spider=spider, response=response, pipe_output=pipe_result)\n-                        self.next_request(spider)\n+                        self.next_request(domain)\n \n                     if domain in self.closing:\n                         return\n@@ -251,7 +258,7 @@ class ExecutionEngine(object):\n             self._scraping[domain].add(response)\n             def _remove(_):\n                 self._scraping[domain].remove(response)\n-                self.next_request(spider)\n+                self.next_request(domain)\n                 return _\n \n             scd.addBoth(_remove)\n@@ -277,7 +284,7 @@ class ExecutionEngine(object):\n             self.scheduler.open_domain(domain)\n             if self.domain_is_closed(domain): # scheduler auto-open\n                 self.domain_scheduler.add_domain(domain)\n-        self.next_request(spider)\n+        self.next_request(domain)\n         return self.scheduler.enqueue_request(domain, request)\n \n     def _mainloop(self):\n@@ -318,7 +325,7 @@ class ExecutionEngine(object):\n             return Failure(IgnoreRequest(str(ex)))\n \n         def _on_complete(_):\n-            self.next_request(spider)\n+            self.next_request(domain)\n             return _\n \n         dwld = mustbe_deferred(self.downloader.fetch, request, spider)\n@@ -329,7 +336,7 @@ class ExecutionEngine(object):\n     def open_domain(self, domain):\n         log.msg(\"Domain opened\", domain=domain)\n         spider = spiders.fromdomain(domain)\n-        self.next_request(spider)\n+        self.next_request(domain)\n \n         self.downloader.open_domain(domain)\n         self.pipeline.open_domain(domain)\n@@ -350,7 +357,7 @@ class ExecutionEngine(object):\n         try:\n             dispatcher.send(signal=signals.domain_idle, sender=self.__class__, domain=domain, spider=spider)\n         except DontCloseDomain:\n-            self.next_request(spider)\n+            self.next_request(domain)\n             return\n         except:\n             log.exc(\"Exception catched on domain_idle signal dispatch\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
