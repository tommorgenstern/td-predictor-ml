{"custom_id": "scrapy#2d932c173cdf7dc530c2193870262bf57b8f2f30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 89 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,6 +26,7 @@ def inside_a_project():\n class ProjectUtilsTest(unittest.TestCase):\n     def test_data_path_outside_project(self):\n         self.assertEquals('.scrapy/somepath', data_path('somepath'))\n+        self.assertEquals('/absolute/path', data_path('/absolute/path'))\n \n     def test_data_path_inside_project(self):\n         with inside_a_project() as proj_path:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c3411373e8a8ee2786588bdad7be469c69a25e2a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 310 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,7 +13,7 @@ try:\n     from twisted.web.client import BrowserLikePolicyForHTTPS\n     from twisted.web.iweb import IPolicyForHTTPS\n \n-    from scrapy.core.downloader.tls import ScrapyClientTLSOptions\n+    from scrapy.core.downloader.tls import ScrapyClientTLSOptions, DEFAULT_CIPHERS\n \n \n     @implementer(IPolicyForHTTPS)\n@@ -46,7 +46,9 @@ try:\n             #   not calling super(..., self).__init__\n             return CertificateOptions(verify=False,\n                         method=getattr(self, 'method',\n-                                       getattr(self, '_ssl_method', None)))\n+                                       getattr(self, '_ssl_method', None)),\n+                        fixBrokenPeers=True,\n+                        acceptableCiphers=DEFAULT_CIPHERS)\n \n         # kept for old-style HTTP/1.0 downloader context twisted calls,\n         # e.g. connectSSL()\n\n@@ -28,6 +28,7 @@ try:\n         SSL_CB_HANDSHAKE_START = 0x10\n         SSL_CB_HANDSHAKE_DONE = 0x20\n \n+    from twisted.internet.ssl import AcceptableCiphers\n     from twisted.internet._sslverify import (ClientTLSOptions,\n                                              _maybeSetHostNameIndication,\n                                              verifyHostname,\n@@ -60,6 +61,8 @@ try:\n                         'from host \"{}\" (exception: {})'.format(\n                             self._hostnameASCII, repr(e)))\n \n+    DEFAULT_CIPHERS = AcceptableCiphers.fromOpenSSLCipherString('DEFAULT')\n+\n except ImportError:\n     # ImportError should not matter for older Twisted versions\n     # as the above is not used in the fallback ScrapyClientContextFactory\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#45e95b79ce17eef0b3b4d324daa8b0d220fdb2fc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 24 | Churn Cumulative: 293 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: 0.8181818181818182\n\nDIFF:\n@@ -21,6 +21,8 @@ else:\n \n from twisted.internet import defer, reactor, ssl\n \n+from utils.misc import arg_to_iter\n+\n logger = logging.getLogger(__name__)\n \n \n@@ -48,6 +50,10 @@ class MailSender(object):\n             msg = MIMEMultipart()\n         else:\n             msg = MIMENonMultipart(*mimetype.split('/', 1))\n+\n+        to = list(arg_to_iter(to))\n+        cc = list(arg_to_iter(cc))\n+\n         msg['From'] = self.mailfrom\n         msg['To'] = COMMASPACE.join(to)\n         msg['Date'] = formatdate(localtime=True)\n\n@@ -10,7 +10,8 @@ class MailSenderTest(unittest.TestCase):\n \n     def test_send(self):\n         mailsender = MailSender(debug=True)\n-        mailsender.send(to=['test@scrapy.org'], subject='subject', body='body', _callback=self._catch_mail_sent)\n+        mailsender.send(to=['test@scrapy.org'], subject='subject', body='body',\n+                        _callback=self._catch_mail_sent)\n \n         assert self.catched_msg\n \n@@ -24,9 +25,16 @@ class MailSenderTest(unittest.TestCase):\n         self.assertEqual(msg.get_payload(), 'body')\n         self.assertEqual(msg.get('Content-Type'), 'text/plain')\n \n+    def test_send_single_values_to_and_cc(self):\n+        mailsender = MailSender(debug=True)\n+        mailsender.send(to='test@scrapy.org', subject='subject', body='body',\n+                        cc='test@scrapy.org', _callback=self._catch_mail_sent)\n+\n     def test_send_html(self):\n         mailsender = MailSender(debug=True)\n-        mailsender.send(to=['test@scrapy.org'], subject='subject', body='<p>body</p>', mimetype='text/html', _callback=self._catch_mail_sent)\n+        mailsender.send(to=['test@scrapy.org'], subject='subject',\n+                        body='<p>body</p>', mimetype='text/html',\n+                        _callback=self._catch_mail_sent)\n \n         msg = self.catched_msg['msg']\n         self.assertEqual(msg.get_payload(), '<p>body</p>')\n@@ -90,7 +98,8 @@ class MailSenderTest(unittest.TestCase):\n \n         mailsender = MailSender(debug=True)\n         mailsender.send(to=['test@scrapy.org'], subject=subject, body=body,\n-                        attachs=attachs, charset='utf-8', _callback=self._catch_mail_sent)\n+                        attachs=attachs, charset='utf-8',\n+                        _callback=self._catch_mail_sent)\n \n         assert self.catched_msg\n         self.assertEqual(self.catched_msg['subject'], subject)\n@@ -99,7 +108,8 @@ class MailSenderTest(unittest.TestCase):\n         msg = self.catched_msg['msg']\n         self.assertEqual(msg['subject'], subject)\n         self.assertEqual(msg.get_charset(), Charset('utf-8'))\n-        self.assertEqual(msg.get('Content-Type'), 'multipart/mixed; charset=\"utf-8\"')\n+        self.assertEqual(msg.get('Content-Type'),\n+                         'multipart/mixed; charset=\"utf-8\"')\n \n         payload = msg.get_payload()\n         assert isinstance(payload, list)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3fb6e52457a7f24d580ed32c07aa12d9547d2970", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 223 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -21,7 +21,7 @@ else:\n \n from twisted.internet import defer, reactor, ssl\n \n-from utils.misc import arg_to_iter\n+from .utils.misc import arg_to_iter\n \n logger = logging.getLogger(__name__)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d5bd44a5b9fa28a44c44396d1f87c3697a5e2d42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 218 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -17,6 +17,7 @@ TEMPLATES_TO_RENDER = (\n     ('${project_name}', 'settings.py.tmpl'),\n     ('${project_name}', 'items.py.tmpl'),\n     ('${project_name}', 'pipelines.py.tmpl'),\n+    ('${project_name}', 'middleware.py.tmpl'),\n )\n \n IGNORE = ignore_patterns('*.pyc', '.svn')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#34f2014c55ed25615066f7017ddd36df871095bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 220 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -17,7 +17,7 @@ TEMPLATES_TO_RENDER = (\n     ('${project_name}', 'settings.py.tmpl'),\n     ('${project_name}', 'items.py.tmpl'),\n     ('${project_name}', 'pipelines.py.tmpl'),\n-    ('${project_name}', 'middleware.py.tmpl'),\n+    ('${project_name}', 'middlewares.py.tmpl'),\n )\n \n IGNORE = ignore_patterns('*.pyc', '.svn')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#db408528928b2d15043593032913fe40d6eb6783", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 170 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -1,9 +1,10 @@\n import logging\n from six.moves.urllib.parse import urljoin\n \n+from w3lib.url import safe_url_string\n+\n from scrapy.http import HtmlResponse\n from scrapy.utils.response import get_meta_refresh\n-from scrapy.utils.python import to_native_str\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n \n logger = logging.getLogger(__name__)\n@@ -65,8 +66,7 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n         if 'Location' not in response.headers or response.status not in allowed_status:\n             return response\n \n-        # HTTP header is ascii or latin1, redirected url will be percent-encoded utf-8\n-        location = to_native_str(response.headers['location'].decode('latin1'))\n+        location = safe_url_string(response.headers['location'])\n \n         redirected_url = urljoin(request.url, location)\n \n\n@@ -157,15 +157,15 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         latin1_location = u'/ação'.encode('latin1')  # HTTP historically supports latin1\n         resp = Response('http://scrapytest.org/first', headers={'Location': latin1_location}, status=302)\n         req_result = self.mw.process_response(req, resp, self.spider)\n-        perc_encoded_utf8_url = 'http://scrapytest.org/a%C3%A7%C3%A3o'\n+        perc_encoded_utf8_url = 'http://scrapytest.org/a%E7%E3o'\n         self.assertEquals(perc_encoded_utf8_url, req_result.url)\n \n-    def test_location_with_wrong_encoding(self):\n+    def test_utf8_location(self):\n         req = Request('http://scrapytest.org/first')\n-        utf8_location = u'/ação'  # header with wrong encoding (utf-8)\n+        utf8_location = u'/ação'.encode('utf-8')  # header using UTF-8 encoding\n         resp = Response('http://scrapytest.org/first', headers={'Location': utf8_location}, status=302)\n         req_result = self.mw.process_response(req, resp, self.spider)\n-        perc_encoded_utf8_url = 'http://scrapytest.org/a%C3%83%C2%A7%C3%83%C2%A3o'\n+        perc_encoded_utf8_url = 'http://scrapytest.org/a%C3%A7%C3%A3o'\n         self.assertEquals(perc_encoded_utf8_url, req_result.url)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f357ccd0d7ab164d107ba22f6e45e9f172aae712", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 979 | Contributors (this commit): 13 | Commits (past 90d): 1 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -119,7 +119,7 @@ class Scraper(object):\n             self._scrape(response, request, spider).chainDeferred(deferred)\n \n     def _scrape(self, response, request, spider):\n-        \"\"\"Handle the downloaded response or failure trough the spider\n+        \"\"\"Handle the downloaded response or failure through the spider\n         callback/errback\"\"\"\n         assert isinstance(response, (Response, Failure))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7727d87f646d03a9f4d0d30d3dcc1161dd1cb1b5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 9/8 | Churn Δ: 4 | Churn Cumulative: 2208 | Contributors (this commit): 18 | Commits (past 90d): 2 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -48,6 +48,7 @@ class Slot(object):\n         if self.closing and not self.inprogress:\n             if self.nextcall:\n                 self.nextcall.cancel()\n+                if self.heartbeat.running:\n                     self.heartbeat.stop()\n             self.closing.callback(None)\n \n\n@@ -15,6 +15,7 @@ class LogStats(object):\n         self.stats = stats\n         self.interval = interval\n         self.multiplier = 60.0 / self.interval\n+        self.task = None\n \n     @classmethod\n     def from_crawler(cls, crawler):\n@@ -47,5 +48,5 @@ class LogStats(object):\n         logger.info(msg, log_args, extra={'spider': spider})\n \n     def spider_closed(self, spider, reason):\n-        if self.task.running:\n+        if self.task and self.task.running:\n             self.task.stop()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#61efacdd1f6fb55cf1f694e56502a3d8a1d5a325", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 23 | Churn Cumulative: 361 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,11 @@\n+\"\"\"\n+Some pipelines used for testing and benchmarking\n+\"\"\"\n+\n+class ZeroDivisionErrorPipeline(object):\n+\n+    def open_spider(self, spider):\n+        a = 1/0\n+\n+    def process_item(self, item, spider):\n+        return item\n\n@@ -250,6 +250,18 @@ with multiples lines\n         yield self.assertFailure(crawler.crawl(), TestError)\n         self.assertFalse(crawler.crawling)\n \n+    @defer.inlineCallbacks\n+    def test_open_spider_error_on_faulty_pipeline(self):\n+        settings = {\n+            \"ITEM_PIPELINES\": {\n+                \"tests.pipelines.ZeroDivisionErrorPipeline\": 300,\n+            }\n+        }\n+        crawler = CrawlerRunner(settings).create_crawler(SimpleSpider)\n+        yield self.assertFailure(\n+            self.runner.crawl(crawler, \"http://localhost:8998/status?n=200\"),\n+            ZeroDivisionError)\n+\n     @defer.inlineCallbacks\n     def test_crawlerrunner_accepts_crawler(self):\n         crawler = self.runner.create_crawler(SimpleSpider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#27456996a904ba25f780d2e3824059e836b6cb90", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 351 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -261,6 +261,7 @@ with multiples lines\n         yield self.assertFailure(\n             self.runner.crawl(crawler, \"http://localhost:8998/status?n=200\"),\n             ZeroDivisionError)\n+        self.assertFalse(crawler.crawling)\n \n     @defer.inlineCallbacks\n     def test_crawlerrunner_accepts_crawler(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#af2280e695f9430fdb0ba452594f3f9c4cb9592d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 13 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,5 @@\n \"\"\"\n-Some pipelines used for testing and benchmarking\n+Some pipelines used for testing\n \"\"\"\n \n class ZeroDivisionErrorPipeline(object):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#28155dfcccc0cd7883ffde714570e8c13b0920a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 417 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,6 +13,7 @@ from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.log import failure_to_exc_info\n+from scrapy.utils.python import to_native_str\n \n logger = logging.getLogger(__name__)\n \n@@ -94,7 +95,9 @@ class RobotsTxtMiddleware(object):\n                 # Running rp.parse() will set rp state from\n                 # 'disallow all' to 'allow any'.\n                 pass\n-        rp.parse(body.splitlines())\n+        # stdlib's robotparser expects native 'str' ;\n+        # with unicode input, non-ASCII encoded bytes decoding fails in Python2\n+        rp.parse(to_native_str(body).splitlines())\n \n         rp_dfd = self._parsers[netloc]\n         self._parsers[netloc] = rp\n\n@@ -1,3 +1,4 @@\n+# -*- coding: utf-8 -*-\n from __future__ import absolute_import\n import re\n from twisted.internet import reactor, error\n@@ -30,11 +31,15 @@ class RobotsTxtMiddlewareTest(unittest.TestCase):\n     def _get_successful_crawler(self):\n         crawler = self.crawler\n         crawler.settings.set('ROBOTSTXT_OBEY', True)\n-        ROBOTS = re.sub(b'^\\s+(?m)', b'', b'''\n+        ROBOTS = re.sub(b'^\\s+(?m)', b'', u'''\n         User-Agent: *\n         Disallow: /admin/\n         Disallow: /static/\n-        ''')\n+\n+        # taken from https://en.wikipedia.org/robots.txt\n+        Disallow: /wiki/K%C3%A4ytt%C3%A4j%C3%A4:\n+        Disallow: /wiki/Käyttäjä:\n+        '''.encode('utf-8'))\n         response = TextResponse('http://site.local/robots.txt', body=ROBOTS)\n         def return_response(request, spider):\n             deferred = Deferred()\n@@ -48,7 +53,9 @@ class RobotsTxtMiddlewareTest(unittest.TestCase):\n         return DeferredList([\n             self.assertNotIgnored(Request('http://site.local/allowed'), middleware),\n             self.assertIgnored(Request('http://site.local/admin/main'), middleware),\n-            self.assertIgnored(Request('http://site.local/static/'), middleware)\n+            self.assertIgnored(Request('http://site.local/static/'), middleware),\n+            self.assertIgnored(Request('http://site.local/wiki/K%C3%A4ytt%C3%A4j%C3%A4:'), middleware),\n+            self.assertIgnored(Request(u'http://site.local/wiki/Käyttäjä:'), middleware)\n         ], fireOnOneErrback=True)\n \n     def test_robotstxt_ready_parser(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e8205f67339bdd3cdd6450e104b8969c9b98692d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 54 | Contributors (this commit): 8 | Commits (past 90d): 12 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -3,6 +3,7 @@ HTMLParser-based link extractor\n \"\"\"\n \n import warnings\n+import six\n from six.moves.html_parser import HTMLParser\n from six.moves.urllib.parse import urljoin\n \n@@ -39,7 +40,7 @@ class HtmlParserLinkExtractor(HTMLParser):\n         ret = []\n         base_url = urljoin(response_url, self.base_url) if self.base_url else response_url\n         for link in links:\n-            if isinstance(link.url, unicode):\n+            if isinstance(link.url, six.text_type):\n                 link.url = link.url.encode(response_encoding)\n             try:\n                 link.url = urljoin(base_url, link.url)\n\n@@ -1,6 +1,7 @@\n \"\"\"\n SGMLParser-based Link extractors\n \"\"\"\n+import six\n from six.moves.urllib.parse import urljoin\n import warnings\n from sgmllib import SGMLParser\n@@ -40,7 +41,7 @@ class BaseSgmlLinkExtractor(SGMLParser):\n         if base_url is None:\n             base_url = urljoin(response_url, self.base_url) if self.base_url else response_url\n         for link in self.links:\n-            if isinstance(link.url, unicode):\n+            if isinstance(link.url, six.text_type):\n                 link.url = link.url.encode(response_encoding)\n             try:\n                 link.url = urljoin(base_url, link.url)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6cd35c77da6601f218b5042f4b9a9919e642e15c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 423 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -41,7 +41,8 @@ class RobotsTxtMiddleware(object):\n         return d\n \n     def process_request_2(self, rp, request, spider):\n-        if rp is not None and not rp.can_fetch(self._useragent, request.url):\n+        if rp is not None and not rp.can_fetch(\n+                 to_native_str(self._useragent), request.url):\n             logger.debug(\"Forbidden by robots.txt: %(request)s\",\n                          {'request': request}, extra={'spider': spider})\n             raise IgnoreRequest()\n\n@@ -39,6 +39,9 @@ class RobotsTxtMiddlewareTest(unittest.TestCase):\n         # taken from https://en.wikipedia.org/robots.txt\n         Disallow: /wiki/K%C3%A4ytt%C3%A4j%C3%A4:\n         Disallow: /wiki/Käyttäjä:\n+\n+        User-Agent: UnicödeBöt\n+        Disallow: /some/randome/page.html\n         '''.encode('utf-8'))\n         response = TextResponse('http://site.local/robots.txt', body=ROBOTS)\n         def return_response(request, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#01142e2ae5e3b82d5f8701858931ba0354808720", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 19 | Churn Cumulative: 113 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -26,12 +26,25 @@ class Command(ScrapyCommand):\n \n     def run(self, args, opts):\n         if opts.verbose:\n+            import cssselect\n+            import parsel\n             import lxml.etree\n+            import w3lib\n+\n             lxml_version = \".\".join(map(str, lxml.etree.LXML_VERSION))\n             libxml2_version = \".\".join(map(str, lxml.etree.LIBXML_VERSION))\n+\n+            try:\n+                w3lib_version = w3lib.__version__\n+            except AttributeError:\n+                w3lib_version = \"<1.14.3\"\n+\n             print(\"Scrapy    : %s\" % scrapy.__version__)\n             print(\"lxml      : %s\" % lxml_version)\n             print(\"libxml2   : %s\" % libxml2_version)\n+            print(\"cssselect : %s\" % cssselect.__version__)\n+            print(\"parsel    : %s\" % parsel.__version__)\n+            print(\"w3lib     : %s\" % w3lib_version)\n             print(\"Twisted   : %s\" % twisted.version.short())\n             print(\"Python    : %s\" % sys.version.replace(\"\\n\", \"- \"))\n             print(\"pyOpenSSL : %s\" % self._get_openssl_version())\n\n@@ -25,5 +25,7 @@ class VersionTest(ProcessTest, unittest.TestCase):\n         _, out, _ = yield self.execute(['-v'])\n         headers = [l.partition(\":\")[0].strip()\n                    for l in out.strip().decode(encoding).splitlines()]\n-        self.assertEqual(headers, ['Scrapy', 'lxml', 'libxml2', 'Twisted',\n-                                   'Python', 'pyOpenSSL', 'Platform'])\n+        self.assertEqual(headers, ['Scrapy', 'lxml', 'libxml2',\n+                                   'cssselect', 'parsel', 'w3lib',\n+                                   'Twisted', 'Python', 'pyOpenSSL',\n+                                   'Platform'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#35b655d2f84d652440c393166f6e19d7384b4f1c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 17 | Churn Cumulative: 1166 | Contributors (this commit): 19 | Commits (past 90d): 5 | Contributors (cumulative): 30 | DMM Complexity: 1.0\n\nDIFF:\n@@ -27,6 +27,8 @@ class Command(ScrapyCommand):\n             help=\"use this spider\")\n         parser.add_option(\"--headers\", dest=\"headers\", action=\"store_true\", \\\n             help=\"print response HTTP headers instead of body\")\n+        parser.add_option(\"--no-status-aware\", dest=\"no_status_aware\", action=\"store_true\", \\\n+            default=False, help=\"do not handle status codes like redirects and print response as-is\")\n \n     def _print_headers(self, headers, prefix):\n         for key, values in headers.items():\n@@ -50,6 +52,7 @@ class Command(ScrapyCommand):\n             raise UsageError()\n         cb = lambda x: self._print_response(x, opts)\n         request = Request(args[0], callback=cb, dont_filter=True)\n+        if opts.no_status_aware:\n             request.meta['handle_httpstatus_all'] = True\n \n         spidercls = DefaultSpider\n\n@@ -36,6 +36,8 @@ class Command(ScrapyCommand):\n             help=\"evaluate the code in the shell, print the result and exit\")\n         parser.add_option(\"--spider\", dest=\"spider\",\n             help=\"use this spider\")\n+        parser.add_option(\"--no-status-aware\", dest=\"no_status_aware\", action=\"store_true\", \\\n+            default=False, help=\"do not transparently handle status codes like redirects\")\n \n     def update_vars(self, vars):\n         \"\"\"You can use this function to update the Scrapy objects that will be\n@@ -68,7 +70,7 @@ class Command(ScrapyCommand):\n         self._start_crawler_thread()\n \n         shell = Shell(crawler, update_vars=self.update_vars, code=opts.code)\n-        shell.start(url=url)\n+        shell.start(url=url, handle_statuses=opts.no_status_aware)\n \n     def _start_crawler_thread(self):\n         t = Thread(target=self.crawler_process.start,\n\n@@ -40,11 +40,11 @@ class Shell(object):\n         self.code = code\n         self.vars = {}\n \n-    def start(self, url=None, request=None, response=None, spider=None):\n+    def start(self, url=None, request=None, response=None, spider=None, handle_statuses=True):\n         # disable accidental Ctrl-C key press from shutting down the engine\n         signal.signal(signal.SIGINT, signal.SIG_IGN)\n         if url:\n-            self.fetch(url, spider)\n+            self.fetch(url, spider, handle_statuses=handle_statuses)\n         elif request:\n             self.fetch(request, spider)\n         elif response:\n@@ -98,13 +98,13 @@ class Shell(object):\n         self.spider = spider\n         return spider\n \n-    def fetch(self, request_or_url, spider=None):\n+    def fetch(self, request_or_url, spider=None, handle_statuses=False, **kwargs):\n         if isinstance(request_or_url, Request):\n             request = request_or_url\n-            url = request.url\n         else:\n             url = any_to_uri(request_or_url)\n-            request = Request(url, dont_filter=True)\n+            request = Request(url, dont_filter=True, **kwargs)\n+            if handle_statuses:\n                 request.meta['handle_httpstatus_all'] = True\n         response = None\n         try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9aefc0a886ca571a49f087e5349e2557fd78d943", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 20 | Churn Cumulative: 80 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,12 +20,20 @@ class SiteTest(object):\n         return urljoin(self.baseurl, path)\n \n \n+class NoMetaRefreshRedirect(util.Redirect):\n+    def render(self, request):\n+        content = util.Redirect.render(self, request)\n+        return content.replace(b'http-equiv=\\\"refresh\\\"',\n+            b'http-no-equiv=\\\"do-not-refresh-me\\\"')\n+\n+\n def test_site():\n     r = resource.Resource()\n     r.putChild(b\"text\", static.Data(b\"Works\", \"text/plain\"))\n     r.putChild(b\"html\", static.Data(b\"<body><p class='one'>Works</p><p class='two'>World</p></body>\", \"text/html\"))\n     r.putChild(b\"enc-gb18030\", static.Data(b\"<p>gb18030 encoding</p>\", \"text/html; charset=gb18030\"))\n     r.putChild(b\"redirect\", util.Redirect(b\"/redirected\"))\n+    r.putChild(b\"redirect-no-meta-refresh\", NoMetaRefreshRedirect(b\"/redirected\"))\n     r.putChild(b\"redirected\", static.Data(b\"Redirected here\", \"text/plain\"))\n     return server.Site(r)\n \n\n@@ -14,6 +14,18 @@ class FetchTest(ProcessTest, SiteTest, unittest.TestCase):\n         _, out, _ = yield self.execute([self.url('/text')])\n         self.assertEqual(out.strip(), b'Works')\n \n+    @defer.inlineCallbacks\n+    def test_redirect_default(self):\n+        _, out, _ = yield self.execute([self.url('/redirect')])\n+        self.assertEqual(out.strip(), b'Redirected here')\n+\n+    @defer.inlineCallbacks\n+    def test_redirect_disabled(self):\n+        _, out, err = yield self.execute(['--no-status-aware', self.url('/redirect-no-meta-refresh')])\n+        err = err.strip()\n+        self.assertIn(b'downloader/response_status_count/302', err, err)\n+        self.assertNotIn(b'downloader/response_status_count/200', err, err)\n+\n     @defer.inlineCallbacks\n     def test_headers(self):\n         _, out, _ = yield self.execute([self.url('/text'), '--headers'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#03cf5f1bd2019127933ea4a6358a7d47743efcf6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 480 | Contributors (this commit): 31 | Commits (past 90d): 1 | Contributors (cumulative): 31 | DMM Complexity: None\n\nDIFF:\n@@ -102,7 +102,6 @@ DOWNLOADER_MIDDLEWARES_BASE = {\n     'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': 600,\n     'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': 700,\n     'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 750,\n-    'scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware': 830,\n     'scrapy.downloadermiddlewares.stats.DownloaderStats': 850,\n     'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware': 900,\n     # Downloader side\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e6f174b01535810fffeae2d020bc66a6a25ba4e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 8 | Churn Cumulative: 8 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,14 @@\n+import warnings\n+\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.http import decode_chunked_transfer\n \n \n+warnings.warn(\"Module `scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware` \"\n+              \"is deprecated, chunked transfers are supported by default.\",\n+              ScrapyDeprecationWarning, stacklevel=2)\n+\n+\n class ChunkedTransferMiddleware(object):\n     \"\"\"This middleware adds support for chunked transfer encoding, as\n     documented in: http://en.wikipedia.org/wiki/Chunked_transfer_encoding\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8cffb4bbefcebd10ec12ee2678fd490edf149576", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 12 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -4,8 +4,8 @@ from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.http import decode_chunked_transfer\n \n \n-warnings.warn(\"Module `scrapy.downloadermiddlewares.chunked.ChunkedTransferMiddleware` \"\n-              \"is deprecated, chunked transfers are supported by default.\",\n+warnings.warn(\"Module `scrapy.downloadermiddlewares.chunked` is deprecated, \"\n+              \"chunked transfers are supported by default.\",\n               ScrapyDeprecationWarning, stacklevel=2)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#27606aad1166afe3eebbee8dc365b1a122639e3c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 154 | Contributors (this commit): 5 | Commits (past 90d): 2 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,6 +15,9 @@ def _embed_ipython_shell(namespace={}, banner=''):\n         config = load_default_config()\n         # Always use .instace() to ensure _instance propagation to all parents\n         # this is needed for <TAB> completion works well for new imports\n+        # and clear the instance to always have the fresh env\n+        # on repeated breaks like with inspect_response()\n+        InteractiveShellEmbed.clear_instance()\n         shell = InteractiveShellEmbed.instance(\n             banner1=banner, user_ns=namespace, config=config)\n         shell()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5ff64ad015aff872ffae78193b67525dd53ae80d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 296 | Contributors (this commit): 12 | Commits (past 90d): 4 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -32,7 +32,7 @@ class SitemapSpider(Spider):\n \n     def _parse_sitemap(self, response):\n         if response.url.endswith('/robots.txt'):\n-            for url in sitemap_urls_from_robots(response.text):\n+            for url in sitemap_urls_from_robots(response.text, base_url=response.url):\n                 yield Request(url, callback=self._parse_sitemap)\n         else:\n             body = self._get_sitemap_body(response)\n\n@@ -4,7 +4,9 @@ Module for processing Sitemaps.\n Note: The main purpose of this module is to provide support for the\n SitemapSpider, its API is subject to change without notice.\n \"\"\"\n+\n import lxml.etree\n+from six.moves.urllib.parse import urljoin\n \n \n class Sitemap(object):\n@@ -34,10 +36,11 @@ class Sitemap(object):\n                 yield d\n \n \n-def sitemap_urls_from_robots(robots_text):\n+def sitemap_urls_from_robots(robots_text, base_url=None):\n     \"\"\"Return an iterator over all sitemap urls contained in the given\n     robots.txt file\n     \"\"\"\n     for line in robots_text.splitlines():\n         if line.lstrip().lower().startswith('sitemap:'):\n-            yield line.split(':', 1)[1].strip()\n+            url = line.split(':', 1)[1].strip()\n+            yield urljoin(base_url, url)\n\n@@ -332,13 +332,17 @@ class SitemapSpiderTest(SpiderTest):\n         robots = b\"\"\"# Sitemap files\n Sitemap: http://example.com/sitemap.xml\n Sitemap: http://example.com/sitemap-product-index.xml\n+Sitemap: HTTP://example.com/sitemap-uppercase.xml\n+Sitemap: /sitemap-relative-url.xml\n \"\"\"\n \n         r = TextResponse(url=\"http://www.example.com/robots.txt\", body=robots)\n         spider = self.spider_class(\"example.com\")\n         self.assertEqual([req.url for req in spider._parse_sitemap(r)],\n                          ['http://example.com/sitemap.xml',\n-                          'http://example.com/sitemap-product-index.xml'])\n+                          'http://example.com/sitemap-product-index.xml',\n+                          'http://example.com/sitemap-uppercase.xml',\n+                          'http://www.example.com/sitemap-relative-url.xml'])\n \n \n class BaseSpiderDeprecationTest(unittest.TestCase):\n\n@@ -119,13 +119,18 @@ Disallow: /s*/*tags\n # Sitemap files\n Sitemap: http://example.com/sitemap.xml\n Sitemap: http://example.com/sitemap-product-index.xml\n+Sitemap: HTTP://example.com/sitemap-uppercase.xml\n+Sitemap: /sitemap-relative-url.xml\n \n # Forums\n Disallow: /forum/search/\n Disallow: /forum/active/\n \"\"\"\n-        self.assertEqual(list(sitemap_urls_from_robots(robots)),\n-             ['http://example.com/sitemap.xml', 'http://example.com/sitemap-product-index.xml'])\n+        self.assertEqual(list(sitemap_urls_from_robots(robots, base_url='http://example.com')),\n+                         ['http://example.com/sitemap.xml',\n+                          'http://example.com/sitemap-product-index.xml',\n+                          'http://example.com/sitemap-uppercase.xml',\n+                          'http://example.com/sitemap-relative-url.xml'])\n \n     def test_sitemap_blanklines(self):\n         \"\"\"Assert we can deal with starting blank lines before <xml> tag\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d9f43e21ba510ee3c95c89c3d551bc30a8d0f2c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 98 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -66,7 +66,7 @@ class SpiderLoaderTest(unittest.TestCase):\n         self.spider_loader = SpiderLoader.from_settings(settings)\n         assert len(self.spider_loader._spiders) == 1\n \n-    def test_load_spider_module(self):\n+    def test_load_spider_module_multiple(self):\n         prefix = 'tests.test_spiderloader.test_spiders.'\n         module = ','.join(prefix + s for s in ('spider1', 'spider2'))\n         settings = Settings({'SPIDER_MODULES': module})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#923b974f0a10c3eb007ee4d183aa63a41d6a6db2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 15 | Churn Cumulative: 113 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,6 +9,7 @@ from twisted.trial import unittest\n # ugly hack to avoid cyclic imports of scrapy.spiders when running this test\n # alone\n import scrapy\n+import tempfile\n from scrapy.interfaces import ISpiderLoader\n from scrapy.spiderloader import SpiderLoader\n from scrapy.settings import Settings\n@@ -22,8 +23,7 @@ class SpiderLoaderTest(unittest.TestCase):\n \n     def setUp(self):\n         orig_spiders_dir = os.path.join(module_dir, 'test_spiders')\n-        self.tmpdir = self.mktemp()\n-        os.mkdir(self.tmpdir)\n+        self.tmpdir = tempfile.mkdtemp()\n         self.spiders_dir = os.path.join(self.tmpdir, 'test_spiders_xxx')\n         shutil.copytree(orig_spiders_dir, self.spiders_dir)\n         sys.path.append(self.tmpdir)\n@@ -40,7 +40,7 @@ class SpiderLoaderTest(unittest.TestCase):\n \n     def test_list(self):\n         self.assertEqual(set(self.spider_loader.list()),\n-            set(['spider1', 'spider2', 'spider3']))\n+            set(['spider1', 'spider2', 'spider3', 'spider4']))\n \n     def test_load(self):\n         spider1 = self.spider_loader.load(\"spider1\")\n\n\n@@ -0,0 +1,9 @@\n+from scrapy.spiders import Spider\n+\n+class Spider4(Spider):\n+    name = \"spider4\"\n+    allowed_domains = ['spider4.com']\n+\n+    @classmethod\n+    def handles_request(cls, request):\n+        return request.url == 'http://spider4.com/onlythis'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e1ea0c433a96c2949b0a7c26cde65aea13fb79b4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 115 | Lines Deleted: 47 | Files Changed: 6 | Hunks: 45 | Methods Changed: 0 | Complexity Δ (Sum/Max): -156/0 | Churn Δ: 162 | Churn Cumulative: 7189 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -15,9 +15,9 @@ else:\n     client = endpoints = _Mock()\n \n \n-Agent = client.Agent\n-ProxyAgent = client.ProxyAgent\n-ResponseDone = client.ResponseDone\n-ResponseFailed = client.ResponseFailed\n-HTTPConnectionPool = client.HTTPConnectionPool\n-TCP4ClientEndpoint = endpoints.TCP4ClientEndpoint\n+Agent = client.Agent  # since < 11.1\n+ProxyAgent = client.ProxyAgent  # since 11.1\n+ResponseDone = client.ResponseDone  # since 11.1\n+ResponseFailed = client.ResponseFailed  # since 11.1\n+HTTPConnectionPool = client.HTTPConnectionPool  # since 12.1\n+TCP4ClientEndpoint = endpoints.TCP4ClientEndpoint  # since 10.1\n\n@@ -39,12 +39,25 @@ from twisted.internet.defer import Deferred, succeed, fail, maybeDeferred\n from twisted.internet.defer import CancelledError\n from twisted.internet.protocol import Protocol\n from twisted.protocols.basic import LineReceiver\n+from twisted.web.iweb import UNKNOWN_LENGTH\n from twisted.web.http_headers import Headers\n from twisted.web.http import NO_CONTENT, NOT_MODIFIED\n from twisted.web.http import _DataLoss, PotentialDataLoss\n from twisted.web.http import _IdentityTransferDecoder, _ChunkedTransferDecoder\n \n-from .iweb import IResponse, UNKNOWN_LENGTH\n+from twisted.web._newclient import (\n+    BadHeaders, ExcessWrite, ParseError, BadResponseVersion, _WrapperException,\n+    RequestGenerationFailed, RequestTransmissionFailed,\n+    WrongBodyLength, ResponseDone, RequestNotSent,\n+    LengthEnforcingConsumer, makeStatefulDispatcher, ChunkedEncoder,\n+    TransportProxyProducer,\n+)\n+# newer than 10.0.0\n+#from twisted.web._newclient import (\n+#    ConnectionAborted, ResponseFailed, ResponseNeverReceived, HTTPParser,\n+#    HTTPClientParser, Request, Response, HTTP11ClientProtocol,\n+#)\n+from .iweb import IResponse\n \n # States HTTPParser can be in\n STATUS = 'STATUS'\n@@ -52,7 +65,7 @@ HEADER = 'HEADER'\n BODY = 'BODY'\n DONE = 'DONE'\n \n-\n+''' {{{\n class BadHeaders(Exception):\n     \"\"\"\n     Headers passed to L{Request} were in some way invalid.\n@@ -117,7 +130,7 @@ class RequestTransmissionFailed(_WrapperException):\n     @ivar reasons: A C{list} of one or more L{Failure} instances giving the\n         reasons the request transmission was considered to have failed.\n     \"\"\"\n-\n+}}} '''\n \n \n class ConnectionAborted(Exception):\n@@ -126,7 +139,7 @@ class ConnectionAborted(Exception):\n     \"\"\"\n \n \n-\n+''' {{{\n class WrongBodyLength(Exception):\n     \"\"\"\n     An L{IBodyProducer} declared the number of bytes it was going to\n@@ -142,7 +155,7 @@ class ResponseDone(Exception):\n     protocol passed to L{Response.deliverBody} and indicates that the entire\n     response has been delivered.\n     \"\"\"\n-\n+}}} '''\n \n \n class ResponseFailed(_WrapperException):\n@@ -169,7 +182,7 @@ class ResponseNeverReceived(ResponseFailed):\n     \"\"\"\n \n \n-\n+''' {{{\n class RequestNotSent(Exception):\n     \"\"\"\n     L{RequestNotSent} indicates that an attempt was made to issue a request but\n@@ -178,7 +191,7 @@ class RequestNotSent(Exception):\n     to send a request using a protocol which is no longer connected to a\n     server.\n     \"\"\"\n-\n+}}} '''\n \n \n def _callAppFunction(function):\n@@ -764,7 +777,7 @@ class Request:\n         _callAppFunction(self.bodyProducer.stopProducing)\n \n \n-\n+''' {{{\n class LengthEnforcingConsumer:\n     \"\"\"\n     An L{IConsumer} proxy which enforces an exact length requirement on the\n@@ -1188,7 +1201,7 @@ class TransportProxyProducer:\n         \"\"\"\n         if self._producer is not None:\n             self._producer.pauseProducing()\n-\n+}}} '''\n \n \n class HTTP11ClientProtocol(Protocol):\n\n@@ -32,19 +32,29 @@ from twisted.internet.interfaces import IProtocol\n from twisted.python import failure\n from twisted.python.components import proxyForInterface\n from twisted.web import error\n+from twisted.web.iweb import UNKNOWN_LENGTH, IBodyProducer\n from twisted.web.http_headers import Headers\n \n+from twisted.web.client import (\n+    PartialDownloadError,\n+)\n+# newer than 10.0.0\n+#from twisted.web.client import (\n+#    CookieAgent, GzipDecoder, ContentDecoderAgent, RedirectAgent, FileBodyProducer,\n+#    HTTPConnectionPool, Agent, ProxyAgent,\n+#)\n+\n from .endpoints import TCP4ClientEndpoint, SSL4ClientEndpoint\n-from .iweb import IResponse, UNKNOWN_LENGTH, IBodyProducer\n-\n+from .iweb import IResponse\n \n+''' {{{\n class PartialDownloadError(error.Error):\n     \"\"\"\n     Page was only partially downloaded, we got disconnected in middle.\n \n     @ivar response: All of the response body which was downloaded.\n     \"\"\"\n-\n+}}} '''\n \n class _URL(tuple):\n     \"\"\"\n@@ -136,10 +146,13 @@ def _makeGetterFactory(url, factoryFactory, contextFactory=None,\n \n from twisted.web.error import SchemeNotSupported\n from ._newclient import Request, Response, HTTP11ClientProtocol\n-from ._newclient import ResponseDone, ResponseFailed\n-from ._newclient import RequestNotSent, RequestTransmissionFailed\n+from twisted.web._newclient import ResponseDone\n+from ._newclient import ResponseFailed\n+from twisted.web._newclient import RequestNotSent, RequestTransmissionFailed\n+from twisted.web._newclient import (\n+    PotentialDataLoss, _WrapperException)\n from ._newclient import (\n-    ResponseNeverReceived, PotentialDataLoss, _WrapperException)\n+    ResponseNeverReceived)\n \n try:\n     from twisted.internet.ssl import ClientContextFactory\n@@ -1161,8 +1174,7 @@ def readBody(response):\n \n \n __all__ = [\n-    'PartialDownloadError', 'HTTPPageGetter', 'HTTPPageDownloader',\n-    'HTTPClientFactory', 'HTTPDownloader', 'getPage', 'downloadPage',\n+    'PartialDownloadError',\n     'ResponseDone', 'Response', 'ResponseFailed', 'Agent', 'CookieAgent',\n     'ProxyAgent', 'ContentDecoderAgent', 'GzipDecoder', 'RedirectAgent',\n     'HTTPConnectionPool', 'readBody']\n\n@@ -15,23 +15,37 @@ parsed by the L{clientFromString} and L{serverFromString} functions.\n from __future__ import division, absolute_import\n \n import os\n-import socket\n+#import socket\n \n from zope.interface import implementer, directlyProvides\n import warnings\n \n-from twisted.internet import interfaces, defer, error, fdesc, threads\n+from twisted.internet import interfaces, defer, error, fdesc\n from twisted.internet.protocol import (\n-        ClientFactory, Protocol, ProcessProtocol, Factory)\n+        ClientFactory, Protocol, Factory)\n+#from twisted.internet import threads, ProcessProtocol\n from twisted.internet.interfaces import IStreamServerEndpointStringParser\n from twisted.internet.interfaces import IStreamClientEndpointStringParser\n from twisted.python.filepath import FilePath\n-from twisted.python.failure import Failure\n-from twisted.python import log\n+#from twisted.python.failure import Failure\n+#from twisted.python import log\n from twisted.python.components import proxyForInterface\n \n from twisted.plugin import IPlugin, getPlugins\n-from twisted.internet import stdio\n+#from twisted.internet import stdio\n+\n+# newer than 10.0.0\n+#from twisted.internet.endpoints import (\n+#    TCP4ServerEndpoint, TCP6ServerEndpoint, TCP4ClientEndpoint, SSL4ServerEndpoint, SSL4ClientEndpoint,\n+#    UNIXServerEndpoint, UNIXClientEndpoint, AdoptedStreamServerEndpoint, connectProtocol,\n+#    quoteStringArgument,\n+#    serverFromString, #> using newer _parseSSL, _tokenize in _serverParsers\n+#    clientFromString, #> using newer _clientParsers\n+#    _WrappingProtocol, _WrappingFactory, _TCPServerEndpoint,\n+#    _parseTCP, _parseUNIX, _loadCAsFromDir,\n+#    _parseSSL, _tokenize,\n+#    _parseClientTCP, _parseClientSSL, _parseClientUNIX,\n+#)\n \n from .interfaces import IFileDescriptorReceiver\n \n\n@@ -11,7 +11,28 @@ from __future__ import division, absolute_import\n \n from zope.interface import Interface, Attribute\n \n+from twisted.internet.interfaces import (\n+    IAddress, IConnector, IResolverSimple, IReactorTCP, IReactorSSL,\n+    IReactorUDP, IReactorMulticast, IReactorProcess,\n+    IReactorTime, IDelayedCall, IReactorThreads, IReactorCore,\n+    IReactorPluggableResolver, IReactorFDSet,\n+    IListeningPort, ILoggingContext, IFileDescriptor, IReadDescriptor,\n+    IWriteDescriptor, IReadWriteDescriptor, IHalfCloseableDescriptor,\n+    ISystemHandle, IConsumer, IProducer, IPushProducer, IPullProducer,\n+    IProtocol, IProcessProtocol, IHalfCloseableProtocol,\n+    IProtocolFactory, ITransport, IProcessTransport, IServiceCollection,\n+    IUDPTransport, IUNIXDatagramTransport, IUNIXDatagramConnectedTransport,\n+    IMulticastTransport,\n+)\n+# newer than 10.0.0\n+#from twisted.internet.interfaces import (\n+#    IResolver, IReactorUNIX, IReactorUNIXDatagram, IReactorWin32Events, IReactorSocket,\n+#    IReactorDaemonize, IFileDescriptorReceiver, ITCPTransport, IUNIXTransport,\n+#    ITLSTransport, ISSLTransport, IStreamClientEndpoint, IStreamServerEndpoint,\n+#    IStreamServerEndpointStringParser, IStreamClientEndpointStringParser,\n+#)\n \n+''' {{{\n class IAddress(Interface):\n     \"\"\"\n     An address, e.g. a TCP C{(host, port)}.\n@@ -74,7 +95,7 @@ class IResolverSimple(Interface):\n         @raise twisted.internet.defer.TimeoutError: Raised (asynchronously)\n         if the name cannot be resolved within the specified timeout period.\n         \"\"\"\n-\n+}}} '''\n \n \n class IResolver(IResolverSimple):\n@@ -614,7 +635,7 @@ class IResolver(IResolverSimple):\n         \"\"\"\n \n \n-\n+''' {{{\n class IReactorTCP(Interface):\n \n     def listenTCP(port, factory, backlog=50, interface=''):\n@@ -701,7 +722,7 @@ class IReactorSSL(Interface):\n \n         @param interface: the hostname to bind to, defaults to '' (all)\n         \"\"\"\n-\n+}}} '''\n \n \n class IReactorUNIX(Interface):\n@@ -829,7 +850,7 @@ class IReactorWin32Events(Interface):\n         \"\"\"\n \n \n-\n+''' {{{\n class IReactorUDP(Interface):\n     \"\"\"\n     UDP socket methods.\n@@ -868,7 +889,7 @@ class IReactorMulticast(Interface):\n         @see: L{twisted.internet.interfaces.IMulticastTransport}\n         @see: U{http://twistedmatrix.com/documents/current/core/howto/udp.html}\n         \"\"\"\n-\n+}}} '''\n \n \n class IReactorSocket(Interface):\n@@ -970,7 +991,7 @@ class IReactorSocket(Interface):\n         \"\"\"\n \n \n-\n+''' {{{\n class IReactorProcess(Interface):\n \n     def spawnProcess(processProtocol, executable, args=(), env={}, path=None,\n@@ -1347,7 +1368,7 @@ class IReactorPluggableResolver(Interface):\n \n         @return: The previously installed resolver.\n         \"\"\"\n-\n+}}} '''\n \n class IReactorDaemonize(Interface):\n     \"\"\"\n@@ -1379,7 +1400,7 @@ class IReactorDaemonize(Interface):\n         \"\"\"\n \n \n-\n+''' {{{\n class IReactorFDSet(Interface):\n     \"\"\"\n     Implement me to be able to use L{IFileDescriptor} type resources.\n@@ -1863,7 +1884,7 @@ class IHalfCloseableProtocol(Interface):\n         This will never be called for TCP connections as TCP does not\n         support notification of this type of half-close.\n         \"\"\"\n-\n+}}} '''\n \n \n class IFileDescriptorReceiver(Interface):\n@@ -1884,7 +1905,7 @@ class IFileDescriptorReceiver(Interface):\n         \"\"\"\n \n \n-\n+''' {{{\n class IProtocolFactory(Interface):\n     \"\"\"\n     Interface for protocol factories.\n@@ -1974,7 +1995,7 @@ class ITransport(Interface):\n \n         @return: An L{IAddress} provider.\n         \"\"\"\n-\n+}}} '''\n \n class ITCPTransport(ITransport):\n     \"\"\"\n@@ -2095,7 +2116,7 @@ class ISSLTransport(ITCPTransport):\n         Return an object with the peer's certificate info.\n         \"\"\"\n \n-\n+''' {{{\n class IProcessTransport(ITransport):\n     \"\"\"\n     A process transport.\n@@ -2324,7 +2345,7 @@ class IMulticastTransport(Interface):\n         \"\"\"\n         Leave multicast group, return L{Deferred} of success.\n         \"\"\"\n-\n+}}} '''\n \n class IStreamClientEndpoint(Interface):\n     \"\"\"\n\n@@ -12,8 +12,16 @@ Interface definitions for L{twisted.web}.\n \n from zope.interface import Interface, Attribute\n \n-from twisted.internet.interfaces import IPushProducer\n+#from twisted.internet.interfaces import IPushProducer\n \n+from twisted.web.iweb import (\n+    ICredentialFactory, IBodyProducer,\n+    UNKNOWN_LENGTH,\n+)\n+# newer than 10.0.0\n+#from twisted.web.iweb import (\n+#    IRequest, IRenderable, ITemplateLoader, IResponse, _IRequestEncoder, _IRequestEncoderFactory,\n+#)\n \n class IRequest(Interface):\n     \"\"\"\n@@ -320,7 +328,7 @@ class IRequest(Interface):\n         \"\"\"\n \n \n-\n+''' {{{\n class ICredentialFactory(Interface):\n     \"\"\"\n     A credential factory defines a way to generate a particular kind of\n@@ -424,7 +432,7 @@ class IBodyProducer(IPushProducer):\n         L{Deferred<twisted.internet.defer.Deferred>} returned by\n         C{startProducing} is never fired.\n         \"\"\"\n-\n+}}} '''\n \n \n class IRenderable(Interface):\n@@ -576,9 +584,9 @@ class _IRequestEncoderFactory(Interface):\n         \"\"\"\n \n \n-\n+''' {{{\n UNKNOWN_LENGTH = u\"twisted.web.iweb.UNKNOWN_LENGTH\"\n-\n+}}} '''\n __all__ = [\n     \"ICredentialFactory\", \"IRequest\",\n     \"IBodyProducer\", \"IRenderable\", \"IResponse\", \"_IRequestEncoder\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
