{"custom_id": "keras#981d23a66e3921ff5a3819b9d512f47b6b4f79f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 15 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 51 | Churn Cumulative: 346 | Contributors (this commit): 5 | Commits (past 90d): 25 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -114,7 +114,7 @@ class SimpleDeepRNN(Layer):\n     def __init__(self, input_dim, output_dim, depth=3,\n         init='glorot_uniform', inner_init='orthogonal', \n         activation='sigmoid', inner_activation='hard_sigmoid',\n-        weights=None, truncate_gradient=-1, return_sequences=False):\n+        weights=None, truncate_gradient=-1, return_sequences=False, mask_val=default_mask_val):\n         super(SimpleDeepRNN,self).__init__()\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n@@ -126,6 +126,7 @@ class SimpleDeepRNN(Layer):\n         self.depth = depth\n         self.return_sequences = return_sequences\n         self.input = T.tensor3()\n+        self.mask_val = shared_scalar(mask_val)\n \n         self.W = self.init((self.input_dim, self.output_dim))\n         self.Us = [self.inner_init((self.output_dim, self.output_dim)) for _ in range(self.depth)]\n@@ -135,23 +136,37 @@ class SimpleDeepRNN(Layer):\n         if weights is not None:\n             self.set_weights(weights)\n \n-    def _step(self, *args):\n-        o = args[0]\n-        for i in range(1, self.depth+1):\n-            o += self.inner_activation(T.dot(args[i], args[i+self.depth]))\n-        return self.activation(o)\n+    def _step(self, x_t, mask_t, *args):\n+        o = x_t\n+        for i in range(self.depth):\n+            mask_tmi = args[i]\n+            h_tmi = args[i + self.depth]\n+            U_tmi = args[i + 2*self.depth]\n+            o += mask_tmi*self.inner_activation(T.dot(h_tmi, U_tmi))\n+        result = mask_t*self.activation(o) + (1 - mask_t)*self.mask_val\n+        return result\n \n     def get_output(self, train):\n         X = self.get_input(train)\n         X = X.dimshuffle((1,0,2)) \n \n+        mask = get_mask(X, self.mask_val, steps_back=self.depth)\n+\n         x = T.dot(X, self.W) + self.b\n         \n+        if self.depth == 1:\n+            initial = T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1)\n+        else:\n+            initial = T.unbroadcast(T.unbroadcast(alloc_zeros_matrix(self.depth, X.shape[1], self.output_dim), 0), 2)\n+\n         outputs, updates = theano.scan(\n             self._step,\n-            sequences=x,\n+            sequences=[x, dict(\n+                input = mask,\n+                taps = [(-i) for i in range(self.depth+1)]\n+            )],\n             outputs_info=[dict(\n-                initial=T.alloc(np.cast[theano.config.floatX](0.), self.depth, X.shape[1], self.output_dim),\n+                initial = initial,\n                 taps = [(-i-1) for i in range(self.depth)]\n             )],\n             non_sequences=self.Us,\n\n@@ -6,19 +6,22 @@ from keras.utils.theano_utils import sharedX\n from keras.models import Sequential\n from keras.layers.core import Dense, Activation, Merge\n from keras.layers.embeddings import Embedding\n-from keras.layers.recurrent import SimpleRNN\n+from keras.layers.recurrent import SimpleRNN, SimpleDeepRNN\n from keras.layers.core import default_mask_val\n import theano\n \n theano.config.exception_verbosity='high' \n \n # (nb_samples, timesteps, dimensions)\n-X = np.random.random_integers(1, 4, size=(400000,3))\n+X = np.random.random_integers(1, 4, size=(400000,4))\n \n model = Sequential()\n model.add(Embedding(5, 2, zero_is_mask=True))\n+#model.add(SimpleRNN(2,3, activation='relu', return_sequences=True))\n+#model.add(SimpleRNN(3,3, activation='relu'))\n model.add(SimpleRNN(2,3, activation='relu', return_sequences=True))\n-model.add(SimpleRNN(3,3, activation='relu'))\n+# This next one is basically just a SimpleRNN, but I'm testing that the masking is\n+model.add(SimpleDeepRNN(3,3, depth=2, activation='relu')) \n model.add(Dense(3,4, activation='softmax'))\n model.compile(loss='categorical_crossentropy',\n         optimizer='rmsprop', theano_mode=theano.compile.mode.FAST_RUN)\n@@ -35,8 +38,9 @@ W = model.get_weights() # We'll save these so we can reset it later\n Xmask0 = X.copy()\n Xmask0[:,0] = 0\n \n-Xmask1 = X.copy()\n-Xmask1[:,1] = 0\n+Xmask12 = X.copy()\n+Xmask12[:,1] = 0\n+Xmask12[:,2] = 0\n \n X0_onehot = np.zeros((X.shape[0], 4))\n X1_onehot = np.zeros((X.shape[0], 4))\n@@ -90,16 +94,16 @@ if score > uniform_score*0.9:\n \n model.set_weights(W)\n \n-# Finally, make sure the mask is actually blocking input, mask out timestep 1, and see if\n+# Finally, make sure the mask is actually blocking input, mask out timesteps 1 and 2, and see if\n # it can learn timestep 0 (should fail)\n-model.fit(Xmask1, X0_onehot, nb_epoch=1, batch_size=batch_size)\n+model.fit(Xmask12, X0_onehot, nb_epoch=1, batch_size=batch_size)\n \n W0 = model.layers[0].W.get_value()[0,:]\n if (W0 != default_mask_val).any():\n     raise Exception(\"After masked training, the W0 of the Embedding's mask value changed to: \",\n             W0)\n \n-score = model.evaluate(Xmask1, X0_onehot, batch_size=batch_size)\n+score = model.evaluate(Xmask12, X0_onehot, batch_size=batch_size)\n if score < uniform_score*0.9:\n     raise Exception('Somehow learned to copy timestep 0 despite masking 1, score %f' % score)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c13154933e35eb34563c8c7d65e7b527fd813a7f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 156 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -17,10 +17,7 @@ X = np.random.random_integers(1, 4, size=(400000,4))\n \n model = Sequential()\n model.add(Embedding(5, 2, zero_is_mask=True))\n-#model.add(SimpleRNN(2,3, activation='relu', return_sequences=True))\n-#model.add(SimpleRNN(3,3, activation='relu'))\n model.add(SimpleRNN(2,3, activation='relu', return_sequences=True))\n-# This next one is basically just a SimpleRNN, but I'm testing that the masking is\n model.add(SimpleDeepRNN(3,3, depth=2, activation='relu')) \n model.add(Dense(3,4, activation='softmax'))\n model.compile(loss='categorical_crossentropy',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a12510ba9742ecc04e877950a7e17f8656061b3c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 10 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 219 | Contributors (this commit): 5 | Commits (past 90d): 21 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,17 +12,17 @@ from six.moves import range\n def get_mask(X, mask_val, steps_back=0):\n     '''\n         Given X, a (timesteps, nb_samples, n_dimensions) tensor, returns a mask\n-        tensor  with dimension (timesteps + steps_back, nb_samples, 1).  This\n-        matrix is left-padded with `steps_back` zeros in the time dimension, and\n-        elsewhere has a 1 for every entry except for those corresponding to a\n-        vector in X that has every entry equal to mask_val.\n+        tensor and, if steps_back>0, a padded_mask tensor  with dimension (timesteps + steps_back, nb_samples, 1).\n+        This tensor is left-padded with `steps_back` zeros in the time dimension.\n+        \n+        The mask has a 1 for every entry except for those corresponding to a vector in X that has every entry equal to mask_val.\n     '''\n     mask = T.neq(X, mask_val).sum(axis=2) > 0 # (time, nb_samples) matrix with a 1 for every unmasked entry\n     mask = T.addbroadcast(mask[:, :, np.newaxis], 2) # (time, nb_samples, 1) matrix.\n     if steps_back > 0:\n         # left-pad in time with 0\n         pad = alloc_zeros_matrix(steps_back, mask.shape[1], 1).astype('uint8')\n-        mask = T.concatenate([pad, mask], axis=0)\n+        return mask, T.concatenate([pad, mask], axis=0)\n     return mask\n \n     \n@@ -64,14 +64,14 @@ class SimpleRNN(Layer):\n \n         '''\n         normal_output = self.activation(x_t + mask_tm1 * T.dot(h_tm1, u))\n-        return mask_t * normal_output + (1 - mask_t) * self.mask_val\n+        return mask_t * normal_output\n \n     def get_output(self, train):\n         X = self.get_input(train) # shape: (nb_samples, time (padded with zeros at the end), input_dim)\n         # new shape: (time, nb_samples, input_dim) -> because theano.scan iterates over main dimension\n         X = X.dimshuffle((1,0,2)) \n \n-        mask = get_mask(X, self.mask_val, steps_back=1)\n+        mask, padded_mask = get_mask(X, self.mask_val, steps_back=1)\n \n         x = T.dot(X, self.W) + self.b\n         \n@@ -80,12 +80,14 @@ class SimpleRNN(Layer):\n         # Iterate over the first dimension of the x array (=time).\n         outputs, updates = theano.scan(\n             self._step, # this will be called with arguments (sequences[i], outputs[i-1], non_sequences[i])\n-            sequences=[x, dict(input=mask,taps=[0, -1])], # tensors to iterate over, inputs to _step\n+            sequences=[x, dict(input=padded_mask,taps=[0, -1])], # tensors to iterate over, inputs to _step\n             # initialization of the output. Input to _step with default tap=-1.\n             outputs_info=T.unbroadcast(alloc_zeros_matrix(X.shape[1], self.output_dim), 1),\n             non_sequences=self.U, # static inputs to _step\n             truncate_gradient=self.truncate_gradient\n         )\n+        # Apply the mask:\n+        outputs = mask*outputs + (1-mask)*self.mask_val\n         if self.return_sequences:\n             return outputs.dimshuffle((1,0,2))\n         return outputs[-1]\n@@ -143,14 +145,13 @@ class SimpleDeepRNN(Layer):\n             h_tmi = args[i + self.depth]\n             U_tmi = args[i + 2*self.depth]\n             o += mask_tmi*self.inner_activation(T.dot(h_tmi, U_tmi))\n-        result = mask_t*self.activation(o) + (1 - mask_t)*self.mask_val\n-        return result\n+        return mask_t*self.activation(o)\n \n     def get_output(self, train):\n         X = self.get_input(train)\n         X = X.dimshuffle((1,0,2)) \n \n-        mask = get_mask(X, self.mask_val, steps_back=self.depth)\n+        mask, padded_mask = get_mask(X, self.mask_val, steps_back=self.depth)\n \n         x = T.dot(X, self.W) + self.b\n         \n@@ -162,7 +163,7 @@ class SimpleDeepRNN(Layer):\n         outputs, updates = theano.scan(\n             self._step,\n             sequences=[x, dict(\n-                input = mask,\n+                input = padded_mask,\n                 taps = [(-i) for i in range(self.depth+1)]\n             )],\n             outputs_info=[dict(\n@@ -172,6 +173,7 @@ class SimpleDeepRNN(Layer):\n             non_sequences=self.Us,\n             truncate_gradient=self.truncate_gradient\n         )\n+        outputs = mask*outputs + (1-mask)*self.mask_val\n         if self.return_sequences:\n             return outputs.dimshuffle((1,0,2))\n         return outputs[-1]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#824f9f5e80ee96891fc07413eaf2df5f7e5737da", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 20 | Churn Cumulative: 1353 | Contributors (this commit): 16 | Commits (past 90d): 59 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -41,17 +41,11 @@ def slice_X(X, start=None, stop=None):\n         else:\n             return X[start:stop]\n \n-def standardize_sample_weights(sample_weight):\n-    if isinstance(sample_weight, list):\n-        w = np.array(sample_weight)\n-    elif isinstance(sample_weight, np.ndarray):\n-        w = sample_weight\n-    else:\n-        w = None\n-    return w\n-\n def calculate_loss_weights(Y, sample_weight=None, class_weight=None):\n     if sample_weight is not None:\n+        if isinstance(sample_weight, list):\n+            w = np.array(sample_weight)\n+        else:\n             w = sample_weight\n     elif isinstance(class_weight, dict):\n         if Y.shape[1] > 1:\n@@ -125,7 +119,6 @@ class Model(object):\n         X = standardize_X(X)\n         y = standardize_y(y)\n \n-        sample_weight = standardize_sample_weights(sample_weight)\n         w = calculate_loss_weights(y, sample_weight=sample_weight, class_weight=class_weight)\n \n         ins = X + [y, w]\n@@ -152,7 +145,7 @@ class Model(object):\n         X = standardize_X(X)\n         y = standardize_y(y)\n \n-        sample_weight = standardize_sample_weights(sample_weight)\n+        sample_weight = calculate_loss_weights(y, sample_weight=sample_weight, class_weight=class_weight)\n \n         do_validation = False\n         if validation_data:\n@@ -207,10 +200,7 @@ class Model(object):\n                 X_batch = slice_X(X, batch_ids)\n                 y_batch = y[batch_ids]\n \n-                if sample_weight is not None:\n-                    w = calculate_loss_weights(y_batch, sample_weight=sample_weight[batch_ids])\n-                else:\n-                    w = calculate_loss_weights(y_batch, class_weight=class_weight)\n+                w = sample_weight[batch_ids]\n \n                 batch_logs = {}\n                 batch_logs['batch'] = batch_index\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#06f22db69a01bcbacc3425954ab2a38d495fdada", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 114 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -13,7 +13,7 @@ import numpy as np\n     Train a simple deep NN on the MNIST dataset.\n '''\n \n-batch_size = 64\n+batch_size = 128\n nb_classes = 10\n nb_epoch = 20\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e23e86ae7a7f3ed021e5b9f013b66203b41a0931", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 72 | Lines Deleted: 7 | Files Changed: 4 | Hunks: 10 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 79 | Churn Cumulative: 440 | Contributors (this commit): 3 | Commits (past 90d): 21 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -30,6 +30,7 @@ data_augmentation = True\n \n # the data, shuffled and split between tran and test sets\n (X_train, y_train), (X_test, y_test) = cifar10.load_data()\n+print('X_train shape:', X_train.shape)\n print(X_train.shape[0], 'train samples')\n print(X_test.shape[0], 'test samples')\n \n@@ -54,11 +55,11 @@ model.add(MaxPooling2D(poolsize=(2, 2)))\n model.add(Dropout(0.25))\n \n model.add(Flatten())\n-model.add(Dense(64*8*8, 512, init='normal'))\n+model.add(Dense(64*8*8, 512))\n model.add(Activation('relu'))\n model.add(Dropout(0.5))\n \n-model.add(Dense(512, nb_classes, init='normal'))\n+model.add(Dense(512, nb_classes))\n model.add(Activation('softmax'))\n \n # let's train the model using SGD + momentum (how original).\n\n@@ -0,0 +1,62 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n+from keras.datasets import mnist\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Dropout, Activation, Flatten\n+from keras.layers.convolutional import Convolution2D, MaxPooling2D\n+from keras.utils import np_utils\n+import numpy as np\n+\n+'''\n+    Train a simple convnet on the MNIST dataset.\n+\n+    Run on GPU: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python mnist_cnn.py\n+\n+    Get to 99.25% test accuracy after 12 epochs (there is still a lot of margin for parameter tuning).\n+    16 seconds per epoch on a GRID K520 GPU.\n+'''\n+\n+batch_size = 128\n+nb_classes = 10\n+nb_epoch = 12\n+\n+# the data, shuffled and split between tran and test sets\n+(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+\n+X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n+X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n+X_train = X_train.astype(\"float32\")\n+X_test = X_test.astype(\"float32\")\n+X_train /= 255\n+X_test /= 255\n+print('X_train shape:', X_train.shape)\n+print(X_train.shape[0], 'train samples')\n+print(X_test.shape[0], 'test samples')\n+\n+# convert class vectors to binary class matrices\n+Y_train = np_utils.to_categorical(y_train, nb_classes)\n+Y_test = np_utils.to_categorical(y_test, nb_classes)\n+\n+model = Sequential()\n+\n+model.add(Convolution2D(32, 1, 3, 3, border_mode='full')) \n+model.add(Activation('relu'))\n+model.add(Convolution2D(32, 32, 3, 3))\n+model.add(Activation('relu'))\n+model.add(MaxPooling2D(poolsize=(2, 2)))\n+model.add(Dropout(0.25))\n+\n+model.add(Flatten())\n+model.add(Dense(32*196, 128))\n+model.add(Activation('relu'))\n+model.add(Dropout(0.5))\n+\n+model.add(Dense(128, nb_classes))\n+model.add(Activation('softmax'))\n+\n+model.compile(loss='categorical_crossentropy', optimizer='adadelta')\n+\n+model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=1, validation_data=(X_test, Y_test))\n+score = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)\n+print('Test score:', score[0])\n+print('Test accuracy:', score[1])\n\n@@ -3,14 +3,15 @@ from __future__ import print_function\n from keras.datasets import mnist\n from keras.models import Sequential\n from keras.layers.core import Dense, Dropout, Activation\n-from keras.regularizers import l2, l1\n-from keras.constraints import maxnorm\n from keras.optimizers import SGD, Adam, RMSprop\n from keras.utils import np_utils\n import numpy as np\n \n '''\n     Train a simple deep NN on the MNIST dataset.\n+\n+    Get to 98.30% test accuracy after 20 epochs (there is *a lot* of margin for parameter tuning).\n+    2 seconds per epoch on a GRID K520 GPU.\n '''\n \n batch_size = 128\n\n@@ -19,6 +19,7 @@ from keras.preprocessing.text import Tokenizer\n \n max_words = 1000\n batch_size = 32\n+nb_epoch = 5\n \n print(\"Loading data...\")\n (X_train, y_train), (X_test, y_test) = reuters.load_data(nb_words=max_words, test_split=0.2)\n@@ -43,15 +44,15 @@ print('Y_test shape:', Y_test.shape)\n \n print(\"Building model...\")\n model = Sequential()\n-model.add(Dense(max_words, 256, init='normal'))\n+model.add(Dense(max_words, 512))\n model.add(Activation('relu'))\n model.add(Dropout(0.5))\n-model.add(Dense(256, nb_classes, init='normal'))\n+model.add(Dense(512, nb_classes))\n model.add(Activation('softmax'))\n \n model.compile(loss='categorical_crossentropy', optimizer='adam')\n \n-history = model.fit(X_train, Y_train, nb_epoch=4, batch_size=batch_size, verbose=1, show_accuracy=True, validation_split=0.1)\n+history = model.fit(X_train, Y_train, nb_epoch=nb_epoch, batch_size=batch_size, verbose=1, show_accuracy=True, validation_split=0.1)\n score = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=1, show_accuracy=True)\n print('Test score:', score[0])\n print('Test accuracy:', score[1])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1f224de9b1c23e18995b6e651adde6ca06033cac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 499 | Contributors (this commit): 4 | Commits (past 90d): 15 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -181,6 +181,7 @@ class History(Callback):\n             if self.params['show_accuracy']:\n                 self.validation_accuracy.append(val_acc)\n \n+\n class ModelCheckpoint(Callback):\n     def __init__(self, filepath, verbose=0, save_best_only=False):\n         super(Callback, self).__init__()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ccbe381dcdf2d43b2d88d33ba3dcf4680fe116a8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/6 | Churn Δ: 38 | Churn Cumulative: 1890 | Contributors (this commit): 16 | Commits (past 90d): 76 | Contributors (cumulative): 20 | DMM Complexity: 0.95\n\nDIFF:\n@@ -195,8 +195,6 @@ class ModelCheckpoint(Callback):\n         self.best_val_loss = np.Inf\n \n     def on_epoch_end(self, epoch, logs={}):\n-        '''currently, on_epoch_end receives epoch_logs from keras.models.Sequential.fit\n-        which does only contain, if at all, the validation loss and validation accuracy'''\n         if self.save_best_only and self.params['do_validation']:\n             cur_val_loss = logs.get('val_loss')\n             self.val_loss.append(cur_val_loss)\n@@ -210,10 +208,33 @@ class ModelCheckpoint(Callback):\n                 if self.verbose > 0:\n                     print(\"Epoch %05d: validation loss did not improve\" % (epoch))\n         elif self.save_best_only and not self.params['do_validation']:\n-            import warnings\n             warnings.warn(\"Can save best model only with validation data, skipping\", RuntimeWarning)\n         elif not self.save_best_only:\n             if self.verbose > 0:\n                 print(\"Epoch %05d: saving model to %s\" % (epoch, self.filepath))\n             self.model.save_weights(self.filepath, overwrite=True)\n \n+\n+class EarlyStopping(Callback):\n+    def __init__(self, patience=1, verbose=0):\n+        super(Callback, self).__init__()\n+\n+        self.patience = patience\n+        self.verbose = verbose\n+        self.best_val_loss = np.Inf\n+        self.wait = 0\n+\n+    def on_epoch_end(self, epoch, logs={}):\n+        if not self.params['do_validation']:\n+            warnings.warn(\"Early stopping requires validation data!\", RuntimeWarning)\n+\n+        cur_val_loss = logs.get('val_loss')\n+        if cur_val_loss < self.best_val_loss:\n+            self.best_val_loss = cur_val_loss\n+            self.wait = 0\n+        else:\n+            if self.wait >= self.patience:\n+                if self.verbose > 0:\n+                    print(\"Epoch %05d: early stopping\" % (epoch))\n+                self.model.stop_training = True\n+            self.wait += 1\n\n@@ -164,10 +164,9 @@ class Model(object):\n \n         index_array = np.arange(len(y))\n \n-        callbacks = cbks.CallbackList(callbacks)\n         if verbose:\n-            callbacks.append(cbks.BaseLogger())\n-        callbacks.append(cbks.History())\n+            callbacks = [cbks.BaseLogger()] + callbacks\n+        callbacks = cbks.CallbackList([cbks.History()] + callbacks)\n \n         callbacks._set_model(self)\n         callbacks._set_params({\n@@ -180,6 +179,7 @@ class Model(object):\n         })\n         callbacks.on_train_begin()\n \n+        self.stop_training = False\n         for epoch in range(nb_epoch):\n             callbacks.on_epoch_begin(epoch)\n             if shuffle:\n@@ -222,10 +222,11 @@ class Model(object):\n                         epoch_logs['val_loss'] = val_loss\n \n             callbacks.on_epoch_end(epoch, epoch_logs)\n+            if self.stop_training:\n+                break\n \n         callbacks.on_train_end()\n-        # return history\n-        return callbacks.callbacks[-1]\n+        return callbacks.callbacks[0] # return history\n \n     def predict(self, X, batch_size=128, verbose=1):\n         X = standardize_X(X)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b44ecab225e3ead586a8034ac1fe1cf1699eadb7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 906 | Contributors (this commit): 9 | Commits (past 90d): 37 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -3,6 +3,7 @@ from __future__ import absolute_import, division\n \n import theano\n import theano.tensor as T\n+import numpy as np\n \n from .. import activations, initializations\n from ..utils.theano_utils import shared_zeros, floatX\n@@ -12,7 +13,7 @@ from .. import constraints\n \n from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n from six.moves import zip\n-srng = RandomStreams()\n+srng = RandomStreams(seed=np.random.randint(10e6))\n \n class Layer(object):\n     def __init__(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1d586440a8649e0e0e0b19220040c4b210bcea1b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 262 | Files Changed: 3 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): -1/2 | Churn Δ: 304 | Churn Cumulative: 1878 | Contributors (this commit): 16 | Commits (past 90d): 79 | Contributors (cumulative): 21 | DMM Complexity: 0.0\n\nDIFF:\n@@ -381,5 +381,3 @@ class Sequential(Model, containers.Sequential):\n             weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n             self.layers[k].set_weights(weights)\n         f.close()\n\\ No newline at end of file\n-\n-\n\n@@ -60,13 +60,3 @@ mae = MAE = mean_absolute_error\n from .utils.generic_utils import get_from_module\n def get(identifier):\n     return get_from_module(identifier, globals(), 'objective')\n\\ No newline at end of file\n-\n-def to_categorical(y):\n-    '''Convert class vector (integers from 0 to nb_classes)\n-    to binary class matrix, for use with categorical_crossentropy\n-    '''\n-    nb_classes = np.max(y)+1\n-    Y = np.zeros((len(y), nb_classes))\n-    for i in range(len(y)):\n-        Y[i, y[i]] = 1.\n-    return Y\n\n@@ -2,278 +2,70 @@ from __future__ import absolute_import\n from __future__ import print_function\n from keras.datasets import mnist\n from keras.models import Sequential\n-from keras.layers.core import Dense, Activation, Merge, Dropout\n-from keras.optimizers import SGD\n+from keras.layers.core import Dense, Activation\n from keras.utils import np_utils\n import numpy as np\n \n nb_classes = 10\n batch_size = 128\n-nb_epoch = 15\n-\n-max_train_samples = 5000\n-max_test_samples = 1000\n+nb_epoch = 5\n+weighted_class = 9\n+standard_weight = 1\n+high_weight = 5\n+max_train_samples = 10000\n+max_test_samples = 10000\n \n np.random.seed(1337) # for reproducibility\n \n # the data, shuffled and split between tran and test sets\n (X_train, y_train), (X_test, y_test) = mnist.load_data()\n-\n X_train = X_train.reshape(60000,784)[:max_train_samples]\n X_test = X_test.reshape(10000,784)[:max_test_samples]\n-X_train = X_train.astype(\"float32\")\n-X_test = X_test.astype(\"float32\")\n-X_train /= 255\n-X_test /= 255\n+X_train = X_train.astype(\"float32\") / 255\n+X_test = X_test.astype(\"float32\") / 255\n \n # convert class vectors to binary class matrices\n y_train = y_train[:max_train_samples]\n y_test = y_test[:max_test_samples]\n Y_train = np_utils.to_categorical(y_train, nb_classes)\n Y_test = np_utils.to_categorical(y_test, nb_classes)\n+test_ids = np.where(y_test == np.array(weighted_class))[0]\n \n-def createMNISTModel():\n+def create_model():\n     model = Sequential()\n     model.add(Dense(784, 50))\n     model.add(Activation('relu'))\n     model.add(Dense(50, 10))\n     model.add(Activation('softmax'))\n+    return model\n+\n+def test_weights(model, class_weight=None, sample_weight=None):\n+    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, \\\n+        class_weight=class_weight, sample_weight=sample_weight)\n+    score = model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n+    return score\n+\n+class_weight = dict([(i, standard_weight) for i in range(nb_classes)])\n+class_weight[weighted_class] = high_weight\n+\n+sample_weight = np.ones((y_train.shape[0])) * standard_weight\n+sample_weight[y_train == weighted_class] = high_weight\n+\n+for loss in ['mae', 'mse', 'categorical_crossentropy']:\n+    print('loss:', loss)\n+    # no weights: reference point\n+    model = create_model()\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-    return model\n-\n-model_classweights_fit = createMNISTModel()\n-model_fit = createMNISTModel()\n-\n-model_classweights_train = createMNISTModel()\n-model_train = createMNISTModel()\n-\n-high_weight = 100\n-class_weight = {0:1,1:1,2:1,3:1,4:1,5:1,6:1,7:1,8:1,9:high_weight}\n-\n-############################\n-# categorical crossentropy #\n-############################\n-\n-print(\"Testing fit methods with and without classweights\")\n-# fit\n-model_classweights_fit.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=(X_test, Y_test), class_weight=class_weight)\n-model_fit.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=(X_test, Y_test))\n-print(\"Testing train methods with and without classweights\")\n-# train\n-model_classweights_train.train(X_train, Y_train, class_weight=class_weight)\n-model_train.train(X_train, Y_train)\n-\n-print('MNIST Classification accuracies on test set for fitted models:')\n-for nb in range(nb_classes):\n-    testIdcs = np.where(y_test == np.array(nb))[0]\n-    X_temp = X_test[testIdcs, :]\n-    Y_temp = Y_test[testIdcs,:]\n-    # eval model which was trained with fit()\n-    score_cw = model_classweights_fit.evaluate(X_temp, Y_temp, show_accuracy=True, verbose=0)\n-    score = model_fit.evaluate(X_temp, Y_temp, show_accuracy=True, verbose=0)\n-    # eval model which was trained with train()\n-    score_cw_train = model_classweights_train.evaluate(X_temp, Y_temp, show_accuracy=True, verbose=0)\n-    score_train = model_train.evaluate(X_temp, Y_temp, show_accuracy=True, verbose=0)\n-    # print test accuracies for class weighted model vs. uniform weights\n-    print(\"Digit %d: class_weight = %d -> %.3f \\t class_weight = %d -> %.3f\" % (nb, class_weight[nb], score_cw[1], 1, score[1]))\n-    if class_weight[nb] == high_weight and (score_cw[1] <= score[1] or score_cw_train[1] <= score_train[1]):\n-        raise Exception('Class weights are not implemented correctly')\n-\n-####################################################\n-# test cases for all remaining objective functions #\n-####################################################\n-batch_size = 64\n-nb_epoch = 10\n-\n-def generateData(n_samples, n_dim):\n-    A_feats = np.random.randn(n_samples, n_dim)\n-    B_feats = np.random.randn(n_samples, n_dim)\n-    A_label = np.zeros((n_samples,1))\n-    B_label = np.ones((n_samples,1))\n-    X = np.vstack((A_feats, B_feats))\n-    y = np.vstack((A_label, B_label)).squeeze()\n-    return X, y\n-\n-n_dim = 100\n-X_train, y_train = generateData(1000, n_dim)\n-X_test, y_test = generateData(5000, n_dim)\n-\n-def createModel(ls, n_dim, activation=\"sigmoid\"):\n-    model = Sequential()\n-    model.add(Dense(n_dim, 50))\n-    model.add(Activation('relu'))\n-    model.add(Dropout(0.5))\n-    model.add(Dense(50, 1))\n-    model.add(Activation(activation))\n-    sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n-    model.compile(loss=ls, optimizer=sgd, class_mode=\"binary\")\n-    return model\n-\n-verbosity = 0\n-cw = {0: 1.5, 1: 1}\n-# binary crossentropy\n-model = createModel('binary_crossentropy', n_dim)\n-model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=verbosity, validation_data=(X_test, y_test), class_weight=cw)\n-res = model.predict(X_test, verbose=verbosity).round()\n-neg_preds, pos_preds = (1.0*np.sum(res == 0)/res.shape[0], 1.0*np.sum(res == 1)/res.shape[0])\n-assert(neg_preds > pos_preds)\n-print(\"binary crossentropy: %0.2f VS %0.2f\" % (neg_preds, pos_preds))\n-\n-# MAE\n-model = createModel('mean_absolute_error', n_dim)\n-model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=verbosity, validation_data=(X_test, y_test), class_weight=cw)\n-res = model.predict(X_test, verbose=verbosity).round()\n-neg_preds, pos_preds = (1.0*np.sum(res == 0)/res.shape[0], 1.0*np.sum(res == 1)/res.shape[0])\n-assert(neg_preds > pos_preds)\n-print(\"MAE: %0.2f VS %0.2f\" % (neg_preds, pos_preds))\n-\n-# MSE\n-model = createModel('mean_squared_error', n_dim)\n-model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=verbosity, validation_data=(X_test, y_test), class_weight=cw)\n-res = model.predict(X_test, verbose=verbosity).round()\n-neg_preds, pos_preds = (1.0*np.sum(res == 0)/res.shape[0], 1.0*np.sum(res == 1)/res.shape[0])\n-assert(neg_preds > pos_preds)\n-print(\"MSE: %0.2f VS %0.2f\" % (neg_preds, pos_preds))\n-\n-# hinge losses, map labels\n-y_train[y_train == 0] = -1\n-y_test[y_test == 0] = -1\n-cw = {-1: 1.5, 1: 1}\n-\n-# hinge\n-model = createModel('hinge', n_dim, \"tanh\")\n-model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=verbosity, validation_data=(X_test, y_test), class_weight=cw)\n-res = model.predict(X_test, verbose=verbosity)\n-res[res < 0] = -1\n-res[res >= 0] = 1\n-neg_preds, pos_preds = (1.0*np.sum(res == -1)/res.shape[0], 1.0*np.sum(res == 1)/res.shape[0])\n-assert(neg_preds > pos_preds)\n-print(\"hinge: %0.2f VS %0.2f\" % (neg_preds, pos_preds))\n-\n-# squared hinge\n-model = createModel('squared_hinge', n_dim, \"tanh\")\n-model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=verbosity, validation_data=(X_test, y_test), class_weight=cw)\n-res = model.predict(X_test, verbose=verbosity)\n-res[res < 0] = -1\n-res[res >= 0] = 1\n-neg_preds, pos_preds = (1.0*np.sum(res == -1)/res.shape[0], 1.0*np.sum(res == 1)/res.shape[0])\n-assert(neg_preds > pos_preds)\n-print(\"sqr hinge: %0.2f VS %0.2f\" % (neg_preds, pos_preds))\n-\n-\n-############################\n-# sample weight test cases #\n-############################\n-\n-batch_size = 128\n-nb_epoch = 15\n-\n-# the data, shuffled and split between tran and test sets\n-(X_train, y_train), (X_test, y_test) = mnist.load_data()\n-\n-X_train = X_train.reshape(60000,784)[:max_train_samples]\n-X_test = X_test.reshape(10000,784)[:max_test_samples]\n-X_train = X_train.astype(\"float32\")\n-X_test = X_test.astype(\"float32\")\n-X_train /= 255\n-X_test /= 255\n-\n-# convert class vectors to binary class matrices\n-y_train = y_train[:max_train_samples]\n-y_test = y_test[:max_test_samples]\n-Y_train = np_utils.to_categorical(y_train, nb_classes)\n-Y_test = np_utils.to_categorical(y_test, nb_classes)\n-\n-print(\"Sample weight test cases\")\n-\n-# categorical crossentropy\n-model_sampleweights_fit = createMNISTModel()\n-model_sampleweights_train = createMNISTModel()\n-model_fit = createMNISTModel()\n-model_train = createMNISTModel()\n-\n-sample_weight = np.ones((Y_train.shape[0]))\n-sample_weight[y_train == 9] = high_weight\n-\n-model_sampleweights_fit.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=(X_test, Y_test), sample_weight=sample_weight)\n-model_fit.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=(X_test, Y_test))\n-model_sampleweights_train.train(X_train, Y_train, sample_weight=sample_weight)\n-model_train.train(X_train, Y_train)\n-\n-print('MNIST Classification accuracies on test set for fitted models:')\n-for nb in range(nb_classes):\n-    testIdcs = np.where(y_test == np.array(nb))[0]\n-    X_temp = X_test[testIdcs, :]\n-    Y_temp = Y_test[testIdcs,:]\n-    # eval model which was trained with fit()\n-    score_sw = model_sampleweights_fit.evaluate(X_temp, Y_temp, show_accuracy=True, verbose=0)\n-    score = model_fit.evaluate(X_temp, Y_temp, show_accuracy=True, verbose=0)\n-    # eval model which was trained with train()\n-    score_sw_train = model_sampleweights_train.evaluate(X_temp, Y_temp, show_accuracy=True, verbose=0)\n-    score_train = model_train.evaluate(X_temp, Y_temp, show_accuracy=True, verbose=0)\n-    # print test accuracies for class weighted model vs. uniform weights\n-    print(\"Digit %d: sample_weight = %d -> %.3f \\t sample_weight = %d -> %.3f\" % (nb, 100 if nb == 9 else 1, score_sw[1], 1, score[1]))\n-    if nb == 9 and (score_sw[1] <= score[1] or score_sw_train[1] <= score_train[1]):\n-        raise Exception('Sample weights are not implemented correctly')\n-\n-\n-n_dim = 100\n-X_train, y_train = generateData(1000, n_dim)\n-X_test, y_test = generateData(5000, n_dim)\n-\n-y_train[y_train == -1] = 0\n-y_test[y_test == -1] = 0\n-\n-sample_weight = np.ones((y_train.shape[0]))\n-sample_weight[y_train == 0] = 1.5\n-\n-# binary crossentropy\n-model = createModel('binary_crossentropy', n_dim)\n-model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=verbosity, validation_data=(X_test, y_test), sample_weight=sample_weight)\n-res = model.predict(X_test, verbose=verbosity).round()\n-neg_preds, pos_preds = (1.0*np.sum(res == 0)/res.shape[0], 1.0*np.sum(res == 1)/res.shape[0])\n-assert(neg_preds > pos_preds)\n-print(\"binary crossentropy: %0.2f VS %0.2f\" % (neg_preds, pos_preds))\n-\n-# MAE\n-model = createModel('mean_absolute_error', n_dim)\n-model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=verbosity, validation_data=(X_test, y_test), sample_weight=sample_weight)\n-res = model.predict(X_test, verbose=verbosity).round()\n-neg_preds, pos_preds = (1.0*np.sum(res == 0)/res.shape[0], 1.0*np.sum(res == 1)/res.shape[0])\n-assert(neg_preds > pos_preds)\n-print(\"MAE: %0.2f VS %0.2f\" % (neg_preds, pos_preds))\n-\n-# MSE\n-model = createModel('mean_squared_error', n_dim)\n-model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=verbosity, validation_data=(X_test, y_test), sample_weight=sample_weight)\n-res = model.predict(X_test, verbose=verbosity).round()\n-neg_preds, pos_preds = (1.0*np.sum(res == 0)/res.shape[0], 1.0*np.sum(res == 1)/res.shape[0])\n-assert(neg_preds > pos_preds)\n-print(\"MSE: %0.2f VS %0.2f\" % (neg_preds, pos_preds))\n-\n-# hinge losses, map labels\n-y_train[y_train == 0] = -1\n-y_test[y_test == 0] = -1\n-sample_weight = np.ones((y_train.shape[0]))\n-sample_weight[y_train == -1] = 1.5\n-\n-# hinge\n-model = createModel('hinge', n_dim, \"tanh\")\n-model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=verbosity, validation_data=(X_test, y_test), sample_weight=sample_weight)\n-res = model.predict(X_test, verbose=verbosity)\n-res[res < 0] = -1\n-res[res >= 0] = 1\n-neg_preds, pos_preds = (1.0*np.sum(res == -1)/res.shape[0], 1.0*np.sum(res == 1)/res.shape[0])\n-assert(neg_preds > pos_preds)\n-print(\"hinge: %0.2f VS %0.2f\" % (neg_preds, pos_preds))\n-\n-# squared hinge\n-model = createModel('squared_hinge', n_dim, \"tanh\")\n-model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=verbosity, validation_data=(X_test, y_test), sample_weight=sample_weight)\n-res = model.predict(X_test, verbose=verbosity)\n-res[res < 0] = -1\n-res[res >= 0] = 1\n-neg_preds, pos_preds = (1.0*np.sum(res == -1)/res.shape[0], 1.0*np.sum(res == 1)/res.shape[0])\n-assert(neg_preds > pos_preds)\n-print(\"sqr hinge: %0.2f VS %0.2f\" % (neg_preds, pos_preds))\n-\n+    standard_score = test_weights(model)\n+    # test class_weight\n+    model = create_model()\n+    model.compile(loss=loss, optimizer='rmsprop')\n+    score = test_weights(model, class_weight=class_weight)\n+    print('score:', score, ' vs.', standard_score)\n+    assert(score < standard_score)\n+    # test sample_weight\n+    model = create_model()\n+    model.compile(loss=loss, optimizer='rmsprop')\n+    score = test_weights(model, sample_weight=sample_weight)\n+    print('score:', score, ' vs.', standard_score)\n+    assert(score < standard_score)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fd5b68dbe3e80aec440f578f6a30ca314615b806", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1369 | Contributors (this commit): 16 | Commits (past 90d): 62 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -54,7 +54,7 @@ def calculate_loss_weights(Y, sample_weight=None, class_weight=None):\n             y_classes = np.reshape(Y, Y.shape[0])\n         else:\n             y_classes = Y\n-        w = np.array(map(lambda x: class_weight[x], y_classes))\n+        w = np.array(list(map(lambda x: class_weight[x], y_classes)))\n     else:\n         w = np.ones((Y.shape[0]))\n     return w\n@@ -144,7 +144,6 @@ class Model(object):\n \n         X = standardize_X(X)\n         y = standardize_y(y)\n-\n         sample_weight = calculate_loss_weights(y, sample_weight=sample_weight, class_weight=class_weight)\n \n         do_validation = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#97f23268c2567721b1aa858a286c30d739170eef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 16 | Churn Cumulative: 427 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,6 +5,7 @@ from keras.models import Sequential\n from keras.layers.core import Dense, Activation\n from keras.utils import np_utils\n import numpy as np\n+import unittest\n \n nb_classes = 10\n batch_size = 128\n@@ -12,8 +13,8 @@ nb_epoch = 5\n weighted_class = 9\n standard_weight = 1\n high_weight = 5\n-max_train_samples = 10000\n-max_test_samples = 10000\n+max_train_samples = 5000\n+max_test_samples = 1000\n \n np.random.seed(1337) # for reproducibility\n \n@@ -45,6 +46,9 @@ def test_weights(model, class_weight=None, sample_weight=None):\n     score = model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n     return score\n \n+class TestConcatenation(unittest.TestCase):\n+\n+    def test_loss_weighting(self):\n         class_weight = dict([(i, standard_weight) for i in range(nb_classes)])\n         class_weight[weighted_class] = high_weight\n \n@@ -62,10 +66,14 @@ for loss in ['mae', 'mse', 'categorical_crossentropy']:\n             model.compile(loss=loss, optimizer='rmsprop')\n             score = test_weights(model, class_weight=class_weight)\n             print('score:', score, ' vs.', standard_score)\n-    assert(score < standard_score)\n+            self.assertTrue(score < standard_score)\n             # test sample_weight\n             model = create_model()\n             model.compile(loss=loss, optimizer='rmsprop')\n             score = test_weights(model, sample_weight=sample_weight)\n             print('score:', score, ' vs.', standard_score)\n-    assert(score < standard_score)\n+            self.assertTrue(score < standard_score)\n+\n+if __name__ == '__main__':\n+    print('Test class_weight and sample_weight')\n+    unittest.main()\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5a1a00e69e5df1661c7d8e5359b8eca7db7f7208", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 6 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 30 | Churn Cumulative: 556 | Contributors (this commit): 4 | Commits (past 90d): 17 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,7 @@ import theano\n import theano.tensor as T\n import numpy as np\n import warnings\n-import time\n+import time, json\n from collections import deque\n \n from .utils.generic_utils import Progbar\n@@ -216,7 +216,7 @@ class ModelCheckpoint(Callback):\n \n \n class EarlyStopping(Callback):\n-    def __init__(self, patience=1, verbose=0):\n+    def __init__(self, patience=0, verbose=0):\n         super(Callback, self).__init__()\n \n         self.patience = patience\n@@ -238,3 +238,29 @@ class EarlyStopping(Callback):\n                     print(\"Epoch %05d: early stopping\" % (epoch))\n                 self.model.stop_training = True\n             self.wait += 1\n+\n+\n+class RemoteMonitor(Callback):\n+    def __init__(self, root='http://localhost:9000'):\n+        self.root = root\n+        self.seen = 0\n+        self.tot_loss = 0.\n+        self.tot_accuracy = 0.\n+\n+    def on_epoch_begin(self, epoch, logs={}):\n+        self.seen = 0\n+        self.tot_loss = 0.\n+        self.tot_accuracy = 0.\n+\n+    def on_batch_end(self, batch, logs={}):\n+        batch_size = logs.get('size', 0)\n+        self.seen += batch_size\n+        self.tot_loss += logs.get('loss', 0.) * batch_size\n+        if self.params['show_accuracy']:\n+            self.tot_accuracy += logs.get('accuracy', 0.) * batch_size\n+\n+    def on_epoch_end(self, epoch, logs={}):\n+        import requests\n+        logs['epoch'] = epoch\n+        logs['loss'] = self.tot_loss / self.seen\n+        r = requests.post(self.root + '/publish/epoch/end/', {'data':json.dumps(logs)})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1fc3c4b22ec11ae098e0d3127346b08b1bc00a04", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 8 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n\n\n\n\n\n\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0a21abc8102771da76fca01800e497d7d014d10c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9763f811859791db876d69c35cd7adf065e10681", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dd82a3944eb449fd765d6186917e4d0be3a6bcf0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 9 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 9 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n\n\n\n\n\n\n\n\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
