{"custom_id": "scrapy#88bd067912ee94e2a6d2e1ba5b0d5db1241b7621", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 136 | Contributors (this commit): 8 | Commits (past 90d): 1 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -13,7 +13,7 @@ from scrapy.settings import Settings\n spider = Spider('foo')\n \n \n-class TestDefaultHeadersMiddleware(TestCase):\n+class TestHttpProxyMiddleware(TestCase):\n \n     failureException = AssertionError\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4740dca8f260bef83eed849b692b3a2c1aaec6cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 20 | Files Changed: 1 | Hunks: 11 | Methods Changed: 7 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 54 | Churn Cumulative: 188 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: 0.375\n\nDIFF:\n@@ -17,6 +17,11 @@ class SpiderMiddlewareManager(MiddlewareManager):\n \n     component_name = 'spider middleware'\n \n+    # List of dicts. Each dict represents a spider middleware and contains the\n+    # 'process_spider_output', 'process_spider_exception' methods.\n+    # The idea is to simulate the behaviour of a Twisted deferred's callback/errback chain\n+    output_methods = []\n+\n     @classmethod\n     def _get_mwlist_from_settings(cls, settings):\n         return build_component_list(settings.getwithbase('SPIDER_MIDDLEWARES'))\n@@ -25,12 +30,12 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         super(SpiderMiddlewareManager, self)._add_middleware(mw)\n         if hasattr(mw, 'process_spider_input'):\n             self.methods['process_spider_input'].append(mw.process_spider_input)\n-        if hasattr(mw, 'process_spider_output'):\n-            self.methods['process_spider_output'].insert(0, mw.process_spider_output)\n-        if hasattr(mw, 'process_spider_exception'):\n-            self.methods['process_spider_exception'].insert(0, mw.process_spider_exception)\n         if hasattr(mw, 'process_start_requests'):\n             self.methods['process_start_requests'].insert(0, mw.process_start_requests)\n+        self.output_methods.insert(0, dict(\n+            process_spider_output=getattr(mw, 'process_spider_output', None),\n+            process_spider_exception=getattr(mw, 'process_spider_exception', None),\n+        ))\n \n     def scrape_response(self, scrape_func, response, request, spider):\n         fname = lambda f:'%s.%s' % (\n@@ -48,45 +53,54 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                     return scrape_func(Failure(), request, spider)\n             return scrape_func(response, request, spider)\n \n-        def process_spider_exception(_failure):\n+        def process_spider_exception(_failure, mw_index):\n             exception = _failure.value\n             # don't handle _InvalidOutput exception\n             if isinstance(exception, _InvalidOutput):\n                 return _failure\n-            for method in self.methods['process_spider_exception']:\n-                result = method(response=response, exception=exception, spider=spider)\n-                if result is not None and not _isiterable(result):\n-                    raise _InvalidOutput('Middleware {} must return None or an iterable ' \\\n-                        'object, got {}'.format(fname(method), type(result)))\n+            for index, mw in enumerate(self.output_methods):\n+                if index < mw_index or mw['process_spider_exception'] is None:\n+                    continue\n+                result = mw['process_spider_exception'](response=response, exception=exception, spider=spider)\n+                mw_index += 1\n+                if _isiterable(result):\n                     # stop exception handling by handing control over to the\n                     # process_spider_output chain if an iterable has been returned\n-                if result is not None:\n-                    return result\n+                    return process_spider_output(result, mw_index)\n+                elif result is None:\n+                    continue\n+                else:\n+                    raise _InvalidOutput('Middleware {} must return None or an iterable, got {}' \\\n+                                         .format(fname(mw['process_spider_exception']), type(result)))\n             return _failure\n \n-        def process_spider_output(result):\n+        def process_spider_output(result, mw_index):\n             def wrapper(result_iterable):\n                 try:\n                     for r in result_iterable:\n                         yield r\n                 except Exception as ex:\n-                    exception_result = process_spider_exception(Failure(ex))\n+                    # process the exception with the method from the next middleware\n+                    exception_result = process_spider_exception(Failure(ex), mw_index)\n                     if exception_result is None or isinstance(exception_result, Failure):\n                         raise\n                     for output in exception_result:\n                         yield output\n-            for method in self.methods['process_spider_output']:\n-                result = method(response=response, result=result, spider=spider)\n+            for index, mw in enumerate(self.output_methods):\n+                if index < mw_index or mw['process_spider_output'] is None:\n+                    continue\n+                result = mw['process_spider_output'](response=response, result=result, spider=spider)\n+                mw_index += 1\n                 if _isiterable(result):\n                     result = wrapper(result)\n                 else:\n-                    raise _InvalidOutput('Middleware {} must return an iterable object, ' \\\n-                        'got {}'.format(fname(method), type(result)))\n+                    raise _InvalidOutput('Middleware {} must return an iterable, got {}' \\\n+                                         .format(fname(mw['process_spider_output']), type(result)))\n             return result\n \n         dfd = mustbe_deferred(process_spider_input, response)\n-        dfd.addErrback(process_spider_exception)\n-        dfd.addCallback(process_spider_output)\n+        dfd.addErrback(process_spider_exception, mw_index=0)\n+        dfd.addCallback(process_spider_output, mw_index=0)\n         return dfd\n \n     def process_start_requests(self, start_requests, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ba294351381c0dd81476603246d2cea6c31486be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 33 | Contributors (this commit): 5 | Commits (past 90d): 1 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,6 +19,9 @@ class OffsiteMiddleware(object):\n \n     def __init__(self, stats):\n         self.stats = stats\n+        # default values\n+        self.host_regex = re.compile('')  # allow all by default\n+        self.domains_seen = set()\n \n     @classmethod\n     def from_crawler(cls, crawler):\n@@ -52,7 +55,7 @@ class OffsiteMiddleware(object):\n         \"\"\"Override this method to implement a different offsite policy\"\"\"\n         allowed_domains = getattr(spider, 'allowed_domains', None)\n         if not allowed_domains:\n-            return re.compile('') # allow all by default\n+            return\n         url_pattern = re.compile(\"^https?://.*$\")\n         for domain in allowed_domains:\n             if url_pattern.match(domain):\n@@ -62,8 +65,9 @@ class OffsiteMiddleware(object):\n         return re.compile(regex)\n \n     def spider_opened(self, spider):\n-        self.host_regex = self.get_host_regex(spider)\n-        self.domains_seen = set()\n+        host_regex = self.get_host_regex(spider)\n+        if host_regex:\n+            self.host_regex = host_regex\n \n \n class URLWarning(Warning):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fac1b2f3516f3db6ce669664943336c08d95d0aa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 479 | Contributors (this commit): 10 | Commits (past 90d): 1 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -1,8 +1,8 @@\n-import os\n+import io\n import hashlib\n import random\n import warnings\n-from tempfile import mkdtemp, TemporaryFile\n+from tempfile import mkdtemp\n from shutil import rmtree\n \n from twisted.trial import unittest\n@@ -401,8 +401,9 @@ class ImagesPipelineTestCaseCustomSettings(unittest.TestCase):\n             self.assertEqual(getattr(pipeline_cls, pipe_attr.lower()),\n                              expected_value)\n \n+\n def _create_image(format, *a, **kw):\n-    buf = TemporaryFile()\n+    buf = io.BytesIO()\n     Image.new(*a, **kw).save(buf, format)\n     buf.seek(0)\n     return Image.open(buf)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#735de8167d3e6b0085710d406c8c2976913baa43", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 77 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 9 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 77 | Churn Cumulative: 77 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,77 @@\n+\n+import logging\n+\n+from testfixtures import LogCapture\n+from twisted.trial.unittest import TestCase\n+from twisted.internet import defer\n+\n+from scrapy import Spider, Request\n+from scrapy.utils.test import get_crawler\n+from tests.mockserver import MockServer\n+\n+\n+class CommonTestCase(TestCase):\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.mockserver = MockServer()\n+        cls.mockserver.__enter__()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.mockserver.__exit__(None, None, None)\n+\n+    @defer.inlineCallbacks\n+    def crawl_log(self, spider):\n+        crawler = get_crawler(spider)\n+        with LogCapture() as log:\n+            yield crawler.crawl()\n+        raise defer.returnValue(log)\n+\n+\n+class LogExceptionMiddleware(object):\n+    def process_spider_exception(self, response, exception, spider):\n+        logging.warn('Middleware: %s exception caught', exception.__class__.__name__)\n+        return None\n+\n+\n+# ================================================================================\n+# (1) exceptions from a spider middleware's process_spider_input method\n+class ProcessSpiderInputSpider(Spider):\n+    name = 'ProcessSpiderInputSpider'\n+    custom_settings = {\n+        'SPIDER_MIDDLEWARES': {\n+            # spider\n+            __name__ + '.LogExceptionMiddleware': 10,\n+            __name__ + '.FailProcessSpiderInputMiddleware': 8,\n+            __name__ + '.LogExceptionMiddleware': 6,\n+            # engine\n+        }\n+    }\n+\n+    def start_requests(self):\n+        yield Request('http://localhost:8998', callback=self.parse, errback=self.errback)\n+\n+    def parse(self, response):\n+        return [{'test': 1}, {'test': 2}]\n+\n+    def errback(self, failure):\n+        self.logger.warn('Got a Failure on the Request errback')\n+\n+\n+class FailProcessSpiderInputMiddleware:\n+    def process_spider_input(self, response, spider):\n+        logging.warn('Middleware: will raise ZeroDivisionError')\n+        raise ZeroDivisionError()\n+\n+\n+class TestProcessSpiderInputSpider(CommonTestCase):\n+    @defer.inlineCallbacks\n+    def test_process_spider_input_errback(self):\n+        \"\"\"\n+        (1) An exception from the process_spider_input chain should not be caught by the\n+        process_spider_exception chain, it should go directly to the Request errback\n+        \"\"\"\n+        log = yield self.crawl_log(ProcessSpiderInputSpider)\n+        self.assertNotIn('Middleware: ZeroDivisionError exception caught', str(log))\n+        self.assertIn('Middleware: will raise ZeroDivisionError', str(log))\n+        self.assertIn('Got a Failure on the Request errback', str(log))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6ed9440ed528ab5c5eece50512e19929e4320b42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 66 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 72 | Churn Cumulative: 149 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -60,11 +60,11 @@ class ProcessSpiderInputSpider(Spider):\n \n class FailProcessSpiderInputMiddleware:\n     def process_spider_input(self, response, spider):\n-        logging.warn('Middleware: will raise ZeroDivisionError')\n-        raise ZeroDivisionError()\n+        logging.warn('Middleware: will raise IndexError')\n+        raise IndexError()\n \n \n-class TestProcessSpiderInputSpider(CommonTestCase):\n+class TestProcessSpiderInput(CommonTestCase):\n     @defer.inlineCallbacks\n     def test_process_spider_input_errback(self):\n         \"\"\"\n@@ -72,6 +72,66 @@ class TestProcessSpiderInputSpider(CommonTestCase):\n         process_spider_exception chain, it should go directly to the Request errback\n         \"\"\"\n         log = yield self.crawl_log(ProcessSpiderInputSpider)\n-        self.assertNotIn('Middleware: ZeroDivisionError exception caught', str(log))\n-        self.assertIn('Middleware: will raise ZeroDivisionError', str(log))\n-        self.assertIn('Got a Failure on the Request errback', str(log))\n+        self.assertNotIn(\"Middleware: IndexError exception caught\", str(log))\n+        self.assertIn(\"Middleware: will raise IndexError\", str(log))\n+        self.assertIn(\"Got a Failure on the Request errback\", str(log))\n+\n+\n+# ================================================================================\n+# (2) exceptions from a spider callback (generator)\n+class GeneratorCallbackSpider(Spider):\n+    name = 'GeneratorCallbackSpider'\n+    start_urls = ['http://localhost:8998']\n+    custom_settings = {\n+        'SPIDER_MIDDLEWARES': {\n+            # spider\n+            __name__ + '.LogExceptionMiddleware': 10,\n+            # engine\n+        },\n+    }\n+\n+    def parse(self, response):\n+        yield {'test': 1}\n+        yield {'test': 2}\n+        raise ImportError()\n+\n+\n+class TestGeneratorCallback(CommonTestCase):\n+    @defer.inlineCallbacks\n+    def test_generator_callback(self):\n+        \"\"\"\n+        (2) An exception from a spider's callback should\n+        be caught by the process_spider_exception chain\n+        \"\"\"\n+        log = yield self.crawl_log(GeneratorCallbackSpider)\n+        self.assertIn(\"Middleware: ImportError exception caught\", str(log))\n+        self.assertIn(\"'item_scraped_count': 2\", str(log))\n+\n+\n+# ================================================================================\n+# (3) exceptions from a spider callback (not a generator)\n+class NotAGeneratorCallbackSpider(Spider):\n+    name = 'NotAGeneratorCallbackSpider'\n+    start_urls = ['http://localhost:8998']\n+    custom_settings = {\n+        'SPIDER_MIDDLEWARES': {\n+            # spider\n+            __name__ + '.LogExceptionMiddleware': 10,\n+            # engine\n+        },\n+    }\n+\n+    def parse(self, response):\n+        return [{'test': 1}, {'test': 1/0}]\n+\n+\n+class TestNotAGeneratorCallback(CommonTestCase):\n+    @defer.inlineCallbacks\n+    def test_not_a_generator_callback(self):\n+        \"\"\"\n+        (3) An exception from a spider's callback should\n+        be caught by the process_spider_exception chain\n+        \"\"\"\n+        log = yield self.crawl_log(NotAGeneratorCallbackSpider)\n+        self.assertIn(\"Middleware: ZeroDivisionError exception caught\", str(log))\n+        self.assertNotIn(\"item_scraped_count\", str(log))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4fca9aba851133fcdc12bb46c7ae229d9537079a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 85 | Lines Deleted: 52 | Files Changed: 1 | Hunks: 13 | Methods Changed: 9 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 137 | Churn Cumulative: 286 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,22 +10,8 @@ from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n \n \n-class CommonTestCase(TestCase):\n-    @classmethod\n-    def setUpClass(cls):\n-        cls.mockserver = MockServer()\n-        cls.mockserver.__enter__()\n-\n-    @classmethod\n-    def tearDownClass(cls):\n-        cls.mockserver.__exit__(None, None, None)\n-\n-    @defer.inlineCallbacks\n-    def crawl_log(self, spider):\n-        crawler = get_crawler(spider)\n-        with LogCapture() as log:\n-            yield crawler.crawl()\n-        raise defer.returnValue(log)\n+# TEST_URL = 'http://example.org'\n+TEST_URL = 'http://localhost:8998'\n \n \n class LogExceptionMiddleware(object):\n@@ -34,6 +20,32 @@ class LogExceptionMiddleware(object):\n         return None\n \n \n+# ================================================================================\n+# recover from an exception on a spider callback\n+class RecoverySpider(Spider):\n+    name = 'RecoverySpider'\n+    start_urls = [TEST_URL]\n+    custom_settings = {\n+        'SPIDER_MIDDLEWARES': {\n+            __name__ + '.RecoveryMiddleware': 10,\n+        },\n+    }\n+\n+    def parse(self, response):\n+        yield {'test': 1}\n+        self.logger.warn('DONT_FAIL: %s', response.meta.get('dont_fail'))\n+        if not response.meta.get('dont_fail'):\n+            raise ModuleNotFoundError()\n+\n+class RecoveryMiddleware(object):\n+    def process_spider_exception(self, response, exception, spider):\n+        logging.warn('Middleware: %s exception caught', exception.__class__.__name__)\n+        return [\n+            {'from': 'process_spider_exception'},\n+            Request(response.url, meta={'dont_fail': True}, dont_filter=True),\n+        ]\n+\n+\n # ================================================================================\n # (1) exceptions from a spider middleware's process_spider_input method\n class ProcessSpiderInputSpider(Spider):\n@@ -49,7 +61,7 @@ class ProcessSpiderInputSpider(Spider):\n     }\n \n     def start_requests(self):\n-        yield Request('http://localhost:8998', callback=self.parse, errback=self.errback)\n+        yield Request(TEST_URL, callback=self.parse, errback=self.errback)\n \n     def parse(self, response):\n         return [{'test': 1}, {'test': 2}]\n@@ -64,29 +76,14 @@ class FailProcessSpiderInputMiddleware:\n         raise IndexError()\n \n \n-class TestProcessSpiderInput(CommonTestCase):\n-    @defer.inlineCallbacks\n-    def test_process_spider_input_errback(self):\n-        \"\"\"\n-        (1) An exception from the process_spider_input chain should not be caught by the\n-        process_spider_exception chain, it should go directly to the Request errback\n-        \"\"\"\n-        log = yield self.crawl_log(ProcessSpiderInputSpider)\n-        self.assertNotIn(\"Middleware: IndexError exception caught\", str(log))\n-        self.assertIn(\"Middleware: will raise IndexError\", str(log))\n-        self.assertIn(\"Got a Failure on the Request errback\", str(log))\n-\n-\n # ================================================================================\n # (2) exceptions from a spider callback (generator)\n class GeneratorCallbackSpider(Spider):\n     name = 'GeneratorCallbackSpider'\n-    start_urls = ['http://localhost:8998']\n+    start_urls = [TEST_URL]\n     custom_settings = {\n         'SPIDER_MIDDLEWARES': {\n-            # spider\n             __name__ + '.LogExceptionMiddleware': 10,\n-            # engine\n         },\n     }\n \n@@ -96,28 +93,14 @@ class GeneratorCallbackSpider(Spider):\n         raise ImportError()\n \n \n-class TestGeneratorCallback(CommonTestCase):\n-    @defer.inlineCallbacks\n-    def test_generator_callback(self):\n-        \"\"\"\n-        (2) An exception from a spider's callback should\n-        be caught by the process_spider_exception chain\n-        \"\"\"\n-        log = yield self.crawl_log(GeneratorCallbackSpider)\n-        self.assertIn(\"Middleware: ImportError exception caught\", str(log))\n-        self.assertIn(\"'item_scraped_count': 2\", str(log))\n-\n-\n # ================================================================================\n # (3) exceptions from a spider callback (not a generator)\n class NotAGeneratorCallbackSpider(Spider):\n     name = 'NotAGeneratorCallbackSpider'\n-    start_urls = ['http://localhost:8998']\n+    start_urls = [TEST_URL]\n     custom_settings = {\n         'SPIDER_MIDDLEWARES': {\n-            # spider\n             __name__ + '.LogExceptionMiddleware': 10,\n-            # engine\n         },\n     }\n \n@@ -125,13 +108,63 @@ class NotAGeneratorCallbackSpider(Spider):\n         return [{'test': 1}, {'test': 1/0}]\n \n \n-class TestNotAGeneratorCallback(CommonTestCase):\n+# ================================================================================\n+class TestSpiderMiddleware(TestCase):\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.mockserver = MockServer()\n+        cls.mockserver.__enter__()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.mockserver.__exit__(None, None, None)\n+    \n+    @defer.inlineCallbacks\n+    def crawl_log(self, spider):\n+        crawler = get_crawler(spider)\n+        with LogCapture() as log:\n+            yield crawler.crawl()\n+        raise defer.returnValue(log)\n+\n+    # @defer.inlineCallbacks\n+    # def test_recovery(self):\n+    #     \"\"\"\n+    #     Recover from an exception from a spider's callback. The final item count should be 3\n+    #     (one from the spider before raising the exception, one from the middleware and one\n+    #     from the spider when processing the response that was enqueued from the middleware)\n+    #     \"\"\"\n+    #     log = yield self.crawl_log(RecoverySpider)\n+    #     self.assertIn(\"Middleware: ModuleNotFoundError exception caught\", str(log))\n+    #     self.assertEqual(str(log).count(\"Middleware: ModuleNotFoundError exception caught\"), 1)\n+    #     self.assertIn(\"'item_scraped_count': 3\", str(log))\n+\n+    @defer.inlineCallbacks\n+    def test_process_spider_input_errback(self):\n+        \"\"\"\n+        (1) An exception from the process_spider_input chain should not be caught by the\n+        process_spider_exception chain, it should go directly to the Request errback\n+        \"\"\"\n+        log1 = yield self.crawl_log(ProcessSpiderInputSpider)\n+        self.assertNotIn(\"Middleware: IndexError exception caught\", str(log1))\n+        self.assertIn(\"Middleware: will raise IndexError\", str(log1))\n+        self.assertIn(\"Got a Failure on the Request errback\", str(log1))\n+    \n+    @defer.inlineCallbacks\n+    def test_generator_callback(self):\n+        \"\"\"\n+        (2) An exception from a spider's callback should\n+        be caught by the process_spider_exception chain\n+        \"\"\"\n+        log2 = yield self.crawl_log(GeneratorCallbackSpider)\n+        self.assertIn(\"Middleware: ImportError exception caught\", str(log2))\n+        self.assertIn(\"'item_scraped_count': 2\", str(log2))\n+    \n     @defer.inlineCallbacks\n     def test_not_a_generator_callback(self):\n         \"\"\"\n         (3) An exception from a spider's callback should\n         be caught by the process_spider_exception chain\n         \"\"\"\n-        log = yield self.crawl_log(NotAGeneratorCallbackSpider)\n-        self.assertIn(\"Middleware: ZeroDivisionError exception caught\", str(log))\n-        self.assertNotIn(\"item_scraped_count\", str(log))\n+        log3 = yield self.crawl_log(NotAGeneratorCallbackSpider)\n+        self.assertIn(\"Middleware: ZeroDivisionError exception caught\", str(log3))\n+        self.assertNotIn(\"item_scraped_count\", str(log3))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#985ab636cfa0825f100b02583bcfd106d1f4cef6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 25 | Files Changed: 1 | Hunks: 13 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 43 | Churn Cumulative: 231 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -17,11 +17,6 @@ class SpiderMiddlewareManager(MiddlewareManager):\n \n     component_name = 'spider middleware'\n \n-    # List of dicts. Each dict represents a spider middleware and contains the\n-    # 'process_spider_output', 'process_spider_exception' methods.\n-    # The idea is to simulate the behaviour of a Twisted deferred's callback/errback chain\n-    output_methods = []\n-\n     @classmethod\n     def _get_mwlist_from_settings(cls, settings):\n         return build_component_list(settings.getwithbase('SPIDER_MIDDLEWARES'))\n@@ -32,10 +27,8 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             self.methods['process_spider_input'].append(mw.process_spider_input)\n         if hasattr(mw, 'process_start_requests'):\n             self.methods['process_start_requests'].insert(0, mw.process_start_requests)\n-        self.output_methods.insert(0, dict(\n-            process_spider_output=getattr(mw, 'process_spider_output', None),\n-            process_spider_exception=getattr(mw, 'process_spider_exception', None),\n-        ))\n+        self.methods['process_spider_output'].insert(0, getattr(mw, 'process_spider_output', None))\n+        self.methods['process_spider_exception'].insert(0, getattr(mw, 'process_spider_exception', None))\n \n     def scrape_response(self, scrape_func, response, request, spider):\n         fname = lambda f:'%s.%s' % (\n@@ -53,54 +46,54 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                     return scrape_func(Failure(), request, spider)\n             return scrape_func(response, request, spider)\n \n-        def process_spider_exception(_failure, mw_index):\n+        def process_spider_exception(_failure, index):\n             exception = _failure.value\n             # don't handle _InvalidOutput exception\n             if isinstance(exception, _InvalidOutput):\n                 return _failure\n-            for index, mw in enumerate(self.output_methods):\n-                if index < mw_index or mw['process_spider_exception'] is None:\n+            for i, method in enumerate(self.methods['process_spider_exception']):\n+                if i < index or method is None:\n                     continue\n-                result = mw['process_spider_exception'](response=response, exception=exception, spider=spider)\n-                mw_index += 1\n+                result = method(response=response, exception=exception, spider=spider)\n+                index += 1\n                 if _isiterable(result):\n                     # stop exception handling by handing control over to the\n                     # process_spider_output chain if an iterable has been returned\n-                    return process_spider_output(result, mw_index)\n+                    return process_spider_output(result, index)\n                 elif result is None:\n                     continue\n                 else:\n                     raise _InvalidOutput('Middleware {} must return None or an iterable, got {}' \\\n-                                         .format(fname(mw['process_spider_exception']), type(result)))\n+                                         .format(fname(method), type(result)))\n             return _failure\n \n-        def process_spider_output(result, mw_index):\n+        def process_spider_output(result, index):\n             def wrapper(result_iterable):\n                 try:\n                     for r in result_iterable:\n                         yield r\n                 except Exception as ex:\n                     # process the exception with the method from the next middleware\n-                    exception_result = process_spider_exception(Failure(ex), mw_index)\n+                    exception_result = process_spider_exception(Failure(ex), index)\n                     if exception_result is None or isinstance(exception_result, Failure):\n                         raise\n                     for output in exception_result:\n                         yield output\n-            for index, mw in enumerate(self.output_methods):\n-                if index < mw_index or mw['process_spider_output'] is None:\n+            for i, method in enumerate(self.methods['process_spider_output']):\n+                if i < index or method is None:\n                     continue\n-                result = mw['process_spider_output'](response=response, result=result, spider=spider)\n-                mw_index += 1\n+                result = method(response=response, result=result, spider=spider)\n+                index += 1\n                 if _isiterable(result):\n                     result = wrapper(result)\n                 else:\n                     raise _InvalidOutput('Middleware {} must return an iterable, got {}' \\\n-                                         .format(fname(mw['process_spider_output']), type(result)))\n+                                         .format(fname(method), type(result)))\n             return result\n \n         dfd = mustbe_deferred(process_spider_input, response)\n-        dfd.addErrback(process_spider_exception, mw_index=0)\n-        dfd.addCallback(process_spider_output, mw_index=0)\n+        dfd.addErrback(process_spider_exception, index=0)\n+        dfd.addCallback(process_spider_output, index=0)\n         return dfd\n \n     def process_start_requests(self, start_requests, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d05c8677c5079b88db4405fe2bed83dc437c9204", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 59 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -1,14 +1,13 @@\n import base64\n+from six.moves.urllib.parse import unquote, urlunparse\n from six.moves.urllib.request import getproxies, proxy_bypass\n-from six.moves.urllib.parse import unquote\n try:\n     from urllib2 import _parse_proxy\n except ImportError:\n     from urllib.request import _parse_proxy\n-from six.moves.urllib.parse import urlunparse\n \n-from scrapy.utils.httpobj import urlparse_cached\n from scrapy.exceptions import NotConfigured\n+from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_bytes\n \n \n@@ -17,8 +16,8 @@ class HttpProxyMiddleware(object):\n     def __init__(self, auth_encoding='latin-1'):\n         self.auth_encoding = auth_encoding\n         self.proxies = {}\n-        for type, url in getproxies().items():\n-            self.proxies[type] = self._get_proxy(url, type)\n+        for type_, url in getproxies().items():\n+            self.proxies[type_] = self._get_proxy(url, type_)\n \n     @classmethod\n     def from_crawler(cls, crawler):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#74ce1561542dac9be5d1363a4a3e623653855e05", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 9 | Methods Changed: 7 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 41 | Churn Cumulative: 1203 | Contributors (this commit): 18 | Commits (past 90d): 4 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -232,6 +232,9 @@ class Scraper(object):\n                 logger.error('Error processing %(item)s', {'item': item},\n                              exc_info=failure_to_exc_info(output),\n                              extra={'spider': spider})\n+                return self.signals.send_catch_log_deferred(\n+                    signal=signals.item_error, item=item, response=response,\n+                    spider=spider, failure=output)\n         else:\n             logkws = self.logformatter.scraped(output, response, spider)\n             logger.log(*logformatter_adapter(logkws), extra={'spider': spider})\n\n@@ -17,6 +17,7 @@ response_received = object()\n response_downloaded = object()\n item_scraped = object()\n item_dropped = object()\n+item_error = object()\n \n # for backwards compatibility\n stats_spider_opened = spider_opened\n\n@@ -9,3 +9,9 @@ class ZeroDivisionErrorPipeline(object):\n \n     def process_item(self, item, spider):\n         return item\n+\n+\n+class ProcessWithZeroDivisionErrorPipiline(object):\n+\n+    def process_item(self, item, spider):\n+        1/0\n\n@@ -74,6 +74,14 @@ class DictItemsSpider(TestSpider):\n     item_cls = dict\n \n \n+class ItemZeroDivisionErrorSpider(TestSpider):\n+    custom_settings = {\n+        \"ITEM_PIPELINES\": {\n+            \"tests.pipelines.ProcessWithZeroDivisionErrorPipiline\": 300,\n+        }\n+    }\n+\n+\n def start_test_site(debug=False):\n     root_dir = os.path.join(tests_datadir, \"test_site\")\n     r = static.File(root_dir)\n@@ -95,6 +103,7 @@ class CrawlerRun(object):\n         self.respplug = []\n         self.reqplug = []\n         self.reqdropped = []\n+        self.itemerror = []\n         self.itemresp = []\n         self.signals_catched = {}\n         self.spider_class = spider_class\n@@ -112,6 +121,7 @@ class CrawlerRun(object):\n \n         self.crawler = get_crawler(self.spider_class)\n         self.crawler.signals.connect(self.item_scraped, signals.item_scraped)\n+        self.crawler.signals.connect(self.item_error, signals.item_error)\n         self.crawler.signals.connect(self.request_scheduled, signals.request_scheduled)\n         self.crawler.signals.connect(self.request_dropped, signals.request_dropped)\n         self.crawler.signals.connect(self.response_downloaded, signals.response_downloaded)\n@@ -136,6 +146,9 @@ class CrawlerRun(object):\n         u = urlparse(url)\n         return u.path\n \n+    def item_error(self, item, response, spider, failure):\n+        self.itemerror.append((item, response, spider, failure))\n+\n     def item_scraped(self, item, spider, response):\n         self.itemresp.append((item, response))\n \n@@ -175,6 +188,10 @@ class EngineTest(unittest.TestCase):\n         self._assert_scheduled_requests(urls_to_visit=7)\n         self._assert_dropped_requests()\n \n+        self.run = CrawlerRun(ItemZeroDivisionErrorSpider)\n+        yield self.run.run()\n+        self._assert_items_error()\n+\n     def _assert_visited_urls(self):\n         must_be_visited = [\"/\", \"/redirect\", \"/redirected\",\n                            \"/item1.html\", \"/item2.html\", \"/item999.html\"]\n@@ -209,6 +226,20 @@ class EngineTest(unittest.TestCase):\n             if self.run.getpath(response.url) == '/redirect':\n                 self.assertEqual(302, response.status)\n \n+    def _assert_items_error(self):\n+        self.assertEqual(2, len(self.run.itemerror))\n+        for item, response, spider, failure in self.run.itemerror:\n+            self.assertEqual(failure.value.__class__, ZeroDivisionError)\n+            self.assertEqual(spider, self.run.spider)\n+\n+            self.assertEqual(item['url'], response.url)\n+            if 'item1.html' in item['url']:\n+                self.assertEqual('Item 1 name', item['name'])\n+                self.assertEqual('100', item['price'])\n+            if 'item2.html' in item['url']:\n+                self.assertEqual('Item 2 name', item['name'])\n+                self.assertEqual('200', item['price'])\n+\n     def _assert_scraped_items(self):\n         self.assertEqual(2, len(self.run.itemresp))\n         for item, response in self.run.itemresp:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6f5c39d65f3e5d74c292f2143b4f84c7f43b155a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 219 | Contributors (this commit): 9 | Commits (past 90d): 1 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -214,7 +214,8 @@ class CsvItemExporter(BaseItemExporter):\n             file,\n             line_buffering=False,\n             write_through=True,\n-            encoding=self.encoding\n+            encoding=self.encoding,\n+            newline='' # Windows needs this https://github.com/scrapy/scrapy/issues/3034\n         ) if six.PY3 else file\n         self.csv_writer = csv.writer(self.stream, **kwargs)\n         self._headers_not_written = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0b2870634af6cc14191faafcba7d58a5f3cc3016", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 290 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -14,7 +14,7 @@ from tests.mockserver import MockServer\n TEST_URL = 'http://localhost:8998'\n \n \n-class LogExceptionMiddleware(object):\n+class LogExceptionMiddleware:\n     def process_spider_exception(self, response, exception, spider):\n         logging.warn('Middleware: %s exception caught', exception.__class__.__name__)\n         return None\n@@ -37,7 +37,7 @@ class RecoverySpider(Spider):\n         if not response.meta.get('dont_fail'):\n             raise ModuleNotFoundError()\n \n-class RecoveryMiddleware(object):\n+class RecoveryMiddleware:\n     def process_spider_exception(self, response, exception, spider):\n         logging.warn('Middleware: %s exception caught', exception.__class__.__name__)\n         return [\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0a0e62272e67aaebe29666017e9b0623b81bf369", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 115 | Lines Deleted: 454 | Files Changed: 2 | Hunks: 18 | Methods Changed: 28 | Complexity Δ (Sum/Max): -45/0 | Churn Δ: 569 | Churn Cumulative: 1326 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,170 +0,0 @@\n-\n-import logging\n-\n-from testfixtures import LogCapture\n-from twisted.trial.unittest import TestCase\n-from twisted.internet import defer\n-\n-from scrapy import Spider, Request\n-from scrapy.utils.test import get_crawler\n-from tests.mockserver import MockServer\n-\n-\n-# TEST_URL = 'http://example.org'\n-TEST_URL = 'http://localhost:8998'\n-\n-\n-class LogExceptionMiddleware:\n-    def process_spider_exception(self, response, exception, spider):\n-        logging.warn('Middleware: %s exception caught', exception.__class__.__name__)\n-        return None\n-\n-\n-# ================================================================================\n-# recover from an exception on a spider callback\n-class RecoverySpider(Spider):\n-    name = 'RecoverySpider'\n-    start_urls = [TEST_URL]\n-    custom_settings = {\n-        'SPIDER_MIDDLEWARES': {\n-            __name__ + '.RecoveryMiddleware': 10,\n-        },\n-    }\n-\n-    def parse(self, response):\n-        yield {'test': 1}\n-        self.logger.warn('DONT_FAIL: %s', response.meta.get('dont_fail'))\n-        if not response.meta.get('dont_fail'):\n-            raise ModuleNotFoundError()\n-\n-class RecoveryMiddleware:\n-    def process_spider_exception(self, response, exception, spider):\n-        logging.warn('Middleware: %s exception caught', exception.__class__.__name__)\n-        return [\n-            {'from': 'process_spider_exception'},\n-            Request(response.url, meta={'dont_fail': True}, dont_filter=True),\n-        ]\n-\n-\n-# ================================================================================\n-# (1) exceptions from a spider middleware's process_spider_input method\n-class ProcessSpiderInputSpider(Spider):\n-    name = 'ProcessSpiderInputSpider'\n-    custom_settings = {\n-        'SPIDER_MIDDLEWARES': {\n-            # spider\n-            __name__ + '.LogExceptionMiddleware': 10,\n-            __name__ + '.FailProcessSpiderInputMiddleware': 8,\n-            __name__ + '.LogExceptionMiddleware': 6,\n-            # engine\n-        }\n-    }\n-\n-    def start_requests(self):\n-        yield Request(TEST_URL, callback=self.parse, errback=self.errback)\n-\n-    def parse(self, response):\n-        return [{'test': 1}, {'test': 2}]\n-\n-    def errback(self, failure):\n-        self.logger.warn('Got a Failure on the Request errback')\n-\n-\n-class FailProcessSpiderInputMiddleware:\n-    def process_spider_input(self, response, spider):\n-        logging.warn('Middleware: will raise IndexError')\n-        raise IndexError()\n-\n-\n-# ================================================================================\n-# (2) exceptions from a spider callback (generator)\n-class GeneratorCallbackSpider(Spider):\n-    name = 'GeneratorCallbackSpider'\n-    start_urls = [TEST_URL]\n-    custom_settings = {\n-        'SPIDER_MIDDLEWARES': {\n-            __name__ + '.LogExceptionMiddleware': 10,\n-        },\n-    }\n-\n-    def parse(self, response):\n-        yield {'test': 1}\n-        yield {'test': 2}\n-        raise ImportError()\n-\n-\n-# ================================================================================\n-# (3) exceptions from a spider callback (not a generator)\n-class NotAGeneratorCallbackSpider(Spider):\n-    name = 'NotAGeneratorCallbackSpider'\n-    start_urls = [TEST_URL]\n-    custom_settings = {\n-        'SPIDER_MIDDLEWARES': {\n-            __name__ + '.LogExceptionMiddleware': 10,\n-        },\n-    }\n-\n-    def parse(self, response):\n-        return [{'test': 1}, {'test': 1/0}]\n-\n-\n-# ================================================================================\n-class TestSpiderMiddleware(TestCase):\n-    @classmethod\n-    def setUpClass(cls):\n-        cls.mockserver = MockServer()\n-        cls.mockserver.__enter__()\n-\n-    @classmethod\n-    def tearDownClass(cls):\n-        cls.mockserver.__exit__(None, None, None)\n-    \n-    @defer.inlineCallbacks\n-    def crawl_log(self, spider):\n-        crawler = get_crawler(spider)\n-        with LogCapture() as log:\n-            yield crawler.crawl()\n-        raise defer.returnValue(log)\n-\n-    # @defer.inlineCallbacks\n-    # def test_recovery(self):\n-    #     \"\"\"\n-    #     Recover from an exception from a spider's callback. The final item count should be 3\n-    #     (one from the spider before raising the exception, one from the middleware and one\n-    #     from the spider when processing the response that was enqueued from the middleware)\n-    #     \"\"\"\n-    #     log = yield self.crawl_log(RecoverySpider)\n-    #     self.assertIn(\"Middleware: ModuleNotFoundError exception caught\", str(log))\n-    #     self.assertEqual(str(log).count(\"Middleware: ModuleNotFoundError exception caught\"), 1)\n-    #     self.assertIn(\"'item_scraped_count': 3\", str(log))\n-\n-    @defer.inlineCallbacks\n-    def test_process_spider_input_errback(self):\n-        \"\"\"\n-        (1) An exception from the process_spider_input chain should not be caught by the\n-        process_spider_exception chain, it should go directly to the Request errback\n-        \"\"\"\n-        log1 = yield self.crawl_log(ProcessSpiderInputSpider)\n-        self.assertNotIn(\"Middleware: IndexError exception caught\", str(log1))\n-        self.assertIn(\"Middleware: will raise IndexError\", str(log1))\n-        self.assertIn(\"Got a Failure on the Request errback\", str(log1))\n-    \n-    @defer.inlineCallbacks\n-    def test_generator_callback(self):\n-        \"\"\"\n-        (2) An exception from a spider's callback should\n-        be caught by the process_spider_exception chain\n-        \"\"\"\n-        log2 = yield self.crawl_log(GeneratorCallbackSpider)\n-        self.assertIn(\"Middleware: ImportError exception caught\", str(log2))\n-        self.assertIn(\"'item_scraped_count': 2\", str(log2))\n-    \n-    @defer.inlineCallbacks\n-    def test_not_a_generator_callback(self):\n-        \"\"\"\n-        (3) An exception from a spider's callback should\n-        be caught by the process_spider_exception chain\n-        \"\"\"\n-        log3 = yield self.crawl_log(NotAGeneratorCallbackSpider)\n-        self.assertIn(\"Middleware: ZeroDivisionError exception caught\", str(log3))\n-        self.assertNotIn(\"item_scraped_count\", str(log3))\n\n@@ -5,256 +5,111 @@ from testfixtures import LogCapture\n from twisted.trial.unittest import TestCase\n from twisted.internet import defer\n \n-from scrapy.spiders import Spider\n-from scrapy.item import Item, Field\n-from scrapy.http import Request\n+from scrapy import Spider, Request\n from scrapy.utils.test import get_crawler\n from tests.mockserver import MockServer\n \n \n-class TestItem(Item):\n-    value = Field()\n+# TEST_URL = 'http://example.org'\n+TEST_URL = 'http://localhost:8998'\n \n \n-class LocalhostSpider(Spider):\n-    start_urls = ['http://localhost:8998']  # tests.mockserver.MockServer\n-\n-\n-# ================================================================================\n-# exceptions from a spider's parse method\n-class BaseExceptionFromParseMethodSpider(LocalhostSpider):\n-    custom_settings = {\n-        'SPIDER_MIDDLEWARES': {'tests.test_spidermiddleware.CatchExceptionMiddleware': 540}\n-    }\n-\n-\n-class NotAGeneratorSpider(BaseExceptionFromParseMethodSpider):\n-    \"\"\" return value is NOT a generator \"\"\"\n-    name = 'not_a_generator'\n-\n-    def parse(self, response):\n-        raise AssertionError\n-\n-\n-class GeneratorErrorBeforeItemsSpider(BaseExceptionFromParseMethodSpider):\n-    \"\"\" return value is a generator; the exception is raised\n-    before the items are yielded: no items should be scraped \"\"\"\n-    name = 'generator_error_before_items'\n-\n-    def parse(self, response):\n-        raise ValueError\n-        for i in range(3):\n-            yield {'value': i}\n-\n-\n-class GeneratorErrorAfterItemsSpider(BaseExceptionFromParseMethodSpider):\n-    \"\"\" return value is a generator; the exception is raised\n-    after the items are yielded: 3 items should be scraped \"\"\"\n-    name = 'generator_error_after_items'\n-\n-    def parse(self, response):\n-        for i in range(3):\n-            yield {'value': i}\n-        raise FloatingPointError\n-\n-\n-class CatchExceptionMiddleware(object):\n+class LogExceptionMiddleware:\n     def process_spider_exception(self, response, exception, spider):\n-        \"\"\" catch an exception and log it \"\"\"\n-        logging.warn('{} exception caught'.format(exception.__class__.__name__))\n+        logging.warn('Middleware: %s exception caught', exception.__class__.__name__)\n         return None\n \n \n # ================================================================================\n-# exception from a previous middleware's process_spider_input method\n-# process_spider_input is not expected to return an iterable, so there are no\n-# separate tests for generator/non-generator implementations\n-class FromPreviousMiddlewareInputSpider(LocalhostSpider):\n-    name = 'not_a_generator_from_previous_middleware_input'\n+# recover from an exception on a spider callback\n+class RecoverySpider(Spider):\n+    name = 'RecoverySpider'\n+    start_urls = [TEST_URL]\n     custom_settings = {\n         'SPIDER_MIDDLEWARES': {\n-            # engine side\n-            'tests.test_spidermiddleware.CatchExceptionMiddleware': 540,\n-            'tests.test_spidermiddleware.RaiseExceptionOnInputMiddleware': 545,\n-            # spider side\n-        }\n-    }\n-\n-    def parse(self, response):\n-        return None\n-\n-\n-class RaiseExceptionOnInputMiddleware(object):\n-    def process_spider_input(self, response, spider):\n-        raise LookupError\n-\n-\n-# ================================================================================\n-# exception from a previous middleware's process_spider_output method (not a generator)\n-class NotAGeneratorFromPreviousMiddlewareOutputSpider(LocalhostSpider):\n-    name = 'not_a_generator_from_previous_middleware_output'\n-    custom_settings = {\n-        'SPIDER_MIDDLEWARES': {\n-            # engine side\n-            'tests.test_spidermiddleware.CatchExceptionMiddleware': 540,\n-            'tests.test_spidermiddleware.RaiseExceptionOnOutputNotAGeneratorMiddleware': 545,\n-            # spider side\n-        }\n-    }\n-\n-    def parse(self, response):\n-        return [{'value': i} for i in range(3)]\n-\n-\n-class RaiseExceptionOnOutputNotAGeneratorMiddleware(object):\n-    def process_spider_output(self, response, result, spider):\n-        raise UnicodeError\n-\n-\n-# ================================================================================\n-# exception from a previous middleware's process_spider_output method (generator)\n-class GeneratorFromPreviousMiddlewareOutputSpider(LocalhostSpider):\n-    name = 'generator_from_previous_middleware_output'\n-    custom_settings = {\n-        'SPIDER_MIDDLEWARES': {\n-            # engine side\n-            'tests.test_spidermiddleware.CatchExceptionMiddleware': 540,\n-            'tests.test_spidermiddleware.RaiseExceptionOnOutputGeneratorMiddleware': 545,\n-            # spider side\n-        }\n-    }\n-\n-    def parse(self, response):\n-        return [{'value': i} for i in range(10, 13)]\n-\n-\n-class RaiseExceptionOnOutputGeneratorMiddleware(object):\n-    def process_spider_output(self, response, result, spider):\n-        for r in result:\n-            yield r\n-        raise NameError\n-\n-\n-# ================================================================================\n-# do something useful from the exception handler\n-class DoSomethingSpider(LocalhostSpider):\n-    name = 'do_something'\n-    custom_settings = {\n-        'SPIDER_MIDDLEWARES': {\n-            # engine side\n-            'tests.test_spidermiddleware.DoSomethingMiddleware': 540,\n-            'tests.test_spidermiddleware.CatchExceptionMiddleware': 545,\n-            # spider side\n-        }\n-    }\n-\n-    def parse(self, response):\n-        yield {'value': response.url}\n-        raise ImportError\n-\n-\n-class DoSomethingMiddleware(object):\n-    def process_spider_exception(self, response, exception, spider):\n-        return [Request('http://localhost:8998?processed=true'), {'value': 10}, TestItem(value='asdf')]\n-\n-\n-# ================================================================================\n-# don't catch _InvalidOutput from scrapy's spider middleware manager\n-class InvalidReturnValueFromPreviousMiddlewareInputSpider(LocalhostSpider):\n-    name = 'invalid_return_value_from_previous_middleware_input'\n-    custom_settings = {\n-        'SPIDER_MIDDLEWARES': {\n-            # engine side\n-            'tests.test_spidermiddleware.InvalidReturnValueInputMiddleware': 540,\n-            'tests.test_spidermiddleware.CatchExceptionMiddleware': 545,\n-            # spider side\n-        }\n-    }\n-\n-    def parse(self, response):\n-        return None\n-\n-\n-class InvalidReturnValueInputMiddleware(object):\n-    def process_spider_input(self, response, spider):\n-        return 1.0  # <type 'float'>, not None\n-\n-\n-class InvalidReturnValueFromPreviousMiddlewareOutputSpider(LocalhostSpider):\n-    name = 'invalid_return_value_from_previous_middleware_output'\n-    custom_settings = {\n-        'SPIDER_MIDDLEWARES': {\n-            # engine side\n-            'tests.test_spidermiddleware.CatchExceptionMiddleware': 540,\n-            'tests.test_spidermiddleware.InvalidReturnValueOutputMiddleware': 545,\n-            # spider side\n-        }\n-    }\n-\n-    def parse(self, response):\n-        return None\n-\n-\n-class InvalidReturnValueOutputMiddleware(object):\n-    def process_spider_output(self, response, result, spider):\n-        return 1  # <type 'int'>, not an iterable\n-\n-\n-# ================================================================================\n-# make sure only non already called process_spider_output methods\n-# are called if process_spider_exception returns an iterable\n-class ExecutionChainSpider(LocalhostSpider):\n-    name = 'execution_chain'\n-    custom_settings = {\n-        'SPIDER_MIDDLEWARES': {\n-            # engine side\n-            'tests.test_spidermiddleware.ThirdMiddleware': 540,\n-            'tests.test_spidermiddleware.SecondMiddleware': 541,\n-            'tests.test_spidermiddleware.FirstMiddleware': 542\n-            # spider side\n+            __name__ + '.RecoveryMiddleware': 10,\n         },\n     }\n \n     def parse(self, response):\n-        return None\n-\n-\n-class FirstMiddleware(object):\n-    def process_spider_output(self, response, result, spider):\n-        for r in result:\n-            if isinstance(r, dict):\n-                r['handled_by_first_middleware'] = True\n-            yield r\n+        yield {'test': 1}\n+        self.logger.warn('DONT_FAIL: %s', response.meta.get('dont_fail'))\n+        if not response.meta.get('dont_fail'):\n+            raise ModuleNotFoundError()\n \n+class RecoveryMiddleware:\n     def process_spider_exception(self, response, exception, spider):\n-        # log exception, handle control to the next middleware's process_spider_exception\n-        logging.warn('{} exception caught'.format(exception.__class__.__name__))\n-        return None\n+        logging.warn('Middleware: %s exception caught', exception.__class__.__name__)\n+        return [\n+            {'from': 'process_spider_exception'},\n+            Request(response.url, meta={'dont_fail': True}, dont_filter=True),\n+        ]\n \n \n-class SecondMiddleware(object):\n-    def process_spider_output(self, response, result, spider):\n-        for r in result:\n-            if isinstance(r, dict):\n-                r['handled_by_second_middleware'] = True\n-            yield r\n-        raise MemoryError\n+# ================================================================================\n+# (1) exceptions from a spider middleware's process_spider_input method\n+class ProcessSpiderInputSpider(Spider):\n+    name = 'ProcessSpiderInputSpider'\n+    custom_settings = {\n+        'SPIDER_MIDDLEWARES': {\n+            # spider\n+            __name__ + '.LogExceptionMiddleware': 10,\n+            __name__ + '.FailProcessSpiderInputMiddleware': 8,\n+            __name__ + '.LogExceptionMiddleware': 6,\n+            # engine\n+        }\n+    }\n+\n+    def start_requests(self):\n+        yield Request(TEST_URL, callback=self.parse, errback=self.errback)\n+\n+    def parse(self, response):\n+        return [{'test': 1}, {'test': 2}]\n+\n+    def errback(self, failure):\n+        self.logger.warn('Got a Failure on the Request errback')\n \n \n-class ThirdMiddleware(object):\n-    def process_spider_output(self, response, result, spider):\n-        for r in result:\n-            if isinstance(r, dict):\n-                r['handled_by_third_middleware'] = True\n-            yield r\n-\n-    def process_spider_exception(self, response, exception, spider):\n-        # handle control to the next middleware's process_spider_output\n-        return [{'item': i} for i in range(3)]\n+class FailProcessSpiderInputMiddleware:\n+    def process_spider_input(self, response, spider):\n+        logging.warn('Middleware: will raise IndexError')\n+        raise IndexError()\n \n \n+# ================================================================================\n+# (2) exceptions from a spider callback (generator)\n+class GeneratorCallbackSpider(Spider):\n+    name = 'GeneratorCallbackSpider'\n+    start_urls = [TEST_URL]\n+    custom_settings = {\n+        'SPIDER_MIDDLEWARES': {\n+            __name__ + '.LogExceptionMiddleware': 10,\n+        },\n+    }\n+\n+    def parse(self, response):\n+        yield {'test': 1}\n+        yield {'test': 2}\n+        raise ImportError()\n+\n+\n+# ================================================================================\n+# (3) exceptions from a spider callback (not a generator)\n+class NotAGeneratorCallbackSpider(Spider):\n+    name = 'NotAGeneratorCallbackSpider'\n+    start_urls = [TEST_URL]\n+    custom_settings = {\n+        'SPIDER_MIDDLEWARES': {\n+            __name__ + '.LogExceptionMiddleware': 10,\n+        },\n+    }\n+\n+    def parse(self, response):\n+        return [{'test': 1}, {'test': 1/0}]\n+\n+\n+# ================================================================================\n class TestSpiderMiddleware(TestCase):\n-\n     @classmethod\n     def setUpClass(cls):\n         cls.mockserver = MockServer()\n@@ -271,69 +126,45 @@ class TestSpiderMiddleware(TestCase):\n             yield crawler.crawl()\n         raise defer.returnValue(log)\n \n-    @defer.inlineCallbacks\n-    def test_process_spider_exception_from_parse_method_non_generator(self):\n-        # non-generator return value\n-        log = yield self.crawl_log(NotAGeneratorSpider)\n-        self.assertIn(\"AssertionError exception caught\", str(log))\n-        self.assertIn(\"spider_exceptions/AssertionError\", str(log))\n+    # @defer.inlineCallbacks\n+    # def test_recovery(self):\n+    #     \"\"\"\n+    #     Recover from an exception from a spider's callback. The final item count should be 3\n+    #     (one from the spider before raising the exception, one from the middleware and one\n+    #     from the spider when processing the response that was enqueued from the middleware)\n+    #     \"\"\"\n+    #     log = yield self.crawl_log(RecoverySpider)\n+    #     self.assertIn(\"Middleware: ModuleNotFoundError exception caught\", str(log))\n+    #     self.assertEqual(str(log).count(\"Middleware: ModuleNotFoundError exception caught\"), 1)\n+    #     self.assertIn(\"'item_scraped_count': 3\", str(log))\n \n     @defer.inlineCallbacks\n-    def test_process_spider_exception_from_parse_method_generator_no_items(self):\n-        # generator return value, no items before the error\n-        log = yield self.crawl_log(GeneratorErrorBeforeItemsSpider)\n-        self.assertIn(\"ValueError exception caught\", str(log))\n-        self.assertIn(\"spider_exceptions/ValueError\", str(log))\n+    def test_process_spider_input_errback(self):\n+        \"\"\"\n+        (1) An exception from the process_spider_input chain should not be caught by the\n+        process_spider_exception chain, it should go directly to the Request errback\n+        \"\"\"\n+        log1 = yield self.crawl_log(ProcessSpiderInputSpider)\n+        self.assertNotIn(\"Middleware: IndexError exception caught\", str(log1))\n+        self.assertIn(\"Middleware: will raise IndexError\", str(log1))\n+        self.assertIn(\"Got a Failure on the Request errback\", str(log1))\n     \n     @defer.inlineCallbacks\n-    def test_process_spider_exception_from_parse_method_generator_with_items(self):\n-        # generator return value, 3 items before the error\n-        log = yield self.crawl_log(GeneratorErrorAfterItemsSpider)\n-        self.assertIn(\"'item_scraped_count': 3\", str(log))\n-        self.assertIn(\"FloatingPointError exception caught\", str(log))\n-        self.assertIn(\"spider_exceptions/FloatingPointError\", str(log))\n+    def test_generator_callback(self):\n+        \"\"\"\n+        (2) An exception from a spider's callback should\n+        be caught by the process_spider_exception chain\n+        \"\"\"\n+        log2 = yield self.crawl_log(GeneratorCallbackSpider)\n+        self.assertIn(\"Middleware: ImportError exception caught\", str(log2))\n+        self.assertIn(\"'item_scraped_count': 2\", str(log2))\n     \n     @defer.inlineCallbacks\n-    def test_process_spider_exception_from_previous_middleware_input(self):\n-        log = yield self.crawl_log(FromPreviousMiddlewareInputSpider)\n-        self.assertIn(\"LookupError exception caught\", str(log))\n-\n-    @defer.inlineCallbacks\n-    def test_process_spider_exception_from_previous_middleware_output(self):\n-        # non-generator output value\n-        log = yield self.crawl_log(NotAGeneratorFromPreviousMiddlewareOutputSpider)\n-        self.assertNotIn(\"UnicodeError exception caught\", str(log))\n-        # generator output value\n-        log = yield self.crawl_log(GeneratorFromPreviousMiddlewareOutputSpider)\n-        self.assertIn(\"'item_scraped_count': 3\", str(log))\n-        self.assertIn(\"NameError exception caught\", str(log))\n-\n-    @defer.inlineCallbacks\n-    def test_process_spider_exception_do_something(self):\n-        log = yield self.crawl_log(DoSomethingSpider)\n-        self.assertIn(\"ImportError exception caught\", str(log))\n-        self.assertIn(\"{'value': 10}\", str(log))\n-        self.assertIn(\"{'value': 'asdf'}\", str(log))\n-        self.assertIn(\"{'value': 'http://localhost:8998'}\", str(log))\n-        self.assertIn(\"{'value': 'http://localhost:8998?processed=true'}\", str(log))\n-\n-    @defer.inlineCallbacks\n-    def test_process_spider_exception_invalid_return_value_previous_middleware(self):\n-        \"\"\" don't catch _InvalidOutput from middleware \"\"\"\n-        # on middleware's input\n-        log1 = yield self.crawl_log(InvalidReturnValueFromPreviousMiddlewareInputSpider)\n-        self.assertNotIn(\"_InvalidOutput exception caught\", str(log1))\n-        self.assertIn(\"'spider_exceptions/_InvalidOutput'\", str(log1))\n-        # on middleware's output\n-        log2 = yield self.crawl_log(InvalidReturnValueFromPreviousMiddlewareOutputSpider)\n-        self.assertNotIn(\"_InvalidOutput exception caught\", str(log2))\n-        self.assertIn(\"'spider_exceptions/_InvalidOutput'\", str(log2))\n-\n-    @defer.inlineCallbacks\n-    def test_process_spider_exception_execution_chain(self):\n-        # on middleware's input\n-        log = yield self.crawl_log(ExecutionChainSpider)\n-        self.assertNotIn(\"handled_by_first_middleware\", str(log))\n-        self.assertNotIn(\"handled_by_second_middleware\", str(log))\n-        self.assertIn(\"MemoryError exception caught\", str(log))\n-        self.assertIn(\"handled_by_third_middleware\", str(log))\n+    def test_not_a_generator_callback(self):\n+        \"\"\"\n+        (3) An exception from a spider's callback should\n+        be caught by the process_spider_exception chain\n+        \"\"\"\n+        log3 = yield self.crawl_log(NotAGeneratorCallbackSpider)\n+        self.assertIn(\"Middleware: ZeroDivisionError exception caught\", str(log3))\n+        self.assertNotIn(\"item_scraped_count\", str(log3))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
