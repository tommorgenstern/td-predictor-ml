{"custom_id": "keras#b95cf4a1b1b5f11ecc13299a6536eb035ea1d530", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 349 | Lines Deleted: 283 | Files Changed: 6 | Hunks: 104 | Methods Changed: 37 | Complexity Δ (Sum/Max): 22/16 | Churn Δ: 632 | Churn Cumulative: 20131 | Contributors (this commit): 103 | Commits (past 90d): 85 | Contributors (cumulative): 125 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,7 +4,8 @@ from __future__ import absolute_import\n from __future__ import division\n \n import numpy as np\n-\n+import json\n+import yaml\n import warnings\n import copy\n import os\n@@ -16,6 +17,11 @@ from .. import backend as K\n from .. import initializers\n from ..utils.io_utils import ask_to_proceed_with_overwrite\n \n+try:\n+    import h5py\n+except ImportError:\n+    h5py = None\n+\n \n class InputSpec(object):\n     \"\"\"Specifies the ndim, dtype and shape of every input to a layer.\n@@ -58,9 +64,11 @@ class Node(object):\n     Each time the output of a layer is used by another layer,\n     a node is added to `layer.outbound_nodes`.\n \n-    # Attributes\n+    # Arguments\n         outbound_layer: the layer that takes\n-            `input_tensors` and turns them into `output_tensors`.\n+            `input_tensors` and turns them into `output_tensors`\n+            (the node gets created when the `call`\n+            method of the layer was called).\n         inbound_layers: a list of layers, the same length as `input_tensors`,\n             the layers from where `input_tensors` originate.\n         node_indices: a list of integers, the same length as `inbound_layers`.\n@@ -80,6 +88,8 @@ class Node(object):\n         output_masks: list of output masks (a mask can be a tensor, or None).\n         input_shapes: list of input shape tuples.\n         output_shapes: list of output shape tuples.\n+        arguments: dictionary of keyword arguments that were passed to the\n+            `call` method of the layer at the call that created the node.\n \n     `node_indices` and `tensor_indices` are basically fine-grained coordinates\n     describing the origin of the `input_tensors`, verifying the following:\n@@ -95,7 +105,8 @@ class Node(object):\n                  inbound_layers, node_indices, tensor_indices,\n                  input_tensors, output_tensors,\n                  input_masks, output_masks,\n-                 input_shapes, output_shapes):\n+                 input_shapes, output_shapes,\n+                 arguments=None):\n         # Layer instance (NOT a list).\n         # this is the layer that takes a list of input tensors\n         # and turns them into a list of output tensors.\n@@ -124,6 +135,9 @@ class Node(object):\n         self.input_shapes = input_shapes  # List of shape tuples, shapes of input_tensors.\n         self.output_shapes = output_shapes  # List of shape tuples, shapes of output_tensors.\n \n+        # Optional keyword arguments to layer's `call`:\n+        self.arguments = arguments\n+\n         # Add nodes to all layers involved.\n         for layer in inbound_layers:\n             if layer is not None:\n@@ -208,7 +222,6 @@ class Layer(object):\n     # Internal methods:\n         build(input_shape)\n         _add_inbound_node(layer, index=0)\n-        create_input_layer()\n         assert_input_compatibility()\n     \"\"\"\n \n@@ -509,7 +522,8 @@ class Layer(object):\n             # this does nothing.\n             self._add_inbound_node(input_tensors=x, output_tensors=output,\n                                    input_masks=previous_mask, output_masks=output_mask,\n-                                   input_shapes=input_shape, output_shapes=output_shape)\n+                                   input_shapes=input_shape, output_shapes=output_shape,\n+                                   arguments=kwargs)\n \n             # Apply activity regularizer if any:\n             if hasattr(self, 'activity_regularizer') and self.activity_regularizer is not None:\n@@ -520,8 +534,10 @@ class Layer(object):\n \n     def _add_inbound_node(self, input_tensors, output_tensors,\n                           input_masks, output_masks,\n-                          input_shapes, output_shapes):\n-        \"\"\"\n+                          input_shapes, output_shapes, arguments=None):\n+        \"\"\"Internal method to create an inbound node for the layer.\n+\n+        # Arguments\n             TODO\n         \"\"\"\n         input_tensors = _to_list(input_tensors)\n@@ -557,7 +573,8 @@ class Layer(object):\n             input_masks=input_masks,\n             output_masks=output_masks,\n             input_shapes=input_shapes,\n-            output_shapes=output_shapes\n+            output_shapes=output_shapes,\n+            arguments=arguments\n         )\n \n         # Update tensor history, _keras_shape and _uses_learning_phase.\n@@ -1869,13 +1886,16 @@ class Container(Layer):\n                 if len(computed_data) == len(reference_input_tensors):\n                     # call layer\n                     with K.name_scope(layer.name):\n+                        if node.arguments:\n+                            kwargs = node.arguments\n+                        else:\n+                            kwargs = {}\n                         if len(computed_data) == 1:\n                             computed_tensor, computed_mask = computed_data[0]\n                             if 'mask' in inspect.getargspec(layer.call).args:\n-                                output_tensors = _to_list(layer.call(computed_tensor,\n-                                                                     mask=computed_mask))\n-                            else:\n-                                output_tensors = _to_list(layer.call(computed_tensor))\n+                                if 'mask' not in kwargs:\n+                                    kwargs['mask'] = computed_mask\n+                            output_tensors = _to_list(layer.call(computed_tensor, **kwargs))\n                             output_masks = _to_list(layer.compute_mask(computed_tensor,\n                                                                        computed_mask))\n                             computed_tensors = [computed_tensor]\n@@ -1884,10 +1904,9 @@ class Container(Layer):\n                             computed_tensors = [x[0] for x in computed_data]\n                             computed_masks = [x[1] for x in computed_data]\n                             if 'mask' in inspect.getargspec(layer.call).args:\n-                                output_tensors = _to_list(layer.call(computed_tensors,\n-                                                                     mask=computed_mask))\n-                            else:\n-                                output_tensors = _to_list(layer.call(computed_tensors))\n+                                if 'mask' not in kwargs:\n+                                    kwargs['mask'] = computed_masks\n+                            output_tensors = _to_list(layer.call(computed_tensors, **kwargs))\n                             output_masks = _to_list(layer.compute_mask(computed_tensors,\n                                                                        computed_masks))\n \n@@ -1925,7 +1944,6 @@ class Container(Layer):\n         output_masks = []\n         output_shapes = []\n         for x in self.outputs:\n-            # TODO: Better error message.\n             assert str(id(x)) in tensor_map, 'Could not compute output ' + str(x)\n             tensor, mask = tensor_map[str(id(x))]\n             if hasattr(tensor, '_keras_shape') and output_shapes is not None:\n@@ -1990,6 +2008,17 @@ class Container(Layer):\n                 if node_key in self.container_nodes:\n                     # The node is relevant to the model:\n                     # add to filtered_inbound_nodes.\n+                    if node.arguments:\n+                        try:\n+                            json.dumps(node.arguments)\n+                            kwargs = node.arguments\n+                        except TypeError:\n+                            raise TypeError(\n+                                'Layer ' + layer.name +\n+                                ' was passed non-serializable keyword arguments: ' +\n+                                str(node.arguments) + '. Cannot serialize model.')\n+                    else:\n+                        kwargs = {}\n                     if node.inbound_layers:\n                         node_data = []\n                         for i in range(len(node.inbound_layers)):\n@@ -2000,7 +2029,8 @@ class Container(Layer):\n                             new_node_index = node_conversion_map.get(node_key, 0)\n                             node_data.append([inbound_layer.name,\n                                               new_node_index,\n-                                              tensor_index])\n+                                              tensor_index,\n+                                              kwargs])\n                         filtered_inbound_nodes.append(node_data)\n             layer_configs.append({\n                 'name': layer.name,\n@@ -2056,7 +2086,13 @@ class Container(Layer):\n             for node_data in inbound_nodes_data:\n                 input_tensors = []\n                 for input_data in node_data:\n+                    if len(input_data) == 3:\n                         inbound_layer_name, inbound_node_index, inbound_tensor_index = input_data\n+                        kwargs = {}\n+                    elif len(input_data) == 4:\n+                        inbound_layer_name, inbound_node_index, inbound_tensor_index, kwargs = input_data\n+                    else:\n+                        raise ValueError('Improperly formatted model config.')\n                     assert inbound_layer_name in created_layers, 'Missing layer: %s' % inbound_layer_name\n                     inbound_layer = created_layers[inbound_layer_name]\n                     inbound_node = inbound_layer.inbound_nodes[inbound_node_index]\n@@ -2065,9 +2101,9 @@ class Container(Layer):\n                 # and building the layer if needed.\n                 if input_tensors:\n                     if len(input_tensors) == 1:\n-                        layer(input_tensors[0])\n+                        layer(input_tensors[0], **kwargs)\n                     else:\n-                        layer(input_tensors)\n+                        layer(input_tensors, **kwargs)\n \n         for layer_data in config['layers']:\n             process_layer(layer_data)\n@@ -2133,41 +2169,18 @@ class Container(Layer):\n                 - For every weight in the layer, a dataset\n                     storing the weight value, named after the weight tensor.\n         \"\"\"\n-        import h5py\n+        if h5py is None:\n+            raise ImportError('`save_weights` requires h5py.')\n         # If file exists and should not be overwritten:\n         if not overwrite and os.path.isfile(filepath):\n             proceed = ask_to_proceed_with_overwrite(filepath)\n             if not proceed:\n                 return\n         f = h5py.File(filepath, 'w')\n-        self.save_weights_to_hdf5_group(f)\n+        save_weights_to_hdf5_group(f, self.layers)\n         f.flush()\n         f.close()\n \n-    def save_weights_to_hdf5_group(self, f):\n-        f.attrs['layer_names'] = [layer.name.encode('utf8') for layer in self.layers]\n-\n-        for layer in self.layers:\n-            g = f.create_group(layer.name)\n-            symbolic_weights = layer.weights\n-            weight_values = K.batch_get_value(symbolic_weights)\n-            weight_names = []\n-            for i, (w, val) in enumerate(zip(symbolic_weights, weight_values)):\n-                if hasattr(w, 'name') and w.name:\n-                    name = str(w.name)\n-                else:\n-                    name = 'param_' + str(i)\n-                weight_names.append(name.encode('utf8'))\n-            g.attrs['weight_names'] = weight_names\n-            for name, val in zip(weight_names, weight_values):\n-                param_dset = g.create_dataset(name, val.shape,\n-                                              dtype=val.dtype)\n-                if not val.shape:\n-                    # scalar\n-                    param_dset[()] = val\n-                else:\n-                    param_dset[:] = val\n-\n     def load_weights(self, filepath, by_name=False):\n         \"\"\"Loads all layer weights from a HDF5 save file.\n \n@@ -2182,101 +2195,25 @@ class Container(Layer):\n         only if they share the same name. This is useful\n         for fine-tuning or transfer-learning models where\n         some of the layers have changed.\n+\n+        # Arguments\n+            filepath: String, path to the weights file to load.\n+            by_name: Boolean, whether to load weights by name\n+                or by topological order.\n         \"\"\"\n-        import h5py\n+        if h5py is None:\n+            raise ImportError('`load_weights` requires h5py.')\n         f = h5py.File(filepath, mode='r')\n         if 'layer_names' not in f.attrs and 'model_weights' in f:\n             f = f['model_weights']\n         if by_name:\n-            self.load_weights_from_hdf5_group_by_name(f)\n+            load_weights_from_hdf5_group_by_name(f, self.layers)\n         else:\n-            self.load_weights_from_hdf5_group(f)\n+            load_weights_from_hdf5_group(f, self.layers)\n \n         if hasattr(f, 'close'):\n             f.close()\n \n-    def load_weights_from_hdf5_group(self, f):\n-        filtered_layers = []\n-        for layer in self.layers:\n-            weights = layer.weights\n-            if weights:\n-                filtered_layers.append(layer)\n-\n-        layer_names = [n.decode('utf8') for n in f.attrs['layer_names']]\n-        filtered_layer_names = []\n-        for name in layer_names:\n-            g = f[name]\n-            weight_names = [n.decode('utf8') for n in g.attrs['weight_names']]\n-            if len(weight_names):\n-                filtered_layer_names.append(name)\n-        layer_names = filtered_layer_names\n-        if len(layer_names) != len(filtered_layers):\n-            raise ValueError('You are trying to load a weight file '\n-                             'containing ' + str(len(layer_names)) +\n-                             ' layers into a model with ' +\n-                             str(len(filtered_layers)) + ' layers.')\n-\n-        # We batch weight value assignments in a single backend call\n-        # which provides a speedup in TensorFlow.\n-        weight_value_tuples = []\n-        for k, name in enumerate(layer_names):\n-            g = f[name]\n-            weight_names = [n.decode('utf8') for n in g.attrs['weight_names']]\n-            weight_values = [g[weight_name] for weight_name in weight_names]\n-            layer = filtered_layers[k]\n-            symbolic_weights = layer.weights\n-            if len(weight_values) != len(symbolic_weights):\n-                raise ValueError('Layer #' + str(k) +\n-                                 ' (named \"' + layer.name +\n-                                 '\" in the current model) was found to '\n-                                 'correspond to layer ' + name +\n-                                 ' in the save file. '\n-                                 'However the new layer ' + layer.name +\n-                                 ' expects ' + str(len(symbolic_weights)) +\n-                                 ' weights, but the saved weights have ' +\n-                                 str(len(weight_values)) +\n-                                 ' elements.')\n-            weight_value_tuples += zip(symbolic_weights, weight_values)\n-        K.batch_set_value(weight_value_tuples)\n-\n-    def load_weights_from_hdf5_group_by_name(self, f):\n-        \"\"\" Name-based weight loading\n-        (instead of topological weight loading).\n-        Layers that have no matching name are skipped.\n-        \"\"\"\n-        # New file format.\n-        layer_names = [n.decode('utf8') for n in f.attrs['layer_names']]\n-\n-        # Reverse index of layer name to list of layers with name.\n-        index = {}\n-        for layer in self.layers:\n-            if layer.name:\n-                index.setdefault(layer.name, []).append(layer)\n-\n-        # We batch weight value assignments in a single backend call\n-        # which provides a speedup in TensorFlow.\n-        weight_value_tuples = []\n-        for k, name in enumerate(layer_names):\n-            g = f[name]\n-            weight_names = [n.decode('utf8') for n in g.attrs['weight_names']]\n-            weight_values = [g[weight_name] for weight_name in weight_names]\n-\n-            for layer in index.get(name, []):\n-                symbolic_weights = layer.weights\n-                if len(weight_values) != len(symbolic_weights):\n-                    raise ValueError('Layer #' + str(k) +\n-                                     ' (named \"' + layer.name +\n-                                     '\") expects ' +\n-                                     str(len(symbolic_weights)) +\n-                                     ' weight(s), but the saved weights' +\n-                                     ' have ' + str(len(weight_values)) +\n-                                     ' element(s).')\n-                # Set values.\n-                for i in range(len(weight_values)):\n-                    weight_value_tuples.append((symbolic_weights[i],\n-                                                weight_values[i]))\n-        K.batch_set_value(weight_value_tuples)\n-\n     def _updated_config(self):\n         \"\"\"Shared between different serialization methods.\"\"\"\n         from keras import __version__ as keras_version\n@@ -2295,8 +2232,6 @@ class Container(Layer):\n         To load a network from a JSON save file, use\n         `keras.models.model_from_json(json_string, custom_objects={})`.\n         \"\"\"\n-        import json\n-\n         def get_json_type(obj):\n             # If obj is any numpy type\n             if type(obj).__module__ == np.__name__:\n@@ -2321,7 +2256,6 @@ class Container(Layer):\n         the names of custom losses / layers / etc to the corresponding\n         functions / classes.\n         \"\"\"\n-        import yaml\n         return yaml.dump(self._updated_config(), **kwargs)\n \n     def summary(self, line_length=None, positions=None):\n@@ -2438,3 +2372,112 @@ def _collect_input_shape(input_tensors):\n     if len(shapes) == 1:\n         return shapes[0]\n     return shapes\n+\n+\n+def save_weights_to_hdf5_group(f, layers):\n+    f.attrs['layer_names'] = [layer.name.encode('utf8') for layer in layers]\n+\n+    for layer in layers:\n+        g = f.create_group(layer.name)\n+        symbolic_weights = layer.weights\n+        weight_values = K.batch_get_value(symbolic_weights)\n+        weight_names = []\n+        for i, (w, val) in enumerate(zip(symbolic_weights, weight_values)):\n+            if hasattr(w, 'name') and w.name:\n+                name = str(w.name)\n+            else:\n+                name = 'param_' + str(i)\n+            weight_names.append(name.encode('utf8'))\n+        g.attrs['weight_names'] = weight_names\n+        for name, val in zip(weight_names, weight_values):\n+            param_dset = g.create_dataset(name, val.shape,\n+                                          dtype=val.dtype)\n+            if not val.shape:\n+                # scalar\n+                param_dset[()] = val\n+            else:\n+                param_dset[:] = val\n+\n+\n+def load_weights_from_hdf5_group(f, layers):\n+    filtered_layers = []\n+    for layer in layers:\n+        weights = layer.weights\n+        if weights:\n+            filtered_layers.append(layer)\n+\n+    layer_names = [n.decode('utf8') for n in f.attrs['layer_names']]\n+    filtered_layer_names = []\n+    for name in layer_names:\n+        g = f[name]\n+        weight_names = [n.decode('utf8') for n in g.attrs['weight_names']]\n+        if len(weight_names):\n+            filtered_layer_names.append(name)\n+    layer_names = filtered_layer_names\n+    if len(layer_names) != len(filtered_layers):\n+        raise ValueError('You are trying to load a weight file '\n+                         'containing ' + str(len(layer_names)) +\n+                         ' layers into a model with ' +\n+                         str(len(filtered_layers)) + ' layers.')\n+\n+    # We batch weight value assignments in a single backend call\n+    # which provides a speedup in TensorFlow.\n+    weight_value_tuples = []\n+    for k, name in enumerate(layer_names):\n+        g = f[name]\n+        weight_names = [n.decode('utf8') for n in g.attrs['weight_names']]\n+        weight_values = [g[weight_name] for weight_name in weight_names]\n+        layer = filtered_layers[k]\n+        symbolic_weights = layer.weights\n+        if len(weight_values) != len(symbolic_weights):\n+            raise ValueError('Layer #' + str(k) +\n+                             ' (named \"' + layer.name +\n+                             '\" in the current model) was found to '\n+                             'correspond to layer ' + name +\n+                             ' in the save file. '\n+                             'However the new layer ' + layer.name +\n+                             ' expects ' + str(len(symbolic_weights)) +\n+                             ' weights, but the saved weights have ' +\n+                             str(len(weight_values)) +\n+                             ' elements.')\n+        weight_value_tuples += zip(symbolic_weights, weight_values)\n+    K.batch_set_value(weight_value_tuples)\n+\n+\n+def load_weights_from_hdf5_group_by_name(f, layers):\n+    \"\"\" Name-based weight loading\n+    (instead of topological weight loading).\n+    Layers that have no matching name are skipped.\n+    \"\"\"\n+    # New file format.\n+    layer_names = [n.decode('utf8') for n in f.attrs['layer_names']]\n+\n+    # Reverse index of layer name to list of layers with name.\n+    index = {}\n+    for layer in layers:\n+        if layer.name:\n+            index.setdefault(layer.name, []).append(layer)\n+\n+    # We batch weight value assignments in a single backend call\n+    # which provides a speedup in TensorFlow.\n+    weight_value_tuples = []\n+    for k, name in enumerate(layer_names):\n+        g = f[name]\n+        weight_names = [n.decode('utf8') for n in g.attrs['weight_names']]\n+        weight_values = [g[weight_name] for weight_name in weight_names]\n+\n+        for layer in index.get(name, []):\n+            symbolic_weights = layer.weights\n+            if len(weight_values) != len(symbolic_weights):\n+                raise ValueError('Layer #' + str(k) +\n+                                 ' (named \"' + layer.name +\n+                                 '\") expects ' +\n+                                 str(len(symbolic_weights)) +\n+                                 ' weight(s), but the saved weights' +\n+                                 ' have ' + str(len(weight_values)) +\n+                                 ' element(s).')\n+            # Set values.\n+            for i in range(len(weight_values)):\n+                weight_value_tuples.append((symbolic_weights[i],\n+                                            weight_values[i]))\n+    K.batch_set_value(weight_value_tuples)\n\n@@ -1,3 +1,4 @@\n+# -*- coding: utf-8 -*-\n from __future__ import print_function\n from __future__ import absolute_import\n \n@@ -7,7 +8,6 @@ import time\n import numpy as np\n import multiprocessing\n import threading\n-\n import six\n \n try:\n\n@@ -9,6 +9,13 @@ from .. import backend as K\n \n class Wrapper(Layer):\n     \"\"\"Abstract wrapper base class.\n+\n+    Wrappers take another layer and augment it in various ways.\n+    Do not use this class as a layer, it is only an abstract base class.\n+    Two usable wrappers are the `TimeDistributed` and `Bidirectional` wrappers.\n+\n+    # Arguments\n+        layer: The layer to be wrapped.\n     \"\"\"\n \n     def __init__(self, layer, **kwargs):\n@@ -118,16 +125,16 @@ class TimeDistributed(Wrapper):\n                                   unroll=False)\n             y = outputs\n         else:\n-            # no batch size specified, therefore the layer will be able\n-            # to process batches of any size\n-            # we can go with reshape-based implementation for performance\n+            # No batch size specified, therefore the layer will be able\n+            # to process batches of any size.\n+            # We can go with reshape-based implementation for performance.\n             input_length = input_shape[1]\n             if not input_length:\n                 input_length = K.shape(inputs)[1]\n-            # (num_samples * timesteps, ...)\n+            # Shape: (num_samples * timesteps, ...)\n             inputs = K.reshape(inputs, (-1,) + input_shape[2:])\n             y = self.layer.call(inputs)  # (num_samples * timesteps, ...)\n-            # (num_samples, timesteps, ...)\n+            # Shape: (num_samples, timesteps, ...)\n             output_shape = self.compute_output_shape(input_shape)\n             y = K.reshape(y, (-1, input_length) + output_shape[2:])\n \n\n@@ -6,19 +6,30 @@ import warnings\n import copy\n import json\n import os\n+import yaml\n import numpy as np\n \n from . import backend as K\n from . import optimizers\n from .utils.io_utils import ask_to_proceed_with_overwrite\n from .engine.training import Model\n-from .engine.topology import get_source_inputs, Node, Layer, Input\n+from .engine import topology\n+from .engine.topology import Layer\n+from .engine.topology import Input\n from .legacy import layers as legacy_layers\n from .legacy import models as legacy_models\n \n+try:\n+    import h5py\n+except ImportError:\n+    h5py = None\n+\n \n def save_model(model, filepath, overwrite=True):\n \n+    if h5py is None:\n+        raise ImportError('`save_model` requires h5py.')\n+\n     def get_json_type(obj):\n         # if obj is a serializable Keras class instance\n         # e.g. optimizer, layer\n@@ -40,7 +51,6 @@ def save_model(model, filepath, overwrite=True):\n \n         raise TypeError('Not JSON Serializable:', obj)\n \n-    import h5py\n     from keras import __version__ as keras_version\n \n     # if file exists and should not be overwritten\n@@ -57,7 +67,11 @@ def save_model(model, filepath, overwrite=True):\n     }, default=get_json_type).encode('utf8')\n \n     model_weights_group = f.create_group('model_weights')\n-    model.save_weights_to_hdf5_group(model_weights_group)\n+    if legacy_models.needs_legacy_support(model):\n+        layers = legacy_models.legacy_sequential_layers(model)\n+    else:\n+        layers = model.layers\n+    topology.save_weights_to_hdf5_group(model_weights_group, layers)\n \n     if hasattr(model, 'optimizer'):\n         if isinstance(model.optimizer, optimizers.TFOptimizer):\n@@ -111,6 +125,9 @@ def save_model(model, filepath, overwrite=True):\n \n \n def load_model(filepath, custom_objects=None):\n+    if h5py is None:\n+        raise ImportError('`save_model` requires h5py.')\n+\n     if not custom_objects:\n         custom_objects = {}\n \n@@ -135,7 +152,6 @@ def load_model(filepath, custom_objects=None):\n             return custom_objects[obj]\n         return obj\n \n-    import h5py\n     f = h5py.File(filepath, mode='r')\n \n     # instantiate model\n@@ -146,7 +162,7 @@ def load_model(filepath, custom_objects=None):\n     model = model_from_config(model_config, custom_objects=custom_objects)\n \n     # set weights\n-    model.load_weights_from_hdf5_group(f['model_weights'])\n+    topology.load_weights_from_hdf5_group(f['model_weights'], model.layers)\n \n     # instantiate optimizer\n     training_config = f.attrs.get('training_config')\n@@ -201,7 +217,6 @@ def model_from_yaml(yaml_string, custom_objects=None):\n     \"\"\"Parses a yaml model configuration file\n     and returns a model instance.\n     \"\"\"\n-    import yaml\n     from keras.utils.layer_utils import layer_from_config\n     config = yaml.load(yaml_string)\n     return layer_from_config(config, custom_objects=custom_objects)\n@@ -211,7 +226,6 @@ def model_from_json(json_string, custom_objects=None):\n     \"\"\"Parses a JSON model configuration file\n     and returns a model instance.\n     \"\"\"\n-    import json\n     from keras.utils.layer_utils import layer_from_config\n     config = json.loads(json_string)\n     return layer_from_config(config, custom_objects=custom_objects)\n@@ -318,11 +332,11 @@ class Sequential(Model):\n                                  'use the functional API.')\n \n             self.outputs = [layer.inbound_nodes[0].output_tensors[0]]\n-            self.inputs = get_source_inputs(self.outputs[0])\n+            self.inputs = topology.get_source_inputs(self.outputs[0])\n \n             # We create an input node, which we will keep updated\n             # as we add more layers\n-            Node(outbound_layer=self,\n+            topology.Node(outbound_layer=self,\n                           inbound_layers=[],\n                           node_indices=[],\n                           tensor_indices=[],\n@@ -557,6 +571,44 @@ class Sequential(Model):\n             self.build()\n         self.model.set_weights(weights)\n \n+    def load_weights(self, filepath, by_name=False):\n+        if h5py is None:\n+            raise ImportError('`load_weights` requires h5py.')\n+        f = h5py.File(filepath, mode='r')\n+        if 'layer_names' not in f.attrs and 'model_weights' in f:\n+            f = f['model_weights']\n+\n+        # Legacy support\n+        if legacy_models.needs_legacy_support(self):\n+            layers = legacy_models.legacy_sequential_layers(self)\n+        else:\n+            layers = self.layers\n+        if by_name:\n+            topology.load_weights_from_hdf5_group_by_name(f, layers)\n+        else:\n+            topology.load_weights_from_hdf5_group(f, layers)\n+        if hasattr(f, 'close'):\n+            f.close()\n+\n+    def save_weights(self, filepath, overwrite=True):\n+        if h5py is None:\n+            raise ImportError('`save_weights` requires h5py.')\n+        # If file exists and should not be overwritten:\n+        if not overwrite and os.path.isfile(filepath):\n+            proceed = ask_to_proceed_with_overwrite(filepath)\n+            if not proceed:\n+                return\n+        # Legacy support\n+        if legacy_models.needs_legacy_support(self):\n+            layers = legacy_models.legacy_sequential_layers(self)\n+        else:\n+            layers = self.layers\n+\n+        f = h5py.File(filepath, 'w')\n+        topology.save_weights_to_hdf5_group(f, layers)\n+        f.flush()\n+        f.close()\n+\n     def compile(self, optimizer, loss,\n                 metrics=None,\n                 sample_weight_mode=None,\n\n@@ -1,17 +1,23 @@\n+from keras.utils.test_utils import keras_test\n+from keras.utils.test_utils import layer_test\n+from keras import layers\n+from keras import models\n+from keras import regularizers\n+from keras import constraints\n+from keras import backend as K\n+import numpy as np\n+\n \n @keras_test\n def test_highway():\n-    from keras import regularizers\n-    from keras import constraints\n-\n-    layer_test(core.Highway,\n+    layer_test(layers.Highway,\n                kwargs={},\n                input_shape=(3, 2))\n \n-    layer_test(core.Highway,\n+    layer_test(layers.Highway,\n                kwargs={'W_regularizer': regularizers.l2(0.01),\n                        'b_regularizer': regularizers.l1(0.01),\n-                       'activity_regularizer': regularizers.activity_l2(0.01),\n+                       'activity_regularizer': regularizers.l2(0.01),\n                        'W_constraint': constraints.MaxNorm(1),\n                        'b_constraint': constraints.MaxNorm(1)},\n                input_shape=(3, 2))\n@@ -27,7 +33,7 @@ def test_maxout_dense():\n                kwargs={'output_dim': 3,\n                        'W_regularizer': regularizers.l2(0.01),\n                        'b_regularizer': regularizers.l1(0.01),\n-                       'activity_regularizer': regularizers.activity_l2(0.01),\n+                       'activity_regularizer': regularizers.l2(0.01),\n                        'W_constraint': constraints.MaxNorm(1),\n                        'b_constraint': constraints.MaxNorm(1)},\n                input_shape=(3, 2))\n@@ -42,10 +48,10 @@ def test_merge():\n     # test functional API\n     for mode in ['sum', 'mul', 'concat', 'ave', 'max']:\n         print(mode)\n-        input_a = Input(shape=input_shapes[0][1:])\n-        input_b = Input(shape=input_shapes[1][1:])\n-        merged = merge([input_a, input_b], mode=mode)\n-        model = Model([input_a, input_b], merged)\n+        input_a = layers.Input(shape=input_shapes[0][1:])\n+        input_b = layers.Input(shape=input_shapes[1][1:])\n+        merged = layers.merge([input_a, input_b], mode=mode)\n+        model = layers.Model([input_a, input_b], merged)\n         model.compile('rmsprop', 'mse')\n \n         expected_output_shape = model.compute_output_shape(input_shapes)\n@@ -53,12 +59,12 @@ def test_merge():\n         assert expected_output_shape == actual_output_shape\n \n         config = model.get_config()\n-        model = Model.from_config(config)\n+        model = models.Model.from_config(config)\n         model.compile('rmsprop', 'mse')\n \n         # test Merge (#2460)\n-        merged = Merge(mode=mode)([input_a, input_b])\n-        model = Model([input_a, input_b], merged)\n+        merged = layers.Merge(mode=mode)([input_a, input_b])\n+        model = models.Model([input_a, input_b], merged)\n         model.compile('rmsprop', 'mse')\n \n         expected_output_shape = model.compute_output_shape(input_shapes)\n@@ -66,18 +72,18 @@ def test_merge():\n         assert expected_output_shape == actual_output_shape\n \n     # test lambda with output_shape lambda\n-    input_a = Input(shape=input_shapes[0][1:])\n-    input_b = Input(shape=input_shapes[1][1:])\n-    merged = merge([input_a, input_b],\n+    input_a = layers.Input(shape=input_shapes[0][1:])\n+    input_b = layers.Input(shape=input_shapes[1][1:])\n+    merged = layers.merge([input_a, input_b],\n                           mode=lambda tup: K.concatenate([tup[0], tup[1]]),\n                           output_shape=lambda tup: tup[0][:-1] + (tup[0][-1] + tup[1][-1],))\n-    model = Model([input_a, input_b], merged)\n+    model = models.Model([input_a, input_b], merged)\n     expected_output_shape = model.compute_output_shape(input_shapes)\n     actual_output_shape = model.predict(inputs).shape\n     assert expected_output_shape == actual_output_shape\n \n     config = model.get_config()\n-    model = Model.from_config(config)\n+    model = models.Model.from_config(config)\n     model.compile('rmsprop', 'mse')\n \n     # test function with output_shape function\n@@ -89,18 +95,18 @@ def test_merge():\n         s1, s2 = tup\n         return (s1[0], s1[1] + s2[1]) + s1[2:]\n \n-    input_a = Input(shape=input_shapes[0][1:])\n-    input_b = Input(shape=input_shapes[1][1:])\n-    merged = merge([input_a, input_b],\n+    input_a = layers.Input(shape=input_shapes[0][1:])\n+    input_b = layers.Input(shape=input_shapes[1][1:])\n+    merged = layers.merge([input_a, input_b],\n                           mode=fn_mode,\n                           output_shape=fn_output_shape)\n-    model = Model([input_a, input_b], merged)\n+    model = models.Model([input_a, input_b], merged)\n     expected_output_shape = model.compute_output_shape(input_shapes)\n     actual_output_shape = model.predict(inputs).shape\n     assert expected_output_shape == actual_output_shape\n \n     config = model.get_config()\n-    model = Model.from_config(config)\n+    model = models.Model.from_config(config)\n     model.compile('rmsprop', 'mse')\n \n     # test function with output_mask function\n@@ -112,18 +118,18 @@ def test_merge():\n         x_mask, y_mask = tup\n         return K.concatenate([x_mask, y_mask])\n \n-    input_a = Input(shape=input_shapes[0][1:])\n-    input_b = Input(shape=input_shapes[1][1:])\n-    a = Masking()(input_a)\n-    b = Masking()(input_b)\n-    merged = merge([a, b], mode=fn_mode, output_shape=fn_output_shape, output_mask=fn_output_mask)\n-    model = Model([input_a, input_b], merged)\n+    input_a = layers.Input(shape=input_shapes[0][1:])\n+    input_b = layers.Input(shape=input_shapes[1][1:])\n+    a = layers.Masking()(input_a)\n+    b = layers.Masking()(input_b)\n+    merged = layers.merge([a, b], mode=fn_mode, output_shape=fn_output_shape, output_mask=fn_output_mask)\n+    model = models.Model([input_a, input_b], merged)\n     expected_output_shape = model.compute_output_shape(input_shapes)\n     actual_output_shape = model.predict(inputs).shape\n     assert expected_output_shape == actual_output_shape\n \n     config = model.get_config()\n-    model = Model.from_config(config)\n+    model = models.Model.from_config(config)\n     model.compile('rmsprop', 'mse')\n \n     mask_inputs = (np.zeros(input_shapes[0][:-1]), np.ones(input_shapes[1][:-1]))\n@@ -133,20 +139,20 @@ def test_merge():\n     assert np.all(K.function(mask_input_placeholders, [mask_output])(mask_inputs)[0] == expected_mask_output)\n \n     # test lambda with output_mask lambda\n-    input_a = Input(shape=input_shapes[0][1:])\n-    input_b = Input(shape=input_shapes[1][1:])\n-    a = Masking()(input_a)\n-    b = Masking()(input_b)\n-    merged = merge([a, b], mode=lambda tup: K.concatenate([tup[0], tup[1]], axis=1),\n+    input_a = layers.Input(shape=input_shapes[0][1:])\n+    input_b = layers.Input(shape=input_shapes[1][1:])\n+    a = layers.Masking()(input_a)\n+    b = layers.Masking()(input_b)\n+    merged = layers.merge([a, b], mode=lambda tup: K.concatenate([tup[0], tup[1]], axis=1),\n                           output_shape=lambda tup: (tup[0][0], tup[0][1] + tup[1][1]) + tup[0][2:],\n                           output_mask=lambda tup: K.concatenate([tup[0], tup[1]]))\n-    model = Model([input_a, input_b], merged)\n+    model = models.Model([input_a, input_b], merged)\n     expected_output_shape = model.compute_output_shape(input_shapes)\n     actual_output_shape = model.predict(inputs).shape\n     assert expected_output_shape == actual_output_shape\n \n     config = model.get_config()\n-    model = Model.from_config(config)\n+    model = models.Model.from_config(config)\n     model.compile('rmsprop', 'mse')\n \n     mask_output = model.layers[-1]._output_mask(mask_input_placeholders)\n@@ -160,127 +166,115 @@ def test_merge():\n         x, y = tup\n         return x * a + y * b\n \n-    input_a = Input(shape=input_shapes[0][1:])\n-    input_b = Input(shape=input_shapes[1][1:])\n-    merged = merge([input_a, input_b], mode=fn_mode, output_shape=lambda s: s[0], arguments={'a': 0.7, 'b': 0.3})\n-    model = Model([input_a, input_b], merged)\n+    input_a = layers.Input(shape=input_shapes[0][1:])\n+    input_b = layers.Input(shape=input_shapes[1][1:])\n+    merged = layers.merge([input_a, input_b], mode=fn_mode, output_shape=lambda s: s[0], arguments={'a': 0.7, 'b': 0.3})\n+    model = models.Model([input_a, input_b], merged)\n     output = model.predict(inputs)\n \n     config = model.get_config()\n-    model = Model.from_config(config)\n+    model = models.Model.from_config(config)\n \n     assert np.all(model.predict(inputs) == output)\n \n \n @keras_test\n def test_merge_mask_2d():\n-    from keras.layers import Input, merge, Masking\n-    from keras.models import Model\n-\n     rand = lambda *shape: np.asarray(np.random.random(shape) > 0.5, dtype='int32')\n \n     # inputs\n-    input_a = Input(shape=(3,))\n-    input_b = Input(shape=(3,))\n+    input_a = layers.Input(shape=(3,))\n+    input_b = layers.Input(shape=(3,))\n \n     # masks\n-    masked_a = Masking(mask_value=0)(input_a)\n-    masked_b = Masking(mask_value=0)(input_b)\n+    masked_a = layers.Masking(mask_value=0)(input_a)\n+    masked_b = layers.Masking(mask_value=0)(input_b)\n \n     # three different types of merging\n-    merged_sum = merge([masked_a, masked_b], mode='sum')\n-    merged_concat = merge([masked_a, masked_b], mode='concat', concat_axis=1)\n-    merged_concat_mixed = merge([masked_a, input_b], mode='concat', concat_axis=1)\n+    merged_sum = layers.merge([masked_a, masked_b], mode='sum')\n+    merged_concat = layers.merge([masked_a, masked_b], mode='concat', concat_axis=1)\n+    merged_concat_mixed = layers.merge([masked_a, input_b], mode='concat', concat_axis=1)\n \n     # test sum\n-    model_sum = Model([input_a, input_b], [merged_sum])\n+    model_sum = models.Model([input_a, input_b], [merged_sum])\n     model_sum.compile(loss='mse', optimizer='sgd')\n     model_sum.fit([rand(2, 3), rand(2, 3)], [rand(2, 3)], epochs=1)\n \n     # test concatenation\n-    model_concat = Model([input_a, input_b], [merged_concat])\n+    model_concat = models.Model([input_a, input_b], [merged_concat])\n     model_concat.compile(loss='mse', optimizer='sgd')\n     model_concat.fit([rand(2, 3), rand(2, 3)], [rand(2, 6)], epochs=1)\n \n     # test concatenation with masked and non-masked inputs\n-    model_concat = Model([input_a, input_b], [merged_concat_mixed])\n+    model_concat = models.Model([input_a, input_b], [merged_concat_mixed])\n     model_concat.compile(loss='mse', optimizer='sgd')\n     model_concat.fit([rand(2, 3), rand(2, 3)], [rand(2, 6)], epochs=1)\n \n \n @keras_test\n def test_merge_mask_3d():\n-    from keras.layers import Input, merge, Embedding, SimpleRNN\n-    from keras.models import Model\n-\n     rand = lambda *shape: np.asarray(np.random.random(shape) > 0.5, dtype='int32')\n \n     # embeddings\n-    input_a = Input(shape=(3,), dtype='int32')\n-    input_b = Input(shape=(3,), dtype='int32')\n-    embedding = Embedding(3, 4, mask_zero=True)\n+    input_a = layers.Input(shape=(3,), dtype='int32')\n+    input_b = layers.Input(shape=(3,), dtype='int32')\n+    embedding = layers.Embedding(3, 4, mask_zero=True)\n     embedding_a = embedding(input_a)\n     embedding_b = embedding(input_b)\n \n     # rnn\n-    rnn = SimpleRNN(3, return_sequences=True)\n+    rnn = layers.SimpleRNN(3, return_sequences=True)\n     rnn_a = rnn(embedding_a)\n     rnn_b = rnn(embedding_b)\n \n     # concatenation\n-    merged_concat = merge([rnn_a, rnn_b], mode='concat', concat_axis=-1)\n-    model = Model([input_a, input_b], [merged_concat])\n+    merged_concat = layers.merge([rnn_a, rnn_b], mode='concat', concat_axis=-1)\n+    model = models.Model([input_a, input_b], [merged_concat])\n     model.compile(loss='mse', optimizer='sgd')\n     model.fit([rand(2, 3), rand(2, 3)], [rand(2, 3, 6)])\n \n \n-\n-    \n-\n @keras_test\n def test_sequential_regression():\n-    from keras.models import Sequential, Model\n-    from keras.layers import Merge, Embedding, BatchNormalization, LSTM, InputLayer, Input\n-\n     # start with a basic example of using a Sequential model\n     # inside the functional API\n-    seq = Sequential()\n-    seq.add(Dense(input_dim=10, output_dim=10))\n+    seq = models.Sequential()\n+    seq.add(layers.Dense(input_dim=10, output_dim=10))\n \n-    x = Input(shape=(10,))\n+    x = layers.Input(shape=(10,))\n     y = seq(x)\n-    model = Model(x, y)\n+    model = models.Model(x, y)\n     model.compile('rmsprop', 'mse')\n     weights = model.get_weights()\n \n     # test serialization\n     config = model.get_config()\n-    model = Model.from_config(config)\n+    model = models.Model.from_config(config)\n     model.compile('rmsprop', 'mse')\n     model.set_weights(weights)\n \n     # more advanced model with multiple branches\n \n-    branch_1 = Sequential(name='branch_1')\n-    branch_1.add(Embedding(input_dim=100,\n+    branch_1 = models.Sequential(name='branch_1')\n+    branch_1.add(layers.Embedding(input_dim=100,\n                                   output_dim=10,\n                                   input_length=2,\n                                   name='embed_1'))\n-    branch_1.add(LSTM(32, name='lstm_1'))\n+    branch_1.add(layers.LSTM(32, name='lstm_1'))\n \n-    branch_2 = Sequential(name='branch_2')\n-    branch_2.add(Dense(32, input_shape=(8,), name='dense_2'))\n+    branch_2 = models.Sequential(name='branch_2')\n+    branch_2.add(layers.Dense(32, input_shape=(8,), name='dense_2'))\n \n-    branch_3 = Sequential(name='branch_3')\n-    branch_3.add(Dense(32, input_shape=(6,), name='dense_3'))\n+    branch_3 = models.Sequential(name='branch_3')\n+    branch_3.add(layers.Dense(32, input_shape=(6,), name='dense_3'))\n \n-    branch_1_2 = Sequential([Merge([branch_1, branch_2], mode='concat')], name='branch_1_2')\n-    branch_1_2.add(Dense(16, name='dense_1_2-0'))\n+    branch_1_2 = models.Sequential([layers.Merge([branch_1, branch_2], mode='concat')], name='branch_1_2')\n+    branch_1_2.add(layers.Dense(16, name='dense_1_2-0'))\n     # test whether impromtu input_shape breaks the model\n-    branch_1_2.add(Dense(16, input_shape=(16,), name='dense_1_2-1'))\n+    branch_1_2.add(layers.Dense(16, input_shape=(16,), name='dense_1_2-1'))\n \n-    model = Sequential([Merge([branch_1_2, branch_3], mode='concat')], name='final')\n-    model.add(Dense(16, name='dense_final'))\n+    model = models.Sequential([layers.Merge([branch_1_2, branch_3], mode='concat')], name='final')\n+    model.add(layers.Dense(16, name='dense_final'))\n     model.compile(optimizer='rmsprop',\n                   loss='categorical_crossentropy',\n                   metrics=['accuracy'])\n@@ -295,12 +289,12 @@ def test_sequential_regression():\n \n     # test if Sequential can be called in the functional API\n \n-    a = Input(shape=(2,), dtype='int32')\n-    b = Input(shape=(8,))\n-    c = Input(shape=(6,))\n+    a = layers.Input(shape=(2,), dtype='int32')\n+    b = layers.Input(shape=(8,))\n+    c = layers.Input(shape=(6,))\n     o = model([a, b, c])\n \n-    outer_model = Model([a, b, c], o)\n+    outer_model = models.Model([a, b, c], o)\n     outer_model.compile(optimizer='rmsprop',\n                         loss='categorical_crossentropy',\n                         metrics=['accuracy'])\n@@ -308,7 +302,7 @@ def test_sequential_regression():\n \n     # test serialization\n     config = outer_model.get_config()\n-    outer_model = Model.from_config(config)\n+    outer_model = models.Model.from_config(config)\n     outer_model.compile(optimizer='rmsprop',\n                         loss='categorical_crossentropy',\n                         metrics=['accuracy'])\n@@ -320,18 +314,18 @@ def test_sequential_regression():\n @keras_test\n def test_merge_sum():\n     (x_train, y_train), (x_test, y_test) = _get_test_data()\n-    left = Sequential()\n-    left.add(Dense(num_hidden, input_shape=(input_dim,)))\n-    left.add(Activation('relu'))\n+    left = models.Sequential()\n+    left.add(layers.Dense(num_hidden, input_shape=(input_dim,)))\n+    left.add(layers.Activation('relu'))\n \n-    right = Sequential()\n-    right.add(Dense(num_hidden, input_shape=(input_dim,)))\n-    right.add(Activation('relu'))\n+    right = models.Sequential()\n+    right.add(layers.Dense(num_hidden, input_shape=(input_dim,)))\n+    right.add(layers.Activation('relu'))\n \n-    model = Sequential()\n-    model.add(Merge([left, right], mode='sum'))\n-    model.add(Dense(num_class))\n-    model.add(Activation('softmax'))\n+    model = models.Sequential()\n+    model.add(layers.Merge([left, right], mode='sum'))\n+    model.add(layers.Dense(num_class))\n+    model.add(layers.Activation('softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n     model.fit([x_train, x_train], y_train, batch_size=batch_size, epochs=epochs, verbose=0, validation_data=([x_test, x_test], y_test))\n\n@@ -1,30 +0,0 @@\n-import pytest\n-import keras\n-from keras import backend as K\n-from keras.utils.generic_utils import custom_object_scope, get_custom_objects, get_from_module\n-\n-\n-def test_custom_object_scope_adds_objects():\n-    get_custom_objects().clear()\n-    assert (len(get_custom_objects()) == 0)\n-    with custom_object_scope({\"Test1\": object, \"Test2\": object}, {\"Test3\": object}):\n-        assert (len(get_custom_objects()) == 3)\n-    assert (len(get_custom_objects()) == 0)\n-\n-\n-class CustomObject(object):\n-    def __init__(self):\n-        pass\n-\n-\n-def test_get_from_module_uses_custom_object():\n-    get_custom_objects().clear()\n-    assert (get_from_module(\"CustomObject\", globals(), \"test_generic_utils\") == CustomObject)\n-    with pytest.raises(ValueError):\n-        get_from_module(\"TestObject\", globals(), \"test_generic_utils\")\n-    with custom_object_scope({\"TestObject\": CustomObject}):\n-        assert (get_from_module(\"TestObject\", globals(), \"test_generic_utils\") == CustomObject)\n-\n-\n-if __name__ == '__main__':\n-    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ebe84eb3a1c3beb44e25fa4f00a89f2c71dde2cc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 43 | Files Changed: 6 | Hunks: 22 | Methods Changed: 13 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 81 | Churn Cumulative: 17239 | Contributors (this commit): 91 | Commits (past 90d): 86 | Contributors (cumulative): 109 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2065,8 +2065,6 @@ class Container(Layer):\n     def from_config(cls, config, custom_objects=None):\n         \"\"\"Instantiates a Model from its config (output of `get_config()`).\n         \"\"\"\n-        from keras.utils.layer_utils import layer_from_config\n-\n         # layer instances created during\n         # the graph reconstruction process\n         created_layers = {}\n@@ -2077,7 +2075,8 @@ class Container(Layer):\n             layer_name = layer_data['name']\n \n             # Instantiate layer.\n-            layer = layer_from_config(layer_data,\n+            from ..layers import deserialize as deserialize_layer\n+            layer = deserialize_layer(layer_data,\n                                       custom_objects=custom_objects)\n             created_layers[layer_name] = layer\n \n\n@@ -16,3 +16,24 @@ from .noise import *\n from .advanced_activations import *\n from .wrappers import *\n from .convolutional_recurrent import *\n+\n+\n+def deserialize(config, custom_objects=None):\n+    \"\"\"Instantiate a layer from a config dictionary.\n+\n+    # Arguments\n+        config: dict of the form {'class_name': str, 'config': dict}\n+        custom_objects: dict mapping class names (or function names)\n+            of custom (non-Keras) objects to class/functions\n+\n+    # Returns\n+        Layer instance (may be Model, Sequential, Layer...)\n+    \"\"\"\n+    from .. import models\n+    globs = globals()  # All layers.\n+    globs['Model'] = models.Model\n+    globs['Sequential'] = models.Sequential\n+    return deserialize_keras_object(config,\n+                                    module_objects=globs,\n+                                    custom_objects=custom_objects,\n+                                    printable_module_name='layer')\n\n@@ -47,8 +47,8 @@ class Wrapper(Layer):\n \n     @classmethod\n     def from_config(cls, config):\n-        from keras.utils.layer_utils import layer_from_config\n-        layer = layer_from_config(config.pop('layer'))\n+        from . import deserialize as deserialize_layer\n+        layer = deserialize_layer(config.pop('layer'))\n         return cls(layer, **config)\n \n \n\n@@ -11,6 +11,7 @@ import numpy as np\n \n from . import backend as K\n from . import optimizers\n+from . import layers\n from .utils.io_utils import ask_to_proceed_with_overwrite\n from .engine.training import Model\n from .engine import topology\n@@ -68,10 +69,10 @@ def save_model(model, filepath, overwrite=True):\n \n     model_weights_group = f.create_group('model_weights')\n     if legacy_models.needs_legacy_support(model):\n-        layers = legacy_models.legacy_sequential_layers(model)\n+        model_layers = legacy_models.legacy_sequential_layers(model)\n     else:\n-        layers = model.layers\n-    topology.save_weights_to_hdf5_group(model_weights_group, layers)\n+        model_layers = model.layers\n+    topology.save_weights_to_hdf5_group(model_weights_group, model_layers)\n \n     if hasattr(model, 'optimizer'):\n         if isinstance(model.optimizer, optimizers.TFOptimizer):\n@@ -205,30 +206,27 @@ def load_model(filepath, custom_objects=None):\n \n \n def model_from_config(config, custom_objects=None):\n-    from keras.utils.layer_utils import layer_from_config\n     if isinstance(config, list):\n         raise TypeError('`model_fom_config` expects a dictionary, not a list. '\n                         'Maybe you meant to use '\n                         '`Sequential.from_config(config)`?')\n-    return layer_from_config(config, custom_objects=custom_objects)\n+    return layers.deserialize(config, custom_objects=custom_objects)\n \n \n def model_from_yaml(yaml_string, custom_objects=None):\n     \"\"\"Parses a yaml model configuration file\n     and returns a model instance.\n     \"\"\"\n-    from keras.utils.layer_utils import layer_from_config\n     config = yaml.load(yaml_string)\n-    return layer_from_config(config, custom_objects=custom_objects)\n+    return layers.deserialize(config, custom_objects=custom_objects)\n \n \n def model_from_json(json_string, custom_objects=None):\n     \"\"\"Parses a JSON model configuration file\n     and returns a model instance.\n     \"\"\"\n-    from keras.utils.layer_utils import layer_from_config\n     config = json.loads(json_string)\n-    return layer_from_config(config, custom_objects=custom_objects)\n+    return layers.deserialize(config, custom_objects=custom_objects)\n \n \n class Sequential(Model):\n@@ -1001,14 +999,12 @@ class Sequential(Model):\n \n     @classmethod\n     def from_config(cls, config):\n-        from keras.utils.layer_utils import layer_from_config\n-\n         if 'class_name' not in config[0] or config[0]['class_name'] == 'Merge':\n             return cls.legacy_from_config(config)\n \n         model = cls()\n         for conf in config:\n-            layer = layer_from_config(conf)\n+            layer = layers.deserialize(conf)\n             model.add(layer)\n         return model\n \n@@ -1038,8 +1034,6 @@ class Sequential(Model):\n     def legacy_from_config(cls, config, layer_cache=None):\n         \"\"\"Supports legacy formats.\n         \"\"\"\n-        from keras.utils.layer_utils import layer_from_config\n-\n         if not layer_cache:\n             layer_cache = {}\n \n@@ -1065,7 +1059,7 @@ class Sequential(Model):\n             name = layer_data['config'].get('name')\n             if name in layer_cache:\n                 return layer_cache[name]\n-            layer = layer_from_config(layer_data)\n+            layer = layers.deserialize(layer_data)\n             layer_cache[name] = layer\n             return layer\n \n@@ -1075,7 +1069,7 @@ class Sequential(Model):\n             merge_inputs = []\n             first_layer_config = first_layer['config']\n             for merge_input_config in first_layer_config.pop('layers'):\n-                merge_input = layer_from_config(merge_input_config)\n+                merge_input = layers.deserialize(merge_input_config)\n                 merge_inputs.append(merge_input)\n             first_layer_config['layers'] = merge_inputs\n             merge = legacy_layers.Merge.from_config(first_layer_config)\n\n@@ -8,25 +8,6 @@ from ..models import Model, Sequential\n from .. import backend as K\n \n \n-def layer_from_config(config, custom_objects=None):\n-    \"\"\"Instantiate a layer from a config dictionary.\n-\n-    # Arguments\n-        config: dict of the form {'class_name': str, 'config': dict}\n-        custom_objects: dict mapping class names (or function names)\n-            of custom (non-Keras) objects to class/functions\n-\n-    # Returns\n-        Layer instance (may be Model, Sequential, Layer...)\n-\n-    # TODO: rename to \"deserialize\" and move to \"layers.__init__.py\"\n-    \"\"\"\n-    return deserialize_keras_object(config,\n-                                    module_objects=globals(),\n-                                    custom_objects=custom_objects,\n-                                    printable_module_name='layer')\n-\n-\n def print_summary(model, line_length=None, positions=None):\n     \"\"\"Prints a summary of a layer.\n \n\n@@ -8,7 +8,7 @@ from keras.utils.test_utils import layer_test\n from keras.utils.test_utils import keras_test\n from keras import regularizers\n from keras import constraints\n-from keras.utils.layer_utils import layer_from_config\n+from keras.layers import deserialize as deserialize_layer\n \n \n @keras_test\n@@ -103,7 +103,7 @@ def test_lambda():\n \n     ld = layers.Lambda(f)\n     config = ld.get_config()\n-    ld = layer_from_config({'class_name': 'Lambda', 'config': config})\n+    ld = deserialize_layer({'class_name': 'Lambda', 'config': config})\n \n     # test with lambda\n     ld = layers.Lambda(\n@@ -121,7 +121,7 @@ def test_lambda():\n \n     ld = layers.Lambda(f, output_shape=f_shape)\n     config = ld.get_config()\n-    ld = layer_from_config({'class_name': 'Lambda', 'config': config})\n+    ld = deserialize_layer({'class_name': 'Lambda', 'config': config})\n \n \n @keras_test\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
