{"custom_id": "keras#883f74ca410e822fba266c4c344a09e364693951", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 110 | Lines Deleted: 90 | Files Changed: 1 | Hunks: 66 | Methods Changed: 12 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 200 | Churn Cumulative: 690 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 0.3780487804878049\n\nDIFF:\n@@ -6,36 +6,31 @@ the different fonts thrown at it...the purpose is more to demonstrate CTC\n inside of Keras.  Note that the font list may need to be updated\n for the particular OS in use.\n \n-This starts off with 4 letter words. After 10 or so epochs, CTC\n-learns translational invariance, so longer words and groups of words\n-with spaces are gradually fed in.  This gradual increase in difficulty\n-is handled using the TextImageGenerator class which is both a generator\n-class for test/train data and a Keras callback class. Every 10 epochs\n-the wordlist that the generator draws from increases in difficulty.\n+This starts off with 4 letter words.  For the first 12 epochs, the\n+difficulty is gradually increased using the TextImageGenerator class\n+which is both a generator class for test/train data and a Keras\n+callback class. After 20 epochs, longer sequences are thrown at it\n+by recompiling the model to handle a wider image and rebuilding\n+the word list to include two words separated by a space.\n \n The table below shows normalized edit distance values. Theano uses\n-a slightly different CTC implementation, so some Theano-specific\n-hyperparameter tuning would be needed to get it to match Tensorflow.\n+a slightly different CTC implementation, hence the different results.\n \n             Norm. ED\n Epoch |   TF   |   TH\n ------------------------\n-    10   0.072    0.272\n-    20   0.032    0.115\n-    30   0.024    0.098\n-    40   0.023    0.108\n+    10   0.027   0.064\n+    15   0.038   0.035\n+    20   0.043   0.045\n+    25   0.014   0.019\n \n This requires cairo and editdistance packages:\n pip install cairocffi\n pip install editdistance\n \n-Due to the use of a dummy loss function, Theano requires the following flags:\n-on_unused_input='ignore'\n-\n Created by Mike Henry\n https://github.com/mbhenry/\n '''\n-\n import os\n import itertools\n import re\n@@ -57,7 +52,8 @@ from keras.utils.data_utils import get_file\n from keras.preprocessing import image\n import keras.callbacks\n \n-OUTPUT_DIR = \"image_ocr\"\n+\n+OUTPUT_DIR = 'image_ocr'\n \n np.random.seed(55)\n \n@@ -79,28 +75,33 @@ def speckle(img):\n # also uses a random font, a slight random rotation,\n # and a random amount of speckle noise\n \n-def paint_text(text, w, h):\n+def paint_text(text, w, h, rotate=False, ud=False, multi_fonts=False):\n     surface = cairo.ImageSurface(cairo.FORMAT_RGB24, w, h)\n     with cairo.Context(surface) as context:\n         context.set_source_rgb(1, 1, 1)  # White\n         context.paint()\n         # this font list works in Centos 7\n+        if multi_fonts:\n             fonts = ['Century Schoolbook', 'Courier', 'STIX', 'URW Chancery L', 'FreeMono']\n             context.select_font_face(np.random.choice(fonts), cairo.FONT_SLANT_NORMAL,\n                                      np.random.choice([cairo.FONT_WEIGHT_BOLD, cairo.FONT_WEIGHT_NORMAL]))\n-        context.set_font_size(40)\n+        else:\n+            context.select_font_face('Courier', cairo.FONT_SLANT_NORMAL, cairo.FONT_WEIGHT_BOLD)\n+        context.set_font_size(25)\n         box = context.text_extents(text)\n-        if box[2] > w or box[3] > h:\n+        border_w_h = (4, 4)\n+        if box[2] > (w - 2 * border_w_h[1]) or box[3] > (h - 2 * border_w_h[0]):\n             raise IOError('Could not fit string into image. Max char count is too large for given image width.')\n \n         # teach the RNN translational invariance by\n         # fitting text box randomly on canvas, with some room to rotate\n-        border_w_h = (10, 16)\n         max_shift_x = w - box[2] - border_w_h[0]\n         max_shift_y = h - box[3] - border_w_h[1]\n         top_left_x = np.random.randint(0, int(max_shift_x))\n+        if ud:\n             top_left_y = np.random.randint(0, int(max_shift_y))\n-\n+        else:\n+            top_left_y = h // 2\n         context.move_to(top_left_x - int(box[0]), top_left_y - int(box[1]))\n         context.set_source_rgb(0, 0, 0)\n         context.show_text(text)\n@@ -111,8 +112,9 @@ def paint_text(text, w, h):\n     a = a[:, :, 0]  # grab single channel\n     a = a.astype(np.float32) / 255\n     a = np.expand_dims(a, 0)\n-    a = speckle(a)\n+    if rotate:\n         a = image.random_rotation(a, 3 * (w - top_left_x) / w + 1)\n+    a = speckle(a)\n \n     return a\n \n@@ -164,7 +166,7 @@ def is_valid_str(in_str):\n class TextImageGenerator(keras.callbacks.Callback):\n \n     def __init__(self, monogram_file, bigram_file, minibatch_size,\n-                 img_w, img_h, downsample_width, val_split,\n+                 img_w, img_h, downsample_factor, val_split,\n                  absolute_max_string_len=16):\n \n         self.minibatch_size = minibatch_size\n@@ -172,7 +174,7 @@ class TextImageGenerator(keras.callbacks.Callback):\n         self.img_h = img_h\n         self.monogram_file = monogram_file\n         self.bigram_file = bigram_file\n-        self.downsample_width = downsample_width\n+        self.downsample_factor = downsample_factor\n         self.val_split = val_split\n         self.blank_label = self.get_output_size() - 1\n         self.absolute_max_string_len = absolute_max_string_len\n@@ -187,7 +189,8 @@ class TextImageGenerator(keras.callbacks.Callback):\n         assert num_words % self.minibatch_size == 0\n         assert (self.val_split * num_words) % self.minibatch_size == 0\n         self.num_words = num_words\n-        self.string_list = []\n+        self.string_list = [''] * self.num_words\n+        tmp_string_list = []\n         self.max_string_len = max_string_len\n         self.Y_data = np.ones([self.num_words, self.absolute_max_string_len]) * -1\n         self.X_text = []\n@@ -196,25 +199,28 @@ class TextImageGenerator(keras.callbacks.Callback):\n         # monogram file is sorted by frequency in english speech\n         with open(self.monogram_file, 'rt') as f:\n             for line in f:\n-                if len(self.string_list) == int(self.num_words * mono_fraction):\n+                if len(tmp_string_list) == int(self.num_words * mono_fraction):\n                     break\n                 word = line.rstrip()\n                 if max_string_len == -1 or max_string_len is None or len(word) <= max_string_len:\n-                    self.string_list.append(word)\n+                    tmp_string_list.append(word)\n \n         # bigram file contains common word pairings in english speech\n         with open(self.bigram_file, 'rt') as f:\n             lines = f.readlines()\n             for line in lines:\n-                if len(self.string_list) == self.num_words:\n+                if len(tmp_string_list) == self.num_words:\n                     break\n                 columns = line.lower().split()\n                 word = columns[0] + ' ' + columns[1]\n                 if is_valid_str(word) and \\\n                         (max_string_len == -1 or max_string_len is None or len(word) <= max_string_len):\n-                    self.string_list.append(word)\n-        if len(self.string_list) != self.num_words:\n+                    tmp_string_list.append(word)\n+        if len(tmp_string_list) != self.num_words:\n             raise IOError('Could not pull enough words from supplied monogram and bigram files. ')\n+        # interlace to mix up the easy and hard words\n+        self.string_list[::2] = tmp_string_list[:self.num_words // 2]\n+        self.string_list[1::2] = tmp_string_list[self.num_words // 2:]\n \n         for i, word in enumerate(self.string_list):\n             self.Y_len[i] = len(word)\n@@ -228,37 +234,38 @@ class TextImageGenerator(keras.callbacks.Callback):\n     # each time an image is requested from train/val/test, a new random\n     # painting of the text is performed\n     def get_batch(self, index, size, train):\n+        # width and height are backwards from typical Keras convention\n+        # because width is the time dimension when it gets fed into the RNN\n         if K.image_dim_ordering() == 'th':\n-            X_data = np.ones([size, 1, self.img_h, self.img_w])\n+            X_data = np.ones([size, 1, self.img_w, self.img_h])\n         else:\n-            X_data = np.ones([size, self.img_h, self.img_w, 1])\n+            X_data = np.ones([size, self.img_w, self.img_h, 1])\n+\n         labels = np.ones([size, self.absolute_max_string_len])\n         input_length = np.zeros([size, 1])\n         label_length = np.zeros([size, 1])\n         source_str = []\n-\n         for i in range(0, size):\n             # Mix in some blank inputs.  This seems to be important for\n             # achieving translational invariance\n             if train and i > size - 4:\n                 if K.image_dim_ordering() == 'th':\n-                    X_data[i, 0, :, :] = paint_text('', self.img_w, self.img_h)\n+                    X_data[i, 0, 0:self.img_w, :] = self.paint_func('')[0, :, :].T\n                 else:\n-                    X_data[i, :, :, 0] = paint_text('', self.img_w, self.img_h)\n+                    X_data[i, 0:self.img_w, :, 0] = self.paint_func('',)[0, :, :].T\n                 labels[i, 0] = self.blank_label\n-                input_length[i] = self.downsample_width\n+                input_length[i] = self.img_w // self.downsample_factor - 2\n                 label_length[i] = 1\n                 source_str.append('')\n             else:\n                 if K.image_dim_ordering() == 'th':\n-                    X_data[i, 0, :, :] = paint_text(self.X_text[index + i], self.img_w, self.img_h)\n+                    X_data[i, 0, 0:self.img_w, :] = self.paint_func(self.X_text[index + i])[0, :, :].T\n                 else:\n-                    X_data[i, :, :, 0] = paint_text(self.X_text[index + i], self.img_w, self.img_h)\n+                    X_data[i, 0:self.img_w, :, 0] = self.paint_func(self.X_text[index + i])[0, :, :].T\n                 labels[i, :] = self.Y_data[index + i]\n-                input_length[i] = self.downsample_width\n+                input_length[i] = self.img_w // self.downsample_factor - 2\n                 label_length[i] = self.Y_len[index + i]\n                 source_str.append(self.X_text[index + i])\n-\n         inputs = {'the_input': X_data,\n                   'the_labels': labels,\n                   'input_length': input_length,\n@@ -287,19 +294,23 @@ class TextImageGenerator(keras.callbacks.Callback):\n             yield ret\n \n     def on_train_begin(self, logs={}):\n-        # translational invariance seems to be the hardest thing\n-        # for the RNN to learn, so start with <= 4 letter words.\n         self.build_word_list(16000, 4, 1)\n+        self.paint_func = lambda text: paint_text(text, self.img_w, self.img_h,\n+                                                  rotate=False, ud=False, multi_fonts=False)\n \n     def on_epoch_begin(self, epoch, logs={}):\n-        # After 10 epochs, translational invariance should be learned\n-        # so start feeding longer words and eventually multiple words with spaces\n-        if epoch == 10:\n-            self.build_word_list(32000, 8, 1)\n-        if epoch == 20:\n-            self.build_word_list(32000, 8, 0.6)\n-        if epoch == 30:\n-            self.build_word_list(64000, 12, 0.5)\n+        # rebind the paint function to implement curriculum learning\n+        if epoch >= 3 and epoch < 6:\n+            self.paint_func = lambda text: paint_text(text, self.img_w, self.img_h,\n+                                                      rotate=False, ud=True, multi_fonts=False)\n+        elif epoch >= 6 and epoch < 9:\n+            self.paint_func = lambda text: paint_text(text, self.img_w, self.img_h,\n+                                                      rotate=False, ud=True, multi_fonts=True)\n+        elif epoch >= 9:\n+            self.paint_func = lambda text: paint_text(text, self.img_w, self.img_h,\n+                                                      rotate=True, ud=True, multi_fonts=True)\n+        if epoch >= 21 and self.max_string_len < 12:\n+            self.build_word_list(32000, 12, 0.5)\n \n \n # the actual loss calc occurs here despite it not being\n@@ -335,12 +346,13 @@ def decode_batch(test_func, word_batch):\n \n class VizCallback(keras.callbacks.Callback):\n \n-    def __init__(self, test_func, text_img_gen, num_display_words=6):\n+    def __init__(self, run_name, test_func, text_img_gen, num_display_words=6):\n         self.test_func = test_func\n         self.output_dir = os.path.join(\n-            OUTPUT_DIR, datetime.datetime.now().strftime('%A, %d. %B %Y %I.%M%p'))\n+            OUTPUT_DIR, run_name)\n         self.text_img_gen = text_img_gen\n         self.num_display_words = num_display_words\n+        if not os.path.exists(self.output_dir):\n             os.makedirs(self.output_dir)\n \n     def show_edit_distance(self, num):\n@@ -362,28 +374,31 @@ class VizCallback(keras.callbacks.Callback):\n               % (num, mean_ed, mean_norm_ed))\n \n     def on_epoch_end(self, epoch, logs={}):\n-        self.model.save_weights(os.path.join(self.output_dir, 'weights%02d.h5' % epoch))\n+        self.model.save_weights(os.path.join(self.output_dir, 'weights%02d.h5' % (epoch)))\n         self.show_edit_distance(256)\n         word_batch = next(self.text_img_gen)[0]\n         res = decode_batch(self.test_func, word_batch['the_input'][0:self.num_display_words])\n-\n+        if word_batch['the_input'][0].shape[0] < 256:\n+            cols = 2\n+        else:\n+            cols = 1\n         for i in range(self.num_display_words):\n-            pylab.subplot(self.num_display_words, 1, i + 1)\n+            pylab.subplot(self.num_display_words // cols, cols, i + 1)\n             if K.image_dim_ordering() == 'th':\n                 the_input = word_batch['the_input'][i, 0, :, :]\n             else:\n                 the_input = word_batch['the_input'][i, :, :, 0]\n-            pylab.imshow(the_input, cmap='Greys_r')\n-            pylab.xlabel('Truth = \\'%s\\' Decoded = \\'%s\\'' % (word_batch['source_str'][i], res[i]))\n+            pylab.imshow(the_input.T, cmap='Greys_r')\n+            pylab.xlabel('Truth = \\'%s\\'\\nDecoded = \\'%s\\'' % (word_batch['source_str'][i], res[i]))\n         fig = pylab.gcf()\n-        fig.set_size_inches(10, 12)\n-        pylab.savefig(os.path.join(self.output_dir, 'e%02d.png' % epoch))\n+        fig.set_size_inches(10, 13)\n+        pylab.savefig(os.path.join(self.output_dir, 'e%02d.png' % (epoch)))\n         pylab.close()\n \n+\n+def train(run_name, start_epoch, stop_epoch, img_w):\n     # Input Parameters\n     img_h = 64\n-img_w = 512\n-nb_epoch = 50\n     minibatch_size = 32\n     words_per_epoch = 16000\n     val_split = 0.2\n@@ -392,16 +407,14 @@ val_words = int(words_per_epoch * (val_split))\n     # Network parameters\n     conv_num_filters = 16\n     filter_size = 3\n-pool_size_1 = 4\n-pool_size_2 = 2\n+    pool_size = 2\n     time_dense_size = 32\n     rnn_size = 512\n-time_steps = img_w // (pool_size_1 * pool_size_2)\n \n     if K.image_dim_ordering() == 'th':\n-    input_shape = (1, img_h, img_w)\n+        input_shape = (1, img_w, img_h)\n     else:\n-    input_shape = (img_h, img_w, 1)\n+        input_shape = (img_w, img_h, 1)\n \n     fdir = os.path.dirname(get_file('wordlists.tgz',\n                                     origin='http://www.isosemi.com/datasets/wordlists.tgz', untar=True))\n@@ -411,35 +424,35 @@ img_gen = TextImageGenerator(monogram_file=os.path.join(fdir, 'wordlist_mono_cle\n                                  minibatch_size=32,\n                                  img_w=img_w,\n                                  img_h=img_h,\n-                             downsample_width=img_w // (pool_size_1 * pool_size_2) - 2,\n-                             val_split=words_per_epoch - val_words)\n-\n+                                 downsample_factor=(pool_size ** 2),\n+                                 val_split=words_per_epoch - val_words\n+                                 )\n     act = 'relu'\n     input_data = Input(name='the_input', shape=input_shape, dtype='float32')\n     inner = Convolution2D(conv_num_filters, filter_size, filter_size, border_mode='same',\n-                      activation=act, name='conv1')(input_data)\n-inner = MaxPooling2D(pool_size=(pool_size_1, pool_size_1), name='max1')(inner)\n+                          activation=act, init='he_normal', name='conv1')(input_data)\n+    inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max1')(inner)\n     inner = Convolution2D(conv_num_filters, filter_size, filter_size, border_mode='same',\n-                      activation=act, name='conv2')(inner)\n-inner = MaxPooling2D(pool_size=(pool_size_2, pool_size_2), name='max2')(inner)\n+                          activation=act, init='he_normal', name='conv2')(inner)\n+    inner = MaxPooling2D(pool_size=(pool_size, pool_size), name='max2')(inner)\n \n-conv_to_rnn_dims = ((img_h // (pool_size_1 * pool_size_2)) * conv_num_filters, img_w // (pool_size_1 * pool_size_2))\n+    conv_to_rnn_dims = (img_w // (pool_size ** 2), (img_h // (pool_size ** 2)) * conv_num_filters)\n     inner = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(inner)\n-inner = Permute(dims=(2, 1), name='permute')(inner)\n \n     # cuts down input size going into RNN:\n-inner = TimeDistributed(Dense(time_dense_size, activation=act, name='dense1'))(inner)\n+    inner = Dense(time_dense_size, activation=act, name='dense1')(inner)\n \n     # Two layers of bidirecitonal GRUs\n     # GRU seems to work as well, if not better than LSTM:\n-gru_1 = GRU(rnn_size, return_sequences=True, name='gru1')(inner)\n-gru_1b = GRU(rnn_size, return_sequences=True, go_backwards=True, name='gru1_b')(inner)\n+    gru_1 = GRU(rnn_size, return_sequences=True, init='he_normal', name='gru1')(inner)\n+    gru_1b = GRU(rnn_size, return_sequences=True, go_backwards=True, init='he_normal', name='gru1_b')(inner)\n     gru1_merged = merge([gru_1, gru_1b], mode='sum')\n-gru_2 = GRU(rnn_size, return_sequences=True, name='gru2')(gru1_merged)\n-gru_2b = GRU(rnn_size, return_sequences=True, go_backwards=True)(gru1_merged)\n+    gru_2 = GRU(rnn_size, return_sequences=True, init='he_normal', name='gru2')(gru1_merged)\n+    gru_2b = GRU(rnn_size, return_sequences=True, go_backwards=True, init='he_normal', name='gru2_b')(gru1_merged)\n \n     # transforms RNN output to character activations:\n-inner = TimeDistributed(Dense(img_gen.get_output_size(), name='dense2'))(merge([gru_2, gru_2b], mode='concat'))\n+    inner = Dense(img_gen.get_output_size(), init='he_normal',\n+                  name='dense2')(merge([gru_2, gru_2b], mode='concat'))\n     y_pred = Activation('softmax', name='softmax')(inner)\n     Model(input=[input_data], output=y_pred).summary()\n \n@@ -448,23 +461,30 @@ input_length = Input(name='input_length', shape=[1], dtype='int64')\n     label_length = Input(name='label_length', shape=[1], dtype='int64')\n     # Keras doesn't currently support loss funcs with extra parameters\n     # so CTC loss is implemented in a lambda layer\n-loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name=\"ctc\")([y_pred, labels, input_length, label_length])\n+    loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([y_pred, labels, input_length, label_length])\n \n-lr = 0.03\n     # clipnorm seems to speeds up convergence\n-clipnorm = 5\n-sgd = SGD(lr=lr, decay=3e-7, momentum=0.9, nesterov=True, clipnorm=clipnorm)\n+    sgd = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)\n \n     model = Model(input=[input_data, labels, input_length, label_length], output=[loss_out])\n \n     # the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n     model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd)\n-\n+    if start_epoch > 0:\n+        weight_file = os.path.join(OUTPUT_DIR, os.path.join(run_name, 'weights%02d.h5' % (start_epoch - 1)))\n+        model.load_weights(weight_file)\n     # captures output of softmax so we can decode the output during visualization\n     test_func = K.function([input_data], [y_pred])\n \n-viz_cb = VizCallback(test_func, img_gen.next_val())\n+    viz_cb = VizCallback(run_name, test_func, img_gen.next_val())\n \n     model.fit_generator(generator=img_gen.next_train(), samples_per_epoch=(words_per_epoch - val_words),\n-                    nb_epoch=nb_epoch, validation_data=img_gen.next_val(), nb_val_samples=val_words,\n-                    callbacks=[viz_cb, img_gen])\n+                        nb_epoch=stop_epoch, validation_data=img_gen.next_val(), nb_val_samples=val_words,\n+                        callbacks=[viz_cb, img_gen], initial_epoch=start_epoch)\n+\n+\n+if __name__ == '__main__':\n+    run_name = datetime.datetime.now().strftime('%Y:%m:%d:%H:%M:%S')\n+    train(run_name, 0, 20, 128)\n+    # increase to wider images and start at epoch 20. The learned weights are reloaded\n+    train(run_name, 20, 25, 512)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c2e36f369b411ad1d0a40ac096fe35f73b9dffd3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 328 | Contributors (this commit): 8 | Commits (past 90d): 9 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -108,7 +108,7 @@ def kullback_leibler_divergence(y_true, y_pred):\n     '''\n     y_true = K.clip(y_true, K.epsilon(), 1)\n     y_pred = K.clip(y_pred, K.epsilon(), 1)\n-    return K.sum(y_true * K.log(y_true / y_pred), axis=-1)\n+    return K.mean(K.sum(y_true * K.log(y_true / y_pred), axis=-1))\n \n \n def poisson(y_true, y_pred):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#737bea8f39b06b6a19fee34b82c535309512637c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 5604 | Contributors (this commit): 31 | Commits (past 90d): 14 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1649,6 +1649,9 @@ class Cropping1D(Layer):\n                 input_shape[2])\n \n     def call(self, x, mask=None):\n+        if self.cropping[1] == 0:\n+            return x[:, self.cropping[0]:, :]\n+        else:\n             return x[:, self.cropping[0]:-self.cropping[1], :]\n \n     def get_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ffe013033e7cb20b03129d346561e85b69e878a4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 7 | Churn Cumulative: 525 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -20,13 +20,6 @@ class Wrapper(Layer):\n         self.losses = getattr(self.layer, 'losses', [])\n         self.constraints = getattr(self.layer, 'constraints', {})\n \n-        # properly attribute the current layer to\n-        # regularizers that need access to it\n-        # (e.g. ActivityRegularizer).\n-        for regularizer in self.regularizers:\n-            if hasattr(regularizer, 'set_layer'):\n-                regularizer.set_layer(self)\n-\n     def get_weights(self):\n         weights = self.layer.get_weights()\n         return weights\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3d4a48b12011717034e985a59e695c3d04a843dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 3199 | Contributors (this commit): 48 | Commits (past 90d): 27 | Contributors (cumulative): 48 | DMM Complexity: 1.0\n\nDIFF:\n@@ -582,6 +582,10 @@ def reshape(x, shape):\n     y = T.reshape(x, shape)\n     if is_explicit_shape(shape):\n         y._keras_shape = shape\n+        if hasattr(x, '_uses_learning_phase'):\n+            y._uses_learning_phase = x._uses_learning_phase\n+        else:\n+            y._uses_learning_phase = False\n     return y\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#50f7f03f6bc373b81ae9407f7857112e062c526f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 3200 | Contributors (this commit): 48 | Commits (past 90d): 28 | Contributors (cumulative): 48 | DMM Complexity: None\n\nDIFF:\n@@ -66,7 +66,6 @@ def is_explicit_shape(shape):\n     return False\n \n \n-\n def variable(value, dtype=None, name=None):\n     '''Instantiates a variable and returns it.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0e18cb3efa2261d1ae1a1ca2bd073a05a31e798b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 900 | Contributors (this commit): 14 | Commits (past 90d): 2 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -79,8 +79,8 @@ class PReLU(Layer):\n         self.param_broadcast = [False] * len(param_shape)\n         if self.shared_axes[0] is not None:\n             for i in self.shared_axes:\n-                param_shape[i] = 1\n-                self.param_broadcast[i] = True\n+                param_shape[i - 1] = 1\n+                self.param_broadcast[i - 1] = True\n \n         self.alphas = self.init(param_shape,\n                                 name='{}_alphas'.format(self.name))\n@@ -182,8 +182,8 @@ class ParametricSoftplus(Layer):\n         self.param_broadcast = [False] * len(param_shape)\n         if self.shared_axes[0] is not None:\n             for i in self.shared_axes:\n-                param_shape[i] = 1\n-                self.param_broadcast[i] = True\n+                param_shape[i - 1] = 1\n+                self.param_broadcast[i - 1] = True\n \n         self.alphas = K.variable(self.alpha_init * np.ones(param_shape),\n                                  name='{}_alphas'.format(self.name))\n@@ -287,8 +287,8 @@ class SReLU(Layer):\n         self.param_broadcast = [False] * len(param_shape)\n         if self.shared_axes[0] is not None:\n             for i in self.shared_axes:\n-                param_shape[i] = 1\n-                self.param_broadcast[i] = True\n+                param_shape[i - 1] = 1\n+                self.param_broadcast[i - 1] = True\n \n         t_left_init = initializations.get(self.t_left_init)\n         a_left_init = initializations.get(self.a_left_init)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f573a86b42e49754386e536358e08e861d40d24c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 27 | Churn Cumulative: 5642 | Contributors (this commit): 26 | Commits (past 90d): 25 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -927,7 +927,10 @@ class Layer(object):\n     def get_updates_for(self, inputs):\n         if not hasattr(self, '_per_input_updates'):\n             return []\n+        if inputs is not None:\n             inputs_hash = object_list_uid(inputs)\n+        else:\n+            inputs_hash = None\n         if inputs_hash in self._per_input_updates:\n             return self._per_input_updates[inputs_hash]\n         return []\n@@ -935,7 +938,10 @@ class Layer(object):\n     def get_losses_for(self, inputs):\n         if not hasattr(self, '_per_input_losses'):\n             return []\n+        if inputs is not None:\n             inputs_hash = object_list_uid(inputs)\n+        else:\n+            inputs_hash = None\n         if inputs_hash in self._per_input_losses:\n             return self._per_input_losses[inputs_hash]\n         return []\n\n@@ -9,6 +9,27 @@ from keras import backend as K\n from keras.models import model_from_json, model_from_yaml\n from keras.utils.test_utils import keras_test\n \n+@keras_test\n+def test_get_updates_for():\n+    a = Input(shape=(2,))\n+    dense_layer = Dense(1)\n+    dense_layer.add_update(0, inputs=a)\n+    dense_layer.add_update(1, inputs=None)\n+\n+    assert dense_layer.get_updates_for(a) == [0]\n+    assert dense_layer.get_updates_for(None) == [1]\n+\n+\n+@keras_test\n+def test_get_losses_for():\n+    a = Input(shape=(2,))\n+    dense_layer = Dense(1)\n+    dense_layer.add_loss(0, inputs=a)\n+    dense_layer.add_loss(1, inputs=None)\n+    \n+    assert dense_layer.get_losses_for(a) == [0]\n+    assert dense_layer.get_losses_for(None) == [1]\n+\n \n @keras_test\n def test_trainable_weights():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#21023f7f9cab8042f1e4359348e8416f410855e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 4476 | Contributors (this commit): 50 | Commits (past 90d): 34 | Contributors (cumulative): 50 | DMM Complexity: None\n\nDIFF:\n@@ -1990,7 +1990,7 @@ def sparse_categorical_crossentropy(output, target, from_logits=False):\n         cast(flatten(target), 'int64'))\n     if len(output_shape) == 3:\n         # if our output includes timesteps we need to reshape\n-        return tf.reshape(res, [-1, int(output_shape[-2])])\n+        return tf.reshape(res, tf.shape(output)[:-1])\n     else:\n         return res\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9ee0c8e63448838e7f877059fdd2061a9319b9c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 11 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 25 | Churn Cumulative: 5788 | Contributors (this commit): 32 | Commits (past 90d): 16 | Contributors (cumulative): 33 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n+import functools\n \n from .. import backend as K\n from .. import activations, initializations, regularizers, constraints\n@@ -87,7 +88,7 @@ class Convolution1D(Layer):\n             raise ValueError('Invalid border mode for Convolution1D:', border_mode)\n         self.nb_filter = nb_filter\n         self.filter_length = filter_length\n-        self.init = initializations.get(init, dim_ordering='th')\n+        self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n         self.border_mode = border_mode\n         self.subsample_length = subsample_length\n@@ -115,7 +116,8 @@ class Convolution1D(Layer):\n         self.W_shape = (self.filter_length, 1, input_dim, self.nb_filter)\n \n         self.W = self.add_weight(self.W_shape,\n-                                 initializer=self.init,\n+                                 initializer=functools.partial(self.init,\n+                                                               dim_ordering='th'),\n                                  name='{}_W'.format(self.name),\n                                  regularizer=self.W_regularizer,\n                                  constraint=self.W_constraint)\n@@ -367,7 +369,7 @@ class Convolution2D(Layer):\n         self.nb_filter = nb_filter\n         self.nb_row = nb_row\n         self.nb_col = nb_col\n-        self.init = initializations.get(init, dim_ordering=dim_ordering)\n+        self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n         self.border_mode = border_mode\n         self.subsample = tuple(subsample)\n@@ -397,7 +399,8 @@ class Convolution2D(Layer):\n         else:\n             raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n         self.W = self.add_weight(self.W_shape,\n-                                 initializer=self.init,\n+                                 initializer=functools.partial(self.init,\n+                                                               dim_ordering=self.dim_ordering),\n                                  name='{}_W'.format(self.name),\n                                  regularizer=self.W_regularizer,\n                                  constraint=self.W_constraint)\n@@ -899,7 +902,7 @@ class SeparableConvolution2D(Layer):\n         self.nb_filter = nb_filter\n         self.nb_row = nb_row\n         self.nb_col = nb_col\n-        self.init = initializations.get(init, dim_ordering=dim_ordering)\n+        self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n         if border_mode not in {'valid', 'same'}:\n             raise ValueError('border_mode must be in {valid, same}.')\n@@ -937,12 +940,14 @@ class SeparableConvolution2D(Layer):\n             raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n \n         self.depthwise_kernel = self.add_weight(depthwise_shape,\n-                                                initializer=self.init,\n+                                                initializer=functools.partial(self.init,\n+                                                                              dim_ordering=self.dim_ordering),\n                                                 regularizer=self.depthwise_regularizer,\n                                                 constraint=self.depthwise_constraint,\n                                                 name='{}_depthwise_kernel'.format(self.name))\n         self.pointwise_kernel = self.add_weight(pointwise_shape,\n-                                                initializer=self.init,\n+                                                initializer=functools.partial(self.init,\n+                                                                              dim_ordering=self.dim_ordering),\n                                                 regularizer=self.pointwise_regularizer,\n                                                 constraint=self.pointwise_constraint,\n                                                 name='{}_pointwise_kernel'.format(self.name))\n@@ -1093,7 +1098,7 @@ class Convolution3D(Layer):\n         self.kernel_dim1 = kernel_dim1\n         self.kernel_dim2 = kernel_dim2\n         self.kernel_dim3 = kernel_dim3\n-        self.init = initializations.get(init, dim_ordering=dim_ordering)\n+        self.init = initializations.get(init)\n         self.activation = activations.get(activation)\n         self.border_mode = border_mode\n         self.subsample = tuple(subsample)\n@@ -1129,7 +1134,8 @@ class Convolution3D(Layer):\n             raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n \n         self.W = self.add_weight(self.W_shape,\n-                                 initializer=self.init,\n+                                 initializer=functools.partial(self.init,\n+                                                               dim_ordering=self.dim_ordering),\n                                  name='{}_W'.format(self.name),\n                                  regularizer=self.W_regularizer,\n                                  constraint=self.W_constraint)\n\n@@ -13,6 +13,7 @@ CONV_SHAPE = (25, 25, 2, 2)\n # The equivalent shape of both test fixtures\n SHAPE = (100, 100)\n \n+\n def _runner(init, shape, target_mean=None, target_std=None,\n             target_max=None, target_min=None):\n     variable = init(shape)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5d575a3effd3bf491ad14fe95bccc10dc81e34d7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 180 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -76,7 +76,7 @@ def getwhere(x):\n     return K.gradients(K.sum(y_postpool), y_prepool)\r\n \r\n if K.backend() == 'tensorflow':\r\n-    raise Exception('This example can only run with the '\r\n+    raise RuntimeError('This example can only run with the '\r\n                        'Theano backend for the time being, '\r\n                        'because it requires taking the gradient '\r\n                        'of a gradient, which isn\\'t '\r\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#44bf298ec3236f4a7281be04716a58163469b4de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 5 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 1275 | Contributors (this commit): 2 | Commits (past 90d): 23 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -83,7 +83,7 @@ def InceptionV3(include_top=True, weights='imagenet',\n             or \"imagenet\" (pre-training on ImageNet).\n         input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n             to use as image input for the model.\n-        inputs_shape: optional shape tuple, only to be specified\n+        input_shape: optional shape tuple, only to be specified\n             if `include_top` is False (otherwise the input shape\n             has to be `(299, 299, 3)` (with `tf` dim ordering)\n             or `(3, 299, 299)` (with `th` dim ordering).\n\n@@ -129,7 +129,7 @@ def ResNet50(include_top=True, weights='imagenet',\n             or \"imagenet\" (pre-training on ImageNet).\n         input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n             to use as image input for the model.\n-        inputs_shape: optional shape tuple, only to be specified\n+        input_shape: optional shape tuple, only to be specified\n             if `include_top` is False (otherwise the input shape\n             has to be `(224, 224, 3)` (with `tf` dim ordering)\n             or `(3, 224, 244)` (with `th` dim ordering).\n\n@@ -48,7 +48,7 @@ def VGG16(include_top=True, weights='imagenet',\n             or \"imagenet\" (pre-training on ImageNet).\n         input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n             to use as image input for the model.\n-        inputs_shape: optional shape tuple, only to be specified\n+        input_shape: optional shape tuple, only to be specified\n             if `include_top` is False (otherwise the input shape\n             has to be `(224, 224, 3)` (with `tf` dim ordering)\n             or `(3, 224, 244)` (with `th` dim ordering).\n\n@@ -48,7 +48,7 @@ def VGG19(include_top=True, weights='imagenet',\n             or \"imagenet\" (pre-training on ImageNet).\n         input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n             to use as image input for the model.\n-        inputs_shape: optional shape tuple, only to be specified\n+        input_shape: optional shape tuple, only to be specified\n             if `include_top` is False (otherwise the input shape\n             has to be `(224, 224, 3)` (with `tf` dim ordering)\n             or `(3, 224, 244)` (with `th` dim ordering).\n\n@@ -54,7 +54,7 @@ def Xception(include_top=True, weights='imagenet',\n             or \"imagenet\" (pre-training on ImageNet).\n         input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n             to use as image input for the model.\n-        inputs_shape: optional shape tuple, only to be specified\n+        input_shape: optional shape tuple, only to be specified\n             if `include_top` is False (otherwise the input shape\n             has to be `(299, 299, 3)`.\n             It should have exactly 3 inputs channels,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b5df1c617029faf675cdb681a9e0033497245992", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 4480 | Contributors (this commit): 50 | Commits (past 90d): 33 | Contributors (cumulative): 50 | DMM Complexity: None\n\nDIFF:\n@@ -783,14 +783,14 @@ def dot(x, y):\n     if ndim(x) is not None and (ndim(x) > 2 or ndim(y) > 2):\n         x_shape = []\n         for i, s in zip(int_shape(x), tf.unpack(tf.shape(x))):\n-            if s is None:\n+            if i is not None:\n                 x_shape.append(i)\n             else:\n                 x_shape.append(s)\n         x_shape = tuple(x_shape)\n         y_shape = []\n         for i, s in zip(int_shape(y), tf.unpack(tf.shape(y))):\n-            if s is None:\n+            if i is not None:\n                 y_shape.append(i)\n             else:\n                 y_shape.append(s)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f6b804263a6e1d3ce6d2131cb9d758e1d7e57888", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1971 | Contributors (this commit): 37 | Commits (past 90d): 14 | Contributors (cumulative): 37 | DMM Complexity: None\n\nDIFF:\n@@ -274,7 +274,7 @@ class ModelCheckpoint(Callback):\n             self.monitor_op = np.greater\n             self.best = -np.Inf\n         else:\n-            if 'acc' in self.monitor:\n+            if self.monitor.startswith(('acc', 'fmeasure')):\n                 self.monitor_op = np.greater\n                 self.best = -np.Inf\n             else:\n@@ -357,7 +357,7 @@ class EarlyStopping(Callback):\n         elif mode == 'max':\n             self.monitor_op = np.greater\n         else:\n-            if 'acc' in self.monitor:\n+            if self.monitor.startswith(('acc', 'fmeasure')):\n                 self.monitor_op = np.greater\n             else:\n                 self.monitor_op = np.less\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a6c9227372c607fc356b14e17c230cb9c1d5f589", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 22 | Churn Cumulative: 3453 | Contributors (this commit): 49 | Commits (past 90d): 32 | Contributors (cumulative): 52 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -493,6 +493,9 @@ def _old_normalize_batch_in_training(x, gamma, beta,\n         try:\n             normed, mean, stdinv = theano.sandbox.cuda.dnn.dnn_batch_normalization_train(\n                 x, broadcast_gamma, broadcast_beta, 'spatial', epsilon)\n+            normed = theano.tensor.as_tensor_variable(normed)\n+            mean = theano.tensor.as_tensor_variable(mean)\n+            stdinv = theano.tensor.as_tensor_variable(stdinv)\n             var = T.inv(stdinv ** 2)\n             return normed, T.flatten(mean), T.flatten(var)\n         except AttributeError:\n@@ -544,7 +547,7 @@ def _old_batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n                 shuffle_pattern = list(range(ndim))\n                 shuffle_pattern[1] = shuffle_pattern[axis]\n                 shuffle_pattern[axis] = 1\n-                return theano.sandbox.cuda.dnn.dnn_batch_normalization_test(\n+                result = theano.sandbox.cuda.dnn.dnn_batch_normalization_test(\n                     x.dimshuffle(shuffle_pattern),\n                     gamma.dimshuffle(shuffle_pattern),\n                     beta.dimshuffle(shuffle_pattern),\n@@ -552,8 +555,9 @@ def _old_batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n                     var.dimshuffle(shuffle_pattern),\n                     'spatial', epsilon).dimshuffle(shuffle_pattern)\n             else:\n-                return theano.sandbox.cuda.dnn.dnn_batch_normalization_test(\n+                result = theano.sandbox.cuda.dnn.dnn_batch_normalization_test(\n                     x, gamma, beta, mean, var, 'spatial', epsilon)\n+            return theano.tensor.as_tensor_variable(result)\n         except AttributeError:\n             pass\n         except ValueError:\n\n@@ -46,6 +46,20 @@ def test_batchnorm_mode_0_or_2():\n         assert_allclose(out.std(), 1.0, atol=1e-1)\n \n \n+@keras_test\n+def test_batchnorm_mode_0_or_2_twice():\n+    # This is a regression test for issue #4881 with the old\n+    # batch normalization functions in the Theano backend.\n+    model = Sequential()\n+    model.add(normalization.BatchNormalization(mode=0, input_shape=(10, 5, 5), axis=1))\n+    model.add(normalization.BatchNormalization(mode=0, input_shape=(10, 5, 5), axis=1))\n+    model.compile(loss='mse', optimizer='sgd')\n+\n+    X = np.random.normal(loc=5.0, scale=10.0, size=(20, 10, 5, 5))\n+    model.fit(X, X, nb_epoch=1, verbose=0)\n+    model.predict(X)\n+\n+\n @keras_test\n def test_batchnorm_mode_0_convnet():\n     model = Sequential()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0f0d837178da8701eda8b0d0a4b08b653cfcfdd1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 529 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -134,7 +134,7 @@ class TimeDistributed(Wrapper):\n class Bidirectional(Wrapper):\n     ''' Bidirectional wrapper for RNNs.\n \n-    # Arguments:\n+    # Arguments\n         layer: `Recurrent` instance.\n         merge_mode: Mode by which outputs of the\n             forward and backward RNNs will be combined.\n@@ -142,7 +142,7 @@ class Bidirectional(Wrapper):\n             If None, the outputs will not be combined,\n             they will be returned as a list.\n \n-    # Examples:\n+    # Examples\n \n     ```python\n         model = Sequential()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cbefd323be1a0a57589dae04eb4479f2acf3c957", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 13 | Churn Cumulative: 6076 | Contributors (this commit): 56 | Commits (past 90d): 42 | Contributors (cumulative): 67 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2076,9 +2076,12 @@ def in_top_k(predictions, targets, k):\n \n # CONVOLUTIONS\n \n-def _preprocess_deconv_output_shape(shape, dim_ordering):\n+def _preprocess_deconv_output_shape(x, shape, dim_ordering):\n     if dim_ordering == 'th':\n         shape = (shape[0], shape[2], shape[3], shape[1])\n+\n+    if shape[0] is None:\n+        shape = (tf.shape(x)[0], ) + shape[1:]\n     return shape\n \n \n@@ -2233,7 +2236,7 @@ def deconv2d(x, kernel, output_shape, strides=(1, 1),\n         raise ValueError('Unknown dim_ordering ' + str(dim_ordering))\n \n     x = _preprocess_conv2d_input(x, dim_ordering)\n-    output_shape = _preprocess_deconv_output_shape(output_shape, dim_ordering)\n+    output_shape = _preprocess_deconv_output_shape(x, output_shape, dim_ordering)\n     kernel = _preprocess_conv2d_kernel(kernel, dim_ordering)\n     kernel = tf.transpose(kernel, (0, 1, 3, 2))\n     padding = _preprocess_border_mode(border_mode)\n@@ -2443,7 +2446,6 @@ def ctc_label_dense_to_sparse(labels, label_lengths):\n \n \n def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n-\n     '''Runs CTC loss algorithm on each batch element.\n \n     # Arguments\n\n@@ -142,6 +142,7 @@ def test_deconvolution_2d():\n     nb_row = 10\n     nb_col = 6\n \n+    for batch_size in [None, nb_samples]:\n         for border_mode in _convolution_border_modes:\n             for subsample in [(1, 1), (2, 2)]:\n                 if border_mode == 'same' and subsample != (1, 1):\n@@ -153,7 +154,7 @@ def test_deconvolution_2d():\n                            kwargs={'nb_filter': nb_filter,\n                                    'nb_row': 3,\n                                    'nb_col': 3,\n-                               'output_shape': (nb_samples, nb_filter, rows, cols),\n+                                   'output_shape': (batch_size, nb_filter, rows, cols),\n                                    'border_mode': border_mode,\n                                    'subsample': subsample,\n                                    'dim_ordering': 'th'},\n@@ -164,7 +165,7 @@ def test_deconvolution_2d():\n                            kwargs={'nb_filter': nb_filter,\n                                    'nb_row': 3,\n                                    'nb_col': 3,\n-                               'output_shape': (nb_samples, nb_filter, rows, cols),\n+                                   'output_shape': (batch_size, nb_filter, rows, cols),\n                                    'border_mode': border_mode,\n                                    'dim_ordering': 'th',\n                                    'W_regularizer': 'l2',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
