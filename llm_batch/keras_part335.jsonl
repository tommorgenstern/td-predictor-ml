{"custom_id": "keras#58fd0863a1808c4a8959ef30f22d368f6e4c2081", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 111 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -64,9 +64,11 @@ setuptools.setup(\n         'Intended Audience :: Science/Research',\n         'License :: OSI Approved :: Apache Software License',\n         'Programming Language :: Python :: 3',\n-        'Programming Language :: Python :: 3.5',\n         'Programming Language :: Python :: 3.6',\n         'Programming Language :: Python :: 3.7',\n+        'Programming Language :: Python :: 3.8',\n+        'Programming Language :: Python :: 3.9',\n+        'Programming Language :: Python :: 3 :: Only',\n         'Topic :: Scientific/Engineering',\n         'Topic :: Scientific/Engineering :: Mathematics',\n         'Topic :: Scientific/Engineering :: Artificial Intelligence',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f1bb0ec4bdc19798fe041e5b28d18c4abcaf5861", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 616 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -146,10 +146,10 @@ class AutoCastVariableTest(tf.test.TestCase, parameterized.TestCase):\n     # underlying variable.\n     with self.test_session(), distribution.scope():\n       for read_dtype in (tf.float32, tf.float16):\n-        if tf.distribute.has_strategy():\n+        if tf.distribute.has_strategy() and not tf.executing_eagerly():\n           # MirroredVariable.assign will (incorrectly) return a Mirrored value\n-          # instead of a MirroredVariable. So we cannot properly wrap it in an\n-          # AutoCastVariable.\n+          # instead of a MirroredVariable in graph mode.\n+          # So we cannot properly wrap it in an AutoCastVariable.\n           evaluate = self.evaluate\n         else:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a120c621ee694a7583bdd270e9e755d08fa219c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 70 | Lines Deleted: 41 | Files Changed: 6 | Hunks: 36 | Methods Changed: 27 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 111 | Churn Cumulative: 11325 | Contributors (this commit): 12 | Commits (past 90d): 35 | Contributors (cumulative): 26 | DMM Complexity: 0.9615384615384616\n\nDIFF:\n@@ -3022,6 +3022,7 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n \n   @property\n   def _tracking_metadata(self):\n+    \"\"\"Info about this layer to be saved into the SavedModel.\"\"\"\n     return self._trackable_saved_model_saver.tracking_metadata\n \n   def _list_extra_dependencies_for_serialization(self, serialization_cache):\n\n@@ -215,8 +215,8 @@ class TpuStrategyTest(tf.test.TestCase):\n       serving_fn = create_serving_signature(model)\n \n       saved_model_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n-      tf.saved_model.save(\n-          model, saved_model_dir, signatures={\"serving_default\": serving_fn})\n+      model.save(saved_model_dir, save_format=\"tf\",\n+                 signatures={\"serving_default\": serving_fn})\n \n     # Test the saved_model.\n     loaded_serving_fn = tf.keras.models.load_model(\n\n@@ -174,6 +174,12 @@ def _read_legacy_metadata(object_graph_def, metadata):\n   for node_id, proto in enumerate(object_graph_def.nodes):\n     if (proto.WhichOneof('kind') == 'user_object' and\n         proto.user_object.identifier in constants.KERAS_OBJECT_IDENTIFIERS):\n+      if not proto.user_object.metadata:\n+        raise ValueError('Unable to create a Keras model from this SavedModel. '\n+                         'This SavedModel was created with '\n+                         '`tf.saved_model.save`, and lacks the Keras metadata.'\n+                         'Please save your Keras model by calling `model.save`'\n+                         'or `tf.keras.models.save_model`.')\n       metadata.nodes.add(\n           node_id=node_id,\n           node_path=node_paths[node_id],\n@@ -232,7 +238,7 @@ class KerasObjectLoader(object):\n   \"\"\"\n \n   def __init__(self, metadata, object_graph_def):\n-    self._metadata = metadata\n+    self._metadata = {x.node_id: x for x in metadata.nodes}\n     self._proto = object_graph_def\n \n     self._node_paths = {node_data.node_id: node_data.node_path\n@@ -288,7 +294,7 @@ class KerasObjectLoader(object):\n     self._traversed_nodes_from_config.add(node_id)\n     obj._maybe_initialize_trackable()\n     if isinstance(obj, base_layer.Layer) and not obj.built:\n-      metadata = json_utils.decode(proto.user_object.metadata)\n+      metadata = json_utils.decode(self._metadata[node_id].metadata)\n       self._try_build_layer(obj, node_id, metadata.get('build_input_shape'))\n \n     # Create list of all possible children\n@@ -357,7 +363,7 @@ class KerasObjectLoader(object):\n     # and layers will create the metric when initialized (this avoids wasting\n     # time by creating objects multiple times).\n     metric_list = []\n-    for node_metadata in self._metadata.nodes:\n+    for node_metadata in self._metadata.values():\n       if node_metadata.identifier == constants.METRIC_IDENTIFIER:\n         metric_list.append(node_metadata)\n         continue\n@@ -645,8 +651,7 @@ class KerasObjectLoader(object):\n \n   def _reconstruct_model(self, model_id, model, layers):\n     \"\"\"Reconstructs the network structure.\"\"\"\n-    config = json_utils.decode(\n-        self._proto.nodes[model_id].user_object.metadata)['config']\n+    config = json_utils.decode(self._metadata[model_id].metadata)['config']\n \n     # Set up model inputs\n     if model.inputs:\n\n@@ -25,6 +25,7 @@ import tensorflow.compat.v2 as tf\n \n import os\n import shutil\n+import sys\n \n from absl.testing import parameterized\n import numpy as np\n@@ -134,7 +135,7 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n \n   def _save_and_load(self, model):\n     saved_model_dir = self._save_model_dir()\n-    tf.saved_model.save(model, saved_model_dir)\n+    model.save(saved_model_dir, save_format='tf')\n     loaded = keras_load.load(saved_model_dir)\n     return loaded\n \n@@ -177,7 +178,7 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n   def test_trainable_weights(self):\n     \"\"\"Tests that trainable status of individual weights is preserved.\"\"\"\n     layer = keras.layers.Dense(4, name='custom_layer')\n-    layer.build([3,])\n+    layer.build([None, 3])\n     layer.add_weight(\n         'extra_weight', shape=[],\n         initializer=tf.compat.v1.constant_initializer(11),\n@@ -186,12 +187,15 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n         'extra_weight_2', shape=[],\n         initializer=tf.compat.v1.constant_initializer(12),\n         trainable=False)\n+    model = keras.Sequential([keras.Input([3,]), layer])\n \n     saved_model_dir = self._save_model_dir()\n     self.evaluate(tf.compat.v1.variables_initializer(layer.variables))\n-    tf.saved_model.save(layer, saved_model_dir)\n-    loaded = keras_load.load(saved_model_dir)\n-    self.evaluate(tf.compat.v1.variables_initializer(loaded.variables))\n+    model.save(saved_model_dir, save_format='tf')\n+    loaded_model = keras_load.load(saved_model_dir)\n+    self.evaluate(tf.compat.v1.variables_initializer(loaded_model.variables))\n+\n+    loaded = loaded_model.layers[-1]\n \n     equal_attrs = ['name', '_expects_training_arg', 'trainable']\n     for attr in equal_attrs:\n@@ -242,7 +246,7 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n     previous_losses = model.losses[:]\n \n     saved_model_dir = self._save_model_dir()\n-    tf.saved_model.save(model, saved_model_dir)\n+    model.save(saved_model_dir, save_format='tf')\n \n     with previous_losses[0].graph.as_default():\n       # If we try to compare symbolic Tensors in eager mode assertAllEqual will\n@@ -253,15 +257,18 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n       # Test that eager losses are maintained.\n       model(input_arr)  # Calls model eagerly, creating eager losses.\n       previous_losses = model.losses[:]\n-      tf.saved_model.save(model, saved_model_dir)\n+      model.save(saved_model_dir, save_format='tf')\n       self.assertAllEqual(previous_losses, model.losses)\n \n   def test_layer_with_learning_phase(self):\n     layer = LayerWithLearningPhase()\n     layer.build([None, None])\n     saved_model_dir = self._save_model_dir()\n-    tf.saved_model.save(layer, saved_model_dir)\n-    loaded = keras_load.load(saved_model_dir)\n+    model = testing_utils.get_model_from_layers(\n+        [layer], input_shape=[None], model_type='functional')\n+    model.save(saved_model_dir, save_format='tf')\n+    loaded_model = keras_load.load(saved_model_dir)\n+    loaded = loaded_model.layers[-1]\n     input_arr = tf.ones((4, 3))\n \n     # Run the layer, and use the keras backend learning phase\n@@ -289,7 +296,7 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n     model.predict(np.random.random((1, 3)).astype(np.float32))\n     saved_model_dir = self._save_model_dir()\n \n-    tf.saved_model.save(model, saved_model_dir)\n+    model.save(saved_model_dir, save_format='tf')\n \n     loaded = tf.saved_model.load(saved_model_dir)\n     self.evaluate(tf.compat.v1.variables_initializer(loaded.variables))\n@@ -321,7 +328,7 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n     # Compile and save model.\n     model.compile('rmsprop', 'mse')\n     saved_model_dir = self._save_model_dir()\n-    tf.saved_model.save(model, saved_model_dir)\n+    model.save(saved_model_dir, save_format='tf')\n \n     loaded = keras_load.load(saved_model_dir)\n     actual_predict = loaded.predict(input_arr)\n@@ -347,7 +354,7 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n         super(LayerWithNestedSpec, self).__init__()\n         self.input_spec = {\n             'a': keras.layers.InputSpec(max_ndim=3, axes={-1: 2}),\n-            'b': keras.layers.InputSpec(shape=(None, 2, 3), dtype='float16')}\n+            'b': keras.layers.InputSpec(shape=(None, 2, 3), dtype='int32')}\n \n       @property\n       def _use_input_spec_as_call_signature(self):\n@@ -355,12 +362,17 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n \n     layer = LayerWithNestedSpec()\n     saved_model_dir = self._save_model_dir()\n-    tf.saved_model.save(layer, saved_model_dir)\n-    loaded = keras_load.load(saved_model_dir)\n+    model = testing_utils.get_model_from_layers(\n+        [layer], model_type='subclass')\n+    model({'a': tf.constant([[2, 4]]),\n+           'b': tf.ones([1, 2, 3], dtype=tf.int32)})\n+    model.save(saved_model_dir, save_format='tf')\n+    loaded_model = keras_load.load(saved_model_dir)\n+    loaded = loaded_model.layers[-1]\n     self.assertEqual(3, loaded.input_spec['a'].max_ndim)\n     self.assertEqual({-1: 2}, loaded.input_spec['a'].axes)\n     self.assertAllEqual([None, 2, 3], loaded.input_spec['b'].shape)\n-    self.assertEqual('float16', loaded.input_spec['b'].dtype)\n+    self.assertEqual('int32', loaded.input_spec['b'].dtype)\n \n   def test_must_restore_from_config_fails_if_layer_is_not_in_scope(self):\n \n@@ -369,7 +381,9 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n \n     layer = LayerThatShouldFailIfNotAdded()\n     saved_model_dir = self._save_model_dir()\n-    tf.saved_model.save(layer, saved_model_dir)\n+    model = testing_utils.get_model_from_layers(\n+        [layer], input_shape=[3], model_type='functional')\n+    model.save(saved_model_dir, save_format='tf')\n     with self.assertRaisesRegex(RuntimeError, 'Unable to restore a layer of'):\n       _ = keras_load.load(saved_model_dir)\n \n@@ -379,8 +393,10 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n       _must_restore_from_config = True\n \n     layer = LayerThatShouldFailIfNotAdded()\n+    model = testing_utils.get_model_from_layers(\n+        [layer], input_shape=[3], model_type='functional')\n     saved_model_dir = self._save_model_dir()\n-    tf.saved_model.save(layer, saved_model_dir)\n+    model.save(saved_model_dir, save_format='tf')\n     with generic_utils.CustomObjectScope(\n         {'LayerThatShouldFailIfNotAdded': LayerThatShouldFailIfNotAdded}):\n       _ = keras_load.load(saved_model_dir)\n@@ -388,7 +404,9 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n   def test_must_restore_from_config_registration(self):\n     layer = GlobalLayerThatShouldFailIfNotAdded()\n     saved_model_dir = self._save_model_dir()\n-    tf.saved_model.save(layer, saved_model_dir)\n+    model = testing_utils.get_model_from_layers(\n+        [layer], input_shape=[3], model_type='functional')\n+    model.save(saved_model_dir, save_format='tf')\n     _ = keras_load.load(saved_model_dir)\n \n   def test_multi_input_model(self):\n@@ -437,16 +455,14 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n     self.evaluate(tf.compat.v1.variables_initializer(model.variables))\n     saved_model_dir = self._save_model_dir()\n \n-    # TODO(kathywu): Re-enable this check after removing the tf.saved_model.save\n-    # metadata warning.\n-    # with self.captureWritesToStream(sys.stderr) as captured_logs:\n+    with self.captureWritesToStream(sys.stderr) as captured_logs:\n       model.save(saved_model_dir, save_format='tf')\n       loaded = keras_load.load(saved_model_dir)\n \n     # Assert that saving does not log deprecation warnings\n     # (even if it needs to set learning phase for compat reasons)\n-    # if context.executing_eagerly():\n-    #   self.assertNotIn('deprecated', captured_logs.contents())\n+    if tf.executing_eagerly():\n+      self.assertNotIn('deprecated', captured_logs.contents())\n \n     input_arr = tf.constant([[11], [12], [13]], dtype=tf.float32)\n     input_arr2 = tf.constant([[14], [15], [16]], dtype=tf.float32)\n@@ -789,7 +805,7 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n         np.zeros((batch, 64)).astype('float32'))\n \n     saved_model_dir = self._save_model_dir()\n-    tf.saved_model.save(model, saved_model_dir)\n+    model.save(saved_model_dir, save_format='tf')\n \n     loaded = keras_load.load(saved_model_dir)\n     loaded_layer = loaded.layers[1]\n@@ -817,7 +833,7 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n     self.evaluate([v.initializer for v in model.variables])\n     saved_model_dir = self._save_model_dir()\n \n-    tf.saved_model.save(model, saved_model_dir)\n+    model.save(saved_model_dir, save_format='tf')\n     del model\n \n     loaded = keras_load.load(saved_model_dir)\n@@ -857,7 +873,7 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n     model = keras.Model(f_inputs, out)\n     self.evaluate(tf.compat.v1.variables_initializer(model.variables))\n     saved_model_dir = self._save_model_dir()\n-    tf.saved_model.save(model, saved_model_dir)\n+    model.save(saved_model_dir, save_format='tf')\n \n     loaded = keras_load.load(saved_model_dir)\n     self.evaluate(tf.compat.v1.variables_initializer(loaded.variables))\n@@ -956,7 +972,7 @@ class TestSavedModelFormat(tf.test.TestCase):\n     inp = tf.constant([[1.0]])\n     model(inp)\n     saved_model_dir = self._save_model_dir()\n-    tf.saved_model.save(model, saved_model_dir)\n+    model.save(saved_model_dir, save_format='tf')\n \n     loaded = keras_load.load(saved_model_dir)\n     self.assertAllEqual([[1.0]], self.evaluate(loaded(inp)))\n@@ -1128,8 +1144,12 @@ class MetricTest(tf.test.TestCase, parameterized.TestCase):\n                                  shape=(1, 5),\n                                  test_sample_weight=True):\n     with self.cached_session():\n-      tf.saved_model.save(metric, save_dir)\n-      loaded = keras_load.load(save_dir)\n+      model = testing_utils.get_model_from_layers(\n+          [keras.layers.Layer()], input_shape=[3], model_type='functional')\n+      model.saved_metric = metric\n+      model.save(save_dir, save_format='tf')\n+      loaded_model = keras_load.load(save_dir)\n+      loaded = loaded_model.saved_metric\n       self.evaluate([v.initializer for v in loaded.variables])\n       self.assertEqual(metric.name, loaded.name)\n       self.assertEqual(metric.dtype, loaded.dtype)\n@@ -1281,7 +1301,7 @@ class MetricTest(tf.test.TestCase, parameterized.TestCase):\n                   metrics=[CustomMetric(), zero_metric])\n     model.fit(x, y)\n     saved_model_dir = self._save_model_dir()\n-    tf.saved_model.save(model, saved_model_dir)\n+    model.save(saved_model_dir, save_format='tf')\n \n     with self.assertRaisesRegex(ValueError, 'custom_objects'):\n       keras_load.load(saved_model_dir)\n\n@@ -565,7 +565,8 @@ def get_model_from_layers(model_layers,\n                           input_dtype=None,\n                           name=None,\n                           input_ragged=None,\n-                          input_sparse=None):\n+                          input_sparse=None,\n+                          model_type=None):\n   \"\"\"Builds a model from a sequence of layers.\n \n   Args:\n@@ -575,11 +576,13 @@ def get_model_from_layers(model_layers,\n     name: Name for the model.\n     input_ragged: Boolean, whether the input data is a ragged tensor.\n     input_sparse: Boolean, whether the input data is a sparse tensor.\n+    model_type: One of \"subclass\", \"subclass_custom_build\", \"sequential\", or\n+      \"functional\". When None, defaults to `get_model_type`.\n \n   Returns:\n     A Keras model.\n   \"\"\"\n-\n+  if model_type is None:\n     model_type = get_model_type()\n   if model_type == 'subclass':\n     inputs = None\n\n@@ -163,8 +163,8 @@ class DistributeKplTestUtils(tf.test.TestCase):\n                                                label_inverse_lookup_layer)\n \n     saved_model_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n-    tf.saved_model.save(\n-        model, saved_model_dir, signatures={\"serving_default\": serving_fn})\n+    model.save(saved_model_dir, save_format=\"tf\",\n+               signatures={\"serving_default\": serving_fn})\n \n     # Test the saved_model.\n     loaded_serving_fn = keras.saving.save.load_model(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8ac61357f1a284005981e37f331a63741930f254", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 87 | Lines Deleted: 25 | Files Changed: 9 | Hunks: 21 | Methods Changed: 18 | Complexity Δ (Sum/Max): 5/2 | Churn Δ: 112 | Churn Cumulative: 27353 | Contributors (this commit): 103 | Commits (past 90d): 64 | Contributors (cumulative): 119 | DMM Complexity: 0.6875\n\nDIFF:\n@@ -2435,6 +2435,7 @@ class InputsOutputsErrorTest(keras_parameterized.TestCase):\n class FunctionalSubclassModel(training_lib.Model):\n \n   def __init__(self, *args, **kwargs):\n+    self.foo = {'foo': 'bar'}  # Make sure users can assign dict attributes\n     my_input = input_layer_lib.Input(shape=(16,))\n     dense = layers.Dense(32, activation='relu')\n     output = dense(my_input)\n\n@@ -309,15 +309,14 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       return\n \n     if all(\n-        isinstance(v, (base_layer.Layer,\n-                       tf.__internal__.tracking.TrackableDataStructure)) or\n+        isinstance(v, (base_layer.Layer, tf.Variable)) or\n         base_layer_utils.has_weights(v) for v in tf.nest.flatten(value)):\n       try:\n         self._base_model_initialized\n       except AttributeError:\n         raise RuntimeError(\n             'It looks like you are subclassing `Model` and you '\n-            'forgot to call `super(YourClass, self).__init__()`.'\n+            'forgot to call `super().__init__()`.'\n             ' Always start with this line.')\n \n     super(Model, self).__setattr__(name, value)\n\n@@ -401,11 +401,13 @@ class RandomFlip(base_layer.Layer):\n     def random_flipped_inputs():\n       flipped_outputs = inputs\n       if self.horizontal:\n-        flipped_outputs = tf.image.random_flip_left_right(\n-            flipped_outputs, self.seed)\n+        flipped_outputs = tf.image.stateless_random_flip_left_right(\n+            flipped_outputs,\n+            self._rng.make_seeds()[:, 0])\n       if self.vertical:\n-        flipped_outputs = tf.image.random_flip_up_down(flipped_outputs,\n-                                                        self.seed)\n+        flipped_outputs = tf.image.stateless_random_flip_up_down(\n+            flipped_outputs,\n+            self._rng.make_seeds()[:, 0])\n       return flipped_outputs\n \n     output = control_flow_util.smart_cond(training, random_flipped_inputs,\n@@ -1072,6 +1074,7 @@ class RandomContrast(base_layer.Layer):\n       raise ValueError('Factor cannot have negative values or greater than 1.0,'\n                        ' got {}'.format(factor))\n     self.seed = seed\n+    self._rng = make_generator(self.seed)\n     self.input_spec = InputSpec(ndim=4)\n     super(RandomContrast, self).__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomContrast').set(\n@@ -1082,8 +1085,9 @@ class RandomContrast(base_layer.Layer):\n       training = backend.learning_phase()\n \n     def random_contrasted_inputs():\n-      return tf.image.random_contrast(inputs, 1. - self.lower, 1. + self.upper,\n-                                       self.seed)\n+      return tf.image.stateless_random_contrast(inputs, 1. - self.lower,\n+                                                 1. + self.upper,\n+                                                 self._rng.make_seeds()[:, 0])\n \n     output = control_flow_util.smart_cond(training, random_contrasted_inputs,\n                                           lambda: inputs)\n@@ -1303,7 +1307,7 @@ def make_generator(seed=None):\n   Returns:\n     A generator object.\n   \"\"\"\n-  if seed:\n+  if seed is not None:\n     return tf.random.Generator.from_seed(seed)\n   else:\n     return tf.random.Generator.from_non_deterministic_state()\n\n@@ -16,6 +16,7 @@\n \n import tensorflow.compat.v2 as tf\n \n+import functools\n from absl.testing import parameterized\n import numpy as np\n from tensorflow.python.distribute.mirrored_strategy import MirroredStrategy\n@@ -26,7 +27,6 @@ from keras.layers.preprocessing import image_preprocessing\n from keras.utils.generic_utils import CustomObjectScope\n from tensorflow.python.ops import gen_stateful_random_ops\n from tensorflow.python.ops import gen_stateless_random_ops_v2\n-from tensorflow.python.ops import random_ops\n from tensorflow.python.ops import stateless_random_ops\n \n \n@@ -373,7 +373,10 @@ class RandomFlipTest(keras_parameterized.TestCase):\n       if mode == 'vertical' or mode == 'horizontal_and_vertical':\n         expected_output = np.flip(expected_output, axis=1)\n     with tf.compat.v1.test.mock.patch.object(\n-        random_ops, 'random_uniform', return_value=mock_random):\n+        stateless_random_ops,\n+        'stateless_random_uniform',\n+        return_value=mock_random,\n+    ):\n       with testing_utils.use_gpu():\n         layer = image_preprocessing.RandomFlip(mode)\n         actual_output = layer(inp, training=1)\n@@ -423,7 +426,10 @@ class RandomFlipTest(keras_parameterized.TestCase):\n       mock_random = [1, 1]\n       mock_random = np.reshape(mock_random, [2, 1, 1, 1])\n       with tf.compat.v1.test.mock.patch.object(\n-          random_ops, 'random_uniform', return_value=mock_random):\n+          stateless_random_ops,\n+          'stateless_random_uniform',\n+          return_value=mock_random,\n+      ):\n         with self.cached_session():\n           layer = image_preprocessing.RandomFlip()\n           actual_output = layer(input_images, training=1)\n@@ -456,7 +462,10 @@ class RandomContrastTest(keras_parameterized.TestCase):\n       inp_mean = np.mean(inp_mean, axis=2, keepdims=True)\n       expected_output = (inp - inp_mean) * mock_random + inp_mean\n     with tf.compat.v1.test.mock.patch.object(\n-        random_ops, 'random_uniform', return_value=mock_random):\n+        stateless_random_ops,\n+        'stateless_random_uniform',\n+        return_value=mock_random,\n+    ):\n       with testing_utils.use_gpu():\n         layer = image_preprocessing.RandomContrast((lower, upper))\n         actual_output = layer(inp, training=True)\n@@ -1445,5 +1454,35 @@ class LearningPhaseTest(keras_parameterized.TestCase):\n     self.assertEqual(tuple(int(i) for i in out.shape[1:]), shape)\n \n \n+@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+class DeterminismTest(keras_parameterized.TestCase):\n+\n+  @parameterized.named_parameters(\n+      ('random_flip', image_preprocessing.RandomFlip),\n+      ('random_contrast',\n+       functools.partial(image_preprocessing.RandomContrast, factor=1.)),\n+      ('random_crop',\n+       functools.partial(image_preprocessing.RandomCrop, height=2, width=2)),\n+      ('random_translation',\n+       functools.partial(image_preprocessing.RandomTranslation, 0.3, 0.2)),\n+      ('random_rotation',\n+       functools.partial(image_preprocessing.RandomRotation, 0.5)),\n+      ('random_zoom', functools.partial(image_preprocessing.RandomZoom, 0.2)),\n+      ('random_height', functools.partial(image_preprocessing.RandomHeight,\n+                                          0.4)),\n+      ('random_width', functools.partial(image_preprocessing.RandomWidth, 0.3)),\n+  )\n+  def test_seed_constructor_arg(self, layer_cls):\n+    input_image = np.random.random((2, 5, 8, 3)).astype(np.float32)\n+\n+    layer1 = layer_cls(seed=0.)\n+    layer2 = layer_cls(seed=0.)\n+    layer1_output = layer1(input_image)\n+    layer2_output = layer2(input_image)\n+\n+    self.assertAllClose(layer1_output.numpy().tolist(),\n+                        layer2_output.numpy().tolist())\n+\n+\n if __name__ == '__main__':\n   tf.test.main()\n\n@@ -616,6 +616,9 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n         updates[_VOCAB_NAME], idf_weights=updates[_IDF_WEIGHTS_NAME])\n \n   def call(self, inputs):\n+    if isinstance(inputs, (list, tuple, np.ndarray)):\n+      inputs = tf.convert_to_tensor(inputs)\n+\n     if not self.max_tokens and self._vocab_size is None:\n       raise ValueError(\"You must set the layer's vocabulary before calling it. \"\n                        \"Either pass a `vocabulary` argument to the layer, or \"\n\n@@ -111,6 +111,14 @@ class IntegerLookupLayerTest(keras_parameterized.TestCase,\n           adapt_data=vocab_data)\n     self.assertAllClose(expected_output, output_data)\n \n+  def test_layer_with_list_input(self):\n+    vocab = [12, 36, 1138, 42]\n+    data = [[12, 1138, 42], [42, 1000, 36]]  # Note OOV tokens\n+    layer = integer_lookup.IntegerLookup(vocabulary=vocab)\n+    output = layer(data)\n+    expected_output = np.array([[1, 3, 4], [4, 0, 2]])\n+    self.assertEqual(output.numpy().tolist(), expected_output.tolist())\n+\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class CategoricalEncodingInputTest(\n\n@@ -855,8 +855,8 @@ class ApplicationModelTest(keras_parameterized.TestCase):\n       ('resnet50', resnet.ResNet50),\n   )\n   def test_application_model(self, app):\n-    self.skipTest('b/190747991: OOMs when a GPU is used')\n-    with policy.policy_scope('mixed_float16'):\n+    # Run on CPU since model weights may exhaust GPU memory\n+    with policy.policy_scope('mixed_float16'), tf.compat.v1.device('/CPU:0'):\n       app(weights=None)\n \n \n\n@@ -24,6 +24,7 @@ from google.protobuf import message\n from keras import backend\n from keras import regularizers\n from keras.engine import input_spec\n+from keras.optimizer_v2 import optimizer_v2\n from keras.protobuf import saved_metadata_pb2\n from keras.protobuf import versions_pb2\n from keras.saving import saving_utils\n@@ -153,6 +154,12 @@ def load(path, compile=True, options=None):  # pylint: disable=redefined-builtin\n       model.compile(**saving_utils.compile_args_from_training_config(\n           training_config), from_serialized=True)\n       saving_utils.try_build_compiled_arguments(model)\n+      if isinstance(model.optimizer, optimizer_v2.OptimizerV2):\n+        if (model.optimizer.get_slot_names()):\n+          logging.warning('Your optimizer uses slots. '\n+                          'Slots cannot be restored from saved_model, '\n+                          'as a result, your model is starting with  '\n+                          'a new initialized optimizer.')\n     else:\n       logging.warning('No training configuration found in save file, so the '\n                       'model was *not* compiled. Compile it manually.')\n\n@@ -1123,6 +1123,16 @@ class TestLayerCallTracing(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllEqual(previous_losses, layer.losses)\n \n \n+@generic_utils.register_keras_serializable('Testing')\n+class CustomMeanMetric(keras.metrics.Mean):\n+\n+  def update_state(self, *args):  # pylint: disable=useless-super-delegation\n+    # Sometimes built-in metrics return an op in update_state. Custom\n+    # metrics don't support returning ops, so wrap the update_state method\n+    # while returning nothing.\n+    super(CustomMeanMetric, self).update_state(*args)\n+\n+\n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n class MetricTest(tf.test.TestCase, parameterized.TestCase):\n \n@@ -1239,15 +1249,6 @@ class MetricTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_registered_custom_metric(self):\n \n-    @generic_utils.register_keras_serializable('Testing')\n-    class CustomMeanMetric(keras.metrics.Mean):\n-\n-      def update_state(self, *args):  # pylint: disable=useless-super-delegation\n-        # Sometimes built-in metrics return an op in update_state. Custom\n-        # metrics don't support returning ops, so wrap the update_state method\n-        # while returning nothing.\n-        super(CustomMeanMetric, self).update_state(*args)\n-\n     with self.cached_session():\n       metric = CustomMeanMetric()\n       save_dir = self._save_model_dir('first_save')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c0fb5051999b783b6c0a5a319f132b1e436f6dbd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 68 | Lines Deleted: 74 | Files Changed: 7 | Hunks: 48 | Methods Changed: 25 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 142 | Churn Cumulative: 5297 | Contributors (this commit): 21 | Commits (past 90d): 35 | Contributors (cumulative): 43 | DMM Complexity: 1.0\n\nDIFF:\n@@ -135,7 +135,7 @@ def load_data(path='imdb.npz',\n                        str(maxlen) + ', no sequence was kept. '\n                        'Increase maxlen.')\n \n-  xs = np.concatenate([x_train, x_test])\n+  xs = x_train + x_test\n   labels = np.concatenate([labels_train, labels_test])\n \n   if not num_words:\n@@ -152,9 +152,8 @@ def load_data(path='imdb.npz',\n     xs = [[w for w in x if skip_top <= w < num_words] for x in xs]\n \n   idx = len(x_train)\n-  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n-  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n-\n+  x_train, y_train = np.array(xs[:idx], dtype='object'), labels[:idx]\n+  x_test, y_test = np.array(xs[idx:], dtype='object'), labels[idx:]\n   return (x_train, y_train), (x_test, y_test)\n \n \n\n@@ -140,8 +140,8 @@ def load_data(path='reuters.npz',\n     xs = [[w for w in x if skip_top <= w < num_words] for x in xs]\n \n   idx = int(len(xs) * (1 - test_split))\n-  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n-  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n+  x_train, y_train = np.array(xs[:idx], dtype='object'), np.array(labels[:idx])\n+  x_test, y_test = np.array(xs[idx:], dtype='object'), np.array(labels[idx:])\n \n   return (x_train, y_train), (x_test, y_test)\n \n\n@@ -20,7 +20,6 @@ import tensorflow.compat.v2 as tf\n import numpy as np\n from keras.engine import base_preprocessing_layer\n from keras.utils import tf_utils\n-from tensorflow.python.ops.parallel_for.control_flow_ops import vectorized_map\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n@@ -153,8 +152,8 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n   ...          bin_boundaries=[0., 1., 2.])\n   >>> layer(input)\n   <tf.Tensor: shape=(2, 4), dtype=int32, numpy=\n-  array([[0, 1, 3, 1],\n-         [0, 3, 2, 0]], dtype=int32)>\n+  array([[0, 2, 3, 1],\n+         [1, 3, 2, 1]], dtype=int32)>\n \n   Bucketize float values based on a number of buckets to compute.\n   >>> input = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])\n@@ -163,8 +162,8 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n   >>> layer.adapt(input)\n   >>> layer(input)\n   <tf.Tensor: shape=(2, 4), dtype=int32, numpy=\n-  array([[0, 2, 3, 1],\n-         [0, 3, 2, 0]], dtype=int32)>\n+  array([[0, 2, 3, 2],\n+         [1, 3, 3, 1]], dtype=int32)>\n   \"\"\"\n \n   def __init__(self,\n@@ -192,21 +191,16 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n       raise ValueError(\"Both `num_bins` and `bin_boundaries` should not be \"\n                        \"set. You passed `num_bins={}` and \"\n                        \"`bin_boundaries={}`\".format(num_bins, bin_boundaries))\n-    self.bin_boundaries = bin_boundaries\n+    bin_boundaries = self._convert_to_list(bin_boundaries)\n+    self.input_bin_boundaries = bin_boundaries\n+    self.bin_boundaries = bin_boundaries if bin_boundaries is not None else []\n     self.num_bins = num_bins\n     self.epsilon = epsilon\n \n-    # If `bin_boundaries` is passed, we set up a constant `bins` tensor for use\n-    # in `call` right away.\n-    if self.bin_boundaries is not None:\n-      boundaries = tf.convert_to_tensor(\n-          self.bin_boundaries, dtype=tf.float32)\n-      self.bins = tf.concat([boundaries, [np.inf]], axis=0)\n-\n   def build(self, input_shape):\n     super().build(input_shape)\n \n-    if self.bin_boundaries is not None:\n+    if self.input_bin_boundaries is not None:\n       return\n \n     # Summary contains two equal length vectors of bins at index 0 and weights\n@@ -217,14 +211,13 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n         dtype=tf.float32,\n         initializer=lambda shape, dtype: [[], []],  # pylint: disable=unused-arguments\n         trainable=False)\n-    self.finalize_state()\n \n   def update_state(self, data):\n-    if self.bin_boundaries is not None:\n+    if self.input_bin_boundaries is not None:\n       raise ValueError(\n           \"Cannot adapt a Discretization layer that has been initialized with \"\n           \"`bin_boundaries`, use `num_bins` instead. You passed \"\n-          \"`bin_boundaries={}`.\".format(self.bin_boundaries))\n+          \"`bin_boundaries={}`.\".format(self.input_bin_boundaries))\n \n     if not self.built:\n       raise RuntimeError(\"`build` must be called before `update_state`.\")\n@@ -237,11 +230,11 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n \n   def merge_state(self, layers):\n     for l in layers + [self]:\n-      if l.bin_boundaries is not None:\n+      if l.input_bin_boundaries is not None:\n         raise ValueError(\n             \"Cannot merge Discretization layer {} that has been initialized \"\n             \"with `bin_boundaries`, use `num_bins` instead. You passed \"\n-            \"`bin_boundaries={}`.\".format(l.name, l.bin_boundaries))\n+            \"`bin_boundaries={}`.\".format(l.name, l.input_bin_boundaries))\n       if not l.built:\n         raise ValueError(\n             \"Cannot merge Discretization layer {}, it has no state. You need \"\n@@ -254,16 +247,15 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n     self.finalize_state()\n \n   def finalize_state(self):\n-    if self.bin_boundaries is not None or not self.built:\n+    if self.input_bin_boundaries is not None or not self.built:\n       return\n \n-    # In the adapt case, we finalize our summary to a constant tensor `bins`\n-    # each time `finalize_state` is called.\n-    boundaries = get_bin_boundaries(self.summary, self.num_bins)\n-    self.bins = tf.concat([boundaries, [np.inf]], axis=0)\n+    # The bucketize op only support list boundaries.\n+    self.bin_boundaries = self._convert_to_list(\n+        get_bin_boundaries(self.summary, self.num_bins))\n \n   def reset_state(self):  # pylint: disable=method-hidden\n-    if self.bin_boundaries is not None or not self.built:\n+    if self.input_bin_boundaries is not None or not self.built:\n       return\n \n     self.summary.assign([[], []])\n@@ -271,7 +263,7 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n   def get_config(self):\n     config = super().get_config()\n     config.update({\n-        \"bin_boundaries\": self.bin_boundaries,\n+        \"bin_boundaries\": self.input_bin_boundaries,\n         \"num_bins\": self.num_bins,\n         \"epsilon\": self.epsilon,\n     })\n@@ -289,35 +281,28 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n     return tf.TensorSpec(shape=output_shape, dtype=output_dtype)\n \n   def call(self, inputs):\n-    bins = [tf.cast(tf.compat.v1.squeeze(self.bins), tf.float32)]\n-\n-    def _bucketize_fn(inputs):\n-      return tf.raw_ops.BoostedTreesBucketize(\n-          float_values=[tf.cast(inputs, tf.float32)],\n-          bucket_boundaries=bins)[0]\n+    def bucketize(inputs):\n+      return tf.raw_ops.Bucketize(\n+          input=inputs, boundaries=self.bin_boundaries)\n \n     if tf_utils.is_ragged(inputs):\n-      integer_buckets = tf.ragged.map_flat_values(\n-          _bucketize_fn, inputs)\n+      integer_buckets = tf.ragged.map_flat_values(bucketize, inputs)\n       # Ragged map_flat_values doesn't touch the non-values tensors in the\n       # ragged composite tensor. If this op is the only op a Keras model,\n       # this can cause errors in Graph mode, so wrap the tensor in an identity.\n       return tf.identity(integer_buckets)\n-    elif isinstance(inputs, tf.SparseTensor):\n+    elif tf_utils.is_sparse(inputs):\n       return tf.SparseTensor(\n           indices=tf.identity(inputs.indices),\n-          values=_bucketize_fn(inputs.values),\n+          values=bucketize(inputs.values),\n           dense_shape=tf.identity(inputs.dense_shape))\n     else:\n-      static_shape = inputs.get_shape()\n-      if any(dim is None for dim in static_shape.as_list()[1:]):\n-        raise NotImplementedError(\n-            \"Discretization Layer requires known non-batch shape,\"\n-            \"found {}\".format(static_shape))\n+      return bucketize(inputs)\n \n-      dynamic_shape = tf.shape(inputs)\n-      # BoostedTreesBucketize only handles rank 1 inputs. We need to flatten our\n-      # inputs after batch size and vectorized_map over each sample.\n-      reshaped = tf.reshape(inputs, [dynamic_shape[0], -1])\n-      return tf.reshape(\n-          vectorized_map(_bucketize_fn, reshaped), dynamic_shape)\n+  def _convert_to_list(self, inputs):\n+    if tf.is_tensor(inputs):\n+      inputs = inputs.numpy()\n+    if isinstance(inputs, (np.ndarray)):\n+      inputs = inputs.tolist()\n+      inputs = list(inputs)\n+    return inputs\n\n@@ -37,7 +37,7 @@ class DiscretizationDistributionTest(\n   def test_distribution(self, strategy):\n     input_array = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])\n \n-    expected_output = [[0, 1, 3, 1], [0, 3, 2, 0]]\n+    expected_output = [[0, 2, 3, 1], [1, 3, 2, 1]]\n     expected_output_shape = [None, 4]\n \n     tf.config.set_soft_device_placement(True)\n\n@@ -36,7 +36,7 @@ class DiscretizationTest(keras_parameterized.TestCase,\n   def test_bucketize_with_explicit_buckets_integer(self):\n     input_array = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])\n \n-    expected_output = [[0, 1, 3, 1], [0, 3, 2, 0]]\n+    expected_output = [[0, 2, 3, 1], [1, 3, 2, 1]]\n     expected_output_shape = [None, 4]\n \n     input_data = keras.Input(shape=(4,))\n@@ -81,7 +81,7 @@ class DiscretizationTest(keras_parameterized.TestCase,\n     input_array = tf.ragged.constant([[-1.5, 1.0, 3.4, .5],\n                                                [0.0, 3.0, 1.3]])\n \n-    expected_output = [[0, 1, 3, 1], [0, 3, 2]]\n+    expected_output = [[0, 2, 3, 1], [1, 3, 2]]\n     expected_output_shape = [None, None]\n \n     input_data = keras.Input(shape=(None,), ragged=True)\n@@ -123,7 +123,7 @@ class DiscretizationTest(keras_parameterized.TestCase,\n     self.assertAllEqual(expected_output, output_dataset.values)\n \n   def test_output_shape(self):\n-    input_data = keras.Input(batch_size=16, shape=(4,), dtype=tf.string)\n+    input_data = keras.Input(batch_size=16, shape=(4,), dtype=tf.int64)\n     layer = discretization.Discretization(bin_boundaries=[-.5, 0.5, 1.5])\n     output = layer(input_data)\n     self.assertAllEqual(output.shape.as_list(), [16, 4])\n@@ -149,7 +149,7 @@ class DiscretizationAdaptTest(keras_parameterized.TestCase,\n           \"adapt_data\": np.array([[1.], [2.], [3.], [4.], [5.]]),\n           \"test_data\": np.array([[1.], [2.], [3.]]),\n           \"use_dataset\": True,\n-          \"expected\": np.array([[0], [1], [2]]),\n+          \"expected\": np.array([[1], [2], [3]]),\n           \"num_bins\": 5,\n           \"epsilon\": 0.01\n       }, {\n@@ -158,7 +158,7 @@ class DiscretizationAdaptTest(keras_parameterized.TestCase,\n                                   [5., 10.]]),\n           \"test_data\": np.array([[1., 10.], [2., 6.], [3., 8.]]),\n           \"use_dataset\": True,\n-          \"expected\": np.array([[0, 4], [0, 2], [1, 3]]),\n+          \"expected\": np.array([[0, 4], [1, 3], [1, 4]]),\n           \"num_bins\": 5,\n           \"epsilon\": 0.01\n       }, {\n@@ -166,7 +166,7 @@ class DiscretizationAdaptTest(keras_parameterized.TestCase,\n           \"adapt_data\": np.array([3., 2., 1., 5., 4.]),\n           \"test_data\": np.array([1., 2., 3.]),\n           \"use_dataset\": True,\n-          \"expected\": np.array([0, 1, 2]),\n+          \"expected\": np.array([1, 2, 3]),\n           \"num_bins\": 5,\n           \"epsilon\": 0.01\n       }, {\n@@ -192,7 +192,7 @@ class DiscretizationAdaptTest(keras_parameterized.TestCase,\n           \"adapt_data\": np.arange(300),\n           \"test_data\": np.arange(300),\n           \"use_dataset\": True,\n-          \"expected\": np.concatenate([np.zeros(137), np.ones(163)]),\n+          \"expected\": np.concatenate([np.zeros(136), np.ones(164)]),\n           \"num_bins\": 2,\n           \"epsilon\": 0.1\n       }])\n@@ -260,7 +260,7 @@ class DiscretizationAdaptTest(keras_parameterized.TestCase,\n     first_adapt = [[1], [2], [3]]\n     second_adapt = [[4], [5], [6]]\n     predict_input = [[2], [2]]\n-    expected_first_output = [[1], [1]]\n+    expected_first_output = [[2], [2]]\n     expected_second_output = [[0], [0]]\n \n     inputs = keras.Input(shape=(1,), dtype=tf.int32)\n@@ -282,15 +282,16 @@ class DiscretizationAdaptTest(keras_parameterized.TestCase,\n \n   def test_saved_model_tf(self):\n     input_data = [[1], [2], [3]]\n+    predict_data = [[0.5], [1.5], [2.5]]\n     expected_output = [[0], [1], [2]]\n \n-    inputs = keras.Input(shape=(1,), dtype=tf.int32)\n+    inputs = keras.Input(shape=(1,), dtype=tf.float32)\n     layer = discretization.Discretization(num_bins=3)\n     layer.adapt(input_data)\n     outputs = layer(inputs)\n     model = keras.Model(inputs=inputs, outputs=outputs)\n \n-    output_data = model.predict(input_data)\n+    output_data = model.predict(predict_data)\n     self.assertAllClose(output_data, expected_output)\n \n     # Save the model to disk.\n@@ -303,21 +304,22 @@ class DiscretizationAdaptTest(keras_parameterized.TestCase,\n     self.assertIsNot(model, loaded_model)\n \n     # Validate correctness of the new model.\n-    new_output_data = f(tf.constant(input_data))[\"discretization\"]\n+    new_output_data = f(tf.constant(predict_data))[\"discretization\"]\n     self.assertAllClose(new_output_data, expected_output)\n \n   def test_saved_model_keras(self):\n     input_data = [[1], [2], [3]]\n+    predict_data = [[0.5], [1.5], [2.5]]\n     expected_output = [[0], [1], [2]]\n \n     cls = discretization.Discretization\n-    inputs = keras.Input(shape=(1,), dtype=tf.int32)\n+    inputs = keras.Input(shape=(1,), dtype=tf.float32)\n     layer = cls(num_bins=3)\n     layer.adapt(input_data)\n     outputs = layer(inputs)\n     model = keras.Model(inputs=inputs, outputs=outputs)\n \n-    output_data = model.predict(input_data)\n+    output_data = model.predict(predict_data)\n     self.assertAllClose(output_data, expected_output)\n \n     # Save the model to disk.\n@@ -330,21 +332,22 @@ class DiscretizationAdaptTest(keras_parameterized.TestCase,\n     self.assertIsNot(model, loaded_model)\n \n     # Validate correctness of the new model.\n-    new_output_data = loaded_model.predict(input_data)\n+    new_output_data = loaded_model.predict(predict_data)\n     self.assertAllClose(new_output_data, expected_output)\n \n   def test_saved_weights_keras(self):\n     input_data = [[1], [2], [3]]\n+    predict_data = [[0.5], [1.5], [2.5]]\n     expected_output = [[0], [1], [2]]\n \n     cls = discretization.Discretization\n-    inputs = keras.Input(shape=(1,), dtype=tf.int32)\n+    inputs = keras.Input(shape=(1,), dtype=tf.float32)\n     layer = cls(num_bins=3)\n     layer.adapt(input_data)\n     outputs = layer(inputs)\n     model = keras.Model(inputs=inputs, outputs=outputs)\n \n-    output_data = model.predict(input_data)\n+    output_data = model.predict(predict_data)\n     self.assertAllClose(output_data, expected_output)\n \n     # Save the model to disk.\n@@ -355,7 +358,7 @@ class DiscretizationAdaptTest(keras_parameterized.TestCase,\n     new_model.load_weights(output_path)\n \n     # Validate correctness of the new model.\n-    new_output_data = new_model.predict(input_data)\n+    new_output_data = new_model.predict(predict_data)\n     self.assertAllClose(new_output_data, expected_output)\n \n \n\n@@ -273,8 +273,8 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n     config = super().get_config()\n     config.update({\n         'axis': self.axis,\n-        'mean': self._make_json_serializable(self.input_mean),\n-        'variance': self._make_json_serializable(self.input_variance),\n+        'mean': self._convert_to_list(self.input_mean),\n+        'variance': self._convert_to_list(self.input_variance),\n     })\n     return config\n \n@@ -286,7 +286,7 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n       inputs = tf.compat.v1.expand_dims(inputs, 1)\n     return inputs\n \n-  def _make_json_serializable(self, inputs):\n+  def _convert_to_list(self, inputs):\n     if tf.is_tensor(inputs):\n       inputs = inputs.numpy()\n     if isinstance(inputs, (np.ndarray)):\n\n@@ -1025,6 +1025,13 @@ class TestSavedModelFormat(tf.test.TestCase):\n     with self.assertRaisesRegex(ValueError, 'I said do not trace'):\n       loaded.attached_layer(tf.constant([1.]))\n \n+  def test_load_non_keras_saved_model(self):\n+    model = testing_utils.get_small_functional_mlp(1, 4, input_dim=3)\n+    saved_model_dir = self._save_model_dir()\n+    tf.saved_model.save(model, saved_model_dir)\n+    with self.assertRaisesRegex(ValueError, 'Unable to create a Keras model'):\n+      keras_load.load(saved_model_dir)\n+\n \n class TestLayerCallTracing(tf.test.TestCase, parameterized.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0eb1954092e2110868e8ef06381cebac891e01e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 7 | Files Changed: 4 | Hunks: 6 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 4497 | Contributors (this commit): 3 | Commits (past 90d): 36 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -133,7 +133,7 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n                oov_token,\n                vocabulary=None,\n                invert=False,\n-               output_mode=INT,\n+               output_mode=\"int\",\n                sparse=False,\n                pad_to_max_tokens=False,\n                **kwargs):\n\n@@ -265,7 +265,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n                oov_token=-1,\n                vocabulary=None,\n                invert=False,\n-               output_mode=index_lookup.INT,\n+               output_mode=\"int\",\n                sparse=False,\n                pad_to_max_tokens=False,\n                **kwargs):\n\n@@ -263,7 +263,7 @@ class StringLookup(index_lookup.IndexLookup):\n                vocabulary=None,\n                encoding=None,\n                invert=False,\n-               output_mode=index_lookup.INT,\n+               output_mode=\"int\",\n                sparse=False,\n                pad_to_max_tokens=False,\n                **kwargs):\n\n@@ -217,15 +217,14 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n \n   def __init__(self,\n                max_tokens=None,\n-               standardize=LOWER_AND_STRIP_PUNCTUATION,\n-               split=SPLIT_ON_WHITESPACE,\n+               standardize=\"lower_and_strip_punctuation\",\n+               split=\"whitespace\",\n                ngrams=None,\n-               output_mode=INT,\n+               output_mode=\"int\",\n                output_sequence_length=None,\n                pad_to_max_tokens=False,\n                vocabulary=None,\n                **kwargs):\n-\n     # This layer only applies to string processing, and so should only have\n     # a dtype of 'string'.\n     if \"dtype\" in kwargs and kwargs[\"dtype\"] != tf.string:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
