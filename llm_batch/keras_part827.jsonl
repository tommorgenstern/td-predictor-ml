{"custom_id": "keras#c03e7b07226ea16ef91239e6efce2dd3010f8fdc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 423 | Contributors (this commit): 7 | Commits (past 90d): 20 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -433,6 +433,7 @@ class NumpyTwoInputOpsStaticShapeTest(testing.TestCase):\n             y = KerasTensor((2, 3, 4))\n             knp.matmul(x, y)\n \n+    @pytest.mark.skipif(testing.tensorflow_uses_gpu(), reason=\"Segfault\")\n     def test_matmul_sparse(self):\n         x = KerasTensor((2, 3), sparse=True)\n         y = KerasTensor((3, 2))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f0bae912201bbd265a3485ccf4f490be2fc675c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 12 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n from keras.src.api_export import keras_export\n \n # Unique source of truth for the version number.\n-__version__ = \"3.3.3\"\n+__version__ = \"3.4.0\"\n \n \n @keras_export(\"keras.version\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9cfe208f8a4e675674e181e3cbe7c0d47b078791", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 424 | Contributors (this commit): 7 | Commits (past 90d): 21 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -2083,6 +2083,7 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase, parameterized.TestCase):\n         not backend.SUPPORTS_SPARSE_TENSORS,\n         reason=\"Backend does not support sparse tensors.\",\n     )\n+    @pytest.mark.skipif(testing.tensorflow_uses_gpu(), reason=\"Segfault\")\n     def test_matmul_sparse(self, dtype, x_shape, y_shape, x_sparse, y_sparse):\n         if backend.backend() == \"tensorflow\":\n             import tensorflow as tf\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b038ce29734e31e5a371aaf2f1cb16c01be1995c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 14 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -72,8 +72,8 @@ class TimeDistributed(Wrapper):\n         self.built = True\n \n     def call(self, inputs, training=None, mask=None):\n-        input_shape = inputs.shape\n-        mask_shape = None if mask is None else tuple(mask.shape)\n+        input_shape = ops.shape(inputs)\n+        mask_shape = None if mask is None else ops.shape(mask)\n         batch_size = input_shape[0]\n         timesteps = input_shape[1]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#558d38c52aadd94fe6df83023c5aae2fb898b782", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 27 | Churn Cumulative: 269 | Contributors (this commit): 4 | Commits (past 90d): 12 | Contributors (cumulative): 7 | DMM Complexity: 0.7368421052631579\n\nDIFF:\n@@ -654,13 +654,18 @@ def _get_input_signature(model):\n         # into plain Python structures because they don't work with jax2tf/JAX.\n         if isinstance(structure, dict):\n             return {k: make_tensor_spec(v) for k, v in structure.items()}\n-        if isinstance(structure, (list, tuple)):\n+        elif isinstance(structure, tuple):\n             if all(isinstance(d, (int, type(None))) for d in structure):\n                 return tf.TensorSpec(\n                     shape=(None,) + structure[1:], dtype=model.input_dtype\n                 )\n-            result = [make_tensor_spec(v) for v in structure]\n-            return tuple(result) if isinstance(structure, tuple) else result\n+            return tuple(make_tensor_spec(v) for v in structure)\n+        elif isinstance(structure, list):\n+            if all(isinstance(d, (int, type(None))) for d in structure):\n+                return tf.TensorSpec(\n+                    shape=[None] + structure[1:], dtype=model.input_dtype\n+                )\n+            return [make_tensor_spec(v) for v in structure]\n         else:\n             raise ValueError(\n                 f\"Unsupported type {type(structure)} for {structure}\"\n\n@@ -196,6 +196,22 @@ class ExportArchiveTest(testing.TestCase, parameterized.TestCase):\n         )\n         revived_model.serve(bigger_input)\n \n+        # Test with keras.saving_lib\n+        temp_filepath = os.path.join(\n+            self.get_temp_dir(), \"exported_model.keras\"\n+        )\n+        saving_lib.save_model(model, temp_filepath)\n+        revived_model = saving_lib.load_model(\n+            temp_filepath,\n+            {\n+                \"TupleModel\": TupleModel,\n+                \"ArrayModel\": ArrayModel,\n+                \"DictModel\": DictModel,\n+            },\n+        )\n+        self.assertAllClose(ref_output, revived_model(ref_input))\n+        export_lib.export_model(revived_model, self.get_temp_dir())\n+\n     def test_model_with_multiple_inputs(self):\n \n         class TwoInputsModel(models.Model):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#656df40ff86ca493b6c03375bd912bcfd332f502", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 60 | Lines Deleted: 74 | Files Changed: 7 | Hunks: 55 | Methods Changed: 30 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 134 | Churn Cumulative: 2205 | Contributors (this commit): 7 | Commits (past 90d): 47 | Contributors (cumulative): 20 | DMM Complexity: 0.0\n\nDIFF:\n@@ -63,17 +63,17 @@ def get(identifier):\n \n     >>> policy = dtype_policies.get(\"mixed_bfloat16\")\n     >>> type(policy)\n-    <class '...FloatDTypePolicy'>\n+    <class '...DTypePolicy'>\n \n     You can also specify `config` of the dtype policy to this function by\n     passing dict containing `class_name` and `config` as an identifier. Also\n     note that the `class_name` must map to a `DTypePolicy` class\n \n-    >>> identifier = {\"class_name\": \"FloatDTypePolicy\",\n+    >>> identifier = {\"class_name\": \"DTypePolicy\",\n     ...               \"config\": {\"name\": \"float32\"}}\n     >>> policy = dtype_policies.get(identifier)\n     >>> type(policy)\n-    <class '...FloatDTypePolicy'>\n+    <class '...DTypePolicy'>\n \n     Args:\n         identifier: A dtype policy identifier. One of `None` or string name of a\n@@ -97,9 +97,9 @@ def get(identifier):\n         if identifier.startswith(QUANTIZATION_MODES):\n             return _get_quantized_dtype_policy_by_str(identifier)\n         else:\n-            return FloatDTypePolicy(identifier)\n+            return DTypePolicy(identifier)\n     try:\n-        return FloatDTypePolicy(backend.standardize_dtype(identifier))\n+        return DTypePolicy(backend.standardize_dtype(identifier))\n     except:\n         raise ValueError(\n             \"Cannot interpret `dtype` argument. Expected a string \"\n\n@@ -88,7 +88,7 @@ class DTypePolicy:\n         except ValueError:\n             raise ValueError(\n                 f\"Cannot convert '{name}' to a mixed precision \"\n-                \"FloatDTypePolicy. Valid policies include 'mixed_float16', \"\n+                \"DTypePolicy. Valid policies include 'mixed_float16', \"\n                 \"'mixed_bfloat16', and the name of any float dtype such as \"\n                 \"'float32'.\"\n             )\n@@ -182,8 +182,8 @@ class DTypePolicy:\n \n     def __repr__(self):\n         class_name = self.__class__.__name__\n-        if class_name == \"DTypePolicy\":\n-            class_name = \"FloatDTypePolicy\"\n+        if class_name == \"FloatDTypePolicy\":\n+            class_name = \"DTypePolicy\"\n         return f'<{class_name} \"{self._name}\">'\n \n     def __eq__(self, other):\n@@ -306,7 +306,7 @@ def set_dtype_policy(policy):\n             if policy.startswith(QUANTIZATION_MODES):\n                 policy = _get_quantized_dtype_policy_by_str(policy)\n             else:\n-                policy = FloatDTypePolicy(policy)\n+                policy = DTypePolicy(policy)\n         else:\n             raise ValueError(\n                 \"Invalid `policy` argument. \"\n@@ -329,7 +329,7 @@ def dtype_policy():\n     \"\"\"Returns the current default dtype policy object.\"\"\"\n     policy = global_state.get_global_attribute(\"dtype_policy\", None)\n     if policy is None:\n-        policy = FloatDTypePolicy(backend.floatx())\n+        policy = DTypePolicy(backend.floatx())\n         set_dtype_policy(policy)\n     return policy\n \n\n@@ -41,13 +41,13 @@ class DTypePolicyMap(DTypePolicy, MutableMapping):\n \n     ```python\n     dtype_policy_map = DTypePolicyMap()\n-    dtype_policy_map[\"layer/dense_0\"] = FloatDTypePolicy(\"bfloat16\")\n+    dtype_policy_map[\"layer/dense_0\"] = DTypePolicy(\"bfloat16\")\n     dtype_policy_map[\"layer/dense_1\"] = QuantizedDTypePolicy(\"int8\", \"bfloat16\")\n \n     policy_0 = dtype_policy_map[\"layer/dense_0\"]\n     policy_1 = dtype_policy_map[\"layer/dense_1\"]\n     policy_2 = dtype_policy_map[\"layer/dense_2\"]  # No hit\n-    assert policy_0 == FloatDTypePolicy(\"bfloat16\")\n+    assert policy_0 == DTypePolicy(\"bfloat16\")\n     assert policy_1 == QuantizedDTypePolicy(\"int8\", \"bfloat16\")\n     assert policy_2 == keras.config.dtype_policy()\n     ```\n@@ -167,7 +167,7 @@ class DTypePolicyMap(DTypePolicy, MutableMapping):\n             # `default_policy=None` enables us to defer to\n             # `keras.config.dtype_policy()` during loading.\n             # To support this feature, we can set `_name` and `_source_name` to\n-            # `None` in `FloatDTypePolicy` and `QuantizedDTypePolicy`,\n+            # `None` in `DTypePolicy` and `QuantizedDTypePolicy`,\n             # respectively.\n             for policy in policy_map.values():\n                 if isinstance(policy, dtype_policies.QuantizedDTypePolicy):\n\n@@ -66,11 +66,11 @@ class DTypePolicyMapTest(testing.TestCase):\n                 )\n             elif isinstance(layer, layers.BatchNormalization):\n                 self.assertEqual(\n-                    layer.dtype_policy, dtype_policies.FloatDTypePolicy()\n+                    layer.dtype_policy, dtype_policies.DTypePolicy()\n                 )\n             elif isinstance(layer, layers.ReLU):\n                 self.assertEqual(\n-                    layer.dtype_policy, dtype_policies.FloatDTypePolicy()\n+                    layer.dtype_policy, dtype_policies.DTypePolicy()\n                 )\n \n         # Verify the output after saving and loading\n@@ -84,7 +84,7 @@ class DTypePolicyMapTest(testing.TestCase):\n \n     def test_add(self):\n         dtype_policy_map = DTypePolicyMap()\n-        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.FloatDTypePolicy(\n+        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.DTypePolicy(\n             \"bfloat16\"\n         )\n         dtype_policy_map[\"layer/dense_1\"] = dtype_policies.QuantizedDTypePolicy(\n@@ -97,7 +97,7 @@ class DTypePolicyMapTest(testing.TestCase):\n         self.assertLen(dtype_policy_map, 3)\n \n         policy = dtype_policy_map[\"layer/dense_0\"]\n-        self.assertIsInstance(policy, dtype_policies.FloatDTypePolicy)\n+        self.assertIsInstance(policy, dtype_policies.DTypePolicy)\n         self.assertEqual(policy.name, \"bfloat16\")\n \n         policy = dtype_policy_map[\"layer/dense_1\"]\n@@ -113,7 +113,7 @@ class DTypePolicyMapTest(testing.TestCase):\n         with self.assertRaisesRegex(\n             ValueError, \"layer/dense_0 already exist in the DTypePolicyMap\"\n         ):\n-            dtype_policy_map[\"layer/dense_0\"] = dtype_policies.FloatDTypePolicy(\n+            dtype_policy_map[\"layer/dense_0\"] = dtype_policies.DTypePolicy(\n                 \"float32\"\n             )\n \n@@ -124,7 +124,7 @@ class DTypePolicyMapTest(testing.TestCase):\n \n     def test_get(self):\n         dtype_policy_map = DTypePolicyMap()\n-        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.FloatDTypePolicy(\n+        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.DTypePolicy(\n             \"bfloat16\"\n         )\n         dtype_policy_map[\"layer/dense_1\"] = dtype_policies.QuantizedDTypePolicy(\n@@ -136,7 +136,7 @@ class DTypePolicyMapTest(testing.TestCase):\n \n         self.assertEqual(\n             dtype_policy_map[\"layer/dense_0\"],\n-            dtype_policies.FloatDTypePolicy(\"bfloat16\"),\n+            dtype_policies.DTypePolicy(\"bfloat16\"),\n         )\n         self.assertEqual(\n             dtype_policy_map[\"layer/dense_1\"],\n@@ -161,8 +161,8 @@ class DTypePolicyMapTest(testing.TestCase):\n         )\n \n         # It will cause a ValueError in the case of one-to-many.\n-        dtype_policy_map[\"dense\"] = dtype_policies.FloatDTypePolicy(\"float32\")\n-        dtype_policy_map[\"dense_1\"] = dtype_policies.FloatDTypePolicy(\"float32\")\n+        dtype_policy_map[\"dense\"] = dtype_policies.DTypePolicy(\"float32\")\n+        dtype_policy_map[\"dense_1\"] = dtype_policies.DTypePolicy(\"float32\")\n         with self.assertRaisesRegex(\n             ValueError, \"Path 'dense_10' matches multiple dtype policy\"\n         ):\n@@ -170,7 +170,7 @@ class DTypePolicyMapTest(testing.TestCase):\n \n     def test_delete(self):\n         dtype_policy_map = DTypePolicyMap()\n-        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.FloatDTypePolicy(\n+        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.DTypePolicy(\n             \"bfloat16\"\n         )\n         dtype_policy_map[\"layer/dense_1\"] = dtype_policies.QuantizedDTypePolicy(\n@@ -179,7 +179,7 @@ class DTypePolicyMapTest(testing.TestCase):\n \n         self.assertEqual(\n             dtype_policy_map.pop(\"layer/dense_0\"),\n-            dtype_policies.FloatDTypePolicy(\"bfloat16\"),\n+            dtype_policies.DTypePolicy(\"bfloat16\"),\n         )\n         with self.assertRaises(KeyError):\n             dtype_policy_map.pop(\"layer/dense_0\")\n@@ -196,7 +196,7 @@ class DTypePolicyMapTest(testing.TestCase):\n         dtype_policy_map = DTypePolicyMap()\n         self.assertLen(dtype_policy_map, 0)\n \n-        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.FloatDTypePolicy(\n+        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.DTypePolicy(\n             \"bfloat16\"\n         )\n         dtype_policy_map[\"layer/dense_1\"] = dtype_policies.QuantizedDTypePolicy(\n@@ -206,7 +206,7 @@ class DTypePolicyMapTest(testing.TestCase):\n \n     def test_iter(self):\n         dtype_policy_map = DTypePolicyMap()\n-        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.FloatDTypePolicy(\n+        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.DTypePolicy(\n             \"bfloat16\"\n         )\n         dtype_policy_map[\"layer/dense_1\"] = dtype_policies.QuantizedDTypePolicy(\n@@ -226,14 +226,14 @@ class DTypePolicyMapTest(testing.TestCase):\n         self.assertEqual(\n             values,\n             [\n-                dtype_policies.FloatDTypePolicy(\"bfloat16\"),\n+                dtype_policies.DTypePolicy(\"bfloat16\"),\n                 dtype_policies.QuantizedDTypePolicy(\"int8\", \"mixed_bfloat16\"),\n             ],\n         )\n \n     def test_in(self):\n         dtype_policy_map = DTypePolicyMap()\n-        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.FloatDTypePolicy(\n+        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.DTypePolicy(\n             \"bfloat16\"\n         )\n         dtype_policy_map[\"layer/dense_1\"] = dtype_policies.QuantizedDTypePolicy(\n@@ -247,7 +247,7 @@ class DTypePolicyMapTest(testing.TestCase):\n     def test_default_policy(self):\n         # Test default_policy is set to `\"float32\"`\n         dtype_policy_map = DTypePolicyMap(default_policy=\"mixed_bfloat16\")\n-        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.FloatDTypePolicy(\n+        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.DTypePolicy(\n             \"mixed_bfloat16\"\n         )\n         dtype_policy_map[\"layer/dense_1\"] = dtype_policies.QuantizedDTypePolicy(\n@@ -257,7 +257,7 @@ class DTypePolicyMapTest(testing.TestCase):\n         dtype_policy_map = DTypePolicyMap.from_config(config)\n         self.assertEqual(\n             dtype_policy_map[\"layer/dense_0\"],\n-            dtype_policies.FloatDTypePolicy(\"mixed_bfloat16\"),\n+            dtype_policies.DTypePolicy(\"mixed_bfloat16\"),\n         )\n         self.assertEqual(\n             dtype_policy_map[\"layer/dense_1\"],\n@@ -272,7 +272,7 @@ class DTypePolicyMapTest(testing.TestCase):\n         # during loading\n         set_dtype_policy(\"bfloat16\")\n         dtype_policy_map = DTypePolicyMap()\n-        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.FloatDTypePolicy(\n+        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.DTypePolicy(\n             \"mixed_bfloat16\"\n         )\n         dtype_policy_map[\"layer/dense_1\"] = dtype_policies.QuantizedDTypePolicy(\n@@ -282,7 +282,7 @@ class DTypePolicyMapTest(testing.TestCase):\n         dtype_policy_map = DTypePolicyMap.from_config(config)\n         self.assertEqual(\n             dtype_policy_map[\"layer/dense_0\"],\n-            dtype_policies.FloatDTypePolicy(\"bfloat16\"),\n+            dtype_policies.DTypePolicy(\"bfloat16\"),\n         )\n         self.assertEqual(\n             dtype_policy_map[\"layer/dense_1\"],\n@@ -299,7 +299,7 @@ class DTypePolicyMapTest(testing.TestCase):\n \n     def test_serialization(self):\n         dtype_policy_map = DTypePolicyMap(default_policy=\"mixed_bfloat16\")\n-        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.FloatDTypePolicy(\n+        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.DTypePolicy(\n             \"mixed_bfloat16\"\n         )\n         dtype_policy_map[\"layer/dense_1\"] = dtype_policies.QuantizedDTypePolicy(\n@@ -323,7 +323,7 @@ class DTypePolicyMapTest(testing.TestCase):\n \n     def test_repr(self):\n         dtype_policy_map = DTypePolicyMap()\n-        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.FloatDTypePolicy(\n+        dtype_policy_map[\"layer/dense_0\"] = dtype_policies.DTypePolicy(\n             \"mixed_bfloat16\"\n         )\n         repr_str = repr(dtype_policy_map)\n@@ -343,5 +343,5 @@ class DTypePolicyMapTest(testing.TestCase):\n             TypeError, \"If specified, `policy_map` must be a dict.\"\n         ):\n             DTypePolicyMap(\n-                policy_map=dtype_policies.FloatDTypePolicy(\"mixed_bfloat16\")\n+                policy_map=dtype_policies.DTypePolicy(\"mixed_bfloat16\")\n             )\n\n@@ -12,8 +12,8 @@ from keras.src.dtype_policies.dtype_policy import set_dtype_policy\n from keras.src.testing import test_case\n \n \n-class FloatDTypePolicyTest(test_case.TestCase, parameterized.TestCase):\n-    \"\"\"Test `FloatDTypePolicy`.\n+class DTypePolicyTest(test_case.TestCase, parameterized.TestCase):\n+    \"\"\"Test `DTypePolicy`.\n \n     In the tests, we also test `DTypePolicy` for historical reasons.\n     \"\"\"\n@@ -136,10 +136,10 @@ class FloatDTypePolicyTest(test_case.TestCase, parameterized.TestCase):\n     def test_repr(self):\n         \"\"\"Test __repr__ method.\"\"\"\n         policy = DTypePolicy(\"mixed_float16\")\n-        self.assertEqual(repr(policy), '<FloatDTypePolicy \"mixed_float16\">')\n+        self.assertEqual(repr(policy), '<DTypePolicy \"mixed_float16\">')\n \n         policy = FloatDTypePolicy(\"mixed_float16\")\n-        self.assertEqual(repr(policy), '<FloatDTypePolicy \"mixed_float16\">')\n+        self.assertEqual(repr(policy), '<DTypePolicy \"mixed_float16\">')\n \n     def test_get_config_from_config(self):\n         \"\"\"Test get_config and from_config methods.\"\"\"\n@@ -184,46 +184,34 @@ class FloatDTypePolicyTest(test_case.TestCase, parameterized.TestCase):\n \n         # copy.deepcopy\n         copied_policy = copy.deepcopy(policy)\n-        self.assertEqual(\n-            repr(copied_policy), '<FloatDTypePolicy \"mixed_float16\">'\n-        )\n+        self.assertEqual(repr(copied_policy), '<DTypePolicy \"mixed_float16\">')\n         # copy.copy\n         copied_policy = copy.copy(policy)\n-        self.assertEqual(\n-            repr(copied_policy), '<FloatDTypePolicy \"mixed_float16\">'\n-        )\n+        self.assertEqual(repr(copied_policy), '<DTypePolicy \"mixed_float16\">')\n         # pickle\n         temp_dir = self.get_temp_dir()\n         with open(f\"{temp_dir}/policy.pickle\", \"wb\") as f:\n             pickle.dump(policy, f)\n         with open(f\"{temp_dir}/policy.pickle\", \"rb\") as f:\n             copied_policy = pickle.load(f)\n-        self.assertEqual(\n-            repr(copied_policy), '<FloatDTypePolicy \"mixed_float16\">'\n-        )\n+        self.assertEqual(repr(copied_policy), '<DTypePolicy \"mixed_float16\">')\n \n         # Test FloatDTypePolicy\n         policy = FloatDTypePolicy(\"mixed_float16\")\n \n         # copy.deepcopy\n         copied_policy = copy.deepcopy(policy)\n-        self.assertEqual(\n-            repr(copied_policy), '<FloatDTypePolicy \"mixed_float16\">'\n-        )\n+        self.assertEqual(repr(copied_policy), '<DTypePolicy \"mixed_float16\">')\n         # copy.copy\n         copied_policy = copy.copy(policy)\n-        self.assertEqual(\n-            repr(copied_policy), '<FloatDTypePolicy \"mixed_float16\">'\n-        )\n+        self.assertEqual(repr(copied_policy), '<DTypePolicy \"mixed_float16\">')\n         # pickle\n         temp_dir = self.get_temp_dir()\n         with open(f\"{temp_dir}/policy.pickle\", \"wb\") as f:\n             pickle.dump(policy, f)\n         with open(f\"{temp_dir}/policy.pickle\", \"rb\") as f:\n             copied_policy = pickle.load(f)\n-        self.assertEqual(\n-            repr(copied_policy), '<FloatDTypePolicy \"mixed_float16\">'\n-        )\n+        self.assertEqual(repr(copied_policy), '<DTypePolicy \"mixed_float16\">')\n \n     def test_eq(self):\n         policy = DTypePolicy(\"mixed_bfloat16\")\n@@ -564,14 +552,14 @@ class DTypePolicyGlobalFunctionsTest(test_case.TestCase):\n         self.assertEqual(policy.name, \"int8_from_mixed_bfloat16\")\n \n     def test_set_dtype_policy_valid_policy(self):\n-        \"\"\"Test set_dtype_policy with a valid FloatDTypePolicy object.\"\"\"\n-        policy_obj = FloatDTypePolicy(\"mixed_float16\")\n+        \"\"\"Test set_dtype_policy with a valid DTypePolicy object.\"\"\"\n+        policy_obj = DTypePolicy(\"mixed_float16\")\n         set_dtype_policy(policy_obj)\n         policy = dtype_policy()\n         self.assertEqual(policy.name, \"mixed_float16\")\n \n     def test_set_dtype_policy_valid_policy_quantized(self):\n-        \"\"\"Test set_dtype_policy with a valid FloatDTypePolicy object.\"\"\"\n+        \"\"\"Test set_dtype_policy with a valid QuantizedDTypePolicy object.\"\"\"\n         policy_obj = QuantizedDTypePolicy(\n             mode=\"int8\", source_name=\"mixed_bfloat16\"\n         )\n@@ -634,26 +622,26 @@ class DTypePolicyGlobalFunctionsTest(test_case.TestCase):\n             get(\"int8_abc_\")\n \n \n-class FloatDTypePolicyEdgeCasesTest(test_case.TestCase):\n+class DTypePolicyEdgeCasesTest(test_case.TestCase):\n     def test_empty_name(self):\n         \"\"\"Test initialization with an empty name.\"\"\"\n         with self.assertRaisesRegex(ValueError, \"Cannot convert\"):\n-            FloatDTypePolicy(\"\")\n+            DTypePolicy(\"\")\n \n     def test_special_character_name(self):\n         \"\"\"Test initialization with special characters in the name.\"\"\"\n         with self.assertRaisesRegex(ValueError, \"Cannot convert\"):\n-            FloatDTypePolicy(\"@mixed_float16!\")\n+            DTypePolicy(\"@mixed_float16!\")\n \n     def test_very_long_name(self):\n         \"\"\"Test initialization with a very long name.\"\"\"\n         with self.assertRaisesRegex(ValueError, \"Cannot convert\"):\n-            FloatDTypePolicy(\"mixed_float16\" * 100)\n+            DTypePolicy(\"mixed_float16\" * 100)\n \n     def test_almost_valid_name(self):\n         \"\"\"Test initialization with a name close to a valid one.\"\"\"\n         with self.assertRaisesRegex(ValueError, \"Cannot convert\"):\n-            FloatDTypePolicy(\"mixed_float15\")\n+            DTypePolicy(\"mixed_float15\")\n \n \n class QuantizedDTypePolicyEdgeCasesTest(test_case.TestCase):\n\n@@ -971,8 +971,8 @@ class LayerTest(testing.TestCase):\n         self.assertEqual(layer.dtype_policy.name, \"mixed_bfloat16\")\n         self.assertEqual(layer.dtype_policy.compute_dtype, \"bfloat16\")\n         self.assertEqual(layer.dtype_policy.variable_dtype, \"float32\")\n-        # Set by FloatDTypePolicy\n-        layer.dtype_policy = dtype_policies.FloatDTypePolicy(\"mixed_float16\")\n+        # Set by DTypePolicy\n+        layer.dtype_policy = dtype_policies.DTypePolicy(\"mixed_float16\")\n         self.assertEqual(layer.dtype_policy.name, \"mixed_float16\")\n         self.assertEqual(layer.dtype_policy.compute_dtype, \"float16\")\n         self.assertEqual(layer.dtype_policy.variable_dtype, \"float32\")\n@@ -986,7 +986,7 @@ class LayerTest(testing.TestCase):\n         )\n         self.assertEqual(\n             layer._dtype_policy[layer.path],\n-            dtype_policies.FloatDTypePolicy(\"mixed_bfloat16\"),\n+            dtype_policies.DTypePolicy(\"mixed_bfloat16\"),\n         )\n \n     def test_pickle_layer(self):\n@@ -1003,7 +1003,7 @@ class LayerTest(testing.TestCase):\n                 dtype = kwargs[\"dtype\"]\n                 if isinstance(dtype, str):\n                     # `dtype` is a plain string, it should be the `name` from a\n-                    # `FloatDTypePolicy`\n+                    # `DTypePolicy`\n                     dtype = dtype_policies.get(dtype)\n                     assertIsNone(dtype.quantization_mode)\n                 else:\n@@ -1018,9 +1018,7 @@ class LayerTest(testing.TestCase):\n         self.assertIn(\"dtype\", config)\n         self.assertEqual(\n             config[\"dtype\"],\n-            dtype_policies.serialize(\n-                dtype_policies.FloatDTypePolicy(\"bfloat16\")\n-            ),\n+            dtype_policies.serialize(dtype_policies.DTypePolicy(\"bfloat16\")),\n         )\n         AssertionDense.from_config(config)  # Assertion inside\n \n\n@@ -106,7 +106,7 @@ class Operation:\n         dtype = kwargs.get(\"dtype\", None)\n         if dtype is not None and isinstance(dtype, dtype_policies.DTypePolicy):\n             # For backward compatibility, we use a str (`name`) for\n-            # `FloatDTypePolicy`\n+            # `DTypePolicy`\n             if dtype.quantization_mode is None:\n                 kwargs[\"dtype\"] = dtype.name\n             # Otherwise, use `dtype_policies.serialize`\n@@ -227,7 +227,7 @@ class Operation:\n                 and policy.quantization_mode is None\n             ):\n                 # For backward compatibility, we use a str (`name`) for\n-                # `FloatDTypePolicy`\n+                # `DTypePolicy`\n                 policy = policy.name\n             config[\"dtype\"] = policy\n         try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a2e9a5252d2eab389bd19d359e6e7325a8232c79", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/5 | Churn Δ: 30 | Churn Cumulative: 67 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -167,9 +167,7 @@ def beta(shape, alpha, beta, dtype=None, seed=None):\n     # such as for output shape of (2, 3) and alpha shape of (1, 3)\n     # So to resolve this, we explicitly broadcast alpha and beta to shape before\n     # passing them to the stateless_gamma function.\n-    if tf.rank(alpha) > 1:\n     alpha = tf.broadcast_to(alpha, shape)\n-    if tf.rank(beta) > 1:\n     beta = tf.broadcast_to(beta, shape)\n \n     gamma_a = tf.cast(\n\n@@ -391,6 +391,34 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n             )\n \n \n+class RandomBehaviorTest(testing.TestCase, parameterized.TestCase):\n+    def test_beta_tf_data_compatibility(self):\n+        import tensorflow as tf\n+\n+        from keras.src.layers.preprocessing.tf_data_layer import TFDataLayer\n+\n+        class BetaLayer(TFDataLayer):\n+            def __init__(self, **kwargs):\n+                super().__init__(**kwargs)\n+\n+            def compute_output_shape(self, input_shape):\n+                return input_shape\n+\n+            def call(self, inputs):\n+                noise = self.backend.random.beta(\n+                    self.backend.shape(inputs), alpha=0.5, beta=0.5\n+                )\n+                inputs = inputs + noise\n+                return inputs\n+\n+        layer = BetaLayer()\n+        input_data = np.random.random([2, 4, 4, 3])\n+        ds = tf.data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n+        for output in ds.take(1):\n+            output = output.numpy()\n+        self.assertEqual(tuple(output.shape), (2, 4, 4, 3))\n+\n+\n class RandomDTypeTest(testing.TestCase, parameterized.TestCase):\n     INT_DTYPES = [x for x in dtypes.INT_TYPES if x != \"uint64\"]\n     FLOAT_DTYPES = dtypes.FLOAT_TYPES\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5f52db2df8a51e4345c15b4624260d36c992e923", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 14 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,7 @@\n from keras.src.api_export import keras_export\n \n # Unique source of truth for the version number.\n-__version__ = \"3.4.0\"\n+__version__ = \"3.4.1\"\n \n \n @keras_export(\"keras.version\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f5e90a2c5a6ce9f90062a5b11cfc678ace8fc237", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 13 | Methods Changed: 3 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 23 | Churn Cumulative: 542 | Contributors (this commit): 4 | Commits (past 90d): 11 | Contributors (cumulative): 7 | DMM Complexity: 0.07142857142857142\n\nDIFF:\n@@ -160,6 +160,9 @@ def _save_model_to_fileobj(model, fileobj, weights_format):\n             f.write(config_json.encode())\n \n         weights_file_path = None\n+        weights_store = None\n+        asset_store = None\n+        write_zf = False\n         try:\n             if weights_format == \"h5\":\n                 if isinstance(fileobj, io.BufferedWriter):\n@@ -168,6 +171,7 @@ def _save_model_to_fileobj(model, fileobj, weights_format):\n                     working_dir = pathlib.Path(fileobj.name).parent\n                     weights_file_path = working_dir / _VARS_FNAME_H5\n                     weights_store = H5IOStore(weights_file_path, mode=\"w\")\n+                    write_zf = True\n                 else:\n                     # Fall back when `fileobj` is an `io.BytesIO`. Typically,\n                     # this usage is for pickling.\n@@ -196,13 +200,17 @@ def _save_model_to_fileobj(model, fileobj, weights_format):\n             )\n         except:\n             # Skip the final `zf.write` if any exception is raised\n-            weights_file_path = None\n+            write_zf = False\n             raise\n         finally:\n+            if weights_store:\n                 weights_store.close()\n+            if asset_store:\n                 asset_store.close()\n-            if weights_file_path:\n+            if write_zf and weights_file_path:\n                 zf.write(weights_file_path, weights_file_path.name)\n+            if weights_file_path:\n+                weights_file_path.unlink()\n \n \n def load_model(filepath, custom_objects=None, compile=True, safe_mode=True):\n@@ -309,15 +317,22 @@ def _load_model_from_fileobj(fileobj, custom_objects, compile, safe_mode):\n \n         all_filenames = zf.namelist()\n         weights_file_path = None\n+        weights_store = None\n+        asset_store = None\n         try:\n             if _VARS_FNAME_H5 in all_filenames:\n                 if isinstance(fileobj, io.BufferedReader):\n                     # First, extract the model.weights.h5 file, then load it\n                     # using h5py.\n                     working_dir = pathlib.Path(fileobj.name).parent\n+                    try:\n                         zf.extract(_VARS_FNAME_H5, working_dir)\n                         weights_file_path = working_dir / _VARS_FNAME_H5\n                         weights_store = H5IOStore(weights_file_path, mode=\"r\")\n+                    except OSError:\n+                        # Fall back when it is a read-only system\n+                        weights_file_path = None\n+                        weights_store = H5IOStore(_VARS_FNAME_H5, zf, mode=\"r\")\n                 else:\n                     # Fall back when `fileobj` is an `io.BytesIO`. Typically,\n                     # this usage is for pickling.\n@@ -331,8 +346,6 @@ def _load_model_from_fileobj(fileobj, custom_objects, compile, safe_mode):\n \n             if len(all_filenames) > 3:\n                 asset_store = DiskIOStore(_ASSETS_DIRNAME, archive=zf, mode=\"r\")\n-            else:\n-                asset_store = None\n \n             failed_saveables = set()\n             error_msgs = {}\n@@ -346,6 +359,7 @@ def _load_model_from_fileobj(fileobj, custom_objects, compile, safe_mode):\n                 error_msgs=error_msgs,\n             )\n         finally:\n+            if weights_store:\n                 weights_store.close()\n             if asset_store:\n                 asset_store.close()\n\n@@ -634,6 +634,7 @@ class SavingTest(testing.TestCase):\n         with zipfile.ZipFile(filepath) as zf:\n             all_filenames = zf.namelist()\n             self.assertNotIn(\"model.weights.h5\", all_filenames)\n+        self.assertFalse(Path(filepath).with_name(\"model.weights.h5\").exists())\n \n     def test_load_model_exception_raised(self):\n         # Assume we have an error in `load_own_variables`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#179241220be72c62ee6d97ae502df743a2b599d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 52 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -415,8 +415,8 @@ class RandomBehaviorTest(testing.TestCase, parameterized.TestCase):\n         input_data = np.random.random([2, 4, 4, 3])\n         ds = tf.data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n         for output in ds.take(1):\n-            output = output.numpy()\n-        self.assertEqual(tuple(output.shape), (2, 4, 4, 3))\n+            output = ops.convert_to_numpy(output)\n+        self.assertEqual(output.shape, (2, 4, 4, 3))\n \n \n class RandomDTypeTest(testing.TestCase, parameterized.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a9fb58e282f97c0d71bf0cfbe0b614d163d6bee9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 16 | Files Changed: 7 | Hunks: 16 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 36 | Churn Cumulative: 442 | Contributors (this commit): 6 | Commits (past 90d): 23 | Contributors (cumulative): 21 | DMM Complexity: 0.0\n\nDIFF:\n@@ -63,11 +63,11 @@ class BackupAndRestore(Callback):\n           When set to an integer, the callback saves the checkpoint every\n           `save_freq` batches. Set `save_freq=False` only if using\n           preemption checkpointing (i.e. with `save_before_preemption=True`).\n-        delete_checkpoint: Boolean, defaults to `True`. This `BackupAndRestore`\n+        delete_checkpoint: Boolean. This `BackupAndRestore`\n           callback works by saving a checkpoint to back up the training state.\n           If `delete_checkpoint=True`, the checkpoint will be deleted after\n           training is finished. Use `False` if you'd like to keep the checkpoint\n-          for future usage.\n+          for future usage. Defaults to `True`.\n     \"\"\"\n \n     def __init__(\n\n@@ -1298,8 +1298,8 @@ def one_hot(x, num_classes, axis=-1, dtype=None, sparse=False):\n         x: Integer tensor to be encoded. The shape can be\n             arbitrary, but the dtype should be integer.\n         num_classes: Number of classes for the one-hot encoding.\n-        axis: Axis along which the encoding is performed. Defaults to\n-            `-1`, which represents the last axis.\n+        axis: Axis along which the encoding is performed.\n+            `-1` represents the last axis. Defaults to `-1`.\n         dtype: (Optional) Data type of the output tensor. If not\n             provided, it defaults to the default data type of the backend.\n         sparse: Whether to return a sparse tensor; for backends that support\n\n@@ -82,8 +82,9 @@ def audio_dataset_from_directory(\n             length of the longest sequence in the batch.\n         ragged: Whether to return a Ragged dataset (where each sequence has its\n             own length). Defaults to `False`.\n-        shuffle: Whether to shuffle the data. Defaults to `True`.\n+        shuffle: Whether to shuffle the data.\n             If set to `False`, sorts the data in alphanumeric order.\n+            Defaults to `True`.\n         seed: Optional random seed for shuffling and transformations.\n         validation_split: Optional float between 0 and 1, fraction of data to\n             reserve for validation.\n\n@@ -83,15 +83,15 @@ def image_dataset_from_directory(\n             (must match names of subdirectories). Used to control the order\n             of the classes (otherwise alphanumerical order is used).\n         color_mode: One of `\"grayscale\"`, `\"rgb\"`, `\"rgba\"`.\n-            Defaults to `\"rgb\"`. Whether the images will be converted to\n-            have 1, 3, or 4 channels.\n+            Whether the images will be converted to\n+            have 1, 3, or 4 channels. Defaults to `\"rgb\"`.\n         batch_size: Size of the batches of data. Defaults to 32.\n             If `None`, the data will not be batched\n             (the dataset will yield individual samples).\n         image_size: Size to resize images to after they are read from disk,\n-            specified as `(height, width)`. Defaults to `(256, 256)`.\n+            specified as `(height, width)`.\n             Since the pipeline processes batches of images that must all have\n-            the same size, this must be provided.\n+            the same size, this must be provided. Defaults to `(256, 256)`.\n         shuffle: Whether to shuffle the data. Defaults to `True`.\n             If set to `False`, sorts the data in alphanumeric order.\n         seed: Optional random seed for shuffling and transformations.\n@@ -103,9 +103,10 @@ def image_dataset_from_directory(\n             When `subset=\"both\"`, the utility returns a tuple of two datasets\n             (the training and validation datasets respectively).\n         interpolation: String, the interpolation method used when\n-            resizing images. Defaults to `\"bilinear\"`.\n+            resizing images.\n             Supports `\"bilinear\"`, `\"nearest\"`, `\"bicubic\"`, `\"area\"`,\n             `\"lanczos3\"`, `\"lanczos5\"`, `\"gaussian\"`, `\"mitchellcubic\"`.\n+            Defaults to `\"bilinear\"`.\n         follow_links: Whether to visit subdirectories pointed to by symlinks.\n             Defaults to `False`.\n         crop_to_aspect_ratio: If `True`, resize the images without aspect\n\n@@ -350,9 +350,9 @@ def smart_resize(\n             or `(batch_size, height, width, channels)`.\n         size: Tuple of `(height, width)` integer. Target size.\n         interpolation: String, interpolation to use for resizing.\n-            Defaults to `'bilinear'`.\n-            Supports `bilinear`, `nearest`, `bicubic`,\n-            `lanczos3`, `lanczos5`.\n+            Supports `\"bilinear\"`, `\"nearest\"`, `\"bicubic\"`,\n+            `\"lanczos3\"`, `\"lanczos5\"`.\n+            Defaults to `\"bilinear\"`.\n         data_format: `\"channels_last\"` or `\"channels_first\"`.\n         backend_module: Backend module to use (if different from the default\n             backend).\n\n@@ -69,7 +69,7 @@ def pad_sequences(\n         truncating: String, \"pre\" or \"post\" (optional, defaults to `\"pre\"`):\n             remove values from sequences larger than\n             `maxlen`, either at the beginning or at the end of the sequences.\n-        value: Float or String, padding value. (Optional, defaults to 0.)\n+        value: Float or String, padding value. (Optional, defaults to `0.`)\n \n     Returns:\n         NumPy array with shape `(len(sequences), maxlen)`\n\n@@ -72,13 +72,15 @@ def text_dataset_from_directory(\n             This is the explicit list of class names\n             (must match names of subdirectories). Used to control the order\n             of the classes (otherwise alphanumerical order is used).\n-        batch_size: Size of the batches of data. Defaults to 32.\n+        batch_size: Size of the batches of data.\n             If `None`, the data will not be batched\n             (the dataset will yield individual samples).\n+            Defaults to `32`.\n         max_length: Maximum size of a text string. Texts longer than this will\n             be truncated to `max_length`.\n-        shuffle: Whether to shuffle the data. Defaults to `True`.\n+        shuffle: Whether to shuffle the data.\n             If set to `False`, sorts the data in alphanumeric order.\n+            Defaults to `True`.\n         seed: Optional random seed for shuffling and transformations.\n         validation_split: Optional float between 0 and 1,\n             fraction of data to reserve for validation.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ab5061289db317023c08b8be3decc528ea38e09c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 58 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -400,13 +400,17 @@ class RandomBehaviorTest(testing.TestCase, parameterized.TestCase):\n         class BetaLayer(TFDataLayer):\n             def __init__(self, **kwargs):\n                 super().__init__(**kwargs)\n+                self.generator = keras.random.SeedGenerator()\n \n             def compute_output_shape(self, input_shape):\n                 return input_shape\n \n             def call(self, inputs):\n                 noise = self.backend.random.beta(\n-                    self.backend.shape(inputs), alpha=0.5, beta=0.5\n+                    self.backend.shape(inputs),\n+                    alpha=0.5,\n+                    beta=0.5,\n+                    seed=self._get_seed_generator(),\n                 )\n                 inputs = inputs + noise\n                 return inputs\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cb131b38a665007e5030d6284c60379e28e2eb75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 122 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 14 | Methods Changed: 7 | Complexity Δ (Sum/Max): 13/11 | Churn Δ: 128 | Churn Cumulative: 774 | Contributors (this commit): 8 | Commits (past 90d): 20 | Contributors (cumulative): 15 | DMM Complexity: 0.0\n\nDIFF:\n@@ -265,7 +265,7 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n         )\n \n     @traceback_utils.filter_traceback\n-    def save(self, filepath, overwrite=True, zipped=True, **kwargs):\n+    def save(self, filepath, overwrite=True, zipped=None, **kwargs):\n         \"\"\"Saves a model as a `.keras` file.\n \n         Args:\n@@ -277,7 +277,9 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n                 the target location, or instead ask the user via\n                 an interactive prompt.\n             zipped: Whether to save the model as a zipped `.keras`\n-                archive (default), or as an unzipped directory.\n+                archive (default when saving locally), or as an\n+                unzipped directory (default when saving on the\n+                Hugging Face Hub).\n \n         Example:\n \n\n@@ -16,7 +16,7 @@ except ImportError:\n \n \n @keras_export([\"keras.saving.save_model\", \"keras.models.save_model\"])\n-def save_model(model, filepath, overwrite=True, zipped=True, **kwargs):\n+def save_model(model, filepath, overwrite=True, zipped=None, **kwargs):\n     \"\"\"Saves a model as a `.keras` file.\n \n     Args:\n@@ -25,7 +25,8 @@ def save_model(model, filepath, overwrite=True, zipped=True, **kwargs):\n         overwrite: Whether we should overwrite any existing model at the target\n             location, or instead ask the user via an interactive prompt.\n         zipped: Whether to save the model as a zipped `.keras`\n-            archive (default), or as an unzipped directory.\n+            archive (default when saving locally), or as an unzipped directory\n+            (default when saving on the Hugging Face Hub).\n \n     Example:\n \n@@ -88,9 +89,13 @@ def save_model(model, filepath, overwrite=True, zipped=True, **kwargs):\n             \"`keras.saving.save_model(model, 'my_model.keras')`. \"\n         )\n \n+    is_hf = str(filepath).startswith(\"hf://\")\n+    if zipped is None:\n+        zipped = not is_hf  # default behavior depends on destination\n+\n     # If file exists and should not be overwritten.\n     try:\n-        exists = os.path.exists(filepath)\n+        exists = (not is_hf) and os.path.exists(filepath)\n     except TypeError:\n         exists = False\n     if exists and not overwrite:\n\n@@ -22,7 +22,10 @@ from keras.src.saving.serialization_lib import deserialize_keras_object\n from keras.src.saving.serialization_lib import serialize_keras_object\n from keras.src.trainers.compile_utils import CompileMetrics\n from keras.src.utils import file_utils\n+from keras.src.utils import io_utils\n from keras.src.utils import naming\n+from keras.src.utils import plot_model\n+from keras.src.utils.model_visualization import check_pydot\n from keras.src.version import __version__ as keras_version\n \n try:\n@@ -30,6 +33,12 @@ try:\n except ImportError:\n     h5py = None\n \n+try:\n+    import huggingface_hub\n+except ImportError:\n+    huggingface_hub = None\n+\n+\n _CONFIG_FILENAME = \"config.json\"\n _METADATA_FILENAME = \"metadata.json\"\n _VARS_FNAME = \"model.weights\"  # Will become e.g. \"model.weights.h5\"\n@@ -38,6 +47,23 @@ _VARS_FNAME_NPZ = _VARS_FNAME + \".npz\"\n _ASSETS_DIRNAME = \"assets\"\n \n \n+_MODEL_CARD_TEMPLATE = \"\"\"\n+---\n+library_name: keras\n+---\n+\n+This model has been uploaded using the Keras library and can be used with JAX,\n+TensorFlow, and PyTorch backends.\n+\n+This model card has been generated automatically and should be completed by the\n+model author.\n+See [Model Cards documentation](https://huggingface.co/docs/hub/model-cards) for\n+more information.\n+\n+For more details about the model architecture, check out\n+[config.json](./config.json).\"\"\"\n+\n+\n def save_model(model, filepath, weights_format=\"h5\", zipped=True):\n     \"\"\"Save a zip-archive representing a Keras model to the given file or path.\n \n@@ -79,6 +105,7 @@ def save_model(model, filepath, weights_format=\"h5\", zipped=True):\n         return\n \n     filepath = str(filepath)\n+    is_hf = filepath.startswith(\"hf://\")\n     if zipped and not filepath.endswith(\".keras\"):\n         raise ValueError(\n             \"Invalid `filepath` argument: expected a `.keras` extension. \"\n@@ -89,7 +116,14 @@ def save_model(model, filepath, weights_format=\"h5\", zipped=True):\n             \"When using `zipped=False`, the `filepath` argument should not \"\n             f\"end in `.keras`. Received: filepath={filepath}\"\n         )\n-    if not zipped:\n+    if zipped and is_hf:\n+        raise ValueError(\n+            \"When saving to the Hugging Face Hub, you should not save the \"\n+            f\"model as zipped. Received: filepath={filepath}, zipped={zipped}\"\n+        )\n+    if is_hf:\n+        _upload_model_to_hf(model, filepath, weights_format)\n+    elif not zipped:\n         _save_model_to_dir(model, filepath, weights_format)\n     else:\n         if file_utils.is_remote_path(filepath):\n@@ -213,12 +247,87 @@ def _save_model_to_fileobj(model, fileobj, weights_format):\n                 weights_file_path.unlink()\n \n \n+def _upload_model_to_hf(model, hf_path, weights_format):\n+    if huggingface_hub is None:\n+        raise ImportError(\n+            \"To save models to the Hugging Face Hub, \"\n+            \"you must install the `huggingface_hub` package.\"\n+        )\n+\n+    original_hf_path = hf_path\n+    if hf_path.startswith(\"hf://\"):\n+        hf_path = hf_path[5:]\n+    if hf_path.count(\"/\") > 1:\n+        raise ValueError(\n+            \"Invalid `hf_path` argument: expected `namespace/model_name`\"\n+            f\" format. Received: hf_path={original_hf_path}\"\n+        )\n+\n+    api = huggingface_hub.HfApi(\n+        library_name=\"keras\", library_version=keras_version\n+    )\n+    repo_url = api.create_repo(hf_path, exist_ok=True)\n+    repo_id = repo_url.repo_id\n+\n+    with tempfile.TemporaryDirectory() as tmp_dir:\n+        _save_model_to_dir(model, tmp_dir, weights_format)\n+\n+        model_card = _MODEL_CARD_TEMPLATE\n+\n+        if check_pydot():\n+            plot_path = file_utils.join(tmp_dir, \"assets\", \"summary_plot.png\")\n+            plot_model(\n+                model,\n+                to_file=plot_path,\n+                show_layer_names=True,\n+                show_shapes=True,\n+                show_dtype=True,\n+            )\n+            if len(model.layers) <= 10:\n+                model_card += \"\\n\\n![](./assets/summary_plot.png)\"\n+            else:\n+                model_card += (\n+                    \"A plot of the model can be found \"\n+                    \"[here](./assets/summary_plot.png).\"\n+                )\n+\n+        with open(file_utils.join(tmp_dir, \"README.md\"), \"w\") as f:\n+            f.write(model_card)\n+\n+        api.upload_folder(\n+            repo_id=repo_id,\n+            folder_path=tmp_dir,\n+            commit_message=\"Save model using Keras.\",\n+        )\n+        io_utils.print_msg(\n+            f\"Model saved to the Hugging Face Hub: {repo_url}\\n\"\n+            \"To load back the model, use \"\n+            f\"`keras.saving.load_model('hf://{repo_id}')`\"\n+        )\n+\n+\n def load_model(filepath, custom_objects=None, compile=True, safe_mode=True):\n     \"\"\"Load a zip archive representing a Keras model.\"\"\"\n     if isinstance(filepath, io.IOBase):\n         return _load_model_from_fileobj(\n             filepath, custom_objects, compile, safe_mode\n         )\n+    elif str(filepath).startswith(\"hf://\"):\n+        if huggingface_hub is None:\n+            raise ImportError(\n+                \"To load models from the Hugging Face Hub, \"\n+                \"you must install the `huggingface_hub` package.\"\n+            )\n+\n+        repo_id = filepath[5:]\n+        folder_path = huggingface_hub.snapshot_download(\n+            repo_id=repo_id,\n+            library_name=\"keras\",\n+            library_version=keras_version,\n+        )\n+        return _load_model_from_dir(\n+            folder_path, custom_objects, compile, safe_mode\n+        )\n     else:\n         filepath = str(filepath)\n         if not filepath.endswith(\".keras\"):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
