{"custom_id": "keras#afd86e95fc91b98dfb30eac27933b1e10b201b97", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 433 | Contributors (this commit): 6 | Commits (past 90d): 7 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -134,9 +134,7 @@ class KerasComponentsBenchmarks(tf.test.Benchmark):\n                 },\n                 {\n                     \"name\": \"us_per_exp\",\n-                    \"value\": float(\n-                        f\"{total_time * 1000000.0 / num_iters:.3f}\"\n-                    ),\n+                    \"value\": float(f\"{total_time * 1000000.0 / num_iters:.3f}\"),\n                 },\n             ],\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#83390b4f32ae41e29223928e2bfc4104c2fb9833", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 623 | Contributors (this commit): 8 | Commits (past 90d): 18 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -42,7 +42,7 @@ def _infer_shape_dtype_and_create_handle(initial_value, shape, dtype, name):\n         device_context_manager = ops.NullContextmanager\n         attr = attr_value_pb2.AttrValue(\n             list=attr_value_pb2.AttrValue.ListValue(\n-                s=[compat.as_bytes(\"loc:@%s\" % handle_name)]\n+                s=[compat.as_bytes(f\"loc:@{handle_name}\")]\n             )\n         )\n         with ops.get_default_graph()._attr_scope({\"_class\": attr}):\n\n@@ -63,8 +63,7 @@ class DTensorBaseTest(tf.test.TestCase, parameterized.TestCase):\n             mesh = device_type_mesh_map.get(device_type, None)\n             if mesh is None:\n                 raise ValueError(\n-                    \"Requires a %s mesh to run test on %s.\"\n-                    % (device_type, device_type)\n+                    f\"Requires a {device_type} mesh to run test on {device_type}.\"\n                 )\n             return mesh\n \n@@ -141,8 +140,7 @@ def reset_logical_devices(device_type, count):\n         )\n     else:\n         raise ValueError(\n-            \"resetting logical device for non-supported device type : \"\n-            \"%s\" % device_type\n+            f\"resetting logical device for non-supported device type : {device_type}\"\n         )\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#78cad61583d5a8fd7a29f18c8b36da376ffef355", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): -12/0 | Churn Δ: 2 | Churn Cumulative: 499 | Contributors (this commit): 9 | Commits (past 90d): 16 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -222,7 +222,7 @@ class MultiHeadAttention(Layer):\n       attention_output: The result of the computation, of shape `(B, T, E)`,\n         where `T` is for target sequence shapes and `E` is the query input last\n         dimension if `output_shape` is `None`. Otherwise, the multi-head outputs\n-        are project to the shape specified by `output_shape`.\n+        are projected to the shape specified by `output_shape`.\n       attention_scores: [Optional] multi-head attention coefficients over\n         attention axes.\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c48142d02f60df9bc6d218ae53093dd6b50bdf1a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 17 | Churn Cumulative: 1312 | Contributors (this commit): 8 | Commits (past 90d): 23 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -132,7 +132,6 @@ class OptimizersTest(test_util.DTensorBaseTest):\n         self.assertEqual(self.evaluate(optimizer.iterations), 1)\n \n         all_names = [var._shared_name for var in optimizer_variables]\n-        expect_variable_names.extend([\"iteration\", \"learning_rate\"])\n         self.assertCountEqual(all_names, expect_variable_names)\n \n \n\n@@ -600,9 +600,19 @@ class _BaseOptimizer(tf.Module):\n         sake of backward compatibility with `optimizer_v2.Optimizer`'s\n         `variable()` method.\n         \"\"\"\n+\n+        def predicate(obj):\n+            if not isinstance(obj, tf.Variable):\n+                return False\n+            # Exclude `iteration` and `learning_rate` to keep backward\n+            # compatibilty with `optimizer_v2.Optimizer`.\n+            return (\n+                \"iteration\" not in obj.name and \"learning_rate\" not in obj.name\n+            )\n+\n         return tuple(\n             self._flatten(\n-                predicate=lambda obj: isinstance(obj, tf.Variable),\n+                predicate=predicate,\n                 expand_composites=True,\n             )\n         )\n\n@@ -178,12 +178,10 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n         optimizer.apply_gradients(zip([grads], [x]))\n         optimizer_variables = optimizer.variables()\n         all_names = [var._shared_name for var in optimizer_variables]\n-        self.assertLen(optimizer_variables, 4)\n+        self.assertLen(optimizer_variables, 2)\n         self.assertCountEqual(\n             all_names,\n             [\n-                \"iteration\",\n-                \"learning_rate\",\n                 \"Adam/m/Variable\",\n                 \"Adam/v/Variable\",\n             ],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2af5d68612f02f3866ecdb626df6467bf1fbe45d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 32 | Churn Cumulative: 1111 | Contributors (this commit): 7 | Commits (past 90d): 19 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -122,6 +122,33 @@ class _BaseOptimizer(tf.Module):\n         # issues on AggregatingVariable.\n         return variable._unique_id\n \n+    def _deduplicate_sparse_grad(self, grads):\n+        \"\"\"Deduplicate sparse gradient.\n+\n+        For sparse gradients, i.e., gradient is of type `tf.IndexedSlices`,\n+        it is possible that `gradient.indices` has duplicated indices.\n+        This function adds up values for the duplicated indices, and returns\n+        a `tf.IndexedSlices` with indices of unique values.\n+        \"\"\"\n+        processed_grads = []\n+        for grad in grads:\n+            if isinstance(grad, tf.IndexedSlices):\n+                values = grad.values\n+                indices = grad.indices\n+                unique_indices, new_index_positions = tf.unique(indices)\n+                summed_values = tf.math.unsorted_segment_sum(\n+                    values, new_index_positions, tf.shape(unique_indices)[0]\n+                )\n+                processed_grads.append(\n+                    tf.IndexedSlices(\n+                        summed_values, unique_indices, grad.dense_shape\n+                    )\n+                )\n+            else:\n+                processed_grads.append(grad)\n+\n+        return processed_grads\n+\n     @abc.abstractmethod\n     def update_step(self, gradient, variable):\n         \"\"\"Function to update variable value based on given gradients.\n@@ -471,6 +498,7 @@ class _BaseOptimizer(tf.Module):\n                 # issues.\n                 self.build(trainable_variables)\n         grads = self._clip_gradients(grads)\n+        grads = self._deduplicate_sparse_grad(grads)\n         grads_and_vars = list(zip(grads, trainable_variables))\n         self._internal_apply_gradients(grads_and_vars)\n \n\n@@ -391,8 +391,8 @@ class OptimizerRegressionTest(tf.test.TestCase, parameterized.TestCase):\n         x2 = tf.Variable(np.ones([10]), dtype=tf.float64)\n         grads = tf.convert_to_tensor(np.arange(0.1, 1.1, 0.1))\n         sparse_grads = tf.IndexedSlices(\n-            tf.convert_to_tensor([0, 0.2, 0.4, 0.8], dtype=tf.float64),\n-            tf.convert_to_tensor([0, 2, 4, 6]),\n+            tf.convert_to_tensor([0, 0.2, 0.4, 0.8, 0.8], dtype=tf.float64),\n+            tf.convert_to_tensor([0, 2, 4, 6, 6]),\n             dense_shape=tf.convert_to_tensor([len(grads)]),\n         )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a38da258d8ef276d1d92a3471147d653bed88090", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 89 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -81,9 +81,9 @@ def filter_empty_gradients(grads_and_vars):\n     if vars_with_empty_grads:\n         logging.warning(\n             (\n-                \"Gradients do not exist for variables %s when minimizing . \"\n-                \"the lossIf you're using `model.compile()`, did you forget \"\n-                \"to provide a `loss` argument?\"\n+                \"Gradients do not exist for variables %s when minimizing the \"\n+                \"loss. If you're using `model.compile()`, did you forget to \"\n+                \"provide a `loss` argument?\"\n             ),\n             ([v.name for v in vars_with_empty_grads]),\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#07ca4dae8b52f9897d28cc441ca0aaff624d00f5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 9264 | Contributors (this commit): 108 | Commits (past 90d): 25 | Contributors (cumulative): 108 | DMM Complexity: None\n\nDIFF:\n@@ -656,7 +656,6 @@ class Callback:\n        callbacks.on_train_end(final_logs)\n     ```\n \n-\n     Attributes:\n         params: Dict. Training parameters\n             (eg. verbosity, batch size, number of epochs...).\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f6e8e9b1b999d22de9830fabc5e6d15a1818f0c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 39 | Churn Cumulative: 2560 | Contributors (this commit): 8 | Commits (past 90d): 16 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -34,7 +34,7 @@ from keras.saving.saved_model import metric_serialization\n from keras.utils import generic_utils\n from keras.utils import losses_utils\n from keras.utils import metrics_utils\n-from keras.utils.tf_utils import is_tensor_or_variable\n+from keras.utils import tf_utils\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n@@ -371,7 +371,7 @@ class Metric(base_layer.Layer, metaclass=abc.ABCMeta):\n         else:\n             additional_kwargs = {}\n \n-        with tf.init_scope():\n+        with tf_utils.maybe_init_scope(layer=self):\n             return super().add_weight(\n                 name=name,\n                 shape=shape,\n@@ -701,15 +701,16 @@ class MeanMetricWrapper(Mean):\n         return super().update_state(matches, sample_weight=sample_weight)\n \n     def get_config(self):\n-        config = {}\n+        config = {\n+            k: backend.eval(v) if tf_utils.is_tensor_or_variable(v) else v\n+            for k, v in self._fn_kwargs.items()\n+        }\n \n         if type(self) is MeanMetricWrapper:\n             # Only include function argument when the object is a\n             # MeanMetricWrapper and not a subclass.\n             config[\"fn\"] = self._fn\n \n-        for k, v in self._fn_kwargs.items():\n-            config[k] = backend.eval(v) if is_tensor_or_variable(v) else v\n         base_config = super().get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -918,9 +919,10 @@ class SumOverBatchSizeMetricWrapper(SumOverBatchSize):\n         return super().update_state(matches, sample_weight=sample_weight)\n \n     def get_config(self):\n-        config = {}\n-        for k, v in self._fn_kwargs.items():\n-            config[k] = backend.eval(v) if is_tensor_or_variable(v) else v\n+        config = {\n+            k: backend.eval(v) if tf_utils.is_tensor_or_variable(v) else v\n+            for k, v in self._fn_kwargs.items()\n+        }\n         base_config = super().get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n\n@@ -149,6 +149,27 @@ class KerasSumTest(tf.test.TestCase, parameterized.TestCase):\n             self.evaluate(restore_update)\n             self.assertEqual(600.0, self.evaluate(restore_sum.result()))\n \n+    def test_init_scope_during_add_weight(self):\n+        seen_variables = 0\n+\n+        def capture_variable_creation(next_creator_fn, **kwargs) -> tf.Variable:\n+            nonlocal seen_variables\n+            seen_variables += 1\n+            return tf.constant(seen_variables)\n+\n+        @tf.function\n+        def create_variables():\n+            # When this method is called in a graph context, any usage of\n+            # `tf.init_scope` will bypass this variable creator scope, resulting\n+            # in different behavior.\n+            with tf.variable_creator_scope(capture_variable_creation):\n+                return metrics.Sum().variables\n+\n+        metric_variables = self.evaluate(create_variables())\n+        # The Sum metric contains a single `total` variable, which the creation\n+        # scope has changed to a `1` tensor.\n+        self.assertAllEqual([1], metric_variables)\n+\n \n class MeanTest(test_combinations.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0f7ce2bf73afd827506b1a9177badfe8eb0b7b08", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 27 | Churn Cumulative: 650 | Contributors (this commit): 6 | Commits (past 90d): 11 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -76,6 +76,13 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n         self.dropout = dropout\n         self.supports_masking = True\n \n+    def build(self, input_shape):\n+        # Skip RNG initialization if dropout rate is 0. This will let the layer\n+        # be purely stateless, with no reference to any variable.\n+        if self.dropout > 0:\n+            super().build(input_shape)\n+        self.built = True\n+\n     def _calculate_scores(self, query, key):\n         \"\"\"Calculates attention scores.\n \n@@ -128,8 +135,12 @@ class BaseDenseAttention(base_layer.BaseRandomLayer):\n             training = backend.learning_phase()\n         weights = tf.nn.softmax(scores)\n \n+        if self.dropout > 0:\n+\n             def dropped_weights():\n-            return self._random_generator.dropout(weights, rate=self.dropout)\n+                return self._random_generator.dropout(\n+                    weights, rate=self.dropout\n+                )\n \n             weights = control_flow_util.smart_cond(\n                 training, dropped_weights, lambda: tf.identity(weights)\n\n@@ -154,6 +154,20 @@ class BaseDenseAttentionTest(tf.test.TestCase, parameterized.TestCase):\n         expected_shape = [batch_size, tq, dim]\n         self.assertAllEqual(expected_shape, tf.shape(actual))\n \n+    def test_skip_rng_init_when_no_dropout(self):\n+        batch_size = 4\n+        tq = 5\n+        tv = 6\n+        dim = 7\n+        scores = np.ones((batch_size, tq, tv))\n+        value = np.ones((batch_size, tv, dim))\n+        layer = BaseDenseAttention()\n+        layer.build(None)  # The input shape is not used by this layer\n+        _, _ = layer._apply_scores(scores=scores, value=value, training=True)\n+        # Make sure the rng is not built and no tf.random.Generator created.\n+        self.assertFalse(layer._random_generator._built)\n+        self.assertIsNone(getattr(layer._random_generator, \"_generator\", None))\n+\n \n @test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n class LowerTriangularMaskTest(tf.test.TestCase, parameterized.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4f1308112f4188c4e14fdf3a59af8fe5f30db61f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 46 | Files Changed: 3 | Hunks: 28 | Methods Changed: 12 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 88 | Churn Cumulative: 10277 | Contributors (this commit): 33 | Commits (past 90d): 36 | Contributors (cumulative): 49 | DMM Complexity: 0.0\n\nDIFF:\n@@ -991,6 +991,11 @@ class SparseCategoricalCrossentropy(LossFunctionWrapper):\n         Args:\n           from_logits: Whether `y_pred` is expected to be a logits tensor. By\n             default, we assume that `y_pred` encodes a probability distribution.\n+          ignore_class: Optional integer. The ID of a class to be ignored during\n+            loss computation. This is useful, for example, in segmentation\n+            problems featuring a \"void\" class (commonly -1 or 255) in\n+            segmentation maps.\n+            By default (`ignore_class=None`), all classes are considered.\n           reduction: Type of `tf.keras.losses.Reduction` to apply to\n             loss. Default value is `AUTO`. `AUTO` indicates that the reduction\n             option will be determined by the usage context. For almost all cases\n\n@@ -2650,10 +2650,10 @@ class _IoUBase(base_metric.Metric):\n         metric computation. This is useful, for example, in segmentation\n         problems featuring a \"void\" class (commonly -1 or 255) in segmentation\n         maps. By default (`ignore_class=None`), all classes are considered.\n-      sparse_labels: Wether labels are encoded using natural numbers or\n+      sparse_y_true: Whether labels are encoded using natural numbers or\n         probability distribution vectors. If `False`, the `tf.argmax` function\n         will be used to determine each sample's most likely associated label.\n-      sparse_preds: Wether predictions are encoded using natural numbers or\n+      sparse_y_pred: Whether predictions are encoded using natural numbers or\n         probability distribution vectors. If `False`, the `tf.argmax` function\n         will be used to determine each sample's most likely associated label.\n       axis: (Optional) Defaults to -1. The dimension containing the logits.\n@@ -2666,15 +2666,15 @@ class _IoUBase(base_metric.Metric):\n         name: Optional[str] = None,\n         dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n         ignore_class: Optional[int] = None,\n-        sparse_labels: bool = True,\n-        sparse_preds: bool = True,\n+        sparse_y_true: bool = True,\n+        sparse_y_pred: bool = True,\n         axis: int = -1,\n     ):\n         super().__init__(name=name, dtype=dtype)\n         self.num_classes = num_classes\n         self.ignore_class = ignore_class\n-        self.sparse_labels = sparse_labels\n-        self.sparse_preds = sparse_preds\n+        self.sparse_y_true = sparse_y_true\n+        self.sparse_y_pred = sparse_y_pred\n         self.axis = axis\n \n         # Variable to accumulate the predictions in the confusion matrix.\n@@ -2698,9 +2698,9 @@ class _IoUBase(base_metric.Metric):\n           Update op.\n         \"\"\"\n \n-        if not self.sparse_labels:\n+        if not self.sparse_y_true:\n             y_true = tf.argmax(y_true, axis=self.axis)\n-        if not self.sparse_preds:\n+        if not self.sparse_y_pred:\n             y_pred = tf.argmax(y_pred, axis=self.axis)\n \n         y_true = tf.cast(y_true, self._dtype)\n@@ -2781,10 +2781,10 @@ class IoU(_IoUBase):\n         metric computation. This is useful, for example, in segmentation\n         problems featuring a \"void\" class (commonly -1 or 255) in segmentation\n         maps. By default (`ignore_class=None`), all classes are considered.\n-      sparse_labels: Wether labels are encoded using natural numbers or\n+      sparse_y_true: Whether labels are encoded using natural numbers or\n         probability distribution vectors. If `False`, the `tf.argmax` function\n         will be used to determine each sample's most likely associated label.\n-      sparse_preds: Wether predictions are encoded using natural numbers or\n+      sparse_y_pred: Whether predictions are encoded using natural numbers or\n         probability distribution vectors. If `False`, the `tf.argmax` function\n         will be used to determine each sample's most likely associated label.\n       axis: (Optional) Defaults to -1. The dimension containing the logits.\n@@ -2830,16 +2830,16 @@ class IoU(_IoUBase):\n         name: Optional[str] = None,\n         dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n         ignore_class: Optional[int] = None,\n-        sparse_labels: bool = True,\n-        sparse_preds: bool = True,\n+        sparse_y_true: bool = True,\n+        sparse_y_pred: bool = True,\n         axis: int = -1,\n     ):\n         super().__init__(\n             name=name,\n             num_classes=num_classes,\n             ignore_class=ignore_class,\n-            sparse_labels=sparse_labels,\n-            sparse_preds=sparse_preds,\n+            sparse_y_true=sparse_y_true,\n+            sparse_y_pred=sparse_y_pred,\n             axis=axis,\n             dtype=dtype,\n         )\n@@ -2887,8 +2887,8 @@ class IoU(_IoUBase):\n             \"num_classes\": self.num_classes,\n             \"target_class_ids\": self.target_class_ids,\n             \"ignore_class\": self.ignore_class,\n-            \"sparse_labels\": self.sparse_labels,\n-            \"sparse_preds\": self.sparse_preds,\n+            \"sparse_y_true\": self.sparse_y_true,\n+            \"sparse_y_pred\": self.sparse_y_pred,\n             \"axis\": self.axis,\n         }\n         base_config = super().get_config()\n@@ -3049,10 +3049,10 @@ class MeanIoU(IoU):\n         metric computation. This is useful, for example, in segmentation\n         problems featuring a \"void\" class (commonly -1 or 255) in segmentation\n         maps. By default (`ignore_class=None`), all classes are considered.\n-      sparse_labels: Wether labels are encoded using natural numbers or\n+      sparse_y_true: Whether labels are encoded using natural numbers or\n         probability distribution vectors. If `False`, the `tf.argmax` function\n         will be used to determine each sample's most likely associated label.\n-      sparse_preds: Wether predictions are encoded using natural numbers or\n+      sparse_y_pred: Whether predictions are encoded using natural numbers or\n         probability distribution vectors. If `False`, the `tf.argmax` function\n         will be used to determine each sample's most likely associated label.\n       axis: (Optional) Defaults to -1. The dimension containing the logits.\n@@ -3092,8 +3092,8 @@ class MeanIoU(IoU):\n         name: Optional[str] = None,\n         dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n         ignore_class: Optional[int] = None,\n-        sparse_labels: bool = True,\n-        sparse_preds: bool = True,\n+        sparse_y_true: bool = True,\n+        sparse_y_pred: bool = True,\n         axis: int = -1,\n     ):\n         target_class_ids = list(range(num_classes))\n@@ -3104,8 +3104,8 @@ class MeanIoU(IoU):\n             axis=axis,\n             dtype=dtype,\n             ignore_class=ignore_class,\n-            sparse_labels=sparse_labels,\n-            sparse_preds=sparse_preds,\n+            sparse_y_true=sparse_y_true,\n+            sparse_y_pred=sparse_y_pred,\n         )\n \n     def get_config(self):\n@@ -3114,8 +3114,8 @@ class MeanIoU(IoU):\n             \"name\": self.name,\n             \"dtype\": self._dtype,\n             \"ignore_class\": self.ignore_class,\n-            \"sparse_labels\": self.sparse_labels,\n-            \"sparse_preds\": self.sparse_preds,\n+            \"sparse_y_true\": self.sparse_y_true,\n+            \"sparse_y_pred\": self.sparse_y_pred,\n             \"axis\": self.axis,\n         }\n \n@@ -3168,7 +3168,7 @@ class OneHotIoU(IoU):\n         metric computation. This is useful, for example, in segmentation\n         problems featuring a \"void\" class (commonly -1 or 255) in segmentation\n         maps. By default (`ignore_class=None`), all classes are considered.\n-      sparse_preds: Wether predictions are encoded using natural numbers or\n+      sparse_y_pred: Whether predictions are encoded using natural numbers or\n         probability distribution vectors. If `False`, the `tf.argmax` function\n         will be used to determine each sample's most likely associated label.\n       axis: (Optional) Defaults to -1. The dimension containing the logits.\n@@ -3210,7 +3210,7 @@ class OneHotIoU(IoU):\n         name=None,\n         dtype=None,\n         ignore_class: Optional[int] = None,\n-        sparse_preds: bool = False,\n+        sparse_y_pred: bool = False,\n         axis: int = -1,\n     ):\n         super().__init__(\n@@ -3219,8 +3219,8 @@ class OneHotIoU(IoU):\n             name=name,\n             dtype=dtype,\n             ignore_class=ignore_class,\n-            sparse_labels=False,\n-            sparse_preds=sparse_preds,\n+            sparse_y_true=False,\n+            sparse_y_pred=sparse_y_pred,\n             axis=axis,\n         )\n \n@@ -3231,7 +3231,7 @@ class OneHotIoU(IoU):\n             \"name\": self.name,\n             \"dtype\": self._dtype,\n             \"ignore_class\": self.ignore_class,\n-            \"sparse_preds\": self.sparse_preds,\n+            \"sparse_y_pred\": self.sparse_y_pred,\n             \"axis\": self.axis,\n         }\n \n@@ -3282,7 +3282,7 @@ class OneHotMeanIoU(MeanIoU):\n         metric computation. This is useful, for example, in segmentation\n         problems featuring a \"void\" class (commonly -1 or 255) in segmentation\n         maps. By default (`ignore_class=None`), all classes are considered.\n-      sparse_preds: Wether predictions are encoded using natural numbers or\n+      sparse_y_pred: Whether predictions are encoded using natural numbers or\n         probability distribution vectors. If `False`, the `tf.argmax` function\n         will be used to determine each sample's most likely associated label.\n       axis: (Optional) Defaults to -1. The dimension containing the logits.\n@@ -3323,7 +3323,7 @@ class OneHotMeanIoU(MeanIoU):\n         name: str = None,\n         dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n         ignore_class: Optional[int] = None,\n-        sparse_preds: bool = False,\n+        sparse_y_pred: bool = False,\n         axis: int = -1,\n     ):\n         super().__init__(\n@@ -3332,8 +3332,8 @@ class OneHotMeanIoU(MeanIoU):\n             name=name,\n             dtype=dtype,\n             ignore_class=ignore_class,\n-            sparse_labels=False,\n-            sparse_preds=sparse_preds,\n+            sparse_y_true=False,\n+            sparse_y_pred=sparse_y_pred,\n         )\n \n     def get_config(self):\n@@ -3342,7 +3342,7 @@ class OneHotMeanIoU(MeanIoU):\n             \"name\": self.name,\n             \"dtype\": self._dtype,\n             \"ignore_class\": self.ignore_class,\n-            \"sparse_preds\": self.sparse_preds,\n+            \"sparse_y_pred\": self.sparse_y_pred,\n             \"axis\": self.axis,\n         }\n \n\n@@ -413,14 +413,13 @@ def apply_mask(y_p, sw, mask):\n \n \n def apply_valid_mask(losses, sw, mask, reduction):\n-    \"\"\"Redistribute pair-wise weights considering only valid entries.\"\"\"\n+    \"\"\"Redistribute sample weights considering only valid entries.\"\"\"\n     if mask is not None:\n         mask = tf.cast(mask, losses.dtype)\n \n         if reduction in (ReductionV2.AUTO, ReductionV2.SUM_OVER_BATCH_SIZE):\n-            # Valid entries have weight `# total / # valid`,\n-            # while invalid ones assume weight 0. When summed\n-            # over batch size, they will be reduced to:\n+            # Valid entries have weight `total/valid`, while invalid ones\n+            # have 0. When summed over batch, they will be reduced to:\n             #\n             # mean(loss * sample_weight * total / valid)\n             #   = sum(loss * sample_weight * total / valid) / total\n@@ -430,13 +429,5 @@ def apply_valid_mask(losses, sw, mask, reduction):\n             total = tf.cast(tf.size(mask), losses.dtype)\n             valid = tf.reduce_sum(mask)\n             mask *= total / valid\n-        elif reduction in (ReductionV2.NONE, ReductionV2.SUM):\n-            # Nothing to do. Nothing is being averaged.\n-            ...\n-        elif reduction == \"weighted_mean\":\n-            # Nothing to do. A binary mask is enough because\n-            # it will also be used in the mean operation's\n-            # denominator as `tf.reduce_sum(sample_weight)`.\n-            ...\n \n     return apply_mask(losses, sw, mask)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5a8218aaba161cee242dd8e16302ed15acc1e80e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 143 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 7 | Complexity Δ (Sum/Max): 14/5 | Churn Δ: 143 | Churn Cumulative: 30667 | Contributors (this commit): 137 | Commits (past 90d): 43 | Contributors (cumulative): 163 | DMM Complexity: 1.0\n\nDIFF:\n@@ -780,6 +780,24 @@ class Functional(training_lib.Model):\n         config = super().get_config()\n         return copy.deepcopy(get_network_config(self, config=config))\n \n+    def get_weight_paths(self):\n+        result = {}\n+        for layer in self.layers:\n+            (\n+                descendants,\n+                object_paths_dict,\n+            ) = tf.__internal__.tracking.ObjectGraphView(\n+                layer\n+            ).breadth_first_traversal()\n+            for descendant in descendants:\n+                if isinstance(descendant, tf.Variable):\n+                    trackable_references = object_paths_dict[descendant]\n+                    object_path = \".\".join(\n+                        [t.name for t in trackable_references]\n+                    )\n+                    result[layer.name + \".\" + object_path] = descendant\n+        return result\n+\n     def _validate_graph_inputs_and_outputs(self):\n         \"\"\"Validates the inputs and outputs of a Graph Network.\"\"\"\n         # Check for redundancy in inputs.\n\n@@ -3284,6 +3284,79 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             \"Provide either a layer name or layer index at \" \"`get_layer`.\"\n         )\n \n+    def get_weight_paths(self):\n+        \"\"\"Retrieve all the variables and their paths for the model.\n+\n+        The variable path (string) is a stable key to indentify a `tf.Variable`\n+        instance owned by the model. It can be used to specify variable-specific\n+        configurations (e.g. DTensor, quantization) from a global view.\n+\n+        This method returns a dict with weight object paths as keys\n+        and the corresponding `tf.Variable` instances as values.\n+\n+        Note that if the model is a subclassed model and the weights haven't\n+        been initialized, an empty dict will be returned.\n+\n+        Returns:\n+            A dict where keys are variable paths and values are `tf.Variable`\n+             instances.\n+\n+        Example:\n+\n+        ```python\n+        class SubclassModel(tf.keras.Model):\n+\n+          def __init__(self, name=None):\n+            super().__init__(name=name)\n+            self.d1 = tf.keras.layers.Dense(10)\n+            self.d2 = tf.keras.layers.Dense(20)\n+\n+          def call(self, inputs):\n+            x = self.d1(inputs)\n+            return self.d2(x)\n+\n+        model = SubclassModel()\n+        model(tf.zeros((10, 10)))\n+        weight_paths = model.get_weight_paths()\n+        # weight_paths:\n+        # {\n+        #    'd1.kernel': model.d1.kernel,\n+        #    'd1.bias': model.d1.bias,\n+        #    'd2.kernel': model.d2.kernel,\n+        #    'd2.bias': model.d2.bias,\n+        # }\n+\n+        # Functional model\n+        inputs = tf.keras.Input((10,), batch_size=10)\n+        x = tf.keras.layers.Dense(20, name='d1')(inputs)\n+        output = tf.keras.layers.Dense(30, name='d2')(x)\n+        model = tf.keras.Model(inputs, output)\n+        d1 = model.layers[1]\n+        d2 = model.layers[2]\n+        weight_paths = model.get_weight_paths()\n+        # weight_paths:\n+        # {\n+        #    'd1.kernel': d1.kernel,\n+        #    'd1.bias': d1.bias,\n+        #    'd2.kernel': d2.kernel,\n+        #    'd2.bias': d2.bias,\n+        # }\n+        ```\n+        \"\"\"\n+        result = {}\n+        (\n+            descendants,\n+            object_paths_dict,\n+        ) = tf.__internal__.tracking.ObjectGraphView(\n+            self\n+        ).breadth_first_traversal()\n+        for descendant in descendants:\n+            if isinstance(descendant, tf.Variable):\n+                trackable_references = object_paths_dict[descendant]\n+                object_path = \".\".join([t.name for t in trackable_references])\n+                result[object_path] = descendant\n+        return result\n+\n     @tf.__internal__.tracking.no_automatic_dependency_tracking\n     def _set_save_spec(self, inputs, args=None, kwargs=None):\n         \"\"\"Defines the save spec so that serialization is able to trace model call.\n\n@@ -4620,6 +4620,58 @@ class ScalarDataModelTest(test_combinations.TestCase):\n         self.assertAllClose(preds, y, atol=2e-1)\n \n \n+# Class used for testing.\n+class SubclassModel(training_module.Model):\n+    def __init__(self, name=None):\n+        super().__init__(name=name)\n+        self.d1 = layers_module.Dense(1000)\n+        self.d2 = layers_module.Dense(1000)\n+        self.dropout = layers_module.Dropout(0.1)\n+\n+    def call(self, inputs, training=None):\n+        x = self.d1(inputs)\n+        x = self.dropout(x, training=training)\n+        return self.d2(x)\n+\n+\n+class TestVariableObjectPathMapping(test_combinations.TestCase):\n+    def test_subclass_model_get_weight_paths(self):\n+        model = SubclassModel()\n+        # Make sure the object path produce nothing when weights are not\n+        # initialized\n+        self.assertEmpty(model.get_weight_paths())\n+\n+        model(tf.zeros((10, 10)))\n+        mapping = model.get_weight_paths()\n+        self.assertEqual(\n+            mapping.keys(), {\"d1.kernel\", \"d1.bias\", \"d2.kernel\", \"d2.bias\"}\n+        )\n+\n+    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+    def test_functional_model_get_weight_paths(self):\n+        inputs = input_layer.Input(shape=(10,))\n+        x = layers_module.Dense(100, name=\"d1\")(inputs)\n+        output = layers_module.Dense(200, name=\"d2\", activation=\"softmax\")(x)\n+        model = training_module.Model(inputs, output)\n+        mapping = model.get_weight_paths()\n+        self.assertEqual(\n+            mapping.keys(), {\"d1.kernel\", \"d1.bias\", \"d2.kernel\", \"d2.bias\"}\n+        )\n+\n+    @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+    def test_sequential_model_get_weight_paths(self):\n+        model = sequential.Sequential(\n+            [\n+                layers_module.Dense(100, name=\"d1\", input_shape=(10,)),\n+                layers_module.Dense(200, name=\"d2\", activation=\"softmax\"),\n+            ]\n+        )\n+        mapping = model.get_weight_paths()\n+        self.assertEqual(\n+            mapping.keys(), {\"d1.kernel\", \"d1.bias\", \"d2.kernel\", \"d2.bias\"}\n+        )\n+\n+\n def _is_oss():\n     \"\"\"Returns whether the test is run under OSS.\"\"\"\n     return len(sys.argv) >= 1 and \"bazel\" in sys.argv[0]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6db75928d738420b51058313c1d5db5cd0e67733", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 7461 | Contributors (this commit): 13 | Commits (past 90d): 31 | Contributors (cumulative): 27 | DMM Complexity: None\n\nDIFF:\n@@ -167,7 +167,7 @@ class Discretization(base_preprocessing_layer.PreprocessingLayer):\n       output_mode: Specification for the output of the layer. Defaults to\n         `\"int\"`.  Values can be `\"int\"`, `\"one_hot\"`, `\"multi_hot\"`, or\n         `\"count\"` configuring the layer as follows:\n-          - `\"int\"`: Return the discritized bin indices directly.\n+          - `\"int\"`: Return the discretized bin indices directly.\n           - `\"one_hot\"`: Encodes each individual element in the input into an\n             array the same size as `num_bins`, containing a 1 at the input's bin\n             index. If the last dimension is size 1, will encode on that\n\n@@ -238,7 +238,7 @@ class CenterCrop(base_layer.Layer):\n \n @keras_export(\"keras.__internal__.layers.BaseImageAugmentationLayer\")\n class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n-    \"\"\"Abstract base layer for image augmentaion.\n+    \"\"\"Abstract base layer for image augmentation.\n \n     This layer contains base functionalities for preprocessing layers which\n     augment image related data, eg. image and in future, label and bounding\n\n@@ -24,7 +24,7 @@ from keras.testing_infra import test_utils\n \n \n class TestPickleProtocol(test_combinations.TestCase):\n-    \"\"\"Tests pickle protoocol support.\"\"\"\n+    \"\"\"Tests pickle protocol support.\"\"\"\n \n     @test_combinations.run_with_all_model_types\n     @test_combinations.parameterized.named_parameters(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#611ce096903a8b2a9bbd40c65fad5cd667a472fc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 246 | Contributors (this commit): 10 | Commits (past 90d): 6 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -29,6 +29,12 @@ def load_data(path=\"boston_housing.npz\", test_split=0.2, seed=113):\n     This is a dataset taken from the StatLib library which is maintained at\n     Carnegie Mellon University.\n \n+    **WARNING:** This dataset has an ethical problem: the authors of this\n+    dataset included a variable, \"B\", that may appear to assume that racial\n+    self-segregation influences house prices. As such, we strongly discourage\n+    the use of this dataset, unless in the context of illustrating ethical\n+    issues in data science and machine learning.\n+\n     Samples contain 13 attributes of houses at different locations around the\n     Boston suburbs in the late 1970s. Targets are the median values of\n     the houses at a location (in k$).\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#701c932d73603bf25ab193db109e04cd52f64024", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 4021 | Contributors (this commit): 7 | Commits (past 90d): 22 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -481,7 +481,7 @@ class RNN(base_layer.Layer):\n             self._validate_state_spec(state_size, self.state_spec)\n         if self.stateful:\n             self.reset_states()\n-        self.built = True\n+        super().build(input_shape)\n \n     @staticmethod\n     def _validate_state_spec(cell_state_sizes, init_state_specs):\n\n@@ -828,6 +828,13 @@ class GRULayerTest(test_combinations.TestCase):\n             \"rnn/gru_cell/StateVar:0\",\n         )\n \n+        layer = keras.layers.GRU(2, dropout=0.1, force_generator=True)\n+        layer(np.random.random((2, 3, 4)))\n+        self.assertEqual(\n+            layer._random_generator._generator._state_var.name,\n+            \"gru/StateVar:0\",\n+        )\n+\n     @parameterized.parameters([0, 1, 2])\n     def test_implementation_mode_gru(self, implementation_mode):\n         num_samples = 2\n\n@@ -1020,6 +1020,13 @@ class LSTMLayerTest(test_combinations.TestCase):\n             \"rnn/lstm_cell/StateVar:0\",\n         )\n \n+        layer = keras.layers.LSTM(2, dropout=0.1, force_generator=True)\n+        layer(np.random.random((2, 3, 4)))\n+        self.assertEqual(\n+            layer._random_generator._generator._state_var.name,\n+            \"lstm/StateVar:0\",\n+        )\n+\n     @parameterized.parameters([0, 1, 2])\n     def test_implementation_mode_LSTM(self, implementation_mode):\n         num_samples = 2\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6b214f3eb15018df714363ec34d1de2861a020e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 20 | Files Changed: 6 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): -18/0 | Churn Δ: 20 | Churn Cumulative: 649 | Contributors (this commit): 4 | Commits (past 90d): 37 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -82,8 +82,6 @@ class GlobalAveragePooling1D(GlobalPooling1D):\n \n     def call(self, inputs, mask=None):\n         steps_axis = 1 if self.data_format == \"channels_last\" else 2\n-        if inputs.shape[steps_axis] == 0 :\n-            raise ValueError(\"Reducing axis cannot be of 0 dimension\")\n         if mask is not None:\n             mask = tf.cast(mask, inputs[0].dtype)\n             mask = tf.expand_dims(\n\n@@ -72,12 +72,8 @@ class GlobalAveragePooling2D(GlobalPooling2D):\n \n     def call(self, inputs):\n         if self.data_format == \"channels_last\":\n-            if [inputs.shape[i] == 0 for i in [1, 2]]:\n-                raise ValueError(\"Reducing axis cannot be of 0 dimension\")\n             return backend.mean(inputs, axis=[1, 2], keepdims=self.keepdims)\n         else:\n-            if [inputs.shape[i] == 0 for i in [2, 3]]:\n-                raise ValueError(\"Reducing axis cannot be of 0 dimension\")\n             return backend.mean(inputs, axis=[2, 3], keepdims=self.keepdims)\n \n \n\n@@ -66,12 +66,8 @@ class GlobalAveragePooling3D(GlobalPooling3D):\n \n     def call(self, inputs):\n         if self.data_format == \"channels_last\":\n-            if [inputs.shape[i] == 0 for i in [1, 2, 3]]:\n-                raise ValueError(\"Reducing axis cannot be of 0 dimension\")\n             return backend.mean(inputs, axis=[1, 2, 3], keepdims=self.keepdims)\n         else:\n-            if [inputs.shape[i] == 0 for i in [2, 3, 4]]:\n-                raise ValueError(\"Reducing axis cannot be of 0 dimension\")\n             return backend.mean(inputs, axis=[2, 3, 4], keepdims=self.keepdims)\n \n \n\n@@ -80,8 +80,6 @@ class GlobalMaxPooling1D(GlobalPooling1D):\n \n     def call(self, inputs):\n         steps_axis = 1 if self.data_format == \"channels_last\" else 2\n-        if inputs.shape[steps_axis] == 0 :\n-            raise ValueError(\"Reducing axis cannot be of 0 dimension\")\n         return backend.max(inputs, axis=steps_axis, keepdims=self.keepdims)\n \n \n\n@@ -70,12 +70,8 @@ class GlobalMaxPooling2D(GlobalPooling2D):\n \n     def call(self, inputs):\n         if self.data_format == \"channels_last\":\n-            if [inputs.shape[i] == 0 for i in [1, 2]]:\n-                raise ValueError(\"Reducing axis cannot be of 0 dimension\")\n             return backend.max(inputs, axis=[1, 2], keepdims=self.keepdims)\n         else:\n-            if [inputs.shape[i] == 0 for i in [2, 3]]:\n-                raise ValueError(\"Reducing axis cannot be of 0 dimension\")\n             return backend.max(inputs, axis=[2, 3], keepdims=self.keepdims)\n \n \n\n@@ -64,12 +64,8 @@ class GlobalMaxPooling3D(GlobalPooling3D):\n \n     def call(self, inputs):\n         if self.data_format == \"channels_last\":\n-            if [inputs.shape[i] == 0 for i in [1, 2, 3]]:\n-                raise ValueError(\"Reducing axis cannot be of 0 dimension\")\n             return backend.max(inputs, axis=[1, 2, 3], keepdims=self.keepdims)\n         else:\n-            if [inputs.shape[i] == 0 for i in [2, 3, 4]]:\n-                raise ValueError(\"Reducing axis cannot be of 0 dimension\")\n             return backend.max(inputs, axis=[2, 3, 4], keepdims=self.keepdims)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9a4a31a63329b593100a92890079809647a8e518", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 8 | Churn Cumulative: 1119 | Contributors (this commit): 7 | Commits (past 90d): 21 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -490,6 +490,10 @@ class _BaseOptimizer(tf.Module):\n                 self._learning_rate(self.iterations)\n             )\n         grads_and_vars = optimizer_utils.filter_empty_gradients(grads_and_vars)\n+        if len(list(grads_and_vars)) == 0:\n+            # It is possible that the grad is empty. In this case,\n+            # `apply_gradients` is a no-op.\n+            return\n         grads, trainable_variables = zip(*grads_and_vars)\n         scope_name = self._name or \"optimizer\"\n         with tf.name_scope(scope_name):\n\n@@ -231,6 +231,10 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n         with self.assertRaisesRegex(RuntimeError, \"Cannot set*\"):\n             optimizer.iterations = 2\n \n+    def testNoGradients(self):\n+        optimizer = adam_new.Adam(jit_compile=False)\n+        optimizer.apply_gradients(zip([], []))\n+\n     def testPassingMissingWDError(self):\n         with self.assertRaises(ValueError):\n             _ = adamw_new.AdamW(0.01, weight_decay=None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2bcd14cedea12c1c926a09e890faf01c8538820e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 7 | Methods Changed: 9 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 29 | Churn Cumulative: 19731 | Contributors (this commit): 132 | Commits (past 90d): 31 | Contributors (cumulative): 132 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -300,6 +300,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             self._distribution_strategy = tf.distribute.get_strategy()\n         else:\n             self._distribution_strategy = None\n+        self._distribute_reduction_method = None\n \n         self._cluster_coordinator = None\n \n@@ -928,6 +929,19 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     def run_eagerly(self, value):\n         self._run_eagerly = value\n \n+    @property\n+    def distribute_reduction_method(self):\n+        \"\"\"Settable attribute indicating how the model should reduce\n+        loss and metric values from replicas.\n+\n+        Default: 'first', which will get the value from the first replica.\n+        \"\"\"\n+        return self._distribute_reduction_method or \"first\"\n+\n+    @distribute_reduction_method.setter\n+    def distribute_reduction_method(self, value):\n+        self._distribute_reduction_method = value\n+\n     def _validate_target_and_loss(self, y, loss):\n         \"\"\"Raises error if target or loss is not found.\n \n@@ -1145,7 +1159,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             data = next(iterator)\n             outputs = model.distribute_strategy.run(run_step, args=(data,))\n             outputs = reduce_per_replica(\n-                outputs, self.distribute_strategy, reduction=\"first\"\n+                outputs,\n+                self.distribute_strategy,\n+                reduction=self.distribute_reduction_method,\n             )\n             return outputs\n \n@@ -1712,7 +1728,9 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             data = next(iterator)\n             outputs = model.distribute_strategy.run(run_step, args=(data,))\n             outputs = reduce_per_replica(\n-                outputs, self.distribute_strategy, reduction=\"first\"\n+                outputs,\n+                self.distribute_strategy,\n+                reduction=self.distribute_reduction_method,\n             )\n             return outputs\n \n@@ -3773,7 +3791,7 @@ def reduce_per_replica(values, strategy, reduction=\"first\"):\n       values: Structure of `PerReplica` objects or `tf.Tensor`s. `tf.Tensor`s\n         are returned as-is.\n       strategy: `tf.distribute.Strategy` object.\n-      reduction: One of `\"first\"`, `\"concat\"`.\n+      reduction: One of `\"first\"`, `\"concat\"`, or `\"sum\"`.\n \n     Returns:\n       Structure of `Tensor`s, representing the result of reduction.\n@@ -3797,9 +3815,12 @@ def reduce_per_replica(values, strategy, reduction=\"first\"):\n                 return _tpu_multi_host_concat(v, strategy)\n             else:\n                 return concat(strategy.experimental_local_results(v))\n+        elif reduction == \"sum\":\n+            values = strategy.experimental_local_results(v)\n+            return tf.reduce_sum(values)\n         else:\n             raise ValueError(\n-                '`reduction` must be \"first\" or \"concat\". Received: '\n+                '`reduction` must be \"first\" or \"concat\" or \"sum\". Received: '\n                 f\"reduction={reduction}.\"\n             )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
