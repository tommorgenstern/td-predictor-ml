{"custom_id": "keras#31af45efac4b4f5a049d12c83459dcb975bfbc26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 2657 | Contributors (this commit): 16 | Commits (past 90d): 11 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -540,10 +540,7 @@ def index_directory(\n     else:\n         subdirs = []\n         for subdir in sorted(tf.io.gfile.listdir(directory)):\n-            if (\n-                tf.io.gfile.isdir(tf.io.gfile.join(directory, subdir))\n-                and not subdir.startswith(\".\")\n-            ):\n+            if tf.io.gfile.isdir(tf.io.gfile.join(directory, subdir)) and not subdir.startswith(\".\"):\n                 if subdir.endswith(\"/\"):\n                     subdir = subdir[:-1]\n                 subdirs.append(subdir)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ab1ab00beb9121b1dc801ed04de4c848fce7f903", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 32 | Files Changed: 12 | Hunks: 15 | Methods Changed: 16 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 65 | Churn Cumulative: 3131 | Contributors (this commit): 8 | Commits (past 90d): 85 | Contributors (cumulative): 30 | DMM Complexity: 0.0\n\nDIFF:\n@@ -10,7 +10,11 @@ from keras_core.utils.naming import auto_name\n DYNAMIC_SHAPES_OK = True\n \n \n-class Variable(KerasVariable, tf.__internal__.types.Tensor):\n+class Variable(\n+    KerasVariable,\n+    tf.__internal__.types.Tensor,\n+    tf.__internal__.tracking.AutoTrackable,\n+):\n     _should_act_as_resource_variable = True\n \n     @property\n@@ -39,6 +43,10 @@ class Variable(KerasVariable, tf.__internal__.types.Tensor):\n     def __tf_tensor__(self, dtype=None, name=None):\n         return tf.convert_to_tensor(self.value, dtype=dtype, name=name)\n \n+    # For SavedModel\n+    def _write_object_proto(self, *args, **kwargs):\n+        return self.value._write_object_proto(*args, **kwargs)\n+\n \n def convert_to_tensor(x, dtype=None):\n     if dtype is not None:\n\n@@ -1,2 +1,5 @@\n-class TFLayer:\n+import tensorflow as tf\n+\n+\n+class TFLayer(tf.__internal__.tracking.AutoTrackable):\n     pass\n\n@@ -41,8 +41,8 @@ class Cropping1D(Layer):\n         3D tensor with shape `(batch_size, cropped_axis, features)`\n     \"\"\"\n \n-    def __init__(self, cropping=(1, 1), name=None, dtype=None):\n-        super().__init__(name=name, dtype=dtype)\n+    def __init__(self, cropping=(1, 1), **kwargs):\n+        super().__init__(**kwargs)\n         self.cropping = argument_validation.standardize_tuple(\n             cropping, 2, \"cropping\", allow_zero=True\n         )\n\n@@ -53,10 +53,8 @@ class Cropping2D(Layer):\n           `(batch_size, channels, cropped_height, cropped_width)`\n     \"\"\"\n \n-    def __init__(\n-        self, cropping=((0, 0), (0, 0)), data_format=None, name=None, dtype=None\n-    ):\n-        super().__init__(name=name, dtype=dtype)\n+    def __init__(self, cropping=((0, 0), (0, 0)), data_format=None, **kwargs):\n+        super().__init__(**kwargs)\n         self.data_format = backend.standardize_data_format(data_format)\n         if isinstance(cropping, int):\n             self.cropping = ((cropping, cropping), (cropping, cropping))\n\n@@ -57,13 +57,9 @@ class Cropping3D(Layer):\n     \"\"\"\n \n     def __init__(\n-        self,\n-        cropping=((1, 1), (1, 1), (1, 1)),\n-        data_format=None,\n-        name=None,\n-        dtype=None,\n+        self, cropping=((1, 1), (1, 1), (1, 1)), data_format=None, **kwargs\n     ):\n-        super().__init__(name=name, dtype=dtype)\n+        super().__init__(**kwargs)\n         self.data_format = backend.standardize_data_format(data_format)\n         if isinstance(cropping, int):\n             self.cropping = (\n\n@@ -32,8 +32,8 @@ class Flatten(Layer):\n     (None, 640)\n     \"\"\"\n \n-    def __init__(self, data_format=None, name=None, dtype=None):\n-        super().__init__(name=name, dtype=dtype)\n+    def __init__(self, data_format=None, **kwargs):\n+        super().__init__(**kwargs)\n         self.data_format = backend.standardize_data_format(data_format)\n         self.input_spec = InputSpec(min_ndim=1)\n         self._channels_first = self.data_format == \"channels_first\"\n\n@@ -31,8 +31,8 @@ class Permute(Layer):\n     (None, 64, 10)\n     \"\"\"\n \n-    def __init__(self, dims, name=None, dtype=None):\n-        super().__init__(name=name, dtype=dtype)\n+    def __init__(self, dims, **kwargs):\n+        super().__init__(**kwargs)\n         self.dims = tuple(dims)\n         if sorted(dims) != list(range(1, len(dims) + 1)):\n             raise ValueError(\n\n@@ -25,8 +25,8 @@ class RepeatVector(Layer):\n         3D tensor with shape `(batch_size, n, features)`.\n     \"\"\"\n \n-    def __init__(self, n, name=None, dtype=None):\n-        super().__init__(name=name, dtype=dtype)\n+    def __init__(self, n, **kwargs):\n+        super().__init__(**kwargs)\n         self.n = n\n         if not isinstance(n, int):\n             raise TypeError(\n\n@@ -34,8 +34,8 @@ class Reshape(Layer):\n     (None, 3, 2, 2)\n     \"\"\"\n \n-    def __init__(self, target_shape, name=None, dtype=None):\n-        super().__init__(name=name, dtype=dtype)\n+    def __init__(self, target_shape, **kwargs):\n+        super().__init__(**kwargs)\n         self.target_shape = tuple(target_shape)\n \n     def compute_output_shape(self, input_shape):\n\n@@ -47,8 +47,8 @@ class ZeroPadding1D(Layer):\n         3D tensor with shape `(batch_size, padded_axis, features)`\n     \"\"\"\n \n-    def __init__(self, padding=1, name=None, dtype=None):\n-        super().__init__(name=name, dtype=dtype)\n+    def __init__(self, padding=1, **kwargs):\n+        super().__init__(**kwargs)\n         self.padding = argument_validation.standardize_tuple(\n             padding, 2, \"padding\", allow_zero=True\n         )\n\n@@ -68,8 +68,8 @@ class ZeroPadding2D(Layer):\n           `(batch_size, channels, padded_height, padded_width)`\n     \"\"\"\n \n-    def __init__(self, padding=(1, 1), data_format=None, name=None, dtype=None):\n-        super().__init__(name=name, dtype=dtype)\n+    def __init__(self, padding=(1, 1), data_format=None, **kwargs):\n+        super().__init__(**kwargs)\n         self.data_format = backend.standardize_data_format(data_format)\n         if isinstance(padding, int):\n             self.padding = ((padding, padding), (padding, padding))\n\n@@ -58,13 +58,9 @@ class ZeroPadding3D(Layer):\n     \"\"\"\n \n     def __init__(\n-        self,\n-        padding=((1, 1), (1, 1), (1, 1)),\n-        data_format=None,\n-        name=None,\n-        dtype=None,\n+        self, padding=((1, 1), (1, 1), (1, 1)), data_format=None, **kwargs\n     ):\n-        super().__init__(name=name, dtype=dtype)\n+        super().__init__(**kwargs)\n         self.data_format = backend.standardize_data_format(data_format)\n         if isinstance(padding, int):\n             self.padding = (\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0b5d8f126ddde55a69e335307e4357580877d50f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 921 | Lines Deleted: 0 | Files Changed: 4 | Hunks: 4 | Methods Changed: 18 | Complexity Δ (Sum/Max): 23/9 | Churn Δ: 921 | Churn Cumulative: 921 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,114 @@\n+\"\"\"\n+Title: Simple custom layer example: Antirectifier\n+Author: [fchollet](https://twitter.com/fchollet)\n+Date created: 2016/01/06\n+Last modified: 2020/04/20\n+Description: Demonstration of custom layer creation.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+This example shows how to create custom layers, using the Antirectifier layer\n+ (originally proposed as a Keras example script in January 2016), an alternative\n+to ReLU. Instead of zeroing-out the negative part of the input, it splits the negative\n+ and positive parts and returns the concatenation of the absolute value\n+of both. This avoids loss of information, at the cost of an increase in dimensionality.\n+ To fix the dimensionality increase, we linearly combine the\n+features back to a space of the original size.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import tensorflow as tf\n+import keras_core as keras\n+from keras_core import layers\n+\n+\"\"\"\n+## The Antirectifier layer\n+\"\"\"\n+\n+\n+class Antirectifier(layers.Layer):\n+    def __init__(self, initializer=\"he_normal\", **kwargs):\n+        super().__init__(**kwargs)\n+        self.initializer = keras.initializers.get(initializer)\n+\n+    def build(self, input_shape):\n+        output_dim = input_shape[-1]\n+        self.kernel = self.add_weight(\n+            shape=(output_dim * 2, output_dim),\n+            initializer=self.initializer,\n+            name=\"kernel\",\n+            trainable=True,\n+        )\n+\n+    def call(self, inputs):\n+        inputs -= tf.reduce_mean(inputs, axis=-1, keepdims=True)\n+        pos = tf.nn.relu(inputs)\n+        neg = tf.nn.relu(-inputs)\n+        concatenated = tf.concat([pos, neg], axis=-1)\n+        mixed = tf.matmul(concatenated, self.kernel)\n+        return mixed\n+\n+    def get_config(self):\n+        # Implement get_config to enable serialization. This is optional.\n+        base_config = super().get_config()\n+        config = {\"initializer\": keras.initializers.serialize(self.initializer)}\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+\"\"\"\n+## Let's test-drive it on MNIST\n+\"\"\"\n+\n+# Training parameters\n+batch_size = 128\n+num_classes = 10\n+epochs = 20\n+\n+# The data, split between train and test sets\n+(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n+\n+x_train = x_train.reshape(-1, 784)\n+x_test = x_test.reshape(-1, 784)\n+x_train = x_train.astype(\"float32\")\n+x_test = x_test.astype(\"float32\")\n+x_train /= 255\n+x_test /= 255\n+print(x_train.shape[0], \"train samples\")\n+print(x_test.shape[0], \"test samples\")\n+\n+# Build the model\n+model = keras.Sequential(\n+    [\n+        keras.Input(shape=(784,)),\n+        layers.Dense(256),\n+        Antirectifier(),\n+        layers.Dense(256),\n+        Antirectifier(),\n+        layers.Dropout(0.5),\n+        layers.Dense(10),\n+    ]\n+)\n+\n+# Compile the model\n+model.compile(\n+    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n+    optimizer=keras.optimizers.RMSprop(),\n+    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n+)\n+\n+# Train the model\n+model.fit(\n+    x_train,\n+    y_train,\n+    batch_size=batch_size,\n+    epochs=epochs,\n+    validation_split=0.15,\n+)\n+\n+# Test the model\n+model.evaluate(x_test, y_test)\n\n@@ -0,0 +1,113 @@\n+\"\"\"\n+Title: Endpoint layer pattern\n+Author: [fchollet](https://twitter.com/fchollet)\n+Date created: 2019/05/10\n+Last modified: 2019/05/10\n+Description: Demonstration of the \"endpoint layer\" pattern (layer that handles loss management).\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import tensorflow as tf\n+import keras_core as keras\n+import numpy as np\n+\n+\"\"\"\n+## Usage of endpoint layers in the Functional API\n+\n+An \"endpoint layer\" has access to the model's targets, and creates arbitrary losses\n+in `call()` using `self.add_loss()` and `Metric.update_state()`.\n+This enables you to define losses and\n+metrics that don't match the usual signature `fn(y_true, y_pred, sample_weight=None)`.\n+\n+Note that you could have separate metrics for training and eval with this pattern.\n+\"\"\"\n+\n+\n+class LogisticEndpoint(keras.layers.Layer):\n+    def __init__(self, name=None):\n+        super().__init__(name=name)\n+        self.loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n+        self.accuracy_metric = keras.metrics.BinaryAccuracy(name=\"accuracy\")\n+\n+    def call(self, logits, targets=None, sample_weight=None):\n+        if targets is not None:\n+            # Compute the training-time loss value and add it\n+            # to the layer using `self.add_loss()`.\n+            loss = self.loss_fn(targets, logits, sample_weight)\n+            self.add_loss(loss)\n+\n+            # Log the accuracy as a metric (we could log arbitrary metrics,\n+            # including different metrics for training and inference.)\n+            self.accuracy_metric.update_state(targets, logits, sample_weight)\n+\n+        # Return the inference-time prediction tensor (for `.predict()`).\n+        return tf.nn.softmax(logits)\n+\n+\n+inputs = keras.Input((764,), name=\"inputs\")\n+logits = keras.layers.Dense(1)(inputs)\n+targets = keras.Input((1,), name=\"targets\")\n+sample_weight = keras.Input((1,), name=\"sample_weight\")\n+preds = LogisticEndpoint()(logits, targets, sample_weight)\n+model = keras.Model([inputs, targets, sample_weight], preds)\n+\n+data = {\n+    \"inputs\": np.random.random((1000, 764)),\n+    \"targets\": np.random.random((1000, 1)),\n+    \"sample_weight\": np.random.random((1000, 1)),\n+}\n+\n+model.compile(keras.optimizers.Adam(1e-3))\n+model.fit(data, epochs=2)\n+\n+\"\"\"\n+## Exporting an inference-only model\n+\n+Simply don't include `targets` in the model. The weights stay the same.\n+\"\"\"\n+\n+inputs = keras.Input((764,), name=\"inputs\")\n+logits = keras.layers.Dense(1)(inputs)\n+preds = LogisticEndpoint()(logits, targets=None, sample_weight=None)\n+inference_model = keras.Model(inputs, preds)\n+\n+inference_model.set_weights(model.get_weights())\n+\n+preds = inference_model.predict(np.random.random((1000, 764)))\n+\n+\"\"\"\n+## Usage of loss endpoint layers in subclassed models\n+\"\"\"\n+\n+\n+class LogReg(keras.Model):\n+    def __init__(self):\n+        super().__init__()\n+        self.dense = keras.layers.Dense(1)\n+        self.logistic_endpoint = LogisticEndpoint()\n+\n+    def call(self, inputs):\n+        # Note that all inputs should be in the first argument\n+        # since we want to be able to call `model.fit(inputs)`.\n+        logits = self.dense(inputs[\"inputs\"])\n+        preds = self.logistic_endpoint(\n+            logits=logits,\n+            targets=inputs[\"targets\"],\n+            sample_weight=inputs[\"sample_weight\"],\n+        )\n+        return preds\n+\n+\n+model = LogReg()\n+data = {\n+    \"inputs\": np.random.random((1000, 764)),\n+    \"targets\": np.random.random((1000, 1)),\n+    \"sample_weight\": np.random.random((1000, 1)),\n+}\n+\n+model.compile(keras.optimizers.Adam(1e-3))\n+model.fit(data, epochs=2)\n\n@@ -0,0 +1,555 @@\n+\"\"\"\n+Title: Serving TensorFlow models with TFServing\n+Author: [Dimitre Oliveira](https://www.linkedin.com/in/dimitre-oliveira-7a1a0113a/)\n+Date created: 2023/01/02\n+Last modified: 2023/01/02\n+Description: How to serve TensorFlow models with TensorFlow Serving.\n+Accelerator: NONE\n+\"\"\"\n+\n+\"\"\"\n+## Introduction\n+\n+Once you build a machine learning model, the next step is to serve it.\n+You may want to do that by exposing your model as an endpoint service.\n+There are many frameworks that you can use to do that, but the TensorFlow\n+ecosystem has its own solution called\n+[TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving).\n+\n+From the TensorFlow Serving\n+[GitHub page](https://github.com/tensorflow/serving):\n+\n+> TensorFlow Serving is a flexible, high-performance serving system for machine\n+learning models, designed for production environments. It deals with the\n+inference aspect of machine learning, taking models after training and\n+managing their lifetimes, providing clients with versioned access via a\n+high-performance, reference-counted lookup table. TensorFlow Serving provides\n+out-of-the-box integration with TensorFlow models, but can be easily extended\n+to serve other types of models and data.\"\n+\n+To note a few features:\n+\n+- It can serve multiple models, or multiple versions of the same model\n+simultaneously\n+- It exposes both gRPC as well as HTTP inference endpoints\n+- It allows deployment of new model versions without changing any client code\n+- It supports canarying new versions and A/B testing experimental models\n+- It adds minimal latency to inference time due to efficient, low-overhead\n+implementation\n+- It features a scheduler that groups individual inference requests into batches\n+for joint execution on GPU, with configurable latency controls\n+- It supports many servables: Tensorflow models, embeddings, vocabularies,\n+feature transformations and even non-Tensorflow-based machine learning models\n+\n+This guide creates a simple [MobileNet](https://arxiv.org/abs/1704.04861)\n+model using the [Keras applications API](https://keras.io/api/applications/),\n+and then serves it with [TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving).\n+The focus is on TensorFlow Serving, rather than the modeling and training in\n+TensorFlow.\n+\n+> Note: you can find a Colab notebook with the full working code at\n+[this link](https://colab.research.google.com/drive/1nwuIJa4so1XzYU0ngq8tX_-SGTO295Mu?usp=sharing).\n+\"\"\"\n+\"\"\"\n+## Dependencies\n+\"\"\"\n+\n+import os\n+import json\n+import shutil\n+import requests\n+import numpy as np\n+import tensorflow as tf\n+import keras_core as keras\n+import matplotlib.pyplot as plt\n+\n+\"\"\"\n+## Model\n+\n+Here we load a pre-trained [MobileNet](https://arxiv.org/abs/1704.04861)\n+from the [Keras applications](https://keras.io/api/applications/), this is the\n+model that we are going to serve.\n+\"\"\"\n+\n+model = keras.applications.MobileNet()\n+\n+\"\"\"\n+## Preprocessing\n+\n+Most models don't work out of the box on raw data, they usually require some\n+kind of preprocessing step to adjust the data to the model requirements,\n+in the case of this MobileNet we can see from its\n+[API page](https://keras.io/api/applications/mobilenet/) that it requires\n+three basic steps for its input images:\n+\n+- Pixel values normalized to the `[0, 1]` range\n+- Pixel values scaled to the `[-1, 1]` range\n+- Images with the shape of `(224, 224, 3)` meaning `(height, width, channels)`\n+\n+We can do all of that with the following function:\n+\"\"\"\n+\n+\n+def preprocess(image, mean=0.5, std=0.5, shape=(224, 224)):\n+    \"\"\"Scale, normalize and resizes images.\"\"\"\n+    image = image / 255.0  # Scale\n+    image = (image - mean) / std  # Normalize\n+    image = tf.image.resize(image, shape)  # Resize\n+    return image\n+\n+\n+\"\"\"\n+**A note regarding preprocessing and postprocessing using the \"keras.applications\" API**\n+\n+All models that are available at the [Keras applications](https://keras.io/api/applications/)\n+API also provide `preprocess_input` and `decode_predictions` functions, those\n+functions are respectively responsible for the preprocessing and postprocessing\n+of each model, and already contains all the logic necessary for those steps.\n+That is the recommended way to process inputs and outputs when using Keras\n+applications models.\n+For this guide, we are not using them to present the advantages of custom\n+signatures in a clearer way.\n+\"\"\"\n+\n+\n+\"\"\"\n+## Postprocessing\n+\n+In the same context most models output values that need extra processing to\n+meet the user requirements, for instance, the user does not want to know the\n+logits values for each class given an image, what the user wants is to know\n+from which class it belongs. For our model, this translates to the following\n+transformations on top of the model outputs:\n+\n+- Get the index of the class with the highest prediction\n+- Get the name of the class from that index\n+\"\"\"\n+\n+# Download human-readable labels for ImageNet.\n+imagenet_labels_url = \"https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt\"\n+response = requests.get(imagenet_labels_url)\n+# Skiping backgroung class\n+labels = [x for x in response.text.split(\"\\n\") if x != \"\"][1:]\n+# Convert the labels to the TensorFlow data format\n+tf_labels = tf.constant(labels, dtype=tf.string)\n+\n+\n+def postprocess(prediction, labels=tf_labels):\n+    \"\"\"Convert from probs to labels.\"\"\"\n+    indices = tf.argmax(prediction, axis=-1)  # Index with highest prediction\n+    label = tf.gather(params=labels, indices=indices)  # Class name\n+    return label\n+\n+\n+\"\"\"\n+Now let's download a banana picture and see how everything comes together.\n+\"\"\"\n+\n+response = requests.get(\"https://i.imgur.com/j9xCCzn.jpeg\", stream=True)\n+\n+with open(\"banana.jpeg\", \"wb\") as f:\n+    shutil.copyfileobj(response.raw, f)\n+\n+sample_img = plt.imread(\"./banana.jpeg\")\n+print(f\"Original image shape: {sample_img.shape}\")\n+print(f\"Original image pixel range: ({sample_img.min()}, {sample_img.max()})\")\n+plt.imshow(sample_img)\n+plt.show()\n+\n+preprocess_img = preprocess(sample_img)\n+print(f\"Preprocessed image shape: {preprocess_img.shape}\")\n+print(\n+    f\"Preprocessed image pixel range: ({preprocess_img.numpy().min()},\",\n+    f\"{preprocess_img.numpy().max()})\",\n+)\n+\n+batched_img = tf.expand_dims(preprocess_img, axis=0)\n+batched_img = tf.cast(batched_img, tf.float32)\n+print(f\"Batched image shape: {batched_img.shape}\")\n+\n+model_outputs = model(batched_img)\n+print(f\"Model output shape: {model_outputs.shape}\")\n+print(f\"Predicted class: {postprocess(model_outputs)}\")\n+\n+\"\"\"\n+## Save the model\n+\n+To load our trained model into TensorFlow Serving, we first need to save it in\n+[SavedModel](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/saved_model)\n+format. This will create a protobuf file in a well-defined directory hierarchy,\n+and will include a version number.\n+[TensorFlow Serving](https://www.tensorflow.org/tfx/guide/serving) allows us\n+to select which version of a model, or \"servable\" we want to use when we make\n+inference requests. Each version will be exported to a different sub-directory\n+under the given path.\n+\"\"\"\n+\n+model_dir = \"./model\"\n+model_version = 1\n+model_export_path = f\"{model_dir}/{model_version}\"\n+\n+tf.saved_model.save(\n+    model,\n+    export_dir=model_export_path,\n+)\n+\n+print(f\"SavedModel files: {os.listdir(model_export_path)}\")\n+\n+\"\"\"\n+## Examine your saved model\n+\n+We'll use the command line utility `saved_model_cli` to look at the\n+[MetaGraphDefs](https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/MetaGraphDef)\n+(the models) and [SignatureDefs](https://www.tensorflow.org/tfx/serving/signature_defs)\n+(the methods you can call) in our SavedModel. See\n+[this discussion of the SavedModel CLI](https://github.com/tensorflow/docs/blob/master/site/en/r1/guide/saved_model.md#cli-to-inspect-and-execute-savedmodel)\n+in the TensorFlow Guide.\n+\"\"\"\n+\n+\n+\"\"\"shell\n+saved_model_cli show --dir {model_export_path} --tag_set serve --signature_def serving_default\n+\"\"\"\n+\n+\"\"\"\n+That tells us a lot about our model! For instance, we can see that its inputs\n+have a 4D shape `(-1, 224, 224, 3)` which means\n+`(batch_size, height, width, channels)`, also note that this model requires a\n+specific image shape `(224, 224, 3)` this means that we may need to reshape\n+our images before sending them to the model. We can also see that the model's\n+outputs have a `(-1, 1000)` shape which are the logits for the 1000 classes of\n+the [ImageNet](https://www.image-net.org) dataset.\n+\n+This information doesn't tell us everything, like the fact that the pixel\n+values needs to be in the `[-1, 1]` range, but it's a great start.\n+\n+## Serve your model with TensorFlow Serving\n+\n+### Install TFServing\n+\n+We're preparing to install TensorFlow Serving using\n+[Aptitude](https://wiki.debian.org/Aptitude) since this Colab runs in a Debian\n+environment. We'll add the `tensorflow-model-server` package to the list of\n+packages that Aptitude knows about. Note that we're running as root.\n+\n+\n+> Note: This example is running TensorFlow Serving natively, but [you can also\n+run it in a Docker container](https://www.tensorflow.org/tfx/serving/docker),\n+which is one of the easiest ways to get started using TensorFlow Serving.\n+\n+```shell\n+wget 'http://storage.googleapis.com/tensorflow-serving-apt/pool/tensorflow-model-server-universal-2.8.0/t/tensorflow-model-server-universal/tensorflow-model-server-universal_2.8.0_all.deb'\n+dpkg -i tensorflow-model-server-universal_2.8.0_all.deb\n+```\n+\"\"\"\n+\n+\"\"\"\n+### Start running TensorFlow Serving\n+\n+This is where we start running TensorFlow Serving and load our model. After it\n+loads, we can start making inference requests using REST. There are some\n+important parameters:\n+\n+- `port`: The port that you'll use for gRPC requests.\n+- `rest_api_port`: The port that you'll use for REST requests.\n+- `model_name`: You'll use this in the URL of REST requests. It can be\n+anything.\n+- `model_base_path`: This is the path to the directory where you've saved your\n+model.\n+\n+Check the [TFServing API reference](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/model_servers/main.cc)\n+to get all the parameters available.\n+\"\"\"\n+\n+# Environment variable with the path to the model\n+os.environ[\"MODEL_DIR\"] = f\"{model_dir}\"\n+\n+\"\"\"\n+```shell\n+%%bash --bg\n+nohup tensorflow_model_server \\\n+  --port=8500 \\\n+  --rest_api_port=8501 \\\n+  --model_name=model \\\n+  --model_base_path=$MODEL_DIR >server.log 2>&1\n+```\n+\n+```shell\n+# We can check the logs to the server to help troubleshooting\n+!cat server.log\n+```\n+outputs:\n+```\n+[warn] getaddrinfo: address family for nodename not supported\n+[evhttp_server.cc : 245] NET_LOG: Entering the event loop ...\n+```\n+\n+```shell\n+# Now we can check if tensorflow is in the active services\n+!sudo lsof -i -P -n | grep LISTEN\n+```\n+outputs:\n+```\n+node         7 root   21u  IPv6  19100      0t0  TCP *:8080 (LISTEN)\n+kernel_ma   34 root    7u  IPv4  18874      0t0  TCP 172.28.0.12:6000 (LISTEN)\n+colab-fil   63 root    5u  IPv4  17975      0t0  TCP *:3453 (LISTEN)\n+colab-fil   63 root    6u  IPv6  17976      0t0  TCP *:3453 (LISTEN)\n+jupyter-n   81 root    6u  IPv4  18092      0t0  TCP 172.28.0.12:9000 (LISTEN)\n+python3    101 root   23u  IPv4  18252      0t0  TCP 127.0.0.1:44915 (LISTEN)\n+python3    132 root    3u  IPv4  20548      0t0  TCP 127.0.0.1:15264 (LISTEN)\n+python3    132 root    4u  IPv4  20549      0t0  TCP 127.0.0.1:37977 (LISTEN)\n+python3    132 root    9u  IPv4  20662      0t0  TCP 127.0.0.1:40689 (LISTEN)\n+tensorflo 1101 root    5u  IPv4  35543      0t0  TCP *:8500 (LISTEN)\n+tensorflo 1101 root   12u  IPv4  35548      0t0  TCP *:8501 (LISTEN)\n+```\n+\n+## Make a request to your model in TensorFlow Serving\n+\n+Now let's create the JSON object for an inference request, and see how well\n+our model classifies it:\n+\n+### REST API\n+\n+#### Newest version of the servable\n+\n+We'll send a predict request as a POST to our server's REST endpoint, and pass\n+it as an example. We'll ask our server to give us the latest version of our\n+servable by not specifying a particular version.\n+\"\"\"\n+\n+data = json.dumps(\n+    {\n+        \"signature_name\": \"serving_default\",\n+        \"instances\": batched_img.numpy().tolist(),\n+    }\n+)\n+url = \"http://localhost:8501/v1/models/model:predict\"\n+\n+\n+def predict_rest(json_data, url):\n+    json_response = requests.post(url, data=json_data)\n+    response = json.loads(json_response.text)\n+    rest_outputs = np.array(response[\"predictions\"])\n+    return rest_outputs\n+\n+\n+\"\"\"\n+```python\n+rest_outputs = predict_rest(data, url)\n+\n+print(f\"REST output shape: {rest_outputs.shape}\")\n+print(f\"Predicted class: {postprocess(rest_outputs)}\")\n+```\n+\n+outputs:\n+```\n+REST output shape: (1, 1000)\n+Predicted class: [b'banana']\n+```\n+\n+### gRPC API\n+\n+[gRPC](https://grpc.io/) is based on the Remote Procedure Call (RPC) model and\n+is a technology for implementing RPC APIs that uses HTTP 2.0 as its underlying\n+transport protocol. gRPC is usually preferred for low-latency, highly scalable,\n+and distributed systems. If you wanna know more about the REST vs gRPC\n+tradeoffs, checkout\n+[this article](https://cloud.google.com/blog/products/api-management/understanding-grpc-openapi-and-rest-and-when-to-use-them).\n+\"\"\"\n+\n+import grpc\n+\n+# Create a channel that will be connected to the gRPC port of the container\n+channel = grpc.insecure_channel(\"localhost:8500\")\n+\n+\"\"\"\n+```shell\n+!pip install -q tensorflow_serving_api\n+```\n+\n+```python\n+from tensorflow_serving.apis import predict_pb2, prediction_service_pb2_grpc\n+\n+# Create a stub made for prediction\n+# This stub will be used to send the gRPCrequest to the TF Server\n+stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n+```\n+\"\"\"\n+\n+# Get the serving_input key\n+loaded_model = tf.saved_model.load(model_export_path)\n+print(loaded_model)\n+print(loaded_model.signatures)\n+input_name = list(\n+    loaded_model.signatures[\"serving_default\"]\n+    .structured_input_signature[1]\n+    .keys()\n+)[0]\n+\n+\n+\"\"\"\n+```python\n+def predict_grpc(data, input_name, stub):\n+    # Create a gRPC request made for prediction\n+    request = predict_pb2.PredictRequest()\n+\n+    # Set the name of the model, for this use case it is \"model\"\n+    request.model_spec.name = \"model\"\n+\n+    # Set which signature is used to format the gRPC query\n+    # here the default one \"serving_default\"\n+    request.model_spec.signature_name = \"serving_default\"\n+\n+    # Set the input as the data\n+    # tf.make_tensor_proto turns a TensorFlow tensor into a Protobuf tensor\n+    request.inputs[input_name].CopyFrom(tf.make_tensor_proto(data.numpy().tolist()))\n+\n+    # Send the gRPC request to the TF Server\n+    result = stub.Predict(request)\n+    return result\n+\n+\n+grpc_outputs = predict_grpc(batched_img, input_name, stub)\n+grpc_outputs = np.array([grpc_outputs.outputs['predictions'].float_val])\n+\n+print(f\"gRPC output shape: {grpc_outputs.shape}\")\n+print(f\"Predicted class: {postprocess(grpc_outputs)}\")\n+```\n+\n+outputs:\n+```\n+gRPC output shape: (1, 1000)\n+Predicted class: [b'banana']\n+```\n+\"\"\"\n+\n+\"\"\"\n+## Custom signature\n+\n+Note that for this model we always need to preprocess and postprocess all\n+samples to get the desired output, this can get quite tricky if are\n+maintaining and serving several models developed by a large team, and each one\n+of them might require different processing logic.\n+\n+TensorFlow allows us to customize the model graph to embed all of that\n+processing logic, which makes model serving much easier, there are different\n+ways to achieve this, but since we are going to server the models using\n+TFServing we can customize the model graph straight into the serving signature.\n+\n+We can just use the following code to export the same model that already\n+contains the preprocessing and postprocessing logic as the default signature,\n+this allows this model to make predictions on raw data.\n+\"\"\"\n+\n+\n+def export_model(model, labels):\n+    @tf.function(\n+        input_signature=[tf.TensorSpec([None, None, None, 3], tf.float32)]\n+    )\n+    def serving_fn(image):\n+        processed_img = preprocess(image)\n+        probs = model(processed_img)\n+        label = postprocess(probs)\n+        return {\"label\": label}\n+\n+    return serving_fn\n+\n+\n+model_sig_version = 2\n+model_sig_export_path = f\"{model_dir}/{model_sig_version}\"\n+\n+tf.saved_model.save(\n+    model,\n+    export_dir=model_sig_export_path,\n+    signatures={\"serving_default\": export_model(model, labels)},\n+)\n+\n+\"\"\"shell\n+saved_model_cli show --dir {model_sig_export_path} --tag_set serve --signature_def serving_default\n+\"\"\"\n+\n+\"\"\"\n+Note that this model has a different signature, its input is still 4D but now\n+with a `(-1, -1, -1, 3)` shape, which means that it supports images with any\n+height and width size. Its output also has a different shape, it no longer\n+outputs the 1000-long logits.\n+\n+We can test the model's prediction using a specific signature using this API\n+below:\n+\"\"\"\n+\n+batched_raw_img = tf.expand_dims(sample_img, axis=0)\n+batched_raw_img = tf.cast(batched_raw_img, tf.float32)\n+\n+loaded_model = tf.saved_model.load(model_sig_export_path)\n+loaded_model.signatures[\"serving_default\"](**{\"image\": batched_raw_img})\n+\n+\"\"\"\n+## Prediction using a particular version of the servable\n+\n+Now let's specify a particular version of our servable. Note that when we\n+saved the model with a custom signature we used a different folder, the first\n+model was saved in folder `/1` (version 1), and the one with a custom\n+signature in folder `/2` (version 2). By default, TFServing will serve all\n+models that share the same base parent folder.\n+\n+### REST API\n+\"\"\"\n+\n+data = json.dumps(\n+    {\n+        \"signature_name\": \"serving_default\",\n+        \"instances\": batched_raw_img.numpy().tolist(),\n+    }\n+)\n+url_sig = \"http://localhost:8501/v1/models/model/versions/2:predict\"\n+\n+\"\"\"\n+```python\n+print(f\"REST output shape: {rest_outputs.shape}\")\n+print(f\"Predicted class: {rest_outputs}\")\n+```\n+\n+outputs:\n+```\n+REST output shape: (1,)\n+Predicted class: ['banana']\n+```\n+\n+### gRPC API\n+\"\"\"\n+\n+\"\"\"\n+```python\n+channel = grpc.insecure_channel(\"localhost:8500\")\n+stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n+```\n+\"\"\"\n+\n+input_name = list(\n+    loaded_model.signatures[\"serving_default\"]\n+    .structured_input_signature[1]\n+    .keys()\n+)[0]\n+\n+\"\"\"\n+```python\n+grpc_outputs = predict_grpc(batched_raw_img, input_name, stub)\n+grpc_outputs = np.array([grpc_outputs.outputs['label'].string_val])\n+\n+print(f\"gRPC output shape: {grpc_outputs.shape}\")\n+print(f\"Predicted class: {grpc_outputs}\")\n+```\n+\n+outputs:\n+```\n+gRPC output shape: (1, 1)\n+Predicted class: [[b'banana']]\n+```\n+\n+## Additional resources\n+\n+- [Colab notebook with the full working code](https://colab.research.google.com/drive/1nwuIJa4so1XzYU0ngq8tX_-SGTO295Mu?usp=sharing)\n+- [Train and serve a TensorFlow model with TensorFlow Serving - TensorFlow blog](https://www.tensorflow.org/tfx/tutorials/serving/rest_simple#make_a_request_to_your_model_in_tensorflow_serving)\n+- [TensorFlow Serving playlist - TensorFlow YouTube channel](https://www.youtube.com/playlist?list=PLQY2H8rRoyvwHdpVQVohY7-qcYf2s1UYK)\n+\"\"\"\n\n@@ -0,0 +1,139 @@\n+\"\"\"\n+Title: Trainer pattern\n+Author: [nkovela1](https://nkovela1.github.io/)\n+Date created: 2022/09/19\n+Last modified: 2022/09/26\n+Description: Guide on how to share a custom training step across multiple Keras models.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+This example shows how to create a custom training step using the \"Trainer pattern\",\n+which can then be shared across multiple Keras models. This pattern overrides the\n+`train_step()` method of the `keras.Model` class, allowing for training loops\n+beyond plain supervised learning.\n+\n+The Trainer pattern can also easily be adapted to more complex models with larger\n+custom training steps, such as\n+[this end-to-end GAN model](https://keras.io/guides/customizing_what_happens_in_fit/#wrapping-up-an-endtoend-gan-example),\n+by putting the custom training step in the Trainer class definition.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import tensorflow as tf\n+import keras_core as keras\n+\n+# Load MNIST dataset and standardize the data\n+mnist = keras.datasets.mnist\n+(x_train, y_train), (x_test, y_test) = mnist.load_data()\n+x_train, x_test = x_train / 255.0, x_test / 255.0\n+\n+\n+\"\"\"\n+## Define the Trainer class\n+\n+A custom training and evaluation step can be created by overriding\n+the `train_step()` and `test_step()` method of a `Model` subclass:\n+\"\"\"\n+\n+\n+class MyTrainer(keras.Model):\n+    def __init__(self, model):\n+        super().__init__()\n+        self.model = model\n+        # Create loss and metrics here.\n+        self.loss_fn = keras.losses.SparseCategoricalCrossentropy()\n+        self.accuracy_metric = keras.metrics.SparseCategoricalAccuracy()\n+\n+    @property\n+    def metrics(self):\n+        # List metrics here.\n+        return [self.accuracy_metric]\n+\n+    def train_step(self, data):\n+        x, y = data\n+        with tf.GradientTape() as tape:\n+            y_pred = self.model(x, training=True)  # Forward pass\n+            # Compute loss value\n+            loss = self.loss_fn(y, y_pred)\n+\n+        # Compute gradients\n+        trainable_vars = self.trainable_variables\n+        gradients = tape.gradient(loss, trainable_vars)\n+\n+        # Update weights\n+        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n+\n+        # Update metrics\n+        for metric in self.metrics:\n+            metric.update_state(y, y_pred)\n+\n+        # Return a dict mapping metric names to current value.\n+        return {m.name: m.result() for m in self.metrics}\n+\n+    def test_step(self, data):\n+        x, y = data\n+\n+        # Inference step\n+        y_pred = self.model(x, training=False)\n+\n+        # Update metrics\n+        for metric in self.metrics:\n+            metric.update_state(y, y_pred)\n+        return {m.name: m.result() for m in self.metrics}\n+\n+    def call(self, x):\n+        # Equivalent to `call()` of the wrapped keras.Model\n+        x = self.model(x)\n+        return x\n+\n+\n+\"\"\"\n+## Define multiple models to share the custom training step\n+\n+Let's define two different models that can share our Trainer class and its custom `train_step()`:\n+\"\"\"\n+\n+# A model defined using Sequential API\n+model_a = keras.models.Sequential(\n+    [\n+        keras.layers.Flatten(input_shape=(28, 28)),\n+        keras.layers.Dense(256, activation=\"relu\"),\n+        keras.layers.Dropout(0.2),\n+        keras.layers.Dense(10, activation=\"softmax\"),\n+    ]\n+)\n+\n+# A model defined using Functional API\n+func_input = keras.Input(shape=(28, 28, 1))\n+x = keras.layers.Flatten(input_shape=(28, 28))(func_input)\n+x = keras.layers.Dense(512, activation=\"relu\")(x)\n+x = keras.layers.Dropout(0.4)(x)\n+func_output = keras.layers.Dense(10, activation=\"softmax\")(x)\n+\n+model_b = keras.Model(func_input, func_output)\n+\n+\"\"\"\n+## Create Trainer class objects from the models\n+\"\"\"\n+\n+trainer_1 = MyTrainer(model_a)\n+trainer_2 = MyTrainer(model_b)\n+\n+\"\"\"\n+## Compile and fit the models to the MNIST dataset\n+\"\"\"\n+\n+trainer_1.compile(optimizer=keras.optimizers.SGD())\n+trainer_1.fit(\n+    x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test)\n+)\n+\n+trainer_2.compile(optimizer=keras.optimizers.Adam())\n+trainer_2.fit(\n+    x_train, y_train, epochs=5, batch_size=64, validation_data=(x_test, y_test)\n+)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#27d6e4aec198360a4034dd9dd0727e18c27980c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 25 | Churn Cumulative: 219 | Contributors (this commit): 4 | Commits (past 90d): 16 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,6 +1,7 @@\n import torch\n \n from keras_core import layers\n+from keras_core.backend.common import KerasVariable\n \n \n class Net(torch.nn.Module):\n@@ -15,6 +16,16 @@ class Net(torch.nn.Module):\n \n net = Net()\n \n+# Test using Keras layer in a nn.Module.\n+# Test forward pass\n assert list(net(torch.empty(100, 10)).shape) == [100, 1]\n+# Test KerasVariables are added as nn.Parameter.\n assert len(list(net.parameters())) == 2\n optimizer = torch.optim.SGD(net.parameters(), lr=1e-3)\n+\n+\n+# Test using KerasVariable as a torch tensor for torch ops.\n+kernel = net.fc1.kernel\n+transposed_kernel = torch.transpose(kernel, 0, 1)\n+assert isinstance(kernel, KerasVariable)\n+assert isinstance(torch.mul(kernel, transposed_kernel), torch.Tensor)\n\n@@ -44,8 +44,18 @@ class Variable(KerasVariable):\n         return convert_to_tensor(value, dtype=dtype)\n \n     # Overload native accessor.\n-    def __torch_function__(self, func, types, args=(), kwargs=None):\n-        return func(self.value, *args, **kwargs)\n+    @classmethod\n+    def __torch_function__(cls, func, types, args=(), kwargs=None):\n+        args = [\n+            arg.value if isinstance(arg, KerasVariable) else arg for arg in args\n+        ]\n+        if kwargs is None:\n+            kwargs = {}\n+        kwargs = {\n+            key: value.value if isinstance(value, KerasVariable) else value\n+            for key, value in kwargs.items()\n+        }\n+        return func(*args, **kwargs)\n \n \n def convert_to_tensor(x, dtype=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5468ab94971b4ab2f4d7de22ac87c52eb73eae23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 150 | Contributors (this commit): 4 | Commits (past 90d): 10 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,12 @@ from keras_core.api_export import keras_core_export\n from keras_core.backend.common import global_state\n \n \n-@keras_core_export(\"keras_core.config.enable_interactive_logging\")\n+@keras_core_export(\n+    [\n+        \"keras_core.config.enable_interactive_logging\",\n+        \"keras_core.utils.enable_interactive_logging\",\n+    ]\n+)\n def enable_interactive_logging():\n     \"\"\"Turn on interactive logging.\n \n@@ -17,7 +22,12 @@ def enable_interactive_logging():\n     global_state.set_global_setting(\"interactive_logging\", True)\n \n \n-@keras_core_export(\"keras_core.config.disable_interactive_logging\")\n+@keras_core_export(\n+    [\n+        \"keras_core.config.disable_interactive_logging\",\n+        \"keras_core.utils.disable_interactive_logging\",\n+    ]\n+)\n def disable_interactive_logging():\n     \"\"\"Turn off interactive logging.\n \n@@ -28,7 +38,12 @@ def disable_interactive_logging():\n     global_state.set_global_setting(\"interactive_logging\", False)\n \n \n-@keras_core_export(\"keras_core.config.is_interactive_logging_enabled\")\n+@keras_core_export(\n+    [\n+        \"keras_core.config.is_interactive_logging_enabled\",\n+        \"keras_core.utils.is_interactive_logging_enabled\",\n+    ]\n+)\n def is_interactive_logging_enabled():\n     \"\"\"Check if interactive logging is enabled.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#35d42bfdd229ccd3298a2db06a9dfa59e430bad8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 32 | Files Changed: 5 | Hunks: 17 | Methods Changed: 12 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 49 | Churn Cumulative: 873 | Contributors (this commit): 4 | Commits (past 90d): 19 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -112,7 +112,7 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         ((2,), (2,), \"valid\", \"channels_last\"),\n     )\n     def test_average_pooling1d(self, pool_size, strides, padding, data_format):\n-        inputs = np.arange(24, dtype=np.float).reshape((2, 3, 4))\n+        inputs = np.arange(24, dtype=\"float32\").reshape((2, 3, 4))\n \n         layer = layers.AveragePooling1D(\n             pool_size=pool_size,\n@@ -136,7 +136,7 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         ((2, 3), (2, 2), \"same\", \"channels_last\"),\n     )\n     def test_average_pooling2d(self, pool_size, strides, padding, data_format):\n-        inputs = np.arange(300, dtype=np.float).reshape((3, 5, 5, 4))\n+        inputs = np.arange(300, dtype=\"float32\").reshape((3, 5, 5, 4))\n \n         layer = layers.AveragePooling2D(\n             pool_size=pool_size,\n@@ -161,7 +161,7 @@ class AveragePoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         ((2, 3, 2), (2, 2, 1), \"valid\", \"channels_last\"),\n     )\n     def test_average_pooling3d(self, pool_size, strides, padding, data_format):\n-        inputs = np.arange(240, dtype=np.float).reshape((2, 3, 4, 5, 2))\n+        inputs = np.arange(240, dtype=\"float32\").reshape((2, 3, 4, 5, 2))\n \n         layer = layers.AveragePooling3D(\n             pool_size=pool_size,\n\n@@ -95,7 +95,7 @@ class GlobalAveragePoolingCorrectnessTest(\n         (\"channels_first\", False),\n     )\n     def test_global_average_pooling1d(self, data_format, keepdims):\n-        inputs = np.arange(24, dtype=np.float).reshape((2, 3, 4))\n+        inputs = np.arange(24, dtype=\"float32\").reshape((2, 3, 4))\n \n         layer = layers.GlobalAveragePooling1D(\n             data_format=data_format,\n@@ -111,9 +111,9 @@ class GlobalAveragePoolingCorrectnessTest(\n         self.assertAllClose(outputs, expected)\n \n         if data_format == \"channels_last\":\n-            mask = np.array([[1, 1, 0], [0, 1, 0]], dtype=np.int)\n+            mask = np.array([[1, 1, 0], [0, 1, 0]], dtype=\"int32\")\n         else:\n-            mask = np.array([[1, 1, 0, 0], [0, 1, 0, 1]], dtype=np.int)\n+            mask = np.array([[1, 1, 0, 0], [0, 1, 0, 1]], dtype=\"int32\")\n         outputs = layer(inputs, mask)\n         expected = tf_keras_layer(inputs, mask)\n         self.assertAllClose(outputs, expected)\n@@ -124,7 +124,7 @@ class GlobalAveragePoolingCorrectnessTest(\n         (\"channels_first\", False),\n     )\n     def test_global_average_pooling2d(self, data_format, keepdims):\n-        inputs = np.arange(96, dtype=np.float).reshape((2, 3, 4, 4))\n+        inputs = np.arange(96, dtype=\"float32\").reshape((2, 3, 4, 4))\n \n         layer = layers.GlobalAveragePooling2D(\n             data_format=data_format,\n@@ -145,7 +145,7 @@ class GlobalAveragePoolingCorrectnessTest(\n         (\"channels_first\", False),\n     )\n     def test_global_average_pooling3d(self, data_format, keepdims):\n-        inputs = np.arange(360, dtype=np.float).reshape((2, 3, 3, 5, 4))\n+        inputs = np.arange(360, dtype=\"float32\").reshape((2, 3, 3, 5, 4))\n \n         layer = layers.GlobalAveragePooling3D(\n             data_format=data_format,\n\n@@ -93,7 +93,7 @@ class GlobalMaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         (\"channels_first\", False),\n     )\n     def test_global_max_pooling1d(self, data_format, keepdims):\n-        inputs = np.arange(24, dtype=np.float).reshape((2, 3, 4))\n+        inputs = np.arange(24, dtype=\"float32\").reshape((2, 3, 4))\n \n         layer = layers.GlobalMaxPooling1D(\n             data_format=data_format,\n@@ -114,7 +114,7 @@ class GlobalMaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         (\"channels_first\", False),\n     )\n     def test_global_max_pooling2d(self, data_format, keepdims):\n-        inputs = np.arange(96, dtype=np.float).reshape((2, 3, 4, 4))\n+        inputs = np.arange(96, dtype=\"float32\").reshape((2, 3, 4, 4))\n \n         layer = layers.GlobalMaxPooling2D(\n             data_format=data_format,\n@@ -135,7 +135,7 @@ class GlobalMaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         (\"channels_first\", False),\n     )\n     def test_global_max_pooling3d(self, data_format, keepdims):\n-        inputs = np.arange(360, dtype=np.float).reshape((2, 3, 3, 5, 4))\n+        inputs = np.arange(360, dtype=\"float32\").reshape((2, 3, 3, 5, 4))\n \n         layer = layers.GlobalMaxPooling3D(\n             data_format=data_format,\n\n@@ -112,7 +112,7 @@ class MaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         ((2,), (2,), \"valid\", \"channels_last\"),\n     )\n     def test_max_pooling1d(self, pool_size, strides, padding, data_format):\n-        inputs = np.arange(24, dtype=np.float).reshape((2, 3, 4))\n+        inputs = np.arange(24, dtype=\"float32\").reshape((2, 3, 4))\n \n         layer = layers.MaxPooling1D(\n             pool_size=pool_size,\n@@ -136,7 +136,7 @@ class MaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         ((2, 3), (2, 2), \"same\", \"channels_last\"),\n     )\n     def test_max_pooling2d(self, pool_size, strides, padding, data_format):\n-        inputs = np.arange(300, dtype=np.float).reshape((3, 5, 5, 4))\n+        inputs = np.arange(300, dtype=\"float32\").reshape((3, 5, 5, 4))\n \n         layer = layers.MaxPooling2D(\n             pool_size=pool_size,\n@@ -161,7 +161,7 @@ class MaxPoolingCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         ((2, 3, 2), (2, 2, 1), \"valid\", \"channels_last\"),\n     )\n     def test_max_pooling3d(self, pool_size, strides, padding, data_format):\n-        inputs = np.arange(240, dtype=np.float).reshape((2, 3, 4, 5, 2))\n+        inputs = np.arange(240, dtype=\"float32\").reshape((2, 3, 4, 5, 2))\n \n         layer = layers.MaxPooling3D(\n             pool_size=pool_size,\n\n@@ -6,12 +6,7 @@ from keras_core.api_export import keras_core_export\n from keras_core.backend.common import global_state\n \n \n-@keras_core_export(\n-    [\n-        \"keras_core.config.enable_interactive_logging\",\n-        \"keras_core.utils.enable_interactive_logging\",\n-    ]\n-)\n+@keras_core_export(\"keras_core.config.enable_interactive_logging\")\n def enable_interactive_logging():\n     \"\"\"Turn on interactive logging.\n \n@@ -22,12 +17,7 @@ def enable_interactive_logging():\n     global_state.set_global_setting(\"interactive_logging\", True)\n \n \n-@keras_core_export(\n-    [\n-        \"keras_core.config.disable_interactive_logging\",\n-        \"keras_core.utils.disable_interactive_logging\",\n-    ]\n-)\n+@keras_core_export(\"keras_core.config.disable_interactive_logging\")\n def disable_interactive_logging():\n     \"\"\"Turn off interactive logging.\n \n@@ -38,12 +28,7 @@ def disable_interactive_logging():\n     global_state.set_global_setting(\"interactive_logging\", False)\n \n \n-@keras_core_export(\n-    [\n-        \"keras_core.config.is_interactive_logging_enabled\",\n-        \"keras_core.utils.is_interactive_logging_enabled\",\n-    ]\n-)\n+@keras_core_export(\"keras_core.config.is_interactive_logging_enabled\")\n def is_interactive_logging_enabled():\n     \"\"\"Check if interactive logging is enabled.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
