{"custom_id": "keras#2157aa6172c7cc397231e8c22e0388ad62c975dc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 18 | Methods Changed: 13 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 51 | Churn Cumulative: 1591 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 2 | DMM Complexity: 0.7096774193548387\n\nDIFF:\n@@ -627,19 +627,42 @@ class Model(Container):\n             train_test_inputs = self.inputs + self.targets + sample_weights\n             predict_inputs = self.inputs\n \n+        # functions for train, test and predict will\n+        # be compiled lazily when required.\n+        # This saves time when the user is not using all functions.\n+        self._train_updates = train_updates\n+        self._train_test_inputs = train_test_inputs\n+        self._all_outputs = all_outputs\n+        self._predict_inputs = predict_inputs\n+        self._function_kwargs = kwargs\n+\n+        self.train_function = None\n+        self.test_function = None\n+        self.predict_function = None\n+\n+    def _make_train_function(self):\n+        if self.train_function is None:\n             # returns loss and metrics. Updates weights at each call.\n-        self.train_function = K.function(train_test_inputs, all_outputs,\n-                                         updates=train_updates, **kwargs)\n+            self.train_function = K.function(self._train_test_inputs,\n+                                             self._all_outputs,\n+                                             updates=self._train_updates,\n+                                             **self._function_kwargs)\n \n+    def _make_test_function(self):\n+        if self.test_function is None:\n             # return loss and metrics, no gradient updates.\n-        self.test_function = K.function(train_test_inputs,\n-                                        all_outputs, **kwargs)\n+            self.test_function = K.function(self._train_test_inputs,\n+                                            self._all_outputs,\n+                                            **self._function_kwargs)\n \n+    def _make_predict_function(self):\n+        if self.predict_function is None:\n             # returns network outputs. Does not update weights.\n             # Does update the network states.\n-        self.predict_function = K.function(predict_inputs, self.outputs,\n+            self.predict_function = K.function(self._predict_inputs,\n+                                               self.outputs,\n                                                updates=self.state_updates,\n-                                           **kwargs)\n+                                               **self._function_kwargs)\n \n     def _fit_loop(self, f, ins, out_labels=[], batch_size=32,\n                   nb_epoch=100, verbose=1, callbacks=[],\n@@ -931,6 +954,7 @@ class Model(Container):\n             ins = x + y + sample_weights + [1.]\n         else:\n             ins = x + y + sample_weights\n+        self._make_train_function()\n         f = self.train_function\n \n         # prepare validation data\n@@ -947,6 +971,7 @@ class Model(Container):\n                                                                            sample_weight=val_sample_weight,\n                                                                            check_batch_dim=False,\n                                                                            batch_size=batch_size)\n+            self._make_test_function()\n             val_f = self.test_function\n             if self.uses_learning_phase:\n                 val_ins = val_x + val_y + val_sample_weights + [0.]\n@@ -959,6 +984,7 @@ class Model(Container):\n             x, val_x = (slice_X(x, 0, split_at), slice_X(x, split_at))\n             y, val_y = (slice_X(y, 0, split_at), slice_X(y, split_at))\n             sample_weights, val_sample_weights = (slice_X(sample_weights, 0, split_at), slice_X(sample_weights, split_at))\n+            self._make_test_function()\n             val_f = self.test_function\n             if self.uses_learning_phase:\n                 val_ins = val_x + val_y + val_sample_weights + [0.]\n@@ -1014,6 +1040,7 @@ class Model(Container):\n             ins = x + y + sample_weights + [0.]\n         else:\n             ins = x + y + sample_weights\n+        self._make_test_function()\n         f = self.test_function\n         return self._test_loop(f, ins,\n                                batch_size=batch_size,\n@@ -1050,6 +1077,7 @@ class Model(Container):\n             ins = x + [0.]\n         else:\n             ins = x\n+        self._make_predict_function()\n         f = self.predict_function\n         return self._predict_loop(f, ins,\n                                   batch_size=batch_size, verbose=verbose)\n@@ -1092,6 +1120,7 @@ class Model(Container):\n             ins = x + y + sample_weights + [1.]\n         else:\n             ins = x + y + sample_weights\n+        self._make_train_function()\n         outputs = self.train_function(ins)\n         if len(outputs) == 1:\n             return outputs[0]\n@@ -1129,6 +1158,7 @@ class Model(Container):\n             ins = x + y + sample_weights + [1.]\n         else:\n             ins = x + y + sample_weights\n+        self._make_test_function()\n         outputs = self.test_function(ins)\n         if len(outputs) == 1:\n             return outputs[0]\n@@ -1143,6 +1173,7 @@ class Model(Container):\n             ins = x + [1.]\n         else:\n             ins = x\n+        self._make_predict_function()\n         outputs = self.predict_function(ins)\n         if len(outputs) == 1:\n             return outputs[0]\n@@ -1206,6 +1237,10 @@ class Model(Container):\n         epoch = 0\n \n         do_validation = bool(validation_data)\n+        self._make_train_function()\n+        if do_validation:\n+            self._make_test_function()\n+\n         # python 2 has 'next', 3 has '__next__'\n         # avoid any explicit version checks\n         val_gen = (hasattr(validation_data, 'next') or\n@@ -1357,6 +1392,8 @@ class Model(Container):\n             and/or metrics). The attribute `model.metrics_names` will give you\n             the display labels for the scalar outputs.\n         '''\n+        self._make_test_function()\n+\n         processed_samples = 0\n         wait_time = 0.05\n         all_outs = []\n@@ -1423,6 +1460,8 @@ class Model(Container):\n         # Returns\n             Numpy array(s) of predictions.\n         '''\n+        self._make_predict_function()\n+\n         processed_samples = 0\n         wait_time = 0.05\n         all_outs = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d6f94c0bc97ace3b82fb1abcb3ec7a4e5cb41b63", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 10267 | Contributors (this commit): 43 | Commits (past 90d): 44 | Contributors (cumulative): 43 | DMM Complexity: None\n\nDIFF:\n@@ -273,15 +273,6 @@ class Flatten(Layer):\n         model.add(Flatten())\n         # now: model.output_shape == (None, 65536)\n     ```\n-\n-    # Input shape\n-        Arbitrary, although all dimensions in the input shape must be fixed.\n-        Use the keyword argument `input_shape`\n-        (tuple of integers, does not include the samples axis)\n-        when using this layer as the first layer in a model.\n-\n-    # Output shape\n-        `(batch_size,)`\n     '''\n     def __init__(self, **kwargs):\n         self.input_spec = [InputSpec(ndim='3+')]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#36cc50803021b21deefb053ad53e39e8442184e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 346 | Lines Deleted: 99 | Files Changed: 1 | Hunks: 23 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 445 | Churn Cumulative: 717 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,57 @@\n # -*- coding: utf-8 -*-\n+'''\n+General documentation architecture:\n+\n+Home\n+Index\n+\n+- Getting started\n+    Getting started with the sequential model\n+    Getting started with the functional api\n+    Examples\n+    FAQ\n+    Installation guide\n+\n+- Models\n+    About Keras models\n+        explain when one should use Sequential or functional API\n+        explain compilation step\n+        explain weight saving, weight loading\n+        explain serialization, deserialization\n+    Sequential\n+    Model (functional API)\n+\n+- Layers\n+    About Keras layers\n+        explain common layer functions: get_weights, set_weights, get_config\n+        explain input_shape\n+        explain usage on non-Keras tensors\n+    Core layers\n+    Convolutional\n+    Recurrent\n+    Embeddings\n+    Normalization\n+    Advanced activations\n+    Noise\n+\n+- Preprocessing\n+    Image preprocessing\n+    Text preprocessing\n+    Sequence preprocessing\n+\n+Objectives\n+Optimizers\n+Activations\n+Callbacks\n+Datasets\n+Backend\n+Initializations\n+Regularizers\n+Constraints\n+Visualization\n+Scikit-learn API\n+\n+'''\n from __future__ import print_function\n import re\n import inspect\n@@ -11,34 +64,143 @@ from keras.layers import core\n from keras.layers import noise\n from keras.layers import normalization\n from keras.layers import advanced_activations\n-from keras.layers import containers\n from keras.layers import embeddings\n+from keras.layers import wrappers\n from keras import optimizers\n from keras import callbacks\n from keras import models\n+from keras.engine import topology\n+from keras import objectives\n+from keras import backend\n+from keras import constraints\n+from keras import activations\n+from keras import regularizers\n \n-MODULES = [(convolutional, 'keras.layers.convolutional'),\n-           (recurrent, 'keras.layers.recurrent'),\n-           (noise, 'keras.layers.noise'),\n-           (normalization, 'keras.layers.normalization'),\n-           (advanced_activations, 'keras.layers.advanced_activations'),\n-           (containers, 'keras.layers.containers'),\n-           (core, 'keras.layers.core'),\n-           (embeddings, 'keras.layers.embeddings'),\n-           (optimizers, 'keras.optimizers'),\n-           (callbacks, 'keras.callbacks'),\n-           (models, 'keras.models')]\n+EXCLUDE = {\n+    'Optimizer',\n+}\n \n-SKIP = ['build', 'get_params', 'MaskedLayer',\n-        'SiameseHead', 'MaskedLambda',\n-        'CallbackList']\n-ROOT = 'http://keras.io/'\n-INCLUDE_METHODS_FOR = [\n-    'Layer',\n-    'Graph',\n-    'Sequential',\n-    'Callback',\n+PAGES = [\n+    {\n+        'page': 'models/sequential.md',\n+        'functions': [\n+            models.Sequential.compile,\n+            models.Sequential.fit,\n+            models.Sequential.evaluate,\n+            models.Sequential.predict,\n+            models.Sequential.predict_classes,\n+            models.Sequential.predict_proba,\n+            models.Sequential.train_on_batch,\n+            models.Sequential.test_on_batch,\n+            models.Sequential.predict_on_batch,\n+            models.Sequential.fit_generator,\n+            models.Sequential.evaluate_generator,\n+        ],\n+    },\n+    {\n+        'page': 'models/model.md',\n+        'functions': [\n+            models.Model.compile,\n+            models.Model.fit,\n+            models.Model.evaluate,\n+            models.Model.predict,\n+            models.Model.train_on_batch,\n+            models.Model.test_on_batch,\n+            models.Model.predict_on_batch,\n+            models.Model.fit_generator,\n+            models.Model.evaluate_generator,\n+            models.Model.get_layer,\n         ]\n+    },\n+    {\n+        'page': 'layers/core.md',\n+        'classes': [\n+            core.Dense,\n+            core.Activation,\n+            core.Dropout,\n+            core.Flatten,\n+            core.Reshape,\n+            core.Permute,\n+            core.RepeatVector,\n+            topology.Merge,\n+            core.Lambda,\n+            core.ActivityRegularization,\n+            core.Masking,\n+            core.Highway,\n+            core.MaxoutDense,\n+            core.TimeDistributedDense,\n+        ],\n+    },\n+    {\n+        'page': 'layers/convolutional.md',\n+        'classes': [\n+            convolutional.Convolution1D,\n+            convolutional.Convolution2D,\n+            convolutional.Convolution3D,\n+            convolutional.MaxPooling1D,\n+            convolutional.MaxPooling2D,\n+            convolutional.MaxPooling3D,\n+            convolutional.AveragePooling1D,\n+            convolutional.AveragePooling2D,\n+            convolutional.AveragePooling3D,\n+            convolutional.UpSampling1D,\n+            convolutional.UpSampling2D,\n+            convolutional.UpSampling3D,\n+            convolutional.ZeroPadding1D,\n+            convolutional.ZeroPadding2D,\n+            convolutional.ZeroPadding3D,\n+        ],\n+    },\n+    {\n+        'page': 'layers/recurrent.md',\n+        'classes': [\n+            recurrent.Recurrent,\n+            recurrent.SimpleRNN,\n+            recurrent.GRU,\n+            recurrent.LSTM,\n+        ],\n+    },\n+    {\n+        'page': 'layers/embeddings.md',\n+        'classes': [\n+            embeddings.Embedding,\n+        ],\n+    },\n+    {\n+        'page': 'layers/normalization.md',\n+        'classes': [\n+            normalization.BatchNormalization,\n+        ],\n+    },\n+    {\n+        'page': 'layers/advanced-activations.md',\n+        'all_module_classes': [advanced_activations],\n+    },\n+    {\n+        'page': 'layers/noise.md',\n+        'all_module_classes': [noise],\n+    },\n+    {\n+        'page': 'layers/wrappers.md',\n+        'all_module_classes': [wrappers],\n+    },\n+\n+\n+    {\n+        'page': 'optimizers.md',\n+        'all_module_classes': [optimizers],\n+    },\n+    {\n+        'page': 'callbacks.md',\n+        'all_module_classes': [callbacks],\n+    },\n+    {\n+        'page': 'backend.md',\n+        'all_module_functions': [backend],\n+    },\n+]\n+\n+ROOT = 'http://keras.io/'\n \n \n def get_earliest_class_that_defined_member(member, cls):\n@@ -67,8 +229,8 @@ def get_classes_ancestors(classes):\n         return filtered_ancestors\n \n \n-def get_method_signature(method):\n-    signature = inspect.getargspec(method)\n+def get_function_signature(function):\n+    signature = inspect.getargspec(function)\n     defaults = signature.defaults\n     args = signature.args[1:]\n     if defaults:\n@@ -76,7 +238,7 @@ def get_method_signature(method):\n         args = args[:-len(defaults)]\n     else:\n         kwargs = []\n-    st = '%s.%s(' % (method.__module__, method.__name__)\n+    st = '%s.%s(' % (function.__module__, function.__name__)\n     for a in args:\n         st += str(a) + ', '\n     for a, v in kwargs:\n@@ -91,6 +253,17 @@ def get_method_signature(method):\n         return st + ')'\n \n \n+def get_class_signature(cls):\n+    try:\n+        class_signature = get_function_signature(cls.__init__)\n+        class_signature = class_signature.replace('__init__', cls.__name__)\n+    except:\n+        # in case the class inherits from object and does not\n+        # define __init__\n+        class_signature = cls.__module__ + '.' + cls.__name__ + '()'\n+    return class_signature\n+\n+\n def class_to_docs_link(cls):\n     module_name = cls.__module__\n     assert module_name[:6] == 'keras.'\n@@ -131,7 +304,7 @@ def process_class_docstring(docstring):\n     return docstring\n \n \n-def process_method_docstring(docstring):\n+def process_function_docstring(docstring):\n     docstring = re.sub(r'\\n        # (.*)\\n',\n                        r'\\n        __\\1__\\n\\n',\n                        docstring)\n@@ -160,101 +333,175 @@ for subdir, dirs, fnames in os.walk('templates'):\n             shutil.copy(fpath, new_fpath)\n \n print('Starting autogeneration.')\n-covered_so_far = set()\n-for module, module_name in MODULES:\n-    class_pages = []\n+for page_data in PAGES:\n+    blocks = []\n+    classes = page_data.get('classes', [])\n+    for module in page_data.get('all_module_classes', []):\n+        module_classes = []\n         for name in dir(module):\n-        if name in SKIP:\n-            continue\n-        if name[0] == '_':\n+            if name[0] == '_' or name in EXCLUDE:\n                 continue\n             module_member = getattr(module, name)\n-        if module_member in covered_so_far:\n-            continue\n             if inspect.isclass(module_member):\n                 cls = module_member\n-            if cls.__module__ == module_name:\n+                if cls.__module__ == module.__name__:\n+                    if cls not in module_classes:\n+                        module_classes.append(cls)\n+        module_classes.sort(key=lambda x: id(x))\n+        classes += module_classes\n \n-                try:\n-                    class_signature = get_method_signature(cls.__init__)\n-                    class_signature = class_signature.replace('__init__', cls.__name__)\n-                except:\n-                    # in case the class inherits from object and does not\n-                    # define __init__\n-                    class_signature = module_name + '.' + cls.__name__ + '()'\n-\n-                methods = []\n-                methods_not_defined_here = []\n-                for name in dir(cls):\n-                    if name in SKIP:\n-                        continue\n-                    if name[0] == '_':\n-                        continue\n-                    cls_member = getattr(cls, name)\n-                    if inspect.ismethod(cls_member):\n-                        method = cls_member\n-                        signature = inspect.getargspec(method)\n-                        defaults = signature.defaults\n-                        args = signature.args[1:]\n-                        if defaults:\n-                            kwargs = zip(args[-len(defaults):], defaults)\n-                            args = args[:-len(defaults)]\n-                        else:\n-                            kwargs = []\n-\n-                        defined_by = get_earliest_class_that_defined_member(method.__name__, cls)\n-                        if cls == defined_by:\n-                            methods.append(method)\n-                        else:\n-                            methods_not_defined_here.append((method, defined_by))\n-\n-                blocks = []\n-                blocks.append('<span style=\"float:right;\">' + class_to_source_link(cls) + '</span>')\n-                blocks.append('# ' + cls.__name__ + '\\n')\n-                blocks.append(code_snippet(class_signature))\n+    for cls in classes:\n+        subblocks = []\n+        signature = get_class_signature(cls)\n+        subblocks.append('<span style=\"float:right;\">' + class_to_source_link(cls) + '</span>')\n+        subblocks.append('### ' + cls.__name__ + '\\n')\n+        subblocks.append(code_snippet(signature))\n         docstring = cls.__doc__\n         if docstring:\n-                    blocks.append(process_class_docstring(docstring))\n+            subblocks.append(process_class_docstring(docstring))\n+        blocks.append('\\n'.join(subblocks))\n \n-                if cls.__name__ in INCLUDE_METHODS_FOR:\n-                    if methods or methods_not_defined_here:\n-                        blocks.append('### Methods\\n')\n-                        for method in methods:\n-                            signature = get_method_signature(method)\n-                            signature = signature.replace(module_name + '.', '')\n-                            blocks.append(code_snippet(signature))\n-                            docstring = method.__doc__\n+    functions = page_data.get('functions', [])\n+    for module in page_data.get('all_module_functions', []):\n+        module_functions = []\n+        for name in dir(module):\n+            if name[0] == '_' or name in EXCLUDE:\n+                continue\n+            module_member = getattr(module, name)\n+            if inspect.isclass(module_member):\n+                function = module_member\n+                if function.__module__ == module.__name__:\n+                    if function not in module_functions:\n+                        module_functions.append(function)\n+        module_functions.sort(key=lambda x: id(x))\n+        functions += module_functions\n+\n+    for function in functions:\n+        subblocks = []\n+        signature = get_function_signature(function)\n+        signature = signature.replace(function.__module__ + '.', '')\n+        subblocks.append('### ' + function.im_class.__name__ + '.' + function.__name__ + '\\n')\n+        subblocks.append(code_snippet(signature))\n+        docstring = function.__doc__\n         if docstring:\n-                                blocks.append(process_method_docstring(docstring))\n-                        for method, defined_by in methods_not_defined_here:\n-                            signature = get_method_signature(method)\n-                            method_module_name = method.__module__\n-                            signature = signature.replace(method_module_name + '.', '')\n-                            link = '[' + defined_by.__name__ + '](' + class_to_docs_link(defined_by) + ')'\n-                            blocks.append(code_snippet(signature))\n-                            blocks.append('Defined by ' + link + '.\\n')\n-\n-                mkdown = '\\n'.join(blocks)\n-                class_pages.append((id(cls), mkdown))\n-                covered_so_far.add(module_member)\n-\n-    class_pages.sort(key=lambda x: x[0])\n-    class_pages = [x[1] for x in class_pages]\n-    module_page = '\\n----\\n\\n'.join(class_pages)\n+            subblocks.append(process_function_docstring(docstring))\n+        blocks.append('\\n\\n'.join(subblocks))\n \n+    mkdown = '\\n----\\n\\n'.join(blocks)\n     # save module page.\n     # Either insert content into existing page,\n     # or create page otherwise\n-    path = 'sources/' + module_name.replace('.', '/')[6:] + '.md'\n+    page_name = page_data['page']\n+    path = os.path.join('sources', page_name)\n     if os.path.exists(path):\n         template = open(path).read()\n         assert '{{autogenerated}}' in template, ('Template found for ' + path +\n                                                  ' but missing {{autogenerated}} tag.')\n-        module_page = template.replace('{{autogenerated}}', module_page)\n+        mkdown = template.replace('{{autogenerated}}', mkdown)\n         print('...inserting autogenerated content into template:', path)\n     else:\n         print('...creating new page with autogenerated content:', path)\n     subdir = os.path.dirname(path)\n     if not os.path.exists(subdir):\n         os.makedirs(subdir)\n-    open(path, 'w').write(module_page)\n+    open(path, 'w').write(mkdown)\n+\n+\n+# covered_so_far = set()\n+# for module, module_name in MODULES:\n+#     class_pages = []\n+#     for name in dir(module):\n+#         if name in SKIP:\n+#             continue\n+#         if name[0] == '_':\n+#             continue\n+#         module_member = getattr(module, name)\n+#         if module_member in covered_so_far:\n+#             continue\n+#         if inspect.isclass(module_member):\n+#             cls = module_member\n+#             if cls.__module__ == module_name:\n+\n+#                 try:\n+#                     class_signature = get_function_signature(cls.__init__)\n+#                     class_signature = class_signature.replace('__init__', cls.__name__)\n+#                 except:\n+#                     # in case the class inherits from object and does not\n+#                     # define __init__\n+#                     class_signature = module_name + '.' + cls.__name__ + '()'\n+\n+#                 functions = []\n+#                 functions_not_defined_here = []\n+#                 for name in dir(cls):\n+#                     if name in SKIP:\n+#                         continue\n+#                     if name[0] == '_':\n+#                         continue\n+#                     cls_member = getattr(cls, name)\n+#                     if inspect.isfunction(cls_member):\n+#                         function = cls_member\n+#                         signature = inspect.getargspec(function)\n+#                         defaults = signature.defaults\n+#                         args = signature.args[1:]\n+#                         if defaults:\n+#                             kwargs = zip(args[-len(defaults):], defaults)\n+#                             args = args[:-len(defaults)]\n+#                         else:\n+#                             kwargs = []\n+\n+#                         defined_by = get_earliest_class_that_defined_member(function.__name__, cls)\n+#                         if cls == defined_by:\n+#                             functions.append(function)\n+#                         else:\n+#                             functions_not_defined_here.append((function, defined_by))\n+\n+#                 blocks = []\n+#                 blocks.append('<span style=\"float:right;\">' + class_to_source_link(cls) + '</span>')\n+#                 blocks.append('# ' + cls.__name__ + '\\n')\n+#                 blocks.append(code_snippet(class_signature))\n+#                 docstring = cls.__doc__\n+#                 if docstring:\n+#                     blocks.append(process_class_docstring(docstring))\n+\n+#                 if cls.__name__ in INCLUDE_functionS_FOR:\n+#                     if functions or functions_not_defined_here:\n+#                         blocks.append('### functions\\n')\n+#                         for function in functions:\n+#                             signature = get_function_signature(function)\n+#                             signature = signature.replace(module_name + '.', '')\n+#                             blocks.append(code_snippet(signature))\n+#                             docstring = function.__doc__\n+#                             if docstring:\n+#                                 blocks.append(process_function_docstring(docstring))\n+#                         for function, defined_by in functions_not_defined_here:\n+#                             signature = get_function_signature(function)\n+#                             function_module_name = function.__module__\n+#                             signature = signature.replace(function_module_name + '.', '')\n+#                             link = '[' + defined_by.__name__ + '](' + class_to_docs_link(defined_by) + ')'\n+#                             blocks.append(code_snippet(signature))\n+#                             blocks.append('Defined by ' + link + '.\\n')\n+\n+#                 mkdown = '\\n'.join(blocks)\n+#                 class_pages.append((id(cls), mkdown))\n+#                 covered_so_far.add(module_member)\n+\n+#     class_pages.sort(key=lambda x: x[0])\n+#     class_pages = [x[1] for x in class_pages]\n+#     module_page = '\\n----\\n\\n'.join(class_pages)\n+\n+#     # save module page.\n+#     # Either insert content into existing page,\n+#     # or create page otherwise\n+#     path = 'sources/' + module_name.replace('.', '/')[6:] + '.md'\n+#     if os.path.exists(path):\n+#         template = open(path).read()\n+#         assert '{{autogenerated}}' in template, ('Template found for ' + path +\n+#                                                  ' but missing {{autogenerated}} tag.')\n+#         module_page = template.replace('{{autogenerated}}', module_page)\n+#         print('...inserting autogenerated content into template:', path)\n+#     else:\n+#         print('...creating new page with autogenerated content:', path)\n+#     subdir = os.path.dirname(path)\n+#     if not os.path.exists(subdir):\n+#         os.makedirs(subdir)\n+#     open(path, 'w').write(module_page)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cc985c3a9ca2e01a51ce461acef2b6a61901d44f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 81 | Lines Deleted: 146 | Files Changed: 6 | Hunks: 31 | Methods Changed: 18 | Complexity Δ (Sum/Max): -12/1 | Churn Δ: 227 | Churn Cumulative: 22069 | Contributors (this commit): 77 | Commits (past 90d): 143 | Contributors (cumulative): 113 | DMM Complexity: 0.21212121212121213\n\nDIFF:\n@@ -78,6 +78,9 @@ from keras import regularizers\n \n EXCLUDE = {\n     'Optimizer',\n+    'Wrapper',\n+    'get_session',\n+    'set_session',\n }\n \n PAGES = [\n@@ -229,10 +232,13 @@ def get_classes_ancestors(classes):\n         return filtered_ancestors\n \n \n-def get_function_signature(function):\n+def get_function_signature(function, method=True):\n     signature = inspect.getargspec(function)\n     defaults = signature.defaults\n+    if method:\n         args = signature.args[1:]\n+    else:\n+        args = signature.args\n     if defaults:\n         kwargs = zip(args[-len(defaults):], defaults)\n         args = args[:-len(defaults)]\n@@ -308,6 +314,9 @@ def process_function_docstring(docstring):\n     docstring = re.sub(r'\\n    # (.*)\\n',\n                        r'\\n    __\\1__\\n\\n',\n                        docstring)\n+    docstring = re.sub(r'\\n        # (.*)\\n',\n+                       r'\\n        __\\1__\\n\\n',\n+                       docstring)\n \n     docstring = re.sub(r'    ([^\\s\\\\]+):(.*)\\n',\n                        r'    - __\\1__:\\2\\n',\n@@ -368,9 +377,9 @@ for page_data in PAGES:\n             if name[0] == '_' or name in EXCLUDE:\n                 continue\n             module_member = getattr(module, name)\n-            if inspect.isclass(module_member):\n+            if inspect.isfunction(module_member):\n                 function = module_member\n-                if function.__module__ == module.__name__:\n+                if module.__name__ in function.__module__:\n                     if function not in module_functions:\n                         module_functions.append(function)\n         module_functions.sort(key=lambda x: id(x))\n@@ -378,9 +387,9 @@ for page_data in PAGES:\n \n     for function in functions:\n         subblocks = []\n-        signature = get_function_signature(function)\n+        signature = get_function_signature(function, method=False)\n         signature = signature.replace(function.__module__ + '.', '')\n-        subblocks.append('### ' + function.im_class.__name__ + '.' + function.__name__ + '\\n')\n+        subblocks.append('### ' + function.__name__ + '\\n')\n         subblocks.append(code_snippet(signature))\n         docstring = function.__doc__\n         if docstring:\n\n@@ -62,7 +62,7 @@ def variable(value, dtype=_FLOATX, name=None):\n def placeholder(shape=None, ndim=None, dtype=_FLOATX, name=None):\n     '''Instantiates a placeholder.\n \n-    # Arguments:\n+    # Arguments\n         shape: shape of the placeholder\n             (integer tuple, may include None entries).\n         ndim: number of axes of the tensor.\n@@ -71,7 +71,7 @@ def placeholder(shape=None, ndim=None, dtype=_FLOATX, name=None):\n         dtype: placeholder type.\n         name: optional name string for the placeholder.\n \n-    # Returns:\n+    # Returns\n         Placeholder tensor instance.\n     '''\n     if not shape:\n@@ -641,6 +641,7 @@ def rnn(step_function, inputs, initial_states,\n \n     # Returns\n         A tuple (last_output, outputs, new_states).\n+\n         last_output: the latest output of the rnn, of shape (samples, ...)\n         outputs: tensor with shape (samples, time, ...) where each\n             entry outputs[s, t] is the output of the step function\n\n@@ -407,9 +407,9 @@ class Model(Container):\n \n         # Arguments\n             optimizer: str (name of optimizer) or optimizer object.\n-                See [optimizers](optimizers.md).\n+                See [optimizers](/optimizers).\n             loss: str (name of objective function) or objective function.\n-                See [objectives](objectives.md).\n+                See [objectives](/objectives).\n                 If the model has multiple outputs, you can use a different loss\n                 on each output by passing a dictionary or a list of objectives.\n             metrics: list of metrics to be evaluated by the model\n@@ -916,7 +916,7 @@ class Model(Container):\n             nb_epoch: integer, the number of times to iterate over the training data arrays.\n             verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = verbose, 2 = one log line per epoch.\n             callbacks: list of callbacks to be called during training.\n-                See [callbacks](callbacks.md).\n+                See [callbacks](/callbacks).\n             validation_split: float between 0 and 1:\n                 fraction of the training data to be used as validation data.\n                 The model will set apart this fraction of the training data,\n\n@@ -18,14 +18,32 @@ from ..regularizers import ActivityRegularizer\n \n \n class Masking(Layer):\n-    '''Masks an input sequence by using a mask value to identify padding.\n+    '''Masks an input sequence by using a mask value to\n+    identify timesteps to be skipped.\n \n-    This layer copies the input to the output layer with identified padding\n-    replaced with 0s and creates an output mask in the process.\n+    For each timestep in the input tensor (dimension #1 in the tensor),\n+    if all values in the input tensor at that timestep\n+    are equal to `mask_value`, then the timestep will masked (skipped)\n+    in all downstream layers (as long as they support masking).\n \n-    At each timestep, if the values all equal `mask_value`,\n-    then the corresponding mask value for the timestep is 0 (skipped),\n-    otherwise it is 1.\n+    If any downstream layer does not support masking yet receives such\n+    an input mask, an exception will be raised.\n+\n+    # Example\n+\n+    Consider a Numpy data array `x` of shape `(samples, timesteps, features)`,\n+    to be fed to a LSTM layer.\n+    You want to mask timestep #3 and #5 because you lack data for\n+    these timesteps. You can:\n+\n+        - set `x[:, 3, :] = 0.` and `x[:, 5, :] = 0.`\n+        - insert a `Masking` layer with `mask_value=0.` before the LSTM layer:\n+\n+    ```python\n+        model = Sequential()\n+        model.add(Masking(mask_value=0., input_shape=(timesteps, features)))\n+        model.add(LSTM(32))\n+    ```\n     '''\n     def __init__(self, mask_value=0., **kwargs):\n         self.supports_masking = True\n\n@@ -293,9 +293,9 @@ class Sequential(Model):\n \n         # Arguments\n             optimizer: str (name of optimizer) or optimizer object.\n-                See [optimizers](optimizers.md).\n+                See [optimizers](/optimizers).\n             loss: str (name of objective function) or objective function.\n-                See [objectives](objectives.md).\n+                See [objectives](/objectives).\n             metrics: list of metrics to be evaluated by the model\n                 during training and testing.\n                 Typically you will use `metrics=['accuracy']`.\n@@ -347,7 +347,7 @@ class Sequential(Model):\n                 1 for progress bar logging, 2 for one log line per epoch.\n             callbacks: list of `keras.callbacks.Callback` instances.\n                 List of callbacks to apply during training.\n-                See [callbacks](callbacks.md).\n+                See [callbacks](/callbacks).\n             validation_split: float (0. < x < 1).\n                 Fraction of the data to use as held-out validation data.\n             validation_data: tuple (X, y) to be used as held-out\n\n@@ -1,7 +1,3 @@\n-import itertools\n-from keras.layers.containers import Graph, Sequential\n-from keras.layers.core import Merge\n-\n try:\n     # pydot-ng is a fork of pydot that is better maintained\n     import pydot_ng as pydot\n@@ -9,55 +5,28 @@ except ImportError:\n     # fall back on pydot if necessary\n     import pydot\n if not pydot.find_graphviz():\n-    raise RuntimeError(\"Failed to import pydot. You must install pydot\"\n-                       \" and graphviz for `pydotprint` to work.\")\n+    raise RuntimeError('Failed to import pydot. You must install pydot'\n+                       ' and graphviz for `pydotprint` to work.')\n \n \n-def layer_typename(layer):\n-    return type(layer).__module__ + \".\" + type(layer).__name__\n+def model_to_dot(model, show_shapes=False):\n+    dot = pydot.Dot()\n+    dot.set('rankdir', 'TB')\n+    dot.set('concentrate', True)\n+    dot.set_node_defaults(shape='record')\n \n+    if model.__class__.__name__ == 'Sequential':\n+        if not model.built:\n+            model.build()\n+        model = model.model\n+    layers = model.layers\n \n-def get_layer_to_name(model):\n-    \"\"\"Returns a dict mapping layer to their name in the model\"\"\"\n-    if not isinstance(model, Graph):\n-        return {}\n-    else:\n-        node_to_name = itertools.chain(\n-            model.nodes.items(), model.inputs.items(), model.outputs.items()\n-        )\n-        return {v: k for k, v in node_to_name}\n+    # first, populate the nodes of the graph\n+    for layer in layers:\n+        layer_id = str(id(layer))\n+        label = str(layer.name) + ' (' + layer.__class__.__name__ + ')'\n \n-\n-class ModelToDot(object):\n-    \"\"\"\n-    This is a helper class which visits a keras model (Sequential or Graph) and\n-    returns a pydot.Graph representation.\n-\n-    This is implemented as a class because we need to maintain various states.\n-\n-    Use it as ```ModelToDot()(model)```\n-\n-    Keras models can have an arbitrary number of inputs and outputs. A given\n-    layer can have multiple inputs but has a single output. We therefore\n-    explore the model by starting at its output and crawling \"up\" the tree.\n-    \"\"\"\n-    def _pydot_node_for_layer(self, layer, label):\n-        \"\"\"\n-        Returns the pydot.Node corresponding to the given layer.\n-        `label` specify the name of the layer (only used if the layer isn't yet\n-            associated with a pydot.Node)\n-        \"\"\"\n-        # Check if this already exists (will be the case for nodes that\n-        # serve as input to more than one layer)\n-        if layer in self.layer_to_pydotnode:\n-            node = self.layer_to_pydotnode[layer]\n-        else:\n-            layer_id = 'layer%d' % self.idgen\n-            self.idgen += 1\n-\n-            label = label + \" (\" + layer_typename(layer) + \")\"\n-\n-            if self.show_shape:\n+        if show_shapes:\n             # Build the label that will actually contain a table with the\n             # input/output\n             outputlabels = str(layer.output_shape)\n@@ -68,87 +37,25 @@ class ModelToDot(object):\n                     [str(ishape) for ishape in layer.input_shapes])\n             else:\n                 inputlabels = ''\n-                label = \"%s\\n|{input:|output:}|{{%s}|{%s}}\" % (\n-                        label, inputlabels, outputlabels)\n+            label = '%s\\n|{input:|output:}|{{%s}|{%s}}' % (label, inputlabels, outputlabels)\n \n         node = pydot.Node(layer_id, label=label)\n-            self.g.add_node(node)\n-            self.layer_to_pydotnode[layer] = node\n-        return node\n+        dot.add_node(node)\n \n-    def _process_layer(self, layer, layer_to_name=None, connect_to=None):\n-        \"\"\"\n-        Process a layer, adding its node to the graph and creating edges to its\n-        outputs.\n-\n-        `connect_to` specify where the output of the current layer will be\n-            connected\n-        `layer_to_name` is a dict mapping layer to their name in the Graph\n-            model. Should be {} when processing a Sequential model\n-        \"\"\"\n-        # The layer can be a container layer, in which case we can recurse\n-        is_graph = isinstance(layer, Graph)\n-        is_seq = isinstance(layer, Sequential)\n-        if self.recursive and (is_graph or is_seq):\n-            # We got a container layer, recursively transform it\n-            if is_graph:\n-                child_layers = layer.outputs.values()\n-            else:\n-                child_layers = [layer.layers[-1]]\n-            for l in child_layers:\n-                self._process_layer(l, layer_to_name=get_layer_to_name(layer),\n-                                    connect_to=connect_to)\n-        else:\n-            # This is a simple layer.\n-            label = layer_to_name.get(layer, '')\n-            layer_node = self._pydot_node_for_layer(layer, label=label)\n-\n-            if connect_to is not None:\n-                self.g.add_edge(pydot.Edge(layer_node, connect_to))\n-\n-            # Proceed upwards to the parent(s). Only Merge layers have more\n-            # than one parent\n-            if isinstance(layer, Merge):  # Merge layer\n-                for l in layer.layers:\n-                    self._process_layer(l, layer_to_name,\n-                                        connect_to=layer_node)\n-            elif hasattr(layer, 'previous') and layer.previous is not None:\n-                self._process_layer(layer.previous, layer_to_name,\n-                                    connect_to=layer_node)\n-\n-    def __call__(self, model, recursive=True, show_shape=False,\n-                 connect_to=None):\n-        self.idgen = 0\n-        # Maps keras layer to the pydot.Node representing them\n-        self.layer_to_pydotnode = {}\n-        self.recursive = recursive\n-        self.show_shape = show_shape\n-\n-        self.g = pydot.Dot()\n-        self.g.set('rankdir', 'TB')\n-        self.g.set('concentrate', True)\n-        self.g.set_node_defaults(shape='record')\n-\n-        if hasattr(model, 'outputs'):\n-            # Graph\n-            for name, l in model.outputs.items():\n-                self._process_layer(l, get_layer_to_name(model),\n-                                    connect_to=connect_to)\n-        else:\n-            # Sequential container\n-            self._process_layer(model.layers[-1], {}, connect_to=connect_to)\n-\n-        return self.g\n+    # second, add the edges\n+    for layer in layers:\n+        layer_id = str(id(layer))\n+        for i, node in enumerate(layer.inbound_nodes):\n+            node_key = layer.name + '_ib-' + str(i)\n+            if node_key in model.container_nodes:\n+                # add edges\n+                for inbound_layer in node.inbound_layers:\n+                    inbound_layer_id = str(id(inbound_layer))\n+                    layer_id = str(id(layer))\n+                    dot.add_edge(pydot.Edge(inbound_layer_id, layer_id))\n+    return dot\n \n \n-def to_graph(model, **kwargs):\n-    \"\"\"\n-    `recursive` controls whether we recursively explore container layers\n-    `show_shape` controls whether the shape is shown in the graph\n-    \"\"\"\n-    return ModelToDot()(model, **kwargs)\n-\n-\n-def plot(model, to_file='model.png', **kwargs):\n-    graph = to_graph(model, **kwargs)\n-    graph.write_png(to_file)\n+def plot(model, to_file='model.png', show_shapes=False):\n+    dot = model_to_dot(model, show_shapes)\n+    dot.write_png(to_file)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dd766c68d929c394cb17dd92b9cca20cbd7d32c4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 984 | Contributors (this commit): 12 | Commits (past 90d): 23 | Contributors (cumulative): 17 | DMM Complexity: 0.25\n\nDIFF:\n@@ -23,7 +23,7 @@ class LeakyReLU(Layer):\n     '''\n     def __init__(self, alpha=0.3, **kwargs):\n         self.supports_masking = True\n-        self.alpha = K.cast_to_floatx(alpha)\n+        self.alpha = alpha\n         super(LeakyReLU, self).__init__(**kwargs)\n \n     def call(self, x, mask=None):\n\n@@ -35,7 +35,7 @@ def get_test_data(nb_train=1000, nb_test=500, input_shape=(10,),\n \n \n def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n-               input_data=None, expected_output=None):\n+               input_data=None, expected_output=None, expected_output_dtype=None):\n     '''Test routine for a layer with a single input tensor\n     and single output tensor.\n     '''\n@@ -47,6 +47,9 @@ def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n     elif input_shape is None:\n         input_shape = input_data.shape\n \n+    if expected_output_dtype is None:\n+        expected_output_dtype = input_dtype\n+\n     # instantiation\n     layer = layer_cls(**kwargs)\n \n@@ -62,6 +65,8 @@ def layer_test(layer_cls, kwargs={}, input_shape=None, input_dtype=None,\n     # test in functional API\n     x = Input(shape=input_shape[1:], dtype=input_dtype)\n     y = layer(x)\n+    assert K.dtype(y) == expected_output_dtype\n+\n     model = Model(input=x, output=y)\n     model.compile('rmsprop', 'mse')\n \n\n@@ -1,13 +1,15 @@\n import pytest\n from keras.utils.test_utils import layer_test\n from keras.layers.embeddings import Embedding\n+import keras.backend as K\n \n \n def test_embedding():\n     layer_test(Embedding,\n                kwargs={'output_dim': 4., 'input_dim': 10, 'input_length': 2},\n                input_shape=(3, 2),\n-               input_dtype='int32')\n+               input_dtype='int32',\n+               expected_output_dtype=K.floatx())\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3b196feda5cce3b362353b0647363d85ef4d3e43", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2590 | Contributors (this commit): 5 | Commits (past 90d): 19 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -31,7 +31,7 @@ class InputSpec(object):\n     Every layer should expose (if appropriate) an `input_spec` attribute:\n     a list of instances of InputSpec (one per input tensor).\n \n-    A None entry in a shaple is compatible with any dimension,\n+    A None entry in a shape is compatible with any dimension,\n     a None shape is compatible with any shape.\n     '''\n     def __init__(self, dtype=None, shape=None, ndim=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3db1f132a7b992969365cdfc2445e7774f53a878", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 2592 | Contributors (this commit): 5 | Commits (past 90d): 20 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -409,8 +409,6 @@ class Layer(object):\n \n         # Arguments\n             x: input tensor, or list/tuple of input tensors.\n-            train: boolean, whether the layer should behave\n-                in train behavior (e.g. dropout on) or test behavior.\n             mask: a masking tensor (or list of tensors). Used mainly in RNNs.\n \n         # Returns:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#50a0d1cad4a83b13026f9e75665979e728140fbc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 2598 | Contributors (this commit): 5 | Commits (past 90d): 21 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -503,14 +503,14 @@ class Layer(object):\n                          node_indices=None, tensor_indices=None):\n         '''\n         # Arguments:\n-            inbound_layer: can be a layer instance\n+            inbound_layers: can be a layer instance\n                 or a list/tuple of layer instances.\n-            node_index: integer (or list of integers).\n+            node_indices: integer (or list of integers).\n                 The input layer might have a number of\n                 parallel output streams;\n                 this is the index of the stream (in the input layer)\n                 where to connect the current layer.\n-            tensor_index: integer or list of integers.\n+            tensor_indices: integer or list of integers.\n                 The output of the inbound node might be a list/tuple\n                 of tensor, and we might only be interested in one sepcific entry.\n                 This index allows you to specify the index of the entry in the output list\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
