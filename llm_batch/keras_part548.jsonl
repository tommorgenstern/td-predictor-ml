{"custom_id": "keras#54ff92a5d7c1b7edf84421a5ba32e9c91838ce5a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 15 | Files Changed: 5 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 21 | Churn Cumulative: 1674 | Contributors (this commit): 5 | Commits (past 90d): 49 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -15,7 +15,3 @@ def get(identifier):\n     if identifier == \"relu\":\n         return relu\n     return identifier\n-\n-\n-def serialize(activation):\n-    return activation.__name__\n\n@@ -42,8 +42,8 @@ def cast(x, dtype):\n     return convert_to_tensor(x, dtype=dtype)\n \n \n-def cond(pred, true_fun, false_fun):\n-    return jax.lax.cond(pred, true_fn=true_fun, false_fun=false_fun)\n+def cond(pred, true_fn, false_fn):\n+    return jax.lax.cond(pred, true_fn=true_fn, false_fun=false_fn)\n \n \n def name_scope(name):\n\n@@ -232,8 +232,8 @@ def cast(x, dtype):\n     return tf.cast(x, dtype=dtype)\n \n \n-def cond(pred, true_fun, false_fun):\n-    return tf.cond(pred, true_fn=true_fun, false_fun=false_fun)\n+def cond(pred, true_fn, false_fn):\n+    return tf.cond(pred, true_fn=true_fn, false_fn=false_fn)\n \n \n def name_scope(name):\n\n@@ -61,15 +61,10 @@ class Dense(Layer):\n         # TODO\n         config = {\n             \"units\": self.units,\n-            \"activation\": activations.serialize(self.activation),\n             \"use_bias\": self.use_bias,\n             \"kernel_initializer\": initializers.serialize(\n                 self.kernel_initializer\n             ),\n             \"bias_initializer\": initializers.serialize(self.bias_initializer),\n-            \"kernel_regularizer\": regularizers.serialize(self.kernel_regularizer),\n-            \"bias_regularizer\": regularizers.serialize(self.bias_regularizer),\n-            \"kernel_constraint\": constraints.serialize(self.kernel_constraint),\n-            \"bias_constraint\": constraints.serialize(self.bias_constraint),\n         }\n         return {**base_config, **config}\n\n@@ -146,7 +146,7 @@ def serialize_keras_object(obj):\n             \"class_name\": \"__tensor__\",\n             \"config\": {\n                 \"value\": np.array(obj).tolist(),\n-                \"dtype\": backend.standardize_dtype(obj.dtype),\n+                \"dtype\": str(obj.dtype),\n             },\n         }\n     if type(obj).__module__ == np.__name__:\n@@ -155,7 +155,7 @@ def serialize_keras_object(obj):\n                 \"class_name\": \"__numpy__\",\n                 \"config\": {\n                     \"value\": obj.tolist(),\n-                    \"dtype\": backend.standardize_dtype(obj.dtype),\n+                    \"dtype\": backend.standardize_dytpe(obj.dtype),\n                 },\n             }\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#53a858cb7fb4f5ad7c5352851fa1ca9d2961a839", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 926 | Contributors (this commit): 4 | Commits (past 90d): 21 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,3 +15,7 @@ def get(identifier):\n     if identifier == \"relu\":\n         return relu\n     return identifier\n+\n+\n+def serialize(activation):\n+    return activation.__name__\n\n@@ -61,10 +61,15 @@ class Dense(Layer):\n         # TODO\n         config = {\n             \"units\": self.units,\n+            \"activation\": activations.serialize(self.activation),\n             \"use_bias\": self.use_bias,\n             \"kernel_initializer\": initializers.serialize(\n                 self.kernel_initializer\n             ),\n             \"bias_initializer\": initializers.serialize(self.bias_initializer),\n+            \"kernel_regularizer\": regularizers.serialize(self.kernel_regularizer),\n+            \"bias_regularizer\": regularizers.serialize(self.bias_regularizer),\n+            \"kernel_constraint\": constraints.serialize(self.kernel_constraint),\n+            \"bias_constraint\": constraints.serialize(self.bias_constraint),\n         }\n         return {**base_config, **config}\n\n@@ -146,7 +146,7 @@ def serialize_keras_object(obj):\n             \"class_name\": \"__tensor__\",\n             \"config\": {\n                 \"value\": np.array(obj).tolist(),\n-                \"dtype\": str(obj.dtype),\n+                \"dtype\": backend.standardize_dtype(obj.dtype),\n             },\n         }\n     if type(obj).__module__ == np.__name__:\n@@ -155,7 +155,7 @@ def serialize_keras_object(obj):\n                 \"class_name\": \"__numpy__\",\n                 \"config\": {\n                     \"value\": obj.tolist(),\n-                    \"dtype\": backend.standardize_dytpe(obj.dtype),\n+                    \"dtype\": backend.standardize_dtype(obj.dtype),\n                 },\n             }\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d6bcc56001b9a89629cbba80f314d3f95a9ac871", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 836 | Lines Deleted: 11 | Files Changed: 15 | Hunks: 19 | Methods Changed: 34 | Complexity Δ (Sum/Max): 88/62 | Churn Δ: 847 | Churn Cumulative: 4035 | Contributors (this commit): 7 | Commits (past 90d): 91 | Contributors (cumulative): 44 | DMM Complexity: 0.4367816091954023\n\nDIFF:\n@@ -15,7 +15,3 @@ def get(identifier):\n     if identifier == \"relu\":\n         return relu\n     return identifier\n-\n-\n-def serialize(activation):\n-    return activation.__name__\n\n@@ -5,6 +5,7 @@ from tensorflow import nest\n \n from keras_core.backend.common import KerasVariable\n from keras_core.backend.common import standardize_dtype\n+from keras_core.backend.jax import math\n from keras_core.backend.jax import nn\n from keras_core.backend.jax import numpy\n from keras_core.backend.jax import random\n@@ -50,6 +51,10 @@ def name_scope(name):\n     return jax.named_scope(name)\n \n \n+def vectorized_map(function, elements):\n+    return jax.vmap(function)(elements)\n+\n+\n class Variable(KerasVariable):\n     def __init__(self, value, dtype=None, trainable=True, name=None):\n         self.name = name or auto_name(self.__class__.__name__)\n\n@@ -0,0 +1,15 @@\n+import jax\n+\n+\n+def segment_sum(data, segment_ids, num_segments=None, sorted=False):\n+    return jax.ops.segment_sum(\n+        data, segment_ids, num_segments, indices_are_sorted=sorted\n+    )\n+\n+\n+def top_k(x, k, sorted=False):\n+    if sorted:\n+        return ValueError(\n+            \"Jax backend does not support `sorted=True` for `ops.top_k`\"\n+        )\n+    return jax.lax.top_k(x, k)\n\n@@ -103,3 +103,7 @@ def conv_transpose(\n ):\n     # TODO: Implement `conv_transpose`.\n     raise NotImplementedError\n+\n+\n+def one_hot(x, num_classes, axis=-1):\n+    return jnn.one_hot(x, num_classes, axis=axis)\n\n@@ -6,6 +6,7 @@ from keras_core.backend.common import standardize_dtype\n from keras_core.backend.keras_tensor import KerasTensor\n from keras_core.backend.stateless_scope import get_stateless_scope\n from keras_core.backend.stateless_scope import in_stateless_scope\n+from keras_core.backend.tensorflow import math\n from keras_core.backend.tensorflow import nn\n from keras_core.backend.tensorflow import numpy\n from keras_core.backend.tensorflow import random\n@@ -240,6 +241,10 @@ def name_scope(name):\n     return tf.name_scope(name)\n \n \n+def vectorized_map(function, elements):\n+    return tf.vectorized_map(function, elements)\n+\n+\n def compute_output_spec(fn, *args, **kwargs):\n     graph_name = auto_name(\"scratch_graph\")\n     with tf.__internal__.FuncGraph(graph_name).as_default():\n\n@@ -0,0 +1,12 @@\n+import tensorflow as tf\n+\n+\n+def segment_sum(data, segment_ids, num_segments=None, sorted=False):\n+    if sorted:\n+        return tf.math.segment_sum(data, segment_ids)\n+    else:\n+        return tf.math.unsorted_segment_sum(data, segment_ids, num_segments)\n+\n+\n+def top_k(x, k, sorted=False):\n+    return tf.math.top_k(x, k, sorted=sorted)\n\n@@ -390,3 +390,7 @@ def conv_transpose(\n         data_format=tf_data_format,\n         dilations=dilation_rate,\n     )\n+\n+\n+def one_hot(x, num_classes, axis=-1):\n+    return tf.one_hot(x, num_classes, axis=axis)\n\n@@ -61,15 +61,10 @@ class Dense(Layer):\n         # TODO\n         config = {\n             \"units\": self.units,\n-            \"activation\": activations.serialize(self.activation),\n             \"use_bias\": self.use_bias,\n             \"kernel_initializer\": initializers.serialize(\n                 self.kernel_initializer\n             ),\n             \"bias_initializer\": initializers.serialize(self.bias_initializer),\n-            \"kernel_regularizer\": regularizers.serialize(self.kernel_regularizer),\n-            \"bias_regularizer\": regularizers.serialize(self.bias_regularizer),\n-            \"kernel_constraint\": constraints.serialize(self.kernel_constraint),\n-            \"bias_constraint\": constraints.serialize(self.bias_constraint),\n         }\n         return {**base_config, **config}\n\n@@ -1,4 +1,5 @@\n from keras_core.api_export import keras_core_export\n+from keras_core.metrics.confusion_metrics import FalsePositives\n from keras_core.metrics.metric import Metric\n from keras_core.metrics.reduction_metrics import Mean\n from keras_core.metrics.reduction_metrics import MeanMetricWrapper\n\n@@ -0,0 +1,115 @@\n+from keras_core import backend\n+from keras_core import initializers\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.metrics import metrics_utils\n+from keras_core.metrics.metric import Metric\n+\n+\n+class _ConfusionMatrixConditionCount(Metric):\n+    \"\"\"Calculates the number of the given confusion matrix condition.\n+\n+    Args:\n+        confusion_matrix_cond: One of `metrics_utils.ConfusionMatrix`\n+            conditions.\n+        thresholds: (Optional) Defaults to 0.5. A float value or a python list /\n+            tuple of float threshold values in [0, 1]. A threshold is compared\n+            with prediction values to determine the truth value of predictions\n+            (i.e., above the threshold is `true`, below is `false`). One metric\n+            value is generated for each threshold value.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+    \"\"\"\n+\n+    def __init__(\n+        self, confusion_matrix_cond, thresholds=None, name=None, dtype=None\n+    ):\n+        super().__init__(name=name, dtype=dtype)\n+        self._confusion_matrix_cond = confusion_matrix_cond\n+        self.init_thresholds = thresholds\n+        self.thresholds = metrics_utils.parse_init_thresholds(\n+            thresholds, default_threshold=0.5\n+        )\n+        self._thresholds_distributed_evenly = (\n+            metrics_utils.is_evenly_distributed_thresholds(self.thresholds)\n+        )\n+        self.accumulator = self.add_variable(\n+            shape=(len(self.thresholds),),\n+            initializer=initializers.Zeros(),\n+            name=\"accumulator\",\n+        )\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        \"\"\"Accumulates the metric statistics.\n+\n+        Args:\n+            y_true: The ground truth values.\n+            y_pred: The predicted values.\n+            sample_weight: Optional weighting of each example. Defaults to 1.\n+                Can be a tensor whose rank is either 0, or the same rank as\n+                `y_true`, and must be broadcastable to `y_true`.\n+        \"\"\"\n+        return metrics_utils.update_confusion_matrix_variables(\n+            {self._confusion_matrix_cond: self.accumulator},\n+            y_true,\n+            y_pred,\n+            thresholds=self.thresholds,\n+            thresholds_distributed_evenly=self._thresholds_distributed_evenly,\n+            sample_weight=sample_weight,\n+        )\n+\n+    def result(self):\n+        if len(self.thresholds) == 1:\n+            result = self.accumulator[0]\n+        else:\n+            result = self.accumulator\n+        return backend.convert_to_tensor(result)\n+\n+    def get_config(self):\n+        config = {\"thresholds\": self.init_thresholds}\n+        base_config = super().get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+@keras_core_export(\"keras_core.metrics.FalsePositives\")\n+class FalsePositives(_ConfusionMatrixConditionCount):\n+    \"\"\"Calculates the number of false positives.\n+\n+    If `sample_weight` is given, calculates the sum of the weights of\n+    false positives. This metric creates one local variable, `accumulator`\n+    that is used to keep track of the number of false positives.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Args:\n+        thresholds: (Optional) Defaults to 0.5. A float value, or a Python\n+            list/tuple of float threshold values in [0, 1]. A threshold is\n+            compared with prediction values to determine the truth value of\n+            predictions (i.e., above the threshold is `true`, below is `false`).\n+            If used with a loss function that sets `from_logits=True` (i.e. no\n+            sigmoid applied to predictions), `thresholds` should be set to 0.\n+            One metric value is generated for each threshold value.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = keras_core.metrics.FalsePositives()\n+    >>> m.update_state([0, 1, 0, 0], [0, 0, 1, 1])\n+    >>> m.result()\n+    2.0\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 1, 0, 0], [0, 0, 1, 1], sample_weight=[0, 0, 1, 0])\n+    >>> m.result()\n+    1.0\n+    \"\"\"\n+\n+    def __init__(self, thresholds=None, name=None, dtype=None):\n+        super().__init__(\n+            confusion_matrix_cond=metrics_utils.ConfusionMatrix.FALSE_POSITIVES,\n+            thresholds=thresholds,\n+            name=name,\n+            dtype=dtype,\n+        )\n\n@@ -0,0 +1,103 @@\n+import numpy as np\n+from tensorflow.python.ops.numpy_ops import np_config\n+\n+from keras_core import metrics\n+from keras_core import testing\n+\n+np_config.enable_numpy_behavior()\n+\n+\n+class FalsePositivesTest(testing.TestCase):\n+    def test_config(self):\n+        fp_obj = metrics.FalsePositives(name=\"my_fp\", thresholds=[0.4, 0.9])\n+        self.assertEqual(fp_obj.name, \"my_fp\")\n+        self.assertLen(fp_obj.variables, 1)\n+        self.assertEqual(fp_obj.thresholds, [0.4, 0.9])\n+\n+        # Check save and restore config\n+        fp_obj2 = metrics.FalsePositives.from_config(fp_obj.get_config())\n+        self.assertEqual(fp_obj2.name, \"my_fp\")\n+        self.assertLen(fp_obj2.variables, 1)\n+        self.assertEqual(fp_obj2.thresholds, [0.4, 0.9])\n+\n+    def test_unweighted(self):\n+        fp_obj = metrics.FalsePositives()\n+\n+        y_true = np.array(\n+            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        )\n+        y_pred = np.array(\n+            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+        )\n+\n+        fp_obj.update_state(y_true, y_pred)\n+        result = fp_obj.result()\n+        self.assertAllClose(7.0, result)\n+\n+    def test_weighted(self):\n+        fp_obj = metrics.FalsePositives()\n+        y_true = np.array(\n+            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        )\n+        y_pred = np.array(\n+            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+        )\n+        sample_weight = np.array((1.0, 1.5, 2.0, 2.5))\n+        result = fp_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(14.0, result)\n+\n+    def test_unweighted_with_thresholds(self):\n+        fp_obj = metrics.FalsePositives(thresholds=[0.15, 0.5, 0.85])\n+\n+        y_pred = np.array(\n+            (\n+                (0.9, 0.2, 0.8, 0.1),\n+                (0.2, 0.9, 0.7, 0.6),\n+                (0.1, 0.2, 0.4, 0.3),\n+                (0, 1, 0.7, 0.3),\n+            )\n+        )\n+        y_true = np.array(\n+            ((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0), (1, 1, 1, 1))\n+        )\n+\n+        fp_obj.update_state(y_true, y_pred)\n+        result = fp_obj.result()\n+        self.assertAllClose([7.0, 4.0, 2.0], result)\n+\n+    def test_weighted_with_thresholds(self):\n+        fp_obj = metrics.FalsePositives(thresholds=[0.15, 0.5, 0.85])\n+\n+        y_pred = np.array(\n+            (\n+                (0.9, 0.2, 0.8, 0.1),\n+                (0.2, 0.9, 0.7, 0.6),\n+                (0.1, 0.2, 0.4, 0.3),\n+                (0, 1, 0.7, 0.3),\n+            )\n+        )\n+        y_true = np.array(\n+            ((0, 1, 1, 0), (1, 0, 0, 0), (0, 0, 0, 0), (1, 1, 1, 1))\n+        )\n+        sample_weight = (\n+            (1.0, 2.0, 3.0, 5.0),\n+            (7.0, 11.0, 13.0, 17.0),\n+            (19.0, 23.0, 29.0, 31.0),\n+            (5.0, 15.0, 10.0, 0),\n+        )\n+\n+        result = fp_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose([125.0, 42.0, 12.0], result)\n+\n+    def test_threshold_limit(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"Threshold values must be in \\[0, 1\\]. Received: \\[-1, 2\\]\",\n+        ):\n+            metrics.FalsePositives(thresholds=[-1, 0.5, 2])\n+\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            r\"Threshold values must be in \\[0, 1\\]. Received: \\[None\\]\",\n+        ):\n+            metrics.FalsePositives(thresholds=[None])\n\n@@ -0,0 +1,540 @@\n+from enum import Enum\n+\n+import numpy as np\n+\n+from keras_core import backend\n+from keras_core import operations as ops\n+from keras_core.losses.loss import squeeze_to_same_rank\n+from keras_core.utils.python_utils import to_list\n+\n+NEG_INF = -1e10\n+\n+\n+def assert_thresholds_range(thresholds):\n+    if thresholds is not None:\n+        invalid_thresholds = [\n+            t for t in thresholds if t is None or t < 0 or t > 1\n+        ]\n+        if invalid_thresholds:\n+            raise ValueError(\n+                \"Threshold values must be in [0, 1]. \"\n+                f\"Received: {invalid_thresholds}\"\n+            )\n+\n+\n+def parse_init_thresholds(thresholds, default_threshold=0.5):\n+    if thresholds is not None:\n+        assert_thresholds_range(to_list(thresholds))\n+    thresholds = to_list(\n+        default_threshold if thresholds is None else thresholds\n+    )\n+    return thresholds\n+\n+\n+class ConfusionMatrix(Enum):\n+    TRUE_POSITIVES = \"tp\"\n+    FALSE_POSITIVES = \"fp\"\n+    TRUE_NEGATIVES = \"tn\"\n+    FALSE_NEGATIVES = \"fn\"\n+\n+\n+def _update_confusion_matrix_variables_optimized(\n+    variables_to_update,\n+    y_true,\n+    y_pred,\n+    thresholds,\n+    multi_label=False,\n+    sample_weights=None,\n+    label_weights=None,\n+    thresholds_with_epsilon=False,\n+):\n+    \"\"\"Update confusion matrix variables with memory efficient alternative.\n+\n+    Note that the thresholds need to be evenly distributed within the list, eg,\n+    the diff between consecutive elements are the same.\n+\n+    To compute TP/FP/TN/FN, we are measuring a binary classifier\n+      C(t) = (predictions >= t)\n+    at each threshold 't'. So we have\n+      TP(t) = sum( C(t) * true_labels )\n+      FP(t) = sum( C(t) * false_labels )\n+\n+    But, computing C(t) requires computation for each t. To make it fast,\n+    observe that C(t) is a cumulative integral, and so if we have\n+      thresholds = [t_0, ..., t_{n-1}];  t_0 < ... < t_{n-1}\n+    where n = num_thresholds, and if we can compute the bucket function\n+      B(i) = Sum( (predictions == t), t_i <= t < t{i+1} )\n+    then we get\n+      C(t_i) = sum( B(j), j >= i )\n+    which is the reversed cumulative sum in ops.cumsum().\n+\n+    We can compute B(i) efficiently by taking advantage of the fact that\n+    our thresholds are evenly distributed, in that\n+      width = 1.0 / (num_thresholds - 1)\n+      thresholds = [0.0, 1*width, 2*width, 3*width, ..., 1.0]\n+    Given a prediction value p, we can map it to its bucket by\n+      bucket_index(p) = floor( p * (num_thresholds - 1) )\n+    so we can use ops.unsorted_segmenet_sum() to update the buckets in one\n+    pass.\n+\n+    Consider following example:\n+    y_true = [0, 0, 1, 1]\n+    y_pred = [0.1, 0.5, 0.3, 0.9]\n+    thresholds = [0.0, 0.5, 1.0]\n+    num_buckets = 2   # [0.0, 1.0], (1.0, 2.0]\n+    bucket_index(y_pred) = ops.floor(y_pred * num_buckets)\n+                         = ops.floor([0.2, 1.0, 0.6, 1.8])\n+                         = [0, 0, 0, 1]\n+    # The meaning of this bucket is that if any of the label is true,\n+    # then 1 will be added to the corresponding bucket with the index.\n+    # Eg, if the label for 0.2 is true, then 1 will be added to bucket 0. If the\n+    # label for 1.8 is true, then 1 will be added to bucket 1.\n+    #\n+    # Note the second item \"1.0\" is floored to 0, since the value need to be\n+    # strictly larger than the bucket lower bound.\n+    # In the implementation, we use ops.ceil() - 1 to achieve this.\n+    tp_bucket_value = ops.unsorted_segmenet_sum(true_labels, bucket_indices,\n+                                                   num_segments=num_thresholds)\n+                    = [1, 1, 0]\n+    # For [1, 1, 0] here, it means there is 1 true value contributed by bucket\n+    # 0, and 1 value contributed by bucket 1. When we aggregate them to\n+    # together, the result become [a + b + c, b + c, c], since large thresholds\n+    # will always contribute to the value for smaller thresholds.\n+    true_positive = ops.cumsum(tp_bucket_value, reverse=True)\n+                  = [2, 1, 0]\n+\n+    This implementation exhibits a run time and space complexity of O(T + N),\n+    where T is the number of thresholds and N is the size of predictions.\n+    Metrics that rely on standard implementation instead exhibit a complexity of\n+    O(T * N).\n+\n+    Args:\n+      variables_to_update: Dictionary with 'tp', 'fn', 'tn', 'fp' as valid keys\n+        and corresponding variables to update as values.\n+      y_true: A floating point `Tensor` whose shape matches `y_pred`. Will be\n+        cast to `bool`.\n+      y_pred: A floating point `Tensor` of arbitrary shape and whose values are\n+        in the range `[0, 1]`.\n+      thresholds: A sorted floating point `Tensor` with value in `[0, 1]`.\n+        It need to be evenly distributed (the diff between each element need to\n+        be the same).\n+      multi_label: Optional boolean indicating whether multidimensional\n+        prediction/labels should be treated as multilabel responses, or\n+        flattened into a single label. When True, the valus of\n+        `variables_to_update` must have a second dimension equal to the number\n+        of labels in y_true and y_pred, and those tensors must not be\n+        RaggedTensors.\n+      sample_weights: Optional `Tensor` whose rank is either 0, or the same rank\n+        as `y_true`, and must be broadcastable to `y_true` (i.e., all dimensions\n+        must be either `1`, or the same as the corresponding `y_true`\n+        dimension).\n+      label_weights: Optional tensor of non-negative weights for multilabel\n+        data. The weights are applied when calculating TP, FP, FN, and TN\n+        without explicit multilabel handling (i.e. when the data is to be\n+        flattened).\n+      thresholds_with_epsilon: Optional boolean indicating whether the leading\n+        and tailing thresholds has any epsilon added for floating point\n+        imprecisions.  It will change how we handle the leading and tailing\n+        bucket.\n+    \"\"\"\n+    num_thresholds = thresholds.shape.as_list()[0]\n+\n+    if sample_weights is None:\n+        sample_weights = 1.0\n+    else:\n+        sample_weights = ops.broadcast_to(\n+            ops.cast(sample_weights, dtype=y_pred.dtype), y_pred.shape\n+        )\n+        if not multi_label:\n+            sample_weights = ops.reshape(sample_weights, [-1])\n+    if label_weights is None:\n+        label_weights = 1.0\n+    else:\n+        label_weights = ops.expand_dims(label_weights, 0)\n+        label_weights = ops.broadcast_to(label_weights, y_pred.shape)\n+        if not multi_label:\n+            label_weights = ops.reshape(label_weights, [-1])\n+    weights = ops.cast(\n+        ops.multiply(sample_weights, label_weights), y_true.dtype\n+    )\n+\n+    # We shouldn't need this, but in case there are predict value that is out of\n+    # the range of [0.0, 1.0]\n+    y_pred = ops.clip(y_pred, clip_value_min=0.0, clip_value_max=1.0)\n+\n+    y_true = ops.cast(ops.cast(y_true, \"bool\"), y_true.dtype)\n+    if not multi_label:\n+        y_true = ops.reshape(y_true, [-1])\n+        y_pred = ops.reshape(y_pred, [-1])\n+\n+    true_labels = ops.multiply(y_true, weights)\n+    false_labels = ops.multiply((1.0 - y_true), weights)\n+\n+    # Compute the bucket indices for each prediction value.\n+    # Since the predict value has to be strictly greater than the thresholds,\n+    # eg, buckets like [0, 0.5], (0.5, 1], and 0.5 belongs to first bucket.\n+    # We have to use math.ceil(val) - 1 for the bucket.\n+    bucket_indices = ops.ceil(y_pred * (num_thresholds - 1)) - 1\n+\n+    if thresholds_with_epsilon:\n+        # In this case, the first bucket should actually take into account since\n+        # the any prediction between [0.0, 1.0] should be larger than the first\n+        # threshold. We change the bucket value from -1 to 0.\n+        bucket_indices = ops.relu(bucket_indices)\n+\n+    bucket_indices = ops.cast(bucket_indices, \"int32\")\n+\n+    if multi_label:\n+        # We need to run bucket segment sum for each of the label class. In the\n+        # multi_label case, the rank of the label is 2. We first transpose it so\n+        # that the label dim becomes the first and we can parallel run though\n+        # them.\n+        true_labels = ops.transpose(true_labels)\n+        false_labels = ops.transpose(false_labels)\n+        bucket_indices = ops.transpose(bucket_indices)\n+\n+        def gather_bucket(label_and_bucket_index):\n+            label, bucket_index = (\n+                label_and_bucket_index[0],\n+                label_and_bucket_index[1],\n+            )\n+            return ops.unsorted_segmenet_sum(\n+                data=label,\n+                segment_ids=bucket_index,\n+                num_segments=num_thresholds,\n+            )\n+\n+        tp_bucket_v = ops.vectorized_map(\n+            gather_bucket, (true_labels, bucket_indices), warn=False\n+        )\n+        fp_bucket_v = ops.vectorized_map(\n+            gather_bucket, (false_labels, bucket_indices), warn=False\n+        )\n+        tp = ops.transpose(ops.cumsum(tp_bucket_v, reverse=True, axis=1))\n+        fp = ops.transpose(ops.cumsum(fp_bucket_v, reverse=True, axis=1))\n+    else:\n+        tp_bucket_v = ops.unsorted_segmenet_sum(\n+            data=true_labels,\n+            segment_ids=bucket_indices,\n+            num_segments=num_thresholds,\n+        )\n+        fp_bucket_v = ops.unsorted_segmenet_sum(\n+            data=false_labels,\n+            segment_ids=bucket_indices,\n+            num_segments=num_thresholds,\n+        )\n+        tp = ops.cumsum(tp_bucket_v, reverse=True)\n+        fp = ops.cumsum(fp_bucket_v, reverse=True)\n+\n+    # fn = sum(true_labels) - tp\n+    # tn = sum(false_labels) - fp\n+    if (\n+        ConfusionMatrix.TRUE_NEGATIVES in variables_to_update\n+        or ConfusionMatrix.FALSE_NEGATIVES in variables_to_update\n+    ):\n+        if multi_label:\n+            total_true_labels = ops.sum(true_labels, axis=1)\n+            total_false_labels = ops.sum(false_labels, axis=1)\n+        else:\n+            total_true_labels = ops.sum(true_labels)\n+            total_false_labels = ops.sum(false_labels)\n+\n+    if ConfusionMatrix.TRUE_POSITIVES in variables_to_update:\n+        variable = variables_to_update[ConfusionMatrix.TRUE_POSITIVES]\n+        variable.assign(variable + tp)\n+    if ConfusionMatrix.FALSE_POSITIVES in variables_to_update:\n+        variable = variables_to_update[ConfusionMatrix.FALSE_POSITIVES]\n+        variable.assign(variable + fp)\n+    if ConfusionMatrix.TRUE_NEGATIVES in variables_to_update:\n+        variable = variables_to_update[ConfusionMatrix.TRUE_NEGATIVES]\n+        tn = total_false_labels - fp\n+        variable.assign(variable + tn)\n+    if ConfusionMatrix.FALSE_NEGATIVES in variables_to_update:\n+        variable = variables_to_update[ConfusionMatrix.FALSE_NEGATIVES]\n+        fn = total_true_labels - tp\n+        variable.assign(variable + fn)\n+\n+\n+def is_evenly_distributed_thresholds(thresholds):\n+    \"\"\"Check if the thresholds list is evenly distributed.\n+\n+    We could leverage evenly distributed thresholds to use less memory when\n+    calculate metrcis like AUC where each individual threshold need to be\n+    evaluated.\n+\n+    Args:\n+      thresholds: A python list or tuple, or 1D numpy array whose value is\n+        ranged in [0, 1].\n+\n+    Returns:\n+      boolean, whether the values in the inputs are evenly distributed.\n+    \"\"\"\n+    # Check the list value and see if it is evenly distributed.\n+    num_thresholds = len(thresholds)\n+    if num_thresholds < 3:\n+        return False\n+    even_thresholds = np.arange(num_thresholds, dtype=np.float32) / (\n+        num_thresholds - 1\n+    )\n+    return np.allclose(thresholds, even_thresholds, atol=backend.epsilon())\n+\n+\n+def update_confusion_matrix_variables(\n+    variables_to_update,\n+    y_true,\n+    y_pred,\n+    thresholds,\n+    top_k=None,\n+    class_id=None,\n+    sample_weight=None,\n+    multi_label=False,\n+    label_weights=None,\n+    thresholds_distributed_evenly=False,\n+):\n+    \"\"\"Updates the given confusion matrix variables.\n+\n+    For every pair of values in y_true and y_pred:\n+\n+    true_positive: y_true == True and y_pred > thresholds\n+    false_negatives: y_true == True and y_pred <= thresholds\n+    true_negatives: y_true == False and y_pred <= thresholds\n+    false_positive: y_true == False and y_pred > thresholds\n+\n+    The results will be weighted and added together. When multiple thresholds\n+    are provided, we will repeat the same for every threshold.\n+\n+    For estimation of these metrics over a stream of data, the function creates\n+    an `update_op` operation that updates the given variables.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use weights of 0 to mask values.\n+\n+    Args:\n+      variables_to_update: Dictionary with 'tp', 'fn', 'tn', 'fp' as valid keys\n+        and corresponding variables to update as values.\n+      y_true: A `Tensor` whose shape matches `y_pred`. Will be cast to `bool`.\n+      y_pred: A floating point `Tensor` of arbitrary shape and whose values are\n+        in the range `[0, 1]`.\n+      thresholds: A float value, float tensor, python list, or tuple of float\n+        thresholds in `[0, 1]`, or NEG_INF (used when top_k is set).\n+      top_k: Optional int, indicates that the positive labels should be limited\n+        to the top k predictions.\n+      class_id: Optional int, limits the prediction and labels to the class\n+        specified by this argument.\n+      sample_weight: Optional `Tensor` whose rank is either 0, or the same rank\n+        as `y_true`, and must be broadcastable to `y_true` (i.e., all dimensions\n+        must be either `1`, or the same as the corresponding `y_true`\n+        dimension).\n+      multi_label: Optional boolean indicating whether multidimensional\n+        prediction/labels should be treated as multilabel responses, or\n+        flattened into a single label. When True, the valus of\n+        `variables_to_update` must have a second dimension equal to the number\n+        of labels in y_true and y_pred, and those tensors must not be\n+        RaggedTensors.\n+      label_weights: (optional) tensor of non-negative weights for multilabel\n+        data. The weights are applied when calculating TP, FP, FN, and TN\n+        without explicit multilabel handling (i.e. when the data is to be\n+        flattened).\n+      thresholds_distributed_evenly: Boolean, whether the thresholds are evenly\n+        distributed within the list. An optimized method will be used if this is\n+        the case. See _update_confusion_matrix_variables_optimized() for more\n+        details.\n+\n+    Raises:\n+      ValueError: If `y_pred` and `y_true` have mismatched shapes, or if\n+        `sample_weight` is not `None` and its shape doesn't match `y_pred`, or\n+        if `variables_to_update` contains invalid keys.\n+    \"\"\"\n+    if multi_label and label_weights is not None:\n+        raise ValueError(\n+            \"`label_weights` for multilabel data should be handled \"\n+            \"outside of `update_confusion_matrix_variables` when \"\n+            \"`multi_label` is True.\"\n+        )\n+    if variables_to_update is None:\n+        return\n+    if not any(\n+        key for key in variables_to_update if key in list(ConfusionMatrix)\n+    ):\n+        raise ValueError(\n+            \"Please provide at least one valid confusion matrix \"\n+            \"variable to update. Valid variable key options are: \"\n+            f'\"{list(ConfusionMatrix)}\". '\n+            f'Received: \"{variables_to_update.keys()}\"'\n+        )\n+\n+    variable_dtype = list(variables_to_update.values())[0].dtype\n+\n+    y_true = ops.cast(y_true, dtype=variable_dtype)\n+    y_pred = ops.cast(y_pred, dtype=variable_dtype)\n+\n+    if thresholds_distributed_evenly:\n+        # Check whether the thresholds has any leading or tailing epsilon added\n+        # for floating point imprecision. The leading and tailing threshold will\n+        # be handled bit differently as the corner case.  At this point,\n+        # thresholds should be a list/array with more than 2 items, and ranged\n+        # between [0, 1]. See is_evenly_distributed_thresholds() for more\n+        # details.\n+        thresholds_with_epsilon = thresholds[0] < 0.0 or thresholds[-1] > 1.0\n+\n+    thresholds = ops.convert_to_tensor(thresholds, dtype=variable_dtype)\n+    num_thresholds = ops.shape(thresholds)[0]\n+\n+    if multi_label:\n+        one_thresh = ops.equal(\n+            ops.cast(1, dtype=\"int32\"),\n+            thresholds.ndim,\n+            name=\"one_set_of_thresholds_cond\",\n+        )\n+    else:\n+        one_thresh = ops.cast(True, dtype=\"bool\")\n+\n+    invalid_keys = [\n+        key for key in variables_to_update if key not in list(ConfusionMatrix)\n+    ]\n+    if invalid_keys:\n+        raise ValueError(\n+            f'Invalid keys: \"{invalid_keys}\". '\n+            f'Valid variable key options are: \"{list(ConfusionMatrix)}\"'\n+        )\n+\n+    y_pred, y_true = squeeze_to_same_rank(y_pred, y_true)\n+    if sample_weight is not None:\n+        sample_weight = ops.expand_dims(\n+            ops.cast(sample_weight, dtype=variable_dtype), axis=-1\n+        )\n+        _, sample_weight = squeeze_to_same_rank(y_true, sample_weight)\n+\n+    if top_k is not None:\n+        y_pred = _filter_top_k(y_pred, top_k)\n+    if class_id is not None:\n+        # Preserve dimension to match with sample_weight\n+        y_true = y_true[..., class_id, None]\n+        y_pred = y_pred[..., class_id, None]\n+\n+    if thresholds_distributed_evenly:\n+        return _update_confusion_matrix_variables_optimized(\n+            variables_to_update,\n+            y_true,\n+            y_pred,\n+            thresholds,\n+            multi_label=multi_label,\n+            sample_weights=sample_weight,\n+            label_weights=label_weights,\n+            thresholds_with_epsilon=thresholds_with_epsilon,\n+        )\n+\n+    pred_shape = ops.shape(y_pred)\n+    num_predictions = pred_shape[0]\n+    if y_pred.ndim == 1:\n+        num_labels = 1\n+    else:\n+        num_labels = ops.cast(\n+            ops.prod(ops.array(pred_shape[1:]), axis=0), \"int32\"\n+        )\n+    thresh_label_tile = ops.where(one_thresh, num_labels, 1)\n+\n+    # Reshape predictions and labels, adding a dim for thresholding.\n+    if multi_label:\n+        predictions_extra_dim = ops.expand_dims(y_pred, 0)\n+        labels_extra_dim = ops.expand_dims(ops.cast(y_true, dtype=\"bool\"), 0)\n+    else:\n+        # Flatten predictions and labels when not multilabel.\n+        predictions_extra_dim = ops.reshape(y_pred, [1, -1])\n+        labels_extra_dim = ops.reshape(ops.cast(y_true, dtype=\"bool\"), [1, -1])\n+\n+    # Tile the thresholds for every prediction.\n+    if multi_label:\n+        thresh_pretile_shape = [num_thresholds, 1, -1]\n+        thresh_tiles = [1, num_predictions, thresh_label_tile]\n+        data_tiles = [num_thresholds, 1, 1]\n+    else:\n+        thresh_pretile_shape = [num_thresholds, -1]\n+        thresh_tiles = [1, num_predictions * num_labels]\n+        data_tiles = [num_thresholds, 1]\n+\n+    thresh_tiled = ops.tile(\n+        ops.reshape(thresholds, thresh_pretile_shape), ops.array(thresh_tiles)\n+    )\n+\n+    # Tile the predictions for every threshold.\n+    preds_tiled = ops.tile(predictions_extra_dim, data_tiles)\n+\n+    # Compare predictions and threshold.\n+    pred_is_pos = ops.greater(preds_tiled, thresh_tiled)\n+\n+    # Tile labels by number of thresholds\n+    label_is_pos = ops.tile(labels_extra_dim, data_tiles)\n+\n+    if sample_weight is not None:\n+        sample_weight = ops.broadcast_to(\n+            ops.cast(sample_weight, dtype=y_pred.dtype), y_pred.shape\n+        )\n+        weights_tiled = ops.tile(\n+            ops.reshape(sample_weight, thresh_tiles), data_tiles\n+        )\n+    else:\n+        weights_tiled = None\n+\n+    if label_weights is not None and not multi_label:\n+        label_weights = ops.expand_dims(label_weights, 0)\n+        label_weights = ops.broadcast_to(label_weights, y_pred.shape)\n+        label_weights_tiled = ops.tile(\n+            ops.reshape(label_weights, thresh_tiles), data_tiles\n+        )\n+        if weights_tiled is None:\n+            weights_tiled = label_weights_tiled\n+        else:\n+            weights_tiled = ops.multiply(weights_tiled, label_weights_tiled)\n+\n+    def weighted_assign_add(label, pred, weights, var):\n+        label_and_pred = ops.cast(ops.logical_and(label, pred), dtype=var.dtype)\n+        if weights is not None:\n+            label_and_pred *= ops.cast(weights, dtype=var.dtype)\n+        var.assign(var + ops.sum(label_and_pred, 1))\n+\n+    loop_vars = {\n+        ConfusionMatrix.TRUE_POSITIVES: (label_is_pos, pred_is_pos),\n+    }\n+    update_tn = ConfusionMatrix.TRUE_NEGATIVES in variables_to_update\n+    update_fp = ConfusionMatrix.FALSE_POSITIVES in variables_to_update\n+    update_fn = ConfusionMatrix.FALSE_NEGATIVES in variables_to_update\n+\n+    if update_fn or update_tn:\n+        pred_is_neg = ops.logical_not(pred_is_pos)\n+        loop_vars[ConfusionMatrix.FALSE_NEGATIVES] = (label_is_pos, pred_is_neg)\n+\n+    if update_fp or update_tn:\n+        label_is_neg = ops.logical_not(label_is_pos)\n+        loop_vars[ConfusionMatrix.FALSE_POSITIVES] = (label_is_neg, pred_is_pos)\n+        if update_tn:\n+            loop_vars[ConfusionMatrix.TRUE_NEGATIVES] = (\n+                label_is_neg,\n+                pred_is_neg,\n+            )\n+\n+    for matrix_cond, (label, pred) in loop_vars.items():\n+        if matrix_cond in variables_to_update:\n+            weighted_assign_add(\n+                label, pred, weights_tiled, variables_to_update[matrix_cond]\n+            )\n+\n+\n+def _filter_top_k(x, k):\n+    \"\"\"Filters top-k values in the last dim of x and set the rest to NEG_INF.\n+\n+    Used for computing top-k prediction values in dense labels (which has the\n+    same shape as predictions) for recall and precision top-k metrics.\n+\n+    Args:\n+      x: tensor with any dimensions.\n+      k: the number of values to keep.\n+\n+    Returns:\n+      tensor with same shape and dtype as x.\n+    \"\"\"\n+    _, top_k_idx = ops.top_k(x, k, sorted=False)\n+    top_k_mask = ops.sum(\n+        ops.one_hot(top_k_idx, ops.shape(x)[-1], axis=-1), axis=-2\n+    )\n+    return x * top_k_mask + NEG_INF * (1 - top_k_mask)\n\n@@ -146,7 +146,7 @@ def serialize_keras_object(obj):\n             \"class_name\": \"__tensor__\",\n             \"config\": {\n                 \"value\": np.array(obj).tolist(),\n-                \"dtype\": backend.standardize_dtype(obj.dtype),\n+                \"dtype\": str(obj.dtype),\n             },\n         }\n     if type(obj).__module__ == np.__name__:\n@@ -155,7 +155,7 @@ def serialize_keras_object(obj):\n                 \"class_name\": \"__numpy__\",\n                 \"config\": {\n                     \"value\": obj.tolist(),\n-                    \"dtype\": backend.standardize_dtype(obj.dtype),\n+                    \"dtype\": backend.standardize_dytpe(obj.dtype),\n                 },\n             }\n         else:\n\n@@ -9,3 +9,16 @@ class TestCase(unittest.TestCase):\n \n     def assertAlmostEqual(self, x1, x2, decimal=3):\n         np.testing.assert_almost_equal(x1, x2, decimal=decimal)\n+\n+    def assertEqual(self, x1, x2):\n+        np.testing.assert_equal(x1, x2)\n+\n+    def assertLen(self, iterable, expected_len):\n+        np.testing.assert_equal(len(iterable), expected_len)\n+\n+    def assertRaisesRegex(\n+        self, exception_class, expected_regexp, *args, **kwargs\n+    ):\n+        return np.testing.assert_raises_regex(\n+            exception_class, expected_regexp, *args, **kwargs\n+        )\n\n@@ -86,3 +86,20 @@ def func_load(code, defaults=None, closure=None, globs=None):\n     return python_types.FunctionType(\n         code, globs, name=code.co_name, argdefs=defaults, closure=closure\n     )\n+\n+\n+def to_list(x):\n+    \"\"\"Normalizes a list/tensor into a list.\n+\n+    If a tensor is passed, we return\n+    a list of size 1 containing the tensor.\n+\n+    Args:\n+        x: target object to be normalized.\n+\n+    Returns:\n+        A list.\n+    \"\"\"\n+    if isinstance(x, list):\n+        return x\n+    return [x]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f4fc1451a3318ab5dd6a035d0b13aa28c7b7d8e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 13 | Churn Cumulative: 952 | Contributors (this commit): 5 | Commits (past 90d): 27 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,3 +15,7 @@ def get(identifier):\n     if identifier == \"relu\":\n         return relu\n     return identifier\n+\n+\n+def serialize(activation):\n+    return activation.__name__\n\n@@ -61,10 +61,15 @@ class Dense(Layer):\n         # TODO\n         config = {\n             \"units\": self.units,\n+            \"activation\": activations.serialize(self.activation),\n             \"use_bias\": self.use_bias,\n             \"kernel_initializer\": initializers.serialize(\n                 self.kernel_initializer\n             ),\n             \"bias_initializer\": initializers.serialize(self.bias_initializer),\n+            \"kernel_regularizer\": regularizers.serialize(self.kernel_regularizer),\n+            \"bias_regularizer\": regularizers.serialize(self.bias_regularizer),\n+            \"kernel_constraint\": constraints.serialize(self.kernel_constraint),\n+            \"bias_constraint\": constraints.serialize(self.bias_constraint),\n         }\n         return {**base_config, **config}\n\n@@ -146,7 +146,7 @@ def serialize_keras_object(obj):\n             \"class_name\": \"__tensor__\",\n             \"config\": {\n                 \"value\": np.array(obj).tolist(),\n-                \"dtype\": str(obj.dtype),\n+                \"dtype\": backend.standardize_dtype(obj.dtype),\n             },\n         }\n     if type(obj).__module__ == np.__name__:\n@@ -155,7 +155,7 @@ def serialize_keras_object(obj):\n                 \"class_name\": \"__numpy__\",\n                 \"config\": {\n                     \"value\": obj.tolist(),\n-                    \"dtype\": backend.standardize_dytpe(obj.dtype),\n+                    \"dtype\": backend.standardize_dtype(obj.dtype),\n                 },\n             }\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7083870babb908c379acaea79d7423991ad5317d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 7878 | Contributors (this commit): 4 | Commits (past 90d): 33 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -1071,11 +1071,10 @@ class ExpandDims(Operation):\n     def compute_output_spec(self, x):\n         x_shape = list(x.shape)\n         if self.axis < 0:\n-            output_shape = (\n-                x_shape[: self.axis + 1] + [1] + x_shape[self.axis + 1 :]\n-            )\n+            axis = len(x.shape) + 1 + self.axis\n         else:\n-            output_shape = x_shape[: self.axis] + [1] + x_shape[self.axis :]\n+            axis = self.axis\n+        output_shape = x_shape[: axis] + [1] + x_shape[axis :]\n         return KerasTensor(output_shape, dtype=x.dtype)\n \n \n\n@@ -788,6 +788,7 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n \n     def test_expand_dims(self):\n         x = KerasTensor([None, 3])\n+        self.assertEqual(knp.expand_dims(x, -1).shape, (None, 3, 1))\n         self.assertEqual(knp.expand_dims(x, 0).shape, (1, None, 3))\n         self.assertEqual(knp.expand_dims(x, 1).shape, (None, 1, 3))\n         self.assertEqual(knp.expand_dims(x, -2).shape, (None, 1, 3))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3f7fdc23109a6c9713b3923a0c92014ad78beba7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 498 | Lines Deleted: 17 | Files Changed: 3 | Hunks: 6 | Methods Changed: 28 | Complexity Δ (Sum/Max): 37/33 | Churn Δ: 515 | Churn Cumulative: 673 | Contributors (this commit): 4 | Commits (past 90d): 22 | Contributors (cumulative): 8 | DMM Complexity: 0.8037383177570093\n\nDIFF:\n@@ -1,21 +1,98 @@\n-from keras_core import backend\n+import types\n+\n+from keras_core.api_export import keras_core_export\n+from keras_core.saving import serialization_lib\n+from keras_core.saving import object_registration\n+\n+from keras_core.activations.activations import relu\n+from keras_core.activations.activations import leaky_relu\n+from keras_core.activations.activations import relu6\n+from keras_core.activations.activations import softmax\n+from keras_core.activations.activations import elu\n+from keras_core.activations.activations import selu\n+from keras_core.activations.activations import softplus\n+from keras_core.activations.activations import softsign\n+from keras_core.activations.activations import silu\n+from keras_core.activations.activations import gelu\n+from keras_core.activations.activations import tanh\n+from keras_core.activations.activations import sigmoid\n+from keras_core.activations.activations import exponential\n+from keras_core.activations.activations import hard_sigmoid\n+from keras_core.activations.activations import linear\n+from keras_core.activations.activations import mish\n+from keras_core.activations.activations import log_softmax\n \n \n-def relu(x):\n-    return backend.nn.relu(x)\n-\n-\n-def identity(x):\n-    return x\n-\n-\n-def get(identifier):\n-    if identifier is None:\n-        return identity\n-    if identifier == \"relu\":\n-        return relu\n-    return identifier\n+ALL_OBJECTS = {\n+    relu,\n+    leaky_relu,\n+    relu6,\n+    softmax,\n+    elu,\n+    selu,\n+    softplus,\n+    softsign,\n+    silu,\n+    gelu,\n+    tanh,\n+    sigmoid,\n+    exponential,\n+    hard_sigmoid,\n+    linear,\n+    mish,\n+    log_softmax,\n+}\n+  \n+ALL_OBJECTS_DICT = {fn.__name__: fn for fn in ALL_OBJECTS}\n \n \n+@keras_core_export(\"keras_core.activations.serialize\")\n def serialize(activation):\n-    return activation.__name__\n+    fn_config = serialization_lib.serialize_keras_object(activation)\n+    if \"config\" not in fn_config:\n+        raise ValueError(\n+            f\"Unknown activation function '{activation}' cannot be \"\n+            \"serialized due to invalid function name. Make sure to use \"\n+            \"an activation name that matches the references defined in \"\n+            \"activations.py or use `@keras_core.saving.register_keras_serializable()`\"\n+            \"to register any custom activations. \"\n+            f\"config={fn_config}\"\n+        )\n+    if not isinstance(activation, types.FunctionType):\n+        # Case for additional custom activations represented by objects\n+        return fn_config\n+    if (\n+        isinstance(fn_config[\"config\"], str)\n+        and fn_config[\"config\"] not in globals()\n+    ):\n+        # Case for custom activation functions from external activations modules\n+        fn_config[\"config\"] = object_registration.get_registered_name(\n+            activation\n+        )\n+        return fn_config\n+    # Case for keras.activations builtins (simply return name)\n+    return fn_config[\"config\"]\n+\n+\n+@keras_core_export(\"keras_core.activations.deserialize\")\n+def deserialize(config, custom_objects=None):\n+    \"\"\"Return a Keras activation function via its config.\"\"\"\n+    return serialization_lib.deserialize_keras_object(\n+        config,\n+        module_objects=ALL_OBJECTS_DICT,\n+        custom_objects=custom_objects,\n+    )\n+\n+\n+@keras_core_export(\"keras_core.activations.get\")\n+def get(identifier):\n+    \"\"\"Retrieve a Keras activation function via an identifier.\"\"\"\n+    if identifier is None:\n+        return linear\n+    if isinstance(identifier, (str, dict)):\n+        return deserialize(identifier)\n+    elif callable(identifier):\n+        return identifier\n+    raise TypeError(\n+        f\"Could not interpret activation function identifier: {identifier}\"\n+    )\n\n@@ -0,0 +1,405 @@\n+from keras_core import backend\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+\n+\n+@keras_core_export(\"keras_core.activations.relu\")\n+def relu(x, negative_slope=0.0, max_value=None, threshold=0.0):\n+    \"\"\"Applies the rectified linear unit activation function.\n+\n+    With default values, this returns the standard ReLU activation:\n+    `max(x, 0)`, the element-wise maximum of 0 and the input tensor.\n+\n+    Modifying default parameters allows you to use non-zero thresholds,\n+    change the max value of the activation,\n+    and to use a non-zero multiple of the input for values below the threshold.\n+\n+    Examples:\n+\n+    >>> x = [-10, -5, 0.0, 5, 10]\n+    >>> keras_core.activations.relu(x)\n+    [ 0.,  0.,  0.,  5., 10.]\n+    >>> keras_core.activations.relu(x, negative_slope=0.5)\n+    [-5. , -2.5,  0. ,  5. , 10. ]\n+    >>> keras_core.activations.relu(x, max_value=5.)\n+    [0., 0., 0., 5., 5.]\n+    >>> keras_core.activations.relu(x, threshold=5.)\n+    [-0., -0.,  0.,  0., 10.]\n+\n+    Args:\n+        x: Input tensor.\n+        negative_slope: A `float` that controls the slope\n+            for values lower than the threshold.\n+        max_value: A `float` that sets the saturation threshold (the largest\n+            value the function will return).\n+        threshold: A `float` giving the threshold value of the activation\n+            function below which values will be damped or set to zero.\n+\n+    Returns:\n+        A tensor with the same shape and dtype as input `x`.\n+    \"\"\"\n+    if backend.any_symbolic_tensors((x,)):\n+        return ReLU(negative_slope=negative_slope, max_value=max_value, threshold=threshold)(x)\n+    return ReLU.static_call(x, negative_slope=negative_slope, max_value=max_value, threshold=threshold)\n+\n+\n+class ReLU(ops.Operation):\n+    def __init__(\n+        self, negative_slope=0.0, max_value=None, threshold=0.0, name=None\n+    ):\n+        super().__init__(name=name)\n+        self.negative_slope = negative_slope\n+        self.max_value = max_value\n+        self.threshold = threshold\n+\n+    def call(self, x):\n+        return self.static_call(\n+            x,\n+            negative_slope=self.negative_slope,\n+            max_value=self.max_value,\n+            threshold=self.threshold,\n+        )\n+\n+    def compute_output_spec(self, x):\n+        return backend.KerasTensor(x.shape, x.dtype)\n+    \n+    @staticmethod\n+    def static_call(x, negative_slope=0.0, max_value=None, threshold=0.0):\n+        if negative_slope != 0.0:\n+            if max_value is None and threshold == 0:\n+                return backend.nn.leaky_relu(x, negative_slope=negative_slope)\n+\n+            if threshold != 0:\n+                negative_part = backend.nn.relu(-x + threshold)\n+            else:\n+                negative_part = backend.nn.relu(-x)\n+\n+        clip_max = max_value is not None\n+        if threshold != 0:\n+            # computes x for x > threshold else 0\n+            x = x * backend.cast(backend.greater(x, threshold), dtype=x.dtype)\n+        elif max_value == 6:\n+            # if no threshold, then can use nn.relu6 native op for performance\n+            x = backend.nn.relu6(x)\n+            clip_max = False\n+        else:\n+            x = backend.nn.relu(x)\n+\n+        if clip_max:\n+            x = backend.clip(x, 0.0, max_value)\n+\n+        if negative_slope != 0.0:\n+            x -= negative_slope * negative_part\n+        return x\n+    \n+\n+\n+@keras_core_export(\"keras_core.activations.leaky_relu\")\n+def leaky_relu(x, negative_slope=0.2):\n+    \"\"\"Leaky relu activation function.\n+\n+    Args:\n+        x: Input tensor.\n+        negative_slope: A `float` that controls the slope\n+            for values lower than the threshold.\n+    \"\"\"\n+    return ops.leaky_relu(x, negative_slope=negative_slope)\n+\n+\n+@keras_core_export(\"keras_core.activations.relu6\")\n+def relu6(x):\n+    \"\"\"Relu6 activation function.\n+\n+    It's the ReLU function, but truncated to a maximum value of 6.\n+\n+    Args:\n+        x: Input tensor.\n+    \"\"\"\n+    return ops.relu6(x)\n+\n+\n+@keras_core_export(\"keras_core.activations.softmax\")\n+def softmax(x, axis=-1):\n+    \"\"\"Softmax converts a vector of values to a probability distribution.\n+\n+    The elements of the output vector are in range `[0, 1]` and sum to 1.\n+\n+    Each input vector is handled independently.\n+    The `axis` argument sets which axis of the input the function\n+    is applied along.\n+\n+    Softmax is often used as the activation for the last\n+    layer of a classification network because the result could be interpreted as\n+    a probability distribution.\n+\n+    The softmax of each vector x is computed as\n+    `exp(x) / sum(exp(x))`.\n+\n+    The input values in are the log-odds of the resulting probability.\n+\n+    Args:\n+        x : Input tensor.\n+        axis: Integer, axis along which the softmax is applied.\n+    \"\"\"\n+    return ops.softmax(x, axis=axis)\n+\n+\n+@keras_core_export(\"keras_core.activations.elu\")\n+def elu(x):\n+    \"\"\"Exponential Linear Unit.\n+\n+    The exponential linear unit (ELU) with `alpha > 0` is define as:\n+\n+    - `x` if `x > 0`\n+    - `exp(x) - 1` if `x < 0`\n+\n+    ELUs have negative values which pushes the mean of the activations\n+    closer to zero.\n+\n+    Mean activations that are closer to zero enable faster learning as they\n+    bring the gradient closer to the natural gradient.\n+    ELUs saturate to a negative value when the argument gets smaller.\n+    Saturation means a small derivative which decreases the variation\n+    and the information that is propagated to the next layer.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Reference:\n+\n+    - [Clevert et al., 2016](https://arxiv.org/abs/1511.07289)\n+    \"\"\"\n+    return ops.elu(x)\n+\n+\n+@keras_core_export(\"keras_core.activations.selu\")\n+def selu(x):\n+    \"\"\"Scaled Exponential Linear Unit (SELU).\n+\n+    The Scaled Exponential Linear Unit (SELU) activation function is defined as:\n+\n+    - `scale * x` if `x > 0`\n+    - `scale * alpha * (exp(x) - 1)` if `x < 0`\n+\n+    where `alpha` and `scale` are pre-defined constants\n+    (`alpha=1.67326324` and `scale=1.05070098`).\n+\n+    Basically, the SELU activation function multiplies `scale` (> 1) with the\n+    output of the `keras_core.activations.elu` function to ensure a slope larger\n+    than one for positive inputs.\n+\n+    The values of `alpha` and `scale` are\n+    chosen so that the mean and variance of the inputs are preserved\n+    between two consecutive layers as long as the weights are initialized\n+    correctly (see `keras_core.initializers.LecunNormal` initializer)\n+    and the number of input units is \"large enough\"\n+    (see reference paper for more information).\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Notes:\n+\n+    - To be used together with the\n+        `keras_core.initializers.LecunNormal` initializer.\n+    - To be used together with the dropout variant\n+        `keras_core.layers.AlphaDropout` (rather than regular dropout).\n+\n+    Reference:\n+\n+    - [Klambauer et al., 2017](https://arxiv.org/abs/1706.02515)\n+    \"\"\"\n+    return ops.selu(x)\n+\n+\n+@keras_core_export(\"keras_core.activations.softplus\")\n+def softplus(x):\n+    \"\"\"Softplus activation function.\n+    \n+    It is defined as: `softplus(x) = log(exp(x) + 1)`.\n+\n+    Args:\n+        x: Input tensor.\n+    \"\"\"\n+    return ops.softplus(x)\n+\n+\n+@keras_core_export(\"keras_core.activations.softsign\")\n+def softsign(x):\n+    \"\"\"Softsign activation function.\n+    \n+    Softsign is defined as: `softsign(x) = x / (abs(x) + 1)`.\n+\n+    Args:\n+        x: Input tensor.\n+    \"\"\"\n+    return ops.softsign(x)\n+\n+\n+@keras_core_export([\"keras_core.activations.silu\", \"keras_core.activations.swish\"])\n+def silu(x):\n+    \"\"\"Swish (or Silu) activation function.\n+    \n+    It is defined as: `swish(x) = x * sigmoid(x)`.\n+\n+    The Swish (or Silu) activation function is a smooth,\n+    non-monotonic function that is unbounded above and\n+    bounded below.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Reference:\n+\n+    - [Ramachandran et al., 2017](https://arxiv.org/abs/1710.05941)\n+    \"\"\"\n+    return ops.silu(x)\n+\n+\n+@keras_core_export(\"keras_core.activations.gelu\")\n+def gelu(x):\n+    \"\"\"Gaussian error linear unit (GELU) activation function.\n+\n+    The Gaussian error linear unit (GELU) is defined as:\n+\n+    `gelu(x) = x * P(X <= x)` where `P(X) ~ N(0, 1)`,\n+    i.e. `gelu(x) = 0.5 * x * (1 + erf(x / sqrt(2)))`.\n+\n+    GELU weights inputs by their value, rather than gating\n+    inputs by their sign as in ReLU.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Reference:\n+\n+    - [Hendrycks et al., 2016](https://arxiv.org/abs/1606.08415)\n+    \"\"\"\n+    return ops.gelu(x)\n+\n+\n+@keras_core_export(\"keras_core.activations.tanh\")\n+def tanh(x):\n+    \"\"\"Hyperbolic tangent activation function.\n+\n+    It is defined as:\n+    `tanh(x) = sinh(x) / cosh(x)`, i.e.\n+    `tanh(x) = ((exp(x) - exp(-x)) / (exp(x) + exp(-x)))`.\n+\n+    Args:\n+        x: Input tensor.\n+    \"\"\"\n+    return ops.tanh(x)\n+\n+\n+@keras_core_export(\"keras_core.activations.sigmoid\")\n+def sigmoid(x):\n+    \"\"\"Sigmoid activation function.\n+    \n+    It is defined as: `sigmoid(x) = 1 / (1 + exp(-x))`.\n+\n+    For small values (<-5),\n+    `sigmoid` returns a value close to zero, and for large values (>5)\n+    the result of the function gets close to 1.\n+\n+    Sigmoid is equivalent to a 2-element softmax, where the second element is\n+    assumed to be zero. The sigmoid function always returns a value between\n+    0 and 1.\n+\n+    Args:\n+        x: Input tensor.\n+    \"\"\"\n+    output = ops.sigmoid(x)\n+    # Cache the logits to use for crossentropy loss.\n+    output._keras_logits = x\n+    return output\n+\n+\n+@keras_core_export(\"keras_core.activations.exponential\")\n+def exponential(x):\n+    \"\"\"Exponential activation function.\n+\n+    Args:\n+        x: Input tensor.\n+    \"\"\"\n+    return ops.exp(x)\n+\n+\n+@keras_core_export(\"keras_core.activations.hard_sigmoid\")\n+def hard_sigmoid(x):\n+    \"\"\"Hard sigmoid activation function.\n+\n+    The hard sigmoid activation is defined as:\n+\n+    - `0` if `if x < -2.5`\n+    - `1` if `x > 2.5`\n+    - `0.2 * x + 0.5` if `-2.5 <= x <= 2.5`\n+\n+    It's a faster, piecewise linear approximation\n+    of the sigmoid activation.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Reference:\n+\n+    - [Wikipedia \"Hard sigmoid\"](https://en.wikipedia.org/wiki/Hard_sigmoid)\n+    \"\"\"\n+    return ops.hard_sigmoid(x)\n+\n+\n+@keras_core_export(\"keras_core.activations.linear\")\n+def linear(x):\n+    \"\"\"Linear activation function (pass-through).\n+\n+    A \"linear\" activation is an identity function:\n+    it returns the input, unmodified.\n+\n+    Args:\n+        x: Input tensor.\n+    \"\"\"\n+    return x\n+\n+\n+class Mish(ops.Operation):\n+    def call(self, x):\n+        return self.static_call(x)\n+\n+    def compute_output_spec(self, x):\n+        return backend.KerasTensor(x.shape, x.dtype)\n+    \n+    @staticmethod\n+    def static_call(x):\n+        return x * backend.tanh(backend.softplus(x))\n+\n+\n+@keras_core_export(\"keras_core.activations.mish\")\n+def mish(x):\n+    \"\"\"Mish activation function.\n+\n+    It is defined as:\n+\n+    `mish(x) = x * tanh(softplus(x))`\n+\n+    where `softplus` is defined as:\n+\n+    `softplus(x) = log(exp(x) + 1)`\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Reference:\n+    \n+    - [Misra, 2019](https://arxiv.org/abs/1908.08681)\n+    \"\"\"\n+    return Mish.static_call(x)\n+\n+\n+@keras_core_export(\"keras_core.activations.log_softmax\")\n+def log_softmax(x):\n+    \"\"\"Log-Softmax activation function.\n+\n+    Args:\n+        x: Input tensor.\n+    \"\"\"\n+    return ops.log_softmax(x)\n\n@@ -58,7 +58,6 @@ class Dense(Layer):\n \n     def get_config(self):\n         base_config = super().get_config()\n-        # TODO\n         config = {\n             \"units\": self.units,\n             \"activation\": activations.serialize(self.activation),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
