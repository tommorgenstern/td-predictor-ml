{"custom_id": "keras#7b022d622a096f543912c3b5d021fb6fdef23490", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 141 | Lines Deleted: 0 | Files Changed: 15 | Hunks: 19 | Methods Changed: 13 | Complexity Δ (Sum/Max): 17/5 | Churn Δ: 141 | Churn Cumulative: 5244 | Contributors (this commit): 23 | Commits (past 90d): 167 | Contributors (cumulative): 106 | DMM Complexity: 1.0\n\nDIFF:\n@@ -32,5 +32,6 @@ from keras.src.activations.activations import soft_shrink\n from keras.src.activations.activations import softmax\n from keras.src.activations.activations import softplus\n from keras.src.activations.activations import softsign\n+from keras.src.activations.activations import squareplus\n from keras.src.activations.activations import tanh\n from keras.src.activations.activations import tanh_shrink\n\n@@ -99,6 +99,7 @@ from keras.src.ops.nn import softmax\n from keras.src.ops.nn import softplus\n from keras.src.ops.nn import softsign\n from keras.src.ops.nn import sparse_categorical_crossentropy\n+from keras.src.ops.nn import squareplus\n from keras.src.ops.nn import tanh_shrink\n from keras.src.ops.numpy import abs\n from keras.src.ops.numpy import absolute\n\n@@ -44,4 +44,5 @@ from keras.src.ops.nn import softmax\n from keras.src.ops.nn import softplus\n from keras.src.ops.nn import softsign\n from keras.src.ops.nn import sparse_categorical_crossentropy\n+from keras.src.ops.nn import squareplus\n from keras.src.ops.nn import tanh_shrink\n\n@@ -32,5 +32,6 @@ from keras.src.activations.activations import soft_shrink\n from keras.src.activations.activations import softmax\n from keras.src.activations.activations import softplus\n from keras.src.activations.activations import softsign\n+from keras.src.activations.activations import squareplus\n from keras.src.activations.activations import tanh\n from keras.src.activations.activations import tanh_shrink\n\n@@ -99,6 +99,7 @@ from keras.src.ops.nn import softmax\n from keras.src.ops.nn import softplus\n from keras.src.ops.nn import softsign\n from keras.src.ops.nn import sparse_categorical_crossentropy\n+from keras.src.ops.nn import squareplus\n from keras.src.ops.nn import tanh_shrink\n from keras.src.ops.numpy import abs\n from keras.src.ops.numpy import absolute\n\n@@ -44,4 +44,5 @@ from keras.src.ops.nn import softmax\n from keras.src.ops.nn import softplus\n from keras.src.ops.nn import softsign\n from keras.src.ops.nn import sparse_categorical_crossentropy\n+from keras.src.ops.nn import squareplus\n from keras.src.ops.nn import tanh_shrink\n\n@@ -23,6 +23,7 @@ from keras.src.activations.activations import soft_shrink\n from keras.src.activations.activations import softmax\n from keras.src.activations.activations import softplus\n from keras.src.activations.activations import softsign\n+from keras.src.activations.activations import squareplus\n from keras.src.activations.activations import tanh\n from keras.src.activations.activations import tanh_shrink\n from keras.src.api_export import keras_export\n@@ -39,6 +40,7 @@ ALL_OBJECTS = {\n     selu,\n     softplus,\n     softsign,\n+    squareplus,\n     soft_shrink,\n     silu,\n     gelu,\n\n@@ -297,6 +297,27 @@ def silu(x):\n     return ops.silu(x)\n \n \n+@keras_export(\"keras.activations.squareplus\")\n+def squareplus(x, b=4):\n+    \"\"\"Squareplus activation function.\n+\n+    The SquarePlus activation function is defined as:\n+\n+    `f(x) = (x + sqrt(x^2 + b)) / 2`\n+\n+    Where `b` is a smoothness parameter.\n+\n+    Args:\n+        x: Input tensor.\n+        b: Smoothness parameter. Defaults to 4.\n+\n+    Reference:\n+\n+    - [Ramachandran et al., 2021](https://arxiv.org/abs/2112.11687)\n+    \"\"\"\n+    return ops.squareplus(x, b=b)\n+\n+\n @keras_export(\"keras.activations.gelu\")\n def gelu(x, approximate=False):\n     \"\"\"Gaussian error linear unit (GELU) activation function.\n\n@@ -683,6 +683,16 @@ class ActivationsTest(testing.TestCase):\n         expected = hard_shrink(x)\n         self.assertAllClose(result, expected, rtol=1e-05)\n \n+    def test_squareplus(self):\n+        def squareplus(x, b=4):\n+            y = x + np.sqrt(x**2 + b)\n+            return y / 2\n+\n+        x = np.random.random((2, 5))\n+        result = activations.squareplus(x[np.newaxis, :])[0]\n+        expected = squareplus(x)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n     def test_soft_shrink(self):\n         def soft_shrink(x, threshold=0.5):\n             return np.where(\n\n@@ -67,6 +67,11 @@ def silu(x):\n     return jnn.silu(x)\n \n \n+def squareplus(x, b=4):\n+    x = convert_to_tensor(x)\n+    return jnn.squareplus(x, b=b)\n+\n+\n def log_sigmoid(x):\n     x = convert_to_tensor(x)\n     return jnn.log_sigmoid(x)\n\n@@ -67,6 +67,13 @@ def silu(x):\n     return x * sigmoid(x)\n \n \n+def squareplus(x, b=4):\n+    x = convert_to_tensor(x)\n+    b = convert_to_tensor(b, dtype=x.dtype)\n+    y = x + np.sqrt(x**2 + b)\n+    return y / 2\n+\n+\n def log_sigmoid(x):\n     x = convert_to_tensor(x)\n     return -softplus(-x)\n\n@@ -54,6 +54,13 @@ def silu(x):\n     return tf.nn.silu(x)\n \n \n+def squareplus(x, b=4):\n+    x = convert_to_tensor(x)\n+    b = convert_to_tensor(b, dtype=x.dtype)\n+    y = x + tf.sqrt(tf.square(x) + b)\n+    return y / 2\n+\n+\n def log_sigmoid(x):\n     return tf.math.log_sigmoid(x)\n \n\n@@ -60,6 +60,13 @@ def silu(x):\n     return tnn.silu(x)\n \n \n+def squareplus(x, b=4):\n+    x = convert_to_tensor(x)\n+    b = convert_to_tensor(b)\n+    y = x + torch.sqrt(x**2 + b)\n+    return y / 2\n+\n+\n def log_sigmoid(x):\n     x = convert_to_tensor(x)\n     return tnn.logsigmoid(x)\n\n@@ -258,6 +258,46 @@ def silu(x):\n     return backend.nn.silu(x)\n \n \n+class Squareplus(Operation):\n+    def __init__(self, b=4):\n+        super().__init__()\n+        self.b = b\n+\n+    def call(self, x):\n+        return backend.nn.squareplus(x, self.b)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=x.dtype)\n+\n+\n+@keras_export([\"keras.ops.squareplus\", \"keras.ops.nn.squareplus\"])\n+def squareplus(x, b=4):\n+    \"\"\"SquarePlus activation function.\n+\n+    The SquarePlus activation function is defined as:\n+\n+    `f(x) = (x + sqrt(x^2 + b)) / 2`\n+\n+    Args:\n+        x: Input tensor.\n+        b: Smoothness parameter. Defaults to 4.\n+\n+    Returns:\n+        A tensor with the same shape as `x`.\n+\n+    Example:\n+\n+    >>> x = np.array([-1.0, 0.0, 1.0])\n+    >>> x_squareplus = keras.ops.squareplus(x)\n+    >>> print(x_squareplus)\n+    array([0.6180, 1.0000, 1.6180], dtype=float32)\n+\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Squareplus(b).symbolic_call(x)\n+    return backend.nn.squareplus(x, b)\n+\n+\n class LogSigmoid(Operation):\n     def call(self, x):\n         return backend.nn.log_sigmoid(x)\n\n@@ -160,6 +160,10 @@ class NNOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor([None, 2, 3])\n         self.assertEqual(knn.hard_shrink(x).shape, (None, 2, 3))\n \n+    def test_squareplus(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.squareplus(x).shape, (None, 2, 3))\n+\n     def test_soft_shrink(self):\n         x = KerasTensor([None, 2, 3])\n         self.assertEqual(knn.soft_shrink(x).shape, (None, 2, 3))\n@@ -829,6 +833,10 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor([1, 2, 3])\n         self.assertEqual(knn.hard_shrink(x).shape, (1, 2, 3))\n \n+    def test_squareplus(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.squareplus(x).shape, (1, 2, 3))\n+\n     def test_soft_shrink(self):\n         x = KerasTensor([1, 2, 3])\n         self.assertEqual(knn.soft_shrink(x).shape, (1, 2, 3))\n@@ -1374,6 +1382,13 @@ class NNOpsCorrectnessTest(testing.TestCase):\n             [0.0, 0.0, 1.0, 2.0, 3.0],\n         )\n \n+    def test_squareplus(self):\n+        x = np.array([-0.5, 0, 1, 2, 3], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.squareplus(x),\n+            [0.780776, 1.0, 1.618034, 2.414214, 3.302776],\n+        )\n+\n     def test_soft_shrink(self):\n         x = np.array([-0.5, 0, 1, 2, 3], dtype=np.float32)\n         self.assertAllClose(\n@@ -2578,6 +2593,27 @@ class NNOpsDtypeTest(testing.TestCase):\n             expected_dtype,\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=FLOAT_DTYPES))\n+    def test_squareplus(self, dtype):\n+        import jax.nn as jnn\n+        import jax.numpy as jnp\n+\n+        if dtype == \"bfloat16\":\n+            self.skipTest(\"Weirdness with numpy\")\n+\n+        x = knp.ones((2), dtype=dtype)\n+        x_jax = jnp.ones((2), dtype=dtype)\n+        expected_dtype = standardize_dtype(jnn.squareplus(x_jax).dtype)\n+\n+        self.assertEqual(\n+            standardize_dtype(knn.squareplus(x).dtype),\n+            expected_dtype,\n+        )\n+        self.assertEqual(\n+            standardize_dtype(knn.Squareplus().symbolic_call(x).dtype),\n+            expected_dtype,\n+        )\n+\n     @parameterized.named_parameters(named_product(dtype=FLOAT_DTYPES))\n     def test_hard_sigmoid(self, dtype):\n         import jax.nn as jnn\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6bc240c3075cc1726df00bf7fd825f2ad8a38a6a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 952 | Contributors (this commit): 10 | Commits (past 90d): 28 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -301,7 +301,7 @@ def silu(x):\n def squareplus(x, b=4):\n     \"\"\"Squareplus activation function.\n \n-    The SquarePlus activation function is defined as:\n+    The Squareplus activation function is defined as:\n \n     `f(x) = (x + sqrt(x^2 + b)) / 2`\n \n\n@@ -272,9 +272,9 @@ class Squareplus(Operation):\n \n @keras_export([\"keras.ops.squareplus\", \"keras.ops.nn.squareplus\"])\n def squareplus(x, b=4):\n-    \"\"\"SquarePlus activation function.\n+    \"\"\"Squareplus activation function.\n \n-    The SquarePlus activation function is defined as:\n+    The Squareplus activation function is defined as:\n \n     `f(x) = (x + sqrt(x^2 + b)) / 2`\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f17a2c2604ca66cfb74bc7441fa0148c68b9b49d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 758 | Contributors (this commit): 10 | Commits (past 90d): 21 | Contributors (cumulative): 18 | DMM Complexity: 0.0\n\nDIFF:\n@@ -617,12 +617,15 @@ class MultiHeadAttention(Layer):\n             # the shape of the causal mask is [1, T, S]\n             mask = self._compute_causal_mask(query, value)\n             auto_mask = mask if auto_mask is None else auto_mask & mask\n+\n+        if attention_mask is not None:\n+            attention_mask = ops.cast(attention_mask, \"bool\")\n         if auto_mask is not None:\n             # merge attention_mask & automatic mask, to shape [B, T, S]\n             attention_mask = (\n                 auto_mask\n                 if attention_mask is None\n-                else ops.cast(attention_mask, bool) & auto_mask\n+                else attention_mask & auto_mask\n             )\n         return attention_mask\n \n\n@@ -340,11 +340,9 @@ class MultiHeadAttentionTest(testing.TestCase):\n             [[[1, 1, 0]] * 3 + [[0, 0, 0]] * 2]\n             + [[[1, 0, 0]] * 5]\n             + [[[1, 1, 1]] + [[0, 0, 0]] * 4]\n-        ).astype(bool)\n+        )\n         if use_causal_mask:\n-            mask = mask & np.array(\n-                [[[1, 0, 0], [1, 1, 0]] + [[1, 1, 1]] * 3]\n-            ).astype(bool)\n+            mask = mask & np.array([[[1, 0, 0], [1, 1, 0]] + [[1, 1, 1]] * 3])\n         del masked_query._keras_mask\n         del masked_value._keras_mask\n         output_with_manual_mask = layer(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b5ddf3228636575d8310ebbdc3c97cc7ad2aa09d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 870 | Contributors (this commit): 11 | Commits (past 90d): 9 | Contributors (cumulative): 19 | DMM Complexity: None\n\nDIFF:\n@@ -7,6 +7,7 @@ from tensorflow.compiler.tf2xla.python.xla import dynamic_update_slice\n from keras.src import tree\n from keras.src.backend.common import KerasVariable\n from keras.src.backend.common import global_state\n+from keras.src.backend.common import is_int_dtype\n from keras.src.backend.common import standardize_dtype\n from keras.src.backend.common.backend_utils import slice_along_axis\n from keras.src.backend.common.keras_tensor import KerasTensor\n@@ -119,9 +120,10 @@ def convert_to_tensor(x, dtype=None, sparse=None):\n     if dtype is not None:\n         dtype = standardize_dtype(dtype)\n     if not tf.is_tensor(x):\n-        if dtype == \"bool\":\n-            # TensorFlow boolean conversion is stricter than other backends.\n-            # It does not allow ints. We convert without dtype and cast instead.\n+        if dtype == \"bool\" or is_int_dtype(dtype):\n+            # TensorFlow conversion is stricter than other backends, it does not\n+            # allow ints for bools or floats for ints. We convert without dtype\n+            # and cast instead.\n             x = tf.convert_to_tensor(x)\n             return tf.cast(x, dtype)\n         return tf.convert_to_tensor(x, dtype=dtype)\n\n@@ -888,6 +888,9 @@ class CoreOpsDtypeTest(testing.TestCase):\n         (bool(0), None, \"bool\"),\n         (int(0), None, \"int32\"),\n         (float(0), None, backend.floatx()),\n+        (1, \"bool\", \"bool\"),\n+        (1.0, \"int32\", \"int32\"),\n+        (1.0, \"float32\", \"float32\"),\n         ([False, True, False], None, \"bool\"),\n         ([1, 2, 3], None, \"int32\"),\n         ([1.0, 2.0, 3.0], None, backend.floatx()),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#76fb2c6a08121951d23fd10725efd0bb0df69a37", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 49 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -31,13 +31,12 @@ class DeviceTest(testing.TestCase):\n     @pytest.mark.skipif(backend.backend() != \"jax\", reason=\"jax only\")\n     def test_jax_device_scope(self):\n         import jax\n-        from jax.lib import xla_bridge\n \n         def get_device(t):\n             # After updating to Jax 0.4.33, Directly access via t.device attr.\n             return list(t.devices())[0]\n \n-        platform = xla_bridge.get_backend().platform\n+        platform = jax.default_backend()\n \n         if platform != \"gpu\":\n             self.skipTest(\"Need at least one GPU for testing\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3b8aba12b3927e0e33b0428dfa2be956ba6f320f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 303 | Lines Deleted: 86 | Files Changed: 8 | Hunks: 38 | Methods Changed: 17 | Complexity Δ (Sum/Max): 29/18 | Churn Δ: 389 | Churn Cumulative: 1309 | Contributors (this commit): 11 | Commits (past 90d): 37 | Contributors (cumulative): 37 | DMM Complexity: 0.21348314606741572\n\nDIFF:\n@@ -5,17 +5,17 @@ since your modifications would be overwritten.\n \"\"\"\n \n from keras.src.backend.config import backend\n+from keras.src.backend.config import disable_flash_attention\n+from keras.src.backend.config import enable_flash_attention\n from keras.src.backend.config import epsilon\n from keras.src.backend.config import floatx\n from keras.src.backend.config import image_data_format\n+from keras.src.backend.config import is_flash_attention_enabled\n from keras.src.backend.config import set_epsilon\n from keras.src.backend.config import set_floatx\n from keras.src.backend.config import set_image_data_format\n from keras.src.dtype_policies.dtype_policy import dtype_policy\n from keras.src.dtype_policies.dtype_policy import set_dtype_policy\n-from keras.src.layers.attention.attention import disable_flash_attention\n-from keras.src.layers.attention.attention import enable_flash_attention\n-from keras.src.layers.attention.attention import is_flash_attention_enabled\n from keras.src.saving.serialization_lib import enable_unsafe_deserialization\n from keras.src.utils.backend_utils import set_backend\n from keras.src.utils.io_utils import disable_interactive_logging\n\n@@ -5,17 +5,17 @@ since your modifications would be overwritten.\n \"\"\"\n \n from keras.src.backend.config import backend\n+from keras.src.backend.config import disable_flash_attention\n+from keras.src.backend.config import enable_flash_attention\n from keras.src.backend.config import epsilon\n from keras.src.backend.config import floatx\n from keras.src.backend.config import image_data_format\n+from keras.src.backend.config import is_flash_attention_enabled\n from keras.src.backend.config import set_epsilon\n from keras.src.backend.config import set_floatx\n from keras.src.backend.config import set_image_data_format\n from keras.src.dtype_policies.dtype_policy import dtype_policy\n from keras.src.dtype_policies.dtype_policy import set_dtype_policy\n-from keras.src.layers.attention.attention import disable_flash_attention\n-from keras.src.layers.attention.attention import enable_flash_attention\n-from keras.src.layers.attention.attention import is_flash_attention_enabled\n from keras.src.saving.serialization_lib import enable_unsafe_deserialization\n from keras.src.utils.backend_utils import set_backend\n from keras.src.utils.io_utils import disable_interactive_logging\n\n@@ -167,6 +167,65 @@ def set_image_data_format(data_format):\n     _IMAGE_DATA_FORMAT = data_format\n \n \n+@keras_export(\"keras.config.enable_flash_attention\")\n+def enable_flash_attention():\n+    \"\"\"Enable flash attention.\n+\n+    Flash attention offers performance optimization for attention layers,\n+    making it especially useful for large language models (LLMs) that\n+    benefit from faster and more memory-efficient attention computations.\n+\n+    Once enabled, supported layers like `MultiHeadAttention` will **attempt** to\n+    use flash attention for faster computations. By default, this feature is\n+    enabled.\n+\n+    Note that enabling flash attention does not guarantee it will always be\n+    used. Typically, the inputs must be in `float16` or `bfloat16` dtype, and\n+    input layout requirements may vary depending on the backend.\n+    \"\"\"\n+    from keras.src.backend.common import global_state\n+\n+    global_state.set_global_attribute(\"flash_attention\", None)\n+\n+\n+@keras_export(\"keras.config.disable_flash_attention\")\n+def disable_flash_attention():\n+    \"\"\"Disable flash attention.\n+\n+    Flash attention offers performance optimization for attention layers,\n+    making it especially useful for large language models (LLMs) that\n+    benefit from faster and more memory-efficient attention computations.\n+\n+    Once disabled, supported layers like `MultiHeadAttention` will not\n+    use flash attention for faster computations.\n+    \"\"\"\n+    from keras.src.backend.common import global_state\n+\n+    global_state.set_global_attribute(\"flash_attention\", False)\n+\n+\n+@keras_export(\"keras.config.is_flash_attention_enabled\")\n+def is_flash_attention_enabled():\n+    \"\"\"Checks whether flash attention is globally enabled in Keras.\n+\n+    Flash attention is a performance-optimized method for computing attention\n+    in large models, such as transformers, allowing for faster and more\n+    memory-efficient operations. This function checks the global Keras\n+    configuration to determine if flash attention is enabled for compatible\n+    layers (e.g., `MultiHeadAttention`).\n+\n+    Note that enabling flash attention does not guarantee it will always be\n+    used. Typically, the inputs must be in `float16` or `bfloat16` dtype, and\n+    input layout requirements may vary depending on the backend.\n+\n+    Returns:\n+        `False` if disabled; otherwise, it indicates that it is enabled.\n+    \"\"\"\n+    from keras.src.backend.common import global_state\n+\n+    return global_state.get_global_attribute(\"flash_attention\", default=None)\n+\n+\n def standardize_data_format(data_format):\n     if data_format is None:\n         return image_data_format()\n\n@@ -1,7 +1,6 @@\n from keras.src import backend\n from keras.src import ops\n from keras.src.api_export import keras_export\n-from keras.src.backend.common import global_state\n from keras.src.layers.layer import Layer\n \n \n@@ -283,56 +282,3 @@ class Attention(Layer):\n             \"dropout\": self.dropout,\n         }\n         return {**base_config, **config}\n-\n-\n-@keras_export(\"keras.config.enable_flash_attention\")\n-def enable_flash_attention():\n-    \"\"\"Enable flash attention.\n-\n-    Flash attention offers performance optimization for attention layers,\n-    making it especially useful for large language models (LLMs) that\n-    benefit from faster and more memory-efficient attention computations.\n-\n-    Once enabled, supported layers like `MultiHeadAttention` will **attempt** to\n-    use flash attention for faster computations. By default, this feature is\n-    enabled.\n-\n-    Note that enabling flash attention does not guarantee it will always be\n-    used. Typically, the inputs must be in `float16` or `bfloat16` dtype, and\n-    input layout requirements may vary depending on the backend.\n-    \"\"\"\n-    global_state.set_global_attribute(\"flash_attention\", None)\n-\n-\n-@keras_export(\"keras.config.disable_flash_attention\")\n-def disable_flash_attention():\n-    \"\"\"Disable flash attention.\n-\n-    Flash attention offers performance optimization for attention layers,\n-    making it especially useful for large language models (LLMs) that\n-    benefit from faster and more memory-efficient attention computations.\n-\n-    Once disabled, supported layers like `MultiHeadAttention` will not\n-    use flash attention for faster computations.\n-    \"\"\"\n-    global_state.set_global_attribute(\"flash_attention\", False)\n-\n-\n-@keras_export(\"keras.config.is_flash_attention_enabled\")\n-def is_flash_attention_enabled():\n-    \"\"\"Checks whether flash attention is globally enabled in Keras.\n-\n-    Flash attention is a performance-optimized method for computing attention\n-    in large models, such as transformers, allowing for faster and more\n-    memory-efficient operations. This function checks the global Keras\n-    configuration to determine if flash attention is enabled for compatible\n-    layers (e.g., `MultiHeadAttention`).\n-\n-    Note that enabling flash attention does not guarantee it will always be\n-    used. Typically, the inputs must be in `float16` or `bfloat16` dtype, and\n-    input layout requirements may vary depending on the backend.\n-\n-    Returns:\n-        `False` if disabled; otherwise, it indicates that it is enabled.\n-    \"\"\"\n-    return global_state.get_global_attribute(\"flash_attention\", default=None)\n\n@@ -1,8 +1,11 @@\n+import math\n+\n from keras.src import constraints\n from keras.src import initializers\n from keras.src import ops\n from keras.src import regularizers\n from keras.src.api_export import keras_export\n+from keras.src.backend.config import is_flash_attention_enabled\n from keras.src.layers.activations.softmax import Softmax\n from keras.src.layers.core.einsum_dense import EinsumDense\n from keras.src.layers.layer import Layer\n@@ -34,6 +37,11 @@ class GroupedQueryAttention(Layer):\n         num_key_value_heads: Number of key and value attention heads.\n         dropout: Dropout probability.\n         use_bias: Boolean, whether the dense layers use bias vectors/matrices.\n+        flash_attention: If `None`, the layer attempts to use flash\n+            attention for faster and more memory-efficient attention\n+            computations when possible. This behavior can be configured using\n+            `keras.config.enable_flash_attention()` or\n+            `keras.config.disable_flash_attention()`.\n         kernel_initializer: Initializer for dense layer kernels.\n         bias_initializer: Initializer for dense layer biases.\n         kernel_regularizer: Regularizer for dense layer kernels.\n@@ -41,6 +49,7 @@ class GroupedQueryAttention(Layer):\n         activity_regularizer: Regularizer for dense layer activity.\n         kernel_constraint: Constraint for dense layer kernels.\n         bias_constraint: Constraint for dense layer kernels.\n+        seed: Optional integer to seed the dropout layer.\n \n     Call arguments:\n         query: Query tensor of shape `(batch_dim, target_seq_len, feature_dim)`,\n@@ -85,6 +94,7 @@ class GroupedQueryAttention(Layer):\n         num_key_value_heads,\n         dropout=0.0,\n         use_bias=True,\n+        flash_attention=None,\n         kernel_initializer=\"glorot_uniform\",\n         bias_initializer=\"zeros\",\n         kernel_regularizer=None,\n@@ -92,6 +102,7 @@ class GroupedQueryAttention(Layer):\n         activity_regularizer=None,\n         kernel_constraint=None,\n         bias_constraint=None,\n+        seed=None,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -107,6 +118,7 @@ class GroupedQueryAttention(Layer):\n         self.num_repeats = num_query_heads // num_key_value_heads\n         self.dropout = dropout\n         self.use_bias = use_bias\n+        self._flash_attention = flash_attention or is_flash_attention_enabled()\n         self.kernel_initializer = initializers.get(kernel_initializer)\n         self.bias_initializer = initializers.get(bias_initializer)\n         self.kernel_regularizer = regularizers.get(kernel_regularizer)\n@@ -114,6 +126,17 @@ class GroupedQueryAttention(Layer):\n         self.activity_regularizer = regularizers.get(activity_regularizer)\n         self.kernel_constraint = constraints.get(kernel_constraint)\n         self.bias_constraint = constraints.get(bias_constraint)\n+        self.seed = seed\n+\n+        self._inverse_sqrt_head_dim = 1.0 / math.sqrt(float(self.head_dim))\n+        self._return_attention_scores = False\n+\n+        # Check for flash attention constraints\n+        if self._flash_attention and self.dropout > 0.0:\n+            raise ValueError(\n+                \"Dropout is not supported when flash attention is enabled. \"\n+                \"Please set dropout to 0.0 to use flash attention.\"\n+            )\n \n     def build(\n         self,\n@@ -160,7 +183,7 @@ class GroupedQueryAttention(Layer):\n \n         self._softmax = Softmax(axis=-1, dtype=self.dtype_policy)\n         self._dropout_layer = Dropout(\n-            rate=self.dropout, dtype=self.dtype_policy\n+            rate=self.dropout, dtype=self.dtype_policy, seed=self.seed\n         )\n \n         self._dot_product_equation = \"bquh,bkuh->buqk\"\n@@ -213,6 +236,7 @@ class GroupedQueryAttention(Layer):\n         training=None,\n         use_causal_mask=False,\n     ):\n+        self._return_attention_scores = return_attention_scores\n         if key is None:\n             key = value\n \n@@ -353,9 +377,52 @@ class GroupedQueryAttention(Layer):\n     def _compute_attention(\n         self, query, key, value, attention_mask=None, training=None\n     ):\n+        # Check for flash attention constraints\n+        if self._flash_attention and self._return_attention_scores:\n+            raise ValueError(\n+                \"Returning attention scores is not supported when flash \"\n+                \"attention is enabled. Please disable flash attention to access\"\n+                \" attention scores.\"\n+            )\n+\n+        # Determine whether to use dot-product attention\n+        use_dot_product_attention = not (\n+            self.dropout > 0.0\n+            or self._return_attention_scores\n+            or (len(query.shape) != 4)\n+        )\n+\n+        if use_dot_product_attention:\n+            if attention_mask is not None:\n+                # Ensure attention_mask has the correct shape for broadcasting\n+                # Expected shape: [batch_size, num_heads, query_seq_len,\n+                # key_seq_len].\n+                mask_expansion_axis = -1 * 2 - 1\n+                len_attention_scores_shape = 4  # Only accepts 4D inputs\n+                for _ in range(\n+                    len_attention_scores_shape - len(attention_mask.shape)\n+                ):\n+                    attention_mask = ops.expand_dims(\n+                        attention_mask, axis=mask_expansion_axis\n+                    )\n+                attention_mask = ops.cast(attention_mask, dtype=\"bool\")\n+            # Directly compute the attention output using dot-product attention\n+            attention_output = ops.dot_product_attention(\n+                query=query,\n+                key=key,\n+                value=value,\n+                bias=None,\n+                mask=attention_mask,\n+                scale=self._inverse_sqrt_head_dim,\n+                is_causal=False,\n+                flash_attention=self._flash_attention,\n+            )\n+            return attention_output, None\n+\n+        # Default behavior without flash attention, with explicit attention\n+        # scores\n         query = ops.multiply(\n-            query,\n-            1.0 / ops.sqrt(ops.cast(self.head_dim, query.dtype)),\n+            query, ops.cast(self._inverse_sqrt_head_dim, query.dtype)\n         )\n         # Take the dot product between \"query\" and \"key\" to get the raw\n         # attention scores.\n@@ -365,7 +432,10 @@ class GroupedQueryAttention(Layer):\n         scores = self._masked_softmax(scores, attention_mask=attention_mask)\n         # This is actually dropping out entire tokens to attend to, which might\n         # seem a bit unusual, but is taken from the original Transformer paper.\n+        if self.dropout > 0.0:\n             scores_dropout = self._dropout_layer(scores, training=training)\n+        else:\n+            scores_dropout = scores\n         output = ops.einsum(self._combine_equation, scores_dropout, value)\n         return output, scores\n \n@@ -428,6 +498,7 @@ class GroupedQueryAttention(Layer):\n             ),\n             \"kernel_constraint\": constraints.serialize(self.kernel_constraint),\n             \"bias_constraint\": constraints.serialize(self.bias_constraint),\n+            \"seed\": self.seed,\n         }\n         base_config = super().get_config()\n         return {**base_config, **config}\n\n@@ -6,10 +6,24 @@ from keras.src import backend\n from keras.src import initializers\n from keras.src import layers\n from keras.src import testing\n+from keras.src.backend.config import disable_flash_attention\n+from keras.src.backend.config import enable_flash_attention\n+from keras.src.backend.config import is_flash_attention_enabled\n \n \n class GroupedQueryAttentionTest(testing.TestCase):\n+    def setUp(self):\n+        super().setUp()\n+        # Flash attention is a newly introduced feature. We need to disable it\n+        # for testing purposes.\n+        disable_flash_attention()\n+\n+    def tearDown(self):\n+        enable_flash_attention()\n+        return super().tearDown()\n+\n     def test_basics(self):\n+        self.assertFalse(is_flash_attention_enabled())\n         self.run_layer_test(\n             layers.GroupedQueryAttention,\n             init_kwargs={\n@@ -46,6 +60,98 @@ class GroupedQueryAttentionTest(testing.TestCase):\n             run_training_check=False,\n         )\n \n+    def test_basics_with_flash_attention(self):\n+        enable_flash_attention()\n+        init_kwargs = {\n+            \"num_query_heads\": 2,\n+            \"num_key_value_heads\": 2,\n+            \"head_dim\": 8,\n+            \"dtype\": \"float16\",\n+        }\n+        input_shape = {\n+            \"query_shape\": (2, 8, 16),\n+            \"value_shape\": (2, 4, 16),\n+        }\n+        expected_output_shape = (2, 8, 16)\n+        if backend.backend() in (\"tensorflow\", \"numpy\"):\n+            self.skipTest(\n+                \"Flash attention is not supported in tensorflow and numpy \"\n+                \"backends.\"\n+            )\n+        elif backend.backend() == \"torch\":\n+            try:\n+                self.run_layer_test(\n+                    layers.GroupedQueryAttention,\n+                    init_kwargs=init_kwargs,\n+                    input_shape=input_shape,\n+                    expected_output_shape=expected_output_shape,\n+                    expected_num_trainable_weights=8,\n+                    expected_num_non_trainable_weights=0,\n+                    expected_num_seed_generators=0,\n+                    expected_num_losses=0,\n+                    supports_masking=True,\n+                    run_training_check=False,\n+                )\n+            except ImportError as e:\n+                if \"Flash attention is not supported\" in str(e.args[0]):\n+                    self.assertTrue(\n+                        (\n+                            \"Flash attention is not supported in your current \"\n+                            \"PyTorch version.\"\n+                        )\n+                        in str(e.args[0])\n+                    )\n+            except RuntimeError as e:\n+                if (\n+                    \"Flash attention is not supported with the provided inputs\"\n+                    in str(e.args[0])\n+                ):\n+                    self.assertTrue(\n+                        (\n+                            \"Flash attention is not supported with the \"\n+                            \"provided inputs\"\n+                        )\n+                        in str(e.args[0])\n+                    )\n+        elif backend.backend() == \"jax\":\n+            try:\n+                self.run_layer_test(\n+                    layers.GroupedQueryAttention,\n+                    init_kwargs=init_kwargs,\n+                    input_shape=input_shape,\n+                    expected_output_shape=expected_output_shape,\n+                    expected_num_trainable_weights=8,\n+                    expected_num_non_trainable_weights=0,\n+                    expected_num_seed_generators=0,\n+                    expected_num_losses=0,\n+                    supports_masking=True,\n+                    run_training_check=False,\n+                )\n+            except ImportError as e:\n+                if \"Flash attention is not supported\" in str(e.args[0]):\n+                    self.assertTrue(\n+                        (\n+                            \"Flash attention is not supported in your current \"\n+                            \"JAX version.\"\n+                        )\n+                        in str(e.args[0])\n+                    )\n+            except RuntimeError as e:\n+                if \"cuDNN\" in str(e.args[0]):\n+                    self.assertTrue(\"cuDNN is not detected.\" in str(e.args[0]))\n+                elif \"Require at least\" in str(e.args[0]):\n+                    self.assertTrue(\n+                        \"Require at least Ampere arch to run\" in str(e.args[0])\n+                    )\n+                elif \"Flash attention\" in str(e.args[0]):\n+                    self.assertTrue(\n+                        (\n+                            \"Flash attention is not supported in your current \"\n+                            \"JAX version.\"\n+                        )\n+                        in str(e.args[0])\n+                    )\n+\n     @parameterized.named_parameters(\n         (\"without_key_proj_mha\", (4, 8), (2, 8), None, 2, 2),\n         (\"with_key_proj_mha\", (4, 8), (2, 8), (2, 3), 2, 2),\n@@ -171,39 +277,74 @@ class GroupedQueryAttentionTest(testing.TestCase):\n         )\n         self.assertAllClose(output, output_with_manual_mask)\n \n-    def test_correctness(self):\n-        query = np.array([[[1.0, 0.0], [0.0, 1.0]]])\n-        key = np.array([[[0.0, 1.0], [1.0, 0.0]]])\n-        value = np.array([[[1.0, 2.0], [3.0, 4.0]]])\n+    @parameterized.named_parameters(\n+        (\"disable_flash_attention\", False),\n+        (\"enable_flash_attention\", True),\n+    )\n+    def test_correctness(self, flash_attention):\n+        if flash_attention:\n+            # Let the backend decide whether to use flase attention\n+            enable_flash_attention()\n+        dtype = \"float16\"  # Flash attention only accepts float16/bfloat16\n+        head_dim = 8  # key_dim % 8 == 0 to enable flash attention\n+        num_query_heads = num_key_value_heads = 8\n+\n+        query = np.identity(head_dim)[np.newaxis, ...]\n+        key = np.identity(head_dim)[np.newaxis, ...]\n+        value = (\n+            np.reshape(np.arange(head_dim * head_dim), (1, head_dim, head_dim))\n+            / 100.0  # Prevent overflow/underflow\n+        )\n \n         # Setup layer.\n-        num_heads = 2\n-        key_dim = 2\n-        layer = layers.MultiHeadAttention(\n-            num_heads=num_heads,\n-            key_dim=key_dim,\n+        layer = layers.GroupedQueryAttention(\n+            head_dim=head_dim,\n+            num_query_heads=num_query_heads,\n+            num_key_value_heads=num_key_value_heads,\n+            dtype=dtype,\n         )\n         layer.build(query.shape, key.shape, value.shape)\n \n         # Set layer weights.\n-        kernel = np.identity(key_dim)\n+        kernel = np.identity(head_dim)\n         # To get an identity kernel we need to add a head dim and repeat on it.\n-        kernel = np.repeat(kernel[:, np.newaxis, :], num_heads, axis=1)\n+        kernel = np.repeat(kernel[:, np.newaxis, :], num_query_heads, axis=1)\n         # Zeros for all biases.\n-        bias = np.zeros((2, 2))\n-        output_bias = np.zeros((2,))\n+        bias = np.zeros((num_query_heads, head_dim))\n+        output_bias = np.zeros((head_dim,))\n         layer.set_weights([kernel, bias] * 3 + [kernel, output_bias])\n \n         # Call layer and assert output.\n+        expected_output = np.array(\n+            [2.406, 2.440, 2.473, 2.504, 2.535, 2.568, 2.602, 2.633]\n+        )\n+        expected_output = np.tile(\n+            expected_output[np.newaxis, :, np.newaxis], (1, 1, head_dim)\n+        )\n+        expected_score = np.array(\n+            [\n+                [0.1187] * 0 + [0.1691] + [0.1187] * 7,\n+                [0.1187] * 1 + [0.1691] + [0.1187] * 6,\n+                [0.1187] * 2 + [0.1691] + [0.1187] * 5,\n+                [0.1187] * 3 + [0.1691] + [0.1187] * 4,\n+                [0.1187] * 4 + [0.1691] + [0.1187] * 3,\n+                [0.1187] * 5 + [0.1691] + [0.1187] * 2,\n+                [0.1187] * 6 + [0.1691] + [0.1187] * 1,\n+                [0.1187] * 7 + [0.1691] + [0.1187] * 0,\n+            ]\n+        )\n+        expected_score = np.tile(\n+            expected_score[np.newaxis, np.newaxis, ...], (1, head_dim, 1, 1)\n+        )\n+        if flash_attention:\n+            output = layer(query=query, value=value, key=key)\n+            self.assertAllClose(output, expected_output, atol=1e-2)\n+        else:\n             output, scores = layer(\n                 query=query,\n                 value=value,\n                 key=key,\n                 return_attention_scores=True,\n             )\n-        self.assertAllClose(output, [[[5.679, 5.679], [4.32, 4.32]]], atol=1e-3)\n-        self.assertAllClose(\n-            scores,\n-            [[[[0.33, 0.67], [0.67, 0.33]], [[0.33, 0.67], [0.67, 0.33]]]],\n-            atol=1e-3,\n-        )\n+            self.assertAllClose(output, expected_output, atol=1e-2)\n+            self.assertAllClose(scores, expected_score, atol=1e-2)\n\n@@ -9,8 +9,8 @@ from keras.src import initializers\n from keras.src import ops\n from keras.src import regularizers\n from keras.src.api_export import keras_export\n+from keras.src.backend.config import is_flash_attention_enabled\n from keras.src.layers.activations.softmax import Softmax\n-from keras.src.layers.attention.attention import is_flash_attention_enabled\n from keras.src.layers.core.einsum_dense import EinsumDense\n from keras.src.layers.layer import Layer\n from keras.src.layers.regularization.dropout import Dropout\n\n@@ -14,9 +14,9 @@ from keras.src import ops\n from keras.src import random\n from keras.src import saving\n from keras.src import testing\n-from keras.src.layers.attention.attention import disable_flash_attention\n-from keras.src.layers.attention.attention import enable_flash_attention\n-from keras.src.layers.attention.attention import is_flash_attention_enabled\n+from keras.src.backend.config import disable_flash_attention\n+from keras.src.backend.config import enable_flash_attention\n+from keras.src.backend.config import is_flash_attention_enabled\n \n \n class MultiHeadAttentionTest(testing.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#660da946d33ccbb53575f15a17abdfd725cd2086", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 17 | Churn Cumulative: 234 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -498,24 +498,25 @@ class FunctionalTest(testing.TestCase):\n         # Note: it's not intended to work in symbolic mode (yet).\n \n     def test_warning_for_mismatched_inputs_structure(self):\n+        def is_input_warning(w):\n+            return str(w.message).startswith(\n+                \"The structure of `inputs` doesn't match the expected structure\"\n+            )\n+\n         i1 = Input((2,))\n         i2 = Input((2,))\n         outputs = layers.Add()([i1, i2])\n-        model = Model({\"i1\": i1, \"i2\": i2}, outputs)\n \n-        with pytest.warns() as record:\n+        model = Model({\"i1\": i1, \"i2\": i2}, outputs)\n+        with pytest.warns() as warning_logs:\n             model.predict([np.ones((2, 2)), np.zeros((2, 2))], verbose=0)\n-        self.assertLen(record, 1)\n-        self.assertStartsWith(\n-            str(record[0].message),\n-            r\"The structure of `inputs` doesn't match the expected structure:\",\n-        )\n+            self.assertLen(list(filter(is_input_warning, warning_logs)), 1)\n \n         # No warning for mismatched tuples and lists.\n         model = Model([i1, i2], outputs)\n         with warnings.catch_warnings(record=True) as warning_logs:\n             model.predict((np.ones((2, 2)), np.zeros((2, 2))), verbose=0)\n-            self.assertLen(warning_logs, 0)\n+            self.assertLen(list(filter(is_input_warning, warning_logs)), 0)\n \n     def test_for_functional_in_sequential(self):\n         # Test for a v3.4.1 regression.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c802ca6421408b0f994a1497717ae3f2c89e1ff1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1147 | Contributors (this commit): 8 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -923,8 +923,8 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n \n     Standalone usage:\n \n-    >>> y_true = [[0, 1, 0], [0, 0, 1]]\n-    >>> y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n+    >>> y_true = np.array([[0, 1, 0], [0, 0, 1]])\n+    >>> y_pred = np.array([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n     >>> # Using 'auto'/'sum_over_batch_size' reduction type.\n     >>> cce = keras.losses.CategoricalCrossentropy()\n     >>> cce(y_true, y_pred)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4895458b7a9ba2b561cb534f63cec10ee393eb76", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 51 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 54 | Churn Cumulative: 245 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -232,14 +232,25 @@ class GroupedQueryAttentionTest(testing.TestCase):\n     )\n     def test_query_mask_propagation(self):\n         \"\"\"Test automatic propagation of the query's mask.\"\"\"\n+        try:\n             layer = layers.GroupedQueryAttention(\n                 num_query_heads=2, num_key_value_heads=2, head_dim=2\n             )\n             self.assertTrue(layer.supports_masking)\n-        query = np.array([[1, 2, 3, 0, 0], [3, 3, 1, 1, 2], [1, 0, 0, 0, 0]])\n+            query = np.array(\n+                [[1, 2, 3, 0, 0], [3, 3, 1, 1, 2], [1, 0, 0, 0, 0]]\n+            )\n             masked_query = layers.Embedding(4, 8, mask_zero=True)(query)\n             value = np.random.normal(size=(3, 3, 8))\n             output = layer(query=masked_query, value=value)\n+        except RuntimeError as e:\n+            if e.args[0].startswith(\n+                \"(*bias): last dimension must be contiguous\"\n+            ):\n+                self.skipTest(\n+                    \"PyTorch errors out on GPU: issue to track bug is here \"\n+                    \"https://github.com/keras-team/keras/issues/20459\"\n+                )\n         self.assertAllClose(masked_query._keras_mask, output._keras_mask)\n \n     @parameterized.named_parameters((\"causal\", True), (\"not_causal\", 0))\n@@ -278,8 +289,7 @@ class GroupedQueryAttentionTest(testing.TestCase):\n         self.assertAllClose(output, output_with_manual_mask)\n \n     @parameterized.named_parameters(\n-        (\"disable_flash_attention\", False),\n-        (\"enable_flash_attention\", True),\n+        (\"disable_flash_attention\", False), (\"enable_flash_attention\", True)\n     )\n     def test_correctness(self, flash_attention):\n         if flash_attention:\n@@ -348,3 +358,41 @@ class GroupedQueryAttentionTest(testing.TestCase):\n             )\n             self.assertAllClose(output, expected_output, atol=1e-2)\n             self.assertAllClose(scores, expected_score, atol=1e-2)\n+\n+    def test_flash_attention_with_errors(self):\n+        if backend.backend() in (\"numpy\", \"tensorflow\"):\n+            pytest.skip(\n+                reason=(\n+                    \"Flash attention is not supported on tensorflow and numpy.\"\n+                )\n+            )\n+        # Check `flash_attention=True` and `dropout=0.1`\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Dropout is not supported when flash attention is enabled.\",\n+        ):\n+            layer = layers.GroupedQueryAttention(\n+                head_dim=2,\n+                num_query_heads=2,\n+                num_key_value_heads=2,\n+                flash_attention=True,\n+                dropout=0.1,\n+            )\n+\n+        # Check `flash_attention=True` and `return_attention_scores=True`\n+        layer = layers.GroupedQueryAttention(\n+            head_dim=2,\n+            num_query_heads=2,\n+            num_key_value_heads=2,\n+            flash_attention=True,\n+        )\n+        self.assertTrue(layer._flash_attention)\n+        query = np.random.random((2, 4, 8))\n+        value = np.random.random((2, 4, 8))\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Returning attention scores is not supported when flash \"\n+            \"attention is enabled. Please disable flash attention to access\"\n+            \" attention scores.\",\n+        ):\n+            layer(query=query, value=value, return_attention_scores=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#495a570c5350e689d686c572009811cc4492d8c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 54 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -63,7 +63,7 @@ class RNN(Layer):\n             merging bidirectional RNNs.\n \n     Call arguments:\n-        inputs: Input tensor.\n+        sequences: A 3-D tensor with shape `(batch_size, timesteps, features)`.\n         initial_state: List of initial state tensors to be passed to the first\n             call of the cell.\n         mask: Binary tensor of shape `[batch_size, timesteps]`\n@@ -76,9 +76,6 @@ class RNN(Layer):\n             to the cell when calling it.\n             This is for use with cells that use dropout.\n \n-    Input shape:\n-        3-D tensor with shape `(batch_size, timesteps, features)`.\n-\n     Output shape:\n \n     - If `return_state`: a list of tensors. The first tensor is\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
