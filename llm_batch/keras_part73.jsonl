{"custom_id": "keras#1a6cb71732fe2a1297a6a3d92644f45df3211d7e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 4306 | Contributors (this commit): 36 | Commits (past 90d): 35 | Contributors (cumulative): 36 | DMM Complexity: None\n\nDIFF:\n@@ -238,8 +238,8 @@ class Model(object):\n         nb_train_sample = len(ins[0])\n         index_array = np.arange(nb_train_sample)\n \n-        history = cbks.History()\n-        callbacks = [cbks.BaseLogger()] + callbacks + [history]\n+        self.history = cbks.History()\n+        callbacks = [cbks.BaseLogger()] + callbacks + [self.history]\n         if verbose:\n             callbacks += [cbks.ProgbarLogger()]\n         callbacks = cbks.CallbackList(callbacks)\n@@ -303,7 +303,7 @@ class Model(object):\n                 break\n \n         callbacks.on_train_end()\n-        return history\n+        return self.history\n \n     def _predict_loop(self, f, ins, batch_size=128, verbose=0):\n         '''Abstract method to loop over some data in batches.\n@@ -410,7 +410,6 @@ class Model(object):\n         import json\n \n         def get_json_type(obj):\n-\n             # if obj is any numpy type\n             if type(obj).__module__ == np.__name__:\n                 return obj.item()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eeb576b12ff617285632bc00b22ebf6d67a025d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 4314 | Contributors (this commit): 37 | Commits (past 90d): 36 | Contributors (cumulative): 37 | DMM Complexity: 0.0\n\nDIFF:\n@@ -202,10 +202,12 @@ def model_from_config(config, custom_objects={}):\n         optimizer = optimizers.get(optimizer_name, optimizer_params)\n \n         if model_name == 'Sequential':\n+            sample_weight_mode = config.get('sample_weight_mode')\n             model.compile(loss=loss, optimizer=optimizer,\n-                          class_mode=class_mode)\n+                          class_mode=class_mode, sample_weight_mode=sample_weight_mode)\n         elif model_name == 'Graph':\n-            model.compile(loss=loss, optimizer=optimizer)\n+            sample_weight_modes = config.get('sample_weight_modes', {})\n+            model.compile(loss=loss, optimizer=optimizer, sample_weight_modes=sample_weight_modes)\n     return model\n \n \n@@ -372,7 +374,7 @@ class Model(object):\n         `keras.models.model_from_config(config, custom_objects={})`.\n         '''\n         config = super(Model, self).get_config()\n-        for p in ['class_mode']:\n+        for p in ['class_mode', 'sample_weight_mode', 'sample_weight_modes']:\n             if hasattr(self, p):\n                 config[p] = getattr(self, p)\n         if hasattr(self, 'optimizer'):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#59a714abe8704ea49447fb956bbb7249c6eb3313", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 273 | Contributors (this commit): 8 | Commits (past 90d): 10 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -34,7 +34,6 @@ def random_shift(x, wrg, hrg, fill_mode=\"nearest\", cval=0.):\n         shift_x = random.uniform(-wrg, wrg) * x.shape[2]\n     if hrg:\n         shift_y = random.uniform(-hrg, hrg) * x.shape[1]\n-        \n     x = ndimage.interpolation.shift(x, (0, shift_y, shift_x),\n                                     order=0,\n                                     mode=fill_mode,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ab179fab89d9e18eca1c86de0e9f581b08f1999c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 21 | Churn Cumulative: 124 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,7 +4,8 @@ from __future__ import print_function\n import tarfile\n import os\n import sys\n-from six.moves.urllib.request import urlopen, build_opener, install_opener\n+import shutil\n+from six.moves.urllib.request import urlopen\n from six.moves.urllib.error import URLError, HTTPError\n \n from ..utils.generic_utils import Progbar\n@@ -17,7 +18,6 @@ if sys.version_info[0] == 2:\n         def chunk_read(response, chunk_size=8192, reporthook=None):\n             total_size = response.info().get('Content-Length').strip()\n             total_size = int(total_size)\n-            bytes_so_far = 0\n             count = 0\n             while 1:\n                 chunk = response.read(chunk_size)\n@@ -63,19 +63,32 @@ def get_file(fname, origin, untar=False):\n                 progbar.update(count*block_size)\n \n         error_msg = 'URL fetch failure on {}: {} -- {}'\n+        try:\n             try:\n                 urlretrieve(origin, fpath, dl_progress)\n             except URLError as e:\n-            raise Exception(error_msg.format(url, e.errno, e.reason))\n+                raise Exception(error_msg.format(origin, e.errno, e.reason))\n             except HTTPError as e:\n-            raise Exception(error_msg.format(url, e.code, e.msg))\n+                raise Exception(error_msg.format(origin, e.code, e.msg))\n+        except (Exception, KeyboardInterrupt) as e:\n+            if os.path.exists(fpath):\n+                os.remove(fpath)\n+            raise e\n         progbar = None\n \n     if untar:\n         if not os.path.exists(untar_fpath):\n             print('Untaring file...')\n             tfile = tarfile.open(fpath, 'r:gz')\n+            try:\n                 tfile.extractall(path=datadir)\n+            except (Exception, KeyboardInterrupt) as e:\n+                if os.path.exists(untar_fpath):\n+                    if os.path.isfile(untar_fpath):\n+                        os.remove(untar_fpath)\n+                    else:\n+                        shutil.rmtree(untar_fpath)\n+                raise e\n             tfile.close()\n         return untar_fpath\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0f3f56327b661ade7824b18811163138755122dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 4332 | Contributors (this commit): 37 | Commits (past 90d): 36 | Contributors (cumulative): 37 | DMM Complexity: 1.0\n\nDIFF:\n@@ -929,11 +929,10 @@ class Sequential(Model, containers.Sequential):\n         metrics = ['loss', 'acc', 'val_loss', 'val_acc']\n \n         # prepare callbacks\n-        history = cbks.History()\n+        self.history = cbks.History()\n+        callbacks = [cbks.BaseLogger()] + callbacks + [self.history]\n         if verbose:\n-            callbacks = [history, cbks.BaseLogger()] + callbacks\n-        else:\n-            callbacks = [history] + callbacks\n+            callbacks += [cbks.ProgbarLogger()]\n         callbacks = cbks.CallbackList(callbacks)\n \n         callbacks._set_model(self)\n@@ -1069,7 +1068,7 @@ class Sequential(Model, containers.Sequential):\n                 break\n         _stop.set()\n         callbacks.on_train_end()\n-        return history\n+        return self.history\n \n \n class Graph(Model, containers.Graph):\n@@ -1401,11 +1400,10 @@ class Graph(Model, containers.Graph):\n             class_weight = {}\n \n         # prepare callbacks\n-        history = cbks.History()\n+        self.history = cbks.History()\n+        callbacks = [cbks.BaseLogger()] + callbacks + [self.history]\n         if verbose:\n-            callbacks = [history, cbks.BaseLogger()] + callbacks\n-        else:\n-            callbacks = [history] + callbacks\n+            callbacks += [cbks.ProgbarLogger()]\n         callbacks = cbks.CallbackList(callbacks)\n \n         callbacks._set_model(self)\n@@ -1536,4 +1534,4 @@ class Graph(Model, containers.Graph):\n                 break\n         _stop.set()\n         callbacks.on_train_end()\n-        return history\n+        return self.history\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#47aafaaca029fcd249651dfdf1aa2e468229e0e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 67 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 70 | Churn Cumulative: 2314 | Contributors (this commit): 21 | Commits (past 90d): 58 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -156,9 +156,9 @@ class Sequential(Layer):\n         return weights\n \n     def set_weights(self, weights):\n-        for i in range(len(self.layers)):\n-            nb_param = len(self.layers[i].trainable_weights) + len(self.layers[i].non_trainable_weights)\n-            self.layers[i].set_weights(weights[:nb_param])\n+        for layer in self.layers:\n+            nb_param = len(layer.get_weights())\n+            layer.set_weights(weights[:nb_param])\n             weights = weights[nb_param:]\n \n     def get_config(self):\n\n@@ -125,6 +125,70 @@ def test_sequential():\n     model = model_from_yaml(yaml_data)\n \n \n+def test_nested_sequential():\n+    (X_train, y_train), (X_test, y_test) = _get_test_data()\n+\n+    inner = Sequential()\n+    inner.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    inner.add(Activation('relu'))\n+    inner.add(Dense(nb_class))\n+\n+    middle = Sequential()\n+    middle.add(inner)\n+\n+    model = Sequential()\n+    model.add(middle)\n+    model.add(Activation('softmax'))\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+    model.summary()\n+\n+    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=1, validation_data=(X_test, y_test))\n+    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=2, validation_data=(X_test, y_test))\n+    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=2, validation_split=0.1)\n+    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=1, validation_split=0.1)\n+    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+    model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, shuffle=False)\n+\n+    model.train_on_batch(X_train[:32], y_train[:32])\n+\n+    loss = model.evaluate(X_test, y_test, verbose=0)\n+    assert(loss < 0.8)\n+\n+    model.predict(X_test, verbose=0)\n+    model.predict_classes(X_test, verbose=0)\n+    model.predict_proba(X_test, verbose=0)\n+    model.get_config(verbose=0)\n+\n+    fname = 'test_nested_sequential_temp.h5'\n+    model.save_weights(fname, overwrite=True)\n+\n+    inner = Sequential()\n+    inner.add(Dense(nb_hidden, input_shape=(input_dim,)))\n+    inner.add(Activation('relu'))\n+    inner.add(Dense(nb_class))\n+\n+    middle = Sequential()\n+    middle.add(inner)\n+\n+    model = Sequential()\n+    model.add(middle)\n+    model.add(Activation('softmax'))\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+    model.load_weights(fname)\n+    os.remove(fname)\n+\n+    nloss = model.evaluate(X_test, y_test, verbose=0)\n+    assert(loss == nloss)\n+\n+    # test json serialization\n+    json_data = model.to_json()\n+    model = model_from_json(json_data)\n+\n+    # test yaml serialization\n+    yaml_data = model.to_yaml()\n+    model = model_from_yaml(yaml_data)\n+\n+\n def test_merge_sum():\n     (X_train, y_train), (X_test, y_test) = _get_test_data()\n     left = Sequential()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e501cd664e19b577560127bb594a38948a0e40dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1335 | Contributors (this commit): 13 | Commits (past 90d): 12 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -255,7 +255,7 @@ class ModelCheckpoint(Callback):\n \n         if mode not in ['auto', 'min', 'max']:\n             warnings.warn('ModelCheckpoint mode %s is unknown, '\n-                          'fallback to auto mode.' % (self.mode),\n+                          'fallback to auto mode.' % (mode),\n                           RuntimeWarning)\n             mode = 'auto'\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5b3809394cc6802909280be4c84009ff6bdd3e4d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 315 | Lines Deleted: 48 | Files Changed: 6 | Hunks: 62 | Methods Changed: 28 | Complexity Δ (Sum/Max): 49/34 | Churn Δ: 363 | Churn Cumulative: 11239 | Contributors (this commit): 59 | Commits (past 90d): 156 | Contributors (cumulative): 100 | DMM Complexity: 0.42613636363636365\n\nDIFF:\n@@ -399,7 +399,7 @@ def gradients(loss, variables):\n # CONTROL FLOW\n \n def rnn(step_function, inputs, initial_states,\n-        go_backwards=False, mask=None):\n+        go_backwards=False, mask=None, constants=None):\n     '''Iterates over the time dimension of a tensor.\n \n     # Arguments\n@@ -422,6 +422,7 @@ def rnn(step_function, inputs, initial_states,\n             the time dimension in reverse order.\n         mask: binary tensor with shape (samples, time, 1),\n             with a zero for every element that is masked.\n+        constants: a list of constant values passed at each step.\n \n     # Returns\n         A tuple (last_output, outputs, new_states).\n@@ -437,6 +438,8 @@ def rnn(step_function, inputs, initial_states,\n     axes = [1, 0] + list(range(2, ndim))\n     inputs = tf.transpose(inputs, (axes))\n     input_list = tf.unpack(inputs)\n+    if constants is None:\n+        constants = []\n \n     states = initial_states\n     successive_states = []\n@@ -453,7 +456,7 @@ def rnn(step_function, inputs, initial_states,\n         mask_list = tf.unpack(mask)\n \n         for input, mask_t in zip(input_list, mask_list):\n-            output, new_states = step_function(input, states)\n+            output, new_states = step_function(input, states + constants)\n \n             # tf.select needs its condition tensor to be the same shape as its two\n             # result tensors, but in our case the condition (mask) tensor is\n@@ -481,7 +484,7 @@ def rnn(step_function, inputs, initial_states,\n             successive_states.append(states)\n     else:\n         for input in input_list:\n-            output, states = step_function(input, states)\n+            output, states = step_function(input, states + constants)\n             successive_outputs.append(output)\n             successive_states.append(states)\n \n@@ -704,3 +707,11 @@ def random_uniform(shape, low=0.0, high=1.0, dtype=_FLOATX, seed=None):\n         seed = np.random.randint(10e6)\n     return tf.random_uniform(shape, minval=low, maxval=high,\n                              dtype=dtype, seed=seed)\n+\n+\n+def random_binomial(shape, p=0.0, dtype=_FLOATX, seed=None):\n+    if seed is None:\n+        seed = np.random.randint(10e6)\n+    return tf.select(tf.random_uniform(shape, dtype=dtype, seed=seed) <= p, \n+        tf.ones(shape), tf.zeros(shape))\n+\n\n@@ -395,7 +395,7 @@ def gradients(loss, variables):\n # CONTROL FLOW\n \n def rnn(step_function, inputs, initial_states,\n-        go_backwards=False, mask=None):\n+        go_backwards=False, mask=None, constants=None):\n     '''Iterates over the time dimension of a tensor.\n \n     Parameters\n@@ -419,6 +419,7 @@ def rnn(step_function, inputs, initial_states,\n         the time dimension in reverse order.\n     mask: binary tensor with shape (samples, time),\n         with a zero for every element that is masked.\n+    constants: a list of constant values passed at each step.\n \n     Returns\n     -------\n@@ -460,6 +461,7 @@ def rnn(step_function, inputs, initial_states,\n             _step,\n             sequences=[inputs, mask],\n             outputs_info=[initial_output] + initial_states,\n+            non_sequences=constants,\n             go_backwards=go_backwards)\n     else:\n         def _step(input, *states):\n@@ -470,6 +472,7 @@ def rnn(step_function, inputs, initial_states,\n             _step,\n             sequences=inputs,\n             outputs_info=[None] + initial_states,\n+            non_sequences=constants,\n             go_backwards=go_backwards)\n \n     # deal with Theano API inconsistency\n@@ -693,6 +696,13 @@ def random_uniform(shape, low=0.0, high=1.0, dtype=_FLOATX, seed=None):\n     rng = RandomStreams(seed=seed)\n     return rng.uniform(shape, low=low, high=high, dtype=dtype)\n \n+\n+def random_binomial(shape, p=0.0, dtype=_FLOATX, seed=None):\n+    if seed is None:\n+        seed = np.random.randint(10e6)\n+    rng = RandomStreams(seed=seed)\n+    return rng.binomial(shape, p=p, dtype=dtype)\n+\n '''\n more TODO:\n \n\n@@ -493,3 +493,74 @@ class TensorBoard(Callback):\n             summary_value.tag = name\n             self.writer.add_summary(summary, epoch)\n         self.writer.flush()\n+\n+\n+class ModelTest(Callback):\n+    ''' Test model at the end of every X epochs.\n+\n+    The model is tested using both MC dropout and the dropout \n+    approximation. Output metrics for various losses are supported.\n+\n+    # Arguments\n+        Xt: model inputs to test.\n+        Yt: model outputs to get accuracy / error (ground truth).\n+        T: number of samples to use in MC dropout.\n+        test_every_X_epochs: test every test_every_X_epochs epochs.\n+        batch_size: number of data points to put in each batch \n+            (often larger than training batch size).\n+        verbose: verbosity mode, 0 or 1.\n+        loss: a string from ['binary', 'categorical', 'euclidean'] \n+            used to calculate the testing metric.\n+        mean_y_train: mean of outputs in regression cases to add back \n+            to model output ('euclidean' loss).\n+        std_y_train: std of outputs in regression cases to add back \n+            to model output ('euclidean' loss).\n+\n+    # References\n+        - [Dropout: A simple way to prevent neural networks from overfitting](http://jmlr.org/papers/v15/srivastava14a.html)\n+        - [Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning](http://arxiv.org/abs/1506.02142)\n+    '''\n+    def __init__(self, Xt, Yt, T=10, test_every_X_epochs=1, batch_size=500, verbose=1, \n+        loss=None, mean_y_train=None, std_y_train=None):\n+        super(ModelTest, self).__init__()\n+        self.Xt = Xt\n+        self.Yt = np.array(Yt)\n+        self.T = T\n+        self.test_every_X_epochs = test_every_X_epochs\n+        self.batch_size = batch_size\n+        self.verbose = verbose\n+        self.loss = loss\n+        self.mean_y_train = mean_y_train\n+        self.std_y_train = std_y_train\n+\n+    def on_epoch_begin(self, epoch, logs={}):\n+        if epoch % self.test_every_X_epochs != 0:\n+            return\n+        model_output = self.model.predict(self.Xt, \n+            batch_size=self.batch_size, verbose=self.verbose)\n+        MC_model_output = []\n+        for _ in xrange(self.T):\n+            MC_model_output += [self.model.predict_stochastic(self.Xt, \n+                batch_size=self.batch_size, verbose=self.verbose)]\n+        MC_model_output = np.array(MC_model_output)\n+        MC_model_output_mean = np.mean(MC_model_output, 0)\n+\n+        if self.loss == 'binary':\n+            standard_acc = np.mean(self.Yt == np.round(model_output.flatten()))\n+            MC_acc = np.mean(self.Yt == np.round(MC_model_output_mean.flatten()))\n+            print(\"Standard accuracy at epoch %05d: %0.5f\" % (epoch, float(standard_acc)))\n+            print(\"MC accuracy at epoch %05d: %0.5f\" % (epoch, float(MC_acc)))\n+        elif self.loss == 'categorical':\n+            standard_acc = np.mean(np.argmax(self.Yt, axis=-1) == np.argmax(model_output, axis=-1))\n+            MC_acc = np.mean(np.argmax(self.Yt, axis=-1) == np.argmax(MC_model_output_mean, axis=-1))\n+            print(\"Standard accuracy at epoch %05d: %0.5f\" % (epoch, float(standard_acc)))\n+            print(\"MC accuracy at epoch %05d: %0.5f\" % (epoch, float(MC_acc)))\n+        elif self.loss == 'euclidean':\n+            model_output = model_output * self.std_y_train + self.mean_y_train\n+            standard_err = np.mean((self.Yt - model_output)**2.0, 0)**0.5\n+            MC_model_output_mean = MC_model_output_mean * self.std_y_train + self.mean_y_train\n+            MC_err = np.mean((self.Yt - MC_model_output_mean)**2.0, 0)**0.5\n+            print(\"Standard error at epoch %05d: %0.5f\" % (epoch, float(standard_err)))\n+            print(\"MC error at epoch %05d: %0.5f\" % (epoch, float(MC_err)))\n+        else:\n+            raise Exception('No loss: ' + loss)\n\n@@ -1,6 +1,7 @@\n from __future__ import absolute_import\n-from .. import backend as K\n+import numpy as np\n \n+from .. import backend as K\n from .. import activations, initializations, regularizers, constraints\n from ..layers.core import Layer, MaskedLayer\n \n@@ -42,6 +43,10 @@ class Embedding(Layer):\n           This argument is required if you are going to connect\n           `Flatten` then `Dense` layers upstream\n           (without it, the shape of the dense outputs cannot be computed).\n+      p: float between 0 and 1. Fraction of the embeddings to drop.\n+\n+    # References\n+        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n     '''\n     input_ndim = 2\n \n@@ -50,12 +55,13 @@ class Embedding(Layer):\n                  W_regularizer=None, activity_regularizer=None,\n                  W_constraint=None,\n                  mask_zero=False,\n-                 weights=None, **kwargs):\n+                 weights=None, p=0., **kwargs):\n         self.input_dim = input_dim\n         self.output_dim = output_dim\n         self.init = initializations.get(init)\n         self.input_length = input_length\n         self.mask_zero = mask_zero\n+        self.p = p\n \n         self.W_constraint = constraints.get(W_constraint)\n         self.constraints = [self.W_constraint]\n@@ -97,7 +103,12 @@ class Embedding(Layer):\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        out = K.gather(self.W, X)\n+        retain_p = 1. - self.p\n+        if train and self.p > 0:\n+            B = K.random_binomial((1, self.input_dim), p=retain_p)\n+        else:\n+            B = K.ones((1, self.input_dim)) * retain_p\n+        out = K.gather(self.W * B[0][:, None], X) # (self.W * B[0][:, None])[X] - we zero-out rows of W at random\n         return out\n \n     def get_config(self):\n@@ -109,6 +120,7 @@ class Embedding(Layer):\n                   \"mask_zero\": self.mask_zero,\n                   \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n                   \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n-                  \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None}\n+                  \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None,\n+                  \"p\": self.p}\n         base_config = super(Embedding, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n\n@@ -3,7 +3,7 @@ from __future__ import absolute_import\n import numpy as np\n \n from .. import backend as K\n-from .. import activations, initializations\n+from .. import activations, initializations, regularizers\n from ..layers.core import MaskedLayer\n \n \n@@ -112,6 +112,9 @@ class Recurrent(MaskedLayer):\n     def step(self, x, states):\n         raise NotImplementedError\n \n+    def get_constants(self, X, train=False):\n+        return None\n+\n     def get_initial_states(self, X):\n         # build an all-zero tensor of shape (samples, output_dim)\n         initial_state = K.zeros_like(X)  # (samples, timesteps, input_dim)\n@@ -125,6 +128,7 @@ class Recurrent(MaskedLayer):\n         # input shape: (nb_samples, time (padded with zeros), input_dim)\n         X = self.get_input(train)\n         mask = self.get_input_mask(train)\n+        constants = self.get_constants(X, train)\n \n         assert K.ndim(X) == 3\n         if K._BACKEND == 'tensorflow':\n@@ -146,7 +150,8 @@ class Recurrent(MaskedLayer):\n         last_output, outputs, states = K.rnn(self.step, X,\n                                              initial_states,\n                                              go_backwards=self.go_backwards,\n-                                             mask=mask)\n+                                             mask=mask, \n+                                             constants=constants)\n         if self.stateful:\n             self.updates = []\n             for i in range(len(states)):\n@@ -173,7 +178,7 @@ class Recurrent(MaskedLayer):\n \n \n class SimpleRNN(Recurrent):\n-    '''Fully-connected RNN where the output is to fed back to input.\n+    '''Fully-connected RNN where the output is to be fed back to input.\n \n     # Arguments\n         output_dim: dimension of the internal projections and the final output.\n@@ -184,14 +189,31 @@ class SimpleRNN(Recurrent):\n         activation: activation function.\n             Can be the name of an existing function (str),\n             or a Theano function (see: [activations](../activations.md)).\n+        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n+            (eg. L1 or L2 regularization), applied to the input weights matrices.\n+        U_regularizer: instance of [WeightRegularizer](../regularizers.md)\n+            (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n+        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n+            applied to the bias.\n+        p_W: float between 0 and 1. Fraction of the input units to drop for inputs.\n+        p_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n+\n+    # References\n+        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n     '''\n     def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal',\n-                 activation='sigmoid', **kwargs):\n+                 activation='sigmoid', \n+                 W_regularizer=None, U_regularizer=None, b_regularizer=None, \n+                 p_W=0., p_U=0., **kwargs):\n         self.output_dim = output_dim\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n         self.activation = activations.get(activation)\n+        self.W_regularizer = W_regularizer\n+        self.U_regularizer = U_regularizer\n+        self.b_regularizer = b_regularizer\n+        self.p_W, self.p_U = p_W, p_U\n         super(SimpleRNN, self).__init__(**kwargs)\n \n     def build(self):\n@@ -207,6 +229,18 @@ class SimpleRNN(Recurrent):\n         self.W = self.init((input_dim, self.output_dim))\n         self.U = self.inner_init((self.output_dim, self.output_dim))\n         self.b = K.zeros((self.output_dim,))\n+\n+        self.regularizers = []\n+        def appendRegulariser(input_regulariser, param, regularizers_list):\n+            regulariser = regularizers.get(input_regulariser)\n+            if regulariser:\n+                regulariser.set_param(param)\n+                regularizers_list.append(regulariser)\n+\n+        appendRegulariser(self.W_regularizer, self.W, self.regularizers)\n+        appendRegulariser(self.U_regularizer, self.U, self.regularizers)\n+        appendRegulariser(self.b_regularizer, self.b, self.regularizers)\n+\n         self.trainable_weights = [self.W, self.U, self.b]\n \n         if self.initial_weights is not None:\n@@ -228,17 +262,36 @@ class SimpleRNN(Recurrent):\n \n     def step(self, x, states):\n         # states only contains the previous output.\n-        assert len(states) == 1\n+        assert len(states) == 3 # 1 state and 2 constants\n         prev_output = states[0]\n-        h = K.dot(x, self.W) + self.b\n-        output = self.activation(h + K.dot(prev_output, self.U))\n+        B_W = states[1]\n+        B_U = states[2]\n+        h = K.dot(x * B_W, self.W) + self.b\n+        output = self.activation(h + K.dot(prev_output * B_U, self.U))\n         return output, [output]\n \n+    def get_constants(self, X, train=False):\n+        nb_samples = X.shape[0]\n+        retain_p_W = 1. - self.p_W\n+        retain_p_U = 1. - self.p_U\n+        if train and self.p_W > 0 and self.p_U > 0:\n+            B_W = K.random_binomial((nb_samples, self.input_dim), p=retain_p_W)\n+            B_U = K.random_binomial((nb_samples, self.output_dim), p=retain_p_U)\n+        else:\n+            B_W = retain_p_W\n+            B_U = retain_p_U\n+        return [B_W, B_U]\n+\n     def get_config(self):\n         config = {\"output_dim\": self.output_dim,\n                   \"init\": self.init.__name__,\n                   \"inner_init\": self.inner_init.__name__,\n-                  \"activation\": self.activation.__name__}\n+                  \"activation\": self.activation.__name__,\n+                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n+                  \"U_regularizer\": self.U_regularizer.get_config() if self.U_regularizer else None,\n+                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n+                  \"p_W\": self.p_W, \n+                  \"p_U\": self.p_U}\n         base_config = super(SimpleRNN, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -256,20 +309,34 @@ class GRU(Recurrent):\n             Can be the name of an existing function (str),\n             or a Theano function (see: [activations](../activations.md)).\n         inner_activation: activation function for the inner cells.\n+        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n+            (eg. L1 or L2 regularization), applied to the input weights matrices.\n+        U_regularizer: instance of [WeightRegularizer](../regularizers.md)\n+            (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n+        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n+            applied to the bias.\n+        p_W: float between 0 and 1. Fraction of the input units to drop for inputs.\n+        p_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n \n     # References\n         - [On the Properties of Neural Machine Translation: Encoder–Decoder Approaches](http://www.aclweb.org/anthology/W14-4012)\n         - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/pdf/1412.3555v1.pdf)\n+        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n     '''\n     def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  activation='sigmoid', inner_activation='hard_sigmoid',\n-                 **kwargs):\n+                 W_regularizer=None, U_regularizer=None, b_regularizer=None, \n+                 p_W=0., p_U=0., **kwargs):\n         self.output_dim = output_dim\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n         self.activation = activations.get(activation)\n         self.inner_activation = activations.get(inner_activation)\n+        self.W_regularizer = W_regularizer\n+        self.U_regularizer = U_regularizer\n+        self.b_regularizer = b_regularizer\n+        self.p_W, self.p_U = p_W, p_U\n         super(GRU, self).__init__(**kwargs)\n \n     def build(self):\n@@ -290,6 +357,20 @@ class GRU(Recurrent):\n         self.U_h = self.inner_init((self.output_dim, self.output_dim))\n         self.b_h = K.zeros((self.output_dim,))\n \n+        self.regularizers = []\n+        def appendRegulariser(input_regulariser, param, regularizers_list):\n+            regulariser = regularizers.get(input_regulariser)\n+            if regulariser:\n+                regulariser.set_param(param)\n+                regularizers_list.append(regulariser)\n+\n+        for W in [self.W_z, self.W_r, self.W_h]:\n+            appendRegulariser(self.W_regularizer, W, self.regularizers)\n+        for U in [self.U_z, self.U_r, self.U_h]:\n+            appendRegulariser(self.U_regularizer, U, self.regularizers)\n+        for b in [self.b_z, self.b_r, self.b_h]:\n+            appendRegulariser(self.b_regularizer, b, self.regularizers)\n+\n         self.trainable_weights = [self.W_z, self.U_z, self.b_z,\n                                   self.W_r, self.U_r, self.b_r,\n                                   self.W_h, self.U_h, self.b_h]\n@@ -317,25 +398,45 @@ class GRU(Recurrent):\n             self.states = [K.zeros((input_shape[0], self.output_dim))]\n \n     def step(self, x, states):\n-        assert len(states) == 1\n-        x_z = K.dot(x, self.W_z) + self.b_z\n-        x_r = K.dot(x, self.W_r) + self.b_r\n-        x_h = K.dot(x, self.W_h) + self.b_h\n-\n+        assert len(states) == 3 # 1 state and 2 constants\n         h_tm1 = states[0]\n-        z = self.inner_activation(x_z + K.dot(h_tm1, self.U_z))\n-        r = self.inner_activation(x_r + K.dot(h_tm1, self.U_r))\n+        B_W = states[1]\n+        B_U = states[2]\n \n-        hh = self.activation(x_h + K.dot(r * h_tm1, self.U_h))\n+        x_z = K.dot(x * B_W[0], self.W_z) + self.b_z\n+        x_r = K.dot(x * B_W[1], self.W_r) + self.b_r\n+        x_h = K.dot(x * B_W[2], self.W_h) + self.b_h\n+\n+        z = self.inner_activation(x_z + K.dot(h_tm1 * B_U[0], self.U_z))\n+        r = self.inner_activation(x_r + K.dot(h_tm1 * B_U[1], self.U_r))\n+\n+        hh = self.activation(x_h + K.dot(r * h_tm1 * B_U[2], self.U_h))\n         h = z * h_tm1 + (1 - z) * hh\n         return h, [h]\n \n+    def get_constants(self, X, train=False):\n+        nb_samples = X.shape[0]\n+        retain_p_W = 1. - self.p_W\n+        retain_p_U = 1. - self.p_U\n+        if train and self.p_W > 0 and self.p_U > 0:\n+            B_W = K.random_binomial((3, nb_samples, self.input_dim), p=retain_p_W)\n+            B_U = K.random_binomial((3, nb_samples, self.output_dim), p=retain_p_U)\n+        else:\n+            B_W = K.ones(3) * retain_p_W\n+            B_U = K.ones(3) * retain_p_U\n+        return [B_W, B_U]\n+\n     def get_config(self):\n         config = {\"output_dim\": self.output_dim,\n                   \"init\": self.init.__name__,\n                   \"inner_init\": self.inner_init.__name__,\n                   \"activation\": self.activation.__name__,\n-                  \"inner_activation\": self.inner_activation.__name__}\n+                  \"inner_activation\": self.inner_activation.__name__,\n+                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n+                  \"U_regularizer\": self.U_regularizer.get_config() if self.U_regularizer else None,\n+                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n+                  \"p_W\": self.p_W, \n+                  \"p_U\": self.p_U}\n         base_config = super(GRU, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -359,22 +460,37 @@ class LSTM(Recurrent):\n             Can be the name of an existing function (str),\n             or a Theano function (see: [activations](../activations.md)).\n         inner_activation: activation function for the inner cells.\n+        W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n+            (eg. L1 or L2 regularization), applied to the input weights matrices.\n+        U_regularizer: instance of [WeightRegularizer](../regularizers.md)\n+            (eg. L1 or L2 regularization), applied to the recurrent weights matrices.\n+        b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n+            applied to the bias.\n+        p_W: float between 0 and 1. Fraction of the input units to drop for inputs.\n+        p_U: float between 0 and 1. Fraction of the input units to drop for recurrent connections.\n \n     # References\n         - [Long short-term memory](http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf) (original 1997 paper)\n         - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n         - [Supervised sequence labelling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n+        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n     '''\n     def __init__(self, output_dim,\n                  init='glorot_uniform', inner_init='orthogonal',\n                  forget_bias_init='one', activation='tanh',\n-                 inner_activation='hard_sigmoid', **kwargs):\n+                 inner_activation='hard_sigmoid', \n+                 W_regularizer=None, U_regularizer=None, b_regularizer=None, \n+                 p_W=0., p_U=0., **kwargs):\n         self.output_dim = output_dim\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n         self.forget_bias_init = initializations.get(forget_bias_init)\n         self.activation = activations.get(activation)\n         self.inner_activation = activations.get(inner_activation)\n+        self.W_regularizer = W_regularizer\n+        self.U_regularizer = U_regularizer\n+        self.b_regularizer = b_regularizer\n+        self.p_W, self.p_U = p_W, p_U\n         super(LSTM, self).__init__(**kwargs)\n \n     def build(self):\n@@ -386,7 +502,7 @@ class LSTM(Recurrent):\n         if self.stateful:\n             self.reset_states()\n         else:\n-            # initial states: 2 all-zero tensor of shape (output_dim)\n+            # initial states: 2 all-zero tensors of shape (output_dim)\n             self.states = [None, None]\n \n         self.W_i = self.init((input_dim, self.output_dim))\n@@ -405,6 +521,20 @@ class LSTM(Recurrent):\n         self.U_o = self.inner_init((self.output_dim, self.output_dim))\n         self.b_o = K.zeros((self.output_dim,))\n \n+        self.regularizers = []\n+        def appendRegulariser(input_regulariser, param, regularizers_list):\n+            regulariser = regularizers.get(input_regulariser)\n+            if regulariser:\n+                regulariser.set_param(param)\n+                regularizers_list.append(regulariser)\n+\n+        for W in [self.W_i, self.W_f, self.W_i, self.W_o]:\n+            appendRegulariser(self.W_regularizer, W, self.regularizers)\n+        for U in [self.U_i, self.U_f, self.U_i, self.U_o]:\n+            appendRegulariser(self.U_regularizer, U, self.regularizers)\n+        for b in [self.b_i, self.b_f, self.b_i, self.b_o]:\n+            appendRegulariser(self.b_regularizer, b, self.regularizers)\n+\n         self.trainable_weights = [self.W_i, self.U_i, self.b_i,\n                                   self.W_c, self.U_c, self.b_c,\n                                   self.W_f, self.U_f, self.b_f,\n@@ -431,28 +561,47 @@ class LSTM(Recurrent):\n                            K.zeros((input_shape[0], self.output_dim))]\n \n     def step(self, x, states):\n-        assert len(states) == 2\n+        assert len(states) == 4 # 2 states and 2 constants\n         h_tm1 = states[0]\n         c_tm1 = states[1]\n+        B_W = states[2]\n+        B_U = states[3]\n \n-        x_i = K.dot(x, self.W_i) + self.b_i\n-        x_f = K.dot(x, self.W_f) + self.b_f\n-        x_c = K.dot(x, self.W_c) + self.b_c\n-        x_o = K.dot(x, self.W_o) + self.b_o\n+        x_i = K.dot(x * B_W[0], self.W_i) + self.b_i\n+        x_f = K.dot(x * B_W[1], self.W_f) + self.b_f\n+        x_c = K.dot(x * B_W[2], self.W_c) + self.b_c\n+        x_o = K.dot(x * B_W[3], self.W_o) + self.b_o\n \n-        i = self.inner_activation(x_i + K.dot(h_tm1, self.U_i))\n-        f = self.inner_activation(x_f + K.dot(h_tm1, self.U_f))\n-        c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1, self.U_c))\n-        o = self.inner_activation(x_o + K.dot(h_tm1, self.U_o))\n+        i = self.inner_activation(x_i + K.dot(h_tm1 * B_U[0], self.U_i))\n+        f = self.inner_activation(x_f + K.dot(h_tm1 * B_U[1], self.U_f))\n+        c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * B_U[2], self.U_c))\n+        o = self.inner_activation(x_o + K.dot(h_tm1 * B_U[3], self.U_o))\n         h = o * self.activation(c)\n         return h, [h, c]\n \n+    def get_constants(self, X, train=False):\n+        nb_samples = X.shape[0]\n+        retain_p_W = 1. - self.p_W\n+        retain_p_U = 1. - self.p_U\n+        if train and self.p_W > 0 and self.p_U > 0:\n+            B_W = K.random_binomial((4, nb_samples, self.input_dim), p=retain_p_W)\n+            B_U = K.random_binomial((4, nb_samples, self.output_dim), p=retain_p_U)\n+        else:\n+            B_W = K.ones(4) * retain_p_W\n+            B_U = K.ones(4) * retain_p_U\n+        return [B_W, B_U]\n+\n     def get_config(self):\n         config = {\"output_dim\": self.output_dim,\n                   \"init\": self.init.__name__,\n                   \"inner_init\": self.inner_init.__name__,\n                   \"forget_bias_init\": self.forget_bias_init.__name__,\n                   \"activation\": self.activation.__name__,\n-                  \"inner_activation\": self.inner_activation.__name__}\n+                  \"inner_activation\": self.inner_activation.__name__,\n+                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n+                  \"U_regularizer\": self.U_regularizer.get_config() if self.U_regularizer else None,\n+                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n+                  \"p_W\": self.p_W, \n+                  \"p_U\": self.p_U}\n         base_config = super(LSTM, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n\n@@ -515,6 +515,7 @@ class Sequential(Model, containers.Sequential):\n         self._train = K.function(train_ins, [train_loss], updates=updates)\n         self._train_with_acc = K.function(train_ins, [train_loss, train_accuracy], updates=updates)\n         self._predict = K.function(predict_ins, [self.y_test], updates=self.state_updates)\n+        self._predict_stochastic = K.function(predict_ins, [self.y_train], updates=self.state_updates)\n         self._test = K.function(test_ins, [test_loss], updates=self.state_updates)\n         self._test_with_acc = K.function(test_ins, [test_loss, test_accuracy], updates=self.state_updates)\n \n@@ -660,6 +661,21 @@ class Sequential(Model, containers.Sequential):\n         X = standardize_X(X)\n         return self._predict_loop(self._predict, X, batch_size, verbose)[0]\n \n+    def predict_stochastic(self, X, batch_size=128, verbose=0):\n+        '''Generate stochastic output predictions for the input samples\n+        batch by batch.\n+\n+        # Arguments\n+            X: the input data, as a numpy array.\n+            batch_size: integer.\n+            verbose: verbosity mode, 0 or 1.\n+\n+        # Returns\n+            A numpy array of predictions.\n+        '''\n+        X = standardize_X(X)\n+        return self._predict_loop(self._predict_stochastic, X, batch_size, verbose)[0]\n+\n     def predict_proba(self, X, batch_size=128, verbose=1):\n         '''Generate class probability predictions for the input samples\n         batch by batch.\n@@ -929,11 +945,10 @@ class Sequential(Model, containers.Sequential):\n         metrics = ['loss', 'acc', 'val_loss', 'val_acc']\n \n         # prepare callbacks\n-        history = cbks.History()\n+        self.history = cbks.History()\n+        callbacks = [cbks.BaseLogger()] + callbacks + [self.history]\n         if verbose:\n-            callbacks = [history, cbks.BaseLogger()] + callbacks\n-        else:\n-            callbacks = [history] + callbacks\n+            callbacks += [cbks.ProgbarLogger()]\n         callbacks = cbks.CallbackList(callbacks)\n \n         callbacks._set_model(self)\n@@ -1069,7 +1084,7 @@ class Sequential(Model, containers.Sequential):\n                 break\n         _stop.set()\n         callbacks.on_train_end()\n-        return history\n+        return self.history\n \n \n class Graph(Model, containers.Graph):\n@@ -1401,11 +1416,10 @@ class Graph(Model, containers.Graph):\n             class_weight = {}\n \n         # prepare callbacks\n-        history = cbks.History()\n+        self.history = cbks.History()\n+        callbacks = [cbks.BaseLogger()] + callbacks + [self.history]\n         if verbose:\n-            callbacks = [history, cbks.BaseLogger()] + callbacks\n-        else:\n-            callbacks = [history] + callbacks\n+            callbacks += [cbks.ProgbarLogger()]\n         callbacks = cbks.CallbackList(callbacks)\n \n         callbacks._set_model(self)\n@@ -1536,4 +1550,4 @@ class Graph(Model, containers.Graph):\n                 break\n         _stop.set()\n         callbacks.on_train_end()\n-        return history\n+        return self.history\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ce79e0a8ef82c49afe6ce487c0581ff40dd9875c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 18 | Churn Cumulative: 1542 | Contributors (this commit): 13 | Commits (past 90d): 57 | Contributors (cumulative): 19 | DMM Complexity: 0.0\n\nDIFF:\n@@ -443,8 +443,10 @@ def rnn(step_function, inputs, initial_states,\n         assert mask.ndim == ndim\n         mask = mask.dimshuffle(axes)\n \n+        if constants is None:\n+            constants = []\n         # build an all-zero tensor of shape (samples, output_dim)\n-        initial_output = step_function(inputs[0], initial_states)[0] * 0\n+        initial_output = step_function(inputs[0], initial_states + constants)[0] * 0\n         # Theano gets confused by broadcasting patterns in the scan op\n         initial_output = T.unbroadcast(initial_output, 0, 1)\n \n\n@@ -355,6 +355,20 @@ class TestBackend(object):\n         assert(np.max(rand) <= max)\n         assert(np.min(rand) >= min)\n \n+    def test_random_binomial(self):\n+        p = 0.5\n+        rand = KTF.eval(KTF.random_binomial((1000, 1000), p))\n+        assert(rand.shape == (1000, 1000))\n+        assert(np.abs(np.mean(rand) - p) < 0.01)\n+        assert(np.max(rand) == 1)\n+        assert(np.min(rand) == 0)\n+\n+        rand = KTH.eval(KTH.random_binomial((1000, 1000), p))\n+        assert(rand.shape == (1000, 1000))\n+        assert(np.abs(np.mean(rand) - p) < 0.01)\n+        assert(np.max(rand) == 1)\n+        assert(np.min(rand) == 0)\n+\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fed7cc257e960a3849978088dccd500b0fbfc753", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 21 | Files Changed: 3 | Hunks: 12 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 50 | Churn Cumulative: 3657 | Contributors (this commit): 23 | Commits (past 90d): 49 | Contributors (cumulative): 34 | DMM Complexity: 1.0\n\nDIFF:\n@@ -105,10 +105,10 @@ class Embedding(Layer):\n         X = self.get_input(train)\n         retain_p = 1. - self.p\n         if train and self.p > 0:\n-            B = K.random_binomial((1, self.input_dim), p=retain_p)\n+            B = K.random_binomial((self.input_dim), p=retain_p)\n         else:\n-            B = K.ones((1, self.input_dim)) * retain_p\n-        out = K.gather(self.W * B[0][:, None], X) # (self.W * B[0][:, None])[X] - we zero-out rows of W at random\n+            B = K.ones((self.input_dim)) * retain_p\n+        out = K.gather(self.W * K.expand_dims(B), X) # we zero-out rows of W at random\n         return out\n \n     def get_config(self):\n\n@@ -271,7 +271,9 @@ class SimpleRNN(Recurrent):\n         return output, [output]\n \n     def get_constants(self, X, train=False):\n-        nb_samples = X.shape[0]\n+        nb_samples = K.shape(X)[0]\n+        if K._BACKEND == 'tensorflow':\n+            nb_samples = int(nb_samples)\n         retain_p_W = 1. - self.p_W\n         retain_p_U = 1. - self.p_U\n         if train and self.p_W > 0 and self.p_U > 0:\n@@ -403,19 +405,21 @@ class GRU(Recurrent):\n         B_W = states[1]\n         B_U = states[2]\n \n-        x_z = K.dot(x * B_W[0], self.W_z) + self.b_z\n-        x_r = K.dot(x * B_W[1], self.W_r) + self.b_r\n-        x_h = K.dot(x * B_W[2], self.W_h) + self.b_h\n+        x_z = K.dot(x * K.gather(B_W, 0), self.W_z) + self.b_z\n+        x_r = K.dot(x * K.gather(B_W, 1), self.W_r) + self.b_r\n+        x_h = K.dot(x * K.gather(B_W, 2), self.W_h) + self.b_h\n \n-        z = self.inner_activation(x_z + K.dot(h_tm1 * B_U[0], self.U_z))\n-        r = self.inner_activation(x_r + K.dot(h_tm1 * B_U[1], self.U_r))\n+        z = self.inner_activation(x_z + K.dot(h_tm1 * K.gather(B_U, 0), self.U_z))\n+        r = self.inner_activation(x_r + K.dot(h_tm1 * K.gather(B_U, 1), self.U_r))\n \n-        hh = self.activation(x_h + K.dot(r * h_tm1 * B_U[2], self.U_h))\n+        hh = self.activation(x_h + K.dot(r * h_tm1 * K.gather(B_U, 2), self.U_h))\n         h = z * h_tm1 + (1 - z) * hh\n         return h, [h]\n \n     def get_constants(self, X, train=False):\n-        nb_samples = X.shape[0]\n+        nb_samples = K.shape(X)[0]\n+        if K._BACKEND == 'tensorflow':\n+            nb_samples = int(nb_samples)\n         retain_p_W = 1. - self.p_W\n         retain_p_U = 1. - self.p_U\n         if train and self.p_W > 0 and self.p_U > 0:\n@@ -567,20 +571,22 @@ class LSTM(Recurrent):\n         B_W = states[2]\n         B_U = states[3]\n \n-        x_i = K.dot(x * B_W[0], self.W_i) + self.b_i\n-        x_f = K.dot(x * B_W[1], self.W_f) + self.b_f\n-        x_c = K.dot(x * B_W[2], self.W_c) + self.b_c\n-        x_o = K.dot(x * B_W[3], self.W_o) + self.b_o\n+        x_i = K.dot(x * K.gather(B_W, 0), self.W_i) + self.b_i\n+        x_f = K.dot(x * K.gather(B_W, 1), self.W_f) + self.b_f\n+        x_c = K.dot(x * K.gather(B_W, 2), self.W_c) + self.b_c\n+        x_o = K.dot(x * K.gather(B_W, 3), self.W_o) + self.b_o\n \n-        i = self.inner_activation(x_i + K.dot(h_tm1 * B_U[0], self.U_i))\n-        f = self.inner_activation(x_f + K.dot(h_tm1 * B_U[1], self.U_f))\n-        c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * B_U[2], self.U_c))\n-        o = self.inner_activation(x_o + K.dot(h_tm1 * B_U[3], self.U_o))\n+        i = self.inner_activation(x_i + K.dot(h_tm1 * K.gather(B_U, 0), self.U_i))\n+        f = self.inner_activation(x_f + K.dot(h_tm1 * K.gather(B_U, 1), self.U_f))\n+        c = f * c_tm1 + i * self.activation(x_c + K.dot(h_tm1 * K.gather(B_U, 2), self.U_c))\n+        o = self.inner_activation(x_o + K.dot(h_tm1 * K.gather(B_U, 3), self.U_o))\n         h = o * self.activation(c)\n         return h, [h, c]\n \n     def get_constants(self, X, train=False):\n-        nb_samples = X.shape[0]\n+        nb_samples = K.shape(X)[0]\n+        if K._BACKEND == 'tensorflow':\n+            nb_samples = int(nb_samples)\n         retain_p_W = 1. - self.p_W\n         retain_p_U = 1. - self.p_U\n         if train and self.p_W > 0 and self.p_U > 0:\n\n@@ -18,9 +18,11 @@ def _runner(layer_class):\n     All the recurrent layers share the same interface,\n     so we can run through them with a single function.\n     \"\"\"\n+    for p in [0., 0.5]:\n         for ret_seq in [True, False]:\n             layer = layer_class(output_dim, return_sequences=ret_seq,\n-                            weights=None, input_shape=(timesteps, embedding_dim))\n+                                weights=None, input_shape=(timesteps, embedding_dim),\n+                                p_W=p, p_U=p)\n             layer.input = K.variable(np.ones((nb_samples, timesteps, embedding_dim)))\n             layer.get_config()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
