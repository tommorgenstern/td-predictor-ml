{"custom_id": "scrapy#1cfd959828baa08d65bbf7dd44e994cf1cb0c29b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 120 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -9,7 +9,6 @@ from collections import defaultdict\n from twisted.internet.defer import Deferred\n \n from scrapy import log\n-from scrapy.http import Response\n from scrapy.core.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.defer import mustbe_deferred\n@@ -55,8 +54,8 @@ class SchedulerMiddlewareManager(object):\n         def _enqueue_request(request):\n             for mwfunc in self.mw_cbs['enqueue_request']:\n                 result = mwfunc(spider=spider, request=request)\n-                assert result is None or isinstance(result, (Response, Deferred)), \\\n-                        'Middleware %s.enqueue_request must return None, Response or Deferred, got %s' % \\\n+                assert result is None or isinstance(result, Deferred), \\\n+                        'Middleware %s.enqueue_request must return None or Deferred, got %s' % \\\n                         (mwfunc.im_self.__class__.__name__, result.__class__.__name__)\n                 if result:\n                     return result\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#93117412362326b73c8b935c18790dee9b29538d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 54 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,6 +25,7 @@ class CachingResolver(object):\n     def spider_closed(self, spider):\n         for hostname in self.spider_hostnames[spider]:\n             self.resolver._cache.pop(hostname, None)\n+        self.spider_hostnames.pop(spider, None)\n \n \n class _CachingThreadedResolver(ThreadedResolver):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a36909a3f9fff5ffee1c0c64e6559b21aac13f47", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 69 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -9,6 +9,7 @@ from twisted.plugin import IPlugin\n from scrapy import log\n from scrapy.http import Request\n from scrapy.utils.misc import arg_to_iter\n+from scrapy.utils.trackref import object_ref\n \n def _valid_domain_name(obj):\n     \"\"\"Check the domain name specified is valid\"\"\"\n@@ -40,7 +41,7 @@ class ISpider(Interface, IPlugin) :\n     invariant(_valid_domain_name)\n     invariant(_valid_download_delay)\n \n-class BaseSpider(object):\n+class BaseSpider(object_ref):\n     \"\"\"Base class for scrapy spiders. All spiders must inherit from this\n     class.\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0dcf47a7068319258807ef51e81724c1c31591ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 91 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -58,7 +58,7 @@ class RobotsTxtMiddleware(object):\n             or settings['USER_AGENT']\n \n     def spider_closed(self, spider):\n-        for netloc in self._spider_netlocs[domain]:\n+        for netloc in self._spider_netlocs[spider]:\n             del self._parsers[netloc]\n-        del self._spider_netlocs[domain]\n+        del self._spider_netlocs[spider]\n         del self._useragents[spider]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a48516b1056404023299d0fb76ee78079aa5ae93", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 43 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -120,9 +120,6 @@ def replace_escape_chars(text, which_ones=('\\n','\\t','\\r'), replace_by=u''):\n         text = text.replace(ec, str_to_unicode(replace_by))\n     return str_to_unicode(text)\n \n-# FIXME: backwards compatibility - should be removed before 0.7 release\n-remove_escape_chars = replace_escape_chars\n-\n def unquote_markup(text, keep=(), remove_illegal=True):\n     \"\"\"\n     This function receives markup as a text (always a unicode string or a utf-8 encoded string) and does the following:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#88417a3ed1d5df5c919d4cd2279d17d6c8d0ce39", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 12 | Churn Cumulative: 75 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -12,7 +12,6 @@ class SpiderStats(object):\n         self.scraped = 0\n         self.crawled = 0\n         self.started = None\n-        self.finished = None\n \n class LiveStats(object):\n     webconsole_id = 'livestats'\n@@ -31,10 +30,9 @@ class LiveStats(object):\n         pstats = SpiderStats()\n         self.domains[spider] = pstats\n         pstats.started = datetime.now().replace(microsecond=0)\n-        pstats.finished = None\n \n     def spider_closed(self, spider):\n-        self.domains[spider].finished = datetime.now().replace(microsecond=0)\n+        del self.domains[spider]\n \n     def item_scraped(self, item, spider):\n         self.domains[spider].scraped += 1\n@@ -52,17 +50,17 @@ class LiveStats(object):\n         totdomains = totscraped = totcrawled = totscheduled = totactive = totdqueued = tottransf = 0\n         s = banner(self)\n         s += \"<table border='1'>\\n\"\n-        s += \"<tr><th>Spider</th><th>Items<br>Scraped</th><th>Pages<br>Crawled</th><th>Scheduler<br>Pending</th><th>Downloader<br/>Queued</th><th>Downloader<br/>Active</th><th>Downloader<br/>Transferring</th><th>Start time</th><th>Finish time</th><th>Run time</th></tr>\\n\"\n+        s += \"<tr><th>Spider</th><th>Items<br>Scraped</th><th>Pages<br>Crawled</th><th>Scheduler<br>Pending</th><th>Downloader<br/>Queued</th><th>Downloader<br/>Active</th><th>Downloader<br/>Transferring</th><th>Start time</th><th>Run time</th></tr>\\n\"\n         for spider in sorted(self.domains.keys()):\n             scheduled = len(sch.pending_requests[spider]) if spider in sch.pending_requests else 0\n             active = len(dwl.sites[spider].active) if spider in dwl.sites else 0\n             dqueued = len(dwl.sites[spider].queue) if spider in dwl.sites else 0\n             transf = len(dwl.sites[spider].transferring) if spider in dwl.sites else 0\n             stats = self.domains[spider]\n-            runtime = stats.finished - stats.started if stats.finished else datetime.now() - stats.started\n+            runtime = datetime.now() - stats.started\n \n-            s += '<tr><td>%s</td><td align=\"right\">%d</td><td align=\"right\">%d</td><td align=\"right\">%d</td><td align=\"right\">%d</td><td align=\"right\">%d</td><td align=\"right\">%d</td><td>%s</td><td>%s</td><td>%s</td></tr>\\n' % \\\n-                 (spider.domain_name, stats.scraped, stats.crawled, scheduled, dqueued, active, transf, str(stats.started), str(stats.finished), str(runtime))\n+            s += '<tr><td>%s</td><td align=\"right\">%d</td><td align=\"right\">%d</td><td align=\"right\">%d</td><td align=\"right\">%d</td><td align=\"right\">%d</td><td align=\"right\">%d</td><td>%s</td><td>%s</td></tr>\\n' % \\\n+                 (spider.domain_name, stats.scraped, stats.crawled, scheduled, dqueued, active, transf, str(stats.started), str(runtime))\n \n             totdomains += 1\n             totscraped += stats.scraped\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c0f1c8de04deabf9477758255312de3de6d17ed8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 14 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -40,5 +40,5 @@ class CloseSpider(object):\n     def spider_closed(self, spider):\n         self.counts.pop(spider, None)\n         tsk = self.tasks.pop(spider, None)\n-        if tsk and not tsk.called:\n+        if tsk and tsk.active():\n             tsk.cancel()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fd50891113cf44af7dca3a45f230ab8f744d4427", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 9 | Methods Changed: 7 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 30 | Churn Cumulative: 1684 | Contributors (this commit): 4 | Commits (past 90d): 30 | Contributors (cumulative): 7 | DMM Complexity: 0.6428571428571429\n\nDIFF:\n@@ -36,6 +36,7 @@ class SiteInfo(object):\n         self.transferring = set()\n         self.closing = False\n         self.lastseen = 0\n+        self.next_request_calls = set()\n \n     def free_transfer_slots(self):\n         return self.max_concurrent_requests - len(self.transferring)\n@@ -44,6 +45,10 @@ class SiteInfo(object):\n         # use self.active to include requests in the downloader middleware\n         return len(self.active) > 2 * self.max_concurrent_requests\n \n+    def cancel_request_calls(self):\n+        for call in self.next_request_calls:\n+            call.cancel()\n+        self.next_request_calls.clear()\n \n class Downloader(object):\n     \"\"\"Mantain many concurrent downloads and provide an HTTP abstraction.\n@@ -97,7 +102,11 @@ class Downloader(object):\n         if site.download_delay:\n             penalty = site.download_delay - now + site.lastseen\n             if penalty > 0:\n-                reactor.callLater(penalty, self._process_queue, spider=spider)\n+                d = defer.Deferred()\n+                d.addCallback(self._process_queue)\n+                call = reactor.callLater(penalty, d.callback, spider)\n+                site.next_request_calls.add(call)\n+                d.addBoth(lambda x: site.next_request_calls.remove(call))\n                 return\n         site.lastseen = now\n \n@@ -153,12 +162,13 @@ class Downloader(object):\n \n     def close_spider(self, spider):\n         \"\"\"Free any resources associated with the given spider\"\"\"\n-        domain = spider.domain_name\n         site = self.sites.get(spider)\n         if not site or site.closing:\n-            raise RuntimeError('Downloader spider already closed: %s' % domain)\n+            raise RuntimeError('Downloader spider already closed: %s' % \\\n+                spider.domain_name)\n \n         site.closing = True\n+        site.cancel_request_calls()\n         self._process_queue(spider)\n \n     def has_capacity(self):\n\n@@ -32,7 +32,7 @@ class ExecutionEngine(object):\n         self.running = False\n         self.killed = False\n         self.paused = False\n-        self._next_request_pending = set()\n+        self._next_request_calls = {}\n         self._mainloop_task = task.LoopingCall(self._mainloop)\n         self._crawled_logline = load_object(settings['LOG_FORMATTER_CRAWLED'])\n \n@@ -107,10 +107,11 @@ class ExecutionEngine(object):\n         The spider is closed if there are no more pages to scrape.\n         \"\"\"\n         if now:\n-            self._next_request_pending.discard(spider)\n-        elif spider not in self._next_request_pending:\n-            self._next_request_pending.add(spider)\n-            return reactor.callLater(0, self.next_request, spider, now=True)\n+            self._next_request_calls.pop(spider, None)\n+        elif spider not in self._next_request_calls:\n+            call = reactor.callLater(0, self.next_request, spider, now=True)\n+            self._next_request_calls[spider] = call\n+            return call\n         else:\n             return\n \n@@ -307,6 +308,9 @@ class ExecutionEngine(object):\n         send_catch_log(signal=signals.spider_closed, sender=self.__class__, \\\n             spider=spider, reason=reason)\n         stats.close_spider(spider, reason=reason)\n+        call = self._next_request_calls.pop(spider, None)\n+        if call and call.active():\n+            call.cancel()\n         dfd = defer.maybeDeferred(spiders.close_spider, spider)\n         dfd.addBoth(log.msg, \"Spider closed (%s)\" % reason, spider=spider)\n         reactor.callLater(0, self._mainloop)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#34fcf6ba5b242e0dca00f61109577b53a8ca7a21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 16 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -34,6 +34,9 @@ if not settings.getbool('TRACK_REFS'):\n     object_ref = object\n \n def print_live_refs():\n+    if object_ref is object:\n+        print \"The trackref module is disabled. Use TRACK_REFS setting to enable it.\"\n+        return\n     print \"Live References\"\n     print\n     now = time()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#207aae2b047b788e30d110ca588a7453b23d881f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1279 | Contributors (this commit): 4 | Commits (past 90d): 22 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -312,7 +312,9 @@ class ExecutionEngine(object):\n         if call and call.active():\n             call.cancel()\n         dfd = defer.maybeDeferred(spiders.close_spider, spider)\n-        dfd.addBoth(log.msg, \"Spider closed (%s)\" % reason, spider=spider)\n+        dfd.addErrback(log.err, \"Unhandled error on SpiderManager.close_spider()\",\n+            spider=spider)\n+        dfd.addBoth(lambda _: log.msg(\"Spider closed (%s)\" % reason, spider=spider))\n         reactor.callLater(0, self._mainloop)\n         return dfd\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6084be3b2eae9bcd63ea7370460e3d1ab46b43ac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 10 | Churn Cumulative: 26 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,6 +14,7 @@ import weakref\n from collections import defaultdict\n from time import time\n from operator import itemgetter\n+from types import NoneType\n \n from scrapy.conf import settings\n \n@@ -33,7 +34,7 @@ class object_ref(object):\n if not settings.getbool('TRACK_REFS'):\n     object_ref = object\n \n-def print_live_refs():\n+def print_live_refs(ignore=NoneType):\n     if object_ref is object:\n         print \"The trackref module is disabled. Use TRACK_REFS setting to enable it.\"\n         return\n@@ -43,6 +44,8 @@ def print_live_refs():\n     for cls, wdict in live_refs.iteritems():\n         if not wdict:\n             continue\n+        if issubclass(cls, ignore):\n+            continue\n         oldest = min(wdict.itervalues())\n         print \"%-30s %6d   oldest: %ds ago\" % (cls.__name__, len(wdict), \\\n             now-oldest)\n@@ -52,3 +55,8 @@ def get_oldest(class_name):\n         if cls.__name__ == class_name:\n             if wdict:\n                 return min(wdict.iteritems(), key=itemgetter(1))[0]\n+\n+def iter_all(class_name):\n+    for cls, wdict in live_refs.iteritems():\n+        if cls.__name__ == class_name:\n+            return wdict.iterkeys()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cabee59d4d44cebf897dea94988bc05613963954", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 122 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -82,6 +82,8 @@ class ScrapyHTTPClientFactory(HTTPClientFactory):\n     protocol = ScrapyHTTPPageGetter\n     waiting = 1\n     noisy = False\n+    followRedirect = False\n+    afterFoundGet = False\n \n     def __init__(self, request, timeout=0):\n         self.url = urldefrag(request.url)[0]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dda7e01d2f0bf1262f5ec10485ca05dde47464b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 10 | Churn Cumulative: 10 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,11 +1,11 @@\n-from unittest import TestCase, main\n+from twisted.trial import unittest\n \n-class ScrapyUtilsTest(TestCase):\n-    def test_openssl(self):\n+class ScrapyUtilsTest(unittest.TestCase):\n+    def test_required_openssl_version(self):\n         try:\n             module = __import__('OpenSSL', {}, {}, [''])\n         except ImportError, ex:\n-            return # no openssl installed\n+            raise unittest.SkipTest(\"OpenSSL is not available\")\n \n         required_version = '0.6'\n         if hasattr(module, '__version__'):\n@@ -13,4 +13,4 @@ class ScrapyUtilsTest(TestCase):\n                 self.assertFalse(cur < req, \"module %s >= %s required\" % ('OpenSSL', required_version))\n \n if __name__ == \"__main__\":\n-    main()\n+    unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#925eecce70da2aa27ea0f33d4b0f088f2f1d5a75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 5 | Churn Cumulative: 15 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -7,10 +7,9 @@ class ScrapyUtilsTest(unittest.TestCase):\n         except ImportError, ex:\n             raise unittest.SkipTest(\"OpenSSL is not available\")\n \n-        required_version = '0.6'\n         if hasattr(module, '__version__'):\n-            for cur, req in zip(module.__version__.split('.'), required_version.split('.')):\n-                self.assertFalse(cur < req, \"module %s >= %s required\" % ('OpenSSL', required_version))\n+            installed_version = map(int, module.__version__.split('.')[:2])\n+            assert installed_version >= [0, 6], \"OpenSSL >= 0.6 required\"\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dbe0a6875cedb8ef44cdf12ef08961f80c2a8312", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 29 | Files Changed: 1 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 70 | Churn Cumulative: 175 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,41 +11,18 @@ from scrapy.conf import default_settings\n \n import_ = lambda x: __import__(x, {}, {}, [''])\n \n+\n class Settings(object):\n \n-    def __init__(self, overrides=None):\n-        self.defaults = {}\n+    def __init__(self, values=None):\n+        self.values = values.copy() if values else {}\n         self.global_defaults = default_settings\n-        self.disabled = os.environ.get('SCRAPY_SETTINGS_DISABLED', False)\n-        settings_module_path = os.environ.get('SCRAPY_SETTINGS_MODULE', \\\n-            'scrapy_settings')\n-        self.set_settings_module(settings_module_path)\n-\n-        # XXX: find a better solution for this hack\n-        pickled_settings = os.environ.get(\"SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE\")\n-        self.overrides = pickle.loads(pickled_settings) if pickled_settings else {}\n-        if overrides:\n-            self.overrides.update(overrides)\n \n     def __getitem__(self, opt_name):\n-        if not self.disabled:\n-            if opt_name in self.overrides:\n-                return self.overrides[opt_name]\n-            if 'SCRAPY_' + opt_name in os.environ:\n-                return os.environ['SCRAPY_' + opt_name]\n-            if hasattr(self.settings_module, opt_name):\n-                return getattr(self.settings_module, opt_name)\n-            if opt_name in self.defaults:\n-                return self.defaults[opt_name]\n+        if opt_name in self.values:\n+            return self.values[opt_name]\n         return getattr(self.global_defaults, opt_name, None)\n \n-    def set_settings_module(self, settings_module_path):\n-        self.settings_module_path = settings_module_path\n-        try:\n-            self.settings_module = import_(settings_module_path)\n-        except ImportError:\n-            self.settings_module = None\n-\n     def get(self, name, default=None):\n         return self[name] if self[name] is not None else default\n \n@@ -71,7 +48,42 @@ class Settings(object):\n         else:\n             return str(value).split(',')\n \n+\n+class EnvironmentSettings(Settings):\n+\n+    def __init__(self):\n+        super(EnvironmentSettings, self).__init__()\n+        self.defaults = {}\n+        self.disabled = os.environ.get('SCRAPY_SETTINGS_DISABLED', False)\n+        settings_module_path = os.environ.get('SCRAPY_SETTINGS_MODULE', \\\n+            'scrapy_settings')\n+        self.set_settings_module(settings_module_path)\n+\n+        # XXX: find a better solution for this hack\n+        pickled_settings = os.environ.get(\"SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE\")\n+        self.overrides = pickle.loads(pickled_settings) if pickled_settings else {}\n+\n+    def set_settings_module(self, settings_module_path):\n+        self.settings_module_path = settings_module_path\n+        try:\n+            self.settings_module = import_(settings_module_path)\n+        except ImportError:\n+            self.settings_module = None\n+\n+    def __getitem__(self, opt_name):\n+        if not self.disabled:\n+            if opt_name in self.overrides:\n+                return self.overrides[opt_name]\n+            if 'SCRAPY_' + opt_name in os.environ:\n+                return os.environ['SCRAPY_' + opt_name]\n+            if hasattr(self.settings_module, opt_name):\n+                return getattr(self.settings_module, opt_name)\n+            if opt_name in self.defaults:\n+                return self.defaults[opt_name]\n+        return super(EnvironmentSettings, self).__getitem__(opt_name)\n+\n     def __str__(self):\n         return \"<Settings %r>\" % self.settings_module_path\n \n-settings = Settings()\n+\n+settings = EnvironmentSettings()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#57bae5335fae095d10d87a2b210c86a0070b32dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 52 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 53 | Churn Cumulative: 228 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -42,7 +42,7 @@ class Settings(object):\n     def getlist(self, name, default=None):\n         value = self.get(name)\n         if value is None:\n-            return []\n+            return default or []\n         elif hasattr(value, '__iter__'):\n             return value\n         else:\n\n@@ -0,0 +1,51 @@\n+import unittest\n+\n+from scrapy.conf import Settings\n+\n+class SettingsTest(unittest.TestCase):\n+\n+    def test_get(self):\n+        settings = Settings({\n+            'TEST_ENABLED1': '1',\n+            'TEST_ENABLED2': True,\n+            'TEST_ENABLED3': 1,\n+            'TEST_DISABLED1': '0',\n+            'TEST_DISABLED2': False,\n+            'TEST_DISABLED3': 0,\n+            'TEST_INT1': 123,\n+            'TEST_INT2': '123',\n+            'TEST_FLOAT1': 123.45,\n+            'TEST_FLOAT2': '123.45',\n+            'TEST_LIST1': ['one', 'two'],\n+            'TEST_LIST2': 'one,two',\n+            'TEST_STR': 'value',\n+        })\n+        assert settings.getbool('TEST_ENABLED1') is True\n+        assert settings.getbool('TEST_ENABLED2') is True\n+        assert settings.getbool('TEST_ENABLED3') is True\n+        assert settings.getbool('TEST_ENABLEDx') is False\n+        assert settings.getbool('TEST_ENABLEDx', True) is True\n+        assert settings.getbool('TEST_DISABLED1') is False\n+        assert settings.getbool('TEST_DISABLED2') is False\n+        assert settings.getbool('TEST_DISABLED3') is False\n+        self.assertEqual(settings.getint('TEST_INT1'), 123)\n+        self.assertEqual(settings.getint('TEST_INT2'), 123)\n+        self.assertEqual(settings.getint('TEST_INTx'), 0)\n+        self.assertEqual(settings.getint('TEST_INTx', 45), 45)\n+        self.assertEqual(settings.getfloat('TEST_FLOAT1'), 123.45)\n+        self.assertEqual(settings.getfloat('TEST_FLOAT2'), 123.45)\n+        self.assertEqual(settings.getfloat('TEST_FLOATx'), 0.0)\n+        self.assertEqual(settings.getfloat('TEST_FLOATx', 55.0), 55.0)\n+        self.assertEqual(settings.getlist('TEST_LIST1'), ['one', 'two'])\n+        self.assertEqual(settings.getlist('TEST_LIST2'), ['one', 'two'])\n+        self.assertEqual(settings.getlist('TEST_LISTx'), [])\n+        self.assertEqual(settings.getlist('TEST_LISTx', ['default']), ['default'])\n+        self.assertEqual(settings['TEST_STR'], 'value')\n+        self.assertEqual(settings.get('TEST_STR'), 'value')\n+        self.assertEqual(settings['TEST_STRx'], None)\n+        self.assertEqual(settings.get('TEST_STRx'), None)\n+        self.assertEqual(settings.get('TEST_STRx', 'default'), 'default')\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#41894c776eb69a40d590daaaf1df2acbbbd5d2dc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 20 | Churn Cumulative: 429 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -15,10 +15,9 @@ from .middleware import DownloaderMiddlewareManager\n from .handlers import download_any\n \n \n-class SiteInfo(object):\n-    \"\"\"This is a simple data record that encapsulates the details we hold on\n-    each domain which we are scraping.\n-    \"\"\"\n+class SpiderInfo(object):\n+    \"\"\"Simple class to keep information and state for each open spider\"\"\"\n+\n     def __init__(self, download_delay=None, max_concurrent_requests=None):\n         if download_delay is None:\n             self.download_delay = settings.getint('DOWNLOAD_DELAY')\n@@ -50,6 +49,7 @@ class SiteInfo(object):\n             call.cancel()\n         self.next_request_calls.clear()\n \n+\n class Downloader(object):\n     \"\"\"Mantain many concurrent downloads and provide an HTTP abstraction.\n     It supports a limited number of connections per spider and many spiders in\n@@ -59,7 +59,7 @@ class Downloader(object):\n     def __init__(self):\n         self.sites = {}\n         self.middleware = DownloaderMiddlewareManager()\n-        self.concurrent_domains = settings.getint('CONCURRENT_SPIDERS')\n+        self.concurrent_spiders = settings.getint('CONCURRENT_SPIDERS')\n \n     def fetch(self, request, spider):\n         \"\"\"Main method to use to request a download\n@@ -151,11 +151,10 @@ class Downloader(object):\n \n     def open_spider(self, spider):\n         \"\"\"Allocate resources to begin processing a spider\"\"\"\n-        domain = spider.domain_name\n         if spider in self.sites:\n-            raise RuntimeError('Downloader spider already opened: %s' % domain)\n+            raise RuntimeError('Downloader spider already opened: %s' % spider)\n \n-        self.sites[spider] = SiteInfo(\n+        self.sites[spider] = SpiderInfo(\n             download_delay=getattr(spider, 'download_delay', None),\n             max_concurrent_requests=getattr(spider, 'max_concurrent_requests', None)\n         )\n@@ -164,8 +163,7 @@ class Downloader(object):\n         \"\"\"Free any resources associated with the given spider\"\"\"\n         site = self.sites.get(spider)\n         if not site or site.closing:\n-            raise RuntimeError('Downloader spider already closed: %s' % \\\n-                spider.domain_name)\n+            raise RuntimeError('Downloader spider already closed: %s' % spider)\n \n         site.closing = True\n         site.cancel_request_calls()\n@@ -173,7 +171,7 @@ class Downloader(object):\n \n     def has_capacity(self):\n         \"\"\"Does the downloader have capacity to handle more spiders\"\"\"\n-        return len(self.sites) < self.concurrent_domains\n+        return len(self.sites) < self.concurrent_spiders\n \n     def is_idle(self):\n         return not self.sites\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fe88aad27d97c9e4a150469033732d6439a79b4a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 20 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 22 | Churn Cumulative: 91 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,32 +14,14 @@ from scrapy.utils.trackref import object_ref\n def _valid_domain_name(obj):\n     \"\"\"Check the domain name specified is valid\"\"\"\n     if not obj.domain_name:\n-        raise ValueError(\"A site domain name is required\")\n-\n-def _valid_download_delay(obj):\n-    \"\"\"Check the download delay is valid, if specified\"\"\"\n-    delay = getattr(obj, 'download_delay', 0)\n-    if not type(delay) in (int, long, float):\n-        raise ValueError(\"download_delay must be numeric\")\n-    if float(delay) < 0.0:\n-        raise ValueError(\"download_delay must be positive\")\n+        raise ValueError(\"Spider 'domain_name' attribute is required\")\n \n class ISpider(Interface, IPlugin) :\n     \"\"\"Interface to be implemented by site-specific web spiders\"\"\"\n \n-    domain_name = Attribute(\n-         \"\"\"The domain name of the site to be scraped.\"\"\")\n-\n-    download_delay = Attribute(\n-         \"\"\"Optional delay in seconds to wait between web page downloads.\n-         Note that this delay does not apply to image downloads.\n-         A delay of less than a second can be specified.\"\"\")\n-\n-    user_agent = Attribute(\n-         \"\"\"Optional User-Agent to use for this domain\"\"\")\n+    domain_name = Attribute(\"The domain name of the site to be scraped.\")\n \n     invariant(_valid_domain_name)\n-    invariant(_valid_download_delay)\n \n class BaseSpider(object_ref):\n     \"\"\"Base class for scrapy spiders. All spiders must inherit from this\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3012030b2faea43b00cb8ae233321c1f9eb8a579", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 194 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,8 @@\n \"\"\"Download handler for file:// scheme\"\"\"\n from __future__ import with_statement\n \n+from urllib import url2pathname\n+\n from twisted.internet import defer\n from scrapy.core.downloader.responsetypes import responsetypes\n \n@@ -10,7 +12,7 @@ def download_file(request, spider):\n     return defer.maybeDeferred(_all_in_one_read_download_file, request, spider)\n \n def _all_in_one_read_download_file(request, spider):\n-    filepath = request.url.split(\"file://\")[1]\n+    filepath = url2pathname(request.url.split(\"file://\")[1])\n     with open(filepath) as f:\n         body = f.read()\n     respcls = responsetypes.from_args(filename=filepath, body=body)\n\n@@ -20,7 +20,7 @@ class FileTestCase(unittest.TestCase):\n \n     def setUp(self):\n         self.tmpname = self.mktemp()\n-        fd = open(self.tmpname, 'w')\n+        fd = open(self.tmpname + '<', 'w')\n         fd.write('0123456789')\n         fd.close()\n \n@@ -30,7 +30,8 @@ class FileTestCase(unittest.TestCase):\n             self.assertEquals(response.status, 200)\n             self.assertEquals(response.body, '0123456789')\n \n-        request = Request('file://%s' % self.tmpname)\n+        request = Request('file://%s' % self.tmpname + '<')\n+        assert request.url.upper().endswith('%3C')\n         return download_file(request, BaseSpider()).addCallback(_test)\n \n     def test_non_existent(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9c31fc447a678d7fa97d7ee3120f7902254ade24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 178 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -20,7 +20,7 @@ class FileTestCase(unittest.TestCase):\n \n     def setUp(self):\n         self.tmpname = self.mktemp()\n-        fd = open(self.tmpname + '<', 'w')\n+        fd = open(self.tmpname + '^', 'w')\n         fd.write('0123456789')\n         fd.close()\n \n@@ -30,8 +30,8 @@ class FileTestCase(unittest.TestCase):\n             self.assertEquals(response.status, 200)\n             self.assertEquals(response.body, '0123456789')\n \n-        request = Request('file://%s' % self.tmpname + '<')\n-        assert request.url.upper().endswith('%3C')\n+        request = Request('file://%s' % self.tmpname + '^')\n+        assert request.url.upper().endswith('%5E')\n         return download_file(request, BaseSpider()).addCallback(_test)\n \n     def test_non_existent(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3d6f598abc3135355deaa491fbad74cb8336b02e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 87 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -49,6 +49,7 @@ class HttpCacheMiddlewareTest(unittest.TestCase):\n         storage = self._get_storage(HTTPCACHE_EXPIRATION_SECS=0)\n         assert storage.retrieve_response(self.spider, self.request) is None\n         storage.store_response(self.spider, self.request, self.response)\n+        time.sleep(0.1) # required for win32\n         assert storage.retrieve_response(self.spider, self.request) is None\n \n     def test_storage_never_expire(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f22fe95ae9e8b5fffc856745fc4ce24a7085df0f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 28 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -2,9 +2,8 @@\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n \n-# IMPORTANT: remember to also update the version in docs/conf.py\n-version_info = (0, 8, 0, '', 0)\n-__version__ = \"0.8-dev\"\n+version_info = (0, 8, 0, 'rc1', 0)\n+__version__ = \"0.8-rc1\"\n \n import sys, os\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#024c00a51287401a0c49db0c8d5b1d52293d9c04", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 87 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -80,7 +80,7 @@ if u'SVN' in version:\n     version = ' '.join(version.split(' ')[:-1])\n \n setup_args = {\n-    'name': 'scrapy',\n+    'name': 'Scrapy',\n     'version': version,\n     'url': 'http://scrapy.org',\n     'description': 'A high-level Python Screen Scraping framework',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cd9a2d8edbfe4069719622f4cfa120aa9512a7df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 32 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -2,8 +2,8 @@\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n \n-version_info = (0, 8, 0, 'rc1', 0)\n-__version__ = \"0.8-rc1\"\n+version_info = (0, 8, 0, '', 0)\n+__version__ = \"0.8\"\n \n import sys, os\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#34971ea6e7a356fb6b5ca86828287356b05e74f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 36 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -2,8 +2,8 @@\n Scrapy - a screen scraping framework written in Python\n \"\"\"\n \n-version_info = (0, 8, 0, '', 0)\n-__version__ = \"0.8\"\n+version_info = (0, 9, 0, 'dev')\n+__version__ = \"0.9-dev\"\n \n import sys, os\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#496db55518bda5a5d2ee9742bb5f3c3d5b7d6b6b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 37 | Churn Cumulative: 73 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,6 +14,11 @@ if sys.version_info < (2,5):\n # monkey patches to fix external library issues\n from scrapy.xlib import twisted_250_monkeypatches\n \n+# add some common encoding aliases not included by default in Python\n+from scrapy.utils.encoding import add_encoding_alias\n+add_encoding_alias('gb2312', 'zh-cn')\n+add_encoding_alias('cp1251', 'win-1251')\n+\n # optional_features is a set containing Scrapy optional features\n optional_features = set()\n \n\n@@ -0,0 +1,21 @@\n+import unittest\n+\n+import scrapy # adds encoding aliases (if not added before)\n+\n+class EncodingAliasesTestCase(unittest.TestCase):\n+\n+    def test_encoding_aliases(self):\n+        \"\"\"Test common encdoing aliases not included in Python\"\"\"\n+\n+        uni = u'\\u041c\\u041e\\u0421K\\u0412\\u0410'\n+        str = uni.encode('windows-1251')\n+        self.assertEqual(uni.encode('windows-1251'), uni.encode('win-1251'))\n+        self.assertEqual(str.decode('windows-1251'), str.decode('win-1251'))\n+\n+        text = u'\\u8f6f\\u4ef6\\u540d\\u79f0'\n+        str = uni.encode('gb2312')\n+        self.assertEqual(uni.encode('gb2312'), uni.encode('zh-cn'))\n+        self.assertEqual(str.decode('gb2312'), str.decode('zh-cn'))\n+\n+if __name__ == \"__main__\":\n+    unittest.main()\n\n@@ -0,0 +1,11 @@\n+import codecs\n+\n+def add_encoding_alias(encoding, alias, overwrite=False):\n+    try:\n+        codecs.lookup(alias)\n+        alias_exists = True\n+    except LookupError:\n+        alias_exists = False\n+    if overwrite or not alias_exists:\n+        codec = codecs.lookup(encoding)\n+        codecs.register(lambda x: codec if x == alias else None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b1c27567782629fd20a889a4c5fbb0d680942764", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 14 | Churn Cumulative: 1875 | Contributors (this commit): 5 | Commits (past 90d): 15 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -137,7 +137,8 @@ class ExecutionEngine(object):\n         if request:\n             dwld = mustbe_deferred(self.download, request, spider)\n             dwld.chainDeferred(deferred).addBoth(lambda _: deferred)\n-            dwld.addErrback(log.err, \"Unhandled error on engine._next_request\")\n+            dwld.addErrback(log.err, \"Unhandled error on engine._next_request()\",\n+                spider=spider)\n             return dwld\n \n     def spider_is_idle(self, spider):\n@@ -173,7 +174,7 @@ class ExecutionEngine(object):\n         # next core refactoring.\n         #schd.addErrback(log.err, \"Error on engine.crawl()\")\n         schd.addBoth(self.scraper.enqueue_scrape, request, spider)\n-        schd.addErrback(log.err, \"Unhandled error on engine.crawl()\")\n+        schd.addErrback(log.err, \"Unhandled error on engine.crawl()\", spider=spider)\n         schd.addBoth(lambda _: self.next_request(spider))\n \n     def schedule(self, request, spider):\n\n@@ -186,7 +186,8 @@ class Scraper(object):\n         if isinstance(spider_failure.value, IgnoreRequest):\n             return\n         elif spider_failure is propagated_failure:\n-            log.err(spider_failure, 'Unhandled error propagated to spider and wasn\\'t handled')\n+            log.err(spider_failure, 'Unhandled error propagated to spider', \\\n+                spider=spider)\n             return # stop propagating this error\n         else:\n             return spider_failure # exceptions raised in the spider code\n\n@@ -87,7 +87,11 @@ def err(_stuff=None, _why=None, **kwargs):\n     domain = kwargs.pop('domain', None)\n     spider = kwargs.pop('spider', None)\n     component = kwargs.pop('component', BOT_NAME)\n-    kwargs['system'] = domain or spider.domain_name if spider else component\n+    if domain is not None:\n+        import warnings\n+        warnings.warn(\"'domain' argument of scrapy.log.err() is deprecated, \" \\\n+            \"use 'spider' argument instead\", DeprecationWarning, stacklevel=2)\n+    kwargs['system'] = domain or (spider.domain_name if spider else component)\n     if _why:\n         _why = unicode_to_str(\"ERROR: %s\" % _why)\n     log.err(_stuff, _why, **kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#51faec5dcdae3dd6bc67441b0c2340798134c1a5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 431 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -20,7 +20,7 @@ class SpiderInfo(object):\n \n     def __init__(self, download_delay=None, max_concurrent_requests=None):\n         if download_delay is None:\n-            self.download_delay = settings.getint('DOWNLOAD_DELAY')\n+            self.download_delay = settings.getfloat('DOWNLOAD_DELAY')\n         else:\n             self.download_delay = download_delay\n         if download_delay:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#53dfc4d3dd5da224d67e9aafa691a8a8ee6a41b6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 433 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -23,7 +23,7 @@ class SpiderInfo(object):\n             self.download_delay = settings.getfloat('DOWNLOAD_DELAY')\n         else:\n             self.download_delay = download_delay\n-        if download_delay:\n+        if self.download_delay:\n             self.max_concurrent_requests = 1\n         elif max_concurrent_requests is None:\n             self.max_concurrent_requests = settings.getint('CONCURRENT_REQUESTS_PER_SPIDER')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
