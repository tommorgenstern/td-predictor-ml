{"custom_id": "keras#d6388ed6457465ab35da8a34ae4abf5847a50cd0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 85 | Files Changed: 1 | Hunks: 21 | Methods Changed: 18 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 119 | Churn Cumulative: 1874 | Contributors (this commit): 31 | Commits (past 90d): 6 | Contributors (cumulative): 31 | DMM Complexity: 0.0\n\nDIFF:\n@@ -34,15 +34,24 @@ train_samples = 20\n test_samples = 20\n \n \n-@keras_test\n-def test_TerminateOnNaN():\n-    np.random.seed(1337)\n-    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n+# Changing the default arguments of get_test_data.\n+def get_data_callbacks(num_train=train_samples,\n                        num_test=test_samples,\n                        input_shape=(input_dim,),\n                        classification=True,\n+                       num_classes=num_classes):\n+    return get_test_data(num_train=num_train,\n+                         num_test=num_test,\n+                         input_shape=input_shape,\n+                         classification=classification,\n                          num_classes=num_classes)\n \n+\n+@keras_test\n+def test_TerminateOnNaN():\n+    np.random.seed(1337)\n+    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n+\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n     cbks = [callbacks.TerminateOnNaN()]\n@@ -85,11 +94,7 @@ def test_TerminateOnNaN():\n def test_stop_training_csv(tmpdir):\n     np.random.seed(1337)\n     fp = str(tmpdir / 'test.csv')\n-    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n-                                                         num_test=test_samples,\n-                                                         input_shape=(input_dim,),\n-                                                         classification=True,\n-                                                         num_classes=num_classes)\n+    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n \n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n@@ -138,11 +143,7 @@ def test_stop_training_csv(tmpdir):\n def test_ModelCheckpoint(tmpdir):\n     np.random.seed(1337)\n     filepath = str(tmpdir / 'checkpoint.h5')\n-    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n-                                                         num_test=test_samples,\n-                                                         input_shape=(input_dim,),\n-                                                         classification=True,\n-                                                         num_classes=num_classes)\n+    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n     # case 1\n@@ -214,11 +215,7 @@ def test_ModelCheckpoint(tmpdir):\n @keras_test\n def test_EarlyStopping():\n     np.random.seed(1337)\n-    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n-                                                         num_test=test_samples,\n-                                                         input_shape=(input_dim,),\n-                                                         classification=True,\n-                                                         num_classes=num_classes)\n+    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n     model = Sequential()\n@@ -414,11 +411,7 @@ def test_EarlyStopping_final_weights_when_restoring_model_weights():\n @keras_test\n def test_LearningRateScheduler():\n     np.random.seed(1337)\n-    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n-                                                         num_test=test_samples,\n-                                                         input_shape=(input_dim,),\n-                                                         classification=True,\n-                                                         num_classes=num_classes)\n+    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n     model = Sequential()\n@@ -437,11 +430,7 @@ def test_LearningRateScheduler():\n @keras_test\n def test_ReduceLROnPlateau():\n     np.random.seed(1337)\n-    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n-                                                         num_test=test_samples,\n-                                                         input_shape=(input_dim,),\n-                                                         classification=True,\n-                                                         num_classes=num_classes)\n+    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n \n@@ -516,11 +505,7 @@ def test_CSVLogger(tmpdir):\n     np.random.seed(1337)\n     filepath = str(tmpdir / 'log.tsv')\n     sep = '\\t'\n-    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n-                                                         num_test=test_samples,\n-                                                         input_shape=(input_dim,),\n-                                                         classification=True,\n-                                                         num_classes=num_classes)\n+    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n \n@@ -576,12 +561,7 @@ def test_TensorBoard(tmpdir):\n     np.random.seed(np.random.randint(1, 1e7))\n     filepath = str(tmpdir / 'logs')\n \n-    (X_train, y_train), (X_test, y_test) = get_test_data(\n-        num_train=train_samples,\n-        num_test=test_samples,\n-        input_shape=(input_dim,),\n-        classification=True,\n-        num_classes=num_classes)\n+    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n \n@@ -666,12 +646,7 @@ def test_TensorBoard_histogram_freq_must_have_validation_data(tmpdir):\n     np.random.seed(np.random.randint(1, 1e7))\n     filepath = str(tmpdir / 'logs')\n \n-    (X_train, y_train), (X_test, y_test) = get_test_data(\n-        num_train=train_samples,\n-        num_test=test_samples,\n-        input_shape=(input_dim,),\n-        classification=True,\n-        num_classes=num_classes)\n+    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n \n@@ -739,12 +714,9 @@ def test_TensorBoard_multi_input_output(tmpdir):\n     np.random.seed(np.random.randint(1, 1e7))\n     filepath = str(tmpdir / 'logs')\n \n-    (X_train, y_train), (X_test, y_test) = get_test_data(\n-        num_train=train_samples,\n-        num_test=test_samples,\n-        input_shape=(input_dim, input_dim),\n-        classification=True,\n-        num_classes=num_classes)\n+    (X_train, y_train), (X_test, y_test) = get_data_callbacks(\n+        input_shape=(input_dim, input_dim))\n+\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n \n@@ -822,11 +794,10 @@ def test_TensorBoard_convnet(tmpdir):\n     filepath = str(tmpdir / 'logs')\n \n     input_shape = (16, 16, 3)\n-    (x_train, y_train), (x_test, y_test) = get_test_data(num_train=500,\n+    (x_train, y_train), (x_test, y_test) = get_data_callbacks(\n+        num_train=500,\n         num_test=200,\n-                                                         input_shape=input_shape,\n-                                                         classification=True,\n-                                                         num_classes=num_classes)\n+        input_shape=input_shape)\n     y_train = np_utils.to_categorical(y_train)\n     y_test = np_utils.to_categorical(y_test)\n \n@@ -862,11 +833,9 @@ def test_TensorBoard_display_float_from_logs(tmpdir):\n     filepath = str(tmpdir / 'logs')\n \n     input_shape = (3,)\n-    (x_train, y_train), _ = get_test_data(num_train=10,\n+    (x_train, y_train), _ = get_data_callbacks(num_train=10,\n                                                num_test=0,\n-                                          input_shape=input_shape,\n-                                          classification=True,\n-                                          num_classes=num_classes)\n+                                               input_shape=input_shape)\n     y_train = np_utils.to_categorical(y_train)\n \n     model = Sequential([\n@@ -894,11 +863,7 @@ def test_TensorBoard_display_float_from_logs(tmpdir):\n @keras_test\n def test_CallbackValData():\n     np.random.seed(1337)\n-    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n-                                                         num_test=test_samples,\n-                                                         input_shape=(input_dim,),\n-                                                         classification=True,\n-                                                         num_classes=num_classes)\n+    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n     model = Sequential()\n@@ -943,11 +908,7 @@ def test_CallbackValData():\n @keras_test\n def test_LambdaCallback():\n     np.random.seed(1337)\n-    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n-                                                         num_test=test_samples,\n-                                                         input_shape=(input_dim,),\n-                                                         classification=True,\n-                                                         num_classes=num_classes)\n+    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n     model = Sequential()\n@@ -981,11 +942,7 @@ def test_TensorBoard_with_ReduceLROnPlateau(tmpdir):\n     np.random.seed(np.random.randint(1, 1e7))\n     filepath = str(tmpdir / 'logs')\n \n-    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n-                                                         num_test=test_samples,\n-                                                         input_shape=(input_dim,),\n-                                                         classification=True,\n-                                                         num_classes=num_classes)\n+    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n \n@@ -1015,11 +972,7 @@ def test_TensorBoard_with_ReduceLROnPlateau(tmpdir):\n \n @keras_test\n def tests_RemoteMonitor():\n-    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n-                                                         num_test=test_samples,\n-                                                         input_shape=(input_dim,),\n-                                                         classification=True,\n-                                                         num_classes=num_classes)\n+    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n     model = Sequential()\n@@ -1037,11 +990,7 @@ def tests_RemoteMonitor():\n \n @keras_test\n def tests_RemoteMonitorWithJsonPayload():\n-    (X_train, y_train), (X_test, y_test) = get_test_data(num_train=train_samples,\n-                                                         num_test=test_samples,\n-                                                         input_shape=(input_dim,),\n-                                                         classification=True,\n-                                                         num_classes=num_classes)\n+    (X_train, y_train), (X_test, y_test) = get_data_callbacks()\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n     model = Sequential()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#03bd8709778eefb611e470c76af44b2ce90fbff8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 211 | Lines Deleted: 107 | Files Changed: 6 | Hunks: 71 | Methods Changed: 14 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 318 | Churn Cumulative: 25524 | Contributors (this commit): 133 | Commits (past 90d): 25 | Contributors (cumulative): 186 | DMM Complexity: 0.864406779661017\n\nDIFF:\n@@ -105,11 +105,13 @@ class _Conv(Layer):\n         super(_Conv, self).__init__(**kwargs)\n         self.rank = rank\n         self.filters = filters\n-        self.kernel_size = conv_utils.normalize_tuple(kernel_size, rank, 'kernel_size')\n+        self.kernel_size = conv_utils.normalize_tuple(kernel_size, rank,\n+                                                      'kernel_size')\n         self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')\n         self.padding = conv_utils.normalize_padding(padding)\n         self.data_format = K.normalize_data_format(data_format)\n-        self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, rank, 'dilation_rate')\n+        self.dilation_rate = conv_utils.normalize_tuple(dilation_rate, rank,\n+                                                        'dilation_rate')\n         self.activation = activations.get(activation)\n         self.use_bias = use_bias\n         self.kernel_initializer = initializers.get(kernel_initializer)\n@@ -227,7 +229,8 @@ class _Conv(Layer):\n             'bias_initializer': initializers.serialize(self.bias_initializer),\n             'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n             'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n-            'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n+            'activity_regularizer':\n+                regularizers.serialize(self.activity_regularizer),\n             'kernel_constraint': constraints.serialize(self.kernel_constraint),\n             'bias_constraint': constraints.serialize(self.bias_constraint)\n         }\n@@ -270,7 +273,8 @@ class Conv1D(_Conv):\n             the output has the same length as the original input.\n             Useful when modeling temporal data where the model\n             should not violate the temporal order. See\n-            [WaveNet: A Generative Model for Raw Audio, section 2.1](https://arxiv.org/abs/1609.03499).\n+            [WaveNet: A Generative Model for Raw Audio, section 2.1]\n+            (https://arxiv.org/abs/1609.03499).\n         data_format: A string,\n             one of `\"channels_last\"` (default) or `\"channels_first\"`.\n             The ordering of the dimensions in the inputs.\n@@ -574,7 +578,8 @@ class Conv3D(_Conv):\n         or 5D tensor with shape:\n         `(batch, new_conv_dim1, new_conv_dim2, new_conv_dim3, filters)`\n         if `data_format` is `\"channels_last\"`.\n-        `new_conv_dim1`, `new_conv_dim2` and `new_conv_dim3` values might have changed due to padding.\n+        `new_conv_dim1`, `new_conv_dim2` and `new_conv_dim3` values might have\n+        changed due to padding.\n     \"\"\"\n \n     @interfaces.legacy_conv3d_support\n@@ -715,13 +720,17 @@ class Conv2DTranspose(Conv2D):\n         If `output_padding` is specified:\n \n         ```\n-        new_rows = (rows - 1) * strides[0] + kernel_size[0] - 2 * padding[0] + output_padding[0]\n-        new_cols = (cols - 1) * strides[1] + kernel_size[1] - 2 * padding[1] + output_padding[1]\n+        new_rows = ((rows - 1) * strides[0] + kernel_size[0]\n+                    - 2 * padding[0] + output_padding[0])\n+        new_cols = ((cols - 1) * strides[1] + kernel_size[1]\n+                    - 2 * padding[1] + output_padding[1])\n         ```\n \n     # References\n-        - [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285v1)\n-        - [Deconvolutional Networks](http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)\n+        - [A guide to convolution arithmetic for deep learning]\n+          (https://arxiv.org/abs/1603.07285v1)\n+        - [Deconvolutional Networks]\n+          (http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)\n     \"\"\"\n \n     @interfaces.legacy_deconv2d_support\n@@ -984,14 +993,19 @@ class Conv3DTranspose(Conv3D):\n         If `output_padding` is specified::\n \n         ```\n-        new_depth = (depth - 1) * strides[0] + kernel_size[0] - 2 * padding[0] + output_padding[0]\n-        new_rows = (rows - 1) * strides[1] + kernel_size[1] - 2 * padding[1] + output_padding[1]\n-        new_cols = (cols - 1) * strides[2] + kernel_size[2] - 2 * padding[2] + output_padding[2]\n+        new_depth = ((depth - 1) * strides[0] + kernel_size[0]\n+                     - 2 * padding[0] + output_padding[0])\n+        new_rows = ((rows - 1) * strides[1] + kernel_size[1]\n+                    - 2 * padding[1] + output_padding[1])\n+        new_cols = ((cols - 1) * strides[2] + kernel_size[2]\n+                    - 2 * padding[2] + output_padding[2])\n         ```\n \n     # References\n-        - [A guide to convolution arithmetic for deep learning](https://arxiv.org/abs/1603.07285v1)\n-        - [Deconvolutional Networks](http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)\n+        - [A guide to convolution arithmetic for deep learning]\n+          (https://arxiv.org/abs/1603.07285v1)\n+        - [Deconvolutional Networks]\n+          (http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)\n     \"\"\"\n \n     def __init__(self, filters,\n@@ -1103,9 +1117,11 @@ class Conv3DTranspose(Conv3D):\n                                              out_pad_w)\n \n         if self.data_format == 'channels_first':\n-            output_shape = (batch_size, self.filters, out_depth, out_height, out_width)\n+            output_shape = (batch_size, self.filters,\n+                            out_depth, out_height, out_width)\n         else:\n-            output_shape = (batch_size, out_depth, out_height, out_width, self.filters)\n+            output_shape = (batch_size, out_depth,\n+                            out_height, out_width, self.filters)\n \n         outputs = K.conv3d_transpose(inputs,\n                                      self.kernel,\n@@ -1318,8 +1334,10 @@ class _SeparableConv(_Conv):\n             raise ValueError('The channel dimension of the inputs '\n                              'should be defined. Found `None`.')\n         input_dim = int(input_shape[channel_axis])\n-        depthwise_kernel_shape = self.kernel_size + (input_dim, self.depth_multiplier)\n-        pointwise_kernel_shape = (1,) * self.rank + (self.depth_multiplier * input_dim, self.filters)\n+        depthwise_kernel_shape = (input_dim, self.depth_multiplier)\n+        depthwise_kernel_shape = self.kernel_size + depthwise_kernel_shape\n+        pointwise_kernel_shape = (self.depth_multiplier * input_dim, self.filters)\n+        pointwise_kernel_shape = (1,) * self.rank + pointwise_kernel_shape\n \n         self.depthwise_kernel = self.add_weight(\n             shape=depthwise_kernel_shape,\n@@ -1384,12 +1402,18 @@ class _SeparableConv(_Conv):\n         config.pop('kernel_regularizer')\n         config.pop('kernel_constraint')\n         config['depth_multiplier'] = self.depth_multiplier\n-        config['depthwise_initializer'] = initializers.serialize(self.depthwise_initializer)\n-        config['pointwise_initializer'] = initializers.serialize(self.pointwise_initializer)\n-        config['depthwise_regularizer'] = regularizers.serialize(self.depthwise_regularizer)\n-        config['pointwise_regularizer'] = regularizers.serialize(self.pointwise_regularizer)\n-        config['depthwise_constraint'] = constraints.serialize(self.depthwise_constraint)\n-        config['pointwise_constraint'] = constraints.serialize(self.pointwise_constraint)\n+        config['depthwise_initializer'] = (\n+            initializers.serialize(self.depthwise_initializer))\n+        config['pointwise_initializer'] = (\n+            initializers.serialize(self.pointwise_initializer))\n+        config['depthwise_regularizer'] = (\n+            regularizers.serialize(self.depthwise_regularizer))\n+        config['pointwise_regularizer'] = (\n+            regularizers.serialize(self.pointwise_regularizer))\n+        config['depthwise_constraint'] = (\n+            constraints.serialize(self.depthwise_constraint))\n+        config['pointwise_constraint'] = (\n+            constraints.serialize(self.pointwise_constraint))\n         return config\n \n \n@@ -1862,9 +1886,12 @@ class DepthwiseConv2D(Conv2D):\n         config.pop('kernel_regularizer')\n         config.pop('kernel_constraint')\n         config['depth_multiplier'] = self.depth_multiplier\n-        config['depthwise_initializer'] = initializers.serialize(self.depthwise_initializer)\n-        config['depthwise_regularizer'] = regularizers.serialize(self.depthwise_regularizer)\n-        config['depthwise_constraint'] = constraints.serialize(self.depthwise_constraint)\n+        config['depthwise_initializer'] = (\n+            initializers.serialize(self.depthwise_initializer))\n+        config['depthwise_regularizer'] = (\n+            regularizers.serialize(self.depthwise_regularizer))\n+        config['depthwise_constraint'] = (\n+            constraints.serialize(self.depthwise_constraint))\n         return config\n \n \n@@ -1982,7 +2009,8 @@ class UpSampling2D(_UpSampling):\n     \"\"\"\n \n     @interfaces.legacy_upsampling2d_support\n-    def __init__(self, size=(2, 2), data_format=None, interpolation='nearest', **kwargs):\n+    def __init__(self, size=(2, 2), data_format=None, interpolation='nearest',\n+                 **kwargs):\n         normalized_size = conv_utils.normalize_tuple(size, 2, 'size')\n         super(UpSampling2D, self).__init__(normalized_size, data_format, **kwargs)\n         if interpolation not in ['nearest', 'bilinear']:\n@@ -2216,7 +2244,9 @@ class ZeroPadding3D(_ZeroPadding):\n                 `(symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad)`.\n             - If tuple of 3 tuples of 2 ints:\n                 interpreted as\n-                `((left_dim1_pad, right_dim1_pad), (left_dim2_pad, right_dim2_pad), (left_dim3_pad, right_dim3_pad))`\n+                `((left_dim1_pad, right_dim1_pad),\n+                  (left_dim2_pad, right_dim2_pad),\n+                  (left_dim3_pad, right_dim3_pad))`\n         data_format: A string,\n             one of `\"channels_last\"` or `\"channels_first\"`.\n             The ordering of the dimensions in the inputs.\n@@ -2231,22 +2261,26 @@ class ZeroPadding3D(_ZeroPadding):\n     # Input shape\n         5D tensor with shape:\n         - If `data_format` is `\"channels_last\"`:\n-            `(batch, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad, depth)`\n+            `(batch, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad,\n+              depth)`\n         - If `data_format` is `\"channels_first\"`:\n-            `(batch, depth, first_axis_to_pad, second_axis_to_pad, third_axis_to_pad)`\n+            `(batch, depth,\n+              first_axis_to_pad, second_axis_to_pad, third_axis_to_pad)`\n \n     # Output shape\n         5D tensor with shape:\n         - If `data_format` is `\"channels_last\"`:\n-            `(batch, first_padded_axis, second_padded_axis, third_axis_to_pad, depth)`\n+            `(batch, first_padded_axis, second_padded_axis, third_axis_to_pad,\n+              depth)`\n         - If `data_format` is `\"channels_first\"`:\n-            `(batch, depth, first_padded_axis, second_padded_axis, third_axis_to_pad)`\n+            `(batch, depth,\n+              first_padded_axis, second_padded_axis, third_axis_to_pad)`\n     \"\"\"\n \n     @interfaces.legacy_zeropadding3d_support\n     def __init__(self, padding=(1, 1, 1), data_format=None, **kwargs):\n         if isinstance(padding, int):\n-            normalized_padding = ((padding, padding), (padding, padding), (padding, padding))\n+            normalized_padding = 3 * ((padding, padding),)\n         elif hasattr(padding, '__len__'):\n             if len(padding) != 3:\n                 raise ValueError('`padding` should have 3 elements. '\n@@ -2259,8 +2293,8 @@ class ZeroPadding3D(_ZeroPadding):\n                                                       '3rd entry of padding')\n             normalized_padding = (dim1_padding, dim2_padding, dim3_padding)\n         else:\n-            raise ValueError('`padding` should be either an int, '\n-                             'a tuple of 3 ints '\n+            raise ValueError(\n+                '`padding` should be either an int, a tuple of 3 ints '\n                 '(symmetric_dim1_pad, symmetric_dim2_pad, symmetric_dim3_pad), '\n                 'or a tuple of 3 tuples of 2 ints '\n                 '((left_dim1_pad, right_dim1_pad),'\n@@ -2466,7 +2500,9 @@ class Cropping3D(_Cropping):\n                 `(symmetric_dim1_crop, symmetric_dim2_crop, symmetric_dim3_crop)`.\n             - If tuple of 3 tuples of 2 ints:\n                 interpreted as\n-                `((left_dim1_crop, right_dim1_crop), (left_dim2_crop, right_dim2_crop), (left_dim3_crop, right_dim3_crop))`\n+                `((left_dim1_crop, right_dim1_crop),\n+                  (left_dim2_crop, right_dim2_crop),\n+                  (left_dim3_crop, right_dim3_crop))`\n         data_format: A string,\n             one of `\"channels_last\"` or `\"channels_first\"`.\n             The ordering of the dimensions in the inputs.\n@@ -2481,16 +2517,20 @@ class Cropping3D(_Cropping):\n     # Input shape\n         5D tensor with shape:\n         - If `data_format` is `\"channels_last\"`:\n-            `(batch, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop, depth)`\n+            `(batch, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop,\n+              depth)`\n         - If `data_format` is `\"channels_first\"`:\n-            `(batch, depth, first_axis_to_crop, second_axis_to_crop, third_axis_to_crop)`\n+            `(batch, depth,\n+              first_axis_to_crop, second_axis_to_crop, third_axis_to_crop)`\n \n     # Output shape\n         5D tensor with shape:\n         - If `data_format` is `\"channels_last\"`:\n-            `(batch, first_cropped_axis, second_cropped_axis, third_cropped_axis, depth)`\n+            `(batch, first_cropped_axis, second_cropped_axis, third_cropped_axis,\n+              depth)`\n         - If `data_format` is `\"channels_first\"`:\n-            `(batch, depth, first_cropped_axis, second_cropped_axis, third_cropped_axis)`\n+            `(batch, depth,\n+              first_cropped_axis, second_cropped_axis, third_cropped_axis)`\n     \"\"\"\n \n     @interfaces.legacy_cropping3d_support\n@@ -2513,8 +2553,8 @@ class Cropping3D(_Cropping):\n                                                        '3rd entry of cropping')\n             normalized_cropping = (dim1_cropping, dim2_cropping, dim3_cropping)\n         else:\n-            raise ValueError('`cropping` should be either an int, '\n-                             'a tuple of 3 ints '\n+            raise ValueError(\n+                '`cropping` should be either an int, a tuple of 3 ints '\n                 '(symmetric_dim1_crop, symmetric_dim2_crop, symmetric_dim3_crop), '\n                 'or a tuple of 3 tuples of 2 ints '\n                 '((left_dim1_crop, right_dim1_crop),'\n\n@@ -87,7 +87,8 @@ class _Merge(Layer):\n                 shape = None\n             else:\n                 shape = input_shape[i][1:]\n-            output_shape = self._compute_elemwise_op_output_shape(output_shape, shape)\n+            output_shape = self._compute_elemwise_op_output_shape(output_shape,\n+                                                                  shape)\n         # If the inputs have different ranks, we have to reshape them\n         # to make them broadcastable.\n         if None not in input_shape and len(set(map(len, input_shape))) == 1:\n@@ -122,8 +123,10 @@ class _Merge(Layer):\n                     if x_ndim is None:\n                         x_shape = K.shape(x)\n                         batch_size = x_shape[0]\n-                        new_shape = K.concatenate([x_shape[1:], K.expand_dims(batch_size)])\n-                        x_transposed = K.reshape(x, K.stack([batch_size, K.prod(x_shape[1:])]))\n+                        new_shape = K.concatenate([x_shape[1:],\n+                                                   K.expand_dims(batch_size)])\n+                        x_transposed = K.reshape(x, K.stack([batch_size,\n+                                                             K.prod(x_shape[1:])]))\n                         x_transposed = K.permute_dimensions(x_transposed, (1, 0))\n                         x_transposed = K.reshape(x_transposed, new_shape)\n                         reshaped_inputs.append(x_transposed)\n@@ -133,17 +136,20 @@ class _Merge(Layer):\n                         reshaped_inputs.append(K.permute_dimensions(x, dims))\n                         transposed = True\n                     else:\n-                        # We don't transpose inputs if they are 1D vectors or scalars.\n+                        # We don't transpose inputs if they are\n+                        # 1D vectors or scalars.\n                         reshaped_inputs.append(x)\n                 y = self._merge_function(reshaped_inputs)\n                 y_ndim = K.ndim(y)\n                 if transposed:\n-                    # If inputs have been transposed, we have to transpose the output too.\n+                    # If inputs have been transposed,\n+                    # we have to transpose the output too.\n                     if y_ndim is None:\n                         y_shape = K.shape(y)\n                         y_ndim = K.shape(y_shape)[0]\n                         batch_size = y_shape[y_ndim - 1]\n-                        new_shape = K.concatenate([K.expand_dims(batch_size), y_shape[:y_ndim - 1]])\n+                        new_shape = K.concatenate([K.expand_dims(batch_size),\n+                                                   y_shape[:y_ndim - 1]])\n                         y = K.reshape(y, (-1, batch_size))\n                         y = K.permute_dimensions(y, (1, 0))\n                         y = K.reshape(y, new_shape)\n@@ -164,7 +170,8 @@ class _Merge(Layer):\n                 shape = None\n             else:\n                 shape = input_shape[i][1:]\n-            output_shape = self._compute_elemwise_op_output_shape(output_shape, shape)\n+            output_shape = self._compute_elemwise_op_output_shape(output_shape,\n+                                                                  shape)\n         batch_sizes = [s[0] for s in input_shape if s is not None]\n         batch_sizes = set(batch_sizes)\n         batch_sizes -= set([None])\n@@ -206,7 +213,8 @@ class Add(_Merge):\n         x1 = keras.layers.Dense(8, activation='relu')(input1)\n         input2 = keras.layers.Input(shape=(32,))\n         x2 = keras.layers.Dense(8, activation='relu')(input2)\n-        added = keras.layers.Add()([x1, x2])  # equivalent to added = keras.layers.add([x1, x2])\n+        # equivalent to added = keras.layers.add([x1, x2])\n+        added = keras.layers.Add()([x1, x2])\n \n         out = keras.layers.Dense(4)(added)\n         model = keras.models.Model(inputs=[input1, input2], outputs=out)\n\n@@ -74,7 +74,8 @@ class GaussianDropout(Layer):\n         Same shape as input.\n \n     # References\n-        - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting Srivastava, Hinton, et al. 2014](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n+        - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting]\n+          (http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\n     \"\"\"\n \n     @interfaces.legacy_gaussiandropout_support\n\n@@ -53,7 +53,8 @@ class BatchNormalization(Layer):\n         Same shape as input.\n \n     # References\n-        - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n+        - [Batch Normalization: Accelerating Deep Network Training by\n+           Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\n     \"\"\"\n \n     @interfaces.legacy_batchnorm_support\n@@ -82,7 +83,8 @@ class BatchNormalization(Layer):\n         self.beta_initializer = initializers.get(beta_initializer)\n         self.gamma_initializer = initializers.get(gamma_initializer)\n         self.moving_mean_initializer = initializers.get(moving_mean_initializer)\n-        self.moving_variance_initializer = initializers.get(moving_variance_initializer)\n+        self.moving_variance_initializer = (\n+            initializers.get(moving_variance_initializer))\n         self.beta_regularizer = regularizers.get(beta_regularizer)\n         self.gamma_regularizer = regularizers.get(gamma_regularizer)\n         self.beta_constraint = constraints.get(beta_constraint)\n@@ -212,8 +214,10 @@ class BatchNormalization(Layer):\n             'scale': self.scale,\n             'beta_initializer': initializers.serialize(self.beta_initializer),\n             'gamma_initializer': initializers.serialize(self.gamma_initializer),\n-            'moving_mean_initializer': initializers.serialize(self.moving_mean_initializer),\n-            'moving_variance_initializer': initializers.serialize(self.moving_variance_initializer),\n+            'moving_mean_initializer':\n+                initializers.serialize(self.moving_mean_initializer),\n+            'moving_variance_initializer':\n+                initializers.serialize(self.moving_variance_initializer),\n             'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n             'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n             'beta_constraint': constraints.serialize(self.beta_constraint),\n\n@@ -921,14 +921,19 @@ class SimpleRNNCell(Layer):\n         config = {'units': self.units,\n                   'activation': activations.serialize(self.activation),\n                   'use_bias': self.use_bias,\n-                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n-                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n+                  'kernel_initializer':\n+                      initializers.serialize(self.kernel_initializer),\n+                  'recurrent_initializer':\n+                      initializers.serialize(self.recurrent_initializer),\n                   'bias_initializer': initializers.serialize(self.bias_initializer),\n-                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n-                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n+                  'kernel_regularizer':\n+                      regularizers.serialize(self.kernel_regularizer),\n+                  'recurrent_regularizer':\n+                      regularizers.serialize(self.recurrent_regularizer),\n                   'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                   'kernel_constraint': constraints.serialize(self.kernel_constraint),\n-                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n+                  'recurrent_constraint':\n+                      constraints.serialize(self.recurrent_constraint),\n                   'bias_constraint': constraints.serialize(self.bias_constraint),\n                   'dropout': self.dropout,\n                   'recurrent_dropout': self.recurrent_dropout}\n@@ -1126,15 +1131,21 @@ class SimpleRNN(RNN):\n         config = {'units': self.units,\n                   'activation': activations.serialize(self.activation),\n                   'use_bias': self.use_bias,\n-                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n-                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n+                  'kernel_initializer':\n+                      initializers.serialize(self.kernel_initializer),\n+                  'recurrent_initializer':\n+                      initializers.serialize(self.recurrent_initializer),\n                   'bias_initializer': initializers.serialize(self.bias_initializer),\n-                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n-                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n+                  'kernel_regularizer':\n+                      regularizers.serialize(self.kernel_regularizer),\n+                  'recurrent_regularizer':\n+                      regularizers.serialize(self.recurrent_regularizer),\n                   'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n-                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n+                  'activity_regularizer':\n+                      regularizers.serialize(self.activity_regularizer),\n                   'kernel_constraint': constraints.serialize(self.kernel_constraint),\n-                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n+                  'recurrent_constraint':\n+                      constraints.serialize(self.recurrent_constraint),\n                   'bias_constraint': constraints.serialize(self.bias_constraint),\n                   'dropout': self.dropout,\n                   'recurrent_dropout': self.recurrent_dropout}\n@@ -1310,7 +1321,8 @@ class GRUCell(Layer):\n             # bias for hidden state - just for compatibility with CuDNN\n             if self.reset_after:\n                 self.recurrent_bias_z = self.recurrent_bias[:self.units]\n-                self.recurrent_bias_r = self.recurrent_bias[self.units: self.units * 2]\n+                self.recurrent_bias_r = (\n+                    self.recurrent_bias[self.units: self.units * 2])\n                 self.recurrent_bias_h = self.recurrent_bias[self.units * 2:]\n         else:\n             self.input_bias_z = None\n@@ -1442,16 +1454,22 @@ class GRUCell(Layer):\n     def get_config(self):\n         config = {'units': self.units,\n                   'activation': activations.serialize(self.activation),\n-                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n+                  'recurrent_activation':\n+                      activations.serialize(self.recurrent_activation),\n                   'use_bias': self.use_bias,\n-                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n-                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n+                  'kernel_initializer':\n+                      initializers.serialize(self.kernel_initializer),\n+                  'recurrent_initializer':\n+                      initializers.serialize(self.recurrent_initializer),\n                   'bias_initializer': initializers.serialize(self.bias_initializer),\n-                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n-                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n+                  'kernel_regularizer':\n+                      regularizers.serialize(self.kernel_regularizer),\n+                  'recurrent_regularizer':\n+                      regularizers.serialize(self.recurrent_regularizer),\n                   'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                   'kernel_constraint': constraints.serialize(self.kernel_constraint),\n-                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n+                  'recurrent_constraint':\n+                      constraints.serialize(self.recurrent_constraint),\n                   'bias_constraint': constraints.serialize(self.bias_constraint),\n                   'dropout': self.dropout,\n                   'recurrent_dropout': self.recurrent_dropout,\n@@ -1548,10 +1566,14 @@ class GRU(RNN):\n             True = \"after\" (CuDNN compatible).\n \n     # References\n-        - [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n-        - [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n-        - [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling](http://arxiv.org/abs/1412.3555v1)\n-        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n+        - [Learning Phrase Representations using RNN Encoder-Decoder for\n+           Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n+        - [On the Properties of Neural Machine Translation:\n+           Encoder-Decoder Approaches](https://arxiv.org/abs/1409.1259)\n+        - [Empirical Evaluation of Gated Recurrent Neural Networks on\n+           Sequence Modeling](https://arxiv.org/abs/1412.3555v1)\n+        - [A Theoretically Grounded Application of Dropout in\n+           Recurrent Neural Networks](https://arxiv.org/abs/1512.05287)\n     \"\"\"\n \n     @interfaces.legacy_recurrent_support\n@@ -1697,17 +1719,24 @@ class GRU(RNN):\n     def get_config(self):\n         config = {'units': self.units,\n                   'activation': activations.serialize(self.activation),\n-                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n+                  'recurrent_activation':\n+                      activations.serialize(self.recurrent_activation),\n                   'use_bias': self.use_bias,\n-                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n-                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n+                  'kernel_initializer':\n+                      initializers.serialize(self.kernel_initializer),\n+                  'recurrent_initializer':\n+                      initializers.serialize(self.recurrent_initializer),\n                   'bias_initializer': initializers.serialize(self.bias_initializer),\n-                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n-                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n+                  'kernel_regularizer':\n+                      regularizers.serialize(self.kernel_regularizer),\n+                  'recurrent_regularizer':\n+                      regularizers.serialize(self.recurrent_regularizer),\n                   'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n-                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n+                  'activity_regularizer':\n+                      regularizers.serialize(self.activity_regularizer),\n                   'kernel_constraint': constraints.serialize(self.kernel_constraint),\n-                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n+                  'recurrent_constraint':\n+                      constraints.serialize(self.recurrent_constraint),\n                   'bias_constraint': constraints.serialize(self.bias_constraint),\n                   'dropout': self.dropout,\n                   'recurrent_dropout': self.recurrent_dropout,\n@@ -1753,7 +1782,8 @@ class LSTMCell(Layer):\n         unit_forget_bias: Boolean.\n             If True, add 1 to the bias of the forget gate at initialization.\n             Setting it to true will also force `bias_initializer=\"zeros\"`.\n-            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n+            This is recommended in [Jozefowicz et al.]\n+            (http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n         kernel_regularizer: Regularizer function applied to\n             the `kernel` weights matrix\n             (see [regularizer](../regularizers.md)).\n@@ -1867,8 +1897,10 @@ class LSTMCell(Layer):\n         self.kernel_o = self.kernel[:, self.units * 3:]\n \n         self.recurrent_kernel_i = self.recurrent_kernel[:, :self.units]\n-        self.recurrent_kernel_f = self.recurrent_kernel[:, self.units: self.units * 2]\n-        self.recurrent_kernel_c = self.recurrent_kernel[:, self.units * 2: self.units * 3]\n+        self.recurrent_kernel_f = (\n+            self.recurrent_kernel[:, self.units: self.units * 2])\n+        self.recurrent_kernel_c = (\n+            self.recurrent_kernel[:, self.units * 2: self.units * 3])\n         self.recurrent_kernel_o = self.recurrent_kernel[:, self.units * 3:]\n \n         if self.use_bias:\n@@ -1974,17 +2006,23 @@ class LSTMCell(Layer):\n     def get_config(self):\n         config = {'units': self.units,\n                   'activation': activations.serialize(self.activation),\n-                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n+                  'recurrent_activation':\n+                      activations.serialize(self.recurrent_activation),\n                   'use_bias': self.use_bias,\n-                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n-                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n+                  'kernel_initializer':\n+                      initializers.serialize(self.kernel_initializer),\n+                  'recurrent_initializer':\n+                      initializers.serialize(self.recurrent_initializer),\n                   'bias_initializer': initializers.serialize(self.bias_initializer),\n                   'unit_forget_bias': self.unit_forget_bias,\n-                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n-                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n+                  'kernel_regularizer':\n+                      regularizers.serialize(self.kernel_regularizer),\n+                  'recurrent_regularizer':\n+                      regularizers.serialize(self.recurrent_regularizer),\n                   'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n                   'kernel_constraint': constraints.serialize(self.kernel_constraint),\n-                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n+                  'recurrent_constraint':\n+                      constraints.serialize(self.recurrent_constraint),\n                   'bias_constraint': constraints.serialize(self.bias_constraint),\n                   'dropout': self.dropout,\n                   'recurrent_dropout': self.recurrent_dropout,\n@@ -2022,7 +2060,8 @@ class LSTM(RNN):\n         unit_forget_bias: Boolean.\n             If True, add 1 to the bias of the forget gate at initialization.\n             Setting it to true will also force `bias_initializer=\"zeros\"`.\n-            This is recommended in [Jozefowicz et al.](http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)\n+            This is recommended in [Jozefowicz et al.]\n+            (http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf).\n         kernel_regularizer: Regularizer function applied to\n             the `kernel` weights matrix\n             (see [regularizer](../regularizers.md)).\n@@ -2072,10 +2111,14 @@ class LSTM(RNN):\n             Unrolling is only suitable for short sequences.\n \n     # References\n-        - [Long short-term memory](http://www.bioinf.jku.at/publications/older/2604.pdf) (original 1997 paper)\n-        - [Learning to forget: Continual prediction with LSTM](http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n-        - [Supervised sequence labeling with recurrent neural networks](http://www.cs.toronto.edu/~graves/preprint.pdf)\n-        - [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](http://arxiv.org/abs/1512.05287)\n+        - [Long short-term memory]\n+          (http://www.bioinf.jku.at/publications/older/2604.pdf)\n+        - [Learning to forget: Continual prediction with LSTM]\n+          (http://www.mitpressjournals.org/doi/pdf/10.1162/089976600300015015)\n+        - [Supervised sequence labeling with recurrent neural networks]\n+          (http://www.cs.toronto.edu/~graves/preprint.pdf)\n+        - [A Theoretically Grounded Application of Dropout in\n+           Recurrent Neural Networks](https://arxiv.org/abs/1512.05287)\n     \"\"\"\n \n     @interfaces.legacy_recurrent_support\n@@ -2221,18 +2264,25 @@ class LSTM(RNN):\n     def get_config(self):\n         config = {'units': self.units,\n                   'activation': activations.serialize(self.activation),\n-                  'recurrent_activation': activations.serialize(self.recurrent_activation),\n+                  'recurrent_activation':\n+                      activations.serialize(self.recurrent_activation),\n                   'use_bias': self.use_bias,\n-                  'kernel_initializer': initializers.serialize(self.kernel_initializer),\n-                  'recurrent_initializer': initializers.serialize(self.recurrent_initializer),\n+                  'kernel_initializer':\n+                      initializers.serialize(self.kernel_initializer),\n+                  'recurrent_initializer':\n+                      initializers.serialize(self.recurrent_initializer),\n                   'bias_initializer': initializers.serialize(self.bias_initializer),\n                   'unit_forget_bias': self.unit_forget_bias,\n-                  'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n-                  'recurrent_regularizer': regularizers.serialize(self.recurrent_regularizer),\n+                  'kernel_regularizer':\n+                      regularizers.serialize(self.kernel_regularizer),\n+                  'recurrent_regularizer':\n+                      regularizers.serialize(self.recurrent_regularizer),\n                   'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n-                  'activity_regularizer': regularizers.serialize(self.activity_regularizer),\n+                  'activity_regularizer':\n+                      regularizers.serialize(self.activity_regularizer),\n                   'kernel_constraint': constraints.serialize(self.kernel_constraint),\n-                  'recurrent_constraint': constraints.serialize(self.recurrent_constraint),\n+                  'recurrent_constraint':\n+                      constraints.serialize(self.recurrent_constraint),\n                   'bias_constraint': constraints.serialize(self.bias_constraint),\n                   'dropout': self.dropout,\n                   'recurrent_dropout': self.recurrent_dropout,\n\n@@ -276,7 +276,8 @@ class TimeDistributed(Wrapper):\n         If the output mask at each time step is not `None`:\n         (E.g., inner layer is Masking or RNN)\n         Concatenate all of them and return the concatenation.\n-        If the output mask at each time step is `None` and the input mask is not `None`:\n+        If the output mask at each time step is `None` and\n+        the input mask is not `None`:\n         (E.g., inner layer is Dense)\n         Reduce the input_mask to 2 dimensions and return it.\n         Otherwise (both the output mask and the input mask are `None`):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
