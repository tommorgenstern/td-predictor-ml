{"custom_id": "keras#7858c4f9ee1abf63f552e1447385e192d8021a37", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 1177 | Contributors (this commit): 5 | Commits (past 90d): 9 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -81,7 +81,7 @@ def build():\n \n         # Find the .whl file path\n         for fname in os.listdir(dist_directory):\n-            if fname.endswith(\".whl\"):\n+            if __version__ in fname and fname.endswith(\".whl\"):\n                 whl_path = os.path.abspath(os.path.join(dist_directory, fname))\n         print(f\"Build successful. Wheel file available at {whl_path}\")\n     finally:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#25167f7ce91c34bdfe935a014392d65b91ea3a75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 172 | Lines Deleted: 2 | Files Changed: 4 | Hunks: 13 | Methods Changed: 15 | Complexity Δ (Sum/Max): 36/23 | Churn Δ: 174 | Churn Cumulative: 1488 | Contributors (this commit): 13 | Commits (past 90d): 64 | Contributors (cumulative): 29 | DMM Complexity: 0.6\n\nDIFF:\n@@ -1,7 +1,10 @@\n+import types\n+\n import jax\n import jax.numpy as jnp\n import numpy as np\n import tree\n+from jax.tree_util import Partial\n \n from keras_core.backend.common import KerasVariable\n from keras_core.backend.common import standardize_dtype\n@@ -68,12 +71,14 @@ def compute_output_spec(fn, *args, **kwargs):\n         built_in_types = (type(None), int, float, str, bool, complex, bytes)\n \n         # First, separate symbolic args from other args\n+        static_args_idx = []\n         static_args = []\n         maybe_symbolic_args = []\n         static_kwargs = {}\n         maybe_symbolic_kwargs = {}\n-        for arg in args:\n+        for idx, arg in enumerate(args):\n             if isinstance(arg, built_in_types):\n+                static_args_idx.append(idx)\n                 static_args.append(arg)\n             else:\n                 maybe_symbolic_args.append(arg)\n@@ -108,10 +113,43 @@ def compute_output_spec(fn, *args, **kwargs):\n                             shape[i] = fill_value\n                 jax_tensor = jax.ShapeDtypeStruct(shape, dtype=x.dtype)\n                 return jax_tensor\n+            if isinstance(x, types.FunctionType):\n+\n+                def _fn(*args, **kwargs):\n+                    out = x(*args, **kwargs)\n+                    out = convert_keras_tensor_to_jax(\n+                        out, fill_value=fill_value\n+                    )\n+                    return out\n+\n+                return Partial(_fn)\n+            if isinstance(x, dict):\n+                return {\n+                    k: convert_keras_tensor_to_jax(v, fill_value=fill_value)\n+                    for k, v in x.items()\n+                }\n+            if isinstance(x, list):\n+                return [\n+                    convert_keras_tensor_to_jax(xi, fill_value=fill_value)\n+                    for xi in x\n+                ]\n             return x\n \n         def wrapped_fn(*args, **kwargs):\n-            return fn(*args, *static_args, **kwargs, **static_kwargs)\n+            rec_args = []\n+            idx_static = 0\n+            idx_sym = 0\n+            i = 0\n+            while idx_static < len(static_args) or idx_sym < len(args):\n+                if i in static_args_idx:\n+                    rec_args.append(static_args[idx_static])\n+                    idx_static += 1\n+                else:\n+                    rec_args.append(args[idx_sym])\n+                    idx_sym += 1\n+\n+                i += 1\n+            return fn(*rec_args, **kwargs, **static_kwargs)\n \n         jax_out = None\n         if none_count:\n\n@@ -1,3 +1,5 @@\n+import types\n+\n import numpy as np\n import tensorflow as tf\n from tensorflow.compiler.tf2xla.python.xla import dynamic_update_slice\n@@ -107,6 +109,14 @@ def compute_output_spec(fn, *args, **kwargs):\n                     return tf.compat.v1.placeholder(\n                         shape=x.shape, dtype=x.dtype\n                     )\n+                if isinstance(x, types.FunctionType):\n+\n+                    def _fn(*x_args, **x_kwargs):\n+                        out = x(*x_args, **x_kwargs)\n+                        out = convert_keras_tensor_to_tf(out)\n+                        return out\n+\n+                    return _fn\n                 return x\n \n             args, kwargs = tf.nest.map_structure(\n\n@@ -9,6 +9,7 @@ shape\n cast\n convert_to_tensor\n convert_to_numpy\n+cond\n \"\"\"\n \n import numpy as np\n@@ -18,6 +19,7 @@ from keras_core.api_export import keras_core_export\n from keras_core.backend import KerasTensor\n from keras_core.backend import any_symbolic_tensors\n from keras_core.ops.operation import Operation\n+from keras_core.utils import traceback_utils\n \n \n class Scatter(Operation):\n@@ -422,3 +424,88 @@ def convert_to_numpy(x):\n         # We trigger it rather than duplicate it here.\n         return np.array(x)\n     return backend.convert_to_numpy(x)\n+\n+\n+class Cond(Operation):\n+    @traceback_utils.filter_traceback\n+    def __call__(self, *args, **kwargs):\n+        def call_fn(*args, **kwargs):\n+            if not any_symbolic_tensors(args, kwargs):\n+                try:\n+                    return self.call(*args, **kwargs)\n+                except (TypeError, ValueError):\n+                    # fallback on symbolic case\n+                    pass\n+            return self.symbolic_call(*args, **kwargs)\n+\n+        if traceback_utils.is_traceback_filtering_enabled():\n+            # Wrap self.call to provide helpful info in case of exception\n+            call_fn = traceback_utils.inject_argument_info_in_traceback(\n+                call_fn,\n+                object_name=(f\"{self.__class__.__name__}.call()\"),\n+            )\n+            return call_fn(*args, **kwargs)\n+\n+        # Plain flow.\n+        return call_fn(*args, **kwargs)\n+\n+    def call(self, pred, true_fn, false_fn):\n+        return backend.core.cond(pred, true_fn, false_fn)\n+\n+    def compute_output_spec(self, pred, true_fn, false_fn):\n+        def call_fn(fn):\n+            return fn()\n+\n+        true_fn_spec = backend.compute_output_spec(call_fn, true_fn)\n+        false_fn_spec = backend.compute_output_spec(call_fn, false_fn)\n+        if not self._check_output_spec(true_fn_spec, false_fn_spec):\n+            raise ValueError(\n+                \"`true_fn` and `false_fn` should return outputs \"\n+                \"of the same kind (struct, dtype and shape). \"\n+                f\"Got {true_fn_spec} and {false_fn_spec} instead.\"\n+            )\n+        return true_fn_spec\n+\n+    def _check_output_spec(self, true_fn_spec, false_fn_spec):\n+        if isinstance(true_fn_spec, dict):\n+            if not isinstance(false_fn_spec, dict):\n+                return False\n+            if true_fn_spec.keys() != false_fn_spec.keys():\n+                return False\n+            if any(\n+                (not self._check_output_spec(true_fn_spec[k], false_fn_spec[k]))\n+                for k in true_fn_spec.keys()\n+            ):\n+                return False\n+        elif isinstance(true_fn_spec, list):\n+            if not isinstance(false_fn_spec, list):\n+                return False\n+            if len(true_fn_spec) != len(false_fn_spec):\n+                return False\n+            if any(\n+                (not self._check_output_spec(ti, fi))\n+                for ti, fi in zip(true_fn_spec, false_fn_spec)\n+            ):\n+                return False\n+        else:\n+            if true_fn_spec.dtype != false_fn_spec.dtype:\n+                return False\n+            if true_fn_spec.shape != false_fn_spec.shape:\n+                return False\n+\n+        return True\n+\n+\n+@keras_core_export(\"keras_core.ops.cond\")\n+def cond(pred, true_fn, false_fn):\n+    \"\"\"Conditionally applies `true_fn` or `false_fn`.\n+\n+    Args:\n+        pred: Boolean scalar type\n+        true_fn: Callable returning the output for the `pred == True` case.\n+        false_fn: Callable returning the output for the `pred == False` case.\n+\n+    Returns:\n+        The output of either `true_fn` or `false_fn` depending on pred.\n+    \"\"\"\n+    return Cond()(pred, true_fn, false_fn)\n\n@@ -263,3 +263,38 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n \n         with self.assertRaises(ValueError):\n             ops.convert_to_numpy(KerasTensor((2,)))\n+\n+    def test_cond(self):\n+        t = ops.cond(True, lambda: 0, lambda: 1)\n+        self.assertEqual(t, 0)\n+        f = ops.cond(False, lambda: 0, lambda: 1)\n+        self.assertEqual(f, 1)\n+\n+        for val in [True, False]:\n+            out = ops.cond(\n+                val,\n+                lambda: KerasTensor((16, 3)),\n+                lambda: KerasTensor((16, 3)),\n+            )\n+            self.assertEqual((16, 3), out.shape)\n+\n+        out = ops.cond(\n+            KerasTensor((), dtype=\"bool\"),\n+            lambda: ops.ones((1, 3)),\n+            lambda: ops.zeros((1, 3)),\n+        )\n+        self.assertEqual((1, 3), out.shape)\n+\n+        out = ops.cond(\n+            KerasTensor((), dtype=\"bool\"),\n+            lambda: KerasTensor((3,)),\n+            lambda: KerasTensor((3,)),\n+        )\n+        self.assertEqual((3,), out.shape)\n+\n+        with self.assertRaises(ValueError):\n+            ops.cond(\n+                KerasTensor((), dtype=\"bool\"),\n+                lambda: KerasTensor((3,)),\n+                lambda: KerasTensor((4,)),\n+            )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#87bc7d5fc9874947bdaf85b148a8ef159e5dceb1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1579 | Contributors (this commit): 28 | Commits (past 90d): 2 | Contributors (cumulative): 28 | DMM Complexity: None\n\nDIFF:\n@@ -58,7 +58,7 @@ def is_interactive_logging_enabled():\n \n     To switch between writing logs to stdout and `absl.logging`, you may use\n     `keras.utils.enable_interactive_logging()` and\n-    `keras.utils.disable_interactie_logging()`.\n+    `keras.utils.disable_interactive_logging()`.\n \n     Returns:\n       Boolean (True if interactive logging is enabled and False otherwise).\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9e1a5adb7dd9ece637feb9569206e35b576d1ba1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 307 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 307 | Churn Cumulative: 307 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,307 @@\n+\"\"\"\n+Title: Using pre-trained word embeddings\n+Author: [fchollet](https://twitter.com/fchollet)\n+Date created: 2020/05/05\n+Last modified: 2020/05/05\n+Description: Text classification on the Newsgroup20 dataset using pre-trained GloVe word embeddings.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import numpy as np\n+import tensorflow.data as tf_data\n+import keras_core as keras\n+\n+\"\"\"\n+## Introduction\n+\n+In this example, we show how to train a text classification model that uses pre-trained\n+word embeddings.\n+\n+We'll work with the Newsgroup20 dataset, a set of 20,000 message board messages\n+belonging to 20 different topic categories.\n+\n+For the pre-trained word embeddings, we'll use\n+[GloVe embeddings](http://nlp.stanford.edu/projects/glove/).\n+\"\"\"\n+\n+\"\"\"\n+## Download the Newsgroup20 data\n+\"\"\"\n+\n+data_path = keras.utils.get_file(\n+    \"news20.tar.gz\",\n+    \"http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.tar.gz\",\n+    untar=True,\n+)\n+\n+\"\"\"\n+## Let's take a look at the data\n+\"\"\"\n+\n+import os\n+import pathlib\n+\n+data_dir = pathlib.Path(data_path).parent / \"20_newsgroup\"\n+dirnames = os.listdir(data_dir)\n+print(\"Number of directories:\", len(dirnames))\n+print(\"Directory names:\", dirnames)\n+\n+fnames = os.listdir(data_dir / \"comp.graphics\")\n+print(\"Number of files in comp.graphics:\", len(fnames))\n+print(\"Some example filenames:\", fnames[:5])\n+\n+\"\"\"\n+Here's a example of what one file contains:\n+\"\"\"\n+\n+print(open(data_dir / \"comp.graphics\" / \"38987\").read())\n+\n+\"\"\"\n+As you can see, there are header lines that are leaking the file's category, either\n+explicitly (the first line is literally the category name), or implicitly, e.g. via the\n+`Organization` filed. Let's get rid of the headers:\n+\"\"\"\n+\n+samples = []\n+labels = []\n+class_names = []\n+class_index = 0\n+for dirname in sorted(os.listdir(data_dir)):\n+    class_names.append(dirname)\n+    dirpath = data_dir / dirname\n+    fnames = os.listdir(dirpath)\n+    print(\"Processing %s, %d files found\" % (dirname, len(fnames)))\n+    for fname in fnames:\n+        fpath = dirpath / fname\n+        f = open(fpath, encoding=\"latin-1\")\n+        content = f.read()\n+        lines = content.split(\"\\n\")\n+        lines = lines[10:]\n+        content = \"\\n\".join(lines)\n+        samples.append(content)\n+        labels.append(class_index)\n+    class_index += 1\n+\n+print(\"Classes:\", class_names)\n+print(\"Number of samples:\", len(samples))\n+\n+\"\"\"\n+There's actually one category that doesn't have the expected number of files, but the\n+difference is small enough that the problem remains a balanced classification problem.\n+\"\"\"\n+\n+\"\"\"\n+## Shuffle and split the data into training & validation sets\n+\"\"\"\n+\n+# Shuffle the data\n+seed = 1337\n+rng = np.random.RandomState(seed)\n+rng.shuffle(samples)\n+rng = np.random.RandomState(seed)\n+rng.shuffle(labels)\n+\n+# Extract a training & validation split\n+validation_split = 0.2\n+num_validation_samples = int(validation_split * len(samples))\n+train_samples = samples[:-num_validation_samples]\n+val_samples = samples[-num_validation_samples:]\n+train_labels = labels[:-num_validation_samples]\n+val_labels = labels[-num_validation_samples:]\n+\n+\"\"\"\n+## Create a vocabulary index\n+\n+Let's use the `TextVectorization` to index the vocabulary found in the dataset.\n+Later, we'll use the same layer instance to vectorize the samples.\n+\n+Our layer will only consider the top 20,000 words, and will truncate or pad sequences to\n+be actually 200 tokens long.\n+\"\"\"\n+\n+from keras_core.layers import TextVectorization\n+\n+vectorizer = TextVectorization(max_tokens=20000, output_sequence_length=200)\n+text_ds = tf_data.Dataset.from_tensor_slices(train_samples).batch(128)\n+vectorizer.adapt(text_ds)\n+\n+\"\"\"\n+You can retrieve the computed vocabulary used via `vectorizer.get_vocabulary()`. Let's\n+print the top 5 words:\n+\"\"\"\n+\n+vectorizer.get_vocabulary()[:5]\n+\n+\"\"\"\n+Let's vectorize a test sentence:\n+\"\"\"\n+\n+output = vectorizer([[\"the cat sat on the mat\"]])\n+output.numpy()[0, :6]\n+\n+\"\"\"\n+As you can see, \"the\" gets represented as \"2\". Why not 0, given that \"the\" was the first\n+word in the vocabulary? That's because index 0 is reserved for padding and index 1 is\n+reserved for \"out of vocabulary\" tokens.\n+\n+Here's a dict mapping words to their indices:\n+\"\"\"\n+\n+voc = vectorizer.get_vocabulary()\n+word_index = dict(zip(voc, range(len(voc))))\n+\n+\"\"\"\n+As you can see, we obtain the same encoding as above for our test sentence:\n+\"\"\"\n+\n+test = [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"]\n+[word_index[w] for w in test]\n+\n+\"\"\"\n+## Load pre-trained word embeddings\n+\"\"\"\n+\n+\"\"\"\n+Let's download pre-trained GloVe embeddings (a 822M zip file).\n+\n+You'll need to run the following commands:\n+\n+```\n+!wget http://nlp.stanford.edu/data/glove.6B.zip\n+!unzip -q glove.6B.zip\n+```\n+\"\"\"\n+\n+\"\"\"\n+The archive contains text-encoded vectors of various sizes: 50-dimensional,\n+100-dimensional, 200-dimensional, 300-dimensional. We'll use the 100D ones.\n+\n+Let's make a dict mapping words (strings) to their NumPy vector representation:\n+\"\"\"\n+\n+path_to_glove_file = os.path.join(\n+    os.path.expanduser(\"~\"), \".keras/datasets/glove.6B.100d.txt\"\n+)\n+\n+embeddings_index = {}\n+with open(path_to_glove_file) as f:\n+    for line in f:\n+        word, coefs = line.split(maxsplit=1)\n+        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n+        embeddings_index[word] = coefs\n+\n+print(\"Found %s word vectors.\" % len(embeddings_index))\n+\n+\"\"\"\n+Now, let's prepare a corresponding embedding matrix that we can use in a Keras\n+`Embedding` layer. It's a simple NumPy matrix where entry at index `i` is the pre-trained\n+vector for the word of index `i` in our `vectorizer`'s vocabulary.\n+\"\"\"\n+\n+num_tokens = len(voc) + 2\n+embedding_dim = 100\n+hits = 0\n+misses = 0\n+\n+# Prepare embedding matrix\n+embedding_matrix = np.zeros((num_tokens, embedding_dim))\n+for word, i in word_index.items():\n+    embedding_vector = embeddings_index.get(word)\n+    if embedding_vector is not None:\n+        # Words not found in embedding index will be all-zeros.\n+        # This includes the representation for \"padding\" and \"OOV\"\n+        embedding_matrix[i] = embedding_vector\n+        hits += 1\n+    else:\n+        misses += 1\n+print(\"Converted %d words (%d misses)\" % (hits, misses))\n+\n+\n+\"\"\"\n+Next, we load the pre-trained word embeddings matrix into an `Embedding` layer.\n+\n+Note that we set `trainable=False` so as to keep the embeddings fixed (we don't want to\n+update them during training).\n+\"\"\"\n+\n+from keras_core.layers import Embedding\n+\n+embedding_layer = Embedding(\n+    num_tokens,\n+    embedding_dim,\n+    trainable=False,\n+)\n+embedding_layer.build((1,))\n+embedding_layer.set_weights([embedding_matrix])\n+\n+\"\"\"\n+## Build the model\n+\n+A simple 1D convnet with global max pooling and a classifier at the end.\n+\"\"\"\n+\n+from keras_core import layers\n+\n+int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n+embedded_sequences = embedding_layer(int_sequences_input)\n+x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n+x = layers.MaxPooling1D(5)(x)\n+x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n+x = layers.MaxPooling1D(5)(x)\n+x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n+x = layers.GlobalMaxPooling1D()(x)\n+x = layers.Dense(128, activation=\"relu\")(x)\n+x = layers.Dropout(0.5)(x)\n+preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n+model = keras.Model(int_sequences_input, preds)\n+model.summary()\n+\n+\"\"\"\n+## Train the model\n+\n+First, convert our list-of-strings data to NumPy arrays of integer indices. The arrays\n+are right-padded.\n+\"\"\"\n+\n+x_train = vectorizer(np.array([[s] for s in train_samples])).numpy()\n+x_val = vectorizer(np.array([[s] for s in val_samples])).numpy()\n+\n+y_train = np.array(train_labels)\n+y_val = np.array(val_labels)\n+\n+\"\"\"\n+We use categorical crossentropy as our loss since we're doing softmax classification.\n+Moreover, we use `sparse_categorical_crossentropy` since our labels are integers.\n+\"\"\"\n+\n+model.compile(\n+    loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n+)\n+model.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_val, y_val))\n+\n+\"\"\"\n+## Export an end-to-end model\n+\n+Now, we may want to export a `Model` object that takes as input a string of arbitrary\n+length, rather than a sequence of indices. It would make the model much more portable,\n+since you wouldn't have to worry about the input preprocessing pipeline.\n+\n+Our `vectorizer` is actually a Keras layer, so it's simple:\n+\"\"\"\n+\n+string_input = keras.Input(shape=(1,), dtype=\"string\")\n+x = vectorizer(string_input)\n+preds = model(x)\n+end_to_end_model = keras.Model(string_input, preds)\n+\n+probabilities = end_to_end_model(\n+    keras.ops.convert_to_tensor(\n+        [[\"this message is about computer graphics and 3D modeling\"]]\n+    )\n+)\n+\n+print(class_names[np.argmax(probabilities[0])])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8d5e9b2163ec9b7d9f70920d1c7992b6df6820ec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 196/196 | Churn Δ: 20 | Churn Cumulative: 17294 | Contributors (this commit): 37 | Commits (past 90d): 6 | Contributors (cumulative): 49 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2209,7 +2209,7 @@ def categorical_crossentropy(\n         )\n \n     def _smooth_labels():\n-        num_classes = tf.cast(tf.shape(y_true)[-1], y_pred.dtype)\n+        num_classes = tf.cast(tf.shape(y_true)[axis], y_pred.dtype)\n         return y_true * (1.0 - label_smoothing) + (\n             label_smoothing / num_classes\n         )\n\n@@ -144,6 +144,23 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n         result = f([t_val, p_val])\n         self.assertArrayNear(result, [0.002, 0, 0.17], 1e-3)\n \n+    def test_categorial_crossentropy_loss_different_axis(self):\n+        target = backend.variable(np.random.randint(0, 1, (5, 2, 3)))\n+        logits = backend.variable(np.random.random((5, 2, 3)))\n+        softmax_output = backend.softmax(logits)\n+        axis = 1\n+        output_from_logit_axis = losses.categorical_crossentropy(\n+            target, logits, from_logits=True, axis=axis\n+        )\n+        output_from_softmax_axis = losses.categorical_crossentropy(\n+            target, softmax_output, axis=axis\n+        )\n+        np.testing.assert_allclose(\n+            backend.eval(output_from_logit_axis),\n+            backend.eval(output_from_softmax_axis),\n+            atol=1e-5,\n+        )\n+\n     @test_combinations.generate(\n         test_combinations.combine(mode=[\"graph\", \"eager\"])\n     )\n@@ -1813,7 +1830,6 @@ class CategoricalCrossentropyTest(tf.test.TestCase):\n @test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n class CategoricalFocalCrossentropyTest(tf.test.TestCase):\n     def test_config(self):\n-\n         cce_obj = losses.CategoricalFocalCrossentropy(\n             name=\"focal_cce\",\n             reduction=losses_utils.ReductionV2.SUM,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#eb9ba701fd95d3db137ffd105ff749b6806905e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 276 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 6 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 276 | Churn Cumulative: 276 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,276 @@\n+\"\"\"\n+Title: Gradient Centralization for Better Training Performance\n+Author: [Rishit Dagli](https://github.com/Rishit-dagli)\n+Converted to Keras Core by: [Muhammad Anas Raza](https://anasrz.com)\n+Date created: 06/18/21\n+Last modified: 07/25/23\n+Description: Implement Gradient Centralization to improve training performance of DNNs.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+This example implements [Gradient Centralization](https://arxiv.org/abs/2004.01461), a\n+new optimization technique for Deep Neural Networks by Yong et al., and demonstrates it\n+on Laurence Moroney's [Horses or Humans\n+Dataset](https://www.tensorflow.org/datasets/catalog/horses_or_humans). Gradient\n+Centralization can both speedup training process and improve the final generalization\n+performance of DNNs. It operates directly on gradients by centralizing the gradient\n+vectors to have zero mean. Gradient Centralization morever improves the Lipschitzness of\n+the loss function and its gradient so that the training process becomes more efficient\n+and stable.\n+\n+This example requires `tensorflow_datasets` which can\n+be installed with this command:\n+\n+```\n+pip install tensorflow-datasets\n+```\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+from time import time\n+\n+import keras_core as keras\n+from keras_core import layers\n+from keras_core.optimizers import RMSprop\n+from keras_core import ops\n+\n+from tensorflow import data as tf_data\n+import tensorflow_datasets as tfds\n+\n+\n+\"\"\"\n+## Prepare the data\n+\n+For this example, we will be using the [Horses or Humans\n+dataset](https://www.tensorflow.org/datasets/catalog/horses_or_humans).\n+\"\"\"\n+\n+num_classes = 2\n+input_shape = (300, 300, 3)\n+dataset_name = \"horses_or_humans\"\n+batch_size = 128\n+AUTOTUNE = tf_data.AUTOTUNE\n+\n+(train_ds, test_ds), metadata = tfds.load(\n+    name=dataset_name,\n+    split=[tfds.Split.TRAIN, tfds.Split.TEST],\n+    with_info=True,\n+    as_supervised=True,\n+)\n+\n+print(f\"Image shape: {metadata.features['image'].shape}\")\n+print(f\"Training images: {metadata.splits['train'].num_examples}\")\n+print(f\"Test images: {metadata.splits['test'].num_examples}\")\n+\n+\"\"\"\n+## Use Data Augmentation\n+\n+We will rescale the data to `[0, 1]` and perform simple augmentations to our data.\n+\"\"\"\n+\n+rescale = layers.Rescaling(1.0 / 255)\n+\n+data_augmentation = [\n+    layers.RandomFlip(\"horizontal_and_vertical\"),\n+    layers.RandomRotation(0.3),\n+    layers.RandomZoom(0.2),\n+]\n+\n+\n+# Helper to apply augmentation\n+def apply_aug(x):\n+    for aug in data_augmentation:\n+        x = aug(x)\n+    return x\n+\n+\n+def prepare(ds, shuffle=False, augment=False):\n+    # Rescale dataset\n+    ds = ds.map(lambda x, y: (rescale(x), y), num_parallel_calls=AUTOTUNE)\n+\n+    if shuffle:\n+        ds = ds.shuffle(1024)\n+\n+    # Batch dataset\n+    ds = ds.batch(batch_size)\n+\n+    # Use data augmentation only on the training set\n+    if augment:\n+        ds = ds.map(\n+            lambda x, y: (apply_aug(x), y),\n+            num_parallel_calls=AUTOTUNE,\n+        )\n+\n+    # Use buffered prefecting\n+    return ds.prefetch(buffer_size=AUTOTUNE)\n+\n+\n+\"\"\"\n+Rescale and augment the data\n+\"\"\"\n+\n+train_ds = prepare(train_ds, shuffle=True, augment=True)\n+test_ds = prepare(test_ds)\n+\"\"\"\n+## Define a model\n+\n+In this section we will define a Convolutional neural network.\n+\"\"\"\n+\n+model = keras.Sequential(\n+    [\n+        layers.Conv2D(16, (3, 3), activation=\"relu\", input_shape=(300, 300, 3)),\n+        layers.MaxPooling2D(2, 2),\n+        layers.Conv2D(32, (3, 3), activation=\"relu\"),\n+        layers.Dropout(0.5),\n+        layers.MaxPooling2D(2, 2),\n+        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n+        layers.Dropout(0.5),\n+        layers.MaxPooling2D(2, 2),\n+        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n+        layers.MaxPooling2D(2, 2),\n+        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n+        layers.MaxPooling2D(2, 2),\n+        layers.Flatten(),\n+        layers.Dropout(0.5),\n+        layers.Dense(512, activation=\"relu\"),\n+        layers.Dense(1, activation=\"sigmoid\"),\n+    ]\n+)\n+\n+\"\"\"\n+## Implement Gradient Centralization\n+\n+We will now\n+subclass the `RMSProp` optimizer class modifying the\n+`keras.optimizers.Optimizer.get_gradients()` method where we now implement Gradient\n+Centralization. On a high level the idea is that let us say we obtain our gradients\n+through back propogation for a Dense or Convolution layer we then compute the mean of the\n+column vectors of the weight matrix, and then remove the mean from each column vector.\n+\n+The experiments in [this paper](https://arxiv.org/abs/2004.01461) on various\n+applications, including general image classification, fine-grained image classification,\n+detection and segmentation and Person ReID demonstrate that GC can consistently improve\n+the performance of DNN learning.\n+\n+Also, for simplicity at the moment we are not implementing gradient cliiping functionality,\n+however this quite easy to implement.\n+\n+At the moment we are just creating a subclass for the `RMSProp` optimizer\n+however you could easily reproduce this for any other optimizer or on a custom\n+optimizer in the same way. We will be using this class in the later section when\n+we train a model with Gradient Centralization.\n+\"\"\"\n+\n+\n+class GCRMSprop(RMSprop):\n+    def get_gradients(self, loss, params):\n+        # We here just provide a modified get_gradients() function since we are\n+        # trying to just compute the centralized gradients.\n+\n+        grads = []\n+        gradients = super().get_gradients()\n+        for grad in gradients:\n+            grad_len = len(grad.shape)\n+            if grad_len > 1:\n+                axis = list(range(grad_len - 1))\n+                grad -= ops.mean(grad, axis=axis, keep_dims=True)\n+            grads.append(grad)\n+\n+        return grads\n+\n+\n+optimizer = GCRMSprop(learning_rate=1e-4)\n+\n+\"\"\"\n+## Training utilities\n+\n+We will also create a callback which allows us to easily measure the total training time\n+and the time taken for each epoch since we are interested in comparing the effect of\n+Gradient Centralization on the model we built above.\n+\"\"\"\n+\n+\n+class TimeHistory(keras.callbacks.Callback):\n+    def on_train_begin(self, logs={}):\n+        self.times = []\n+\n+    def on_epoch_begin(self, batch, logs={}):\n+        self.epoch_time_start = time()\n+\n+    def on_epoch_end(self, batch, logs={}):\n+        self.times.append(time() - self.epoch_time_start)\n+\n+\n+\"\"\"\n+## Train the model without GC\n+\n+We now train the model we built earlier without Gradient Centralization which we can\n+compare to the training performance of the model trained with Gradient Centralization.\n+\"\"\"\n+\n+time_callback_no_gc = TimeHistory()\n+model.compile(\n+    loss=\"binary_crossentropy\",\n+    optimizer=RMSprop(learning_rate=1e-4),\n+    metrics=[\"accuracy\"],\n+)\n+\n+model.summary()\n+\n+\"\"\"\n+We also save the history since we later want to compare our model trained with and not\n+trained with Gradient Centralization\n+\"\"\"\n+\n+history_no_gc = model.fit(\n+    train_ds, epochs=10, verbose=1, callbacks=[time_callback_no_gc]\n+)\n+\n+\"\"\"\n+## Train the model with GC\n+\n+We will now train the same model, this time using Gradient Centralization,\n+notice our optimizer is the one using Gradient Centralization this time.\n+\"\"\"\n+\n+time_callback_gc = TimeHistory()\n+model.compile(\n+    loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"]\n+)\n+\n+model.summary()\n+\n+history_gc = model.fit(\n+    train_ds, epochs=10, verbose=1, callbacks=[time_callback_gc]\n+)\n+\n+\"\"\"\n+## Comparing performance\n+\"\"\"\n+\n+print(\"Not using Gradient Centralization\")\n+print(f\"Loss: {history_no_gc.history['loss'][-1]}\")\n+print(f\"Accuracy: {history_no_gc.history['accuracy'][-1]}\")\n+print(f\"Training Time: {sum(time_callback_no_gc.times)}\")\n+\n+print(\"Using Gradient Centralization\")\n+print(f\"Loss: {history_gc.history['loss'][-1]}\")\n+print(f\"Accuracy: {history_gc.history['accuracy'][-1]}\")\n+print(f\"Training Time: {sum(time_callback_gc.times)}\")\n+\n+\"\"\"\n+Readers are encouraged to try out Gradient Centralization on different datasets from\n+different domains and experiment with it's effect. You are strongly advised to check out\n+the [original paper](https://arxiv.org/abs/2004.01461) as well - the authors present\n+several studies on Gradient Centralization showing how it can improve general\n+performance, generalization, training time as well as more efficient.\n+\n+Many thanks to [Ali Mustufa Shaikh](https://github.com/ialimustufa) for reviewing this\n+implementation.\n+\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f73d98df609137d92fe1a78538540738f6a6e285", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 342 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 13 | Complexity Δ (Sum/Max): 22/22 | Churn Δ: 342 | Churn Cumulative: 342 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,342 @@\n+\"\"\"\n+Title: Named Entity Recognition using Transformers\n+Author: [Varun Singh](https://www.linkedin.com/in/varunsingh2/)\n+Date created: 2021/6/23\n+Last modified: 2023/7/25\n+Description: NER using the Transformers and data from CoNLL 2003 shared task.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\n+Named Entity Recognition (NER) is the process of identifying named entities in text.\n+Example of named entities are: \"Person\", \"Location\", \"Organization\", \"Dates\" etc. NER is\n+essentially a token classification task where every token is classified into one or more\n+predetermined categories.\n+\n+In this exercise, we will train a simple Transformer based model to perform NER. We will\n+be using the data from CoNLL 2003 shared task. For more information about the dataset,\n+please visit [the dataset website](https://www.clips.uantwerpen.be/conll2003/ner/).\n+However, since obtaining this data requires an additional step of getting a free license, we will be using\n+HuggingFace's datasets library which contains a processed version of this dataset.\n+\"\"\"\n+\n+\"\"\"\n+## Install the open source datasets library from HuggingFace\n+\n+We also download the script used to evaluate NER models.\n+\"\"\"\n+\n+\"\"\"shell\n+pip3 install datasets\n+wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n+\"\"\"\n+\n+import os\n+import numpy as np\n+import keras_core as keras\n+from keras_core import layers\n+from datasets import load_dataset\n+from collections import Counter\n+from conlleval import evaluate\n+\n+# imports for data preprocessing \n+from tensorflow import data as tf_data\n+from tensorflow import strings as tf_strings\n+\n+\n+class TransformerBlock(layers.Layer):\n+    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n+        super().__init__()\n+        self.att = keras.layers.MultiHeadAttention(\n+            num_heads=num_heads, key_dim=embed_dim\n+        )\n+        self.ffn = keras.Sequential(\n+            [\n+                keras.layers.Dense(ff_dim, activation=\"relu\"),\n+                keras.layers.Dense(embed_dim),\n+            ]\n+        )\n+        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n+        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n+        self.dropout1 = keras.layers.Dropout(rate)\n+        self.dropout2 = keras.layers.Dropout(rate)\n+\n+    def call(self, inputs, training=False):\n+        attn_output = self.att(inputs, inputs)\n+        attn_output = self.dropout1(attn_output, training=training)\n+        out1 = self.layernorm1(inputs + attn_output)\n+        ffn_output = self.ffn(out1)\n+        ffn_output = self.dropout2(ffn_output, training=training)\n+        return self.layernorm2(out1 + ffn_output)\n+\n+\n+\"\"\"\n+Next, let's define a `TokenAndPositionEmbedding` layer:\n+\"\"\"\n+\n+\n+class TokenAndPositionEmbedding(layers.Layer):\n+    def __init__(self, maxlen, vocab_size, embed_dim):\n+        super().__init__()\n+        self.token_emb = keras.layers.Embedding(\n+            input_dim=vocab_size, output_dim=embed_dim\n+        )\n+        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n+\n+    def call(self, inputs):\n+        maxlen = keras.ops.backend.shape(inputs)[-1]\n+        positions = keras.ops.arange(start=0, stop=maxlen, step=1)\n+        position_embeddings = self.pos_emb(positions)\n+        token_embeddings = self.token_emb(inputs)\n+        return token_embeddings + position_embeddings\n+\n+\n+\"\"\"\n+## Build the NER model class as a `keras.Model` subclass\n+\"\"\"\n+\n+\n+class NERModel(keras.Model):\n+    def __init__(\n+        self, num_tags, vocab_size, maxlen=128, embed_dim=32, num_heads=2, ff_dim=32\n+    ):\n+        super().__init__()\n+        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n+        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n+        self.dropout1 = layers.Dropout(0.1)\n+        self.ff = layers.Dense(ff_dim, activation=\"relu\")\n+        self.dropout2 = layers.Dropout(0.1)\n+        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n+\n+    def call(self, inputs, training=False):\n+        x = self.embedding_layer(inputs)\n+        x = self.transformer_block(x)\n+        x = self.dropout1(x, training=training)\n+        x = self.ff(x)\n+        x = self.dropout2(x, training=training)\n+        x = self.ff_final(x)\n+        return x\n+\n+\n+\"\"\"\n+## Load the CoNLL 2003 dataset from the datasets library and process it\n+\"\"\"\n+\n+conll_data = load_dataset(\"conll2003\")\n+\n+\"\"\"\n+We will export this data to a tab-separated file format which will be easy to read as a\n+`tf.data.Dataset` object.\n+\"\"\"\n+\n+\n+def export_to_file(export_file_path, data):\n+    with open(export_file_path, \"w\") as f:\n+        for record in data:\n+            ner_tags = record[\"ner_tags\"]\n+            tokens = record[\"tokens\"]\n+            if len(tokens) > 0:\n+                f.write(\n+                    str(len(tokens))\n+                    + \"\\t\"\n+                    + \"\\t\".join(tokens)\n+                    + \"\\t\"\n+                    + \"\\t\".join(map(str, ner_tags))\n+                    + \"\\n\"\n+                )\n+\n+\n+os.mkdir(\"data\")\n+export_to_file(\"./data/conll_train.txt\", conll_data[\"train\"])\n+export_to_file(\"./data/conll_val.txt\", conll_data[\"validation\"])\n+\n+\"\"\"\n+## Make the NER label lookup table\n+\n+NER labels are usually provided in IOB, IOB2 or IOBES formats. Checkout this link for\n+more information:\n+[Wikipedia](https://en.wikipedia.org/wiki/Inside%E2%80%93outside%E2%80%93beginning_(tagging))\n+\n+Note that we start our label numbering from 1 since 0 will be reserved for padding. We\n+have a total of 10 labels: 9 from the NER dataset and one for padding.\n+\"\"\"\n+\n+\n+def make_tag_lookup_table():\n+    iob_labels = [\"B\", \"I\"]\n+    ner_labels = [\"PER\", \"ORG\", \"LOC\", \"MISC\"]\n+    all_labels = [(label1, label2) for label2 in ner_labels for label1 in iob_labels]\n+    all_labels = [\"-\".join([a, b]) for a, b in all_labels]\n+    all_labels = [\"[PAD]\", \"O\"] + all_labels\n+    return dict(zip(range(0, len(all_labels) + 1), all_labels))\n+\n+\n+mapping = make_tag_lookup_table()\n+print(mapping)\n+\n+\"\"\"\n+Get a list of all tokens in the training dataset. This will be used to create the\n+vocabulary.\n+\"\"\"\n+\n+all_tokens = sum(conll_data[\"train\"][\"tokens\"], [])\n+all_tokens_array = np.array(list(map(str.lower, all_tokens)))\n+\n+counter = Counter(all_tokens_array)\n+print(len(counter))\n+\n+num_tags = len(mapping)\n+vocab_size = 20000\n+\n+# We only take (vocab_size - 2) most commons words from the training data since\n+# the `StringLookup` class uses 2 additional tokens - one denoting an unknown\n+# token and another one denoting a masking token\n+vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]\n+\n+# The StringLook class will convert tokens to token IDs\n+lookup_layer = keras.layers.StringLookup(vocabulary=vocabulary)\n+\n+\"\"\"\n+Create 2 new `Dataset` objects from the training and validation data\n+\"\"\"\n+\n+train_data = tf_data.TextLineDataset(\"./data/conll_train.txt\")\n+val_data = tf_data.TextLineDataset(\"./data/conll_val.txt\")\n+\n+\"\"\"\n+Print out one line to make sure it looks good. The first record in the line is the number of tokens. \n+After that we will have all the tokens followed by all the ner tags.\n+\"\"\"\n+\n+print(list(train_data.take(1).as_numpy_iterator()))\n+\n+\"\"\"\n+We will be using the following map function to transform the data in the dataset:\n+\"\"\"\n+\n+# Data preprocessing using Tensorflow \n+def map_record_to_training_data(record):\n+    record = tf_strings.split(record, sep=\"\\t\")\n+    length = tf_strings.to_number(record[0], out_type=\"int32\")\n+    tokens = record[1 : length + 1]\n+    tags = record[length + 1 :]\n+    tags = tf_strings.to_number(tags, out_type=\"int64\")\n+    tags += 1\n+    return tokens, tags\n+\n+\n+def lowercase_and_convert_to_ids(tokens):\n+    tokens = tf_strings.lower(tokens)\n+    return lookup_layer(tokens)\n+\n+\n+# We use `padded_batch` here because each record in the dataset has a\n+# different length.\n+batch_size = 32\n+train_dataset = (\n+    train_data.map(map_record_to_training_data)\n+    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n+    .padded_batch(batch_size)\n+)\n+val_dataset = (\n+    val_data.map(map_record_to_training_data)\n+    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n+    .padded_batch(batch_size)\n+)\n+\n+ner_model = NERModel(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)\n+\n+\"\"\"\n+We will be using a custom loss function that will ignore the loss from padded tokens.\n+\"\"\"\n+\n+\n+class CustomNonPaddingTokenLoss(keras.losses.Loss):\n+    def __init__(self, name=\"custom_ner_loss\"):\n+        super().__init__(name=name)\n+\n+    def call(self, y_true, y_pred):\n+        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n+            from_logits=True, reduction=keras.losses.Reduction.NONE\n+        )\n+        loss = loss_fn(y_true, y_pred)\n+        mask = keras.backend.cast((y_true > 0), dtype=\"float32\")\n+        loss = loss * mask\n+        return keras.ops.sum(loss) / keras.ops.sum(mask)\n+\n+\n+loss = CustomNonPaddingTokenLoss()\n+\n+\"\"\"\n+## Compile and fit the model\n+\"\"\"\n+\n+ner_model.compile(optimizer=\"adam\", loss=loss)\n+ner_model.fit(train_dataset, epochs=10)\n+\n+\n+def tokenize_and_convert_to_ids(text):\n+    tokens = text.split()\n+    return lowercase_and_convert_to_ids(tokens)\n+\n+\n+# Sample inference using the trained model\n+sample_input = tokenize_and_convert_to_ids(\n+    \"eu rejects german call to boycott british lamb\"\n+)\n+sample_input = keras.ops.reshape(sample_input, new_shape=[1, -1])\n+print(sample_input)\n+\n+output = ner_model.predict(sample_input)\n+prediction = np.argmax(output, axis=-1)[0]\n+prediction = [mapping[i] for i in prediction]\n+\n+# eu -> B-ORG, german -> B-MISC, british -> B-MISC\n+print(prediction)\n+\n+\"\"\"\n+## Metrics calculation\n+\n+Here is a function to calculate the metrics. The function calculates F1 score for the\n+overall NER dataset as well as individual scores for each NER tag.\n+\"\"\"\n+\n+\n+def calculate_metrics(dataset):\n+    all_true_tag_ids, all_predicted_tag_ids = [], []\n+\n+    for x, y in dataset:\n+        output = ner_model.predict(x)\n+        predictions = np.argmax(output, axis=-1)\n+        predictions = np.reshape(predictions, [-1])\n+\n+        true_tag_ids = np.reshape(y, [-1])\n+\n+        mask = (true_tag_ids > 0) & (predictions > 0)\n+        true_tag_ids = true_tag_ids[mask]\n+        predicted_tag_ids = predictions[mask]\n+\n+        all_true_tag_ids.append(true_tag_ids)\n+        all_predicted_tag_ids.append(predicted_tag_ids)\n+\n+    all_true_tag_ids = np.concatenate(all_true_tag_ids)\n+    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)\n+\n+    predicted_tags = [mapping[tag] for tag in all_predicted_tag_ids]\n+    real_tags = [mapping[tag] for tag in all_true_tag_ids]\n+\n+    evaluate(real_tags, predicted_tags)\n+\n+\n+calculate_metrics(val_dataset)\n+\n+\"\"\"\n+## Conclusions\n+\n+In this exercise, we created a simple transformer based named entity recognition model.\n+We trained it on the CoNLL 2003 shared task data and got an overall F1 score of around 70%.\n+State of the art NER models fine-tuned on pretrained models such as BERT or ELECTRA can easily\n+get much higher F1 score -between 90-95% on this dataset owing to the inherent knowledge\n+of words as part of the pretraining process and the usage of subword tokenization.\n+\"\"\"\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#544282f889b307194164ed3f49d9572fe63c4bb5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 219 | Contributors (this commit): 6 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -65,7 +65,6 @@ class Zeros(Initializer):\n                 are supported. If not specified, `keras_core.backend.floatx()`\n                 is used, which default to `float32` unless you configured it\n                 otherwise (via `keras_core.backend.set_floatx(float_dtype)`).\n-            **kwargs: Additional keyword arguments.\n         \"\"\"\n         dtype = standardize_dtype(dtype)\n         return ops.zeros(shape, dtype=dtype)\n@@ -99,7 +98,6 @@ class Ones(Initializer):\n                 are supported. If not specified, `keras_core.backend.floatx()`\n                 is used, which default to `float32` unless you configured it\n                 otherwise (via `keras_core.backend.set_floatx(float_dtype)`).\n-            **kwargs: Additional keyword arguments.\n         \"\"\"\n         dtype = standardize_dtype(dtype)\n         return ops.ones(shape, dtype=dtype)\n@@ -143,7 +141,6 @@ class Identity(Initializer):\n                 are supported. If not specified, `keras_core.backend.floatx()`\n                 is used, which default to `float32` unless you configured it\n                 otherwise (via `keras_core.backend.set_floatx(float_dtype)`).\n-            **kwargs: Additional keyword arguments.\n         \"\"\"\n         if len(shape) != 2:\n             raise ValueError(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ff4bae8a70d1f263bacbc2f7b1ae737b8a0c4b77", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 819 | Contributors (this commit): 9 | Commits (past 90d): 52 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -228,7 +228,7 @@ def compute_output_spec(fn, *args, **kwargs):\n                 )\n                 return fn(*eager_args, **eager_kwargs)\n \n-    with StatelessScope():\n+    with StatelessScope(), torch.no_grad():\n         outputs = symbolic_call(fn, args, kwargs, fill_value=83)\n \n         none_in_shape = any(map(has_none_shape, tree.flatten((args, kwargs))))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ad4b71fba3ae8980ace7d3646751df45ea2126f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 2587 | Contributors (this commit): 17 | Commits (past 90d): 80 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -250,8 +250,9 @@ class Layer(BackendLayer, Operation):\n         self._losses = []\n         self._loss_ids = set()\n \n+        self._call_signature = inspect.signature(self.call)\n         call_signature_parameters = [\n-            p.name for p in inspect.signature(self.call).parameters.values()\n+            p.name for p in self._call_signature.parameters.values()\n         ]\n         self._call_has_training_arg = \"training\" in call_signature_parameters\n         self._call_has_mask_arg = \"mask\" in call_signature_parameters\n@@ -664,7 +665,7 @@ class Layer(BackendLayer, Operation):\n                     )\n \n         # Caches info about `call()` signature, args, kwargs.\n-        call_spec = CallSpec(self.call, args, kwargs)\n+        call_spec = CallSpec(self._call_signature, args, kwargs)\n \n         ############################################\n         # 3. Check input spec for 1st positional arg.\n@@ -875,7 +876,7 @@ class Layer(BackendLayer, Operation):\n             return super().compute_output_spec(*args, **kwargs)\n         else:\n             # Use compute_output_shape() to return the right output spec\n-            call_spec = CallSpec(self.call, args, kwargs)\n+            call_spec = CallSpec(self._call_signature, args, kwargs)\n             shapes_dict = get_shapes_dict(call_spec)\n             shapes_dict = update_shapes_dict_for_target_fn(\n                 self.compute_output_shape,\n@@ -1271,19 +1272,17 @@ def is_backend_tensor_or_symbolic(x):\n \n \n class CallSpec:\n-    def __init__(self, call_fn, args, kwargs):\n-        sig = inspect.signature(call_fn)\n-\n+    def __init__(self, signature, args, kwargs):\n         # `training` and `mask` are special kwargs that are always available in\n         # a layer, if user specifies them in their call without adding to spec,\n         # we remove them to be able to bind variables. User is not using\n         # `training` anyway so we can ignore.\n         # TODO: If necessary use workaround for `mask`\n-        if \"training\" in kwargs and \"training\" not in sig.parameters:\n+        if \"training\" in kwargs and \"training\" not in signature.parameters:\n             kwargs.pop(\"training\")\n-            bound_args = sig.bind(*args, **kwargs)\n+            bound_args = signature.bind(*args, **kwargs)\n         else:\n-            bound_args = sig.bind(*args, **kwargs)\n+            bound_args = signature.bind(*args, **kwargs)\n         self.user_arguments_dict = {\n             k: v for k, v in bound_args.arguments.items()\n         }\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
