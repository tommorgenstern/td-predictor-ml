{"custom_id": "keras#9b17b682922a762531a7673238b3485323c07a48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0ca14146ab678b2e26206d0bee920c7d5c43f3e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 47 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -30,7 +30,7 @@ def get_file(fname, origin, untar=False):\n         progbar = None\n \n     if untar:\n-        if os.path.exists(fpath):\n+        if os.path.exists(untar_fpath):\n             pass\n         else:\n             print 'Unraring file...'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e2359453d355c988e2301fe9bd6891c1efb5c0d1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 163 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: 0.0\n\nDIFF:\n@@ -30,10 +30,8 @@ def get_file(fname, origin, untar=False):\n         progbar = None\n \n     if untar:\n-        if os.path.exists(untar_fpath):\n-            pass\n-        else:\n-            print 'Unraring file...'\n+        if not os.path.exists(untar_fpath):\n+            print 'Untaring file...'\n             tfile = tarfile.open(fpath, 'r:gz')\n             tfile.extractall(path=datadir)\n             tfile.close()\n\n@@ -33,7 +33,7 @@ class Sequential(object):\n         self.y_train = self.layers[-1].output(train=True)\n         self.y_test = self.layers[-1].output(train=False)\n \n-        Y = T.matrix() # ouput of model\n+        Y = T.matrix() # output of model\n         self.Y = Y\n \n         train_loss = self.loss(self.Y, self.y_train)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b365fe450d9fb86e8c6b8799c1b706312d7b9900", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 18 | Churn Cumulative: 170 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -59,7 +59,7 @@ class Tokenizer(object):\n         self.word_index = dict(zip(sorted_voc, range(len(sorted_voc))))\n \n         self.index_docs = {}\n-        for w, c in self.word_docs:\n+        for w, c in self.word_docs.items():\n             self.index_docs[self.word_index[w]] = c\n \n \n@@ -102,9 +102,9 @@ class Tokenizer(object):\n \n     def texts_to_matrix(self, texts, mode=\"binary\"):\n         '''\n-            modes: binary, count, tfidf\n+            modes: binary, count, tfidf, freq\n         '''\n-        sequences = self.to_sequences(texts)\n+        sequences = self.texts_to_sequences(texts)\n         return self.sequences_to_matrix(sequences, mode=mode)\n \n     def sequences_to_matrix(self, sequences, mode=\"binary\"):\n@@ -112,17 +112,23 @@ class Tokenizer(object):\n             modes: binary, count, tfidf, freq\n         '''\n         if not self.nb_words:\n-            raise Exception(\"Specify a dimension (nb_words argument\")\n+            if self.word_index:\n+                nb_words = len(self.word_index)\n+            else:\n+                raise Exception(\"Specify a dimension (nb_words argument), or fit on some text data first\")\n+        else:\n+            nb_words = self.nb_words\n+\n         if mode == \"tfidf\" and not self.document_count:\n             raise Exception(\"Fit the Tokenizer on some data before using tfidf mode\")\n \n-        X = np.zeros((len(sequences), self.nb_words))\n+        X = np.zeros((len(sequences), nb_words))\n         for i, seq in enumerate(sequences):\n             if not seq:\n                 pass\n             counts = {}\n             for j in seq:\n-                if j >= self.nb_words:\n+                if j >= nb_words:\n                     pass\n                 if j not in counts:\n                     counts[j] = 1.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c589c524d73acb4f2b46a7dabd0e4529d4696048", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 4 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,6 +5,8 @@ from ..utils.generic_utils import Progbar\n def get_file(fname, origin, untar=False):\n     datadir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n     datadir = os.path.join(datadir, 'data')\n+    if not os.path.exists(datadir):\n+        os.makedirs(datadir)\n \n     if untar:\n         untar_fpath = os.path.join(datadir, fname)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#28f354657d811fb193b2079e111318e35a8fb81e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 4 | Churn Cumulative: 4 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,6 +2,8 @@ import theano\n import theano.tensor as T\n import numpy as np\n \n+epsilon = 1.0e-15\n+\n def mean_squared_error(y_true, y_pred):\n     return T.sqr(y_pred - y_true).mean()\n \n@@ -17,9 +19,11 @@ def hinge(y_true, y_pred):\n def categorical_crossentropy(y_true, y_pred):\n     '''Expects a binary class matrix instead of a vector of scalar classes\n     '''\n+    y_pred = T.clip(y_pred, epsilon, 1.0 - epsilon)\n     return T.nnet.categorical_crossentropy(y_pred, y_true).mean()\n \n def binary_crossentropy(y_true, y_pred):\n+    y_pred = T.clip(y_pred, epsilon, 1.0 - epsilon)\n     return T.nnet.binary_crossentropy(y_pred, y_true).mean()\n \n # aliases\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2219704aaa0662dd9a276d6b21e0352999be54df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 10 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -116,7 +116,7 @@ class RepeatVector(Layer):\n \n class Dense(Layer):\n     '''\n-        Just your regular fully connecter NN layer.\n+        Just your regular fully connected NN layer.\n     '''\n     def __init__(self, input_dim, output_dim, init='uniform', activation='linear', weights=None):\n         self.init = initializations.get(init)\n@@ -141,7 +141,7 @@ class Dense(Layer):\n \n class Embedding(Layer):\n     '''\n-        Turns a list of integers into a dense vector of fixed size. \n+        Turn a list of integers >=0 into a dense vector of fixed size. \n         eg. [4, 50, 123, 26] -> [0.25, 0.1]\n \n         @input_dim: size of vocabulary (highest input integer + 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#23050e0a423f3cd664b40aa0f524852d09b85b90", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 24 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,6 @@\n #!/usr/bin/env python\n \n-from distutils.core import setup\n+from setuptools import setup\n \n setup(name='Keras',\n       version='0.0.1',\n@@ -16,5 +16,5 @@ setup(name='Keras',\n         'keras.datasets', \n         'keras.utils',\n       ],\n-      # TODO: dependencies\n+      install_requires=['numpy', 'scipy', 'theano']\n )\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d84c8dbcec7024ffa82b87a2326fc80d705a1d0b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 27 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,6 +1,9 @@\n #!/usr/bin/env python\n \n+try:\n     from setuptools import setup\n+except ImportError:\n+    from distutils.core import setup\n \n setup(name='Keras',\n       version='0.0.1',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#040863f6e6986a8d4d3ebaff7fb0671db7f7aadf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 33 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -19,5 +19,9 @@ setup(name='Keras',\n           'keras.datasets',\n           'keras.utils',\n       ],\n-      install_requires=['numpy', 'scipy', 'theano']\n+      install_requires=['numpy', 'scipy', 'theano'],\n+      extras_require={\n+          'images': ['pil'], # working with images\n+          'CNNs': ['cudnn-python-wrappers'], # working with CNNs\n+      }\n )\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#abd01d116b3570aa06999d6bc19cf5fcdfbd33bc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 6 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,6 +20,8 @@ def categorical_crossentropy(y_true, y_pred):\n     '''Expects a binary class matrix instead of a vector of scalar classes\n     '''\n     y_pred = T.clip(y_pred, epsilon, 1.0 - epsilon)\n+    # scale preds so that the class probas of each sample sum to 1\n+    y_pred /= y_pred.sum(axis=1, keepdims=True) \n     return T.nnet.categorical_crossentropy(y_pred, y_true).mean()\n \n def binary_crossentropy(y_true, y_pred):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#33d82a418ab7cf0f650481cd8a9347dbc5d96191", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 14 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -25,7 +25,7 @@ class SimpleRNN(Layer):\n         self.truncate_gradient = truncate_gradient\n         self.activation = activations.get(activation)\n         self.return_sequences = return_sequences\n-        self.input = T.matrix()\n+        self.input = T.tensor3()\n \n         self.W = self.init((self.input_dim, self.output_dim))\n         self.U = self.init((self.output_dim, self.output_dim))\n@@ -89,7 +89,7 @@ class SimpleDeepRNN(Layer):\n         self.inner_activation = activations.get(inner_activation)\n         self.depth = depth\n         self.return_sequences = return_sequences\n-        self.input = T.matrix()\n+        self.input = T.tensor3()\n \n         self.W = self.init((self.input_dim, self.output_dim))\n         self.Us = [self.init((self.output_dim, self.output_dim)) for _ in range(self.depth)]\n@@ -163,7 +163,7 @@ class GRU(Layer):\n         self.inner_init = initializations.get(inner_init)\n         self.activation = activations.get(activation)\n         self.inner_activation = activations.get(inner_activation)\n-        self.input = T.matrix()\n+        self.input = T.tensor3()\n \n         self.W_z = self.init((self.input_dim, self.output_dim))\n         self.U_z = self.inner_init((self.output_dim, self.output_dim))\n@@ -255,7 +255,7 @@ class LSTM(Layer):\n         self.inner_init = initializations.get(inner_init)\n         self.activation = activations.get(activation)\n         self.inner_activation = activations.get(inner_activation)\n-        self.input = T.matrix()\n+        self.input = T.tensor3()\n \n         self.W_i = self.init((self.input_dim, self.output_dim))\n         self.U_i = self.inner_init((self.output_dim, self.output_dim))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#33530ce7de699fefadeaebd531603cd36e357ce5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 26/26 | Churn Δ: 7 | Churn Cumulative: 137 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n\n@@ -12,7 +12,8 @@ from keras.utils import np_utils, generic_utils\n     GPU run command:\n         THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python cifar10_cnn.py\n \n-    It gets to 0.65 test logloss in 25 epochs (still way underfitting at that point, though)\n+    It gets down to 0.65 test logloss in 25 epochs, and down to 0.55 after 50 epochs.\n+    (it's still underfitting at that point, though).\n '''\n \n batch_size = 32\n\n@@ -33,8 +33,8 @@ class Sequential(object):\n         self.y_train = self.layers[-1].output(train=True)\n         self.y_test = self.layers[-1].output(train=False)\n \n-        Y = T.matrix() # output of model\n-        self.Y = Y\n+        # output of model\n+        self.Y = T.matrix() # TODO: support for custom output shapes\n \n         train_loss = self.loss(self.Y, self.y_train)\n         test_score = self.loss(self.Y, self.y_test)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#629c34376ddf5e57f8c1113cbf3cacbe406b6ace", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 14 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -57,21 +57,25 @@ class Sequential(object):\n         score = self._test(X, y)\n         return score\n \n-    def fit(self, X, y, batch_size=128, nb_epoch=100, verbose=1):\n+    def fit(self, X, y, batch_size=128, nb_epoch=100, verbose=1, shuffle=True):\n         y = standardize_y(y)\n+        index_array = np.arange(len(X))\n         for epoch in range(nb_epoch):\n             if verbose:\n                 print 'Epoch', epoch\n-            \n+            if shuffle:\n+                np.random.shuffle(index_array)\n             nb_batch = len(X)/batch_size+1\n             progbar = Progbar(target=len(X))\n             for batch_index in range(0, nb_batch):\n                 batch = range(batch_index*batch_size, min(len(X), (batch_index+1)*batch_size))\n                 if not batch:\n                     break\n+                prog = batch[-1]+1\n+                batch = index_array[batch]\n                 loss = self._train(X[batch], y[batch])\n                 if verbose:\n-                    progbar.update(batch[-1]+1, [('loss', loss)])\n+                    progbar.update(prog, [('loss', loss)])\n             \n     def predict_proba(self, X, batch_size=128):\n         for batch_index in range(0, len(X)/batch_size+1):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6abf916fac83cc0615cfd655bb00fde811b79a73", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 33/33 | Churn Δ: 7 | Churn Cumulative: 140 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -55,17 +55,16 @@ model.add(Dense(512, nb_classes, init='normal'))\n model.add(Activation('softmax'))\n \n # let's train the model using SGD + momentum (how original).\n-sgd = SGD(lr=0.01, decay=1e-7, momentum=0.9, nesterov=True)\n+sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n model.compile(loss='categorical_crossentropy', optimizer=sgd)\n \n if not data_augmentation:\n     print \"Not using data augmentation or normalization\"\n \n     X_train = X_train.astype(\"float32\")\n-    X_test = X_train.astype(\"float32\")\n+    X_test = X_test.astype(\"float32\")\n     X_train /= 255\n     X_test /= 255\n-    print X_train[0]\n     model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=10)\n     score = model.evaluate(X_test, Y_test, batch_size=batch_size)\n     print 'Test score:', score\n\n@@ -45,7 +45,7 @@ class SGD(Optimizer):\n \n     def get_updates(self, params, cost):\n         grads = self.get_gradients(cost, params)\n-        lr = self.lr - self.decay * self.iterations\n+        lr = self.lr * (1.0 / (1.0 + self.decay * self.iterations))\n         updates = [(self.iterations, self.iterations+1.)]\n \n         for p, g in zip(params, grads):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c57d5cce7903511edd4048f8bfed2ad0dc6f6b6b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 37 | Churn Cumulative: 51 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -57,25 +57,46 @@ class Sequential(object):\n         score = self._test(X, y)\n         return score\n \n-    def fit(self, X, y, batch_size=128, nb_epoch=100, verbose=1, shuffle=True):\n+    def fit(self, X, y, batch_size=128, nb_epoch=100, verbose=1,\n+            validation_split=0., shuffle=True):\n+        # If a validation split size is given (e.g. validation_split=0.2)\n+        # then split X into smaller X and X_val,\n+        # and split y into smaller y and y_val.\n         y = standardize_y(y)\n+\n+        do_validation = False\n+        if validation_split > 0 and validation_split < 1:\n+            do_validation = True\n+            split_at = int(len(X) * (1 - validation_split))\n+            (X, X_val) = (X[0:split_at], X[split_at:])\n+            (y, y_val) = (y[0:split_at], y[split_at:])\n+            if verbose:\n+                print \"Train on %d samples, validate on %d samples\" % (len(y), len(y_val))\n+        \n         index_array = np.arange(len(X))\n         for epoch in range(nb_epoch):\n             if verbose:\n                 print 'Epoch', epoch\n             if shuffle:\n                 np.random.shuffle(index_array)\n+\n             nb_batch = len(X)/batch_size+1\n             progbar = Progbar(target=len(X))\n             for batch_index in range(0, nb_batch):\n-                batch = range(batch_index*batch_size, min(len(X), (batch_index+1)*batch_size))\n-                if not batch:\n-                    break\n-                prog = batch[-1]+1\n-                batch = index_array[batch]\n-                loss = self._train(X[batch], y[batch])\n+                batch_start = batch_index*batch_size\n+                batch_end = min(len(X), (batch_index+1)*batch_size)\n+                batch_ids = index_array[batch_start:batch_end]\n+\n+                X_batch = X[batch_ids]\n+                y_batch = y[batch_ids]\n+                loss = self._train(X_batch, y_batch)\n+                \n                 if verbose:\n-                    progbar.update(prog, [('loss', loss)])\n+                    is_last_batch = (batch_index == nb_batch - 1)\n+                    if not is_last_batch or not do_validation:\n+                        progbar.update(batch_end, [('loss', loss)])\n+                    else:\n+                        progbar.update(batch_end, [('loss', loss), ('val. loss', self.test(X_val, y_val))])\n             \n     def predict_proba(self, X, batch_size=128):\n         for batch_index in range(0, len(X)/batch_size+1):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bf43f611a04448d15a9174cbd79804cd5605e4f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 20/20 | Churn Δ: 13 | Churn Cumulative: 13 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.8\n\nDIFF:\n@@ -7,7 +7,7 @@ import cPickle\n def make_reuters_dataset(path='datasets/temp/reuters21578/', min_samples_per_topic=15):\n     import os\n     import re\n-    from preprocessing.text import Tokenizer\n+    from ..preprocessing.text import Tokenizer\n \n     wire_topics = []\n     topic_counts = {}\n@@ -61,8 +61,8 @@ def make_reuters_dataset(path='datasets/temp/reuters21578/', min_samples_per_top\n \n     # vectorize wires\n     tokenizer = Tokenizer()\n-    tokenizer.fit(kept_wires)\n-    X = tokenizer.transform(kept_wires)\n+    tokenizer.fit_on_texts(kept_wires)\n+    X = tokenizer.texts_to_sequences(kept_wires)\n \n     print 'Sanity check:'\n     for w in [\"banana\", \"oil\", \"chocolate\", \"the\", \"dsft\"]:\n@@ -72,6 +72,7 @@ def make_reuters_dataset(path='datasets/temp/reuters21578/', min_samples_per_top\n     print '-'\n     print 'Saving...'\n     cPickle.dump(dataset, open('datasets/data/reuters.pkl', 'w'))\n+    cPickle.dump(tokenizer.word_index, open('datasets/data/reuters_word_index.pkl', 'w'))\n \n \n \n@@ -106,6 +107,12 @@ def load_data(path=\"reuters.pkl\", nb_words=100000, maxlen=None, test_split=0.2,\n     return (X_train, y_train), (X_test, y_test)\n \n \n+def get_word_index(path=\"reuters_word_index.pkl\"):\n+    path = get_file(path, origin=\"https://s3.amazonaws.com/text-datasets/reuters_word_index.pkl\")\n+    f = open(path, 'rb')\n+    return cPickle.load(f)\n+\n+\n if __name__ == \"__main__\":\n     make_reuters_dataset()\n     (X_train, y_train), (X_test, y_test) = load_data()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#82cd5612451733554638bdaac05e8415c4285659", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 11 | Churn Cumulative: 62 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -106,16 +106,19 @@ class Sequential(object):\n             batch_preds = self._predict(X[batch])\n \n             if batch_index == 0:\n-                preds = np.zeros((len(X), batch_preds.shape[1]))\n+                shape = (len(X),) + batch_preds.shape[1:]\n+                preds = np.zeros(shape)\n             preds[batch] = batch_preds\n         return preds\n \n     def predict_classes(self, X, batch_size=128):\n         proba = self.predict_proba(X, batch_size=batch_size)\n-        if proba.shape[1] > 1:\n-            return proba.argmax(axis=1)\n+        # The last dimension is the one containing the class probas\n+        classes_dim = proba.ndim - 1\n+        if proba.shape[classes_dim] > 1:\n+            return proba.argmax(axis=classes_dim)\n         else:\n-            return np.array([1 if p > 0.5 else 0 for p in proba])\n+            return (proba>0.5).astype('int32')\n \n     def evaluate(self, X, y, batch_size=128):\n         y = standardize_y(y)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6819d38e2980e55d05d6f8cdae76fdd799fe7834", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 7 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,8 +3,7 @@ import inspect, os\n from ..utils.generic_utils import Progbar\n \n def get_file(fname, origin, untar=False):\n-    datadir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n-    datadir = os.path.join(datadir, 'data')\n+    datadir = os.path.expanduser(\"~/.keras/datasets\")\n     if not os.path.exists(datadir):\n         os.makedirs(datadir)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#465d8d14bc7d0b9e6c109cf2ff41e0aab1e627e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 5 | Churn Cumulative: 5 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,6 +5,11 @@ import types\n def softmax(x):\n     return T.nnet.softmax(x)\n \n+def time_distributed_softmax(x):\n+    xshape = x.shape\n+    X = x.reshape((xshape[0] * xshape[1], xshape[2]))\n+    return T.nnet.softmax(X).reshape(xshape)\n+\n def softplus(x):\n     return T.nnet.softplus(x)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3f3896518e33609862e9b8fab554ecca40e581ce", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 64 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -80,7 +80,7 @@ class Sequential(object):\n             if shuffle:\n                 np.random.shuffle(index_array)\n \n-            nb_batch = len(X)/batch_size+1\n+            nb_batch = np.ceil(len(X)/float(batch_size))\n             progbar = Progbar(target=len(X))\n             for batch_index in range(0, nb_batch):\n                 batch_start = batch_index*batch_size\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cb36a5ea695d16d751b53482d9c934c163c06b83", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 66 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -80,7 +80,7 @@ class Sequential(object):\n             if shuffle:\n                 np.random.shuffle(index_array)\n \n-            nb_batch = np.ceil(len(X)/float(batch_size))\n+            nb_batch = int(np.ceil(len(X)/float(batch_size)))\n             progbar = Progbar(target=len(X))\n             for batch_index in range(0, nb_batch):\n                 batch_start = batch_index*batch_size\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ef7f89ec716809241528482cdbceccd83747fa06", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 5 | Churn Cumulative: 5 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,7 +2,7 @@ import theano\n import theano.tensor as T\n import numpy as np\n \n-from utils.theano_utils import sharedX\n+from utils.theano_utils import sharedX, shared_zeros\n \n def uniform(shape, scale=0.05):\n     return sharedX(np.random.uniform(low=-scale, high=scale, size=shape))\n@@ -30,6 +30,9 @@ def orthogonal(shape, scale=1.1):\n     q = q.reshape(shape)\n     return sharedX(scale * q[:shape[0], :shape[1]])\n \n+def zero(shape):\n+    return shared_zeros(shape)\n+\n \n from utils.generic_utils import get_from_module\n def get(identifier):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f6cd4c8314af339baa9f4c9e75382c98cca054a5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 17/17 | Churn Δ: 2 | Churn Cumulative: 2 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,7 @@ def get_from_module(identifier, module_params, module_name, instantiate=False):\n     if type(identifier) is str:\n         res = module_params.get(identifier)\n         if not res:\n-            raise Exception('Invalid', module_name, ': ' + identifier)\n+            raise Exception('Invalid ' + str(module_name) + ': ' + str(identifier))\n         if instantiate:\n             return res()\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1711d37b1669aecc48d55aa62c6814235e5321f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 17/17 | Churn Δ: 5 | Churn Cumulative: 5 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,6 +13,11 @@ def to_categorical(y, nb_classes=None):\n         Y[i, y[i]] = 1.\n     return Y\n \n+def normalize(a, axis=-1, order=2):\n+    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n+    l2[l2==0] = 1\n+    return a / np.expand_dims(l2, axis)\n+\n \n def binary_logloss(p, y):\n     epsilon = 1e-15\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#746b5d17bc9b032f86253444332c08644ae90fb2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 14/11 | Churn Δ: 16 | Churn Cumulative: 33 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -11,7 +11,7 @@ def load_data(test_split=0.1, seed=113):\n \n     nb_samples = 50000\n     X = np.zeros((nb_samples, 3, 32, 32), dtype=\"uint8\")\n-    y = np.zeros((nb_samples,))\n+    y = np.zeros((nb_samples,), dtype=\"uint8\")\n     for i in range(1, 6):\n         fpath = path + '/data_batch_' + str(i)\n         f = open(fpath, 'rb')\n\n@@ -3,7 +3,7 @@ import gzip\n from data_utils import get_file\n import random\n \n-def load_data(path=\"imdb.pkl\", nb_words=100000, maxlen=None, test_split=0.2, seed=113):\n+def load_data(path=\"imdb.pkl\", nb_words=None, skip_top=0, maxlen=None, test_split=0.2, seed=113):\n     path = get_file(path, origin=\"https://s3.amazonaws.com/text-datasets/imdb.pkl\")\n \n     if path.endswith(\".gz\"):\n@@ -29,7 +29,10 @@ def load_data(path=\"imdb.pkl\", nb_words=100000, maxlen=None, test_split=0.2, see\n         X = new_X\n         labels = new_labels\n \n-    X = [[1 if w >= nb_words else w for w in x] for x in X]\n+    if not nb_words:\n+        nb_words = max([max(x) for x in X])\n+\n+    X = [[0 if (w >= nb_words or w < skip_top) else w for w in x] for x in X]\n     X_train = X[:int(len(X)*(1-test_split))]\n     y_train = labels[:int(len(X)*(1-test_split))]\n \n\n@@ -76,7 +76,7 @@ def make_reuters_dataset(path='datasets/temp/reuters21578/', min_samples_per_top\n \n \n \n-def load_data(path=\"reuters.pkl\", nb_words=100000, maxlen=None, test_split=0.2, seed=113):\n+def load_data(path=\"reuters.pkl\", nb_words=None, skip_top=0, maxlen=None, test_split=0.2, seed=113):\n     path = get_file(path, origin=\"https://s3.amazonaws.com/text-datasets/reuters.pkl\")\n     f = open(path, 'rb')\n \n@@ -97,7 +97,10 @@ def load_data(path=\"reuters.pkl\", nb_words=100000, maxlen=None, test_split=0.2,\n         X = new_X\n         labels = new_labels\n \n-    X = [[1 if w >= nb_words else w for w in x] for x in X]\n+    if not nb_words:\n+        nb_words = max([max(x) for x in X])\n+\n+    X = [[0 if (w >= nb_words or w < skip_top) else w for w in x] for x in X]\n     X_train = X[:int(len(X)*(1-test_split))]\n     y_train = labels[:int(len(X)*(1-test_split))]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7386dff509604d2794df3c3c9af782cbe7b31447", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 42/42 | Churn Δ: 34 | Churn Cumulative: 34 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,6 +9,7 @@ import numpy as np\n \n def base_filter():\n     f = string.punctuation\n+    f = f.replace(\"'\", '')\n     f += '\\t\\n'\n     return f\n \n@@ -17,8 +18,9 @@ def text_to_word_sequence(text, filters=base_filter(), lower=True, split=\" \"):\n     '''\n     if lower:\n         text = text.lower()\n-    text = text.translate(string.maketrans(\"\",\"\"), filters)\n-    return text.split(split)\n+    text = text.translate(string.maketrans(filters, split*len(filters)))\n+    seq = text.split(split)\n+    return filter(None, seq)\n \n \n def one_hot(text, n):\n@@ -38,8 +40,11 @@ class Tokenizer(object):\n     def fit_on_texts(self, texts):\n         '''\n             required before using texts_to_sequences or texts_to_matrix\n+            @param texts: can be a list or a generator (for memory-efficiency)\n         '''\n+        self.document_count = 0\n         for text in texts:\n+            self.document_count += 1\n             seq = text_to_word_sequence(text, self.filters, self.lower)\n             for w in seq:\n                 if w in self.word_counts:\n@@ -51,12 +56,11 @@ class Tokenizer(object):\n                     self.word_docs[w] += 1\n                 else:\n                     self.word_docs[w] = 1\n-        self.document_count = len(texts)\n \n         wcounts = self.word_counts.items()\n         wcounts.sort(key = lambda x: x[1], reverse=True)\n         sorted_voc = [wc[0] for wc in wcounts]\n-        self.word_index = dict(zip(sorted_voc, range(len(sorted_voc))))\n+        self.word_index = dict(zip(sorted_voc, range(1, len(sorted_voc)+1)))\n \n         self.index_docs = {}\n         for w, c in self.word_docs.items():\n@@ -83,10 +87,24 @@ class Tokenizer(object):\n         '''\n             Transform each text in texts in a sequence of integers.\n             Only top \"nb_words\" most frequent words will be taken into account.\n-            Only words know by the tokenizer will be taken into account.\n+            Only words known by the tokenizer will be taken into account.\n+\n+            Returns a list of sequences.\n+        '''\n+        res = []\n+        for vect in self.texts_to_sequences_generator(texts):\n+            res.append(vect)\n+        return res\n+\n+    def texts_to_sequences_generator(self, texts):\n+        '''\n+            Transform each text in texts in a sequence of integers.\n+            Only top \"nb_words\" most frequent words will be taken into account.\n+            Only words known by the tokenizer will be taken into account.\n+\n+            Yields individual sequences.\n         '''\n         nb_words = self.nb_words\n-        res = []\n         for text in texts:\n             seq = text_to_word_sequence(text, self.filters, self.lower)\n             vect = []\n@@ -97,8 +115,8 @@ class Tokenizer(object):\n                         pass\n                     else:\n                         vect.append(i)\n-            res.append(vect)\n-        return res\n+            yield vect\n+\n \n     def texts_to_matrix(self, texts, mode=\"binary\"):\n         '''\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b10a563751ead496eed2b4bb44f33d5afb492e08", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 17/17 | Churn Δ: 57 | Churn Cumulative: 57 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n import numpy as np\n+import random\n \n def pad_sequences(seqs, maxlen=None, dtype='int32'):\n     \"\"\"\n@@ -19,3 +20,59 @@ def pad_sequences(seqs, maxlen=None, dtype='int32'):\n         x[idx, :lengths[idx]] = s[:maxlen]\n \n     return x\n+\n+\n+def skipgrams(sequence, vocabulary_size, \n+    window_size=4, negative_samples=1., shuffle=True, categorical=False, seed=None):\n+    ''' \n+        Take a sequence (list of indexes of words), \n+        returns couples of [word_index, other_word index] and labels (1s or 0s),\n+        where label = 1 if 'other_word' belongs to the context of 'word',\n+        and label=0 if 'other_word' is ramdomly sampled\n+\n+        @param vocabulary_size: int. maximum possible word index + 1\n+        @param window_size: int. actually half-window. The window of a word wi will be [i-window_size, i+window_size+1]\n+        @param negative_samples: float >= 0. 0 for no negative (=random) samples. 1 for same number as positive samples. etc.\n+        @param categorical: bool. if False, labels will be integers (eg. [0, 1, 1 .. ]), \n+            if True labels will be categorical eg. [[1,0],[0,1],[0,1] .. ]\n+\n+        Note: by convention, index 0 in the vocabulary is a non-word and will be skipped.\n+    '''\n+    couples = []\n+    labels = []\n+    for i, wi in enumerate(sequence):\n+        if not wi:\n+            pass\n+        window_start = max(0, i-window_size)\n+        window_end = min(len(sequence), i+window_size+1)\n+        for j in range(window_start, window_end):\n+            if j != i:\n+                wj = sequence[j]\n+                if not wj:\n+                    pass\n+                couples.append([wi, wj])\n+                if categorical:\n+                    labels.append([0,1])\n+                else:\n+                    labels.append(1)\n+\n+    if negative_samples > 0:\n+        nb_negative_samples = int(len(labels) * negative_samples)\n+        words = [c[0] for c in couples]\n+        random.shuffle(words)\n+        \n+        couples += [[words[i%len(words)], random.randint(1, vocabulary_size-1)] for i in range(nb_negative_samples)]\n+        if categorical:\n+            labels += [[1,0]]*nb_negative_samples\n+        else:\n+            labels += [0]*nb_negative_samples\n+\n+    if shuffle:\n+        if not seed:\n+            seed = random.randint(0,10e6)\n+        random.seed(seed)\n+        random.shuffle(couples)\n+        random.seed(seed)\n+        random.shuffle(labels)\n+\n+    return couples, labels\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f35d529f42198f30c81011e5d0c60a37fe5ab7ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 16 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -86,13 +86,13 @@ class Flatten(Layer):\n         Reshape input to flat shape.\n         First dimension is assumed to be nb_samples.\n     '''\n-    def __init__(self, size):\n-        self.size = size\n+    def __init__(self):\n         self.params = []\n \n     def output(self, train):\n         X = self.get_input(train)\n-        nshape = (X.shape[0], self.size)\n+        size = theano.tensor.prod(X.shape) / X.shape[0]\n+        nshape = (X.shape[0], size)\n         return theano.tensor.reshape(X, nshape)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8b2849bd314fc8f8f42f65fda95916b5f1cddafc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 32 | Churn Cumulative: 89 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 0.7\n\nDIFF:\n@@ -22,8 +22,30 @@ def pad_sequences(seqs, maxlen=None, dtype='int32'):\n     return x\n \n \n+def make_sampling_table(size, sampling_factor=1e-5):\n+    '''\n+        This generates an array where the ith element\n+        is the probability that a word of rank i would be sampled,\n+        according to the sampling distribution used in word2vec.\n+        \n+        The word2vec formula is:\n+            p(word) = min(1, sqrt(word.frequency/sampling_factor) / (word.frequency/sampling_factor))\n+\n+        We assume that the word frequencies follow Zipf's law (s=1) to derive \n+        a numerical approximation of frequency(rank):\n+           frequency(rank) ~ 1/(rank * (log(rank) + gamma) + 1/2 - 1/(12*rank))\n+        where gamma is the Euler–Mascheroni constant.\n+    '''\n+    gamma = 0.577\n+    rank = np.array(range(size))\n+    rank[0] = 1\n+    inv_fq = rank * (np.log(rank) + gamma) + 0.5 - 1./(12.*rank)\n+    f = sampling_factor * inv_fq\n+    return np.minimum(1., f / np.sqrt(f))\n+\n+\n def skipgrams(sequence, vocabulary_size, \n-    window_size=4, negative_samples=1., shuffle=True, categorical=False, seed=None):\n+    window_size=4, negative_samples=1., shuffle=True, categorical=False, seed=None, sampling_table=None):\n     ''' \n         Take a sequence (list of indexes of words), \n         returns couples of [word_index, other_word index] and labels (1s or 0s),\n@@ -42,14 +64,18 @@ def skipgrams(sequence, vocabulary_size,\n     labels = []\n     for i, wi in enumerate(sequence):\n         if not wi:\n-            pass\n+            continue\n+        if sampling_table is not None:\n+            if sampling_table[i] < random.random():\n+                continue\n+\n         window_start = max(0, i-window_size)\n         window_end = min(len(sequence), i+window_size+1)\n         for j in range(window_start, window_end):\n             if j != i:\n                 wj = sequence[j]\n                 if not wj:\n-                    pass\n+                    continue\n                 couples.append([wi, wj])\n                 if categorical:\n                     labels.append([0,1])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6cd076a14fa163cae38ff7768daf810d851c4885", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 20 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -103,7 +103,7 @@ class SimpleDeepRNN(Layer):\n         o = args[0]\n         for i in range(1, self.depth+1):\n             o += self.inner_activation(T.dot(args[i], args[i+self.depth]))\n-        return o        \n+        return self.activation(o)\n \n     def output(self, train):\n         X = self.get_input(train)\n@@ -152,7 +152,7 @@ class GRU(Layer):\n     def __init__(self, input_dim, output_dim=128, \n         init='uniform', inner_init='orthogonal',\n         activation='sigmoid', inner_activation='hard_sigmoid',\n-        truncate_gradient=-1, weights=None, return_sequences=False):\n+        weights=None, truncate_gradient=-1, return_sequences=False):\n \n         self.input_dim = input_dim\n         self.output_dim = output_dim\n@@ -244,7 +244,7 @@ class LSTM(Layer):\n     def __init__(self, input_dim, output_dim=128, \n         init='uniform', inner_init='orthogonal', \n         activation='tanh', inner_activation='hard_sigmoid',\n-        truncate_gradient=-1, weights=None, return_sequences=False):\n+        weights=None, truncate_gradient=-1, return_sequences=False):\n \n         self.input_dim = input_dim\n         self.output_dim = output_dim\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aa7f9cdae951bba824883cfa392224a292b284bb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 33 | Churn Cumulative: 49 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -138,6 +138,39 @@ class Dense(Layer):\n         output = self.activation(T.dot(X, self.W) + self.b)\n         return output\n \n+class TimeDistributedDense(Layer):\n+    '''\n+       Apply a same DenseLayer for each dimension[1] (shared_dimension) input \n+       Especially useful after a recurrent network with 'return_sequence=True'\n+       Tensor input dimensions:   (nb_sample, shared_dimension, input_dim)\n+       Tensor output dimensions:  (nb_sample, shared_dimension, output_dim)\n+\n+    '''\n+    def __init__(self, input_dim, output_dim, init='uniform', activation='linear', weights=None):\n+        self.init = initializations.get(init)\n+        self.activation = activations.get(activation)\n+        self.input_dim = input_dim\n+        self.output_dim = output_dim\n+\n+        self.input = T.matrix()\n+        self.W = self.init((self.input_dim, self.output_dim))\n+        self.b = shared_zeros((self.output_dim))\n+\n+        self.params = [self.W, self.b]\n+\n+        if weights is not None:\n+            self.set_weights(weights)\n+\n+    def output(self, train):\n+        X = self.get_input(train)\n+\n+        def act_func(X):\n+            return self.activation(T.dot(X, self.W) + self.b)\n+\n+        output, _ = theano.scan(fn = act_func,\n+                                sequences = X.dimshuffle(1,0,2),\n+                                outputs_info=None)\n+        return output.dimshuffle(1,0,2)\n \n class Embedding(Layer):\n     '''\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
