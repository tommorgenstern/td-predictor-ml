{"custom_id": "keras#8d34c7ed3c632a31f1d491d733eb9a7bfa11043f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 498 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -323,7 +323,7 @@ def add(inputs, **kwargs):\n     # Returns\n         A tensor, the sum of the inputs.\n     \"\"\"\n-    return Sum(**kwargs)(inputs)\n+    return Add(**kwargs)(inputs)\n \n \n def multiply(inputs, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fbe7873fc0f43090e2df52b85867b8b6179516ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 4 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 2537 | Contributors (this commit): 8 | Commits (past 90d): 33 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -192,7 +192,7 @@ encoded_question = layers.Dropout(0.3)(encoded_question)\n encoded_question = RNN(EMBED_HIDDEN_SIZE)(encoded_question)\n encoded_question = layers.RepeatVector(story_maxlen)(encoded_question)\n \n-merged = layers.sum([encoded_sentence, encoded_question])\n+merged = layers.add([encoded_sentence, encoded_question])\n merged = RNN(EMBED_HIDDEN_SIZE)(merged)\n merged = layers.Dropout(0.3)(merged)\n preds = layers.Dense(vocab_size, activation='softmax')(merged)\n\n@@ -71,7 +71,7 @@ def identity_block(input_tensor, kernel_size, filters, stage, block):\n     x = Conv2D(filters3, (1, 1), name=conv_name_base + '2c')(x)\n     x = BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')(x)\n \n-    x = layers.sum([x, input_tensor])\n+    x = layers.add([x, input_tensor])\n     x = Activation('relu')(x)\n     return x\n \n@@ -117,7 +117,7 @@ def conv_block(input_tensor, kernel_size, filters, stage, block, strides=(2, 2))\n                       name=conv_name_base + '1')(input_tensor)\n     shortcut = BatchNormalization(axis=bn_axis, name=bn_name_base + '1')(shortcut)\n \n-    x = layers.sum([x, shortcut])\n+    x = layers.add([x, shortcut])\n     x = Activation('relu')(x)\n     return x\n \n\n@@ -156,7 +156,7 @@ def Xception(include_top=True, weights='imagenet',\n     x = BatchNormalization(name='block2_sepconv2_bn')(x)\n \n     x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block2_pool')(x)\n-    x = layers.sum([x, residual])\n+    x = layers.add([x, residual])\n \n     residual = Conv2D(256, (1, 1), strides=(2, 2),\n                       padding='same', use_bias=False)(x)\n@@ -170,7 +170,7 @@ def Xception(include_top=True, weights='imagenet',\n     x = BatchNormalization(name='block3_sepconv2_bn')(x)\n \n     x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block3_pool')(x)\n-    x = layers.sum([x, residual])\n+    x = layers.add([x, residual])\n \n     residual = Conv2D(728, (1, 1), strides=(2, 2),\n                       padding='same', use_bias=False)(x)\n@@ -184,7 +184,7 @@ def Xception(include_top=True, weights='imagenet',\n     x = BatchNormalization(name='block4_sepconv2_bn')(x)\n \n     x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block4_pool')(x)\n-    x = layers.sum([x, residual])\n+    x = layers.add([x, residual])\n \n     for i in range(8):\n         residual = x\n@@ -200,7 +200,7 @@ def Xception(include_top=True, weights='imagenet',\n         x = SeparableConv2D(728, (3, 3), padding='same', use_bias=False, name=prefix + '_sepconv3')(x)\n         x = BatchNormalization(name=prefix + '_sepconv3_bn')(x)\n \n-        x = layers.sum([x, residual])\n+        x = layers.add([x, residual])\n \n     residual = Conv2D(1024, (1, 1), strides=(2, 2),\n                       padding='same', use_bias=False)(x)\n@@ -214,7 +214,7 @@ def Xception(include_top=True, weights='imagenet',\n     x = BatchNormalization(name='block13_sepconv2_bn')(x)\n \n     x = MaxPooling2D((3, 3), strides=(2, 2), padding='same', name='block13_pool')(x)\n-    x = layers.sum([x, residual])\n+    x = layers.add([x, residual])\n \n     x = SeparableConv2D(1536, (3, 3), padding='same', use_bias=False, name='block14_sepconv1')(x)\n     x = BatchNormalization(name='block14_sepconv1_bn')(x)\n\n@@ -479,7 +479,7 @@ def test_recursion():\n \n         # test merge\n         layers.concatenate([j_tf, k_tf], axis=1)\n-        layers.sum([j_tf, k_tf])\n+        layers.add([j_tf, k_tf])\n \n         # test tensor input\n         x = tf.placeholder(shape=(None, 2), dtype=K.floatx())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#91bab91f3c8e714c3c9604090fe48bfc8f3d982c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 7 | Churn Cumulative: 505 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -273,7 +273,12 @@ class Dot(_Merge):\n             else:\n                 axes = [self.axes] * 2\n         else:\n-            axes = self.axes\n+            axes = []\n+            for i in range(len(self.axes)):\n+                if self.axes[i] < 0:\n+                    axes.append(self.axes[i] % K.ndim(inputs[i]))\n+                else:\n+                    axes.append(self.axes[i])\n         if self.normalize:\n             x1 = K.l2_normalize(x1, axis=axes[0])\n             x2 = K.l2_normalize(x2, axis=axes[1])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#50d83f7ab80c04283236a986a1c99550cecc5140", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 1309 | Contributors (this commit): 16 | Commits (past 90d): 15 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -118,6 +118,10 @@ class PReLU(Layer):\n \n     def call(self, inputs, mask=None):\n         pos = K.relu(inputs)\n+        if K.backend() == 'theano':\n+            neg = (K.pattern_broadcast(self.alpha, self.param_broadcast) *\n+                   (inputs - K.abs(inputs)) * 0.5)\n+        else:\n             neg = -self.alpha * K.relu(-inputs)\n         return pos + neg\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4e504462796ee90e9e38cda6af1bcc3a40fbb39a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 37 | Files Changed: 2 | Hunks: 17 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 79 | Churn Cumulative: 574 | Contributors (this commit): 10 | Commits (past 90d): 6 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -9,7 +9,7 @@ from scipy.stats import norm\n from keras.layers import Input, Dense, Lambda\n from keras.models import Model\n from keras import backend as K\n-from keras import objectives\n+from keras import metrics\n from keras.datasets import mnist\n \n batch_size = 100\n@@ -28,7 +28,7 @@ z_log_var = Dense(latent_dim)(h)\n def sampling(args):\n     z_mean, z_log_var = args\n     epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0.,\n-                              std=epsilon_std)\n+                              stddev=epsilon_std)\n     return z_mean + K.exp(z_log_var / 2) * epsilon\n \n # note that \"output_shape\" isn't necessary with the TensorFlow backend\n@@ -42,7 +42,7 @@ x_decoded_mean = decoder_mean(h_decoded)\n \n \n def vae_loss(x, x_decoded_mean):\n-    xent_loss = original_dim * objectives.binary_crossentropy(x, x_decoded_mean)\n+    xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n     kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n     return xent_loss + kl_loss\n \n\n@@ -8,16 +8,16 @@ import matplotlib.pyplot as plt\n from scipy.stats import norm\n \n from keras.layers import Input, Dense, Lambda, Flatten, Reshape\n-from keras.layers import Convolution2D, Deconvolution2D\n+from keras.layers import Conv2D, Conv2DTranspose\n from keras.models import Model\n from keras import backend as K\n-from keras import objectives\n+from keras import metrics\n from keras.datasets import mnist\n \n # input image dimensions\n img_rows, img_cols, img_chns = 28, 28, 1\n # number of convolutional filters to use\n-filterss = 64\n+filters = 64\n # convolution kernel size\n num_conv = 3\n \n@@ -32,16 +32,21 @@ epsilon_std = 1.0\n epochs = 5\n \n x = Input(batch_shape=(batch_size,) + original_img_size)\n-conv_1 = Convolution2D(img_chns, 2, 2, border_mode='same', activation='relu')(x)\n-conv_2 = Convolution2D(filterss, 2, 2,\n-                       border_mode='same', activation='relu',\n-                       subsample=(2, 2))(conv_1)\n-conv_3 = Convolution2D(filterss, num_conv, num_conv,\n-                       border_mode='same', activation='relu',\n-                       subsample=(1, 1))(conv_2)\n-conv_4 = Convolution2D(filterss, num_conv, num_conv,\n-                       border_mode='same', activation='relu',\n-                       subsample=(1, 1))(conv_3)\n+conv_1 = Conv2D(img_chns,\n+                kernel_size=(2, 2),\n+                padding='same', activation='relu')(x)\n+conv_2 = Conv2D(filters,\n+                kernel_size=(2, 2),\n+                padding='same', activation='relu',\n+                strides=(2, 2))(conv_1)\n+conv_3 = Conv2D(filters,\n+                kernel_size=num_conv,\n+                padding='same', activation='relu',\n+                strides=1)(conv_2)\n+conv_4 = Conv2D(filters,\n+                kernel_size=num_conv,\n+                padding='same', activation='relu',\n+                strides=1)(conv_3)\n flat = Flatten()(conv_4)\n hidden = Dense(intermediate_dim, activation='relu')(flat)\n \n@@ -52,7 +57,7 @@ z_log_var = Dense(latent_dim)(hidden)\n def sampling(args):\n     z_mean, z_log_var = args\n     epsilon = K.random_normal(shape=(batch_size, latent_dim),\n-                              mean=0., std=epsilon_std)\n+                              mean=0., stddev=epsilon_std)\n     return z_mean + K.exp(z_log_var) * epsilon\n \n # note that \"output_shape\" isn't necessary with the TensorFlow backend\n@@ -61,35 +66,35 @@ z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n \n # we instantiate these layers separately so as to reuse them later\n decoder_hid = Dense(intermediate_dim, activation='relu')\n-decoder_upsample = Dense(filterss * 14 * 14, activation='relu')\n+decoder_upsample = Dense(filters * 14 * 14, activation='relu')\n \n if K.image_data_format() == 'channels_first':\n-    output_shape = (batch_size, filterss, 14, 14)\n+    output_shape = (batch_size, filters, 14, 14)\n else:\n-    output_shape = (batch_size, 14, 14, filterss)\n+    output_shape = (batch_size, 14, 14, filters)\n \n decoder_reshape = Reshape(output_shape[1:])\n-decoder_deconv_1 = Deconvolution2D(filterss, num_conv, num_conv,\n-                                   output_shape,\n-                                   border_mode='same',\n-                                   subsample=(1, 1),\n+decoder_deconv_1 = Conv2DTranspose(filters,\n+                                   kernel_size=num_conv,\n+                                   padding='same',\n+                                   strides=1,\n                                    activation='relu')\n-decoder_deconv_2 = Deconvolution2D(filterss, num_conv, num_conv,\n-                                   output_shape,\n-                                   border_mode='same',\n-                                   subsample=(1, 1),\n+decoder_deconv_2 = Conv2DTranspose(filters, num_conv,\n+                                   padding='same',\n+                                   strides=1,\n                                    activation='relu')\n if K.image_data_format() == 'channels_first':\n-    output_shape = (batch_size, filterss, 29, 29)\n+    output_shape = (batch_size, filters, 29, 29)\n else:\n-    output_shape = (batch_size, 29, 29, filterss)\n-decoder_deconv_3_upsamp = Deconvolution2D(filterss, 2, 2,\n-                                          output_shape,\n-                                          border_mode='valid',\n-                                          subsample=(2, 2),\n+    output_shape = (batch_size, 29, 29, filters)\n+decoder_deconv_3_upsamp = Conv2DTranspose(filters,\n+                                          kernel_size=(3, 3),\n+                                          strides=(2, 2),\n+                                          padding='valid',\n                                           activation='relu')\n-decoder_mean_squash = Convolution2D(img_chns, 2, 2,\n-                                    border_mode='valid',\n+decoder_mean_squash = Conv2D(img_chns,\n+                             kernel_size=2,\n+                             padding='valid',\n                              activation='sigmoid')\n \n hid_decoded = decoder_hid(z)\n@@ -106,7 +111,7 @@ def vae_loss(x, x_decoded_mean):\n     # for x and x_decoded_mean, so we MUST flatten these!\n     x = K.flatten(x)\n     x_decoded_mean = K.flatten(x_decoded_mean)\n-    xent_loss = img_rows * img_cols * objectives.binary_crossentropy(x, x_decoded_mean)\n+    xent_loss = img_rows * img_cols * metrics.binary_crossentropy(x, x_decoded_mean)\n     kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n     return xent_loss + kl_loss\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0e9ac3dae0359db9fba655e76da5a443bf1d935d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 17/17 | Churn Δ: 5 | Churn Cumulative: 5 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -86,7 +86,10 @@ def model_to_dot(model, show_shapes=False, show_layer_names=True):\n     return dot\n \n \n-def plot(model, to_file='model.png', show_shapes=False, show_layer_names=True):\n+def plot_model(model,\n+               to_file='model.png',\n+               show_shapes=False,\n+               show_layer_names=True):\n     dot = model_to_dot(model, show_shapes, show_layer_names)\n     _, extension = os.path.splitext(to_file)\n     if not extension:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5d0cb109496d46edb6359a50893cab07e9f386ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 141 | Lines Deleted: 169 | Files Changed: 14 | Hunks: 78 | Methods Changed: 14 | Complexity Δ (Sum/Max): -12/0 | Churn Δ: 310 | Churn Cumulative: 3762 | Contributors (this commit): 39 | Commits (past 90d): 65 | Contributors (cumulative): 65 | DMM Complexity: 0.0\n\nDIFF:\n@@ -11,11 +11,11 @@ backend (`K`), our code can run both on TensorFlow and Theano.\n '''\n \n from __future__ import print_function\n+import keras\n from keras.models import Sequential\n from keras.layers import Dense, Dropout, Layer, Activation\n from keras.datasets import mnist\n from keras import backend as K\n-from keras.utils import np_utils\n \n \n class Antirectifier(Layer):\n@@ -77,8 +77,8 @@ print(x_train.shape[0], 'train samples')\n print(x_test.shape[0], 'test samples')\n \n # convert class vectors to binary class matrices\n-Y_train = np_utils.to_categorical(y_train, num_classes)\n-Y_test = np_utils.to_categorical(y_test, num_classes)\n+y_train = keras.utils.to_categorical(y_train, num_classes)\n+y_test = keras.utils.to_categorical(y_test, num_classes)\n \n # build the model\n model = Sequential()\n@@ -97,9 +97,9 @@ model.compile(loss='categorical_crossentropy',\n               metrics=['accuracy'])\n \n # train the model\n-model.fit(x_train, Y_train,\n+model.fit(x_train, y_train,\n           batch_size=batch_size, epochs=epochs,\n-          verbose=1, validation_data=(x_test, Y_test))\n+          verbose=1, validation_data=(x_test, y_test))\n \n # next, compare with an equivalent network\n # with2x bigger Dense layers and ReLU\n\n@@ -8,12 +8,12 @@ It gets down to 0.65 test logloss in 25 epochs, and down to 0.55 after 50 epochs\n '''\n \n from __future__ import print_function\n+import keras\n from keras.datasets import cifar10\n from keras.preprocessing.image import ImageDataGenerator\n from keras.models import Sequential\n from keras.layers import Dense, Dropout, Activation, Flatten\n from keras.layers import Conv2D, MaxPooling2D\n-from keras.utils import np_utils\n \n batch_size = 32\n num_classes = 10\n@@ -32,8 +32,8 @@ print(x_train.shape[0], 'train samples')\n print(x_test.shape[0], 'test samples')\n \n # Convert class vectors to binary class matrices.\n-y_train = np_utils.to_categorical(y_train, num_classes)\n-y_test = np_utils.to_categorical(y_test, num_classes)\n+y_train = keras.utils.to_categorical(y_train, num_classes)\n+y_test = keras.utils.to_categorical(y_test, num_classes)\n \n model = Sequential()\n \n\n@@ -6,9 +6,6 @@ Gets to 99.25% test accuracy after 12 epochs\n '''\n \n from __future__ import print_function\n-import numpy as np\n-np.random.seed(1337)  # for reproducibility\n-\n import keras\n from keras.datasets import mnist\n from keras.models import Sequential\n@@ -44,8 +41,8 @@ print(x_train.shape[0], 'train samples')\n print(x_test.shape[0], 'test samples')\n \n # convert class vectors to binary class matrices\n-y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n-y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n+y_train = keras.utils.to_categorical(y_train, num_classes)\n+y_test = keras.utils.to_categorical(y_test, num_classes)\n \n model = Sequential()\n model.add(Conv2D(32, kernel_size=(3, 3),\n\n@@ -26,11 +26,11 @@ After 5 epochs: train acc: 0.9858, val acc: 0.9864\n \"\"\"\n from __future__ import print_function\n \n+import keras\n from keras.datasets import mnist\n from keras.models import Model\n from keras.layers import Input, Dense, TimeDistributed\n from keras.layers import LSTM\n-from keras.utils import np_utils\n \n # Training parameters.\n batch_size = 32\n@@ -56,8 +56,8 @@ print(x_train.shape[0], 'train samples')\n print(x_test.shape[0], 'test samples')\n \n # Converts class vectors to binary class matrices.\n-Y_train = np_utils.to_categorical(y_train, num_classes)\n-Y_test = np_utils.to_categorical(y_test, num_classes)\n+Y_train = keras.utils.to_categorical(y_train, num_classes)\n+Y_test = keras.utils.to_categorical(y_test, num_classes)\n \n row, col, pixel = x_train.shape[1:]\n \n\n@@ -3,7 +3,7 @@ with pixel-by-pixel sequential MNIST in\n \"A Simple Way to Initialize Recurrent Networks of Rectified Linear Units\"\n by Quoc V. Le, Navdeep Jaitly, Geoffrey E. Hinton\n \n-arXiv:1504.00941v2 [cs.NE] 7 Apr 2015\n+arxiv:1504.00941v2 [cs.NE] 7 Apr 2015\n http://arxiv.org/pdf/1504.00941v2.pdf\n \n Optimizer is replaced with RMSprop which yields more stable and steady\n@@ -15,13 +15,13 @@ Reaches 0.93 train/test accuracy after 900 epochs\n \n from __future__ import print_function\n \n+import keras\n from keras.datasets import mnist\n from keras.models import Sequential\n from keras.layers import Dense, Activation\n from keras.layers import SimpleRNN\n from keras import initializers\n from keras.optimizers import RMSprop\n-from keras.utils import np_utils\n \n batch_size = 32\n num_classes = 10\n@@ -32,21 +32,21 @@ learning_rate = 1e-6\n clip_norm = 1.0\n \n # the data, shuffled and split between train and test sets\n-(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+(x_train, y_train), (x_test, y_test) = mnist.load_data()\n \n-X_train = X_train.reshape(X_train.shape[0], -1, 1)\n-X_test = X_test.reshape(X_test.shape[0], -1, 1)\n-X_train = X_train.astype('float32')\n-X_test = X_test.astype('float32')\n-X_train /= 255\n-X_test /= 255\n-print('X_train shape:', X_train.shape)\n-print(X_train.shape[0], 'train samples')\n-print(X_test.shape[0], 'test samples')\n+x_train = x_train.reshape(x_train.shape[0], -1, 1)\n+x_test = x_test.reshape(x_test.shape[0], -1, 1)\n+x_train = x_train.astype('float32')\n+x_test = x_test.astype('float32')\n+x_train /= 255\n+x_test /= 255\n+print('x_train shape:', x_train.shape)\n+print(x_train.shape[0], 'train samples')\n+print(x_test.shape[0], 'test samples')\n \n # convert class vectors to binary class matrices\n-Y_train = np_utils.to_categorical(y_train, num_classes)\n-Y_test = np_utils.to_categorical(y_test, num_classes)\n+y_train = keras.utils.to_categorical(y_train, num_classes)\n+y_test = keras.utils.to_categorical(y_test, num_classes)\n \n print('Evaluate IRNN...')\n model = Sequential()\n@@ -54,7 +54,7 @@ model.add(SimpleRNN(hidden_units,\n                     kernel_initializer=initializers.RandomNormal(stddev=0.001),\n                     recurrent_initializer=initializers.Identity(gain=1.0),\n                     activation='relu',\n-                    input_shape=X_train.shape[1:]))\n+                    input_shape=x_train.shape[1:]))\n model.add(Dense(num_classes))\n model.add(Activation('softmax'))\n rmsprop = RMSprop(lr=learning_rate)\n@@ -62,9 +62,9 @@ model.compile(loss='categorical_crossentropy',\n               optimizer=rmsprop,\n               metrics=['accuracy'])\n \n-model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs,\n-          verbose=1, validation_data=(X_test, Y_test))\n+model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n+          verbose=1, validation_data=(x_test, y_test))\n \n-scores = model.evaluate(X_test, Y_test, verbose=0)\n+scores = model.evaluate(x_test, y_test, verbose=0)\n print('IRNN test score:', scores[0])\n print('IRNN test accuracy:', scores[1])\n\n@@ -6,14 +6,12 @@ Gets to 98.40% test accuracy after 20 epochs\n '''\n \n from __future__ import print_function\n-import numpy as np\n-np.random.seed(1337)  # for reproducibility\n \n+import keras\n from keras.datasets import mnist\n from keras.models import Sequential\n-from keras.layers import Dense, Dropout, Activation\n+from keras.layers import Dense, Dropout\n from keras.optimizers import RMSprop\n-from keras.utils import np_utils\n \n \n batch_size = 128\n@@ -33,8 +31,8 @@ print(x_train.shape[0], 'train samples')\n print(x_test.shape[0], 'test samples')\n \n # convert class vectors to binary class matrices\n-y_train = np_utils.to_categorical(y_train, num_classes)\n-y_test = np_utils.to_categorical(y_test, num_classes)\n+y_train = keras.utils.to_categorical(y_train, num_classes)\n+y_test = keras.utils.to_categorical(y_test, num_classes)\n \n model = Sequential()\n model.add(Dense(512, activation='relu', input_shape=(784,)))\n\n@@ -57,15 +57,16 @@ Results\n from __future__ import print_function\n from six.moves import xrange\n import numpy as np\n-np.random.seed(1337)\n-\n+import keras\n from keras.models import Sequential\n from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten\n from keras.optimizers import SGD\n-from keras.utils import np_utils\n from keras.datasets import mnist\n \n+if keras.backend.image_data_format() == 'channels_first':\n     input_shape = (1, 28, 28)  # image shape\n+else:\n+    input_shape = (28, 28, 1)  # image shape\n num_class = 10  # number of class\n \n \n@@ -75,7 +76,7 @@ def preprocess_input(x):\n \n \n def preprocess_output(y):\n-    return np_utils.to_categorical(y)\n+    return keras.utils.to_categorical(y)\n \n (train_x, train_y), (validation_x, validation_y) = mnist.load_data()\n train_x, validation_x = map(preprocess_input, [train_x, validation_x])\n@@ -217,11 +218,11 @@ def make_teacher_model(train_data, validation_data, epochs=3):\n     '''Train a simple CNN as teacher model.\n     '''\n     model = Sequential()\n-    model.add(Conv2D(64, 3, 3, input_shape=input_shape,\n-                     border_mode='same', name='conv1'))\n-    model.add(MaxPooling2D(name='pool1'))\n-    model.add(Conv2D(64, 3, 3, border_mode='same', name='conv2'))\n-    model.add(MaxPooling2D(name='pool2'))\n+    model.add(Conv2D(64, 3, input_shape=input_shape,\n+                     padding='same', name='conv1'))\n+    model.add(MaxPooling2D(2, name='pool1'))\n+    model.add(Conv2D(64, 3, padding='same', name='conv2'))\n+    model.add(MaxPooling2D(2, name='pool2'))\n     model.add(Flatten(name='flatten'))\n     model.add(Dense(64, activation='relu', name='fc1'))\n     model.add(Dense(num_class, activation='softmax', name='fc2'))\n@@ -245,11 +246,11 @@ def make_wider_student_model(teacher_model, train_data,\n \n     model = Sequential()\n     # a wider conv1 compared to teacher_model\n-    model.add(Conv2D(new_conv1_width, 3, 3, input_shape=input_shape,\n-                     border_mode='same', name='conv1'))\n-    model.add(MaxPooling2D(name='pool1'))\n-    model.add(Conv2D(64, 3, 3, border_mode='same', name='conv2'))\n-    model.add(MaxPooling2D(name='pool2'))\n+    model.add(Conv2D(new_conv1_width, 3, input_shape=input_shape,\n+                     padding='same', name='conv1'))\n+    model.add(MaxPooling2D(2, name='pool1'))\n+    model.add(Conv2D(64, 3, padding='same', name='conv2'))\n+    model.add(MaxPooling2D(2, name='pool2'))\n     model.add(Flatten(name='flatten'))\n     # a wider fc1 compared to teacher model\n     model.add(Dense(new_fc1_width, activation='relu', name='fc1'))\n@@ -290,21 +291,21 @@ def make_deeper_student_model(teacher_model, train_data,\n        with either 'random-init' (baseline) or 'net2deeper'\n     '''\n     model = Sequential()\n-    model.add(Conv2D(64, 3, 3, input_shape=input_shape,\n-                     border_mode='same', name='conv1'))\n-    model.add(MaxPooling2D(name='pool1'))\n-    model.add(Conv2D(64, 3, 3, border_mode='same', name='conv2'))\n+    model.add(Conv2D(64, 3, input_shape=input_shape,\n+                     padding='same', name='conv1'))\n+    model.add(MaxPooling2D(2, name='pool1'))\n+    model.add(Conv2D(64, 3, padding='same', name='conv2'))\n     # add another conv2d layer to make original conv2 deeper\n     if init == 'net2deeper':\n         prev_w, _ = model.get_layer('conv2').get_weights()\n         new_weights = deeper2net_conv2d(prev_w)\n-        model.add(Conv2D(64, 3, 3, border_mode='same',\n+        model.add(Conv2D(64, 3, padding='same',\n                          name='conv2-deeper', weights=new_weights))\n     elif init == 'random-init':\n-        model.add(Conv2D(64, 3, 3, border_mode='same', name='conv2-deeper'))\n+        model.add(Conv2D(64, 3, padding='same', name='conv2-deeper'))\n     else:\n         raise ValueError('Unsupported weight initializer: %s' % init)\n-    model.add(MaxPooling2D(name='pool2'))\n+    model.add(MaxPooling2D(2, name='pool2'))\n     model.add(Flatten(name='flatten'))\n     model.add(Dense(64, activation='relu', name='fc1'))\n     # add another fc layer to make original fc1 deeper\n\n@@ -4,14 +4,12 @@ Builds simple CNN models on MNIST and uses sklearn's GridSearchCV to find best m\n '''\n \n from __future__ import print_function\n-import numpy as np\n-np.random.seed(1337)  # for reproducibility\n \n+import keras\n from keras.datasets import mnist\n from keras.models import Sequential\n from keras.layers import Dense, Dropout, Activation, Flatten\n from keras.layers import Convolution2D, MaxPooling2D\n-from keras.utils import np_utils\n from keras.wrappers.scikit_learn import KerasClassifier\n from keras import backend as K\n from sklearn.grid_search import GridSearchCV\n@@ -23,45 +21,45 @@ num_classes = 10\n img_rows, img_cols = 28, 28\n \n # load training data and do basic data normalization\n-(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+(x_train, y_train), (x_test, y_test) = mnist.load_data()\n \n-if K.image_dim_ordering() == 'th':\n-    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n-    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n+if K.image_data_format() == 'channels_first':\n+    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n+    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n     input_shape = (1, img_rows, img_cols)\n else:\n-    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n-    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n+    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n+    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n     input_shape = (img_rows, img_cols, 1)\n \n-X_train = X_train.astype('float32')\n-X_test = X_test.astype('float32')\n-X_train /= 255\n-X_test /= 255\n+x_train = x_train.astype('float32')\n+x_test = x_test.astype('float32')\n+x_train /= 255\n+x_test /= 255\n \n # convert class vectors to binary class matrices\n-y_train = np_utils.to_categorical(y_train, num_classes)\n-y_test = np_utils.to_categorical(y_test, num_classes)\n+y_train = keras.utils.to_categorical(y_train, num_classes)\n+y_test = keras.utils.to_categorical(y_test, num_classes)\n \n \n-def make_model(dense_layer_sizes, filterss, num_conv, num_pool):\n+def make_model(dense_layer_sizes, filters, kernel_size, pool_size):\n     '''Creates model comprised of 2 convolutional layers followed by dense layers\n \n-    dense_layer_sizes: List of layer sizes. This list has one number for each layer\n-    filterss: Number of convolutional filters in each convolutional layer\n-    num_conv: Convolutional kernel size\n-    num_pool: Size of pooling area for max pooling\n+    dense_layer_sizes: List of layer sizes.\n+        This list has one number for each layer\n+    filters: Number of convolutional filters in each convolutional layer\n+    kernel_size: Convolutional kernel size\n+    pool_size: Size of pooling area for max pooling\n     '''\n \n     model = Sequential()\n-\n-    model.add(Convolution2D(filterss, num_conv, num_conv,\n-                            border_mode='valid',\n+    model.add(Convolution2D(filters, kernel_size,\n+                            padding='valid',\n                             input_shape=input_shape))\n     model.add(Activation('relu'))\n-    model.add(Convolution2D(filterss, num_conv, num_conv))\n+    model.add(Convolution2D(filters, kernel_size))\n     model.add(Activation('relu'))\n-    model.add(MaxPooling2D(pool_size=(num_pool, num_pool)))\n+    model.add(MaxPooling2D(pool_size=pool_size))\n     model.add(Dropout(0.25))\n \n     model.add(Flatten())\n@@ -85,12 +83,12 @@ validator = GridSearchCV(my_classifier,\n                                      # epochs is avail for tuning even when not\n                                      # an argument to model building function\n                                      'epochs': [3, 6],\n-                                     'filterss': [8],\n-                                     'num_conv': [3],\n-                                     'num_pool': [2]},\n+                                     'filters': [8],\n+                                     'kernel_size': [3],\n+                                     'pool_size': [2]},\n                          scoring='neg_log_loss',\n                          n_jobs=1)\n-validator.fit(X_train, y_train)\n+validator.fit(x_train, y_train)\n \n print('The parameters of the best model are: ')\n print(validator.best_params_)\n@@ -99,6 +97,6 @@ print(validator.best_params_)\n # validator.best_estimator_.model returns the (unwrapped) keras model\n best_model = validator.best_estimator_.model\n metric_names = best_model.metrics_names\n-metric_values = best_model.evaluate(X_test, y_test)\n+metric_values = best_model.evaluate(x_test, y_test)\n for metric, value in zip(metric_names, metric_values):\n     print(metric, ': ', value)\n\n@@ -12,16 +12,13 @@ and 99.2% for the last five digits after transfer + fine-tuning.\n '''\n \n from __future__ import print_function\n-import numpy as np\n+\n import datetime\n-\n-np.random.seed(1337)  # for reproducibility\n-\n+import keras\n from keras.datasets import mnist\n from keras.models import Sequential\n from keras.layers import Dense, Dropout, Activation, Flatten\n from keras.layers import Conv2D, MaxPooling2D\n-from keras.utils import np_utils\n from keras import backend as K\n \n now = datetime.datetime.now\n@@ -57,8 +54,8 @@ def train_model(model, train, test, num_classes):\n     print(x_test.shape[0], 'test samples')\n \n     # convert class vectors to binary class matrices\n-    y_train = np_utils.to_categorical(train[1], num_classes)\n-    y_test = np_utils.to_categorical(test[1], num_classes)\n+    y_train = keras.utils.to_categorical(train[1], num_classes)\n+    y_test = keras.utils.to_categorical(test[1], num_classes)\n \n     model.compile(loss='categorical_crossentropy',\n                   optimizer='adadelta',\n@@ -85,8 +82,8 @@ x_test_lt5 = x_test[y_test < 5]\n y_test_lt5 = y_test[y_test < 5]\n \n x_train_gte5 = x_train[y_train >= 5]\n-y_train_gte5 = y_train[y_train >= 5] - 5  # make classes start at 0 for\n-x_test_gte5 = x_test[y_test >= 5]         # np_utils.to_categorical\n+y_train_gte5 = y_train[y_train >= 5] - 5\n+x_test_gte5 = x_test[y_test >= 5]\n y_test_gte5 = y_test[y_test >= 5] - 5\n \n # define two groups of layers: feature (convolutions) and classification (dense)\n\n@@ -12,17 +12,17 @@ http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/news20.html\n '''\n \n from __future__ import print_function\n-import os\n-import numpy as np\n-np.random.seed(1337)\n \n+import os\n+import sys\n+import numpy as np\n from keras.preprocessing.text import Tokenizer\n from keras.preprocessing.sequence import pad_sequences\n-from keras.utils.np_utils import to_categorical\n+from keras.utils import to_categorical\n from keras.layers import Dense, Input, Flatten\n from keras.layers import Conv1D, MaxPooling1D, Embedding\n from keras.models import Model\n-import sys\n+\n \n BASE_DIR = ''\n GLOVE_DIR = BASE_DIR + '/glove.6B/'\n\n@@ -1,15 +1,13 @@\n '''Trains and evaluate a simple MLP\n on the Reuters newswire topic classification task.\n '''\n-\n from __future__ import print_function\n-import numpy as np\n-np.random.seed(1337)  # for reproducibility\n \n+import numpy as np\n+import keras\n from keras.datasets import reuters\n from keras.models import Sequential\n from keras.layers import Dense, Dropout, Activation\n-from keras.utils import np_utils\n from keras.preprocessing.text import Tokenizer\n \n max_words = 1000\n@@ -17,25 +15,27 @@ batch_size = 32\n epochs = 5\n \n print('Loading data...')\n-(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=max_words, test_split=0.2)\n-print(len(X_train), 'train sequences')\n-print(len(X_test), 'test sequences')\n+(x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=max_words,\n+                                                         test_split=0.2)\n+print(len(x_train), 'train sequences')\n+print(len(x_test), 'test sequences')\n \n num_classes = np.max(y_train) + 1\n print(num_classes, 'classes')\n \n print('Vectorizing sequence data...')\n tokenizer = Tokenizer(num_words=max_words)\n-X_train = tokenizer.sequences_to_matrix(X_train, mode='binary')\n-X_test = tokenizer.sequences_to_matrix(X_test, mode='binary')\n-print('X_train shape:', X_train.shape)\n-print('X_test shape:', X_test.shape)\n+x_train = tokenizer.sequences_to_matrix(x_train, mode='binary')\n+x_test = tokenizer.sequences_to_matrix(x_test, mode='binary')\n+print('x_train shape:', x_train.shape)\n+print('x_test shape:', x_test.shape)\n \n-print('Convert class vector to binary class matrix (for use with categorical_crossentropy)')\n-Y_train = np_utils.to_categorical(y_train, num_classes)\n-Y_test = np_utils.to_categorical(y_test, num_classes)\n-print('Y_train shape:', Y_train.shape)\n-print('Y_test shape:', Y_test.shape)\n+print('Convert class vector to binary class matrix '\n+      '(for use with categorical_crossentropy)')\n+y_train = keras.utils.to_categorical(y_train, num_classes)\n+y_test = keras.utils.to_categorical(y_test, num_classes)\n+print('y_train shape:', y_train.shape)\n+print('y_test shape:', y_test.shape)\n \n print('Building model...')\n model = Sequential()\n@@ -49,10 +49,10 @@ model.compile(loss='categorical_crossentropy',\n               optimizer='adam',\n               metrics=['accuracy'])\n \n-history = model.fit(X_train, Y_train,\n+history = model.fit(x_train, y_train,\n                     epochs=epochs, batch_size=batch_size,\n                     verbose=1, validation_split=0.1)\n-score = model.evaluate(X_test, Y_test,\n+score = model.evaluate(x_test, y_test,\n                        batch_size=batch_size, verbose=1)\n print('Test score:', score[0])\n print('Test accuracy:', score[1])\n\n@@ -4,3 +4,17 @@ from . import conv_utils\n from . import data_utils\n from . import generic_utils\n from . import io_utils\n+\n+# Globally-importable utils.\n+from .io_utils import HDF5Matrix\n+from .data_utils import get_file\n+from .generic_utils import CustomObjectScope\n+from .generic_utils import custom_object_scope\n+from .generic_utils import get_custom_objects\n+from .generic_utils import serialize_keras_object\n+from .generic_utils import deserialize_keras_object\n+from .generic_utils import Progbar\n+from .layer_utils import convert_all_kernels_in_model\n+from .vis_utils import plot_model\n+from .np_utils import to_categorical\n+from .np_utils import normalize\n\n@@ -11,11 +11,6 @@ try:\n except ImportError:\n     h5py = None\n \n-try:\n-    import tables\n-except ImportError:\n-    tables = None\n-\n \n class HDF5Matrix(object):\n     \"\"\"Representation of HDF5 dataset to be used instead of a Numpy array.\n@@ -97,29 +92,6 @@ class HDF5Matrix(object):\n         return (self.end - self.start,) + self.data.shape[1:]\n \n \n-def save_array(array, name):\n-    if tables is None:\n-        raise ImportError('The use of `save_array` requires '\n-                          'the tables module.')\n-    f = tables.open_file(name, 'w')\n-    atom = tables.Atom.from_dtype(array.dtype)\n-    ds = f.create_carray(f.root, 'data', atom, array.shape)\n-    ds[:] = array\n-    f.close()\n-\n-\n-def load_array(name):\n-    if tables is None:\n-        raise ImportError('The use of `load_array` requires '\n-                          'the tables module.')\n-    f = tables.open_file(name)\n-    array = f.root.data\n-    a = np.empty(shape=array.shape, dtype=array.dtype)\n-    a[:] = array[:]\n-    f.close()\n-    return a\n-\n-\n def ask_to_proceed_with_overwrite(filepath):\n     \"\"\"Produces a prompt asking about overwriting a file.\n \n\n@@ -2,7 +2,6 @@\n from __future__ import absolute_import\n \n import numpy as np\n-from six.moves import zip\n \n \n def to_categorical(y, num_classes=None):\n@@ -27,21 +26,17 @@ def to_categorical(y, num_classes=None):\n     return categorical\n \n \n-def normalize(a, axis=-1, order=2):\n-    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n+def normalize(x, axis=-1, order=2):\n+    \"\"\"Normalizes a Numpy array.\n+\n+    # Arguments\n+        x: Numpy array to normalize.\n+        axis: axis along which to normalize.\n+        order: Normalization order (e.g. 2 for L2 norm).\n+\n+    # Returns\n+        A normalized copy of the array.\n+    \"\"\"\n+    l2 = np.atleast_1d(np.linalg.norm(x, order, axis))\n     l2[l2 == 0] = 1\n-    return a / np.expand_dims(l2, axis)\n-\n-\n-def accuracy(p, y):\n-    return np.mean([a == b for a, b in zip(p, y)])\n-\n-\n-def probas_to_classes(y_pred):\n-    if len(y_pred.shape) > 1 and y_pred.shape[1] > 1:\n-        return categorical_probas_to_classes(y_pred)\n-    return np.array([1 if p > 0.5 else 0 for p in y_pred])\n-\n-\n-def categorical_probas_to_classes(p):\n-    return np.argmax(p, axis=1)\n+    return x / np.expand_dims(l2, axis)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2b10c6898039e0cc75dc06ea08e8a75499a2f512", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 9 | Churn Cumulative: 14 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.8\n\nDIFF:\n@@ -9,8 +9,14 @@ try:\n     import pydot_ng as pydot\n except ImportError:\n     # Fall back on pydot if necessary.\n+    try:\n         import pydot\n-if not pydot.find_graphviz():\n+    except ImportError:\n+        pydot = None\n+\n+\n+def _check_pydot():\n+    if not (pydot and pydot.find_graphviz()):\n         raise ImportError('Failed to import pydot. You must install pydot'\n                           ' and graphviz for `pydotprint` to work.')\n \n@@ -26,6 +32,7 @@ def model_to_dot(model, show_shapes=False, show_layer_names=True):\n     # Returns\n         A `pydot.Dot` instance representing the Keras model.\n     \"\"\"\n+    _check_pydot()\n     dot = pydot.Dot()\n     dot.set('rankdir', 'TB')\n     dot.set('concentrate', True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7da568d59ff768ab84c944733071cdc79943b99e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 226 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -82,6 +82,8 @@ class HDF5Matrix(object):\n                 idx = [x + self.start for x in key]\n             else:\n                 raise IndexError\n+        else:\n+            raise IndexError\n         if self.normalizer is not None:\n             return self.normalizer(self.data[idx])\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ada7aa12bfed8193cc4bb80e090dd3b4cf222c05", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 2 | Churn Cumulative: 179 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -144,4 +144,6 @@ def deconv_length(dim_size, stride_size, kernel_size, padding):\n         dim_size *= stride_size\n     if padding == 'valid' and dim_size is not None:\n         dim_size += max(kernel_size - stride_size, 0)\n+    if padding == 'full' and dim_size is not None:\n+        dim_size -= stride_size + kernel_size - 2\n     return dim_size\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#136f1b429201a298173493f83ee8672421ac6e06", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1569 | Contributors (this commit): 6 | Commits (past 90d): 12 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -667,3 +667,9 @@ AvgPool2D = AveragePooling2D\n MaxPool2D = MaxPooling2D\n AvgPool3D = AveragePooling3D\n MaxPool3D = MaxPooling3D\n+GlobalMaxPool1D = GlobalMaxPooling1D\n+GlobalMaxPool2D = GlobalMaxPooling2D\n+GlobalMaxPool3D = GlobalMaxPooling3D\n+GlobalAvgPool1D = GlobalAveragePooling1D\n+GlobalAvgPool2D = GlobalAveragePooling2D\n+GlobalAvgPool3D = GlobalAveragePooling3D\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5b0967a08ff5a277d318506a9c28a31acd04794a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 20 | Churn Cumulative: 118 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,6 +4,26 @@ from keras import applications\n from keras import backend as K\n \n \n+@keras_test\n+def test_resnet50():\n+    model = applications.ResNet50(weights=None)\n+    assert model.output_shape == (None, 1000)\n+\n+\n+@keras_test\n+def test_resnet50_notop():\n+    model = applications.ResNet50(weights=None, include_top=False)\n+    assert model.output_shape == (None, None, None, 2048)\n+\n+\n+@keras_test\n+def test_resnet50_pooling():\n+    model = applications.ResNet50(weights=None,\n+                                  include_top=False,\n+                                  pooling='avg')\n+    assert model.output_shape == (None, 2048)\n+\n+\n @keras_test\n def test_vgg16():\n     model = applications.VGG16(weights=None)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a214f4e64d9e1f09a6ffdcad5e9595d0534e08f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 41 | Churn Cumulative: 6122 | Contributors (this commit): 33 | Commits (past 90d): 19 | Contributors (cumulative): 33 | DMM Complexity: 0.0\n\nDIFF:\n@@ -128,8 +128,9 @@ class Recurrent(Layer):\n             at the level of the first layer\n             (e.g. via the `input_shape` argument)\n \n-    # Input shape\n-        3D tensor with shape `(batch_size, timesteps, input_dim)`.\n+    # Input shapes\n+        3D tensor with shape `(batch_size, timesteps, input_dim)`,\n+        (Optional) 2D tensors with shape `(batch_size, output_dim)`.\n \n     # Output shape\n         - if `return_sequences`: 3D tensor with shape\n@@ -162,6 +163,16 @@ class Recurrent(Layer):\n \n         To reset the states of your model, call `.reset_states()` on either\n         a specific layer, or on your entire model.\n+\n+    # Note on specifying initial states in RNNs\n+        Most RNN layers can be called with either a single tensor, or a list\n+        of tensors.\n+        If the an RNN layer is called with a single tensor, that tensor is\n+        treated as the input, and the initial states are assigned an\n+        appropriate default value.\n+        If an RNN layer is called with a list of tensors, the first element is\n+        treated as the input, and the remaining tensors are treated as the\n+        initial states.\n     \"\"\"\n \n     def __init__(self, return_sequences=False,\n@@ -177,11 +188,13 @@ class Recurrent(Layer):\n         self.unroll = unroll\n         self.implementation = 0\n         self.supports_masking = True\n-        self.input_spec = InputSpec(ndim=3)\n+        self.input_spec = None\n         self.dropout = 0\n         self.recurrent_dropout = 0\n \n     def compute_output_shape(self, input_shape):\n+        if isinstance(input_shape, list):\n+            input_shape = input_shape[0]\n         if self.return_sequences:\n             return (input_shape[0], input_shape[1], self.units)\n         else:\n@@ -215,6 +228,13 @@ class Recurrent(Layer):\n         # input shape: (nbias_samples, time (padded with zeros), input_dim)\n         # note that the .build() method of subclasses MUST define\n         # self.input_spec with a complete input shape.\n+        if isinstance(inputs, list):\n+            initial_states = inputs[1:]\n+            inputs = inputs[0]\n+        elif self.stateful:\n+            initial_states = self.states\n+        else:\n+            initial_states = self.get_initial_states(inputs)\n         input_shape = K.int_shape(inputs)\n         if self.unroll and input_shape[1] is None:\n             raise ValueError('Cannot unroll a RNN if the '\n@@ -228,10 +248,6 @@ class Recurrent(Layer):\n                              '- If using the functional API, specify '\n                              'the time dimension by passing a `shape` '\n                              'or `batch_shape` argument to your Input layer.')\n-        if self.stateful:\n-            initial_states = self.states\n-        else:\n-            initial_states = self.get_initial_states(inputs)\n         constants = self.get_constants(inputs, training=None)\n         preprocessed_input = self.preprocess_input(inputs, training=None)\n         last_output, outputs, states = K.rnn(self.step,\n@@ -354,8 +370,8 @@ class SimpleRNN(Recurrent):\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n \n     def build(self, input_shape):\n-        # TODO: handle variable-length seqs in input_spec\n-        self.input_spec = InputSpec(shape=input_shape)\n+        if isinstance(input_shape, list):\n+            input_shape = input_shape[0]\n         if self.stateful:\n             self.reset_states()\n         else:\n@@ -589,8 +605,8 @@ class GRU(Recurrent):\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n \n     def build(self, input_shape):\n-        # TODO: handle variable-length sequences in input spec.\n-        self.input_spec = InputSpec(shape=input_shape)\n+        if isinstance(input_shape, list):\n+            input_shape = input_shape[0]\n         self.input_dim = input_shape[2]\n \n         if self.stateful:\n@@ -878,7 +894,8 @@ class LSTM(Recurrent):\n         self.recurrent_dropout = min(1., max(0., recurrent_dropout))\n \n     def build(self, input_shape):\n-        self.input_spec = InputSpec(shape=input_shape)\n+        if isinstance(input_shape, list):\n+            input_shape = input_shape[0]\n         self.input_dim = input_shape[2]\n \n         if self.stateful:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
