{"custom_id": "keras#550dea8da84ddd37f3d293437f6aee172d95e156", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 630 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -41,7 +41,7 @@ def get_current_layout_map():\n   return getattr(_LAYOUT_MAP, 'layout_map', None)\n \n \n-class LayoutMap(collections.MutableMapping):\n+class LayoutMap(collections.abc.MutableMapping):\n \n   def __init__(self, mesh=None):\n     \"\"\"A dict like object that maps between string name and dtensor.Layout.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#780d5bc51777e1855b7df121d86f1547a30e9f54", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1321 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -622,8 +622,8 @@ def ResNetRS(\n \n     if include_preprocessing:\n         num_channels = input_shape[bn_axis - 1]\n-        if num_channels == 3:\n         x = layers.Rescaling(scale=1.0 / 255)(x)\n+        if weights == \"imagenet\":\n             x = layers.Normalization(\n                 mean=[0.485, 0.456, 0.406],\n                 variance=[0.229**2, 0.224**2, 0.225**2],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aada299b09e909d435743e5aa9a9d4197ad3620a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 17 | Files Changed: 3 | Hunks: 23 | Methods Changed: 18 | Complexity Δ (Sum/Max): -92/0 | Churn Δ: 66 | Churn Cumulative: 3325 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: 0.5833333333333334\n\nDIFF:\n@@ -659,7 +659,8 @@ class RNN(base_layer.Layer):\n         unroll=self.unroll,\n         input_length=row_lengths if row_lengths is not None else timesteps,\n         time_major=self.time_major,\n-        zero_output_for_mask=self.zero_output_for_mask)\n+        zero_output_for_mask=self.zero_output_for_mask,\n+        return_all_outputs=self.return_sequences)\n \n     if self.stateful:\n       updates = [\n\n@@ -376,7 +376,8 @@ class GRU(DropoutRNNCellMixin, gru_v1.GRU, base_layer.BaseRandomLayer):\n           unroll=self.unroll,\n           input_length=row_lengths if row_lengths is not None else timesteps,\n           time_major=self.time_major,\n-          zero_output_for_mask=self.zero_output_for_mask)\n+          zero_output_for_mask=self.zero_output_for_mask,\n+          return_all_outputs=self.return_sequences)\n       # This is a dummy tensor for testing purpose.\n       runtime = gru_lstm_utils.runtime(gru_lstm_utils.RUNTIME_UNKNOWN)\n     else:\n@@ -461,6 +462,7 @@ class GRU(DropoutRNNCellMixin, gru_v1.GRU, base_layer.BaseRandomLayer):\n       normal_gru_kwargs = gpu_gru_kwargs.copy()\n       normal_gru_kwargs.update({\n           'zero_output_for_mask': self.zero_output_for_mask,\n+          'return_sequences': self.return_sequences,\n       })\n \n       if tf.executing_eagerly():\n@@ -488,7 +490,7 @@ class GRU(DropoutRNNCellMixin, gru_v1.GRU, base_layer.BaseRandomLayer):\n \n def standard_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask,\n                  time_major, go_backwards, sequence_lengths,\n-                 zero_output_for_mask):\n+                 zero_output_for_mask, return_sequences):\n   \"\"\"GRU with standard kernel implementation.\n \n   This implementation can be run on all types of hardware.\n@@ -517,12 +519,18 @@ def standard_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask,\n       input, such as ragged tensors. If the input has a fixed timestep size,\n       this should be None.\n     zero_output_for_mask: Boolean, whether to output zero for masked timestep.\n+    return_sequences: Boolean. If True, all outputs of the process will be\n+      returned, whereas in the False case, only last_output will be kept\n+      during the process, saving the corresponding memory, and\n+      outputs=[last_output] is returned.\n \n   Returns:\n     last_output: output tensor for the last timestep, which has shape\n       [batch, units].\n-    outputs: output tensor for all timesteps, which has shape\n-      [batch, time, units].\n+    outputs: \n+      - If `return_sequences`: output tensor for all timesteps, \n+        which has shape [batch, time, units].\n+      - Else, a tensor equal to last_output\n     state_0: the cell output, which has same shape as init_h.\n     runtime: constant string tensor which indicate real runtime hardware. This\n       value is for testing purpose and should be used by user.\n@@ -565,7 +573,8 @@ def standard_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask,\n       go_backwards=go_backwards,\n       input_length=sequence_lengths\n       if sequence_lengths is not None else timesteps,\n-      zero_output_for_mask=zero_output_for_mask)\n+      zero_output_for_mask=zero_output_for_mask,\n+      return_all_outputs=return_sequences)\n   return last_output, outputs, new_states[0], gru_lstm_utils.runtime(\n       gru_lstm_utils.RUNTIME_CPU)\n \n@@ -660,7 +669,7 @@ def gpu_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask, time_major,\n \n def gru_with_backend_selection(inputs, init_h, kernel, recurrent_kernel, bias,\n                                mask, time_major, go_backwards, sequence_lengths,\n-                               zero_output_for_mask):\n+                               zero_output_for_mask, return_sequences):\n   \"\"\"Call the GRU with optimized backend kernel selection.\n \n   Under the hood, this function will create two TF function, one with the most\n@@ -691,6 +700,10 @@ def gru_with_backend_selection(inputs, init_h, kernel, recurrent_kernel, bias,\n       input, such as ragged tensors. If the input has a fixed timestep size,\n       this should be None.\n     zero_output_for_mask: Boolean, whether to output zero for masked timestep.\n+    return_sequences: Boolean. If True, all outputs of the process will be\n+      returned, whereas in the False case, only last_output will be kept\n+      during the process, saving the corresponding memory, and\n+      outputs=[last_output] is returned.\n \n   Returns:\n     List of output tensors, same as standard_gru.\n@@ -706,11 +719,12 @@ def gru_with_backend_selection(inputs, init_h, kernel, recurrent_kernel, bias,\n       'go_backwards': go_backwards,\n       'sequence_lengths': sequence_lengths,\n       'zero_output_for_mask': zero_output_for_mask,\n+      'return_sequences': return_sequences,\n   }\n \n   def gpu_gru_with_fallback(inputs, init_h, kernel, recurrent_kernel, bias,\n                             mask, time_major, go_backwards, sequence_lengths,\n-                            zero_output_for_mask):\n+                            zero_output_for_mask, return_sequences):\n     \"\"\"Use cuDNN kernel when mask is none or strictly right padded.\"\"\"\n     if mask is None:\n       return gpu_gru(\n@@ -747,7 +761,8 @@ def gru_with_backend_selection(inputs, init_h, kernel, recurrent_kernel, bias,\n           time_major=time_major,\n           go_backwards=go_backwards,\n           sequence_lengths=sequence_lengths,\n-          zero_output_for_mask=zero_output_for_mask)\n+          zero_output_for_mask=zero_output_for_mask,\n+          return_sequences=return_sequences)\n \n     return tf.cond(\n         gru_lstm_utils.is_cudnn_supported_inputs(mask, time_major),\n\n@@ -370,7 +370,8 @@ class LSTM(DropoutRNNCellMixin, lstm_v1.LSTM, base_layer.BaseRandomLayer):\n           unroll=self.unroll,\n           input_length=row_lengths if row_lengths is not None else timesteps,\n           time_major=self.time_major,\n-          zero_output_for_mask=self.zero_output_for_mask)\n+          zero_output_for_mask=self.zero_output_for_mask,\n+          return_all_outputs=self.return_sequences)\n       runtime = gru_lstm_utils.runtime(gru_lstm_utils.RUNTIME_UNKNOWN)\n     else:\n       # Use the new defun approach for backend implementation swap.\n@@ -435,6 +436,7 @@ class LSTM(DropoutRNNCellMixin, lstm_v1.LSTM, base_layer.BaseRandomLayer):\n         normal_lstm_kwargs = gpu_lstm_kwargs.copy()\n         normal_lstm_kwargs.update({\n             'zero_output_for_mask': self.zero_output_for_mask,\n+            'return_sequences': self.return_sequences,\n         })\n \n         if tf.executing_eagerly():\n@@ -483,7 +485,7 @@ class LSTM(DropoutRNNCellMixin, lstm_v1.LSTM, base_layer.BaseRandomLayer):\n \n def standard_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias,\n                   mask, time_major, go_backwards, sequence_lengths,\n-                  zero_output_for_mask):\n+                  zero_output_for_mask, return_sequences):\n   \"\"\"LSTM with standard kernel implementation.\n \n   This implementation can be run on all types for hardware.\n@@ -518,12 +520,18 @@ def standard_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias,\n       input, such as ragged tensors. If the input has a fixed timestep size,\n       this should be None.\n     zero_output_for_mask: Boolean, whether to output zero for masked timestep.\n+    return_sequences: Boolean. If True, all outputs of the process will be\n+      returned, whereas in the False case, only last_output will be kept\n+      during the process, saving the corresponding memory, and\n+      outputs=[last_output] is returned.\n \n   Returns:\n     last_output: output tensor for the last timestep, which has shape\n       [batch, units].\n-    outputs: output tensor for all timesteps, which has shape\n-      [batch, time, units].\n+    outputs: \n+      - If `return_sequences`: output tensor for all timesteps, \n+        which has shape [batch, time, units].\n+      - Else, a tensor equal to last_output\n     state_0: the cell output, which has same shape as init_h.\n     state_1: the cell hidden state, which has same shape as init_c.\n     runtime: constant string tensor which indicate real runtime hardware. This\n@@ -561,7 +569,8 @@ def standard_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias,\n       go_backwards=go_backwards,\n       input_length=(sequence_lengths\n                     if sequence_lengths is not None else timesteps),\n-      zero_output_for_mask=zero_output_for_mask)\n+      zero_output_for_mask=zero_output_for_mask,\n+      return_all_outputs=return_sequences)\n   return (last_output, outputs, new_states[0], new_states[1],\n           gru_lstm_utils.runtime(gru_lstm_utils.RUNTIME_CPU))\n \n@@ -696,7 +705,7 @@ def gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask,\n def lstm_with_backend_selection(inputs, init_h, init_c, kernel,\n                                 recurrent_kernel, bias, mask, time_major,\n                                 go_backwards, sequence_lengths,\n-                                zero_output_for_mask):\n+                                zero_output_for_mask, return_sequences):\n   \"\"\"Call the LSTM with optimized backend kernel selection.\n \n   Under the hood, this function will create two TF function, one with the most\n@@ -728,6 +737,10 @@ def lstm_with_backend_selection(inputs, init_h, init_c, kernel,\n       input, such as ragged tensors. If the input has a fixed timestep size,\n       this should be None.\n     zero_output_for_mask: Boolean, whether to output zero for masked timestep.\n+    return_sequences: Boolean. If True, all outputs of the process will be\n+      returned, whereas in the False case, only last_output will be kept\n+      during the process, saving the corresponding memory, and\n+      outputs=[last_output] is returned.\n \n   Returns:\n     List of output tensors, same as standard_lstm.\n@@ -744,11 +757,13 @@ def lstm_with_backend_selection(inputs, init_h, init_c, kernel,\n       'go_backwards': go_backwards,\n       'sequence_lengths': sequence_lengths,\n       'zero_output_for_mask': zero_output_for_mask,\n+      'return_sequences': return_sequences,\n   }\n \n   def gpu_lstm_with_fallback(inputs, init_h, init_c, kernel, recurrent_kernel,\n                              bias, mask, time_major, go_backwards,\n-                             sequence_lengths, zero_output_for_mask):\n+                             sequence_lengths, zero_output_for_mask,\n+                             return_sequences):\n     \"\"\"Use cuDNN kernel when mask is none or strictly right padded.\"\"\"\n     if mask is None:\n       return gpu_lstm(\n@@ -788,7 +803,8 @@ def lstm_with_backend_selection(inputs, init_h, init_c, kernel,\n           time_major=time_major,\n           go_backwards=go_backwards,\n           sequence_lengths=sequence_lengths,\n-          zero_output_for_mask=zero_output_for_mask)\n+          zero_output_for_mask=zero_output_for_mask,\n+          return_sequences=return_sequences)\n \n     return tf.cond(\n         gru_lstm_utils.is_cudnn_supported_inputs(mask, time_major),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#36f99a3871da38d30003255729193c1294ad8bf8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 21 | Churn Cumulative: 6385 | Contributors (this commit): 8 | Commits (past 90d): 24 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -345,12 +345,12 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n \n   def call(self, inputs, training=True):\n     if training:\n-      inputs = self._format_inputs(inputs)\n+      inputs, is_dict = self._format_inputs(inputs)\n       images = inputs['images']\n       if images.shape.rank == 3:\n-        return self._format_output(self._augment(inputs))\n+        return self._format_output(self._augment(inputs), is_dict)\n       elif images.shape.rank == 4:\n-        return self._format_output(self._batch_augment(inputs))\n+        return self._format_output(self._batch_augment(inputs), is_dict)\n       else:\n         raise ValueError('Image augmentation layers are expecting inputs to be '\n                          'rank 3 (HWC) or 4D (NHWC) tensors. Got shape: '\n@@ -382,16 +382,16 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n   def _format_inputs(self, inputs):\n     if tf.is_tensor(inputs):\n       # single image input tensor\n-      return {'images': inputs}\n+      return {'images': inputs}, False\n     elif isinstance(inputs, dict):\n       # TODO(scottzhu): Check if it only contains the valid keys\n-      return inputs\n+      return inputs, True\n     else:\n       raise ValueError(\n           f'Expect the inputs to be image tensor or dict. Got {inputs}')\n \n-  def _format_output(self, output):\n-    if isinstance(output, dict) and len(output) == 1:\n+  def _format_output(self, output, is_dict):\n+    if not is_dict:\n       return output['images']\n     else:\n       return output\n\n@@ -1895,6 +1895,13 @@ class BaseImageAugmentationLayerTest(test_combinations.TestCase):\n \n     self.assertAllClose(image + 2.0, output)\n \n+  def test_augment_dict_return_type(self):\n+    add_layer = RandomAddLayer(fixed_value=2.0)\n+    image = np.random.random(size=(8, 8, 3)).astype('float32')\n+    output = add_layer({'images': image})\n+\n+    self.assertIsInstance(output, dict)\n+\n   def test_augment_batch_images(self):\n     add_layer = RandomAddLayer()\n     images = np.random.random(size=(2, 8, 8, 3)).astype('float32')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#72721ce5b68215c7ff04d474bc042849deabcef9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 7800 | Contributors (this commit): 12 | Commits (past 90d): 5 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -4431,19 +4431,18 @@ def rnn(step_function,\n       zero_output_for_mask: Boolean. If True, the output for masked timestep\n           will be zeros, whereas in the False case, output from previous\n           timestep is returned.\n-      return_all_outputs: Boolean. If True, all outputs of the process will be\n-          returned, whereas in the False case, only last_output will be kept\n-          during the process, saving the corresponding memory, and\n-          outputs=[last_output] is returned.\n+      return_all_outputs: Boolean. If True, return the recurrent outputs for all\n+          timesteps in the sequence. If False, only return the output for the \n+          last timestep (which consumes less memory).\n \n   Returns:\n       A tuple, `(last_output, outputs, new_states)`.\n           last_output: the latest output of the rnn, of shape `(samples, ...)`\n           outputs:\n-              - If `return_all_outputs`: a tensor with shape\n+              - If `return_all_outputs=True`: a tensor with shape\n                 `(samples, time, ...)` where each entry `outputs[s, t]` is the\n                 output of the step function at time `t` for sample `s`\n-              - Else, a tensor equal to last_output\n+              - Else, a tensor equal to `last_output`\n           new_states: list of tensors, latest states returned by\n               the step function, of shape `(samples, ...)`.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b7e75edad2331152e1c262e95c01522d77798ea2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 20 | Files Changed: 2 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 36 | Churn Cumulative: 2497 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -519,18 +519,17 @@ def standard_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask,\n       input, such as ragged tensors. If the input has a fixed timestep size,\n       this should be None.\n     zero_output_for_mask: Boolean, whether to output zero for masked timestep.\n-    return_sequences: Boolean. If True, all outputs of the process will be\n-      returned, whereas in the False case, only last_output will be kept\n-      during the process, saving the corresponding memory, and\n-      outputs=[last_output] is returned.\n+    return_sequences: Boolean. If True, return the recurrent outputs for all\n+      timesteps in the sequence. If False, only return the output for the \n+      last timestep (which consumes less memory).\n \n   Returns:\n     last_output: output tensor for the last timestep, which has shape\n       [batch, units].\n     outputs: \n-      - If `return_sequences`: output tensor for all timesteps, \n+      - If `return_sequences=True`: output tensor for all timesteps, \n         which has shape [batch, time, units].\n-      - Else, a tensor equal to last_output\n+      - Else, a tensor equal to `last_output`\n     state_0: the cell output, which has same shape as init_h.\n     runtime: constant string tensor which indicate real runtime hardware. This\n       value is for testing purpose and should be used by user.\n@@ -700,10 +699,9 @@ def gru_with_backend_selection(inputs, init_h, kernel, recurrent_kernel, bias,\n       input, such as ragged tensors. If the input has a fixed timestep size,\n       this should be None.\n     zero_output_for_mask: Boolean, whether to output zero for masked timestep.\n-    return_sequences: Boolean. If True, all outputs of the process will be\n-      returned, whereas in the False case, only last_output will be kept\n-      during the process, saving the corresponding memory, and\n-      outputs=[last_output] is returned.\n+    return_sequences: Boolean. If True, return the recurrent outputs for all\n+      timesteps in the sequence. If False, only return the output for the \n+      last timestep (which consumes less memory).\n \n   Returns:\n     List of output tensors, same as standard_gru.\n\n@@ -520,18 +520,17 @@ def standard_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias,\n       input, such as ragged tensors. If the input has a fixed timestep size,\n       this should be None.\n     zero_output_for_mask: Boolean, whether to output zero for masked timestep.\n-    return_sequences: Boolean. If True, all outputs of the process will be\n-      returned, whereas in the False case, only last_output will be kept\n-      during the process, saving the corresponding memory, and\n-      outputs=[last_output] is returned.\n+    return_sequences: Boolean. If True, return the recurrent outputs for all\n+      timesteps in the sequence. If False, only return the output for the \n+      last timestep (which consumes less memory).\n \n   Returns:\n     last_output: output tensor for the last timestep, which has shape\n       [batch, units].\n     outputs: \n-      - If `return_sequences`: output tensor for all timesteps, \n+      - If `return_sequences=True`: output tensor for all timesteps, \n         which has shape [batch, time, units].\n-      - Else, a tensor equal to last_output\n+      - Else, a tensor equal to `last_output`\n     state_0: the cell output, which has same shape as init_h.\n     state_1: the cell hidden state, which has same shape as init_c.\n     runtime: constant string tensor which indicate real runtime hardware. This\n@@ -737,10 +736,9 @@ def lstm_with_backend_selection(inputs, init_h, init_c, kernel,\n       input, such as ragged tensors. If the input has a fixed timestep size,\n       this should be None.\n     zero_output_for_mask: Boolean, whether to output zero for masked timestep.\n-    return_sequences: Boolean. If True, all outputs of the process will be\n-      returned, whereas in the False case, only last_output will be kept\n-      during the process, saving the corresponding memory, and\n-      outputs=[last_output] is returned.\n+    return_sequences: Boolean. If True, return the recurrent outputs for all\n+      timesteps in the sequence. If False, only return the output for the \n+      last timestep (which consumes less memory).\n \n   Returns:\n     List of output tensors, same as standard_lstm.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#18935312a1a08f1f3bdcb50b402256bd7289b66a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 219 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -92,7 +92,7 @@ class Concatenate(_Merge):\n   @tf_utils.shape_type_conversion\n   def build(self, input_shape):\n     # Used purely for shape validation.\n-    if not isinstance(input_shape[0], tuple) or len(input_shape) < 1:\n+    if len(input_shape) < 1 or not isinstance(input_shape[0], tuple):\n       raise ValueError(\n           'A `Concatenate` layer should be called on a list of '\n           f'at least 1 input. Received: input_shape={input_shape}')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7b0a05f5013f03b4897e3aac462352fa6b0e26c6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1598 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -451,7 +451,7 @@ class TextVectorization(base_preprocessing_layer.PreprocessingLayer):\n     \"\"\"Gets the current size of the layer's vocabulary.\n \n     Returns:\n-      The integer size of the voculary, including optional mask and oov indices.\n+      The integer size of the vocabulary, including optional mask and oov indices.\n     \"\"\"\n     return self._lookup_layer.vocabulary_size()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dbc4526978865b372e794accbaa7c584f9c86a0f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 74 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 11 | Methods Changed: 11 | Complexity Δ (Sum/Max): 9/6 | Churn Δ: 79 | Churn Cumulative: 6464 | Contributors (this commit): 8 | Commits (past 90d): 26 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -260,10 +260,22 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n   mode, unpack the inputs, forward to the correct function, and pack the output\n   back to the same structure as the inputs.\n \n+  By default the `call()` method leverages the `tf.vectorized_map()` function.\n+  Auto-vectorization can be disabled by setting `self.auto_vectorize = False`\n+  in your `__init__()` method.  When disabled, `call()` instead relies\n+  on `tf.map_fn()`. For example:\n+\n+  ```python\n+  class SubclassLayer(BaseImageAugmentationLayer):\n+    def __init__(self):\n+      super().__init__()\n+      self.auto_vectorize = False\n+  ```\n+\n   Example:\n \n   ```python\n-  class RandomContrast(BaseAugmentationLayer):\n+  class RandomContrast(BaseImageAugmentationLayer):\n \n     def __init__(self, factor=(0.5, 1.5), **kwargs):\n       super().__init__(**kwargs)\n@@ -272,7 +284,7 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     def augment_image(self, image, transformation=None):\n       random_factor = tf.random.uniform([], self._factor[0], self._factor[1])\n       mean = tf.math.reduced_mean(inputs, axis=-1, keep_dim=True)\n-      output = (inputs - mean) * random_factor + mean\n+      return (inputs - mean) * random_factor + mean\n   ```\n \n   Note that since the randomness is also a common functionnality, this layer\n@@ -285,6 +297,35 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     super().__init__(seed=seed, **kwargs)\n     self.rate = rate\n \n+  @property\n+  def auto_vectorize(self):\n+    \"\"\"Control whether automatic vectorization occurs.\n+\n+    By default the `call()` method leverages the `tf.vectorized_map()` function.\n+    Auto-vectorization can be disabled by setting `self.auto_vectorize = False`\n+    in your `__init__()` method.  When disabled, `call()` instead relies\n+    on `tf.map_fn()`. For example:\n+\n+    ```python\n+    class SubclassLayer(BaseImageAugmentationLayer):\n+      def __init__(self):\n+        super().__init__()\n+        self.auto_vectorize = False\n+    ```\n+    \"\"\"\n+    return getattr(self, '_auto_vectorize', True)\n+\n+  @auto_vectorize.setter\n+  def auto_vectorize(self, auto_vectorize):\n+    self._auto_vectorize = auto_vectorize\n+\n+  @property\n+  def _map_fn(self):\n+    if self.auto_vectorize:\n+      return tf.vectorized_map\n+    else:\n+      return tf.map_fn\n+\n   @doc_controls.for_subclass_implementers\n   def augment_image(self, image, transformation=None):\n     \"\"\"Augment a single image during training.\n@@ -344,6 +385,7 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     return None\n \n   def call(self, inputs, training=True):\n+    inputs = self._ensure_inputs_are_compute_dtype(inputs)\n     if training:\n       inputs, is_dict = self._format_inputs(inputs)\n       images = inputs['images']\n@@ -363,8 +405,6 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     image = inputs.get('images', None)\n     label = inputs.get('labels', None)\n     bounding_box = inputs.get('bounding_boxes', None)\n-\n-    image = utils.ensure_tensor(image, self.compute_dtype)\n     image = self.augment_image(image, transformation=transformation)\n     result = {'images': image}\n     if label is not None:\n@@ -377,7 +417,7 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     return result\n \n   def _batch_augment(self, inputs):\n-    return tf.map_fn(self._augment, inputs)\n+    return self._map_fn(self._augment, inputs)\n \n   def _format_inputs(self, inputs):\n     if tf.is_tensor(inputs):\n@@ -396,6 +436,14 @@ class BaseImageAugmentationLayer(base_layer.BaseRandomLayer):\n     else:\n       return output\n \n+  def _ensure_inputs_are_compute_dtype(self, inputs):\n+    if isinstance(inputs, dict):\n+      inputs['images'] = utils.ensure_tensor(inputs['images'],\n+                                             self.compute_dtype)\n+    else:\n+      inputs = utils.ensure_tensor(inputs, self.compute_dtype)\n+    return inputs\n+\n \n @keras_export('keras.layers.RandomCrop',\n               'keras.layers.experimental.preprocessing.RandomCrop',\n@@ -601,6 +649,7 @@ class RandomFlip(BaseImageAugmentationLayer):\n       raise ValueError('RandomFlip layer {name} received an unknown mode '\n                        'argument {arg}'.format(name=self.name, arg=mode))\n     self.seed = seed\n+    self.auto_vectorize = False\n \n   def augment_image(self, image, transformation=None):\n     flipped_outputs = image\n\n@@ -1885,6 +1885,13 @@ class RandomAddLayer(image_preprocessing.BaseImageAugmentationLayer):\n     return label + transformation\n \n \n+class VectorizeDisabledLayer(image_preprocessing.BaseImageAugmentationLayer):\n+\n+  def __init__(self, **kwargs):\n+    self.auto_vectorize = False\n+    super().__init__(**kwargs)\n+\n+\n @test_combinations.run_all_keras_modes(always_skip_v1=True)\n class BaseImageAugmentationLayerTest(test_combinations.TestCase):\n \n@@ -1902,6 +1909,19 @@ class BaseImageAugmentationLayerTest(test_combinations.TestCase):\n \n     self.assertIsInstance(output, dict)\n \n+  def test_auto_vectorize_disabled(self):\n+    vectorize_disabled_layer = VectorizeDisabledLayer()\n+    self.assertFalse(vectorize_disabled_layer.auto_vectorize)\n+    self.assertEqual(vectorize_disabled_layer._map_fn, tf.map_fn)\n+\n+  @test_utils.run_v2_only\n+  def test_augment_casts_dtypes(self):\n+    add_layer = RandomAddLayer(fixed_value=2.0)\n+    images = tf.ones((2, 8, 8, 3), dtype='uint8')\n+    output = add_layer(images)\n+\n+    self.assertAllClose(tf.ones((2, 8, 8, 3), dtype='float32') * 3.0, output)\n+\n   def test_augment_batch_images(self):\n     add_layer = RandomAddLayer()\n     images = np.random.random(size=(2, 8, 8, 3)).astype('float32')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#14f71177ad28a60a4ea41775b2ac159d3688c792", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 266 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 16/13 | Churn Δ: 266 | Churn Cumulative: 329 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: 0.5851851851851851\n\nDIFF:\n@@ -0,0 +1,242 @@\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"FTRL optimizer implementation.\"\"\"\n+\n+from keras.optimizers.optimizer_experimental import optimizer\n+from keras.utils import generic_utils\n+import tensorflow.compat.v2 as tf\n+\n+\n+# pylint: disable=g-classes-have-attributes\n+@generic_utils.register_keras_serializable()\n+class Ftrl(optimizer.Optimizer):\n+  r\"\"\"Optimizer that implements the FTRL algorithm.\n+\n+  \"Follow The Regularized Leader\" (FTRL) is an optimization algorithm developed\n+  at Google for click-through rate prediction in the early 2010s. It is most\n+  suitable for shallow models with large and sparse feature spaces.\n+  The algorithm is described by\n+  [McMahan et al., 2013](https://research.google.com/pubs/archive/41159.pdf).\n+  The Keras version has support for both online L2 regularization\n+  (the L2 regularization described in the paper\n+  above) and shrinkage-type L2 regularization\n+  (which is the addition of an L2 penalty to the loss function).\n+\n+  Initialization:\n+\n+  ```python\n+  n = 0\n+  sigma = 0\n+  z = 0\n+  ```\n+\n+  Update rule for one variable `w`:\n+\n+  ```python\n+  prev_n = n\n+  n = n + g ** 2\n+  sigma = (n ** -lr_power - prev_n ** -lr_power) / lr\n+  z = z + g - sigma * w\n+  if abs(z) < lambda_1:\n+    w = 0\n+  else:\n+    w = (sgn(z) * lambda_1 - z) / ((beta + sqrt(n)) / alpha + lambda_2)\n+  ```\n+\n+  Notation:\n+\n+  - `lr` is the learning rate\n+  - `g` is the gradient for the variable\n+  - `lambda_1` is the L1 regularization strength\n+  - `lambda_2` is the L2 regularization strength\n+  - `lr_power` is the power to scale n.\n+\n+  Check the documentation for the `l2_shrinkage_regularization_strength`\n+  parameter for more details when shrinkage is enabled, in which case gradient\n+  is replaced with a gradient with shrinkage.\n+\n+  Args:\n+    learning_rate: A `Tensor`, floating point value, a schedule that is a\n+      `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable that\n+      takes no arguments and returns the actual value to use. The learning rate.\n+      Defaults to 0.001.\n+    learning_rate_power: A float value, must be less or equal to zero. Controls\n+      how the learning rate decreases during training. Use zero for a fixed\n+      learning rate.\n+    initial_accumulator_value: The starting value for accumulators. Only zero or\n+      positive values are allowed.\n+    l1_regularization_strength: A float value, must be greater than or equal to\n+      zero. Defaults to 0.0.\n+    l2_regularization_strength: A float value, must be greater than or equal to\n+      zero. Defaults to 0.0.\n+    l2_shrinkage_regularization_strength: A float value, must be greater than or\n+      equal to zero. This differs from L2 above in that the L2 above is a\n+      stabilization penalty, whereas this L2 shrinkage is a magnitude penalty.\n+      When input is sparse shrinkage will only happen on the active weights.\n+    beta: A float value, representing the beta value from the paper. Defaults to\n+      0.0.\n+    clipnorm: see the `clipnorm` argument of `optimizer_experimental.Optimizer`.\n+    clipvalue: see the `clipvalue` argument of\n+      `optimizer_experimental.Optimizer`.\n+    global_clipnorm: see the `global_clipnorm` argument of\n+      `optimizer_experimental.Optimizer`.\n+    use_ema: see the `use_ema` argument of `optimizer_experimental.Optimizer`.\n+    ema_momentum: see the `ema_momentum` argument of\n+      `optimizer_experimental.Optimizer`.\n+    ema_overwrite_frequency: see the `ema_overwrite_frequency` argument of\n+      `optimizer_experimental.Optimizer`.\n+    jit_compile: see the `jit_compile` argument of\n+      `optimizer_experimental.Optimizer`.\n+    name: Optional name prefix for the operations created when applying\n+      gradients. Defaults to `\"Ftrl\"`.\n+    **kwargs: see the `**kwargs` argument of `optimizer_experimental.Optimizer`.\n+  \"\"\"\n+\n+  def __init__(self,\n+               learning_rate=0.001,\n+               learning_rate_power=-0.5,\n+               initial_accumulator_value=0.1,\n+               l1_regularization_strength=0.0,\n+               l2_regularization_strength=0.0,\n+               l2_shrinkage_regularization_strength=0.0,\n+               beta=0.0,\n+               clipnorm=None,\n+               clipvalue=None,\n+               global_clipnorm=None,\n+               use_ema=False,\n+               ema_momentum=0.99,\n+               ema_overwrite_frequency=None,\n+               jit_compile=False,\n+               name='Ftrl',\n+               **kwargs):\n+    super().__init__(\n+        name=name,\n+        clipnorm=clipnorm,\n+        clipvalue=clipvalue,\n+        global_clipnorm=global_clipnorm,\n+        use_ema=use_ema,\n+        ema_momentum=ema_momentum,\n+        ema_overwrite_frequency=ema_overwrite_frequency,\n+        jit_compile=jit_compile,\n+        **kwargs)\n+\n+    if initial_accumulator_value < 0.0:\n+      raise ValueError(\n+          '`initial_accumulator_value` needs to be positive or zero. Received: '\n+          f'initial_accumulator_value={initial_accumulator_value}.')\n+    if learning_rate_power > 0.0:\n+      raise ValueError(\n+          '`learning_rate_power` needs to be negative or zero. Received: '\n+          f'learning_rate_power={learning_rate_power}.')\n+    if l1_regularization_strength < 0.0:\n+      raise ValueError(\n+          '`l1_regularization_strength` needs to be positive or zero. '\n+          f'Received: l1_regularization_strength={l1_regularization_strength}.')\n+    if l2_regularization_strength < 0.0:\n+      raise ValueError(\n+          '`l2_regularization_strength` needs to be positive or zero. '\n+          f'Received: l2_regularization_strength={l2_regularization_strength}.')\n+    if l2_shrinkage_regularization_strength < 0.0:\n+      raise ValueError(\n+          '`l2_shrinkage_regularization_strength` needs to be positive or '\n+          'zero. Received: l2_shrinkage_regularization_strength'\n+          f'={l2_shrinkage_regularization_strength}.')\n+\n+    self._learning_rate = self._build_learning_rate(learning_rate)\n+    self.learning_rate_power = learning_rate_power\n+    self.initial_accumulator_value = initial_accumulator_value\n+    self.l1_regularization_strength = l1_regularization_strength\n+    self.l2_regularization_strength = l2_regularization_strength\n+    self.l2_shrinkage_regularization_strength = (\n+        l2_shrinkage_regularization_strength)\n+    self.beta = beta\n+\n+  def build(self, var_list):\n+    \"\"\"Initialize optimizer variables.\n+\n+    Args:\n+      var_list: list of model variables to build Ftrl variables on.\n+    \"\"\"\n+    super().build(var_list)\n+    if hasattr(self, '_built') and self._built:\n+      return\n+    self._accumulators = []\n+    self._linears = []\n+    for var in var_list:\n+      self._accumulators.append(\n+          self.add_variable_from_reference(\n+              model_variable=var,\n+              variable_name='accumulator',\n+              initial_value=tf.cast(\n+                  tf.fill(dims=var.shape, value=self.initial_accumulator_value),\n+                  dtype=var.dtype)))\n+      self._linears.append(\n+          self.add_variable_from_reference(\n+              model_variable=var, variable_name='linear'))\n+    self._built = True\n+\n+  def update_step(self, gradient, variable):\n+    \"\"\"Update step given gradient and the associated model variable.\"\"\"\n+    if self._var_key(variable) not in self._index_dict:\n+      raise KeyError(f'Optimizer cannot recognize variable {variable.name}, '\n+                     f'this usually means you are calling an optimizer '\n+                     f'previously used on a different model. Please try '\n+                     f'creating a new optimizer instance.')\n+\n+    lr = tf.cast(self.learning_rate, variable.dtype)\n+    var_key = self._var_key(variable)\n+    accum = self._accumulators[self._index_dict[var_key]]\n+    linear = self._linears[self._index_dict[var_key]]\n+\n+    lr_power = self.learning_rate_power\n+    l2_reg = self.l2_regularization_strength\n+    l2_reg = (l2_reg + self.beta / (2. * lr))\n+\n+    # Ftrl optimizer has the same implementation for sparse and dense\n+    # gradients update.\n+    grad_to_use = (\n+        gradient + 2 * self.l2_shrinkage_regularization_strength * variable)\n+    new_accum = accum + tf.pow(gradient, 2)\n+    linear.assign_add(grad_to_use -\n+                      (tf.pow(new_accum, -lr_power) -\n+                       tf.pow(accum, -lr_power)) / lr * variable)\n+    quadratic = tf.pow(new_accum,\n+                       (-lr_power)) / lr + 2 * l2_reg\n+    linear_clipped = tf.clip_by_value(linear,\n+                                      -self.l1_regularization_strength,\n+                                      self.l1_regularization_strength)\n+    variable.assign((linear_clipped - linear) / quadratic)\n+    accum.assign(new_accum)\n+\n+  def get_config(self):\n+    config = super().get_config()\n+\n+    config.update({\n+        'learning_rate':\n+            self._serialize_hyperparameter(self._learning_rate),\n+        'learning_rate_power':\n+            self.learning_rate_power,\n+        'initial_accumulator_value':\n+            self.initial_accumulator_value,\n+        'l1_regularization_strength':\n+            self.l1_regularization_strength,\n+        'l2_regularization_strength':\n+            self.l2_regularization_strength,\n+        'l2_shrinkage_regularization_strength':\n+            self.l2_shrinkage_regularization_strength,\n+        'beta':\n+            self.beta,\n+    })\n+    return config\n\n@@ -15,11 +15,13 @@ from keras.optimizers.optimizer_experimental import adagrad as adagrad_new\n from keras.optimizers.optimizer_experimental import adam as adam_new\n from keras.optimizers.optimizer_experimental import adamax as adamax_new\n from keras.optimizers.optimizer_experimental import adamw as adamw_new\n+from keras.optimizers.optimizer_experimental import ftrl as ftrl_new\n from keras.optimizers.optimizer_experimental import rmsprop as rmsprop_new\n from keras.optimizers.optimizer_experimental import sgd as sgd_new\n from keras.optimizers.optimizer_v2 import adadelta as adadelta_old\n from keras.optimizers.optimizer_v2 import adagrad as adagrad_old\n from keras.optimizers.optimizer_v2 import adam as adam_old\n+from keras.optimizers.optimizer_v2 import ftrl as ftrl_old\n from keras.optimizers.optimizer_v2 import gradient_descent as sgd_old\n from keras.optimizers.optimizer_v2 import rmsprop as rmsprop_old\n from keras.utils import losses_utils\n@@ -54,6 +56,8 @@ adamax_new_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentaladamax\", lambda: adamax_new.Adamax(0.002))\n adamw_new_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentaladamw\", lambda: adamw_new.AdamW(0.002, weight_decay=0.004))\n+ftrl_new_fn = tf.__internal__.test.combinations.NamedObject(\n+    \"experimentalftrl\", lambda: ftrl_new.Ftrl(0.002))\n rmsprop_new_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentalrmsprop\", lambda: rmsprop_new.RMSprop(0.002))\n sgd_new_fn = tf.__internal__.test.combinations.NamedObject(\n@@ -69,6 +73,7 @@ OPTIMIZER_FN = [\n     adam_new_fn,\n     adamax_new_fn,\n     adamw_new_fn,\n+    ftrl_new_fn,\n     rmsprop_new_fn,\n     sgd_new_fn,\n ]\n@@ -299,6 +304,22 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(loaded_optimizer.learning_rate, 0.002)\n     self.assertEqual(loaded_optimizer.clipnorm, 0.1)\n \n+  @parameterized.product(optimizer_fn=OPTIMIZER_FN)\n+  def testSparseGradientsWorkAsExpected(self, optimizer_fn):\n+    optimizer_1 = optimizer_fn()\n+    optimizer_2 = optimizer_fn()\n+    x1 = tf.Variable(np.ones([5]), dtype=tf.float64)\n+    x2 = tf.Variable(np.ones([5]), dtype=tf.float64)\n+    grads = tf.convert_to_tensor([0, 1., 1.5, 0, 0], dtype=tf.float64)\n+    sparse_grads = tf.IndexedSlices(\n+        tf.convert_to_tensor([1., 1.5], dtype=tf.float64),\n+        [1, 2],\n+        dense_shape=[len(grads)])\n+    for _ in range(5):\n+      optimizer_1.apply_gradients(zip([grads], [x1]))\n+      optimizer_2.apply_gradients(zip([sparse_grads], [x2]))\n+      self.assertAllClose(x1, x2)\n+\n \n class OptimizerRegressionTest(tf.test.TestCase, parameterized.TestCase):\n   \"\"\"Test optimizer outputs the same numerical results as optimizer_v2.\"\"\"\n@@ -332,6 +353,9 @@ class OptimizerRegressionTest(tf.test.TestCase, parameterized.TestCase):\n   def testAdagrad(self):\n     self._compare_numerical(adagrad_old.Adagrad(), adagrad_new.Adagrad())\n \n+  def testFtrl(self):\n+    self._compare_numerical(ftrl_old.Ftrl(), ftrl_new.Ftrl())\n+\n   def testRMSprop(self):\n     self._compare_numerical(rmsprop_old.RMSprop(), rmsprop_new.RMSprop())\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c503d08b2ceac24e471a0739130093b66d3d0819", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 25 | Files Changed: 1 | Hunks: 16 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 54 | Churn Cumulative: 1375 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,6 +20,7 @@ Reference:\n - [Revisiting ResNets: Improved Training and Scaling Strategies](\n     https://arxiv.org/pdf/2103.07579.pdf)\n \"\"\"\n+from cgi import print_directory\n import sys\n from keras import backend\n from keras.utils import data_utils\n@@ -186,6 +187,7 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n             `classifier_activation=None` to return the logits of the \"top\" layer.\n         include_preprocessing: Boolean, whether to include the preprocessing layer\n             (`Rescaling`) at the bottom of the network. Defaults to `True`.\n+            Note: if weights are 'imagenet' then Input image is normalized by ImageNet mean and standard deviation.\n \n     Returns:\n         A `keras.Model` instance.\n@@ -520,30 +522,30 @@ def ResNetRS(\n ):\n     \"\"\"Build Resnet-RS model, given provided parameters.\n \n-    Parameters:\n-        :param depth: Depth of ResNet network.\n-        :param dropout_rate: dropout rate before final classifier layer.\n-        :param bn_momentum: Momentum parameter for Batch Normalization layers.\n-        :param bn_epsilon: Epsilon parameter for Batch Normalization layers.\n-        :param activation: activation function.\n-        :param block_args: list of dicts, parameters to construct block modules.\n-        :param se_ratio: Squeeze and Excitation layer ratio.\n-        :param model_name: name of the model.\n-        :param drop_connect_rate: dropout rate at skip connections.\n-        :param include_top: whether to include the fully-connected layer at the top of\n+    Args:\n+        depth: Depth of ResNet network.\n+        dropout_rate: dropout rate before final classifier layer.\n+        bn_momentum: Momentum parameter for Batch Normalization layers.\n+        bn_epsilon: Epsilon parameter for Batch Normalization layers.\n+        activation: activation function.\n+        block_args: list of dicts, parameters to construct block modules.\n+        se_ratio: Squeeze and Excitation layer ratio.\n+        model_name: name of the model.\n+        drop_connect_rate: dropout rate at skip connections.\n+        include_top: whether to include the fully-connected layer at the top of\n         the network.\n-        :param weights: one of `None` (random initialization), `'imagenet'`\n+        weights: one of `None` (random initialization), `'imagenet'`\n             (pre-training on ImageNet), or the path to the weights file to be loaded.\n             Note: one model can have multiple imagenet variants depending on\n             input shape it was trained with. For input_shape 224x224 pass\n             `imagenet-i224` as argument. By default, highest input shape weights are\n             downloaded.\n-        :param input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to\n+        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to\n             use as image input for the model.\n-        :param input_shape: optional shape tuple. It should have exactly 3 inputs\n+        input_shape: optional shape tuple. It should have exactly 3 inputs\n             channels, and width and height should be no smaller than 32.\n             E.g. (200, 200, 3) would be one valid value.\n-        :param  pooling: optional pooling mode for feature extraction when `include_top`\n+        pooling: optional pooling mode for feature extraction when `include_top`\n             is `False`.\n             - `None` means that the output of the model will be\n                 the 4D tensor output of the\n@@ -554,16 +556,19 @@ def ResNetRS(\n                 the output of the model will be a 2D tensor.\n             - `max` means that global max pooling will\n                 be applied.\n-        :param classes: optional number of classes to classify images into, only to be\n+        classes: optional number of classes to classify images into, only to be\n             specified if `include_top` is True, and if no `weights` argument is\n             specified.\n-        :param classifier_activation: A `str` or callable. The activation function to\n+        classifier_activation: A `str` or callable. The activation function to\n             use on the \"top\" layer. Ignored unless `include_top=True`. Set\n             `classifier_activation=None` to return the logits of the \"top\" layer.\n-        :param include_preprocessing: Boolean, whether to include the preprocessing layer\n+        include_preprocessing: Boolean, whether to include the preprocessing layer\n             (`Rescaling`) at the bottom of the network. Defaults to `True`.\n+            Note: if weights are 'imagenet' then Input image is normalized by ImageNet mean and standard deviation.\n+\n     Returns:\n         A `tf.keras.Model` instance.\n+\n     Raises:\n         ValueError: in case of invalid argument for `weights`, or invalid input\n             shape.\n@@ -574,15 +579,13 @@ def ResNetRS(\n     available_weight_variants = DEPTH_TO_WEIGHT_VARIANTS[depth]\n     if weights == \"imagenet\":\n         max_input_shape = max(available_weight_variants)\n-        \"\"\"logging.warning(\n-            f\"Received `imagenet` argument without \"\n-            f\"explicit weights input size. Picking weights trained with \"\n-            f\"biggest available shape: imagenet-i{max_input_shape}\"\n-        )\"\"\"\n+        \"\"\" `imagenet` argument without explicit weights input size. \n+            Picking weights trained with biggest available shape: imagenet-i{max_input_shape}\"\n+        \"\"\"\n         weights = f\"{weights}-i{max_input_shape}\"\n \n     weights_allow_list = [f\"imagenet-i{x}\" for x in available_weight_variants]\n-    if not (weights in {*weights_allow_list, None} or file_io.file_exists_v2(weights)):\n+    if not (weights in {*weights_allow_list, None} or tf.io.gfile.exists(weights)):\n         raise ValueError(\n             \"The `weights` argument should be either \"\n             \"`None` (random initialization), `'imagenet'` \"\n@@ -599,6 +602,7 @@ def ResNetRS(\n             f\"`include_top` as true, `classes` should be 1000. \"\n             f\"Received classes={classes}\"\n         )\n+\n     input_shape = imagenet_utils.obtain_input_shape(\n         input_shape,\n         default_size=224,\n@@ -623,7 +627,7 @@ def ResNetRS(\n     if include_preprocessing:\n         num_channels = input_shape[bn_axis - 1]\n         x = layers.Rescaling(scale=1.0 / 255)(x)\n-        if weights == \"imagenet\":\n+        if num_channels == 3:\n             x = layers.Normalization(\n                 mean=[0.485, 0.456, 0.406],\n                 variance=[0.229**2, 0.224**2, 0.225**2],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
