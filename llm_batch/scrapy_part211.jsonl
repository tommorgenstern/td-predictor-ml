{"custom_id": "scrapy#de98d8d00658181ed46a96835e2ab95f2f6cd457", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 337 | Contributors (this commit): 7 | Commits (past 90d): 15 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -327,12 +327,12 @@ class Http11MockServerTestCase(unittest.TestCase):\n                 request = request.replace(url='http://localhost:8998/xpayload')\n                 yield crawler.crawl(seed=request)\n                 # download_maxsize = 50 is enough for the gzipped response\n-                # See issue https://twistedmatrix.com/trac/ticket/8175\n                 failure = crawler.spider.meta.get('failure')\n                 self.assertTrue(failure == None)\n                 reason = crawler.spider.meta['close_reason']\n                 self.assertTrue(reason, 'finished')\n             else:\n+                # See issue https://twistedmatrix.com/trac/ticket/8175\n                 raise unittest.SkipTest(\"xpayload only enabled for PY2\")\n         else:\n             raise unittest.SkipTest(\"xpayload and payload endpoint only enabled for twisted > 12.3.0\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fd99ef86dfca50dbd36b2c1a022cf30a0720dbea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 4 | Churn Cumulative: 4 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -25,7 +25,9 @@ def _serializable_queue(queue_class, serialize, deserialize):\n def _pickle_serialize(obj):\n     try:\n         return pickle.dumps(obj, protocol=2)\n-    except pickle.PicklingError as e:\n+    # Python>=3.5 raises AttributeError here while\n+    # Python<=3.4 raises pickle.PicklingError\n+    except (pickle.PicklingError, AttributeError) as e:\n         raise ValueError(str(e))\n \n PickleFifoDiskQueue = _serializable_queue(queue.FifoDiskQueue, \\\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ff235fa19ad30f34be7ff435497e19d8e97a8970", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 130 | Files Changed: 3 | Hunks: 6 | Methods Changed: 10 | Complexity Δ (Sum/Max): -8/15 | Churn Δ: 131 | Churn Cumulative: 749 | Contributors (this commit): 10 | Commits (past 90d): 4 | Contributors (cumulative): 15 | DMM Complexity: 0.2696629213483146\n\nDIFF:\n@@ -7,7 +7,6 @@ import pkg_resources\n \n import scrapy\n from scrapy.crawler import CrawlerProcess\n-from scrapy.xlib import lsprofcalltree\n from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n from scrapy.utils.misc import walk_modules\n@@ -144,7 +143,7 @@ def execute(argv=None, settings=None):\n     sys.exit(cmd.exitcode)\n \n def _run_command(cmd, args, opts):\n-    if opts.profile or opts.lsprof:\n+    if opts.profile:\n         _run_command_profiled(cmd, args, opts)\n     else:\n         cmd.run(args, opts)\n@@ -152,17 +151,11 @@ def _run_command(cmd, args, opts):\n def _run_command_profiled(cmd, args, opts):\n     if opts.profile:\n         sys.stderr.write(\"scrapy: writing cProfile stats to %r\\n\" % opts.profile)\n-    if opts.lsprof:\n-        sys.stderr.write(\"scrapy: writing lsprof stats to %r\\n\" % opts.lsprof)\n     loc = locals()\n     p = cProfile.Profile()\n     p.runctx('cmd.run(args, opts)', globals(), loc)\n     if opts.profile:\n         p.dump_stats(opts.profile)\n-    k = lsprofcalltree.KCacheGrind(p)\n-    if opts.lsprof:\n-        with open(opts.lsprof, 'w') as f:\n-            k.output(f)\n \n if __name__ == '__main__':\n     execute()\n\n@@ -65,8 +65,6 @@ class ScrapyCommand(object):\n             help=\"disable logging completely\")\n         group.add_option(\"--profile\", metavar=\"FILE\", default=None,\n             help=\"write python cProfile stats to FILE\")\n-        group.add_option(\"--lsprof\", metavar=\"FILE\", default=None,\n-            help=\"write lsprof profiling stats to FILE\")\n         group.add_option(\"--pidfile\", metavar=\"FILE\",\n             help=\"write process ID to FILE\")\n         group.add_option(\"-s\", \"--set\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n\n@@ -1,120 +0,0 @@\n-# lsprofcalltree.py: lsprof output which is readable by kcachegrind\n-# David Allouche\n-# Jp Calderone & Itamar Shtull-Trauring\n-# Johan Dahlin\n-\n-from __future__ import print_function\n-import optparse\n-import os\n-import sys\n-\n-try:\n-    import cProfile\n-except ImportError:\n-    raise SystemExit(\"This script requires cProfile from Python 2.5\")\n-\n-def label(code):\n-    if isinstance(code, str):\n-        return ('~', 0, code)    # built-in functions ('~' sorts at the end)\n-    else:\n-        return '%s %s:%d' % (code.co_name,\n-                             code.co_filename,\n-                             code.co_firstlineno)\n-\n-class KCacheGrind(object):\n-    def __init__(self, profiler):\n-        self.data = profiler.getstats()\n-        self.out_file = None\n-\n-    def output(self, out_file):\n-        self.out_file = out_file\n-        print('events: Ticks', file=out_file)\n-        self._print_summary()\n-        for entry in self.data:\n-            self._entry(entry)\n-\n-    def _print_summary(self):\n-        max_cost = 0\n-        for entry in self.data:\n-            totaltime = int(entry.totaltime * 1000)\n-            max_cost = max(max_cost, totaltime)\n-        print('summary: %d' % (max_cost,), file=self.out_file)\n-\n-    def _entry(self, entry):\n-        out_file = self.out_file\n-\n-        code = entry.code\n-        #print >> out_file, 'ob=%s' % (code.co_filename,)\n-        if isinstance(code, str):\n-            print('fi=~', file=out_file)\n-        else:\n-            print('fi=%s' % (code.co_filename,), file=out_file)\n-        print('fn=%s' % (label(code),), file=out_file)\n-\n-        inlinetime = int(entry.inlinetime * 1000)\n-        if isinstance(code, str):\n-            print('0 ', inlinetime, file=out_file)\n-        else:\n-            print('%d %d' % (code.co_firstlineno, inlinetime), file=out_file)\n-\n-        # recursive calls are counted in entry.calls\n-        if entry.calls:\n-            calls = entry.calls\n-        else:\n-            calls = []\n-\n-        if isinstance(code, str):\n-            lineno = 0\n-        else:\n-            lineno = code.co_firstlineno\n-\n-        for subentry in calls:\n-            self._subentry(lineno, subentry)\n-        print(file=out_file)\n-\n-    def _subentry(self, lineno, subentry):\n-        out_file = self.out_file\n-        code = subentry.code\n-        #print >> out_file, 'cob=%s' % (code.co_filename,)\n-        print('cfn=%s' % (label(code),), file=out_file)\n-        if isinstance(code, str):\n-            print('cfi=~', file=out_file)\n-            print('calls=%d 0' % (subentry.callcount,), file=out_file)\n-        else:\n-            print('cfi=%s' % (code.co_filename,), file=out_file)\n-            print('calls=%d %d' % (\n-                subentry.callcount, code.co_firstlineno), file=out_file)\n-\n-        totaltime = int(subentry.totaltime * 1000)\n-        print('%d %d' % (lineno, totaltime), file=out_file)\n-\n-def main(args):\n-    usage = \"%s [-o output_file_path] scriptfile [arg] ...\"\n-    parser = optparse.OptionParser(usage=usage % sys.argv[0])\n-    parser.allow_interspersed_args = False\n-    parser.add_option('-o', '--outfile', dest=\"outfile\",\n-                      help=\"Save stats to <outfile>\", default=None)\n-\n-    if not sys.argv[1:]:\n-        parser.print_usage()\n-        sys.exit(2)\n-\n-    options, args = parser.parse_args()\n-\n-    if not options.outfile:\n-        options.outfile = '%s.log' % os.path.basename(args[0])\n-\n-    sys.argv[:] = args\n-\n-    prof = cProfile.Profile()\n-    try:\n-        try:\n-            prof = prof.run('execfile(%r)' % (sys.argv[0],))\n-        except SystemExit:\n-            pass\n-    finally:\n-        kg = KCacheGrind(prof)\n-        kg.output(file(options.outfile, 'w'))\n-\n-if __name__ == '__main__':\n-    sys.exit(main(sys.argv))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bb50c0be2fd7e737f2e2bd772f333e68cd02db06", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 223 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,6 @@ responses in Scrapy.\n \n See documentation in docs/topics/request-response.rst\n \"\"\"\n-import six\n from six.moves.urllib.parse import urljoin\n \n from scrapy.http.headers import Headers\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9c3117a914c27b3acbb2b0300d8938d6e3b49b9e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1246 | Contributors (this commit): 12 | Commits (past 90d): 10 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -186,7 +186,7 @@ class ScrapyAgent(object):\n             _, _, proxyHost, proxyPort, proxyParams = _parse(proxy)\n             scheme = _parse(request.url)[0]\n             proxyHost = to_unicode(proxyHost)\n-            omitConnectTunnel = proxyParams.find(b'noconnect') >= 0\n+            omitConnectTunnel = b'noconnect' in proxyParams\n             if  scheme == b'https' and not omitConnectTunnel:\n                 proxyConf = (proxyHost, proxyPort,\n                              request.headers.get(b'Proxy-Authorization', None))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2b5245839ceaaa256c7edccee351ea79129fa3e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 105 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,6 @@ try:\n     from cStringIO import StringIO as BytesIO\n except ImportError:\n     from io import BytesIO\n-from io import UnsupportedOperation\n from gzip import GzipFile\n \n import six\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#176610f91068cb9663be184094e450de0aac77bc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 13 | Churn Cumulative: 350 | Contributors (this commit): 7 | Commits (past 90d): 16 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -431,8 +431,12 @@ class HttpDownloadHandlerMock(object):\n     def download_request(self, request, spider):\n         return request\n \n+\n class S3AnonTestCase(unittest.TestCase):\n-    skip = 'boto' not in optional_features and 'missing boto library'\n+    try:\n+        import boto\n+    except ImportError:\n+        skip = 'missing boto library'\n \n     def setUp(self):\n         self.s3reqh = S3DownloadHandler(Settings(),\n@@ -448,14 +452,13 @@ class S3AnonTestCase(unittest.TestCase):\n         self.assertEqual(hasattr(self.s3reqh.conn, 'anon'), True)\n         self.assertEqual(self.s3reqh.conn.anon, True)\n \n+\n class S3TestCase(unittest.TestCase):\n     download_handler_cls = S3DownloadHandler\n     try:\n-        # can't instance without settings, but ignore that\n-        download_handler_cls({})\n-    except NotConfigured:\n+        import boto\n+    except ImportError:\n         skip = 'missing boto library'\n-    except KeyError: pass\n \n     # test use same example keys than amazon developer guide\n     # http://s3.amazonaws.com/awsdocs/S3/20060301/s3-dg-20060301.pdf\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f0cf5463c85bd38243ac1539487d7118003a790e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 8 | Churn Cumulative: 1254 | Contributors (this commit): 12 | Commits (past 90d): 11 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -92,11 +92,9 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n \n     def requestTunnel(self, protocol):\n         \"\"\"Asks the proxy to open a tunnel.\"\"\"\n-        tunnelReq = (\n-            b'CONNECT ' +\n-            to_bytes(self._tunneledHost, encoding='ascii') + b':' +\n-            to_bytes(str(self._tunneledPort)) +\n-            b' HTTP/1.1\\r\\n')\n+        tunnelReq = to_bytes(\n+            'CONNECT %s:%s HTTP/1.1\\r\\n' % (\n+                self._tunneledHost, self._tunneledPort), encoding='ascii')\n         if self._proxyAuthHeader:\n             tunnelReq += \\\n                 b'Proxy-Authorization: ' + self._proxyAuthHeader + b'\\r\\n'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#29ff84a7920dd27a7b723f21b68ccc6e2076c08a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 113 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -16,12 +16,12 @@ import six\n #   works here but is only available from Python>=3.3\n # - scrapy does not support Python 3.2\n # - Python 2.7 GzipFile works fine with standard read() + extrabuf\n-if six.PY3:\n-    def read1(gzf, size=-1):\n-        return gzf.read1(size)\n-else:\n+if six.PY2:\n     def read1(gzf, size=-1):\n         return gzf.read(size)\n+else:\n+    def read1(gzf, size=-1):\n+        return gzf.read1(size)\n \n \n def gunzip(data):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a32b59ac7507b9b909edfde4cfa9a9effae27641", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 113 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -55,11 +55,12 @@ class TestSpider(Spider):\n \n     def parse_item(self, response):\n         item = self.item_cls()\n-        m = self.name_re.search(response.body)\n+        body = response.body_as_unicode()\n+        m = self.name_re.search(body)\n         if m:\n             item['name'] = m.group(1)\n         item['url'] = response.url\n-        m = self.price_re.search(response.body)\n+        m = self.price_re.search(body)\n         if m:\n             item['price'] = m.group(1)\n         return item\n@@ -77,8 +78,8 @@ class DictItemsSpider(TestSpider):\n def start_test_site(debug=False):\n     root_dir = os.path.join(tests_datadir, \"test_site\")\n     r = static.File(root_dir)\n-    r.putChild(\"redirect\", util.Redirect(\"/redirected\"))\n-    r.putChild(\"redirected\", static.Data(\"Redirected here\", \"text/plain\"))\n+    r.putChild(b\"redirect\", util.Redirect(b\"/redirected\"))\n+    r.putChild(b\"redirected\", static.Data(b\"Redirected here\", \"text/plain\"))\n \n     port = reactor.listenTCP(0, server.Site(r), interface=\"127.0.0.1\")\n     if debug:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0b08b4bfcfb629aefd4f83cf85e64c74bc1077c3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 119 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -238,12 +238,12 @@ class EngineTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_close_downloader(self):\n-        e = ExecutionEngine(get_crawler(TestSpider), lambda: None)\n+        e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n         yield e.close()\n \n     @defer.inlineCallbacks\n     def test_close_spiders_downloader(self):\n-        e = ExecutionEngine(get_crawler(TestSpider), lambda: None)\n+        e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n         yield e.open_spider(TestSpider(), [])\n         self.assertEqual(len(e.open_spiders), 1)\n         yield e.close()\n@@ -251,7 +251,7 @@ class EngineTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_close_engine_spiders_downloader(self):\n-        e = ExecutionEngine(get_crawler(TestSpider), lambda: None)\n+        e = ExecutionEngine(get_crawler(TestSpider), lambda _: None)\n         yield e.open_spider(TestSpider(), [])\n         e.start()\n         self.assertTrue(e.running)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#47d3c63338c4550341cc7518bcb48d0fc606b67d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 18 | Files Changed: 4 | Hunks: 13 | Methods Changed: 12 | Complexity Δ (Sum/Max): 10/8 | Churn Δ: 38 | Churn Cumulative: 173 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,6 +5,7 @@ from scrapy.commands import ScrapyCommand\n from scrapy.http import Request\n from scrapy.exceptions import UsageError\n from scrapy.utils.spider import spidercls_for_request, DefaultSpider\n+from scrapy.utils.python import to_unicode\n \n class Command(ScrapyCommand):\n \n@@ -30,7 +31,8 @@ class Command(ScrapyCommand):\n     def _print_headers(self, headers, prefix):\n         for key, values in headers.items():\n             for value in values:\n-                print('%s %s: %s' % (prefix, key, value))\n+                print('%s %s: %s' % (\n+                      prefix, to_unicode(key), to_unicode(value)))\n \n     def _print_response(self, response, opts):\n         if opts.headers:\n@@ -38,7 +40,7 @@ class Command(ScrapyCommand):\n             print('>')\n             self._print_headers(response.headers, '<')\n         else:\n-            print(response.body)\n+            print(to_unicode(response.body))\n \n     def run(self, args, opts):\n         if len(args) != 1 or not is_url(args[0]):\n\n@@ -22,11 +22,11 @@ class SiteTest(object):\n \n def test_site():\n     r = resource.Resource()\n-    r.putChild(\"text\", static.Data(\"Works\", \"text/plain\"))\n-    r.putChild(\"html\", static.Data(\"<body><p class='one'>Works</p><p class='two'>World</p></body>\", \"text/html\"))\n-    r.putChild(\"enc-gb18030\", static.Data(\"<p>gb18030 encoding</p>\", \"text/html; charset=gb18030\"))\n-    r.putChild(\"redirect\", util.Redirect(\"/redirected\"))\n-    r.putChild(\"redirected\", static.Data(\"Redirected here\", \"text/plain\"))\n+    r.putChild(b\"text\", static.Data(b\"Works\", \"text/plain\"))\n+    r.putChild(b\"html\", static.Data(b\"<body><p class='one'>Works</p><p class='two'>World</p></body>\", \"text/html\"))\n+    r.putChild(b\"enc-gb18030\", static.Data(b\"<p>gb18030 encoding</p>\", \"text/html; charset=gb18030\"))\n+    r.putChild(b\"redirect\", util.Redirect(b\"/redirected\"))\n+    r.putChild(b\"redirected\", static.Data(b\"Redirected here\", \"text/plain\"))\n     return server.Site(r)\n \n \n\n@@ -12,11 +12,11 @@ class FetchTest(ProcessTest, SiteTest, unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_output(self):\n         _, out, _ = yield self.execute([self.url('/text')])\n-        self.assertEqual(out.strip(), 'Works')\n+        self.assertEqual(out.strip(), b'Works')\n \n     @defer.inlineCallbacks\n     def test_headers(self):\n         _, out, _ = yield self.execute([self.url('/text'), '--headers'])\n-        out = out.replace('\\r', '') # required on win32\n-        assert 'Server: TwistedWeb' in out\n-        assert 'Content-Type: text/plain' in out\n+        out = out.replace(b'\\r', b'') # required on win32\n+        assert b'Server: TwistedWeb' in out, out\n+        assert b'Content-Type: text/plain' in out\n\n@@ -12,38 +12,38 @@ class ShellTest(ProcessTest, SiteTest, unittest.TestCase):\n     @defer.inlineCallbacks\n     def test_empty(self):\n         _, out, _ = yield self.execute(['-c', 'item'])\n-        assert '{}' in out\n+        assert b'{}' in out\n \n     @defer.inlineCallbacks\n     def test_response_body(self):\n         _, out, _ = yield self.execute([self.url('/text'), '-c', 'response.body'])\n-        assert 'Works' in out\n+        assert b'Works' in out\n \n     @defer.inlineCallbacks\n     def test_response_type_text(self):\n         _, out, _ = yield self.execute([self.url('/text'), '-c', 'type(response)'])\n-        assert 'TextResponse' in out\n+        assert b'TextResponse' in out\n \n     @defer.inlineCallbacks\n     def test_response_type_html(self):\n         _, out, _ = yield self.execute([self.url('/html'), '-c', 'type(response)'])\n-        assert 'HtmlResponse' in out\n+        assert b'HtmlResponse' in out\n \n     @defer.inlineCallbacks\n     def test_response_selector_html(self):\n         xpath = 'response.xpath(\"//p[@class=\\'one\\']/text()\").extract()[0]'\n         _, out, _ = yield self.execute([self.url('/html'), '-c', xpath])\n-        self.assertEqual(out.strip(), 'Works')\n+        self.assertEqual(out.strip(), b'Works')\n \n     @defer.inlineCallbacks\n     def test_response_encoding_gb18030(self):\n         _, out, _ = yield self.execute([self.url('/enc-gb18030'), '-c', 'response.encoding'])\n-        self.assertEqual(out.strip(), 'gb18030')\n+        self.assertEqual(out.strip(), b'gb18030')\n \n     @defer.inlineCallbacks\n     def test_redirect(self):\n         _, out, _ = yield self.execute([self.url('/redirect'), '-c', 'response.url'])\n-        assert out.strip().endswith('/redirected')\n+        assert out.strip().endswith(b'/redirected')\n \n     @defer.inlineCallbacks\n     def test_request_replace(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c6d013ec85747fe7a0ec57852667bc4e29ec3e9c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 12 | Churn Cumulative: 111 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,11 +1,11 @@\n from __future__ import print_function\n+import sys, six\n from w3lib.url import is_url\n \n from scrapy.commands import ScrapyCommand\n from scrapy.http import Request\n from scrapy.exceptions import UsageError\n from scrapy.utils.spider import spidercls_for_request, DefaultSpider\n-from scrapy.utils.python import to_unicode\n \n class Command(ScrapyCommand):\n \n@@ -29,10 +29,10 @@ class Command(ScrapyCommand):\n             help=\"print response HTTP headers instead of body\")\n \n     def _print_headers(self, headers, prefix):\n+        prefix = prefix.encode()\n         for key, values in headers.items():\n             for value in values:\n-                print('%s %s: %s' % (\n-                      prefix, to_unicode(key), to_unicode(value)))\n+                self._print_bytes(prefix + b' ' + key + b': ' + value)\n \n     def _print_response(self, response, opts):\n         if opts.headers:\n@@ -40,7 +40,11 @@ class Command(ScrapyCommand):\n             print('>')\n             self._print_headers(response.headers, '<')\n         else:\n-            print(to_unicode(response.body))\n+            self._print_bytes(response.body)\n+\n+    def _print_bytes(self, bytes_):\n+        bytes_writer = sys.stdout if six.PY2 else sys.stdout.buffer\n+        bytes_writer.write(bytes_ + b'\\n')\n \n     def run(self, args, opts):\n         if len(args) != 1 or not is_url(args[0]):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fd24e22442d900170e7a5888b27698c9414b2983", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 116 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -29,16 +29,15 @@ class Command(ScrapyCommand):\n             help=\"print response HTTP headers instead of body\")\n \n     def _print_headers(self, headers, prefix):\n-        prefix = prefix.encode()\n         for key, values in headers.items():\n             for value in values:\n                 self._print_bytes(prefix + b' ' + key + b': ' + value)\n \n     def _print_response(self, response, opts):\n         if opts.headers:\n-            self._print_headers(response.request.headers, '>')\n+            self._print_headers(response.request.headers, b'>')\n             print('>')\n-            self._print_headers(response.headers, '<')\n+            self._print_headers(response.headers, b'<')\n         else:\n             self._print_bytes(response.body)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#659715ecd92c3f39ed0b52509adefb73c49fa56c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 28 | Churn Cumulative: 1074 | Contributors (this commit): 19 | Commits (past 90d): 21 | Contributors (cumulative): 26 | DMM Complexity: 1.0\n\nDIFF:\n@@ -34,8 +34,14 @@ class FormRequest(Request):\n \n     @classmethod\n     def from_response(cls, response, formname=None, formid=None, formnumber=0, formdata=None,\n-                      clickdata=None, dont_click=False, formxpath=None, **kwargs):\n+                      clickdata=None, dont_click=False, formxpath=None, formcss=None, **kwargs):\n+\n         kwargs.setdefault('encoding', response.encoding)\n+\n+        if formcss is not None:\n+            from parsel.csstranslator import HTMLTranslator\n+            formxpath = HTMLTranslator().css_to_xpath(formcss)\n+\n         form = _get_form(response, formname, formid, formnumber, formxpath)\n         formdata = _get_inputs(form, formdata, dont_click, clickdata, response)\n         url = _get_form_url(form, kwargs.pop('url', None))\n\n@@ -846,6 +846,26 @@ class FormRequestTest(RequestTest):\n         req = self.request_class.from_response(response)\n         self.assertEqual(req.url, 'http://b.com/test_form')\n \n+    def test_from_response_css(self):\n+        response = _buildresponse(\n+            \"\"\"<form action=\"post.php\" method=\"POST\">\n+            <input type=\"hidden\" name=\"one\" value=\"1\">\n+            <input type=\"hidden\" name=\"two\" value=\"2\">\n+            </form>\n+            <form action=\"post2.php\" method=\"POST\">\n+            <input type=\"hidden\" name=\"three\" value=\"3\">\n+            <input type=\"hidden\" name=\"four\" value=\"4\">\n+            </form>\"\"\")\n+        r1 = self.request_class.from_response(response, formcss=\"form[action='post.php']\")\n+        fs = _qs(r1)\n+        self.assertEqual(fs[b'one'], [b'1'])\n+\n+        r1 = self.request_class.from_response(response, formcss=\"input[name='four']\")\n+        fs = _qs(r1)\n+        self.assertEqual(fs[b'three'], [b'3'])\n+\n+        self.assertRaises(ValueError, self.request_class.from_response,\n+                          response, formcss=\"input[name='abc']\")\n \n def _buildresponse(body, **kwargs):\n     kwargs.setdefault('body', body)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#80c55f19a143d8938ced81a599100259509567a1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 206 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,15 +16,15 @@ class Root(Resource):\n         total = _getarg(request, 'total', 100, int)\n         show = _getarg(request, 'show', 10, int)\n         nlist = [random.randint(1, total) for _ in range(show)]\n-        request.write(\"<html><head></head><body>\")\n+        request.write(b\"<html><head></head><body>\")\n         args = request.args.copy()\n         for nl in nlist:\n             args['n'] = nl\n             argstr = urlencode(args, doseq=True)\n             request.write(\"<a href='/follow?{0}'>follow {1}</a><br>\"\n-                          .format(argstr, nl))\n-        request.write(\"</body></html>\")\n-        return ''\n+                          .format(argstr, nl).encode('utf8'))\n+        request.write(b\"</body></html>\")\n+        return b''\n \n \n def _getarg(request, name, default=None, type=str):\n\n@@ -276,3 +276,4 @@ class BenchCommandTest(CommandTest):\n                 '-s', 'CLOSESPIDER_TIMEOUT=0.01')\n         log = to_native_str(p.stderr.read())\n         self.assertIn('INFO: Crawled', log)\n+        self.assertNotIn('Unhandled Error', log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a18dc24471f121ed85d1bee4281d43c3a3728162", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 56 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -13,8 +13,8 @@ class Root(Resource):\n         return self\n \n     def render(self, request):\n-        total = _getarg(request, 'total', 100, int)\n-        show = _getarg(request, 'show', 10, int)\n+        total = _getarg(request, b'total', 100, int)\n+        show = _getarg(request, b'show', 10, int)\n         nlist = [random.randint(1, total) for _ in range(show)]\n         request.write(b\"<html><head></head><body>\")\n         args = request.args.copy()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f042ad0f39594d59a1a2032e6294ff1890638138", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 9 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 52 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -9,7 +9,7 @@ from six.moves.urllib.parse import urlunparse\n \n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.exceptions import NotConfigured\n-\n+from scrapy.utils.python import to_bytes\n \n class HttpProxyMiddleware(object):\n \n@@ -26,7 +26,7 @@ class HttpProxyMiddleware(object):\n         proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n \n         if user:\n-            user_pass = '%s:%s' % (unquote(user), unquote(password))\n+            user_pass = to_bytes('%s:%s' % (unquote(user), unquote(password)))\n             creds = base64.b64encode(user_pass).strip()\n         else:\n             creds = None\n@@ -52,4 +52,4 @@ class HttpProxyMiddleware(object):\n         creds, proxy = self.proxies[scheme]\n         request.meta['proxy'] = proxy\n         if creds:\n-            request.headers['Proxy-Authorization'] = 'Basic ' + creds\n+            request.headers['Proxy-Authorization'] = b'Basic ' + creds\n\n@@ -52,7 +52,7 @@ class TestDefaultHeadersMiddleware(TestCase):\n         req = Request('http://scrapytest.org')\n         assert mw.process_request(req, spider) is None\n         self.assertEquals(req.meta, {'proxy': 'https://proxy:3128'})\n-        self.assertEquals(req.headers.get('Proxy-Authorization'), 'Basic dXNlcjpwYXNz')\n+        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic dXNlcjpwYXNz')\n \n     def test_proxy_auth_empty_passwd(self):\n         os.environ['http_proxy'] = 'https://user:@proxy:3128'\n@@ -60,7 +60,7 @@ class TestDefaultHeadersMiddleware(TestCase):\n         req = Request('http://scrapytest.org')\n         assert mw.process_request(req, spider) is None\n         self.assertEquals(req.meta, {'proxy': 'https://proxy:3128'})\n-        self.assertEquals(req.headers.get('Proxy-Authorization'), 'Basic dXNlcjo=')\n+        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic dXNlcjo=')\n \n     def test_proxy_already_seted(self):\n         os.environ['http_proxy'] = http_proxy = 'https://proxy.for.http:3128'\n\n@@ -21,20 +21,20 @@ class RetryTest(unittest.TestCase):\n \n     def test_priority_adjust(self):\n         req = Request('http://www.scrapytest.org/503')\n-        rsp = Response('http://www.scrapytest.org/503', body='', status=503)\n+        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)\n         req2 = self.mw.process_response(req, rsp, self.spider)\n         assert req2.priority < req.priority\n \n     def test_404(self):\n         req = Request('http://www.scrapytest.org/404')\n-        rsp = Response('http://www.scrapytest.org/404', body='', status=404)\n+        rsp = Response('http://www.scrapytest.org/404', body=b'', status=404)\n \n         # dont retry 404s\n         assert self.mw.process_response(req, rsp, self.spider) is rsp\n \n     def test_dont_retry(self):\n         req = Request('http://www.scrapytest.org/503', meta={'dont_retry': True})\n-        rsp = Response('http://www.scrapytest.org/503', body='', status=503)\n+        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)\n \n         # first retry\n         r = self.mw.process_response(req, rsp, self.spider)\n@@ -56,7 +56,7 @@ class RetryTest(unittest.TestCase):\n \n     def test_503(self):\n         req = Request('http://www.scrapytest.org/503')\n-        rsp = Response('http://www.scrapytest.org/503', body='', status=503)\n+        rsp = Response('http://www.scrapytest.org/503', body=b'', status=503)\n \n         # first retry\n         req = self.mw.process_response(req, rsp, self.spider)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a06a5f00f4a62028416ada4919994efd7d439eb2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 34 | Churn Cumulative: 494 | Contributors (this commit): 25 | Commits (past 90d): 8 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,9 +11,11 @@ from scrapy.utils.httpobj import urlparse_cached\n from scrapy.exceptions import NotConfigured\n from scrapy.utils.python import to_bytes\n \n+\n class HttpProxyMiddleware(object):\n \n-    def __init__(self):\n+    def __init__(self, auth_encoding='latin-1'):\n+        self.auth_encoding = auth_encoding\n         self.proxies = {}\n         for type, url in getproxies().items():\n             self.proxies[type] = self._get_proxy(url, type)\n@@ -21,12 +23,19 @@ class HttpProxyMiddleware(object):\n         if not self.proxies:\n             raise NotConfigured\n \n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        auth_encoding = crawler.settings.get('HTTPPROXY_AUTH_ENCODING')\n+        return cls(auth_encoding)\n+\n     def _get_proxy(self, url, orig_type):\n         proxy_type, user, password, hostport = _parse_proxy(url)\n         proxy_url = urlunparse((proxy_type or orig_type, hostport, '', '', '', ''))\n \n         if user:\n-            user_pass = to_bytes('%s:%s' % (unquote(user), unquote(password)))\n+            user_pass = to_bytes(\n+                '%s:%s' % (unquote(user), unquote(password)),\n+                encoding=self.auth_encoding)\n             creds = base64.b64encode(user_pass).strip()\n         else:\n             creds = None\n\n@@ -169,6 +169,8 @@ HTTPCACHE_DBM_MODULE = 'anydbm' if six.PY2 else 'dbm'\n HTTPCACHE_POLICY = 'scrapy.extensions.httpcache.DummyPolicy'\n HTTPCACHE_GZIP = False\n \n+HTTPPROXY_AUTH_ENCODING = 'latin-1'\n+\n ITEM_PROCESSOR = 'scrapy.pipelines.ItemPipelineManager'\n \n ITEM_PIPELINES = {}\n\n@@ -9,6 +9,7 @@ from scrapy.spiders import Spider\n \n spider = Spider('foo')\n \n+\n class TestDefaultHeadersMiddleware(TestCase):\n \n     failureException = AssertionError\n@@ -62,6 +63,22 @@ class TestDefaultHeadersMiddleware(TestCase):\n         self.assertEquals(req.meta, {'proxy': 'https://proxy:3128'})\n         self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic dXNlcjo=')\n \n+    def test_proxy_auth_encoding(self):\n+        # utf-8 encoding\n+        os.environ['http_proxy'] = u'https://m\\u00E1n:pass@proxy:3128'\n+        mw = HttpProxyMiddleware(auth_encoding='utf-8')\n+        req = Request('http://scrapytest.org')\n+        assert mw.process_request(req, spider) is None\n+        self.assertEquals(req.meta, {'proxy': 'https://proxy:3128'})\n+        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic bcOhbjpwYXNz')\n+\n+        # default latin-1 encoding\n+        mw = HttpProxyMiddleware(auth_encoding='latin-1')\n+        req = Request('http://scrapytest.org')\n+        assert mw.process_request(req, spider) is None\n+        self.assertEquals(req.meta, {'proxy': 'https://proxy:3128'})\n+        self.assertEquals(req.headers.get('Proxy-Authorization'), b'Basic beFuOnBhc3M=')\n+\n     def test_proxy_already_seted(self):\n         os.environ['http_proxy'] = http_proxy = 'https://proxy.for.http:3128'\n         mw = HttpProxyMiddleware()\n@@ -69,7 +86,6 @@ class TestDefaultHeadersMiddleware(TestCase):\n         assert mw.process_request(req, spider) is None\n         assert 'proxy' in req.meta and req.meta['proxy'] is None\n \n-\n     def test_no_proxy(self):\n         os.environ['http_proxy'] = http_proxy = 'https://proxy.for.http:3128'\n         mw = HttpProxyMiddleware()\n@@ -88,4 +104,3 @@ class TestDefaultHeadersMiddleware(TestCase):\n         req = Request('http://noproxy.com')\n         assert mw.process_request(req, spider) is None\n         assert 'proxy' not in req.meta\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#20b839b44ba5795aa0a6cf96ac71d4524072fcab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 11 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 33 | Churn Cumulative: 99 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -7,6 +7,7 @@ from twisted.web.server import Site, NOT_DONE_YET\n from twisted.web.resource import Resource\n from twisted.internet import reactor, defer, ssl\n from scrapy import twisted_version\n+from scrapy.utils.python import to_bytes\n \n \n if twisted_version < (11, 0, 0):\n@@ -55,12 +56,12 @@ class LeafResource(Resource):\n class Follow(LeafResource):\n \n     def render(self, request):\n-        total = getarg(request, \"total\", 100, type=int)\n-        show = getarg(request, \"show\", 1, type=int)\n-        order = getarg(request, \"order\", \"desc\")\n-        maxlatency = getarg(request, \"maxlatency\", 0, type=float)\n-        n = getarg(request, \"n\", total, type=int)\n-        if order == \"rand\":\n+        total = getarg(request, b\"total\", 100, type=int)\n+        show = getarg(request, b\"show\", 1, type=int)\n+        order = getarg(request, b\"order\", b\"desc\")\n+        maxlatency = getarg(request, b\"maxlatency\", 0, type=float)\n+        n = getarg(request, b\"n\", total, type=int)\n+        if order == b\"rand\":\n             nlist = [random.randint(1, total) for _ in range(show)]\n         else:  # order == \"desc\"\n             nlist = range(n, max(n - show, 0), -1)\n@@ -73,19 +74,19 @@ class Follow(LeafResource):\n         s = \"\"\"<html> <head></head> <body>\"\"\"\n         args = request.args.copy()\n         for nl in nlist:\n-            args[\"n\"] = [str(nl)]\n+            args[b\"n\"] = [to_bytes(str(nl))]\n             argstr = urlencode(args, doseq=True)\n             s += \"<a href='/follow?%s'>follow %d</a><br>\" % (argstr, nl)\n         s += \"\"\"</body>\"\"\"\n-        request.write(s)\n+        request.write(to_bytes(s))\n         request.finish()\n \n \n class Delay(LeafResource):\n \n     def render_GET(self, request):\n-        n = getarg(request, \"n\", 1, type=float)\n-        b = getarg(request, \"b\", 1, type=int)\n+        n = getarg(request, b\"n\", 1, type=float)\n+        b = getarg(request, b\"b\", 1, type=int)\n         if b:\n             # send headers now and delay body\n             request.write('')\n@@ -93,16 +94,16 @@ class Delay(LeafResource):\n         return NOT_DONE_YET\n \n     def _delayedRender(self, request, n):\n-        request.write(\"Response delayed for %0.3f seconds\\n\" % n)\n+        request.write(to_bytes(\"Response delayed for %0.3f seconds\\n\" % n))\n         request.finish()\n \n \n class Status(LeafResource):\n \n     def render_GET(self, request):\n-        n = getarg(request, \"n\", 200, type=int)\n+        n = getarg(request, b\"n\", 200, type=int)\n         request.setResponseCode(n)\n-        return \"\"\n+        return b\"\"\n \n \n class Raw(LeafResource):\n@@ -114,7 +115,7 @@ class Raw(LeafResource):\n     render_POST = render_GET\n \n     def _delayedRender(self, request):\n-        raw = getarg(request, 'raw', 'HTTP 1.1 200 OK\\n')\n+        raw = getarg(request, b'raw', b'HTTP 1.1 200 OK\\n')\n         request.startedWriting = 1\n         request.write(raw)\n         request.channel.transport.loseConnection()\n@@ -128,7 +129,7 @@ class Echo(LeafResource):\n             'headers': dict(request.requestHeaders.getAllRawHeaders()),\n             'body': request.content.read(),\n         }\n-        return json.dumps(output)\n+        return to_bytes(json.dumps(output))\n \n \n class Partial(LeafResource):\n@@ -146,7 +147,7 @@ class Partial(LeafResource):\n class Drop(Partial):\n \n     def _delayedRender(self, request):\n-        abort = getarg(request, \"abort\", 0, type=int)\n+        abort = getarg(request, b\"abort\", 0, type=int)\n         request.write(b\"this connection will be dropped\\n\")\n         tr = request.channel.transport\n         try:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0680950b9898aa77e4c494ab8a318d791ef0d55f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/2 | Churn Δ: 18 | Churn Cumulative: 446 | Contributors (this commit): 8 | Commits (past 90d): 8 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,13 +1,12 @@\n from __future__ import print_function\n import sys, time, random, os, json\n-import six\n from six.moves.urllib.parse import urlencode\n from subprocess import Popen, PIPE\n from twisted.web.server import Site, NOT_DONE_YET\n from twisted.web.resource import Resource\n from twisted.internet import reactor, defer, ssl\n from scrapy import twisted_version\n-from scrapy.utils.python import to_bytes\n+from scrapy.utils.python import to_bytes, to_unicode\n \n \n if twisted_version < (11, 0, 0):\n@@ -126,8 +125,10 @@ class Echo(LeafResource):\n \n     def render_GET(self, request):\n         output = {\n-            'headers': dict(request.requestHeaders.getAllRawHeaders()),\n-            'body': request.content.read(),\n+            'headers': dict(\n+                (to_unicode(k), [to_unicode(v) for v in vs])\n+                for k, vs in request.requestHeaders.getAllRawHeaders()),\n+            'body': to_unicode(request.content.read()),\n         }\n         return to_bytes(json.dumps(output))\n \n\n@@ -8,6 +8,7 @@ from twisted.trial.unittest import TestCase\n \n from scrapy.http import Request\n from scrapy.crawler import CrawlerRunner\n+from scrapy.utils.python import to_unicode\n from tests import mock\n from tests.spiders import FollowAllSpider, DelaySpider, SimpleSpider, \\\n     BrokenStartRequestsSpider, SingleRequestSpider, DuplicateStartRequestsSpider\n@@ -201,16 +202,16 @@ with multiples lines\n         self.assertIn('responses', crawler.spider.meta)\n         self.assertNotIn('failures', crawler.spider.meta)\n         # start requests doesn't set Referer header\n-        echo0 = json.loads(crawler.spider.meta['responses'][2].body)\n+        echo0 = json.loads(to_unicode(crawler.spider.meta['responses'][2].body))\n         self.assertNotIn('Referer', echo0['headers'])\n         # following request sets Referer to start request url\n-        echo1 = json.loads(crawler.spider.meta['responses'][1].body)\n+        echo1 = json.loads(to_unicode(crawler.spider.meta['responses'][1].body))\n         self.assertEqual(echo1['headers'].get('Referer'), [req0.url])\n         # next request avoids Referer header\n-        echo2 = json.loads(crawler.spider.meta['responses'][2].body)\n+        echo2 = json.loads(to_unicode(crawler.spider.meta['responses'][2].body))\n         self.assertNotIn('Referer', echo2['headers'])\n         # last request explicitly sets a Referer header\n-        echo3 = json.loads(crawler.spider.meta['responses'][3].body)\n+        echo3 = json.loads(to_unicode(crawler.spider.meta['responses'][3].body))\n         self.assertEqual(echo3['headers'].get('Referer'), ['http://example.com'])\n \n     @defer.inlineCallbacks\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ad2b3321b90245e59b2e4b99eb7a4296f6c7e768", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 11 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -119,7 +119,7 @@ class BrokenStartRequestsSpider(FollowAllSpider):\n         if self.fail_before_yield:\n             1 / 0\n \n-        for s in xrange(100):\n+        for s in range(100):\n             qargs = {'total': 10, 'seed': s}\n             url = \"http://localhost:8998/follow?%s\" % urlencode(qargs, doseq=1)\n             yield Request(url, meta={'seed': s})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
