{"custom_id": "keras#7222b95ea5de9e88fbc288b84d3805efc3f5cb9a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 3193 | Contributors (this commit): 20 | Commits (past 90d): 10 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -885,8 +885,8 @@ class TestBackend(object):\n         mean = 0.\n         std = 1.\n         for k in BACKENDS:\n-            rand = k.eval(k.random_normal((300, 100), mean=mean, stddev=std))\n-            assert rand.shape == (300, 100)\n+            rand = k.eval(k.random_normal((300, 200), mean=mean, stddev=std))\n+            assert rand.shape == (300, 200)\n             assert np.abs(np.mean(rand) - mean) < 0.015\n             assert np.abs(np.std(rand) - std) < 0.015\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c7efb4e30f914555954f1d07c585c7a2ff9150de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 49 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 50 | Churn Cumulative: 1383 | Contributors (this commit): 19 | Commits (past 90d): 13 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -42,6 +42,14 @@ class Wrapper(Layer):\n         else:\n             return None\n \n+    @property\n+    def trainable(self):\n+        return self.layer.trainable\n+\n+    @trainable.setter\n+    def trainable(self, value):\n+        self.layer.trainable = value\n+\n     @property\n     def trainable_weights(self):\n         return self.layer.trainable_weights\n@@ -246,7 +254,6 @@ class Bidirectional(Wrapper):\n     \"\"\"\n \n     def __init__(self, layer, merge_mode='concat', weights=None, **kwargs):\n-        super(Bidirectional, self).__init__(layer, **kwargs)\n         if merge_mode not in ['sum', 'mul', 'ave', 'concat', None]:\n             raise ValueError('Invalid merge mode. '\n                              'Merge mode should be one of '\n@@ -266,6 +273,18 @@ class Bidirectional(Wrapper):\n         self.return_sequences = layer.return_sequences\n         self.return_state = layer.return_state\n         self.supports_masking = True\n+        self._trainable = True\n+        super(Bidirectional, self).__init__(layer, **kwargs)\n+\n+    @property\n+    def trainable(self):\n+        return self._trainable\n+\n+    @trainable.setter\n+    def trainable(self, value):\n+        self._trainable = value\n+        self.forward_layer.trainable = value\n+        self.backward_layer.trainable = value\n \n     def get_weights(self):\n         return self.forward_layer.get_weights() + self.backward_layer.get_weights()\n\n@@ -137,6 +137,22 @@ def test_TimeDistributed_learning_phase():\n     assert_allclose(np.mean(y), 0., atol=1e-1, rtol=1e-1)\n \n \n+@keras_test\n+def test_TimeDistributed_trainable():\n+    # test layers that need learning_phase to be set\n+    x = Input(shape=(3, 2))\n+    layer = wrappers.TimeDistributed(layers.BatchNormalization())\n+    _ = layer(x)\n+    assert len(layer.updates) == 2\n+    assert len(layer.trainable_weights) == 2\n+    layer.trainable = False\n+    assert len(layer.updates) == 0\n+    assert len(layer.trainable_weights) == 0\n+    layer.trainable = True\n+    assert len(layer.updates) == 2\n+    assert len(layer.trainable_weights) == 2\n+\n+\n @keras_test\n def test_regularizers():\n     model = Sequential()\n@@ -325,5 +341,18 @@ def test_Bidirectional_state_reuse():\n     outputs = model.predict(inputs)\n \n \n+@keras_test\n+def test_Bidirectional_trainable():\n+    # test layers that need learning_phase to be set\n+    x = Input(shape=(3, 2))\n+    layer = wrappers.Bidirectional(layers.SimpleRNN(3))\n+    _ = layer(x)\n+    assert len(layer.trainable_weights) == 6\n+    layer.trainable = False\n+    assert len(layer.trainable_weights) == 0\n+    layer.trainable = True\n+    assert len(layer.trainable_weights) == 6\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a3dcce76451751ff8462a2069984f76230fb7572", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 22 | Churn Cumulative: 2405 | Contributors (this commit): 29 | Commits (past 90d): 10 | Contributors (cumulative): 33 | DMM Complexity: 0.6923076923076923\n\nDIFF:\n@@ -76,6 +76,10 @@ class Optimizer(object):\n \n     def get_gradients(self, loss, params):\n         grads = K.gradients(loss, params)\n+        if None in grads:\n+            raise ValueError('An operation has `None` for gradient.',\n+                             'Please be sure that all of your ops have a gradient defined \\\"i.e. are differentiable\\\".',\n+                             'Common ops without gradient : K.argmax, K.round, K.cast, K.eval.')\n         if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n             norm = K.sqrt(sum([K.sum(K.square(g)) for g in grads]))\n             grads = [clip_norm(g, self.clipnorm, norm) for g in grads]\n\n@@ -4,9 +4,9 @@ import numpy as np\n from numpy.testing import assert_allclose\n \n from keras.utils import test_utils\n-from keras import optimizers\n-from keras.models import Sequential\n-from keras.layers.core import Dense, Activation\n+from keras import optimizers, Input\n+from keras.models import Sequential, Model\n+from keras.layers.core import Dense, Activation, Lambda\n from keras.utils.test_utils import keras_test\n from keras.utils.np_utils import to_categorical\n from keras import backend as K\n@@ -64,10 +64,22 @@ def _test_optimizer(optimizer, target=0.75):\n     assert_allclose(bias, 2.)\n \n \n+@keras_test\n+def _test_no_grad(optimizer):\n+    inp = Input([3])\n+    x = Dense(10)(inp)\n+    x = Lambda(lambda l: 1.0 * K.reshape(K.cast(K.argmax(l), 'float32'), [-1, 1]))(x)\n+    mod = Model(inp, x)\n+    mod.compile(optimizer, 'mse')\n+    with pytest.raises(ValueError):\n+        mod.fit(np.zeros([10, 3]), np.zeros([10, 1], np.float32), batch_size=10, epochs=10)\n+\n+\n @keras_test\n def test_sgd():\n     sgd = optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n     _test_optimizer(sgd)\n+    _test_no_grad(sgd)\n \n \n @keras_test\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0478e07ccab79db931061329484d6e97c4a6e072", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 2211 | Contributors (this commit): 27 | Commits (past 90d): 9 | Contributors (cumulative): 27 | DMM Complexity: 0.0\n\nDIFF:\n@@ -77,9 +77,11 @@ class Optimizer(object):\n     def get_gradients(self, loss, params):\n         grads = K.gradients(loss, params)\n         if None in grads:\n-            raise ValueError('An operation has `None` for gradient.',\n-                             'Please be sure that all of your ops have a gradient defined \\\"i.e. are differentiable\\\".',\n-                             'Common ops without gradient : K.argmax, K.round, K.cast, K.eval.')\n+            raise ValueError('An operation has `None` for gradient. '\n+                             'Please make sure that all of your ops have a '\n+                             'gradient defined (i.e. are differentiable). '\n+                             'Common ops without gradient: '\n+                             'K.argmax, K.round, K.eval.')\n         if hasattr(self, 'clipnorm') and self.clipnorm > 0:\n             norm = K.sqrt(sum([K.sum(K.square(g)) for g in grads]))\n             grads = [clip_norm(g, self.clipnorm, norm) for g in grads]\n@@ -606,8 +608,10 @@ class Nadam(Optimizer):\n         t = K.cast(self.iterations, K.floatx()) + 1\n \n         # Due to the recommendations in [2], i.e. warming momentum schedule\n-        momentum_cache_t = self.beta_1 * (1. - 0.5 * (K.pow(K.cast_to_floatx(0.96), t * self.schedule_decay)))\n-        momentum_cache_t_1 = self.beta_1 * (1. - 0.5 * (K.pow(K.cast_to_floatx(0.96), (t + 1) * self.schedule_decay)))\n+        momentum_cache_t = self.beta_1 * (\n+            1. - 0.5 * (K.pow(K.cast_to_floatx(0.96), t * self.schedule_decay)))\n+        momentum_cache_t_1 = self.beta_1 * (\n+            1. - 0.5 * (K.pow(K.cast_to_floatx(0.96), (t + 1) * self.schedule_decay)))\n         m_schedule_new = self.m_schedule * momentum_cache_t\n         m_schedule_next = self.m_schedule * momentum_cache_t * momentum_cache_t_1\n         self.updates.append((self.m_schedule, m_schedule_new))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5822ee2b95ab2fff640abcc7233c0ee67620cdf6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 61 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): 9/5 | Churn Δ: 66 | Churn Cumulative: 14668 | Contributors (this commit): 71 | Commits (past 90d): 35 | Contributors (cumulative): 93 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -447,7 +447,7 @@ def random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n                              'Please provide fixed dimension '\n                              'instead of `None`.')\n     # how to apply mean and stddev\n-    return random_normal_variable(shape=shape, mean=mean, scale=1.0)\n+    return random_normal_variable(shape=shape, mean=mean, scale=1.0, seed=seed)\n \n \n def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n\n@@ -1943,7 +1943,10 @@ class Container(Layer):\n                 losses += layer.get_losses_for(None)\n         # Add any potential unconditional model-level loss.\n         losses += self.get_losses_for(None)\n-        return losses\n+\n+        unique_tensors = list(set(x for x in losses if not isinstance(x, (float, int))))\n+        non_tensors = [x for x in losses if isinstance(x, (float, int))]\n+        return unique_tensors + non_tensors\n \n     @property\n     def uses_learning_phase(self):\n\n@@ -885,7 +885,7 @@ class TestBackend(object):\n         mean = 0.\n         std = 1.\n         for k in BACKENDS:\n-            rand = k.eval(k.random_normal((300, 200), mean=mean, stddev=std))\n+            rand = k.eval(k.random_normal((300, 200), mean=mean, stddev=std, seed=1337))\n             assert rand.shape == (300, 200)\n             assert np.abs(np.mean(rand) - mean) < 0.015\n             assert np.abs(np.std(rand) - std) < 0.015\n\n@@ -1,10 +1,11 @@\n import pytest\n \n-from keras.models import Sequential\n-from keras.layers import Dense\n+from keras.models import Sequential, Model\n+from keras.layers import Dense, Input, Average\n from keras.utils import np_utils\n from keras.utils import test_utils\n from keras import regularizers\n+from keras import backend as K\n \n data_dim = 5\n num_classes = 2\n@@ -32,6 +33,19 @@ def create_model(kernel_regularizer=None, activity_regularizer=None):\n     return model\n \n \n+def create_multi_input_model_from(layer1, layer2):\n+    input_1 = Input(shape=(data_dim,))\n+    input_2 = Input(shape=(data_dim,))\n+    out1 = layer1(input_1)\n+    out2 = layer2(input_2)\n+    out = Average()([out1, out2])\n+    model = Model([input_1, input_2], out)\n+    model.add_loss(K.mean(out2))\n+    model.add_loss(1)\n+    model.add_loss(1)\n+    return model\n+\n+\n def test_kernel_regularization():\n     x_train, y_train = get_data()\n     for reg in [regularizers.l1(),\n@@ -52,5 +66,44 @@ def test_activity_regularization():\n         model.train_on_batch(x_train, y_train)\n \n \n+def test_regularization_shared_layer():\n+    dense_layer = Dense(num_classes,\n+                        kernel_regularizer=regularizers.l1(),\n+                        activity_regularizer=regularizers.l1())\n+\n+    model = create_multi_input_model_from(dense_layer, dense_layer)\n+    model.compile(loss='categorical_crossentropy', optimizer='sgd')\n+    assert len(model.losses) == 6\n+\n+\n+def test_regularization_shared_model():\n+    dense_layer = Dense(num_classes,\n+                        kernel_regularizer=regularizers.l1(),\n+                        activity_regularizer=regularizers.l1())\n+\n+    input_tensor = Input(shape=(data_dim,))\n+    dummy_model = Model(input_tensor, dense_layer(input_tensor))\n+\n+    model = create_multi_input_model_from(dummy_model, dummy_model)\n+    model.compile(loss='categorical_crossentropy', optimizer='sgd')\n+    assert len(model.losses) == 6\n+\n+\n+def test_regularization_shared_layer_in_different_models():\n+    shared_dense = Dense(num_classes,\n+                         kernel_regularizer=regularizers.l1(),\n+                         activity_regularizer=regularizers.l1())\n+    models = []\n+    for _ in range(2):\n+        input_tensor = Input(shape=(data_dim,))\n+        unshared_dense = Dense(num_classes, kernel_regularizer=regularizers.l1())\n+        out = unshared_dense(shared_dense(input_tensor))\n+        models.append(Model(input_tensor, out))\n+\n+    model = create_multi_input_model_from(*models)\n+    model.compile(loss='categorical_crossentropy', optimizer='sgd')\n+    assert len(model.losses) == 8\n+\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7d1e0bc5872855af5bf35a725025d3bdb6f07d6c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 10 | Methods Changed: 2 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 40 | Churn Cumulative: 4297 | Contributors (this commit): 69 | Commits (past 90d): 13 | Contributors (cumulative): 88 | DMM Complexity: 0.0\n\nDIFF:\n@@ -585,7 +585,7 @@ class LearningRateScheduler(Callback):\n \n \n class TensorBoard(Callback):\n-    \"\"\"Tensorboard basic visualizations.\n+    \"\"\"TensorBoard basic visualizations.\n \n     [TensorBoard](https://www.tensorflow.org/get_started/summaries_and_tensorboard)\n     is a visualization tool provided with TensorFlow.\n@@ -601,6 +601,10 @@ class TensorBoard(Callback):\n     tensorboard --logdir=/full_path_to_your_logs\n     ```\n \n+    When using a backend other than TensorFlow, TensorBoard will still work\n+    (if you have TensorFlow installed), but the only feature available will\n+    be the display of the losses and metrics plots.\n+\n     # Arguments\n         log_dir: the path of the directory where to save the log\n             files to be parsed by TensorBoard.\n@@ -638,12 +642,31 @@ class TensorBoard(Callback):\n                  embeddings_layer_names=None,\n                  embeddings_metadata=None):\n         super(TensorBoard, self).__init__()\n-        if K.backend() != 'tensorflow':\n-            raise RuntimeError('TensorBoard callback only works '\n-                               'with the TensorFlow backend.')\n         global tf, projector\n+        try:\n             import tensorflow as tf\n             from tensorflow.contrib.tensorboard.plugins import projector\n+        except ImportError:\n+            raise ImportError('You need the TensorFlow module installed to use TensorBoard.')\n+\n+        if K.backend() != 'tensorflow':\n+            if histogram_freq != 0:\n+                warnings.warn('You are not using the TensorFlow backend. '\n+                              'histogram_freq was set to 0')\n+                histogram_freq = 0\n+            if write_graph:\n+                warnings.warn('You are not using the TensorFlow backend. '\n+                              'write_graph was set to False')\n+                write_graph = False\n+            if write_images:\n+                warnings.warn('You are not using the TensorFlow backend. '\n+                              'write_images was set to False')\n+                write_images = False\n+            if embeddings_freq != 0:\n+                warnings.warn('You are not using the TensorFlow backend. '\n+                              'embeddings_freq was set to 0')\n+                embeddings_freq = 0\n+\n         self.log_dir = log_dir\n         self.histogram_freq = histogram_freq\n         self.merged = None\n@@ -657,6 +680,7 @@ class TensorBoard(Callback):\n \n     def set_model(self, model):\n         self.model = model\n+        if K.backend() == 'tensorflow':\n             self.sess = K.get_session()\n         if self.histogram_freq and self.merged is None:\n             for layer in self.model.layers:\n\n@@ -403,8 +403,6 @@ def test_CSVLogger(tmpdir):\n \n \n @keras_test\n-@pytest.mark.skipif((K.backend() != 'tensorflow'),\n-                    reason='Requires TensorFlow backend')\n def test_TensorBoard(tmpdir):\n     np.random.seed(np.random.randint(1, 1e7))\n     filepath = str(tmpdir / 'logs')\n@@ -551,8 +549,6 @@ def test_TensorBoard_histogram_freq_must_have_validation_data(tmpdir):\n \n \n @keras_test\n-@pytest.mark.skipif((K.backend() != 'tensorflow'),\n-                    reason='Requires TensorFlow backend')\n def test_TensorBoard_multi_input_output(tmpdir):\n     np.random.seed(np.random.randint(1, 1e7))\n     filepath = str(tmpdir / 'logs')\n@@ -628,8 +624,6 @@ def test_TensorBoard_multi_input_output(tmpdir):\n \n \n @keras_test\n-@pytest.mark.skipif((K.backend() != 'tensorflow'),\n-                    reason='Requires TensorFlow backend')\n def test_TensorBoard_convnet(tmpdir):\n     np.random.seed(np.random.randint(1, 1e7))\n     filepath = str(tmpdir / 'logs')\n@@ -753,8 +747,6 @@ def test_LambdaCallback():\n \n \n @keras_test\n-@pytest.mark.skipif((K.backend() != 'tensorflow'),\n-                    reason=\"Requires TensorFlow backend\")\n def test_TensorBoard_with_ReduceLROnPlateau(tmpdir):\n     import shutil\n     np.random.seed(np.random.randint(1, 1e7))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c8bef99ec7a2032b9bea6e9a1260d05a2b6a80f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 329 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -33,11 +33,11 @@ This example demonstrates how to train with input\n tensors, save the model weights, and then evaluate the\n model using the numpy based Keras API.\n \n-Gets to ~99.1% validation accuracy after 5 epochs\n+Gets to ~99.1% test accuracy after 5 epochs\n (high variance from run to run: 98.9-99.3).\n '''\n import numpy as np\n-\n+import os\n import tensorflow as tf\n import keras\n from keras import backend as K\n@@ -118,7 +118,6 @@ sess = K.get_session()\n \n batch_size = 100\n batch_shape = (batch_size, 28, 28, 1)\n-steps_per_epoch = 600\n epochs = 5\n num_classes = 10\n \n@@ -141,7 +140,10 @@ min_after_dequeue = 3000\n # output will have shape `[batch_size, x, y, z]`.\n enqueue_many = True\n \n-data = mnist.load_mnist()\n+cache_dir = os.path.expanduser(\n+    os.path.join('~', '.keras', 'datasets', 'MNIST-data'))\n+data = mnist.read_data_sets(cache_dir, validation_size=0)\n+\n x_train_batch, y_train_batch = tf.train.shuffle_batch(\n     tensors=[data.train.images, data.train.labels.astype(np.int32)],\n     batch_size=batch_size,\n@@ -205,7 +207,7 @@ coord = tf.train.Coordinator()\n threads = tf.train.start_queue_runners(sess, coord)\n \n train_model.fit(epochs=epochs,\n-                steps_per_epoch=steps_per_epoch,\n+                steps_per_epoch=int(np.ceil(data.train.num_examples / float(batch_size))),\n                 callbacks=[EvaluateInputTensor(test_model, steps=100)])\n \n # Save the model weights.\n@@ -217,8 +219,8 @@ coord.join(threads)\n K.clear_session()\n \n # Second Session to test loading trained model without tensors\n-x_test = np.reshape(data.validation.images, (data.validation.images.shape[0], 28, 28, 1))\n-y_test = data.validation.labels\n+x_test = np.reshape(data.test.images, (data.test.images.shape[0], 28, 28, 1))\n+y_test = data.test.labels\n x_test_inp = layers.Input(shape=(x_test.shape[1:]))\n test_out = cnn_layers(x_test_inp)\n test_model = keras.models.Model(inputs=x_test_inp, outputs=test_out)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#10b46cbdc47f7164cd92209077939a7f7882b3c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 46 | Files Changed: 3 | Hunks: 12 | Methods Changed: 8 | Complexity Δ (Sum/Max): -6/0 | Churn Δ: 70 | Churn Cumulative: 6939 | Contributors (this commit): 91 | Commits (past 90d): 41 | Contributors (cumulative): 99 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,11 +5,13 @@\n  - Auto-Encoding Variational Bayes\n    https://arxiv.org/abs/1312.6114\n '''\n+from __future__ import print_function\n+\n import numpy as np\n import matplotlib.pyplot as plt\n from scipy.stats import norm\n \n-from keras.layers import Input, Dense, Lambda, Layer\n+from keras.layers import Input, Dense, Lambda\n from keras.models import Model\n from keras import backend as K\n from keras import metrics\n@@ -44,29 +46,17 @@ decoder_mean = Dense(original_dim, activation='sigmoid')\n h_decoded = decoder_h(z)\n x_decoded_mean = decoder_mean(h_decoded)\n \n+# instantiate VAE model\n+vae = Model(x, x_decoded_mean)\n \n-# Custom loss layer\n-class CustomVariationalLayer(Layer):\n-    def __init__(self, **kwargs):\n-        self.is_placeholder = True\n-        super(CustomVariationalLayer, self).__init__(**kwargs)\n-\n-    def vae_loss(self, x, x_decoded_mean):\n+# Compute VAE loss\n xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n-        return K.mean(xent_loss + kl_loss)\n+vae_loss = K.mean(xent_loss + kl_loss)\n \n-    def call(self, inputs):\n-        x = inputs[0]\n-        x_decoded_mean = inputs[1]\n-        loss = self.vae_loss(x, x_decoded_mean)\n-        self.add_loss(loss, inputs=inputs)\n-        # We won't actually use the output.\n-        return x\n-\n-y = CustomVariationalLayer()([x, x_decoded_mean])\n-vae = Model(x, y)\n-vae.compile(optimizer='rmsprop', loss=None)\n+vae.add_loss(vae_loss)\n+vae.compile(optimizer='rmsprop')\n+vae.summary()\n \n \n # train the VAE on MNIST digits\n\n@@ -6,11 +6,13 @@ with Keras and deconvolution layers.\n - Auto-Encoding Variational Bayes\n   https://arxiv.org/abs/1312.6114\n '''\n+from __future__ import print_function\n+\n import numpy as np\n import matplotlib.pyplot as plt\n from scipy.stats import norm\n \n-from keras.layers import Input, Dense, Lambda, Flatten, Reshape, Layer\n+from keras.layers import Input, Dense, Lambda, Flatten, Reshape\n from keras.layers import Conv2D, Conv2DTranspose\n from keras.models import Model\n from keras import backend as K\n@@ -109,32 +111,18 @@ deconv_2_decoded = decoder_deconv_2(deconv_1_decoded)\n x_decoded_relu = decoder_deconv_3_upsamp(deconv_2_decoded)\n x_decoded_mean_squash = decoder_mean_squash(x_decoded_relu)\n \n+# instantiate VAE model\n+vae = Model(x, x_decoded_mean_squash)\n \n-# Custom loss layer\n-class CustomVariationalLayer(Layer):\n-    def __init__(self, **kwargs):\n-        self.is_placeholder = True\n-        super(CustomVariationalLayer, self).__init__(**kwargs)\n+# Compute VAE loss\n+xent_loss = img_rows * img_cols * metrics.binary_crossentropy(\n+    K.flatten(x),\n+    K.flatten(x_decoded_mean_squash))\n+kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n+vae_loss = K.mean(xent_loss + kl_loss)\n+vae.add_loss(vae_loss)\n \n-    def vae_loss(self, x, x_decoded_mean_squash):\n-        x = K.flatten(x)\n-        x_decoded_mean_squash = K.flatten(x_decoded_mean_squash)\n-        xent_loss = img_rows * img_cols * metrics.binary_crossentropy(x, x_decoded_mean_squash)\n-        kl_loss = - 0.5 * K.mean(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n-        return K.mean(xent_loss + kl_loss)\n-\n-    def call(self, inputs):\n-        x = inputs[0]\n-        x_decoded_mean_squash = inputs[1]\n-        loss = self.vae_loss(x, x_decoded_mean_squash)\n-        self.add_loss(loss, inputs=inputs)\n-        # We don't use this output.\n-        return x\n-\n-\n-y = CustomVariationalLayer()([x, x_decoded_mean_squash])\n-vae = Model(x, y)\n-vae.compile(optimizer='rmsprop', loss=None)\n+vae.compile(optimizer='rmsprop')\n vae.summary()\n \n # train the VAE on MNIST digits\n\n@@ -543,7 +543,7 @@ class Model(Container):\n     \"\"\"The `Model` class adds training & evaluation routines to a `Container`.\n     \"\"\"\n \n-    def compile(self, optimizer, loss, metrics=None, loss_weights=None,\n+    def compile(self, optimizer, loss=None, metrics=None, loss_weights=None,\n                 sample_weight_mode=None, weighted_metrics=None,\n                 target_tensors=None, **kwargs):\n         \"\"\"Configures the model for training.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#13669e80933f9270608e5bb1c9ead28173864110", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 9 | Methods Changed: 7 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 17 | Churn Cumulative: 6180 | Contributors (this commit): 80 | Commits (past 90d): 38 | Contributors (cumulative): 80 | DMM Complexity: 0.5\n\nDIFF:\n@@ -181,7 +181,7 @@ def _standardize_sample_weights(sample_weight, output_names):\n \n \n def _check_array_lengths(inputs, targets, weights=None):\n-    \"\"\"Does user input validation for numpy arrays.\n+    \"\"\"Checks if batch axes are the same for numpy arrays.\n \n     # Arguments\n         inputs: list of Numpy arrays of inputs.\n@@ -1401,7 +1401,7 @@ class Model(Container):\n \n     def _standardize_user_data(self, x, y,\n                                sample_weight=None, class_weight=None,\n-                               check_batch_axis=True, batch_size=None):\n+                               check_array_lengths=True, batch_size=None):\n         if not hasattr(self, 'optimizer'):\n             raise RuntimeError('You must compile a model before '\n                                'training/testing. '\n@@ -1420,6 +1420,8 @@ class Model(Container):\n                 output_shapes.append(None)\n             else:\n                 output_shapes.append(output_shape)\n+        # `check_batch_axis` is set to False since `x` may contain multiple batches\n+        #  and in general `x[0].shape[0] != self._feed_input_shapes[0][0]`\n         x = _standardize_input_data(x, self._feed_input_names,\n                                     self._feed_input_shapes,\n                                     check_batch_axis=False,\n@@ -1435,6 +1437,8 @@ class Model(Container):\n         sample_weights = [_standardize_weights(ref, sw, cw, mode)\n                           for (ref, sw, cw, mode)\n                           in zip(y, sample_weights, class_weights, self._feed_sample_weight_modes)]\n+\n+        if check_array_lengths:\n             _check_array_lengths(x, y, sample_weights)\n         _check_loss_and_target_compatibility(y,\n                                              self._feed_loss_fns,\n@@ -1589,7 +1593,6 @@ class Model(Container):\n             x, y,\n             sample_weight=sample_weight,\n             class_weight=class_weight,\n-            check_batch_axis=False,\n             batch_size=batch_size)\n         # Prepare validation data.\n         do_validation = False\n@@ -1610,7 +1613,6 @@ class Model(Container):\n             val_x, val_y, val_sample_weights = self._standardize_user_data(\n                 val_x, val_y,\n                 sample_weight=val_sample_weight,\n-                check_batch_axis=False,\n                 batch_size=batch_size)\n             if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n                 val_ins = val_x + val_y + val_sample_weights + [0.]\n@@ -1729,7 +1731,6 @@ class Model(Container):\n         x, y, sample_weights = self._standardize_user_data(\n             x, y,\n             sample_weight=sample_weight,\n-            check_batch_axis=False,\n             batch_size=batch_size)\n         # Prepare inputs, delegate logic to `_test_loop`.\n         if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n@@ -1839,8 +1840,7 @@ class Model(Container):\n         x, y, sample_weights = self._standardize_user_data(\n             x, y,\n             sample_weight=sample_weight,\n-            class_weight=class_weight,\n-            check_batch_axis=True)\n+            class_weight=class_weight)\n         if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n             ins = x + y + sample_weights + [1.]\n         else:\n@@ -1881,8 +1881,7 @@ class Model(Container):\n         \"\"\"\n         x, y, sample_weights = self._standardize_user_data(\n             x, y,\n-            sample_weight=sample_weight,\n-            check_batch_axis=True)\n+            sample_weight=sample_weight)\n         if self.uses_learning_phase and not isinstance(K.learning_phase(), int):\n             ins = x + y + sample_weights + [0.]\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#84d06a530ebbe59679acdc03b3b13460037582e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 19 | Files Changed: 3 | Hunks: 25 | Methods Changed: 12 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 76 | Churn Cumulative: 19168 | Contributors (this commit): 124 | Commits (past 90d): 38 | Contributors (cumulative): 152 | DMM Complexity: 0.25\n\nDIFF:\n@@ -2608,7 +2608,8 @@ class Container(Layer):\n         f.flush()\n         f.close()\n \n-    def load_weights(self, filepath, by_name=False, skip_mismatch=False):\n+    def load_weights(self, filepath, by_name=False,\n+                     skip_mismatch=False, reshape=False):\n         \"\"\"Loads all layer weights from a HDF5 save file.\n \n         If `by_name` is False (default) weights are loaded\n@@ -2631,6 +2632,8 @@ class Container(Layer):\n                 where there is a mismatch in the number of weights,\n                 or a mismatch in the shape of the weight\n                 (only valid when `by_name`=True).\n+        reshape: Reshape weights to fit the layer when the correct number\n+            of values are present but the shape does not match.\n \n \n         # Raises\n@@ -2643,9 +2646,11 @@ class Container(Layer):\n             f = f['model_weights']\n         if by_name:\n             load_weights_from_hdf5_group_by_name(\n-                f, self.layers, skip_mismatch=skip_mismatch)\n+                f, self.layers, skip_mismatch=skip_mismatch,\n+                reshape=reshape)\n         else:\n-            load_weights_from_hdf5_group(f, self.layers)\n+            load_weights_from_hdf5_group(\n+                f, self.layers, reshape=reshape)\n \n         if hasattr(f, 'close'):\n             f.close()\n@@ -2897,7 +2902,8 @@ def save_weights_to_hdf5_group(f, layers):\n \n def preprocess_weights_for_loading(layer, weights,\n                                    original_keras_version=None,\n-                                   original_backend=None):\n+                                   original_backend=None,\n+                                   reshape=False):\n     \"\"\"Converts layers weights from Keras 1 format to Keras 2.\n \n     # Arguments\n@@ -2906,6 +2912,8 @@ def preprocess_weights_for_loading(layer, weights,\n         original_keras_version: Keras version for the weights, as a string.\n         original_backend: Keras backend the weights were trained with,\n             as a string.\n+        reshape: Reshape weights to fit the layer when the correct number\n+            of values are present but the shape does not match.\n \n     # Returns\n         A list of weights values (Numpy arrays).\n@@ -3045,11 +3053,24 @@ def preprocess_weights_for_loading(layer, weights,\n                    'Conv2DTranspose',\n                    'ConvLSTM2D']\n     if layer.__class__.__name__ in conv_layers:\n+        layer_weights_shape = K.int_shape(layer.weights[0])\n         if _need_convert_kernel(original_backend):\n             weights[0] = conv_utils.convert_kernel(weights[0])\n             if layer.__class__.__name__ == 'ConvLSTM2D':\n                 weights[1] = conv_utils.convert_kernel(weights[1])\n-        if K.int_shape(layer.weights[0]) != weights[0].shape:\n+        if reshape and layer_weights_shape != weights[0].shape:\n+            if weights[0].size != np.prod(layer_weights_shape):\n+                raise ValueError('Weights must be of equal size to ' +\n+                                 'apply a reshape operation. ' +\n+                                 'Layer ' + layer.name +\n+                                 '\\'s weights have shape ' +\n+                                 str(layer_weights_shape) + ' and size ' +\n+                                 str(np.prod(layer_weights_shape)) + '. ' +\n+                                 'The weights for loading have shape ' +\n+                                 str(weights[0].shape) + ' and size ' +\n+                                 str(weights[0].size) + '. ')\n+            weights[0] = np.reshape(weights[0], layer_weights_shape)\n+        elif layer_weights_shape != weights[0].shape:\n             weights[0] = np.transpose(weights[0], (3, 2, 0, 1))\n             if layer.__class__.__name__ == 'ConvLSTM2D':\n                 weights[1] = np.transpose(weights[1], (3, 2, 0, 1))\n@@ -3101,12 +3122,14 @@ def _need_convert_kernel(original_backend):\n     return uses_correlation[original_backend] != uses_correlation[K.backend()]\n \n \n-def load_weights_from_hdf5_group(f, layers):\n+def load_weights_from_hdf5_group(f, layers, reshape=False):\n     \"\"\"Implements topological (order-based) weight loading.\n \n     # Arguments\n         f: A pointer to a HDF5 group.\n         layers: a list of target layers.\n+        reshape: Reshape weights to fit the layer when the correct number\n+            of values are present but the shape does not match.\n \n     # Raises\n         ValueError: in case of mismatch between provided layers\n@@ -3153,7 +3176,8 @@ def load_weights_from_hdf5_group(f, layers):\n         weight_values = preprocess_weights_for_loading(layer,\n                                                        weight_values,\n                                                        original_keras_version,\n-                                                       original_backend)\n+                                                       original_backend,\n+                                                       reshape=reshape)\n         if len(weight_values) != len(symbolic_weights):\n             raise ValueError('Layer #' + str(k) +\n                              ' (named \"' + layer.name +\n@@ -3169,7 +3193,8 @@ def load_weights_from_hdf5_group(f, layers):\n     K.batch_set_value(weight_value_tuples)\n \n \n-def load_weights_from_hdf5_group_by_name(f, layers, skip_mismatch=False):\n+def load_weights_from_hdf5_group_by_name(f, layers, skip_mismatch=False,\n+                                         reshape=False):\n     \"\"\"Implements name-based weight loading.\n \n     (instead of topological weight loading).\n@@ -3182,6 +3207,8 @@ def load_weights_from_hdf5_group_by_name(f, layers, skip_mismatch=False):\n         skip_mismatch: Boolean, whether to skip loading of layers\n             where there is a mismatch in the number of weights,\n             or a mismatch in the shape of the weights.\n+        reshape: Reshape weights to fit the layer when the correct number\n+            of values are present but the shape does not match.\n \n     # Raises\n         ValueError: in case of mismatch between provided layers\n@@ -3219,7 +3246,8 @@ def load_weights_from_hdf5_group_by_name(f, layers, skip_mismatch=False):\n                 layer,\n                 weight_values,\n                 original_keras_version,\n-                original_backend)\n+                original_backend,\n+                reshape=reshape)\n             if len(weight_values) != len(symbolic_weights):\n                 if skip_mismatch:\n                     warnings.warn('Skipping loading of weights for layer {}'.format(layer.name) +\n\n@@ -718,7 +718,7 @@ class Sequential(Model):\n             self.build()\n         self.model.set_weights(weights)\n \n-    def load_weights(self, filepath, by_name=False, skip_mismatch=False):\n+    def load_weights(self, filepath, by_name=False, skip_mismatch=False, reshape=False):\n         if h5py is None:\n             raise ImportError('`load_weights` requires h5py.')\n         f = h5py.File(filepath, mode='r')\n@@ -732,9 +732,10 @@ class Sequential(Model):\n             layers = self.layers\n         if by_name:\n             topology.load_weights_from_hdf5_group_by_name(f, layers,\n-                                                          skip_mismatch=skip_mismatch)\n+                                                          skip_mismatch=skip_mismatch,\n+                                                          reshape=reshape)\n         else:\n-            topology.load_weights_from_hdf5_group(f, layers)\n+            topology.load_weights_from_hdf5_group(f, layers, reshape=reshape)\n         if hasattr(f, 'close'):\n             f.close()\n \n\n@@ -7,6 +7,7 @@ from numpy.testing import assert_allclose, assert_raises\n from keras import backend as K\n from keras.models import Model, Sequential\n from keras.layers import Dense, Lambda, RepeatVector, TimeDistributed, LSTM\n+from keras.layers import Conv2D, Flatten\n from keras.layers import Input\n from keras import optimizers\n from keras import losses\n@@ -173,7 +174,7 @@ def test_saving_unused_layers_is_ok():\n \n \n @keras_test\n-def test_loading_weights_by_name():\n+def test_loading_weights_by_name_and_reshape():\n     \"\"\"\n     test loading model weights by name on:\n         - sequential model\n@@ -185,11 +186,12 @@ def test_loading_weights_by_name():\n \n     # sequential model\n     model = Sequential()\n-    model.add(Dense(2, input_shape=(3,), name='rick'))\n+    model.add(Conv2D(2, (1, 1), input_shape=(1, 1, 1), name='rick'))\n+    model.add(Flatten())\n     model.add(Dense(3, name='morty'))\n     model.compile(loss=custom_loss, optimizer=custom_opt(), metrics=['acc'])\n \n-    x = np.random.random((1, 3))\n+    x = np.random.random((1, 1, 1, 1))\n     y = np.random.random((1, 3))\n     model.train_on_batch(x, y)\n \n@@ -202,19 +204,26 @@ def test_loading_weights_by_name():\n     # delete and recreate model\n     del(model)\n     model = Sequential()\n-    model.add(Dense(2, input_shape=(3,), name='rick'))\n-    model.add(Dense(3, name='morty'))\n+    model.add(Conv2D(2, (1, 1), input_shape=(1, 1, 1), name='rick'))\n+    model.add(Conv2D(3, (1, 1), name='morty'))\n     model.compile(loss=custom_loss, optimizer=custom_opt(), metrics=['acc'])\n \n     # load weights from first model\n-    model.load_weights(fname, by_name=True)\n+    with pytest.raises(ValueError):\n+        model.load_weights(fname, by_name=True, reshape=False)\n+    with pytest.raises(ValueError):\n+        model.load_weights(fname, by_name=False, reshape=False)\n+    model.load_weights(fname, by_name=False, reshape=True)\n+    model.load_weights(fname, by_name=True, reshape=True)\n     os.remove(fname)\n \n     out2 = model.predict(x)\n-    assert_allclose(out, out2, atol=1e-05)\n+    assert_allclose(np.squeeze(out), np.squeeze(out2), atol=1e-05)\n     for i in range(len(model.layers)):\n         new_weights = model.layers[i].get_weights()\n         for j in range(len(new_weights)):\n+            # only compare layers that have weights, skipping Flatten()\n+            if old_weights[i]:\n                 assert_allclose(old_weights[i][j], new_weights[j], atol=1e-05)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#39b1dd34ff97fc949340695cc93d0a900f02cca3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 47 | Lines Deleted: 24 | Files Changed: 1 | Hunks: 10 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 71 | Churn Cumulative: 6251 | Contributors (this commit): 80 | Commits (past 90d): 39 | Contributors (cumulative): 80 | DMM Complexity: None\n\nDIFF:\n@@ -1928,7 +1928,7 @@ class Model(Container):\n                       use_multiprocessing=False,\n                       shuffle=True,\n                       initial_epoch=0):\n-        \"\"\"Fits the model on data yielded batch-by-batch by a Python generator.\n+        \"\"\"Trains the model on data yielded batch-by-batch by a Python generator.\n \n         The generator is run in parallel to the model, for efficiency.\n         For instance, this allows you to do real-time data augmentation\n@@ -1939,41 +1939,60 @@ class Model(Container):\n         using `use_multiprocessing=True`.\n \n         # Arguments\n-            generator: A generator or an instance of `Sequence` (`keras.utils.Sequence`)\n-                object in order to avoid duplicate data\n-                when using multiprocessing.\n+            generator: A generator or an instance of `Sequence`\n+                (`keras.utils.Sequence`) object in order to avoid\n+                duplicate data when using multiprocessing.\n                 The output of the generator must be either\n                 - a tuple `(inputs, targets)`\n                 - a tuple `(inputs, targets, sample_weights)`.\n-                This tuple (a single output of the generator) makes a single batch.\n-                Therefore, all arrays in this tuple must have the same length (equal\n-                to the size of this batch). Different batches may have different sizes.\n-                For example, the last batch of the epoch is commonly smaller than the\n-                others, if the size of the dataset is not divisible by the batch size.\n+                This tuple (a single output of the generator) makes a single\n+                batch. Therefore, all arrays in this tuple must have the same\n+                length (equal to the size of this batch). Different batches\n+                may have different sizes. For example, the last batch of the\n+                epoch is commonly smaller than the others, if the size of the\n+                dataset is not divisible by the batch size.\n                 The generator is expected to loop over its data\n                 indefinitely. An epoch finishes when `steps_per_epoch`\n                 batches have been seen by the model.\n-            steps_per_epoch: Total number of steps (batches of samples)\n+            steps_per_epoch: Integer.\n+                Total number of steps (batches of samples)\n                 to yield from `generator` before declaring one epoch\n                 finished and starting the next epoch. It should typically\n                 be equal to the number of samples of your dataset\n                 divided by the batch size.\n                 Optional for `Sequence`: if unspecified, will use\n                 the `len(generator)` as a number of steps.\n-            epochs: Integer, total number of iterations on the data.\n-            verbose: Verbosity mode, 0, 1, or 2.\n-            callbacks: List of callbacks to be called during training.\n+            epochs: Integer. Number of epochs to train the model.\n+                An epoch is an iteration over the entire data provided,\n+                as defined by `steps_per_epoch`.\n+                Note that in conjunction with `initial_epoch`,\n+                `epochs` is to be understood as \"final epoch\".\n+                The model is not trained for a number of iterations\n+                given by `epochs`, but merely until the epoch\n+                of index `epochs` is reached.\n+            verbose: Integer. 0, 1, or 2. Verbosity mode.\n+                0 = silent, 1 = progress bar, 2 = one line per epoch.\n+            callbacks: List of `keras.callbacks.Callback` instances.\n+                List of callbacks to apply during training.\n+                See [callbacks](/callbacks).\n             validation_data: This can be either\n                 - a generator for the validation data\n-                - a tuple (inputs, targets)\n-                - a tuple (inputs, targets, sample_weights).\n+                - tuple `(x_val, y_val)`\n+                - tuple `(x_val, y_val, val_sample_weights)`\n+                on which to evaluate\n+                the loss and any model metrics at the end of each epoch.\n+                The model will not be trained on this data.\n             validation_steps: Only relevant if `validation_data`\n                 is a generator. Total number of steps (batches of samples)\n-                to yield from `generator` before stopping.\n+                to yield from `validation_data` generator before stopping.\n                 Optional for `Sequence`: if unspecified, will use\n                 the `len(validation_data)` as a number of steps.\n-            class_weight: Dictionary mapping class indices to a weight\n-                for the class.\n+            class_weight: Optional dictionary mapping class indices (integers)\n+                to a weight (float) value, used for weighting the loss function\n+                (during training only).\n+                This can be useful to tell the model to\n+                \"pay more attention\" to samples from\n+                an under-represented class.\n             max_queue_size: Integer. Maximum size for the generator queue.\n                 If unspecified, `max_queue_size` will default to 10.\n             workers: Integer. Maximum number of processes to spin up\n@@ -1988,14 +2007,18 @@ class Model(Container):\n                 non picklable arguments to the generator\n                 as they can't be passed\n                 easily to children processes.\n-            shuffle: Whether to shuffle the order of the batches at\n-                the beginning of each epoch. Only used with instances\n-                of `Sequence` (keras.utils.Sequence).\n-            initial_epoch: Epoch at which to start training\n-                (useful for resuming a previous training run)\n+            shuffle: Boolean. Whether to shuffle the training data\n+                in batch-sized chunks before each epoch.\n+                Only used with instances of `Sequence` (`keras.utils.Sequence`).\n+            initial_epoch: Integer.\n+                Epoch at which to start training\n+                (useful for resuming a previous training run).\n \n         # Returns\n-            A `History` object.\n+            A `History` object. Its `History.history` attribute is\n+            a record of training loss values and metrics values\n+            at successive epochs, as well as validation loss values\n+            and validation metrics values (if applicable).\n \n         # Example\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f5e8226b111ae0d75d3b491592e3984580907724", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 28 | Files Changed: 5 | Hunks: 24 | Methods Changed: 14 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 56 | Churn Cumulative: 13521 | Contributors (this commit): 129 | Commits (past 90d): 46 | Contributors (cumulative): 155 | DMM Complexity: None\n\nDIFF:\n@@ -1939,7 +1939,7 @@ def resize_images(x, height_factor, width_factor, data_format):\n                      original_shape[2] * width_factor if original_shape[2] is not None else None, None))\n         return x\n     else:\n-        raise ValueError('Invalid data_format:', data_format)\n+        raise ValueError('Unknown data_format: ' + str(data_format))\n \n \n def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n@@ -1969,7 +1969,7 @@ def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n         output = repeat_elements(output, width_factor, axis=3)\n         return output\n     else:\n-        raise ValueError('Invalid data_format:', data_format)\n+        raise ValueError('Unknown data_format: ' + str(data_format))\n \n \n def repeat_elements(x, rep, axis):\n@@ -2183,7 +2183,7 @@ def spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n     if data_format is None:\n         data_format = image_data_format()\n     if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+        raise ValueError('Unknown data_format: ' + str(data_format))\n \n     if data_format == 'channels_first':\n         pattern = [[0, 0],\n@@ -2227,7 +2227,7 @@ def spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n     if data_format is None:\n         data_format = image_data_format()\n     if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+        raise ValueError('Unknown data_format: ' + str(data_format))\n \n     if data_format == 'channels_first':\n         pattern = [\n@@ -3247,7 +3247,7 @@ def _preprocess_padding(padding):\n     elif padding == 'valid':\n         padding = 'VALID'\n     else:\n-        raise ValueError('Invalid padding:', padding)\n+        raise ValueError('Invalid padding: ' + str(padding))\n     return padding\n \n \n@@ -3272,7 +3272,7 @@ def conv1d(x, kernel, strides=1, padding='valid',\n     if data_format is None:\n         data_format = image_data_format()\n     if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+        raise ValueError('Unknown data_format: ' + str(data_format))\n \n     kernel_shape = kernel.get_shape().as_list()\n     if padding == 'causal':\n@@ -3318,7 +3318,7 @@ def conv2d(x, kernel, strides=(1, 1), padding='valid',\n     if data_format is None:\n         data_format = image_data_format()\n     if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+        raise ValueError('Unknown data_format: ' + str(data_format))\n \n     x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n \n@@ -3359,7 +3359,7 @@ def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n     if data_format is None:\n         data_format = image_data_format()\n     if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+        raise ValueError('Unknown data_format: ' + str(data_format))\n     if isinstance(output_shape, (tuple, list)):\n         output_shape = tf.stack(output_shape)\n \n@@ -3410,7 +3410,7 @@ def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n     if data_format is None:\n         data_format = image_data_format()\n     if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+        raise ValueError('Unknown data_format: ' + str(data_format))\n \n     x, tf_data_format = _preprocess_conv1d_input(x, data_format)\n     padding = _preprocess_padding(padding)\n@@ -3462,7 +3462,7 @@ def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n     if data_format is None:\n         data_format = image_data_format()\n     if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+        raise ValueError('Unknown data_format: ' + str(data_format))\n \n     x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n     padding = _preprocess_padding(padding)\n@@ -3503,7 +3503,7 @@ def depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n     if data_format is None:\n         data_format = image_data_format()\n     if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+        raise ValueError('Unknown data_format: ' + str(data_format))\n \n     x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n     padding = _preprocess_padding(padding)\n@@ -3545,7 +3545,7 @@ def conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n     if data_format is None:\n         data_format = image_data_format()\n     if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+        raise ValueError('Unknown data_format: ' + str(data_format))\n \n     x, tf_data_format = _preprocess_conv3d_input(x, data_format)\n     padding = _preprocess_padding(padding)\n@@ -3584,7 +3584,7 @@ def conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n     if data_format is None:\n         data_format = image_data_format()\n     if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+        raise ValueError('Unknown data_format: ' + str(data_format))\n     if isinstance(output_shape, (tuple, list)):\n         output_shape = tf.stack(output_shape)\n \n@@ -3637,7 +3637,7 @@ def pool2d(x, pool_size, strides=(1, 1),\n     if data_format is None:\n         data_format = image_data_format()\n     if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+        raise ValueError('Unknown data_format: ' + str(data_format))\n \n     x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n     padding = _preprocess_padding(padding)\n@@ -3657,7 +3657,7 @@ def pool2d(x, pool_size, strides=(1, 1),\n                            padding=padding,\n                            data_format=tf_data_format)\n     else:\n-        raise ValueError('Invalid pooling mode:', pool_mode)\n+        raise ValueError('Invalid pool_mode: ' + str(pool_mode))\n \n     if data_format == 'channels_first' and tf_data_format == 'NHWC':\n         x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n@@ -3686,7 +3686,7 @@ def pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n     if data_format is None:\n         data_format = image_data_format()\n     if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+        raise ValueError('Unknown data_format: ' + str(data_format))\n \n     x, tf_data_format = _preprocess_conv3d_input(x, data_format)\n     padding = _preprocess_padding(padding)\n@@ -3706,7 +3706,7 @@ def pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n                              padding=padding,\n                              data_format=tf_data_format)\n     else:\n-        raise ValueError('Invalid pooling mode:', pool_mode)\n+        raise ValueError('Invalid pool_mode: ' + str(pool_mode))\n \n     if data_format == 'channels_first' and tf_data_format == 'NDHWC':\n         x = tf.transpose(x, (0, 4, 1, 2, 3))\n@@ -3734,7 +3734,7 @@ def bias_add(x, bias, data_format=None):\n     if data_format is None:\n         data_format = image_data_format()\n     if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+        raise ValueError('Unknown data_format: ' + str(data_format))\n     bias_shape = int_shape(bias)\n     if len(bias_shape) != 1 and len(bias_shape) != ndim(x) - 1:\n         raise ValueError('Unexpected bias dimensions %d, expect to be 1 or %d dimensions'\n@@ -4065,7 +4065,7 @@ def local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n     if data_format is None:\n         data_format = image_data_format()\n     if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+        raise ValueError('Unknown data_format: ' + str(data_format))\n \n     stride = strides[0]\n     kernel_shape = int_shape(kernel)\n@@ -4117,7 +4117,7 @@ def local_conv2d(inputs, kernel, kernel_size, strides, output_shape, data_format\n     if data_format is None:\n         data_format = image_data_format()\n     if data_format not in {'channels_first', 'channels_last'}:\n-        raise ValueError('Unknown data_format ' + str(data_format))\n+        raise ValueError('Unknown data_format: ' + str(data_format))\n \n     stride_row, stride_col = strides\n     output_row, output_col = output_shape\n\n@@ -181,5 +181,5 @@ def get(identifier):\n     elif callable(identifier):\n         return identifier\n     else:\n-        raise ValueError('Could not interpret constraint identifier:',\n-                         identifier)\n+        raise ValueError('Could not interpret constraint identifier: ' +\n+                         str(identifier))\n\n@@ -499,5 +499,5 @@ def get(identifier):\n     elif callable(identifier):\n         return identifier\n     else:\n-        raise ValueError('Could not interpret initializer identifier:',\n-                         identifier)\n+        raise ValueError('Could not interpret initializer identifier: ' +\n+                         str(identifier))\n\n@@ -759,5 +759,5 @@ def get(identifier):\n     if isinstance(identifier, Optimizer):\n         return identifier\n     else:\n-        raise ValueError('Could not interpret optimizer identifier:',\n-                         identifier)\n+        raise ValueError('Could not interpret optimizer identifier: ' +\n+                         str(identifier))\n\n@@ -84,5 +84,5 @@ def get(identifier):\n     elif callable(identifier):\n         return identifier\n     else:\n-        raise ValueError('Could not interpret regularizer identifier:',\n-                         identifier)\n+        raise ValueError('Could not interpret regularizer identifier: ' +\n+                         str(identifier))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
