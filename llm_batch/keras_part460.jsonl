{"custom_id": "keras#2ba062a28b6c646e1a0a88d0e0d30aab3438c757", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 169 | Lines Deleted: 145 | Files Changed: 12 | Hunks: 51 | Methods Changed: 28 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 314 | Churn Cumulative: 22650 | Contributors (this commit): 32 | Commits (past 90d): 75 | Contributors (cumulative): 109 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -125,8 +125,8 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n     include_top: Whether to include the fully-connected\n       layer at the top of the network. Defaults to True.\n     weights: One of `None` (random initialization),\n-      `\"imagenet\"` (pre-training on ImageNet-1k), or the path to the weights file\n-      to be loaded. Defaults to `\"imagenet\"`.\n+      `\"imagenet\"` (pre-training on ImageNet-1k), or the path to the weights\n+      file to be loaded. Defaults to `\"imagenet\"`.\n     input_tensor: Optional Keras tensor\n       (i.e. output of `layers.Input()`)\n       to use as image input for the model.\n@@ -370,30 +370,32 @@ def ConvNeXt(\n       depths: An iterable containing depths for each individual stages.\n       projection_dims: An iterable containing output number of channels of\n       each individual stages.\n-      drop_path_rate: Stochastic depth probability. If 0.0, then stochastic depth\n+      drop_path_rate: Stochastic depth probability. If 0.0, then stochastic\n+        depth won't be used.\n+      layer_scale_init_value: Layer scale coefficient. If 0.0, layer scaling\n         won't be used.\n-      layer_scale_init_value: Layer scale coefficient. If 0.0, layer scaling won't\n-        be used.\n       default_size: Default input image size.\n       model_name: An optional name for the model.\n       include_preprocessing: boolean denoting whther to include preprocessing in\n         the model. When `weights=\"imagenet\"` this should be always set to True.\n         But for other models (e.g., randomly initialized) users should set it\n         to False and apply preprocessing to data accordingly.\n-      include_top: Boolean denoting whether to include classification head to the\n-        model.\n+      include_top: Boolean denoting whether to include classification head to\n+        the model.\n       weights: one of `None` (random initialization), `\"imagenet\"` (pre-training\n         on ImageNet-1k), or the path to the weights file to be loaded.\n-      input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to use\n-        as image input for the model.\n-      input_shape: optional shape tuple, only to be specified if `include_top` is\n-        False. It should have exactly 3 inputs channels.\n-      pooling: optional pooling mode for feature extraction when `include_top` is\n-        `False`. - `None` means that the output of the model will be the 4D tensor\n-        output of the last convolutional layer. - `avg` means that global average\n-        pooling will be applied to the output of the last convolutional layer, and\n-        thus the output of the model will be a 2D tensor. - `max` means that\n-        global max pooling will be applied.\n+      input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to\n+        use as image input for the model.\n+      input_shape: optional shape tuple, only to be specified if `include_top`\n+        is False. It should have exactly 3 inputs channels.\n+      pooling: optional pooling mode for feature extraction when `include_top`\n+        is `False`.\n+        - `None` means that the output of the model will be the 4D tensor output\n+          of the last convolutional layer.\n+        - `avg` means that global average pooling will be applied to the output\n+          of the last convolutional layer, and thus the output of the model will\n+          be a 2D tensor.\n+        - `max` means that global max pooling will be applied.\n       classes: optional number of classes to classify images into, only to be\n         specified if `include_top` is True, and if no `weights` argument is\n         specified.\n@@ -734,16 +736,16 @@ def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n     \"\"\"A placeholder method for backward compatibility.\n \n     The preprocessing logic has been included in the convnext model\n-    implementation. Users are no longer required to call this method to normalize\n-    the input data. This method does nothing and only kept as a placeholder to\n-    align the API surface between old and new version of model.\n+    implementation. Users are no longer required to call this method to\n+    normalize the input data. This method does nothing and only kept as a\n+    placeholder to align the API surface between old and new version of model.\n \n     Args:\n       x: A floating point `numpy.array` or a `tf.Tensor`.\n       data_format: Optional data format of the image tensor/array. Defaults to\n         None, in which case the global setting\n-        `tf.keras.backend.image_data_format()` is used (unless you changed it, it\n-        defaults to \"channels_last\").{mode}\n+        `tf.keras.backend.image_data_format()` is used (unless you changed it,\n+        it defaults to \"channels_last\").{mode}\n \n     Returns:\n       Unchanged `numpy.array` or `tf.Tensor`.\n\n@@ -360,7 +360,8 @@ def EfficientNet(\n         # However, the original implemenetation uses (input - mean) / var to\n         # normalize the input, we need to divide another sqrt(var) to match the\n         # original implementation.\n-        # See https://github.com/tensorflow/tensorflow/issues/49930 for more details\n+        # See https://github.com/tensorflow/tensorflow/issues/49930 for more\n+        # details\n         x = layers.Rescaling(1.0 / tf.math.sqrt(IMAGENET_STDDEV_RGB))(x)\n \n     x = layers.ZeroPadding2D(\n@@ -390,7 +391,8 @@ def EfficientNet(\n         args[\"filters_out\"] = round_filters(args[\"filters_out\"])\n \n         for j in range(round_repeats(args.pop(\"repeats\"))):\n-            # The first block needs to take care of stride and filter size increase.\n+            # The first block needs to take care of stride and filter size\n+            # increase.\n             if j > 0:\n                 args[\"strides\"] = 1\n                 args[\"filters_in\"] = args[\"filters_out\"]\n@@ -840,9 +842,9 @@ def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n     \"\"\"A placeholder method for backward compatibility.\n \n     The preprocessing logic has been included in the efficientnet model\n-    implementation. Users are no longer required to call this method to normalize\n-    the input data. This method does nothing and only kept as a placeholder to\n-    align the API surface between old and new version of model.\n+    implementation. Users are no longer required to call this method to\n+    normalize the input data. This method does nothing and only kept as a\n+    placeholder to align the API surface between old and new version of model.\n \n     Args:\n       x: A floating point `numpy.array` or a `tf.Tensor`.\n\n@@ -560,15 +560,15 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n     https://keras.io/guides/transfer_learning/).\n \n   Note: each Keras Application expects a specific kind of input preprocessing.\n-  For EfficientNetV2, by default input preprocessing is included as a part of the\n-  model (as a `Rescaling` layer), and thus\n+  For EfficientNetV2, by default input preprocessing is included as a part of\n+  the model (as a `Rescaling` layer), and thus\n   `tf.keras.applications.efficientnet_v2.preprocess_input` is actually a\n-  pass-through function. In this use case, EfficientNetV2 models expect their inputs\n-  to be float tensors of pixels with values in the [0-255] range.\n+  pass-through function. In this use case, EfficientNetV2 models expect their\n+  inputs to be float tensors of pixels with values in the [0-255] range.\n   At the same time, preprocessing as a part of the model (i.e. `Rescaling`\n   layer) can be disabled by setting `include_preprocessing` argument to False.\n-  With preprocessing disabled EfficientNetV2 models expect their inputs to be float\n-  tensors of pixels with values in the [-1, 1] range.\n+  With preprocessing disabled EfficientNetV2 models expect their inputs to be\n+  float tensors of pixels with values in the [-1, 1] range.\n \n   Args:\n     include_top: Boolean, whether to include the fully-connected\n@@ -752,7 +752,8 @@ def FusedMBConvBlock(\n     survival_probability: float = 0.8,\n     name=None,\n ):\n-    \"\"\"Fused MBConv Block: Fusing the proj conv1x1 and depthwise_conv into a conv2d.\"\"\"\n+    \"\"\"Fused MBConv Block: Fusing the proj conv1x1 and depthwise_conv into a\n+    conv2d.\"\"\"\n     bn_axis = 3 if backend.image_data_format() == \"channels_last\" else 1\n \n     if name is None:\n@@ -863,7 +864,8 @@ def EfficientNetV2(\n     classifier_activation=\"softmax\",\n     include_preprocessing=True,\n ):\n-    \"\"\"Instantiates the EfficientNetV2 architecture using given scaling coefficients.\n+    \"\"\"Instantiates the EfficientNetV2 architecture using given scaling\n+    coefficients.\n \n     Args:\n       width_coefficient: float, scaling coefficient for network width.\n@@ -877,25 +879,27 @@ def EfficientNetV2(\n       activation: activation function.\n       blocks_args: list of dicts, parameters to construct block modules.\n       model_name: string, model name.\n-      include_top: whether to include the fully-connected layer at the top of the\n-        network.\n+      include_top: whether to include the fully-connected layer at the top of\n+        the network.\n       weights: one of `None` (random initialization), `\"imagenet\"` (pre-training\n         on ImageNet), or the path to the weights file to be loaded.\n       input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) or\n         numpy array to use as image input for the model.\n-      input_shape: optional shape tuple, only to be specified if `include_top` is\n-        False. It should have exactly 3 inputs channels.\n-      pooling: optional pooling mode for feature extraction when `include_top` is\n-        `False`. - `None` means that the output of the model will be the 4D tensor\n-        output of the last convolutional layer. - \"avg\" means that global average\n-        pooling will be applied to the output of the last convolutional layer, and\n-        thus the output of the model will be a 2D tensor. - `\"max\"` means that\n-        global max pooling will be applied.\n+      input_shape: optional shape tuple, only to be specified if `include_top`\n+        is False. It should have exactly 3 inputs channels.\n+      pooling: optional pooling mode for feature extraction when `include_top`\n+        is `False`.\n+        - `None` means that the output of the model will be the 4D tensor output\n+          of the last convolutional layer.\n+        - \"avg\" means that global average pooling will be applied to the output\n+          of the last convolutional layer, and thus the output of the model will\n+          be a 2D tensor.\n+        - `\"max\"` means that global max pooling will be applied.\n       classes: optional number of classes to classify images into, only to be\n         specified if `include_top` is True, and if no `weights` argument is\n         specified.\n-      classifier_activation: A string or callable. The activation function to use\n-        on the `\"top\"` layer. Ignored unless `include_top=True`. Set\n+      classifier_activation: A string or callable. The activation function to\n+        use on the `\"top\"` layer. Ignored unless `include_top=True`. Set\n         `classifier_activation=None` to return the logits of the `\"top\"` layer.\n       include_preprocessing: Boolean, whether to include the preprocessing layer\n         (`Rescaling`) at the bottom of the network. Defaults to `True`.\n@@ -1016,7 +1020,8 @@ def EfficientNetV2(\n             repeats=args.pop(\"num_repeat\"), depth_coefficient=depth_coefficient\n         )\n         for j in range(repeats):\n-            # The first block needs to take care of stride and filter size increase.\n+            # The first block needs to take care of stride and filter size\n+            # increase.\n             if j > 0:\n                 args[\"strides\"] = 1\n                 args[\"input_filters\"] = args[\"output_filters\"]\n@@ -1328,16 +1333,16 @@ def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n     \"\"\"A placeholder method for backward compatibility.\n \n     The preprocessing logic has been included in the EfficientNetV2 model\n-    implementation. Users are no longer required to call this method to normalize\n-    the input data. This method does nothing and only kept as a placeholder to\n-    align the API surface between old and new version of model.\n+    implementation. Users are no longer required to call this method to\n+    normalize the input data. This method does nothing and only kept as a\n+    placeholder to align the API surface between old and new version of model.\n \n     Args:\n       x: A floating point `numpy.array` or a `tf.Tensor`.\n       data_format: Optional data format of the image tensor/array. Defaults to\n         None, in which case the global setting\n-        `tf.keras.backend.image_data_format()` is used (unless you changed it, it\n-        defaults to \"channels_last\").{mode}\n+        `tf.keras.backend.image_data_format()` is used (unless you changed it,\n+        it defaults to \"channels_last\").{mode}\n \n     Returns:\n       Unchanged `numpy.array` or `tf.Tensor`.\n\n@@ -331,8 +331,8 @@ def inception_resnet_block(x, scale, block_type, block_idx, activation=\"relu\"):\n       x: input tensor.\n       scale: scaling factor to scale the residuals (i.e., the output of passing\n         `x` through an inception module) before adding them to the shortcut\n-        branch. Let `r` be the output from the residual branch, the output of this\n-        block will be `x + scale * r`.\n+        branch. Let `r` be the output from the residual branch, the output of\n+        this block will be `x + scale * r`.\n       block_type: `'block35'`, `'block17'` or `'block8'`, determines the network\n         structure in the residual branch.\n       block_idx: an `int` used for generating layer names. The Inception-ResNet\n\n@@ -73,9 +73,10 @@ def InceptionV3(\n       https://keras.io/guides/transfer_learning/).\n \n     Note: each Keras Application expects a specific kind of input preprocessing.\n-    For `InceptionV3`, call `tf.keras.applications.inception_v3.preprocess_input`\n-    on your inputs before passing them to the model.\n-    `inception_v3.preprocess_input` will scale input pixels between -1 and 1.\n+    For `InceptionV3`, call\n+    `tf.keras.applications.inception_v3.preprocess_input` on your inputs before\n+    passing them to the model. `inception_v3.preprocess_input` will scale input\n+    pixels between -1 and 1.\n \n     Args:\n       include_top: Boolean, whether to include the fully-connected\n@@ -84,8 +85,8 @@ def InceptionV3(\n         `imagenet` (pre-training on ImageNet),\n         or the path to the weights file to be loaded. Default to `imagenet`.\n       input_tensor: Optional Keras tensor (i.e. output of `layers.Input()`)\n-        to use as image input for the model. `input_tensor` is useful for sharing\n-        inputs between multiple different networks. Default to None.\n+        to use as image input for the model. `input_tensor` is useful for\n+        sharing inputs between multiple different networks. Default to None.\n       input_shape: Optional shape tuple, only to be specified\n         if `include_top` is False (otherwise the input shape\n         has to be `(299, 299, 3)` (with `channels_last` data format)\n\n@@ -160,7 +160,8 @@ def MobileNetV2(\n       include_top: Boolean, whether to include the fully-connected layer at the\n         top of the network. Defaults to `True`.\n       weights: String, one of `None` (random initialization), 'imagenet'\n-        (pre-training on ImageNet), or the path to the weights file to be loaded.\n+        (pre-training on ImageNet), or the path to the weights file to be\n+        loaded.\n       input_tensor: Optional Keras tensor (i.e. output of `layers.Input()`)\n         to use as image input for the model.\n       pooling: String, optional pooling mode for feature extraction when\n@@ -175,9 +176,9 @@ def MobileNetV2(\n             2D tensor.\n         - `max` means that global max pooling will\n             be applied.\n-      classes: Optional integer number of classes to classify images into, only to\n-        be specified if `include_top` is True, and if no `weights` argument is\n-        specified.\n+      classes: Optional integer number of classes to classify images into, only\n+        to be specified if `include_top` is True, and if no `weights` argument\n+        is specified.\n       classifier_activation: A `str` or callable. The activation function to use\n         on the \"top\" layer. Ignored unless `include_top=True`. Set\n         `classifier_activation=None` to return the logits of the \"top\" layer.\n@@ -491,7 +492,8 @@ def _inverted_res_block(inputs, expansion, stride, alpha, filters, block_id):\n \n     in_channels = backend.int_shape(inputs)[channel_axis]\n     pointwise_conv_filters = int(filters * alpha)\n-    # Ensure the number of filters on the last 1x1 convolution is divisible by 8.\n+    # Ensure the number of filters on the last 1x1 convolution is divisible by\n+    # 8.\n     pointwise_filters = _make_divisible(pointwise_conv_filters, 8)\n     x = inputs\n     prefix = \"block_{}_\".format(block_id)\n\n@@ -93,8 +93,8 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n   For MobileNetV3, by default input preprocessing is included as a part of the\n   model (as a `Rescaling` layer), and thus\n   `tf.keras.applications.mobilenet_v3.preprocess_input` is actually a\n-  pass-through function. In this use case, MobileNetV3 models expect their inputs\n-  to be float tensors of pixels with values in the [0-255] range.\n+  pass-through function. In this use case, MobileNetV3 models expect their\n+  inputs to be float tensors of pixels with values in the [0-255] range.\n   At the same time, preprocessing as a part of the model (i.e. `Rescaling`\n   layer) can be disabled by setting `include_preprocessing` argument to False.\n   With preprocessing disabled MobileNetV3 models expect their inputs to be float\n@@ -672,9 +672,9 @@ def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n     \"\"\"A placeholder method for backward compatibility.\n \n     The preprocessing logic has been included in the mobilenet_v3 model\n-    implementation. Users are no longer required to call this method to normalize\n-    the input data. This method does nothing and only kept as a placeholder to\n-    align the API surface between old and new version of model.\n+    implementation. Users are no longer required to call this method to\n+    normalize the input data. This method does nothing and only kept as a\n+    placeholder to align the API surface between old and new version of model.\n \n     Args:\n       x: A floating point `numpy.array` or a `tf.Tensor`.\n\n@@ -388,8 +388,8 @@ def NASNetMobile(\n         include_top: Whether to include the fully-connected\n             layer at the top of the network.\n         weights: `None` (random initialization) or\n-            `imagenet` (ImageNet weights)\n-            For loading `imagenet` weights, `input_shape` should be (224, 224, 3)\n+            `imagenet` (ImageNet weights). For loading `imagenet` weights,\n+            `input_shape` should be (224, 224, 3)\n         input_tensor: Optional Keras tensor (i.e. output of\n             `layers.Input()`)\n             to use as image input for the model.\n@@ -408,11 +408,11 @@ def NASNetMobile(\n         classes: Optional number of classes to classify images\n             into, only to be specified if `include_top` is True, and\n             if no `weights` argument is specified.\n-        classifier_activation: A `str` or callable. The activation function to use\n-            on the \"top\" layer. Ignored unless `include_top=True`. Set\n-            `classifier_activation=None` to return the logits of the \"top\" layer.\n-            When loading pretrained weights, `classifier_activation` can only\n-            be `None` or `\"softmax\"`.\n+        classifier_activation: A `str` or callable. The activation function to\n+            use on the \"top\" layer. Ignored unless `include_top=True`. Set\n+            `classifier_activation=None` to return the logits of the \"top\"\n+            layer.  When loading pretrained weights, `classifier_activation` can\n+            only be `None` or `\"softmax\"`.\n \n     Returns:\n         A Keras model instance.\n@@ -476,8 +476,8 @@ def NASNetLarge(\n         include_top: Whether to include the fully-connected\n             layer at the top of the network.\n         weights: `None` (random initialization) or\n-            `imagenet` (ImageNet weights)\n-            For loading `imagenet` weights, `input_shape` should be (331, 331, 3)\n+            `imagenet` (ImageNet weights).  For loading `imagenet` weights,\n+            `input_shape` should be (331, 331, 3)\n         input_tensor: Optional Keras tensor (i.e. output of\n             `layers.Input()`)\n             to use as image input for the model.\n@@ -496,11 +496,11 @@ def NASNetLarge(\n         classes: Optional number of classes to classify images\n             into, only to be specified if `include_top` is True, and\n             if no `weights` argument is specified.\n-        classifier_activation: A `str` or callable. The activation function to use\n-            on the \"top\" layer. Ignored unless `include_top=True`. Set\n-            `classifier_activation=None` to return the logits of the \"top\" layer.\n-            When loading pretrained weights, `classifier_activation` can only\n-            be `None` or `\"softmax\"`.\n+        classifier_activation: A `str` or callable. The activation function to\n+            use on the \"top\" layer. Ignored unless `include_top=True`. Set\n+            `classifier_activation=None` to return the logits of the \"top\"\n+            layer.  When loading pretrained weights, `classifier_activation` can\n+            only be `None` or `\"softmax\"`.\n \n     Returns:\n         A Keras model instance.\n\n@@ -490,9 +490,10 @@ def XBlock(filters_in, filters_out, group_width, stride=1, name=None):\n     def apply(inputs):\n         if filters_in != filters_out and stride == 1:\n             raise ValueError(\n-                f\"Input filters({filters_in}) and output filters({filters_out}) \"\n-                f\"are not equal for stride {stride}. Input and output filters must \"\n-                f\"be equal for stride={stride}.\"\n+                f\"Input filters({filters_in}) and output \"\n+                f\"filters({filters_out}) \"\n+                f\"are not equal for stride {stride}. Input and output filters \"\n+                f\"must be equal for stride={stride}.\"\n             )\n \n         # Declare layers\n@@ -590,9 +591,10 @@ def YBlock(\n     def apply(inputs):\n         if filters_in != filters_out and stride == 1:\n             raise ValueError(\n-                f\"Input filters({filters_in}) and output filters({filters_out}) \"\n-                f\"are not equal for stride {stride}. Input and output filters must  \"\n-                f\"be equal for stride={stride}.\"\n+                f\"Input filters({filters_in}) and output \"\n+                f\"filters({filters_out}) \"\n+                f\"are not equal for stride {stride}. Input and output filters \"\n+                f\"must be equal for stride={stride}.\"\n             )\n \n         groups = filters_out // group_width\n@@ -674,7 +676,8 @@ def ZBlock(\n     bottleneck_ratio=0.25,\n     name=None,\n ):\n-    \"\"\"Implementation of Z block Reference: [Fast and Accurate Model Scaling](https://arxiv.org/abs/2103.06877).\n+    \"\"\"Implementation of Z block Reference: [Fast and Accurate Model\n+    Scaling](https://arxiv.org/abs/2103.06877).\n \n     Args:\n       filters_in: filters in the input tensor\n@@ -694,8 +697,8 @@ def ZBlock(\n         if filters_in != filters_out and stride == 1:\n             raise ValueError(\n                 f\"Input filters({filters_in}) and output filters({filters_out})\"\n-                f\"are not equal for stride {stride}. Input and output filters must be\"\n-                f\" equal for stride={stride}.\"\n+                f\"are not equal for stride {stride}. Input and output filters \"\n+                f\"must be equal for stride={stride}.\"\n             )\n \n         groups = filters_out // group_width\n@@ -882,20 +885,20 @@ def RegNet(\n       model_name: An optional name for the model.\n       include_preprocessing: boolean denoting whther to include preprocessing in\n         the model\n-      include_top: Boolean denoting whether to include classification head to the\n-        model.\n-      weights: one of `None` (random initialization), \"imagenet\" (pre-training on\n-        ImageNet), or the path to the weights file to be loaded.\n-      input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to use\n-        as image input for the model.\n-      input_shape: optional shape tuple, only to be specified if `include_top` is\n-        False. It should have exactly 3 inputs channels.\n-      pooling: optional pooling mode for feature extraction when `include_top` is\n-        `False`. - `None` means that the output of the model will be the 4D tensor\n-        output of the last convolutional layer. - `avg` means that global average\n-        pooling will be applied to the output of the last convolutional layer, and\n-        thus the output of the model will be a 2D tensor. - `max` means that\n-        global max pooling will be applied.\n+      include_top: Boolean denoting whether to include classification head to\n+        the model.\n+      weights: one of `None` (random initialization), \"imagenet\" (pre-training\n+        on ImageNet), or the path to the weights file to be loaded.\n+      input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to\n+        use as image input for the model.\n+      input_shape: optional shape tuple, only to be specified if `include_top`\n+        is False. It should have exactly 3 inputs channels.\n+      pooling: optional pooling mode for feature extraction when `include_top`\n+        is `False`. - `None` means that the output of the model will be the 4D\n+        tensor output of the last convolutional layer. - `avg` means that global\n+        average pooling will be applied to the output of the last convolutional\n+        layer, and thus the output of the model will be a 2D tensor. - `max`\n+        means that global max pooling will be applied.\n       classes: optional number of classes to classify images into, only to be\n         specified if `include_top` is True, and if no `weights` argument is\n         specified.\n@@ -1810,16 +1813,16 @@ def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n     \"\"\"A placeholder method for backward compatibility.\n \n     The preprocessing logic has been included in the regnet model\n-    implementation. Users are no longer required to call this method to normalize\n-    the input data. This method does nothing and only kept as a placeholder to\n-    align the API surface between old and new version of model.\n+    implementation. Users are no longer required to call this method to\n+    normalize the input data. This method does nothing and only kept as a\n+    placeholder to align the API surface between old and new version of model.\n \n     Args:\n       x: A floating point `numpy.array` or a `tf.Tensor`.\n       data_format: Optional data format of the image tensor/array. Defaults to\n         None, in which case the global setting\n-        `tf.keras.backend.image_data_format()` is used (unless you changed it, it\n-        defaults to \"channels_last\").{mode}\n+        `tf.keras.backend.image_data_format()` is used (unless you changed it,\n+        it defaults to \"channels_last\").{mode}\n \n     Returns:\n       Unchanged `numpy.array` or `tf.Tensor`.\n\n@@ -191,10 +191,12 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n             specified.\n         classifier_activation: A `str` or callable. The activation function to\n             use on the \"top\" layer. Ignored unless `include_top=True`. Set\n-            `classifier_activation=None` to return the logits of the \"top\" layer.\n-        include_preprocessing: Boolean, whether to include the preprocessing layer\n-            (`Rescaling`) at the bottom of the network. Defaults to `True`.\n-            Note: Input image is normalized by ImageNet mean and standard deviation.\n+            `classifier_activation=None` to return the logits of the \"top\"\n+            layer.\n+        include_preprocessing: Boolean, whether to include the preprocessing\n+            layer (`Rescaling`) at the bottom of the network. Defaults to\n+            `True`.  Note: Input image is normalized by ImageNet mean and\n+            standard deviation.\n \n     Returns:\n         A `keras.Model` instance.\n@@ -461,7 +463,8 @@ def BlockGroup(\n         name = f\"block_group_{counter}\"\n \n     def apply(inputs):\n-        # Only the first block per block_group uses projection shortcut and strides.\n+        # Only the first block per block_group uses projection shortcut and\n+        # strides.\n         x = BottleneckBlock(\n             filters=filters,\n             strides=strides,\n@@ -504,7 +507,8 @@ def allow_bigger_recursion(target_limit: int):\n \n \n def fixed_padding(inputs, kernel_size):\n-    \"\"\"Pad the input along the spatial dimensions independently of input size.\"\"\"\n+    \"\"\"Pad the input along the spatial dimensions independently of input\n+    size.\"\"\"\n     pad_total = kernel_size - 1\n     pad_beg = pad_total // 2\n     pad_end = pad_total - pad_beg\n@@ -542,8 +546,8 @@ def ResNetRS(\n     Args:\n         depth: Depth of ResNet network.\n         input_shape: optional shape tuple. It should have exactly 3 inputs\n-          channels, and width and height should be no smaller than 32. E.g. (200,\n-          200, 3) would be one valid value.\n+          channels, and width and height should be no smaller than 32. E.g.\n+          (200, 200, 3) would be one valid value.\n         bn_momentum: Momentum parameter for Batch Normalization layers.\n         bn_epsilon: Epsilon parameter for Batch Normalization layers.\n         activation: activation function.\n@@ -555,27 +559,31 @@ def ResNetRS(\n         block_args: list of dicts, parameters to construct block modules.\n         model_name: name of the model.\n         pooling: optional pooling mode for feature extraction when `include_top`\n-          is `False`. - `None` means that the output of the model will be the 4D\n-          tensor output of the last convolutional layer. - `avg` means that global\n-          average pooling will be applied to the output of the last convolutional\n-          layer, and thus the output of the model will be a 2D tensor. - `max`\n-          means that global max pooling will be applied.\n-        weights: one of `None` (random initialization), `'imagenet'` (pre-training\n-          on ImageNet), or the path to the weights file to be loaded. Note- one\n-          model can have multiple imagenet variants depending on input shape it\n-          was trained with. For input_shape 224x224 pass `imagenet-i224` as\n-          argument. By default, highest input shape weights are downloaded.\n+          is `False`.\n+          - `None` means that the output of the model will be the 4D tensor\n+            output of the last convolutional layer.\n+          - `avg` means that global average pooling will be applied to the\n+            output of the last convolutional layer, and thus the output of the\n+            model will be a 2D tensor.\n+          - `max` means that global max pooling will be applied.\n+        weights: one of `None` (random initialization), `'imagenet'`\n+          (pre-training on ImageNet), or the path to the weights file to be\n+          loaded. Note- one model can have multiple imagenet variants depending\n+          on input shape it was trained with. For input_shape 224x224 pass\n+          `imagenet-i224` as argument. By default, highest input shape weights\n+          are downloaded.\n         input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to\n           use as image input for the model.\n         classes: optional number of classes to classify images into, only to be\n           specified if `include_top` is True, and if no `weights` argument is\n           specified.\n-        classifier_activation: A `str` or callable. The activation function to use\n-          on the \"top\" layer. Ignored unless `include_top=True`. Set\n+        classifier_activation: A `str` or callable. The activation function to\n+          use on the \"top\" layer. Ignored unless `include_top=True`. Set\n           `classifier_activation=None` to return the logits of the \"top\" layer.\n-        include_preprocessing: Boolean, whether to include the preprocessing layer\n-          (`Rescaling`) at the bottom of the network. Defaults to `True`. Note-\n-          Input image is normalized by ImageNet mean and standard deviation.\n+        include_preprocessing: Boolean, whether to include the preprocessing\n+          layer (`Rescaling`) at the bottom of the network. Defaults to `True`.\n+          Note- Input image is normalized by ImageNet mean and standard\n+          deviation.\n \n     Returns:\n         A `tf.keras.Model` instance.\n@@ -610,7 +618,8 @@ def ResNetRS(\n \n     if weights in weights_allow_list and include_top and classes != 1000:\n         raise ValueError(\n-            f\"If using `weights` as `'imagenet'` or any of {weights_allow_list} \"\n+            f\"If using `weights` as `'imagenet'` or any \"\n+            f\"of {weights_allow_list} \"\n             f\"with `include_top` as true, `classes` should be 1000. \"\n             f\"Received classes={classes}\"\n         )\n\n@@ -73,8 +73,8 @@ def VGG16(\n     For VGG16, call `tf.keras.applications.vgg16.preprocess_input` on your\n     inputs before passing them to the model.\n     `vgg16.preprocess_input` will convert the input images from RGB to BGR,\n-    then will zero-center each color channel with respect to the ImageNet dataset,\n-    without scaling.\n+    then will zero-center each color channel with respect to the ImageNet\n+    dataset, without scaling.\n \n     Args:\n         include_top: whether to include the 3 fully-connected\n@@ -107,11 +107,11 @@ def VGG16(\n         classes: optional number of classes to classify images\n             into, only to be specified if `include_top` is True, and\n             if no `weights` argument is specified.\n-        classifier_activation: A `str` or callable. The activation function to use\n-            on the \"top\" layer. Ignored unless `include_top=True`. Set\n-            `classifier_activation=None` to return the logits of the \"top\" layer.\n-            When loading pretrained weights, `classifier_activation` can only\n-            be `None` or `\"softmax\"`.\n+        classifier_activation: A `str` or callable. The activation function to\n+            use on the \"top\" layer. Ignored unless `include_top=True`. Set\n+            `classifier_activation=None` to return the logits of the \"top\"\n+            layer.  When loading pretrained weights, `classifier_activation` can\n+            only be `None` or `\"softmax\"`.\n \n     Returns:\n       A `keras.Model` instance.\n\n@@ -73,8 +73,8 @@ def VGG19(\n     For VGG19, call `tf.keras.applications.vgg19.preprocess_input` on your\n     inputs before passing them to the model.\n     `vgg19.preprocess_input` will convert the input images from RGB to BGR,\n-    then will zero-center each color channel with respect to the ImageNet dataset,\n-    without scaling.\n+    then will zero-center each color channel with respect to the ImageNet\n+    dataset, without scaling.\n \n     Args:\n       include_top: whether to include the 3 fully-connected\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#25cdbaa23b15b6e5bb0b236d3fc829b390389fd7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1622 | Contributors (this commit): 25 | Commits (past 90d): 7 | Contributors (cumulative): 41 | DMM Complexity: None\n\nDIFF:\n@@ -175,7 +175,8 @@ def get_word_index(path=\"imdb_word_index.json\"):\n         path: where to cache the data (relative to `~/.keras/dataset`).\n \n     Returns:\n-        The word index dictionary. Keys are word strings, values are their index.\n+        The word index dictionary. Keys are word strings, values are their\n+        index.\n \n     Example:\n \n\n@@ -167,7 +167,8 @@ def get_word_index(path=\"reuters_word_index.json\"):\n         path: where to cache the data (relative to `~/.keras/dataset`).\n \n     Returns:\n-        The word index dictionary. Keys are word strings, values are their index.\n+        The word index dictionary. Keys are word strings, values are their\n+        index.\n     \"\"\"\n     origin_folder = (\n         \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4cb8159165ca3738e18ce699fee8f7af829fc1f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 26 | Churn Cumulative: 578 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -173,7 +173,8 @@ def _FilterGoldenProtoDict(golden_proto_dict, omit_golden_symbols_map):\n                 filtered_members = [\n                     m for m in members if m.name not in symbol_list\n                 ]\n-                # Two steps because protobuf repeated fields disallow slice assignment.\n+                # Two steps because protobuf repeated fields disallow slice\n+                # assignment.\n                 del members[:]\n                 members.extend(filtered_members)\n     return filtered_proto_dict\n@@ -203,11 +204,12 @@ class ApiCompatibilityTest(tf.test.TestCase):\n         \"\"\"Diff given dicts of protobufs and report differences a readable way.\n \n         Args:\n-          expected_dict: a dict of TFAPIObject protos constructed from golden files.\n-          actual_dict: a ict of TFAPIObject protos constructed by reading from the\n-            TF package linked to the test.\n-          verbose: Whether to log the full diffs, or simply report which files were\n-            different.\n+          expected_dict: a dict of TFAPIObject protos constructed from golden\n+            files.\n+          actual_dict: a ict of TFAPIObject protos constructed by reading from\n+            the TF package linked to the test.\n+          verbose: Whether to log the full diffs, or simply report which files\n+            were different.\n           update_goldens: Whether to update goldens when there are diffs found.\n           additional_missing_object_message: Message to print when a symbol is\n             missing.\n@@ -249,8 +251,8 @@ class ApiCompatibilityTest(tf.test.TestCase):\n                     diff_message = \"Change detected in python object: %s.\" % key\n                     verbose_diff_message = str(e)\n \n-            # All difference cases covered above. If any difference found, add to the\n-            # list.\n+            # All difference cases covered above. If any difference found, add\n+            # to the list.\n             if diff_message:\n                 diffs.append(diff_message)\n                 verbose_diffs.append(verbose_diff_message)\n@@ -273,8 +275,9 @@ class ApiCompatibilityTest(tf.test.TestCase):\n                     filepath = _KeyToFilePath(key, api_version)\n                     tf.io.gfile.remove(filepath)\n \n-                # If the files are only in actual (current library), these are new\n-                # modules. Write them to files. Also record all updates in files.\n+                # If the files are only in actual (current library), these are\n+                # new modules. Write them to files. Also record all updates in\n+                # files.\n                 for key in only_in_actual | set(updated_keys):\n                     filepath = _KeyToFilePath(key, api_version)\n                     file_io.write_string_to_file(\n@@ -332,7 +335,8 @@ class ApiCompatibilityTest(tf.test.TestCase):\n         )\n \n         # Diff them. Do not fail if called with update.\n-        # If the test is run to update goldens, only report diffs but do not fail.\n+        # If the test is run to update goldens, only report diffs but do not\n+        # fail.\n         self._AssertProtoDictEquals(\n             golden_proto_dict,\n             proto_dict,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f9fb9b44cea161cea007a963aead676888281c6e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 26 | Files Changed: 9 | Hunks: 14 | Methods Changed: 13 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 67 | Churn Cumulative: 2556 | Contributors (this commit): 10 | Commits (past 90d): 28 | Contributors (cumulative): 46 | DMM Complexity: 1.0\n\nDIFF:\n@@ -48,7 +48,8 @@ def generate_benchmark_params_cpu_gpu(*params_list):\n       *params_list: A list of tuples represents the benchmark parameters.\n \n     Returns:\n-      A list of strings with the benchmark name extended with CPU and GPU suffix.\n+      A list of strings with the benchmark name extended with CPU and GPU\n+      suffix.\n     \"\"\"\n     benchmark_params = []\n     for params in params_list:\n@@ -114,16 +115,16 @@ def measure_performance(\n       y: Target data. See `y` in the `fit()` method of `keras.Model`.\n       epochs: Integer. Number of epochs to train the model.\n         If unspecified, `epochs` will default to 2.\n-      batch_size: Integer. Number of samples per gradient update. If unspecified,\n-        `batch_size` will default to 32.\n-      run_iters: Integer. Number of iterations to run the performance measurement.\n-        If unspecified, `run_iters` will default to 4.\n+      batch_size: Integer. Number of samples per gradient update. If\n+        unspecified, `batch_size` will default to 32.\n+      run_iters: Integer. Number of iterations to run the performance\n+        measurement.  If unspecified, `run_iters` will default to 4.\n       optimizer: String (name of optimizer) or optimizer instance. See\n         `tf.keras.optimizers`.\n       loss: String (name of objective function), objective function or\n         `tf.keras.losses.Loss` instance. See `tf.keras.losses`.\n-      metrics: Lists of metrics to be evaluated by the model during training. See\n-        `metrics` in the `compile()` method of  `keras.Model`.\n+      metrics: Lists of metrics to be evaluated by the model during training.\n+        See `metrics` in the `compile()` method of  `keras.Model`.\n       verbose: 0, 1, 2. Verbosity mode. See `verbose` in the `fit()` method of\n         `keras.Model`. If unspecified, `verbose` will default to 0.\n       num_gpus: Number of GPUs to run the model.\n\n@@ -28,8 +28,8 @@ def _collective_communication(all_reduce_alg):\n     \"\"\"Return a CollectiveCommunication based on all_reduce_alg.\n \n     Args:\n-      all_reduce_alg: a string specifying which collective communication to pick,\n-        or None.\n+      all_reduce_alg: a string specifying which collective communication to\n+        pick, or None.\n \n     Returns:\n       tf.distribute.experimental.CollectiveCommunication object\n@@ -56,14 +56,16 @@ def _mirrored_cross_device_ops(all_reduce_alg, num_packs):\n     \"\"\"Return a CrossDeviceOps based on all_reduce_alg and num_packs.\n \n     Args:\n-      all_reduce_alg: a string specifying which cross device op to pick, or None.\n+      all_reduce_alg: a string specifying which cross device op to pick, or\n+        None.\n       num_packs: an integer specifying number of packs for the cross device op.\n \n     Returns:\n       tf.distribute.CrossDeviceOps object or None.\n \n     Raises:\n-      ValueError: if `all_reduce_alg` not in [None, \"nccl\", \"hierarchical_copy\"].\n+      ValueError: if `all_reduce_alg` not in [None, \"nccl\",\n+        \"hierarchical_copy\"].\n     \"\"\"\n     if all_reduce_alg is None:\n         return None\n\n@@ -73,10 +73,11 @@ class MicroBenchmarksBase(tf.test.Benchmark):\n             f_self = f_locals.get(\"self\", None)\n             if isinstance(f_self, tf.test.Benchmark):\n                 name = frame[3]  # Get the method name\n-                # This is a hack to get around the fact that some methods might have a\n-                # disable_tfrt decorator around them. In that case a function called\n-                # 'decorated' wraps the real called function underneath and so we\n-                # peek one deeper into the stack to get the real name.\n+                # This is a hack to get around the fact that some methods might\n+                # have a disable_tfrt decorator around them. In that case a\n+                # function called 'decorated' wraps the real called function\n+                # underneath and so we peek one deeper into the stack to get the\n+                # real name.\n                 if name == \"decorated\":\n                     continue\n                 else:\n\n@@ -36,7 +36,9 @@ class Cifar10CNNBenchmark(tf.test.Benchmark):\n         self.epochs = 5\n \n     def _build_model(self):\n-        \"\"\"Model from https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py.\"\"\"\n+        \"\"\"Model from\n+        https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py.\n+        \"\"\"\n         model = tf.keras.Sequential()\n         model.add(\n             tf.keras.layers.Conv2D(\n\n@@ -112,8 +112,8 @@ class CustomMnistBenchmark(tf.test.Benchmark):\n           loss_fn: See `loss_fn` in `train_function()` method.\n           optimizer: See `optimizer` in `train_function()` method.\n           batch_size: See `batch_size` in `train_function()` method.\n-          distribution_strategy: See `distribution_strategy` in `train_function()`\n-            method.\n+          distribution_strategy: See `distribution_strategy` in\n+            `train_function()` method.\n \n         Returns:\n           Sum of per_replica_losses.\n@@ -148,8 +148,8 @@ class CustomMnistBenchmark(tf.test.Benchmark):\n \n         Args:\n           model: Model function to be benchmarked.\n-          train_dataset: `tf.data` dataset. Should return a tuple of either (inputs,\n-            targets) or (inputs, targets, sample_weights).\n+          train_dataset: `tf.data` dataset. Should return a tuple of either\n+            (inputs, targets) or (inputs, targets, sample_weights).\n           loss_fn: `tf.keras.losses.Loss` instance.\n           optimizer: `tf.keras.optimizers` instance.\n           epochs: Integer. Number of epochs to train the model. If unspecified,\n\n@@ -39,7 +39,8 @@ class TextWithTransformerBenchmark(tf.test.Benchmark):\n         )\n \n     def _build_model(self):\n-        \"\"\"Model from https://keras.io/examples/nlp/text_classification_with_transformer/.\"\"\"\n+        \"\"\"Model from\n+        https://keras.io/examples/nlp/text_classification_with_transformer/.\"\"\"\n         embed_dim = 32\n         num_heads = 2\n         ff_dim = 32\n\n@@ -21,8 +21,14 @@ import uuid\n \n from tensorflow.python.profiler import profiler_v2 as profiler\n \n-def run_with_xprof(self, func, num_iters_xprof=100, enable_python_trace=True,\n-                   logdir='/tmp/layer_benchmark_xprof/'):\n+\n+def run_with_xprof(\n+    self,\n+    func,\n+    num_iters_xprof=100,\n+    enable_python_trace=True,\n+    logdir=\"/tmp/layer_benchmark_xprof/\",\n+):\n     suid = str(uuid.uuid4())\n     if enable_python_trace:\n         options = profiler.ProfilerOptions(python_tracer_level=1)\n\n@@ -42,8 +42,9 @@ class KerasMetricMemoryBenchmark(tf.test.Benchmark):\n             memory_usage_2 = memory_profiler.memory_usage(\n                 (self.uneven_thresholds_auc)\n             )\n-            # memory usage is a list of number which sampled when running the function\n-            # The pure memory consumption is approximately max(usage) - min(usage)\n+            # memory usage is a list of number which sampled when running the\n+            # function The pure memory consumption is approximately max(usage) -\n+            # min(usage)\n             memory_usage_1 = max(memory_usage_1) - min(memory_usage_1)\n             memory_usage_2 = max(memory_usage_2) - min(memory_usage_2)\n \n\n@@ -62,7 +62,8 @@ class KerasOptimizerBenchmark(\n \n         Args:\n           optimizer: The optimizer instance to be benchmarked.\n-          num_iters: The number of iterations to run for performance measurement.\n+          num_iters: The number of iterations to run for performance\n+            measurement.\n         \"\"\"\n         model, train_x, train_y = bidirect_imdb_lstm_config()\n         metrics, wall_time, extras = benchmark_util.measure_performance(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
