{"custom_id": "scrapy#ffdf6fe100cf0ac99a726eab9d927e65f4c01364", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 442 | Contributors (this commit): 12 | Commits (past 90d): 6 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -177,7 +177,7 @@ class AsyncDefAsyncioGenComplexSpider(SimpleSpider):\n     depth = 2\n \n     def _get_req(self, index, cb=None):\n-        return Request(self.mockserver.url(\"/status?n=200&request=%d\" % index),\n+        return Request(self.mockserver.url(f\"/status?n=200&request={index}\"),\n                        meta={'index': index},\n                        dont_filter=True,\n                        callback=cb)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a8aedbeb7c20c7e2ec041e49d1567a499cd3acdb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 2770 | Contributors (this commit): 2 | Commits (past 90d): 68 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -3,6 +3,7 @@ import itertools\n import logging\n from collections import deque\n from ipaddress import IPv4Address, IPv6Address\n+from typing import Dict, List, Optional, Union\n \n from h2.config import H2Configuration\n from h2.connection import H2Connection\n@@ -21,7 +22,6 @@ from twisted.internet.ssl import Certificate\n from twisted.protocols.policies import TimeoutMixin\n from twisted.python.failure import Failure\n from twisted.web.client import URI\n-from typing import Dict, List, Optional, Union\n from zope.interface import implementer\n \n from scrapy.core.http2.stream import Stream, StreamCloseReason\n\n@@ -2,6 +2,7 @@ import logging\n from enum import Enum\n from io import BytesIO\n from urllib.parse import urlparse\n+from typing import Dict, List, Optional, Tuple, TYPE_CHECKING\n \n from h2.errors import ErrorCodes\n from h2.exceptions import H2Error, ProtocolError, StreamClosedError\n@@ -10,8 +11,6 @@ from twisted.internet.defer import Deferred, CancelledError\n from twisted.internet.error import ConnectionClosed\n from twisted.python.failure import Failure\n from twisted.web.client import ResponseFailed\n-from typing import Dict\n-from typing import List, Optional, Tuple, TYPE_CHECKING\n \n from scrapy.http import Request\n from scrapy.http.headers import Headers\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8123c427373778398036bb375bf03b3cd6b240de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 35 | Files Changed: 1 | Hunks: 8 | Methods Changed: 10 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 56 | Churn Cumulative: 957 | Contributors (this commit): 16 | Commits (past 90d): 5 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -335,6 +335,19 @@ class CrawlSpiderTestCase(TestCase):\n     def tearDown(self):\n         self.mockserver.__exit__(None, None, None)\n \n+    @defer.inlineCallbacks\n+    def _run_spider(self, spider_cls):\n+        items = []\n+\n+        def _on_item_scraped(item):\n+            items.append(item)\n+\n+        crawler = self.runner.create_crawler(spider_cls)\n+        crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n+        with LogCapture() as log:\n+            yield crawler.crawl(self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n+        return log, items, crawler.stats\n+\n     @defer.inlineCallbacks\n     def test_crawlspider_with_parse(self):\n         self.runner.crawl(CrawlSpiderWithParseMethod, mockserver=self.mockserver)\n@@ -379,15 +392,7 @@ class CrawlSpiderTestCase(TestCase):\n     @mark.only_asyncio()\n     @defer.inlineCallbacks\n     def test_async_def_asyncio_parse_items_list(self):\n-        items = []\n-\n-        def _on_item_scraped(item):\n-            items.append(item)\n-\n-        crawler = self.runner.create_crawler(AsyncDefAsyncioReturnSpider)\n-        crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n-        with LogCapture() as log:\n-            yield crawler.crawl(self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n+        log, items, _ = yield self._run_spider(AsyncDefAsyncioReturnSpider)\n         self.assertIn(\"Got response 200\", str(log))\n         self.assertIn({'id': 1}, items)\n         self.assertIn({'id': 2}, items)\n@@ -410,27 +415,17 @@ class CrawlSpiderTestCase(TestCase):\n     @mark.only_asyncio()\n     @defer.inlineCallbacks\n     def test_async_def_asyncgen_parse(self):\n-        crawler = self.runner.create_crawler(AsyncDefAsyncioGenSpider)\n-        with LogCapture() as log:\n-            yield crawler.crawl(self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n+        log, _, stats = yield self._run_spider(AsyncDefAsyncioGenSpider)\n         self.assertIn(\"Got response 200\", str(log))\n-        itemcount = crawler.stats.get_value('item_scraped_count')\n+        itemcount = stats.get_value('item_scraped_count')\n         self.assertEqual(itemcount, 1)\n \n     @mark.only_asyncio()\n     @defer.inlineCallbacks\n     def test_async_def_asyncgen_parse_loop(self):\n-        items = []\n-\n-        def _on_item_scraped(item):\n-            items.append(item)\n-\n-        crawler = self.runner.create_crawler(AsyncDefAsyncioGenLoopSpider)\n-        crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n-        with LogCapture() as log:\n-            yield crawler.crawl(self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n+        log, items, stats = yield self._run_spider(AsyncDefAsyncioGenLoopSpider)\n         self.assertIn(\"Got response 200\", str(log))\n-        itemcount = crawler.stats.get_value('item_scraped_count')\n+        itemcount = stats.get_value('item_scraped_count')\n         self.assertEqual(itemcount, 10)\n         for i in range(10):\n             self.assertIn({'foo': i}, items)\n@@ -438,15 +433,8 @@ class CrawlSpiderTestCase(TestCase):\n     @mark.only_asyncio()\n     @defer.inlineCallbacks\n     def test_async_def_asyncgen_parse_complex(self):\n-        items = []\n-\n-        def _on_item_scraped(item):\n-            items.append(item)\n-\n-        crawler = self.runner.create_crawler(AsyncDefAsyncioGenComplexSpider)\n-        crawler.signals.connect(_on_item_scraped, signals.item_scraped)\n-        yield crawler.crawl(mockserver=self.mockserver)\n-        itemcount = crawler.stats.get_value('item_scraped_count')\n+        _, items, stats = yield self._run_spider(AsyncDefAsyncioGenComplexSpider)\n+        itemcount = stats.get_value('item_scraped_count')\n         self.assertEqual(itemcount, 156)\n         # some random items\n         for i in [1, 4, 21, 22, 207, 311]:\n@@ -457,9 +445,7 @@ class CrawlSpiderTestCase(TestCase):\n     @mark.only_asyncio()\n     @defer.inlineCallbacks\n     def test_async_def_asyncio_parse_reqs_list(self):\n-        crawler = self.runner.create_crawler(AsyncDefAsyncioReqsReturnSpider)\n-        with LogCapture() as log:\n-            yield crawler.crawl(self.mockserver.url(\"/status?n=200\"), mockserver=self.mockserver)\n+        log, *_ = yield self._run_spider(AsyncDefAsyncioReqsReturnSpider)\n         for req_id in range(3):\n             self.assertIn(\"Got response 200, req_id %d\" % req_id, str(log))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#90ca9350f56c8c450c3385590d77390ad1d77365", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 337 | Contributors (this commit): 15 | Commits (past 90d): 2 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -17,7 +17,7 @@ class TextTestResult(_TextTestResult):\n         plural = \"s\" if run != 1 else \"\"\n \n         writeln(self.separator2)\n-        writeln(f\"Ran {run} contract{plural} in {stop - start:.3f}\")\n+        writeln(f\"Ran {run} contract{plural} in {stop - start:.3f}s\")\n         writeln()\n \n         infos = []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a8e895e684184e967a751ea28a7102f21dd74834", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 39 | Churn Cumulative: 4553 | Contributors (this commit): 34 | Commits (past 90d): 35 | Contributors (cumulative): 64 | DMM Complexity: 1.0\n\nDIFF:\n@@ -360,6 +360,7 @@ class FeedExporter:\n             fields_to_export=feed_options['fields'],\n             encoding=feed_options['encoding'],\n             indent=feed_options['indent'],\n+            **feed_options['item_export_kwargs'],\n         )\n         slot = _FeedSlot(\n             file=file,\n\n@@ -121,6 +121,7 @@ def feed_complete_default_values_from_settings(feed, settings):\n     out.setdefault(\"fields\", settings.getlist(\"FEED_EXPORT_FIELDS\") or None)\n     out.setdefault(\"store_empty\", settings.getbool(\"FEED_STORE_EMPTY\"))\n     out.setdefault(\"uri_params\", settings[\"FEED_URI_PARAMS\"])\n+    out.setdefault(\"item_export_kwargs\", dict())\n     if settings[\"FEED_EXPORT_INDENT\"] is None:\n         out.setdefault(\"indent\", None)\n     else:\n\n@@ -1252,6 +1252,43 @@ class FeedExportTest(FeedExportTestBase):\n         for fmt in ['json', 'xml', 'csv']:\n             self.assertIn(f'Error storing {fmt} feed (2 items)', str(log))\n \n+    @defer.inlineCallbacks\n+    def test_extend_kwargs(self):\n+        items = [{'foo': 'FOO', 'bar': 'BAR'}]\n+\n+        expected_with_title_csv = 'foo,bar\\r\\nFOO,BAR\\r\\n'.encode('utf-8')\n+        expected_without_title_csv = 'FOO,BAR\\r\\n'.encode('utf-8')\n+        test_cases = [\n+            # with title\n+            {\n+                'options': {\n+                    'format': 'csv',\n+                    'item_export_kwargs': dict(include_headers_line=True),\n+                },\n+                'expected': expected_with_title_csv,\n+            },\n+            # without title\n+            {\n+                'options': {\n+                    'format': 'csv',\n+                    'item_export_kwargs': dict(include_headers_line=False),\n+                },\n+                'expected': expected_without_title_csv,\n+            },\n+        ]\n+\n+        for row in test_cases:\n+            feed_options = row['options']\n+            settings = {\n+                'FEEDS': {\n+                    self._random_temp_filename(): feed_options,\n+                },\n+                'FEED_EXPORT_INDENT': None,\n+            }\n+\n+            data = yield self.exported_data(items, settings)\n+            self.assertEqual(row['expected'], data[feed_options['format']])\n+\n \n class BatchDeliveriesTest(FeedExportTestBase):\n     __test__ = True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fc3c66ce950e945e7ac6e19fea31e8454147ac29", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 277 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -176,6 +176,7 @@ class FeedExportConfigTestCase(unittest.TestCase):\n             \"store_empty\": True,\n             \"uri_params\": (1, 2, 3, 4),\n             \"batch_item_count\": 2,\n+            \"item_export_kwargs\": dict(),\n         })\n \n     def test_feed_complete_default_values_from_settings_non_empty(self):\n@@ -198,6 +199,7 @@ class FeedExportConfigTestCase(unittest.TestCase):\n             \"store_empty\": True,\n             \"uri_params\": None,\n             \"batch_item_count\": 2,\n+            \"item_export_kwargs\": dict(),\n         })\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#71d2c2f1a319b570eb2026707d49c4f62d59296e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 3076 | Contributors (this commit): 26 | Commits (past 90d): 18 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -1263,7 +1263,7 @@ class FeedExportTest(FeedExportTestBase):\n             {\n                 'options': {\n                     'format': 'csv',\n-                    'item_export_kwargs': dict(include_headers_line=True),\n+                    'item_export_kwargs': {'include_headers_line': True},\n                 },\n                 'expected': expected_with_title_csv,\n             },\n@@ -1271,7 +1271,7 @@ class FeedExportTest(FeedExportTestBase):\n             {\n                 'options': {\n                     'format': 'csv',\n-                    'item_export_kwargs': dict(include_headers_line=False),\n+                    'item_export_kwargs': {'include_headers_line': False},\n                 },\n                 'expected': expected_without_title_csv,\n             },\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eff33a2e7950b29fd69adc40b17b7fe9b0e637c2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 16/16 | Churn Δ: 12 | Churn Cumulative: 1975 | Contributors (this commit): 25 | Commits (past 90d): 16 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -601,6 +601,7 @@ class Https11CustomCiphers(unittest.TestCase):\n \n class Http11MockServerTestCase(unittest.TestCase):\n     \"\"\"HTTP 1.1 test case with MockServer\"\"\"\n+    settings_dict = None\n \n     def setUp(self):\n         self.mockserver = MockServer()\n@@ -611,7 +612,7 @@ class Http11MockServerTestCase(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_download_with_content_length(self):\n-        crawler = get_crawler(SingleRequestSpider)\n+        crawler = get_crawler(SingleRequestSpider, self.settings_dict)\n         # http://localhost:8998/partial set Content-Length to 1024, use download_maxsize= 1000 to avoid\n         # download it\n         yield crawler.crawl(seed=Request(url=self.mockserver.url('/partial'), meta={'download_maxsize': 1000}))\n@@ -620,7 +621,7 @@ class Http11MockServerTestCase(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_download(self):\n-        crawler = get_crawler(SingleRequestSpider)\n+        crawler = get_crawler(SingleRequestSpider, self.settings_dict)\n         yield crawler.crawl(seed=Request(url=self.mockserver.url('')))\n         failure = crawler.spider.meta.get('failure')\n         self.assertTrue(failure is None)\n@@ -629,7 +630,7 @@ class Http11MockServerTestCase(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_download_gzip_response(self):\n-        crawler = get_crawler(SingleRequestSpider)\n+        crawler = get_crawler(SingleRequestSpider, self.settings_dict)\n         body = b'1' * 100  # PayloadResource requires body length to be 100\n         request = Request(self.mockserver.url('/payload'), method='POST',\n                           body=body, meta={'download_maxsize': 50})\n\n@@ -110,6 +110,11 @@ class Https2CustomCiphers(Https11CustomCiphers):\n \n class Http2MockServerTestCase(Http11MockServerTestCase):\n     \"\"\"HTTP 2.0 test case with MockServer\"\"\"\n+    settings_dict = {\n+        'DOWNLOAD_HANDLERS': {\n+            'https': 'scrapy.core.downloader.handlers.http2.H2DownloadHandler'\n+        }\n+    }\n \n \n class Https2ProxyTestCase(Http11ProxyTestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#307e35c6641c0d9cf4b251996ebefd1347c2c1fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 488 | Contributors (this commit): 14 | Commits (past 90d): 4 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -74,7 +74,7 @@ class CookiesMiddleware:\n         \"\"\"\n         decoded = {}\n         for key in (\"name\", \"value\", \"path\", \"domain\"):\n-            if not cookie.get(key):\n+            if cookie.get(key) is None:\n                 if key in (\"name\", \"value\"):\n                     msg = \"Invalid cookie found in request {}: {} ('{}' is missing)\"\n                     logger.warning(msg.format(request, cookie, key))\n\n@@ -322,6 +322,9 @@ class CookiesMiddlewareTest(TestCase):\n             cookies2 = [{'name': 'foo'}, {'name': 'key', 'value': 'value2'}]\n             req2 = Request('http://example.org/2', cookies=cookies2)\n             assert self.mw.process_request(req2, self.spider) is None\n+            cookies3 = [{'name': 'foo', 'value': None}, {'name': 'key', 'value': ''}]\n+            req3 = Request('http://example.org/3', cookies=cookies3)\n+            assert self.mw.process_request(req3, self.spider) is None\n             lc.check(\n                 (\"scrapy.downloadermiddlewares.cookies\",\n                  \"WARNING\",\n@@ -331,6 +334,11 @@ class CookiesMiddlewareTest(TestCase):\n                  \"WARNING\",\n                  \"Invalid cookie found in request <GET http://example.org/2>:\"\n                  \" {'name': 'foo'} ('value' is missing)\"),\n+                (\"scrapy.downloadermiddlewares.cookies\",\n+                 \"WARNING\",\n+                 \"Invalid cookie found in request <GET http://example.org/3>:\"\n+                 \" {'name': 'foo', 'value': None} ('value' is missing)\"),\n             )\n         self.assertCookieValEqual(req1.headers['Cookie'], 'key=value1')\n         self.assertCookieValEqual(req2.headers['Cookie'], 'key=value2')\n+        self.assertCookieValEqual(req3.headers['Cookie'], 'key=')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6565adc471360c2f542392befb7bb9448e8f171f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 77 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 16/9 | Churn Δ: 81 | Churn Cumulative: 1479 | Contributors (this commit): 22 | Commits (past 90d): 10 | Contributors (cumulative): 34 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,7 +18,7 @@ from scrapy.pipelines.files import FileException, FilesPipeline\n # TODO: from scrapy.pipelines.media import MediaPipeline\n from scrapy.settings import Settings\n from scrapy.utils.misc import md5sum\n-from scrapy.utils.python import to_bytes, get_func_args\n+from scrapy.utils.python import get_func_args, to_bytes\n \n \n class NoimagesDrop(DropItem):\n@@ -136,7 +136,7 @@ class ImagesPipeline(FilesPipeline):\n             if self._deprecated_convert_image:\n                 warnings.warn('ImagesPipeline.convert_image() method overriden in a deprecated way, '\n                               'overriden method does not accept response_body argument.',\n-                              category=ScrapyDeprecationWarning, stacklevel=1)\n+                              category=ScrapyDeprecationWarning)\n \n         if self._deprecated_convert_image:\n             image, buf = self.convert_image(orig_image)\n@@ -156,7 +156,7 @@ class ImagesPipeline(FilesPipeline):\n         if response_body is None:\n             warnings.warn('ImagesPipeline.convert_image() method called in a deprecated way, '\n                           'method called without response_body argument.',\n-                          category=ScrapyDeprecationWarning, stacklevel=1)\n+                          category=ScrapyDeprecationWarning, stacklevel=2)\n \n         if image.format == 'PNG' and image.mode == 'RGBA':\n             background = Image.new('RGBA', image.size, (255, 255, 255))\n\n@@ -5,6 +5,7 @@ import warnings\n from shutil import rmtree\n from tempfile import mkdtemp\n from unittest import skipIf\n+from unittest.mock import patch\n \n import attr\n from itemadapter import ItemAdapter\n@@ -12,7 +13,7 @@ from twisted.trial import unittest\n \n from scrapy.http import Request, Response\n from scrapy.item import Field, Item\n-from scrapy.pipelines.images import ImagesPipeline\n+from scrapy.pipelines.images import ImageException, ImagesPipeline\n from scrapy.settings import Settings\n from scrapy.utils.python import to_bytes\n \n@@ -93,6 +94,78 @@ class ImagesPipelineTestCase(unittest.TestCase):\n                                     info=object()),\n                          'thumbs/50/850233df65a5b83361798f532f1fc549cd13cbe9.jpg')\n \n+    def test_get_images_exception(self):\n+        self.pipeline.min_width = 100\n+        self.pipeline.min_height = 100\n+\n+        _, buf1 = _create_image('JPEG', 'RGB', (50, 50), (0, 0, 0))\n+        _, buf2 = _create_image('JPEG', 'RGB', (150, 50), (0, 0, 0))\n+        _, buf3 = _create_image('JPEG', 'RGB', (50, 150), (0, 0, 0))\n+\n+        resp1 = Response(url=\"https://dev.mydeco.com/mydeco.gif\", body=buf1.getvalue())\n+        resp2 = Response(url=\"https://dev.mydeco.com/mydeco.gif\", body=buf2.getvalue())\n+        resp3 = Response(url=\"https://dev.mydeco.com/mydeco.gif\", body=buf3.getvalue())\n+        req = Request(url=\"https://dev.mydeco.com/mydeco.gif\")\n+\n+        with self.assertRaises(ImageException):\n+            next(self.pipeline.get_images(response=resp1, request=req, info=object()))\n+        with self.assertRaises(ImageException):\n+            next(self.pipeline.get_images(response=resp2, request=req, info=object()))\n+        with self.assertRaises(ImageException):\n+            next(self.pipeline.get_images(response=resp3, request=req, info=object()))\n+\n+    def test_get_images_new(self):\n+        self.pipeline.min_width = 0\n+        self.pipeline.min_height = 0\n+        self.pipeline.thumbs = {'small': (20, 20)}\n+\n+        orig_im, buf = _create_image('JPEG', 'RGB', (50, 50), (0, 0, 0))\n+        orig_thumb, orig_thumb_buf = _create_image('JPEG', 'RGB', (20, 20), (0, 0, 0))\n+        resp = Response(url=\"https://dev.mydeco.com/mydeco.gif\", body=buf.getvalue())\n+        req = Request(url=\"https://dev.mydeco.com/mydeco.gif\")\n+\n+        get_images_gen = self.pipeline.get_images(response=resp, request=req, info=object())\n+\n+        path, new_im, new_buf = next(get_images_gen)\n+        self.assertEqual(path, 'full/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg')\n+        self.assertEqual(orig_im, new_im)\n+        self.assertEqual(buf.getvalue(), new_buf.getvalue())\n+\n+        thumb_path, thumb_img, thumb_buf = next(get_images_gen)\n+        self.assertEqual(thumb_path, 'thumbs/small/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg')\n+        self.assertEqual(thumb_img, thumb_img)\n+        self.assertEqual(orig_thumb_buf.getvalue(), thumb_buf.getvalue())\n+\n+    def test_get_images_old(self):\n+        self.pipeline.thumbs = {'small': (20, 20)}\n+        orig_im, buf = _create_image('JPEG', 'RGB', (50, 50), (0, 0, 0))\n+        resp = Response(url=\"https://dev.mydeco.com/mydeco.gif\", body=buf.getvalue())\n+        req = Request(url=\"https://dev.mydeco.com/mydeco.gif\")\n+\n+        def overridden_convert_image(image, size=None):\n+            im, buf = _create_image('JPEG', 'RGB', (50, 50), (0, 0, 0))\n+            return im, buf\n+\n+        with patch.object(self.pipeline, 'convert_image', overridden_convert_image):\n+            with warnings.catch_warnings(record=True) as w:\n+                warnings.simplefilter('always')\n+                get_images_gen = self.pipeline.get_images(response=resp, request=req, info=object())\n+                path, new_im, new_buf = next(get_images_gen)\n+                self.assertEqual(path, 'full/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg')\n+                self.assertEqual(orig_im.mode, new_im.mode)\n+                self.assertEqual(orig_im.getcolors(), new_im.getcolors())\n+                self.assertEqual(buf.getvalue(), new_buf.getvalue())\n+\n+                thumb_path, thumb_img, thumb_buf = next(get_images_gen)\n+                self.assertEqual(thumb_path, 'thumbs/small/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg')\n+                self.assertEqual(orig_im.mode, thumb_img.mode)\n+                self.assertEqual(orig_im.getcolors(), thumb_img.getcolors())\n+                self.assertEqual(buf.getvalue(), thumb_buf.getvalue())\n+\n+                expected_warning_msg = ('ImagesPipeline.convert_image() method overriden in a deprecated way, '\n+                                        'overriden method does not accept response_body argument.')\n+                self.assertEqual(len([warning for warning in w if expected_warning_msg in str(warning.message)]), 1)\n+\n     def test_convert_image_old(self):\n         # tests for old API\n         with warnings.catch_warnings(record=True) as w:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#959222df7e80334810636166a8adb60ef35728de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 244 | Contributors (this commit): 16 | Commits (past 90d): 4 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,7 +13,12 @@ class HttpProxyMiddleware:\n         self.auth_encoding = auth_encoding\n         self.proxies = {}\n         for type_, url in getproxies().items():\n+            try:\n                 self.proxies[type_] = self._get_proxy(url, type_)\n+            # some values such as '/var/run/docker.sock' can't be parsed\n+            # by _parse_proxy and as such should be skipped\n+            except ValueError:\n+                continue\n \n     @classmethod\n     def from_crawler(cls, crawler):\n\n@@ -123,7 +123,11 @@ class TestHttpProxyMiddleware(TestCase):\n \n     def test_no_proxy(self):\n         os.environ['http_proxy'] = 'https://proxy.for.http:3128'\n+        os.environ['no_proxy'] = '/var/run/docker.sock'\n         mw = HttpProxyMiddleware()\n+        # '/var/run/docker.sock' may be used by the user for\n+        # no_proxy value but is not parseable and should be skipped\n+        assert 'no' not in mw.proxies\n \n         os.environ['no_proxy'] = '*'\n         req = Request('http://noproxy.com')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#82ba7c8b529e004a62db4681f16a482ae4e769f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 171 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -123,11 +123,7 @@ class TestHttpProxyMiddleware(TestCase):\n \n     def test_no_proxy(self):\n         os.environ['http_proxy'] = 'https://proxy.for.http:3128'\n-        os.environ['no_proxy'] = '/var/run/docker.sock'\n         mw = HttpProxyMiddleware()\n-        # '/var/run/docker.sock' may be used by the user for\n-        # no_proxy value but is not parseable and should be skipped\n-        assert 'no' not in mw.proxies\n \n         os.environ['no_proxy'] = '*'\n         req = Request('http://noproxy.com')\n@@ -149,3 +145,10 @@ class TestHttpProxyMiddleware(TestCase):\n         req = Request('http://noproxy.com', meta={'proxy': 'http://proxy.com'})\n         assert mw.process_request(req, spider) is None\n         self.assertEqual(req.meta, {'proxy': 'http://proxy.com'})\n+\n+    def test_no_proxy_invalid_values(self):\n+        os.environ['no_proxy'] = '/var/run/docker.sock'\n+        mw = HttpProxyMiddleware()\n+        # '/var/run/docker.sock' may be used by the user for\n+        # no_proxy value but is not parseable and should be skipped\n+        assert 'no' not in mw.proxies\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6e8d20a07a8c1a1f3ad7b3c3d74b7d37f2b47327", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 56 | Lines Deleted: 38 | Files Changed: 6 | Hunks: 37 | Methods Changed: 25 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 94 | Churn Cumulative: 6461 | Contributors (this commit): 26 | Commits (past 90d): 109 | Contributors (cumulative): 36 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,8 +1,9 @@\n import warnings\n from time import time\n-from typing import Optional\n+from typing import Optional, Type, TypeVar\n from urllib.parse import urldefrag\n \n+from twisted.internet.base import DelayedCall\n from twisted.internet.defer import Deferred\n from twisted.internet.error import TimeoutError\n from twisted.web.client import URI\n@@ -10,14 +11,18 @@ from twisted.web.client import URI\n from scrapy.core.downloader.contextfactory import load_context_factory_from_settings\n from scrapy.core.downloader.webclient import _parse\n from scrapy.core.http2.agent import H2Agent, H2ConnectionPool, ScrapyProxyH2Agent\n+from scrapy.crawler import Crawler\n from scrapy.http import Request, Response\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n from scrapy.utils.python import to_bytes\n \n \n+H2DownloadHandlerOrSubclass = TypeVar(\"H2DownloadHandlerOrSubclass\", bound=\"H2DownloadHandler\")\n+\n+\n class H2DownloadHandler:\n-    def __init__(self, settings: Settings, crawler=None):\n+    def __init__(self, settings: Settings, crawler: Optional[Crawler] = None):\n         self._crawler = crawler\n \n         from twisted.internet import reactor\n@@ -25,14 +30,14 @@ class H2DownloadHandler:\n         self._context_factory = load_context_factory_from_settings(settings, crawler)\n \n     @classmethod\n-    def from_crawler(cls, crawler):\n+    def from_crawler(cls: Type[H2DownloadHandlerOrSubclass], crawler: Crawler) -> H2DownloadHandlerOrSubclass:\n         return cls(crawler.settings, crawler)\n \n     def download_request(self, request: Request, spider: Spider) -> Deferred:\n         agent = ScrapyH2Agent(\n             context_factory=self._context_factory,\n             pool=self._pool,\n-            crawler=self._crawler\n+            crawler=self._crawler,\n         )\n         return agent.download_request(request, spider)\n \n@@ -47,8 +52,9 @@ class ScrapyH2Agent:\n     def __init__(\n         self, context_factory,\n         pool: H2ConnectionPool,\n-        connect_timeout=10, bind_address: Optional[bytes] = None,\n-        crawler=None\n+        connect_timeout: int = 10,\n+        bind_address: Optional[bytes] = None,\n+        crawler: Optional[Crawler] = None,\n     ) -> None:\n         self._context_factory = context_factory\n         self._connect_timeout = connect_timeout\n@@ -80,7 +86,7 @@ class ScrapyH2Agent:\n                 proxy_uri=URI.fromBytes(to_bytes(proxy, encoding='ascii')),\n                 connect_timeout=timeout,\n                 bind_address=bind_address,\n-                pool=self._pool\n+                pool=self._pool,\n             )\n \n         return self._Agent(\n@@ -88,7 +94,7 @@ class ScrapyH2Agent:\n             context_factory=self._context_factory,\n             connect_timeout=timeout,\n             bind_address=bind_address,\n-            pool=self._pool\n+            pool=self._pool,\n         )\n \n     def download_request(self, request: Request, spider: Spider) -> Deferred:\n@@ -110,7 +116,7 @@ class ScrapyH2Agent:\n         return response\n \n     @staticmethod\n-    def _cb_timeout(response: Response, request: Request, timeout: float, timeout_cl) -> Response:\n+    def _cb_timeout(response: Response, request: Request, timeout: float, timeout_cl: DelayedCall) -> Response:\n         if timeout_cl.active():\n             timeout_cl.cancel()\n             return response\n\n@@ -1,5 +1,5 @@\n from collections import deque\n-from typing import Deque, Dict, List, Tuple, Optional\n+from typing import Deque, Dict, List, Optional, Tuple\n \n from twisted.internet import defer\n from twisted.internet.base import ReactorBase\n@@ -95,16 +95,18 @@ class H2ConnectionPool:\n \n class H2Agent:\n     def __init__(\n-        self, reactor: ReactorBase, pool: H2ConnectionPool,\n-        context_factory=BrowserLikePolicyForHTTPS(),\n-        connect_timeout: Optional[float] = None, bind_address: Optional[bytes] = None\n+        self,\n+        reactor: ReactorBase,\n+        pool: H2ConnectionPool,\n+        context_factory: BrowserLikePolicyForHTTPS = BrowserLikePolicyForHTTPS(),\n+        connect_timeout: Optional[float] = None,\n+        bind_address: Optional[bytes] = None,\n     ) -> None:\n         self._reactor = reactor\n         self._pool = pool\n         self._context_factory = AcceptableProtocolsContextFactory(context_factory, acceptable_protocols=[b'h2'])\n         self.endpoint_factory = _StandardEndpointFactory(\n-            self._reactor, self._context_factory,\n-            connect_timeout, bind_address\n+            self._reactor, self._context_factory, connect_timeout, bind_address\n         )\n \n     def get_endpoint(self, uri: URI):\n@@ -132,17 +134,20 @@ class H2Agent:\n \n class ScrapyProxyH2Agent(H2Agent):\n     def __init__(\n-        self, reactor: ReactorBase,\n-        proxy_uri: URI, pool: H2ConnectionPool,\n-        context_factory=BrowserLikePolicyForHTTPS(),\n-        connect_timeout: Optional[float] = None, bind_address: Optional[bytes] = None\n+        self,\n+        reactor: ReactorBase,\n+        proxy_uri: URI,\n+        pool: H2ConnectionPool,\n+        context_factory: BrowserLikePolicyForHTTPS = BrowserLikePolicyForHTTPS(),\n+        connect_timeout: Optional[float] = None,\n+        bind_address: Optional[bytes] = None,\n     ) -> None:\n         super(ScrapyProxyH2Agent, self).__init__(\n             reactor=reactor,\n             pool=pool,\n             context_factory=context_factory,\n             connect_timeout=connect_timeout,\n-            bind_address=bind_address\n+            bind_address=bind_address,\n         )\n         self._proxy_uri = proxy_uri\n \n\n@@ -29,6 +29,7 @@ from scrapy.http import Request\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n \n+\n logger = logging.getLogger(__name__)\n \n \n@@ -42,7 +43,11 @@ class InvalidNegotiatedProtocol(H2Error):\n \n \n class RemoteTerminatedConnection(H2Error):\n-    def __init__(self, remote_ip_address: Optional[Union[IPv4Address, IPv6Address]], event: ConnectionTerminated):\n+    def __init__(\n+        self,\n+        remote_ip_address: Optional[Union[IPv4Address, IPv6Address]],\n+        event: ConnectionTerminated,\n+    ) -> None:\n         self.remote_ip_address = remote_ip_address\n         self.terminate_event = event\n \n@@ -51,7 +56,7 @@ class RemoteTerminatedConnection(H2Error):\n \n \n class MethodNotAllowed405(H2Error):\n-    def __init__(self, remote_ip_address: Optional[Union[IPv4Address, IPv6Address]]):\n+    def __init__(self, remote_ip_address: Optional[Union[IPv4Address, IPv6Address]]) -> None:\n         self.remote_ip_address = remote_ip_address\n \n     def __str__(self) -> str:\n@@ -220,13 +225,13 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         self.conn.initiate_connection()\n         self._write_to_transport()\n \n-    def _lose_connection_with_error(self, errors: List[BaseException]):\n+    def _lose_connection_with_error(self, errors: List[BaseException]) -> None:\n         \"\"\"Helper function to lose the connection with the error sent as a\n         reason\"\"\"\n         self._conn_lost_errors += errors\n         self.transport.loseConnection()\n \n-    def handshakeCompleted(self):\n+    def handshakeCompleted(self) -> None:\n         \"\"\"We close the connection with InvalidNegotiatedProtocol exception\n         when the connection was not made via h2 protocol\"\"\"\n         negotiated_protocol = self.transport.negotiatedProtocol\n@@ -263,7 +268,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         finally:\n             self._write_to_transport()\n \n-    def timeoutConnection(self):\n+    def timeoutConnection(self) -> None:\n         \"\"\"Called when the connection times out.\n         We lose the connection with TimeoutError\"\"\"\n \n\n@@ -19,6 +19,7 @@ from scrapy.responsetypes import responsetypes\n if TYPE_CHECKING:\n     from scrapy.core.http2.protocol import H2ClientProtocol\n \n+\n logger = logging.getLogger(__name__)\n \n \n@@ -27,7 +28,7 @@ class InactiveStreamClosed(ConnectionClosed):\n     of the stream. This happens when a stream is waiting for other\n     streams to close and connection is lost.\"\"\"\n \n-    def __init__(self, request: Request):\n+    def __init__(self, request: Request) -> None:\n         self.request = request\n \n     def __str__(self) -> str:\n@@ -139,7 +140,7 @@ class Stream:\n             'headers': Headers({}),\n         }\n \n-        def _cancel(_):\n+        def _cancel(_) -> None:\n             # Close this stream as gracefully as possible\n             # If the associated request is initiated we reset this stream\n             # else we directly call close() method\n@@ -360,7 +361,7 @@ class Stream:\n         self,\n         reason: StreamCloseReason,\n         errors: Optional[List[BaseException]] = None,\n-        from_protocol: bool = False\n+        from_protocol: bool = False,\n     ) -> None:\n         \"\"\"Based on the reason sent we will handle each case.\n         \"\"\"\n\n@@ -2,6 +2,7 @@ import contextlib\n import os\n import shutil\n import tempfile\n+from typing import Optional, Type\n from unittest import mock\n \n from testfixtures import LogCapture\n@@ -206,7 +207,7 @@ class LargeChunkedFileResource(resource.Resource):\n \n class HttpTestCase(unittest.TestCase):\n     scheme = 'http'\n-    download_handler_cls = HTTPDownloadHandler\n+    download_handler_cls: Type = HTTPDownloadHandler\n \n     # only used for HTTPS tests\n     keyfile = 'keys/localhost.key'\n@@ -365,7 +366,7 @@ class HttpTestCase(unittest.TestCase):\n \n class Http10TestCase(HttpTestCase):\n     \"\"\"HTTP 1.0 test case\"\"\"\n-    download_handler_cls = HTTP10DownloadHandler\n+    download_handler_cls: Type = HTTP10DownloadHandler\n \n \n class Https10TestCase(Http10TestCase):\n@@ -374,7 +375,7 @@ class Https10TestCase(Http10TestCase):\n \n class Http11TestCase(HttpTestCase):\n     \"\"\"HTTP 1.1 test case\"\"\"\n-    download_handler_cls = HTTP11DownloadHandler\n+    download_handler_cls: Type = HTTP11DownloadHandler\n \n     def test_download_without_maxsize_limit(self):\n         request = Request(self.getURL('file'))\n@@ -561,7 +562,7 @@ class Https11InvalidDNSPattern(Https11TestCase):\n \n class Https11CustomCiphers(unittest.TestCase):\n     scheme = 'https'\n-    download_handler_cls = HTTP11DownloadHandler\n+    download_handler_cls: Type = HTTP11DownloadHandler\n \n     keyfile = 'keys/localhost.key'\n     certfile = 'keys/localhost.crt'\n@@ -601,7 +602,7 @@ class Https11CustomCiphers(unittest.TestCase):\n \n class Http11MockServerTestCase(unittest.TestCase):\n     \"\"\"HTTP 1.1 test case with MockServer\"\"\"\n-    settings_dict = None\n+    settings_dict: Optional[dict] = None\n \n     def setUp(self):\n         self.mockserver = MockServer()\n@@ -668,7 +669,7 @@ class UriResource(resource.Resource):\n \n \n class HttpProxyTestCase(unittest.TestCase):\n-    download_handler_cls = HTTPDownloadHandler\n+    download_handler_cls: Type = HTTPDownloadHandler\n     expected_http_proxy_request_body = b'http://example.com'\n \n     def setUp(self):\n@@ -721,14 +722,14 @@ class HttpProxyTestCase(unittest.TestCase):\n \n \n class Http10ProxyTestCase(HttpProxyTestCase):\n-    download_handler_cls = HTTP10DownloadHandler\n+    download_handler_cls: Type = HTTP10DownloadHandler\n \n     def test_download_with_proxy_https_noconnect(self):\n         raise unittest.SkipTest('noconnect is not supported in HTTP10DownloadHandler')\n \n \n class Http11ProxyTestCase(HttpProxyTestCase):\n-    download_handler_cls = HTTP11DownloadHandler\n+    download_handler_cls: Type = HTTP11DownloadHandler\n \n     @defer.inlineCallbacks\n     def test_download_with_proxy_https_timeout(self):\n@@ -776,7 +777,7 @@ class S3AnonTestCase(unittest.TestCase):\n \n \n class S3TestCase(unittest.TestCase):\n-    download_handler_cls = S3DownloadHandler\n+    download_handler_cls: Type = S3DownloadHandler\n \n     # test use same example keys than amazon developer guide\n     # http://s3.amazonaws.com/awsdocs/S3/20060301/s3-dg-20060301.pdf\n\n@@ -45,7 +45,7 @@ def make_html_body(val):\n \n class DummySpider(Spider):\n     name = 'dummy'\n-    start_urls = []\n+    start_urls: list = []\n \n     def parse(self, response):\n         print(response)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5e997587d9b13344a0afa9bb4cf781829a66ce23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 170 | Files Changed: 7 | Hunks: 39 | Methods Changed: 22 | Complexity Δ (Sum/Max): -21/2 | Churn Δ: 209 | Churn Cumulative: 6308 | Contributors (this commit): 50 | Commits (past 90d): 38 | Contributors (cumulative): 124 | DMM Complexity: 0.3050847457627119\n\nDIFF:\n@@ -2,41 +2,20 @@ from urllib.parse import unquote\n \n from scrapy.core.downloader.handlers.http import HTTPDownloadHandler\n from scrapy.exceptions import NotConfigured\n-from scrapy.utils.boto import is_botocore\n+from scrapy.utils.boto import is_botocore_available\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import create_instance\n \n \n-def _get_boto_connection():\n-    from boto.s3.connection import S3Connection\n-\n-    class _v19_S3Connection(S3Connection):\n-        \"\"\"A dummy S3Connection wrapper that doesn't do any synchronous download\"\"\"\n-        def _mexe(self, method, bucket, key, headers, *args, **kwargs):\n-            return headers\n-\n-    class _v20_S3Connection(S3Connection):\n-        \"\"\"A dummy S3Connection wrapper that doesn't do any synchronous download\"\"\"\n-        def _mexe(self, http_request, *args, **kwargs):\n-            http_request.authorize(connection=self)\n-            return http_request.headers\n-\n-    try:\n-        import boto.auth  # noqa: F401\n-    except ImportError:\n-        _S3Connection = _v19_S3Connection\n-    else:\n-        _S3Connection = _v20_S3Connection\n-\n-    return _S3Connection\n-\n-\n class S3DownloadHandler:\n \n     def __init__(self, settings, *,\n                  crawler=None,\n                  aws_access_key_id=None, aws_secret_access_key=None,\n                  httpdownloadhandler=HTTPDownloadHandler, **kw):\n+        if not is_botocore_available():\n+            raise NotConfigured('missing botocore library')\n+\n         if not aws_access_key_id:\n             aws_access_key_id = settings['AWS_ACCESS_KEY_ID']\n         if not aws_secret_access_key:\n@@ -51,7 +30,6 @@ class S3DownloadHandler:\n         self.anon = kw.get('anon')\n \n         self._signer = None\n-        if is_botocore():\n         import botocore.auth\n         import botocore.credentials\n         kw.pop('anon', None)\n@@ -61,13 +39,6 @@ class S3DownloadHandler:\n             SignerCls = botocore.auth.AUTH_TYPE_MAPS['s3']\n             self._signer = SignerCls(botocore.credentials.Credentials(\n                 aws_access_key_id, aws_secret_access_key))\n-        else:\n-            _S3Connection = _get_boto_connection()\n-            try:\n-                self.conn = _S3Connection(\n-                    aws_access_key_id, aws_secret_access_key, **kw)\n-            except Exception as ex:\n-                raise NotConfigured(str(ex))\n \n         _http_handler = create_instance(\n             objcls=httpdownloadhandler,\n\n@@ -19,7 +19,7 @@ from zope.interface import implementer, Interface\n \n from scrapy import signals\n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n-from scrapy.utils.boto import is_botocore\n+from scrapy.utils.boto import is_botocore_available\n from scrapy.utils.conf import feed_complete_default_values_from_settings\n from scrapy.utils.ftp import ftp_store_file\n from scrapy.utils.log import failure_to_exc_info\n@@ -120,22 +120,19 @@ class S3FeedStorage(BlockingFeedStorage):\n \n     def __init__(self, uri, access_key=None, secret_key=None, acl=None, *,\n                  feed_options=None):\n+        if not is_botocore_available():\n+            raise NotConfigured('missing botocore library')\n         u = urlparse(uri)\n         self.bucketname = u.hostname\n         self.access_key = u.username or access_key\n         self.secret_key = u.password or secret_key\n-        self.is_botocore = is_botocore()\n         self.keyname = u.path[1:]  # remove first \"/\"\n         self.acl = acl\n-        if self.is_botocore:\n         import botocore.session\n         session = botocore.session.get_session()\n         self.s3_client = session.create_client(\n             's3', aws_access_key_id=self.access_key,\n             aws_secret_access_key=self.secret_key)\n-        else:\n-            import boto\n-            self.connect_s3 = boto.connect_s3\n         if feed_options and feed_options.get('overwrite', True) is False:\n             logger.warning('S3 does not support appending to files. To '\n                            'suppress this warning, remove the overwrite '\n@@ -154,18 +151,10 @@ class S3FeedStorage(BlockingFeedStorage):\n \n     def _store_in_thread(self, file):\n         file.seek(0)\n-        if self.is_botocore:\n         kwargs = {'ACL': self.acl} if self.acl else {}\n         self.s3_client.put_object(\n             Bucket=self.bucketname, Key=self.keyname, Body=file,\n             **kwargs)\n-        else:\n-            conn = self.connect_s3(self.access_key, self.secret_key)\n-            bucket = conn.get_bucket(self.bucketname, validate=False)\n-            key = bucket.new_key(self.keyname)\n-            kwargs = {'policy': self.acl} if self.acl else {}\n-            key.set_contents_from_file(file, **kwargs)\n-            key.close()\n         file.close()\n \n \n\n@@ -11,7 +11,6 @@ import os\n import time\n from collections import defaultdict\n from contextlib import suppress\n-from email.utils import mktime_tz, parsedate_tz\n from ftplib import FTP\n from io import BytesIO\n from urllib.parse import urlparse\n@@ -23,7 +22,7 @@ from scrapy.exceptions import IgnoreRequest, NotConfigured\n from scrapy.http import Request\n from scrapy.pipelines.media import MediaPipeline\n from scrapy.settings import Settings\n-from scrapy.utils.boto import is_botocore\n+from scrapy.utils.boto import is_botocore_available\n from scrapy.utils.datatypes import CaselessDict\n from scrapy.utils.ftp import ftp_store_file\n from scrapy.utils.log import failure_to_exc_info\n@@ -91,8 +90,8 @@ class S3FilesStore:\n     }\n \n     def __init__(self, uri):\n-        self.is_botocore = is_botocore()\n-        if self.is_botocore:\n+        if not is_botocore_available():\n+            raise NotConfigured('missing botocore library')\n         import botocore.session\n         session = botocore.session.get_session()\n         self.s3_client = session.create_client(\n@@ -104,50 +103,30 @@ class S3FilesStore:\n             use_ssl=self.AWS_USE_SSL,\n             verify=self.AWS_VERIFY\n         )\n-        else:\n-            from boto.s3.connection import S3Connection\n-            self.S3Connection = S3Connection\n         if not uri.startswith(\"s3://\"):\n             raise ValueError(f\"Incorrect URI scheme in {uri}, expected 's3'\")\n         self.bucket, self.prefix = uri[5:].split('/', 1)\n \n     def stat_file(self, path, info):\n         def _onsuccess(boto_key):\n-            if self.is_botocore:\n             checksum = boto_key['ETag'].strip('\"')\n             last_modified = boto_key['LastModified']\n             modified_stamp = time.mktime(last_modified.timetuple())\n-            else:\n-                checksum = boto_key.etag.strip('\"')\n-                last_modified = boto_key.last_modified\n-                modified_tuple = parsedate_tz(last_modified)\n-                modified_stamp = int(mktime_tz(modified_tuple))\n             return {'checksum': checksum, 'last_modified': modified_stamp}\n \n         return self._get_boto_key(path).addCallback(_onsuccess)\n \n-    def _get_boto_bucket(self):\n-        # disable ssl (is_secure=False) because of this python bug:\n-        # https://bugs.python.org/issue5103\n-        c = self.S3Connection(self.AWS_ACCESS_KEY_ID, self.AWS_SECRET_ACCESS_KEY, is_secure=False)\n-        return c.get_bucket(self.bucket, validate=False)\n-\n     def _get_boto_key(self, path):\n         key_name = f'{self.prefix}{path}'\n-        if self.is_botocore:\n         return threads.deferToThread(\n             self.s3_client.head_object,\n             Bucket=self.bucket,\n             Key=key_name)\n-        else:\n-            b = self._get_boto_bucket()\n-            return threads.deferToThread(b.get_key, key_name)\n \n     def persist_file(self, path, buf, info, meta=None, headers=None):\n         \"\"\"Upload file to S3 storage\"\"\"\n         key_name = f'{self.prefix}{path}'\n         buf.seek(0)\n-        if self.is_botocore:\n         extra = self._headers_to_botocore_kwargs(self.HEADERS)\n         if headers:\n             extra.update(self._headers_to_botocore_kwargs(headers))\n@@ -159,18 +138,6 @@ class S3FilesStore:\n             Metadata={k: str(v) for k, v in (meta or {}).items()},\n             ACL=self.POLICY,\n             **extra)\n-        else:\n-            b = self._get_boto_bucket()\n-            k = b.new_key(key_name)\n-            if meta:\n-                for metakey, metavalue in meta.items():\n-                    k.set_metadata(metakey, str(metavalue))\n-            h = self.HEADERS.copy()\n-            if headers:\n-                h.update(headers)\n-            return threads.deferToThread(\n-                k.set_contents_from_string, buf.getvalue(),\n-                headers=h, policy=self.POLICY)\n \n     def _headers_to_botocore_kwargs(self, headers):\n         \"\"\" Convert headers to botocore keyword agruments.\n\n@@ -1,11 +1,32 @@\n \"\"\"Boto/botocore helpers\"\"\"\n+import warnings\n \n-from scrapy.exceptions import NotConfigured\n+from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n \n \n def is_botocore():\n+    \"\"\" Returns True if botocore is available, otherwise raises NotConfigured. Never returns False.\n+\n+    Previously, when boto was supported in addition to botocore, this returned False if boto was available\n+    but botocore wasn't.\n+    \"\"\"\n+    message = (\n+        'is_botocore() is deprecated and always returns True or raises an Exception, '\n+        'so it cannot be used for checking if boto is available instead of botocore. '\n+        'You can use scrapy.utils.boto.is_botocore_available() to check if botocore '\n+        'is available.'\n+    )\n+    warnings.warn(message, ScrapyDeprecationWarning, stacklevel=2)\n     try:\n         import botocore  # noqa: F401\n         return True\n     except ImportError:\n         raise NotConfigured('missing botocore library')\n+\n+\n+def is_botocore_available():\n+    try:\n+        import botocore  # noqa: F401\n+        return True\n+    except ImportError:\n+        return False\n\n@@ -10,8 +10,7 @@ from unittest import mock\n from importlib import import_module\n from twisted.trial.unittest import SkipTest\n \n-from scrapy.exceptions import NotConfigured\n-from scrapy.utils.boto import is_botocore\n+from scrapy.utils.boto import is_botocore_available\n \n \n def assert_aws_environ():\n@@ -29,29 +28,19 @@ def assert_gcs_environ():\n \n \n def skip_if_no_boto():\n-    try:\n-        is_botocore()\n-    except NotConfigured as e:\n-        raise SkipTest(e)\n+    if not is_botocore_available():\n+        raise SkipTest('missing botocore library')\n \n \n def get_s3_content_and_delete(bucket, path, with_key=False):\n     \"\"\" Get content from s3 key, and delete key afterwards.\n     \"\"\"\n-    if is_botocore():\n     import botocore.session\n     session = botocore.session.get_session()\n     client = session.create_client('s3')\n     key = client.get_object(Bucket=bucket, Key=path)\n     content = key['Body'].read()\n     client.delete_object(Bucket=bucket, Key=path)\n-    else:\n-        import boto\n-        # assuming boto=2.2.2\n-        bucket = boto.connect_s3().get_bucket(bucket, validate=False)\n-        key = bucket.get_key(path)\n-        content = key.get_contents_as_string()\n-        bucket.delete_key(path)\n     return (content, key) if with_key else content\n \n \n\n@@ -45,6 +45,7 @@ from scrapy.utils.test import (\n     get_s3_content_and_delete,\n     get_crawler,\n     mock_google_cloud_storage,\n+    skip_if_no_boto,\n )\n \n from tests.mockserver import MockFTPServer, MockServer\n@@ -227,10 +228,7 @@ class BlockingFeedStorageTest(unittest.TestCase):\n class S3FeedStorageTest(unittest.TestCase):\n \n     def test_parse_credentials(self):\n-        try:\n-            import botocore  # noqa: F401\n-        except ImportError:\n-            raise unittest.SkipTest(\"S3FeedStorage requires botocore\")\n+        skip_if_no_boto()\n         aws_credentials = {'AWS_ACCESS_KEY_ID': 'settings_key',\n                            'AWS_SECRET_ACCESS_KEY': 'settings_secret'}\n         crawler = get_crawler(settings_dict=aws_credentials)\n@@ -324,11 +322,7 @@ class S3FeedStorageTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_store_botocore_without_acl(self):\n-        try:\n-            import botocore  # noqa: F401\n-        except ImportError:\n-            raise unittest.SkipTest('botocore is required')\n-\n+        skip_if_no_boto()\n         storage = S3FeedStorage(\n             's3://mybucket/export.csv',\n             'access_key',\n@@ -344,11 +338,7 @@ class S3FeedStorageTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_store_botocore_with_acl(self):\n-        try:\n-            import botocore  # noqa: F401\n-        except ImportError:\n-            raise unittest.SkipTest('botocore is required')\n-\n+        skip_if_no_boto()\n         storage = S3FeedStorage(\n             's3://mybucket/export.csv',\n             'access_key',\n@@ -366,57 +356,6 @@ class S3FeedStorageTest(unittest.TestCase):\n             'custom-acl'\n         )\n \n-    @defer.inlineCallbacks\n-    def test_store_not_botocore_without_acl(self):\n-        storage = S3FeedStorage(\n-            's3://mybucket/export.csv',\n-            'access_key',\n-            'secret_key',\n-        )\n-        self.assertEqual(storage.access_key, 'access_key')\n-        self.assertEqual(storage.secret_key, 'secret_key')\n-        self.assertEqual(storage.acl, None)\n-\n-        storage.is_botocore = False\n-        storage.connect_s3 = mock.MagicMock()\n-        self.assertFalse(storage.is_botocore)\n-\n-        yield storage.store(BytesIO(b'test file'))\n-\n-        conn = storage.connect_s3(*storage.connect_s3.call_args)\n-        bucket = conn.get_bucket(*conn.get_bucket.call_args)\n-        key = bucket.new_key(*bucket.new_key.call_args)\n-        self.assertNotIn(\n-            dict(policy='custom-acl'),\n-            key.set_contents_from_file.call_args\n-        )\n-\n-    @defer.inlineCallbacks\n-    def test_store_not_botocore_with_acl(self):\n-        storage = S3FeedStorage(\n-            's3://mybucket/export.csv',\n-            'access_key',\n-            'secret_key',\n-            'custom-acl'\n-        )\n-        self.assertEqual(storage.access_key, 'access_key')\n-        self.assertEqual(storage.secret_key, 'secret_key')\n-        self.assertEqual(storage.acl, 'custom-acl')\n-\n-        storage.is_botocore = False\n-        storage.connect_s3 = mock.MagicMock()\n-        self.assertFalse(storage.is_botocore)\n-\n-        yield storage.store(BytesIO(b'test file'))\n-\n-        conn = storage.connect_s3(*storage.connect_s3.call_args)\n-        bucket = conn.get_bucket(*conn.get_bucket.call_args)\n-        key = bucket.new_key(*bucket.new_key.call_args)\n-        self.assertIn(\n-            dict(policy='custom-acl'),\n-            key.set_contents_from_file.call_args\n-        )\n-\n     def test_overwrite_default(self):\n         with LogCapture() as log:\n             S3FeedStorage(\n\n@@ -22,7 +22,6 @@ from scrapy.pipelines.files import (\n     S3FilesStore,\n )\n from scrapy.settings import Settings\n-from scrapy.utils.boto import is_botocore\n from scrapy.utils.test import (\n     assert_aws_environ,\n     assert_gcs_environ,\n@@ -437,16 +436,10 @@ class TestS3FilesStore(unittest.TestCase):\n         content, key = get_s3_content_and_delete(\n             u.hostname, u.path[1:], with_key=True)\n         self.assertEqual(content, data)\n-        if is_botocore():\n         self.assertEqual(key['Metadata'], {'foo': 'bar'})\n         self.assertEqual(\n             key['CacheControl'], S3FilesStore.HEADERS['Cache-Control'])\n         self.assertEqual(key['ContentType'], 'image/png')\n-        else:\n-            self.assertEqual(key.metadata, {'foo': 'bar'})\n-            self.assertEqual(\n-                key.cache_control, S3FilesStore.HEADERS['Cache-Control'])\n-            self.assertEqual(key.content_type, 'image/png')\n \n \n class TestGCSFilesStore(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
