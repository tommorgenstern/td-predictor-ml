{"custom_id": "keras#21a78464c191c40a90ed4e3ddfed747ae994703e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 140 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 11 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 140 | Churn Cumulative: 140 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,140 @@\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Test for custom Keras object saving with `register_keras_serializable`.\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+import os\n+import sys\n+\n+from keras import testing_utils\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+\n+# `tf.print` message is only available in stderr in TF2, which this test checks.\n+@testing_utils.run_v2_only\n+class CustomObjectSavingTest(tf.test.TestCase):\n+  \"\"\"Test for custom Keras object saving with `register_keras_serializable`.\"\"\"\n+\n+  def test_register_keras_serializable_correct_class(self):\n+    train_step_message = 'This is my training step'\n+    temp_dir = os.path.join(self.get_temp_dir(), 'my_model')\n+\n+    @tf.keras.utils.register_keras_serializable('CustomModelX')\n+    class CustomModelX(tf.keras.Model):\n+\n+      def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.dense1 = MyDense(\n+            1,\n+            kernel_regularizer=MyRegularizer(0.01),\n+            activity_regularizer=MyRegularizer(0.01))\n+\n+      def call(self, inputs):\n+        return self.dense1(inputs)\n+\n+      def train_step(self, data):\n+        tf.print(train_step_message)\n+        x, y = data\n+        with tf.GradientTape() as tape:\n+          y_pred = self(x)\n+          loss = self.compiled_loss(y, y_pred)\n+\n+        gradients = tape.gradient(loss, self.trainable_variables)\n+        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n+        return {}\n+\n+      def one(self):\n+        return 1\n+\n+    @tf.keras.utils.register_keras_serializable('MyDense')\n+    class MyDense(tf.keras.layers.Dense):\n+\n+      def two(self):\n+        return 2\n+\n+    @tf.keras.utils.register_keras_serializable('MyAdam')\n+    class MyAdam(tf.keras.optimizers.Adam):\n+\n+      def three(self):\n+        return 3\n+\n+    @tf.keras.utils.register_keras_serializable('MyLoss')\n+    class MyLoss(tf.keras.losses.MeanSquaredError):\n+\n+      def four(self):\n+        return 4\n+\n+    @tf.keras.utils.register_keras_serializable('MyMetric')\n+    class MyMetric(tf.keras.metrics.MeanAbsoluteError):\n+\n+      def five(self):\n+        return 5\n+\n+    @tf.keras.utils.register_keras_serializable('MyRegularizer')\n+    class MyRegularizer(tf.keras.regularizers.L2):\n+\n+      def six(self):\n+        return 6\n+\n+    @tf.keras.utils.register_keras_serializable('my_sq_diff')\n+    def my_sq_diff(y_true, y_pred):\n+      y_pred = tf.convert_to_tensor(y_pred)\n+      y_true = tf.cast(y_true, y_pred.dtype)\n+      sq_diff_plus_x = tf.math.squared_difference(y_pred, y_true)\n+      return tf.reduce_mean(sq_diff_plus_x, axis=-1)\n+\n+    subclassed_model = CustomModelX()\n+    subclassed_model.compile(\n+        optimizer=MyAdam(), loss=MyLoss(), metrics=[MyMetric(), my_sq_diff])\n+\n+    x = np.random.random((100, 32))\n+    y = np.random.random((100, 1))\n+    subclassed_model.fit(x, y, epochs=1)\n+    subclassed_model.save(temp_dir, save_format='tf')\n+\n+    loaded_model = tf.keras.models.load_model(temp_dir)\n+\n+    # `tf.print` writes to stderr.\n+    with self.captureWritesToStream(sys.stderr) as printed:\n+      loaded_model.fit(x, y, epochs=1)\n+      self.assertRegex(printed.contents(), train_step_message)\n+\n+    # Check that the custom classes do get used.\n+    self.assertIs(loaded_model.__class__, CustomModelX)\n+    self.assertIs(loaded_model.optimizer.__class__, MyAdam)\n+    self.assertIs(loaded_model.compiled_loss._losses[0].__class__, MyLoss)\n+    self.assertIs(loaded_model.compiled_metrics._metrics[0].__class__, MyMetric)\n+    self.assertIs(loaded_model.compiled_metrics._metrics[1], my_sq_diff)\n+    self.assertIs(loaded_model.layers[0].__class__, MyDense)\n+    self.assertIs(loaded_model.layers[0].activity_regularizer.__class__,\n+                  MyRegularizer)\n+    self.assertIs(loaded_model.layers[0].kernel_regularizer.__class__,\n+                  MyRegularizer)\n+\n+    # Check that the custom methods are available.\n+    self.assertEqual(loaded_model.one(), 1)\n+    self.assertEqual(loaded_model.layers[0].two(), 2)\n+    self.assertEqual(loaded_model.optimizer.three(), 3)\n+    self.assertEqual(loaded_model.compiled_loss._losses[0].four(), 4)\n+    self.assertEqual(loaded_model.compiled_metrics._metrics[0].five(), 5)\n+    self.assertEqual(loaded_model.layers[0].activity_regularizer.six(), 6)\n+    self.assertEqual(loaded_model.layers[0].kernel_regularizer.six(), 6)\n+    self.assertEqual(loaded_model.compiled_metrics._metrics[1]([1], [3]), 4)\n+\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b96f9fdc2d5d0d375809ad9c16608830b01fc59a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1043 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -3209,12 +3209,12 @@ def binary_accuracy(y_true, y_pred, threshold=0.5):\n       prediction values are 1 or 0.\n \n   Returns:\n-    Binary accuracy values. shape = `[batch_size, d0, .. dN-1]`\n+    Binary accuracy values. shape = `[batch_size, d0, .. dN]`\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n   threshold = tf.cast(threshold, y_pred.dtype)\n   y_pred = tf.cast(y_pred > threshold, y_pred.dtype)\n-  return backend.mean(tf.equal(y_true, y_pred), axis=-1)\n+  return tf.cast(tf.equal(y_true, y_pred), tf.int8)\n \n \n @keras_export('keras.metrics.categorical_accuracy')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8bb1b365ca6bb21b32a1ee1654eecb02570970ac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 1047 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -3209,12 +3209,12 @@ def binary_accuracy(y_true, y_pred, threshold=0.5):\n       prediction values are 1 or 0.\n \n   Returns:\n-    Binary accuracy values. shape = `[batch_size, d0, .. dN]`\n+    Binary accuracy values. shape = `[batch_size, d0, .. dN-1]`\n   \"\"\"\n   y_pred = tf.convert_to_tensor(y_pred)\n   threshold = tf.cast(threshold, y_pred.dtype)\n   y_pred = tf.cast(y_pred > threshold, y_pred.dtype)\n-  return tf.cast(tf.equal(y_true, y_pred), tf.int8)\n+  return backend.mean(tf.equal(y_true, y_pred), axis=-1)\n \n \n @keras_export('keras.metrics.categorical_accuracy')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b96518a22bfd92a29811e507dec0b34248a8a3f5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3613 | Lines Deleted: 3433 | Files Changed: 207 | Hunks: 2717 | Methods Changed: 773 | Complexity Δ (Sum/Max): 542/180 | Churn Δ: 7046 | Churn Cumulative: 104148 | Contributors (this commit): 60 | Commits (past 90d): 446 | Contributors (cumulative): 690 | DMM Complexity: None\n\nDIFF:\n@@ -21,7 +21,7 @@ import numpy as np\n \n from keras import activations\n from keras import backend\n-from keras import combinations\n+from keras.testing_infra import test_combinations\n import keras.layers.activation as activation_layers\n from keras.layers import core\n from keras.layers import serialization\n@@ -33,7 +33,7 @@ def _ref_softmax(values):\n   return e / np.sum(e)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class KerasActivationsTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_serialization(self):\n\n@@ -20,12 +20,12 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras.applications import imagenet_utils as utils\n from keras.mixed_precision.policy import set_global_policy\n \n \n-class TestImageNetUtils(keras_parameterized.TestCase):\n+class TestImageNetUtils(test_combinations.TestCase):\n \n   def test_preprocess_input(self):\n     # Test invalid mode check\n\n@@ -18,10 +18,10 @@ import tensorflow.compat.v2 as tf\n \n from keras import backend\n from keras import backend_config\n-from keras import combinations\n+from keras.testing_infra import test_combinations\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class BackendConfigTest(tf.test.TestCase):\n \n   def test_backend(self):\n\n@@ -24,10 +24,10 @@ import numpy as np\n import scipy.sparse\n from tensorflow.python.eager import context\n from tensorflow.python.eager.context import get_config\n-from tensorflow.python.framework import test_util\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n from keras import activations\n from keras import backend\n-from keras import combinations\n+from keras.testing_infra import test_combinations\n from keras.engine import input_layer\n from keras.layers import activation\n from keras.layers.normalization import batch_normalization_v1\n@@ -134,7 +134,7 @@ class BackendResetTest(tf.test.TestCase, parameterized.TestCase):\n       assert g_old is not g\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class BackendUtilsTest(tf.test.TestCase):\n \n   def test_backend(self):\n@@ -282,7 +282,7 @@ class BackendUtilsTest(tf.test.TestCase):\n     self.assertEqual(x.dtype.name, 'float32')\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class BackendVariableTest(tf.test.TestCase):\n \n   def test_zeros(self):\n@@ -345,7 +345,7 @@ class BackendVariableTest(tf.test.TestCase):\n     self.assertFalse(backend.is_sparse(y))\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class BackendLinearAlgebraTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_dot(self):\n@@ -560,7 +560,7 @@ class BackendLinearAlgebraTest(tf.test.TestCase, parameterized.TestCase):\n     _ = activation.ReLU(max_value=100., dtype='int64')(x)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class BackendShapeOpsTest(tf.test.TestCase):\n \n   def test_reshape(self):\n@@ -747,7 +747,7 @@ class BackendShapeOpsTest(tf.test.TestCase):\n         np_kwargs={'data_format': 'channels_first'})\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class BackendNNOpsTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_bias_add(self):\n@@ -1624,7 +1624,8 @@ class BackendNNOpsTest(tf.test.TestCase, parameterized.TestCase):\n \n class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_binary_crossentropy_with_sigmoid(self):\n     t = backend.constant([[0, 1, 0]])\n     logits = backend.constant([[8., 1., 1.]])\n@@ -1633,7 +1634,8 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n     result = self.evaluate(backend.binary_crossentropy(t, p))\n     self.assertArrayNear(result[0], [8., 0.313, 1.313], 1e-3)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_categorical_crossentropy_loss(self):\n     t = backend.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n \n@@ -1653,7 +1655,8 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n     result = backend.categorical_crossentropy(t, p, from_logits=True, axis=0),\n     self.assertArrayNear(self.evaluate(result)[0], [.002, 0, .17], 1e-3)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_categorical_crossentropy_loss_with_unknown_rank_tensor(self):\n     t = backend.placeholder()\n     p = backend.placeholder()\n@@ -1692,7 +1695,8 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n     result = f([t_val, p_val])\n     self.assertArrayNear(result, [.002, .003, .036], 1e-3)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_categorical_crossentropy_with_softmax(self):\n     t = backend.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n     logits = backend.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])\n@@ -1701,7 +1705,8 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n     result = self.evaluate(backend.categorical_crossentropy(t, p))\n     self.assertArrayNear(result, [0.002, 0.0005, 0.17], 1e-3)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_sparse_categorical_crossentropy_loss(self):\n     t = backend.constant([0, 1, 2])\n \n@@ -1722,7 +1727,7 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n         t, p, from_logits=True, axis=0),\n     self.assertArrayNear(self.evaluate(result)[0], [.002, 0, .17], 1e-3)\n \n-  @combinations.generate(combinations.combine(mode=['graph']))\n+  @test_combinations.generate(test_combinations.combine(mode=['graph']))\n   def test_sparse_categorical_crossentropy_loss_with_unknown_rank_tensor(self):\n     # This test only runs in graph because the TF op layer is not supported yet\n     # for sparse ops.\n@@ -1767,7 +1772,8 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n \n       _ = f([t_val, p_val])\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_sparse_categorical_crossentropy_with_softmax(self):\n     t = backend.constant([0, 1, 2])\n     logits = backend.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])\n@@ -1776,7 +1782,8 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n     result = self.evaluate(backend.sparse_categorical_crossentropy(t, p))\n     self.assertArrayNear(result, [0.002, 0.0005, 0.17], 1e-3)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_binary_crossentropy_from_logits_no_warnings(self):\n     t = backend.constant([[0, 1, 0]])\n     logits = backend.constant([[8., 1., 1.]])\n@@ -1784,7 +1791,8 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n       self.evaluate(backend.binary_crossentropy(t, logits, from_logits=True))\n       self.assertEmpty(w)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_binary_crossentropy_from_logits_with_sigmoid(self):\n     t = backend.constant([[0, 1, 0]])\n     logits = backend.constant([[8., 1., 1.]])\n@@ -1794,7 +1802,8 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertLen(w, 1)\n       self.assertIn('received `from_logits=True`', str(w[0].message))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_categorical_crossentropy_from_logits_with_softmax(self):\n     t = backend.constant([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n     logits = backend.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])\n@@ -1804,7 +1813,8 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertLen(w, 1)\n       self.assertIn('received `from_logits=True`', str(w[0].message))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_sparse_categorical_crossentropy_from_logits_with_softmax(self):\n     t = backend.constant([0, 1, 2])\n     logits = backend.constant([[8., 1., 1.], [0., 9., 1.], [2., 3., 5.]])\n@@ -1815,7 +1825,8 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertLen(w, 1)\n       self.assertIn('received `from_logits=True`', str(w[0].message))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_binary_focal_crossentropy_with_sigmoid(self):\n     t = backend.constant([[0, 1, 0]])\n     logits = backend.constant([[8., 1., 1.]])\n@@ -1824,7 +1835,8 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n     result = self.evaluate(backend.binary_focal_crossentropy(t, p, gamma=2.0))\n     self.assertArrayNear(result[0], [7.995, 0.022, 0.701], 1e-3)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_binary_focal_crossentropy_from_logits(self):\n     t = backend.constant([[0, 1, 0]])\n     logits = backend.constant([[8., 1., 1.]])\n@@ -1837,7 +1849,8 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n         ))\n     self.assertArrayNear(result[0], [7.995, 0.022, 0.701], 1e-3)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_binary_focal_crossentropy_no_focal_effect_with_zero_gamma(self):\n     t = backend.constant([[0, 1, 0]])\n     logits = backend.constant([[8., 1., 1.]])\n@@ -1853,7 +1866,8 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n     non_focal_result = self.evaluate(backend.binary_crossentropy(t, p))\n     self.assertArrayNear(focal_result[0], non_focal_result[0], 1e-3)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_binary_weighted_focal_crossentropy_with_sigmoid(self):\n     t = backend.constant([[0, 1, 0]])\n     logits = backend.constant([[8., 1., 1.]])\n@@ -1862,7 +1876,8 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n     result = self.evaluate(backend.binary_weighted_focal_crossentropy(t, p))\n     self.assertArrayNear(result[0], [5.996, 0.006, 0.526], 1e-3)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_binary_weighted_focal_crossentropy_from_logits(self):\n     t = backend.constant([[0, 1, 0]])\n     logits = backend.constant([[8., 1., 1.]])\n@@ -1875,8 +1890,8 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertArrayNear(result[0], [5.996, 0.006, 0.526], 1e-3)\n \n \n-@test_util.with_control_flow_v2\n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@tf_test_utils.with_control_flow_v2\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class TestCTC(tf.test.TestCase):\n \n   def test_ctc_decode(self):\n@@ -1987,7 +2002,7 @@ class TestCTC(tf.test.TestCase):\n       self.assertAllClose(res[:, 0], ref, atol=1e-05)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class TestRandomOps(tf.test.TestCase):\n \n   def test_random_normal(self):\n@@ -2021,7 +2036,7 @@ class TestRandomOps(tf.test.TestCase):\n     self.assertAllClose(np.min(y), -2., atol=0.01)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class FunctionTest(tf.test.TestCase):\n \n   def test_function_basics(self):\n@@ -2089,7 +2104,7 @@ class FunctionTest(tf.test.TestCase):\n \n class BackendGraphTests(tf.test.TestCase, parameterized.TestCase):\n \n-  @combinations.generate(combinations.combine(mode=['graph']))\n+  @test_combinations.generate(test_combinations.combine(mode=['graph']))\n   def test_function_placeholder_with_default(self):\n     with backend.get_graph().as_default():\n       x1 = tf.compat.v1.placeholder_with_default(\n@@ -2254,7 +2269,7 @@ class BackendGraphTests(tf.test.TestCase, parameterized.TestCase):\n       self.assertIsNot(session, backend.get_session())\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class ControlOpsTests(tf.test.TestCase):\n \n   def test_function_switch_basics(self):\n@@ -2337,7 +2352,7 @@ class ContextValueCacheTest(tf.test.TestCase):\n     self.assertEqual(self.evaluate(fn()), 5)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class RandomGeneratorTest(tf.test.TestCase):\n \n   def test_generator_reproducibility(self):\n\n@@ -29,8 +29,6 @@ from unittest import mock\n \n from absl.testing import parameterized\n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n from keras.callbacks import BackupAndRestore\n from keras.callbacks import BackupAndRestoreExperimental\n from keras.engine import sequential\n@@ -38,6 +36,8 @@ from keras.layers import Activation\n from keras.layers import Dense\n from keras.optimizers import learning_rate_schedule\n from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.utils import io_utils\n from keras.utils import np_utils\n import numpy as np\n@@ -122,9 +122,9 @@ def _get_sequence():\n   return MySequence(), None\n \n \n-@keras_parameterized.run_with_all_model_types\n-@keras_parameterized.run_all_keras_modes\n-class CallbackCountsTest(keras_parameterized.TestCase):\n+@test_combinations.run_with_all_model_types\n+@test_combinations.run_all_keras_modes\n+class CallbackCountsTest(test_combinations.TestCase):\n \n   def _check_counts(self, counter, expected_counts):\n     \"\"\"Checks that the counts registered by `counter` are those expected.\"\"\"\n@@ -140,11 +140,11 @@ class CallbackCountsTest(keras_parameterized.TestCase):\n         keras.layers.Dense(10, activation='relu'),\n         keras.layers.Dense(1, activation='sigmoid')\n     ]\n-    model = testing_utils.get_model_from_layers(layers, input_shape=(10,))\n+    model = test_utils.get_model_from_layers(layers, input_shape=(10,))\n     model.compile(\n         tf.compat.v1.train.AdamOptimizer(0.001),\n         'binary_crossentropy',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     return model\n \n   @parameterized.named_parameters(('with_numpy', _get_numpy()),\n@@ -248,7 +248,7 @@ class CallbackCountsTest(keras_parameterized.TestCase):\n         })\n \n \n-class KerasCallbacksTest(keras_parameterized.TestCase):\n+class KerasCallbacksTest(test_combinations.TestCase):\n \n   def _get_model(self, input_shape=None, additional_metrics=None):\n     additional_metrics = additional_metrics or []\n@@ -256,17 +256,17 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n         keras.layers.Dense(3, activation='relu'),\n         keras.layers.Dense(2, activation='softmax')\n     ]\n-    model = testing_utils.get_model_from_layers(layers, input_shape=input_shape)\n+    model = test_utils.get_model_from_layers(layers, input_shape=input_shape)\n     model.compile(\n         loss='mse',\n         optimizer='rmsprop',\n         metrics=[keras.metrics.CategoricalAccuracy(name='my_acc')] +\n         additional_metrics,\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     return model\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_progbar_logging(self):\n     model = self._get_model(input_shape=(3,))\n \n@@ -280,8 +280,8 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n       model.fit(dataset, epochs=2, steps_per_epoch=10)\n       self.assertRegex(printed.contents(), expected_log)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_progbar_logging_with_stateful_metrics(self):\n \n     class AddAllOnes(keras.metrics.Metric):\n@@ -320,9 +320,9 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n         model.evaluate(x_train, y_train, verbose=1, batch_size=4)\n         self.assertRegex(printed.contents(), expected_log)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_trivial_backup_restore(self):\n-    if testing_utils.should_run_eagerly():\n+    if test_utils.should_run_eagerly():\n       model = keras.Sequential([keras.layers.Dense(1)])\n       model.compile('sgd', 'mse')\n       cbk = BackupAndRestore(self.get_temp_dir())\n@@ -426,7 +426,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     warning_msg = ('***Handling interruption***')\n     self.assertIn(warning_msg, '\\n'.join(warning_messages))\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_callback_warning(self):\n \n     class SleepCallback(keras.callbacks.Callback):\n@@ -439,7 +439,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     warning_messages = []\n \n@@ -457,7 +457,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n                    'to the batch time')\n     self.assertIn(warning_msg, '\\n'.join(warning_messages))\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_default_callbacks_no_warning(self):\n     # Test that without the callback no warning is raised\n     model = sequential.Sequential()\n@@ -465,7 +465,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     warning_messages = []\n \n@@ -480,8 +480,8 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n           epochs=1)\n     self.assertListEqual(warning_messages, [])\n \n-  @keras_parameterized.run_with_all_model_types(exclude_models='functional')\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types(exclude_models='functional')\n+  @test_combinations.run_all_keras_modes\n   def test_progbar_logging_deferred_model_build(self):\n     model = self._get_model()\n     self.assertFalse(model.built)\n@@ -496,8 +496,8 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n       model.fit(dataset, epochs=2, steps_per_epoch=10)\n       self.assertRegex(printed.contents(), expected_log)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_progbar_logging_validation_data(self):\n     model = self._get_model(input_shape=(3,))\n \n@@ -512,8 +512,8 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n       model.fit(training_dataset, epochs=2, validation_data=val_dataset)\n       self.assertRegex(printed.contents(), expected_log)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_progbar_logging_validation_split(self):\n     model = self._get_model(input_shape=(3,))\n \n@@ -528,8 +528,8 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n       model.fit(x, y, batch_size=10, epochs=2, validation_split=0.2)\n       self.assertRegex(printed.contents(), expected_log)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_progbar_logging_training_validation(self):\n     model = self._get_model(input_shape=(2,))\n \n@@ -560,8 +560,8 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n           x=training, validation_data=validation, epochs=2, steps_per_epoch=20)\n       self.assertRegex(printed.contents(), expected_log)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_progbar_logging_with_dataset_and_partial_batch(self):\n     model = self._get_model(input_shape=(2,))\n \n@@ -593,12 +593,12 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n       self.assertLen(val_loss, 1)\n       self.assertGreater(float(val_loss[0]), 0.0)\n \n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_with_all_model_types\n   def test_ModelCheckpoint(self):\n     if h5py is None:\n       return  # Skip test if models cannot be saved.\n \n-    model_type = testing_utils.get_model_type()\n+    model_type = test_utils.get_model_type()\n     if model_type == 'subclass':\n       return  # Skip test since subclassed models cannot be saved in .h5 format.\n     if not tf.__internal__.tf2.enabled():\n@@ -608,7 +608,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n         keras.layers.Dense(NUM_HIDDEN, input_dim=INPUT_DIM, activation='relu'),\n         keras.layers.Dense(NUM_CLASSES, activation='softmax')\n     ]\n-    model = testing_utils.get_model_from_layers(layers, input_shape=(3,))\n+    model = test_utils.get_model_from_layers(layers, input_shape=(3,))\n     model.compile(\n         loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n \n@@ -616,7 +616,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     self.addCleanup(shutil.rmtree, temp_dir, ignore_errors=True)\n \n     filepath = os.path.join(temp_dir, 'checkpoint.h5')\n-    (x_train, y_train), (x_test, y_test) = testing_utils.get_test_data(\n+    (x_train, y_train), (x_test, y_test) = test_utils.get_test_data(\n         train_samples=TRAIN_SAMPLES,\n         test_samples=TEST_SAMPLES,\n         input_shape=(INPUT_DIM,),\n@@ -1020,9 +1020,9 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n         verbose=0)\n     assert not os.path.exists(filepath)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_ModelCheckpoint_subclass_save_weights_false(self):\n-    model = testing_utils.get_small_subclass_mlp(NUM_HIDDEN, NUM_CLASSES)\n+    model = test_utils.get_small_subclass_mlp(NUM_HIDDEN, NUM_CLASSES)\n     model.compile(\n         loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n     temp_dir = self.get_temp_dir()\n@@ -1031,7 +1031,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     cbks = [keras.callbacks.ModelCheckpoint(\n         filepath, save_weights_only=False)]\n \n-    (x_train, y_train), _ = testing_utils.get_test_data(\n+    (x_train, y_train), _ = test_utils.get_test_data(\n         train_samples=TRAIN_SAMPLES,\n         test_samples=TEST_SAMPLES,\n         input_shape=(INPUT_DIM,),\n@@ -1059,7 +1059,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     # Very simple bias model to eliminate randomness.\n     optimizer = gradient_descent.SGD(0.1)\n     model = sequential.Sequential()\n-    model.add(testing_utils.Bias(input_shape=(1,)))\n+    model.add(test_utils.Bias(input_shape=(1,)))\n     model.compile(loss='mae', optimizer=optimizer, metrics=['mae'])\n     train_ds = get_input_datasets()\n \n@@ -1339,14 +1339,14 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n   def test_EarlyStopping(self):\n     with self.cached_session():\n       np.random.seed(123)\n-      (x_train, y_train), (x_test, y_test) = testing_utils.get_test_data(\n+      (x_train, y_train), (x_test, y_test) = test_utils.get_test_data(\n           train_samples=TRAIN_SAMPLES,\n           test_samples=TEST_SAMPLES,\n           input_shape=(INPUT_DIM,),\n           num_classes=NUM_CLASSES)\n       y_test = np_utils.to_categorical(y_test)\n       y_train = np_utils.to_categorical(y_train)\n-      model = testing_utils.get_small_sequential_mlp(\n+      model = test_utils.get_small_sequential_mlp(\n           num_hidden=NUM_HIDDEN, num_classes=NUM_CLASSES, input_dim=INPUT_DIM)\n       model.compile(\n           loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n@@ -1397,12 +1397,12 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     with self.cached_session():\n       np.random.seed(1337)\n       baseline = 0.6\n-      (data, labels), _ = testing_utils.get_test_data(\n+      (data, labels), _ = test_utils.get_test_data(\n           train_samples=100,\n           test_samples=50,\n           input_shape=(1,),\n           num_classes=NUM_CLASSES)\n-      model = testing_utils.get_small_sequential_mlp(\n+      model = test_utils.get_small_sequential_mlp(\n           num_hidden=1, num_classes=1, input_dim=1)\n       model.compile(\n           optimizer='sgd', loss='binary_crossentropy', metrics=['acc'])\n@@ -1486,14 +1486,14 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n   def test_LearningRateScheduler(self):\n     with self.cached_session():\n       np.random.seed(1337)\n-      (x_train, y_train), (x_test, y_test) = testing_utils.get_test_data(\n+      (x_train, y_train), (x_test, y_test) = test_utils.get_test_data(\n           train_samples=TRAIN_SAMPLES,\n           test_samples=TEST_SAMPLES,\n           input_shape=(INPUT_DIM,),\n           num_classes=NUM_CLASSES)\n       y_test = np_utils.to_categorical(y_test)\n       y_train = np_utils.to_categorical(y_train)\n-      model = testing_utils.get_small_sequential_mlp(\n+      model = test_utils.get_small_sequential_mlp(\n           num_hidden=NUM_HIDDEN, num_classes=NUM_CLASSES, input_dim=INPUT_DIM)\n       model.compile(\n           loss='categorical_crossentropy',\n@@ -1563,7 +1563,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n   def test_ReduceLROnPlateau(self):\n     with self.cached_session():\n       np.random.seed(1337)\n-      (x_train, y_train), (x_test, y_test) = testing_utils.get_test_data(\n+      (x_train, y_train), (x_test, y_test) = test_utils.get_test_data(\n           train_samples=TRAIN_SAMPLES,\n           test_samples=TEST_SAMPLES,\n           input_shape=(INPUT_DIM,),\n@@ -1574,7 +1574,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n       def make_model():\n         tf.compat.v1.set_random_seed(1234)\n         np.random.seed(1337)\n-        model = testing_utils.get_small_sequential_mlp(\n+        model = test_utils.get_small_sequential_mlp(\n             num_hidden=NUM_HIDDEN, num_classes=NUM_CLASSES, input_dim=INPUT_DIM)\n         model.compile(\n             loss='categorical_crossentropy',\n@@ -1670,7 +1670,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n       filepath = os.path.join(temp_dir, 'log.tsv')\n \n       sep = '\\t'\n-      (x_train, y_train), (x_test, y_test) = testing_utils.get_test_data(\n+      (x_train, y_train), (x_test, y_test) = test_utils.get_test_data(\n           train_samples=TRAIN_SAMPLES,\n           test_samples=TEST_SAMPLES,\n           input_shape=(INPUT_DIM,),\n@@ -1680,7 +1680,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n \n       def make_model():\n         np.random.seed(1337)\n-        model = testing_utils.get_small_sequential_mlp(\n+        model = test_utils.get_small_sequential_mlp(\n             num_hidden=NUM_HIDDEN, num_classes=NUM_CLASSES, input_dim=INPUT_DIM)\n         model.compile(\n             loss='categorical_crossentropy',\n@@ -1748,7 +1748,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n \n     with self.cached_session():\n       fp = os.path.join(tmpdir, 'test.csv')\n-      (x_train, y_train), (x_test, y_test) = testing_utils.get_test_data(\n+      (x_train, y_train), (x_test, y_test) = test_utils.get_test_data(\n           train_samples=TRAIN_SAMPLES,\n           test_samples=TEST_SAMPLES,\n           input_shape=(INPUT_DIM,),\n@@ -1796,10 +1796,10 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n \n       assert 'nan' in values[-1], 'The last epoch was not logged.'\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_TerminateOnNaN(self):\n     np.random.seed(1337)\n-    (x_train, y_train), (x_test, y_test) = testing_utils.get_test_data(\n+    (x_train, y_train), (x_test, y_test) = test_utils.get_test_data(\n         train_samples=TRAIN_SAMPLES,\n         test_samples=TEST_SAMPLES,\n         input_shape=(INPUT_DIM,),\n@@ -1837,7 +1837,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n   def test_LambdaCallback(self):\n     with self.cached_session():\n       np.random.seed(1337)\n-      (x_train, y_train), (x_test, y_test) = testing_utils.get_test_data(\n+      (x_train, y_train), (x_test, y_test) = test_utils.get_test_data(\n           train_samples=TRAIN_SAMPLES,\n           test_samples=TEST_SAMPLES,\n           input_shape=(INPUT_DIM,),\n@@ -1908,7 +1908,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n       self.skipTest('`requests` required to run this test')\n       return None\n     with self.cached_session():\n-      (x_train, y_train), (x_test, y_test) = testing_utils.get_test_data(\n+      (x_train, y_train), (x_test, y_test) = test_utils.get_test_data(\n           train_samples=TRAIN_SAMPLES,\n           test_samples=TEST_SAMPLES,\n           input_shape=(INPUT_DIM,),\n@@ -1947,7 +1947,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     model.fit(data, epochs=2, callbacks=[progbar])\n     self.assertEqual(progbar.target, 5)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_callback_passed_floats(self):\n \n     class MyCallback(keras.callbacks.Callback):\n@@ -1964,14 +1964,14 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n \n     x, y = np.ones((10, 1)), np.ones((10, 1))\n     model = keras.Sequential([keras.layers.Dense(1)])\n-    model.compile('sgd', 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+    model.compile('sgd', 'mse', run_eagerly=test_utils.should_run_eagerly())\n \n     callback = MyCallback()\n     model.fit(x, y, epochs=2, callbacks=[callback])\n     self.assertTrue(callback.on_batch_end_called)\n     self.assertTrue(callback.on_batch_end_called)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_implements_batch_hooks(self):\n \n     class MyCallbackWithBatchHooks(keras.callbacks.Callback):\n@@ -2038,7 +2038,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     model.evaluate(x, y, batch_size=10, callbacks=[my_cb], verbose=0)\n     model.predict(x, batch_size=10, callbacks=[my_cb], verbose=0)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_logs_conversion(self):\n     assert_dict_equal = self.assertDictEqual\n \n@@ -2093,7 +2093,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     cb_list.on_train_begin(logs={'all': 0})\n     cb_list.on_train_end(logs={'all': 0})\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_implements_batch_hooks_override(self):\n \n     class MyCallback(keras.callbacks.Callback):\n@@ -2154,7 +2154,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     self.assertEqual(my_cb.test_batches, 0)\n     self.assertEqual(my_cb.predict_batches, 0)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_default_callbacks_do_not_call_batch_hooks(self):\n     model = keras.Sequential([keras.layers.Dense(1)])\n     log_dir = self.get_temp_dir()\n@@ -2171,7 +2171,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     self.assertFalse(cb_list._should_call_test_batch_hooks)\n     self.assertFalse(cb_list._should_call_predict_batch_hooks)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_change_tf_functions_during_fit(self):\n \n     class ChangeFunctions(keras.callbacks.Callback):\n@@ -2196,7 +2196,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     with self.assertRaisesRegexp(ValueError, 'New function '):\n       model.predict(x, batch_size=2)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_stop_training_batch_level(self):\n \n     class MyCallback(keras.callbacks.Callback):\n@@ -2218,7 +2218,7 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n     model.fit(x, y, batch_size=2, callbacks=[my_cb])\n     self.assertEqual(my_cb.batch_counter, 3)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_built_in_callback_order(self):\n \n     class CustomCallback(keras.callbacks.Callback):\n@@ -2324,9 +2324,9 @@ def list_summaries(logdir):\n   return result\n \n \n-@keras_parameterized.run_with_all_model_types\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class TestTensorBoardV2(keras_parameterized.TestCase):\n+@test_combinations.run_with_all_model_types\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class TestTensorBoardV2(test_combinations.TestCase):\n \n   def setUp(self):\n     super(TestTensorBoardV2, self).setUp()\n@@ -2340,10 +2340,10 @@ class TestTensorBoardV2(keras_parameterized.TestCase):\n         keras.layers.Flatten(),\n         keras.layers.Dense(1)\n     ]\n-    model = testing_utils.get_model_from_layers(layers, input_shape=(10, 10, 1))\n+    model = test_utils.get_model_from_layers(layers, input_shape=(10, 10, 1))\n     if compile_model:\n       opt = gradient_descent.SGD(learning_rate=0.001)\n-      model.compile(opt, 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+      model.compile(opt, 'mse', run_eagerly=test_utils.should_run_eagerly())\n     return model\n \n   def test_TensorBoard_default_logdir(self):\n@@ -2470,7 +2470,7 @@ class TestTensorBoardV2(keras_parameterized.TestCase):\n   def test_TensorBoard_learning_rate_schedules(self):\n     model = self._get_model(compile_model=False)\n     opt = gradient_descent.SGD(learning_rate_schedule.CosineDecay(0.01, 1))\n-    model.compile(opt, 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+    model.compile(opt, 'mse', run_eagerly=test_utils.should_run_eagerly())\n \n     x, y = np.ones((10, 10, 10, 1)), np.ones((10, 1))\n \n@@ -2493,7 +2493,7 @@ class TestTensorBoardV2(keras_parameterized.TestCase):\n   def test_TensorBoard_global_step(self):\n     model = self._get_model(compile_model=False)\n     opt = gradient_descent.SGD(learning_rate_schedule.CosineDecay(0.01, 1))\n-    model.compile(opt, 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+    model.compile(opt, 'mse', run_eagerly=test_utils.should_run_eagerly())\n \n     x, y = np.ones((10, 10, 10, 1)), np.ones((10, 1))\n \n@@ -2528,7 +2528,7 @@ class TestTensorBoardV2(keras_parameterized.TestCase):\n     model = self._get_model()\n     x, y = np.ones((10, 10, 10, 1)), np.ones((10, 1))\n     tb_cbk = keras.callbacks.TensorBoard(self.logdir, histogram_freq=1)\n-    model_type = testing_utils.get_model_type()\n+    model_type = test_utils.get_model_type()\n \n     model.fit(\n         x,\n@@ -2562,7 +2562,7 @@ class TestTensorBoardV2(keras_parameterized.TestCase):\n     x, y = np.ones((10, 10, 10, 1)), np.ones((10, 1))\n     tb_cbk = keras.callbacks.TensorBoard(\n         self.logdir, histogram_freq=1, write_images=True)\n-    model_type = testing_utils.get_model_type()\n+    model_type = test_utils.get_model_type()\n \n     model.fit(\n         x,\n@@ -2613,11 +2613,11 @@ class TestTensorBoardV2(keras_parameterized.TestCase):\n         keras.layers.Dense(10, activation='relu'),\n         keras.layers.Dense(1, activation='sigmoid')\n     ]\n-    model = testing_utils.get_model_from_layers(layers, input_shape=(10,))\n+    model = test_utils.get_model_from_layers(layers, input_shape=(10,))\n     model.compile(\n         optimizer='adam',\n         loss=keras.losses.BinaryCrossentropy(from_logits=True),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     x, y = np.ones((10, 10)), np.ones((10, 10))\n     tb_cbk = keras.callbacks.TensorBoard(\n         self.logdir,\n@@ -2663,14 +2663,14 @@ class TestTensorBoardV2(keras_parameterized.TestCase):\n         scalar_v2_mock('custom_summary', tf.reduce_sum(x))\n         return x\n \n-    model = testing_utils.get_model_from_layers([LayerWithSummary()],\n+    model = test_utils.get_model_from_layers([LayerWithSummary()],\n                                              input_shape=(5,),\n                                              name='model')\n \n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     tb_cbk = keras.callbacks.TensorBoard(self.logdir, update_freq=1)\n     x, y = np.ones((10, 5)), np.ones((10, 5))\n     model.fit(x, y, batch_size=2, validation_data=(x, y), callbacks=[tb_cbk])\n@@ -2758,8 +2758,8 @@ class TestTensorBoardV2(keras_parameterized.TestCase):\n \n \n # Note that this test specifies model_type explicitly.\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class TestTensorBoardV2NonParameterizedTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class TestTensorBoardV2NonParameterizedTest(test_combinations.TestCase):\n \n   def setUp(self):\n     super(TestTensorBoardV2NonParameterizedTest, self).setUp()\n@@ -2777,7 +2777,7 @@ class TestTensorBoardV2NonParameterizedTest(keras_parameterized.TestCase):\n     model.compile(\n         opt,\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     return model\n \n   def _count_trace_file(self, logdir):\n@@ -2827,7 +2827,7 @@ class TestTensorBoardV2NonParameterizedTest(keras_parameterized.TestCase):\n         keras.layers.Flatten(),\n         keras.layers.Dense(1),\n     ])\n-    model.compile('sgd', 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+    model.compile('sgd', 'mse', run_eagerly=test_utils.should_run_eagerly())\n     self.fitModelAndAssertKerasModelWritten(model)\n \n   def test_TensorBoard_writeSequentialModel_withInputShape(self):\n@@ -2836,7 +2836,7 @@ class TestTensorBoardV2NonParameterizedTest(keras_parameterized.TestCase):\n         keras.layers.Flatten(),\n         keras.layers.Dense(1),\n     ])\n-    model.compile('sgd', 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+    model.compile('sgd', 'mse', run_eagerly=test_utils.should_run_eagerly())\n     self.fitModelAndAssertKerasModelWritten(model)\n \n   def test_TensorBoard_writeModel(self):\n@@ -2845,7 +2845,7 @@ class TestTensorBoardV2NonParameterizedTest(keras_parameterized.TestCase):\n     x = keras.layers.Flatten()(x)\n     x = keras.layers.Dense(1)(x)\n     model = keras.models.Model(inputs=inputs, outputs=[x])\n-    model.compile('sgd', 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+    model.compile('sgd', 'mse', run_eagerly=test_utils.should_run_eagerly())\n     self.fitModelAndAssertKerasModelWritten(model)\n \n   def test_TensorBoard_autoTrace(self):\n@@ -3179,7 +3179,7 @@ class SummaryOpsTest(tf.test.TestCase):\n     # the second event.\n     return events[1]\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def testKerasModel(self):\n     model = keras.Sequential(\n         [Dense(10, input_shape=(100,)),\n@@ -3188,7 +3188,7 @@ class SummaryOpsTest(tf.test.TestCase):\n     first_val = event.summary.value[0]\n     self.assertEqual(model.to_json(), first_val.tensor.string_val[0].decode())\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def testKerasModel_usesDefaultStep(self):\n     model = keras.Sequential(\n         [Dense(10, input_shape=(100,)),\n@@ -3201,7 +3201,7 @@ class SummaryOpsTest(tf.test.TestCase):\n       # Reset to default state for other tests.\n       tf.summary.experimental.set_step(None)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def testKerasModel_subclass(self):\n \n     class SimpleSubclass(keras.Model):\n@@ -3227,7 +3227,7 @@ class SummaryOpsTest(tf.test.TestCase):\n       self.assertRegex(\n           str(mock_log.call_args), 'Model failed to serialize as JSON.')\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def testKerasModel_otherExceptions(self):\n     model = keras.Sequential()\n \n\n@@ -24,9 +24,9 @@ from absl.testing import parameterized\n import numpy as np\n from keras import callbacks\n from keras import callbacks_v1\n-from keras import combinations\n+from keras.testing_infra import test_combinations\n from keras import layers\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.engine import input_layer\n from keras.engine import sequential\n from keras.engine import training\n@@ -49,7 +49,7 @@ class TestTensorBoardV1(tf.test.TestCase, parameterized.TestCase):\n     temp_dir = self.get_temp_dir()\n     self.addCleanup(shutil.rmtree, temp_dir, ignore_errors=True)\n \n-    (x_train, y_train), (x_test, y_test) = testing_utils.get_test_data(\n+    (x_train, y_train), (x_test, y_test) = test_utils.get_test_data(\n         train_samples=TRAIN_SAMPLES,\n         test_samples=TEST_SAMPLES,\n         input_shape=(INPUT_DIM,),\n@@ -157,7 +157,7 @@ class TestTensorBoardV1(tf.test.TestCase, parameterized.TestCase):\n     with tf.Graph().as_default(), self.cached_session():\n       filepath = os.path.join(tmpdir, 'logs')\n \n-      (x_train, y_train), (x_test, y_test) = testing_utils.get_test_data(\n+      (x_train, y_train), (x_test, y_test) = test_utils.get_test_data(\n           train_samples=TRAIN_SAMPLES,\n           test_samples=TEST_SAMPLES,\n           input_shape=(INPUT_DIM,),\n@@ -260,7 +260,7 @@ class TestTensorBoardV1(tf.test.TestCase, parameterized.TestCase):\n     np.random.seed(1337)\n     tmpdir = self.get_temp_dir()\n     self.addCleanup(shutil.rmtree, tmpdir, ignore_errors=True)\n-    (x_train, y_train), (x_test, y_test) = testing_utils.get_test_data(\n+    (x_train, y_train), (x_test, y_test) = test_utils.get_test_data(\n         train_samples=TRAIN_SAMPLES,\n         test_samples=TEST_SAMPLES,\n         input_shape=(INPUT_DIM,),\n@@ -313,7 +313,7 @@ class TestTensorBoardV1(tf.test.TestCase, parameterized.TestCase):\n         yield x, y\n \n     with tf.Graph().as_default(), self.cached_session():\n-      model = testing_utils.get_small_sequential_mlp(\n+      model = test_utils.get_small_sequential_mlp(\n           num_hidden=10, num_classes=10, input_dim=100)\n       model.compile(\n           loss='categorical_crossentropy',\n@@ -355,7 +355,7 @@ class TestTensorBoardV1(tf.test.TestCase, parameterized.TestCase):\n       temp_dir = self.get_temp_dir()\n       self.addCleanup(shutil.rmtree, temp_dir, ignore_errors=True)\n \n-      (x_train, y_train), (x_test, y_test) = testing_utils.get_test_data(\n+      (x_train, y_train), (x_test, y_test) = test_utils.get_test_data(\n           train_samples=TRAIN_SAMPLES,\n           test_samples=TEST_SAMPLES,\n           input_shape=(INPUT_DIM,),\n@@ -363,7 +363,7 @@ class TestTensorBoardV1(tf.test.TestCase, parameterized.TestCase):\n       y_test = np_utils.to_categorical(y_test)\n       y_train = np_utils.to_categorical(y_train)\n \n-      model = testing_utils.get_small_sequential_mlp(\n+      model = test_utils.get_small_sequential_mlp(\n           num_hidden=NUM_HIDDEN, num_classes=NUM_CLASSES, input_dim=INPUT_DIM)\n       model.compile(\n           loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n@@ -461,12 +461,13 @@ class TestTensorBoardV1(tf.test.TestCase, parameterized.TestCase):\n       self.assertEqual(epoch_step, 0)\n       self.assertEqual(epoch_summary.value[0].simple_value, 10.0)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_Tensorboard_eager(self):\n     temp_dir = tempfile.mkdtemp(dir=self.get_temp_dir())\n     self.addCleanup(shutil.rmtree, temp_dir, ignore_errors=True)\n \n-    (x_train, y_train), (x_test, y_test) = testing_utils.get_test_data(\n+    (x_train, y_train), (x_test, y_test) = test_utils.get_test_data(\n         train_samples=TRAIN_SAMPLES,\n         test_samples=TEST_SAMPLES,\n         input_shape=(INPUT_DIM,),\n@@ -474,7 +475,7 @@ class TestTensorBoardV1(tf.test.TestCase, parameterized.TestCase):\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n \n-    model = testing_utils.get_small_sequential_mlp(\n+    model = test_utils.get_small_sequential_mlp(\n         num_hidden=NUM_HIDDEN, num_classes=NUM_CLASSES, input_dim=INPUT_DIM)\n     model.compile(\n         loss='binary_crossentropy',\n\n@@ -1,104 +0,0 @@\n-# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"This module customizes `test_combinations` for `tf.keras` related tests.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-import functools\n-from keras import testing_utils\n-\n-KERAS_MODEL_TYPES = ['functional', 'subclass', 'sequential']\n-\n-\n-def keras_mode_combinations(mode=None, run_eagerly=None):\n-  \"\"\"Returns the default test combinations for tf.keras tests.\n-\n-  Note that if tf2 is enabled, then v1 session test will be skipped.\n-\n-  Args:\n-    mode: List of modes to run the tests. The valid options are 'graph' and\n-      'eager'. Default to ['graph', 'eager'] if not specified. If a empty list\n-      is provide, then the test will run under the context based on tf's\n-      version, eg graph for v1 and eager for v2.\n-    run_eagerly: List of `run_eagerly` value to be run with the tests.\n-      Default to [True, False] if not specified. Note that for `graph` mode,\n-      run_eagerly value will only be False.\n-\n-  Returns:\n-    A list contains all the combinations to be used to generate test cases.\n-  \"\"\"\n-  if mode is None:\n-    mode = ['eager'] if tf.__internal__.tf2.enabled() else ['graph', 'eager']\n-  if run_eagerly is None:\n-    run_eagerly = [True, False]\n-  result = []\n-  if 'eager' in mode:\n-    result += tf.__internal__.test.combinations.combine(mode=['eager'], run_eagerly=run_eagerly)\n-  if 'graph' in mode:\n-    result += tf.__internal__.test.combinations.combine(mode=['graph'], run_eagerly=[False])\n-  return result\n-\n-\n-def keras_model_type_combinations():\n-  return tf.__internal__.test.combinations.combine(model_type=KERAS_MODEL_TYPES)\n-\n-\n-class KerasModeCombination(tf.__internal__.test.combinations.TestCombination):\n-  \"\"\"Combination for Keras test mode.\n-\n-  It by default includes v1_session, v2_eager and v2_tf_function.\n-  \"\"\"\n-\n-  def context_managers(self, kwargs):\n-    run_eagerly = kwargs.pop('run_eagerly', None)\n-\n-    if run_eagerly is not None:\n-      return [testing_utils.run_eagerly_scope(run_eagerly)]\n-    else:\n-      return []\n-\n-  def parameter_modifiers(self):\n-    return [tf.__internal__.test.combinations.OptionalParameter('run_eagerly')]\n-\n-\n-class KerasModelTypeCombination(tf.__internal__.test.combinations.TestCombination):\n-  \"\"\"Combination for Keras model types when doing model test.\n-\n-  It by default includes 'functional', 'subclass', 'sequential'.\n-\n-  Various methods in `testing_utils` to get models will auto-generate a model\n-  of the currently active Keras model type. This allows unittests to confirm\n-  the equivalence between different Keras models.\n-  \"\"\"\n-\n-  def context_managers(self, kwargs):\n-    model_type = kwargs.pop('model_type', None)\n-    if model_type in KERAS_MODEL_TYPES:\n-      return [testing_utils.model_type_scope(model_type)]\n-    else:\n-      return []\n-\n-  def parameter_modifiers(self):\n-    return [tf.__internal__.test.combinations.OptionalParameter('model_type')]\n-\n-\n-_defaults = tf.__internal__.test.combinations.generate.keywords['test_combinations']\n-generate = functools.partial(\n-    tf.__internal__.test.combinations.generate,\n-    test_combinations=_defaults +\n-    (KerasModeCombination(), KerasModelTypeCombination()))\n-combine = tf.__internal__.test.combinations.combine\n-times = tf.__internal__.test.combinations.times\n-NamedObject = tf.__internal__.test.combinations.NamedObject\n\n@@ -1,171 +0,0 @@\n-# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Tests for Keras combinations.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-\n-import unittest\n-from absl.testing import parameterized\n-from keras import combinations\n-from keras import models as keras_models\n-from keras import testing_utils\n-\n-\n-class CombinationsTest(tf.test.TestCase):\n-\n-  def test_run_all_keras_modes(self):\n-    test_params = []\n-\n-    class ExampleTest(parameterized.TestCase):\n-\n-      def runTest(self):\n-        pass\n-\n-      @combinations.generate(combinations.keras_mode_combinations())\n-      def testBody(self):\n-        mode = \"eager\" if tf.executing_eagerly() else \"graph\"\n-        should_run_eagerly = testing_utils.should_run_eagerly()\n-        test_params.append((mode, should_run_eagerly))\n-\n-    e = ExampleTest()\n-    if not tf.__internal__.tf2.enabled():\n-      e.testBody_test_mode_graph_runeagerly_False()\n-    e.testBody_test_mode_eager_runeagerly_True()\n-    e.testBody_test_mode_eager_runeagerly_False()\n-\n-    if not tf.__internal__.tf2.enabled():\n-      self.assertLen(test_params, 3)\n-      self.assertAllEqual(test_params, [\n-          (\"graph\", False),\n-          (\"eager\", True),\n-          (\"eager\", False),\n-      ])\n-\n-      ts = unittest.makeSuite(ExampleTest)\n-      res = unittest.TestResult()\n-      ts.run(res)\n-      self.assertLen(test_params, 6)\n-    else:\n-      self.assertLen(test_params, 2)\n-      self.assertAllEqual(test_params, [\n-          (\"eager\", True),\n-          (\"eager\", False),\n-      ])\n-\n-      ts = unittest.makeSuite(ExampleTest)\n-      res = unittest.TestResult()\n-      ts.run(res)\n-      self.assertLen(test_params, 4)\n-\n-  def test_generate_keras_mode_eager_only(self):\n-    result = combinations.keras_mode_combinations(mode=[\"eager\"])\n-    self.assertLen(result, 2)\n-    self.assertEqual(result[0], {\"mode\": \"eager\", \"run_eagerly\": True})\n-    self.assertEqual(result[1], {\"mode\": \"eager\", \"run_eagerly\": False})\n-\n-  def test_generate_keras_mode_skip_run_eagerly(self):\n-    result = combinations.keras_mode_combinations(run_eagerly=[False])\n-    if tf.__internal__.tf2.enabled():\n-      self.assertLen(result, 1)\n-      self.assertEqual(result[0], {\"mode\": \"eager\", \"run_eagerly\": False})\n-    else:\n-      self.assertLen(result, 2)\n-      self.assertEqual(result[0], {\"mode\": \"eager\", \"run_eagerly\": False})\n-      self.assertEqual(result[1], {\"mode\": \"graph\", \"run_eagerly\": False})\n-\n-  def test_run_all_keras_model_types(self):\n-    model_types = []\n-    models = []\n-\n-    class ExampleTest(parameterized.TestCase):\n-\n-      def runTest(self):\n-        pass\n-\n-      @combinations.generate(combinations.keras_model_type_combinations())\n-      def testBody(self):\n-        model_types.append(testing_utils.get_model_type())\n-        models.append(testing_utils.get_small_mlp(1, 4, input_dim=3))\n-\n-    e = ExampleTest()\n-    e.testBody_test_modeltype_functional()\n-    e.testBody_test_modeltype_subclass()\n-    e.testBody_test_modeltype_sequential()\n-\n-    self.assertLen(model_types, 3)\n-    self.assertAllEqual(model_types, [\n-        \"functional\",\n-        \"subclass\",\n-        \"sequential\"\n-    ])\n-\n-    # Validate that the models are what they should be\n-    self.assertTrue(models[0]._is_graph_network)\n-    self.assertFalse(models[1]._is_graph_network)\n-    self.assertNotIsInstance(models[0], keras_models.Sequential)\n-    self.assertNotIsInstance(models[1], keras_models.Sequential)\n-    self.assertIsInstance(models[2], keras_models.Sequential)\n-\n-    ts = unittest.makeSuite(ExampleTest)\n-    res = unittest.TestResult()\n-    ts.run(res)\n-\n-    self.assertLen(model_types, 6)\n-\n-  def test_combine_combinations(self):\n-    test_cases = []\n-\n-    @combinations.generate(combinations.times(\n-        combinations.keras_mode_combinations(),\n-        combinations.keras_model_type_combinations()))\n-    class ExampleTest(parameterized.TestCase):\n-\n-      def runTest(self):\n-        pass\n-\n-      @parameterized.named_parameters(dict(testcase_name=\"_arg\",\n-                                           arg=True))\n-      def testBody(self, arg):\n-        del arg\n-        mode = \"eager\" if tf.executing_eagerly() else \"graph\"\n-        should_run_eagerly = testing_utils.should_run_eagerly()\n-        test_cases.append((mode, should_run_eagerly,\n-                           testing_utils.get_model_type()))\n-\n-    ts = unittest.makeSuite(ExampleTest)\n-    res = unittest.TestResult()\n-    ts.run(res)\n-\n-    expected_combinations = [\n-        (\"eager\", False, \"functional\"),\n-        (\"eager\", False, \"sequential\"),\n-        (\"eager\", False, \"subclass\"),\n-        (\"eager\", True, \"functional\"),\n-        (\"eager\", True, \"sequential\"),\n-        (\"eager\", True, \"subclass\"),\n-    ]\n-\n-    if not tf.__internal__.tf2.enabled():\n-      expected_combinations.extend([\n-          (\"graph\", False, \"functional\"),\n-          (\"graph\", False, \"sequential\"),\n-          (\"graph\", False, \"subclass\"),\n-      ])\n-\n-    self.assertAllEqual(sorted(test_cases), expected_combinations)\n-\n-\n-if __name__ == \"__main__\":\n-  tf.test.main()\n\n@@ -21,7 +21,7 @@ import math\n import numpy as np\n \n from keras import backend\n-from keras import combinations\n+from keras.testing_infra import test_combinations\n from keras import constraints\n \n \n@@ -42,7 +42,7 @@ def get_example_kernel(width):\n   return example_array\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class KerasConstraintsTest(tf.test.TestCase):\n \n   def test_serialization(self):\n\n@@ -18,12 +18,12 @@ import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n from keras import layers\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.engine import training\n from keras.optimizers.optimizer_v2 import gradient_descent as gradient_descent_keras\n \n \n-@testing_utils.run_v2_only\n+@test_utils.run_v2_only\n @tf.__internal__.distribute.combinations.generate(\n     tf.__internal__.test.combinations.combine(\n         strategy=[\n\n@@ -17,11 +17,11 @@\n from absl.testing import parameterized\n import keras\n from keras import optimizers\n-from keras import testing_utils\n from keras.applications import resnet_v2\n from keras.datasets import fashion_mnist\n from keras.distribute import optimizer_combinations\n from keras.distribute import strategy_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n@@ -217,7 +217,7 @@ def iteration_outside_func(initial_weights,\n             training_accuracy.result())\n \n \n-@testing_utils.run_v2_only\n+@test_utils.run_v2_only\n class TestDistributionStrategyDnnCorrectness(tf.test.TestCase,\n                                              parameterized.TestCase):\n   \"\"\"Test custom training loop correctness with a simple DNN model.\"\"\"\n\n@@ -18,7 +18,7 @@ import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n import numpy as np\n-from tensorflow.python.framework import test_util\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n from keras import metrics\n from keras.distribute import strategy_combinations\n \n@@ -94,7 +94,8 @@ class KerasMetricsTest(tf.test.TestCase, parameterized.TestCase):\n   @tf.__internal__.distribute.combinations.generate(\n       tf.__internal__.test.combinations.combine(\n           distribution=strategy_combinations.all_strategies, mode=[\"eager\"]))\n-  @test_util.disable_mlir_bridge(\"TODO(b/168036682): Support dynamic padder\")\n+  @tf_test_utils.disable_mlir_bridge(\n+      \"TODO(b/168036682): Support dynamic padder\")\n   def test_update_keras_metrics_dynamic_shape(self, distribution):\n     with distribution.scope():\n       metric = metrics.Mean(\"test_metric\", dtype=np.float32)\n\n@@ -16,13 +16,13 @@\n \"\"\"Tests for `DatasetCreator` with `Model.fit` across usages and strategies.\"\"\"\n \n from keras import callbacks as callbacks_lib\n-from keras import testing_utils\n from keras.distribute import dataset_creator_model_fit_test_base as test_base\n from keras.distribute import strategy_combinations\n+from keras.testing_infra import test_utils\n import tensorflow.compat.v2 as tf\n \n \n-@testing_utils.run_v2_only\n+@test_utils.run_v2_only\n @tf.__internal__.distribute.combinations.generate(\n     tf.__internal__.test.combinations.combine(\n         strategy=strategy_combinations.parameter_server_strategies_multi_worker,\n\n@@ -18,8 +18,8 @@\n import tensorflow.compat.v2 as tf\n \n import numpy as np\n-from tensorflow.python.framework import test_util\n-from keras import testing_utils\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n+from keras.testing_infra import test_utils\n from keras.distribute import dataset_creator_model_fit_test_base as test_base\n from keras.distribute import strategy_combinations\n from keras.utils import dataset_creator\n@@ -27,7 +27,7 @@ from keras.utils import dataset_creator\n \n # TODO(rchao): Investigate why there cannot be single worker and multi worker\n # PS strategies running in the same shard.\n-@testing_utils.run_v2_only\n+@test_utils.run_v2_only\n @tf.__internal__.distribute.combinations.generate(\n     tf.__internal__.test.combinations.combine(\n         strategy=strategy_combinations.all_strategies +\n@@ -38,7 +38,7 @@ class DatasetCreatorModelFitTest(test_base.DatasetCreatorModelFitTestBase):\n \n   def setUp(self):\n     super().setUp()\n-    if test_util.is_xla_enabled():\n+    if tf_test_utils.is_xla_enabled():\n       self.skipTest(\"model.optimizer.iterations values is not as expected \"\n                     \"with XLA: b/184384487\")\n \n\n@@ -24,7 +24,7 @@ import numpy as np\n import keras\n from tensorflow.python.distribute.cluster_resolver import SimpleClusterResolver\n from keras import backend\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.distribute import distributed_training_utils\n from keras.distribute import distributed_training_utils_v1\n from keras.distribute import multi_worker_testing_utils\n@@ -87,19 +87,19 @@ def simple_multi_inputs_multi_outputs_model():\n \n \n def get_multi_inputs_multi_outputs_data():\n-  (a_train, c_train), (a_test, c_test) = testing_utils.get_test_data(\n+  (a_train, c_train), (a_test, c_test) = test_utils.get_test_data(\n       train_samples=_TRAIN_SIZE,\n       test_samples=50,\n       input_shape=(16,),\n       num_classes=3,\n       random_seed=_RANDOM_SEED)\n-  (b_train, d_train), (b_test, d_test) = testing_utils.get_test_data(\n+  (b_train, d_train), (b_test, d_test) = test_utils.get_test_data(\n       train_samples=_TRAIN_SIZE,\n       test_samples=50,\n       input_shape=(16,),\n       num_classes=2,\n       random_seed=_RANDOM_SEED)\n-  (m_train, _), (m_test, _) = testing_utils.get_test_data(\n+  (m_train, _), (m_test, _) = test_utils.get_test_data(\n       train_samples=_TRAIN_SIZE,\n       test_samples=50,\n       input_shape=(8,),\n@@ -1667,7 +1667,7 @@ class TestRegularizerLoss(tf.test.TestCase, parameterized.TestCase):\n       self.assertEqual(-1.0, v)\n \n \n-@testing_utils.run_all_without_tensor_float_32(\n+@test_utils.run_all_without_tensor_float_32(\n     'Uses Dense layers, which call matmul')\n class TestDistributionStrategyWithKerasModels(tf.test.TestCase,\n                                               parameterized.TestCase):\n\n@@ -20,7 +20,7 @@ import numpy as np\n \n import keras\n from keras import backend\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.distribute import keras_correctness_test_base\n from keras.distribute import strategy_combinations\n from keras.optimizers.optimizer_v2 import gradient_descent as gradient_descent_keras\n@@ -45,7 +45,7 @@ def is_default_strategy(strategy):\n     return not tf.distribute.has_strategy()\n \n \n-@testing_utils.run_all_without_tensor_float_32(\n+@test_utils.run_all_without_tensor_float_32(\n     'Uses Dense layers, which call matmul')\n class TestDistributionStrategyDnnCorrectness(\n     keras_correctness_test_base.TestDistributionStrategyCorrectnessBase):\n@@ -246,7 +246,7 @@ class SubclassedModel(keras.Model):\n     return self.dense4(x)\n \n \n-@testing_utils.run_all_without_tensor_float_32(\n+@test_utils.run_all_without_tensor_float_32(\n     'Uses Dense layers, which call matmul')\n class TestDistributionStrategyDnnCorrectnessWithSubclassedModel(\n     TestDistributionStrategyDnnCorrectness):\n\n@@ -18,12 +18,12 @@ import tensorflow.compat.v2 as tf\n \n import numpy as np\n import keras\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.distribute import keras_correctness_test_base\n from keras.optimizers.optimizer_v2 import gradient_descent\n \n \n-@testing_utils.run_all_without_tensor_float_32(\n+@test_utils.run_all_without_tensor_float_32(\n     'Uses Dense layers, which call matmul. Even if Dense layers run in '\n     'float64, the test sometimes fails with TensorFloat-32 enabled for unknown '\n     'reasons')\n\n@@ -19,7 +19,7 @@ import tensorflow.compat.v2 as tf\n import numpy as np\n \n import keras\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.distribute import keras_correctness_test_base\n from keras.layers import recurrent as rnn_v1\n from keras.layers import recurrent_v2 as rnn_v2\n@@ -64,7 +64,7 @@ class _DistributionStrategyRnnModelCorrectnessTest(\n     return model\n \n \n-@testing_utils.run_all_without_tensor_float_32(\n+@test_utils.run_all_without_tensor_float_32(\n     'Uses Dense layers, which call matmul')\n class DistributionStrategyGruModelCorrectnessTest(\n     _DistributionStrategyRnnModelCorrectnessTest):\n@@ -85,7 +85,7 @@ class DistributionStrategyGruModelCorrectnessTest(\n     self.run_correctness_test(distribution, use_numpy, use_validation_data)\n \n \n-@testing_utils.run_all_without_tensor_float_32(\n+@test_utils.run_all_without_tensor_float_32(\n     'Uses Dense layers, which call matmul')\n class DistributionStrategyLstmModelCorrectnessTest(\n     _DistributionStrategyRnnModelCorrectnessTest):\n@@ -108,7 +108,7 @@ class DistributionStrategyLstmModelCorrectnessTest(\n   @tf.__internal__.distribute.combinations.generate(\n       keras_correctness_test_base.test_combinations_for_embedding_model() +\n       keras_correctness_test_base.multi_worker_mirrored_eager())\n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_lstm_model_correctness_mixed_precision(self, distribution, use_numpy,\n                                                   use_validation_data):\n     if isinstance(distribution,\n\n@@ -14,13 +14,13 @@\n # ==============================================================================\n \"\"\"Tests for saving and loading using keras save/load APIs with DS.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-from keras import testing_utils\n from keras.distribute import saved_model_test_base as test_base\n from keras.saving import save\n+from keras.testing_infra import test_utils\n+import tensorflow.compat.v2 as tf\n \n \n-@testing_utils.run_all_without_tensor_float_32(\n+@test_utils.run_all_without_tensor_float_32(\n     'Uses Dense layers, which call matmul')\n class KerasSaveLoadTest(test_base.TestSavedModelBase):\n \n\n@@ -20,7 +20,7 @@ import tensorflow.compat.v2 as tf\n import time\n \n import keras\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from tensorflow.python.distribute import multi_worker_test_base\n from tensorflow.python.distribute.cluster_resolver import SimpleClusterResolver\n from tensorflow.python.ops import resource_variable_ops\n@@ -88,7 +88,7 @@ class MeanMetricAsCompositeTensor(keras.metrics.Mean,\n     return KerasMetricTypeSpec(self.__class__, self.get_config(), self.weights)\n \n \n-@testing_utils.run_v2_only\n+@test_utils.run_v2_only\n class EvaluationTest(tf.test.TestCase):\n \n   @classmethod\n\n@@ -20,15 +20,15 @@ saved_model's load() API is used. Keras's export_save_model() when used with\n tf.saved_model.save().\n \"\"\"\n \n-import tensorflow.compat.v2 as tf\n-from keras import testing_utils\n from keras.distribute import saved_model_test_base as test_base\n from keras.saving import save\n+from keras.testing_infra import test_utils\n+import tensorflow.compat.v2 as tf\n \n _DEFAULT_FUNCTION_KEY = 'serving_default'\n \n \n-@testing_utils.run_all_without_tensor_float_32(\n+@test_utils.run_all_without_tensor_float_32(\n     'Uses Dense layers, which call matmul')\n class SavedModelSaveAndLoadTest(test_base.TestSavedModelBase):\n \n\n@@ -17,13 +17,13 @@\n import tensorflow.compat.v2 as tf\n \n import os\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.distribute import model_combinations\n from keras.distribute import saved_model_test_base as test_base\n \n \n-@testing_utils.run_v2_only\n-@testing_utils.run_all_without_tensor_float_32(\n+@test_utils.run_v2_only\n+@test_utils.run_all_without_tensor_float_32(\n     'Uses Dense layers, which call matmul')\n class SavedModelKerasModelTest(test_base.TestSavedModelBase):\n \n@@ -84,7 +84,7 @@ class SavedModelKerasModelTest(test_base.TestSavedModelBase):\n           self.assertEmpty(n.device)\n \n \n-@testing_utils.run_v2_only\n+@test_utils.run_v2_only\n class SavedModelTFModuleTest(test_base.TestSavedModelBase):\n \n   def setUp(self):\n\n@@ -22,9 +22,9 @@ import time\n \n from absl.testing import parameterized\n import keras\n-from keras import testing_utils\n from keras.distribute import sidecar_evaluator as sidecar_evaluator_lib\n from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n from tensorflow.python.platform import tf_logging as logging  # pylint: disable=g-direct-tensorflow-import\n@@ -72,7 +72,7 @@ def _test_model_builder(model_type: ModelType, compile_model, build_model):\n   return model\n \n \n-@testing_utils.run_v2_only\n+@test_utils.run_v2_only\n class SidecarEvaluatorTest(tf.test.TestCase, parameterized.TestCase):\n \n   def assertSummaryEventsWritten(self, log_dir):\n\n@@ -21,11 +21,10 @@ import os\n \n import numpy as np\n from keras import backend\n-from keras import combinations\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras import layers\n from keras import regularizers\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.engine import base_layer\n from keras.engine import input_layer\n from keras.engine import sequential\n@@ -57,10 +56,10 @@ class InvalidLayer(base_layer.Layer):\n     raise ValueError('You did something wrong!')\n \n \n-@testing_utils.run_v2_only\n-class BaseLayerTest(keras_parameterized.TestCase):\n+@test_utils.run_v2_only\n+class BaseLayerTest(test_combinations.TestCase):\n \n-  @combinations.generate(combinations.keras_mode_combinations())\n+  @test_combinations.generate(test_combinations.keras_mode_combinations())\n   def test_layer_instrumentation(self):\n     layer = layers.Add()\n     self.assertTrue(layer._instrumented_keras_api)\n@@ -73,23 +72,23 @@ class BaseLayerTest(keras_parameterized.TestCase):\n         base_layer.keras_api_gauge.get_cell('legacy_layer').value())\n     base_layer.keras_api_gauge.get_cell('tf.keras.layers.Add').set(False)\n \n-  @combinations.generate(combinations.keras_model_type_combinations())\n+  @test_combinations.generate(test_combinations.keras_model_type_combinations())\n   def test_dynamic_layer(self):\n-    model = testing_utils.get_model_from_layers([DynamicLayer(dynamic=True)],\n+    model = test_utils.get_model_from_layers([DynamicLayer(dynamic=True)],\n                                              input_shape=(3,))\n     self.assertEqual(model.dynamic, True)\n     model.compile(rmsprop.RMSprop(0.001), loss='mse')\n     self.assertEqual(model.run_eagerly, True)\n     model.train_on_batch(np.random.random((2, 3)), np.random.random((2, 3)))\n \n-  @combinations.generate(combinations.keras_model_type_combinations())\n+  @test_combinations.generate(test_combinations.keras_model_type_combinations())\n   def test_dynamic_layer_error(self):\n     # Functional Models hit the `dyanamic=True` error during construction.\n     # Subclass Models should just throw the original autograph error during\n     # execution.\n     raised_error = False\n     try:\n-      model = testing_utils.get_model_from_layers([DynamicLayer()],\n+      model = test_utils.get_model_from_layers([DynamicLayer()],\n                                                input_shape=(3,))\n       model.compile(rmsprop.RMSprop(0.001), loss='mse')\n       model.train_on_batch(np.random.random((2, 3)), np.random.random((2, 3)))\n@@ -105,10 +104,10 @@ class BaseLayerTest(keras_parameterized.TestCase):\n         raised_error = True\n     self.assertTrue(raised_error)\n \n-  @combinations.generate(combinations.keras_model_type_combinations())\n+  @test_combinations.generate(test_combinations.keras_model_type_combinations())\n   def test_dynamic_layer_error_running_in_graph_mode(self):\n     with tf.compat.v1.get_default_graph().as_default():\n-      model = testing_utils.get_model_from_layers([DynamicLayer(dynamic=True)],\n+      model = test_utils.get_model_from_layers([DynamicLayer(dynamic=True)],\n                                                input_shape=(3,))\n       self.assertEqual(model.dynamic, True)\n       # But then you cannot run the model since you're in a graph scope.\n@@ -227,7 +226,8 @@ class BaseLayerTest(keras_parameterized.TestCase):\n     self.assertEqual(new_layer.bias_regularizer, bias_reg)\n     self.assertEqual(layer.get_config(), new_layer.get_config())\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_invalid_forward_pass(self):\n     inputs = input_layer.Input((3,))\n     with self.assertRaisesRegex(ValueError, 'You did something wrong!'):\n@@ -267,20 +267,21 @@ class BaseLayerTest(keras_parameterized.TestCase):\n       for l in layer:\n         model.add(l)\n \n-  @combinations.generate(\n-      combinations.times(\n-          combinations.keras_model_type_combinations(),\n-          combinations.combine(mode=['graph', 'eager'])))\n+  @test_combinations.generate(\n+      test_combinations.times(\n+          test_combinations.keras_model_type_combinations(),\n+          test_combinations.combine(mode=['graph', 'eager'])))\n   def test_build_with_numpy_data(self):\n     model_layers = [\n         layers.Dense(3, activation='relu', kernel_initializer='ones'),\n         layers.Dense(1, activation='sigmoid', kernel_initializer='ones')\n     ]\n-    model = testing_utils.get_model_from_layers(model_layers, input_shape=(4,))\n+    model = test_utils.get_model_from_layers(model_layers, input_shape=(4,))\n     model(np.zeros((2, 4), dtype='float32'))\n     self.assertTrue(model.built)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_default_add_weight(self):\n \n     class TestLayer(base_layer.Layer):\n@@ -302,7 +303,8 @@ class BaseLayerTest(keras_parameterized.TestCase):\n       # Cannot access tensor.name in eager execution.\n       self.assertIn('Variable_2/Regularizer', layer.losses[0].name)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_add_weight_by_getter(self):\n     layer = base_layer.Layer()\n     variable = tf.Variable('abc')\n@@ -310,7 +312,8 @@ class BaseLayerTest(keras_parameterized.TestCase):\n         dtype=tf.string, getter=lambda *_, **__: variable)\n     self.assertIs(variable, added)\n \n-  @combinations.generate(combinations.keras_mode_combinations(mode=['eager']))\n+  @test_combinations.generate(\n+      test_combinations.keras_mode_combinations(mode=['eager']))\n   def test_learning_phase_freezing_for_layers(self):\n \n     class LearningPhaseLayer(base_layer.Layer):\n@@ -321,7 +324,7 @@ class BaseLayerTest(keras_parameterized.TestCase):\n \n     def get_learning_phase_value():\n       model = sequential.Sequential([LearningPhaseLayer(input_shape=(1,))])\n-      model._run_eagerly = testing_utils.should_run_eagerly()\n+      model._run_eagerly = test_utils.should_run_eagerly()\n       return np.sum(model(np.ones((1, 1))))\n \n     self.assertEqual(get_learning_phase_value(), 0)\n@@ -340,7 +343,8 @@ class BaseLayerTest(keras_parameterized.TestCase):\n     self.assertEqual(get_learning_phase_value(), 0)\n \n   # Cannot be enabled with `run_eagerly=True`, see b/123904578\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_layer_can_return_variable(self):\n \n     class ComputeSum(base_layer.Layer):\n@@ -376,12 +380,13 @@ class BaseLayerTest(keras_parameterized.TestCase):\n     return TrainingLayer()\n \n   # b/124459427: can't test with `run_eagerly=True` for now.\n-  @combinations.generate(\n-      combinations.times(combinations.keras_mode_combinations(),\n-                         combinations.keras_model_type_combinations()))\n+  @test_combinations.generate(\n+      test_combinations.times(\n+          test_combinations.keras_mode_combinations(),\n+          test_combinations.keras_model_type_combinations()))\n   def test_training_arg_in_defun(self):\n     layer = self._get_layer_with_training_arg()\n-    model = testing_utils.get_model_from_layers([layer], input_shape=(1,))\n+    model = test_utils.get_model_from_layers([layer], input_shape=(1,))\n     model.compile(rmsprop.RMSprop(0.),\n                   loss='mae')\n     history = model.fit(np.zeros((1, 1)), np.zeros((1, 1)))\n@@ -401,9 +406,10 @@ class BaseLayerTest(keras_parameterized.TestCase):\n     history = model.fit(np.zeros((1, 1)), np.zeros((1, 1)))\n     self.assertEqual(history.history['loss'][0], 0.)\n \n-  @combinations.generate(\n-      combinations.times(combinations.keras_mode_combinations(),\n-                         combinations.keras_model_type_combinations()))\n+  @test_combinations.generate(\n+      test_combinations.times(\n+          test_combinations.keras_mode_combinations(),\n+          test_combinations.keras_model_type_combinations()))\n   def test_raw_variable_assignment(self):\n \n     class RawVariableLayer(base_layer.Layer):\n@@ -416,17 +422,17 @@ class BaseLayerTest(keras_parameterized.TestCase):\n       def call(self, inputs):\n         return inputs * self.var_list[0] * self.var_list[1]['a']\n \n-    model = testing_utils.get_model_from_layers([RawVariableLayer()],\n+    model = test_utils.get_model_from_layers([RawVariableLayer()],\n                                              input_shape=(10,))\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     x, y = np.ones((10, 10)), np.ones((10, 10))\n     # Checks that variables get initialized.\n     model.fit(x, y, batch_size=2, epochs=2)\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def test_composite_variable_assignment(self):\n \n     class Spec(tf.TypeSpec):\n@@ -490,7 +496,8 @@ class BaseLayerTest(keras_parameterized.TestCase):\n                                 'Expected `name` argument to be a string'):\n       base_layer.Layer(name=0)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_layer_names(self):\n     inputs = input_layer.Input(shape=[2])\n     add1 = inputs + inputs\n@@ -510,7 +517,7 @@ class BaseLayerTest(keras_parameterized.TestCase):\n     for actual, eager, graph in zip(actual_names, graph_names, eager_names):\n       self.assertIn(actual, {eager, graph})\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def test_layer_names_after_loading(self):\n     backend.clear_session()\n     # Mimic loading a model that already contained add layers with\n@@ -550,25 +557,27 @@ class BaseLayerTest(keras_parameterized.TestCase):\n     layer.trainable = True\n     self.assertListEqual(layer.trainable_weights, [layer.w])\n \n-  @combinations.generate(\n-      combinations.times(combinations.keras_mode_combinations(),\n-                         combinations.keras_model_type_combinations()))\n+  @test_combinations.generate(\n+      test_combinations.times(\n+          test_combinations.keras_mode_combinations(),\n+          test_combinations.keras_model_type_combinations()))\n   def test_passing_initial_weights_values(self):\n     kernel_value = np.random.random((10, 2))\n     layer_with_weights = layers.Dense(2, use_bias=False, weights=[kernel_value])\n \n-    model = testing_utils.get_model_from_layers([layer_with_weights],\n+    model = test_utils.get_model_from_layers([layer_with_weights],\n                                              input_shape=(10,))\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     inputs = np.random.random((3, 10))\n     out = model.predict(inputs)\n     self.assertAllClose(model.layers[-1].get_weights()[0], kernel_value)\n     self.assertAllClose(out, np.dot(inputs, kernel_value))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_set_weights_and_get_weights(self):\n     layer = layers.Dense(2)\n     layer.build((None, 10))\n@@ -586,7 +595,8 @@ class BaseLayerTest(keras_parameterized.TestCase):\n                                 'not compatible with provided weight shape'):\n       layer.set_weights([kernel.T, bias])\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_set_weights_accepts_output_of_get_weights(self):\n     layer = layers.Layer()\n     layer.add_weight(name='scalar_float', shape=(), dtype=tf.float32)\n@@ -634,7 +644,8 @@ class BaseLayerTest(keras_parameterized.TestCase):\n     # arguments, no error is thrown:\n     self.assertEqual(MyLayerNew2(name='New').get_config()['name'], 'New')\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_count_params(self):\n     dense = layers.Dense(16)\n     dense.build((None, 4))\n@@ -663,7 +674,8 @@ class BaseLayerTest(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(RuntimeError, 'You must call `super()'):\n       layer(np.random.random((10, 2)))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_first_arg_not_called_inputs(self):\n     x, y = tf.ones((10, 1)), tf.ones((10, 1))\n \n@@ -735,7 +747,8 @@ class BaseLayerTest(keras_parameterized.TestCase):\n     layer([tf.constant([1.0]), None, tf.constant([2.0])])\n     self.assertEqual(layer.build_shape, [[1], None, [1]])\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_layer_input_shape_raises_error(self):\n     layer = layers.Dense(3)\n     with self.assertRaisesRegex(AttributeError, 'no defined input shape'):\n@@ -745,7 +758,7 @@ class BaseLayerTest(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(AttributeError, 'no defined input shape'):\n       _ = layer.input_shape\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def test_custom_layer_training_arg(self):\n     class CustomLayerNoTrainingArg(base_layer.Layer):\n \n@@ -811,7 +824,7 @@ class BaseLayerTest(keras_parameterized.TestCase):\n         CustomLayerDefaultTrainingFalse=CustomLayerDefaultTrainingFalse,\n         CustomLayerDefaultTrainingTrue=CustomLayerDefaultTrainingTrue)\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def test_custom_layer_training_arg_kwargonly(self):\n     class CustomLayerNoTrainingArg(base_layer.Layer):\n \n@@ -1003,8 +1016,8 @@ class BaseLayerTest(keras_parameterized.TestCase):\n     self.assertLen(model.non_trainable_variables, 2)\n \n \n-@testing_utils.run_v2_only\n-class SymbolicSupportTest(keras_parameterized.TestCase):\n+@test_utils.run_v2_only\n+class SymbolicSupportTest(test_combinations.TestCase):\n \n   def test_using_symbolic_tensors_with_tf_ops(self):\n     # Single-input.\n@@ -1040,7 +1053,8 @@ class SymbolicSupportTest(keras_parameterized.TestCase):\n     with self.assertRaises(TypeError):\n       tf.matmul(x1, x2)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_mixing_keras_symbolic_tensors_and_eager_tensors(self):\n     x1 = input_layer.Input((3,))\n     x2 = tf.ones((3, 3))\n@@ -1053,7 +1067,8 @@ class SymbolicSupportTest(keras_parameterized.TestCase):\n                         np.matmul(x_val, y_val),\n                         atol=1e-5)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_mixing_keras_symbolic_tensors_and_numpy_arrays(self):\n     x1 = input_layer.Input((3,))\n     x2 = np.ones((3, 3), dtype='float32')\n@@ -1066,7 +1081,8 @@ class SymbolicSupportTest(keras_parameterized.TestCase):\n                         np.matmul(x_val, y_val),\n                         atol=1e-5)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_reraising_exception(self):\n     # When layer is not dynamic, we have some pattern matching during exception\n     # handling to detect when the user is trying to use python control flow.\n@@ -1088,7 +1104,8 @@ class SymbolicSupportTest(keras_parameterized.TestCase):\n     except TypeError as e:\n       self.assertIn('easily_identifiable_name', str(e))  # pylint: disable=g-assert-in-except\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_summaries_in_tf_function(self):\n     if not tf.executing_eagerly():\n       return\n@@ -1119,7 +1136,8 @@ class SymbolicSupportTest(keras_parameterized.TestCase):\n         tags.add(val.tag)\n     self.assertEqual(set(['my_layer/mean']), tags)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_error_when_passing_non_tensor(self):\n     # layers that have an `input_spec` will raise an error when called on\n     # non-tensors. This covers all built-in layers.\n@@ -1129,8 +1147,8 @@ class SymbolicSupportTest(keras_parameterized.TestCase):\n       layer(x)\n \n \n-@testing_utils.run_v2_only\n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_utils.run_v2_only\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class NestedTrackingTest(tf.test.TestCase):\n \n   def test_nested_layer_variable_tracking(self):\n@@ -1303,9 +1321,9 @@ class NestedTrackingTest(tf.test.TestCase):\n     self.assertLen(s.weights, 4)\n \n \n-@testing_utils.run_v2_only\n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-class NameScopingTest(keras_parameterized.TestCase):\n+@test_utils.run_v2_only\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n+class NameScopingTest(test_combinations.TestCase):\n \n   def test_name_scope_layer(self):\n     x = backend.placeholder(shape=(10, 10))\n@@ -1362,7 +1380,7 @@ class NameScopingTest(keras_parameterized.TestCase):\n     self.assertEqual(layer.bias.name, 'MyName3/bias:0')\n     self.assertEqual(layer.kernel.name, 'MyName3/kernel:0')\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_apply_name_scope_on_model_declaration(self):\n     if not tf.executing_eagerly():\n       self.skipTest('`apply_name_scope_on_model_declaration` API is supported'\n@@ -1434,9 +1452,10 @@ class NameScopingTest(keras_parameterized.TestCase):\n     ]\n \n \n-@testing_utils.run_v2_only\n-@combinations.generate(combinations.keras_mode_combinations(mode=['eager']))\n-class AutographControlFlowTest(keras_parameterized.TestCase):\n+@test_utils.run_v2_only\n+@test_combinations.generate(\n+    test_combinations.keras_mode_combinations(mode=['eager']))\n+class AutographControlFlowTest(test_combinations.TestCase):\n \n   def test_disabling_in_context_is_matched(self):\n \n@@ -1471,7 +1490,7 @@ class AutographControlFlowTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     train_loss = model.train_on_batch(np.ones((2, 3)), np.ones((2, 3)))\n     self.assertEqual(train_loss, 0.)\n     test_loss = model.test_on_batch(np.ones((2, 3)), np.ones((2, 3)))\n@@ -1495,7 +1514,7 @@ class AutographControlFlowTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     train_loss = model.train_on_batch(np.ones((2, 3)), np.ones((2, 3)))\n     self.assertEqual(train_loss, 2 * 3)\n     test_loss = model.test_on_batch(np.ones((2, 3)), np.ones((2, 3)))\n@@ -1519,7 +1538,7 @@ class AutographControlFlowTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     for _ in range(3):\n       _, train_metric = model.train_on_batch(np.ones((2, 3)),\n                                              np.ones((2, 3)))\n@@ -1552,7 +1571,7 @@ class AutographControlFlowTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(np.ones((2, 3)), np.ones((2, 3)))\n     self.assertEqual(backend.get_value(layer.counter), 1.)\n \n@@ -1562,7 +1581,7 @@ class AutographControlFlowTest(keras_parameterized.TestCase):\n \n       def __init__(self):\n         super(MyLayer,\n-              self).__init__(dynamic=testing_utils.should_run_eagerly())\n+              self).__init__(dynamic=test_utils.should_run_eagerly())\n \n       def call(self, inputs, training=None):\n         if training:\n@@ -1576,7 +1595,7 @@ class AutographControlFlowTest(keras_parameterized.TestCase):\n     layer = MyLayer()\n     outputs = layer(inputs)\n     model = training_lib.Model(inputs, outputs)\n-    model.compile('sgd', 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+    model.compile('sgd', 'mse', run_eagerly=test_utils.should_run_eagerly())\n     loss = model.train_on_batch(np.ones((2, 3)), np.ones((2, 3)))\n     self.assertEqual(loss, 2 * 3)\n \n@@ -1585,7 +1604,7 @@ class AutographControlFlowTest(keras_parameterized.TestCase):\n         layers.Dense(\n             1, kernel_regularizer=regularizers.l2(1e-4), input_shape=(1,))\n     ])\n-    model._run_eagerly = testing_utils.should_run_eagerly()\n+    model._run_eagerly = test_utils.should_run_eagerly()\n \n     def assert_graph(t):\n       if not tf.executing_eagerly():\n@@ -1607,7 +1626,7 @@ class AutographControlFlowTest(keras_parameterized.TestCase):\n \n       def __init__(self):\n         super(MyLayer,\n-              self).__init__(dynamic=testing_utils.should_run_eagerly())\n+              self).__init__(dynamic=test_utils.should_run_eagerly())\n \n       def call(self, inputs, training=None):\n         if training:\n@@ -1623,7 +1642,7 @@ class AutographControlFlowTest(keras_parameterized.TestCase):\n     layer = MyLayer()\n     outputs = layer(inputs)\n     model = training_lib.Model(inputs, outputs)\n-    model.compile('sgd', 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+    model.compile('sgd', 'mse', run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(np.ones((2, 3)), np.ones((2, 3)))\n     self.assertEqual(history.history['sum'][-1], 2 * 3)\n \n@@ -1633,7 +1652,7 @@ class AutographControlFlowTest(keras_parameterized.TestCase):\n \n       def __init__(self):\n         super(TestModel, self).__init__(\n-            name='test_model', dynamic=testing_utils.should_run_eagerly())\n+            name='test_model', dynamic=test_utils.should_run_eagerly())\n         self.layer = layers.Dense(2, activity_regularizer='l2')\n \n       def call(self, x, training=None):\n@@ -1646,12 +1665,12 @@ class AutographControlFlowTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='mse',\n         optimizer='sgd',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x = np.ones(shape=(10, 1))\n     y = np.ones(shape=(10, 2))\n \n-    if testing_utils.should_run_eagerly():\n+    if test_utils.should_run_eagerly():\n       model.fit(x, y, epochs=2, batch_size=5)\n     else:\n       with self.assertRaisesRegex(ValueError, 'ActivityRegularizer'):\n@@ -1663,7 +1682,7 @@ class AutographControlFlowTest(keras_parameterized.TestCase):\n \n       def __init__(self):\n         super(TestModel, self).__init__(\n-            name='test_model', dynamic=testing_utils.should_run_eagerly())\n+            name='test_model', dynamic=test_utils.should_run_eagerly())\n         self.layer = layers.TimeDistributed(\n             layers.Dense(2, activity_regularizer='l2'), input_shape=(3, 4))\n \n@@ -1677,12 +1696,12 @@ class AutographControlFlowTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='mse',\n         optimizer='sgd',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x = np.ones(shape=(10, 3, 4))\n     y = np.ones(shape=(10, 3, 2))\n \n-    if testing_utils.should_run_eagerly():\n+    if test_utils.should_run_eagerly():\n       model.fit(x, y, epochs=2, batch_size=5)\n     else:\n       with self.assertRaisesRegex(ValueError, 'ActivityRegularizer'):\n@@ -1713,14 +1732,14 @@ class IdentityLayer(base_layer.Layer):\n     return inputs\n \n \n-@testing_utils.run_v2_only\n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-class DTypeTest(keras_parameterized.TestCase):\n+@test_utils.run_v2_only\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n+class DTypeTest(test_combinations.TestCase):\n \n   def _const(self, dtype):\n     return tf.constant(1, dtype=dtype)\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_dtype_defaults_to_floatx(self):\n     layer = AddLayer()\n     self.assertEqual(layer.dtype, 'float32')\n@@ -1734,7 +1753,7 @@ class DTypeTest(keras_parameterized.TestCase):\n     finally:\n       backend.set_floatx('float32')\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_passing_dtype_to_constructor(self):\n     layer = IdentityLayer(dtype='float64')\n     layer(self._const('float32'))\n@@ -1748,7 +1767,7 @@ class DTypeTest(keras_parameterized.TestCase):\n     layer(self._const('float32'))\n     self.assertEqual(layer.dtype, 'float64')\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def input_cast_to_dtype(self):\n     layer = AddLayer()\n \n@@ -1774,7 +1793,7 @@ class DTypeTest(keras_parameterized.TestCase):\n     layer = IdentityLayer(dtype='float64')\n     self.assertEqual(layer(1.).dtype, 'float64')\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def multiple_inputs_cast_to_dtype(self):\n \n     class MultiIdentityLayer(base_layer.Layer):\n@@ -1803,7 +1822,7 @@ class DTypeTest(keras_parameterized.TestCase):\n     self.assertEqual(z.dtype, 'float64')\n     self.assertEqual(w.dtype, 'complex64')\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_extra_args_and_kwargs_not_casted(self):\n \n     class IdentityLayerWithArgs(base_layer.Layer):\n@@ -1819,7 +1838,7 @@ class DTypeTest(keras_parameterized.TestCase):\n     self.assertEqual(y.dtype, 'float16')\n     self.assertEqual(z.dtype, 'float16')\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_layer_without_autocast(self):\n \n     class IdentityLayerWithoutAutocast(IdentityLayer):\n@@ -1831,7 +1850,7 @@ class DTypeTest(keras_parameterized.TestCase):\n     layer = IdentityLayerWithoutAutocast(dtype='float64')\n     self.assertEqual(layer(self._const('float32')).dtype, 'float32')\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_compute_output_signature(self):\n \n     class IdentityLayerWithOutputShape(IdentityLayer):\n@@ -1845,7 +1864,7 @@ class DTypeTest(keras_parameterized.TestCase):\n     self.assertEqual(output_signature.shape, ())\n     self.assertEqual(output_signature.dtype, 'float64')\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_composite_tensors_input_casting(self):\n     sparse = tf.SparseTensor(\n         indices=tf.constant([[0, 1], [2, 3]], dtype='int64'),\n@@ -1863,14 +1882,14 @@ class DTypeTest(keras_parameterized.TestCase):\n       self.assertEqual(y.dtype, 'float16')\n       self.assertEqual(type(x), type(y))\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_passing_non_tensor(self):\n     layer = IdentityLayer()\n     x = object()\n     y = layer(x)  # Layer should not cast 'x', as it's not a tensor\n     self.assertIs(x, y)\n \n-  @testing_utils.disable_v2_dtype_behavior\n+  @test_utils.disable_v2_dtype_behavior\n   def test_v1_behavior(self):\n     # Test dtype defaults to None and inferred from input\n     layer = IdentityLayer()\n\n@@ -19,13 +19,12 @@ import tensorflow.compat.v2 as tf\n \n import keras\n from keras import backend\n-from keras import combinations\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras.engine import base_layer_utils\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-class TrackableWeightHandlerTest(keras_parameterized.TestCase):\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n+class TrackableWeightHandlerTest(test_combinations.TestCase):\n \n   def get_table_handler(self):\n     # Note: There is some repetition in these tests' setup. However, Tensorflow\n@@ -65,8 +64,8 @@ class TrackableWeightHandlerTest(keras_parameterized.TestCase):\n     _ = backend.batch_get_value(table_handler.get_tensors())\n \n \n-@combinations.generate(combinations.combine(mode=['eager']))\n-class OpLayerTest(keras_parameterized.TestCase):\n+@test_combinations.generate(test_combinations.combine(mode=['eager']))\n+class OpLayerTest(test_combinations.TestCase):\n \n   def test_tensor_op_layer(self):\n     int_values = keras.Input(shape=(2,), dtype=tf.int32)\n\n@@ -17,9 +17,9 @@\n import os\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n from keras.engine import base_preprocessing_layer\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n@@ -50,8 +50,8 @@ class AddingPreprocessingLayer(base_preprocessing_layer.PreprocessingLayer):\n     return inputs + self.sum\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class PreprocessingLayerTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class PreprocessingLayerTest(test_combinations.TestCase):\n \n   def test_adapt_bad_input_fails(self):\n     \"\"\"Test that non-Dataset/Numpy inputs cause a reasonable error.\"\"\"\n@@ -85,7 +85,7 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n     layer = AddingPreprocessingLayer()\n     output = layer(input_data)\n     model = keras.Model(input_data, output)\n-    model._run_eagerly = testing_utils.should_run_eagerly()\n+    model._run_eagerly = test_utils.should_run_eagerly()\n \n     layer.set_total(15)\n \n@@ -101,7 +101,7 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n     input_data = keras.Input(shape=(1,))\n     output = layer(input_data)\n     model = keras.Model(input_data, output)\n-    model._run_eagerly = testing_utils.should_run_eagerly()\n+    model._run_eagerly = test_utils.should_run_eagerly()\n \n     self.assertAllEqual([[16], [17], [18]], model.predict([1., 2., 3.]))\n \n@@ -113,7 +113,7 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n     layer = AddingPreprocessingLayer()\n     output = layer(input_data)\n     model = keras.Model(input_data, output)\n-    model._run_eagerly = testing_utils.should_run_eagerly()\n+    model._run_eagerly = test_utils.should_run_eagerly()\n \n     layer.adapt(input_dataset)\n \n@@ -130,7 +130,7 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n     input_data = keras.Input(shape=(1,))\n     output = layer(input_data)\n     model = keras.Model(input_data, output)\n-    model._run_eagerly = testing_utils.should_run_eagerly()\n+    model._run_eagerly = test_utils.should_run_eagerly()\n \n     self.assertAllEqual([[16], [17], [18]], model.predict([1., 2., 3.]))\n \n@@ -143,7 +143,7 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n     layer = AddingPreprocessingLayer()\n     output = layer(input_data)\n     model = keras.Model(input_data, output)\n-    model._run_eagerly = testing_utils.should_run_eagerly()\n+    model._run_eagerly = test_utils.should_run_eagerly()\n \n     layer.adapt(input_dataset)\n \n@@ -157,7 +157,7 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n       layer = AddingPreprocessingLayer()\n       output = layer(input_data)\n       model = keras.Model(input_data, output)\n-      model._run_eagerly = testing_utils.should_run_eagerly()\n+      model._run_eagerly = test_utils.should_run_eagerly()\n       return (model, layer)\n \n     input_dataset = np.array([1, 2, 3, 4, 5])\n@@ -221,7 +221,7 @@ class PreprocessingLayerTest(keras_parameterized.TestCase):\n     self.assertEqual(model.input_shape, (None, 1, 2))\n \n \n-class PreprocessingLayerV1Test(keras_parameterized.TestCase):\n+class PreprocessingLayerV1Test(test_combinations.TestCase):\n \n   def test_adapt_fails(self):\n     \"\"\"Test that calling adapt leads to a runtime error.\"\"\"\n\n@@ -16,13 +16,13 @@\n \n import tensorflow.compat.v2 as tf\n from keras import backend\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras import losses as losses_mod\n from keras import metrics as metrics_mod\n from keras.engine import compile_utils\n \n \n-class LossesContainerTest(keras_parameterized.TestCase):\n+class LossesContainerTest(test_combinations.TestCase):\n \n   def test_single_loss(self):\n     loss_container = compile_utils.LossesContainer('mse')\n@@ -395,7 +395,7 @@ class LossesContainerTest(keras_parameterized.TestCase):\n     self.assertEqual(loss_container._losses[0].name, 'custom_loss_fn')\n \n \n-class MetricsContainerTest(keras_parameterized.TestCase):\n+class MetricsContainerTest(test_combinations.TestCase):\n \n   def test_single_metric(self):\n     metric_container = compile_utils.MetricsContainer('mse')\n\n@@ -20,8 +20,8 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.engine import base_layer\n from keras.optimizers.optimizer_v2 import rmsprop\n \n@@ -96,15 +96,15 @@ class FunctionControlFlowModel(keras.Model):\n       return tf.square(inputs)\n \n \n-@keras_parameterized.run_all_keras_modes\n-class AutographWrapperTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class AutographWrapperTest(test_combinations.TestCase):\n \n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_with_all_model_types\n   @parameterized.named_parameters(('with_if', ControlFlowLayer1),\n                                   ('with_for', ControlFlowLayer2),\n                                   ('nested', NestedControlFlowLayer))\n   def test_control_flow_layer(self, layer_class):\n-    model = testing_utils.get_model_from_layers([layer_class()],\n+    model = test_utils.get_model_from_layers([layer_class()],\n                                              input_shape=(3,))\n     model.compile(rmsprop.RMSprop(0.001), loss='mse')\n     model.train_on_batch(np.random.random((2, 3)), np.random.random((2, 3)))\n\n@@ -20,8 +20,8 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n \n \n class MultiInputSubclassed(keras.Model):\n@@ -30,7 +30,7 @@ class MultiInputSubclassed(keras.Model):\n   def __init__(self):\n     super(MultiInputSubclassed, self).__init__()\n     self.add = keras.layers.Add()\n-    self.bias = testing_utils.Bias()\n+    self.bias = test_utils.Bias()\n \n   def call(self, inputs):\n     added = self.add(inputs)\n@@ -43,21 +43,21 @@ def multi_input_functional():\n   input_2 = keras.Input(shape=(1,))\n   input_3 = keras.Input(shape=(1,))\n   added = keras.layers.Add()([input_1, input_2, input_3])\n-  output = testing_utils.Bias()(added)\n+  output = test_utils.Bias()(added)\n   return keras.Model([input_1, input_2, input_3], output)\n \n \n-@keras_parameterized.run_with_all_model_types\n-@keras_parameterized.run_all_keras_modes\n-class SimpleBiasTest(keras_parameterized.TestCase):\n+@test_combinations.run_with_all_model_types\n+@test_combinations.run_all_keras_modes\n+class SimpleBiasTest(test_combinations.TestCase):\n \n   def _get_simple_bias_model(self):\n-    model = testing_utils.get_model_from_layers([testing_utils.Bias()],\n+    model = test_utils.get_model_from_layers([test_utils.Bias()],\n                                              input_shape=(1,))\n     model.compile(\n         keras.optimizers.optimizer_v2.gradient_descent.SGD(0.1),\n         'mae',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     return model\n \n   def test_simple_bias_fit(self):\n@@ -84,8 +84,8 @@ class SimpleBiasTest(keras_parameterized.TestCase):\n     self.assertAllClose(x, pred)\n \n \n-@keras_parameterized.run_all_keras_modes\n-class MultipleInputTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class MultipleInputTest(test_combinations.TestCase):\n \n   def _get_multiple_input_model(self, subclassed=True):\n     if subclassed:\n@@ -95,7 +95,7 @@ class MultipleInputTest(keras_parameterized.TestCase):\n     model.compile(\n         keras.optimizers.optimizer_v2.gradient_descent.SGD(0.1),\n         'mae',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     return model\n \n   @parameterized.named_parameters(('subclassed', True), ('functional', False))\n\n@@ -22,8 +22,8 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.engine import data_adapter\n from keras.utils import data_utils\n \n@@ -56,7 +56,7 @@ def fail_on_convert(x, **kwargs):\n tf.register_tensor_conversion_function(DummyArrayLike, fail_on_convert)\n \n \n-class DataAdapterTestBase(keras_parameterized.TestCase):\n+class DataAdapterTestBase(test_combinations.TestCase):\n \n   def setUp(self):\n     super(DataAdapterTestBase, self).setUp()\n@@ -163,10 +163,10 @@ class TensorLikeDataAdapterTest(DataAdapterTestBase):\n     with self.assertRaises(StopIteration):\n       next(ds_iter)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_training_numpy(self):\n     self.model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',\n-                       run_eagerly=testing_utils.should_run_eagerly())\n+                       run_eagerly=test_utils.should_run_eagerly())\n     self.model.fit(self.numpy_input, self.numpy_target, batch_size=5)\n \n   def test_can_handle_pandas(self):\n@@ -182,7 +182,7 @@ class TensorLikeDataAdapterTest(DataAdapterTestBase):\n             pd.DataFrame(self.numpy_input),\n             pd.DataFrame(self.numpy_input)[0]))\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_training_pandas(self):\n     try:\n       import pandas as pd  # pylint: disable=g-import-not-at-top\n@@ -272,10 +272,10 @@ class TensorLikeDataAdapterTest(DataAdapterTestBase):\n     self.assertFalse(self.adapter_cls.can_handle(self.text_input))\n     self.assertFalse(self.adapter_cls.can_handle(self.bytes_input))\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_training(self):\n     self.model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',\n-                       run_eagerly=testing_utils.should_run_eagerly())\n+                       run_eagerly=test_utils.should_run_eagerly())\n     self.model.fit(self.tensor_input, self.tensor_target, batch_size=5)\n \n   def test_size(self):\n@@ -284,7 +284,7 @@ class TensorLikeDataAdapterTest(DataAdapterTestBase):\n     self.assertEqual(adapter.get_size(), 10)\n     self.assertFalse(adapter.has_partial_batch())\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_shuffle_correctness(self):\n     num_samples = 100\n     batch_size = 32\n@@ -317,7 +317,7 @@ class TensorLikeDataAdapterTest(DataAdapterTestBase):\n     # Check that each elements appears, and only once.\n     self.assertAllClose(x, np.sort(second_epoch_data))\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_batch_shuffle_correctness(self):\n     num_samples = 100\n     batch_size = 6\n@@ -462,7 +462,7 @@ class GenericArrayLikeDataAdapterTest(DataAdapterTestBase):\n     with self.assertRaises(StopIteration):\n       next(ds_iter)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_training(self):\n     # First verify that DummyArrayLike can't be converted to a Tensor\n     with self.assertRaises(TypeError):\n@@ -472,7 +472,7 @@ class GenericArrayLikeDataAdapterTest(DataAdapterTestBase):\n     # It should not be converted to a tensor directly (which would force it into\n     # memory), only the sliced data should be converted.\n     self.model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',\n-                       run_eagerly=testing_utils.should_run_eagerly())\n+                       run_eagerly=test_utils.should_run_eagerly())\n     self.model.fit(self.arraylike_input,\n                    self.arraylike_target, batch_size=5)\n     self.model.fit(self.arraylike_input,\n@@ -485,10 +485,10 @@ class GenericArrayLikeDataAdapterTest(DataAdapterTestBase):\n                         self.arraylike_target, batch_size=5)\n     self.model.predict(self.arraylike_input, batch_size=5)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_training_numpy_target(self):\n     self.model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',\n-                       run_eagerly=testing_utils.should_run_eagerly())\n+                       run_eagerly=test_utils.should_run_eagerly())\n     self.model.fit(self.arraylike_input,\n                    self.numpy_target, batch_size=5)\n     self.model.fit(self.arraylike_input,\n@@ -500,10 +500,10 @@ class GenericArrayLikeDataAdapterTest(DataAdapterTestBase):\n     self.model.evaluate(self.arraylike_input,\n                         self.numpy_target, batch_size=5)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_training_tensor_target(self):\n     self.model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',\n-                       run_eagerly=testing_utils.should_run_eagerly())\n+                       run_eagerly=test_utils.should_run_eagerly())\n     self.model.fit(self.arraylike_input,\n                    self.tensor_target, batch_size=5)\n     self.model.fit(self.arraylike_input,\n@@ -515,7 +515,7 @@ class GenericArrayLikeDataAdapterTest(DataAdapterTestBase):\n     self.model.evaluate(self.arraylike_input,\n                         self.tensor_target, batch_size=5)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_shuffle_correctness(self):\n     num_samples = 100\n     batch_size = 32\n@@ -548,7 +548,7 @@ class GenericArrayLikeDataAdapterTest(DataAdapterTestBase):\n     # Check that each elements appears, and only once.\n     self.assertAllClose(x, np.sort(second_epoch_data))\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_batch_shuffle_correctness(self):\n     num_samples = 100\n     batch_size = 6\n@@ -646,11 +646,11 @@ class DatasetAdapterTest(DataAdapterTestBase):\n     self.assertFalse(self.adapter_cls.can_handle(self.generator_input))\n     self.assertFalse(self.adapter_cls.can_handle(self.sequence_input))\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_training(self):\n     dataset = self.adapter_cls(self.dataset_input).get_dataset()\n     self.model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',\n-                       run_eagerly=testing_utils.should_run_eagerly())\n+                       run_eagerly=test_utils.should_run_eagerly())\n     self.model.fit(dataset)\n \n   def test_size(self):\n@@ -691,18 +691,18 @@ class GeneratorDataAdapterTest(DataAdapterTestBase):\n     self.assertFalse(self.adapter_cls.can_handle(self.text_input))\n     self.assertFalse(self.adapter_cls.can_handle(self.bytes_input))\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_training(self):\n     self.model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',\n-                       run_eagerly=testing_utils.should_run_eagerly())\n+                       run_eagerly=test_utils.should_run_eagerly())\n     self.model.fit(self.generator_input, steps_per_epoch=10)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-  @testing_utils.run_v2_only\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+  @test_utils.run_v2_only\n   @data_utils.dont_use_multiprocessing_pool\n   def test_with_multiprocessing_training(self):\n     self.model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',\n-                       run_eagerly=testing_utils.should_run_eagerly())\n+                       run_eagerly=test_utils.should_run_eagerly())\n     self.model.fit(self.iterator_input, workers=1, use_multiprocessing=True,\n                    max_queue_size=10, steps_per_epoch=10)\n     # Fit twice to ensure there isn't any duplication that prevent the worker\n@@ -734,7 +734,7 @@ class GeneratorDataAdapterTest(DataAdapterTestBase):\n       self.adapter_cls(\n           self.generator_input, sample_weights=self.generator_input)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_not_shuffled(self):\n     def generator():\n       for i in range(10):\n@@ -776,18 +776,18 @@ class KerasSequenceAdapterTest(DataAdapterTestBase):\n     self.assertFalse(self.adapter_cls.can_handle(self.text_input))\n     self.assertFalse(self.adapter_cls.can_handle(self.bytes_input))\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_training(self):\n     self.model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',\n-                       run_eagerly=testing_utils.should_run_eagerly())\n+                       run_eagerly=test_utils.should_run_eagerly())\n     self.model.fit(self.sequence_input)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-  @testing_utils.run_v2_only\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+  @test_utils.run_v2_only\n   @data_utils.dont_use_multiprocessing_pool\n   def test_with_multiprocessing_training(self):\n     self.model.compile(loss='sparse_categorical_crossentropy', optimizer='sgd',\n-                       run_eagerly=testing_utils.should_run_eagerly())\n+                       run_eagerly=test_utils.should_run_eagerly())\n     self.model.fit(self.sequence_input, workers=1, use_multiprocessing=True,\n                    max_queue_size=10, steps_per_epoch=10)\n     # Fit twice to ensure there isn't any duplication that prevent the worker\n@@ -840,7 +840,7 @@ class KerasSequenceAdapterRaggedTest(KerasSequenceAdapterTest):\n     ])\n \n \n-class DataHandlerTest(keras_parameterized.TestCase):\n+class DataHandlerTest(test_combinations.TestCase):\n \n   def test_finite_dataset_with_steps_per_epoch(self):\n     data = tf.data.Dataset.from_tensor_slices([0, 1, 2, 3]).batch(1)\n@@ -1078,7 +1078,7 @@ class DataHandlerTest(keras_parameterized.TestCase):\n         self.assertIsInstance(next(iterator), tf.Tensor)\n \n \n-class TestValidationSplit(keras_parameterized.TestCase):\n+class TestValidationSplit(test_combinations.TestCase):\n \n   @parameterized.named_parameters(('numpy_arrays', True), ('tensors', False))\n   def test_validation_split_unshuffled(self, use_numpy):\n\n@@ -21,8 +21,8 @@ import unittest\n import numpy as np\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n \n try:\n   import h5py  # pylint:disable=g-import-not-at-top\n@@ -30,10 +30,10 @@ except ImportError:\n   h5py = None\n \n \n-@testing_utils.run_v2_only\n-class TestDeferredSequential(keras_parameterized.TestCase):\n+@test_utils.run_v2_only\n+class TestDeferredSequential(test_combinations.TestCase):\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_build_behavior(self):\n     # Test graph network creation after __call__\n     model = get_model()\n@@ -73,7 +73,7 @@ class TestDeferredSequential(keras_parameterized.TestCase):\n         loss='mse',\n         optimizer='rmsprop',\n         metrics=[keras.metrics.CategoricalAccuracy()],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(np.zeros((2, 6)), np.zeros((2, 2)))\n     self.assertLen(model.weights, 4)\n     self.assertTrue(model._is_graph_network)\n@@ -86,7 +86,7 @@ class TestDeferredSequential(keras_parameterized.TestCase):\n     self.assertEqual(model.inputs[0].shape.as_list()[-1], 6)\n     self.assertEqual(model.outputs[0].shape.as_list()[-1], 2)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_add_and_pop(self):\n     model = get_model()\n     model.build((None, 6))\n@@ -105,7 +105,7 @@ class TestDeferredSequential(keras_parameterized.TestCase):\n     self.assertLen(model.layers, 3)\n     self.assertLen(model.weights, 4)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_feature_extraction(self):\n     # This tests layer connectivity reset when rebuilding\n     model = get_model()\n@@ -117,7 +117,7 @@ class TestDeferredSequential(keras_parameterized.TestCase):\n     # Check that inputs and outputs are connected\n     _ = extractor(np.random.random((4, 6)))\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_saving_savedmodel(self):\n     model = get_model()\n     model(np.random.random((3, 6)))  # Build model\n@@ -134,7 +134,7 @@ class TestDeferredSequential(keras_parameterized.TestCase):\n         self.assertAllClose(w1, w2)\n \n   @unittest.skipIf(h5py is None, 'Test requires h5py')\n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_saving_h5(self):\n     path = os.path.join(self.get_temp_dir(), 'model_path.h5')\n     model = get_model()\n@@ -151,7 +151,7 @@ class TestDeferredSequential(keras_parameterized.TestCase):\n       for w1, w2 in zip(layer1.weights, layer2.weights):\n         self.assertAllClose(w1, w2)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_shared_layer(self):\n     # This tests that preexisting layer connectivity is preserved\n     # when auto-building graph networks\n@@ -166,7 +166,7 @@ class TestDeferredSequential(keras_parameterized.TestCase):\n     m2 = keras.Sequential([shared_layer, m1])\n     m2(np.random.random((3, 2)))\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_loss_layer(self):\n     class LossLayer(keras.layers.Layer):\n \n@@ -176,7 +176,7 @@ class TestDeferredSequential(keras_parameterized.TestCase):\n \n     # Test loss layer alone\n     model = keras.Sequential([LossLayer()])\n-    model.compile('rmsprop', run_eagerly=testing_utils.should_run_eagerly())\n+    model.compile('rmsprop', run_eagerly=test_utils.should_run_eagerly())\n     loss = model.train_on_batch(np.ones((2, 2)))\n     self.assertAllClose(loss, 4.)\n     model(np.random.random((4, 2)))  # Triggers a rebuild\n@@ -187,7 +187,7 @@ class TestDeferredSequential(keras_parameterized.TestCase):\n     model = keras.Sequential([\n         keras.layers.Dense(1, kernel_initializer='ones'),\n         LossLayer()])\n-    model.compile('rmsprop', run_eagerly=testing_utils.should_run_eagerly())\n+    model.compile('rmsprop', run_eagerly=test_utils.should_run_eagerly())\n     loss = model.train_on_batch(np.ones((2, 2)))\n     self.assertAllClose(loss, 4.)\n     model(np.random.random((4, 2)))  # Triggers a rebuild\n@@ -199,7 +199,7 @@ class TestDeferredSequential(keras_parameterized.TestCase):\n         keras.layers.Dense(1, kernel_initializer='ones'),\n         LossLayer()])\n     model.compile('rmsprop', 'mse',\n-                  run_eagerly=testing_utils.should_run_eagerly())\n+                  run_eagerly=test_utils.should_run_eagerly())\n     loss = model.train_on_batch(np.ones((2, 2)), np.ones((2, 2)))\n     model(np.random.random((4, 2)))  # Triggers a rebuild\n     loss = model.train_on_batch(np.ones((1, 2)), np.ones((1, 2)))\n\n@@ -19,9 +19,9 @@ import tensorflow.compat.v2 as tf\n import numpy as np\n \n import keras\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras import metrics as metrics_module\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.feature_column import dense_features as df\n from keras.utils import np_utils\n \n@@ -39,12 +39,12 @@ class TestDNNModel(keras.models.Model):\n     return net\n \n \n-class FeatureColumnsIntegrationTest(keras_parameterized.TestCase):\n+class FeatureColumnsIntegrationTest(test_combinations.TestCase):\n   \"\"\"Most Sequential model API tests are covered in `training_test.py`.\n \n   \"\"\"\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_sequential_model(self):\n     columns = [tf.feature_column.numeric_column('a')]\n     model = keras.models.Sequential([\n@@ -56,7 +56,7 @@ class FeatureColumnsIntegrationTest(keras_parameterized.TestCase):\n         optimizer='rmsprop',\n         loss='categorical_crossentropy',\n         metrics=['accuracy'],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x = {'a': np.random.random((10, 1))}\n     y = np.random.randint(20, size=(10, 1))\n@@ -66,7 +66,7 @@ class FeatureColumnsIntegrationTest(keras_parameterized.TestCase):\n     model.evaluate(x, y, batch_size=5)\n     model.predict(x, batch_size=5)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_sequential_model_with_ds_input(self):\n     columns = [tf.feature_column.numeric_column('a')]\n     model = keras.models.Sequential([\n@@ -78,7 +78,7 @@ class FeatureColumnsIntegrationTest(keras_parameterized.TestCase):\n         optimizer='rmsprop',\n         loss='categorical_crossentropy',\n         metrics=['accuracy'],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     y = np.random.randint(20, size=(100, 1))\n     y = np_utils.to_categorical(y, num_classes=20)\n@@ -91,7 +91,7 @@ class FeatureColumnsIntegrationTest(keras_parameterized.TestCase):\n     model.evaluate(ds, steps=1)\n     model.predict(ds, steps=1)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_sequential_model_with_crossed_column(self):\n     feature_columns = []\n     age_buckets = tf.feature_column.bucketized_column(\n@@ -130,7 +130,7 @@ class FeatureColumnsIntegrationTest(keras_parameterized.TestCase):\n     model.evaluate(ds)\n     model.predict(ds)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_subclassed_model_with_feature_columns(self):\n     col_a = tf.feature_column.numeric_column('a')\n     col_b = tf.feature_column.numeric_column('b')\n@@ -141,7 +141,7 @@ class FeatureColumnsIntegrationTest(keras_parameterized.TestCase):\n         optimizer='rmsprop',\n         loss='categorical_crossentropy',\n         metrics=['accuracy'],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x = {'a': np.random.random((10, 1)), 'b': np.random.random((10, 1))}\n     y = np.random.randint(20, size=(10, 1))\n@@ -151,7 +151,7 @@ class FeatureColumnsIntegrationTest(keras_parameterized.TestCase):\n     dnn_model.evaluate(x=x, y=y, batch_size=5)\n     dnn_model.predict(x=x, batch_size=5)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_subclassed_model_with_feature_columns_with_ds_input(self):\n     col_a = tf.feature_column.numeric_column('a')\n     col_b = tf.feature_column.numeric_column('b')\n@@ -162,7 +162,7 @@ class FeatureColumnsIntegrationTest(keras_parameterized.TestCase):\n         optimizer='rmsprop',\n         loss='categorical_crossentropy',\n         metrics=['accuracy'],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     y = np.random.randint(20, size=(100, 1))\n     y = np_utils.to_categorical(y, num_classes=20)\n@@ -176,7 +176,7 @@ class FeatureColumnsIntegrationTest(keras_parameterized.TestCase):\n     dnn_model.predict(ds, steps=1)\n \n   # TODO(kaftan) seems to throw an error when enabled.\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def DISABLED_test_function_model_feature_layer_input(self):\n     col_a = tf.feature_column.numeric_column('a')\n     col_b = tf.feature_column.numeric_column('b')\n@@ -203,7 +203,7 @@ class FeatureColumnsIntegrationTest(keras_parameterized.TestCase):\n     model.fit(*data, epochs=1)\n \n   # TODO(kaftan) seems to throw an error when enabled.\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def DISABLED_test_function_model_multiple_feature_layer_inputs(self):\n     col_a = tf.feature_column.numeric_column('a')\n     col_b = tf.feature_column.numeric_column('b')\n@@ -274,7 +274,7 @@ class FeatureColumnsIntegrationTest(keras_parameterized.TestCase):\n     }, np.arange(10, 100))\n     model.fit(*data_bloated_dict, epochs=1)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_string_input(self):\n     x = {'age': np.random.random((1024, 1)),\n          'cabin': np.array(['a'] * 1024)}\n\n@@ -17,18 +17,17 @@\n import warnings\n \n from keras import backend\n-from keras import combinations\n from keras import initializers\n-from keras import keras_parameterized\n from keras import layers\n from keras import losses\n from keras import models\n-from keras import testing_utils\n from keras.engine import base_layer\n from keras.engine import functional\n from keras.engine import input_layer as input_layer_lib\n from keras.engine import sequential\n from keras.engine import training as training_lib\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.utils import layer_utils\n from keras.utils import tf_utils\n \n@@ -41,7 +40,7 @@ from tensorflow.python.training.tracking.util import Checkpoint\n # pylint: enable=g-direct-tensorflow-import\n \n \n-class NetworkConstructionTest(keras_parameterized.TestCase):\n+class NetworkConstructionTest(test_combinations.TestCase):\n \n   def test_default_model_name(self):\n     inputs = input_layer_lib.Input(shape=(1,))\n@@ -107,7 +106,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n       network.add_update(tf.compat.v1.assign_add(layer.b, x4), inputs=True)\n       self.assertEqual(len(network.updates), 7)\n \n-  @combinations.generate(combinations.combine(mode=['graph']))\n+  @test_combinations.generate(test_combinations.combine(mode=['graph']))\n   def test_get_updates_bn(self):\n     x1 = input_layer_lib.Input(shape=(1,))\n     layer = layers.BatchNormalization()\n@@ -149,7 +148,8 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(ValueError, 'No such layer: dense_c.'):\n       network.get_layer(name='dense_c')\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testTopologicalAttributes(self):\n     # test layer attributes / methods related to cross-layer connectivity.\n     a = input_layer_lib.Input(shape=(32,), name='input_a')\n@@ -210,7 +210,8 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n   def _assertAllIs(self, a, b):\n     self.assertTrue(all(x is y for x, y in zip(a, b)))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testTopologicalAttributesMultiOutputLayer(self):\n \n     class PowersLayer(layers.Layer):\n@@ -227,7 +228,8 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     self.assertEqual(test_layer.input_shape, (None, 32))\n     self.assertEqual(test_layer.output_shape, [(None, 32), (None, 32)])\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testTopologicalAttributesMultiInputLayer(self):\n \n     class AddLayer(layers.Layer):\n@@ -280,7 +282,8 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n       self._assertAllIs(network.non_trainable_weights,\n                         dense.trainable_weights + dense.non_trainable_weights)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_trainable_weights(self):\n     a = layers.Input(shape=(2,))\n     b = layers.Dense(1)(a)\n@@ -401,7 +404,8 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     self.assertEqual(dense.get_output_mask_at(0), None)\n     self.assertEqual(dense.get_output_mask_at(1), None)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_multi_input_layer(self):\n     with self.cached_session():\n       # test multi-input layer\n@@ -546,7 +550,8 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n       fn_outputs = fn([input_a_np, input_b_np])\n       self.assertListEqual([x.shape for x in fn_outputs], [(10, 7), (10, 64)])\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_multi_input_multi_output_recursion(self):\n     with self.cached_session():\n       # test multi-input multi-output\n@@ -615,7 +620,8 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n       json_str = model.to_json()\n       models.model_from_json(json_str)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_invalid_graphs(self):\n     a = layers.Input(shape=(32,), name='input_a')\n     b = layers.Input(shape=(32,), name='input_b')\n@@ -696,7 +702,8 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n       x = layers.Input(tensor=x)\n       layers.Dense(2)(x)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_basic_masking(self):\n     a = layers.Input(shape=(10, 32), name='input_a')\n     b = layers.Masking()(a)\n@@ -761,7 +768,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     loss = model_b.evaluate(x)\n     self.assertEqual(loss, 4.)\n \n-  @combinations.generate(combinations.keras_mode_combinations())\n+  @test_combinations.generate(test_combinations.keras_mode_combinations())\n   def test_layer_sharing_at_heterogenous_depth(self):\n     x_val = np.random.random((10, 5))\n \n@@ -770,7 +777,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     b = layers.Dense(5, name='B')\n     output = a(b(a(b(x))))\n     m = training_lib.Model(x, output)\n-    m.run_eagerly = testing_utils.should_run_eagerly()\n+    m.run_eagerly = test_utils.should_run_eagerly()\n \n     output_val = m.predict(x_val)\n \n@@ -783,7 +790,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     output_val_2 = m2.predict(x_val)\n     self.assertAllClose(output_val, output_val_2, atol=1e-6)\n \n-  @combinations.generate(combinations.keras_mode_combinations())\n+  @test_combinations.generate(test_combinations.keras_mode_combinations())\n   def test_layer_sharing_at_heterogenous_depth_with_concat(self):\n     input_shape = (16, 9, 3)\n     input_layer = input_layer_lib.Input(shape=input_shape)\n@@ -797,7 +804,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     output = layers.concatenate([x1, x2])\n \n     m = training_lib.Model(inputs=input_layer, outputs=output)\n-    m.run_eagerly = testing_utils.should_run_eagerly()\n+    m.run_eagerly = test_utils.should_run_eagerly()\n \n     x_val = np.random.random((10, 16, 9, 3))\n     output_val = m.predict(x_val)\n@@ -831,7 +838,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n         m2.predict_on_batch(tf.zeros([1, 5])),\n         m.predict_on_batch(tf.zeros([1, 5])))\n \n-  @combinations.generate(combinations.keras_mode_combinations())\n+  @test_combinations.generate(test_combinations.keras_mode_combinations())\n   def test_explicit_training_argument(self):\n     a = layers.Input(shape=(2,))\n     b = layers.Dropout(0.5)(a)\n@@ -846,7 +853,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     loss = model.train_on_batch(x, y)\n     self.assertEqual(loss, 0)  # In inference mode, output is equal to input.\n \n@@ -856,7 +863,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     preds = model.predict(x)\n     self.assertEqual(np.min(preds), 0.)  # At least one unit was dropped.\n \n-  @combinations.generate(combinations.keras_mode_combinations())\n+  @test_combinations.generate(test_combinations.keras_mode_combinations())\n   def test_mask_derived_from_keras_layer(self):\n     inputs = input_layer_lib.Input((5, 10))\n     mask = input_layer_lib.Input((5,))\n@@ -865,7 +872,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(\n         x=[np.ones((10, 5, 10)), np.zeros((10, 5))],\n         y=np.zeros((10, 100)),\n@@ -883,7 +890,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(\n         x=[np.ones((10, 5, 10)), np.zeros((10, 5))],\n         y=np.zeros((10, 100)),\n@@ -897,7 +904,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     # Data is not masked, returned values are random.\n     self.assertGreater(history.history['loss'][0], 0.0)\n \n-  @combinations.generate(combinations.keras_mode_combinations())\n+  @test_combinations.generate(test_combinations.keras_mode_combinations())\n   def test_call_arg_derived_from_keras_layer(self):\n \n     class MyAdd(layers.Layer):\n@@ -912,7 +919,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(\n         x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n         y=10 * np.ones((10, 10)),\n@@ -926,7 +933,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(\n         x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n         y=10 * np.ones((10, 10)),\n@@ -934,7 +941,8 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     # Check that second input was correctly added to first.\n     self.assertEqual(history.history['loss'][0], 0.0)\n \n-  @combinations.generate(combinations.keras_mode_combinations(mode='eager'),)\n+  @test_combinations.generate(\n+      test_combinations.keras_mode_combinations(mode='eager'),)\n   def test_only_some_in_first_arg_derived_from_keras_layer_keras_tensors(self):\n     # This functionality is unsupported in v1 graphs\n \n@@ -956,7 +964,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(\n         x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n         y=10 * np.ones((10, 10)),\n@@ -970,7 +978,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(\n         x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n         y=10 * np.ones((10, 10)),\n@@ -978,10 +986,10 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     # Check that second input was correctly added to first.\n     self.assertEqual(history.history['loss'][0], 0.0)\n \n-  @combinations.generate(\n-      combinations.times(\n-          combinations.keras_mode_combinations(),\n-          combinations.combine(share_already_used_layer=[True, False])))\n+  @test_combinations.generate(\n+      test_combinations.times(\n+          test_combinations.keras_mode_combinations(),\n+          test_combinations.combine(share_already_used_layer=[True, False])))\n   def test_call_kwarg_derived_from_keras_layer(self, share_already_used_layer):\n \n     class MaybeAdd(layers.Layer):\n@@ -1015,7 +1023,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(\n         x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n         y=10 * np.ones((10, 10)),\n@@ -1032,7 +1040,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(\n         x=[3 * np.ones((10, 10)), 7 * np.ones((10, 10))],\n         y=10 * np.ones((10, 10)),\n@@ -1040,7 +1048,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     # Check that second input was correctly added to first.\n     self.assertEqual(history.history['loss'][0], 0.0)\n \n-  @combinations.generate(combinations.keras_mode_combinations())\n+  @test_combinations.generate(test_combinations.keras_mode_combinations())\n   def test_call_kwarg_dtype_serialization(self):\n \n     class Double(layers.Layer):\n@@ -1054,7 +1062,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(\n         x=[3 * np.ones((10, 10))],\n         y=6 * np.ones((10, 10)),\n@@ -1070,7 +1078,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(\n         x=[3 * np.ones((10, 10))],\n         y=6 * np.ones((10, 10)),\n@@ -1081,7 +1089,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     # Check the output dtype\n     self.assertEqual(model(tf.ones((3, 10))).dtype, tf.float16)\n \n-  @combinations.generate(combinations.keras_mode_combinations())\n+  @test_combinations.generate(test_combinations.keras_mode_combinations())\n   def test_call_kwarg_nonserializable(self):\n \n     class Double(layers.Layer):\n@@ -1100,7 +1108,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(\n         x=[3 * np.ones((10, 10))],\n         y=6 * np.ones((10, 10)),\n@@ -1111,10 +1119,10 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n         TypeError, 'Layer double was passed non-JSON-serializable arguments.'):\n       model.get_config()\n \n-  @combinations.generate(\n-      combinations.times(\n-          combinations.keras_mode_combinations(),\n-          combinations.combine(share_already_used_layer=[True, False])))\n+  @test_combinations.generate(\n+      test_combinations.times(\n+          test_combinations.keras_mode_combinations(),\n+          test_combinations.combine(share_already_used_layer=[True, False])))\n   def test_call_kwarg_derived_from_keras_layer_and_first_arg_is_constant(\n       self, share_already_used_layer):\n \n@@ -1147,7 +1155,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(\n         x=7 * np.ones((10, 10)),\n         y=10 * np.ones((10, 10)),\n@@ -1164,7 +1172,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(\n         x=7 * np.ones((10, 10)),\n         y=10 * np.ones((10, 10)),\n@@ -1172,7 +1180,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     # Check that second input was correctly added to first.\n     self.assertEqual(history.history['loss'][0], 0.0)\n \n-  @combinations.generate(combinations.keras_mode_combinations())\n+  @test_combinations.generate(test_combinations.keras_mode_combinations())\n   def test_dont_cast_composite_unless_necessary(self):\n     if not tf.executing_eagerly():\n       return  # Creating Keras inputs from a type_spec only supported in eager.\n@@ -1197,12 +1205,12 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     my_spec = MyType.Spec(tf.TensorSpec([5], tf.float32))\n     input1 = input_layer_lib.Input(type_spec=my_spec)\n     model = training_lib.Model([input1], input1)\n-    model.compile(run_eagerly=testing_utils.should_run_eagerly())\n+    model.compile(run_eagerly=test_utils.should_run_eagerly())\n     model(MyType([1., 2., 3., 4., 5.]))  # Does not require cast.\n     with self.assertRaises((ValueError, TypeError)):\n       model(MyType([1, 2, 3, 4, 5]))\n \n-  @combinations.generate(combinations.keras_mode_combinations())\n+  @test_combinations.generate(test_combinations.keras_mode_combinations())\n   def test_composite_call_kwarg_derived_from_keras_layer(self):\n \n     # Create a test layer that accepts composite tensor inputs.\n@@ -1222,7 +1230,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     input_data = [\n         tf.ragged.constant([[3.0, 3.0], [3.0, 3.0], [3.0]]),\n         tf.ragged.constant([[7.0, 7.0], [7.0, 7.0], [7.0]])\n@@ -1238,12 +1246,13 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(x=input_data, y=expected_data)\n     # Check that second input was correctly added to first.\n     self.assertEqual(history.history['loss'][0], 0.0)\n \n-  @combinations.generate(combinations.keras_mode_combinations(mode='eager'))\n+  @test_combinations.generate(\n+      test_combinations.keras_mode_combinations(mode='eager'))\n   def test_call_some_not_all_nested_in_first_arg_derived_from_keras_layer(self):\n     # This functionality is unsupported in v1 graphs\n \n@@ -1274,7 +1283,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(\n         x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],\n         y=15 * np.ones((10, 10)),\n@@ -1287,7 +1296,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(\n         x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],\n         y=15 * np.ones((10, 10)),\n@@ -1295,7 +1304,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     # Check that all inputs were correctly added.\n     self.assertEqual(history.history['loss'][0], 0.0)\n \n-  @combinations.generate(combinations.keras_mode_combinations())\n+  @test_combinations.generate(test_combinations.keras_mode_combinations())\n   def test_call_nested_arg_derived_from_keras_layer(self):\n \n     class AddAll(layers.Layer):\n@@ -1322,7 +1331,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(\n         x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],\n         y=15 * np.ones((10, 10)),\n@@ -1335,7 +1344,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(\n         x=[np.ones((10, 10)), 2 * np.ones((10, 10)), 3 * np.ones((10, 10))],\n         y=15 * np.ones((10, 10)),\n@@ -1343,7 +1352,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     # Check that all inputs were correctly added.\n     self.assertEqual(history.history['loss'][0], 0.0)\n \n-  @combinations.generate(combinations.keras_mode_combinations())\n+  @test_combinations.generate(test_combinations.keras_mode_combinations())\n   def test_multi_output_model_with_none_masking(self):\n     def func(x):\n       return [x * 0.2, x * 0.3]\n@@ -1359,19 +1368,19 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n \n     o = layers.add(o)\n     model = training_lib.Model(i, o)\n-    model.run_eagerly = testing_utils.should_run_eagerly()\n+    model.run_eagerly = test_utils.should_run_eagerly()\n \n     i2 = layers.Input(shape=(3, 2, 1))\n     o2 = model(i2)\n     model2 = training_lib.Model(i2, o2)\n-    model2.run_eagerly = testing_utils.should_run_eagerly()\n+    model2.run_eagerly = test_utils.should_run_eagerly()\n \n     x = np.random.random((4, 3, 2, 1))\n     out = model2.predict(x)\n     assert out.shape == (4, 3, 2, 1)\n     self.assertAllClose(out, x * 0.2 + x * 0.3, atol=1e-4)\n \n-  @combinations.generate(combinations.keras_mode_combinations())\n+  @test_combinations.generate(test_combinations.keras_mode_combinations())\n   def test_constant_initializer_with_numpy(self):\n     initializer = tf.compat.v1.constant_initializer(np.ones((3, 2)))\n     model = sequential.Sequential()\n@@ -1381,7 +1390,7 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n         loss='mse',\n         optimizer='sgd',\n         metrics=['acc'],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     json_str = model.to_json()\n     models.model_from_json(json_str)\n@@ -1396,7 +1405,8 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(RuntimeError, 'forgot to call'):\n       MyNetwork()\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_int_input_shape(self):\n     inputs = input_layer_lib.Input(10)\n     self.assertEqual([None, 10], inputs.shape.as_list())\n@@ -1404,7 +1414,8 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     inputs_with_batch = input_layer_lib.Input(batch_size=20, shape=5)\n     self.assertEqual([20, 5], inputs_with_batch.shape.as_list())\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_model_initialization(self):\n     # Functional model\n     inputs = input_layer_lib.Input(shape=(32,))\n@@ -1447,14 +1458,15 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     self.assertEqual('a', net2.layers[0].name)\n     self.assertEqual('b', net2.layers[1].name)\n \n-  @combinations.generate(combinations.keras_model_type_combinations())\n+  @test_combinations.generate(test_combinations.keras_model_type_combinations())\n   def test_dependency_tracking(self):\n-    model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n+    model = test_utils.get_small_mlp(1, 4, input_dim=3)\n     model.trackable = Checkpoint()\n     self.assertIn('trackable', model._unconditional_dependency_names)\n     self.assertEqual(model.trackable, model._lookup_dependency('trackable'))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_model_construction_in_tf_function(self):\n \n     d = {'model': None}\n@@ -1497,9 +1509,10 @@ class NetworkConstructionTest(keras_parameterized.TestCase):\n     self.assertAllEqual(tf.int32, input_spec['y'].dtype)\n \n \n-class DeferredModeTest(keras_parameterized.TestCase):\n+class DeferredModeTest(test_combinations.TestCase):\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testSimpleNetworkBuilding(self):\n     inputs = input_layer_lib.Input(shape=(32,))\n     if tf.executing_eagerly():\n@@ -1522,7 +1535,8 @@ class DeferredModeTest(keras_parameterized.TestCase):\n       outputs = network(inputs)\n       self.assertEqual(outputs.shape.as_list(), [10, 4])\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testMultiIONetworkBuilding(self):\n     input_a = input_layer_lib.Input(shape=(32,))\n     input_b = input_layer_lib.Input(shape=(16,))\n@@ -1548,14 +1562,15 @@ class DeferredModeTest(keras_parameterized.TestCase):\n       self.assertEqual(outputs[1].shape.as_list(), [10, 2])\n \n \n-class DefaultShapeInferenceBehaviorTest(keras_parameterized.TestCase):\n+class DefaultShapeInferenceBehaviorTest(test_combinations.TestCase):\n \n   def _testShapeInference(self, model, input_shape, expected_output_shape):\n     input_value = np.random.random(input_shape)\n     output_value = model.predict(input_value)\n     self.assertEqual(output_value.shape, expected_output_shape)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testSingleInputCase(self):\n \n     class LayerWithOneInput(layers.Layer):\n@@ -1583,7 +1598,8 @@ class DefaultShapeInferenceBehaviorTest(keras_parameterized.TestCase):\n     model = training_lib.Model(inputs, outputs)\n     self._testShapeInference(model, (2, 3), (2, 4))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testMultiInputOutputCase(self):\n \n     class MultiInputOutputLayer(layers.Layer):\n@@ -1605,7 +1621,8 @@ class DefaultShapeInferenceBehaviorTest(keras_parameterized.TestCase):\n     self.assertEqual(output_a_val.shape, (2, 4))\n     self.assertEqual(output_b_val.shape, (2, 4))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testTrainingArgument(self):\n \n     class LayerWithTrainingArg(layers.Layer):\n@@ -1621,7 +1638,8 @@ class DefaultShapeInferenceBehaviorTest(keras_parameterized.TestCase):\n     model = training_lib.Model(inputs, outputs)\n     self._testShapeInference(model, (2, 3), (2, 4))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testNoneInShape(self):\n \n     class Model(training_lib.Model):\n@@ -1648,7 +1666,8 @@ class DefaultShapeInferenceBehaviorTest(keras_parameterized.TestCase):\n     output = model(sample_input)\n     self.assertEqual(output.shape, (1, 3))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testNoneInShapeWithCompoundModel(self):\n \n     class BasicBlock(training_lib.Model):\n@@ -1685,7 +1704,8 @@ class DefaultShapeInferenceBehaviorTest(keras_parameterized.TestCase):\n     output = model(sample_input)  # pylint: disable=not-callable\n     self.assertEqual(output.shape, (1, 3))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testNoneInShapeWithFunctionalAPI(self):\n \n     class BasicBlock(training_lib.Model):\n@@ -1716,7 +1736,7 @@ class DefaultShapeInferenceBehaviorTest(keras_parameterized.TestCase):\n     output = model(sample_input)\n     self.assertEqual(output.shape, (1, 3))\n \n-  @combinations.generate(combinations.keras_mode_combinations())\n+  @test_combinations.generate(test_combinations.keras_mode_combinations())\n   def test_sequential_as_downstream_of_masking_layer(self):\n     inputs = layers.Input(shape=(3, 4))\n     x = layers.Masking(mask_value=0., input_shape=(3, 4))(inputs)\n@@ -1729,7 +1749,7 @@ class DefaultShapeInferenceBehaviorTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     model_input = np.random.randint(\n         low=1, high=5, size=(10, 3, 4)).astype('float32')\n@@ -1749,7 +1769,8 @@ class DefaultShapeInferenceBehaviorTest(keras_parameterized.TestCase):\n       self.assertAllClose(mask_outputs_val[0], np.any(model_input, axis=-1))\n       self.assertAllClose(mask_outputs_val[1], np.any(model_input, axis=-1))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_external_keras_serialization_compat_input_layers(self):\n     inputs = input_layer_lib.Input(shape=(10,))\n     outputs = layers.Dense(1)(inputs)\n@@ -1761,7 +1782,8 @@ class DefaultShapeInferenceBehaviorTest(keras_parameterized.TestCase):\n     self.assertLen(config['input_layers'], 1)\n     self.assertLen(config['output_layers'], 1)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_external_keras_serialization_compat_inbound_nodes(self):\n     # Check single Tensor input.\n     inputs = input_layer_lib.Input(shape=(10,), name='in')\n@@ -1779,7 +1801,7 @@ class DefaultShapeInferenceBehaviorTest(keras_parameterized.TestCase):\n     self.assertEqual(config['layers'][2]['inbound_nodes'],\n                      [[['in1', 0, 0, {}], ['in2', 0, 0, {}]]])\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def test_dict_inputs_tensors(self):\n     # Note that this test is running with v2 eager only, since the v1\n     # will behave differently wrt to dict input for training.\n@@ -1849,9 +1871,10 @@ class GraphUtilsTest(tf.test.TestCase):\n           tf_utils.get_reachable_from_inputs([x_3]), {x_3, x_5, x_5.op})\n \n \n-class NestedNetworkTest(keras_parameterized.TestCase):\n+class NestedNetworkTest(test_combinations.TestCase):\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_nested_inputs_network(self):\n     inputs = {\n         'x1': input_layer_lib.Input(shape=(1,)),\n@@ -1876,7 +1899,8 @@ class NestedNetworkTest(keras_parameterized.TestCase):\n     })\n     self.assertListEqual(output_shape.as_list(), [None, 1])\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_nested_outputs_network(self):\n     inputs = input_layer_lib.Input(shape=(1,))\n     outputs = {\n@@ -1897,7 +1921,8 @@ class NestedNetworkTest(keras_parameterized.TestCase):\n     self.assertListEqual(output_shape['x+x'].as_list(), [None, 1])\n     self.assertListEqual(output_shape['x*x'].as_list(), [None, 1])\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_nested_network_inside_network(self):\n     inner_inputs = {\n         'x1': input_layer_lib.Input(shape=(1,)),\n@@ -1930,7 +1955,7 @@ class NestedNetworkTest(keras_parameterized.TestCase):\n     output_shape = network.compute_output_shape([(None, 1), (None, 1)])\n     self.assertListEqual(output_shape.as_list(), [None, 1])\n \n-  @combinations.generate(combinations.combine(mode=['graph']))\n+  @test_combinations.generate(test_combinations.combine(mode=['graph']))\n   def test_updates_with_direct_call(self):\n     inputs = input_layer_lib.Input(shape=(10,))\n     x = layers.BatchNormalization()(inputs)\n@@ -1942,7 +1967,8 @@ class NestedNetworkTest(keras_parameterized.TestCase):\n \n     self.assertLen(model.updates, 4)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_dict_mapping_input(self):\n \n     class ReturnFirst(layers.Layer):\n@@ -1968,7 +1994,8 @@ class NestedNetworkTest(keras_parameterized.TestCase):\n     res = reversed_model({'a': a_val, 'b': b_val})\n     self.assertAllClose(self.evaluate(res), self.evaluate(b_val))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_dict_mapping_single_input(self):\n     b = input_layer_lib.Input(shape=(1,), name='b')\n     outputs = b * 2\n@@ -1983,7 +2010,8 @@ class NestedNetworkTest(keras_parameterized.TestCase):\n     # Check that 'b' was used and 'a' was ignored.\n     self.assertEqual(res.shape.as_list(), [1, 1])\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_nested_dict_mapping(self):\n     a = input_layer_lib.Input(shape=(1,), dtype='int32', name='a')\n     b = input_layer_lib.Input(shape=(1,), dtype='int32', name='b')\n@@ -2006,8 +2034,8 @@ class NestedNetworkTest(keras_parameterized.TestCase):\n     self.assertEqual(self.evaluate(res), [1234])\n \n \n-@combinations.generate(combinations.keras_mode_combinations())\n-class AddLossTest(keras_parameterized.TestCase):\n+@test_combinations.generate(test_combinations.keras_mode_combinations())\n+class AddLossTest(test_combinations.TestCase):\n \n   def test_add_loss_outside_call_only_loss(self):\n     inputs = input_layer_lib.Input((10,))\n@@ -2022,13 +2050,13 @@ class AddLossTest(keras_parameterized.TestCase):\n     x = np.ones((10, 10))\n     model.compile(\n         'sgd',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(x, batch_size=2, epochs=1)\n \n     model2 = model.from_config(model.get_config())\n     model2.compile(\n         'sgd',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model2.set_weights(initial_weights)\n     model2.fit(x, batch_size=2, epochs=1)\n \n@@ -2052,14 +2080,14 @@ class AddLossTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(x, y, batch_size=2, epochs=1)\n \n     model2 = model.from_config(model.get_config())\n     model2.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model2.set_weights(initial_weights)\n     model2.fit(x, y, batch_size=2, epochs=1)\n \n@@ -2097,8 +2125,8 @@ class AddLossTest(keras_parameterized.TestCase):\n     model.fit([x, y])\n \n \n-@combinations.generate(combinations.keras_mode_combinations())\n-class WeightAccessTest(keras_parameterized.TestCase):\n+@test_combinations.generate(test_combinations.keras_mode_combinations())\n+class WeightAccessTest(test_combinations.TestCase):\n \n   def test_functional_model(self):\n     inputs = input_layer_lib.Input((10,))\n@@ -2161,17 +2189,17 @@ class WeightAccessTest(keras_parameterized.TestCase):\n     self.assertEqual(len(model.weights), 1)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-class DTypeTest(keras_parameterized.TestCase):\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n+class DTypeTest(test_combinations.TestCase):\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_graph_network_dtype(self):\n     inputs = input_layer_lib.Input((10,))\n     outputs = layers.Dense(10)(inputs)\n     network = functional.Functional(inputs, outputs)\n     self.assertEqual(network.dtype, 'float32')\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_subclassed_network_dtype(self):\n \n     class IdentityNetwork(training_lib.Model):\n@@ -2213,8 +2241,8 @@ class AttrTrackingLayer(base_layer.Layer):\n     return super(AttrTrackingLayer, self).dynamic\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-class CacheCorrectnessTest(keras_parameterized.TestCase):\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n+class CacheCorrectnessTest(test_combinations.TestCase):\n \n   def layer_and_network_test(self):\n     # Top level layer\n@@ -2414,9 +2442,9 @@ class CacheCorrectnessTest(keras_parameterized.TestCase):\n       self.assertAllEqual(network(x), _call(x, None))\n \n \n-class InputsOutputsErrorTest(keras_parameterized.TestCase):\n+class InputsOutputsErrorTest(test_combinations.TestCase):\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_input_error(self):\n     inputs = input_layer_lib.Input((10,))\n     outputs = layers.Dense(10)(inputs)\n@@ -2424,7 +2452,7 @@ class InputsOutputsErrorTest(keras_parameterized.TestCase):\n         TypeError, \"('Keyword argument not understood:', 'input')\"):\n       models.Model(input=inputs, outputs=outputs)\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_output_error(self):\n     inputs = input_layer_lib.Input((10,))\n     outputs = layers.Dense(10)(inputs)\n@@ -2515,7 +2543,7 @@ class SubclassedModel(training_lib.Model):\n     return self._bar\n \n \n-class MultipleInheritanceModelTest(keras_parameterized.TestCase):\n+class MultipleInheritanceModelTest(test_combinations.TestCase):\n \n   def testFunctionalSubclass(self):\n     m = FunctionalSubclassModel()\n\n@@ -17,18 +17,18 @@\n import collections\n import os\n \n-from keras import keras_parameterized\n from keras import layers\n from keras import models\n from keras.engine import functional_utils\n from keras.engine import input_layer as input_layer_lib\n+from keras.testing_infra import test_combinations\n \n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class FunctionalModelSlideTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class FunctionalModelSlideTest(test_combinations.TestCase):\n \n   def test_find_nodes_by_inputs_and_outputs(self):\n     inputs = input_layer_lib.Input((10,))\n\n@@ -17,8 +17,7 @@\n import tensorflow.compat.v2 as tf\n from tensorflow.python.framework import type_spec\n from keras import backend\n-from keras import combinations\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras.engine import functional\n from keras.engine import input_layer as input_layer_lib\n from keras.layers import core\n@@ -105,9 +104,10 @@ type_spec.register_type_spec_from_value_converter(\n     TwoTensors, TwoTensorsSpecNoOneDtype.from_value)\n \n \n-class InputLayerTest(keras_parameterized.TestCase):\n+class InputLayerTest(test_combinations.TestCase):\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testBasicOutputShapeNoBatchSize(self):\n     # Create a Keras Input\n     x = input_layer_lib.Input(shape=(32,), name='input_a')\n@@ -118,7 +118,8 @@ class InputLayerTest(keras_parameterized.TestCase):\n     self.assertAllEqual(model(tf.ones((3, 32))),\n                         tf.ones((3, 32)) * 2.0)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testBasicOutputShapeWithBatchSize(self):\n     # Create a Keras Input\n     x = input_layer_lib.Input(batch_size=6, shape=(32,), name='input_b')\n@@ -129,7 +130,7 @@ class InputLayerTest(keras_parameterized.TestCase):\n     self.assertAllEqual(model(tf.ones(x.shape)),\n                         tf.ones(x.shape) * 2.0)\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def testBasicOutputShapeNoBatchSizeInTFFunction(self):\n     model = None\n     @tf.function\n@@ -147,7 +148,8 @@ class InputLayerTest(keras_parameterized.TestCase):\n     self.assertAllEqual(run_model(tf.ones((10, 8))),\n                         tf.ones((10, 8)) * 2.0)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testInputTensorArg(self):\n     # Create a Keras Input\n     x = input_layer_lib.Input(tensor=tf.zeros((7, 32)))\n@@ -158,7 +160,7 @@ class InputLayerTest(keras_parameterized.TestCase):\n     self.assertAllEqual(model(tf.ones(x.shape)),\n                         tf.ones(x.shape) * 2.0)\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def testInputTensorArgInTFFunction(self):\n     # We use a mutable model container instead of a model python variable,\n     # because python 2.7 does not have `nonlocal`\n@@ -178,7 +180,7 @@ class InputLayerTest(keras_parameterized.TestCase):\n     self.assertAllEqual(run_model(tf.ones((10, 16))),\n                         tf.ones((10, 16)) * 3.0)\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def testCompositeInputTensorArg(self):\n     # Create a Keras Input\n     rt = tf.RaggedTensor.from_row_splits(\n@@ -193,7 +195,7 @@ class InputLayerTest(keras_parameterized.TestCase):\n         values=[3, 21, 4, 1, 53, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])\n     self.assertAllEqual(model(rt), rt * 2)\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def testCompositeInputTensorArgInTFFunction(self):\n     # We use a mutable model container instead of a model python variable,\n     # because python 2.7 does not have `nonlocal`\n@@ -216,7 +218,7 @@ class InputLayerTest(keras_parameterized.TestCase):\n         values=[3, 21, 4, 1, 53, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])\n     self.assertAllEqual(run_model(rt), rt * 3)\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def testNoMixingArgsWithTypeSpecArg(self):\n     with self.assertRaisesRegexp(\n         ValueError, 'all other args except `name` must be None'):\n@@ -244,7 +246,7 @@ class InputLayerTest(keras_parameterized.TestCase):\n           ragged=True,\n           type_spec=tf.TensorSpec((7, 32), tf.float32))\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def testTypeSpecArg(self):\n     # Create a Keras Input\n     x = input_layer_lib.Input(\n@@ -265,7 +267,7 @@ class InputLayerTest(keras_parameterized.TestCase):\n     self.assertAllEqual(model(tf.ones(x.shape)),\n                         tf.ones(x.shape) * 2.0)\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def testTypeSpecArgInTFFunction(self):\n     # We use a mutable model container instead of a model python variable,\n     # because python 2.7 does not have `nonlocal`\n@@ -286,7 +288,7 @@ class InputLayerTest(keras_parameterized.TestCase):\n     self.assertAllEqual(run_model(tf.ones((10, 16))),\n                         tf.ones((10, 16)) * 3.0)\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def testCompositeTypeSpecArg(self):\n     # Create a Keras Input\n     rt = tf.RaggedTensor.from_row_splits(\n@@ -307,7 +309,7 @@ class InputLayerTest(keras_parameterized.TestCase):\n     model = model_config.model_from_json(model.to_json())\n     self.assertAllEqual(model(rt), rt * 2)\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def testCompositeTypeSpecArgInTFFunction(self):\n     # We use a mutable model container instead of a model pysthon variable,\n     # because python 2.7 does not have `nonlocal`\n@@ -330,7 +332,7 @@ class InputLayerTest(keras_parameterized.TestCase):\n         values=[3, 21, 4, 1, 53, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])\n     self.assertAllEqual(run_model(rt), rt * 3)\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def testCompositeTypeSpecArgWithoutDtype(self):\n     for assign_variant_dtype in [False, True]:\n       # Create a Keras Input\n\n@@ -18,9 +18,9 @@\n import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras import layers\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.engine import keras_tensor\n from keras.engine import training\n \n@@ -47,8 +47,8 @@ class CustomTypeSpec2(CustomTypeSpec):\n     return CustomTypeSpec2(new_shape, self.dtype)\n \n \n-@testing_utils.run_v2_only\n-class KerasTensorTest(keras_parameterized.TestCase):\n+@test_utils.run_v2_only\n+class KerasTensorTest(test_combinations.TestCase):\n \n   def test_repr_and_string(self):\n     kt = keras_tensor.KerasTensor(\n\n@@ -14,11 +14,10 @@\n #,============================================================================\n \"\"\"Tests for layer graphs construction & handling.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n-from keras import keras_parameterized\n from keras.engine import base_layer\n from keras.engine import node as node_module\n+from keras.testing_infra import test_combinations\n+import tensorflow.compat.v2 as tf\n \n \n class DummyTensor(tf.__internal__.types.Tensor):\n@@ -35,7 +34,7 @@ class DummyLayer(base_layer.Layer):\n   pass\n \n \n-class NetworkConstructionTest(keras_parameterized.TestCase):\n+class NetworkConstructionTest(test_combinations.TestCase):\n \n   def test_chained_node_construction(self):\n     # test basics\n\n@@ -18,14 +18,14 @@ import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n import numpy as np\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras import layers\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.engine import training\n \n \n-@testing_utils.run_v2_only\n-class RaggedKerasTensorTest(keras_parameterized.TestCase):\n+@test_utils.run_v2_only\n+class RaggedKerasTensorTest(test_combinations.TestCase):\n \n   @parameterized.parameters(\n       {'batch_size': None, 'shape': (None, 5), 'ragged_rank': 1},\n@@ -181,8 +181,8 @@ class RaggedKerasTensorTest(keras_parameterized.TestCase):\n       self.assertAllEqual(a, b)\n \n \n-@testing_utils.run_v2_only\n-class RaggedTensorClassMethodAsLayerTest(keras_parameterized.TestCase):\n+@test_utils.run_v2_only\n+class RaggedTensorClassMethodAsLayerTest(test_combinations.TestCase):\n \n   def test_from_value_rowids(self):\n     inp = layers.Input(shape=[None])\n\n@@ -20,16 +20,16 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from tensorflow.python.framework import test_util\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n \n \n-class TestSequential(keras_parameterized.TestCase):\n+class TestSequential(test_combinations.TestCase):\n   \"\"\"Most Sequential model API tests are covered in `training_test.py`.\n   \"\"\"\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_basic_methods(self):\n     model = keras.models.Sequential()\n     model.add(keras.layers.Dense(1, input_dim=2))\n@@ -40,7 +40,7 @@ class TestSequential(keras_parameterized.TestCase):\n     self.assertEqual(len(model.weights), 2 * 2)\n     self.assertEqual(model.get_layer(name='dp').name, 'dp')\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_input_defined_first_layer(self):\n     model = keras.models.Sequential()\n     model.add(keras.Input(shape=(2,), name='input_layer'))\n@@ -52,24 +52,24 @@ class TestSequential(keras_parameterized.TestCase):\n     self.assertLen(model.weights, 2 * 2)\n     self.assertEqual(model.get_layer(name='dp').name, 'dp')\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_single_layer_in_init(self):\n     model = keras.models.Sequential(keras.layers.Dense(1))\n     self.assertLen(model.layers, 1)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_sequential_pop(self):\n     num_hidden = 5\n     input_dim = 3\n     batch_size = 5\n     num_classes = 2\n \n-    model = testing_utils.get_small_sequential_mlp(\n+    model = test_utils.get_small_sequential_mlp(\n         num_hidden, num_classes, input_dim)\n     model.compile(\n         loss='mse',\n         optimizer='rmsprop',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     x = np.random.random((batch_size, input_dim))\n     y = np.random.random((batch_size, num_classes))\n     model.fit(x, y, epochs=1)\n@@ -79,7 +79,7 @@ class TestSequential(keras_parameterized.TestCase):\n     model.compile(\n         loss='mse',\n         optimizer='rmsprop',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     y = np.random.random((batch_size, num_hidden))\n     model.fit(x, y, epochs=1)\n \n@@ -95,19 +95,19 @@ class TestSequential(keras_parameterized.TestCase):\n     with self.assertRaises(TypeError):\n       model.pop()\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_sequential_deferred_build_with_np_arrays(self):\n     num_hidden = 5\n     input_dim = 3\n     batch_size = 5\n     num_classes = 2\n \n-    model = testing_utils.get_small_sequential_mlp(num_hidden, num_classes)\n+    model = test_utils.get_small_sequential_mlp(num_hidden, num_classes)\n     model.compile(\n         loss='mse',\n         optimizer='rmsprop',\n         metrics=[keras.metrics.CategoricalAccuracy()],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     self.assertEqual(len(model.layers), 2)\n     with self.assertRaisesRegex(\n         ValueError, 'Weights for model .* have not yet been created'):\n@@ -120,7 +120,7 @@ class TestSequential(keras_parameterized.TestCase):\n     self.assertTrue(model.built)\n     self.assertEqual(len(model.weights), 2 * 2)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_sequential_deferred_build_with_dataset_iterators(self):\n     num_hidden = 5\n     input_dim = 3\n@@ -128,12 +128,12 @@ class TestSequential(keras_parameterized.TestCase):\n     num_samples = 50\n     steps_per_epoch = 10\n \n-    model = testing_utils.get_small_sequential_mlp(num_hidden, num_classes)\n+    model = test_utils.get_small_sequential_mlp(num_hidden, num_classes)\n     model.compile(\n         loss='mse',\n         optimizer='rmsprop',\n         metrics=[keras.metrics.CategoricalAccuracy()],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     self.assertEqual(len(model.layers), 2)\n     with self.assertRaisesRegex(\n         ValueError, 'Weights for model .* have not yet been created'):\n@@ -157,9 +157,9 @@ class TestSequential(keras_parameterized.TestCase):\n \n       def get_model():\n         if deferred:\n-          model = testing_utils.get_small_sequential_mlp(10, 4)\n+          model = test_utils.get_small_sequential_mlp(10, 4)\n         else:\n-          model = testing_utils.get_small_sequential_mlp(10, 4, input_dim=3)\n+          model = test_utils.get_small_sequential_mlp(10, 4, input_dim=3)\n         model.compile(\n             optimizer='rmsprop',\n             loss='categorical_crossentropy',\n@@ -194,14 +194,14 @@ class TestSequential(keras_parameterized.TestCase):\n           validation_data=(inputs, targets),\n           validation_steps=2)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_invalid_use_cases(self):\n     # Added objects must be layer instances\n     with self.assertRaises(TypeError):\n       model = keras.models.Sequential()\n       model.add(None)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_nested_sequential_trainability(self):\n     input_dim = 20\n     num_units = 10\n@@ -222,7 +222,7 @@ class TestSequential(keras_parameterized.TestCase):\n     inner_model.trainable = True\n     self.assertEqual(len(model.trainable_weights), 4)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_sequential_update_disabling(self):\n     val_a = np.random.random((10, 4))\n     val_out = np.random.random((10, 4))\n@@ -245,19 +245,19 @@ class TestSequential(keras_parameterized.TestCase):\n     x2 = model.predict(val_a)\n     assert np.abs(np.sum(x1 - x2)) > 1e-5\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_sequential_deferred_build_serialization(self):\n     num_hidden = 5\n     input_dim = 3\n     batch_size = 5\n     num_classes = 2\n \n-    model = testing_utils.get_small_sequential_mlp(num_hidden, num_classes)\n+    model = test_utils.get_small_sequential_mlp(num_hidden, num_classes)\n     model.compile(\n         loss='mse',\n         optimizer='rmsprop',\n         metrics=[keras.metrics.CategoricalAccuracy()],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     self.assertFalse(model.built)\n \n     x = np.random.random((batch_size, input_dim))\n@@ -271,63 +271,63 @@ class TestSequential(keras_parameterized.TestCase):\n         loss='mse',\n         optimizer='rmsprop',\n         metrics=[keras.metrics.CategoricalAccuracy()],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     x = np.random.random((batch_size, input_dim))\n     y = np.random.random((batch_size, num_classes))\n     new_model.train_on_batch(x, y)\n     self.assertEqual(len(new_model.layers), 2)\n     self.assertEqual(len(new_model.weights), 4)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_sequential_shape_inference_deferred(self):\n-    model = testing_utils.get_small_sequential_mlp(4, 5)\n+    model = test_utils.get_small_sequential_mlp(4, 5)\n     output_shape = model.compute_output_shape((None, 7))\n     self.assertEqual(tuple(output_shape.as_list()), (None, 5))\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_sequential_build_deferred(self):\n-    model = testing_utils.get_small_sequential_mlp(4, 5)\n+    model = test_utils.get_small_sequential_mlp(4, 5)\n \n     model.build((None, 10))\n     self.assertTrue(model.built)\n     self.assertEqual(len(model.weights), 4)\n \n     # Test with nested model\n-    model = testing_utils.get_small_sequential_mlp(4, 3)\n-    inner_model = testing_utils.get_small_sequential_mlp(4, 5)\n+    model = test_utils.get_small_sequential_mlp(4, 3)\n+    inner_model = test_utils.get_small_sequential_mlp(4, 5)\n     model.add(inner_model)\n \n     model.build((None, 10))\n     self.assertTrue(model.built)\n     self.assertEqual(len(model.weights), 8)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_sequential_deferred_manual_build(self):\n-    model = testing_utils.get_small_sequential_mlp(4, 5)\n+    model = test_utils.get_small_sequential_mlp(4, 5)\n     self.assertFalse(model.built)\n     model(tf.zeros([1, 2]))\n     self.assertTrue(model.built)\n     model.compile(\n         'rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(np.zeros((1, 2)), np.zeros((1, 5)))\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_sequential_nesting(self):\n-    model = testing_utils.get_small_sequential_mlp(4, 3)\n-    inner_model = testing_utils.get_small_sequential_mlp(4, 5)\n+    model = test_utils.get_small_sequential_mlp(4, 3)\n+    inner_model = test_utils.get_small_sequential_mlp(4, 5)\n     model.add(inner_model)\n \n     model.compile(\n         loss='mse',\n         optimizer='rmsprop',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     x = np.random.random((2, 6))\n     y = np.random.random((2, 5))\n     model.fit(x, y, epochs=1)\n \n-  @test_util.run_v1_only('Behavior changed in V2.')\n+  @tf_test_utils.run_v1_only('Behavior changed in V2.')\n   def test_variable_names_deferred(self):\n     model = keras.models.Sequential([keras.layers.Dense(3)])\n     model.add(keras.layers.Dense(2))\n@@ -342,7 +342,7 @@ class TestSequential(keras_parameterized.TestCase):\n          'sequential/dense_1/kernel:0', 'sequential/dense_1/bias:0'],\n         [v.name for v in model.variables])\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_input_assumptions_propagation(self):\n     model = keras.models.Sequential()\n     model.add(keras.layers.Dense(1))\n@@ -351,17 +351,17 @@ class TestSequential(keras_parameterized.TestCase):\n                                   'expected min_ndim=2, found ndim=0'):\n         model(1.0)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_string_input(self):\n     seq = keras.Sequential([\n         keras.layers.InputLayer(input_shape=(1,), dtype=tf.string),\n         keras.layers.Lambda(lambda x: x[0])\n     ])\n-    seq.run_eagerly = testing_utils.should_run_eagerly()\n+    seq.run_eagerly = test_utils.should_run_eagerly()\n     preds = seq.predict([['tensorflow eager']])\n     self.assertEqual(preds.shape, (1,))\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_multi_output_layer_not_accepted(self):\n \n     class MultiOutputLayer(keras.layers.Layer):\n@@ -384,7 +384,7 @@ class TestSequential(keras_parameterized.TestCase):\n                                 'should have a single output tensor'):\n       keras.Sequential([MultiOutputLayer()])(np.zeros((10, 10)))\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_layer_add_after_compile_deferred(self):\n     model = keras.Sequential([keras.layers.Dense(3)])\n     self.assertFalse(model.built)\n@@ -448,7 +448,7 @@ class TestSequential(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(ValueError, 'should have unique names'):\n       model.add(keras.layers.Dense(3, name='specific_name'))\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_tf_module_call(self):\n \n     class MyModule(tf.Module):\n@@ -466,7 +466,7 @@ class TestSequential(keras_parameterized.TestCase):\n     model.fit(x, y, batch_size=2)\n     self.assertLen(model.trainable_variables, 1)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_tf_module_training(self):\n \n     class MyModule(tf.Module):\n@@ -486,7 +486,7 @@ class TestSequential(keras_parameterized.TestCase):\n     model.fit(x, y, batch_size=2)\n     self.assertLen(model.trainable_variables, 1)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_tf_module_error(self):\n \n     class MyModule(tf.Module):\n@@ -499,9 +499,9 @@ class TestSequential(keras_parameterized.TestCase):\n       model.add(MyModule())\n \n \n-class TestSequentialEagerIntegration(keras_parameterized.TestCase):\n+class TestSequentialEagerIntegration(test_combinations.TestCase):\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_defun_on_call(self):\n     # Check that one can subclass Sequential and place the `call` in a `defun`.\n \n@@ -518,20 +518,20 @@ class TestSequentialEagerIntegration(keras_parameterized.TestCase):\n     model.compile(\n         loss='mse',\n         optimizer='rmsprop',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x = np.random.random((2, 6))\n     y = np.random.random((2, 5))\n     model.fit(x, y, epochs=1)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_build_before_fit(self):\n     # Fix for b/112433577\n-    model = testing_utils.get_small_sequential_mlp(4, 5)\n+    model = test_utils.get_small_sequential_mlp(4, 5)\n     model.compile(\n         loss='mse',\n         optimizer='rmsprop',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     model.build((None, 6))\n \n@@ -539,7 +539,7 @@ class TestSequentialEagerIntegration(keras_parameterized.TestCase):\n     y = np.random.random((2, 5))\n     model.fit(x, y, epochs=1)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_build_empty_network(self):\n     x = np.random.random((2, 6))\n     y = np.random.random((2, 5))\n@@ -554,7 +554,7 @@ class TestSequentialEagerIntegration(keras_parameterized.TestCase):\n     model.compile(\n         loss='mse',\n         optimizer='rmsprop',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(x, y)\n \n     model.pop()\n\n@@ -23,16 +23,16 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from tensorflow.python.framework import test_util\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.layers import core\n from keras.utils import io_utils\n \n \n-@keras_parameterized.run_with_all_model_types\n-@keras_parameterized.run_all_keras_modes\n-class ValidationDatasetNoLimitTest(keras_parameterized.TestCase):\n+@test_combinations.run_with_all_model_types\n+@test_combinations.run_all_keras_modes\n+class ValidationDatasetNoLimitTest(test_combinations.TestCase):\n \n   def create_dataset(self, num_samples, batch_size):\n     input_data = np.random.rand(num_samples, 1)\n@@ -44,7 +44,7 @@ class ValidationDatasetNoLimitTest(keras_parameterized.TestCase):\n   def test_validation_dataset_with_no_step_arg(self):\n     # Create a model that learns y=Mx.\n     layers = [core.Dense(1)]\n-    model = testing_utils.get_model_from_layers(layers, input_shape=(1,))\n+    model = test_utils.get_model_from_layers(layers, input_shape=(1,))\n     model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mean_absolute_error\"])\n \n     train_dataset = self.create_dataset(num_samples=200, batch_size=10)\n@@ -60,10 +60,10 @@ class ValidationDatasetNoLimitTest(keras_parameterized.TestCase):\n                            evaluation[-1], places=5)\n \n \n-class PrintTrainingInfoTest(keras_parameterized.TestCase,\n+class PrintTrainingInfoTest(test_combinations.TestCase,\n                             parameterized.TestCase):\n \n-  @test_util.run_v1_only(\"Only relevant in graph mode.\")\n+  @tf_test_utils.run_v1_only(\"Only relevant in graph mode.\")\n   def test_print_info_with_datasets(self):\n     \"\"\"Print training info should work with val datasets (b/133391839).\"\"\"\n \n@@ -86,7 +86,7 @@ class PrintTrainingInfoTest(keras_parameterized.TestCase,\n \n   @parameterized.named_parameters(\n       (\"with_validation\", True), (\"without_validation\", False))\n-  @test_util.run_v1_only(\"Only relevant in graph mode.\")\n+  @tf_test_utils.run_v1_only(\"Only relevant in graph mode.\")\n   def test_print_info_with_numpy(self, do_validation):\n     \"\"\"Print training info should work with val datasets (b/133391839).\"\"\"\n \n@@ -109,7 +109,7 @@ class PrintTrainingInfoTest(keras_parameterized.TestCase,\n     if do_validation:\n       self.assertIn(\", validate on 50 samples\", mock_stdout.getvalue())\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_dict_float64_input(self):\n \n     class MyModel(keras.Model):\n@@ -131,7 +131,7 @@ class PrintTrainingInfoTest(keras_parameterized.TestCase,\n     model.compile(\n         loss=\"mae\",\n         optimizer=\"adam\",\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     model.fit(\n         x={\n\n@@ -23,9 +23,9 @@ import numpy as np\n \n import keras\n from keras import callbacks\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras import metrics as metrics_module\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.utils import io_utils\n from tensorflow.python.platform import tf_logging as logging\n \n@@ -43,12 +43,12 @@ class BatchCounterCallback(callbacks.Callback):\n     self.batch_end_count += 1\n \n \n-class TestTrainingWithDataset(keras_parameterized.TestCase):\n+class TestTrainingWithDataset(test_combinations.TestCase):\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_calling_model_on_same_dataset(self):\n-    model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n+    model = test_utils.get_small_mlp(1, 4, input_dim=3)\n     optimizer = 'rmsprop'\n     loss = 'mse'\n     metrics = ['mae']\n@@ -56,7 +56,7 @@ class TestTrainingWithDataset(keras_parameterized.TestCase):\n         optimizer,\n         loss,\n         metrics=metrics,\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     inputs = np.zeros((10, 3), np.float32)\n     targets = np.zeros((10, 4), np.float32)\n@@ -80,10 +80,10 @@ class TestTrainingWithDataset(keras_parameterized.TestCase):\n         validation_data=dataset,\n         validation_steps=2)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_training_and_eval_methods_on_dataset(self):\n-    model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n+    model = test_utils.get_small_mlp(1, 4, input_dim=3)\n     optimizer = 'rmsprop'\n     loss = 'mse'\n     metrics = ['mae', metrics_module.CategoricalAccuracy()]\n@@ -91,7 +91,7 @@ class TestTrainingWithDataset(keras_parameterized.TestCase):\n         optimizer,\n         loss,\n         metrics=metrics,\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     inputs = np.zeros((10, 3), np.float32)\n     targets = np.zeros((10, 4), np.float32)\n@@ -146,8 +146,8 @@ class TestTrainingWithDataset(keras_parameterized.TestCase):\n     with self.assertRaises(ValueError):\n       model.predict(dataset, verbose=0)\n \n-  @keras_parameterized.run_with_all_model_types(exclude_models='sequential')\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types(exclude_models='sequential')\n+  @test_combinations.run_all_keras_modes\n   def test_training_and_eval_methods_on_multi_input_output_dataset(self):\n     input_a = keras.layers.Input(shape=(3,), name='input_1')\n     input_b = keras.layers.Input(shape=(3,), name='input_2')\n@@ -156,11 +156,11 @@ class TestTrainingWithDataset(keras_parameterized.TestCase):\n     branch_a = [input_a, dense]\n     branch_b = [input_b, dense, dropout]\n \n-    model = testing_utils.get_multi_io_model(branch_a, branch_b)\n+    model = test_utils.get_multi_io_model(branch_a, branch_b)\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     input_a_np = np.random.random((10, 3)).astype(dtype=np.float32)\n     input_b_np = np.random.random((10, 3)).astype(dtype=np.float32)\n@@ -178,7 +178,7 @@ class TestTrainingWithDataset(keras_parameterized.TestCase):\n \n     # Test with dict\n     input_dict = {'input_1': input_a_np, 'input_2': input_b_np}\n-    if testing_utils.get_model_type() == 'subclass':\n+    if test_utils.get_model_type() == 'subclass':\n       output_dict = {'output_1': output_d_np, 'output_2': output_e_np}\n     else:\n       output_dict = {'dense': output_d_np, 'dropout': output_e_np}\n@@ -196,10 +196,10 @@ class TestTrainingWithDataset(keras_parameterized.TestCase):\n     predict_dataset_dict = predict_dataset_dict.batch(10)\n     model.predict(predict_dataset_dict, steps=1)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_dataset_with_sample_weights(self):\n-    model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n+    model = test_utils.get_small_mlp(1, 4, input_dim=3)\n     optimizer = 'rmsprop'\n     loss = 'mse'\n     metrics = ['mae', metrics_module.CategoricalAccuracy()]\n@@ -207,7 +207,7 @@ class TestTrainingWithDataset(keras_parameterized.TestCase):\n         optimizer,\n         loss,\n         metrics=metrics,\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     inputs = np.zeros((10, 3), np.float32)\n     targets = np.zeros((10, 4), np.float32)\n@@ -221,8 +221,8 @@ class TestTrainingWithDataset(keras_parameterized.TestCase):\n     model.evaluate(dataset, steps=2, verbose=1)\n     model.predict(dataset, steps=2)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_dataset_with_sample_weights_correctness(self):\n     x = keras.layers.Input(shape=(1,), name='input')\n     y = keras.layers.Dense(\n@@ -246,15 +246,15 @@ class TestTrainingWithDataset(keras_parameterized.TestCase):\n     #  equals 42.5 / 4 = 10.625\n     self.assertEqual(result, 10.625)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_dataset_with_sparse_labels(self):\n-    model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n+    model = test_utils.get_small_mlp(1, 4, input_dim=3)\n     optimizer = 'rmsprop'\n     model.compile(\n         optimizer,\n         loss='sparse_categorical_crossentropy',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     inputs = np.zeros((10, 3), dtype=np.float32)\n     targets = np.random.randint(0, 4, size=10, dtype=np.int32)\n@@ -264,7 +264,7 @@ class TestTrainingWithDataset(keras_parameterized.TestCase):\n \n     model.fit(dataset, epochs=1, steps_per_epoch=2, verbose=1)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_dataset_fit_correctness(self):\n \n     class SumLayer(keras.layers.Layer):\n@@ -277,7 +277,7 @@ class TestTrainingWithDataset(keras_parameterized.TestCase):\n \n     model = keras.Sequential([SumLayer(input_shape=(2,))])\n     model.compile(\n-        'rmsprop', loss='mae', run_eagerly=testing_utils.should_run_eagerly())\n+        'rmsprop', loss='mae', run_eagerly=test_utils.should_run_eagerly())\n \n     inputs = np.zeros((40, 2), dtype=np.float32)\n     inputs[10:20, :] = 2\n@@ -319,7 +319,7 @@ class TestTrainingWithDataset(keras_parameterized.TestCase):\n \n   def test_dataset_input_shape_validation(self):\n     with tf.compat.v1.get_default_graph().as_default(), self.cached_session():\n-      model = testing_utils.get_small_functional_mlp(1, 4, input_dim=3)\n+      model = test_utils.get_small_functional_mlp(1, 4, input_dim=3)\n       model.compile(optimizer='rmsprop', loss='mse')\n \n       # User forgets to batch the dataset\n@@ -345,12 +345,12 @@ class TestTrainingWithDataset(keras_parameterized.TestCase):\n                                   r'expected (.*?) to have shape \\(3,\\)'):\n         model.train_on_batch(dataset)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_finite_dataset_known_cardinality_no_steps_arg(self):\n-    model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n+    model = test_utils.get_small_mlp(1, 4, input_dim=3)\n     model.compile(\n-        'rmsprop', 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+        'rmsprop', 'mse', run_eagerly=test_utils.should_run_eagerly())\n \n     inputs = np.zeros((100, 3), dtype=np.float32)\n     targets = np.random.randint(0, 4, size=100, dtype=np.int32)\n@@ -366,12 +366,12 @@ class TestTrainingWithDataset(keras_parameterized.TestCase):\n     out = model.predict(dataset)\n     self.assertEqual(out.shape[0], 100)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_finite_dataset_unknown_cardinality_no_steps_arg(self):\n-    model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n+    model = test_utils.get_small_mlp(1, 4, input_dim=3)\n     model.compile(\n-        'rmsprop', 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+        'rmsprop', 'mse', run_eagerly=test_utils.should_run_eagerly())\n \n     inputs = np.zeros((100, 3), dtype=np.float32)\n     targets = np.random.randint(0, 4, size=100, dtype=np.int32)\n@@ -390,8 +390,8 @@ class TestTrainingWithDataset(keras_parameterized.TestCase):\n     out = model.predict(dataset)\n     self.assertEqual(out.shape[0], 100)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_finite_dataset_unknown_cardinality_no_step_with_train_and_val(self):\n \n     class CaptureStdout:\n@@ -407,9 +407,9 @@ class TestTrainingWithDataset(keras_parameterized.TestCase):\n         self.output = self._stringio.getvalue()\n         sys.stdout = self._stdout\n \n-    model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n+    model = test_utils.get_small_mlp(1, 4, input_dim=3)\n     model.compile(\n-        'rmsprop', 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+        'rmsprop', 'mse', run_eagerly=test_utils.should_run_eagerly())\n \n     inputs = np.zeros((100, 3), dtype=np.float32)\n     targets = np.random.randint(0, 4, size=100, dtype=np.int32)\n@@ -439,12 +439,12 @@ class TestTrainingWithDataset(keras_parameterized.TestCase):\n     out = model.predict(dataset)\n     self.assertEqual(out.shape[0], 100)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_finite_dataset_unknown_cardinality_out_of_data(self):\n-    model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n+    model = test_utils.get_small_mlp(1, 4, input_dim=3)\n     model.compile(\n-        'rmsprop', 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+        'rmsprop', 'mse', run_eagerly=test_utils.should_run_eagerly())\n \n     inputs = np.zeros((100, 3), dtype=np.float32)\n     targets = np.random.randint(0, 4, size=100, dtype=np.int32)\n@@ -478,7 +478,7 @@ class TestTrainingWithDataset(keras_parameterized.TestCase):\n     out = model.predict(dataset)\n     self.assertEqual(out.shape[0], 100)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_with_external_loss(self):\n     inp = keras.Input(shape=(4,), name='inp1')\n     out = keras.layers.Dense(2)(inp)\n@@ -491,14 +491,14 @@ class TestTrainingWithDataset(keras_parameterized.TestCase):\n     dataset = tf.data.Dataset.from_tensor_slices(x).repeat(10).batch(10)\n     model.fit(dataset)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_train_eval_with_steps(self):\n     # See b/142880049 for more details.\n     inp = keras.Input(shape=(4,), name='inp1')\n     out = keras.layers.Dense(2)(inp)\n     model = keras.Model(inp, out)\n     model.compile(\n-        'rmsprop', loss='mse', run_eagerly=testing_utils.should_run_eagerly())\n+        'rmsprop', loss='mse', run_eagerly=test_utils.should_run_eagerly())\n \n     inputs = np.zeros((100, 4), dtype=np.float32)\n     targets = np.random.randint(0, 2, size=100, dtype=np.int32)\n@@ -530,10 +530,10 @@ class TestTrainingWithDataset(keras_parameterized.TestCase):\n     self.assertEqual(batch_counter.batch_end_count, 100)\n \n \n-class TestMetricsWithDatasets(keras_parameterized.TestCase):\n+class TestMetricsWithDatasets(test_combinations.TestCase):\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_metrics_correctness_with_dataset(self):\n     layers = [\n         keras.layers.Dense(\n@@ -541,13 +541,13 @@ class TestMetricsWithDatasets(keras_parameterized.TestCase):\n         keras.layers.Dense(1, activation='sigmoid', kernel_initializer='ones')\n     ]\n \n-    model = testing_utils.get_model_from_layers(layers, (4,))\n+    model = test_utils.get_model_from_layers(layers, (4,))\n \n     model.compile(\n         loss='binary_crossentropy',\n         metrics=['accuracy', metrics_module.BinaryAccuracy()],\n         optimizer='rmsprop',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     np.random.seed(123)\n     x = np.random.randint(10, size=(100, 4)).astype(np.float32)\n\n@@ -20,15 +20,15 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras import metrics as metrics_module\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.optimizers.optimizer_v2 import rmsprop\n \n \n-class TrainingTest(keras_parameterized.TestCase):\n+class TrainingTest(test_combinations.TestCase):\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_dynamic_model_has_trainable_weights(self):\n     if not tf.executing_eagerly():\n       # Only test Eager modes, as Graph mode is not relevant for dynamic models.\n@@ -56,8 +56,8 @@ class TrainingTest(keras_parameterized.TestCase):\n     # account during tracking.\n     self.assertLess(loss, 1)\n \n-  @keras_parameterized.run_with_all_model_types(exclude_models='sequential')\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types(exclude_models='sequential')\n+  @test_combinations.run_all_keras_modes\n   def test_model_methods_with_eager_tensors_multi_io(self):\n     if not tf.executing_eagerly():\n       # Only test V2 Function and V2 Eager modes, as V1 Graph mode with\n@@ -70,7 +70,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     dense = keras.layers.Dense(4, name='dense')\n     dropout = keras.layers.Dropout(0.5, name='dropout')\n \n-    model = testing_utils.get_multi_io_model(\n+    model = test_utils.get_multi_io_model(\n         [input_a, dense], [input_b, dense, dropout])\n \n     optimizer = rmsprop.RMSprop(learning_rate=0.001)\n@@ -82,7 +82,7 @@ class TrainingTest(keras_parameterized.TestCase):\n         loss,\n         metrics=metrics,\n         loss_weights=loss_weights,\n-        run_eagerly=testing_utils.should_run_eagerly(),\n+        run_eagerly=test_utils.should_run_eagerly(),\n         sample_weight_mode=None)\n \n     input_a = tf.zeros(shape=(10, 3))\n@@ -135,15 +135,15 @@ class TrainingTest(keras_parameterized.TestCase):\n                    batch_size=2, verbose=0)\n     model.test_on_batch([input_a, input_b], [target_a, target_b])\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_model_methods_with_eager_tensors_single_io(self):\n     if not tf.executing_eagerly():\n       # Only test V2 Function and V2 Eager modes, as V1 Graph mode with\n       # symbolic tensors has different requirements.\n       return\n \n-    model = testing_utils.get_small_mlp(10, 4, 3)\n+    model = test_utils.get_small_mlp(10, 4, 3)\n \n     optimizer = rmsprop.RMSprop(learning_rate=0.001)\n     loss = 'mse'\n@@ -152,7 +152,7 @@ class TrainingTest(keras_parameterized.TestCase):\n         optimizer,\n         loss,\n         metrics=metrics,\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     inputs = tf.zeros(shape=(10, 3))\n     targets = tf.zeros(shape=(10, 4))\n@@ -166,9 +166,9 @@ class TrainingTest(keras_parameterized.TestCase):\n     model.train_on_batch(inputs, targets)\n     model.test_on_batch(inputs, targets)\n \n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_with_all_model_types\n   def test_model_fit_and_validation_with_missing_arg_errors(self):\n-    model = testing_utils.get_small_mlp(10, 4, 3)\n+    model = test_utils.get_small_mlp(10, 4, 3)\n     model.compile(optimizer=rmsprop.RMSprop(learning_rate=0.001),\n                   loss='mse',\n                   run_eagerly=True)\n@@ -191,9 +191,9 @@ class TrainingTest(keras_parameterized.TestCase):\n \n   # TODO(b/120931266): Enable test on subclassed models after bug causing an\n   # extra dimension to be added to predict outputs is fixed.\n-  @keras_parameterized.run_with_all_model_types(exclude_models='subclass')\n+  @test_combinations.run_with_all_model_types(exclude_models='subclass')\n   def test_generator_methods(self):\n-    model = testing_utils.get_small_mlp(10, 4, 3)\n+    model = test_utils.get_small_mlp(10, 4, 3)\n     optimizer = rmsprop.RMSprop(learning_rate=0.001)\n     model.compile(\n         optimizer,\n@@ -219,10 +219,10 @@ class TrainingTest(keras_parameterized.TestCase):\n     self.assertEqual(out.shape, (30, 4))\n \n \n-class CorrectnessTest(keras_parameterized.TestCase):\n+class CorrectnessTest(test_combinations.TestCase):\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   @parameterized.named_parameters([\n       ('', dict()),\n       ('_clipvalue_inf', {'clipvalue': 999999}),\n@@ -235,19 +235,19 @@ class CorrectnessTest(keras_parameterized.TestCase):\n         keras.layers.Dense(3, activation='relu',\n                            kernel_initializer='ones'),\n         keras.layers.Dense(2, activation='softmax', kernel_initializer='ones')]\n-    model = testing_utils.get_model_from_layers(layers, input_shape=(4,))\n+    model = test_utils.get_model_from_layers(layers, input_shape=(4,))\n     model.compile(\n         loss='sparse_categorical_crossentropy',\n         optimizer=rmsprop.RMSprop(learning_rate=0.001, **optimizer_kwargs),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     x = np.ones((100, 4))\n     np.random.seed(123)\n     y = np.random.randint(0, 1, size=(100, 1))\n     history = model.fit(x, y, epochs=1, batch_size=10)\n     self.assertAlmostEqual(history.history['loss'][-1], 0.5836, 4)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_loss_correctness_clipvalue_zero(self):\n     # Test that training loss is the same in eager and graph\n     # (by comparing it to a reference value in a deterministic case)\n@@ -256,11 +256,11 @@ class CorrectnessTest(keras_parameterized.TestCase):\n         keras.layers.Dense(3, activation='relu',\n                            kernel_initializer='ones'),\n         keras.layers.Dense(2, activation='softmax', kernel_initializer='ones')]\n-    model = testing_utils.get_model_from_layers(layers, input_shape=(4,))\n+    model = test_utils.get_model_from_layers(layers, input_shape=(4,))\n     model.compile(\n         loss='sparse_categorical_crossentropy',\n         optimizer=rmsprop.RMSprop(learning_rate=0.001, clipvalue=0.0),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     x = np.ones((100, 4))\n     np.random.seed(123)\n     y = np.random.randint(0, 1, size=(100, 1))\n@@ -269,8 +269,8 @@ class CorrectnessTest(keras_parameterized.TestCase):\n     self.assertAlmostEqual(history.history['loss'][-2], 0.6931, 4)\n     self.assertAlmostEqual(history.history['loss'][-1], 0.6931, 4)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_loss_correctness_with_iterator(self):\n     # Test that training loss is the same in eager and graph\n     # (by comparing it to a reference value in a deterministic case)\n@@ -278,11 +278,11 @@ class CorrectnessTest(keras_parameterized.TestCase):\n         keras.layers.Dense(3, activation='relu',\n                            kernel_initializer='ones'),\n         keras.layers.Dense(2, activation='softmax', kernel_initializer='ones')]\n-    model = testing_utils.get_model_from_layers(layers, input_shape=(4,))\n+    model = test_utils.get_model_from_layers(layers, input_shape=(4,))\n     model.compile(\n         loss='sparse_categorical_crossentropy',\n         optimizer=rmsprop.RMSprop(learning_rate=0.001),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     x = np.ones((100, 4), dtype=np.float32)\n     np.random.seed(123)\n     y = np.random.randint(0, 1, size=(100, 1))\n\n@@ -20,12 +20,11 @@ import itertools\n \n from absl.testing import parameterized\n import numpy as np\n-from keras import combinations\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras import layers as layers_module\n from keras import losses\n from keras import metrics as metrics_module\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.engine import input_layer\n from keras.engine import training\n from keras.engine import training_generator_v1\n@@ -84,13 +83,13 @@ def custom_generator_changing_batch_size(mode=2):\n custom_generator_threads = data_utils.threadsafe_generator(custom_generator)\n \n \n-class TestGeneratorMethods(keras_parameterized.TestCase):\n+class TestGeneratorMethods(test_combinations.TestCase):\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   @data_utils.dont_use_multiprocessing_pool\n   def test_fit_generator_method(self):\n-    model = testing_utils.get_small_mlp(\n+    model = test_utils.get_small_mlp(\n         num_hidden=3, num_classes=4, input_dim=2)\n     model.compile(\n         loss='mse',\n@@ -124,17 +123,17 @@ class TestGeneratorMethods(keras_parameterized.TestCase):\n                         validation_steps=1,\n                         workers=0)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   @data_utils.dont_use_multiprocessing_pool\n   def test_evaluate_generator_method(self):\n-    model = testing_utils.get_small_mlp(\n+    model = test_utils.get_small_mlp(\n         num_hidden=3, num_classes=4, input_dim=2)\n     model.compile(\n         loss='mse',\n         optimizer=rmsprop.RMSprop(1e-3),\n         metrics=['mae', metrics_module.CategoricalAccuracy()],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     model.evaluate_generator(custom_generator_threads(),\n                              steps=5,\n@@ -152,13 +151,13 @@ class TestGeneratorMethods(keras_parameterized.TestCase):\n                              use_multiprocessing=False,\n                              workers=0)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   @data_utils.dont_use_multiprocessing_pool\n   def test_predict_generator_method(self):\n-    model = testing_utils.get_small_mlp(\n+    model = test_utils.get_small_mlp(\n         num_hidden=3, num_classes=4, input_dim=2)\n-    model.run_eagerly = testing_utils.should_run_eagerly()\n+    model.run_eagerly = test_utils.should_run_eagerly()\n \n     model.predict_generator(custom_generator_threads(),\n                             steps=5,\n@@ -188,16 +187,16 @@ class TestGeneratorMethods(keras_parameterized.TestCase):\n                             max_queue_size=10,\n                             workers=0)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_generator_methods_with_sample_weights(self):\n-    model = testing_utils.get_small_mlp(\n+    model = test_utils.get_small_mlp(\n         num_hidden=3, num_classes=4, input_dim=2)\n     model.compile(\n         loss='mse',\n         optimizer=rmsprop.RMSprop(1e-3),\n         metrics=['mae', metrics_module.CategoricalAccuracy()],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     model.fit_generator(custom_generator(mode=3),\n                         steps_per_epoch=5,\n@@ -222,19 +221,19 @@ class TestGeneratorMethods(keras_parameterized.TestCase):\n                              max_queue_size=10,\n                              use_multiprocessing=False)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_generator_methods_invalid_use_case(self):\n     def invalid_generator():\n       while 1:\n         yield (0, 0, 0, 0)\n \n-    model = testing_utils.get_small_mlp(\n+    model = test_utils.get_small_mlp(\n         num_hidden=3, num_classes=4, input_dim=2)\n     model.compile(\n         loss='mse',\n         optimizer=rmsprop.RMSprop(1e-3),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     with self.assertRaises(ValueError):\n       model.fit_generator(invalid_generator(),\n@@ -263,8 +262,8 @@ class TestGeneratorMethods(keras_parameterized.TestCase):\n                                max_queue_size=10,\n                                use_multiprocessing=False)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_generator_input_to_fit_eval_predict(self):\n     val_data = np.ones([10, 10], np.float32), np.ones([10, 1], np.float32)\n \n@@ -272,13 +271,13 @@ class TestGeneratorMethods(keras_parameterized.TestCase):\n       while True:\n         yield np.ones([10, 10], np.float32), np.ones([10, 1], np.float32)\n \n-    model = testing_utils.get_small_mlp(\n+    model = test_utils.get_small_mlp(\n         num_hidden=10, num_classes=1, input_dim=10)\n \n     model.compile(\n         rmsprop.RMSprop(0.001),\n         'binary_crossentropy',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(\n         ones_generator(),\n         steps_per_epoch=2,\n@@ -288,7 +287,7 @@ class TestGeneratorMethods(keras_parameterized.TestCase):\n     model.predict(ones_generator(), steps=2)\n \n     # Test with a changing batch size\n-    model = testing_utils.get_small_mlp(\n+    model = test_utils.get_small_mlp(\n         num_hidden=3, num_classes=4, input_dim=2)\n     model.compile(\n         loss='mse',\n@@ -318,8 +317,8 @@ class TestGeneratorMethods(keras_parameterized.TestCase):\n     model.evaluate(custom_generator_changing_batch_size(), steps=5)\n     model.predict(custom_generator_changing_batch_size(), steps=5)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   @data_utils.dont_use_multiprocessing_pool\n   def test_generator_dynamic_shapes(self):\n \n@@ -359,21 +358,20 @@ class TestGeneratorMethods(keras_parameterized.TestCase):\n         # Last partial batch\n         yield pack_and_pad(queue)\n \n-    model = testing_utils.get_model_from_layers([\n+    model = test_utils.get_model_from_layers([\n         layers_module.Embedding(input_dim=len(vocab) + 1, output_dim=4),\n         layers_module.SimpleRNN(units=1),\n         layers_module.Activation('sigmoid')\n-    ],\n-                                                input_shape=(None,))\n+    ], input_shape=(None,))\n \n     model.compile(loss=losses.binary_crossentropy, optimizer='sgd')\n     model.fit(data_gen(), epochs=1, steps_per_epoch=5)\n \n \n-class TestGeneratorMethodsWithSequences(keras_parameterized.TestCase):\n+class TestGeneratorMethodsWithSequences(test_combinations.TestCase):\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   @data_utils.dont_use_multiprocessing_pool\n   def test_training_with_sequences(self):\n \n@@ -385,7 +383,7 @@ class TestGeneratorMethodsWithSequences(keras_parameterized.TestCase):\n       def __len__(self):\n         return 10\n \n-    model = testing_utils.get_small_mlp(\n+    model = test_utils.get_small_mlp(\n         num_hidden=3, num_classes=4, input_dim=2)\n     model.compile(loss='mse', optimizer=rmsprop.RMSprop(1e-3))\n \n@@ -404,8 +402,8 @@ class TestGeneratorMethodsWithSequences(keras_parameterized.TestCase):\n                         workers=0,\n                         use_multiprocessing=False)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   @data_utils.dont_use_multiprocessing_pool\n   def test_sequence_input_to_fit_eval_predict(self):\n     val_data = np.ones([10, 10], np.float32), np.ones([10, 1], np.float32)\n@@ -428,7 +426,7 @@ class TestGeneratorMethodsWithSequences(keras_parameterized.TestCase):\n       def __len__(self):\n         return 2\n \n-    model = testing_utils.get_small_mlp(\n+    model = test_utils.get_small_mlp(\n         num_hidden=10, num_classes=1, input_dim=10)\n \n     model.compile(rmsprop.RMSprop(0.001), 'binary_crossentropy')\n@@ -449,7 +447,7 @@ class TestGeneratorMethodsWithSequences(keras_parameterized.TestCase):\n     model.evaluate(CustomSequenceChangingBatchSize())\n     model.predict(CustomSequenceChangingBatchSize())\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_sequence_on_epoch_end(self):\n \n     class MySequence(data_utils.Sequence):\n@@ -475,7 +473,7 @@ class TestGeneratorMethodsWithSequences(keras_parameterized.TestCase):\n     self.assertEqual(my_seq.epochs, 2)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class TestConvertToGeneratorLike(tf.test.TestCase, parameterized.TestCase):\n   simple_inputs = (np.ones((10, 10)), np.ones((10, 1)))\n   nested_inputs = ((np.ones((10, 10)), np.ones((10, 20))), (np.ones((10, 1)),\n\n@@ -19,8 +19,8 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n import numpy as np\n from keras import backend\n-from keras import combinations\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.engine import input_layer\n from keras.engine import training\n from keras.layers.convolutional import Conv2D\n@@ -28,7 +28,8 @@ from keras.layers.convolutional import Conv2D\n \n class TrainingGPUTest(tf.test.TestCase, parameterized.TestCase):\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_model_with_crossentropy_losses_channels_first(self):\n     \"\"\"Tests use of all crossentropy losses with `channels_first`.\n \n@@ -68,7 +69,7 @@ class TrainingGPUTest(tf.test.TestCase, parameterized.TestCase):\n       return simple_model\n \n     if tf.test.is_gpu_available(cuda_only=True):\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         losses_to_test = ['sparse_categorical_crossentropy',\n                           'categorical_crossentropy', 'binary_crossentropy']\n \n\n@@ -23,8 +23,8 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n \n \n def _conv2d_filter(**kwargs):\n@@ -112,11 +112,11 @@ def _gather_test_cases():\n OUTPUT_TEST_CASES = _gather_test_cases()\n \n \n-class CoreLayerIntegrationTest(keras_parameterized.TestCase):\n+class CoreLayerIntegrationTest(test_combinations.TestCase):\n   \"\"\"Test that layers and models produce the correct tensor types.\"\"\"\n \n   # In v1 graph there are only symbolic tensors.\n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   @parameterized.named_parameters(*OUTPUT_TEST_CASES)\n   def test_layer_output_type(self, layer_to_test, input_shape, _, layer_kwargs):\n     layer = layer_to_test(**layer_kwargs)\n@@ -142,7 +142,7 @@ class CoreLayerIntegrationTest(keras_parameterized.TestCase):\n   def _run_fit_eval_predict(self, layer_to_test, input_shape, data_shape,\n                             layer_kwargs):\n     batch_size = 2\n-    run_eagerly = testing_utils.should_run_eagerly()\n+    run_eagerly = test_utils.should_run_eagerly()\n \n     def map_fn(_):\n       x = keras.backend.random_uniform(shape=data_shape)\n@@ -177,7 +177,7 @@ class CoreLayerIntegrationTest(keras_parameterized.TestCase):\n     pred_dataset = pred_dataset.map(pred_map_fn).batch(batch_size)\n     model.predict(pred_dataset, verbose=2)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=False)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=False)\n   @parameterized.named_parameters(*OUTPUT_TEST_CASES)\n   def test_model_loops(self, layer_to_test, input_shape, fuzz_dims,\n                        layer_kwargs):\n\n@@ -23,12 +23,9 @@ import tempfile\n from absl.testing import parameterized\n import keras\n from keras import backend\n-from keras import combinations\n-from keras import keras_parameterized\n from keras import layers as layers_module\n from keras import losses\n from keras import metrics as metrics_module\n-from keras import testing_utils\n from keras.callbacks import Callback\n from keras.engine import input_layer\n from keras.engine import sequential\n@@ -37,13 +34,15 @@ from keras.engine import training_utils_v1\n from keras.layers.preprocessing import string_lookup\n from keras.optimizers import optimizer_v2\n from keras.optimizers.optimizer_experimental import sgd as sgd_experimental\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.utils import data_utils\n from keras.utils import io_utils\n from keras.utils import np_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n-from tensorflow.python.framework import test_util as tf_test_util\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.training.rmsprop import RMSPropOptimizer\n \n@@ -53,23 +52,23 @@ except ImportError:\n   scipy_sparse = None\n \n \n-class TrainingTest(keras_parameterized.TestCase):\n+class TrainingTest(test_combinations.TestCase):\n \n-  @keras_parameterized.run_all_keras_modes\n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n   def test_model_instrumentation(self):\n     layers = [\n         layers_module.Dense(10, dtype=np.float64),\n         layers_module.Dense(10, dtype=np.float64)\n     ]\n-    model = testing_utils.get_model_from_layers(layers, input_shape=(1,))\n+    model = test_utils.get_model_from_layers(layers, input_shape=(1,))\n \n     self.assertTrue(model._instrumented_keras_api)\n     self.assertTrue(model._instrumented_keras_model_class)\n     self.assertFalse(model._instrumented_keras_layer_class)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_fit_training_arg(self):\n \n     class ReturnTraining(layers_module.Layer):\n@@ -84,19 +83,19 @@ class TrainingTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     hist = model.fit(x=np.array([0.]), y=np.array([0.]))\n     self.assertAllClose(hist.history['loss'][0], 10000)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_fit_on_empty(self):\n     model = sequential.Sequential([layers_module.Dense(1)])\n-    model.compile('sgd', 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+    model.compile('sgd', 'mse', run_eagerly=test_utils.should_run_eagerly())\n     with self.assertRaisesRegex(ValueError,\n                                 'Unexpected result of `train_function`.*'):\n       model.fit(x=np.array([]), y=np.array([]))\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_compile_fit_with_jit_compile(self):\n     # Test with jit_compile = True\n     model = sequential.Sequential([layers_module.Dense(1)])\n@@ -124,7 +123,7 @@ class TrainingTest(keras_parameterized.TestCase):\n       model_input[i, i:, i:] = 0\n     model.fit(model_input, np.random.random((10, 1)), epochs=1, batch_size=10)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_compile_fit_with_mirrored_strategy(self):\n     # Test with jit_compile = True\n     strategy = tf.distribute.MirroredStrategy()\n@@ -134,7 +133,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     x, y = np.ones((10, 1)), np.ones((10, 1))\n     model.fit(x, y, epochs=2)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_verify_xla_compile_with_jit_compile(self):\n     vocab_data = ['earth', 'wind', 'and', 'fire']\n     input_array = np.array([['earth', 'wind', 'and', 'fire'],\n@@ -155,15 +154,15 @@ class TrainingTest(keras_parameterized.TestCase):\n         model.fit(input_array, expected_output, epochs=1)\n       model.predict(input_array)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_fit_without_loss_at_compile(self):\n     model = sequential.Sequential([layers_module.Dense(1)])\n-    model.compile('sgd', run_eagerly=testing_utils.should_run_eagerly())\n+    model.compile('sgd', run_eagerly=test_utils.should_run_eagerly())\n     x, y = np.ones((10, 1)), np.ones((10, 1))\n     with self.assertRaisesRegex(ValueError, 'No loss found..*'):\n       model.fit(x, y, epochs=2)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_fit_without_loss_at_compile_but_with_add_loss(self):\n \n     class MyModel(sequential.Sequential):\n@@ -173,18 +172,18 @@ class TrainingTest(keras_parameterized.TestCase):\n         return x\n \n     model = MyModel([layers_module.Dense(1)])\n-    model.compile('sgd', run_eagerly=testing_utils.should_run_eagerly())\n+    model.compile('sgd', run_eagerly=test_utils.should_run_eagerly())\n     x, y = np.ones((10, 1)), np.ones((10, 1))\n     model.fit(x, y, epochs=2)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_run_eagerly_setting(self):\n     model = sequential.Sequential([layers_module.Dense(1)])\n-    run_eagerly = testing_utils.should_run_eagerly()\n+    run_eagerly = test_utils.should_run_eagerly()\n     model.compile('sgd', 'mse', run_eagerly=run_eagerly)\n     self.assertEqual(model.run_eagerly, run_eagerly)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   @parameterized.named_parameters(\n       ('train_on_batch', 'train_on_batch'),\n       ('test_on_batch', 'test_on_batch'),\n@@ -195,7 +194,7 @@ class TrainingTest(keras_parameterized.TestCase):\n   )\n   def test_disallow_methods_inside_tf_function(self, method_name):\n     model = sequential.Sequential([layers_module.Dense(1)])\n-    run_eagerly = testing_utils.should_run_eagerly()\n+    run_eagerly = test_utils.should_run_eagerly()\n     model.compile('sgd', 'mse', run_eagerly=run_eagerly)\n \n     @tf.function\n@@ -206,7 +205,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(RuntimeError, error_msg):\n       my_fn()\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_fit_and_validate_learning_phase(self):\n \n     class ReturnTraining(layers_module.Layer):\n@@ -219,7 +218,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         loss='mae',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     inputs = np.ones((40, 2), dtype=np.float32)\n     targets = np.ones((40, 1), dtype=np.float32)\n@@ -237,7 +236,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     # The validation loss should be 1.0.\n     self.assertAllClose(history.history['val_loss'][0], 1.0)\n \n-  @keras_parameterized.run_all_keras_modes(\n+  @test_combinations.run_all_keras_modes(\n       always_skip_v1=True)\n   def test_warn_on_evaluate(self):\n     i = layers_module.Input((1,))\n@@ -254,7 +253,7 @@ class TrainingTest(keras_parameterized.TestCase):\n         any('`evaluate()` received a value for `sample_weight`' in log\n             for log in logs.output))\n \n-  @keras_parameterized.run_all_keras_modes(\n+  @test_combinations.run_all_keras_modes(\n       always_skip_v1=True)\n   def test_sample_weight_warning_disable(self):\n     i = layers_module.Input((1,))\n@@ -271,7 +270,7 @@ class TrainingTest(keras_parameterized.TestCase):\n         any('`evaluate()` received a value for `sample_weight`' in log\n             for log in logs.output))\n \n-  @keras_parameterized.run_all_keras_modes(\n+  @test_combinations.run_all_keras_modes(\n       always_skip_v1=True)\n   def test_warn_on_evaluate_with_tf_dataset(self):\n     i = layers_module.Input((1,))\n@@ -291,7 +290,7 @@ class TrainingTest(keras_parameterized.TestCase):\n         any('`evaluate()` received a value for `sample_weight`' in log\n             for log in logs.output))\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_fit_and_validate_training_arg(self):\n \n     class ReturnTraining(layers_module.Layer):\n@@ -306,7 +305,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         loss='mae',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     inputs = np.ones((40, 2), dtype=np.float32)\n     targets = np.ones((40, 1), dtype=np.float32)\n@@ -324,8 +323,8 @@ class TrainingTest(keras_parameterized.TestCase):\n     # The validation loss should be 1.0.\n     self.assertAllClose(history.history['val_loss'][0], 1.0)\n \n-  @keras_parameterized.run_all_keras_modes\n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n   def test_target_dtype_matches_output(self):\n \n     def loss_fn(labels, preds):\n@@ -336,18 +335,18 @@ class TrainingTest(keras_parameterized.TestCase):\n         layers_module.Dense(10, dtype=np.float64),\n         layers_module.Dense(10, dtype=np.float64)\n     ]\n-    model = testing_utils.get_model_from_layers(layers, input_shape=(1,))\n+    model = test_utils.get_model_from_layers(layers, input_shape=(1,))\n     inputs = np.ones(shape=(10, 1), dtype=np.float64)\n     targets = np.ones(shape=(10, 1), dtype=np.float64)\n     model.compile(\n         'sgd',\n         loss=loss_fn,\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(inputs, targets)\n     model.test_on_batch(inputs, targets)\n     self.assertEqual(model.predict(inputs).dtype, np.float64)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_fit_and_validate_nested_training_arg(self):\n \n     class NestedReturnTraining(layers_module.Layer):\n@@ -375,7 +374,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         loss='mae',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     inputs = np.ones((40, 2), dtype=np.float32)\n     targets = np.ones((40, 1), dtype=np.float32)\n@@ -393,8 +392,8 @@ class TrainingTest(keras_parameterized.TestCase):\n     # The validation loss should be 1.0.\n     self.assertAllClose(history.history['val_loss'][0], 1.0)\n \n-  @keras_parameterized.run_with_all_model_types(exclude_models='sequential')\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types(exclude_models='sequential')\n+  @test_combinations.run_all_keras_modes\n   def test_fit_on_arrays(self):\n     input_a = layers_module.Input(shape=(3,), name='input_a')\n     input_b = layers_module.Input(shape=(3,), name='input_b')\n@@ -404,7 +403,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     branch_a = [input_a, dense]\n     branch_b = [input_b, dense, dropout]\n \n-    model = testing_utils.get_multi_io_model(branch_a, branch_b)\n+    model = test_utils.get_multi_io_model(branch_a, branch_b)\n \n     optimizer = RMSPropOptimizer(learning_rate=0.001)\n     loss = 'mse'\n@@ -414,7 +413,7 @@ class TrainingTest(keras_parameterized.TestCase):\n         loss,\n         metrics=[metrics_module.CategoricalAccuracy(), 'mae'],\n         loss_weights=loss_weights,\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     input_a_np = np.random.random((10, 3))\n     input_b_np = np.random.random((10, 3))\n@@ -475,7 +474,7 @@ class TrainingTest(keras_parameterized.TestCase):\n         verbose=0,\n         validation_split=0.2)\n \n-    if testing_utils.get_model_type() == 'functional':\n+    if test_utils.get_model_type() == 'functional':\n       # Test with dictionary inputs\n       model.fit(\n           {\n@@ -531,7 +530,7 @@ class TrainingTest(keras_parameterized.TestCase):\n         optimizer,\n         loss,\n         metrics=[metrics_module.CategoricalAccuracy(), 'mae'],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(\n         [input_a_np, input_b_np], [output_d_np, output_e_np],\n         epochs=1,\n@@ -539,7 +538,7 @@ class TrainingTest(keras_parameterized.TestCase):\n         verbose=0)\n \n     # Test with dictionaries for loss, metrics, loss weights\n-    if testing_utils.get_model_type() == 'functional':\n+    if test_utils.get_model_type() == 'functional':\n       loss = {'dense': 'mse', 'dropout': 'mae'}\n       loss_weights = {'dense': 1., 'dropout': 0.5}\n       metrics = {\n@@ -551,7 +550,7 @@ class TrainingTest(keras_parameterized.TestCase):\n           loss,\n           metrics=metrics,\n           loss_weights=loss_weights,\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n     model.fit(\n         [input_a_np, input_b_np], [output_d_np, output_e_np],\n         epochs=1,\n@@ -565,7 +564,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer,\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     # This will work\n     model.fit([input_a_np], output_d_np, epochs=1)\n \n@@ -598,7 +597,7 @@ class TrainingTest(keras_parameterized.TestCase):\n                 batch_size=5,\n                 verbose=2)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_evaluate_predict_on_arrays(self):\n     a = layers_module.Input(shape=(3,), name='input_a')\n     b = layers_module.Input(shape=(3,), name='input_b')\n@@ -619,7 +618,7 @@ class TrainingTest(keras_parameterized.TestCase):\n         metrics=['mae', metrics_module.CategoricalAccuracy()],\n         loss_weights=loss_weights,\n         sample_weight_mode=None,\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     input_a_np = np.random.random((10, 3))\n     input_b_np = np.random.random((10, 3))\n@@ -753,8 +752,8 @@ class TrainingTest(keras_parameterized.TestCase):\n \n       return XYSequence, XSequence\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_with_all_model_types\n   @parameterized.named_parameters(\n       ('dataset', 'dataset'),\n       ('generator', 'generator'),\n@@ -772,17 +771,17 @@ class TrainingTest(keras_parameterized.TestCase):\n       evaluate_kwargs['steps'] = 4\n       predict_kwargs['steps'] = 4\n \n-    model = testing_utils.get_small_mlp(1, 1, 1)\n+    model = test_utils.get_small_mlp(1, 1, 1)\n     model.compile(\n         loss='mse',\n         optimizer='sgd',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     model.fit(xy_function(use_namedtuple=False), **fit_kwargs)\n     model.evaluate(xy_function(use_namedtuple=False), **evaluate_kwargs)\n     model.predict(x_function(use_namedtuple=False), **predict_kwargs)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_custom_mapping_in_config(self):\n \n     class MyModel(training_module.Model):\n@@ -823,7 +822,7 @@ class TrainingTest(keras_parameterized.TestCase):\n                 epochs=1, batch_size=2, validation_split=0.5)\n       model.evaluate(test_inputs, test_outputs, batch_size=2)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_compile_with_sparse_placeholders(self):\n     inputs = layers_module.Input(shape=(10,), sparse=True)\n     weights = tf.Variable(\n@@ -835,9 +834,9 @@ class TrainingTest(keras_parameterized.TestCase):\n         loss='binary_crossentropy',\n         optimizer='adam',\n         metrics=['accuracy'],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_that_trainable_disables_updates(self):\n     val_a = np.random.random((10, 4))\n     val_out = np.random.random((10, 4))\n@@ -854,7 +853,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     if not tf.compat.v1.executing_eagerly_outside_functions():\n       self.assertEmpty(model.updates)\n \n@@ -867,7 +866,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     if not tf.compat.v1.executing_eagerly_outside_functions():\n       self.assertAllGreater(len(model.updates), 0)\n \n@@ -879,7 +878,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     if not tf.compat.v1.executing_eagerly_outside_functions():\n       self.assertEmpty(model.updates)\n \n@@ -905,7 +904,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     self.assertLen(model.non_trainable_weights, 2)\n     self.assertLen(model.weights, 6)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_weight_deduplication(self):\n \n     class WatchingLayer(layers_module.Layer):\n@@ -933,7 +932,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='mse',\n         optimizer=optimizer_v2.gradient_descent.SGD(0.24),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x = np.ones((64 * 2,))\n     y = 4.5 * x - 3.\n@@ -944,7 +943,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     # be ~0.15, compared to the correct answer of O(1e-7).\n     self.assertLess(history.history['loss'][-1], 1e-6)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_weight_shared_across_layers(self):\n \n     class AddWeightLayer(layers_module.Layer):\n@@ -981,7 +980,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     self.assertEqual(l.non_trainable_variables, [l.layer1.non_trainable_var])\n     self.assertLen(l.get_weights(), 2)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_weight_tracking_for_template(self):\n     def variable_scoped_function(trainable=True):\n       return tf.compat.v1.get_variable(\n@@ -1040,7 +1039,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     self.assertEqual([], model.templates.updates)\n     self.assertEqual([], model.templates.losses)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_logs_passed_to_callbacks(self):\n     input_dim = 5\n     num_classes = 1\n@@ -1062,17 +1061,17 @@ class TrainingTest(keras_parameterized.TestCase):\n         self.batch_end_logs = logs\n         self.batch_end_call_count += 1\n \n-    model = testing_utils.get_small_sequential_mlp(\n+    model = test_utils.get_small_sequential_mlp(\n         num_hidden=10, num_classes=num_classes, input_dim=input_dim)\n     model.compile(\n         loss='binary_crossentropy',\n         metrics=['acc'],\n         weighted_metrics=['mae'],\n         optimizer=RMSPropOptimizer(learning_rate=0.01),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     np.random.seed(1337)\n-    (x_train, y_train), (_, _) = testing_utils.get_test_data(\n+    (x_train, y_train), (_, _) = test_utils.get_test_data(\n         train_samples=10,\n         test_samples=10,\n         input_shape=(input_dim,),\n@@ -1096,7 +1095,7 @@ class TrainingTest(keras_parameterized.TestCase):\n         set(test_callback.epoch_end_logs.keys()),\n         set(['acc', 'loss', 'mae', 'val_acc', 'val_loss', 'val_mae']))\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_mismatched_output_shape_and_target_shape(self):\n     model = sequential.Sequential([\n         layers_module.Dense(2, input_shape=(3, 4)),\n@@ -1105,7 +1104,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     model.compile(\n         RMSPropOptimizer(learning_rate=0.001),\n         loss='sparse_categorical_crossentropy',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     # Test with Numpy data\n     x_train = np.random.random((10, 3, 4)).astype(np.float32)\n     y_train = np.random.randint(0, 5, size=(10, 3)).astype(np.float32)\n@@ -1127,7 +1126,7 @@ class TrainingTest(keras_parameterized.TestCase):\n       # Test with eager execution and iterator\n       model.fit(dataset, epochs=1, steps_per_epoch=2)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_losses_in_defun(self):\n     layer = layers_module.Dense(1, kernel_regularizer='l1')\n     layer(tf.ones([1, 10]))\n@@ -1139,7 +1138,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     self.assertAllEqual(\n         self.evaluate(layer.losses), self.evaluate(get_losses()))\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_logging(self):\n     mock_stdout = io.StringIO()\n     model = sequential.Sequential()\n@@ -1148,14 +1147,15 @@ class TrainingTest(keras_parameterized.TestCase):\n     model.compile(\n         RMSPropOptimizer(learning_rate=0.001),\n         loss='binary_crossentropy',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     io_utils.enable_interactive_logging()\n     with tf.compat.v1.test.mock.patch.object(sys, 'stdout', mock_stdout):\n       model.fit(\n           np.ones((10, 10), 'float32'), np.ones((10, 1), 'float32'), epochs=10)\n     self.assertTrue('Epoch 5/10' in mock_stdout.getvalue())\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_training_with_loss_instance(self):\n     a = layers_module.Input(shape=(3,), name='input_a')\n     b = layers_module.Input(shape=(3,), name='input_b')\n@@ -1183,7 +1183,8 @@ class TrainingTest(keras_parameterized.TestCase):\n               epochs=1,\n               batch_size=5)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_static_batch_in_input_layer(self):\n     if tf.executing_eagerly():\n       self.skipTest('Not inferred in eager.')\n@@ -1215,7 +1216,8 @@ class TrainingTest(keras_parameterized.TestCase):\n       model.fit(x, y, callbacks=[counter])\n       self.assertEqual(counter.batches, expected_batches)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_static_batch_in_input_layer_consistency_checks(self):\n     if tf.executing_eagerly():\n       self.skipTest('Not inferred in eager.')\n@@ -1229,7 +1231,8 @@ class TrainingTest(keras_parameterized.TestCase):\n                                 'incompatible with the specified batch size'):\n       model.fit(x, y, batch_size=4)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_compatible_batch_size_functional_model(self):\n \n     class MyLayer(layers_module.Layer):\n@@ -1248,7 +1251,8 @@ class TrainingTest(keras_parameterized.TestCase):\n           'Found incompatible static batch sizes among the inputs. '\n           'Batch sizes: [2, 3]')\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_calling_subclass_model_on_different_datasets(self):\n \n     class SubclassedModel(training_module.Model):\n@@ -1264,13 +1268,13 @@ class TrainingTest(keras_parameterized.TestCase):\n     self.assertAllEqual([[6], [8], [10], [12]],\n                         model.predict(dataset_two, steps=2))\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def test_training_on_sparse_categorical_crossentropy_loss_with_softmax(self):\n     np.random.seed(1337)\n     train_x = np.ones((100, 4))\n     train_y = np.random.randint(0, 1, size=(100, 1))\n \n-    reference_model = testing_utils.get_small_sequential_mlp(16, 2,\n+    reference_model = test_utils.get_small_sequential_mlp(16, 2,\n                                                           input_dim=4)\n     reference_model.compile(loss='sparse_categorical_crossentropy',\n                             optimizer=RMSPropOptimizer(learning_rate=0.001),\n@@ -1278,7 +1282,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     fixed_weights = reference_model.get_weights()\n     reference_model_loss = reference_model.train_on_batch(train_x, train_y)\n \n-    test_model = testing_utils.get_small_sequential_mlp(16, 2, input_dim=4)\n+    test_model = test_utils.get_small_sequential_mlp(16, 2, input_dim=4)\n     test_model.compile(loss='sparse_categorical_crossentropy',\n                        optimizer=RMSPropOptimizer(learning_rate=0.001),\n                        run_eagerly=False)\n@@ -1286,14 +1290,14 @@ class TrainingTest(keras_parameterized.TestCase):\n     test_model_loss = test_model.train_on_batch(train_x, train_y)\n     self.assertAlmostEqual(test_model_loss, reference_model_loss, places=4)\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def test_training_on_categorical_crossentropy_loss_with_softmax(self):\n     np.random.seed(1337)\n     train_x = np.ones((100, 4))\n     train_y = np_utils.to_categorical(\n         np.random.randint(0, 1, size=(100, 1)), 2)\n \n-    reference_model = testing_utils.get_small_sequential_mlp(16, 2,\n+    reference_model = test_utils.get_small_sequential_mlp(16, 2,\n                                                           input_dim=4)\n     reference_model.compile(loss='categorical_crossentropy',\n                             optimizer=RMSPropOptimizer(learning_rate=0.001),\n@@ -1301,7 +1305,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     fixed_weights = reference_model.get_weights()\n     reference_model_loss = reference_model.train_on_batch(train_x, train_y)\n \n-    test_model = testing_utils.get_small_sequential_mlp(16, 2, input_dim=4)\n+    test_model = test_utils.get_small_sequential_mlp(16, 2, input_dim=4)\n     test_model.compile(loss='categorical_crossentropy',\n                        optimizer=RMSPropOptimizer(learning_rate=0.001),\n                        run_eagerly=False)\n@@ -1309,11 +1313,11 @@ class TrainingTest(keras_parameterized.TestCase):\n     test_model_loss = test_model.train_on_batch(train_x, train_y)\n     self.assertAlmostEqual(test_model_loss, reference_model_loss, places=4)\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def test_training_on_binary_crossentropy_loss(self):\n     train_x = np.ones((100, 4), dtype=np.float32)\n     train_y = np.ones((100, 1), dtype=np.float32)\n-    reference_model = testing_utils.get_small_sequential_mlp(16, 1,\n+    reference_model = test_utils.get_small_sequential_mlp(16, 1,\n                                                           input_dim=4)\n     reference_model.compile(loss='binary_crossentropy',\n                             optimizer=RMSPropOptimizer(learning_rate=0.001),\n@@ -1321,7 +1325,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     fixed_weights = reference_model.get_weights()\n     reference_model_loss = reference_model.train_on_batch(train_x, train_y)\n \n-    test_model = testing_utils.get_small_sequential_mlp(16, 1, input_dim=4)\n+    test_model = test_utils.get_small_sequential_mlp(16, 1, input_dim=4)\n     test_model.compile(loss='binary_crossentropy',\n                        optimizer=RMSPropOptimizer(learning_rate=0.001),\n                        run_eagerly=False)\n@@ -1329,18 +1333,18 @@ class TrainingTest(keras_parameterized.TestCase):\n     test_model_loss = test_model.train_on_batch(train_x, train_y)\n     self.assertAlmostEqual(test_model_loss, reference_model_loss, places=4)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   @parameterized.named_parameters(\n       ('default', 1, 4), ('integer_two', 2, 2), ('integer_four', 4, 1),\n       ('simple_list', [1, 3, 4], 3), ('duplicated_list', [4, 2, 2], 2))\n   def test_validation_freq(self, validation_freq, expected_runs):\n     x, y = np.ones((10, 10)), np.ones((10, 1))\n-    model = testing_utils.get_small_mlp(2, 1, 10)\n+    model = test_utils.get_small_mlp(2, 1, 10)\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     class ValCounter(Callback):\n \n@@ -1360,25 +1364,25 @@ class TrainingTest(keras_parameterized.TestCase):\n         callbacks=[val_counter])\n     self.assertEqual(val_counter.val_runs, expected_runs)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_validation_steps_without_data(self):\n     if tf.executing_eagerly():\n       self.skipTest('Check removed in new `fit`')\n     x, y = np.ones((10, 10)), np.ones((10, 1))\n-    model = testing_utils.get_small_mlp(2, 1, 10)\n+    model = test_utils.get_small_mlp(2, 1, 10)\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     with self.assertRaisesRegex(\n         ValueError, '`validation_steps` should not be specified if '\n         '`validation_data` is None.'):\n       model.fit(x, y, epochs=4, validation_data=None, validation_steps=3)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_layer_with_variable_output(self):\n \n     class VariableOutputLayer(layers_module.Layer):\n@@ -1389,7 +1393,7 @@ class TrainingTest(keras_parameterized.TestCase):\n       def call(self, inputs):\n         return self.v\n \n-    model = testing_utils.get_model_from_layers(\n+    model = test_utils.get_model_from_layers(\n         [VariableOutputLayer(), layers_module.Dense(1)], input_shape=(10,))\n     # TODO(omalleyt): Make this work with `run_eagerly=True`.\n     model.compile('sgd', 'mse', run_eagerly=False)\n@@ -1397,9 +1401,9 @@ class TrainingTest(keras_parameterized.TestCase):\n \n     self.assertLen(model.trainable_variables, 3)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n+  @test_utils.enable_v2_dtype_behavior\n   def test_model_dtype(self):\n \n     class AssertTypeLayer(layers_module.Layer):\n@@ -1411,12 +1415,12 @@ class TrainingTest(keras_parameterized.TestCase):\n         return inputs + 1.\n \n     for dtype in ('float16', 'float32', 'float64'):\n-      model = testing_utils.get_model_from_layers(\n+      model = test_utils.get_model_from_layers(\n           [AssertTypeLayer(dtype=dtype)], input_shape=(10,))\n       model.compile(\n           'sgd',\n           'mse',\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n \n       x = np.ones((10, 10))\n       y = np.ones((10, 10))\n@@ -1424,22 +1428,22 @@ class TrainingTest(keras_parameterized.TestCase):\n       model.test_on_batch(x, y)\n       model(x)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n+  @test_utils.enable_v2_dtype_behavior\n   def test_model_input_dtype(self):\n-    model = testing_utils.get_small_mlp(1, 10, 10)\n+    model = test_utils.get_small_mlp(1, 10, 10)\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     x = np.ones((10, 10)).astype(np.float64)\n     y = np.ones((10, 10)).astype(np.float64)\n     dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(2)\n     model.fit(dataset)\n     self.assertEqual(model._compute_dtype, 'float32')\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_subclassed_model_with_training_arg(self):\n \n     class LayerWithTrainingArg(layers_module.Layer):\n@@ -1464,7 +1468,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='mse',\n         optimizer='sgd',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(x, x, epochs=1)\n \n     if tf.executing_eagerly():\n@@ -1475,7 +1479,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     self.assertIs(model.training, expected_training_arg)\n     self.assertIs(model.l1.training, expected_training_arg)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_error_when_model_is_not_compiled(self):\n     inputs = input_layer.Input(shape=(1,))\n     outputs = layers_module.Dense(1)(inputs)\n@@ -1493,8 +1497,8 @@ class TrainingTest(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(RuntimeError, 'must compile your model'):\n       model.fit(np.random.random((32, 1)), epochs=2)\n \n-  @keras_parameterized.run_all_keras_modes\n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_combinations.run_all_keras_modes\n+  @test_utils.enable_v2_dtype_behavior\n   def test_losses_of_different_dtypes(self):\n     inp = input_layer.Input(shape=(2,))\n     out_1 = layers_module.Dense(\n@@ -1507,12 +1511,12 @@ class TrainingTest(keras_parameterized.TestCase):\n     extra_loss = tf.reduce_sum(tf.cast(out_2, 'float64'))\n     model.add_loss(extra_loss)\n     model.compile('sgd', ['mse', 'mse'],\n-                  run_eagerly=testing_utils.should_run_eagerly())\n+                  run_eagerly=test_utils.should_run_eagerly())\n     x, y = np.ones((10, 2)), np.ones((10, 2))\n     model.fit(x, [y, y])\n \n-  @keras_parameterized.run_all_keras_modes\n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_combinations.run_all_keras_modes\n+  @test_utils.enable_v2_dtype_behavior\n   def test_losses_of_different_dtypes_with_subclassed_model(self):\n \n     class MyModel(training_module.Model):\n@@ -1525,12 +1529,12 @@ class TrainingTest(keras_parameterized.TestCase):\n         return self.dense(inputs)\n \n     model = MyModel(dtype='float32')\n-    model.compile('sgd', 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+    model.compile('sgd', 'mse', run_eagerly=test_utils.should_run_eagerly())\n     x, y = np.ones((10, 2)), np.ones((10, 2))\n     model.fit(x, y)\n \n-  @keras_parameterized.run_all_keras_modes\n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_combinations.run_all_keras_modes\n+  @test_utils.enable_v2_dtype_behavior\n   def test_regularizer_of_different_dtype(self):\n     inp = input_layer.Input(shape=(2,))\n \n@@ -1541,16 +1545,16 @@ class TrainingTest(keras_parameterized.TestCase):\n         2, dtype='float32', kernel_regularizer=regularizer)(\n             inp)\n     model = training_module.Model(inp, out)\n-    model.compile('sgd', 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+    model.compile('sgd', 'mse', run_eagerly=test_utils.should_run_eagerly())\n     x, y = np.ones((10, 2)), np.ones((10, 2))\n     model.fit(x, y)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_outputs_are_floats(self):\n     x, y = np.ones((10, 1)), np.ones((10, 1))\n     model = sequential.Sequential([layers_module.Dense(1)])\n     model.compile('sgd', 'mse', metrics=['accuracy'],\n-                  run_eagerly=testing_utils.should_run_eagerly())\n+                  run_eagerly=test_utils.should_run_eagerly())\n \n     history = model.fit(x, y, epochs=2)\n     self.assertIsInstance(history.history['loss'][0], float)\n@@ -1568,7 +1572,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     self.assertIsInstance(loss, float)\n     self.assertIsInstance(accuracy, float)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_int_output(self):\n     x, y = np.ones((10, 1)), np.ones((10, 1))\n     model = sequential.Sequential([layers_module.Dense(1)])\n@@ -1582,11 +1586,11 @@ class TrainingTest(keras_parameterized.TestCase):\n         return tf.constant(1, dtype='int64')\n \n     model.compile('sgd', 'mse', metrics=[MyMetric()],\n-                  run_eagerly=testing_utils.should_run_eagerly())\n+                  run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(x, y, epochs=2)\n     self.assertIsInstance(history.history['my_metric'][0], int)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_calling_aggregate_gradient(self):\n \n     class _Optimizer(optimizer_v2.gradient_descent.SGD):\n@@ -1608,7 +1612,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     model.add(layers_module.Dense(10, activation='relu'))\n \n     model.compile(mock_optimizer, 'mse',\n-                  run_eagerly=testing_utils.should_run_eagerly())\n+                  run_eagerly=test_utils.should_run_eagerly())\n     x, y = np.ones((10, 10)), np.ones((10, 10))\n     model.fit(x, y)\n     self.assertEqual(model.optimizer.aggregate_gradients_called, True)\n@@ -1628,12 +1632,12 @@ class TrainingTest(keras_parameterized.TestCase):\n \n     mock_optimizer = _OptimizerOverrideApplyGradients()\n     model.compile(mock_optimizer, 'mse',\n-                  run_eagerly=testing_utils.should_run_eagerly())\n+                  run_eagerly=test_utils.should_run_eagerly())\n     x, y = np.ones((10, 10)), np.ones((10, 10))\n     model.fit(x, y)\n     self.assertEqual(model.optimizer.aggregate_gradients_called, True)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_gradients_are_none(self):\n \n     class DenseWithExtraWeight(layers_module.Dense):\n@@ -1649,7 +1653,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     model = sequential.Sequential([DenseWithExtraWeight(4, input_shape=(4,))])\n     # Test clipping can handle None gradients\n     opt = optimizer_v2.adam.Adam(clipnorm=1.0, clipvalue=1.0)\n-    model.compile(opt, 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+    model.compile(opt, 'mse', run_eagerly=test_utils.should_run_eagerly())\n     inputs = np.random.normal(size=(64, 4))\n     targets = np.random.normal(size=(64, 4))\n     old_kernel = model.get_weights()[1]\n@@ -1657,7 +1661,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     new_kernel = model.get_weights()[1]\n     self.assertNotAllEqual(old_kernel, new_kernel)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_layer_ordering(self):\n \n     class MyLayer(layers_module.Layer):\n@@ -1683,7 +1687,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     # Only direct sublayers, including those in data structures.\n     self.assertEqual(['direct', 'dict'], [l.name for l in model.layers])\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_trainable_state_setting(self):\n \n     class UpdateLayer(layers_module.Layer):\n@@ -1699,12 +1703,12 @@ class TrainingTest(keras_parameterized.TestCase):\n     layer = UpdateLayer()\n     model_with_updates = sequential.Sequential([layer])\n     model_with_updates.compile(\n-        'sgd', 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+        'sgd', 'mse', run_eagerly=test_utils.should_run_eagerly())\n \n     layer.trainable = False\n     model_without_updates = sequential.Sequential([layer])\n     model_without_updates.compile(\n-        'sgd', 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+        'sgd', 'mse', run_eagerly=test_utils.should_run_eagerly())\n \n     x, y = np.ones((10, 1)), np.ones((10, 1))\n \n@@ -1716,7 +1720,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     # assign_add not called.\n     self.assertEqual(self.evaluate(layer.v), 1.)\n \n-  @keras_parameterized.run_all_keras_modes(\n+  @test_combinations.run_all_keras_modes(\n       always_skip_v1=True)\n   @parameterized.named_parameters(\n       ('numpy_array', 'numpy_array'),\n@@ -1764,7 +1768,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     model.evaluate(x, batch_size=batch_size)\n     model.predict(x, batch_size=batch_size)\n \n-  @keras_parameterized.run_all_keras_modes(\n+  @test_combinations.run_all_keras_modes(\n       always_skip_v1=True)\n   @parameterized.named_parameters(\n       ('custom_metrics', False, True),\n@@ -1797,10 +1801,10 @@ class TrainingTest(keras_parameterized.TestCase):\n     model = MyModel(inputs, outputs)\n     if use_compiled_metrics:\n       model.compile('adam', 'mse', metrics=['mae', 'mape'],\n-                    run_eagerly=testing_utils.should_run_eagerly())\n+                    run_eagerly=test_utils.should_run_eagerly())\n     else:\n       model.compile('adam', 'mse',\n-                    run_eagerly=testing_utils.should_run_eagerly())\n+                    run_eagerly=test_utils.should_run_eagerly())\n     x = np.random.random((4, 2))\n     y = np.random.random((4, 3))\n     results_list = model.evaluate(x, y)\n@@ -1822,15 +1826,15 @@ class TrainingTest(keras_parameterized.TestCase):\n       self.assertEqual(results_list,\n                        [results_dict['mean'], results_dict['sum']])\n \n-  @keras_parameterized.run_all_keras_modes\n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n   def test_model_make_function(self):\n     layers = [\n         layers_module.Dense(10, dtype=np.float64),\n         layers_module.Dense(10, dtype=np.float64)\n     ]\n-    model = testing_utils.get_model_from_layers(layers, input_shape=(1,))\n-    model.compile('sgd', 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+    model = test_utils.get_model_from_layers(layers, input_shape=(1,))\n+    model.compile('sgd', 'mse', run_eagerly=test_utils.should_run_eagerly())\n \n     original_train_function = model.make_train_function()\n     self.assertIsNotNone(original_train_function)\n@@ -1853,7 +1857,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     self.assertNotEqual(\n         model.make_predict_function(force=True), original_predict_function)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_custom_compute_metrics(self):\n \n     class CustomMetric(metrics_module.Mean):\n@@ -1892,7 +1896,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     self.assertNotEqual(self.evaluate(initial_result),\n                         self.evaluate(after_fit_result))\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_custom_compute_loss(self):\n \n     class MyModel(training_module.Model):\n@@ -1928,7 +1932,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     self.assertLen(history.history['loss'], 2)\n     self.assertAllClose(history.history['loss'][1], model.loss_metric.result())\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_ema_overwrite(self):\n \n     model = sequential.Sequential()\n@@ -1945,7 +1949,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     self.assertLen(history.history['loss'], 2)\n     self.assertAllClose(initial_value, model.trainable_variables[0])\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_get_verbosity(self):\n     class MyStrategy(tf.distribute.Strategy):\n \n@@ -1965,7 +1969,7 @@ class TrainingTest(keras_parameterized.TestCase):\n     self.assertEqual(training_module._get_verbosity(\n         'auto', tf.distribute.MirroredStrategy()), 2)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_save_spec(self):\n \n     class Model(training_module.Model):\n@@ -1999,10 +2003,10 @@ class TrainingTest(keras_parameterized.TestCase):\n     self.assertEqual(input_specs[2].shape.as_list(), [3, 3])\n \n \n-class TestExceptionsAndWarnings(keras_parameterized.TestCase):\n+class TestExceptionsAndWarnings(test_combinations.TestCase):\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_with_all_model_types\n   def test_fit_on_no_output(self):\n     inputs = layers_module.Input((3,))\n     outputs = layers_module.Dense(2)(inputs)\n@@ -2012,8 +2016,8 @@ class TestExceptionsAndWarnings(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(ValueError, 'Target data is missing..*'):\n       model.fit(x)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_with_all_model_types\n   def test_fit_on_wrong_output_type(self):\n     inputs1 = layers_module.Input((3,), name='a')\n     inputs2 = layers_module.Input((3,), name='b')\n@@ -2026,7 +2030,7 @@ class TestExceptionsAndWarnings(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(ValueError, 'Target data is missing..*'):\n       model.fit({'a': x, 'b': x, 'c': y})\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_compile_warning_for_loss_missing_output(self):\n     with self.cached_session():\n       inp = layers_module.Input(shape=(16,), name='input_a')\n@@ -2046,9 +2050,9 @@ class TestExceptionsAndWarnings(keras_parameterized.TestCase):\n               'dense_2': 'categorical_accuracy',\n               'dense_1': metrics_module.CategoricalAccuracy(),\n           },\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_predict_error_with_empty_x(self):\n     inputs = layers_module.Input(shape=(2,))\n     outputs = layers_module.Dense(4)(inputs)\n@@ -2059,7 +2063,7 @@ class TestExceptionsAndWarnings(keras_parameterized.TestCase):\n                                 'Unexpected result of `predict_function`.*'):\n       model.predict(np.array([]))\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_on_batch_error_inconsistent_batch_size(self):\n     input_node1 = layers_module.Input(shape=(5,))\n     input_node2 = layers_module.Input(shape=(5,))\n@@ -2080,9 +2084,9 @@ class TestExceptionsAndWarnings(keras_parameterized.TestCase):\n       model.predict_on_batch([np.ones((10, 5)), np.ones((11, 5))])\n \n \n-class LossWeightingTest(keras_parameterized.TestCase):\n+class LossWeightingTest(test_combinations.TestCase):\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_class_weights(self):\n     num_classes = 5\n     batch_size = 5\n@@ -2094,17 +2098,17 @@ class LossWeightingTest(keras_parameterized.TestCase):\n     input_dim = 5\n     learning_rate = 0.001\n \n-    model = testing_utils.get_small_sequential_mlp(\n+    model = test_utils.get_small_sequential_mlp(\n         num_hidden=10, num_classes=num_classes, input_dim=input_dim)\n     model.compile(\n         loss='categorical_crossentropy',\n         metrics=['acc', metrics_module.CategoricalAccuracy()],\n         weighted_metrics=['mae', metrics_module.CategoricalAccuracy()],\n         optimizer=RMSPropOptimizer(learning_rate=learning_rate),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     np.random.seed(1337)\n-    (x_train, y_train), (x_test, y_test) = testing_utils.get_test_data(\n+    (x_train, y_train), (x_test, y_test) = test_utils.get_test_data(\n         train_samples=train_samples,\n         test_samples=test_samples,\n         input_shape=(input_dim,),\n@@ -2150,7 +2154,7 @@ class LossWeightingTest(keras_parameterized.TestCase):\n     # TODO(b/152990697): Fix the class weights test here.\n     # self.assertLess(score[0], ref_score[0])\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_temporal_sample_weights(self):\n     num_classes = 5\n     batch_size = 5\n@@ -2172,7 +2176,7 @@ class LossWeightingTest(keras_parameterized.TestCase):\n       model.add(layers_module.Activation('softmax'))\n \n       np.random.seed(1337)\n-      (x_train, y_train), (x_test, y_test) = testing_utils.get_test_data(\n+      (x_train, y_train), (x_test, y_test) = test_utils.get_test_data(\n           train_samples=train_samples,\n           test_samples=test_samples,\n           input_shape=(input_dim,),\n@@ -2210,7 +2214,7 @@ class LossWeightingTest(keras_parameterized.TestCase):\n           metrics=['acc', metrics_module.CategoricalAccuracy()],\n           weighted_metrics=['mae', metrics_module.CategoricalAccuracy()],\n           sample_weight_mode='temporal',\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n \n       model.fit(\n           temporal_x_train,\n@@ -2242,8 +2246,8 @@ class LossWeightingTest(keras_parameterized.TestCase):\n             temporal_x_test[test_ids], temporal_y_test[test_ids], verbose=0)\n         self.assertLess(score[0], ref_score[0])\n \n-  @keras_parameterized.run_all_keras_modes\n-  @keras_parameterized.run_with_all_model_types(exclude_models='sequential')\n+  @test_combinations.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types(exclude_models='sequential')\n   def test_fit_with_incorrect_weights(self):\n     input_a = layers_module.Input(shape=(3,), name='input_a')\n     input_b = layers_module.Input(shape=(3,), name='input_b')\n@@ -2253,11 +2257,11 @@ class LossWeightingTest(keras_parameterized.TestCase):\n     branch_a = [input_a, dense]\n     branch_b = [input_b, dense, dropout]\n \n-    model = testing_utils.get_multi_io_model(branch_a, branch_b)\n+    model = test_utils.get_multi_io_model(branch_a, branch_b)\n     model.compile(\n         optimizer='adam',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     x = np.random.random((10, 3))\n     y = np.random.random((10, 2))\n \n@@ -2267,7 +2271,7 @@ class LossWeightingTest(keras_parameterized.TestCase):\n     with self.assertRaises(ValueError):\n       model.fit([x, x], [y, y], epochs=1, class_weight={'unknown': 1})\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_default_sample_weight(self):\n     \"\"\"Verifies that fit works without having to set sample_weight.\"\"\"\n     num_classes = 5\n@@ -2291,7 +2295,7 @@ class LossWeightingTest(keras_parameterized.TestCase):\n           optimizer,\n           loss='mse',\n           sample_weight_mode=[None],\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n       model.fit(x, y, epochs=1, batch_size=10)\n \n       # sample_weight_mode is a list and mode value is `temporal`\n@@ -2299,7 +2303,7 @@ class LossWeightingTest(keras_parameterized.TestCase):\n           optimizer,\n           loss='mse',\n           sample_weight_mode=['temporal'],\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n       model.fit(x, y, epochs=1, batch_size=10)\n \n       # sample_weight_mode is a dict and mode value is None\n@@ -2307,7 +2311,7 @@ class LossWeightingTest(keras_parameterized.TestCase):\n           optimizer,\n           loss='mse',\n           sample_weight_mode={'time_distributed': None},\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n       model.fit(x, y, epochs=1, batch_size=10)\n \n       # sample_weight_mode is a dict and mode value is `temporal`\n@@ -2315,7 +2319,7 @@ class LossWeightingTest(keras_parameterized.TestCase):\n           optimizer,\n           loss='mse',\n           sample_weight_mode={'time_distributed': 'temporal'},\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n       model.fit(x, y, epochs=1, batch_size=10)\n \n       # sample_weight_mode is a not a list/dict and mode value is None\n@@ -2323,7 +2327,7 @@ class LossWeightingTest(keras_parameterized.TestCase):\n           optimizer,\n           loss='mse',\n           sample_weight_mode=None,\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n       model.fit(x, y, epochs=1, batch_size=10)\n \n       # sample_weight_mode is a not a list/dict and mode value is `temporal`\n@@ -2331,7 +2335,7 @@ class LossWeightingTest(keras_parameterized.TestCase):\n           optimizer,\n           loss='mse',\n           sample_weight_mode='temporal',\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n       model.fit(x, y, epochs=1, batch_size=10)\n \n   def test_sample_weight_tensor(self):\n@@ -2348,7 +2352,8 @@ class LossWeightingTest(keras_parameterized.TestCase):\n       sample_weights = tf.constant(\n           [[0, .4, 1, 1], [2, .4, .3, 1]])\n       dataset = tf.data.Dataset.from_tensor_slices(sample_weights)\n-      sample_weights = tf.compat.v1.data.make_one_shot_iterator(dataset).get_next()\n+      sample_weights = tf.compat.v1.data.make_one_shot_iterator(\n+          dataset).get_next()\n       sample_weights = training_utils_v1.standardize_sample_weights(\n           sample_weights, model.output_names)\n \n@@ -2364,8 +2369,8 @@ class LossWeightingTest(keras_parameterized.TestCase):\n             (2+ .4 + .3 + 1) / 4, sess.run(model.total_loss, feed_dict=feeds))\n \n \n-@keras_parameterized.run_all_keras_modes\n-class MaskingTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class MaskingTest(test_combinations.TestCase):\n \n   def _get_model(self, input_shape=None):\n     layers = [\n@@ -2373,14 +2378,14 @@ class MaskingTest(keras_parameterized.TestCase):\n         layers_module.TimeDistributed(\n             layers_module.Dense(1, kernel_initializer='one'))\n     ]\n-    model = testing_utils.get_model_from_layers(layers, input_shape)\n+    model = test_utils.get_model_from_layers(layers, input_shape)\n     model.compile(\n         loss='mse',\n         optimizer=RMSPropOptimizer(learning_rate=0.001),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     return model\n \n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_with_all_model_types\n   def test_masking(self):\n     model = self._get_model(input_shape=(2, 1))\n     x = np.array([[[1], [1]], [[0], [0]]])\n@@ -2388,7 +2393,7 @@ class MaskingTest(keras_parameterized.TestCase):\n     loss = model.train_on_batch(x, y)\n     self.assertEqual(loss, 0)\n \n-  @keras_parameterized.run_with_all_model_types(exclude_models='functional')\n+  @test_combinations.run_with_all_model_types(exclude_models='functional')\n   def test_masking_deferred(self):\n     model = self._get_model()\n     x = np.array([[[1], [1]], [[0], [0]]])\n@@ -2422,13 +2427,13 @@ class MaskingTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='mse',\n         optimizer=RMSPropOptimizer(learning_rate=0.001),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     y = np.random.random((5, 3))\n     model.train_on_batch(x, y)\n \n \n-@keras_parameterized.run_all_keras_modes\n-class TestDynamicTrainability(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class TestDynamicTrainability(test_combinations.TestCase):\n \n   def test_trainable_warning(self):\n     x = np.random.random((5, 3))\n@@ -2440,7 +2445,7 @@ class TestDynamicTrainability(keras_parameterized.TestCase):\n     model.compile(\n         'rmsprop',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.trainable = True\n     model.train_on_batch(x, y)\n     self.assertRaises(Warning)\n@@ -2455,7 +2460,7 @@ class TestDynamicTrainability(keras_parameterized.TestCase):\n       model.compile(\n           'rmsprop',\n           'mse',\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n       out = model.predict(x)\n       model.train_on_batch(x, y)\n       out_2 = model.predict(x)\n@@ -2468,7 +2473,7 @@ class TestDynamicTrainability(keras_parameterized.TestCase):\n       model.compile(\n           'rmsprop',\n           'mse',\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n       out = model.predict(x)\n       model.train_on_batch(x, y)\n       out_2 = model.predict(x)\n@@ -2585,7 +2590,7 @@ class TestDynamicTrainability(keras_parameterized.TestCase):\n     model1.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     inputs2 = input_layer.Input(10)\n     outputs2 = shared_layer(inputs2)\n@@ -2594,7 +2599,7 @@ class TestDynamicTrainability(keras_parameterized.TestCase):\n     model2.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x, y = np.ones((10, 10)), np.ones((10, 10))\n \n@@ -2620,7 +2625,7 @@ class TestDynamicTrainability(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x = np.ones((10, 1))\n     y = 5 * x + 2\n@@ -2634,7 +2639,7 @@ class TestDynamicTrainability(keras_parameterized.TestCase):\n     self.assertAllClose([kernel[0, 0], bias[0]], [1.1176, 1.1176])\n \n \n-class TestTrainingWithDataTensors(keras_parameterized.TestCase):\n+class TestTrainingWithDataTensors(test_combinations.TestCase):\n \n   def test_training_and_eval_methods_on_symbolic_tensors_single_io(self):\n     with tf.Graph().as_default():\n@@ -2760,7 +2765,7 @@ class TestTrainingWithDataTensors(keras_parameterized.TestCase):\n     model.predict([input_a_tf, input_b_tf], steps=2)\n     model.test_on_batch([input_a_tf, input_b_tf], [output_d_tf, output_e_tf])\n \n-  @tf_test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def test_model_with_input_feed_tensor(self):\n     \"\"\"We test building a model with a TF variable as input.\n \n@@ -2897,7 +2902,7 @@ class TestTrainingWithDataTensors(keras_parameterized.TestCase):\n       out = model.predict(None, steps=3)\n       self.assertEqual(out.shape, (10 * 3, 4))\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_model_with_partial_loss(self):\n     with self.cached_session():\n       a = input_layer.Input(shape=(3,), name='input_a')\n@@ -3152,10 +3157,10 @@ class TestTrainingWithDataTensors(keras_parameterized.TestCase):\n                            [output_a_np, output_b_np])\n \n \n-class TestTrainingWithMetrics(keras_parameterized.TestCase):\n+class TestTrainingWithMetrics(test_combinations.TestCase):\n   \"\"\"Training tests related to metrics.\"\"\"\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_metrics_names(self):\n     a = layers_module.Input(shape=(3,), name='input_a')\n     b = layers_module.Input(shape=(3,), name='input_b')\n@@ -3173,7 +3178,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n         optimizer,\n         loss='mae',\n         metrics=metrics,\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     mse_metric = 'mse' if tf.executing_eagerly() else 'mean_squared_error'\n     reference_metric_names = [\n@@ -3193,7 +3198,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n               batch_size=5)\n     self.assertEqual(reference_metric_names, model.metrics_names)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_metric_state_reset_between_fit_and_evaluate(self):\n     model = sequential.Sequential()\n     model.add(layers_module.Dense(3, activation='relu', input_dim=4))\n@@ -3203,7 +3208,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n         loss='mae',\n         metrics=[acc_obj],\n         optimizer=RMSPropOptimizer(learning_rate=0.001),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x_train = np.random.random((100, 4))\n     y_train = np.random.random((100, 1))\n@@ -3215,7 +3220,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n     model.evaluate(x_test, y_test, batch_size=5)\n     self.assertEqual(self.evaluate(acc_obj.count), 10)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_metric_state_reset_between_test_on_batch_and_evaluate(self):\n     model = sequential.Sequential()\n     model.add(layers_module.Dense(3, activation='relu', input_dim=4))\n@@ -3225,7 +3230,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n         loss='mae',\n         metrics=[acc_obj],\n         optimizer=RMSPropOptimizer(learning_rate=0.001),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x_test = np.random.random((10, 4))\n     y_test = np.random.random((10, 1))\n@@ -3238,8 +3243,8 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n     self.assertEqual(loss_eval, loss_eval_1)\n     self.assertEqual(acc_eval, acc_eval_1)\n \n-  @keras_parameterized.run_with_all_model_types(exclude_models=['sequential'])\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types(exclude_models=['sequential'])\n+  @test_combinations.run_all_keras_modes\n   def test_metrics_valid_compile_input_formats(self):\n     inp_1 = layers_module.Input(shape=(1,), name='input_1')\n     inp_2 = layers_module.Input(shape=(1,), name='input_2')\n@@ -3251,7 +3256,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n \n     branch_a = [inp_1, x, out_1]\n     branch_b = [inp_2, x, out_2]\n-    model = testing_utils.get_multi_io_model(branch_a, branch_b)\n+    model = test_utils.get_multi_io_model(branch_a, branch_b)\n \n     # list of metrics.\n     model.compile(\n@@ -3259,7 +3264,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n         loss='mse',\n         metrics=[metrics_module.MeanSquaredError()],\n         weighted_metrics=[metrics_module.MeanSquaredError()],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     # list of list of metrics.\n     model.compile(\n@@ -3275,7 +3280,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n             [metrics_module.MeanSquaredError(),\n              metrics_module.Accuracy()]\n         ],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     # dict of metrics.\n     model.compile(\n@@ -3297,9 +3302,9 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n                 metrics_module.Accuracy()\n             ],\n         },\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_metrics_masking(self):\n     np.random.seed(1337)\n     model = sequential.Sequential()\n@@ -3311,7 +3316,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n         RMSPropOptimizer(learning_rate=0.001),\n         loss='mse',\n         weighted_metrics=['accuracy'],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     # verify that masking is applied.\n     x = np.array([[[1], [1]], [[1], [1]], [[0], [0]]])\n@@ -3324,7 +3329,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n     scores = model.train_on_batch(x, y, sample_weight=w)\n     self.assertArrayNear(scores, [0.3328, 0.8], 0.001)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_add_metric_with_tensor_on_model(self):\n     x = layers_module.Input(shape=(1,))\n     y = layers_module.Dense(1, kernel_initializer='ones')(x)\n@@ -3347,7 +3352,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     inputs = np.ones(shape=(10, 1))\n     targets = np.ones(shape=(10, 1))\n@@ -3367,7 +3372,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n     model.train_on_batch(inputs, targets)\n     model.test_on_batch(inputs, targets)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_add_metric_in_model_call(self):\n \n     class TestModel(training_module.Model):\n@@ -3389,7 +3394,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n     model.compile(\n         loss='mse',\n         optimizer=RMSPropOptimizer(0.01),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x = np.ones(shape=(10, 1))\n     y = np.ones(shape=(10, 2))\n@@ -3407,8 +3412,8 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n     model.train_on_batch(x, y)\n     model.test_on_batch(x, y)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_add_metric_in_layer_call(self):\n \n     class TestLayer(layers_module.Layer):\n@@ -3427,11 +3432,11 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n         TestLayer(input_shape=(1,)),\n         layers_module.Dense(2, kernel_initializer='ones')\n     ]\n-    model = testing_utils.get_model_from_layers(layers, input_shape=(1,))\n+    model = test_utils.get_model_from_layers(layers, input_shape=(1,))\n     model.compile(\n         loss='mse',\n         optimizer=RMSPropOptimizer(0.01),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x = np.ones(shape=(10, 1))\n     y = np.ones(shape=(10, 2))\n@@ -3439,7 +3444,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n     self.assertEqual(history.history['metric_1'][-1], 5)\n     self.assertAlmostEqual(history.history['val_metric_1'][-1], 5, 0)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_model_metrics_list(self):\n \n     class LayerWithAddMetric(layers_module.Layer):\n@@ -3490,7 +3495,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n         'sgd',\n         loss='mse',\n         metrics=[metrics_module.Accuracy('metric_4')],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     model.fit(np.ones((10, 1)), np.ones((10, 1)), batch_size=10)\n \n@@ -3499,7 +3504,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n     self.assertEqual([m.name for m in model.metrics],\n                      ['loss', 'metric_4', 'metric_2', 'metric_1', 'metric_3'])\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_model_metrics_list_in_call(self):\n \n     class TestModel(training_module.Model):\n@@ -3518,7 +3523,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n         loss='mse',\n         optimizer=RMSPropOptimizer(0.01),\n         metrics=[metrics_module.Accuracy('acc')],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     x = np.ones(shape=(10, 1))\n     y = np.ones(shape=(10, 2))\n     model.fit(x, y, epochs=2, batch_size=5, validation_data=(x, y))\n@@ -3526,7 +3531,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n     self.assertEqual([m.name for m in model.metrics],\n                      ['loss', 'acc', 'metric_1'])\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_multiple_add_metric_calls(self):\n \n     class TestModel(training_module.Model):\n@@ -3550,7 +3555,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n     model.compile(\n         loss='mse',\n         optimizer=RMSPropOptimizer(0.01),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x = np.ones(shape=(10, 1))\n     y = np.ones(shape=(10, 2))\n@@ -3566,7 +3571,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n     model.train_on_batch(x, y)\n     model.test_on_batch(x, y)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_multiple_add_metric_calls_layer(self):\n \n     class TestLayer(layers_module.Layer):\n@@ -3601,7 +3606,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n     self.assertListEqual([m.name for m in layer.metrics],\n                          ['m_1', 'm_2', 'm_3', 'm_4', 'm_5', 'm_6'])\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_duplicate_metric_name_in_add_metric(self):\n \n     class TestModel(training_module.Model):\n@@ -3620,7 +3625,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n     model.compile(\n         loss='mse',\n         optimizer=RMSPropOptimizer(0.01),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x = np.ones(shape=(10, 1))\n     y = np.ones(shape=(10, 2))\n@@ -3630,7 +3635,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n         'We found 2 metrics with the name: \"metric_1\"'):\n       model.fit(x, y, epochs=2, batch_size=5, validation_data=(x, y))\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_add_metric_without_name(self):\n \n     class TestModel(training_module.Model):\n@@ -3647,7 +3652,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n     model.compile(\n         loss='mse',\n         optimizer=RMSPropOptimizer(0.01),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     x = np.ones(shape=(10, 1))\n     y = np.ones(shape=(10, 2))\n \n@@ -3655,7 +3660,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n                                 'Please provide a name for your metric like'):\n       model.fit(x, y, epochs=2, batch_size=5, validation_data=(x, y))\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_add_metric_correctness(self):\n     inputs = input_layer.Input(shape=(1,))\n     targets = input_layer.Input(shape=(1,))\n@@ -3684,7 +3689,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n         loss='mae',\n         optimizer=optimizer_v2.gradient_descent.SGD(0.1),\n         metrics=[metrics_module.MeanAbsoluteError(name='mae_3')],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x = np.array([[0.], [1.], [2.]])\n     y = np.array([[0.5], [2.], [3.5]])\n@@ -3694,7 +3699,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n     for key in ['loss', 'mae_1', 'mae_2', 'mae_3']:\n       self.assertAllClose(history.history[key], expected_val, 1e-3)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_add_metric_order(self):\n \n     class MyLayer(layers_module.Layer):\n@@ -3726,7 +3731,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(dataset_train, epochs=3)\n     self.assertDictEqual(\n         history.history, {\n@@ -3736,7 +3741,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n             'one': [1.0, 1.0, 1.0]\n         })\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_add_metric_aggregation_mean(self):\n \n     class TestModel(training_module.Model):\n@@ -3752,10 +3757,10 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n \n     model = TestModel()\n     model.compile(\n-        'rmsprop', 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+        'rmsprop', 'mse', run_eagerly=test_utils.should_run_eagerly())\n     model.fit(np.ones(shape=(10, 1)), np.ones(shape=(10, 2)), batch_size=5)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_add_metric_aggregation_none(self):\n \n     class TestModel(training_module.Model):\n@@ -3771,10 +3776,10 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n \n     model = TestModel()\n     model.compile(\n-        'rmsprop', 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+        'rmsprop', 'mse', run_eagerly=test_utils.should_run_eagerly())\n     model.fit(np.ones(shape=(10, 1)), np.ones(shape=(10, 2)), batch_size=5)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def DISABLED_test_add_metric_invalid_aggregation(self):\n     # TODO(psv): Re-enable test once it is fixed.\n     x = layers_module.Input(shape=(1,))\n@@ -3790,7 +3795,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n       model.add_metric(\n           tf.reduce_sum(y), name='metric_1', aggregation=None)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_calling_evaluate_in_callback_during_fit(self):\n     # Check fix for a bug that caused `evaluate` to hit a cached dataset\n     # when run from inside a fit callback.\n@@ -3806,7 +3811,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n         (ones, ones)).batch(5)\n     val_ds_2 = tf.data.Dataset.from_tensor_slices(\n         (zeros, zeros)).batch(5)\n-    model.compile('sgd', 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+    model.compile('sgd', 'mse', run_eagerly=test_utils.should_run_eagerly())\n \n     class MyCallback(Callback):\n \n@@ -3825,7 +3830,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n     eval_result = model.evaluate(val_ds_2)\n     self.assertLess(abs(eval_result), 1e-7)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_model_with_nested_compiled_model(self):\n \n     class LayerWithAddMetric(layers_module.Layer):\n@@ -3851,7 +3856,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n         'sgd',\n         loss='mse',\n         metrics=[metrics_module.Accuracy('acc')],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     inner_model.fit(np.ones((10, 1)), np.ones((10, 1)), batch_size=10)\n \n     self.assertEqual([m.name for m in inner_model.metrics],\n@@ -3867,12 +3872,12 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n         'sgd',\n         loss='mse',\n         metrics=[metrics_module.Accuracy('acc2')],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     outer_model.fit(np.ones((10, 1)), np.ones((10, 1)), batch_size=10)\n     self.assertEqual([m.name for m in outer_model.metrics],\n                      ['loss', 'acc2', 'mean', 'mean1', 'mean2'])\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_model_with_metric_class_that_returns_dict(self):\n     x = layers_module.Input(shape=(2,))\n     y = layers_module.Dense(3)(x)\n@@ -3903,7 +3908,7 @@ class TestTrainingWithMetrics(keras_parameterized.TestCase):\n     model.compile('sgd',\n                   'mse',\n                   metrics=['mae', DictMetric()],\n-                  run_eagerly=testing_utils.should_run_eagerly())\n+                  run_eagerly=test_utils.should_run_eagerly())\n \n     history = model.fit(np.ones((10, 2)), np.ones((10, 3)))\n     self.assertEqual(list(history.history.keys()),\n@@ -3995,10 +4000,10 @@ class SubgraphUpdateLayer(layers_module.Layer):\n     return inputs\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class TestAutoUpdates(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class TestAutoUpdates(test_combinations.TestCase):\n \n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_with_all_model_types\n   @parameterized.named_parameters(\n       ('bare_update', BareUpdateLayer),\n       ('lambda_update', LambdaUpdateLayer),\n@@ -4006,45 +4011,45 @@ class TestAutoUpdates(keras_parameterized.TestCase):\n   def test_updates_in_model(self, layer_builder):\n     layer = layer_builder()\n     x, y = np.ones((10, 10)), np.ones((10, 1))\n-    model = testing_utils.get_model_from_layers(\n+    model = test_utils.get_model_from_layers(\n         [layer, layers_module.Dense(1)], input_shape=(10,))\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(x, y, batch_size=2, epochs=1)\n     self.assertEqual(self.evaluate(layer.counter), 5)\n \n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_with_all_model_types\n   def test_lambda_updates_trainable_false(self):\n     x, y = np.ones((10, 10)), np.ones((10, 1))\n     layer = LambdaUpdateLayer()\n-    model = testing_utils.get_model_from_layers(\n+    model = test_utils.get_model_from_layers(\n         [layer, layers_module.Dense(1)], input_shape=(10,))\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(x, y, batch_size=2, epochs=1)\n     self.assertEqual(self.evaluate(layer.counter), 5)\n     layer.trainable = False\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(x, y, batch_size=2, epochs=1)\n     self.assertEqual(self.evaluate(layer.counter), 5)\n \n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_with_all_model_types\n   def test_subgraph_updates_in_model(self):\n     layer = SubgraphUpdateLayer()\n     x, y = np.ones((10, 10)), np.ones((10, 1))\n-    model = testing_utils.get_model_from_layers(\n+    model = test_utils.get_model_from_layers(\n         [layer, layers_module.Dense(1)], input_shape=(10,))\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(x, y, batch_size=2, epochs=1)\n     self.assertEqual(self.evaluate(layer.counter), 5)\n \n@@ -4070,23 +4075,23 @@ class TestAutoUpdates(keras_parameterized.TestCase):\n     self.evaluate(y)\n     self.assertEqual(self.evaluate(layer.counter), 1)\n \n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_with_all_model_types\n   def test_batchnorm_trainable_false(self):\n     bn = layers_module.BatchNormalization()\n-    model = testing_utils.get_model_from_layers([bn, layers_module.Dense(1)],\n+    model = test_utils.get_model_from_layers([bn, layers_module.Dense(1)],\n                                              input_shape=(10,))\n     bn.trainable = False\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     x, y = np.ones((10, 10)), np.ones((10, 1))\n     model.fit(x, y, batch_size=2, epochs=1)\n     self.assertAllEqual(self.evaluate(bn.moving_mean), np.zeros((10,)))\n     self.assertAllEqual(self.evaluate(bn.moving_variance), np.ones((10,)))\n \n \n-class TestFunctionTracing(keras_parameterized.TestCase):\n+class TestFunctionTracing(test_combinations.TestCase):\n \n   def _seq_model_and_data(self):\n     model = sequential.Sequential([layers_module.Dense(4, activation='relu')])\n@@ -4095,7 +4100,7 @@ class TestFunctionTracing(keras_parameterized.TestCase):\n     y = np.random.random((10, 4))\n     return model, x, y\n \n-  @keras_parameterized.run_all_keras_modes(\n+  @test_combinations.run_all_keras_modes(\n       always_skip_v1=True, always_skip_eager=True)\n   def test_no_tracing_between_epoch(self):\n     if _is_oss():\n@@ -4110,7 +4115,7 @@ class TestFunctionTracing(keras_parameterized.TestCase):\n     new_func_graph = 'INFO:absl:Creating new FuncGraph for Python function'\n     self.assertEqual(sum(new_func_graph in log for log in logs.output), 9)\n \n-  @keras_parameterized.run_all_keras_modes(\n+  @test_combinations.run_all_keras_modes(\n       always_skip_v1=True, always_skip_eager=True)\n   def test_evaluate_no_cached_data(self):\n     if _is_oss():\n@@ -4126,9 +4131,9 @@ class TestFunctionTracing(keras_parameterized.TestCase):\n     self.assertEqual(sum(new_func_graph in log for log in eval_logs.output), 20)\n \n \n-class TestBuildCustomModel(keras_parameterized.TestCase):\n+class TestBuildCustomModel(test_combinations.TestCase):\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_build_list_of_inputs(self):\n \n     class MyModel(training_module.Model):\n@@ -4153,7 +4158,7 @@ class TestBuildCustomModel(keras_parameterized.TestCase):\n     self.assertEqual(model.l1.kernel.shape.as_list(), [1, 1])\n     self.assertEqual(model.l2.kernel.shape.as_list(), [2, 2])\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_build_single_inputs(self):\n \n     class MyModel(training_module.Model):\n@@ -4172,7 +4177,7 @@ class TestBuildCustomModel(keras_parameterized.TestCase):\n     model.build([None, 1])\n     self.assertEqual(model.l1.kernel.shape.as_list(), [1, 1])\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_build_dict_inputs(self):\n \n     class MyModel(training_module.Model):\n@@ -4213,9 +4218,9 @@ class TestBuildCustomModel(keras_parameterized.TestCase):\n     self.assertAllEqual(m1.get_weights(), m2.get_weights())\n \n \n-class ScalarDataModelTest(keras_parameterized.TestCase):\n+class ScalarDataModelTest(test_combinations.TestCase):\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_scalar_loss_reduction(self):\n \n     class MyModel(training_module.Model):\n\n@@ -23,8 +23,8 @@ import time\n from absl.testing import parameterized\n import numpy as np\n from keras import backend\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.engine import keras_tensor\n from keras.engine import training_utils_v1\n from tensorflow.python.platform import tf_logging as logging\n@@ -152,7 +152,7 @@ class DatasetUtilsTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertTrue(training_utils_v1.verify_dataset_shuffled(dataset))\n \n \n-class StandardizeWeightsTest(keras_parameterized.TestCase):\n+class StandardizeWeightsTest(test_combinations.TestCase):\n \n   def test_sample_weights(self):\n     y = np.array([0, 1, 0, 0, 2])\n@@ -177,7 +177,7 @@ class StandardizeWeightsTest(keras_parameterized.TestCase):\n     self.assertAllClose(weights, expected)\n \n   def test_dataset_with_class_weight(self):\n-    model = testing_utils.get_small_functional_mlp(1, 4, input_dim=3)\n+    model = test_utils.get_small_functional_mlp(1, 4, input_dim=3)\n     model.compile('rmsprop', 'mse')\n \n     inputs = np.zeros((10, 3), np.float32)\n@@ -235,7 +235,7 @@ _TEST_DATA = np.array((\n     (1, 0, 3, 3, 3, 2, 1, 2, 3, 1),))\n \n \n-class AggregationTest(keras_parameterized.TestCase):\n+class AggregationTest(test_combinations.TestCase):\n \n   def setUp(self):\n     super(AggregationTest, self).setUp()\n@@ -354,7 +354,7 @@ class AggregationTest(keras_parameterized.TestCase):\n       self._run_without_steps()\n \n \n-class CompositeTensorTestUtils(keras_parameterized.TestCase):\n+class CompositeTensorTestUtils(test_combinations.TestCase):\n \n   def test_is_composite(self):\n     # Validate that all composite tensor and value types return true.\n\n@@ -23,9 +23,8 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n import numpy as np\n from tensorflow.python.eager import backprop\n-from tensorflow.python.framework import test_util\n-from keras import combinations\n-from keras import keras_parameterized\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n+from keras.testing_infra import test_combinations\n from keras.feature_column import dense_features as df\n \n \n@@ -36,16 +35,17 @@ def _initialized_session(config=None):\n   return sess\n \n \n-class DenseFeaturesTest(keras_parameterized.TestCase):\n+class DenseFeaturesTest(test_combinations.TestCase):\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_retrieving_input(self):\n     features = {'a': [0.]}\n     dense_features = df.DenseFeatures(tf.feature_column.numeric_column('a'))\n     inputs = self.evaluate(dense_features(features))\n     self.assertAllClose([[0.]], inputs)\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def test_reuses_variables(self):\n     sparse_input = tf.SparseTensor(\n         indices=((0, 0), (1, 0), (2, 0)), values=(0, 1, 2), dense_shape=(3, 3))\n@@ -88,7 +88,7 @@ class DenseFeaturesTest(keras_parameterized.TestCase):\n     self.assertEqual(1, len(variables))\n     self.assertIs(variables[0], dense_features.variables[0])\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def test_dense_feature_with_partitioner(self):\n     sparse_input = tf.SparseTensor(\n         indices=((0, 0), (1, 0), (2, 0), (3, 0)),\n@@ -139,7 +139,7 @@ class DenseFeaturesTest(keras_parameterized.TestCase):\n     self.assertIs(variables[0], dense_features.variables[0])\n     self.assertIs(variables[1], dense_features.variables[1])\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def test_feature_column_dense_features_gradient(self):\n     sparse_input = tf.SparseTensor(\n         indices=((0, 0), (1, 0), (2, 0)), values=(0, 1, 2), dense_shape=(3, 3))\n@@ -444,7 +444,7 @@ class DenseFeaturesTest(keras_parameterized.TestCase):\n               tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)\n       ])\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def test_multiple_layers_with_same_shared_embedding_column(self):\n     categorical_column_a = tf.feature_column.categorical_column_with_identity(\n         key='aaa', num_buckets=3)\n@@ -482,7 +482,7 @@ class DenseFeaturesTest(keras_parameterized.TestCase):\n               tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)\n       ])\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def test_multiple_layers_with_same_shared_embedding_column_diff_graphs(self):\n     categorical_column_a = tf.feature_column.categorical_column_with_identity(\n         key='aaa', num_buckets=3)\n@@ -541,7 +541,7 @@ class DenseFeaturesTest(keras_parameterized.TestCase):\n               tf.compat.v1.GraphKeys.GLOBAL_VARIABLES)\n       ])\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def test_with_1d_sparse_tensor(self):\n     embedding_values = (\n         (1., 2., 3., 4., 5.),  # id 0\n@@ -598,7 +598,7 @@ class DenseFeaturesTest(keras_parameterized.TestCase):\n                            [1., 0., 0., 1., 2., 3., 4., 5., 12.]],\n                           sess.run(net))\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def test_with_1d_unknown_shape_sparse_tensor(self):\n     embedding_values = (\n         (1., 2.),  # id 0\n@@ -657,7 +657,7 @@ class DenseFeaturesTest(keras_parameterized.TestCase):\n                   features['country']: country_data\n               }))\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def test_with_rank_0_feature(self):\n     # price has 1 dimension in dense_features\n     price = tf.feature_column.numeric_column('price')\n@@ -683,7 +683,7 @@ class DenseFeaturesTest(keras_parameterized.TestCase):\n \n class IndicatorColumnTest(tf.test.TestCase):\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def test_dense_features(self):\n     animal = tf.feature_column.indicator_column(\n         tf.feature_column.categorical_column_with_identity(\n@@ -722,7 +722,7 @@ class EmbeddingColumnTest(tf.test.TestCase, parameterized.TestCase):\n           'use_safe_embedding_lookup': False,\n           'partition_variables': True,\n       })\n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def test_dense_features(self, use_safe_embedding_lookup, partition_variables):\n     # Inputs.\n     vocabulary_size = 4\n@@ -826,7 +826,7 @@ class EmbeddingColumnTest(tf.test.TestCase, parameterized.TestCase):\n           'SparseFillEmptyRows',\n           [x.type for x in tf.compat.v1.get_default_graph().get_operations()])\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def test_dense_features_not_trainable(self):\n     # Inputs.\n     vocabulary_size = 3\n@@ -1014,16 +1014,16 @@ class SharedEmbeddingColumnTest(tf.test.TestCase, parameterized.TestCase):\n                         self.evaluate(shared_embedding_vars[0]))\n     self.assertAllEqual(expected_lookups, self.evaluate(dense_features))\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def test_dense_features(self):\n     self._test_dense_features()\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def test_dense_features_no_trainable(self):\n     self._test_dense_features(trainable=False)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class DenseFeaturesSerializationTest(tf.test.TestCase, parameterized.TestCase):\n \n   @parameterized.named_parameters(('trainable', True, 'trainable'),\n@@ -1093,7 +1093,7 @@ class DenseFeaturesSerializationTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(new_layer._feature_columns[0].name, 'a_X_b_indicator')\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class SequenceFeatureColumnsTest(tf.test.TestCase):\n   \"\"\"Tests DenseFeatures with sequence feature columns.\"\"\"\n \n\n@@ -22,8 +22,7 @@ import tensorflow.compat.v2 as tf\n \n import numpy as np\n from tensorflow.python.eager import backprop\n-from keras import combinations\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras.feature_column import dense_features_v2 as df\n \n \n@@ -34,16 +33,17 @@ def _initialized_session(config=None):\n   return sess\n \n \n-class DenseFeaturesTest(keras_parameterized.TestCase):\n+class DenseFeaturesTest(test_combinations.TestCase):\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_retrieving_input(self):\n     features = {'a': [0.]}\n     dense_features = df.DenseFeatures(tf.feature_column.numeric_column('a'))\n     inputs = self.evaluate(dense_features(features))\n     self.assertAllClose([[0.]], inputs)\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def test_reuses_variables(self):\n     sparse_input = tf.SparseTensor(\n         indices=((0, 0), (1, 0), (2, 0)),\n@@ -88,7 +88,7 @@ class DenseFeaturesTest(keras_parameterized.TestCase):\n     self.assertEqual(1, len(variables))\n     self.assertIs(variables[0], dense_features.variables[0])\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def test_feature_column_dense_features_gradient(self):\n     sparse_input = tf.SparseTensor(\n         indices=((0, 0), (1, 0), (2, 0)),\n\n@@ -25,7 +25,7 @@ from google.protobuf import text_format\n \n from tensorflow.core.example import example_pb2\n from tensorflow.core.example import feature_pb2\n-from tensorflow.python.framework import test_util\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n from keras import backend\n from keras.feature_column import dense_features\n from keras.feature_column import sequence_feature_column as ksfc\n@@ -106,7 +106,7 @@ class SequenceFeatureColumnIntegrationTest(tf.test.TestCase):\n       output_r = sess.run(output)\n       self.assertAllEqual(output_r.shape, [20, 10])\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def test_shared_sequence_non_sequence_into_input_layer(self):\n     non_seq = tf.feature_column.categorical_column_with_identity('non_seq',\n                                                   num_buckets=10)\n\n@@ -25,7 +25,7 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import combinations\n+from keras.testing_infra import test_combinations\n from keras.feature_column import sequence_feature_column as ksfc\n from keras.saving import model_config\n \n@@ -37,7 +37,7 @@ def _initialized_session(config=None):\n   return sess\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class SequenceFeaturesTest(tf.test.TestCase, parameterized.TestCase):\n \n   @parameterized.named_parameters(\n@@ -560,7 +560,7 @@ class SequenceFeaturesTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllClose([2, 1, 1, 1], self.evaluate(seq_len))\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class SequenceFeaturesSerializationTest(tf.test.TestCase, parameterized.TestCase):\n \n   @parameterized.named_parameters(('trainable', True, 'trainable'),\n@@ -602,7 +602,8 @@ class SequenceFeaturesSerializationTest(tf.test.TestCase, parameterized.TestCase\n \n class SequenceFeaturesSavingTest(tf.test.TestCase, parameterized.TestCase):\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_saving_with_sequence_features(self):\n     cols = [\n         tf.feature_column.sequence_numeric_column('a'),\n\n@@ -19,10 +19,10 @@ import tensorflow.compat.v2 as tf\n import numpy as np\n \n from keras import backend\n-from keras import combinations\n+from keras.testing_infra import test_combinations\n from keras import initializers\n from keras import models\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.engine import input_layer\n from keras.layers import core\n \n@@ -54,7 +54,7 @@ def _compute_fans(shape):\n   return int(fan_in), int(fan_out)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class KerasInitializersTest(tf.test.TestCase):\n \n   def _runner(self, init, shape, target_mean=None, target_std=None,\n@@ -240,7 +240,7 @@ class KerasInitializersTest(tf.test.TestCase):\n         model.get_config(), custom_objects={'my_initializer': my_initializer})\n     self.assertEqual(model2.layers[1].kernel_initializer, my_initializer)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_load_external_variance_scaling_v2(self):\n     external_serialized_json = {\n         'class_name': 'VarianceScaling',\n\n@@ -19,13 +19,13 @@ from __future__ import print_function\n import os\n import sys\n \n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n # `tf.print` message is only available in stderr in TF2, which this test checks.\n-@testing_utils.run_v2_only\n+@test_utils.run_v2_only\n class CustomObjectSavingTest(tf.test.TestCase):\n   \"\"\"Test for custom Keras object saving with `register_keras_serializable`.\"\"\"\n \n\n@@ -17,7 +17,7 @@ import gc\n \n import tensorflow.compat.v2 as tf\n \n-from tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n from tensorflow.python.platform import test as test_lib\n \n layers = tf.keras.layers\n@@ -139,7 +139,7 @@ def _train_with_recompute(n_steps):\n   return losses\n \n \n-@test_util.with_eager_op_as_function\n+@tf_test_utils.with_eager_op_as_function\n class GradientCheckpointTest(tf.test.TestCase):\n \n   def test_raises_oom_exception(self):\n@@ -149,7 +149,7 @@ class GradientCheckpointTest(tf.test.TestCase):\n       _train_no_recompute(1)\n     self.assertIsInstance(context.exception, tf.errors.ResourceExhaustedError)\n \n-  @test_util.disable_xla(\n+  @tf_test_utils.disable_xla(\n       'xla does not support searching for memory-limited solvers.')\n   def test_does_not_raise_oom_exception(self):\n     if not _limit_gpu_memory():\n\n@@ -20,7 +20,7 @@ import os\n import random\n import tempfile\n from absl.testing import parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n import numpy as np\n import portpicker\n import tensorflow.compat.v2 as tf\n@@ -67,7 +67,7 @@ def create_in_process_cluster(num_workers, num_ps):\n   return cluster_spec\n \n \n-@testing_utils.run_v2_only\n+@test_utils.run_v2_only\n class KPLTest(tf.test.TestCase, parameterized.TestCase):\n \n   def setUp(self):\n@@ -264,7 +264,7 @@ class KPLTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertIn(prediction1, (\"yes\", \"no\"))\n \n \n-@testing_utils.run_v2_only\n+@test_utils.run_v2_only\n class KPLCreatedInDatasetsFromFunctionTest(tf.test.TestCase,\n                                            parameterized.TestCase):\n \n\n@@ -20,7 +20,7 @@ import tempfile\n from absl import flags\n \n import tensorflow.compat.v2 as tf\n-from tensorflow.python.framework import test_util\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n \n FLAGS = flags.FLAGS\n flags.DEFINE_string(\"tpu\", \"\", \"Name of TPU to connect to.\")\n@@ -114,7 +114,8 @@ class TpuStrategyTest(tf.test.TestCase):\n         for i in dataset:\n           strategy.run(step_fn, args=(i,))\n \n-  @test_util.disable_mlir_bridge(\"TODO(b/168036682): Support dynamic padder\")\n+  @tf_test_utils.disable_mlir_bridge(\n+      \"TODO(b/168036682): Support dynamic padder\")\n   def test_train_and_serve(self):\n     if not tf.compat.v1.executing_eagerly():\n       self.skipTest(\"connect_to_cluster() can only be called in eager mode\")\n\n@@ -15,17 +15,17 @@\n \"\"\"Tests for ELU layer.\"\"\"\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class ELUTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class ELUTest(test_combinations.TestCase):\n \n   def test_elu(self):\n     for alpha in [0., .5, -1.]:\n-      testing_utils.layer_test(keras.layers.ELU,\n+      test_utils.layer_test(keras.layers.ELU,\n                             kwargs={'alpha': alpha},\n                             input_shape=(2, 3, 4),\n                             supports_masking=True)\n@@ -35,7 +35,7 @@ class ELUTest(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(\n         ValueError, 'Alpha of an ELU layer cannot be None, '\n         'expecting a float. Received: None'):\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.ELU,\n           kwargs={'alpha': None},\n           input_shape=(2, 3, 4),\n\n@@ -15,17 +15,17 @@\n \"\"\"Tests for LeakyReLU layer.\"\"\"\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class LeakyReLUTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class LeakyReLUTest(test_combinations.TestCase):\n \n   def test_leaky_relu(self):\n     for alpha in [0., .5]:\n-      testing_utils.layer_test(keras.layers.LeakyReLU,\n+      test_utils.layer_test(keras.layers.LeakyReLU,\n                             kwargs={'alpha': alpha},\n                             input_shape=(2, 3, 4),\n                             supports_masking=True)\n@@ -35,7 +35,7 @@ class LeakyReLUTest(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(\n         ValueError, 'The alpha value of a Leaky ReLU layer '\n         'cannot be None. Expecting a float. Received: None'):\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.LeakyReLU,\n           kwargs={'alpha': None},\n           input_shape=(2, 3, 4),\n\n@@ -15,21 +15,21 @@\n \"\"\"Tests for PReLU layer.\"\"\"\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class PReLUTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class PReLUTest(test_combinations.TestCase):\n \n   def test_prelu(self):\n-    testing_utils.layer_test(keras.layers.PReLU, kwargs={},\n+    test_utils.layer_test(keras.layers.PReLU, kwargs={},\n                           input_shape=(2, 3, 4),\n                           supports_masking=True)\n \n   def test_prelu_share(self):\n-    testing_utils.layer_test(keras.layers.PReLU,\n+    test_utils.layer_test(keras.layers.PReLU,\n                           kwargs={'shared_axes': 1},\n                           input_shape=(2, 3, 4),\n                           supports_masking=True)\n\n@@ -15,17 +15,17 @@\n \"\"\"Tests for ReLU layer.\"\"\"\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class ReLUTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class ReLUTest(test_combinations.TestCase):\n \n   def test_relu(self):\n-    testing_utils.layer_test(keras.layers.ReLU,\n+    test_utils.layer_test(keras.layers.ReLU,\n                           kwargs={'max_value': 10},\n                           input_shape=(2, 3, 4),\n                           supports_masking=True)\n@@ -42,7 +42,7 @@ class ReLUTest(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(\n         ValueError, 'max_value of a ReLU layer cannot be a negative '\n         'value. Received: -10'):\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.ReLU,\n           kwargs={'max_value': -10},\n           input_shape=(2, 3, 4),\n@@ -52,7 +52,7 @@ class ReLUTest(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(\n         ValueError, 'negative_slope of a ReLU layer cannot be a negative '\n         'value. Received: None'):\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.ReLU,\n           kwargs={'negative_slope': None},\n           input_shape=(2, 3, 4),\n@@ -61,7 +61,7 @@ class ReLUTest(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(\n         ValueError, 'negative_slope of a ReLU layer cannot be a negative '\n         'value. Received: -10'):\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.ReLU,\n           kwargs={'negative_slope': -10},\n           input_shape=(2, 3, 4),\n@@ -71,7 +71,7 @@ class ReLUTest(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(\n         ValueError, 'threshold of a ReLU layer cannot be a negative '\n         'value. Received: None'):\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.ReLU,\n           kwargs={'threshold': None},\n           input_shape=(2, 3, 4),\n@@ -80,20 +80,20 @@ class ReLUTest(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(\n         ValueError, 'threshold of a ReLU layer cannot be a negative '\n         'value. Received: -10'):\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.ReLU,\n           kwargs={'threshold': -10},\n           input_shape=(2, 3, 4),\n           supports_masking=True)\n \n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_with_all_model_types\n   def test_relu_layer_as_activation(self):\n     layer = keras.layers.Dense(1, activation=keras.layers.ReLU())\n-    model = testing_utils.get_model_from_layers([layer], input_shape=(10,))\n+    model = test_utils.get_model_from_layers([layer], input_shape=(10,))\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(np.ones((10, 10)), np.ones((10, 1)), batch_size=2)\n \n \n\n@@ -15,16 +15,16 @@\n \"\"\"Tests for Softmax layer.\"\"\"\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class SoftmaxTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class SoftmaxTest(test_combinations.TestCase):\n \n   def test_softmax(self):\n-    testing_utils.layer_test(keras.layers.Softmax,\n+    test_utils.layer_test(keras.layers.Softmax,\n                           kwargs={'axis': 1},\n                           input_shape=(2, 3, 4),\n                           supports_masking=True)\n\n@@ -15,16 +15,16 @@\n \"\"\"Tests for ThresholdedReLU layer.\"\"\"\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class ThresholdedReLUTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class ThresholdedReLUTest(test_combinations.TestCase):\n \n   def test_thresholded_relu(self):\n-    testing_utils.layer_test(keras.layers.ThresholdedReLU,\n+    test_utils.layer_test(keras.layers.ThresholdedReLU,\n                           kwargs={'theta': 0.5},\n                           input_shape=(2, 3, 4),\n                           supports_masking=True)\n@@ -33,7 +33,7 @@ class ThresholdedReLUTest(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(\n         ValueError, 'Theta of a Thresholded ReLU layer cannot '\n         'be None, expecting a float. Received: None'):\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.ThresholdedReLU,\n           kwargs={'theta': None},\n           input_shape=(2, 3, 4),\n@@ -42,7 +42,7 @@ class ThresholdedReLUTest(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(\n         ValueError, 'The theta value of a Thresholded ReLU '\n         'layer should be >=0. Received: -10'):\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.ThresholdedReLU,\n           kwargs={'theta': -10},\n           input_shape=(2, 3, 4),\n\n@@ -16,14 +16,14 @@\n \n from absl.testing import parameterized\n import keras\n-from keras import combinations\n-from keras import testing_utils\n from keras.mixed_precision import policy\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class AdditiveAttentionTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_calculate_scores_one_dim(self):\n@@ -263,7 +263,7 @@ class AdditiveAttentionTest(tf.test.TestCase, parameterized.TestCase):\n     new_layer = keras.layers.AdditiveAttention.from_config(config)\n     self.assertEqual(new_layer.use_scale, True)\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_mixed_float16_policy(self):\n     # Test case for GitHub issue:\n     # https://github.com/tensorflow/tensorflow/issues/46064\n\n@@ -16,13 +16,13 @@\n \n from absl.testing import parameterized\n import keras\n-from keras import combinations\n from keras.layers import core\n+from keras.testing_infra import test_combinations\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class AttentionTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_calculate_scores_one_dim(self):\n\n@@ -15,14 +15,14 @@\n \"\"\"Tests BaseDenseAttention layer.\"\"\"\n \n from absl.testing import parameterized\n-from keras import combinations\n from keras.layers.attention.base_dense_attention import _lower_triangular_mask\n from keras.layers.attention.base_dense_attention import BaseDenseAttention\n+from keras.testing_infra import test_combinations\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class BaseDenseAttentionTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_one_dim_with_mask(self):\n@@ -148,7 +148,7 @@ class BaseDenseAttentionTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllEqual(expected_shape, tf.shape(actual))\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class LowerTriangularMaskTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_square_shape(self):\n\n@@ -16,15 +16,15 @@\n \n from absl.testing import parameterized\n import keras\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n # This decorator runs the test in V1, V2-Eager, and V2-Functional mode. It\n # guarantees forward compatibility of this code for the V2 switchover.\n-@keras_parameterized.run_all_keras_modes\n-class MultiHeadAttentionTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class MultiHeadAttentionTest(test_combinations.TestCase):\n \n   @parameterized.named_parameters(\n       (\"key_value_same_proj\", None, None, [40, 80]),\n@@ -249,8 +249,8 @@ class SubclassAttention(keras.layers.MultiHeadAttention):\n     return value_tensor, None\n \n \n-@keras_parameterized.run_all_keras_modes\n-class AttentionSubclassTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class AttentionSubclassTest(test_combinations.TestCase):\n \n   def test_initializer(self):\n     \"\"\"Test with a specified initializer.\"\"\"\n@@ -284,8 +284,8 @@ class TestModel(keras.Model):\n     return self.attention(x, x, training=training)\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class KerasModelSavingTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class KerasModelSavingTest(test_combinations.TestCase):\n \n   def test_keras_saving_subclass(self):\n     model = TestModel()\n\n@@ -17,16 +17,16 @@\n \n from absl.testing import parameterized\n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n-from tensorflow.python.framework import test_util\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n \n \n-@keras_parameterized.run_all_keras_modes\n-class Conv1DTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class Conv1DTest(test_combinations.TestCase):\n \n   def _run_test(self, kwargs, expected_output_shape):\n     num_samples = 2\n@@ -34,7 +34,7 @@ class Conv1DTest(keras_parameterized.TestCase):\n     length = 7\n \n     with self.cached_session():\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.Conv1D,\n           kwargs=kwargs,\n           input_shape=(num_samples, length, stack_size),\n@@ -49,7 +49,7 @@ class Conv1DTest(keras_parameterized.TestCase):\n       if expected_output_shape is not None:\n         expected_output_shape = (None,) + expected_output_shape\n \n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.Conv1D,\n           kwargs=kwargs,\n           input_shape=batch_shape + (length, stack_size),\n@@ -166,8 +166,8 @@ class Conv1DTest(keras_parameterized.TestCase):\n       layer.build((None, 5, 2))\n \n \n-@keras_parameterized.run_all_keras_modes\n-class Conv2DTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class Conv2DTest(test_combinations.TestCase):\n \n   def _run_test(self, kwargs, expected_output_shape, spatial_shape=(7, 6)):\n     num_samples = 2\n@@ -180,7 +180,7 @@ class Conv2DTest(keras_parameterized.TestCase):\n       input_data = 10 * np.random.random(input_data_shape).astype(np.float32)\n \n     with self.cached_session():\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.Conv2D,\n           kwargs=kwargs,\n           input_shape=(num_samples, num_row, num_col, stack_size),\n@@ -203,7 +203,7 @@ class Conv2DTest(keras_parameterized.TestCase):\n     with self.cached_session():\n       if expected_output_shape is not None:\n         expected_output_shape = (None,) + expected_output_shape\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.Conv2D,\n           kwargs=kwargs,\n           input_shape=batch_shape + (num_row, num_col, stack_size),\n@@ -303,8 +303,8 @@ class Conv2DTest(keras_parameterized.TestCase):\n       layer.build((None, 5, 5, 2))\n \n \n-@keras_parameterized.run_all_keras_modes\n-class Conv3DTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class Conv3DTest(test_combinations.TestCase):\n \n   def _run_test(self, kwargs, expected_output_shape, validate_training=True):\n     num_samples = 2\n@@ -314,7 +314,7 @@ class Conv3DTest(keras_parameterized.TestCase):\n     depth = 5\n \n     with self.cached_session():\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.Conv3D,\n           kwargs=kwargs,\n           input_shape=(num_samples, depth, num_row, num_col, stack_size),\n@@ -335,7 +335,7 @@ class Conv3DTest(keras_parameterized.TestCase):\n       if expected_output_shape is not None:\n         expected_output_shape = (None,) + expected_output_shape\n \n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.Conv3D,\n           kwargs=kwargs,\n           input_shape=batch_shape + (depth, num_row, num_col, stack_size),\n@@ -369,7 +369,7 @@ class Conv3DTest(keras_parameterized.TestCase):\n     kwargs['filters'] = kwargs.get('filters', 2)\n     kwargs['kernel_size'] = (3, 3, 3)\n     # train_on_batch currently fails with XLA enabled on GPUs\n-    test_training = 'groups' not in kwargs or not test_util.is_xla_enabled()\n+    test_training = 'groups' not in kwargs or not tf_test_utils.is_xla_enabled()\n     if not requires_gpu or tf.test.is_gpu_available(cuda_only=True):\n       self._run_test(kwargs, expected_output_shape, test_training)\n       self._run_test_extra_batch_dim(kwargs, expected_output_shape,\n@@ -415,7 +415,7 @@ class Conv3DTest(keras_parameterized.TestCase):\n     input_data = np.random.random((1, 3, 3, 3, 3)).astype(np.float32)\n     with self.cached_session():\n       # Won't raise error here.\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.Conv3D,\n           kwargs={\n               'data_format': 'channels_last',\n@@ -425,7 +425,7 @@ class Conv3DTest(keras_parameterized.TestCase):\n           input_shape=(None, None, None, None, 3),\n           input_data=input_data)\n       if tf.test.is_gpu_available(cuda_only=True):\n-        testing_utils.layer_test(\n+        test_utils.layer_test(\n             keras.layers.Conv3D,\n             kwargs={\n                 'data_format': 'channels_first',\n@@ -443,8 +443,8 @@ class Conv3DTest(keras_parameterized.TestCase):\n       layer.build((None, 5, 5, 5, 2))\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class GroupedConvTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class GroupedConvTest(test_combinations.TestCase):\n \n   @parameterized.named_parameters(\n       ('Conv1D', keras.layers.Conv1D),\n@@ -464,7 +464,7 @@ class GroupedConvTest(keras_parameterized.TestCase):\n   )\n   def test_group_conv(self, layer_cls, input_shape):\n     if tf.test.is_gpu_available(cuda_only=True):\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         inputs = tf.random.uniform(shape=input_shape)\n \n         layer = layer_cls(16, 3, groups=4, use_bias=False)\n@@ -482,7 +482,7 @@ class GroupedConvTest(keras_parameterized.TestCase):\n \n   def test_group_conv_depthwise(self):\n     if tf.test.is_gpu_available(cuda_only=True):\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         inputs = tf.random.uniform(shape=(3, 27, 27, 32))\n \n         layer = keras.layers.Conv2D(32, 3, groups=32, use_bias=False)\n@@ -495,8 +495,8 @@ class GroupedConvTest(keras_parameterized.TestCase):\n         self.assertAllClose(layer(inputs), expected_outputs, rtol=1e-5)\n \n \n-@keras_parameterized.run_all_keras_modes\n-class ConvSequentialTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class ConvSequentialTest(test_combinations.TestCase):\n \n   def _run_test(self, conv_layer_cls, kwargs, input_shape1, input_shape2,\n                 expected_output_shape1, expected_output_shape2):\n\n@@ -16,22 +16,22 @@\n \n from absl.testing import parameterized\n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class Conv1DTransposeTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class Conv1DTransposeTest(test_combinations.TestCase):\n \n   def _run_test(self, kwargs, expected_output_shape):\n     num_samples = 2\n     stack_size = 3\n     num_col = 6\n \n-    with testing_utils.use_gpu():\n-      testing_utils.layer_test(\n+    with test_utils.use_gpu():\n+      test_utils.layer_test(\n           keras.layers.Conv1DTranspose,\n           kwargs=kwargs,\n           input_shape=(num_samples, num_col, stack_size),\n@@ -55,8 +55,8 @@ class Conv1DTransposeTest(keras_parameterized.TestCase):\n       self._run_test(kwargs, expected_output_shape)\n \n \n-@keras_parameterized.run_all_keras_modes\n-class Conv2DTransposeTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class Conv2DTransposeTest(test_combinations.TestCase):\n \n   def _run_test(self, kwargs):\n     num_samples = 2\n@@ -65,7 +65,7 @@ class Conv2DTransposeTest(keras_parameterized.TestCase):\n     num_col = 6\n \n     with self.cached_session():\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.Conv2DTranspose,\n           kwargs=kwargs,\n           input_shape=(num_samples, num_row, num_col, stack_size))\n@@ -121,7 +121,8 @@ class Conv2DTransposeTest(keras_parameterized.TestCase):\n       self.assertEqual(layer.bias.constraint, b_constraint)\n \n   def test_conv2d_transpose_dilation(self):\n-    testing_utils.layer_test(keras.layers.Conv2DTranspose,\n+    test_utils.layer_test(\n+        keras.layers.Conv2DTranspose,\n         kwargs={'filters': 2,\n                 'kernel_size': 3,\n                 'padding': 'same',\n@@ -137,7 +138,7 @@ class Conv2DTransposeTest(keras_parameterized.TestCase):\n         [192, 228, 192, 228],\n         [336, 372, 336, 372]\n     ]).reshape((1, 4, 4, 1))\n-    testing_utils.layer_test(keras.layers.Conv2DTranspose,\n+    test_utils.layer_test(keras.layers.Conv2DTranspose,\n                           input_data=input_data,\n                           kwargs={'filters': 1,\n                                   'kernel_size': 3,\n@@ -148,8 +149,8 @@ class Conv2DTransposeTest(keras_parameterized.TestCase):\n                           expected_output=expected_output)\n \n \n-@keras_parameterized.run_all_keras_modes\n-class Conv3DTransposeTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class Conv3DTransposeTest(test_combinations.TestCase):\n \n   def _run_test(self, kwargs, expected_output_shape):\n     num_samples = 2\n@@ -158,8 +159,8 @@ class Conv3DTransposeTest(keras_parameterized.TestCase):\n     num_col = 6\n     depth = 5\n \n-    with testing_utils.use_gpu():\n-      testing_utils.layer_test(\n+    with test_utils.use_gpu():\n+      test_utils.layer_test(\n           keras.layers.Conv3DTranspose,\n           kwargs=kwargs,\n           input_shape=(num_samples, depth, num_row, num_col, stack_size),\n@@ -233,7 +234,7 @@ class Conv3DTransposeTest(keras_parameterized.TestCase):\n     input_data = np.random.random((1, 3, 3, 3, 3)).astype(np.float32)\n     with self.cached_session():\n       # Won't raise error here.\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.Conv3DTranspose,\n           kwargs={\n               'data_format': 'channels_last',\n@@ -243,7 +244,7 @@ class Conv3DTransposeTest(keras_parameterized.TestCase):\n           input_shape=(None, None, None, None, 3),\n           input_data=input_data)\n       if tf.test.is_gpu_available(cuda_only=True):\n-        testing_utils.layer_test(\n+        test_utils.layer_test(\n             keras.layers.Conv3DTranspose,\n             kwargs={\n                 'data_format': 'channels_first',\n\n@@ -16,13 +16,13 @@\n \n from absl.testing import parameterized\n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class DepthwiseConv1DTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class DepthwiseConv1DTest(test_combinations.TestCase):\n \n   def _run_test(self, kwargs, expected_output_shape=None):\n     num_samples = 2\n@@ -30,7 +30,7 @@ class DepthwiseConv1DTest(keras_parameterized.TestCase):\n     num_row = 7\n \n     with self.cached_session():\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.DepthwiseConv1D,\n           kwargs=kwargs,\n           input_shape=(num_samples, num_row, stack_size),\n@@ -84,8 +84,8 @@ class DepthwiseConv1DTest(keras_parameterized.TestCase):\n     self._run_test(kwargs)\n \n \n-@keras_parameterized.run_all_keras_modes\n-class DepthwiseConv2DTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class DepthwiseConv2DTest(test_combinations.TestCase):\n \n   def _run_test(self, kwargs, expected_output_shape=None):\n     num_samples = 2\n@@ -94,7 +94,7 @@ class DepthwiseConv2DTest(keras_parameterized.TestCase):\n     num_col = 6\n \n     with self.cached_session():\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.DepthwiseConv2D,\n           kwargs=kwargs,\n           input_shape=(num_samples, num_row, num_col, stack_size),\n\n@@ -16,14 +16,14 @@\n \n from absl.testing import parameterized\n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class SeparableConv1DTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class SeparableConv1DTest(test_combinations.TestCase):\n \n   def _run_test(self, kwargs):\n     num_samples = 2\n@@ -31,7 +31,7 @@ class SeparableConv1DTest(keras_parameterized.TestCase):\n     length = 7\n \n     with self.cached_session():\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.SeparableConv1D,\n           kwargs=kwargs,\n           input_shape=(num_samples, length, stack_size))\n@@ -90,8 +90,8 @@ class SeparableConv1DTest(keras_parameterized.TestCase):\n       self.assertEqual(layer.bias.constraint, b_constraint)\n \n \n-@keras_parameterized.run_all_keras_modes\n-class SeparableConv2DTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class SeparableConv2DTest(test_combinations.TestCase):\n \n   def _run_test(self, kwargs):\n     num_samples = 2\n@@ -100,7 +100,7 @@ class SeparableConv2DTest(keras_parameterized.TestCase):\n     num_col = 6\n \n     with self.cached_session():\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.SeparableConv2D,\n           kwargs=kwargs,\n           input_shape=(num_samples, num_row, num_col, stack_size))\n\n@@ -20,15 +20,15 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n \n \n-@keras_parameterized.run_all_keras_modes\n-class ConvLSTM1DTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class ConvLSTM1DTest(test_combinations.TestCase):\n \n   @parameterized.named_parameters(\n-      *testing_utils.generate_combinations_with_testcase_name(\n+      *test_utils.generate_combinations_with_testcase_name(\n           data_format=['channels_first', 'channels_last'],\n           return_sequences=[True, False]))\n   def test_conv_lstm(self, data_format, return_sequences):\n@@ -68,7 +68,7 @@ class ConvLSTM1DTest(keras_parameterized.TestCase):\n     self.assertAllClose(keras.backend.eval(layer.states[0]), state, atol=1e-4)\n \n     # test for output shape:\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.ConvLSTM1D,\n         kwargs={\n             'data_format': data_format,\n@@ -80,11 +80,11 @@ class ConvLSTM1DTest(keras_parameterized.TestCase):\n         input_shape=inputs.shape)\n \n \n-@keras_parameterized.run_all_keras_modes\n-class ConvLSTM2DTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class ConvLSTM2DTest(test_combinations.TestCase):\n \n   @parameterized.named_parameters(\n-      *testing_utils.generate_combinations_with_testcase_name(\n+      *test_utils.generate_combinations_with_testcase_name(\n           data_format=['channels_first', 'channels_last'],\n           return_sequences=[True, False]))\n   def test_conv_lstm(self, data_format, return_sequences):\n@@ -125,7 +125,7 @@ class ConvLSTM2DTest(keras_parameterized.TestCase):\n     self.assertAllClose(keras.backend.eval(layer.states[0]), state, atol=1e-4)\n \n     # test for output shape:\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.ConvLSTM2D,\n         kwargs={'data_format': data_format,\n                 'return_sequences': return_sequences,\n@@ -224,7 +224,7 @@ class ConvLSTM2DTest(keras_parameterized.TestCase):\n   def test_conv_lstm_dropout(self):\n     # check dropout\n     with self.cached_session():\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.ConvLSTM2D,\n           kwargs={'data_format': 'channels_last',\n                   'return_sequences': False,\n@@ -276,7 +276,7 @@ class ConvLSTM2DTest(keras_parameterized.TestCase):\n \n     model.compile(\n         optimizer='sgd', loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     x_1 = np.random.rand(num_samples, sequence_len, 32, 32, 3)\n     x_2 = np.random.rand(num_samples, sequence_len, 32, 32, 4)\n     y = np.random.rand(num_samples, 32, 32, 1)\n@@ -285,11 +285,11 @@ class ConvLSTM2DTest(keras_parameterized.TestCase):\n     model.predict([x_1, x_2])\n \n \n-@keras_parameterized.run_all_keras_modes\n-class ConvLSTM3DTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class ConvLSTM3DTest(test_combinations.TestCase):\n \n   @parameterized.named_parameters(\n-      *testing_utils.generate_combinations_with_testcase_name(\n+      *test_utils.generate_combinations_with_testcase_name(\n           data_format=['channels_first', 'channels_last'],\n           return_sequences=[True, False]))\n   def test_conv_lstm(self, data_format, return_sequences):\n@@ -333,7 +333,7 @@ class ConvLSTM3DTest(keras_parameterized.TestCase):\n     self.assertAllClose(keras.backend.eval(layer.states[0]), state, atol=1e-4)\n \n     # test for output shape:\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.ConvLSTM3D,\n         kwargs={\n             'data_format': data_format,\n\n@@ -19,23 +19,23 @@ import textwrap\n \n import keras\n from keras import initializers\n-from keras import keras_parameterized\n-from keras import testing_utils\n from keras.layers import core\n from keras.mixed_precision import policy\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n \n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class DropoutLayersTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class DropoutLayersTest(test_combinations.TestCase):\n \n   def test_dropout(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.Dropout, kwargs={'rate': 0.5}, input_shape=(3, 2))\n \n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.Dropout,\n         kwargs={\n             'rate': 0.5,\n@@ -48,18 +48,18 @@ class DropoutLayersTest(keras_parameterized.TestCase):\n     self.assertEqual(True, dropout.supports_masking)\n \n   def test_spatial_dropout_1d(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.SpatialDropout1D,\n         kwargs={'rate': 0.5},\n         input_shape=(2, 3, 4))\n \n   def test_spatial_dropout_2d(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.SpatialDropout2D,\n         kwargs={'rate': 0.5},\n         input_shape=(2, 3, 4, 5))\n \n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.SpatialDropout2D,\n         kwargs={\n             'rate': 0.5,\n@@ -68,12 +68,12 @@ class DropoutLayersTest(keras_parameterized.TestCase):\n         input_shape=(2, 3, 4, 5))\n \n   def test_spatial_dropout_3d(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.SpatialDropout3D,\n         kwargs={'rate': 0.5},\n         input_shape=(2, 3, 4, 4, 5))\n \n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.SpatialDropout3D,\n         kwargs={\n             'rate': 0.5,\n@@ -130,16 +130,16 @@ class DropoutLayersTest(keras_parameterized.TestCase):\n       self.assertNotIn('dropout', name)\n \n \n-@keras_parameterized.run_all_keras_modes\n-class LambdaLayerTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class LambdaLayerTest(test_combinations.TestCase):\n \n   def test_lambda(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.Lambda,\n         kwargs={'function': lambda x: x + 1},\n         input_shape=(3, 2))\n \n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.Lambda,\n         kwargs={\n             'function': lambda x, a, b: x * a + b,\n@@ -313,10 +313,10 @@ class LambdaLayerTest(keras_parameterized.TestCase):\n     self.assertNotIn(self.__class__.__name__, dir(core))\n \n \n-class TestStatefulLambda(keras_parameterized.TestCase):\n+class TestStatefulLambda(test_combinations.TestCase):\n \n-  @keras_parameterized.run_all_keras_modes\n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n   def test_lambda_with_variable_in_model(self):\n     v = tf.Variable(1., trainable=True)\n \n@@ -331,18 +331,18 @@ class TestStatefulLambda(keras_parameterized.TestCase):\n     layer.v = v\n     self.assertLen(layer.trainable_weights, 1)\n \n-    model = testing_utils.get_model_from_layers([layer], input_shape=(10,))\n+    model = test_utils.get_model_from_layers([layer], input_shape=(10,))\n     model.compile(\n         keras.optimizers.optimizer_v2.gradient_descent.SGD(0.1),\n         'mae',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     x, y = np.ones((10, 10), 'float32'), 2 * np.ones((10, 10), 'float32')\n     model.fit(x, y, batch_size=2, epochs=2, validation_data=(x, y))\n     self.assertLen(model.trainable_weights, 1)\n     self.assertAllClose(keras.backend.get_value(model.trainable_weights[0]), 2.)\n \n-  @keras_parameterized.run_all_keras_modes\n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n   def test_creation_inside_lambda(self):\n \n     def lambda_fn(x):\n@@ -359,11 +359,11 @@ class TestStatefulLambda(keras_parameterized.TestCase):\n \n     with self.assertRaisesRegex(ValueError, expected_error):\n       layer = keras.layers.Lambda(lambda_fn, name='shift_and_scale')\n-      model = testing_utils.get_model_from_layers([layer], input_shape=(1,))\n+      model = test_utils.get_model_from_layers([layer], input_shape=(1,))\n       model(tf.ones((4, 1)))\n \n-  @keras_parameterized.run_all_keras_modes\n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n   def test_transitive_variable_creation(self):\n     dense = keras.layers.Dense(1, use_bias=False, kernel_initializer='ones')\n \n@@ -378,11 +378,11 @@ class TestStatefulLambda(keras_parameterized.TestCase):\n \n     with self.assertRaisesRegex(ValueError, expected_error):\n       layer = keras.layers.Lambda(bad_lambda_fn, name='bias_dense')\n-      model = testing_utils.get_model_from_layers([layer], input_shape=(1,))\n+      model = test_utils.get_model_from_layers([layer], input_shape=(1,))\n       model(tf.ones((4, 1)))\n \n-  @keras_parameterized.run_all_keras_modes\n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n   def test_warns_on_variable_capture(self):\n     v = tf.Variable(1., trainable=True)\n \n@@ -403,11 +403,11 @@ class TestStatefulLambda(keras_parameterized.TestCase):\n     layer._warn = patched_warn\n \n     with self.assertRaisesRegex(ValueError, expected_warning):\n-      model = testing_utils.get_model_from_layers([layer], input_shape=(1,))\n+      model = test_utils.get_model_from_layers([layer], input_shape=(1,))\n       model(tf.ones((4, 1)))\n \n-  @keras_parameterized.run_all_keras_modes\n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n   def test_lambda_skip_state_variable_from_initializer(self):\n     # Force the initializers to use the tf.random.Generator, which will contain\n     # the state variable.\n@@ -425,21 +425,21 @@ class TestStatefulLambda(keras_parameterized.TestCase):\n     layer = keras.layers.Lambda(lambda_fn)\n     layer.dense = dense\n \n-    model = testing_utils.get_model_from_layers([layer], input_shape=(10,))\n+    model = test_utils.get_model_from_layers([layer], input_shape=(10,))\n     model.compile(\n         keras.optimizers.optimizer_v2.gradient_descent.SGD(0.1),\n         'mae',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     x, y = np.ones((10, 10), 'float32'), 2 * np.ones((10, 10), 'float32')\n     model.fit(x, y, batch_size=2, epochs=2, validation_data=(x, y))\n     self.assertLen(model.trainable_weights, 1)\n \n \n-@keras_parameterized.run_all_keras_modes\n-class CoreLayersTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class CoreLayersTest(test_combinations.TestCase):\n \n   def test_masking(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.Masking, kwargs={}, input_shape=(3, 2, 3))\n \n   def test_keras_mask(self):\n@@ -470,28 +470,28 @@ class CoreLayersTest(keras_parameterized.TestCase):\n \n   def test_activation(self):\n     # with string argument\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.Activation,\n         kwargs={'activation': 'relu'},\n         input_shape=(3, 2))\n \n     # with function argument\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.Activation,\n         kwargs={'activation': keras.backend.relu},\n         input_shape=(3, 2))\n \n   def test_dense(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.Dense, kwargs={'units': 3}, input_shape=(3, 2))\n \n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.Dense, kwargs={'units': 3}, input_shape=(3, 4, 2))\n \n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.Dense, kwargs={'units': 3}, input_shape=(None, None, 2))\n \n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.Dense, kwargs={'units': 3}, input_shape=(3, 4, 5, 2))\n \n   def test_dense_output(self):\n@@ -625,8 +625,8 @@ class CoreLayersTest(keras_parameterized.TestCase):\n       layer.call(tf.ragged.constant([[1., 2], [3, 4, 5]]))\n \n \n-@keras_parameterized.run_all_keras_modes\n-class TFOpLambdaTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class TFOpLambdaTest(test_combinations.TestCase):\n \n   def test_non_tf_symbol(self):\n \n\n@@ -23,43 +23,42 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from tensorflow.python.framework import test_util\n-from keras import combinations\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.optimizers.optimizer_v2.rmsprop import RMSprop\n \n \n-@keras_parameterized.run_all_keras_modes\n-class CuDNNTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class CuDNNTest(test_combinations.TestCase):\n \n   @parameterized.named_parameters(\n-      *testing_utils.generate_combinations_with_testcase_name(\n+      *test_utils.generate_combinations_with_testcase_name(\n           layer_class=[keras.layers.CuDNNGRU, keras.layers.CuDNNLSTM],\n           return_sequences=[True, False]))\n-  @test_util.run_gpu_only\n+  @tf_test_utils.run_gpu_only\n   def test_cudnn_rnn_return_sequence(self, layer_class, return_sequences):\n     input_size = 10\n     timesteps = 6\n     units = 2\n     num_samples = 32\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         layer_class,\n         kwargs={'units': units,\n                 'return_sequences': return_sequences},\n         input_shape=(num_samples, timesteps, input_size))\n \n   @parameterized.named_parameters(\n-      *testing_utils.generate_combinations_with_testcase_name(\n+      *test_utils.generate_combinations_with_testcase_name(\n           layer_class=[keras.layers.CuDNNGRU, keras.layers.CuDNNLSTM],\n           go_backwards=[True, False]))\n-  @test_util.run_gpu_only\n+  @tf_test_utils.run_gpu_only\n   def test_cudnn_rnn_go_backward(self, layer_class, go_backwards):\n     input_size = 10\n     timesteps = 6\n     units = 2\n     num_samples = 32\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         layer_class,\n         kwargs={'units': units,\n                 'go_backwards': go_backwards},\n@@ -69,7 +68,7 @@ class CuDNNTest(keras_parameterized.TestCase):\n       ('cudnngru', keras.layers.CuDNNGRU),\n       ('cudnnlstm', keras.layers.CuDNNLSTM),\n   )\n-  @test_util.run_gpu_only\n+  @tf_test_utils.run_gpu_only\n   def test_return_state(self, layer_class):\n     input_size = 10\n     timesteps = 6\n@@ -83,7 +82,7 @@ class CuDNNTest(keras_parameterized.TestCase):\n     _, state = outputs[0], outputs[1:]\n     self.assertEqual(len(state), num_states)\n     model = keras.models.Model(inputs, state[0])\n-    model.run_eagerly = testing_utils.should_run_eagerly()\n+    model.run_eagerly = test_utils.should_run_eagerly()\n \n     inputs = np.random.random((num_samples, timesteps, input_size))\n     state = model.predict(inputs)\n@@ -94,7 +93,7 @@ class CuDNNTest(keras_parameterized.TestCase):\n       ('cudnngru', keras.layers.CuDNNGRU),\n       ('cudnnlstm', keras.layers.CuDNNLSTM),\n   )\n-  @test_util.run_gpu_only\n+  @tf_test_utils.run_gpu_only\n   def test_time_major_input(self, layer_class):\n     input_size = 10\n     timesteps = 6\n@@ -120,7 +119,7 @@ class CuDNNTest(keras_parameterized.TestCase):\n       ('cudnngru', keras.layers.CuDNNGRU),\n       ('cudnnlstm', keras.layers.CuDNNLSTM),\n   )\n-  @test_util.run_gpu_only\n+  @tf_test_utils.run_gpu_only\n   def test_specify_initial_state_keras_tensor(self, layer_class):\n     input_size = 10\n     timesteps = 6\n@@ -143,7 +142,7 @@ class CuDNNTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='categorical_crossentropy',\n         optimizer=RMSprop(learning_rate=0.001),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     inputs = np.random.random((num_samples, timesteps, input_size))\n     initial_state = [\n@@ -153,13 +152,13 @@ class CuDNNTest(keras_parameterized.TestCase):\n     model.fit([inputs] + initial_state, targets)\n \n \n-class CuDNNGraphOnlyTest(keras_parameterized.TestCase):\n+class CuDNNGraphOnlyTest(test_combinations.TestCase):\n \n   @parameterized.named_parameters(\n       ('cudnngru', keras.layers.CuDNNGRU),\n       ('cudnnlstm', keras.layers.CuDNNLSTM),\n   )\n-  @test_util.run_gpu_only\n+  @tf_test_utils.run_gpu_only\n   def test_regularizer(self, layer_class):\n     input_size = 10\n     timesteps = 6\n@@ -191,8 +190,8 @@ class CuDNNGraphOnlyTest(keras_parameterized.TestCase):\n       ('cudnngru', keras.layers.CuDNNGRU),\n       ('cudnnlstm', keras.layers.CuDNNLSTM),\n   )\n-  @test_util.run_gpu_only\n-  @test_util.run_v1_only('b/120941292')\n+  @tf_test_utils.run_gpu_only\n+  @tf_test_utils.run_v1_only('b/120941292')\n   def test_statefulness(self, layer_class):\n     input_size = 10\n     timesteps = 6\n@@ -239,10 +238,10 @@ class CuDNNGraphOnlyTest(keras_parameterized.TestCase):\n       self.assertNotEqual(out4.max(), out5.max())\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-class CuDNNV1OnlyTest(keras_parameterized.TestCase):\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n+class CuDNNV1OnlyTest(test_combinations.TestCase):\n \n-  @test_util.run_gpu_only\n+  @tf_test_utils.run_gpu_only\n   def test_trainability(self):\n     input_size = 10\n     units = 2\n@@ -262,12 +261,12 @@ class CuDNNV1OnlyTest(keras_parameterized.TestCase):\n       self.assertEqual(len(layer.non_trainable_weights), 0)\n \n   @parameterized.named_parameters(\n-      *testing_utils.generate_combinations_with_testcase_name(\n+      *test_utils.generate_combinations_with_testcase_name(\n           rnn_type=['LSTM', 'GRU'], to_cudnn=[True, False],\n           bidirectional=[True, False], implementation=[1, 2],\n           model_nest_level=[1, 2], model_type=['seq', 'func']))\n-  @test_util.run_v1_only('b/120911602, b/112083752')\n-  @test_util.run_gpu_only\n+  @tf_test_utils.run_v1_only('b/120911602, b/112083752')\n+  @tf_test_utils.run_gpu_only\n   def test_load_weights_between_noncudnn_rnn(self, rnn_type, to_cudnn,\n                                              bidirectional, implementation,\n                                              model_nest_level, model_type):\n@@ -345,10 +344,10 @@ class CuDNNV1OnlyTest(keras_parameterized.TestCase):\n     os.remove(fname)\n \n   @parameterized.named_parameters(\n-      *testing_utils.generate_combinations_with_testcase_name(\n+      *test_utils.generate_combinations_with_testcase_name(\n           rnn_type=['LSTM', 'GRU'], to_cudnn=[True, False]))\n-  @test_util.run_v1_only('b/120911602')\n-  @test_util.run_gpu_only\n+  @tf_test_utils.run_v1_only('b/120911602')\n+  @tf_test_utils.run_gpu_only\n   def test_load_weights_between_noncudnn_rnn_time_distributed(self, rnn_type,\n                                                               to_cudnn):\n     # Similar test as test_load_weights_between_noncudnn_rnn() but has different\n@@ -391,7 +390,7 @@ class CuDNNV1OnlyTest(keras_parameterized.TestCase):\n     self.assertAllClose(model.predict(inputs), cudnn_model.predict(inputs),\n                         atol=1e-4)\n \n-  @test_util.run_gpu_only\n+  @tf_test_utils.run_gpu_only\n   def test_cudnnrnn_bidirectional(self):\n     rnn = keras.layers.CuDNNGRU\n     samples = 2\n@@ -446,7 +445,7 @@ class CuDNNV1OnlyTest(keras_parameterized.TestCase):\n     model.compile(loss='mse', optimizer='rmsprop')\n     model.fit(x, y, epochs=1, batch_size=1)\n \n-  @test_util.run_gpu_only\n+  @tf_test_utils.run_gpu_only\n   def test_preprocess_weights_for_loading_gru_incompatible(self):\n     \"\"\"Test loading weights between incompatible layers.\n \n\n@@ -22,12 +22,12 @@ import numpy as np\n \n import keras\n \n-from keras import keras_parameterized  # pylint: disable=g-direct-tensorflow-import\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations  # pylint: disable=g-direct-tensorflow-import\n+from keras.testing_infra import test_utils\n from keras.layers import einsum_dense\n \n \n-@keras_parameterized.run_all_keras_modes\n+@test_combinations.run_all_keras_modes\n @parameterized.named_parameters(\n     {\n         \"testcase_name\": \"_1d_end_weight\",\n@@ -219,7 +219,7 @@ from keras.layers import einsum_dense\n         \"expected_bias_shape\": [3, 4, 1],\n         \"expected_output_shape\": (None, 3, 4, 2)\n     })\n-class TestEinsumDenseLayer(keras_parameterized.TestCase):\n+class TestEinsumDenseLayer(test_combinations.TestCase):\n \n   def test_weight_shapes(self, equation, bias_axes, input_shape, output_shape,\n                          expected_weight_shape, expected_bias_shape,\n@@ -251,8 +251,8 @@ class TestEinsumDenseLayer(keras_parameterized.TestCase):\n     self.assertAllEqual(expected_output_shape, output_tensor.shape.as_list())\n \n \n-@keras_parameterized.run_all_keras_modes\n-class TestEinsumLayerAPI(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class TestEinsumLayerAPI(test_combinations.TestCase):\n \n   def test_layer_api(self):\n     input_data = np.array([[1.0, 2.0], [3.0, 4.0]])\n@@ -267,7 +267,7 @@ class TestEinsumLayerAPI(keras_parameterized.TestCase):\n     expected_output = np.array([[1.53, 1.53, 1.53, 1.53],\n                                 [3.53, 3.53, 3.53, 3.53]])\n \n-    output_data = testing_utils.layer_test(\n+    output_data = test_utils.layer_test(\n         einsum_dense.EinsumDense,\n         kwargs=kwargs,\n         input_shape=(None, 2),\n\n@@ -16,22 +16,21 @@\n # pylint: disable=g-classes-have-attributes,g-direct-tensorflow-import\n \n import keras\n-from keras import combinations\n-from keras import keras_parameterized\n-from keras import testing_utils\n from keras.mixed_precision import policy\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-class EmbeddingTest(keras_parameterized.TestCase):\n+class EmbeddingTest(test_combinations.TestCase):\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_embedding(self):\n     if tf.test.is_gpu_available():\n       self.skipTest('Only test embedding on CPU.')\n \n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.Embedding,\n         kwargs={'output_dim': 4,\n                 'input_dim': 10,\n@@ -40,7 +39,7 @@ class EmbeddingTest(keras_parameterized.TestCase):\n         input_dtype='int32',\n         expected_output_dtype='float32')\n \n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.Embedding,\n         kwargs={'output_dim': 4,\n                 'input_dim': 10,\n@@ -49,7 +48,7 @@ class EmbeddingTest(keras_parameterized.TestCase):\n         input_dtype='int32',\n         expected_output_dtype='float32')\n \n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.Embedding,\n         kwargs={'output_dim': 4,\n                 'input_dim': 10,\n@@ -58,7 +57,7 @@ class EmbeddingTest(keras_parameterized.TestCase):\n         input_dtype='int32',\n         expected_output_dtype='float32')\n \n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.Embedding,\n         kwargs={'output_dim': 4,\n                 'input_dim': 10,\n@@ -68,13 +67,13 @@ class EmbeddingTest(keras_parameterized.TestCase):\n         input_dtype='int32',\n         expected_output_dtype='float32')\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_embedding_correctness(self):\n     layer = keras.layers.Embedding(output_dim=2, input_dim=2)\n     model = keras.models.Sequential([layer])\n \n     layer.set_weights([np.array([[1, 1], [2, 2]])])\n-    model.run_eagerly = testing_utils.should_run_eagerly()\n+    model.run_eagerly = test_utils.should_run_eagerly()\n     outputs = model.predict(np.array([[0, 1, 0]], dtype='int32'))\n     self.assertAllClose(outputs, [[[1, 1], [2, 2], [1, 1]]])\n \n@@ -85,7 +84,8 @@ class EmbeddingTest(keras_parameterized.TestCase):\n     with self.assertRaises(ValueError):\n       keras.layers.Embedding(input_dim=1, output_dim=0)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_eager_gpu_cpu(self):\n     l = keras.layers.Embedding(output_dim=2, input_dim=2)\n     l.build((None, 2))\n@@ -97,7 +97,7 @@ class EmbeddingTest(keras_parameterized.TestCase):\n     opt.apply_gradients(zip(gs, l.weights))\n     self.assertAllEqual(len(gs), 1)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_embedding_with_ragged_input(self):\n     layer = keras.layers.Embedding(\n         input_dim=3,\n@@ -112,7 +112,7 @@ class EmbeddingTest(keras_parameterized.TestCase):\n     outputs = layer(outputs)\n \n     model = keras.Model(inputs, outputs)\n-    model.run_eagerly = testing_utils.should_run_eagerly()\n+    model.run_eagerly = test_utils.should_run_eagerly()\n     outputs = model.predict(\n         tf.ragged.constant([[1., 2., 2.], [0.], [1., 2.]],\n                                     ragged_rank=1))\n@@ -122,7 +122,7 @@ class EmbeddingTest(keras_parameterized.TestCase):\n             [[[1., 1.], [2., 2.], [2., 2.]], [[0., 0.]], [[1., 1.], [2., 2.]]],\n             ragged_rank=1))\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_mixed_precision_embedding(self):\n     try:\n       policy.set_global_policy('mixed_float16')\n\n@@ -22,21 +22,20 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import combinations\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.utils import np_utils\n \n \n-@keras_parameterized.run_all_keras_modes\n-class GRULayerTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class GRULayerTest(test_combinations.TestCase):\n \n   def test_return_sequences_GRU(self):\n     num_samples = 2\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GRU,\n         kwargs={'units': units,\n                 'return_sequences': True},\n@@ -45,13 +44,13 @@ class GRULayerTest(keras_parameterized.TestCase):\n   @tf.test.disable_with_predicate(\n       pred=tf.test.is_built_with_rocm,\n       skip_message='Double type is not yet supported in ROCm')\n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_float64_GRU(self):\n     num_samples = 2\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GRU,\n         kwargs={'units': units,\n                 'return_sequences': True,\n@@ -70,7 +69,7 @@ class GRULayerTest(keras_parameterized.TestCase):\n     model.compile(\n         'rmsprop',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     x = np.random.random((num_samples, timesteps, embedding_dim))\n     y = np.random.random((num_samples, units))\n     model.train_on_batch(x, y)\n@@ -80,7 +79,7 @@ class GRULayerTest(keras_parameterized.TestCase):\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GRU,\n         kwargs={'units': units,\n                 'dropout': 0.1,\n@@ -98,7 +97,7 @@ class GRULayerTest(keras_parameterized.TestCase):\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GRU,\n         kwargs={'units': units,\n                 'implementation': implementation_mode},\n@@ -110,7 +109,7 @@ class GRULayerTest(keras_parameterized.TestCase):\n     embedding_dim = 4\n     units = 2\n \n-    (x_train, y_train), _ = testing_utils.get_test_data(\n+    (x_train, y_train), _ = test_utils.get_test_data(\n         train_samples=num_samples,\n         test_samples=0,\n         input_shape=(timesteps, embedding_dim),\n@@ -125,7 +124,7 @@ class GRULayerTest(keras_parameterized.TestCase):\n     gru_model.compile(\n         'rmsprop',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     gru_model.fit(x_train, y_train)\n     gru_model.predict(x_train)\n \n@@ -143,7 +142,7 @@ class GRULayerTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='categorical_crossentropy',\n         optimizer='rmsprop',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)\n \n   @tf.test.disable_with_predicate(\n@@ -170,7 +169,7 @@ class GRULayerTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     out1 = model.predict(np.ones((num_samples, timesteps)))\n     self.assertEqual(out1.shape, (num_samples, units))\n \n@@ -223,7 +222,7 @@ class GRULayerTest(keras_parameterized.TestCase):\n     self.assertEqual(state.shape, initial_state.shape)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class GRULayerGenericTest(tf.test.TestCase):\n \n   def test_constraints_GRU(self):\n\n@@ -25,9 +25,8 @@ import numpy as np\n from tensorflow.core.protobuf import rewriter_config_pb2\n import keras\n from tensorflow.python.framework import test_util as tf_test_util\n-from keras import combinations\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.layers import recurrent as rnn_v1\n from keras.layers import recurrent_v2 as rnn\n from keras.utils import np_utils\n@@ -41,9 +40,9 @@ _graph_options = tf.compat.v1.GraphOptions(rewrite_options=_rewrites)\n _config = tf.compat.v1.ConfigProto(graph_options=_graph_options)\n \n \n-@testing_utils.run_all_without_tensor_float_32('RNN GRU can use TF32 on GPU')\n-@keras_parameterized.run_all_keras_modes(config=_config)\n-class GRUV2Test(keras_parameterized.TestCase):\n+@test_utils.run_all_without_tensor_float_32('RNN GRU can use TF32 on GPU')\n+@test_combinations.run_all_keras_modes(config=_config)\n+class GRUV2Test(test_combinations.TestCase):\n \n   @parameterized.named_parameters(\n       ('non_tan_activation', 'relu', 'sigmoid', 0, False, True, True),\n@@ -65,7 +64,7 @@ class GRUV2Test(keras_parameterized.TestCase):\n                     reset_after=reset_after)\n     self.assertFalse(layer._could_use_gpu_kernel)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_use_on_default_activation_with_gpu_kernel(self):\n     layer = rnn.GRU(1, activation=tf.tanh)\n     self.assertTrue(layer._could_use_gpu_kernel)\n@@ -81,7 +80,7 @@ class GRUV2Test(keras_parameterized.TestCase):\n     batch = 100\n     epoch = 10\n \n-    (x_train, y_train), _ = testing_utils.get_test_data(\n+    (x_train, y_train), _ = test_utils.get_test_data(\n         train_samples=batch,\n         test_samples=0,\n         input_shape=(timestep, input_shape),\n@@ -135,14 +134,14 @@ class GRUV2Test(keras_parameterized.TestCase):\n   @tf.test.disable_with_predicate(\n       pred=tf.test.is_built_with_rocm,\n       skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_gru_v2_feature_parity_with_canonical_gru(self):\n     input_shape = 10\n     rnn_state_size = 8\n     timestep = 4\n     batch = 20\n \n-    (x_train, y_train), _ = testing_utils.get_test_data(\n+    (x_train, y_train), _ = test_utils.get_test_data(\n         train_samples=batch,\n         test_samples=0,\n         input_shape=(timestep, input_shape),\n@@ -168,7 +167,7 @@ class GRUV2Test(keras_parameterized.TestCase):\n     gru_model.fit(x_train, y_train)\n     y_2 = gru_model.predict(x_train)\n \n-    with testing_utils.device(should_use_gpu=True):\n+    with test_utils.device(should_use_gpu=True):\n       cudnn_layer = rnn.GRU(rnn_state_size,\n                             recurrent_activation='sigmoid',\n                             reset_after=True)\n@@ -231,14 +230,14 @@ class GRUV2Test(keras_parameterized.TestCase):\n \n     inputs = keras.layers.Input(\n         shape=[timestep, input_shape], dtype=tf.float32)\n-    with testing_utils.device(should_use_gpu=False):\n+    with test_utils.device(should_use_gpu=False):\n       layer = rnn.GRU(rnn_state_size)\n       output = layer(inputs)\n       cpu_model = keras.models.Model(inputs, output)\n       weights = cpu_model.get_weights()\n       y_1 = cpu_model.predict(x_train)\n \n-    with testing_utils.device(should_use_gpu=True):\n+    with test_utils.device(should_use_gpu=True):\n       layer = rnn.GRU(rnn_state_size)\n       output = layer(inputs)\n       gpu_model = keras.models.Model(inputs, output)\n@@ -248,7 +247,7 @@ class GRUV2Test(keras_parameterized.TestCase):\n     # Note that cuDNN uses 'sigmoid' as activation, so the GRU V2 uses\n     # 'sigmoid' as default. Construct the canonical GRU with sigmoid to achieve\n     # the same output.\n-    with testing_utils.device(should_use_gpu=True):\n+    with test_utils.device(should_use_gpu=True):\n       layer = rnn_v1.GRU(rnn_state_size,\n                          recurrent_activation='sigmoid',\n                          reset_after=True)\n@@ -340,7 +339,7 @@ class GRUV2Test(keras_parameterized.TestCase):\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         rnn.GRU,\n         kwargs={'units': units,\n                 'return_sequences': True},\n@@ -349,13 +348,13 @@ class GRUV2Test(keras_parameterized.TestCase):\n   @tf.test.disable_with_predicate(\n       pred=tf.test.is_built_with_rocm,\n       skip_message='Double type is not yet supported in ROCm')\n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_float64_GRU(self):\n     num_samples = 2\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         rnn.GRU,\n         kwargs={'units': units,\n                 'return_sequences': True,\n@@ -386,7 +385,7 @@ class GRUV2Test(keras_parameterized.TestCase):\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         rnn.GRU,\n         kwargs={'units': units,\n                 'dropout': 0.1,\n@@ -418,7 +417,7 @@ class GRUV2Test(keras_parameterized.TestCase):\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         rnn.GRU,\n         kwargs={'units': units,\n                 'implementation': implementation_mode},\n@@ -469,7 +468,7 @@ class GRUV2Test(keras_parameterized.TestCase):\n     model.compile(\n         optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     out1 = model.predict(np.ones((num_samples, timesteps)))\n     self.assertEqual(out1.shape, (num_samples, units))\n \n@@ -541,13 +540,13 @@ class GRUV2Test(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='adam',\n         loss='sparse_categorical_crossentropy',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(x, y, epochs=1, shuffle=False)\n \n   @tf.test.disable_with_predicate(\n       pred=tf.test.is_built_with_rocm,\n       skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_explicit_device_with_go_backward_and_mask(self):\n     batch_size = 8\n     timestep = 7\n@@ -560,14 +559,14 @@ class GRUV2Test(keras_parameterized.TestCase):\n \n     # Test for V1 behavior.\n     lstm_v1 = rnn_v1.GRU(units, return_sequences=True, go_backwards=True)\n-    with testing_utils.device(should_use_gpu=True):\n+    with test_utils.device(should_use_gpu=True):\n       outputs_masked_v1 = lstm_v1(inputs, mask=tf.constant(mask))\n       outputs_trimmed_v1 = lstm_v1(inputs[:, :masksteps])\n     self.assertAllClose(outputs_masked_v1[:, -masksteps:], outputs_trimmed_v1)\n \n     # Test for V2 behavior.\n     lstm = rnn.GRU(units, return_sequences=True, go_backwards=True)\n-    with testing_utils.device(should_use_gpu=True):\n+    with test_utils.device(should_use_gpu=True):\n       outputs_masked = lstm(inputs, mask=tf.constant(mask))\n       outputs_trimmed = lstm(inputs[:, :masksteps])\n     self.assertAllClose(outputs_masked[:, -masksteps:], outputs_trimmed)\n@@ -614,7 +613,7 @@ class GRUV2Test(keras_parameterized.TestCase):\n     model.compile(\n         optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     # Make sure it doesn't crash with cudnn kernel.\n     model.predict(inputs)\n \n@@ -644,10 +643,10 @@ class GRUV2Test(keras_parameterized.TestCase):\n     self.assertAllClose(self.evaluate(outputs), self.evaluate(copied_outputs))\n \n \n-@testing_utils.run_all_without_tensor_float_32('RNN GRU can use TF32 on GPU')\n-class GRULayerGradientTapeTest(keras_parameterized.TestCase):\n+@test_utils.run_all_without_tensor_float_32('RNN GRU can use TF32 on GPU')\n+class GRULayerGradientTapeTest(test_combinations.TestCase):\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def test_in_tape(self):\n     with self.test_session(config=_config):\n       time_steps = 10\n@@ -672,9 +671,9 @@ class GRULayerGradientTapeTest(keras_parameterized.TestCase):\n       tape.gradient(loss, gru.variables)\n \n \n-@testing_utils.run_all_without_tensor_float_32('RNN GRU can use TF32 on GPU')\n-@keras_parameterized.run_all_keras_modes(config=_config)\n-class GRUGraphRewriteTest(keras_parameterized.TestCase):\n+@test_utils.run_all_without_tensor_float_32('RNN GRU can use TF32 on GPU')\n+@test_combinations.run_all_keras_modes(config=_config)\n+class GRUGraphRewriteTest(test_combinations.TestCase):\n \n   input_shape = 10\n   output_shape = 8\n@@ -684,7 +683,7 @@ class GRUGraphRewriteTest(keras_parameterized.TestCase):\n   epoch = 1\n \n   def _test_runtime_with_model(self, model):\n-    (x_train, y_train), _ = testing_utils.get_test_data(\n+    (x_train, y_train), _ = test_utils.get_test_data(\n         train_samples=self.batch,\n         test_samples=0,\n         input_shape=(self.timestep, self.input_shape),\n@@ -709,7 +708,7 @@ class GRUGraphRewriteTest(keras_parameterized.TestCase):\n     else:\n       self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_GRU_runtime(self):\n     layer = rnn.GRU(self.rnn_state_size, return_runtime=True)\n \n@@ -728,7 +727,7 @@ class GRUGraphRewriteTest(keras_parameterized.TestCase):\n   @tf.test.disable_with_predicate(\n       pred=tf.test.is_built_with_rocm,\n       skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_GRU_runtime_with_mask(self):\n     # Masking will affect which backend is selected based on whether the mask\n     # is strictly right padded.\n@@ -746,7 +745,7 @@ class GRUGraphRewriteTest(keras_parameterized.TestCase):\n         lambda x: tf.expand_dims(x, axis=-1))(runtime)\n     model = keras.models.Model(inputs=inputs, outputs=[outputs, runtime])\n \n-    (x_train, y_train), _ = testing_utils.get_test_data(\n+    (x_train, y_train), _ = test_utils.get_test_data(\n         train_samples=self.batch,\n         test_samples=0,\n         input_shape=(self.timestep, self.input_shape),\n@@ -756,7 +755,7 @@ class GRUGraphRewriteTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='sgd',\n         loss=['categorical_crossentropy', None],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     model.fit(x_train, y_train)\n \n@@ -783,7 +782,7 @@ class GRUGraphRewriteTest(keras_parameterized.TestCase):\n     _, runtime_value = model.predict(x_train)\n     self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_GRU_runtime_with_cond(self):\n     # This test is to demonstrate the graph rewrite of grappler plugin under\n     # the condition that the function returns different number of internal\n\n@@ -23,11 +23,11 @@ import shutil\n \n from absl.testing import parameterized\n import numpy as np\n-from tensorflow.python.framework import test_util\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n from keras import backend as keras_backend\n-from keras import combinations\n+from keras.testing_infra import test_combinations\n from keras import initializers\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.engine import base_layer_utils\n from keras.engine import input_layer\n from keras.engine import training\n@@ -46,7 +46,7 @@ def _exact_laplacian(stddev):\n       kernelized_utils.exact_laplacian_kernel, stddev=stddev)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class RandomFourierFeaturesTest(tf.test.TestCase, parameterized.TestCase):\n \n   def _assert_all_close(self, expected, actual, atol=0.001):\n@@ -57,7 +57,7 @@ class RandomFourierFeaturesTest(tf.test.TestCase, parameterized.TestCase):\n     else:\n       self.assertAllClose(expected, actual, atol=atol)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_state_saving_and_loading(self):\n     with self.cached_session():\n       input_data = np.random.random((1, 2))\n@@ -129,7 +129,7 @@ class RandomFourierFeaturesTest(tf.test.TestCase, parameterized.TestCase):\n     num_trainable_vars = 1 if trainable else 0\n     self.assertLen(rff_layer.non_trainable_variables, 3 - num_trainable_vars)\n \n-  @test_util.assert_no_new_pyobjects_executing_eagerly\n+  @tf_test_utils.assert_no_new_pyobjects_executing_eagerly\n   def test_no_eager_Leak(self):\n     # Tests that repeatedly constructing and building a Layer does not leak\n     # Python objects.\n\n@@ -19,10 +19,10 @@ import os\n \n from absl.testing import parameterized\n import keras\n-from keras import combinations\n-from keras import testing_utils\n from keras.layers.locally_connected import locally_connected_utils\n from keras.optimizers.optimizer_v2 import rmsprop\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n@@ -81,7 +81,7 @@ _DATA_FORMAT_PADDING_IMPLEMENTATION = [{\n }]\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class LocallyConnected1DLayersTest(tf.test.TestCase, parameterized.TestCase):\n \n   @parameterized.parameters(_DATA_FORMAT_PADDING_IMPLEMENTATION)\n@@ -109,7 +109,7 @@ class LocallyConnected1DLayersTest(tf.test.TestCase, parameterized.TestCase):\n           self.assertRaises(ValueError, keras.layers.LocallyConnected1D,\n                             **kwargs)\n         else:\n-          testing_utils.layer_test(\n+          test_utils.layer_test(\n               keras.layers.LocallyConnected1D,\n               kwargs=kwargs,\n               input_shape=(num_samples, num_steps, input_dim))\n@@ -167,7 +167,7 @@ class LocallyConnected1DLayersTest(tf.test.TestCase, parameterized.TestCase):\n       layer.build((None, 5, 2))\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class LocallyConnected2DLayersTest(tf.test.TestCase, parameterized.TestCase):\n \n   @parameterized.parameters(_DATA_FORMAT_PADDING_IMPLEMENTATION)\n@@ -198,7 +198,7 @@ class LocallyConnected2DLayersTest(tf.test.TestCase, parameterized.TestCase):\n           self.assertRaises(ValueError, keras.layers.LocallyConnected2D,\n                             **kwargs)\n         else:\n-          testing_utils.layer_test(\n+          test_utils.layer_test(\n               keras.layers.LocallyConnected2D,\n               kwargs=kwargs,\n               input_shape=(num_samples, num_row, num_col, stack_size))\n@@ -223,7 +223,7 @@ class LocallyConnected2DLayersTest(tf.test.TestCase, parameterized.TestCase):\n       if padding == 'same' and implementation == 1:\n         self.assertRaises(ValueError, keras.layers.LocallyConnected2D, **kwargs)\n       else:\n-        testing_utils.layer_test(\n+        test_utils.layer_test(\n             keras.layers.LocallyConnected2D,\n             kwargs=kwargs,\n             input_shape=(num_samples, num_row, num_col, stack_size))\n@@ -281,7 +281,7 @@ class LocallyConnected2DLayersTest(tf.test.TestCase, parameterized.TestCase):\n       layer.build((None, 5, 5, 2))\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class LocallyConnectedImplementationModeTest(tf.test.TestCase,\n                                              parameterized.TestCase):\n \n\n@@ -22,19 +22,19 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n \n \n-@keras_parameterized.run_all_keras_modes\n-class LSTMLayerTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class LSTMLayerTest(test_combinations.TestCase):\n \n   def test_return_sequences_LSTM(self):\n     num_samples = 2\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.LSTM,\n         kwargs={'units': units,\n                 'return_sequences': True},\n@@ -43,13 +43,13 @@ class LSTMLayerTest(keras_parameterized.TestCase):\n   @tf.test.disable_with_predicate(\n       pred=tf.test.is_built_with_rocm,\n       skip_message='Double type is yet not supported in ROCm')\n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_float64_LSTM(self):\n     num_samples = 2\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.LSTM,\n         kwargs={'units': units,\n                 'return_sequences': True,\n@@ -83,7 +83,7 @@ class LSTMLayerTest(keras_parameterized.TestCase):\n     model.compile(\n         'rmsprop',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x = np.random.random((num_samples, timesteps, embedding_dim))\n     y = np.random.random((num_samples, units))\n@@ -94,7 +94,7 @@ class LSTMLayerTest(keras_parameterized.TestCase):\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.LSTM,\n         kwargs={'units': units,\n                 'dropout': 0.1,\n@@ -112,7 +112,7 @@ class LSTMLayerTest(keras_parameterized.TestCase):\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.LSTM,\n         kwargs={'units': units,\n                 'implementation': implementation_mode},\n@@ -152,7 +152,7 @@ class LSTMLayerTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='categorical_crossentropy',\n         optimizer='rmsprop',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)\n \n   @parameterized.parameters([True, False])\n@@ -168,7 +168,7 @@ class LSTMLayerTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='categorical_crossentropy',\n         optimizer='rmsprop',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(inputs, targets, epochs=1, batch_size=2, verbose=1)\n \n   def test_from_config_LSTM(self):\n@@ -207,7 +207,7 @@ class LSTMLayerTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='categorical_crossentropy',\n         optimizer=tf.compat.v1.train.AdamOptimizer(),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     inputs = np.random.random((num_samples, timesteps, embedding_dim))\n     initial_state = [np.random.random((num_samples, units))\n@@ -234,7 +234,7 @@ class LSTMLayerTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='categorical_crossentropy',\n         optimizer=tf.compat.v1.train.AdamOptimizer(),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     inputs = np.random.random((num_samples, timesteps, embedding_dim))\n     targets = np.random.random((num_samples, units))\n@@ -286,7 +286,7 @@ class LSTMLayerTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='categorical_crossentropy',\n         optimizer='rmsprop',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     inputs = np.random.random((num_samples, timesteps, embedding_dim))\n     initial_state = [np.random.random((num_samples, units))\n@@ -351,7 +351,7 @@ class LSTMLayerTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='categorical_crossentropy',\n         optimizer=tf.compat.v1.train.AdamOptimizer(),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     main_inputs = np.random.random((num_samples, timesteps, embedding_dim))\n     initial_state = [np.random.random((num_samples, units))\n@@ -403,7 +403,7 @@ class LSTMLayerTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     out1 = model.predict(np.ones((num_samples, timesteps)))\n     self.assertEqual(out1.shape, (num_samples, units))\n \n\n@@ -26,8 +26,8 @@ import numpy as np\n from tensorflow.core.protobuf import rewriter_config_pb2\n import keras\n from tensorflow.python.framework import test_util as tf_test_util\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.layers import recurrent as rnn_v1\n from keras.layers import recurrent_v2 as rnn\n from keras.utils import np_utils\n@@ -42,8 +42,8 @@ _graph_options = tf.compat.v1.GraphOptions(rewrite_options=_rewrites)\n _config = tf.compat.v1.ConfigProto(graph_options=_graph_options)\n \n \n-@keras_parameterized.run_all_keras_modes(config=_config)\n-class LSTMV2Test(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(config=_config)\n+class LSTMV2Test(test_combinations.TestCase):\n \n   @parameterized.named_parameters(\n       ('non_tan_activation', 'relu', 'sigmoid', 0, False, True),\n@@ -63,7 +63,7 @@ class LSTMV2Test(keras_parameterized.TestCase):\n         use_bias=use_bias)\n     self.assertFalse(layer._could_use_gpu_kernel)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_use_on_default_activation_with_gpu_kernel(self):\n     layer = rnn.LSTM(1, activation=tf.tanh)\n     self.assertTrue(layer._could_use_gpu_kernel)\n@@ -309,14 +309,14 @@ class LSTMV2Test(keras_parameterized.TestCase):\n   @tf.test.disable_with_predicate(\n       pred=tf.test.is_built_with_rocm,\n       skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_lstm_v2_feature_parity_with_canonical_lstm(self):\n     input_shape = 10\n     rnn_state_size = 8\n     timestep = 4\n     batch = 20\n \n-    (x_train, y_train), _ = testing_utils.get_test_data(\n+    (x_train, y_train), _ = test_utils.get_test_data(\n         train_samples=batch,\n         test_samples=0,\n         input_shape=(timestep, input_shape),\n@@ -341,7 +341,7 @@ class LSTMV2Test(keras_parameterized.TestCase):\n     lstm_model.fit(x_train, y_train)\n     y_2 = lstm_model.predict(x_train)\n \n-    with testing_utils.device(should_use_gpu=True):\n+    with test_utils.device(should_use_gpu=True):\n       cudnn_layer = rnn.LSTM(rnn_state_size)\n       cudnn_model = keras.models.Model(inputs, cudnn_layer(masked_input))\n     cudnn_model.set_weights(weights)\n@@ -362,7 +362,7 @@ class LSTMV2Test(keras_parameterized.TestCase):\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         rnn.LSTM,\n         kwargs={\n             'units': units,\n@@ -465,7 +465,7 @@ class LSTMV2Test(keras_parameterized.TestCase):\n     batch = 100\n     epoch = 10\n \n-    (x_train, y_train), _ = testing_utils.get_test_data(\n+    (x_train, y_train), _ = test_utils.get_test_data(\n         train_samples=batch,\n         test_samples=0,\n         input_shape=(timestep, input_shape),\n@@ -533,14 +533,14 @@ class LSTMV2Test(keras_parameterized.TestCase):\n \n     inputs = keras.layers.Input(\n         shape=[timestep, input_shape], dtype=tf.float32)\n-    with testing_utils.device(should_use_gpu=False):\n+    with test_utils.device(should_use_gpu=False):\n       layer = rnn.LSTM(rnn_state_size)\n       output = layer(inputs)\n       cpu_model = keras.models.Model(inputs, output)\n       weights = cpu_model.get_weights()\n     y_1 = cpu_model.predict(x_train)\n \n-    with testing_utils.device(should_use_gpu=True):\n+    with test_utils.device(should_use_gpu=True):\n       layer = rnn.LSTM(rnn_state_size)\n       output = layer(inputs)\n       gpu_model = keras.models.Model(inputs, output)\n@@ -550,7 +550,7 @@ class LSTMV2Test(keras_parameterized.TestCase):\n     # Note that cuDNN uses 'sigmoid' as activation, so the LSTM V2 uses\n     # 'sigmoid' as default. Construct the canonical LSTM with sigmoid to achieve\n     # the same output.\n-    with testing_utils.device(should_use_gpu=True):\n+    with test_utils.device(should_use_gpu=True):\n       layer = rnn_v1.LSTM(rnn_state_size, recurrent_activation='sigmoid')\n       output = layer(inputs)\n       canonical_model = keras.models.Model(inputs, output)\n@@ -566,7 +566,7 @@ class LSTMV2Test(keras_parameterized.TestCase):\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         rnn.LSTM,\n         kwargs={\n             'units': units,\n@@ -577,13 +577,13 @@ class LSTMV2Test(keras_parameterized.TestCase):\n   @tf.test.disable_with_predicate(\n       pred=tf.test.is_built_with_rocm,\n       skip_message='Skipping as ROCm MIOpen does not support float64 yet.')\n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_float64_LSTM(self):\n     num_samples = 2\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         rnn.LSTM,\n         kwargs={\n             'units': units,\n@@ -637,7 +637,7 @@ class LSTMV2Test(keras_parameterized.TestCase):\n     model.compile(\n         optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     out1 = model.predict(np.ones((num_samples, timesteps)))\n     self.assertEqual(out1.shape, (num_samples, units))\n \n@@ -709,7 +709,7 @@ class LSTMV2Test(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='adam',\n         loss='sparse_categorical_crossentropy',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(x, y, epochs=1, shuffle=False)\n \n   def test_dropout_LSTM(self):\n@@ -717,7 +717,7 @@ class LSTMV2Test(keras_parameterized.TestCase):\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         rnn.LSTM,\n         kwargs={\n             'units': units,\n@@ -752,7 +752,7 @@ class LSTMV2Test(keras_parameterized.TestCase):\n   @tf.test.disable_with_predicate(\n       pred=tf.test.is_built_with_rocm,\n       skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_explicit_device_with_go_backward_and_mask(self):\n     batch_size = 8\n     timestep = 7\n@@ -765,14 +765,14 @@ class LSTMV2Test(keras_parameterized.TestCase):\n \n     # Test for V1 behavior.\n     lstm_v1 = rnn_v1.LSTM(units, return_sequences=True, go_backwards=True)\n-    with testing_utils.device(should_use_gpu=True):\n+    with test_utils.device(should_use_gpu=True):\n       outputs_masked_v1 = lstm_v1(inputs, mask=tf.constant(mask))\n       outputs_trimmed_v1 = lstm_v1(inputs[:, :masksteps])\n     self.assertAllClose(outputs_masked_v1[:, -masksteps:], outputs_trimmed_v1)\n \n     # Test for V2 behavior.\n     lstm = rnn.LSTM(units, return_sequences=True, go_backwards=True)\n-    with testing_utils.device(should_use_gpu=True):\n+    with test_utils.device(should_use_gpu=True):\n       outputs_masked = lstm(inputs, mask=tf.constant(mask))\n       outputs_trimmed = lstm(inputs[:, :masksteps])\n     self.assertAllClose(outputs_masked[:, -masksteps:], outputs_trimmed)\n@@ -819,7 +819,7 @@ class LSTMV2Test(keras_parameterized.TestCase):\n     model.compile(\n         optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     # Make sure it doesn't crash with cudnn kernel.\n     model.predict(inputs)\n \n@@ -849,8 +849,8 @@ class LSTMV2Test(keras_parameterized.TestCase):\n     self.assertAllClose(self.evaluate(outputs), self.evaluate(copied_outputs))\n \n \n-@keras_parameterized.run_all_keras_modes(config=_config)\n-class LSTMGraphRewriteTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(config=_config)\n+class LSTMGraphRewriteTest(test_combinations.TestCase):\n \n   input_shape = 10\n   output_shape = 8\n@@ -861,7 +861,7 @@ class LSTMGraphRewriteTest(keras_parameterized.TestCase):\n \n   def _test_runtime_with_model(self, model):\n \n-    (x_train, y_train), _ = testing_utils.get_test_data(\n+    (x_train, y_train), _ = test_utils.get_test_data(\n         train_samples=self.batch,\n         test_samples=0,\n         input_shape=(self.timestep, self.input_shape),\n@@ -871,7 +871,7 @@ class LSTMGraphRewriteTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='sgd',\n         loss=['categorical_crossentropy', None],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     existing_loss = 0\n     for _ in range(self.epoch):\n@@ -887,7 +887,7 @@ class LSTMGraphRewriteTest(keras_parameterized.TestCase):\n     else:\n       self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_LSTM_runtime(self):\n     layer = rnn.LSTM(self.rnn_state_size, return_runtime=True)\n \n@@ -906,7 +906,7 @@ class LSTMGraphRewriteTest(keras_parameterized.TestCase):\n   @tf.test.disable_with_predicate(\n       pred=tf.test.is_built_with_rocm,\n       skip_message='Skipping as ROCm MIOpen does not support padded input yet.')\n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_LSTM_runtime_with_mask(self):\n     # Masking will affect which backend is selected based on whether the mask\n     # is strictly right padded.\n@@ -924,7 +924,7 @@ class LSTMGraphRewriteTest(keras_parameterized.TestCase):\n         lambda x: tf.expand_dims(x, axis=-1))(runtime)\n     model = keras.models.Model(inputs=inputs, outputs=[outputs, runtime])\n \n-    (x_train, y_train), _ = testing_utils.get_test_data(\n+    (x_train, y_train), _ = test_utils.get_test_data(\n         train_samples=self.batch,\n         test_samples=0,\n         input_shape=(self.timestep, self.input_shape),\n@@ -934,7 +934,7 @@ class LSTMGraphRewriteTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='sgd',\n         loss=['categorical_crossentropy', None],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     model.fit(x_train, y_train)\n \n@@ -961,7 +961,7 @@ class LSTMGraphRewriteTest(keras_parameterized.TestCase):\n     _, runtime_value = model.predict(x_train)\n     self.assertEqual(runtime_value[0], rnn._RUNTIME_CPU)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_LSTM_runtime_with_cond(self):\n     # This test is to demonstrate the graph rewrite of grappler plugin under\n     # the condition that the function returns different number of internal\n@@ -1083,7 +1083,7 @@ class LSTMPerformanceTest(tf.test.Benchmark):\n         # The performance for warmup epoch is ignored.\n         'warmup_epoch': 1,\n     }\n-    (x_train, y_train), _ = testing_utils.get_test_data(\n+    (x_train, y_train), _ = test_utils.get_test_data(\n         train_samples=(batch * num_batch),\n         test_samples=0,\n         input_shape=(test_config['timestep'], test_config['input_shape']),\n\n@@ -17,15 +17,14 @@\n from absl.testing import parameterized\n import keras\n from keras import backend\n-from keras import combinations\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class MergingLayersTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class MergingLayersTest(test_combinations.TestCase):\n \n   def test_add(self):\n     i1 = keras.layers.Input(shape=(4, 5))\n@@ -36,7 +35,7 @@ class MergingLayersTest(keras_parameterized.TestCase):\n     o = add_layer([i1, i2, i3])\n     self.assertListEqual(o.shape.as_list(), [None, 4, 5])\n     model = keras.models.Model([i1, i2, i3], o)\n-    model.run_eagerly = testing_utils.should_run_eagerly()\n+    model.run_eagerly = test_utils.should_run_eagerly()\n \n     x1 = np.random.random((2, 4, 5))\n     x2 = np.random.random((2, 4, 5))\n@@ -68,7 +67,7 @@ class MergingLayersTest(keras_parameterized.TestCase):\n     o = subtract_layer([i1, i2])\n     self.assertListEqual(o.shape.as_list(), [None, 4, 5])\n     model = keras.models.Model([i1, i2], o)\n-    model.run_eagerly = testing_utils.should_run_eagerly()\n+    model.run_eagerly = test_utils.should_run_eagerly()\n \n     x1 = np.random.random((2, 4, 5))\n     x2 = np.random.random((2, 4, 5))\n@@ -101,7 +100,7 @@ class MergingLayersTest(keras_parameterized.TestCase):\n     o = keras.layers.multiply([i1, i2, i3])\n     self.assertListEqual(o.shape.as_list(), [None, 4, 5])\n     model = keras.models.Model([i1, i2, i3], o)\n-    model.run_eagerly = testing_utils.should_run_eagerly()\n+    model.run_eagerly = test_utils.should_run_eagerly()\n \n     x1 = np.random.random((2, 4, 5))\n     x2 = np.random.random((2, 4, 5))\n@@ -116,7 +115,7 @@ class MergingLayersTest(keras_parameterized.TestCase):\n     o = keras.layers.average([i1, i2])\n     self.assertListEqual(o.shape.as_list(), [None, 4, 5])\n     model = keras.models.Model([i1, i2], o)\n-    model.run_eagerly = testing_utils.should_run_eagerly()\n+    model.run_eagerly = test_utils.should_run_eagerly()\n \n     x1 = np.random.random((2, 4, 5))\n     x2 = np.random.random((2, 4, 5))\n@@ -130,7 +129,7 @@ class MergingLayersTest(keras_parameterized.TestCase):\n     o = keras.layers.maximum([i1, i2])\n     self.assertListEqual(o.shape.as_list(), [None, 4, 5])\n     model = keras.models.Model([i1, i2], o)\n-    model.run_eagerly = testing_utils.should_run_eagerly()\n+    model.run_eagerly = test_utils.should_run_eagerly()\n \n     x1 = np.random.random((2, 4, 5))\n     x2 = np.random.random((2, 4, 5))\n@@ -144,7 +143,7 @@ class MergingLayersTest(keras_parameterized.TestCase):\n     o = keras.layers.minimum([i1, i2])\n     self.assertListEqual(o.shape.as_list(), [None, 4, 5])\n     model = keras.models.Model([i1, i2], o)\n-    model.run_eagerly = testing_utils.should_run_eagerly()\n+    model.run_eagerly = test_utils.should_run_eagerly()\n \n     x1 = np.random.random((2, 4, 5))\n     x2 = np.random.random((2, 4, 5))\n@@ -159,7 +158,7 @@ class MergingLayersTest(keras_parameterized.TestCase):\n     o = concat_layer([i1, i2])\n     self.assertListEqual(o.shape.as_list(), [None, 8, 5])\n     model = keras.models.Model([i1, i2], o)\n-    model.run_eagerly = testing_utils.should_run_eagerly()\n+    model.run_eagerly = test_utils.should_run_eagerly()\n \n     x1 = np.random.random((2, 4, 5))\n     x2 = np.random.random((2, 4, 5))\n@@ -200,7 +199,7 @@ class MergingLayersTest(keras_parameterized.TestCase):\n     o = keras.layers.dot([i1, i2], axes=1)\n     self.assertListEqual(o.shape.as_list(), [None, 1])\n     model = keras.models.Model([i1, i2], o)\n-    model.run_eagerly = testing_utils.should_run_eagerly()\n+    model.run_eagerly = test_utils.should_run_eagerly()\n     _ = keras.layers.Dot(axes=1).get_config()\n \n     x1 = np.random.random((2, 4))\n@@ -216,7 +215,7 @@ class MergingLayersTest(keras_parameterized.TestCase):\n     o = keras.layers.dot([i1, i2], axes=(-1, -1))\n     self.assertListEqual(o.shape.as_list(), [None, 1])\n     model = keras.models.Model([i1, i2], o)\n-    model.run_eagerly = testing_utils.should_run_eagerly()\n+    model.run_eagerly = test_utils.should_run_eagerly()\n     out = model.predict([x1, x2])\n     self.assertEqual(out.shape, (2, 1))\n     self.assertAllClose(out, expected, atol=1e-4)\n@@ -226,7 +225,7 @@ class MergingLayersTest(keras_parameterized.TestCase):\n     self.assertEqual(layer.compute_output_shape([(4, 5), (4, 5)]), (4, 1))\n \n   @parameterized.named_parameters(\n-      *testing_utils.generate_combinations_with_testcase_name(\n+      *test_utils.generate_combinations_with_testcase_name(\n           layer=[keras.layers.Add, keras.layers.Subtract,\n                  keras.layers.Multiply, keras.layers.Minimum,\n                  keras.layers.Maximum, keras.layers.Average]))\n@@ -262,7 +261,7 @@ class MergingLayersTest(keras_parameterized.TestCase):\n     self.assertAllEqual(out_ragged, expected_concatenated_ragged)\n \n   @parameterized.named_parameters(\n-      *testing_utils.generate_combinations_with_testcase_name(\n+      *test_utils.generate_combinations_with_testcase_name(\n           layer=[keras.layers.Add, keras.layers.Subtract,\n                  keras.layers.Multiply, keras.layers.Minimum,\n                  keras.layers.Maximum, keras.layers.Average]))\n@@ -273,7 +272,7 @@ class MergingLayersTest(keras_parameterized.TestCase):\n     self.assertEqual(out.shape, ())\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class MergingLayersTestNoExecution(tf.test.TestCase):\n \n   def test_add_elementwise_errors(self):\n\n@@ -20,18 +20,17 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import combinations\n-from keras import keras_parameterized\n-from keras import testing_utils\n from keras.layers.normalization import batch_normalization\n from keras.layers.normalization import batch_normalization_v1\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n \n \n-class BatchNormalizationTest(keras_parameterized.TestCase):\n+class BatchNormalizationTest(test_combinations.TestCase):\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_basic_batchnorm(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.BatchNormalization,\n         kwargs={\n             'momentum': 0.9,\n@@ -40,7 +39,7 @@ class BatchNormalizationTest(keras_parameterized.TestCase):\n             'beta_regularizer': keras.regularizers.l2(0.01)\n         },\n         input_shape=(3, 4, 2))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.BatchNormalization,\n         kwargs={\n             'gamma_initializer': 'ones',\n@@ -49,12 +48,12 @@ class BatchNormalizationTest(keras_parameterized.TestCase):\n             'moving_variance_initializer': 'ones'\n         },\n         input_shape=(3, 4, 2))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.BatchNormalization,\n         kwargs={'scale': False,\n                 'center': False},\n         input_shape=(3, 3))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.BatchNormalization,\n         kwargs={\n             'gamma_initializer': 'ones',\n@@ -64,7 +63,8 @@ class BatchNormalizationTest(keras_parameterized.TestCase):\n         },\n         input_shape=(3, 2, 4, 2))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_batchnorm_weights(self):\n     layer = keras.layers.BatchNormalization(scale=False, center=False)\n     layer.build((None, 3, 4))\n@@ -76,7 +76,8 @@ class BatchNormalizationTest(keras_parameterized.TestCase):\n     self.assertEqual(len(layer.trainable_weights), 2)\n     self.assertEqual(len(layer.weights), 4)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_batchnorm_regularization(self):\n     layer = keras.layers.BatchNormalization(\n         gamma_regularizer='l1', beta_regularizer='l1')\n@@ -89,7 +90,7 @@ class BatchNormalizationTest(keras_parameterized.TestCase):\n     self.assertEqual(layer.gamma.constraint, max_norm)\n     self.assertEqual(layer.beta.constraint, max_norm)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_batchnorm_convnet(self):\n     if tf.test.is_gpu_available(cuda_only=True):\n       with self.session():\n@@ -100,7 +101,7 @@ class BatchNormalizationTest(keras_parameterized.TestCase):\n         model.compile(\n             loss='mse',\n             optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),\n-            run_eagerly=testing_utils.should_run_eagerly())\n+            run_eagerly=test_utils.should_run_eagerly())\n \n         # centered on 5.0, variance 10.0\n         x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 3, 4, 4))\n@@ -112,7 +113,7 @@ class BatchNormalizationTest(keras_parameterized.TestCase):\n         np.testing.assert_allclose(np.mean(out, axis=(0, 2, 3)), 0.0, atol=1e-1)\n         np.testing.assert_allclose(np.std(out, axis=(0, 2, 3)), 1.0, atol=1e-1)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_batchnorm_convnet_channel_last(self):\n     model = keras.models.Sequential()\n     norm = keras.layers.BatchNormalization(\n@@ -121,7 +122,7 @@ class BatchNormalizationTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='mse',\n         optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     # centered on 5.0, variance 10.0\n     x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 4, 4, 3))\n@@ -133,22 +134,23 @@ class BatchNormalizationTest(keras_parameterized.TestCase):\n     np.testing.assert_allclose(np.mean(out, axis=(0, 1, 2)), 0.0, atol=1e-1)\n     np.testing.assert_allclose(np.std(out, axis=(0, 1, 2)), 1.0, atol=1e-1)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_batchnorm_correctness(self):\n     _run_batchnorm_correctness_test(\n         batch_normalization_v1.BatchNormalization, dtype='float32')\n     _run_batchnorm_correctness_test(\n         batch_normalization.BatchNormalization, dtype='float32')\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_batchnorm_float16(self):\n     _run_batchnorm_correctness_test(\n         batch_normalization_v1.BatchNormalization, dtype='float16')\n     _run_batchnorm_correctness_test(\n         batch_normalization.BatchNormalization, dtype='float16')\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n+  @test_utils.enable_v2_dtype_behavior\n   def test_batchnorm_mixed_precision(self):\n     norm = keras.layers.BatchNormalization(\n         axis=-1,\n@@ -164,9 +166,9 @@ class BatchNormalizationTest(keras_parameterized.TestCase):\n     y = norm(x)\n     self.assertEqual(y.dtype, 'float16')\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager'],\n+  @test_combinations.generate(test_combinations.combine(mode=['graph', 'eager'],\n                                                         fused=[True, False]))\n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_batchnorm_mixed_precision_does_not_overflow(self, fused):\n     norm = keras.layers.BatchNormalization(\n         axis=-1,\n@@ -178,7 +180,7 @@ class BatchNormalizationTest(keras_parameterized.TestCase):\n     expected_y = np.array([-1.0, 1.0]).reshape((2, 1, 1, 1))\n     self.assertAllClose(keras.backend.eval(y), expected_y)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_batchnorm_non_trainable_with_fit(self):\n     # We use the same data shape for all the data we use in this test.\n     # This will prevent any used tf.functions from retracing.\n@@ -194,7 +196,7 @@ class BatchNormalizationTest(keras_parameterized.TestCase):\n     model.compile(\n         'rmsprop',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(np.random.random(data_shape), np.random.random(data_shape))\n \n     test_data = np.random.random(data_shape)\n@@ -205,11 +207,11 @@ class BatchNormalizationTest(keras_parameterized.TestCase):\n     model.compile(\n         'rmsprop',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     train_loss = model.train_on_batch(test_data, test_targets)\n     self.assertAlmostEqual(test_loss, train_loss)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_eager_batchnorm_in_custom_model_call_with_tf_function(self):\n \n     class MyModel(keras.Model):\n@@ -232,7 +234,7 @@ class BatchNormalizationTest(keras_parameterized.TestCase):\n     self.assertAllClose(model.bn.moving_mean.numpy(), [0.047], atol=3e-3)\n     self.assertAllClose(model.bn.moving_variance.numpy(), [0.9], atol=3e-2)\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def test_bessels_correction(self):\n     # Bessel's correction is currently only used in the fused case. In the\n     # future, it may be used in the nonfused case as well.\n@@ -258,9 +260,10 @@ class BatchNormalizationTest(keras_parameterized.TestCase):\n     self.assertAllEqual(self.evaluate(layer.moving_variance), [1.])\n \n \n-class BatchNormalizationV1Test(keras_parameterized.TestCase):\n+class BatchNormalizationV1Test(test_combinations.TestCase):\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_v1_fused_attribute(self):\n     norm = batch_normalization_v1.BatchNormalization()\n     inp = keras.layers.Input((4, 4, 4))\n@@ -280,20 +283,21 @@ class BatchNormalizationV1Test(keras_parameterized.TestCase):\n     self.assertEqual(norm.fused, False)\n \n \n-class BatchNormalizationV2Test(keras_parameterized.TestCase):\n+class BatchNormalizationV2Test(test_combinations.TestCase):\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_basic_batchnorm_v2(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         batch_normalization.BatchNormalization,\n         kwargs={'fused': True},\n         input_shape=(3, 3, 3, 3))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         batch_normalization.BatchNormalization,\n         kwargs={'fused': None},\n         input_shape=(3, 3, 3))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_v2_fused_attribute(self):\n     norm = batch_normalization.BatchNormalization()\n     self.assertIsNone(norm.fused)\n@@ -366,14 +370,14 @@ class BatchNormalizationV2Test(keras_parameterized.TestCase):\n     wrapped_fn = tf.compat.v1.wrap_function(my_func, [])\n     wrapped_fn()\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_basic_batchnorm_v2_none_shape_and_virtual_batch_size(self):\n     # Test case for GitHub issue for 32380\n     norm = batch_normalization.BatchNormalization(virtual_batch_size=8)\n     inp = keras.layers.Input(shape=(None, None, 3))\n     _ = norm(inp)\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def test_fused_batchnorm_empty_batch(self):\n     # Test case for https://github.com/tensorflow/tensorflow/issues/52986\n     # create a simple strategy with the enable_partial_batch_handling flag\n@@ -412,7 +416,7 @@ def _run_batchnorm_correctness_test(layer, dtype='float32', fused=False):\n   model.compile(\n       loss='mse',\n       optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),\n-      run_eagerly=testing_utils.should_run_eagerly())\n+      run_eagerly=test_utils.should_run_eagerly())\n \n   # centered on 5.0, variance 10.0\n   x = (np.random.normal(loc=5.0, scale=10.0, size=(1000, 2, 2, 2))\n\n@@ -19,9 +19,8 @@ import tensorflow.compat.v2 as tf\n import numpy as np\n \n import keras\n-from keras import combinations\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.layers.normalization import layer_normalization\n \n \n@@ -33,7 +32,7 @@ def _run_layernorm_correctness_test(layer, dtype='float32'):\n   model.compile(\n       loss='mse',\n       optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),\n-      run_eagerly=testing_utils.should_run_eagerly())\n+      run_eagerly=test_utils.should_run_eagerly())\n \n   # centered on 5.0, variance 10.0\n   x = (np.random.normal(loc=5.0, scale=10.0, size=(1000, 2, 2, 2))\n@@ -47,53 +46,54 @@ def _run_layernorm_correctness_test(layer, dtype='float32'):\n   np.testing.assert_allclose(out.std(), 1.0, atol=1e-1)\n \n \n-class LayerNormalizationTest(keras_parameterized.TestCase):\n+class LayerNormalizationTest(test_combinations.TestCase):\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_basic_layernorm(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.LayerNormalization,\n         kwargs={\n             'gamma_regularizer': keras.regularizers.l2(0.01),\n             'beta_regularizer': keras.regularizers.l2(0.01)\n         },\n         input_shape=(3, 4, 2))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.LayerNormalization,\n         kwargs={\n             'gamma_initializer': 'ones',\n             'beta_initializer': 'ones',\n         },\n         input_shape=(3, 4, 2))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.LayerNormalization,\n         kwargs={'scale': False,\n                 'center': False},\n         input_shape=(3, 3))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.LayerNormalization,\n         kwargs={'axis': (-3, -2, -1)},\n         input_shape=(2, 8, 8, 3))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.LayerNormalization,\n         input_shape=(1, 0, 10))\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_non_fused_layernorm(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.LayerNormalization,\n         kwargs={'axis': -2},\n         input_shape=(3, 4, 2))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.LayerNormalization,\n         kwargs={'axis': (-3, -2)},\n         input_shape=(2, 8, 8, 3))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.LayerNormalization,\n         kwargs={'axis': (-3, -1)},\n         input_shape=(2, 8, 8, 3))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_layernorm_weights(self):\n     layer = keras.layers.LayerNormalization(scale=False, center=False)\n     layer.build((None, 3, 4))\n@@ -105,7 +105,8 @@ class LayerNormalizationTest(keras_parameterized.TestCase):\n     self.assertEqual(len(layer.trainable_weights), 2)\n     self.assertEqual(len(layer.weights), 2)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_layernorm_regularization(self):\n     layer = keras.layers.LayerNormalization(\n         gamma_regularizer='l1', beta_regularizer='l1')\n@@ -118,7 +119,7 @@ class LayerNormalizationTest(keras_parameterized.TestCase):\n     self.assertEqual(layer.gamma.constraint, max_norm)\n     self.assertEqual(layer.beta.constraint, max_norm)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_layernorm_convnet_channel_last(self):\n     model = keras.models.Sequential()\n     norm = keras.layers.LayerNormalization(input_shape=(4, 4, 3))\n@@ -126,7 +127,7 @@ class LayerNormalizationTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='mse',\n         optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     # centered on 5.0, variance 10.0\n     x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 4, 4, 3))\n@@ -138,23 +139,25 @@ class LayerNormalizationTest(keras_parameterized.TestCase):\n     np.testing.assert_allclose(np.mean(out, axis=(0, 1, 2)), 0.0, atol=1e-1)\n     np.testing.assert_allclose(np.std(out, axis=(0, 1, 2)), 1.0, atol=1e-1)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_layernorm_correctness(self):\n     _run_layernorm_correctness_test(\n         layer_normalization.LayerNormalization, dtype='float32')\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_layernorm_mixed_precision(self):\n     _run_layernorm_correctness_test(\n         layer_normalization.LayerNormalization, dtype='float16')\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testIncorrectAxisType(self):\n     with self.assertRaisesRegex(TypeError,\n                                 r'Expected an int or a list/tuple of ints'):\n       _ = layer_normalization.LayerNormalization(axis={'axis': -1})\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testInvalidAxis(self):\n     with self.assertRaisesRegex(\n         ValueError,\n@@ -162,20 +165,22 @@ class LayerNormalizationTest(keras_parameterized.TestCase):\n       layer_norm = layer_normalization.LayerNormalization(axis=3)\n       layer_norm.build(input_shape=(2, 2, 2))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testDuplicateAxis(self):\n     with self.assertRaisesRegex(ValueError, r'Duplicate axis:'):\n       layer_norm = layer_normalization.LayerNormalization(axis=[-1, -1])\n       layer_norm.build(input_shape=(2, 2, 2))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testFusedAttr(self):\n     layer_norm = layer_normalization.LayerNormalization(axis=[-2, -1])\n     layer_norm.build(input_shape=(2, 2, 2))\n     self.assertEqual(layer_norm._fused, True)\n \n \n-class LayerNormalizationNumericsTest(keras_parameterized.TestCase):\n+class LayerNormalizationNumericsTest(test_combinations.TestCase):\n   \"\"\"Tests LayerNormalization has correct and numerically stable outputs.\"\"\"\n \n   def _expected_layer_norm(self, x, beta, gamma, batch_input_shape, axis,\n@@ -234,7 +239,8 @@ class LayerNormalizationNumericsTest(keras_parameterized.TestCase):\n         # some of the values are very close to zero.\n         self.assertAllClose(expected, actual, rtol=tol, atol=tol)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_forward(self):\n     # For numeric stability, we ensure the axis's dimension(s) have at least 4\n     # elements.\n@@ -316,7 +322,7 @@ class LayerNormalizationNumericsTest(keras_parameterized.TestCase):\n         self.assertAllClose(gamma_grad_t, gamma_grad_ref, rtol=tol, atol=tol)\n \n   # The gradient_checker_v2 does not work properly with LayerNorm in graph mode.\n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_backward(self):\n     # For numeric stability, we ensure the axis's dimension(s) have at least 4\n     # elements.\n\n@@ -18,24 +18,24 @@\n import tensorflow.compat.v2 as tf\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n \n \n def squared_l2_norm(x):\n   return tf.reduce_sum(x ** 2)\n \n \n-@testing_utils.run_v2_only\n-class UnitNormalizationTest(keras_parameterized.TestCase):\n+@test_utils.run_v2_only\n+class UnitNormalizationTest(test_combinations.TestCase):\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_basics(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.UnitNormalization,\n         kwargs={'axis': -1},\n         input_shape=(2, 3))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.UnitNormalization,\n         kwargs={'axis': (1, 2)},\n         input_shape=(1, 3, 3))\n\n@@ -16,18 +16,18 @@\n \n from absl.testing import parameterized\n import keras\n-from keras import combinations\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import tensorflow.compat.v2 as tf\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class AveragePoolingTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_average_pooling_1d(self):\n     for padding in ['valid', 'same']:\n       for stride in [1, 2]:\n-        testing_utils.layer_test(\n+        test_utils.layer_test(\n             keras.layers.AveragePooling1D,\n             kwargs={\n                 'strides': stride,\n@@ -35,13 +35,13 @@ class AveragePoolingTest(tf.test.TestCase, parameterized.TestCase):\n             },\n             input_shape=(3, 5, 4))\n \n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.AveragePooling1D,\n         kwargs={'data_format': 'channels_first'},\n         input_shape=(3, 2, 6))\n \n   def test_average_pooling_2d(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.AveragePooling2D,\n         kwargs={\n             'strides': (2, 2),\n@@ -49,7 +49,7 @@ class AveragePoolingTest(tf.test.TestCase, parameterized.TestCase):\n             'pool_size': (2, 2)\n         },\n         input_shape=(3, 5, 6, 4))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.AveragePooling2D,\n         kwargs={\n             'strides': (2, 2),\n@@ -64,7 +64,7 @@ class AveragePoolingTest(tf.test.TestCase, parameterized.TestCase):\n       # Only runs on GPU with CUDA, channels_first is not supported on CPU.\n       # TODO(b/62340061): Support channels_first on CPU.\n       if tf.test.is_gpu_available(cuda_only=True):\n-        testing_utils.layer_test(\n+        test_utils.layer_test(\n             keras.layers.AveragePooling2D,\n             kwargs={\n                 'strides': (1, 1),\n@@ -76,7 +76,7 @@ class AveragePoolingTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_average_pooling_3d(self):\n     pool_size = (3, 3, 3)\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.AveragePooling3D,\n         kwargs={\n             'strides': 2,\n@@ -84,7 +84,7 @@ class AveragePoolingTest(tf.test.TestCase, parameterized.TestCase):\n             'pool_size': pool_size\n         },\n         input_shape=(3, 11, 12, 10, 4))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.AveragePooling3D,\n         kwargs={\n             'strides': 3,\n\n@@ -16,17 +16,17 @@\n \n from absl.testing import parameterized\n import keras\n-from keras import combinations\n-from keras import testing_utils\n from keras.mixed_precision import policy\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class GlobalAveragePoolingTest(tf.test.TestCase, parameterized.TestCase):\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_mixed_float16_policy(self):\n     with policy.policy_scope('mixed_float16'):\n       inputs1 = keras.Input(shape=(36, 512), dtype='float16')\n@@ -35,9 +35,9 @@ class GlobalAveragePoolingTest(tf.test.TestCase, parameterized.TestCase):\n       _ = average_layer(inputs1, inputs2)\n \n   def test_global_average_pooling_1d(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GlobalAveragePooling1D, input_shape=(3, 4, 5))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GlobalAveragePooling1D,\n         kwargs={'data_format': 'channels_first'},\n         input_shape=(3, 4, 5))\n@@ -73,56 +73,56 @@ class GlobalAveragePoolingTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllEqual(output_ragged, output_dense)\n \n   def test_global_average_pooling_2d(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GlobalAveragePooling2D,\n         kwargs={'data_format': 'channels_first'},\n         input_shape=(3, 4, 5, 6))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GlobalAveragePooling2D,\n         kwargs={'data_format': 'channels_last'},\n         input_shape=(3, 5, 6, 4))\n \n   def test_global_average_pooling_3d(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GlobalAveragePooling3D,\n         kwargs={'data_format': 'channels_first'},\n         input_shape=(3, 4, 3, 4, 3))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GlobalAveragePooling3D,\n         kwargs={'data_format': 'channels_last'},\n         input_shape=(3, 4, 3, 4, 3))\n \n   def test_global_average_pooling_1d_keepdims(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GlobalAveragePooling1D,\n         kwargs={'keepdims': True},\n         input_shape=(3, 4, 5),\n         expected_output_shape=(None, 1, 5))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GlobalAveragePooling1D,\n         kwargs={'data_format': 'channels_first', 'keepdims': True},\n         input_shape=(3, 4, 5),\n         expected_output_shape=(None, 4, 1))\n \n   def test_global_average_pooling_2d_keepdims(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GlobalAveragePooling2D,\n         kwargs={'data_format': 'channels_first', 'keepdims': True},\n         input_shape=(3, 4, 5, 6),\n         expected_output_shape=(None, 4, 1, 1))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GlobalAveragePooling2D,\n         kwargs={'data_format': 'channels_last', 'keepdims': True},\n         input_shape=(3, 4, 5, 6),\n         expected_output_shape=(None, 1, 1, 6))\n \n   def test_global_average_pooling_3d_keepdims(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GlobalAveragePooling3D,\n         kwargs={'data_format': 'channels_first', 'keepdims': True},\n         input_shape=(3, 4, 3, 4, 3),\n         expected_output_shape=(None, 4, 1, 1, 1))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GlobalAveragePooling3D,\n         kwargs={'data_format': 'channels_last', 'keepdims': True},\n         input_shape=(3, 4, 3, 4, 3),\n\n@@ -16,18 +16,18 @@\n \n from absl.testing import parameterized\n import keras\n-from keras import combinations\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import tensorflow.compat.v2 as tf\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class GlobalMaxPoolingTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_global_max_pooling_1d(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GlobalMaxPooling1D, input_shape=(3, 4, 5))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GlobalMaxPooling1D,\n         kwargs={'data_format': 'channels_first'},\n         input_shape=(3, 4, 5))\n@@ -52,56 +52,56 @@ class GlobalMaxPoolingTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllEqual(output_ragged, output_dense)\n \n   def test_global_max_pooling_2d(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GlobalMaxPooling2D,\n         kwargs={'data_format': 'channels_first'},\n         input_shape=(3, 4, 5, 6))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GlobalMaxPooling2D,\n         kwargs={'data_format': 'channels_last'},\n         input_shape=(3, 5, 6, 4))\n \n   def test_global_maxpooling_3d(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GlobalMaxPooling3D,\n         kwargs={'data_format': 'channels_first'},\n         input_shape=(3, 4, 3, 4, 3))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GlobalMaxPooling3D,\n         kwargs={'data_format': 'channels_last'},\n         input_shape=(3, 4, 3, 4, 3))\n \n   def test_global_max_pooling_1d_keepdims(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GlobalMaxPooling1D,\n         kwargs={'keepdims': True},\n         input_shape=(3, 4, 5),\n         expected_output_shape=(None, 1, 5))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GlobalMaxPooling1D,\n         kwargs={'data_format': 'channels_first', 'keepdims': True},\n         input_shape=(3, 4, 5),\n         expected_output_shape=(None, 4, 1))\n \n   def test_global_max_pooling_2d_keepdims(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GlobalMaxPooling2D,\n         kwargs={'data_format': 'channels_first', 'keepdims': True},\n         input_shape=(3, 4, 5, 6),\n         expected_output_shape=(None, 4, 1, 1))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GlobalMaxPooling2D,\n         kwargs={'data_format': 'channels_last', 'keepdims': True},\n         input_shape=(3, 4, 5, 6),\n         expected_output_shape=(None, 1, 1, 6))\n \n   def test_global_max_pooling_3d_keepdims(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GlobalMaxPooling3D,\n         kwargs={'data_format': 'channels_first', 'keepdims': True},\n         input_shape=(3, 4, 3, 4, 3),\n         expected_output_shape=(None, 4, 1, 1, 1))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GlobalMaxPooling3D,\n         kwargs={'data_format': 'channels_last', 'keepdims': True},\n         input_shape=(3, 4, 3, 4, 3),\n\n@@ -16,25 +16,25 @@\n \n from absl.testing import parameterized\n import keras\n-from keras import combinations\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import tensorflow.compat.v2 as tf\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class MaxPoolingTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_max_pooling_1d(self):\n     for padding in ['valid', 'same']:\n       for stride in [1, 2]:\n-        testing_utils.layer_test(\n+        test_utils.layer_test(\n             keras.layers.MaxPooling1D,\n             kwargs={\n                 'strides': stride,\n                 'padding': padding\n             },\n             input_shape=(3, 5, 4))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.MaxPooling1D,\n         kwargs={'data_format': 'channels_first'},\n         input_shape=(3, 2, 6))\n@@ -42,7 +42,7 @@ class MaxPoolingTest(tf.test.TestCase, parameterized.TestCase):\n   def test_max_pooling_2d(self):\n     pool_size = (3, 3)\n     for strides in [(1, 1), (2, 2)]:\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.MaxPooling2D,\n           kwargs={\n               'strides': strides,\n@@ -53,7 +53,7 @@ class MaxPoolingTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_max_pooling_3d(self):\n     pool_size = (3, 3, 3)\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.MaxPooling3D,\n         kwargs={\n             'strides': 2,\n@@ -61,7 +61,7 @@ class MaxPoolingTest(tf.test.TestCase, parameterized.TestCase):\n             'pool_size': pool_size\n         },\n         input_shape=(3, 11, 12, 10, 4))\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.MaxPooling3D,\n         kwargs={\n             'strides': 3,\n\n@@ -18,14 +18,14 @@\n \n import keras\n from keras import backend\n-from keras import keras_parameterized\n-from keras import testing_utils\n from keras.distribute import strategy_combinations\n from keras.layers.preprocessing import category_encoding\n from keras.layers.preprocessing import preprocessing_test_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n-from tensorflow.python.framework import test_util\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n \n \n def batch_wrapper(dataset, batch_size, strategy, repeat=None):\n@@ -39,7 +39,7 @@ def batch_wrapper(dataset, batch_size, strategy, repeat=None):\n     return dataset.batch(batch_size)\n \n \n-@testing_utils.run_v2_only\n+@test_utils.run_v2_only\n @tf.__internal__.distribute.combinations.generate(\n     tf.__internal__.test.combinations.combine(\n         strategy=strategy_combinations.all_strategies +\n@@ -48,12 +48,12 @@ def batch_wrapper(dataset, batch_size, strategy, repeat=None):\n         strategy_combinations.parameter_server_strategies_multi_worker,\n         mode=[\"eager\"]))\n class CategoryEncodingDistributionTest(\n-    keras_parameterized.TestCase,\n+    test_combinations.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_strategy(self, strategy):\n     if (backend.is_tpu_strategy(strategy) and\n-        not test_util.is_mlir_bridge_enabled()):\n+        not tf_test_utils.is_mlir_bridge_enabled()):\n       self.skipTest(\"TPU tests require MLIR bridge\")\n \n     input_array = np.array([[1, 2, 3, 1], [0, 3, 1, 0]])\n\n@@ -18,16 +18,16 @@\n from absl.testing import parameterized\n import keras\n from keras import backend\n-from keras import keras_parameterized\n from keras.layers import core\n from keras.layers.preprocessing import category_encoding\n from keras.layers.preprocessing import preprocessing_test_utils\n+from keras.testing_infra import test_combinations\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class CategoryEncodingInputTest(keras_parameterized.TestCase,\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class CategoryEncodingInputTest(test_combinations.TestCase,\n                                 preprocessing_test_utils.PreprocessingLayerTest\n                                ):\n \n@@ -311,8 +311,8 @@ class CategoryEncodingInputTest(keras_parameterized.TestCase,\n     self.assertAllEqual(expected_output, output_dataset)\n \n \n-@keras_parameterized.run_all_keras_modes\n-class CategoryEncodingOutputTest(keras_parameterized.TestCase,\n+@test_combinations.run_all_keras_modes\n+class CategoryEncodingOutputTest(test_combinations.TestCase,\n                                  preprocessing_test_utils.PreprocessingLayerTest\n                                 ):\n \n@@ -491,7 +491,7 @@ class CategoryEncodingOutputTest(keras_parameterized.TestCase,\n \n \n class CategoryEncodingModelBuildingTest(\n-    keras_parameterized.TestCase,\n+    test_combinations.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n \n   @parameterized.named_parameters(\n\n@@ -17,16 +17,16 @@\n # pylint: disable=g-direct-tensorflow-import\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n from keras.distribute import strategy_combinations\n from keras.layers.preprocessing import discretization\n from keras.layers.preprocessing import preprocessing_test_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@testing_utils.run_v2_only\n+@test_utils.run_v2_only\n @tf.__internal__.distribute.combinations.generate(\n     tf.__internal__.test.combinations.combine(\n         strategy=strategy_combinations.all_strategies +\n@@ -35,7 +35,7 @@ import tensorflow.compat.v2 as tf\n         strategy_combinations.parameter_server_strategies_multi_worker,\n         mode=[\"eager\"]))\n class DiscretizationDistributionTest(\n-    keras_parameterized.TestCase,\n+    test_combinations.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_strategy(self, strategy):\n\n@@ -18,16 +18,16 @@ import os\n \n from absl.testing import parameterized\n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n from keras.layers.preprocessing import discretization\n from keras.layers.preprocessing import preprocessing_test_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class DiscretizationTest(keras_parameterized.TestCase,\n+@test_combinations.run_all_keras_modes\n+class DiscretizationTest(test_combinations.TestCase,\n                          preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_bucketize_with_explicit_buckets_integer(self):\n@@ -222,8 +222,8 @@ class DiscretizationTest(keras_parameterized.TestCase,\n       _ = discretization.Discretization(num_bins=5, bins=[1, 2])\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class DiscretizationAdaptTest(keras_parameterized.TestCase,\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class DiscretizationAdaptTest(test_combinations.TestCase,\n                               preprocessing_test_utils.PreprocessingLayerTest):\n \n   @parameterized.named_parameters([\n@@ -297,7 +297,7 @@ class DiscretizationAdaptTest(keras_parameterized.TestCase,\n     input_data = keras.Input(shape=input_shape)\n     output = layer(input_data)\n     model = keras.Model(input_data, output)\n-    model._run_eagerly = testing_utils.should_run_eagerly()\n+    model._run_eagerly = test_utils.should_run_eagerly()\n     output_data = model.predict(test_data)\n     self.assertAllClose(expected, output_data)\n \n\n@@ -18,15 +18,15 @@ import os\n from absl.testing import parameterized\n \n import keras\n-from keras import keras_parameterized\n from keras.layers.preprocessing import hashed_crossing\n from keras.layers.preprocessing import preprocessing_test_utils\n+from keras.testing_infra import test_combinations\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class HashedCrossingTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class HashedCrossingTest(test_combinations.TestCase):\n \n   @parameterized.named_parameters(\n       ('python_value', lambda x: x),\n\n@@ -18,17 +18,17 @@\n \n import keras\n from keras import backend\n-from keras import keras_parameterized\n-from keras import testing_utils\n from keras.distribute import strategy_combinations\n from keras.layers.preprocessing import hashing\n from keras.layers.preprocessing import preprocessing_test_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n-from tensorflow.python.framework import test_util\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n \n \n-@testing_utils.run_v2_only\n+@test_utils.run_v2_only\n @tf.__internal__.distribute.combinations.generate(\n     tf.__internal__.test.combinations.combine(\n         strategy=strategy_combinations.all_strategies +\n@@ -36,12 +36,12 @@ from tensorflow.python.framework import test_util\n         strategy_combinations.parameter_server_strategies_single_worker +\n         strategy_combinations.parameter_server_strategies_multi_worker,\n         mode=[\"eager\"]))\n-class HashingDistributionTest(keras_parameterized.TestCase,\n+class HashingDistributionTest(test_combinations.TestCase,\n                               preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_strategy(self, strategy):\n     if (backend.is_tpu_strategy(strategy) and\n-        not test_util.is_mlir_bridge_enabled()):\n+        not tf_test_utils.is_mlir_bridge_enabled()):\n       self.skipTest(\"TPU tests require MLIR bridge\")\n \n     input_data = np.asarray([[\"omar\"], [\"stringer\"], [\"marlo\"], [\"wire\"]])\n\n@@ -18,18 +18,18 @@ import os\n from absl.testing import parameterized\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n from keras.engine import input_layer\n from keras.engine import training\n from keras.layers.preprocessing import hashing\n from keras.layers.preprocessing import preprocessing_test_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class HashingTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class HashingTest(test_combinations.TestCase):\n \n   @parameterized.named_parameters(\n       ('list', list),\n@@ -356,7 +356,7 @@ class HashingTest(keras_parameterized.TestCase):\n     self.assertEqual(output_spec.shape.dims, input_shape.dims)\n     self.assertEqual(output_spec.dtype, tf.int64)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_config_with_custom_name(self):\n     layer = hashing.Hashing(num_bins=2, name='hashing')\n     config = layer.get_config()\n\n@@ -15,23 +15,23 @@\n \"\"\"Distribution tests for keras.layers.preprocessing.image_preprocessing.\"\"\"\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n from keras.distribute import strategy_combinations\n from keras.layers.preprocessing import image_preprocessing\n from keras.layers.preprocessing import preprocessing_test_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@testing_utils.run_v2_only\n+@test_utils.run_v2_only\n @tf.__internal__.distribute.combinations.generate(\n     tf.__internal__.test.combinations.combine(\n         strategy=strategy_combinations.all_strategies +\n         strategy_combinations.multi_worker_mirrored_strategies,\n         mode=[\"eager\", \"graph\"]))\n class ImagePreprocessingDistributionTest(\n-    keras_parameterized.TestCase,\n+    test_combinations.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_distribution(self, strategy):\n\n@@ -18,18 +18,18 @@ import functools\n from absl.testing import parameterized\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n from keras.engine import sequential\n from keras.layers.preprocessing import image_preprocessing\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n # pylint: disable=g-direct-tensorflow-import\n from tensorflow.python.ops import stateless_random_ops\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class ResizingTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class ResizingTest(test_combinations.TestCase):\n \n   def _run_test(self, kwargs, expected_height, expected_width):\n     np.random.seed(1337)\n@@ -38,8 +38,8 @@ class ResizingTest(keras_parameterized.TestCase):\n     orig_width = 8\n     channels = 3\n     kwargs.update({'height': expected_height, 'width': expected_width})\n-    with testing_utils.use_gpu():\n-      testing_utils.layer_test(\n+    with test_utils.use_gpu():\n+      test_utils.layer_test(\n           image_preprocessing.Resizing,\n           kwargs=kwargs,\n           input_shape=(num_samples, orig_height, orig_width, channels),\n@@ -86,7 +86,7 @@ class ResizingTest(keras_parameterized.TestCase):\n \n   def test_down_sampling_numeric(self):\n     for dtype in (np.int64, np.float32):\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         input_image = np.reshape(np.arange(0, 16), (1, 4, 4, 1)).astype(dtype)\n         layer = image_preprocessing.Resizing(\n             height=2, width=2, interpolation='nearest')\n@@ -102,7 +102,7 @@ class ResizingTest(keras_parameterized.TestCase):\n \n   def test_up_sampling_numeric(self):\n     for dtype in (np.int64, np.float32):\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         input_image = np.reshape(np.arange(0, 4), (1, 2, 2, 1)).astype(dtype)\n         layer = image_preprocessing.Resizing(\n             height=4, width=4, interpolation='nearest')\n@@ -135,7 +135,7 @@ class ResizingTest(keras_parameterized.TestCase):\n     self.assertEqual(layer_1.name, layer.name)\n \n   def test_crop_to_aspect_ratio(self):\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       input_image = np.reshape(np.arange(0, 16), (1, 4, 4, 1)).astype('float32')\n       layer = image_preprocessing.Resizing(4, 2, crop_to_aspect_ratio=True)\n       output_image = layer(input_image)\n@@ -149,7 +149,7 @@ class ResizingTest(keras_parameterized.TestCase):\n       self.assertAllEqual(expected_output, output_image)\n \n   def test_unbatched_image(self):\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       input_image = np.reshape(np.arange(0, 16), (4, 4, 1)).astype('float32')\n       layer = image_preprocessing.Resizing(2, 2, interpolation='nearest')\n       output_image = layer(input_image)\n@@ -163,7 +163,7 @@ class ResizingTest(keras_parameterized.TestCase):\n   @parameterized.named_parameters(('crop_to_aspect_ratio_false', False),\n                                   ('crop_to_aspect_ratio_true', True))\n   def test_ragged_image(self, crop_to_aspect_ratio):\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       inputs = tf.ragged.constant([\n           np.ones((8, 8, 1)),\n           np.ones((8, 4, 1)),\n@@ -184,7 +184,7 @@ class ResizingTest(keras_parameterized.TestCase):\n       self.assertNotIsInstance(outputs, tf.RaggedTensor)\n       self.assertAllEqual(expected_output, outputs)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_output_dtypes(self):\n     inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n     layer = image_preprocessing.Resizing(2, 2)\n@@ -217,7 +217,7 @@ class ResizingTest(keras_parameterized.TestCase):\n                                 img.shape.as_list()[-3:])\n       return img\n \n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       if batch:\n         input_shape = (2, input_height, input_width, channels)\n       else:\n@@ -236,8 +236,8 @@ def get_numpy_center_crop(images, expected_height, expected_width):\n   return images[:, height_start:height_end, width_start:width_end, :]\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class CenterCropTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class CenterCropTest(test_combinations.TestCase):\n \n   def _run_test(self, expected_height, expected_width):\n     np.random.seed(1337)\n@@ -250,8 +250,8 @@ class CenterCropTest(keras_parameterized.TestCase):\n         (num_samples, orig_height, orig_width, channels)).astype(np.float32)\n     expected_output = get_numpy_center_crop(input_images, expected_height,\n                                             expected_width)\n-    with testing_utils.use_gpu():\n-      testing_utils.layer_test(\n+    with test_utils.use_gpu():\n+      test_utils.layer_test(\n           image_preprocessing.CenterCrop,\n           kwargs=kwargs,\n           input_shape=(num_samples, orig_height, orig_width, channels),\n@@ -279,7 +279,7 @@ class CenterCropTest(keras_parameterized.TestCase):\n     np.random.seed(1337)\n     height, width = 10, 8\n     inp = np.random.random((12, 3, 3, 3))\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       layer = image_preprocessing.CenterCrop(height, width)\n       actual_output = layer(inp)\n       # In this case, output should equal resizing with crop_to_aspect ratio.\n@@ -295,7 +295,7 @@ class CenterCropTest(keras_parameterized.TestCase):\n     self.assertEqual(layer_1.name, layer.name)\n \n   def test_unbatched_image(self):\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       input_image = np.reshape(np.arange(0, 16), (4, 4, 1)).astype('float32')\n       layer = image_preprocessing.CenterCrop(2, 2)\n       output_image = layer(input_image)\n@@ -306,7 +306,7 @@ class CenterCropTest(keras_parameterized.TestCase):\n       expected_output = np.reshape(expected_output, (2, 2, 1))\n       self.assertAllEqual(expected_output, output_image)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_output_dtypes(self):\n     inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n     layer = image_preprocessing.CenterCrop(2, 2)\n@@ -315,8 +315,8 @@ class CenterCropTest(keras_parameterized.TestCase):\n     self.assertAllEqual(layer(inputs).dtype, 'uint8')\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class RandomCropTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class RandomCropTest(test_combinations.TestCase):\n \n   def _run_test(self, expected_height, expected_width):\n     np.random.seed(1337)\n@@ -325,8 +325,8 @@ class RandomCropTest(keras_parameterized.TestCase):\n     orig_width = 8\n     channels = 3\n     kwargs = {'height': expected_height, 'width': expected_width}\n-    with testing_utils.use_gpu():\n-      testing_utils.layer_test(\n+    with test_utils.use_gpu():\n+      test_utils.layer_test(\n           image_preprocessing.RandomCrop,\n           kwargs=kwargs,\n           input_shape=(num_samples, orig_height, orig_width, channels),\n@@ -337,7 +337,7 @@ class RandomCropTest(keras_parameterized.TestCase):\n     np.random.seed(1337)\n     height, width = 10, 8\n     inp = np.random.random((12, 3, 3, 3))\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       layer = image_preprocessing.RandomCrop(height, width)\n       actual_output = layer(inp)\n       # In this case, output should equal resizing with crop_to_aspect ratio.\n@@ -352,7 +352,7 @@ class RandomCropTest(keras_parameterized.TestCase):\n     height_offset = np.random.randint(low=0, high=3)\n     width_offset = np.random.randint(low=0, high=5)\n     mock_offset = [height_offset, width_offset]\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       layer = image_preprocessing.RandomCrop(height, width)\n       with tf.compat.v1.test.mock.patch.object(\n           layer._random_generator, 'random_uniform', return_value=mock_offset):\n@@ -377,7 +377,7 @@ class RandomCropTest(keras_parameterized.TestCase):\n     np.random.seed(1337)\n     height, width = 8, 16\n     inp = np.random.random((12, 8, 16, 3))\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       layer = image_preprocessing.RandomCrop(height, width)\n       actual_output = layer(inp, training=False)\n       self.assertAllClose(inp, actual_output)\n@@ -386,7 +386,7 @@ class RandomCropTest(keras_parameterized.TestCase):\n     np.random.seed(1337)\n     height, width = 3, 3\n     inp = np.random.random((12, 10, 6, 3))\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       layer = image_preprocessing.RandomCrop(height, width)\n       actual_output = layer(inp, training=False)\n       resized_inp = tf.image.resize(inp, size=[5, 3])\n@@ -397,7 +397,7 @@ class RandomCropTest(keras_parameterized.TestCase):\n     np.random.seed(1337)\n     height, width = 4, 6\n     inp = np.random.random((12, 8, 16, 3))\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       layer = image_preprocessing.RandomCrop(height, width)\n       actual_output = layer(inp, training=False)\n       resized_inp = tf.image.resize(inp, size=[4, 8])\n@@ -414,7 +414,7 @@ class RandomCropTest(keras_parameterized.TestCase):\n     np.random.seed(1337)\n     inp = np.random.random((16, 16, 3))\n     mock_offset = [2, 2]\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       layer = image_preprocessing.RandomCrop(8, 8)\n       with tf.compat.v1.test.mock.patch.object(\n           layer._random_generator,\n@@ -423,13 +423,13 @@ class RandomCropTest(keras_parameterized.TestCase):\n         actual_output = layer(inp, training=True)\n         self.assertAllClose(inp[2:10, 2:10, :], actual_output)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_uint8_input(self):\n     inputs = keras.Input((128, 128, 3), batch_size=2, dtype=tf.uint8)\n     layer = image_preprocessing.RandomCrop(64, 64)\n     self.assertAllEqual(layer(inputs).dtype, 'float32')\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_output_dtypes(self):\n     inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n     layer = image_preprocessing.RandomCrop(2, 2)\n@@ -438,25 +438,25 @@ class RandomCropTest(keras_parameterized.TestCase):\n     self.assertAllEqual(layer(inputs).dtype, 'uint8')\n \n \n-class RescalingTest(keras_parameterized.TestCase):\n+class RescalingTest(test_combinations.TestCase):\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_rescaling_base(self):\n     kwargs = {'scale': 1. / 127.5, 'offset': -1.}\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         image_preprocessing.Rescaling,\n         kwargs=kwargs,\n         input_shape=(2, 5, 6, 3),\n         expected_output_shape=(None, 5, 6, 3))\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_rescaling_correctness_float(self):\n     layer = image_preprocessing.Rescaling(scale=1. / 127.5, offset=-1.)\n     inputs = tf.random.uniform((2, 4, 5, 3))\n     outputs = layer(inputs)\n     self.assertAllClose(outputs.numpy(), inputs.numpy() * (1. / 127.5) - 1)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_rescaling_correctness_int(self):\n     layer = image_preprocessing.Rescaling(scale=1. / 127.5, offset=-1)\n     inputs = tf.random.uniform((2, 4, 5, 3), 0, 100, dtype='int32')\n@@ -470,14 +470,14 @@ class RescalingTest(keras_parameterized.TestCase):\n     layer_1 = image_preprocessing.Rescaling.from_config(config)\n     self.assertEqual(layer_1.name, layer.name)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_unbatched_image(self):\n     layer = image_preprocessing.Rescaling(scale=1. / 127.5, offset=-1)\n     inputs = tf.random.uniform((4, 5, 3))\n     outputs = layer(inputs)\n     self.assertAllClose(outputs.numpy(), inputs.numpy() * (1. / 127.5) - 1)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_output_dtypes(self):\n     inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n     layer = image_preprocessing.Rescaling(0.5)\n@@ -486,8 +486,8 @@ class RescalingTest(keras_parameterized.TestCase):\n     self.assertAllEqual(layer(inputs).dtype, 'uint8')\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class RandomFlipTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class RandomFlipTest(test_combinations.TestCase):\n \n   def _run_test(self, mode, expected_output=None, mock_random=None):\n     np.random.seed(1337)\n@@ -510,7 +510,7 @@ class RandomFlipTest(keras_parameterized.TestCase):\n         'stateless_random_uniform',\n         return_value=mock_random,\n     ):\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         layer = image_preprocessing.RandomFlip(mode)\n         actual_output = layer(inp, training=True)\n         self.assertAllClose(expected_output, actual_output)\n@@ -543,7 +543,7 @@ class RandomFlipTest(keras_parameterized.TestCase):\n   def test_random_flip_inference(self):\n     input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n     expected_output = input_images\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       layer = image_preprocessing.RandomFlip()\n       actual_output = layer(input_images, training=False)\n       self.assertAllClose(expected_output, actual_output)\n@@ -563,7 +563,7 @@ class RandomFlipTest(keras_parameterized.TestCase):\n         actual_output = layer(input_images, training=True)\n         self.assertAllClose(expected_output, actual_output)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_config_with_custom_name(self):\n     layer = image_preprocessing.RandomFlip(name='image_preproc')\n     config = layer.get_config()\n@@ -584,7 +584,7 @@ class RandomFlipTest(keras_parameterized.TestCase):\n         actual_output = layer(input_image, training=True)\n         self.assertAllClose(expected_output, actual_output)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_output_dtypes(self):\n     inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n     layer = image_preprocessing.RandomFlip()\n@@ -593,8 +593,8 @@ class RandomFlipTest(keras_parameterized.TestCase):\n     self.assertAllEqual(layer(inputs).dtype, 'uint8')\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class RandomContrastTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class RandomContrastTest(test_combinations.TestCase):\n \n   def _run_test(self, lower, upper, expected_output=None, mock_random=None):\n     np.random.seed(1337)\n@@ -616,7 +616,7 @@ class RandomContrastTest(keras_parameterized.TestCase):\n         'stateless_random_uniform',\n         return_value=mock_random,\n     ):\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         layer = image_preprocessing.RandomContrast((lower, upper))\n         actual_output = layer(inp, training=True)\n         self.assertAllClose(expected_output, actual_output)\n@@ -632,21 +632,21 @@ class RandomContrastTest(keras_parameterized.TestCase):\n                                   ('random_contrast_amplitude_5', 0.5))\n   def test_random_contrast_amplitude(self, amplitude):\n     input_images = np.random.random((2, 5, 8, 3))\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       layer = image_preprocessing.RandomContrast(amplitude)\n       layer(input_images)\n \n   def test_random_contrast_inference(self):\n     input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n     expected_output = input_images\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       layer = image_preprocessing.RandomContrast((0.1, 0.2))\n       actual_output = layer(input_images, training=False)\n       self.assertAllClose(expected_output, actual_output)\n \n   def test_random_contrast_int_dtype(self):\n     input_images = np.random.randint(low=0, high=255, size=(2, 5, 8, 3))\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       layer = image_preprocessing.RandomContrast((0.1, 0.2))\n       layer(input_images)\n \n@@ -660,7 +660,7 @@ class RandomContrastTest(keras_parameterized.TestCase):\n     with self.assertRaises(ValueError):\n       image_preprocessing.RandomContrast((0.1, -0.2))\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_config_with_custom_name(self):\n     layer = image_preprocessing.RandomContrast((.5, .6), name='image_preproc')\n     config = layer.get_config()\n@@ -679,12 +679,12 @@ class RandomContrastTest(keras_parameterized.TestCase):\n         'stateless_random_uniform',\n         return_value=mock_random,\n     ):\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         layer = image_preprocessing.RandomContrast((0.2, 0.5))\n         actual_output = layer(inp, training=True)\n         self.assertAllClose(expected_output, actual_output)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_output_dtypes(self):\n     inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n     layer = image_preprocessing.RandomContrast((.5, .6))\n@@ -693,8 +693,8 @@ class RandomContrastTest(keras_parameterized.TestCase):\n     self.assertAllEqual(layer(inputs).dtype, 'uint8')\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class RandomTranslationTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class RandomTranslationTest(test_combinations.TestCase):\n \n   def _run_test(self, height_factor, width_factor):\n     np.random.seed(1337)\n@@ -703,8 +703,8 @@ class RandomTranslationTest(keras_parameterized.TestCase):\n     orig_width = 8\n     channels = 3\n     kwargs = {'height_factor': height_factor, 'width_factor': width_factor}\n-    with testing_utils.use_gpu():\n-      testing_utils.layer_test(\n+    with test_utils.use_gpu():\n+      test_utils.layer_test(\n           image_preprocessing.RandomTranslation,\n           kwargs=kwargs,\n           input_shape=(num_samples, orig_height, orig_width, channels),\n@@ -718,7 +718,7 @@ class RandomTranslationTest(keras_parameterized.TestCase):\n \n   def test_random_translation_up_numeric_reflect(self):\n     for dtype in (np.int64, np.float32):\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(dtype)\n         # Shifting by -.2 * 5 = 1 pixel.\n         layer = image_preprocessing.RandomTranslation(\n@@ -736,7 +736,7 @@ class RandomTranslationTest(keras_parameterized.TestCase):\n \n   def test_random_translation_up_numeric_constant(self):\n     for dtype in (np.int64, np.float32):\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(dtype)\n         # Shifting by -.2 * 5 = 1 pixel.\n         layer = image_preprocessing.RandomTranslation(\n@@ -754,7 +754,7 @@ class RandomTranslationTest(keras_parameterized.TestCase):\n \n   def test_random_translation_down_numeric_reflect(self):\n     for dtype in (np.int64, np.float32):\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(dtype)\n         # Shifting by .2 * 5 = 1 pixel.\n         layer = image_preprocessing.RandomTranslation(\n@@ -772,7 +772,7 @@ class RandomTranslationTest(keras_parameterized.TestCase):\n \n   def test_random_translation_asymmetric_size_numeric_reflect(self):\n     for dtype in (np.int64, np.float32):\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         input_image = np.reshape(np.arange(0, 16), (1, 8, 2, 1)).astype(dtype)\n         # Shifting by .5 * 8 = 1 pixel.\n         layer = image_preprocessing.RandomTranslation(\n@@ -795,7 +795,7 @@ class RandomTranslationTest(keras_parameterized.TestCase):\n \n   def test_random_translation_down_numeric_constant(self):\n     for dtype in (np.int64, np.float32):\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(dtype)\n         # Shifting by -.2 * 5 = 1 pixel.\n         layer = image_preprocessing.RandomTranslation(\n@@ -813,7 +813,7 @@ class RandomTranslationTest(keras_parameterized.TestCase):\n \n   def test_random_translation_left_numeric_reflect(self):\n     for dtype in (np.int64, np.float32):\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(dtype)\n         # Shifting by .2 * 5 = 1 pixel.\n         layer = image_preprocessing.RandomTranslation(\n@@ -831,7 +831,7 @@ class RandomTranslationTest(keras_parameterized.TestCase):\n \n   def test_random_translation_left_numeric_constant(self):\n     for dtype in (np.int64, np.float32):\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         input_image = np.reshape(np.arange(0, 25), (1, 5, 5, 1)).astype(dtype)\n         # Shifting by -.2 * 5 = 1 pixel.\n         layer = image_preprocessing.RandomTranslation(\n@@ -850,12 +850,12 @@ class RandomTranslationTest(keras_parameterized.TestCase):\n   def test_random_translation_inference(self):\n     input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n     expected_output = input_images\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       layer = image_preprocessing.RandomTranslation(.5, .5)\n       actual_output = layer(input_images, training=False)\n       self.assertAllClose(expected_output, actual_output)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_config_with_custom_name(self):\n     layer = image_preprocessing.RandomTranslation(.5, .6, name='image_preproc')\n     config = layer.get_config()\n@@ -863,7 +863,7 @@ class RandomTranslationTest(keras_parameterized.TestCase):\n     self.assertEqual(layer_1.name, layer.name)\n \n   def test_unbatched_image(self):\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(np.int64)\n       # Shifting by -.2 * 5 = 1 pixel.\n       layer = image_preprocessing.RandomTranslation(\n@@ -879,7 +879,7 @@ class RandomTranslationTest(keras_parameterized.TestCase):\n       expected_output = np.reshape(expected_output, (5, 5, 1))\n       self.assertAllEqual(expected_output, output_image)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_output_dtypes(self):\n     inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n     layer = image_preprocessing.RandomTranslation(.5, .6)\n@@ -888,8 +888,8 @@ class RandomTranslationTest(keras_parameterized.TestCase):\n     self.assertAllEqual(layer(inputs).dtype, 'uint8')\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class RandomTransformTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class RandomTransformTest(test_combinations.TestCase):\n \n   def _run_random_transform_with_mock(self,\n                                       transform_matrix,\n@@ -1252,8 +1252,8 @@ class RandomTransformTest(keras_parameterized.TestCase):\n         interpolation='nearest')\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class RandomRotationTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class RandomRotationTest(test_combinations.TestCase):\n \n   def _run_test(self, factor):\n     np.random.seed(1337)\n@@ -1262,8 +1262,8 @@ class RandomRotationTest(keras_parameterized.TestCase):\n     orig_width = 8\n     channels = 3\n     kwargs = {'factor': factor}\n-    with testing_utils.use_gpu():\n-      testing_utils.layer_test(\n+    with test_utils.use_gpu():\n+      test_utils.layer_test(\n           image_preprocessing.RandomRotation,\n           kwargs=kwargs,\n           input_shape=(num_samples, orig_height, orig_width, channels),\n@@ -1278,7 +1278,7 @@ class RandomRotationTest(keras_parameterized.TestCase):\n   def test_random_rotation_inference(self):\n     input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n     expected_output = input_images\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       layer = image_preprocessing.RandomRotation(.5)\n       actual_output = layer(input_images, training=False)\n       self.assertAllClose(expected_output, actual_output)\n@@ -1286,7 +1286,7 @@ class RandomRotationTest(keras_parameterized.TestCase):\n   def test_distribution_strategy(self):\n     \"\"\"Tests that RandomRotation can be created within distribution strategies.\"\"\"\n     input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       strat = tf.distribute.MirroredStrategy(devices=['cpu', 'gpu'])\n       with strat.scope():\n         layer = image_preprocessing.RandomRotation(.5)\n@@ -1294,7 +1294,7 @@ class RandomRotationTest(keras_parameterized.TestCase):\n       values = output.values\n       self.assertAllEqual(2, len(values))\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_config_with_custom_name(self):\n     layer = image_preprocessing.RandomRotation(.5, name='image_preproc')\n     config = layer.get_config()\n@@ -1302,7 +1302,7 @@ class RandomRotationTest(keras_parameterized.TestCase):\n     self.assertEqual(layer_1.name, layer.name)\n \n   def test_unbatched_image(self):\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(np.float32)\n       # 180 rotation.\n       layer = image_preprocessing.RandomRotation(factor=(0.5, 0.5))\n@@ -1317,7 +1317,7 @@ class RandomRotationTest(keras_parameterized.TestCase):\n       expected_output = np.reshape(expected_output, (5, 5, 1))\n       self.assertAllClose(expected_output, output_image)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_output_dtypes(self):\n     inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n     layer = image_preprocessing.RandomRotation(.5)\n@@ -1326,8 +1326,8 @@ class RandomRotationTest(keras_parameterized.TestCase):\n     self.assertAllEqual(layer(inputs).dtype, 'uint8')\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class RandomZoomTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class RandomZoomTest(test_combinations.TestCase):\n \n   def _run_test(self, height_factor, width_factor):\n     np.random.seed(1337)\n@@ -1336,8 +1336,8 @@ class RandomZoomTest(keras_parameterized.TestCase):\n     orig_width = 8\n     channels = 3\n     kwargs = {'height_factor': height_factor, 'width_factor': width_factor}\n-    with testing_utils.use_gpu():\n-      testing_utils.layer_test(\n+    with test_utils.use_gpu():\n+      test_utils.layer_test(\n           image_preprocessing.RandomZoom,\n           kwargs=kwargs,\n           input_shape=(num_samples, orig_height, orig_width, channels),\n@@ -1357,7 +1357,7 @@ class RandomZoomTest(keras_parameterized.TestCase):\n \n   def test_random_zoom_in_numeric(self):\n     for dtype in (np.int64, np.float32):\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(dtype)\n         layer = image_preprocessing.RandomZoom((-.5, -.5), (-.5, -.5),\n                                                interpolation='nearest')\n@@ -1374,7 +1374,7 @@ class RandomZoomTest(keras_parameterized.TestCase):\n \n   def test_random_zoom_out_numeric(self):\n     for dtype in (np.int64, np.float32):\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(dtype)\n         layer = image_preprocessing.RandomZoom((.5, .5), (.8, .8),\n                                                fill_mode='constant',\n@@ -1392,7 +1392,7 @@ class RandomZoomTest(keras_parameterized.TestCase):\n \n   def test_random_zoom_out_numeric_preserve_aspect_ratio(self):\n     for dtype in (np.int64, np.float32):\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(dtype)\n         layer = image_preprocessing.RandomZoom((.5, .5),\n                                                fill_mode='constant',\n@@ -1411,12 +1411,12 @@ class RandomZoomTest(keras_parameterized.TestCase):\n   def test_random_zoom_inference(self):\n     input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n     expected_output = input_images\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       layer = image_preprocessing.RandomZoom(.5, .5)\n       actual_output = layer(input_images, training=False)\n       self.assertAllClose(expected_output, actual_output)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_config_with_custom_name(self):\n     layer = image_preprocessing.RandomZoom(.5, .6, name='image_preproc')\n     config = layer.get_config()\n@@ -1424,7 +1424,7 @@ class RandomZoomTest(keras_parameterized.TestCase):\n     self.assertEqual(layer_1.name, layer.name)\n \n   def test_unbatched_image(self):\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(np.int64)\n       layer = image_preprocessing.RandomZoom((-.5, -.5), (-.5, -.5),\n                                              interpolation='nearest')\n@@ -1439,7 +1439,7 @@ class RandomZoomTest(keras_parameterized.TestCase):\n       expected_output = np.reshape(expected_output, (5, 5, 1))\n       self.assertAllEqual(expected_output, output_image)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_output_dtypes(self):\n     inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n     layer = image_preprocessing.RandomZoom(.5, .5)\n@@ -1448,8 +1448,8 @@ class RandomZoomTest(keras_parameterized.TestCase):\n     self.assertAllEqual(layer(inputs).dtype, 'uint8')\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class RandomHeightTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class RandomHeightTest(test_combinations.TestCase):\n \n   def _run_test(self, factor):\n     np.random.seed(1337)\n@@ -1457,7 +1457,7 @@ class RandomHeightTest(keras_parameterized.TestCase):\n     orig_height = 5\n     orig_width = 8\n     channels = 3\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       img = np.random.random((num_samples, orig_height, orig_width, channels))\n       layer = image_preprocessing.RandomHeight(factor)\n       img_out = layer(img, training=True)\n@@ -1474,7 +1474,7 @@ class RandomHeightTest(keras_parameterized.TestCase):\n   def test_valid_random_height(self):\n     # need (maxval - minval) * rnd + minval = 0.6\n     mock_factor = 0.6\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       img = np.random.random((12, 5, 8, 3))\n       layer = image_preprocessing.RandomHeight(.4)\n       with tf.compat.v1.test.mock.patch.object(\n@@ -1484,7 +1484,7 @@ class RandomHeightTest(keras_parameterized.TestCase):\n \n   def test_random_height_longer_numeric(self):\n     for dtype in (np.int64, np.float32):\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         input_image = np.reshape(np.arange(0, 6), (2, 3, 1)).astype(dtype)\n         layer = image_preprocessing.RandomHeight(factor=(1., 1.))\n         # Return type of RandomHeight() is float32 if `interpolation` is not\n@@ -1504,7 +1504,7 @@ class RandomHeightTest(keras_parameterized.TestCase):\n \n   def test_random_height_shorter_numeric(self):\n     for dtype in (np.int64, np.float32):\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         input_image = np.reshape(np.arange(0, 8), (4, 2, 1)).astype(dtype)\n         layer = image_preprocessing.RandomHeight(\n             factor=(-.5, -.5), interpolation='nearest')\n@@ -1525,12 +1525,12 @@ class RandomHeightTest(keras_parameterized.TestCase):\n   def test_random_height_inference(self):\n     input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n     expected_output = input_images\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       layer = image_preprocessing.RandomHeight(.5)\n       actual_output = layer(input_images, training=False)\n       self.assertAllClose(expected_output, actual_output)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_config_with_custom_name(self):\n     layer = image_preprocessing.RandomHeight(.5, name='image_preproc')\n     config = layer.get_config()\n@@ -1540,7 +1540,7 @@ class RandomHeightTest(keras_parameterized.TestCase):\n   def test_unbatched_image(self):\n     # need (maxval - minval) * rnd + minval = 0.6\n     mock_factor = 0.6\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       img = np.random.random((5, 8, 3))\n       layer = image_preprocessing.RandomHeight(.4)\n       with tf.compat.v1.test.mock.patch.object(\n@@ -1548,7 +1548,7 @@ class RandomHeightTest(keras_parameterized.TestCase):\n         img_out = layer(img, training=True)\n         self.assertEqual(img_out.shape[0], 3)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_output_dtypes(self):\n     inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n     layer = image_preprocessing.RandomHeight(.2)\n@@ -1557,8 +1557,8 @@ class RandomHeightTest(keras_parameterized.TestCase):\n     self.assertAllEqual(layer(inputs).dtype, 'uint8')\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class RandomWidthTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class RandomWidthTest(test_combinations.TestCase):\n \n   def _run_test(self, factor):\n     np.random.seed(1337)\n@@ -1566,7 +1566,7 @@ class RandomWidthTest(keras_parameterized.TestCase):\n     orig_height = 5\n     orig_width = 8\n     channels = 3\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       img = np.random.random((num_samples, orig_height, orig_width, channels))\n       layer = image_preprocessing.RandomWidth(factor)\n       img_out = layer(img, training=True)\n@@ -1583,7 +1583,7 @@ class RandomWidthTest(keras_parameterized.TestCase):\n   def test_valid_random_width(self):\n     # need (maxval - minval) * rnd + minval = 0.6\n     mock_factor = 0.6\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       img = np.random.random((12, 8, 5, 3))\n       layer = image_preprocessing.RandomWidth(.4)\n       with tf.compat.v1.test.mock.patch.object(\n@@ -1593,7 +1593,7 @@ class RandomWidthTest(keras_parameterized.TestCase):\n \n   def test_random_width_longer_numeric(self):\n     for dtype in (np.int64, np.float32):\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         input_image = np.reshape(np.arange(0, 6), (3, 2, 1)).astype(dtype)\n         layer = image_preprocessing.RandomWidth(factor=(1., 1.))\n         # Return type of RandomWidth() is float32 if `interpolation` is not\n@@ -1612,7 +1612,7 @@ class RandomWidthTest(keras_parameterized.TestCase):\n \n   def test_random_width_shorter_numeric(self):\n     for dtype in (np.int64, np.float32):\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         input_image = np.reshape(np.arange(0, 8), (2, 4, 1)).astype(dtype)\n         layer = image_preprocessing.RandomWidth(\n             factor=(-.5, -.5), interpolation='nearest')\n@@ -1633,12 +1633,12 @@ class RandomWidthTest(keras_parameterized.TestCase):\n   def test_random_width_inference(self):\n     input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n     expected_output = input_images\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       layer = image_preprocessing.RandomWidth(.5)\n       actual_output = layer(input_images, training=False)\n       self.assertAllClose(expected_output, actual_output)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_config_with_custom_name(self):\n     layer = image_preprocessing.RandomWidth(.5, name='image_preproc')\n     config = layer.get_config()\n@@ -1648,7 +1648,7 @@ class RandomWidthTest(keras_parameterized.TestCase):\n   def test_unbatched_image(self):\n     # need (maxval - minval) * rnd + minval = 0.6\n     mock_factor = 0.6\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       img = np.random.random((8, 5, 3))\n       layer = image_preprocessing.RandomWidth(.4)\n       with tf.compat.v1.test.mock.patch.object(\n@@ -1656,7 +1656,7 @@ class RandomWidthTest(keras_parameterized.TestCase):\n         img_out = layer(img, training=True)\n         self.assertEqual(img_out.shape[1], 3)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_output_dtypes(self):\n     inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n     layer = image_preprocessing.RandomWidth(.2)\n@@ -1665,8 +1665,8 @@ class RandomWidthTest(keras_parameterized.TestCase):\n     self.assertAllEqual(layer(inputs).dtype, 'uint8')\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class LearningPhaseTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class LearningPhaseTest(test_combinations.TestCase):\n \n   def test_plain_call(self):\n     layer = image_preprocessing.RandomWidth(.5, seed=123)\n@@ -1698,8 +1698,8 @@ class LearningPhaseTest(keras_parameterized.TestCase):\n     self.assertEqual(tuple(int(i) for i in out.shape[1:]), shape)\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class DeterminismTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class DeterminismTest(test_combinations.TestCase):\n \n   @parameterized.named_parameters(\n       ('random_flip', image_preprocessing.RandomFlip),\n\n@@ -20,17 +20,17 @@ import os\n \n import keras\n from keras import backend\n-from keras import keras_parameterized\n-from keras import testing_utils\n from keras.distribute import strategy_combinations\n from keras.layers.preprocessing import index_lookup\n from keras.layers.preprocessing import preprocessing_test_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n-from tensorflow.python.framework import test_util\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n \n \n-@testing_utils.run_v2_only\n+@test_utils.run_v2_only\n @tf.__internal__.distribute.combinations.generate(\n     tf.__internal__.test.combinations.combine(\n         strategy=strategy_combinations.all_strategies +\n@@ -39,7 +39,7 @@ from tensorflow.python.framework import test_util\n         strategy_combinations.parameter_server_strategies_multi_worker,\n         mode=[\"eager\"]))\n class IndexLookupDistributionTest(\n-    keras_parameterized.TestCase,\n+    test_combinations.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n \n   def _write_to_temp_file(self, file_name, vocab_list):\n@@ -53,7 +53,7 @@ class IndexLookupDistributionTest(\n \n   def test_strategy(self, strategy):\n     if (backend.is_tpu_strategy(strategy) and\n-        not test_util.is_mlir_bridge_enabled()):\n+        not tf_test_utils.is_mlir_bridge_enabled()):\n       self.skipTest(\"TPU tests require MLIR bridge\")\n \n     vocab_data = [[\n@@ -86,7 +86,7 @@ class IndexLookupDistributionTest(\n \n   def test_strategy_with_file(self, strategy):\n     if (backend.is_tpu_strategy(strategy) and\n-        not test_util.is_mlir_bridge_enabled()):\n+        not tf_test_utils.is_mlir_bridge_enabled()):\n       self.skipTest(\"TPU tests require MLIR bridge\")\n \n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n\n@@ -23,10 +23,10 @@ import string\n from absl.testing import parameterized\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n from keras.layers.preprocessing import index_lookup\n from keras.layers.preprocessing import preprocessing_test_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.utils.generic_utils import CustomObjectScope\n import numpy as np\n import tensorflow.compat.v2 as tf\n@@ -294,8 +294,8 @@ def _get_end_to_end_test_cases():\n   return crossed_test_cases\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class IndexLookupLayerTest(keras_parameterized.TestCase,\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class IndexLookupLayerTest(test_combinations.TestCase,\n                            preprocessing_test_utils.PreprocessingLayerTest):\n \n   @parameterized.named_parameters(*_get_end_to_end_test_cases())\n@@ -331,7 +331,7 @@ class IndexLookupLayerTest(keras_parameterized.TestCase,\n           input_shape[0])\n \n     with CustomObjectScope({\"IndexLookup\": cls}):\n-      output_data = testing_utils.layer_test(\n+      output_data = test_utils.layer_test(\n           cls,\n           kwargs=kwargs,\n           input_shape=input_shape,\n@@ -346,9 +346,9 @@ class IndexLookupLayerTest(keras_parameterized.TestCase,\n       self.assertAllClose(expected_output, output_data)\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n class CategoricalEncodingInputTest(\n-    keras_parameterized.TestCase,\n+    test_combinations.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_sparse_string_input(self):\n@@ -461,9 +461,9 @@ class CategoricalEncodingInputTest(\n     self.assertAllEqual(expected_output, output_dataset)\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n class CategoricalEncodingMultiOOVTest(\n-    keras_parameterized.TestCase,\n+    test_combinations.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_sparse_string_input_multi_bucket(self):\n@@ -555,9 +555,9 @@ class CategoricalEncodingMultiOOVTest(\n     self.assertAllEqual(expected_output, output_dataset)\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n class CategoricalEncodingAdaptTest(\n-    keras_parameterized.TestCase,\n+    test_combinations.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_sparse_adapt(self):\n@@ -676,8 +676,8 @@ class CategoricalEncodingAdaptTest(\n     layer.adapt(batched_ds)\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class IndexLookupOutputTest(keras_parameterized.TestCase,\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class IndexLookupOutputTest(test_combinations.TestCase,\n                             preprocessing_test_utils.PreprocessingLayerTest):\n \n   def _write_to_temp_file(self, file_name, vocab_list):\n@@ -1357,8 +1357,8 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n     self.assertAllEqual(list(ds.as_numpy_iterator()), [[0], [1], [2]])\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class IndexLookupVocabularyTest(test_combinations.TestCase,\n                                 preprocessing_test_utils.PreprocessingLayerTest\n                                ):\n \n@@ -1640,9 +1640,9 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n           vocabulary_dtype=tf.int64)\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n class IndexLookupInverseVocabularyTest(\n-    keras_parameterized.TestCase,\n+    test_combinations.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_int_output_explicit_vocab(self):\n@@ -1763,8 +1763,8 @@ class IndexLookupInverseVocabularyTest(\n       layer.set_vocabulary(vocab_data)\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class IndexLookupErrorTest(keras_parameterized.TestCase,\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class IndexLookupErrorTest(test_combinations.TestCase,\n                            preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_too_long_vocab_fails_in_single_setting(self):\n@@ -1790,8 +1790,8 @@ class IndexLookupErrorTest(keras_parameterized.TestCase,\n           vocabulary_dtype=tf.string)\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class IndexLookupSavingTest(keras_parameterized.TestCase,\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class IndexLookupSavingTest(test_combinations.TestCase,\n                             preprocessing_test_utils.PreprocessingLayerTest):\n \n   def _write_to_temp_file(self, file_name, vocab_list):\n@@ -2210,7 +2210,7 @@ class IndexLookupSavingTest(keras_parameterized.TestCase,\n     self.assertAllEqual(tf.sparse.to_dense(output), expected_output)\n \n \n-class EagerExecutionDisabled(keras_parameterized.TestCase,\n+class EagerExecutionDisabled(test_combinations.TestCase,\n                              preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_lookup(self):\n@@ -2218,7 +2218,7 @@ class EagerExecutionDisabled(keras_parameterized.TestCase,\n     # which will call the layer in a legacy session. This could also happen\n     # directly if a user calls disable_v2_behavior or disable_eager_execution.\n     with tf.compat.v1.Session():\n-      with testing_utils.run_eagerly_scope(False):\n+      with test_utils.run_eagerly_scope(False):\n         vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n         input_array = np.array([\"earth\", \"wind\", \"and\", \"fire\"])\n         expected_output = [1, 2, 3, 4]\n\n@@ -25,8 +25,8 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.layers.preprocessing import integer_lookup\n from keras.layers.preprocessing import preprocessing_test_utils\n \n@@ -68,8 +68,8 @@ def _get_end_to_end_test_cases():\n   return crossed_test_cases\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class IntegerLookupLayerTest(keras_parameterized.TestCase,\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class IntegerLookupLayerTest(test_combinations.TestCase,\n                              preprocessing_test_utils.PreprocessingLayerTest):\n \n   @parameterized.named_parameters(*_get_end_to_end_test_cases())\n@@ -98,7 +98,7 @@ class IntegerLookupLayerTest(keras_parameterized.TestCase,\n       vocab_data = tf.data.Dataset.from_tensor_slices(vocab_data).batch(\n           input_shape[0])\n \n-    output_data = testing_utils.layer_test(\n+    output_data = test_utils.layer_test(\n         cls,\n         kwargs=kwargs,\n         input_shape=input_shape,\n@@ -118,9 +118,9 @@ class IntegerLookupLayerTest(keras_parameterized.TestCase,\n     self.assertEqual(output.numpy().tolist(), expected_output.tolist())\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n class CategoricalEncodingInputTest(\n-    keras_parameterized.TestCase,\n+    test_combinations.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_sparse_int_input(self):\n@@ -159,9 +159,9 @@ class CategoricalEncodingInputTest(\n     self.assertAllEqual(expected_output, output_dataset)\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n class CategoricalEncodingMultiOOVTest(\n-    keras_parameterized.TestCase,\n+    test_combinations.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_sparse_int_input_multi_bucket(self):\n@@ -205,9 +205,9 @@ class CategoricalEncodingMultiOOVTest(\n     self.assertAllEqual(expected_output, output_dataset)\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n class CategoricalEncodingAdaptTest(\n-    keras_parameterized.TestCase,\n+    test_combinations.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_sparse_adapt(self):\n@@ -246,8 +246,8 @@ class CategoricalEncodingAdaptTest(\n     layer.adapt(batched_ds)\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class IntegerLookupOutputTest(keras_parameterized.TestCase,\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class IntegerLookupOutputTest(test_combinations.TestCase,\n                               preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_int_output(self):\n@@ -375,9 +375,9 @@ class IntegerLookupOutputTest(keras_parameterized.TestCase,\n     self.assertAllEqual(expected_output, output_dataset)\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n class IntegerLookupVocabularyTest(\n-    keras_parameterized.TestCase,\n+    test_combinations.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n \n   def _write_to_temp_file(self, file_name, vocab_list):\n@@ -552,8 +552,8 @@ class IntegerLookupVocabularyTest(\n       fn()\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class IntegerLookupErrorTest(keras_parameterized.TestCase,\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class IntegerLookupErrorTest(test_combinations.TestCase,\n                              preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_too_long_vocab_fails_in_single_setting(self):\n@@ -569,8 +569,8 @@ class IntegerLookupErrorTest(keras_parameterized.TestCase,\n       _ = integer_lookup.IntegerLookup(max_tokens=0, num_oov_indices=1)\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class IntegerLookupSavingTest(keras_parameterized.TestCase,\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class IntegerLookupSavingTest(test_combinations.TestCase,\n                               preprocessing_test_utils.PreprocessingLayerTest):\n \n   def tearDown(self):\n\n@@ -16,11 +16,11 @@\n \n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n from keras.distribute import strategy_combinations\n from keras.layers.preprocessing import normalization\n from keras.layers.preprocessing import preprocessing_test_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n@@ -90,7 +90,7 @@ def _get_layer_computation_test_cases():\n   return crossed_test_cases\n \n \n-@testing_utils.run_v2_only\n+@test_utils.run_v2_only\n @tf.__internal__.distribute.combinations.generate(\n     tf.__internal__.test.combinations.times(\n         tf.__internal__.test.combinations.combine(\n@@ -99,7 +99,7 @@ def _get_layer_computation_test_cases():\n             strategy_combinations.parameter_server_strategies_single_worker +\n             strategy_combinations.parameter_server_strategies_multi_worker,\n             mode=[\"eager\"]), _get_layer_computation_test_cases()))\n-class NormalizationTest(keras_parameterized.TestCase,\n+class NormalizationTest(test_combinations.TestCase,\n                         preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_layer_computation(self, strategy, adapt_data, axis, test_data,\n\n@@ -23,8 +23,8 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.layers.preprocessing import normalization\n from keras.layers.preprocessing import preprocessing_test_utils\n from keras.mixed_precision import policy\n@@ -111,8 +111,8 @@ def _get_layer_computation_test_cases():\n   return crossed_test_cases\n \n \n-@keras_parameterized.run_all_keras_modes\n-class NormalizationTest(keras_parameterized.TestCase,\n+@test_combinations.run_all_keras_modes\n+class NormalizationTest(test_combinations.TestCase,\n                         preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_broadcasting_during_direct_setting(self):\n@@ -199,13 +199,13 @@ class NormalizationTest(keras_parameterized.TestCase,\n     self.assertAllEqual(output.dtype, tf.float64)\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class NormalizationAdaptTest(keras_parameterized.TestCase,\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class NormalizationAdaptTest(test_combinations.TestCase,\n                              preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_layer_api_compatibility(self):\n     cls = normalization.Normalization\n-    output_data = testing_utils.layer_test(\n+    output_data = test_utils.layer_test(\n         cls,\n         kwargs={\"axis\": -1},\n         input_shape=(None, 3),\n@@ -232,7 +232,7 @@ class NormalizationAdaptTest(keras_parameterized.TestCase,\n     input_data = keras.Input(shape=input_shape)\n     output = layer(input_data)\n     model = keras.Model(input_data, output)\n-    model._run_eagerly = testing_utils.should_run_eagerly()\n+    model._run_eagerly = test_utils.should_run_eagerly()\n     output_data = model.predict(test_data)\n     self.assertAllClose(expected, output_data)\n \n\n@@ -19,7 +19,7 @@ import tensorflow.compat.v2 as tf\n \n import time\n import numpy as np\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras.engine import base_preprocessing_layer\n from keras.engine.input_layer import Input\n from keras.layers import convolutional\n@@ -58,8 +58,8 @@ class PLSplit(PL):\n     return inputs + 1, inputs - 1\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class PreprocessingStageTest(keras_parameterized.TestCase,\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class PreprocessingStageTest(test_combinations.TestCase,\n                              preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_adapt_preprocessing_stage_with_single_input_output(self):\n\n@@ -19,15 +19,15 @@ import tensorflow.compat.v2 as tf\n \n import time\n import numpy as np\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras.engine import base_preprocessing_layer\n from keras.layers.preprocessing import preprocessing_stage\n from keras.layers.preprocessing import preprocessing_test_utils\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n class PreprocessingStageTest(\n-    keras_parameterized.TestCase,\n+    test_combinations.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_adapt(self):\n\n@@ -15,14 +15,14 @@\n \"\"\"Tests for preprocessing utils.\"\"\"\n \n from absl.testing import parameterized\n-from keras import keras_parameterized\n from keras.layers.preprocessing import preprocessing_utils\n+from keras.testing_infra import test_combinations\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class ListifyTensorsTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class ListifyTensorsTest(test_combinations.TestCase):\n \n   def test_tensor_input(self):\n     inputs = tf.constant([0, 1, 2, 3, 4])\n@@ -37,8 +37,8 @@ class ListifyTensorsTest(keras_parameterized.TestCase):\n     self.assertIsInstance(outputs, list)\n \n \n-@keras_parameterized.run_all_keras_modes\n-class EncodeCategoricalInputsTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class EncodeCategoricalInputsTest(test_combinations.TestCase):\n \n   def test_int_encoding(self):\n     inputs = tf.constant([0, 1, 2])\n\n@@ -21,8 +21,8 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.layers.preprocessing import preprocessing_test_utils\n from keras.layers.preprocessing import string_lookup\n \n@@ -62,8 +62,8 @@ def _get_end_to_end_test_cases():\n   return crossed_test_cases\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class StringLookupLayerTest(keras_parameterized.TestCase,\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class StringLookupLayerTest(test_combinations.TestCase,\n                             preprocessing_test_utils.PreprocessingLayerTest):\n \n   @parameterized.named_parameters(*_get_end_to_end_test_cases())\n@@ -92,7 +92,7 @@ class StringLookupLayerTest(keras_parameterized.TestCase,\n       vocab_data = tf.data.Dataset.from_tensor_slices(vocab_data).batch(\n           input_shape[0])\n \n-    output_data = testing_utils.layer_test(\n+    output_data = test_utils.layer_test(\n         cls,\n         kwargs=kwargs,\n         input_shape=input_shape,\n@@ -104,8 +104,8 @@ class StringLookupLayerTest(keras_parameterized.TestCase,\n     self.assertAllClose(expected_output, output_data)\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class StringLookupVocabularyTest(keras_parameterized.TestCase,\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class StringLookupVocabularyTest(test_combinations.TestCase,\n                                  preprocessing_test_utils.PreprocessingLayerTest\n                                 ):\n \n\n@@ -18,17 +18,17 @@\n \n import keras\n from keras import backend\n-from keras import keras_parameterized\n-from keras import testing_utils as keras_testing_utils\n from keras.distribute import strategy_combinations\n from keras.layers.preprocessing import preprocessing_test_utils\n from keras.layers.preprocessing import text_vectorization\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n-from tensorflow.python.framework import test_util\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n \n \n-@keras_testing_utils.run_v2_only\n+@test_utils.run_v2_only\n @tf.__internal__.distribute.combinations.generate(\n     tf.__internal__.test.combinations.combine(\n         strategy=strategy_combinations.all_strategies +\n@@ -37,12 +37,12 @@ from tensorflow.python.framework import test_util\n         strategy_combinations.parameter_server_strategies_multi_worker,\n         mode=[\"eager\"]))\n class TextVectorizationDistributionTest(\n-    keras_parameterized.TestCase,\n+    test_combinations.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_distribution_strategy_output(self, strategy):\n     if (backend.is_tpu_strategy(strategy) and\n-        not test_util.is_mlir_bridge_enabled()):\n+        not tf_test_utils.is_mlir_bridge_enabled()):\n       self.skipTest(\"TPU tests require MLIR bridge\")\n \n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n\n@@ -24,8 +24,8 @@ import numpy as np\n \n import keras\n from keras import backend\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.layers import convolutional\n from keras.layers import core\n from keras.layers import embeddings\n@@ -266,9 +266,9 @@ def _get_end_to_end_test_cases():\n   return crossed_test_cases\n \n \n-@testing_utils.run_v2_only\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class TextVectorizationLayerTest(keras_parameterized.TestCase,\n+@test_utils.run_v2_only\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class TextVectorizationLayerTest(test_combinations.TestCase,\n                                  preprocessing_test_utils.PreprocessingLayerTest\n                                 ):\n \n@@ -300,7 +300,7 @@ class TextVectorizationLayerTest(keras_parameterized.TestCase,\n       vocab_data = tf.data.Dataset.from_tensor_slices(vocab_data).batch(\n           input_shape[0])\n \n-    output_data = testing_utils.layer_test(\n+    output_data = test_utils.layer_test(\n         cls,\n         kwargs=kwargs,\n         input_shape=input_shape,\n@@ -480,10 +480,10 @@ class TextVectorizationLayerTest(keras_parameterized.TestCase,\n     self.assertAllEqual(expected, output)\n \n \n-@testing_utils.run_v2_only\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+@test_utils.run_v2_only\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n class TextVectorizationPreprocessingTest(\n-    keras_parameterized.TestCase,\n+    test_combinations.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n \n   def _write_to_temp_file(self, file_name, vocab_list):\n@@ -860,10 +860,10 @@ class TextVectorizationPreprocessingTest(\n     self.assertAllEqual(expected_output, output_dataset)\n \n \n-@testing_utils.run_v2_only\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+@test_utils.run_v2_only\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n class TextVectorizationDistributionTest(\n-    keras_parameterized.TestCase,\n+    test_combinations.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_distribution_strategy_output(self):\n@@ -888,10 +888,10 @@ class TextVectorizationDistributionTest(\n     self.assertAllEqual(expected_output, output_dataset)\n \n \n-@testing_utils.run_v2_only\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+@test_utils.run_v2_only\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n class TextVectorizationOutputTest(\n-    keras_parameterized.TestCase,\n+    test_combinations.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_int_output(self):\n@@ -1449,10 +1449,10 @@ class TextVectorizationOutputTest(\n     _ = layer(input_array)\n \n \n-@testing_utils.run_v2_only\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+@test_utils.run_v2_only\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n class TextVectorizationModelBuildingTest(\n-    keras_parameterized.TestCase,\n+    test_combinations.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n \n   @parameterized.named_parameters(\n@@ -1534,10 +1534,10 @@ class TextVectorizationModelBuildingTest(\n     _ = model.predict(input_array)\n \n \n-@testing_utils.run_v2_only\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+@test_utils.run_v2_only\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n class TextVectorizationVocbularyTest(\n-    keras_parameterized.TestCase,\n+    test_combinations.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest,\n ):\n \n@@ -1565,9 +1565,9 @@ class TextVectorizationVocbularyTest(\n         [\"earth\", \"wind\", \"and\", \"fire\"])\n \n \n-@testing_utils.run_v2_only\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class TextVectorizationErrorTest(keras_parameterized.TestCase,\n+@test_utils.run_v2_only\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class TextVectorizationErrorTest(test_combinations.TestCase,\n                                  preprocessing_test_utils.PreprocessingLayerTest\n                                 ):\n \n@@ -1695,10 +1695,10 @@ def custom_split_fn(x):\n   return tf.strings.split(x, sep=\">\")\n \n \n-@testing_utils.run_v2_only\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+@test_utils.run_v2_only\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n class TextVectorizationSavingTest(\n-    keras_parameterized.TestCase,\n+    test_combinations.TestCase,\n     preprocessing_test_utils.PreprocessingLayerTest):\n \n   def tearDown(self):\n@@ -1878,9 +1878,9 @@ class TextVectorizationSavingTest(\n     self.assertAllEqual(expected_output, new_output_dataset)\n \n \n-@testing_utils.run_v2_only\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class TextVectorizationE2ETest(keras_parameterized.TestCase,\n+@test_utils.run_v2_only\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class TextVectorizationE2ETest(test_combinations.TestCase,\n                                preprocessing_test_utils.PreprocessingLayerTest):\n \n   def test_keras_vocab_trimming_example(self):\n\n@@ -25,8 +25,8 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.engine import base_layer_utils\n from keras.layers import recurrent as rnn_v1\n from keras.layers import recurrent_v2 as rnn_v2\n@@ -39,8 +39,8 @@ NestedInput = collections.namedtuple('NestedInput', ['t1', 't2'])\n NestedState = collections.namedtuple('NestedState', ['s1', 's2'])\n \n \n-@keras_parameterized.run_all_keras_modes\n-class RNNTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class RNNTest(test_combinations.TestCase):\n \n   def test_minimal_rnn_cell_non_layer(self):\n \n@@ -66,7 +66,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n \n     # Test stacking.\n@@ -79,7 +79,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n \n   def test_minimal_rnn_cell_non_layer_multiple_states(self):\n@@ -109,7 +109,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n \n     # Test stacking.\n@@ -124,7 +124,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n \n   def test_minimal_rnn_cell_layer(self):\n@@ -166,7 +166,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n \n     # Test basic case serialization.\n@@ -192,7 +192,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n \n     # Test stacked RNN serialization.\n@@ -248,7 +248,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer=\"rmsprop\",\n         loss=\"mse\",\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n \n     # Test stacking.\n@@ -261,7 +261,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n \n   def test_rnn_with_time_major(self):\n@@ -289,7 +289,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(\n         np.zeros((batch, time_step, embedding_dim)),\n         np.zeros((batch, time_step, units)))\n@@ -309,7 +309,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(\n         np.zeros((batch, time_step, embedding_dim)),\n         np.zeros((batch, time_step, cell_units[-1])))\n@@ -326,7 +326,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(\n         np.zeros((batch, time_step, embedding_dim)),\n         np.zeros((batch, time_step, units)))\n@@ -340,7 +340,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(\n         np.zeros((batch, time_step, embedding_dim)),\n         np.zeros((batch, time_step, units)))\n@@ -374,7 +374,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(\n         [np.zeros((6, 5, 5)), np.zeros((6, 3))],\n         np.zeros((6, 32))\n@@ -414,7 +414,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(\n         [np.zeros((6, 5, 5)), np.zeros((6, 3))],\n         np.zeros((6, 32))\n@@ -430,7 +430,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(\n         [np.zeros((6, 5, 5)), np.zeros((6, 3))],\n         np.zeros((6, 32))\n@@ -462,7 +462,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n \n     # Test stacking.\n@@ -475,7 +475,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n \n   def test_rnn_cell_with_constants_layer_passing_initial_state(self):\n@@ -490,7 +490,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(\n         [np.zeros((6, 5, 5)), np.zeros((6, 32)), np.zeros((6, 3))],\n         np.zeros((6, 32))\n@@ -539,7 +539,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n \n     # Test stacking.\n@@ -555,7 +555,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 32)))\n \n   def test_stacked_rnn_attributes(self):\n@@ -695,7 +695,7 @@ class RNNTest(keras_parameterized.TestCase):\n       model.compile(\n           optimizer='rmsprop',\n           loss='mse',\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n \n       # Test basic case serialization.\n       x_np = np.random.random((6, 5, 5))\n@@ -719,7 +719,7 @@ class RNNTest(keras_parameterized.TestCase):\n       model.compile(\n           optimizer='rmsprop',\n           loss='mse',\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n \n       # Test stacked RNN serialization.\n       x_np = np.random.random((6, 5, 5))\n@@ -734,7 +734,7 @@ class RNNTest(keras_parameterized.TestCase):\n       self.assertAllClose(y_np, y_np_2, atol=1e-4)\n \n   @parameterized.named_parameters(\n-      *testing_utils.generate_combinations_with_testcase_name(\n+      *test_utils.generate_combinations_with_testcase_name(\n           layer=[rnn_v1.SimpleRNN, rnn_v1.GRU, rnn_v1.LSTM,\n                  rnn_v2.GRU, rnn_v2.LSTM],\n           unroll=[True, False]))\n@@ -749,13 +749,13 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     x_np = np.random.random((6, 5, 5))\n     y_np = np.random.random((6, 3))\n     model.train_on_batch(x_np, y_np)\n \n   @parameterized.named_parameters(\n-      *testing_utils.generate_combinations_with_testcase_name(\n+      *test_utils.generate_combinations_with_testcase_name(\n           cell=[keras.layers.SimpleRNNCell, keras.layers.GRUCell,\n                 keras.layers.LSTMCell],\n           unroll=[True, False]))\n@@ -773,7 +773,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     x_np = np.random.random((6, 5, 5))\n     y_np = np.random.random((6, 3))\n     model.train_on_batch(x_np, y_np)\n@@ -930,7 +930,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(\n         [np.zeros((batch, t, i1)),\n          np.zeros((batch, t, i2, i3))],\n@@ -964,7 +964,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(\n         [np.zeros((batch, t, i1)),\n          np.zeros((batch, t, i2, i3))],\n@@ -981,7 +981,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(x, y, epochs=1, batch_size=1)\n \n     # check whether the model variables are present in the\n@@ -1015,7 +1015,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(\n         np.zeros((batch, time_step, input_a, input_b)),\n         np.zeros((batch, unit_a, unit_b)))\n@@ -1033,7 +1033,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(\n         np.zeros((batch, time_step, input_a, input_b)),\n         np.zeros((batch, unit_a * 4, unit_b * 4)))\n@@ -1058,7 +1058,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch([\n         np.zeros((batch, time_step, input_a, input_b)),\n         np.zeros((batch, unit_a, unit_b))\n@@ -1096,7 +1096,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(\n         np.zeros((batch, time_step, input_size)),\n         np.zeros((batch, input_size)))\n@@ -1155,7 +1155,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(\n         [np.zeros((batch, t, i1)), np.zeros((batch, t, i2, i3))],\n         [np.zeros((batch, o1)), np.zeros((batch, o2, o3))])\n@@ -1179,7 +1179,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(\n         [np.zeros((batch, t, i1)),\n          np.zeros((batch, t, i2, i3))],\n@@ -1210,7 +1210,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(\n         [np.zeros((batch, t, i1)),\n          np.zeros((batch, t, i2, i3))],\n@@ -1236,7 +1236,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(\n         [np.zeros((batch, t, i1)),\n          np.zeros((batch, t, i2, i3))],\n@@ -1271,7 +1271,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(\n         [np.zeros((batch, t, i1)),\n          np.zeros((batch, t, i2, i3)),\n@@ -1304,7 +1304,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(\n         [np.zeros((batch, t, i1)),\n          np.zeros((batch, t, i2, i3)),\n@@ -1381,7 +1381,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     # last time step masked\n     x_np = np.array([[[1.], [2.], [0.]]])\n@@ -1407,7 +1407,7 @@ class RNNTest(keras_parameterized.TestCase):\n       model.compile(\n           optimizer='rmsprop',\n           loss='mse',\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n \n       np_x = np.ones((6, 5, 5))\n       result_1 = model.predict(np_x)\n@@ -1431,7 +1431,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     np_x = np.ones((6, 1, 5))\n     result = model.predict(np_x)\n@@ -1494,7 +1494,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(\n         np.zeros((batch, timesteps, input_dim)),\n         np.zeros((batch, output_dim)))\n@@ -1529,7 +1529,7 @@ class RNNTest(keras_parameterized.TestCase):\n       model = keras.Model(input_layer, rnn_output)\n       model.compile(\n           optimizer='rmsprop', loss='mse',\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n       return model\n \n     # Define a model with a constant state initialization\n@@ -1627,14 +1627,14 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(\n         np.zeros((batch, timesteps, input_dim)),\n         np.zeros((batch, output_dim)))\n     model.predict(np.ones((batch, timesteps, input_dim)))\n \n   @parameterized.named_parameters(\n-      *testing_utils.generate_combinations_with_testcase_name(layer=[\n+      *test_utils.generate_combinations_with_testcase_name(layer=[\n           rnn_v1.SimpleRNN, rnn_v1.GRU, rnn_v1.LSTM, rnn_v2.GRU, rnn_v2.LSTM\n       ]))\n   def test_rnn_with_ragged_input(self, layer):\n@@ -1695,7 +1695,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(ragged_data, label_data)\n \n     # Test stateful and full shape specification\n@@ -1707,7 +1707,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(ragged_data, label_data)\n \n     # Must raise error when unroll is set to True\n@@ -1796,7 +1796,7 @@ class RNNTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(np.zeros((6, 5, 5)), np.zeros((6, 5)))\n \n   @parameterized.parameters(\n\n@@ -24,14 +24,14 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.layers import embeddings\n from keras.layers import recurrent_v2 as rnn_v2\n \n \n-@keras_parameterized.run_all_keras_modes\n-class RNNV2Test(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class RNNV2Test(test_combinations.TestCase):\n \n   @parameterized.parameters([rnn_v2.LSTM, rnn_v2.GRU])\n   def test_device_placement(self, layer):\n@@ -47,7 +47,7 @@ class RNNV2Test(keras_parameterized.TestCase):\n \n     # Test when GPU is available but not used, the graph should be properly\n     # created with CPU ops.\n-    with testing_utils.device(should_use_gpu=False):\n+    with test_utils.device(should_use_gpu=False):\n       model = keras.Sequential([\n           keras.layers.Embedding(vocab_size, embedding_dim,\n                                  batch_input_shape=[batch_size, timestep]),\n@@ -57,7 +57,7 @@ class RNNV2Test(keras_parameterized.TestCase):\n       model.compile(\n           optimizer='adam',\n           loss='sparse_categorical_crossentropy',\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n       model.fit(x, y, epochs=1, shuffle=False)\n \n   @parameterized.parameters([rnn_v2.LSTM, rnn_v2.GRU])\n@@ -120,7 +120,7 @@ class RNNV2Test(keras_parameterized.TestCase):\n     lstm(embedded_inputs)\n \n   @parameterized.parameters([rnn_v2.LSTM, rnn_v2.GRU])\n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_compare_ragged_with_masks(self, layer):\n     vocab_size = 100\n     timestep = 20\n@@ -133,9 +133,9 @@ class RNNV2Test(keras_parameterized.TestCase):\n     data_ragged = tf.ragged.boolean_mask(data, mask)\n \n     outputs = []\n-    devices = [testing_utils.device(should_use_gpu=False)]\n+    devices = [test_utils.device(should_use_gpu=False)]\n     if tf.test.is_gpu_available():\n-      devices.append(testing_utils.device(should_use_gpu=True))\n+      devices.append(test_utils.device(should_use_gpu=True))\n     for device in devices:\n       with device:\n         outputs.append(tf.boolean_mask(layer(embedder(data), mask=mask), mask))\n\n@@ -15,13 +15,13 @@\n \"\"\"Tests for activity regularization layer.\"\"\"\n \n import keras\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class ActivityRegularizationTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class ActivityRegularizationTest(test_combinations.TestCase):\n \n   def test_activity_regularization(self):\n     layer = keras.layers.ActivityRegularization(l1=0.1)\n\n@@ -15,17 +15,17 @@\n \"\"\"Tests for alpha dropout layer.\"\"\"\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class AlphaDropoutTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class AlphaDropoutTest(test_combinations.TestCase):\n \n   def test_AlphaDropout(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.AlphaDropout, kwargs={'rate': 0.2}, input_shape=(3, 2, 3))\n \n   def _make_model(self, dtype):\n@@ -41,7 +41,7 @@ class AlphaDropoutTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(np.zeros((8, 32)), np.zeros((8, 8)))\n \n   def test_alpha_dropout_float32(self):\n\n@@ -17,20 +17,20 @@\n import os\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class DropoutTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class DropoutTest(test_combinations.TestCase):\n \n   def test_dropout(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.Dropout, kwargs={'rate': 0.5}, input_shape=(3, 2))\n \n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.Dropout,\n         kwargs={\n             'rate': 0.5,\n\n@@ -15,17 +15,17 @@\n \"\"\"Tests for gaussian dropout layer.\"\"\"\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class NoiseLayersTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class NoiseLayersTest(test_combinations.TestCase):\n \n   def test_GaussianDropout(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GaussianDropout,\n         kwargs={'rate': 0.5},\n         input_shape=(3, 2, 3))\n@@ -43,7 +43,7 @@ class NoiseLayersTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(np.zeros((8, 32)), np.zeros((8, 8)))\n \n   def test_gaussian_dropout_float32(self):\n\n@@ -15,17 +15,17 @@\n \"\"\"Tests for gaussian noise layer.\"\"\"\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class NoiseLayersTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class NoiseLayersTest(test_combinations.TestCase):\n \n   def test_GaussianNoise(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.GaussianNoise,\n         kwargs={'stddev': 1.},\n         input_shape=(3, 2, 3))\n@@ -43,7 +43,7 @@ class NoiseLayersTest(keras_parameterized.TestCase):\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(np.zeros((8, 32)), np.zeros((8, 8)))\n \n   def test_gaussian_noise_float32(self):\n\n@@ -15,27 +15,27 @@\n \"\"\"Tests for spatial dropout layers.\"\"\"\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class SpacialDropoutTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class SpacialDropoutTest(test_combinations.TestCase):\n \n   def test_spatial_dropout_1d(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.SpatialDropout1D,\n         kwargs={'rate': 0.5},\n         input_shape=(2, 3, 4))\n \n   def test_spatial_dropout_2d(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.SpatialDropout2D,\n         kwargs={'rate': 0.5},\n         input_shape=(2, 3, 4, 5))\n \n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.SpatialDropout2D,\n         kwargs={\n             'rate': 0.5,\n@@ -44,12 +44,12 @@ class SpacialDropoutTest(keras_parameterized.TestCase):\n         input_shape=(2, 3, 4, 5))\n \n   def test_spatial_dropout_3d(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.SpatialDropout3D,\n         kwargs={'rate': 0.5},\n         input_shape=(2, 3, 4, 4, 5))\n \n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.SpatialDropout3D,\n         kwargs={\n             'rate': 0.5,\n\n@@ -15,14 +15,14 @@\n \"\"\"Tests for cropping layers.\"\"\"\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class CroppingTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class CroppingTest(test_combinations.TestCase):\n \n   def test_cropping_1d(self):\n     num_samples = 2\n@@ -31,7 +31,7 @@ class CroppingTest(keras_parameterized.TestCase):\n     inputs = np.random.rand(num_samples, time_length, input_len_dim1)\n \n     with self.cached_session():\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.Cropping1D,\n           kwargs={'cropping': (1, 1)},\n           input_shape=inputs.shape)\n@@ -62,7 +62,7 @@ class CroppingTest(keras_parameterized.TestCase):\n                                 stack_size)\n       with self.cached_session():\n         # basic test\n-        testing_utils.layer_test(\n+        test_utils.layer_test(\n             keras.layers.Cropping2D,\n             kwargs={'cropping': cropping,\n                     'data_format': data_format},\n@@ -134,7 +134,7 @@ class CroppingTest(keras_parameterized.TestCase):\n                                   input_len_dim3, stack_size)\n         # basic test\n         with self.cached_session():\n-          testing_utils.layer_test(\n+          test_utils.layer_test(\n               keras.layers.Cropping3D,\n               kwargs={'cropping': cropping,\n                       'data_format': data_format},\n\n@@ -15,22 +15,22 @@\n \"\"\"Tests for flatten layer.\"\"\"\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class FlattenTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class FlattenTest(test_combinations.TestCase):\n \n   def test_flatten(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.Flatten, kwargs={}, input_shape=(3, 2, 4))\n \n     # Test channels_first\n     inputs = np.random.random((10, 3, 5, 5)).astype('float32')\n-    outputs = testing_utils.layer_test(\n+    outputs = test_utils.layer_test(\n         keras.layers.Flatten,\n         kwargs={'data_format': 'channels_first'},\n         input_data=inputs)\n@@ -39,11 +39,11 @@ class FlattenTest(keras_parameterized.TestCase):\n     self.assertAllClose(outputs, target_outputs)\n \n   def test_flatten_scalar_channels(self):\n-    testing_utils.layer_test(keras.layers.Flatten, kwargs={}, input_shape=(3,))\n+    test_utils.layer_test(keras.layers.Flatten, kwargs={}, input_shape=(3,))\n \n     # Test channels_first\n     inputs = np.random.random((10,)).astype('float32')\n-    outputs = testing_utils.layer_test(\n+    outputs = test_utils.layer_test(\n         keras.layers.Flatten,\n         kwargs={'data_format': 'channels_first'},\n         input_data=inputs)\n\n@@ -15,29 +15,29 @@\n \"\"\"Tests for Keras permute layer.\"\"\"\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n \n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class PermuteTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class PermuteTest(test_combinations.TestCase):\n \n   def test_permute(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.Permute, kwargs={'dims': (2, 1)}, input_shape=(3, 2, 4))\n \n   def test_permute_errors_on_invalid_starting_dims_index(self):\n     with self.assertRaisesRegex(ValueError, r'Invalid permutation .*dims.*'):\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.Permute,\n           kwargs={'dims': (0, 1, 2)},\n           input_shape=(3, 2, 4))\n \n   def test_permute_errors_on_invalid_set_of_dims_indices(self):\n     with self.assertRaisesRegex(ValueError, r'Invalid permutation .*dims.*'):\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.Permute,\n           kwargs={'dims': (1, 4, 2)},\n           input_shape=(3, 2, 4))\n\n@@ -15,18 +15,18 @@\n \"\"\"Tests for repeat vector layer.\"\"\"\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n \n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class RepeatVectorTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class RepeatVectorTest(test_combinations.TestCase):\n \n   def test_repeat_vector(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.RepeatVector, kwargs={'n': 3}, input_shape=(3, 2))\n \n   def test_numpy_inputs(self):\n\n@@ -15,32 +15,32 @@\n \"\"\"Tests for reshape layer.\"\"\"\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n \n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class ReshapeTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class ReshapeTest(test_combinations.TestCase):\n \n   def test_reshape(self):\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.Reshape,\n         kwargs={'target_shape': (8, 1)},\n         input_shape=(3, 2, 4))\n \n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.Reshape,\n         kwargs={'target_shape': (-1, 1)},\n         input_shape=(3, 2, 4))\n \n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.Reshape,\n         kwargs={'target_shape': (1, -1)},\n         input_shape=(3, 2, 4))\n \n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.Reshape,\n         kwargs={'target_shape': (-1, 1)},\n         input_shape=(None, None, 2))\n\n@@ -16,22 +16,22 @@\n # pylint: disable=g-direct-tensorflow-import\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n-from tensorflow.python.framework import test_util\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n \n \n-@test_util.for_all_test_methods(test_util.disable_xla,\n+@tf_test_utils.for_all_test_methods(tf_test_utils.disable_xla,\n                                     'align_corners=False not supported by XLA')\n-@keras_parameterized.run_all_keras_modes\n-class UpSamplingTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class UpSamplingTest(test_combinations.TestCase):\n \n   def test_upsampling_1d(self):\n     with self.cached_session():\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.UpSampling1D, kwargs={'size': 2}, input_shape=(3, 5, 4))\n \n   def test_upsampling_2d(self):\n@@ -50,7 +50,7 @@ class UpSamplingTest(keras_parameterized.TestCase):\n \n       # basic test\n       with self.cached_session():\n-        testing_utils.layer_test(\n+        test_utils.layer_test(\n             keras.layers.UpSampling2D,\n             kwargs={'size': (2, 2),\n                     'data_format': data_format},\n@@ -96,7 +96,7 @@ class UpSamplingTest(keras_parameterized.TestCase):\n         inputs = np.random.rand(num_samples, input_num_row, input_num_col,\n                                 stack_size)\n \n-      testing_utils.layer_test(keras.layers.UpSampling2D,\n+      test_utils.layer_test(keras.layers.UpSampling2D,\n                             kwargs={'size': (2, 2),\n                                     'data_format': data_format,\n                                     'interpolation': 'bilinear'},\n@@ -135,7 +135,7 @@ class UpSamplingTest(keras_parameterized.TestCase):\n \n       # basic test\n       with self.cached_session():\n-        testing_utils.layer_test(\n+        test_utils.layer_test(\n             keras.layers.UpSampling3D,\n             kwargs={'size': (2, 2, 2),\n                     'data_format': data_format},\n\n@@ -16,14 +16,14 @@\n \n from absl.testing import parameterized\n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class ZeroPaddingTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class ZeroPaddingTest(test_combinations.TestCase):\n \n   def test_zero_padding_1d(self):\n     num_samples = 2\n@@ -34,11 +34,11 @@ class ZeroPaddingTest(keras_parameterized.TestCase):\n \n     with self.cached_session():\n       # basic test\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.ZeroPadding1D,\n           kwargs={'padding': 2},\n           input_shape=inputs.shape)\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.ZeroPadding1D,\n           kwargs={'padding': (1, 2)},\n           input_shape=inputs.shape)\n@@ -89,14 +89,14 @@ class ZeroPaddingTest(keras_parameterized.TestCase):\n \n     # basic test\n     with self.cached_session():\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.ZeroPadding2D,\n           kwargs={\n               'padding': (2, 2),\n               'data_format': data_format\n           },\n           input_shape=inputs.shape)\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.ZeroPadding2D,\n           kwargs={\n               'padding': ((1, 2), (3, 4)),\n@@ -178,14 +178,14 @@ class ZeroPaddingTest(keras_parameterized.TestCase):\n \n     with self.cached_session():\n       # basic test\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.ZeroPadding3D,\n           kwargs={\n               'padding': (2, 2, 2),\n               'data_format': data_format\n           },\n           input_shape=inputs.shape)\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.ZeroPadding3D,\n           kwargs={\n               'padding': ((1, 2), (3, 4), (0, 2)),\n\n@@ -18,7 +18,7 @@ import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n import numpy as np\n-from keras import combinations\n+from keras.testing_infra import test_combinations\n from keras import layers\n from keras.layers import rnn_cell_wrapper_v2\n from keras.layers.legacy_rnn import rnn_cell_impl\n@@ -26,7 +26,7 @@ from keras.legacy_tf_layers import base as legacy_base_layer\n from keras.utils import generic_utils\n \n \n-@combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n class RNNCellWrapperTest(tf.test.TestCase, parameterized.TestCase):\n \n   def testResidualWrapper(self):\n\n@@ -19,7 +19,7 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n import keras\n-from keras import combinations\n+from keras.testing_infra import test_combinations\n from keras.layers import recurrent as rnn_v1\n from keras.layers import recurrent_v2 as rnn_v2\n from keras.layers.normalization import batch_normalization as batchnorm_v2\n@@ -39,7 +39,7 @@ class SerializableInt(int):\n     return cls(**config)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class LayerSerializationTest(parameterized.TestCase, tf.test.TestCase):\n \n   def test_serialize_deserialize(self):\n\n@@ -22,11 +22,11 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import combinations\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n \n \n-@combinations.generate(combinations.keras_mode_combinations())\n+@test_combinations.generate(test_combinations.keras_mode_combinations())\n class SimpleRNNLayerTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_return_sequences_SimpleRNN(self):\n@@ -34,19 +34,19 @@ class SimpleRNNLayerTest(tf.test.TestCase, parameterized.TestCase):\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.SimpleRNN,\n         kwargs={'units': units,\n                 'return_sequences': True},\n         input_shape=(num_samples, timesteps, embedding_dim))\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_float64_SimpleRNN(self):\n     num_samples = 2\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.SimpleRNN,\n         kwargs={'units': units,\n                 'return_sequences': True,\n@@ -72,7 +72,7 @@ class SimpleRNNLayerTest(tf.test.TestCase, parameterized.TestCase):\n     timesteps = 3\n     embedding_dim = 4\n     units = 2\n-    testing_utils.layer_test(\n+    test_utils.layer_test(\n         keras.layers.SimpleRNN,\n         kwargs={'units': units,\n                 'dropout': 0.1,\n@@ -85,7 +85,7 @@ class SimpleRNNLayerTest(tf.test.TestCase, parameterized.TestCase):\n     embedding_dim = 4\n     units = 2\n     for mode in [0, 1, 2]:\n-      testing_utils.layer_test(\n+      test_utils.layer_test(\n           keras.layers.SimpleRNN,\n           kwargs={'units': units,\n                   'implementation': mode},\n@@ -176,7 +176,7 @@ class SimpleRNNLayerTest(tf.test.TestCase, parameterized.TestCase):\n     model.compile(\n         optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     out1 = model.predict(np.ones((num_samples, timesteps)))\n     self.assertEqual(out1.shape, (num_samples, units))\n \n\n@@ -17,14 +17,14 @@\n import tensorflow.compat.v2 as tf\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.utils import tf_utils\n \n \n-@keras_parameterized.run_all_keras_modes\n-@keras_parameterized.run_with_all_model_types\n-class SubclassedLayersTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+@test_combinations.run_with_all_model_types\n+class SubclassedLayersTest(test_combinations.TestCase):\n \n   def test_simple_build_with_constant(self):\n \n@@ -37,7 +37,7 @@ class SubclassedLayersTest(keras_parameterized.TestCase):\n         return self.b * inputs\n \n     layer = BuildConstantLayer()\n-    model = testing_utils.get_model_from_layers(\n+    model = test_utils.get_model_from_layers(\n         [layer, keras.layers.Dense(1)], input_shape=(1,))\n \n     x = tf.convert_to_tensor([[3.0]])\n@@ -61,7 +61,7 @@ class SubclassedLayersTest(keras_parameterized.TestCase):\n         return self.variable * self.constant * inputs\n \n     layer = BuildDerivedConstantLayer()\n-    model = testing_utils.get_model_from_layers(\n+    model = test_utils.get_model_from_layers(\n         [layer, keras.layers.Dense(1)], input_shape=(1,))\n \n     x = tf.convert_to_tensor([[3.0]])\n\n@@ -22,8 +22,8 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.engine import keras_tensor\n from keras.optimizers.optimizer_v2 import adam\n from keras.saving import model_config\n@@ -276,8 +276,8 @@ def _reuse_ancillary_layer():\n   return model\n \n \n-@keras_parameterized.run_all_keras_modes()\n-class AutoLambdaTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes()\n+class AutoLambdaTest(test_combinations.TestCase):\n \n   @parameterized.named_parameters(\n       ('single_op_at_end', _single_op_at_end),\n@@ -310,7 +310,7 @@ class AutoLambdaTest(keras_parameterized.TestCase):\n     model.compile(\n         adam.Adam(0.001),\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     np_inputs = tf.nest.map_structure(\n         lambda x: np.ones((2,) + tuple(x.shape[1:]), 'float32'), model.inputs)\n@@ -328,7 +328,7 @@ class AutoLambdaTest(keras_parameterized.TestCase):\n     new_model.compile(\n         adam.Adam(0.001),\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     new_model.fit(np_inputs, np_outputs, batch_size=2)\n     new_model(np_inputs)  # Test calling the new model directly on inputs.\n     # Assert that metrics are preserved and in the right order.\n@@ -348,7 +348,7 @@ class AutoLambdaTest(keras_parameterized.TestCase):\n     model.compile(\n         adam.Adam(0.001),\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x = tf.ones(shape=(4, 4))\n     expected = tf.stack([x])\n@@ -373,7 +373,7 @@ class AutoLambdaTest(keras_parameterized.TestCase):\n     model.compile(\n         adam.Adam(0.001),\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x = tf.ones(shape=(4, 4))\n     expected = tf.stack(x)\n@@ -401,7 +401,7 @@ class AutoLambdaTest(keras_parameterized.TestCase):\n     model.compile(\n         adam.Adam(0.001),\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     batch_size = 7\n     step = 3\n     x = tf.stack([\n@@ -438,7 +438,7 @@ class AutoLambdaTest(keras_parameterized.TestCase):\n     model.compile(\n         adam.Adam(0.001),\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     batch_size = 7\n     stop = 6\n     args = tf.constant(stop, shape=(batch_size,))\n@@ -472,7 +472,7 @@ class AutoLambdaTest(keras_parameterized.TestCase):\n     model.compile(\n         adam.Adam(0.001),\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     batch_size = 7\n     index = 6\n     args = tf.constant(index, shape=(batch_size,))\n@@ -507,7 +507,7 @@ class AutoLambdaTest(keras_parameterized.TestCase):\n     model.compile(\n         adam.Adam(0.001),\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     batch_size = 7\n     stop = 6\n     x = tf.stack([\n@@ -543,7 +543,7 @@ class AutoLambdaTest(keras_parameterized.TestCase):\n     model.compile(\n         adam.Adam(0.001),\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     batch_size = 7\n     stop = 6\n     x = tf.stack([\n@@ -583,7 +583,7 @@ class AutoLambdaTest(keras_parameterized.TestCase):\n     model.compile(\n         adam.Adam(0.001),\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     batch_size = 7\n     start = 1\n     stop = 6\n@@ -720,8 +720,8 @@ class AutoLambdaTest(keras_parameterized.TestCase):\n     model.summary()\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class InputInEagerTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class InputInEagerTest(test_combinations.TestCase):\n   \"\"\"Tests ops on keras inputs in Eager runtime.\n \n   Input returns graph/symbolic tensors in the Eager runtime (this\n\n@@ -23,9 +23,8 @@ import numpy as np\n \n import keras\n from tensorflow.python.framework import test_util as tf_test_util\n-from keras import combinations\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.engine import base_layer_utils\n from keras.layers import core\n from keras.layers.rnn_cell_wrapper_v2 import ResidualWrapper\n@@ -96,9 +95,10 @@ class _AddOneCell(keras.layers.AbstractRNNCell):\n     return outputs, state\n \n \n-class TimeDistributedTest(keras_parameterized.TestCase):\n+class TimeDistributedTest(test_combinations.TestCase):\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_timedistributed_dense(self):\n     model = keras.models.Sequential()\n     model.add(\n@@ -258,7 +258,8 @@ class TimeDistributedTest(keras_parameterized.TestCase):\n         self.assertAllEqual(mask_outputs_val[i], ref_mask_val[i])\n       self.assertIs(mask_outputs[-1], None)  # final layer\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_TimeDistributed_with_masking_layer(self):\n     # test with Masking layer\n     model = keras.models.Sequential()\n@@ -303,7 +304,8 @@ class TimeDistributedTest(keras_parameterized.TestCase):\n         '`TimeDistributed` Layer should be passed an `input_shape `'):\n       time_dist(ph)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_TimeDistributed_reshape(self):\n \n     class NoReshapeLayer(keras.layers.Layer):\n@@ -324,7 +326,8 @@ class TimeDistributedTest(keras_parameterized.TestCase):\n     td3 = keras.layers.TimeDistributed(NoReshapeLayer())\n     self.assertFalse(td3._always_use_reshape)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_TimeDistributed_output_shape_return_types(self):\n \n     class TestLayer(keras.layers.Layer):\n@@ -361,7 +364,7 @@ class TimeDistributedTest(keras_parameterized.TestCase):\n           input_layer.compute_output_shape([None, 2, 4]).as_list(),\n           [None, 2, 8])\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   # TODO(scottzhu): check why v1 session failed.\n   def test_TimeDistributed_with_mask_first_implementation(self):\n     np.random.seed(100)\n@@ -377,7 +380,7 @@ class TimeDistributedTest(keras_parameterized.TestCase):\n     model_1.compile(\n         'rmsprop',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     output_with_mask = model_1.predict(data, steps=1)\n \n     y = keras.layers.TimeDistributed(rnn_layer)(x)\n@@ -385,14 +388,14 @@ class TimeDistributedTest(keras_parameterized.TestCase):\n     model_2.compile(\n         'rmsprop',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     output = model_2.predict(data, steps=1)\n \n     self.assertNotAllClose(output_with_mask, output, atol=1e-7)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   @parameterized.named_parameters(\n-      *testing_utils.generate_combinations_with_testcase_name(\n+      *test_utils.generate_combinations_with_testcase_name(\n           layer=[keras.layers.LSTM,\n                  keras.layers.Dense]))\n   def test_TimeDistributed_with_ragged_input(self, layer):\n@@ -409,7 +412,7 @@ class TimeDistributedTest(keras_parameterized.TestCase):\n     x_ragged = keras.Input(shape=(None, 2, 1), dtype='float32', ragged=True)\n     y_ragged = keras.layers.TimeDistributed(layer)(x_ragged)\n     model_1 = keras.models.Model(x_ragged, y_ragged)\n-    model_1._run_eagerly = testing_utils.should_run_eagerly()\n+    model_1._run_eagerly = test_utils.should_run_eagerly()\n     output_ragged = model_1.predict(ragged_data, steps=1)\n \n     x_dense = keras.Input(shape=(None, 2, 1), dtype='float32')\n@@ -417,13 +420,13 @@ class TimeDistributedTest(keras_parameterized.TestCase):\n     y_dense = keras.layers.TimeDistributed(layer)(masking)\n     model_2 = keras.models.Model(x_dense, y_dense)\n     dense_data = ragged_data.to_tensor()\n-    model_2._run_eagerly = testing_utils.should_run_eagerly()\n+    model_2._run_eagerly = test_utils.should_run_eagerly()\n     output_dense = model_2.predict(dense_data, steps=1)\n \n     output_ragged = convert_ragged_tensor_value(output_ragged)\n     self.assertAllEqual(output_ragged.to_tensor(), output_dense)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_TimeDistributed_with_ragged_input_with_batch_size(self):\n     np.random.seed(100)\n     layer = keras.layers.Dense(16)\n@@ -458,7 +461,7 @@ class TimeDistributedTest(keras_parameterized.TestCase):\n     # Make sure the batch dim is not lost after array_ops.reshape.\n     self.assertListEqual(outputs.shape.as_list(), [1, None, 30, 30, 16])\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_TimeDistributed_with_mimo(self):\n     dense_1 = keras.layers.Dense(8)\n     dense_2 = keras.layers.Dense(16)\n@@ -496,7 +499,7 @@ class TimeDistributedTest(keras_parameterized.TestCase):\n     model_1.compile(\n         optimizer='rmsprop',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     output_1 = model_1.predict((data_1, data_2), steps=1)\n \n     y1 = dense_1(x1)\n@@ -537,7 +540,7 @@ class TimeDistributedTest(keras_parameterized.TestCase):\n     model.summary()\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n \n   @parameterized.parameters(['sum', 'concat', 'ave', 'mul'])\n@@ -1250,7 +1253,7 @@ class BidirectionalTest(tf.test.TestCase, parameterized.TestCase):\n         epochs=1,\n         batch_size=10)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_wrapped_rnn_cell(self):\n     # See https://github.com/tensorflow/tensorflow/issues/26581.\n     batch = 20\n\n@@ -25,7 +25,7 @@ import copy\n from absl.testing import parameterized\n import numpy as np\n from keras import backend\n-from keras import combinations\n+from keras.testing_infra import test_combinations\n from keras.engine import base_layer as keras_base_layer\n from keras.engine import input_spec\n from keras.legacy_tf_layers import base as base_layers\n@@ -34,7 +34,8 @@ from keras.legacy_tf_layers import core as core_layers\n \n class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testLayerProperties(self):\n     layer = base_layers.Layer(name='my_layer')\n     self.assertEqual(layer.variables, [])\n@@ -56,13 +57,15 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n         keras_base_layer.keras_api_gauge.get_cell('legacy_layer').value())\n     keras_base_layer.keras_api_gauge.get_cell('legacy_layer').set(False)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testInt64Layer(self):\n     layer = base_layers.Layer(name='my_layer', dtype='int64')\n     layer.add_variable('my_var', [2, 2])\n     self.assertEqual(layer.name, 'my_layer')\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testKerasStyleAddWeight(self):\n     keras_layer = keras_base_layer.Layer(name='keras_layer')\n     with backend.name_scope('foo'):\n@@ -87,7 +90,8 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n           'my_var', [2, 2], initializer=tf.compat.v1.zeros_initializer())\n     self.assertEqual(variable.name, 'bar/my_var:0')\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testAddWeight(self):\n     layer = base_layers.Layer(name='my_layer')\n \n@@ -177,7 +181,8 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertEqual(\n           len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)), 3)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testCall(self):\n \n     class MyLayer(base_layers.Layer):\n@@ -193,7 +198,8 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n       # op is only supported in GRAPH mode\n       self.assertEqual(outputs.op.name, 'my_layer/Square')\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testDeepCopy(self):\n \n     class MyLayer(base_layers.Layer):\n@@ -215,7 +221,8 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(layer_copy._scope.name, layer._scope.name)\n     self.assertEqual(layer_copy._private_tensor, layer._private_tensor)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testScopeNaming(self):\n \n     class PrivateLayer(base_layers.Layer):\n@@ -263,7 +270,8 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n       my_layer_scoped1.apply(inputs)\n       self.assertEqual(my_layer_scoped1._scope.name, 'var_scope/my_layer_1')\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testInputSpecNdimCheck(self):\n \n     class CustomerLayer(base_layers.Layer):\n@@ -285,7 +293,8 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n     layer = CustomerLayer()\n     layer.apply(tf.constant([[1], [2]]))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testInputSpecMinNdimCheck(self):\n \n     class CustomLayer(base_layers.Layer):\n@@ -308,7 +317,8 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n     layer = CustomLayer()\n     layer.apply(tf.constant([[[1], [2]]]))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testInputSpecMaxNdimCheck(self):\n \n     class CustomerLayer(base_layers.Layer):\n@@ -331,7 +341,8 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n     layer = CustomerLayer()\n     layer.apply(tf.constant([[1], [2]]))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testInputSpecDtypeCheck(self):\n \n     class CustomerLayer(base_layers.Layer):\n@@ -351,7 +362,8 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n     layer = CustomerLayer()\n     layer.apply(tf.constant(1.0, dtype=tf.float32))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testInputSpecAxesCheck(self):\n \n     class CustomerLayer(base_layers.Layer):\n@@ -373,7 +385,8 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n     layer = CustomerLayer()\n     layer.apply(tf.constant([[1, 2], [3, 4], [5, 6]]))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testInputSpecShapeCheck(self):\n \n     class CustomerLayer(base_layers.Layer):\n@@ -393,7 +406,8 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n     layer = CustomerLayer()\n     layer.apply(tf.constant([[1, 2, 3], [4, 5, 6]]))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testNoInputSpec(self):\n \n     class CustomerLayer(base_layers.Layer):\n@@ -414,7 +428,8 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n       layer.apply(tf.compat.v1.placeholder('int32'))\n       layer.apply(tf.compat.v1.placeholder('int32', shape=(2, 3)))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_count_params(self):\n     dense = core_layers.Dense(16)\n     dense.build((None, 4))\n@@ -424,7 +439,8 @@ class BaseLayerTest(tf.test.TestCase, parameterized.TestCase):\n     with self.assertRaises(ValueError):\n       dense.count_params()\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testDictInputOutput(self):\n \n     class DictLayer(base_layers.Layer):\n@@ -634,7 +650,7 @@ class IdentityLayer(base_layers.Layer):\n     return inputs\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class DTypeTest(tf.test.TestCase, parameterized.TestCase):\n \n   def _const(self, dtype):\n\n@@ -25,15 +25,16 @@ import platform\n \n from absl.testing import parameterized\n import numpy as np\n-from tensorflow.python.framework import test_util\n-from keras import combinations\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n+from keras.testing_infra import test_combinations\n from keras.legacy_tf_layers import core as core_layers\n from tensorflow.python.ops import variable_scope\n \n \n class DenseTest(tf.test.TestCase, parameterized.TestCase):\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testDenseProperties(self):\n     dense = core_layers.Dense(2, activation=tf.nn.relu, name='my_dense')\n     self.assertEqual(dense.units, 2)\n@@ -51,7 +52,7 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n     dense.apply(tf.random.uniform((5, 2)))\n     self.assertEqual(dense.name, 'dense_2')\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def testVariableInput(self):\n     with self.cached_session():\n       v = tf.compat.v1.get_variable(\n@@ -60,7 +61,8 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n       self.evaluate(tf.compat.v1.global_variables_initializer())\n       self.assertAllEqual(x, [[0.0]])\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testCall(self):\n     dense = core_layers.Dense(2, activation=tf.nn.relu, name='my_dense')\n     inputs = tf.random.uniform((5, 4), seed=1)\n@@ -76,7 +78,7 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(dense.kernel.name, 'my_dense/kernel:0')\n     self.assertEqual(dense.bias.name, 'my_dense/bias:0')\n \n-  @test_util.assert_no_new_pyobjects_executing_eagerly\n+  @tf_test_utils.assert_no_new_pyobjects_executing_eagerly\n   def testNoEagerLeak(self):\n     # Tests that repeatedly constructing and building a Layer does not leak\n     # Python objects.\n@@ -84,14 +86,16 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n     core_layers.Dense(5)(inputs)\n     core_layers.Dense(2, activation=tf.nn.relu, name='my_dense')(inputs)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testCallTensorDot(self):\n     dense = core_layers.Dense(2, activation=tf.nn.relu, name='my_dense')\n     inputs = tf.random.uniform((5, 4, 3), seed=1)\n     outputs = dense(inputs)\n     self.assertListEqual([5, 4, 2], outputs.get_shape().as_list())\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testNoBias(self):\n     dense = core_layers.Dense(2, use_bias=False, name='my_dense')\n     inputs = tf.random.uniform((5, 2), seed=1)\n@@ -105,7 +109,8 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(dense.kernel.name, 'my_dense/kernel:0')\n     self.assertEqual(dense.bias, None)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testNonTrainable(self):\n     dense = core_layers.Dense(2, trainable=False, name='my_dense')\n     inputs = tf.random.uniform((5, 2), seed=1)\n@@ -118,7 +123,8 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertEqual(\n           len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES)), 0)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testOutputShape(self):\n     dense = core_layers.Dense(7, activation=tf.nn.relu, name='my_dense')\n     inputs = tf.random.uniform((5, 3), seed=1)\n@@ -133,7 +139,7 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n     outputs = dense.apply(inputs)\n     self.assertEqual(outputs.get_shape().as_list(), [1, 2, 4, 7])\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def testCallOnPlaceHolder(self):\n     inputs = tf.compat.v1.placeholder(dtype=tf.float32)\n     dense = core_layers.Dense(4, name='my_dense')\n@@ -159,7 +165,8 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n     dense = core_layers.Dense(4, name='my_dense')\n     dense(inputs)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testActivation(self):\n     dense = core_layers.Dense(2, activation=tf.nn.relu, name='dense1')\n     inputs = tf.random.uniform((5, 3), seed=1)\n@@ -173,7 +180,7 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n     if not tf.executing_eagerly():\n       self.assertEqual(outputs.op.name, 'dense2/BiasAdd')\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def testActivityRegularizer(self):\n     regularizer = lambda x: tf.reduce_sum(x) * 1e-3\n     dense = core_layers.Dense(\n@@ -184,7 +191,7 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(len(loss_keys), 1)\n     self.assertListEqual(dense.losses, loss_keys)\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def testKernelRegularizer(self):\n     regularizer = lambda x: tf.reduce_sum(x) * 1e-3\n     dense = core_layers.Dense(\n@@ -196,7 +203,7 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n     self.evaluate([v.initializer for v in dense.variables])\n     self.assertAllEqual(self.evaluate(dense.losses), self.evaluate(loss_keys))\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def testKernelRegularizerWithReuse(self):\n     regularizer = lambda x: tf.reduce_sum(x) * 1e-3\n     inputs = tf.random.uniform((5, 3), seed=1)\n@@ -209,7 +216,7 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(\n         len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES)), 1)\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def testBiasRegularizer(self):\n     regularizer = lambda x: tf.reduce_sum(x) * 1e-3\n     dense = core_layers.Dense(2, name='my_dense', bias_regularizer=regularizer)\n@@ -220,7 +227,7 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n     self.evaluate([v.initializer for v in dense.variables])\n     self.assertAllEqual(self.evaluate(dense.losses), self.evaluate(loss_keys))\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def testFunctionalDense(self):\n     with self.cached_session():\n       inputs = tf.random.uniform((5, 3), seed=1)\n@@ -230,7 +237,7 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n           len(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.TRAINABLE_VARIABLES)), 2)\n       self.assertEqual(outputs.op.name, 'my_dense/Relu')\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def testFunctionalDenseTwice(self):\n     inputs = tf.random.uniform((5, 3), seed=1)\n     core_layers.dense(inputs, 2)\n@@ -262,7 +269,7 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n         vars2 = tf.compat.v1.trainable_variables()\n       self.assertEqual(vars1, vars2)\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def testFunctionalDenseInitializerFromScope(self):\n     with tf.compat.v1.variable_scope(\n         'scope',\n@@ -291,7 +298,7 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n       core_layers.dense(inputs, 2)\n     self.assertEqual(called[0], 2)\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def testFunctionalDenseInScope(self):\n     with self.cached_session():\n       with tf.compat.v1.variable_scope('test'):\n@@ -313,7 +320,8 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n         var_key = 'test2/dense/kernel'\n         self.assertEqual(var_dict[var_key].name, '%s:0' % var_key)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testComputeOutputShape(self):\n     dense = core_layers.Dense(2, activation=tf.nn.relu, name='dense1')\n     ts = tf.TensorShape\n@@ -335,7 +343,8 @@ class DenseTest(tf.test.TestCase, parameterized.TestCase):\n         dense.compute_output_shape(ts([None, 4, 3])).as_list())\n     # pylint: enable=protected-access\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testConstraints(self):\n     k_constraint = lambda x: x / tf.reduce_sum(x)\n     b_constraint = lambda x: x / tf.reduce_max(x)\n@@ -357,7 +366,8 @@ def _get_variable_dict_from_varstore():\n \n class DropoutTest(tf.test.TestCase, parameterized.TestCase):\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testDropoutProperties(self):\n     dp = core_layers.Dropout(0.5, name='dropout')\n     self.assertEqual(dp.rate, 0.5)\n@@ -365,7 +375,8 @@ class DropoutTest(tf.test.TestCase, parameterized.TestCase):\n     dp.apply(tf.ones(()))\n     self.assertEqual(dp.name, 'dropout')\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testBooleanLearningPhase(self):\n     dp = core_layers.Dropout(0.5)\n     inputs = tf.ones((5, 3))\n@@ -378,7 +389,7 @@ class DropoutTest(tf.test.TestCase, parameterized.TestCase):\n     np_output = self.evaluate(dropped)\n     self.assertAllClose(np.ones((5, 3)), np_output)\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def testDynamicLearningPhase(self):\n     with self.cached_session() as sess:\n       dp = core_layers.Dropout(0.5, seed=1)\n@@ -391,7 +402,8 @@ class DropoutTest(tf.test.TestCase, parameterized.TestCase):\n       np_output = sess.run(dropped, feed_dict={training: False})\n       self.assertAllClose(np.ones((5, 5)), np_output)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testDynamicNoiseShape(self):\n     inputs = tf.ones((5, 3, 2))\n     noise_shape = [None, 1, None]\n@@ -412,7 +424,7 @@ class DropoutTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAlmostEqual(0., np_output.min())\n     self.assertAllClose(np_output[:, 0, :], np_output[:, 1, :])\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def testFunctionalDropout(self):\n     with self.cached_session():\n       inputs = tf.ones((5, 5))\n@@ -424,7 +436,7 @@ class DropoutTest(tf.test.TestCase, parameterized.TestCase):\n       np_output = self.evaluate(dropped)\n       self.assertAllClose(np.ones((5, 5)), np_output)\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def testDynamicRate(self):\n     with self.cached_session() as sess:\n       rate = tf.compat.v1.placeholder(dtype='float32', name='rate')\n@@ -440,7 +452,7 @@ class DropoutTest(tf.test.TestCase, parameterized.TestCase):\n \n class FlattenTest(tf.test.TestCase):\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def testCreateFlatten(self):\n     with self.cached_session() as sess:\n       x = tf.compat.v1.placeholder(shape=(None, 2, 3), dtype='float32')\n@@ -465,7 +477,7 @@ class FlattenTest(tf.test.TestCase):\n     shape = core_layers.Flatten().compute_output_shape((None, 3, None))\n     self.assertEqual(shape.as_list(), [None, None])\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def testDataFormat5d(self):\n     np_input_channels_last = np.arange(\n         120, dtype='float32').reshape([1, 5, 4, 3, 2])\n@@ -483,7 +495,7 @@ class FlattenTest(tf.test.TestCase):\n \n       self.assertAllEqual(np_output_cl, np_output_cf)\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def testDataFormat4d(self):\n     np_input_channels_last = np.arange(\n         24, dtype='float32').reshape([1, 4, 3, 2])\n@@ -501,13 +513,13 @@ class FlattenTest(tf.test.TestCase):\n \n       self.assertAllEqual(np_output_cl, np_output_cf)\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def testFunctionalFlatten(self):\n     x = tf.compat.v1.placeholder(shape=(None, 2, 3), dtype='float32')\n     y = core_layers.flatten(x, name='flatten')\n     self.assertEqual(y.get_shape().as_list(), [None, 6])\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def testFlatten0D(self):\n     x = tf.compat.v1.placeholder(shape=(None,), dtype='float32')\n     y = core_layers.Flatten()(x)\n@@ -516,7 +528,7 @@ class FlattenTest(tf.test.TestCase):\n     self.assertEqual(list(np_output.shape), [5, 1])\n     self.assertEqual(y.shape.as_list(), [None, 1])\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def testFlattenUnknownAxes(self):\n     with self.cached_session() as sess:\n       x = tf.compat.v1.placeholder(shape=(5, None, None), dtype='float32')\n@@ -531,7 +543,7 @@ class FlattenTest(tf.test.TestCase):\n       self.assertEqual(list(np_output.shape), [5, 6])\n       self.assertEqual(y.get_shape().as_list(), [5, None])\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def testFlattenLargeDim(self):\n     if any(platform.win32_ver()):\n       self.skipTest('values are truncated on windows causing test failures')\n@@ -540,7 +552,7 @@ class FlattenTest(tf.test.TestCase):\n     y = core_layers.Flatten()(x)\n     self.assertEqual(y.shape.as_list(), [None, 21316 * 21316 * 80])\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def testFlattenLargeBatchDim(self):\n     batch_size = np.iinfo(np.int32).max + 10\n     x = tf.compat.v1.placeholder(\n\n@@ -25,12 +25,12 @@ import os\n import numpy as np\n \n from tensorflow.core.protobuf import saver_pb2\n-from tensorflow.python.framework import test_util\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n from keras.legacy_tf_layers import convolutional as conv_layers\n from keras.legacy_tf_layers import normalization as normalization_layers\n \n \n-@test_util.run_v1_only('b/120545219')\n+@tf_test_utils.run_v1_only('b/120545219')\n class BNTest(tf.test.TestCase):\n \n   def _simple_model(self, image, fused, freeze_mode):\n\n@@ -20,7 +20,7 @@ from __future__ import print_function\n \n import tensorflow.compat.v2 as tf\n \n-from tensorflow.python.framework import test_util\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n from keras.legacy_tf_layers import pooling as pooling_layers\n \n \n@@ -64,7 +64,7 @@ class PoolingTest(tf.test.TestCase):\n     output = layer.apply(images)\n     self.assertListEqual(output.get_shape().as_list(), [5, 3, 4, 4])\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def testCreateMaxPooling2DChannelsFirst(self):\n     height, width = 7, 9\n     images = tf.random.uniform((5, 2, height, width))\n@@ -74,7 +74,7 @@ class PoolingTest(tf.test.TestCase):\n     output = layer.apply(images)\n     self.assertListEqual(output.get_shape().as_list(), [5, 2, 6, 8])\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def testCreateAveragePooling2DChannelsFirst(self):\n     height, width = 5, 6\n     images = tf.random.uniform((3, 4, height, width))\n@@ -85,7 +85,7 @@ class PoolingTest(tf.test.TestCase):\n     output = layer.apply(images)\n     self.assertListEqual(output.get_shape().as_list(), [3, 4, 4, 5])\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def testCreateAveragePooling2DChannelsFirstWithNoneBatch(self):\n     height, width = 5, 6\n     images = tf.compat.v1.placeholder(dtype='float32',\n\n@@ -22,7 +22,6 @@ import gc\n import threading\n \n from absl.testing import parameterized\n-from keras import combinations\n from keras import models\n from keras import regularizers\n from keras.engine import base_layer\n@@ -31,11 +30,12 @@ from keras.engine import training as training_module\n from keras.layers import core\n from keras.legacy_tf_layers import core as core_layers\n from keras.legacy_tf_layers import variable_scope_shim\n+from keras.testing_infra import test_combinations\n \n import numpy\n import tensorflow as tf\n \n-from tensorflow.python.framework import test_util   # pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n from tensorflow.python.ops import variable_scope   # pylint: disable=g-direct-tensorflow-import\n \n \n@@ -68,7 +68,7 @@ class VariableScopeTest(tf.test.TestCase):\n     # involving objects with __del__ defined.\n     self.assertEqual(0, len(gc.garbage))\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testGetVar(self):\n     vs = variable_scope._get_default_variable_store()\n@@ -76,7 +76,7 @@ class VariableScopeTest(tf.test.TestCase):\n     v1 = vs.get_variable(\"v\", [1])\n     self.assertIs(v, v1)\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testNameExists(self):\n     vs = variable_scope._get_default_variable_store()\n@@ -87,7 +87,7 @@ class VariableScopeTest(tf.test.TestCase):\n \n     self.assertIsNot(v, vs.get_variable(\"u\", [1], reuse=False))\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testNamelessStore(self):\n     vs = variable_scope._get_default_variable_store()\n@@ -100,7 +100,7 @@ class VariableScopeTest(tf.test.TestCase):\n   # TODO(mihaimaruseac): Not converted to use wrap_function because of\n   # TypeError: Expected tf.group() expected Tensor arguments not 'None' with\n   # type '<type 'NoneType'>'\n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   def testVarScopeInitializer(self):\n     init = tf.compat.v1.constant_initializer(0.3)\n     with tf.compat.v1.variable_scope(\"tower0\") as tower:\n@@ -113,7 +113,7 @@ class VariableScopeTest(tf.test.TestCase):\n         self.evaluate(tf.compat.v1.variables_initializer([w]))\n         self.assertAllClose(self.evaluate(w.value()), 0.3)\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testVarScopeConstraint(self):\n     constraint = lambda x: 0. * x\n@@ -125,7 +125,7 @@ class VariableScopeTest(tf.test.TestCase):\n         w = tf.compat.v1.get_variable(\"w\", [])\n         self.assertIsNotNone(w.constraint)\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testVarScopeDType(self):\n     with tf.compat.v1.variable_scope(\"tower2\") as tower:\n@@ -136,7 +136,7 @@ class VariableScopeTest(tf.test.TestCase):\n         w = tf.compat.v1.get_variable(\"w\", [])\n         self.assertEqual(w.dtype.base_dtype, tf.float16)\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testInitFromNonTensorValue(self):\n     v = tf.compat.v1.get_variable(\"v4\", initializer=4, dtype=tf.int32)\n@@ -153,7 +153,7 @@ class VariableScopeTest(tf.test.TestCase):\n     with self.assertRaises(error):\n       tf.compat.v1.get_variable(\"x4\", initializer={})\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testInitFromNonInitializer(self):\n     # Test various dtypes with zeros initializer as following:\n@@ -175,7 +175,7 @@ class VariableScopeTest(tf.test.TestCase):\n       self.evaluate(tf.compat.v1.global_variables_initializer())\n       self.assertAllEqual(self.evaluate(x.value()), self.evaluate(y.value()))\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testVarScopeRegularizer(self):\n     init = tf.compat.v1.constant_initializer(0.3)\n@@ -204,7 +204,7 @@ class VariableScopeTest(tf.test.TestCase):\n         vs.set_regularizer(tf.compat.v1.no_regularizer)\n         tf.compat.v1.get_variable(\"z\", [])\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testInitializeFromValue(self):\n     init = tf.constant(0.1)\n@@ -231,7 +231,7 @@ class VariableScopeTest(tf.test.TestCase):\n     with self.assertRaisesRegex(ValueError, \"don't match\"):\n       tf.compat.v1.get_variable(\"s\", initializer=init, dtype=tf.float64)\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testVarScopeGetOrCreateReuse(self):\n     with self.cached_session():\n@@ -252,7 +252,7 @@ class VariableScopeTest(tf.test.TestCase):\n       test_value(13.)  # Variable is reused hereafter.\n       test_value(17.)\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testVarScopeGetOrCreateReuseIgnoreFalse(self):\n     with self.cached_session():\n@@ -275,7 +275,7 @@ class VariableScopeTest(tf.test.TestCase):\n       test_value(13.)  # Variable is reused hereafter.\n       test_value(17.)\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testVarOpScope(self):\n     with self.cached_session():\n@@ -292,7 +292,7 @@ class VariableScopeTest(tf.test.TestCase):\n           self.assertEqual(\n               tf.compat.v1.get_variable(\"w\", []).name, \"default_1/w:0\")\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testVarOpScopeUniqueNamesInterleavedSubstringScopes(self):\n     with self.cached_session():\n@@ -317,7 +317,7 @@ class VariableScopeTest(tf.test.TestCase):\n               tf.compat.v1.get_variable(\"w\", []).name,\n               \"defaultScope1_2/layer/w:0\")\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testVarOpScopeUniqueNamesWithJump(self):\n     with self.cached_session():\n@@ -337,7 +337,7 @@ class VariableScopeTest(tf.test.TestCase):\n               tf.compat.v1.get_variable(\"w\", []).name,\n               \"default/layer_2/w:0\")\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testVarOpScopeReuse(self):\n     with self.cached_session():\n@@ -357,7 +357,7 @@ class VariableScopeTest(tf.test.TestCase):\n           self.assertEqual(\n               tf.compat.v1.get_variable(\"w\", []).name, \"outer/default/w:0\")\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testVarScopeGetVar(self):\n     with self.cached_session():\n@@ -396,7 +396,7 @@ class VariableScopeTest(tf.test.TestCase):\n             tf.compat.v1.get_variable(\"v\", [1], dtype=tf.int32)\n         self.assertEqual(\"dtype\" in str(exc.exception), True)\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testVarScopeOuterScope(self):\n     with self.cached_session():\n@@ -416,7 +416,7 @@ class VariableScopeTest(tf.test.TestCase):\n           self.assertEqual(\n               tf.compat.v1.get_variable(\"w\", []).name, \"outer/default/w:0\")\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testVarScopeNestedOuterScope(self):\n     with self.cached_session():\n@@ -435,7 +435,7 @@ class VariableScopeTest(tf.test.TestCase):\n           self.assertEqual(\n               tf.compat.v1.get_variable(\"w\", []).name, \"outer/default/w:0\")\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testVarOpScopeReuseParam(self):\n     with self.cached_session():\n@@ -456,7 +456,7 @@ class VariableScopeTest(tf.test.TestCase):\n           self.assertEqual(\n               tf.compat.v1.get_variable(\"w\", []).name, \"outer/default/w:0\")\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testVarOpScopeReuseError(self):\n     with self.cached_session():\n@@ -465,7 +465,7 @@ class VariableScopeTest(tf.test.TestCase):\n           self.assertEqual(\n               tf.compat.v1.get_variable(\"w\", []).name, \"outer/tower/w:0\")\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testVarOpScopeOuterScope(self):\n     with self.cached_session():\n@@ -486,7 +486,7 @@ class VariableScopeTest(tf.test.TestCase):\n           self.assertEqual(\n               tf.compat.v1.get_variable(\"w\", []).name, \"outer/default/w:0\")\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testVarOpScopeNestedOuterScope(self):\n     with self.cached_session():\n@@ -505,7 +505,7 @@ class VariableScopeTest(tf.test.TestCase):\n           self.assertEqual(\n               tf.compat.v1.get_variable(\"w\", []).name, \"outer/default/w:0\")\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testBasicWhenAuxiliaryNameScopeIsFalse(self):\n     with self.cached_session():\n@@ -529,7 +529,7 @@ class VariableScopeTest(tf.test.TestCase):\n           self.assertEqual(\n               tf.compat.v1.get_variable(\"w1\", []).name, \"outer/inner/w1:0\")\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testCreatedByDefaultNameWhenAuxiliaryNameScopeIsFalse(self):\n     with self.cached_session():\n@@ -546,7 +546,7 @@ class VariableScopeTest(tf.test.TestCase):\n           self.assertEqual(\n               tf.compat.v1.get_variable(\"w\", []).name, \"outer/default/w:0\")\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testReenterRootScopeWhenAuxiliaryNameScopeIsFalse(self):\n     with self.cached_session():\n@@ -561,7 +561,7 @@ class VariableScopeTest(tf.test.TestCase):\n           self.assertEqual(inner.original_name_scope, \"\")\n           self.assertEqual(tf.compat.v1.get_variable(\"w1\", []).name, \"w1:0\")\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testAuxiliaryNameScopeIsInvalid(self):\n     with self.cached_session():\n@@ -582,7 +582,7 @@ class VariableScopeTest(tf.test.TestCase):\n             scope, auxiliary_name_scope=\"invalid\"):\n           pass\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testReuseScopeWithoutNameScopeCollision(self):\n     # Github issue: #13429\n@@ -605,7 +605,7 @@ class VariableScopeTest(tf.test.TestCase):\n                 tf.compat.v1.get_variable(\"w1\", []).name,\n                 \"outer/inner/w1:0\")\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testGetVarWithDevice(self):\n     g = tf.Graph()\n@@ -624,26 +624,26 @@ class VariableScopeTest(tf.test.TestCase):\n     self.assertEqual(varname_type[0], (\"x\", tf.float32))\n     self.assertEqual(varname_type[1], (\"y\", tf.int64))\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testGetVariableWithRefDtype(self):\n     v = tf.compat.v1.get_variable(\"v\", shape=[3, 4], dtype=tf.float32)\n     # Ensure it is possible to do get_variable with a _ref dtype passed in.\n     _ = tf.compat.v1.get_variable(\"w\", shape=[5, 6], dtype=v.dtype)\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testGetVariableWithInitializerWhichTakesNoArgs(self):\n     v = tf.compat.v1.get_variable(\"foo\", initializer=lambda: [2])\n     self.assertEqual(v.name, \"foo:0\")\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testGetVariableWithInitializerWhichTakesOptionalArgs(self):\n     v = tf.compat.v1.get_variable(\"foo\", initializer=lambda x=True: [2])\n     self.assertEqual(v.name, \"foo:0\")\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testTwoGraphs(self):\n \n@@ -661,7 +661,7 @@ class VariableScopeTest(tf.test.TestCase):\n \n class VariableScopeWithCustomGetterTest(tf.test.TestCase):\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testNonCallableGetterFails(self):\n     with self.assertRaisesRegex(ValueError, r\"custom_getter .* not callable:\"):\n@@ -670,7 +670,7 @@ class VariableScopeWithCustomGetterTest(tf.test.TestCase):\n     with self.assertRaisesRegex(ValueError, r\"custom_getter .* not callable:\"):\n       tf.compat.v1.get_variable(\"name0\", custom_getter=3)\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testNoSideEffectsWithIdentityCustomGetter(self):\n     called = [0]\n@@ -694,7 +694,7 @@ class VariableScopeWithCustomGetterTest(tf.test.TestCase):\n     self.assertIs(v3, v4)\n     self.assertEqual(3, called[0])  # skipped one in the first new_scope\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testSynchronizationAndAggregationWithCustomGetter(self):\n     called = [0]\n@@ -721,7 +721,7 @@ class VariableScopeWithCustomGetterTest(tf.test.TestCase):\n \n     self.assertEqual(2, called[0])\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testVariableCreator(self):\n     variable_names = []\n@@ -757,7 +757,7 @@ class VariableScopeWithCustomGetterTest(tf.test.TestCase):\n           aggregation=tf.compat.v1.VariableAggregation.MEAN)\n     self.assertTrue(called[0])\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testVariableCreatorNestingError(self):\n \n@@ -780,7 +780,7 @@ class VariableScopeWithCustomGetterTest(tf.test.TestCase):\n \n class VariableScopeMultithreadedTest(tf.test.TestCase):\n \n-  @test_util.run_in_graph_and_eager_modes\n+  @tf_test_utils.run_in_graph_and_eager_modes\n   @run_inside_wrap_function_in_eager_mode\n   def testReenterMainScope(self):\n \n@@ -839,7 +839,7 @@ class VariableScopeModule(tf.Module):\n             in self._tf1_style_var_store._regularizers.items()}  # pylint: disable=protected-access\n \n \n-@combinations.generate(combinations.combine(mode=[\"eager\"]))\n+@test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_get_variable(self):\n@@ -1451,7 +1451,7 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n \n class GetOrCreateLayerTest(tf.test.TestCase, parameterized.TestCase):\n \n-  @combinations.generate(combinations.combine(mode=[\"eager\"]))\n+  @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n   def test_get_or_create_layer_with_regularizer_eager(self):\n \n     class NestedLayer(base_layer.Layer):\n@@ -1497,7 +1497,7 @@ class GetOrCreateLayerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllEqual(out2, tf.ones(shape=(1, 5, 10)) * 5)\n     self.assertAllEqual(layer.losses, [0.5])\n \n-  @combinations.generate(combinations.combine(mode=[\"eager\"]))\n+  @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n   def test_get_or_create_layer_no_regularizer_eager(self):\n \n     class NestedLayer(base_layer.Layer):\n@@ -1543,7 +1543,7 @@ class GetOrCreateLayerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllEqual(out2, tf.ones(shape=(1, 5, 10)) * 5)\n     self.assertAllEqual(layer.losses, [0.0])\n \n-  @combinations.generate(combinations.combine(mode=[\"eager\"]))\n+  @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n   def test_get_or_create_layer_tf_function(self):\n \n     class NestedLayer(base_layer.Layer):\n@@ -1578,7 +1578,7 @@ class GetOrCreateLayerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllEqual(out1, out2)\n     self.assertAllEqual(loss1, loss2)\n \n-  @test_util.run_deprecated_v1\n+  @tf_test_utils.run_deprecated_v1\n   def test_get_or_create_layer_graph(self):\n \n     class NestedLayer(object):\n\n@@ -22,7 +22,7 @@ import numpy as np\n from tensorflow.python.autograph.impl import api as autograph\n from keras import activations\n from keras import backend\n-from keras import combinations\n+from keras.testing_infra import test_combinations\n from keras import losses\n from keras.utils import losses_utils\n \n@@ -68,7 +68,8 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n       objective_output = losses.sparse_categorical_crossentropy(y_a, y_b)\n       assert backend.eval(objective_output).shape == (6,)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_categorical_crossentropy_loss(self):\n     target = backend.variable(np.random.randint(0, 1, (5, 1)))\n     logits = backend.variable(np.random.random((5, 1)))\n@@ -93,7 +94,8 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n         backend.eval(output_from_softmax_axis),\n         atol=1e-5)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_categorical_crossentropy_loss_with_unknown_rank_tensor(self):\n     t = backend.placeholder()\n     p = backend.placeholder()\n@@ -115,7 +117,8 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n     result = f([t_val, p_val])\n     self.assertArrayNear(result, [.002, 0, .17], 1e-3)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_sparse_categorical_crossentropy_loss(self):\n     target = backend.variable(np.random.randint(0, 1, (5, 1)))\n     logits = backend.variable(np.random.random((5, 1)))\n@@ -129,7 +132,7 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n         backend.eval(output_from_softmax),\n         atol=1e-5)\n \n-  @combinations.generate(combinations.combine(mode=['graph']))\n+  @test_combinations.generate(test_combinations.combine(mode=['graph']))\n   def test_sparse_categorical_crossentropy_loss_with_unknown_rank_tensor(self):\n     # This test only runs in graph because the TF op layer is not supported yet\n     # for sparse ops.\n@@ -153,7 +156,7 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n     result = f([t_val, p_val])\n     self.assertArrayNear(result, [.002, 0, .17], 1e-3)\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def test_sparse_categorical_crossentropy_with_float16(self):\n     # See https://github.com/keras-team/keras/issues/15012 for more details.\n     # we don't cast y_true to have same dtype as y_pred, since y_pred could be\n@@ -174,7 +177,8 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n     scce = losses.SparseCategoricalCrossentropy()\n     self.assertAllClose(scce(y_true, y_pred_16).numpy(), 0.0, atol=1e-3)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_binary_crossentropy_loss(self):\n     target = backend.variable(np.random.randint(0, 1, (5, 1)))\n     logits = backend.variable(np.random.random((5, 1)))\n@@ -233,7 +237,8 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n     # reduced_weighted_mse = (6 + 26) / 2 =\n     self.assertAllClose(self.evaluate(loss), 16, 1e-2)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_loss_wrapper_autograph(self):\n     # Test that functions with control flow wrapped in a LossFunctionWrapper\n     # get autographed when in a tf.function\n@@ -295,7 +300,8 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n     with self.assertRaisesRegex(ValueError, 'Could not interpret loss'):\n       losses.get(0)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_binary_crossentropy_uses_cached_logits(self):\n     logits = tf.constant([[-30., 30.]])\n     y_pred = activations.sigmoid(logits)\n@@ -306,7 +312,8 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n     # collapse to 0 from underflow.\n     self.assertNotEqual(self.evaluate(loss), 0.)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_categorical_crossentropy_uses_cached_logits(self):\n     logits = tf.constant([[-5., 0., 5.]])\n     y_pred = activations.softmax(logits)\n@@ -317,7 +324,8 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n     # collapse to 0 from underflow.\n     self.assertNotEqual(self.evaluate(loss), 0.)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_sparse_categorical_crossentropy_uses_cached_logits(self):\n     logits = tf.constant([[-5., 0., 5.]])\n     y_pred = activations.softmax(logits)\n@@ -329,7 +337,7 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n     # collapse to 0 from underflow.\n     self.assertNotEqual(self.evaluate(loss), 0.)\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def test_loss_not_autographed_in_eager(self):\n \n     class MyLoss(losses.Loss):\n@@ -350,7 +358,7 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n       loss(y_true, y_pred)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class MeanSquaredErrorTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -442,7 +450,7 @@ class MeanSquaredErrorTest(tf.test.TestCase):\n     self.assertAlmostEqual(self.evaluate(loss), 227.69998, 3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class MeanAbsoluteErrorTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -529,7 +537,7 @@ class MeanAbsoluteErrorTest(tf.test.TestCase):\n     self.assertAlmostEqual(self.evaluate(loss), 6.8, 5)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class MeanAbsolutePercentageErrorTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -599,7 +607,7 @@ class MeanAbsolutePercentageErrorTest(tf.test.TestCase):\n     self.assertArrayNear(loss, [621.8518, 352.6666], 1e-3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class MeanSquaredLogarithmicErrorTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -657,7 +665,7 @@ class MeanSquaredLogarithmicErrorTest(tf.test.TestCase):\n     self.assertAlmostEqual(self.evaluate(loss), 5.1121, 3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class CosineSimilarityTest(tf.test.TestCase):\n \n   def l2_norm(self, x, axis):\n@@ -739,7 +747,7 @@ class CosineSimilarityTest(tf.test.TestCase):\n     self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class BinaryCrossentropyTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -947,7 +955,7 @@ class BinaryCrossentropyTest(tf.test.TestCase):\n     self.assertAlmostEqual(self.evaluate(loss), 75., 3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class BinaryFocalCrossentropyTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -1130,7 +1138,7 @@ class BinaryFocalCrossentropyTest(tf.test.TestCase):\n     self.assertAlmostEqual(self.evaluate(loss), 0.18166, 3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class CategoricalCrossentropyTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -1296,7 +1304,7 @@ class CategoricalCrossentropyTest(tf.test.TestCase):\n     self.assertAlmostEqual(self.evaluate(loss), 0.3181, 3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class SparseCategoricalCrossentropyTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -1427,7 +1435,7 @@ class SparseCategoricalCrossentropyTest(tf.test.TestCase):\n     self.assertAlmostEqual(self.evaluate(loss), 0.1054, 3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class HingeTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -1523,7 +1531,7 @@ class HingeTest(tf.test.TestCase):\n     self.assertAllClose(self.evaluate(loss), 0., 1e-3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class SquaredHingeTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -1628,7 +1636,7 @@ class SquaredHingeTest(tf.test.TestCase):\n     self.assertAllClose(self.evaluate(loss), 0., 1e-3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class CategoricalHingeTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -1684,7 +1692,7 @@ class CategoricalHingeTest(tf.test.TestCase):\n     self.assertAlmostEqual(self.evaluate(loss), 0., 3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class LogCoshTest(tf.test.TestCase):\n \n   def setup(self):\n@@ -1763,7 +1771,7 @@ class LogCoshTest(tf.test.TestCase):\n     self.assertAlmostEqual(self.evaluate(loss), 0., 3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class PoissonTest(tf.test.TestCase):\n \n   def setup(self):\n@@ -1842,7 +1850,7 @@ class PoissonTest(tf.test.TestCase):\n     self.assertAlmostEqual(self.evaluate(loss), 0., 3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class KLDivergenceTest(tf.test.TestCase):\n \n   def setup(self):\n@@ -1921,7 +1929,7 @@ class KLDivergenceTest(tf.test.TestCase):\n     self.assertAlmostEqual(self.evaluate(loss), 0., 3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class HuberLossTest(tf.test.TestCase):\n \n   def huber_loss(self, y_true, y_pred, delta=1.0):\n@@ -2048,7 +2056,7 @@ class BinaryTruePositivesViaControlFlow(losses.Loss):\n     return result\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class CustomLossTest(tf.test.TestCase):\n \n   def test_autograph(self):\n\n@@ -18,19 +18,18 @@ import copy\n import os\n \n from absl.testing import parameterized\n-from keras import combinations\n-from keras import keras_parameterized\n from keras import layers\n from keras import metrics\n from keras import Model\n-from keras import testing_utils\n from keras.engine import base_layer\n from keras.engine import training as training_module\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class KerasSumTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_sum(self):\n@@ -148,11 +147,11 @@ class KerasSumTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertEqual(600., self.evaluate(restore_sum.result()))\n \n \n-class MeanTest(keras_parameterized.TestCase):\n+class MeanTest(test_combinations.TestCase):\n \n   # TODO(b/120949004): Re-enable garbage collection check\n-  # @test_util.run_in_graph_and_eager_modes(assert_no_eager_garbage=True)\n-  @keras_parameterized.run_all_keras_modes\n+  # @tf_test_util.run_in_graph_and_eager_modes(assert_no_eager_garbage=True)\n+  @test_combinations.run_all_keras_modes\n   def test_mean(self):\n     m = metrics.Mean(name='my_mean')\n \n@@ -194,7 +193,7 @@ class MeanTest(keras_parameterized.TestCase):\n     self.assertEqual(m2.dtype, tf.float32)\n     self.assertEqual(len(m2.variables), 2)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_function_wrapped_reset_state(self):\n     m = metrics.Mean(name='my_mean')\n \n@@ -208,7 +207,7 @@ class MeanTest(keras_parameterized.TestCase):\n       self.evaluate(reset_in_fn())\n     self.assertEqual(self.evaluate(m.count), 1)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_mean_with_sample_weight(self):\n     m = metrics.Mean(dtype=tf.float64)\n     self.assertEqual(m.dtype, tf.float64)\n@@ -252,7 +251,7 @@ class MeanTest(keras_parameterized.TestCase):\n     self.assertEqual(np.round(self.evaluate(m.total), decimals=2), 58.54)\n     self.assertEqual(np.round(self.evaluate(m.count), decimals=2), 5.6)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_mean_graph_with_placeholder(self):\n     with tf.compat.v1.get_default_graph().as_default(), self.cached_session() as sess:\n       m = metrics.Mean()\n@@ -273,7 +272,7 @@ class MeanTest(keras_parameterized.TestCase):\n       self.assertAlmostEqual(self.evaluate(m.count), 1.7, 2)  # 0.5 + 1.2\n       self.assertAlmostEqual(result, 52 / 1.7, 2)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_save_restore(self):\n     checkpoint_directory = self.get_temp_dir()\n     checkpoint_prefix = os.path.join(checkpoint_directory, 'ckpt')\n@@ -304,7 +303,7 @@ class MeanTest(keras_parameterized.TestCase):\n     self.assertEqual(200., self.evaluate(restore_mean.result()))\n     self.assertEqual(3, self.evaluate(restore_mean.count))\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_multiple_instances(self):\n     m = metrics.Mean()\n     m2 = metrics.Mean()\n@@ -313,10 +312,10 @@ class MeanTest(keras_parameterized.TestCase):\n     self.assertEqual(m2.name, 'mean')\n \n     self.assertEqual([v.name for v in m.variables],\n-                     testing_utils.get_expected_metric_variable_names(\n+                     test_utils.get_expected_metric_variable_names(\n                          ['total', 'count']))\n     self.assertEqual([v.name for v in m2.variables],\n-                     testing_utils.get_expected_metric_variable_names(\n+                     test_utils.get_expected_metric_variable_names(\n                          ['total', 'count'], name_suffix='_1'))\n \n     self.evaluate(tf.compat.v1.variables_initializer(m.variables))\n@@ -342,7 +341,7 @@ class MeanTest(keras_parameterized.TestCase):\n     self.assertEqual(self.evaluate(m.total), 100)\n     self.assertEqual(self.evaluate(m.count), 1)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_deepcopy_of_metrics(self):\n     m = metrics.Mean(name='my_mean')\n \n@@ -362,7 +361,8 @@ class MeanTest(keras_parameterized.TestCase):\n \n class MeanTensorTest(tf.test.TestCase, parameterized.TestCase):\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_config(self):\n     with self.test_session():\n       m = metrics.MeanTensor(name='mean_by_element')\n@@ -385,7 +385,8 @@ class MeanTensorTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertEqual(m2.dtype, tf.float32)\n       self.assertEmpty(m2.variables)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_unweighted(self):\n     with self.test_session():\n       m = metrics.MeanTensor(dtype=tf.float64)\n@@ -410,7 +411,8 @@ class MeanTensorTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertAllClose(self.evaluate(m.total), [0, 0])\n       self.assertAllClose(self.evaluate(m.count), [0, 0])\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_weighted(self):\n     with self.test_session():\n       m = metrics.MeanTensor(dtype=tf.float64)\n@@ -449,7 +451,8 @@ class MeanTensorTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertAllClose(self.evaluate(m.total), [[1], [1]])\n       self.assertAllClose(self.evaluate(m.count), [[1], [0.2]])\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_invalid_value_shape(self):\n     m = metrics.MeanTensor(dtype=tf.float64)\n     m([1])\n@@ -457,7 +460,8 @@ class MeanTensorTest(tf.test.TestCase, parameterized.TestCase):\n         ValueError, 'MeanTensor input values must always have the same shape'):\n       m([1, 5])\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_build_in_tf_function(self):\n     \"\"\"Ensure that variables are created correctly in a tf function.\"\"\"\n     m = metrics.MeanTensor(dtype=tf.float64)\n@@ -472,7 +476,7 @@ class MeanTensorTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertAllClose(self.evaluate(m.count), [1, 1])\n       self.assertAllClose(self.evaluate(call_metric([20, 2])), [60, 21])\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def test_in_keras_model(self):\n     class ModelWithMetric(Model):\n \n@@ -563,7 +567,7 @@ class BinaryTruePositivesViaControlFlow(metrics.Metric):\n     return 0.0\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class CustomMetricsTest(tf.test.TestCase):\n \n   def test_config(self):\n\n@@ -20,7 +20,7 @@ import json\n \n from absl.testing import parameterized\n import numpy as np\n-from keras import combinations\n+from keras.testing_infra import test_combinations\n from keras import layers\n from keras import metrics\n from keras import models\n@@ -28,7 +28,7 @@ from keras.utils import metrics_utils\n from tensorflow.python.platform import tf_logging\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class FalsePositivesTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_config(self):\n@@ -108,7 +108,7 @@ class FalsePositivesTest(tf.test.TestCase, parameterized.TestCase):\n       metrics.FalsePositives(thresholds=[None])\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class FalseNegativesTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_config(self):\n@@ -176,7 +176,7 @@ class FalseNegativesTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllClose([4., 16., 23.], self.evaluate(result))\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class TrueNegativesTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_config(self):\n@@ -244,7 +244,7 @@ class TrueNegativesTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllClose([5., 15., 23.], self.evaluate(result))\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class TruePositivesTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_config(self):\n@@ -311,7 +311,7 @@ class TruePositivesTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllClose([222., 111., 37.], self.evaluate(result))\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class PrecisionTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_config(self):\n@@ -516,7 +516,7 @@ class PrecisionTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAlmostEqual(0, self.evaluate(p_obj.false_positives))\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class RecallTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_config(self):\n@@ -720,7 +720,7 @@ class RecallTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAlmostEqual(3, self.evaluate(r_obj.false_negatives))\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class SensitivityAtSpecificityTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_config(self):\n@@ -834,7 +834,7 @@ class SensitivityAtSpecificityTest(tf.test.TestCase, parameterized.TestCase):\n       metrics.SensitivityAtSpecificity(0.4, num_thresholds=-1)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class SpecificityAtSensitivityTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_config(self):\n@@ -947,7 +947,7 @@ class SpecificityAtSensitivityTest(tf.test.TestCase, parameterized.TestCase):\n       metrics.SpecificityAtSensitivity(0.4, num_thresholds=-1)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class PrecisionAtRecallTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_config(self):\n@@ -1061,7 +1061,7 @@ class PrecisionAtRecallTest(tf.test.TestCase, parameterized.TestCase):\n       metrics.PrecisionAtRecall(0.4, num_thresholds=-1)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class RecallAtPrecisionTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_config(self):\n@@ -1194,7 +1194,7 @@ class RecallAtPrecisionTest(tf.test.TestCase, parameterized.TestCase):\n       metrics.RecallAtPrecision(0.4, num_thresholds=-1)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class AUCTest(tf.test.TestCase, parameterized.TestCase):\n \n   def setup(self):\n@@ -1492,7 +1492,7 @@ class AUCTest(tf.test.TestCase, parameterized.TestCase):\n       tf_logging.warning('Cannot test special functions: %s' % str(e))\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class MultiAUCTest(tf.test.TestCase, parameterized.TestCase):\n \n   def setup(self):\n@@ -1778,7 +1778,7 @@ class MultiAUCTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertAllEqual(auc_obj.true_positives, np.zeros((5, 2)))\n \n \n-@combinations.generate(combinations.combine(mode=['eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['eager']))\n class ThresholdsTest(tf.test.TestCase, parameterized.TestCase):\n \n   @parameterized.parameters([\n\n@@ -19,11 +19,11 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n import numpy as np\n \n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras import layers\n from keras import losses\n from keras import metrics\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.utils import losses_utils\n \n \n@@ -38,7 +38,7 @@ def get_multi_io_model():\n \n   branch_a = [inp_1, x, out_1]\n   branch_b = [inp_2, x, out_2]\n-  return testing_utils.get_multi_io_model(branch_a, branch_b)\n+  return test_utils.get_multi_io_model(branch_a, branch_b)\n \n \n def custom_generator_multi_io(sample_weights=None):\n@@ -62,9 +62,9 @@ def custom_generator_multi_io(sample_weights=None):\n     yield x, y, sw\n \n \n-@keras_parameterized.run_with_all_model_types(exclude_models=['sequential'])\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class TestMetricsCorrectnessMultiIO(keras_parameterized.TestCase):\n+@test_combinations.run_with_all_model_types(exclude_models=['sequential'])\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class TestMetricsCorrectnessMultiIO(test_combinations.TestCase):\n \n   def _get_compiled_multi_io_model(self):\n     model = get_multi_io_model()\n@@ -75,7 +75,7 @@ class TestMetricsCorrectnessMultiIO(keras_parameterized.TestCase):\n         weighted_metrics=[\n             metrics.MeanSquaredError(name='mean_squared_error_2')\n         ],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     return model\n \n   def setUp(self):\n@@ -339,15 +339,15 @@ class TestMetricsCorrectnessMultiIO(keras_parameterized.TestCase):\n                         self.expected_batch_result_with_weights_output_2, 1e-3)\n \n \n-@keras_parameterized.run_with_all_model_types\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class TestMetricsCorrectnessSingleIO(keras_parameterized.TestCase):\n+@test_combinations.run_with_all_model_types\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class TestMetricsCorrectnessSingleIO(test_combinations.TestCase):\n \n   def _get_model(self):\n     x = layers.Dense(3, kernel_initializer='ones', trainable=False)\n     out = layers.Dense(\n         1, kernel_initializer='ones', name='output', trainable=False)\n-    model = testing_utils.get_model_from_layers([x, out], input_shape=(1,))\n+    model = test_utils.get_model_from_layers([x, out], input_shape=(1,))\n     model.compile(\n         optimizer='rmsprop',\n         loss='mse',\n@@ -355,7 +355,7 @@ class TestMetricsCorrectnessSingleIO(keras_parameterized.TestCase):\n         weighted_metrics=[\n             metrics.MeanSquaredError(name='mean_squared_error_2')\n         ],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     return model\n \n   def _custom_generator(self, sample_weight=None):\n@@ -550,21 +550,21 @@ class TestMetricsCorrectnessSingleIO(keras_parameterized.TestCase):\n                         1e-3)\n \n \n-@keras_parameterized.run_with_all_model_types(exclude_models=['sequential'])\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+@test_combinations.run_with_all_model_types(exclude_models=['sequential'])\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n @parameterized.parameters([\n     losses_utils.ReductionV2.SUM_OVER_BATCH_SIZE,\n     losses_utils.ReductionV2.AUTO,\n     losses_utils.ReductionV2.SUM\n ])\n-class TestOutputLossMetrics(keras_parameterized.TestCase):\n+class TestOutputLossMetrics(test_combinations.TestCase):\n \n   def _get_compiled_multi_io_model(self, loss):\n     model = get_multi_io_model()\n     model.compile(\n         optimizer='rmsprop',\n         loss=loss,\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     return model\n \n   def setUp(self):\n\n@@ -20,7 +20,7 @@ from absl.testing import parameterized\n import numpy as np\n \n from keras import backend\n-from keras import combinations\n+from keras.testing_infra import test_combinations\n from keras import metrics\n \n \n@@ -71,7 +71,7 @@ class KerasFunctionalMetricsTest(tf.test.TestCase, parameterized.TestCase):\n       y_pred = backend.variable(np.random.random((6, 7)))\n       self.assertEqual(backend.eval(metric(y_true, y_pred)).shape, (6,))\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def test_sparse_categorical_accuracy_eager(self):\n     \"\"\"Tests that ints passed in via Eager return results. See b/113504761.\"\"\"\n     metric = metrics.sparse_categorical_accuracy\n@@ -79,7 +79,7 @@ class KerasFunctionalMetricsTest(tf.test.TestCase, parameterized.TestCase):\n     y_pred = np.arange(36).reshape([6, 6])\n     self.assertAllEqual(metric(y_true, y_pred), [0., 0., 0., 0., 0., 1.])\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def test_sparse_categorical_accuracy_float_eager(self):\n     \"\"\"Tests that floats passed in via Eager return results. See b/113504761.\"\"\"\n     metric = metrics.sparse_categorical_accuracy\n\n@@ -18,16 +18,15 @@ import json\n import math\n \n from keras import backend\n-from keras import combinations\n-from keras import keras_parameterized\n from keras import layers\n from keras import metrics\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class KerasAccuracyTest(tf.test.TestCase):\n \n   def test_accuracy(self):\n@@ -271,7 +270,7 @@ class KerasAccuracyTest(tf.test.TestCase):\n     self.assertEqual(acc_fn, metrics.accuracy)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class CosineSimilarityTest(tf.test.TestCase):\n \n   def l2_norm(self, x, axis):\n@@ -332,7 +331,7 @@ class CosineSimilarityTest(tf.test.TestCase):\n     self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class MeanAbsoluteErrorTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -370,7 +369,7 @@ class MeanAbsoluteErrorTest(tf.test.TestCase):\n     self.assertAllClose(0.54285, self.evaluate(result), atol=1e-5)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class MeanAbsolutePercentageErrorTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -410,7 +409,7 @@ class MeanAbsolutePercentageErrorTest(tf.test.TestCase):\n     self.assertAllClose(40e7, self.evaluate(result), atol=1e-5)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class MeanSquaredErrorTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -448,7 +447,7 @@ class MeanSquaredErrorTest(tf.test.TestCase):\n     self.assertAllClose(0.54285, self.evaluate(result), atol=1e-5)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class MeanSquaredLogarithmicErrorTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -488,7 +487,7 @@ class MeanSquaredLogarithmicErrorTest(tf.test.TestCase):\n     self.assertAllClose(0.26082, self.evaluate(result), atol=1e-5)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class HingeTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -543,7 +542,7 @@ class HingeTest(tf.test.TestCase):\n     self.assertAllClose(0.493, self.evaluate(result), atol=1e-3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class SquaredHingeTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -604,7 +603,7 @@ class SquaredHingeTest(tf.test.TestCase):\n     self.assertAllClose(0.347, self.evaluate(result), atol=1e-3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class CategoricalHingeTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -644,7 +643,7 @@ class CategoricalHingeTest(tf.test.TestCase):\n     self.assertAllClose(0.5, self.evaluate(result), atol=1e-5)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class RootMeanSquaredErrorTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -678,7 +677,7 @@ class RootMeanSquaredErrorTest(tf.test.TestCase):\n     self.assertAllClose(math.sqrt(13), self.evaluate(result), atol=1e-3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class TopKCategoricalAccuracyTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -725,7 +724,7 @@ class TopKCategoricalAccuracyTest(tf.test.TestCase):\n     self.assertAllClose(1.0, self.evaluate(result), atol=1e-5)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class SparseTopKCategoricalAccuracyTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -772,7 +771,7 @@ class SparseTopKCategoricalAccuracyTest(tf.test.TestCase):\n     self.assertAllClose(1.0, self.evaluate(result), atol=1e-5)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class LogCoshErrorTest(tf.test.TestCase):\n \n   def setup(self):\n@@ -815,7 +814,7 @@ class LogCoshErrorTest(tf.test.TestCase):\n     self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class PoissonTest(tf.test.TestCase):\n \n   def setup(self):\n@@ -861,7 +860,7 @@ class PoissonTest(tf.test.TestCase):\n     self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class KLDivergenceTest(tf.test.TestCase):\n \n   def setup(self):\n@@ -908,7 +907,7 @@ class KLDivergenceTest(tf.test.TestCase):\n     self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class MeanRelativeErrorTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -964,7 +963,7 @@ class MeanRelativeErrorTest(tf.test.TestCase):\n     self.assertEqual(self.evaluate(result), 0)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class IoUTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -1051,7 +1050,7 @@ class IoUTest(tf.test.TestCase):\n     self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class BinaryIoUTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -1158,7 +1157,7 @@ class BinaryIoUTest(tf.test.TestCase):\n     self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class MeanIoUTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -1241,7 +1240,7 @@ class MeanIoUTest(tf.test.TestCase):\n     self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class OneHotIoUTest(tf.test.TestCase):\n \n   def test_unweighted(self):\n@@ -1281,7 +1280,7 @@ class OneHotIoUTest(tf.test.TestCase):\n     self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class OneHotMeanIoUTest(tf.test.TestCase):\n \n   def test_unweighted(self):\n@@ -1333,7 +1332,7 @@ class OneHotMeanIoUTest(tf.test.TestCase):\n     self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class BinaryCrossentropyTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -1453,7 +1452,7 @@ class BinaryCrossentropyTest(tf.test.TestCase):\n     self.assertAllClose(expected_value, self.evaluate(result), atol=1e-3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class CategoricalCrossentropyTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -1579,7 +1578,7 @@ class CategoricalCrossentropyTest(tf.test.TestCase):\n     self.assertAllClose(self.evaluate(loss), 3.667, atol=1e-3)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class SparseCategoricalCrossentropyTest(tf.test.TestCase):\n \n   def test_config(self):\n@@ -1783,18 +1782,18 @@ def _get_model(compile_metrics):\n       layers.Dense(3, activation='relu', kernel_initializer='ones'),\n       layers.Dense(1, activation='sigmoid', kernel_initializer='ones')]\n \n-  model = testing_utils.get_model_from_layers(model_layers, input_shape=(4,))\n+  model = test_utils.get_model_from_layers(model_layers, input_shape=(4,))\n   model.compile(\n       loss='mae',\n       metrics=compile_metrics,\n       optimizer='rmsprop',\n-      run_eagerly=testing_utils.should_run_eagerly())\n+      run_eagerly=test_utils.should_run_eagerly())\n   return model\n \n \n-@keras_parameterized.run_with_all_model_types\n-@keras_parameterized.run_all_keras_modes\n-class ResetStatesTest(keras_parameterized.TestCase):\n+@test_combinations.run_with_all_model_types\n+@test_combinations.run_all_keras_modes\n+class ResetStatesTest(test_combinations.TestCase):\n \n   def test_reset_state_false_positives(self):\n     fp_obj = metrics.FalsePositives()\n@@ -1939,12 +1938,12 @@ class ResetStatesTest(keras_parameterized.TestCase):\n     auc_obj = metrics.AUC(num_thresholds=3, from_logits=True)\n \n     model_layers = [layers.Dense(1, kernel_initializer='ones', use_bias=False)]\n-    model = testing_utils.get_model_from_layers(model_layers, input_shape=(4,))\n+    model = test_utils.get_model_from_layers(model_layers, input_shape=(4,))\n     model.compile(\n         loss='mae',\n         metrics=[auc_obj],\n         optimizer='rmsprop',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x = np.concatenate((np.ones((25, 4)), -np.ones((25, 4)), -np.ones(\n         (25, 4)), np.ones((25, 4))))\n@@ -2004,8 +2003,8 @@ class ResetStatesTest(keras_parameterized.TestCase):\n       backend.set_floatx('float32')\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-class MergeStateTest(keras_parameterized.TestCase):\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n+class MergeStateTest(test_combinations.TestCase):\n \n   def test_merge_state_incompatible_metrics(self):\n     with self.assertRaisesRegex(ValueError,\n\n@@ -18,7 +18,7 @@ import tensorflow.compat.v2 as tf\n \n import re\n \n-from keras import combinations\n+from keras.testing_infra import test_combinations\n from keras.mixed_precision import device_compatibility_check\n from tensorflow.python.platform import tf_logging\n \n@@ -32,7 +32,7 @@ def device_details(device_name, compute_capability=None):\n   return details\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class DeviceCompatibilityCheckTest(tf.test.TestCase):\n \n   def _test_compat_check(self, device_attr_list, should_warn, expected_regex,\n\n@@ -18,10 +18,10 @@ import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n import numpy as np\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras import layers\n from keras import models\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.layers import activation\n from keras.layers import attention\n from keras.layers import convolutional\n@@ -62,8 +62,8 @@ def _create_normalization_layer_without_adapt():\n   )\n \n \n-@testing_utils.run_v2_only\n-class LayerCorrectnessTest(keras_parameterized.TestCase):\n+@test_utils.run_v2_only\n+class LayerCorrectnessTest(test_combinations.TestCase):\n \n   def setUp(self):\n     super(LayerCorrectnessTest, self).setUp()\n\n@@ -20,8 +20,7 @@ import os\n \n from absl.testing import parameterized\n import numpy as np\n-from keras import combinations\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras import layers\n from keras import models\n from keras.engine import base_layer\n@@ -71,8 +70,8 @@ TESTCASES = ({\n })\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-class LayerTest(keras_parameterized.TestCase):\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n+class LayerTest(test_combinations.TestCase):\n   \"\"\"Test mixed precision with Keras layers.\"\"\"\n \n   @parameterized.named_parameters(*TESTCASES)\n\n@@ -19,7 +19,6 @@ from unittest import mock\n \n from absl.testing import parameterized\n \n-from keras import combinations\n from keras import optimizers\n from keras.mixed_precision import loss_scale_optimizer\n from keras.mixed_precision import test_util as mp_test_util\n@@ -28,11 +27,12 @@ from keras.optimizers.optimizer_experimental import sgd as sgd_experimental\n from keras.optimizers.optimizer_v2 import adam\n from keras.optimizers.optimizer_v2 import gradient_descent\n from keras.optimizers.optimizer_v2 import optimizer_v2\n+from keras.testing_infra import test_combinations\n \n import numpy as np\n import tensorflow.compat.v2 as tf\n # pylint: disable=g-direct-tensorflow-import\n-from tensorflow.python.framework import test_util\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n from tensorflow.python.keras.optimizer_v2 import gradient_descent as legacy_sgd\n from tensorflow.python.platform import tf_logging\n \n@@ -116,12 +116,12 @@ def opt_and_strategy_and_mode_combinations():\n   # For the experimental optimizer, don't use graph mode directly since it's\n   # unsupported. Instead, run both without and with a tf.function, in order to\n   # test both graph and eager mode.\n-  experimental_opt_combinations = combinations.combine(\n+  experimental_opt_combinations = test_combinations.combine(\n       opt_cls=optimizer_experimental.Optimizer,\n       strategy_fn=STRATEGY_FNS,\n       mode='eager',\n       use_tf_function=[False, True])\n-  orig_opt_combinations = combinations.combine(\n+  orig_opt_combinations = test_combinations.combine(\n       opt_cls=optimizer_v2.OptimizerV2,\n       strategy_fn=STRATEGY_FNS,\n       mode=['graph', 'eager'],\n@@ -131,13 +131,14 @@ def opt_and_strategy_and_mode_combinations():\n \n def opt_combinations_only():\n   \"\"\"Returns two combinations for running with the two base optimizers.\"\"\"\n-  experimental_opt_combinations = combinations.combine(\n+  experimental_opt_combinations = test_combinations.combine(\n       mode='eager', opt_cls=optimizer_experimental.Optimizer)\n-  orig_opt_combination = combinations.combine(opt_cls=optimizer_v2.OptimizerV2)\n+  orig_opt_combination = test_combinations.combine(\n+      opt_cls=optimizer_v2.OptimizerV2)\n   return experimental_opt_combinations + orig_opt_combination\n \n \n-@test_util.with_control_flow_v2\n+@tf_test_utils.with_control_flow_v2\n class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n \n   def _run_if_in_graph_mode(self, val):\n@@ -170,7 +171,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertIsInstance(optimizer,\n                           loss_scale_optimizer.BaseLossScaleOptimizer)\n \n-  @combinations.generate(opt_and_strategy_and_mode_combinations())\n+  @test_combinations.generate(opt_and_strategy_and_mode_combinations())\n   def testFixedLossScaleAppliedToLossWithMinimize(self, opt_cls, strategy_fn,\n                                                   use_tf_function):\n     with strategy_fn().scope() as strategy:\n@@ -211,7 +212,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       # mp_test_util.create_identity_with_grad_check_fn added an assertion op.\n       self.evaluate(run_op)\n \n-  @combinations.generate(opt_combinations_only())\n+  @test_combinations.generate(opt_combinations_only())\n   def testDynamicAttrsWithFixedLossScale(self, opt_cls):\n     opt = create_sgd(opt_cls)\n     opt = create_lso(opt, dynamic=False, initial_scale=2.)\n@@ -219,7 +220,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertIsNone(opt.dynamic_counter)\n     self.assertIsNone(opt.dynamic_growth_steps)\n \n-  @combinations.generate(opt_combinations_only())\n+  @test_combinations.generate(opt_combinations_only())\n   def testGetScaledLoss(self, opt_cls):\n     opt = create_sgd(opt_cls)\n     opt = create_lso(opt, dynamic=False, initial_scale=2.)\n@@ -230,7 +231,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(10., self.evaluate(opt.get_scaled_loss(loss)))\n     self.assertEqual(10., self.evaluate(opt.get_scaled_loss(lambda: loss)()))\n \n-  @combinations.generate(opt_combinations_only())\n+  @test_combinations.generate(opt_combinations_only())\n   def testGetUnscaledGradients(self, opt_cls):\n     opt = create_sgd(opt_cls)\n     opt = create_lso(opt, dynamic=False, initial_scale=2)\n@@ -242,21 +243,20 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     grads = [self.evaluate(g) if g is not None else g for g in grads]\n     self.assertEqual([1.5, None, -2.], grads)\n \n-  @combinations.generate(opt_combinations_only())\n+  @test_combinations.generate(opt_combinations_only())\n   def testGetUnscaledSparseGradients(self, opt_cls):\n     opt = create_sgd(opt_cls)\n     opt = create_lso(opt, dynamic=False, initial_scale=2)\n     sparse_scaled_grad = tf.IndexedSlices(\n         tf.convert_to_tensor([[4., 2.], [8., 5.]]),\n         tf.convert_to_tensor([1, 3], dtype='int32'),\n-        dense_shape=tf.convert_to_tensor([5, 2],\n-                                                           dtype='int32'))\n+        dense_shape=tf.convert_to_tensor([5, 2], dtype='int32'))\n     sparse_grad = opt.get_unscaled_gradients([sparse_scaled_grad])[0]\n     self.assertIsInstance(sparse_grad, tf.IndexedSlices)\n     self.assertAllEqual([[2., 1.], [4., 2.5]],\n                         self.evaluate(sparse_grad.values))\n \n-  @combinations.generate(opt_and_strategy_and_mode_combinations())\n+  @test_combinations.generate(opt_and_strategy_and_mode_combinations())\n   def testDynamicLossScale(self, opt_cls, strategy_fn, use_tf_function):\n     strategy = strategy_fn()\n     learning_rate = 2.\n@@ -292,7 +292,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       # 1.\n       self.assertAllClose([1.], self.evaluate(var))\n \n-  @combinations.generate(opt_combinations_only())\n+  @test_combinations.generate(opt_combinations_only())\n   def testDynamicLossScaleDefaultValues(self, opt_cls):\n     opt = create_sgd(opt_cls)\n     opt = create_lso(opt)\n@@ -302,7 +302,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(self.evaluate(opt.loss_scale), 2 ** 15)\n \n   # pylint: disable=cell-var-from-loop\n-  @combinations.generate(opt_and_strategy_and_mode_combinations())\n+  @test_combinations.generate(opt_and_strategy_and_mode_combinations())\n   def testClipping(self, opt_cls, strategy_fn, use_tf_function):\n     strategy = strategy_fn()\n     learning_rate = 2.\n@@ -352,7 +352,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertEqual(self.evaluate(opt.loss_scale), 4)\n   # pylint: enable=cell-var-from-loop\n \n-  @combinations.generate(opt_and_strategy_and_mode_combinations())\n+  @test_combinations.generate(opt_and_strategy_and_mode_combinations())\n   def testDynamicUpdate(self, opt_cls, strategy_fn, use_tf_function):\n     with strategy_fn().scope() as strategy:\n       var = tf.Variable([1.0, 2.0])\n@@ -382,7 +382,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       # Loss scale should half due to NaN gradients.\n       self.assertEqual(2., self.evaluate(opt.loss_scale))\n \n-  @combinations.generate(opt_and_strategy_and_mode_combinations())\n+  @test_combinations.generate(opt_and_strategy_and_mode_combinations())\n   def testDynamicLossScaleWithFloat16Loss(self, opt_cls, strategy_fn,\n                                           use_tf_function):\n     strategy = strategy_fn()\n@@ -404,7 +404,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       # and so the variable will be init_val - grad * lr == 5 - 1 * 2 == 3\n       self.assertAllClose([3.], self.evaluate(var))\n \n-  @combinations.generate(opt_and_strategy_and_mode_combinations())\n+  @test_combinations.generate(opt_and_strategy_and_mode_combinations())\n   def testNanOnOneReplicaOnly(self, opt_cls, strategy_fn, use_tf_function):\n     if strategy_fn == default_strategy_fn:\n       self.skipTest('The test is only useful for non-default strategies')\n@@ -459,7 +459,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     # Loss scale should half due to NaN gradients.\n     self.assertEqual(1., self.evaluate(opt.loss_scale))\n \n-  @combinations.generate(opt_and_strategy_and_mode_combinations())\n+  @test_combinations.generate(opt_and_strategy_and_mode_combinations())\n   def testDynamicLossScaleWithSlots(self, opt_cls, strategy_fn,\n                                     use_tf_function):\n     strategy_obj = strategy_fn()\n@@ -507,7 +507,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(lso.iterations, 7)\n     self.assertEqual(opt.iterations, 7)\n \n-  @combinations.generate(opt_and_strategy_and_mode_combinations())\n+  @test_combinations.generate(opt_and_strategy_and_mode_combinations())\n   def testIterationsIncremented(self, opt_cls, strategy_fn, use_tf_function):\n     with strategy_fn().scope() as strategy:\n       # Test iterations is incremented in opt.minimize.\n@@ -594,7 +594,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       with self.assertRaises(AttributeError):\n         lso.loss_scale = 2.\n \n-  @combinations.generate(opt_combinations_only())\n+  @test_combinations.generate(opt_combinations_only())\n   def testArbitraryAttributesNotExposed(self, opt_cls):\n     opt = create_sgd(opt_cls)\n     lso = create_lso(opt)\n@@ -618,7 +618,8 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertNotIn('nesterov', dir_result)  # Attribute on inner optimizer\n     self.assertIn('nesterov', dir(lso.inner_optimizer))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testApplyGradientsGetsUnwrappedTensors(self):\n     # Tests that gradients passed to apply_gradients are not wrapped in a\n     # DistributionStrategy wrapper, such as PerReplica, but instead are raw\n@@ -648,8 +649,8 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       run_fn = lambda: opt.minimize(loss, [var])\n       strategy.experimental_run(run_fn)\n \n-  @combinations.generate(\n-      combinations.combine(mode='eager', use_tf_function=[False, True]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode='eager', use_tf_function=[False, True]))\n   def testApplyGradientsGetsUnwrappedTensorsWithNewOptimizer(\n       self, use_tf_function):\n     outer_self = self\n@@ -675,8 +676,9 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n         run_fn = tf.function(run_fn)\n       strategy.experimental_run(run_fn)\n \n-  @combinations.generate(\n-      combinations.combine(strategy_fn=STRATEGY_FNS, mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(strategy_fn=STRATEGY_FNS,\n+                                mode=['graph', 'eager']))\n   def testV1Optimizer(self, strategy_fn):\n     strategy = strategy_fn()\n     learning_rate = 2.\n@@ -720,8 +722,9 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       for s in strategy.experimental_local_results(opt.dynamic_counter):\n         self.assertEqual(self.evaluate(s), 0)\n \n-  @combinations.generate(\n-      combinations.combine(strategy_fn=STRATEGY_FNS, mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(strategy_fn=STRATEGY_FNS,\n+                                mode=['graph', 'eager']))\n   def testPassingV1LossScale(self, strategy_fn):\n     strategy = strategy_fn()\n     learning_rate = 2.\n@@ -790,7 +793,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                    'DynamicLossScale is no longer supported. Got:'):\n       loss_scale_optimizer.LossScaleOptimizerV1(opt, MyLossScale())\n \n-  @combinations.generate(opt_combinations_only())\n+  @test_combinations.generate(opt_combinations_only())\n   def testLossScaleDelegationWithWrapper(self, opt_cls):\n     # Test learning_rate is exposed when LossScaleOptimizer wraps another\n     # wrapper.\n@@ -828,14 +831,14 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertEqual(self.evaluate(\n           opt.inner_optimizer.inner_optimizer.learning_rate), 2.0)\n \n-  @combinations.generate(\n-      combinations.combine(\n+  @test_combinations.generate(\n+      test_combinations.combine(\n           opt_cls=optimizer_v2.OptimizerV2,\n           strategy_fn=STRATEGY_FNS,\n           mode=['graph', 'eager'],\n           use_tf_function=False,\n           save_with_ls=[False, True],\n-          restore_with_ls=[False, True]) + combinations.combine(\n+          restore_with_ls=[False, True]) + test_combinations.combine(\n               opt_cls=optimizer_experimental.Optimizer,\n               strategy_fn=STRATEGY_FNS,\n               mode='eager',\n@@ -955,10 +958,11 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       if opt_cls == optimizer_v2.OptimizerV2:\n         self.assertEqual(self.evaluate(slot_var).item(), -1)\n \n-  @combinations.generate(\n-      combinations.combine(\n+  @test_combinations.generate(\n+      test_combinations.combine(\n           get_config=['v1', 'v2', 'tf2_3'], from_config=['v1', 'v2']) +\n-      combinations.combine(get_config='v3', from_config='v3', mode='eager'))\n+      test_combinations.combine(\n+          get_config='v3', from_config='v3', mode='eager'))\n   def testGetConfigFixed(self, get_config, from_config):\n     # Get a config from LossScaleOptimizerV1, LossScaleOptimizer,\n     # LossScaleOptimizerV3, or the LossScaleOptimizer from TF 2.3. Then restore\n@@ -1027,10 +1031,11 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     self._run_if_in_graph_mode(run_op)\n     self.assertEqual(self.evaluate(var), [3.])\n \n-  @combinations.generate(\n-      combinations.combine(\n+  @test_combinations.generate(\n+      test_combinations.combine(\n           get_config=['v1', 'v2', 'tf2_3'], from_config=['v1', 'v2']) +\n-      combinations.combine(get_config='v3', from_config='v3', mode='eager'))\n+      test_combinations.combine(\n+          get_config='v3', from_config='v3', mode='eager'))\n   def testGetConfigDynamic(self, get_config, from_config):\n     # Get a config from LossScaleOptimizerV1, LossScaleOptimizer,\n     # LossScaleOptimizerV3, or the LossScaleOptimizer from TF 2.3. Then restore\n@@ -1135,9 +1140,9 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     with self.assertRaisesRegex(ValueError, expected_error):\n       loss_scale_optimizer.LossScaleOptimizerV1.from_config(config)\n \n-  @combinations.generate(\n-      combinations.combine(lso_type=['v1', 'v2']) +\n-      combinations.combine(lso_type='v3', mode='eager'))\n+  @test_combinations.generate(\n+      test_combinations.combine(lso_type=['v1', 'v2']) +\n+      test_combinations.combine(lso_type='v3', mode='eager'))\n   def testSerializationWithBuiltInOptimizer(self, lso_type):\n     if lso_type == 'v1':\n       opt = gradient_descent.SGD(2., momentum=0.5)\n@@ -1178,7 +1183,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(self.evaluate(var), [3.])\n     self.assertEqual(self.evaluate(opt.dynamic_counter), 1)\n \n-  @combinations.generate(opt_combinations_only())\n+  @test_combinations.generate(opt_combinations_only())\n   def testSerializationWithCustomOptimizer(self, opt_cls):\n     sgd_cls = type(create_sgd(opt_cls))\n \n@@ -1203,7 +1208,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(opt.dynamic_growth_steps, 3.)\n     self.assertEqual(opt.inner_optimizer.my_attribute, 123)\n \n-  @combinations.generate(opt_combinations_only())\n+  @test_combinations.generate(opt_combinations_only())\n   def testUnsupportedStrategy(self, opt_cls):\n     strategy = tf.distribute.experimental.CentralStorageStrategy()\n     expected_error = (\n@@ -1220,7 +1225,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       with self.assertRaisesRegex(ValueError, expected_error):\n         strategy.experimental_run(run_fn)\n \n-  @combinations.generate(opt_combinations_only())\n+  @test_combinations.generate(opt_combinations_only())\n   def testInvalidArgsWithFixedLossScale(self, opt_cls):\n     opt = create_sgd(opt_cls)\n     with self.assertRaisesRegex(\n@@ -1232,7 +1237,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                     'False, but got: 2'):\n       create_lso(opt, dynamic=False, initial_scale=1, dynamic_growth_steps=2)\n \n-  @combinations.generate(opt_combinations_only())\n+  @test_combinations.generate(opt_combinations_only())\n   def testDynamicMustBeBool(self, opt_cls):\n     opt = create_sgd(opt_cls)\n     with self.assertRaisesRegex(\n@@ -1240,7 +1245,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                    \"a bool, but got: 'dynamic'\"):\n       create_lso(opt, 'dynamic')\n \n-  @combinations.generate(opt_combinations_only())\n+  @test_combinations.generate(opt_combinations_only())\n   def testScalingWarning(self, opt_cls):\n     var = tf.Variable(1.0)\n     lso = create_lso(create_sgd(opt_cls))\n@@ -1272,7 +1277,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       lso.apply_gradients([(tf.constant(1.0), var)])\n       mock_warn.assert_not_called()\n \n-  @combinations.generate(opt_combinations_only())\n+  @test_combinations.generate(opt_combinations_only())\n   def testErrorWhenNesting(self, opt_cls):\n     opt = create_sgd(opt_cls)\n     opt = create_lso(opt)\n@@ -1280,7 +1285,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n         TypeError, 'LossScaleOptimizer cannot wrap another LossScaleOptimizer'):\n       create_lso(opt)\n \n-  @combinations.generate(opt_combinations_only())\n+  @test_combinations.generate(opt_combinations_only())\n   def testErrorWrappingSameOptimizerMultipleTimes(self, opt_cls):\n     inner_opt = create_sgd(opt_cls)\n     create_lso(inner_opt)\n\n@@ -17,15 +17,14 @@\n import tensorflow.compat.v2 as tf\n \n import os\n-from keras import combinations\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.mixed_precision import loss_scale_optimizer as loss_scale_optimizer_v2\n from keras.mixed_precision import policy\n from keras.optimizers.optimizer_v2 import gradient_descent as gradient_descent_v2\n \n \n-class MixedPrecisionTest(keras_parameterized.TestCase):\n+class MixedPrecisionTest(test_combinations.TestCase):\n \n   IGNORE_PERF_VAR = 'TF_AUTO_MIXED_PRECISION_GRAPH_REWRITE_IGNORE_PERFORMANCE'\n \n@@ -46,7 +45,8 @@ class MixedPrecisionTest(keras_parameterized.TestCase):\n     tf.compat.v1.mixed_precision.disable_mixed_precision_graph_rewrite()\n     super(MixedPrecisionTest, self).tearDown()\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_wrap_optimizer(self):\n     opt = gradient_descent_v2.SGD(1.0)\n     opt = tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite(opt, 123.)\n@@ -54,7 +54,8 @@ class MixedPrecisionTest(keras_parameterized.TestCase):\n         opt, loss_scale_optimizer_v2.LossScaleOptimizerV1)\n     self.assertEqual(self.evaluate(opt.loss_scale), 123.)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_optimizer_errors(self):\n     opt = gradient_descent_v2.SGD(1.0)\n     opt = loss_scale_optimizer_v2.LossScaleOptimizerV1(opt, 'dynamic')\n@@ -65,7 +66,7 @@ class MixedPrecisionTest(keras_parameterized.TestCase):\n     self.assertFalse(tf.config.optimizer.get_experimental_options()\n                      .get('auto_mixed_precision', False))\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_error_if_policy_is_set(self):\n     with policy.policy_scope('mixed_float16'):\n       with self.assertRaisesRegex(ValueError,\n\n@@ -22,12 +22,11 @@ from absl import flags\n from absl.testing import parameterized\n import numpy as np\n from keras import backend\n-from keras import combinations\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras import layers\n from keras import models\n from keras.optimizers import optimizer_v1\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.applications import densenet\n from keras.applications import efficientnet\n from keras.applications import inception_resnet_v2\n@@ -72,17 +71,17 @@ TESTCASES = ({\n })\n \n \n-class KerasModelTest(keras_parameterized.TestCase):\n+class KerasModelTest(test_combinations.TestCase):\n   \"\"\"Test mixed precision with Keras models.\"\"\"\n \n   def _skip_if_strategy_unsupported(self, strategy_fn):\n     if (strategy_fn != default_strategy_fn and\n-        testing_utils.get_model_type() == 'subclass'):\n+        test_utils.get_model_type() == 'subclass'):\n       self.skipTest('Non-default strategies are unsupported with subclassed '\n                     'models')\n \n   def _skip_if_save_format_unsupported(self, save_format):\n-    model_type = testing_utils.get_model_type()\n+    model_type = test_utils.get_model_type()\n     if save_format == 'h5' and model_type == 'subclass':\n       self.skipTest('Saving subclassed models with the HDF5 format is '\n                     'unsupported')\n@@ -91,8 +90,8 @@ class KerasModelTest(keras_parameterized.TestCase):\n       self.skipTest('b/148820505: This combination of features is currently '\n                     'broken.')\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   @parameterized.named_parameters(\n       {\n           'testcase_name': 'base',\n@@ -178,7 +177,7 @@ class KerasModelTest(keras_parameterized.TestCase):\n             input_shape=(1,))\n         if use_input_spec:\n           layer.input_spec = input_spec.InputSpec(shape=(None, 1))\n-        model = testing_utils.get_model_from_layers([layer], input_shape=(1,),\n+        model = test_utils.get_model_from_layers([layer], input_shape=(1,),\n                                                  input_dtype=tf.float16)\n         if get_config:\n           config = model.get_config()\n@@ -203,7 +202,7 @@ class KerasModelTest(keras_parameterized.TestCase):\n         model.compile(\n             opt,\n             loss=loss_fn,\n-            run_eagerly=testing_utils.should_run_eagerly())\n+            run_eagerly=test_utils.should_run_eagerly())\n \n     x = np.ones((2, 1))\n     y = np.ones((2, 1))\n@@ -262,7 +261,7 @@ class KerasModelTest(keras_parameterized.TestCase):\n                      {'class_name': 'Policy', 'config': {\n                          'name': 'mixed_float16'}})\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   @parameterized.named_parameters(\n       {\n           'testcase_name': 'base',\n@@ -300,7 +299,7 @@ class KerasModelTest(keras_parameterized.TestCase):\n       model.compile(\n           opt,\n           loss=loss_fn,\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n \n     self.assertEqual(backend.eval(layer.v), 1)\n     x = np.ones((batch_size, 1))\n@@ -311,7 +310,7 @@ class KerasModelTest(keras_parameterized.TestCase):\n     expected = 0\n     self.assertEqual(backend.eval(layer.v), expected)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   @parameterized.named_parameters(\n       {\n           'testcase_name': 'base',\n@@ -379,7 +378,7 @@ class KerasModelTest(keras_parameterized.TestCase):\n         model.compile(\n             opt,\n             loss=loss_fn,\n-            run_eagerly=testing_utils.should_run_eagerly())\n+            run_eagerly=test_utils.should_run_eagerly())\n \n     x = np.ones((2, 1))\n     y = np.ones((2, 1))\n@@ -393,7 +392,7 @@ class KerasModelTest(keras_parameterized.TestCase):\n         # Layer does not have weight regularizer\n         self.assertEqual(backend.eval(layer.v), 1 - learning_rate)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   @parameterized.named_parameters(\n       {\n           'testcase_name': 'base',\n@@ -478,7 +477,7 @@ class KerasModelTest(keras_parameterized.TestCase):\n         model.compile(\n             opt,\n             loss=loss_fn,\n-            run_eagerly=testing_utils.should_run_eagerly())\n+            run_eagerly=test_utils.should_run_eagerly())\n \n     self.assertEqual(backend.eval(layer.v), 1)\n     x = np.ones((batch_size, 1))\n@@ -513,7 +512,8 @@ class KerasModelTest(keras_parameterized.TestCase):\n     model.fit(dataset)\n     self.assertEqual(backend.eval(layer.v), -3)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_loss_scale_optimizer_overrides_policy_v1_loss_scale(self):\n     with policy.policy_scope(policy.PolicyV1('float32', loss_scale=10.)):\n       opt = gradient_descent.SGD(1.)\n@@ -525,7 +525,8 @@ class KerasModelTest(keras_parameterized.TestCase):\n       model.compile(opt, loss='mse')\n       self.assertEqual(self.evaluate(model.optimizer.loss_scale), 5.)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_policy_v1_without_loss_scale(self):\n     with policy.policy_scope(policy.PolicyV1('mixed_float16',\n                                              loss_scale=None)):\n@@ -537,7 +538,8 @@ class KerasModelTest(keras_parameterized.TestCase):\n       self.assertNotIsInstance(model.optimizer,\n                                loss_scale_optimizer.LossScaleOptimizer)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_pass_invalid_optimizer_with_loss_scaling(self):\n     with policy.policy_scope(policy.PolicyV1('float32', loss_scale=10.)):\n       x = layers.Input(shape=(1,))\n@@ -550,7 +552,8 @@ class KerasModelTest(keras_parameterized.TestCase):\n       with self.assertRaisesRegex(ValueError, error_msg):\n         model.compile(optimizer_v1.SGD(1.), 'mse')\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_functional_model_loss_dtype(self):\n     with policy.policy_scope('float16'):\n       x = layers.Input(shape=(1,))\n@@ -560,7 +563,7 @@ class KerasModelTest(keras_parameterized.TestCase):\n       # The loss should not be casted to the policy's dtype.\n       self.assertEqual(model.losses[0].dtype, 'float32')\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   @parameterized.named_parameters(\n       {\n           'testcase_name': 'base',\n@@ -598,7 +601,7 @@ class KerasModelTest(keras_parameterized.TestCase):\n     self.assertAllClose(backend.get_value(model(x)), x * 100.)\n     self.assertEqual(model.get_weights(), [np.array(100.)])\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   @parameterized.named_parameters(\n       {\n           'testcase_name': 'base',\n@@ -635,7 +638,7 @@ class KerasModelTest(keras_parameterized.TestCase):\n       model.compile(\n           optimizer=opt,\n           loss='mse',\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n \n     model.fit(np.ones((2, 2)), np.zeros((2, 2)), batch_size=2)\n     weights_file = os.path.join(self.get_temp_dir(), 'weights')\n@@ -650,7 +653,7 @@ class KerasModelTest(keras_parameterized.TestCase):\n     restored_slot = backend.get_value(opt.get_slot(layer.v, 'momentum'))\n     self.assertEqual(restored_slot, saved_slot)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   @parameterized.named_parameters(*TESTCASES)\n   def test_save_weights_with_dynamic_loss_scaling(self, strategy_fn):\n     strategy = strategy_fn()\n@@ -671,7 +674,7 @@ class KerasModelTest(keras_parameterized.TestCase):\n       model.compile(\n           optimizer=opt,\n           loss='mse',\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n     # Run for 3 steps (6 examples with a batch size of 2)\n     model.fit(np.zeros((6, 2)), np.zeros((6, 2)), batch_size=2)\n     self.assertEqual(backend.get_value(opt.loss_scale), 2)\n@@ -691,7 +694,7 @@ class KerasModelTest(keras_parameterized.TestCase):\n     self.assertEqual(backend.get_value(opt.loss_scale), 2)\n     self.assertEqual(backend.get_value(opt.dynamic_counter), 1)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_restore_old_loss_scale_checkpoint(self):\n     # Ensure a checkpoint from TF 2.2 can be loaded. The checkpoint format\n     # of LossScaleOptimizer changed, but old checkpoints can still be loaded\n@@ -708,7 +711,7 @@ class KerasModelTest(keras_parameterized.TestCase):\n     # ckpt_dir = test.test_src_dir_path(\n     #     'python/keras/mixed_precision/testdata/lso_ckpt_tf2.2')\n     model.load_weights(os.path.join(ckpt_dir, 'ckpt'))\n-    model.compile(opt, 'mse', run_eagerly=testing_utils.should_run_eagerly())\n+    model.compile(opt, 'mse', run_eagerly=test_utils.should_run_eagerly())\n     model(np.zeros((2, 2)))  # Create model weights\n     opt._create_all_weights(model.weights)\n     expected_kernel = np.array([[9.229685, 10.901115], [10.370763, 9.757362]])\n@@ -749,7 +752,7 @@ class KerasModelTest(keras_parameterized.TestCase):\n     self.assertEqual(type(model.optimizer),\n                      loss_scale_optimizer.LossScaleOptimizer)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   @parameterized.named_parameters(\n       {\n           'testcase_name': 'base',\n@@ -797,7 +800,7 @@ class KerasModelTest(keras_parameterized.TestCase):\n       model.compile(\n           optimizer=opt,\n           loss='mse',\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n     # Run for 3 steps (6 examples with a batch size of 2)\n     model.fit(np.ones((6, 2)), np.zeros((6, 2)), batch_size=2)\n     self.assertEqual(backend.get_value(opt.loss_scale), 2)\n@@ -836,7 +839,7 @@ class KerasModelTest(keras_parameterized.TestCase):\n                      loss_scale_optimizer.LossScaleOptimizer)\n \n \n-class ApplicationModelTest(keras_parameterized.TestCase):\n+class ApplicationModelTest(test_combinations.TestCase):\n   \"\"\"Tests that application models can be built with mixed precision.\n \n   This does not test that such models can be trained in mixed precision, as\n\n@@ -17,8 +17,8 @@\n import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n-from keras import combinations\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.engine import base_layer_utils\n from keras.mixed_precision import device_compatibility_check\n from keras.mixed_precision import policy as mp_policy\n@@ -26,11 +26,11 @@ from keras.optimizers.optimizer_v2 import gradient_descent\n from tensorflow.python.platform import tf_logging\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class PolicyTest(tf.test.TestCase, parameterized.TestCase):\n   \"\"\"Tests Policies.\"\"\"\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_dtype_attributes(self):\n     for dtype in 'int32', 'bool', 'float16', 'float32':\n       policy = mp_policy.Policy(dtype)\n@@ -48,7 +48,7 @@ class PolicyTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(policy.compute_dtype, None)\n     self.assertEqual(policy.variable_dtype, None)\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_repr(self):\n     # Test Policy repr\n     for policy in ('float32', 'int8', 'mixed_float16', 'mixed_bfloat16',\n@@ -67,7 +67,7 @@ class PolicyTest(tf.test.TestCase, parameterized.TestCase):\n         repr(mp_policy.PolicyV1('mixed_float16')),\n         '<PolicyV1 \"mixed_float16\", loss_scale=DynamicLossScale(')\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_policy_errors(self):\n     # Test passing invalid strings\n \n@@ -109,7 +109,7 @@ class PolicyTest(tf.test.TestCase, parameterized.TestCase):\n         '\\'int8_with_float32_vars\\''):\n       mp_policy.Policy('int8_with_float32_vars')\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_loss_scale(self):\n     policy = mp_policy.PolicyV1('float32')\n     self.assertEqual(policy.loss_scale, None)\n@@ -133,7 +133,7 @@ class PolicyTest(tf.test.TestCase, parameterized.TestCase):\n     policy = mp_policy.PolicyV1('mixed_bfloat16')\n     self.assertEqual(policy.loss_scale, None)\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_global_policy(self):\n     if base_layer_utils.v2_dtype_behavior_enabled():\n       default_policy = 'float32'\n@@ -153,7 +153,7 @@ class PolicyTest(tf.test.TestCase, parameterized.TestCase):\n     finally:\n       mp_policy.set_global_policy(None)\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_global_policy_dtype_error(self):\n     with self.assertRaisesRegex(\n         ValueError,\n@@ -168,7 +168,7 @@ class PolicyTest(tf.test.TestCase, parameterized.TestCase):\n         'got policy: complex64'):\n       mp_policy.set_global_policy(mp_policy.Policy('complex64'))\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_loss_scale_warning(self):\n     with tf.compat.v1.test.mock.patch.object(tf_logging, 'warning') as mock_warn:\n       mp_policy.PolicyV1('float32', loss_scale=2.)\n@@ -185,7 +185,7 @@ class PolicyTest(tf.test.TestCase, parameterized.TestCase):\n         mp_policy.PolicyV1(policy_name, loss_scale=2.)\n         mock_warn.assert_not_called()\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_device_compatibility_warning(self):\n     if not tf.executing_eagerly():\n       self.skipTest('Run in eager mode only.')\n@@ -206,7 +206,7 @@ class PolicyTest(tf.test.TestCase, parameterized.TestCase):\n         mp_policy.Policy('mixed_float16')\n       mock_warn.assert_not_called()\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_policy_scope(self):\n     if base_layer_utils.v2_dtype_behavior_enabled():\n       default_policy = 'float32'\n@@ -219,7 +219,7 @@ class PolicyTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertEqual(mp_policy.global_policy().name, 'mixed_float16')\n     self.assertEqual(mp_policy.global_policy().name, default_policy)\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_config(self):\n     for policy in (\n         mp_policy.Policy('float16'),\n@@ -235,7 +235,7 @@ class PolicyTest(tf.test.TestCase, parameterized.TestCase):\n       # same, as policy does not override the == operator.\n       self.assertEqual(str(policy), str(new_policy))\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_serialization(self):\n     # Test policies that are equivalent to a single dtype\n     for policy_name in 'float16', 'float32', 'int8', 'string', 'bool':\n@@ -293,7 +293,7 @@ class PolicyTest(tf.test.TestCase, parameterized.TestCase):\n               }\n           })\n \n-  @testing_utils.enable_v2_dtype_behavior\n+  @test_utils.enable_v2_dtype_behavior\n   def test_error_if_graph_rewrite_enabled(self):\n     try:\n       tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite(\n@@ -307,7 +307,7 @@ class PolicyTest(tf.test.TestCase, parameterized.TestCase):\n     finally:\n       tf.compat.v1.mixed_precision.disable_mixed_precision_graph_rewrite()\n \n-  @testing_utils.disable_v2_dtype_behavior\n+  @test_utils.disable_v2_dtype_behavior\n   def test_v1_dtype_behavior(self):\n     # Setting global policies are not allowed with V1 dtype behavior\n     with self.assertRaisesRegex(\n\n@@ -24,11 +24,11 @@ import numpy as np\n \n import keras\n from keras import backend\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras import metrics\n from keras import models\n from keras.optimizers import optimizer_v1\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n \n \n class TestModel(keras.Model):\n@@ -63,13 +63,13 @@ def _get_layers(input_shape=(4,), add_input_layer=False):\n \n def _get_model(input_shape=(4,)):\n   model_layers = _get_layers(input_shape=None, add_input_layer=False)\n-  return testing_utils.get_model_from_layers(\n+  return test_utils.get_model_from_layers(\n       model_layers, input_shape=input_shape)\n \n \n-class TestModelCloning(keras_parameterized.TestCase):\n+class TestModelCloning(test_combinations.TestCase):\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   @parameterized.named_parameters([\n       {'testcase_name': 'has_input_layer',\n        'input_shape': (4,),\n@@ -146,7 +146,7 @@ class TestModelCloning(keras_parameterized.TestCase):\n           keras.layers.InputLayer)\n       self.assertTrue(new_model._is_graph_network)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   @parameterized.named_parameters([\n       {'testcase_name': 'clone_weights', 'share_weights': False},\n       {'testcase_name': 'share_weights', 'share_weights': True},\n@@ -180,9 +180,9 @@ class TestModelCloning(keras_parameterized.TestCase):\n     if not tf.compat.v1.executing_eagerly_outside_functions():\n       self.assertGreaterEqual(len(new_model.updates), 2)\n     new_model.compile(\n-        testing_utils.get_v2_optimizer('rmsprop'),\n+        test_utils.get_v2_optimizer('rmsprop'),\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     new_model.train_on_batch([val_a, val_b], val_out)\n \n     # On top of new tensors\n@@ -193,9 +193,9 @@ class TestModelCloning(keras_parameterized.TestCase):\n     if not tf.compat.v1.executing_eagerly_outside_functions():\n       self.assertLen(new_model.updates, 2)\n     new_model.compile(\n-        testing_utils.get_v2_optimizer('rmsprop'),\n+        test_utils.get_v2_optimizer('rmsprop'),\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     new_model.train_on_batch([val_a, val_b], val_out)\n \n     # New model should use provided input tensors\n@@ -210,12 +210,12 @@ class TestModelCloning(keras_parameterized.TestCase):\n       new_model = clone_fn(model, input_tensors=[input_a, input_b])\n       self.assertGreaterEqual(len(new_model.updates), 2)\n       new_model.compile(\n-          testing_utils.get_v2_optimizer('rmsprop'),\n+          test_utils.get_v2_optimizer('rmsprop'),\n           'mse',\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n       new_model.train_on_batch(None, val_out)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   @parameterized.named_parameters([\n       {'testcase_name': 'clone_weights', 'share_weights': False},\n       {'testcase_name': 'share_weights', 'share_weights': True},\n@@ -237,8 +237,8 @@ class TestModelCloning(keras_parameterized.TestCase):\n     model = clone_fn(model)\n     model.compile(\n         loss='mse',\n-        optimizer=testing_utils.get_v2_optimizer('adam'),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        optimizer=test_utils.get_v2_optimizer('adam'),\n+        run_eagerly=test_utils.should_run_eagerly())\n     y = np.array([[[1], [1]], [[1], [1]]])\n     loss = model.train_on_batch(x, y)\n     self.assertEqual(float(loss), 0.)\n@@ -308,7 +308,7 @@ class TestModelCloning(keras_parameterized.TestCase):\n       has_placeholder = _has_placeholder(graph)\n       self.assertFalse(has_placeholder)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   @parameterized.named_parameters([\n       {'testcase_name': 'clone_weights', 'share_weights': False},\n       {'testcase_name': 'share_weights', 'share_weights': True},\n@@ -358,17 +358,17 @@ def _has_placeholder(graph):\n   return any('Placeholder' in s for s in ops_types)\n \n \n-class CheckpointingTests(keras_parameterized.TestCase):\n+class CheckpointingTests(test_combinations.TestCase):\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_optimizer_dependency(self):\n     model = _get_model()\n     opt = tf.compat.v1.train.AdamOptimizer(.01)\n     model.compile(\n         optimizer=opt,\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     model.fit(\n         x=np.array([[1., 2., 3., 4.]]),\n@@ -383,8 +383,8 @@ class CheckpointingTests(keras_parameterized.TestCase):\n     self.assertEqual(12., self.evaluate(beta1_power))\n \n \n-@keras_parameterized.run_all_keras_modes\n-class TestModelBackend(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class TestModelBackend(test_combinations.TestCase):\n \n   def test_model_backend_float64_use_cases(self):\n     # Test case for GitHub issue 19318\n@@ -395,17 +395,17 @@ class TestModelBackend(keras_parameterized.TestCase):\n     y = keras.layers.Dense(1)(x)\n     model = keras.models.Model(x, y)\n     model.compile(\n-        testing_utils.get_v2_optimizer('rmsprop'),\n+        test_utils.get_v2_optimizer('rmsprop'),\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     keras.backend.set_floatx(floatx)\n \n \n-class TestCloneAndBuildModel(keras_parameterized.TestCase):\n+class TestCloneAndBuildModel(test_combinations.TestCase):\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_clone_and_build_non_compiled_model(self):\n     inp = np.random.random((10, 4))\n     out = np.random.random((10, 4))\n@@ -415,7 +415,7 @@ class TestCloneAndBuildModel(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(ValueError, 'has not been compiled'):\n       models.clone_and_build_model(model, compile_clone=True)\n \n-    is_subclassed = (testing_utils.get_model_type() == 'subclass')\n+    is_subclassed = (test_utils.get_model_type() == 'subclass')\n     # With placeholder creation\n     new_model = models.clone_and_build_model(\n         model, compile_clone=False, in_place_reset=is_subclassed)\n@@ -424,9 +424,9 @@ class TestCloneAndBuildModel(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(RuntimeError, 'must compile'):\n       new_model.train_on_batch(inp, out)\n     new_model.compile(\n-        testing_utils.get_v2_optimizer('rmsprop'),\n+        test_utils.get_v2_optimizer('rmsprop'),\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     new_model.train_on_batch(inp, out)\n \n     # Create new tensors for inputs.\n@@ -441,9 +441,9 @@ class TestCloneAndBuildModel(keras_parameterized.TestCase):\n     with self.assertRaisesRegex(RuntimeError, 'must compile'):\n       new_model.train_on_batch(inp, out)\n     new_model.compile(\n-        testing_utils.get_v2_optimizer('rmsprop'),\n+        test_utils.get_v2_optimizer('rmsprop'),\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     new_model.train_on_batch(inp, out)\n \n   def _assert_same_compile_params(self, model):\n@@ -487,26 +487,26 @@ class TestCloneAndBuildModel(keras_parameterized.TestCase):\n     new_model.train_on_batch(inp, out)\n     new_model.evaluate(inp, out)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_clone_and_build_compiled(self):\n     model = _get_model()\n     model.compile(\n-        testing_utils.get_v2_optimizer('rmsprop'),\n+        test_utils.get_v2_optimizer('rmsprop'),\n         'mse',\n         metrics=['acc', metrics.categorical_accuracy],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n-    self._clone_and_build_test_helper(model, testing_utils.get_model_type())\n+    self._clone_and_build_test_helper(model, test_utils.get_model_type())\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_clone_and_build_sequential_without_inputs_defined(self):\n     model = models.Sequential(_get_layers(input_shape=None))\n     model.compile(\n-        testing_utils.get_v2_optimizer('rmsprop'),\n+        test_utils.get_v2_optimizer('rmsprop'),\n         'mse',\n         metrics=['acc', metrics.categorical_accuracy],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     self._clone_and_build_test_helper(model, 'sequential')\n \n     inp = np.random.random((10, 4))\n@@ -520,12 +520,12 @@ class TestCloneAndBuildModel(keras_parameterized.TestCase):\n         optimizer,\n         'mse',\n         metrics=['acc', metrics.categorical_accuracy],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     global_step = keras.backend.variable(123, dtype=tf.int64)\n     clone_model = models.clone_and_build_model(\n         model, compile_clone=True, optimizer_iterations=global_step,\n-        in_place_reset=(testing_utils.get_model_type() == 'subclass'))\n+        in_place_reset=(test_utils.get_model_type() == 'subclass'))\n \n     inp = np.random.random((10, 4))\n     out = np.random.random((10, 4))\n@@ -533,22 +533,22 @@ class TestCloneAndBuildModel(keras_parameterized.TestCase):\n \n     self.assertEqual(backend.eval(global_step), 124)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_replace_tf_optimizer_iterations_variable(self):\n     if tf.executing_eagerly():\n       self.skipTest('v1 optimizers not supported with eager.')\n     self.assert_optimizer_iterations_increases(tf.compat.v1.train.AdamOptimizer(0.01))\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_replace_keras_optimizer_iterations_variable(self):\n     self.assert_optimizer_iterations_increases('adam')\n \n   def test_clone_optimizer_in_different_graph(self):\n     with tf.Graph().as_default():\n       with self.session():\n-        model = testing_utils.get_small_sequential_mlp(3, 4)\n+        model = test_utils.get_small_sequential_mlp(3, 4)\n         optimizer = keras.optimizers.optimizer_v2.adam.Adam()\n         model.compile(\n             optimizer, 'mse', metrics=['acc', metrics.categorical_accuracy],\n\n@@ -18,9 +18,9 @@ import math\n \n from absl.testing import parameterized\n \n-from keras import combinations\n from keras.optimizers import learning_rate_schedule\n from keras.optimizers.optimizer_v2 import gradient_descent\n+from keras.testing_infra import test_combinations\n import numpy as np\n \n import tensorflow.compat.v2 as tf\n@@ -34,7 +34,7 @@ def _maybe_serialized(lr_decay, serialize_and_deserialize):\n     return lr_decay\n \n \n-@combinations.generate(combinations.combine(serialize=[False, True],\n+@test_combinations.generate(test_combinations.combine(serialize=[False, True],\n                                                       mode=[\"graph\", \"eager\"]))\n class LRDecayTestV2(tf.test.TestCase, parameterized.TestCase):\n \n@@ -155,7 +155,7 @@ class LRDecayTestV2(tf.test.TestCase, parameterized.TestCase):\n # @parameterized.named_parameters(\n #     (\"NotSerialized\", False),\n #     (\"Serialized\", True))\n-@combinations.generate(combinations.combine(serialize=[False, True],\n+@test_combinations.generate(test_combinations.combine(serialize=[False, True],\n                                                       mode=[\"graph\", \"eager\"]))\n class LinearDecayTestV2(tf.test.TestCase, parameterized.TestCase):\n \n@@ -209,7 +209,7 @@ class LinearDecayTestV2(tf.test.TestCase, parameterized.TestCase):\n # @parameterized.named_parameters(\n #     (\"NotSerialized\", False),\n #     (\"Serialized\", True))\n-@combinations.generate(combinations.combine(serialize=[False, True],\n+@test_combinations.generate(test_combinations.combine(serialize=[False, True],\n                                                       mode=[\"graph\", \"eager\"]))\n class SqrtDecayTestV2(tf.test.TestCase,\n                       parameterized.TestCase):\n@@ -273,7 +273,7 @@ class SqrtDecayTestV2(tf.test.TestCase,\n # @parameterized.named_parameters(\n #     (\"NotSerialized\", False),\n #     (\"Serialized\", True))\n-@combinations.generate(combinations.combine(serialize=[False, True],\n+@test_combinations.generate(test_combinations.combine(serialize=[False, True],\n                                                       mode=[\"graph\", \"eager\"]))\n class PolynomialDecayTestV2(tf.test.TestCase,\n                             parameterized.TestCase):\n@@ -292,7 +292,7 @@ class PolynomialDecayTestV2(tf.test.TestCase,\n # @parameterized.named_parameters(\n #     (\"NotSerialized\", False),\n #     (\"Serialized\", True))\n-@combinations.generate(combinations.combine(serialize=[False, True],\n+@test_combinations.generate(test_combinations.combine(serialize=[False, True],\n                                                       mode=[\"graph\", \"eager\"]))\n class InverseDecayTestV2(tf.test.TestCase, parameterized.TestCase):\n \n@@ -327,7 +327,7 @@ class InverseDecayTestV2(tf.test.TestCase, parameterized.TestCase):\n       self.evaluate(step.assign_add(1))\n \n \n-@combinations.generate(combinations.combine(serialize=[False, True],\n+@test_combinations.generate(test_combinations.combine(serialize=[False, True],\n                                                       mode=[\"graph\", \"eager\"]))\n class CosineDecayTestV2(tf.test.TestCase, parameterized.TestCase):\n \n@@ -370,7 +370,7 @@ class CosineDecayTestV2(tf.test.TestCase, parameterized.TestCase):\n       self.assertAllClose(self.evaluate(decayed_lr(step)), expected, 1e-6)\n \n \n-@combinations.generate(combinations.combine(serialize=[False, True],\n+@test_combinations.generate(test_combinations.combine(serialize=[False, True],\n                                                       mode=[\"graph\", \"eager\"]))\n class CosineDecayRestartsTestV2(tf.test.TestCase,\n                                 parameterized.TestCase):\n\n@@ -17,12 +17,11 @@\n import tensorflow.compat.v2 as tf\n \n import math\n-from keras import combinations\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n \n \n-@combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n-class LRDecayTest(keras_parameterized.TestCase):\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class LRDecayTest(test_combinations.TestCase):\n \n   def testContinuous(self):\n     self.evaluate(tf.compat.v1.global_variables_initializer())\n@@ -135,8 +134,8 @@ class LRDecayTest(keras_parameterized.TestCase):\n     self.assertAllClose(self.evaluate(decayed_lr), 0.7, 1e-6)\n \n \n-@combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n-class LinearDecayTest(keras_parameterized.TestCase):\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class LinearDecayTest(test_combinations.TestCase):\n \n   def testHalfWay(self):\n     step = 5\n@@ -180,8 +179,8 @@ class LinearDecayTest(keras_parameterized.TestCase):\n     self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)\n \n \n-@combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n-class SqrtDecayTest(keras_parameterized.TestCase):\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class SqrtDecayTest(test_combinations.TestCase):\n \n   def testHalfWay(self):\n     step = 5\n@@ -234,8 +233,8 @@ class SqrtDecayTest(keras_parameterized.TestCase):\n     self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)\n \n \n-@combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n-class PolynomialDecayTest(keras_parameterized.TestCase):\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class PolynomialDecayTest(test_combinations.TestCase):\n \n   def testBeginWithCycle(self):\n     lr = 0.001\n@@ -247,8 +246,8 @@ class PolynomialDecayTest(keras_parameterized.TestCase):\n     self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)\n \n \n-@combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n-class ExponentialDecayTest(keras_parameterized.TestCase):\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class ExponentialDecayTest(test_combinations.TestCase):\n \n   def testDecay(self):\n     initial_lr = 0.1\n@@ -279,8 +278,8 @@ class ExponentialDecayTest(keras_parameterized.TestCase):\n       self.evaluate(step.assign_add(1))\n \n \n-@combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n-class InverseDecayTest(keras_parameterized.TestCase):\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class InverseDecayTest(test_combinations.TestCase):\n \n   def testDecay(self):\n     initial_lr = 0.1\n@@ -311,8 +310,8 @@ class InverseDecayTest(keras_parameterized.TestCase):\n       self.evaluate(step.assign_add(1))\n \n \n-@combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n-class CosineDecayTest(keras_parameterized.TestCase):\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class CosineDecayTest(test_combinations.TestCase):\n \n   def np_cosine_decay(self, step, decay_steps, alpha=0.0):\n     step = min(step, decay_steps)\n@@ -340,8 +339,8 @@ class CosineDecayTest(keras_parameterized.TestCase):\n       self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)\n \n \n-@combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n-class CosineDecayRestartsTest(keras_parameterized.TestCase):\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class CosineDecayRestartsTest(test_combinations.TestCase):\n \n   def np_cosine_decay_restarts(self, step, decay_steps, t_mul=2.0, m_mul=1.0,\n                                alpha=0.0):\n@@ -398,8 +397,8 @@ class CosineDecayRestartsTest(keras_parameterized.TestCase):\n       self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)\n \n \n-@combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n-class LinearCosineDecayTest(keras_parameterized.TestCase):\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class LinearCosineDecayTest(test_combinations.TestCase):\n \n   def np_linear_cosine_decay(self,\n                              step,\n@@ -438,8 +437,8 @@ class LinearCosineDecayTest(keras_parameterized.TestCase):\n       self.assertAllClose(self.evaluate(decayed_lr), expected, 1e-6)\n \n \n-@combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n-class NoisyLinearCosineDecayTest(keras_parameterized.TestCase):\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class NoisyLinearCosineDecayTest(test_combinations.TestCase):\n \n   def testDefaultNoisyLinearCosine(self):\n     num_training_steps = 1000\n\n@@ -18,7 +18,7 @@ import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n import numpy as np\n-from keras import combinations\n+from keras.testing_infra import test_combinations\n from keras.optimizers.optimizer_v2 import adadelta\n \n _DATA_TYPES = [\n@@ -132,11 +132,12 @@ class AdadeltaOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                   self.evaluate(var1),\n                   rtol=1e-5)\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testResourceBasic(self):\n     self.doTestBasic(use_resource=True)\n \n-  @combinations.generate(combinations.combine(mode=[\"eager\"]))\n+  @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n   def testBasicCallableParams(self):\n     self.doTestBasic(use_resource=True, use_callable_params=True)\n \n\n@@ -20,7 +20,7 @@ import copy\n \n from absl.testing import parameterized\n import numpy as np\n-from keras import combinations\n+from keras.testing_infra import test_combinations\n from keras.optimizers.optimizer_v2 import adagrad\n from keras.optimizers import learning_rate_schedule\n \n@@ -102,11 +102,12 @@ class AdagradOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertAllCloseAccordingToType(var0_np, self.evaluate(var0))\n         self.assertAllCloseAccordingToType(var1_np, self.evaluate(var1))\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testBasic(self):\n     self.doTestBasic()\n \n-  @combinations.generate(combinations.combine(mode=[\"eager\"]))\n+  @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n   def testBasicCallableParams(self):\n     self.doTestBasic(use_callable_params=True)\n \n\n@@ -18,7 +18,7 @@ import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n import numpy as np\n-from keras import combinations\n+from keras.testing_infra import test_combinations\n from keras.optimizers import optimizer_v1\n from keras.optimizers.optimizer_v2 import adam\n from keras.optimizers import learning_rate_schedule\n@@ -239,15 +239,17 @@ class AdamOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n           self.assertAllCloseAccordingToType(var0_np, self.evaluate(var0))\n           self.assertAllCloseAccordingToType(var1_np, self.evaluate(var1))\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testResourceBasic(self):\n     self.doTestBasic()\n \n-  @combinations.generate(combinations.combine(mode=[\"eager\"]))\n+  @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n   def testBasicCallableParams(self):\n     self.doTestBasic(use_callable_params=True)\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testBasicWithAmsgrad(self):\n     for i, dtype in enumerate([tf.half, tf.float32, tf.float64]):\n       with self.cached_session():\n@@ -289,7 +291,8 @@ class AdamOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n           self.assertAllCloseAccordingToType(var0_np, self.evaluate(var0))\n           self.assertAllCloseAccordingToType(var1_np, self.evaluate(var1))\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testSparseWithAmsgrad(self):\n     # dtypes.half does not work on gpu + eager.\n     for dtype in [tf.float32, tf.float64]:\n@@ -514,7 +517,7 @@ class AdamOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n           self.assertAllCloseAccordingToType(var0_np, self.evaluate(var0))\n           self.assertAllCloseAccordingToType(var1_np, self.evaluate(var1))\n \n-  @combinations.generate(combinations.combine(mode=[\"eager\"]))\n+  @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n   def testSlotsUniqueEager(self):\n     v1 = tf.Variable(1.)\n     v2 = tf.Variable(1.)\n@@ -693,15 +696,17 @@ class NonFusedAdamOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n           self.assertAllCloseAccordingToType(\n               var1_np, self.evaluate(var1), rtol=1e-4, atol=1e-4)\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testResourceBasic(self):\n     self.doTestBasic()\n \n-  @combinations.generate(combinations.combine(mode=[\"eager\"]))\n+  @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n   def testBasicCallableParams(self):\n     self.doTestBasic(use_callable_params=True)\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testBasicWithAmsgrad(self):\n     for i, dtype in enumerate([tf.half, tf.float32, tf.float64]):\n       with self.cached_session():\n@@ -745,7 +750,8 @@ class NonFusedAdamOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n           self.assertAllCloseAccordingToType(\n               var1_np, self.evaluate(var1), rtol=1e-4, atol=1e-4)\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testSparseWithAmsgrad(self):\n     # dtypes.half does not work on gpu + eager.\n     for dtype in [tf.float32, tf.float64]:\n\n@@ -18,7 +18,7 @@ import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n import numpy as np\n-from keras import combinations\n+from keras.testing_infra import test_combinations\n from keras.optimizers.optimizer_v2 import adamax\n \n \n@@ -160,7 +160,8 @@ class AdamaxOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n           self.assertAllClose(aggregated_update_var,\n                               repeated_index_update_var.eval())\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testBasic(self):\n     for i, dtype in enumerate([tf.half, tf.float32, tf.float64]):\n       with self.session(graph=tf.Graph(), use_gpu=True):\n@@ -209,7 +210,8 @@ class AdamaxOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n           self.assertAllCloseAccordingToType(\n               var1_np, self.evaluate(var1), rtol=1e-2)\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testBasicWithLearningRateDecay(self):\n     for i, dtype in enumerate([tf.half, tf.float32, tf.float64]):\n       with self.session(graph=tf.Graph(), use_gpu=True):\n@@ -339,7 +341,7 @@ class AdamaxOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n           self.assertAllCloseAccordingToType(var0_np, var0)\n           self.assertAllCloseAccordingToType(var1_np, var1)\n \n-  @combinations.generate(combinations.combine(mode=[\"eager\"]))\n+  @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n   def testSlotsUniqueEager(self):\n     v1 = tf.Variable(1.)\n     v2 = tf.Variable(1.)\n\n@@ -18,14 +18,15 @@ import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n import numpy as np\n-from keras import combinations\n+from keras.testing_infra import test_combinations\n from keras.optimizers.optimizer_v2 import gradient_descent\n from keras.optimizers import learning_rate_schedule\n \n \n class GradientDescentOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testBasic(self):\n     for dtype in [tf.half, tf.float32, tf.float64]:\n       var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n@@ -74,7 +75,8 @@ class GradientDescentOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n         [3.0 - 3.0 * 0.01 - 2.0 * 0.01, 4.0 - 3.0 * 0.01 - 2.0 * 0.01],\n         self.evaluate(var1))\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testBasicWithLearningRateDecay(self):\n     for dtype in [tf.half, tf.float32, tf.float64]:\n       learning_rate = 3.0\n@@ -82,7 +84,8 @@ class GradientDescentOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       sgd = gradient_descent.SGD(learning_rate=learning_rate, decay=decay)\n       self._test_basic_sgd_with_learning_rate_decay(sgd, dtype)\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testBasicWithLearningRateInverseTimeDecay(self):\n     for dtype in [tf.half, tf.float32, tf.float64]:\n       learning_rate = learning_rate_schedule.InverseTimeDecay(\n@@ -90,7 +93,8 @@ class GradientDescentOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       sgd = gradient_descent.SGD(learning_rate=learning_rate)\n       self._test_basic_sgd_with_learning_rate_decay(sgd, dtype)\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testBasicWithLearningRateInverseTimeDecaySerializeAndDeserialize(self):\n     for dtype in [tf.half, tf.float32, tf.float64]:\n       learning_rate = learning_rate_schedule.InverseTimeDecay(\n@@ -99,7 +103,8 @@ class GradientDescentOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       sgd = gradient_descent.SGD.from_config(sgd.get_config())\n       self._test_basic_sgd_with_learning_rate_decay(sgd, dtype)\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testBasicCallableParams(self):\n     for dtype in [tf.half, tf.float32, tf.float64]:\n       var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n@@ -118,7 +123,8 @@ class GradientDescentOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertAllCloseAccordingToType([3.0 - 3.0 * 0.01, 4.0 - 3.0 * 0.01],\n                                          self.evaluate(var1))\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testMinimizeResourceVariable(self):\n     for dtype in [tf.half, tf.float32, tf.float64]:\n       var0 = tf.Variable([[1.0, 2.0]], dtype=dtype)\n@@ -244,7 +250,7 @@ class GradientDescentOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertAllCloseAccordingToType(\n             [[3.0], [4.0 - 3.0 * 0.01 - 2.0 * 0.01]], self.evaluate(var1))\n \n-  @combinations.generate(combinations.combine(mode=[\"eager\"]))\n+  @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n   def testCapturingInFunctionWhileExecutingEagerly(self):\n     optimizer = gradient_descent.SGD(1.0)\n \n@@ -290,7 +296,8 @@ class MomentumOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     var += (accum * momentum - g * lr)\n     return var, accum\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testBasic(self):\n     for _, dtype in enumerate([tf.half, tf.float32, tf.float64]):\n       var0 = tf.Variable([1.0, 2.0], dtype=dtype, name=\"var0\")\n@@ -435,7 +442,8 @@ class MomentumOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n         # Validate updated params\n         self.assertAllCloseAccordingToType([[-111, -138]], self.evaluate(var0))\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testMinimizeWith2DIndicesForEmbeddingLookup(self):\n     var0 = tf.Variable(tf.ones([2, 2]))\n \n@@ -643,7 +651,8 @@ class MomentumOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                 3.98 - ((0.9 * 0.01 + 0.01) * 2.0)\n             ]), self.evaluate(var1))\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testConfig(self):\n     opt = gradient_descent.SGD(learning_rate=1.0, momentum=0.9, nesterov=True)\n     config = opt.get_config()\n@@ -693,7 +702,7 @@ class MomentumOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllClose(self.evaluate(opt_2.lr), (1.0))\n     self.assertAllClose(self.evaluate(opt_3.lr), (0.1))\n \n-  @combinations.generate(combinations.combine(mode=[\"eager\"]))\n+  @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n   def testMinimizeLossTensor(self):\n     for dtype in [tf.half, tf.float32, tf.float64]:\n       var0 = tf.Variable([[1.0, 2.0]], dtype=dtype)\n\n@@ -22,14 +22,11 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from tensorflow.python.framework import test_util\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n from keras import backend\n from keras import callbacks\n-from keras import combinations\n-from keras import keras_parameterized\n from keras import losses\n from keras.optimizers import optimizer_v1\n-from keras import testing_utils\n from keras.engine import input_layer\n from keras.engine import sequential\n from keras.engine import training\n@@ -45,21 +42,24 @@ from keras.optimizers import learning_rate_schedule\n from keras.optimizers.optimizer_v2 import nadam\n from keras.optimizers.optimizer_v2 import optimizer_v2\n from keras.optimizers.optimizer_v2 import rmsprop\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.utils import np_utils\n \n \n _DATA_TYPES = [tf.half, tf.float32, tf.float64]\n # TODO(b/141710709): complex support in NVCC and ROCM.\n-if (not test_util.IsBuiltWithNvcc() and not tf.test.is_built_with_rocm()):\n+if (not tf_test_utils.IsBuiltWithNvcc() and not tf.test.is_built_with_rocm()):\n   _DATA_TYPES += [tf.complex64, tf.complex128]\n \n \n class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testBasic(self):\n     for dtype in _DATA_TYPES:\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n         var1 = tf.Variable([3.0, 4.0], dtype=dtype)\n         loss = lambda: 5 * var0 + 3 * var1  # pylint: disable=cell-var-from-loop\n@@ -77,7 +77,8 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertAllClose([-14., -13.], self.evaluate(var0))\n         self.assertAllClose([-6., -5.], self.evaluate(var1))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testAdaptiveLearningRate(self):\n     for dtype in _DATA_TYPES:\n       with self.test_session():\n@@ -121,10 +122,11 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n         else:\n           self.evaluate(opt_op)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testPrecomputedGradient(self):\n     for dtype in _DATA_TYPES:\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n         var1 = tf.Variable([3.0, 4.0], dtype=dtype)\n         loss = lambda: 5 * var0 + 3 * var1  # pylint: disable=cell-var-from-loop\n@@ -145,10 +147,11 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertAllClose([3.0 - 3 * 3 * 42.0, 4.0 - 3 * 3 * (-42.0)],\n                             self.evaluate(var1))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testNoGradients(self):\n     for dtype in _DATA_TYPES:\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n         var1 = tf.Variable([3.0, 4.0], dtype=dtype)\n         loss = lambda: 5 * var0  # pylint: disable=cell-var-from-loop\n@@ -157,10 +160,11 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n           # var1 has no gradient\n           sgd_op.minimize(loss, var_list=[var1])\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testNoGradientsForAnyVariables_Minimize(self):\n     for dtype in _DATA_TYPES:\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n         var1 = tf.Variable([3.0, 4.0], dtype=dtype)\n         loss = lambda: tf.constant(5.0)\n@@ -170,10 +174,11 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                                     'No gradients provided for any variable'):\n           sgd_op.minimize(loss, var_list=[var0, var1])\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testNoGradientsForAnyVariables_ApplyGradients(self):\n     for dtype in _DATA_TYPES:\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n         var1 = tf.Variable([3.0, 4.0], dtype=dtype)\n         sgd_op = gradient_descent.SGD(3.0)\n@@ -181,10 +186,11 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n                                     'No gradients provided for any variable'):\n           sgd_op.apply_gradients([(None, var0), (None, var1)])\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testGradientsAsVariables(self):\n     for i, dtype in enumerate(_DATA_TYPES):\n-      with testing_utils.use_gpu():\n+      with test_utils.use_gpu():\n         var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n         var1 = tf.Variable([3.0, 4.0], dtype=dtype)\n         loss = lambda: 5 * var0 + 3 * var1  # pylint: disable=cell-var-from-loop\n@@ -220,9 +226,10 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n         self.assertAllClose([-14., -13.], self.evaluate(var0))\n         self.assertAllClose([-6., -5.], self.evaluate(var1))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testComputeGradientsWithTensors(self):\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       x = tf.convert_to_tensor(1.0)\n \n       def f():\n@@ -238,11 +245,12 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       with self.assertRaises(NotImplementedError):\n         sgd.apply_gradients(grads_and_vars)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testConstraint(self):\n     constraint_01 = lambda x: tf.clip_by_value(x, -0.1, 0.)\n     constraint_0 = lambda x: tf.clip_by_value(x, 0., 1.)\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       var0 = tf.Variable([1.0, 2.0],\n                                 constraint=constraint_01)\n       var1 = tf.Variable([3.0, 4.0],\n@@ -262,16 +270,18 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertAllClose([-0.1, -0.1], self.evaluate(var0))\n       self.assertAllClose([0., 0.], self.evaluate(var1))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testIterationWithoutMinimize(self):\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       sgd = gradient_descent.SGD(3.0)\n       self.evaluate(sgd.iterations.initializer)\n       self.assertEqual(0, self.evaluate(sgd.iterations))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testConfig(self):\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       opt = gradient_descent.SGD(learning_rate=1.0)\n       config = opt.get_config()\n       opt2 = gradient_descent.SGD.from_config(config)\n@@ -289,9 +299,10 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       self.evaluate(tf.compat.v1.global_variables_initializer())\n       self.assertEqual(self.evaluate(lr), self.evaluate(lr3))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testConfigWithLearningRateDecay(self):\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       var0 = tf.Variable([[1.0], [2.0]], dtype=tf.float32)\n       for decay_schedule in [\n           learning_rate_schedule.InverseTimeDecay(\n@@ -320,9 +331,10 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n             self.evaluate(opt._get_hyper('learning_rate')(step)),\n             opt3._get_hyper('learning_rate')(step))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testGradClipValue(self):\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       var = tf.Variable([1.0, 2.0])\n       loss = lambda: 3 * var\n       opt = gradient_descent.SGD(learning_rate=1.0, clipvalue=1.0)\n@@ -331,9 +343,10 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       self.evaluate(opt_op)\n       self.assertAllClose([0., 1.], self.evaluate(var))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testGradClipNorm(self):\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       var = tf.Variable([1.0])\n       loss = lambda: 3 * var\n       opt = gradient_descent.SGD(learning_rate=1.0, clipnorm=1.0)\n@@ -342,9 +355,10 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       self.evaluate(opt_op)\n       self.assertAllClose([0.], self.evaluate(var))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testGradGlobalClipNorm(self):\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       # l2 norm is 5.0\n       var1 = tf.Variable([1.0])\n       var2 = tf.Variable([2.0])\n@@ -358,13 +372,14 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       # grad2 = 4.0 * 2.0 / 5.0 = 1.6\n       self.assertAllClose([.4], self.evaluate(var2))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testInvalidClipNorm(self):\n     with self.assertRaisesRegex(ValueError, '>= 0'):\n       gradient_descent.SGD(learning_rate=1.0, clipnorm=-1.0)\n \n-  @combinations.generate(\n-      combinations.combine(\n+  @test_combinations.generate(\n+      test_combinations.combine(\n           mode=['graph', 'eager'],\n           clip_type=['clipnorm', 'global_clipnorm', 'clipvalue']))\n   def testConfigWithCliping(self, clip_type):\n@@ -373,14 +388,16 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     opt = gradient_descent.SGD.from_config(config)\n     self.assertEqual(getattr(opt, clip_type), 2.0)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testInvalidKwargs(self):\n     with self.assertRaisesRegex(TypeError, 'Unexpected keyword argument'):\n       gradient_descent.SGD(learning_rate=1.0, invalidkwargs=1.0)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testWeights(self):\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       opt1 = adam.Adam(learning_rate=1.0)\n       var1 = tf.Variable([1.0, 2.0], dtype=tf.float32)\n       loss1 = lambda: 3 * var1\n@@ -423,7 +440,8 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertAllClose(\n           self.evaluate([var3, var4]), self.evaluate([var5, var6]))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testGettingHyperParameters(self):\n     with self.test_session():\n       opt = adam.Adam(learning_rate=1.0)\n@@ -447,7 +465,8 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       with self.assertRaises(AttributeError):\n         opt.not_an_attr += 3\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testGettingHyperParametersWithLrInConstructor(self):\n     with self.test_session():\n       opt = gradient_descent.SGD(lr=3.0)\n@@ -471,7 +490,8 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       lr = self.evaluate(opt.lr)\n       self.assertEqual(4.0, lr)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testDir(self):\n     opt = gradient_descent.SGD(learning_rate=1.0, momentum=0.1)\n     dir_result = set(dir(opt))\n@@ -481,7 +501,8 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertIn('nesterov', dir_result)  # Attribute\n     self.assertIn('minimize', dir_result)  # Attribute\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testOptimizerWithKerasModel(self):\n     a = input_layer.Input(shape=(3,), name='input_a')\n     b = input_layer.Input(shape=(3,), name='input_b')\n@@ -507,7 +528,8 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n               epochs=1,\n               batch_size=5)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testOptimizerWithCallbacks(self):\n     np.random.seed(1331)\n     input_np = np.random.random((10, 3))\n@@ -569,12 +591,13 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     new_step_value = self.evaluate(global_step)\n     self.assertEqual(new_step_value, init_step_value + 1)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testOptimizerWithCallableVarList(self):\n     train_samples = 20\n     input_dim = 1\n     num_classes = 2\n-    (x, y), _ = testing_utils.get_test_data(\n+    (x, y), _ = test_utils.get_test_data(\n         train_samples=train_samples,\n         test_samples=10,\n         input_shape=(input_dim,),\n@@ -582,7 +605,7 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     y = np_utils.to_categorical(y)\n \n     num_hidden = 1\n-    model = testing_utils.get_small_sequential_mlp(\n+    model = test_utils.get_small_sequential_mlp(\n         num_hidden=num_hidden, num_classes=num_classes)\n     opt = adam.Adam()\n \n@@ -631,13 +654,15 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertLen(opt_vars, 5)\n       self.assertEqual('outter/Adam/var_2/m:0', opt_vars[3].name)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testEmptyVarList(self):\n     opt = gradient_descent.SGD(1.)\n     opt.minimize(lambda: tf.constant(1.), [])\n     opt.apply_gradients([])\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testAggregationTrue(self):\n     # Test that experimental_aggregate_gradients=True works without distributed\n     # strategy.\n@@ -652,7 +677,8 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     self.evaluate(opt_op)\n     self.assertAllClose([0.7, 1.7], self.evaluate(var))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def testAggregationFalse(self):\n     # Test that experimental_aggregate_gradients=False works without distributed\n     # strategy.\n@@ -667,7 +693,7 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     self.evaluate(opt_op)\n     self.assertAllClose([0.7, 1.7], self.evaluate(var))\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def testRestoringIterationsWithoutAnOptimizer(self):\n     opt = gradient_descent.SGD(3.0)\n     opt.iterations.assign(5)\n@@ -684,7 +710,7 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n \n     self.assertEqual(5, self.evaluate(iterations_var))\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def testSlotWithNonstandardShapeRestoresBasedOnCheckpoint(self):\n     # First create an optimizer and a slot variable with a non-standard shape.\n     x = tf.Variable([[1.0, 2.0], [3.0, 4.0]], dtype=tf.float32)\n@@ -704,7 +730,8 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(slot_shape,\n                      optimizer_2.get_slot(x, 'test_slot').shape.as_list())\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_gradient_aggregator(self):\n     def gradient_aggregator(grads_and_vars):\n       # Simulate an all-reduce where the other replica has zeros for gradients,\n@@ -722,7 +749,8 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     self.evaluate(opt_op)\n     self.assertEqual(self.evaluate(var), 1.0)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_override_aggregate_gradients(self):\n     class MyOptimizer(gradient_descent.SGD):\n \n@@ -743,19 +771,19 @@ class OptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(self.evaluate(var), 1.0)\n \n \n-@keras_parameterized.run_all_keras_modes\n-class OptimizersCompatibilityTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class OptimizersCompatibilityTest(test_combinations.TestCase):\n \n   def _testOptimizersCompatibility(self, opt_v1, opt_v2, test_weights=True):\n     if tf.executing_eagerly():\n       self.skipTest(\n           'v1 optimizer does not run in eager mode')\n     np.random.seed(1331)\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       train_samples = 20\n       input_dim = 3\n       num_classes = 2\n-      (x, y), _ = testing_utils.get_test_data(\n+      (x, y), _ = test_utils.get_test_data(\n           train_samples=train_samples,\n           test_samples=10,\n           input_shape=(input_dim,),\n@@ -763,23 +791,23 @@ class OptimizersCompatibilityTest(keras_parameterized.TestCase):\n       y = np_utils.to_categorical(y)\n \n       num_hidden = 5\n-      model_v1 = testing_utils.get_small_sequential_mlp(\n+      model_v1 = test_utils.get_small_sequential_mlp(\n           num_hidden=num_hidden, num_classes=num_classes, input_dim=input_dim)\n       model_v1.compile(\n           opt_v1,\n           loss='categorical_crossentropy',\n           metrics=[],\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n       model_v1.fit(x, y, batch_size=5, epochs=1)\n \n-      model_v2 = testing_utils.get_small_sequential_mlp(\n+      model_v2 = test_utils.get_small_sequential_mlp(\n           num_hidden=num_hidden, num_classes=num_classes, input_dim=input_dim)\n       model_v2.set_weights(model_v1.get_weights())\n       model_v2.compile(\n           opt_v2,\n           loss='categorical_crossentropy',\n           metrics=[],\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n       if not tf.compat.v1.executing_eagerly_outside_functions():\n         model_v2._make_train_function()\n       if test_weights:\n@@ -837,11 +865,11 @@ class OptimizersCompatibilityTest(keras_parameterized.TestCase):\n       self.skipTest(\n           'v1 optimizer does not run in eager mode')\n     np.random.seed(1331)\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       train_samples = 20\n       input_dim = 3\n       num_classes = 2\n-      (x, y), _ = testing_utils.get_test_data(\n+      (x, y), _ = test_utils.get_test_data(\n           train_samples=train_samples,\n           test_samples=10,\n           input_shape=(input_dim,),\n@@ -849,12 +877,12 @@ class OptimizersCompatibilityTest(keras_parameterized.TestCase):\n       y = np_utils.to_categorical(y)\n \n       num_hidden = 5\n-      model_k_v1 = testing_utils.get_small_sequential_mlp(\n+      model_k_v1 = test_utils.get_small_sequential_mlp(\n           num_hidden=num_hidden, num_classes=num_classes, input_dim=input_dim)\n-      model_k_v2 = testing_utils.get_small_sequential_mlp(\n+      model_k_v2 = test_utils.get_small_sequential_mlp(\n           num_hidden=num_hidden, num_classes=num_classes, input_dim=input_dim)\n       model_k_v2.set_weights(model_k_v1.get_weights())\n-      model_tf = testing_utils.get_small_sequential_mlp(\n+      model_tf = test_utils.get_small_sequential_mlp(\n           num_hidden=num_hidden, num_classes=num_classes, input_dim=input_dim)\n       model_tf.set_weights(model_k_v2.get_weights())\n \n@@ -867,17 +895,17 @@ class OptimizersCompatibilityTest(keras_parameterized.TestCase):\n           opt_k_v1,\n           loss='categorical_crossentropy',\n           metrics=[],\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n       model_k_v2.compile(\n           opt_k_v2,\n           loss='categorical_crossentropy',\n           metrics=[],\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n       model_tf.compile(\n           opt_tf,\n           loss='categorical_crossentropy',\n           metrics=[],\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n \n       hist_k_v1 = model_k_v1.fit(x, y, batch_size=5, epochs=10, shuffle=False)\n       hist_k_v2 = model_k_v2.fit(x, y, batch_size=5, epochs=10, shuffle=False)\n@@ -894,11 +922,11 @@ class OptimizersCompatibilityTest(keras_parameterized.TestCase):\n       self.skipTest(\n           'v1 optimizer does not run in eager mode')\n     np.random.seed(1331)\n-    with testing_utils.use_gpu():\n+    with test_utils.use_gpu():\n       train_samples = 20\n       input_dim = 3\n       num_classes = 2\n-      (x, y), _ = testing_utils.get_test_data(\n+      (x, y), _ = test_utils.get_test_data(\n           train_samples=train_samples,\n           test_samples=10,\n           input_shape=(input_dim,),\n@@ -906,9 +934,9 @@ class OptimizersCompatibilityTest(keras_parameterized.TestCase):\n       y = np_utils.to_categorical(y)\n \n       num_hidden = 5\n-      model_k_v1 = testing_utils.get_small_sequential_mlp(\n+      model_k_v1 = test_utils.get_small_sequential_mlp(\n           num_hidden=num_hidden, num_classes=num_classes, input_dim=input_dim)\n-      model_k_v2 = testing_utils.get_small_sequential_mlp(\n+      model_k_v2 = test_utils.get_small_sequential_mlp(\n           num_hidden=num_hidden, num_classes=num_classes, input_dim=input_dim)\n       model_k_v2.set_weights(model_k_v1.get_weights())\n \n@@ -919,12 +947,12 @@ class OptimizersCompatibilityTest(keras_parameterized.TestCase):\n           opt_k_v1,\n           loss='categorical_crossentropy',\n           metrics=[],\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n       model_k_v2.compile(\n           opt_k_v2,\n           loss='categorical_crossentropy',\n           metrics=[],\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n \n       hist_k_v1 = model_k_v1.fit(x, y, batch_size=5, epochs=10, shuffle=False)\n       hist_k_v2 = model_k_v2.fit(x, y, batch_size=5, epochs=10, shuffle=False)\n@@ -936,7 +964,7 @@ class OptimizersCompatibilityTest(keras_parameterized.TestCase):\n \n # Note: These tests are kept in a separate class to avoid bugs in some\n # distributions of Python that break AutoGraph which is used by tf.function.\n-@combinations.generate(combinations.combine(mode=['eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['eager']))\n class OptimizerWithFunctionTest(tf.test.TestCase, parameterized.TestCase):\n \n   def testBasic(self):\n@@ -969,7 +997,7 @@ class OptimizerWithFunctionTest(tf.test.TestCase, parameterized.TestCase):\n     a = tf.Variable([1., 2.], name='var')\n     b = tf.Variable([1.], name='var')\n \n-    @test_util.also_run_as_tf_function\n+    @tf_test_utils.also_run_as_tf_function\n     def var_key_test():\n       self.assertFalse(a._in_graph_mode)\n       self.assertFalse(b._in_graph_mode)\n@@ -1218,7 +1246,7 @@ COEFFICIENT_PARAMS = (\n )\n \n \n-class OptimizerCoefficientTest(keras_parameterized.TestCase):\n+class OptimizerCoefficientTest(test_combinations.TestCase):\n \n   @parameterized.named_parameters(*COEFFICIENT_PARAMS)\n   def test_duplicate_ops(self, optimizer_class, init_kwargs=None):\n\n@@ -22,9 +22,9 @@ import math\n \n from absl.testing import parameterized\n import numpy as np\n-from tensorflow.python.framework import test_util\n-from keras import combinations\n-from keras import testing_utils\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.optimizers import learning_rate_schedule\n from keras.optimizers.optimizer_v2 import rmsprop\n \n@@ -94,7 +94,7 @@ class RMSpropOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n   def testDense(self):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for (dtype, learning_rate, rho, momentum, epsilon, centered) in _TESTPARAMS:\n-      with tf.compat.v1.get_default_graph().as_default(), testing_utils.use_gpu():\n+      with tf.compat.v1.get_default_graph().as_default(), test_utils.use_gpu():\n         # Initialize variables for numpy implementation.\n         var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n         grads0_np = np.array([0.1, 0.2], dtype=dtype.as_numpy_dtype)\n@@ -367,7 +367,7 @@ class RMSpropOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n   def testSparse(self):\n     # TODO(tanzheny, omalleyt): Fix test in eager mode.\n     for (dtype, learning_rate, rho, momentum, epsilon, centered) in _TESTPARAMS:\n-      with tf.compat.v1.get_default_graph().as_default(), testing_utils.use_gpu():\n+      with tf.compat.v1.get_default_graph().as_default(), test_utils.use_gpu():\n         # Initialize variables for numpy implementation.\n         var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n         grads0_np = np.array([0.1], dtype=dtype.as_numpy_dtype)\n@@ -446,7 +446,7 @@ class RMSpropOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n           self.assertAllCloseAccordingToType(var0_np, self.evaluate(var0))\n           self.assertAllCloseAccordingToType(var1_np, self.evaluate(var1))\n \n-  @combinations.generate(combinations.combine(mode=[\"eager\"]))\n+  @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n   def testCallableParams(self):\n     for dtype in _DATA_TYPES:\n       var0 = tf.Variable([1.0, 2.0], dtype=dtype)\n@@ -508,7 +508,7 @@ class RMSpropOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllClose(self.evaluate(opt_2.lr), (1.0))\n     self.assertAllClose(self.evaluate(opt_3.lr), (0.1))\n \n-  @combinations.generate(combinations.combine(mode=[\"eager\"]))\n+  @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n   def testSlotsUniqueEager(self):\n     v1 = tf.Variable(1.)\n     v2 = tf.Variable(1.)\n@@ -534,7 +534,7 @@ class RMSpropOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(\n         self.evaluate(opt.variables()[0]), self.evaluate(opt.iterations))\n \n-  @combinations.generate(combinations.combine(mode=[\"eager\"]))\n+  @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n   def testMomentumProperValue(self):\n     with self.assertRaisesRegex(ValueError,\n                                 r\"`momentum` must be between \\[0, 1\\]. \"\n@@ -543,11 +543,11 @@ class RMSpropOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n       rmsprop.RMSprop(1., momentum=2.5, centered=False)\n \n \n-@combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n class SlotColocationTest(tf.test.TestCase, parameterized.TestCase):\n \n   @parameterized.parameters([True, False])\n-  @test_util.run_gpu_only\n+  @tf_test_utils.run_gpu_only\n   def testRunMinimizeOnGPUForCPUVariables(self, use_resource):\n     with tf.device(\"/device:CPU:0\"):\n       if use_resource:\n\n@@ -22,9 +22,9 @@ import weakref\n import numpy as np\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n from keras.optimizers import optimizer_v1\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.utils import np_utils\n from tensorflow.python.training.adam import AdamOptimizer\n from tensorflow.python.training.experimental.loss_scale_optimizer import MixedPrecisionLossScaleOptimizer\n@@ -39,15 +39,15 @@ def _get_model(input_dim, num_hidden, output_dim):\n   return model\n \n \n-@keras_parameterized.run_all_keras_modes\n-class KerasOptimizersTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class KerasOptimizersTest(test_combinations.TestCase):\n \n   def _test_optimizer(self, optimizer, target=0.75):\n     if tf.executing_eagerly():\n       self.skipTest(\n           'v1 optimizer does not run in eager mode')\n     np.random.seed(1337)\n-    (x_train, y_train), _ = testing_utils.get_test_data(\n+    (x_train, y_train), _ = test_utils.get_test_data(\n         train_samples=1000, test_samples=200, input_shape=(10,), num_classes=2)\n     y_train = np_utils.to_categorical(y_train)\n     model = _get_model(x_train.shape[1], 20, y_train.shape[1])\n@@ -55,7 +55,7 @@ class KerasOptimizersTest(keras_parameterized.TestCase):\n         loss='categorical_crossentropy',\n         optimizer=optimizer,\n         metrics=['acc'],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     np.testing.assert_equal(\n         keras.backend.get_value(model.optimizer.iterations), 0)\n     history = model.fit(x_train, y_train, epochs=2, batch_size=16, verbose=0)\n@@ -92,7 +92,7 @@ class KerasOptimizersTest(keras_parameterized.TestCase):\n         loss='categorical_crossentropy',\n         optimizer=optimizer,\n         metrics=['accuracy'],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     np.testing.assert_equal(\n         keras.backend.get_value(model.optimizer.iterations),\n         126)  # Using same optimizer from before\n@@ -169,7 +169,7 @@ class KerasOptimizersTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='mean_squared_error',\n         optimizer=optimizer,\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     keras.backend.track_tf_optimizer(optimizer)\n     model.fit(np.random.random((5, 3)),\n               np.random.random((5, 2)),\n@@ -212,7 +212,7 @@ class KerasOptimizersTest(keras_parameterized.TestCase):\n       model.compile(\n           loss='mean_squared_error',\n           optimizer=optimizer,\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n       keras.backend.track_tf_optimizer(optimizer)\n       self.assertEqual(keras.backend.get_value(model.optimizer.iterations), 0)\n \n@@ -241,7 +241,7 @@ class KerasOptimizersTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='mean_squared_error',\n         optimizer=optimizer,\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(\n         np.random.random((5, 3)),\n         np.random.random((5, 2)),\n\n@@ -18,7 +18,7 @@ import tensorflow.compat.v2 as tf\n \n import numpy as np\n from keras import backend\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras import losses\n from keras.engine import input_layer\n from keras.engine import sequential\n@@ -29,8 +29,8 @@ from keras.optimizers.optimizer_v2 import gradient_descent\n from keras.premade_models import linear\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class LinearModelTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class LinearModelTest(test_combinations.TestCase):\n \n   def test_linear_model_with_single_input(self):\n     model = linear.LinearModel()\n\n@@ -17,8 +17,8 @@\n import tensorflow.compat.v2 as tf\n \n import numpy as np\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.engine import input_layer\n from keras.engine import sequential\n from keras.engine import training\n@@ -29,8 +29,8 @@ from keras.premade_models import linear\n from keras.premade_models import wide_deep\n \n \n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class WideDeepModelTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class WideDeepModelTest(test_combinations.TestCase):\n \n   def test_wide_deep_model(self):\n     linear_model = linear.LinearModel(units=1)\n@@ -44,7 +44,7 @@ class WideDeepModelTest(keras_parameterized.TestCase):\n         optimizer=['sgd', 'adam'],\n         loss='mse',\n         metrics=[],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     wide_deep_model.fit(inputs, output, epochs=5)\n     self.assertTrue(wide_deep_model.built)\n \n@@ -64,7 +64,7 @@ class WideDeepModelTest(keras_parameterized.TestCase):\n           optimizer=[linear_opt, dnn_opt],\n           loss='mse',\n           metrics=[],\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n       self.evaluate(tf.compat.v1.global_variables_initializer())\n       wide_deep_model.fit(inputs, output, epochs=1)\n       self.assertAllClose(\n@@ -84,7 +84,7 @@ class WideDeepModelTest(keras_parameterized.TestCase):\n         optimizer=['sgd', 'adam'],\n         loss='mse',\n         metrics=[],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     wide_deep_model.fit(inputs, output, epochs=5)\n \n   def test_wide_deep_model_with_multi_outputs(self):\n@@ -123,7 +123,7 @@ class WideDeepModelTest(keras_parameterized.TestCase):\n         optimizer='sgd',\n         loss='mse',\n         metrics=[],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     wide_deep_model.fit(inputs, output, epochs=5)\n     self.assertTrue(wide_deep_model.built)\n \n@@ -147,7 +147,7 @@ class WideDeepModelTest(keras_parameterized.TestCase):\n         optimizer='sgd',\n         loss='mse',\n         metrics=[],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit([linear_input_np, dnn_input_np, input_b_np], output_np, epochs=5)\n \n   def test_wide_deep_model_with_sub_model_trained(self):\n@@ -164,19 +164,19 @@ class WideDeepModelTest(keras_parameterized.TestCase):\n         optimizer='sgd',\n         loss='mse',\n         metrics=[],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     dnn_model.compile(\n         optimizer='adam',\n         loss='mse',\n         metrics=[],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     linear_model.fit(linear_inp, output, epochs=50)\n     dnn_model.fit(dnn_inp, output, epochs=50)\n     wide_deep_model.compile(\n         optimizer=['sgd', 'adam'],\n         loss='mse',\n         metrics=[],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     wide_deep_model.fit(inputs, output, epochs=50)\n \n   # This test is an example for cases where linear and dnn model accepts\n@@ -204,7 +204,7 @@ class WideDeepModelTest(keras_parameterized.TestCase):\n     combined.compile(\n         opt,\n         'mse', [],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     combined.fit(x={'symbol': data}, y=y, batch_size=32, epochs=10)\n \n   # This test is an example for cases where linear and dnn model accepts\n@@ -237,7 +237,7 @@ class WideDeepModelTest(keras_parameterized.TestCase):\n     wide_deep_model.compile(\n         opt,\n         'mse', [],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     wide_deep_model.fit(x={'symbol': data}, y=y, batch_size=32, epochs=10)\n \n   def test_config(self):\n\n@@ -20,8 +20,8 @@ import os\n import shutil\n \n import numpy as np\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.preprocessing import image as image_preproc\n from keras.preprocessing import image_dataset\n \n@@ -31,8 +31,8 @@ except ImportError:\n   PIL = None\n \n \n-@testing_utils.run_v2_only\n-class ImageDatasetFromDirectoryTest(keras_parameterized.TestCase):\n+@test_utils.run_v2_only\n+class ImageDatasetFromDirectoryTest(test_combinations.TestCase):\n \n   def _get_images(self, count=16, color_mode='rgb'):\n     width = height = 24\n\n@@ -22,9 +22,9 @@ import tempfile\n \n from absl.testing import parameterized\n import numpy as np\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras import layers\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.engine import sequential\n from keras.preprocessing import image as preprocessing_image\n \n@@ -52,8 +52,8 @@ def _generate_test_images():\n   return [rgb_images, gray_images]\n \n \n-@testing_utils.run_v2_only\n-class TestImage(keras_parameterized.TestCase):\n+@test_utils.run_v2_only\n+class TestImage(test_combinations.TestCase):\n \n   def test_smart_resize(self):\n     test_input = np.random.random((20, 40, 3))\n@@ -348,15 +348,15 @@ class TestImage(keras_parameterized.TestCase):\n \n     shutil.rmtree(tmp_folder)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_directory_iterator_with_validation_split_25_percent(self):\n     self.directory_iterator_with_validation_split_test_helper(0.25)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_directory_iterator_with_validation_split_40_percent(self):\n     self.directory_iterator_with_validation_split_test_helper(0.40)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_directory_iterator_with_validation_split_50_percent(self):\n     self.directory_iterator_with_validation_split_test_helper(0.50)\n \n\n@@ -20,13 +20,13 @@ import os\n import random\n import shutil\n import string\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.preprocessing import text_dataset\n \n \n-@testing_utils.run_v2_only\n-class TextDatasetFromDirectoryTest(keras_parameterized.TestCase):\n+@test_utils.run_v2_only\n+class TextDatasetFromDirectoryTest(test_combinations.TestCase):\n \n   def _prepare_directory(self,\n                          num_classes=2,\n\n@@ -17,11 +17,11 @@\n import tensorflow.compat.v2 as tf\n \n import numpy as np\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.preprocessing import timeseries\n \n \n-@testing_utils.run_v2_only\n+@test_utils.run_v2_only\n class TimeseriesDatasetTest(tf.test.TestCase):\n \n   def test_basics(self):\n\n@@ -20,9 +20,9 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras import regularizers\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.utils import np_utils\n \n \n@@ -30,7 +30,7 @@ DATA_DIM = 5\n NUM_CLASSES = 2\n \n \n-class KerasRegularizersTest(keras_parameterized.TestCase,\n+class KerasRegularizersTest(test_combinations.TestCase,\n                             parameterized.TestCase):\n \n   def create_model(self, kernel_regularizer=None, activity_regularizer=None):\n@@ -42,7 +42,7 @@ class KerasRegularizersTest(keras_parameterized.TestCase,\n     return model\n \n   def get_data(self):\n-    (x_train, y_train), (x_test, y_test) = testing_utils.get_test_data(\n+    (x_train, y_train), (x_test, y_test) = test_utils.get_test_data(\n         train_samples=10,\n         test_samples=10,\n         input_shape=(DATA_DIM,),\n@@ -62,7 +62,7 @@ class KerasRegularizersTest(keras_parameterized.TestCase,\n     model.add_loss(tf.reduce_sum(input_1))\n     return model\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   @parameterized.named_parameters([\n       ('l1', regularizers.l1()),\n       ('l2', regularizers.l2()),\n@@ -74,11 +74,11 @@ class KerasRegularizersTest(keras_parameterized.TestCase,\n     model.compile(\n         loss='categorical_crossentropy',\n         optimizer='sgd',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     self.assertEqual(len(model.losses), 1)\n     model.fit(x_train, y_train, batch_size=10, epochs=1, verbose=0)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   @parameterized.named_parameters([\n       ('l1', regularizers.l1()),\n       ('l2', regularizers.l2()),\n@@ -91,22 +91,22 @@ class KerasRegularizersTest(keras_parameterized.TestCase,\n     model.compile(\n         loss='categorical_crossentropy',\n         optimizer='sgd',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     self.assertEqual(len(model.losses), 1 if tf.executing_eagerly() else 1)\n     model.fit(x_train, y_train, batch_size=10, epochs=1, verbose=0)\n \n-  @keras_parameterized.run_all_keras_modes\n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n   def test_zero_regularization(self):\n     # Verifies that training with zero regularization works.\n     x, y = np.ones((10, 10)), np.ones((10, 3))\n-    model = testing_utils.get_model_from_layers(\n+    model = test_utils.get_model_from_layers(\n         [keras.layers.Dense(3, kernel_regularizer=keras.regularizers.l2(0))],\n         input_shape=(10,))\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(x, y, batch_size=5, epochs=1)\n \n   def test_custom_regularizer_saving(self):\n@@ -121,7 +121,7 @@ class KerasRegularizersTest(keras_parameterized.TestCase,\n         model.get_config(), custom_objects={'my_regularizer': my_regularizer})\n     self.assertEqual(model2.layers[1].kernel_regularizer, my_regularizer)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   @parameterized.named_parameters([\n       ('l1', regularizers.l1()),\n       ('l2', regularizers.l2()),\n@@ -136,10 +136,10 @@ class KerasRegularizersTest(keras_parameterized.TestCase,\n     model.compile(\n         loss='categorical_crossentropy',\n         optimizer='sgd',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     self.assertLen(model.losses, 5)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   @parameterized.named_parameters([\n       ('l1', regularizers.l1()),\n       ('l2', regularizers.l2()),\n@@ -158,10 +158,10 @@ class KerasRegularizersTest(keras_parameterized.TestCase,\n     model.compile(\n         loss='categorical_crossentropy',\n         optimizer='sgd',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     self.assertLen(model.losses, 6)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   @parameterized.named_parameters([\n       ('l1', regularizers.l1()),\n       ('l2', regularizers.l2()),\n@@ -185,7 +185,7 @@ class KerasRegularizersTest(keras_parameterized.TestCase,\n     model.compile(\n         loss='categorical_crossentropy',\n         optimizer='sgd',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     # We expect to see 9 losses on the model:\n     # - 2 from the 2 add_loss calls on the outer model.\n\n@@ -23,11 +23,11 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras import layers\n from keras import losses\n from keras.optimizers import optimizer_v2\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.utils import generic_utils\n from keras.utils import losses_utils\n \n@@ -55,13 +55,13 @@ def my_mae(y_true, y_pred):\n def _get_multi_io_model():\n   inp_1 = layers.Input(shape=(1,), name='input_1')\n   inp_2 = layers.Input(shape=(1,), name='input_2')\n-  d = testing_utils.Bias(name='output')\n+  d = test_utils.Bias(name='output')\n   out_1 = d(inp_1)\n   out_2 = d(inp_2)\n   return keras.Model([inp_1, inp_2], [out_1, out_2])\n \n \n-@keras_parameterized.run_all_keras_modes\n+@test_combinations.run_all_keras_modes\n @parameterized.named_parameters([\n     dict(testcase_name='string', value='mae'),\n     dict(testcase_name='built_in_fn', value=losses.mae),\n@@ -110,7 +110,7 @@ def _get_multi_io_model():\n             'output_1': MyMeanAbsoluteError(),\n         }),\n ])\n-class LossesSerialization(keras_parameterized.TestCase):\n+class LossesSerialization(test_combinations.TestCase):\n \n   def setUp(self):\n     super(LossesSerialization, self).setUp()\n@@ -125,13 +125,13 @@ class LossesSerialization(keras_parameterized.TestCase):\n     with generic_utils.custom_object_scope({\n         'MyMeanAbsoluteError': MyMeanAbsoluteError,\n         'my_mae': my_mae,\n-        'Bias': testing_utils.Bias,\n+        'Bias': test_utils.Bias,\n     }):\n       model = _get_multi_io_model()\n       model.compile(\n           optimizer_v2.gradient_descent.SGD(0.1),\n           loss=value,\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n       history = model.fit([self.x, self.x], [self.y, self.y],\n                           batch_size=3,\n                           epochs=3,\n@@ -158,7 +158,7 @@ class LossesSerialization(keras_parameterized.TestCase):\n     model.compile(\n         optimizer_v2.gradient_descent.SGD(0.1),\n         loss=value,\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit([self.x, self.x], [self.y, self.y],\n                         batch_size=3,\n                         epochs=3,\n@@ -177,7 +177,7 @@ class LossesSerialization(keras_parameterized.TestCase):\n         custom_objects={\n             'MyMeanAbsoluteError': MyMeanAbsoluteError,\n             'my_mae': my_mae,\n-            'Bias': testing_utils.Bias,\n+            'Bias': test_utils.Bias,\n         })\n     loaded_model.predict([self.x, self.x])\n     loaded_eval_results = loaded_model.evaluate([self.x, self.x],\n\n@@ -23,11 +23,11 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras import layers\n from keras import metrics\n from keras.optimizers import optimizer_v2\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.utils import generic_utils\n \n try:\n@@ -51,13 +51,13 @@ def _my_mae(y_true, y_pred):\n def _get_multi_io_model():\n   inp_1 = layers.Input(shape=(1,), name='input_1')\n   inp_2 = layers.Input(shape=(1,), name='input_2')\n-  d = testing_utils.Bias(name='output')\n+  d = test_utils.Bias(name='output')\n   out_1 = d(inp_1)\n   out_2 = d(inp_2)\n   return keras.Model([inp_1, inp_2], [out_1, out_2])\n \n \n-@keras_parameterized.run_all_keras_modes\n+@test_combinations.run_all_keras_modes\n @parameterized.named_parameters(\n     dict(testcase_name='string', value=['mae']),\n     dict(testcase_name='built_in_fn', value=[metrics.mae]),\n@@ -142,7 +142,7 @@ def _get_multi_io_model():\n             'output_1': MyMeanAbsoluteError,\n         }),\n )\n-class MetricsSerialization(keras_parameterized.TestCase):\n+class MetricsSerialization(test_combinations.TestCase):\n \n   def setUp(self):\n     super(MetricsSerialization, self).setUp()\n@@ -168,7 +168,7 @@ class MetricsSerialization(keras_parameterized.TestCase):\n     with generic_utils.custom_object_scope({\n         'MyMeanAbsoluteError': MyMeanAbsoluteError,\n         '_my_mae': _my_mae,\n-        'Bias': testing_utils.Bias,\n+        'Bias': test_utils.Bias,\n     }):\n       model = _get_multi_io_model()\n       model.compile(\n@@ -176,7 +176,7 @@ class MetricsSerialization(keras_parameterized.TestCase):\n           'mae',\n           metrics=metric_input,\n           weighted_metrics=weighted_metric_input,\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n       history = model.fit([self.x, self.x], [self.y, self.y],\n                           batch_size=3,\n                           epochs=3,\n@@ -216,7 +216,7 @@ class MetricsSerialization(keras_parameterized.TestCase):\n         'mae',\n         metrics=metric_input,\n         weighted_metrics=weighted_metric_input,\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit([self.x, self.x], [self.y, self.y],\n                         batch_size=3,\n                         epochs=3,\n@@ -235,7 +235,7 @@ class MetricsSerialization(keras_parameterized.TestCase):\n         custom_objects={\n             'MyMeanAbsoluteError': MyMeanAbsoluteError,\n             '_my_mae': _my_mae,\n-            'Bias': testing_utils.Bias,\n+            'Bias': test_utils.Bias,\n         })\n     loaded_model.predict([self.x, self.x])\n     loaded_eval_results = loaded_model.evaluate([self.x, self.x],\n\n@@ -20,15 +20,15 @@ import copy\n import pickle\n import numpy as np\n \n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n \n \n-class TestPickleProtocol(keras_parameterized.TestCase):\n+class TestPickleProtocol(test_combinations.TestCase):\n   \"\"\"Tests pickle protoocol support.\"\"\"\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.parameterized.named_parameters(\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.parameterized.named_parameters(\n       ('copy', copy.copy), ('deepcopy', copy.deepcopy),\n       *((f'pickle_protocol_level_{protocol}',\n          lambda model: pickle.loads(pickle.dumps(model, protocol=protocol)))  # pylint: disable=cell-var-from-loop\n@@ -37,7 +37,7 @@ class TestPickleProtocol(keras_parameterized.TestCase):\n     \"\"\"Built models should be copyable and picklable for all model types.\"\"\"\n     if not tf.__internal__.tf2.enabled():\n       self.skipTest('pickle model only available in v2 when tf format is used.')\n-    model = testing_utils.get_small_mlp(\n+    model = test_utils.get_small_mlp(\n         num_hidden=1, num_classes=2, input_dim=3)\n     model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy')\n \n@@ -57,8 +57,8 @@ class TestPickleProtocol(keras_parameterized.TestCase):\n     # check that the predictions are the same\n     self.assertNotAllClose(y2, y3)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.parameterized.named_parameters(\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.parameterized.named_parameters(\n       ('copy', copy.copy),\n       ('deepcopy', copy.deepcopy),\n   )\n@@ -66,7 +66,7 @@ class TestPickleProtocol(keras_parameterized.TestCase):\n     \"\"\"Unbuilt models should be copyable & deepcopyable for all model types.\"\"\"\n     if not tf.__internal__.tf2.enabled():\n       self.skipTest('pickle model only available in v2 when tf format is used.')\n-    original_model = testing_utils.get_small_mlp(\n+    original_model = test_utils.get_small_mlp(\n         num_hidden=1, num_classes=2, input_dim=3)\n     # roundtrip without compiling or training\n     model = serializer(original_model)\n\n@@ -27,12 +27,9 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import combinations\n-from keras import keras_parameterized\n from keras import losses\n from keras.optimizers import optimizer_v1\n from keras import optimizers\n-from keras import testing_utils\n from keras.engine import functional\n from keras.engine import sequential\n from keras.feature_column import dense_features\n@@ -41,6 +38,8 @@ from keras.layers import core\n from keras.premade_models.linear import LinearModel\n from keras.saving import model_config\n from keras.saving import save\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.utils import generic_utils\n \n \n@@ -54,8 +53,8 @@ class TestSaveModel(tf.test.TestCase, parameterized.TestCase):\n \n   def setUp(self):\n     super(TestSaveModel, self).setUp()\n-    self.model = testing_utils.get_small_sequential_mlp(1, 2, 3)\n-    self.subclassed_model = testing_utils.get_small_subclass_mlp(1, 2)\n+    self.model = test_utils.get_small_sequential_mlp(1, 2, 3)\n+    self.subclassed_model = test_utils.get_small_subclass_mlp(1, 2)\n \n   def assert_h5_format(self, path):\n     if h5py is not None:\n@@ -66,25 +65,25 @@ class TestSaveModel(tf.test.TestCase, parameterized.TestCase):\n   def assert_saved_model(self, path):\n     tf.__internal__.saved_model.parse_saved_model(path)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_load_file_not_found(self):\n     path = pathlib.Path(self.get_temp_dir()) / 'does_not_exist'\n     with self.assertRaisesRegex(IOError, 'No file or directory found at'):\n       save.load_model(path)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_save_format_defaults(self):\n     path = os.path.join(self.get_temp_dir(), 'model_path')\n     save.save_model(self.model, path)\n     self.assert_saved_model(path)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_save_format_defaults_pathlib(self):\n     path = pathlib.Path(self.get_temp_dir()) / 'model_path'\n     save.save_model(self.model, path)\n     self.assert_saved_model(path)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_save_hdf5(self):\n     path = os.path.join(self.get_temp_dir(), 'model')\n     save.save_model(self.model, path, save_format='h5')\n@@ -94,13 +93,13 @@ class TestSaveModel(tf.test.TestCase, parameterized.TestCase):\n         'requires the model to be a Functional model or a Sequential model.'):\n       save.save_model(self.subclassed_model, path, save_format='h5')\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_save_load_hdf5_pathlib(self):\n     path = pathlib.Path(self.get_temp_dir()) / 'model'\n     save.save_model(self.model, path, save_format='h5')\n     save.load_model(path)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_save_tf(self):\n     path = os.path.join(self.get_temp_dir(), 'model')\n     save.save_model(self.model, path, save_format='tf')\n@@ -112,31 +111,32 @@ class TestSaveModel(tf.test.TestCase, parameterized.TestCase):\n     save.save_model(self.subclassed_model, path, save_format='tf')\n     self.assert_saved_model(path)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_save_load_tf_string(self):\n     path = os.path.join(self.get_temp_dir(), 'model')\n     save.save_model(self.model, path, save_format='tf')\n     save.load_model(path)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_save_load_tf_pathlib(self):\n     path = pathlib.Path(self.get_temp_dir()) / 'model'\n     save.save_model(self.model, path, save_format='tf')\n     save.load_model(path)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_save_load_weights_tf_pathlib(self):\n     path = pathlib.Path(self.get_temp_dir()) / 'model'\n     self.model.save_weights(path, save_format='tf')\n     self.model.load_weights(path)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def test_save_load_weights_hdf5_pathlib(self):\n     path = pathlib.Path(self.get_temp_dir()) / 'model'\n     self.model.save_weights(path, save_format='h5')\n     self.model.load_weights(path)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_saving_h5_for_rnn_layers(self):\n     # See https://github.com/tensorflow/tensorflow/issues/35731 for details.\n     inputs = keras.Input([10, 91], name='train_input')\n@@ -157,7 +157,8 @@ class TestSaveModel(tf.test.TestCase, parameterized.TestCase):\n                         rnn_layers[1].kernel.name)\n     self.assertIn('rnn_cell1', rnn_layers[1].kernel.name)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_saving_optimizer_weights(self):\n \n     class MyModel(keras.Model):\n@@ -190,7 +191,8 @@ class TestSaveModel(tf.test.TestCase, parameterized.TestCase):\n \n     self.assertAllClose(batch_loss, new_batch_loss)\n \n-  @combinations.generate(combinations.combine(mode=['eager', 'graph']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['eager', 'graph']))\n   def test_save_include_optimizer_false(self):\n \n     def get_variables(file_name):\n@@ -212,7 +214,8 @@ class TestSaveModel(tf.test.TestCase, parameterized.TestCase):\n     for v in variables:\n       self.assertNotIn('optimizer', v)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_saving_model_with_custom_object(self):\n     with generic_utils.custom_object_scope(), self.cached_session():\n \n@@ -274,10 +277,11 @@ class RegisteredSubLayer(keras.layers.Layer):\n   pass\n \n \n-class TestJson(keras_parameterized.TestCase):\n+class TestJson(test_combinations.TestCase):\n   \"\"\"Tests to_json()/from_json().\"\"\"\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_saving_with_dense_features(self):\n     cols = [\n         tf.feature_column.numeric_column('a'),\n@@ -313,7 +317,8 @@ class TestJson(keras_parameterized.TestCase):\n \n       self.assertLen(loaded_model.predict({'a': inputs_a, 'b': inputs_b}), 10)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_saving_with_sequence_features(self):\n     cols = [\n         tf.feature_column.sequence_numeric_column('a'),\n@@ -372,7 +377,8 @@ class TestJson(keras_parameterized.TestCase):\n               'b': inputs_b\n           }, steps=1), batch_size)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_nested_layers(self):\n \n     class MyLayer(keras.layers.Layer):\n@@ -402,8 +408,8 @@ class TestJson(keras_parameterized.TestCase):\n     self.assertEqual(loaded_layer.sublayers[1].name, 'MySubLayer')\n \n \n-@keras_parameterized.run_with_all_saved_model_formats\n-class TestWholeModelSaving(keras_parameterized.TestCase):\n+@test_combinations.run_with_all_saved_model_formats\n+class TestWholeModelSaving(test_combinations.TestCase):\n \n   def _save_model_dir(self, dirname='saved_model'):\n     temp_dir = self.get_temp_dir()\n@@ -420,7 +426,7 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n     self.assertAllClose(model.weights, loaded_model.weights)\n \n     if loaded_model.optimizer:\n-      if testing_utils.get_save_format() == 'tf':\n+      if test_utils.get_save_format() == 'tf':\n         # TODO(b/153110928): Keras TF format doesn't restore optimizer weights\n         # currently.\n         return\n@@ -436,22 +442,22 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n       self.assertAllEqual([m.name for m in model.metrics],\n                           [m.name for m in loaded_model.metrics])\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_save_and_load(self):\n     saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n-    save_kwargs = testing_utils.get_save_kwargs()\n+    save_format = test_utils.get_save_format()\n+    save_kwargs = test_utils.get_save_kwargs()\n \n     if ((save_format == 'h5' or not save_kwargs.get('save_traces', True)) and\n-        testing_utils.get_model_type() == 'subclass'):\n+        test_utils.get_model_type() == 'subclass'):\n       # HDF5 format currently does not allow saving subclassed models.\n       # When saving with `save_traces=False`, the subclassed model must have a\n       # get_config/from_config, which the autogenerated model does not have.\n       return\n \n     with self.cached_session():\n-      model = testing_utils.get_model_from_layers(\n+      model = test_utils.get_model_from_layers(\n           [keras.layers.Dense(2),\n            keras.layers.RepeatVector(3),\n            keras.layers.TimeDistributed(keras.layers.Dense(3))],\n@@ -490,10 +496,11 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n       eval_out2 = loaded_model.evaluate(x, y)\n       self.assertArrayNear(eval_out, eval_out2, 0.001)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_sequential_model_saving_without_input_shape(self):\n     saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n+    save_format = test_utils.get_save_format()\n     with self.cached_session():\n       model = keras.models.Sequential()\n       model.add(keras.layers.Dense(2))\n@@ -525,10 +532,11 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n       out2 = new_model.predict(x)\n       self.assertAllClose(out, out2, atol=1e-05)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_sequential_model_saving_without_compile(self):\n     saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n+    save_format = test_utils.get_save_format()\n     with self.cached_session():\n       model = keras.models.Sequential()\n       model.add(keras.layers.Dense(2, input_shape=(3,)))\n@@ -549,7 +557,7 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n \n   def test_sequential_model_saving_2(self):\n     saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n+    save_format = test_utils.get_save_format()\n \n     with tf.Graph().as_default(), self.cached_session():\n       # test with custom optimizer, loss\n@@ -583,7 +591,7 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n \n   def test_saving_without_compilation(self):\n     saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n+    save_format = test_utils.get_save_format()\n     model = keras.models.Sequential()\n     model.add(keras.layers.Dense(2, input_shape=(3,)))\n     model.add(keras.layers.Dense(3))\n@@ -594,7 +602,7 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n \n   def test_saving_with_tf_optimizer(self):\n     saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n+    save_format = test_utils.get_save_format()\n \n     model = keras.models.Sequential()\n     model.add(keras.layers.Dense(2, input_shape=(3,)))\n@@ -608,7 +616,7 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n \n   def test_saving_right_after_compilation(self):\n     saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n+    save_format = test_utils.get_save_format()\n     with self.cached_session():\n       model = keras.models.Sequential()\n       model.add(keras.layers.Dense(2, input_shape=(3,)))\n@@ -621,7 +629,7 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n \n   def test_saving_lambda_numpy_array_arguments(self):\n     saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n+    save_format = test_utils.get_save_format()\n \n     if h5py is None:\n       self.skipTest('h5py required to run this test')\n@@ -643,7 +651,7 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n \n   def test_saving_model_with_long_layer_names(self):\n     saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n+    save_format = test_utils.get_save_format()\n     with self.cached_session():\n       # This layer name will make the `layers_name` HDF5 attribute blow\n       # out of proportion. Note that it fits into the internal HDF5\n@@ -681,7 +689,7 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n \n   def test_saving_model_with_long_weights_names(self):\n     saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n+    save_format = test_utils.get_save_format()\n \n     with self.cached_session():\n       x = keras.Input(shape=(2,), name='nested_model_input')\n@@ -722,7 +730,7 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n \n   def test_model_saving_to_pre_created_h5py_file(self):\n     saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n+    save_format = test_utils.get_save_format()\n     with tf.Graph().as_default(), self.cached_session():\n       inputs = keras.Input(shape=(3,))\n       x = keras.layers.Dense(2)(inputs)\n@@ -772,7 +780,7 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n   def test_model_saving_to_new_dir_path(self):\n     saved_model_dir = os.path.join(self._save_model_dir(), 'newdir',\n                                    'saved_model')\n-    save_format = testing_utils.get_save_format()\n+    save_format = test_utils.get_save_format()\n \n     with self.cached_session():\n       model = keras.models.Sequential()\n@@ -810,7 +818,7 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n \n   def test_saving_constant_initializer_with_numpy(self):\n     saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n+    save_format = test_utils.get_save_format()\n \n     model = keras.models.Sequential()\n     model.add(\n@@ -844,7 +852,7 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n       self.skipTest('h5py required to run this test')\n \n     saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n+    save_format = test_utils.get_save_format()\n     model = keras.models.Sequential()\n     model.add(keras.layers.Dense(1, input_shape=[2]))\n     model.save(saved_model_dir, save_format=save_format)\n@@ -854,7 +862,8 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n     h5file = h5py.File(saved_model_dir, 'r')\n     self.assertRegex(h5file.attrs['keras_version'], r'^[\\d]+\\.[\\d]+\\.[\\S]+$')\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_functional_model_with_custom_loss_and_metric(self):\n     def _make_model():\n       inputs = keras.Input(shape=(4,))\n@@ -867,7 +876,7 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n       return model\n \n     saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n+    save_format = test_utils.get_save_format()\n \n     with self.cached_session():\n       model = _make_model()\n@@ -894,11 +903,12 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n           evaluation_results['sparse_categorical_crossentropy'] +\n           evaluation_results['custom_loss'], evaluation_results['loss'], 1e-6)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_save_uncompiled_model_with_optimizer(self):\n     with self.cached_session() as session:\n       saved_model_dir = self._save_model_dir()\n-      save_format = testing_utils.get_save_format()\n+      save_format = test_utils.get_save_format()\n       model = keras.models.Sequential([keras.layers.Dense(1, input_shape=(3,))])\n       # Set the model's optimizer but don't compile. This can happen if the\n       # model is trained with a custom training loop.\n@@ -913,7 +923,7 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n             loaded.optimizer,\n             keras.optimizers.optimizer_v2.optimizer_v2.OptimizerV2)\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def test_functional_model_with_getitem_op_layer(self):\n     inp = keras.Input(shape=(8))\n \n@@ -931,7 +941,7 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n     self.assertAllEqual(model.predict(args, batch_size=batch_size), expected)\n \n     # Make sure it can be successfully saved and loaded.\n-    save_format = testing_utils.get_save_format()\n+    save_format = test_utils.get_save_format()\n     saved_model_dir = self._save_model_dir()\n     keras.models.save_model(model, saved_model_dir, save_format=save_format)\n \n@@ -941,7 +951,8 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n     self.assertAllEqual(loaded_model.predict(args, batch_size=batch_size),\n                         expected)\n \n-  @combinations.generate(combinations.combine(mode=['eager', 'graph']))\n+  @test_combinations.generate(test_combinations.combine(\n+      mode=['eager', 'graph']))\n   def test_custom_functional_registered(self):\n \n     def _get_cls_definition():\n@@ -965,7 +976,7 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n       if not tf.executing_eagerly():\n         sess.run([v.initializer for v in model.variables])\n \n-      save_format = testing_utils.get_save_format()\n+      save_format = test_utils.get_save_format()\n       saved_model_dir = self._save_model_dir()\n       keras.models.save_model(model, saved_model_dir, save_format=save_format)\n \n@@ -981,7 +992,7 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n         saved_model_dir, custom_objects={'CustomModel': new_cls})\n     self.assertIsInstance(reloaded_model, new_cls)\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def test_shared_objects(self):\n     class OuterLayer(keras.layers.Layer):\n \n@@ -1066,7 +1077,7 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n         'OuterLayer': OuterLayer, 'InnerLayer': InnerLayer}):\n \n       # Test saving and loading to disk\n-      save_format = testing_utils.get_save_format()\n+      save_format = test_utils.get_save_format()\n       saved_model_dir = self._save_model_dir()\n       keras.models.save_model(model, saved_model_dir, save_format=save_format)\n       loaded = keras.models.load_model(saved_model_dir)\n@@ -1079,7 +1090,7 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n       loaded = keras.Model.from_config(config)\n       _do_assertions(loaded)\n \n-  @combinations.generate(combinations.combine(mode=['eager']))\n+  @test_combinations.generate(test_combinations.combine(mode=['eager']))\n   def test_shared_objects_wrapper(self):\n     \"\"\"Tests that shared layers wrapped with `Wrapper` restore correctly.\"\"\"\n     input_ = keras.Input(shape=(1,))\n@@ -1094,14 +1105,14 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n     self.assertIs(loaded.layers[1], loaded.layers[2].layer)\n \n     # Test saving and loading to disk\n-    save_format = testing_utils.get_save_format()\n+    save_format = test_utils.get_save_format()\n     saved_model_dir = self._save_model_dir()\n     keras.models.save_model(model, saved_model_dir, save_format=save_format)\n     loaded = keras.models.load_model(saved_model_dir)\n     self.assertIs(loaded.layers[1], loaded.layers[2].layer)\n \n-  @combinations.generate(\n-      combinations.combine(mode=['graph', 'eager'], fit=[True, False]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager'], fit=[True, False]))\n   def test_multi_output_metrics_name_stay_same(self, fit):\n     \"\"\"Tests that metric names don't change with each save/load cycle.\n \n@@ -1136,7 +1147,7 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n       model.fit(x, y, verbose=0)\n \n     # Save and reload.\n-    save_format = testing_utils.get_save_format()\n+    save_format = test_utils.get_save_format()\n     saved_model_dir = self._save_model_dir()\n     keras.models.save_model(model, saved_model_dir, save_format=save_format)\n     loaded = keras.models.load_model(saved_model_dir)\n@@ -1145,7 +1156,8 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n     # model.\n     self.assertSequenceEqual(model.metrics_names, loaded.metrics_names)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_warning_when_saving_invalid_custom_mask_layer(self):\n \n     class MyMasking(keras.layers.Layer):\n@@ -1166,7 +1178,7 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n     model = keras.Sequential([MyMasking(), MyLayer()])\n     model.predict(samples)\n     with warnings.catch_warnings(record=True) as w:\n-      model.save(self._save_model_dir(), testing_utils.get_save_format())\n+      model.save(self._save_model_dir(), test_utils.get_save_format())\n     self.assertIn(generic_utils.CustomMaskWarning,\n                   {warning.category for warning in w})\n \n@@ -1189,7 +1201,7 @@ class TestWholeModelSaving(keras_parameterized.TestCase):\n     model = keras.Sequential([MyCorrectMasking(), MyLayer()])\n     model.predict(samples)\n     with warnings.catch_warnings(record=True) as w:\n-      model.save(self._save_model_dir(), testing_utils.get_save_format())\n+      model.save(self._save_model_dir(), test_utils.get_save_format())\n     self.assertNotIn(generic_utils.CustomMaskWarning,\n                      {warning.category for warning in w})\n \n@@ -1258,7 +1270,7 @@ class _make_subclassed_built(_make_subclassed):  # pylint: disable=invalid-name\n     self.build((None, input_size))\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class TestWholeModelSavingWithNesting(tf.test.TestCase, parameterized.TestCase):\n   \"\"\"Tests saving a whole model that contains other models.\"\"\"\n \n\n@@ -24,10 +24,9 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import combinations\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras.optimizers import optimizer_v1\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.engine import training\n from keras.saving import hdf5_format\n \n@@ -37,7 +36,7 @@ except ImportError:\n   h5py = None\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class TestWeightSavingAndLoading(tf.test.TestCase, parameterized.TestCase):\n \n   def _save_model_dir(self, dirname='saved_model'):\n@@ -45,10 +44,10 @@ class TestWeightSavingAndLoading(tf.test.TestCase, parameterized.TestCase):\n     self.addCleanup(shutil.rmtree, temp_dir, ignore_errors=True)\n     return os.path.join(temp_dir, dirname)\n \n-  @keras_parameterized.run_with_all_weight_formats\n+  @test_combinations.run_with_all_weight_formats\n   def test_weight_loading(self):\n     saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n+    save_format = test_utils.get_save_format()\n     with self.cached_session():\n       a = keras.layers.Input(shape=(2,))\n       x = keras.layers.Dense(3)(a)\n@@ -227,10 +226,10 @@ class TestWeightSavingAndLoading(tf.test.TestCase, parameterized.TestCase):\n \n       self.assertAllClose(y, ref_y)\n \n-  @keras_parameterized.run_with_all_saved_model_formats(\n+  @test_combinations.run_with_all_saved_model_formats(\n       exclude_formats=['tf_no_traces'])\n   def test_nested_model_weight_loading(self):\n-    save_format = testing_utils.get_save_format()\n+    save_format = test_utils.get_save_format()\n     saved_model_dir = self._save_model_dir()\n \n     batch_size = 5\n@@ -347,27 +346,27 @@ class TestWeightSavingAndLoading(tf.test.TestCase, parameterized.TestCase):\n       self.assertAllClose([3.5] * num_classes,\n                           keras.backend.get_value(model.layers[1].bias))\n \n-  @keras_parameterized.run_with_all_saved_model_formats(\n+  @test_combinations.run_with_all_saved_model_formats(\n       exclude_formats=['tf_no_traces'])\n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_with_all_model_types\n   def test_load_weights_from_saved_model(self):\n     save_path = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n+    save_format = test_utils.get_save_format()\n \n-    if save_format == 'h5' and testing_utils.get_model_type() == 'subclass':\n+    if save_format == 'h5' and test_utils.get_model_type() == 'subclass':\n       # TODO(b/173646281): HDF5 format currently does not allow saving\n       # subclassed models.\n       return\n \n     with self.cached_session():\n-      model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n+      model = test_utils.get_small_mlp(1, 4, input_dim=3)\n       data = np.random.random((1, 3))\n       labels = np.random.random((1, 4))\n       model.compile(loss='mse', optimizer='rmsprop')\n       model.fit(data, labels)\n       model.save(save_path, save_format=save_format)\n-      new_model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n-      if testing_utils.get_model_type() == 'subclass':\n+      new_model = test_utils.get_small_mlp(1, 4, input_dim=3)\n+      if test_utils.get_model_type() == 'subclass':\n         # Call on test data to build the model.\n         new_model.predict(data)\n       new_model.load_weights(save_path)\n@@ -387,7 +386,8 @@ class SubclassedModel(training.Model):\n \n class TestWeightSavingAndLoadingTFFormat(tf.test.TestCase, parameterized.TestCase):\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_tensorflow_format_overwrite(self):\n     with self.cached_session() as session:\n       model = SubclassedModel()\n@@ -492,7 +492,8 @@ class TestWeightSavingAndLoadingTFFormat(tf.test.TestCase, parameterized.TestCas\n       load_model.train_on_batch(train_x, train_y)\n       self.assertAllClose(ref_y_after_train, self.evaluate(load_model(x)))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_weight_loading_graph_model(self):\n     def _make_graph_model():\n       a = keras.layers.Input(shape=(2,))\n@@ -502,7 +503,8 @@ class TestWeightSavingAndLoadingTFFormat(tf.test.TestCase, parameterized.TestCas\n \n     self._weight_loading_test_template(_make_graph_model)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_weight_loading_subclassed_model(self):\n     self._weight_loading_test_template(SubclassedModel)\n \n@@ -540,7 +542,8 @@ class TestWeightSavingAndLoadingTFFormat(tf.test.TestCase, parameterized.TestCas\n       y = self.evaluate(model(x))\n       self.assertAllClose(ref_y, y)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_weight_loading_graph_model_added_layer(self):\n     def _save_graph_model():\n       a = keras.layers.Input(shape=(2,))\n@@ -557,7 +560,8 @@ class TestWeightSavingAndLoadingTFFormat(tf.test.TestCase, parameterized.TestCas\n     self._new_layer_weight_loading_test_template(\n         _save_graph_model, _restore_graph_model)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_weight_loading_graph_model_added_no_weight_layer(self):\n     def _save_graph_model():\n       a = keras.layers.Input(shape=(2,))\n@@ -574,7 +578,8 @@ class TestWeightSavingAndLoadingTFFormat(tf.test.TestCase, parameterized.TestCas\n     self._new_layer_weight_loading_test_template(\n         _save_graph_model, _restore_graph_model)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_weight_loading_subclassed_model_added_layer(self):\n \n     class SubclassedModelRestore(training.Model):\n@@ -591,7 +596,8 @@ class TestWeightSavingAndLoadingTFFormat(tf.test.TestCase, parameterized.TestCas\n     self._new_layer_weight_loading_test_template(\n         SubclassedModel, SubclassedModelRestore)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_incompatible_checkpoint(self):\n     save_path = tf.train.Checkpoint().save(\n         os.path.join(self.get_temp_dir(), 'ckpt'))\n@@ -604,7 +610,8 @@ class TestWeightSavingAndLoadingTFFormat(tf.test.TestCase, parameterized.TestCas\n                                 'Nothing except the root object matched'):\n       m.load_weights(save_path)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_directory_passed(self):\n     with self.cached_session():\n       m = DummySubclassModel()\n@@ -616,7 +623,8 @@ class TestWeightSavingAndLoadingTFFormat(tf.test.TestCase, parameterized.TestCas\n       m.load_weights(prefix)\n       self.assertEqual(42., self.evaluate(v))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_relative_path(self):\n     with self.cached_session():\n       m = DummySubclassModel()\n@@ -647,7 +655,8 @@ class TestWeightSavingAndLoadingTFFormat(tf.test.TestCase, parameterized.TestCas\n       m.load_weights(prefix)\n       self.assertEqual(44., self.evaluate(v))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_nonexistent_prefix_directory(self):\n     with self.cached_session():\n       m = DummySubclassModel()\n\n@@ -7,7 +7,7 @@ different processes.\n from absl import app\n from absl import flags\n from keras import regularizers\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n \n import tensorflow.compat.v2 as tf\n \n@@ -17,8 +17,8 @@ FLAGS = flags.FLAGS\n \n \n def main(_) -> None:\n-  with testing_utils.model_type_scope('functional'):\n-    model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n+  with test_utils.model_type_scope('functional'):\n+    model = test_utils.get_small_mlp(1, 4, input_dim=3)\n     model.layers[-1].activity_regularizer = regularizers.get('l2')\n     model.activity_regularizer = regularizers.get('l2')\n     model.compile(\n\n@@ -28,8 +28,8 @@ import numpy as np\n \n import keras\n from keras import backend\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.saving.saved_model import load as keras_load\n from keras.utils import generic_utils\n \n@@ -199,7 +199,7 @@ class WideDeepModel(SubclassedModelWithConfig):\n   pass\n \n \n-class ReviveTestBase(keras_parameterized.TestCase):\n+class ReviveTestBase(test_combinations.TestCase):\n \n   def setUp(self):\n     super(ReviveTestBase, self).setUp()\n@@ -260,10 +260,10 @@ class ReviveTestBase(keras_parameterized.TestCase):\n # (putting them in the same TestCase resolves this).\n class TestBigModelRevive(ReviveTestBase):\n \n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_with_all_model_types\n   def test_revive(self):\n     input_shape = None\n-    if testing_utils.get_model_type() == 'functional':\n+    if test_utils.get_model_type() == 'functional':\n       input_shape = (2, 3)\n \n     layer_with_config = CustomLayerWithConfig(1., 2)\n@@ -308,7 +308,7 @@ class TestBigModelRevive(ReviveTestBase):\n               inner_model_functional,\n               inner_model_sequential,\n               inner_model_subclassed]\n-    model = testing_utils.get_model_from_layers(\n+    model = test_utils.get_model_from_layers(\n         layers, input_shape=input_shape)\n     # Run data through the Model to create save spec and weights.\n     model.predict(np.ones((10, 2, 3)), batch_size=10)\n@@ -384,7 +384,7 @@ class TestModelRevive(ReviveTestBase):\n       keras_load.load(self.path, compile=False)\n \n   def test_load_compiled_metrics(self):\n-    model = testing_utils.get_small_sequential_mlp(1, 3)\n+    model = test_utils.get_small_sequential_mlp(1, 3)\n \n     # Compile with dense categorical accuracy\n     model.compile('rmsprop', 'mse', 'acc')\n\n@@ -27,16 +27,15 @@ import sys\n \n from absl.testing import parameterized\n import keras\n-from keras import combinations\n-from keras import keras_parameterized\n from keras import regularizers\n-from keras import testing_utils\n from keras.feature_column.dense_features import DenseFeatures\n from keras.protobuf import saved_metadata_pb2\n from keras.protobuf import versions_pb2\n from keras.saving.saved_model import json_utils\n from keras.saving.saved_model import load as keras_load\n from keras.saving.saved_model import save_impl as keras_save\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.utils import control_flow_util\n from keras.utils import generic_utils\n from keras.utils import tf_contextlib\n@@ -99,8 +98,8 @@ class GlobalLayerThatShouldFailIfNotAdded(keras.layers.Layer):\n   _must_restore_from_config = True\n \n \n-@keras_parameterized.run_all_keras_modes\n-class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class TestSavedModelFormatAllModes(test_combinations.TestCase):\n \n   def _save_model_dir(self, dirname='saved_model'):\n     temp_dir = self.get_temp_dir()\n@@ -108,7 +107,7 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n     return os.path.join(temp_dir, dirname)\n \n   def _get_model(self):\n-    model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n+    model = test_utils.get_small_mlp(1, 4, input_dim=3)\n     model.layers[-1].activity_regularizer = regularizers.get('l2')\n     model.activity_regularizer = regularizers.get('l2')\n     model.compile(\n@@ -163,14 +162,14 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n           sorted(self.evaluate(model.get_losses_for(input_arr))),\n           sorted(self.evaluate(loaded.get_losses_for(input_arr))))\n \n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_with_all_model_types\n   def test_model_save_and_load(self):\n     model = self._get_model()\n     self._train_model(model, use_dataset=False)\n     loaded = self._save_and_load(model)\n     self._test_evaluation(model, loaded)\n \n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_with_all_model_types\n   def test_model_save_and_load_dataset(self):\n     model = self._get_model()\n     self._train_model(model, use_dataset=True)\n@@ -208,7 +207,7 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n       self.assertAllClose(self.evaluate(getattr(layer, attr)),\n                           self.evaluate(getattr(loaded, attr)))\n \n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_with_all_model_types\n   def test_trainable_layers(self):\n     \"\"\"Tests that trainable status of individual layers is preserved.\"\"\"\n     model = model = self._get_model()\n@@ -224,7 +223,7 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n   def test_trainable_custom_model_false(self):\n     \"\"\"Tests that overall False trainable status of Model is preserved.\"\"\"\n     # Set all layers to *not* be trainable.\n-    model = testing_utils.SmallSubclassMLP(1, 4, trainable=False)\n+    model = test_utils.SmallSubclassMLP(1, 4, trainable=False)\n     model.compile(loss='mse', optimizer='rmsprop')\n     self._train_model(model, use_dataset=False)\n     loaded = self._save_and_load(model)\n@@ -266,7 +265,7 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n     layer = LayerWithLearningPhase()\n     layer.build([None, None])\n     saved_model_dir = self._save_model_dir()\n-    model = testing_utils.get_model_from_layers(\n+    model = test_utils.get_model_from_layers(\n         [layer], input_shape=[None], model_type='functional')\n     model.save(saved_model_dir, save_format='tf')\n     loaded_model = keras_load.load(saved_model_dir)\n@@ -286,9 +285,9 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n         tf.zeros((4, 3)),\n         loaded(input_arr, training=tf.constant(True)))\n \n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_with_all_model_types\n   def test_standard_loader(self):\n-    model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n+    model = test_utils.get_small_mlp(1, 4, input_dim=3)\n     model.activity_regularizer = regularizers.get('l2')\n     def eager_loss():\n       return tf.reduce_sum(model.weights[0])\n@@ -314,7 +313,7 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n     self.assertAllClose(self.evaluate(model(input_arr)),\n                         self.evaluate(loaded(input_arr, training=False)))\n \n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_with_all_model_types\n   def test_compiled_model(self):\n     # TODO(b/134519980): Issue with model.fit if the model call function uses\n     # a tf.function (Graph mode only).\n@@ -324,7 +323,7 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n     input_arr = np.random.random((1, 3))\n     target_arr = np.random.random((1, 4))\n \n-    model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n+    model = test_utils.get_small_mlp(1, 4, input_dim=3)\n     expected_predict = model.predict(input_arr)\n \n     # Compile and save model.\n@@ -364,7 +363,7 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n \n     layer = LayerWithNestedSpec()\n     saved_model_dir = self._save_model_dir()\n-    model = testing_utils.get_model_from_layers(\n+    model = test_utils.get_model_from_layers(\n         [layer], model_type='subclass')\n     model({'a': tf.constant([[2, 4]]),\n            'b': tf.ones([1, 2, 3], dtype=tf.int32)})\n@@ -383,7 +382,7 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n \n     layer = LayerThatShouldFailIfNotAdded()\n     saved_model_dir = self._save_model_dir()\n-    model = testing_utils.get_model_from_layers(\n+    model = test_utils.get_model_from_layers(\n         [layer], input_shape=[3], model_type='functional')\n     model.save(saved_model_dir, save_format='tf')\n     with self.assertRaisesRegex(ValueError,\n@@ -396,7 +395,7 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n       _must_restore_from_config = True\n \n     layer = LayerThatShouldFailIfNotAdded()\n-    model = testing_utils.get_model_from_layers(\n+    model = test_utils.get_model_from_layers(\n         [layer], input_shape=[3], model_type='functional')\n     saved_model_dir = self._save_model_dir()\n     model.save(saved_model_dir, save_format='tf')\n@@ -407,7 +406,7 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n   def test_must_restore_from_config_registration(self):\n     layer = GlobalLayerThatShouldFailIfNotAdded()\n     saved_model_dir = self._save_model_dir()\n-    model = testing_utils.get_model_from_layers(\n+    model = test_utils.get_model_from_layers(\n         [layer], input_shape=[3], model_type='functional')\n     model.save(saved_model_dir, save_format='tf')\n     _ = keras_load.load(saved_model_dir)\n@@ -745,11 +744,11 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n   def _testAddUpdate(self, scope):\n     with scope:\n       layer_with_update = LayerWithUpdate()\n-      model = testing_utils.get_model_from_layers([layer_with_update],\n+      model = test_utils.get_model_from_layers([layer_with_update],\n                                                input_shape=(3,))\n \n       x = np.ones((10, 3))\n-      if testing_utils.get_model_type() == 'subclass':\n+      if test_utils.get_model_type() == 'subclass':\n         model.predict(x, batch_size=10)\n       self.evaluate(tf.compat.v1.variables_initializer(model.variables))\n       saved_model_dir = self._save_model_dir()\n@@ -764,14 +763,14 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n     loaded.fit(x, x, batch_size=10)\n     self.assertEqual(self.evaluate(loaded_layer.v), 1.)\n \n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_with_all_model_types\n   def testSaveLayerWithUpdates(self):\n     @tf_contextlib.contextmanager\n     def nullcontextmanager():\n       yield\n     self._testAddUpdate(nullcontextmanager())\n \n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_with_all_model_types\n   def testSaveInStrategyScope(self):\n     self._testAddUpdate(tf.distribute.MirroredStrategy().scope())\n \n@@ -809,7 +808,7 @@ class TestSavedModelFormatAllModes(keras_parameterized.TestCase):\n \n     model = keras.Model(x, y)\n     model.compile('rmsprop', 'mse',\n-                  run_eagerly=testing_utils.should_run_eagerly())\n+                  run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch(\n         np.zeros((batch, timesteps, input_dim)).astype('float32'),\n         np.zeros((batch, 64)).astype('float32'))\n@@ -1078,7 +1077,7 @@ class TestSavedModelFormat(tf.test.TestCase):\n       loaded.attached_layer(tf.constant([1.]))\n \n   def test_load_non_keras_saved_model(self):\n-    model = testing_utils.get_small_functional_mlp(1, 4, input_dim=3)\n+    model = test_utils.get_small_functional_mlp(1, 4, input_dim=3)\n     saved_model_dir = self._save_model_dir()\n     tf.saved_model.save(model, saved_model_dir)\n     with self.assertRaisesRegex(ValueError, 'Unable to create a Keras model'):\n@@ -1169,7 +1168,8 @@ class TestLayerCallTracing(tf.test.TestCase, parameterized.TestCase):\n \n     assert_num_traces(LayerWithChildLayer, training_keyword=False)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_maintains_losses(self):\n     layer = LayerWithLoss()\n     layer(np.ones((2, 3)))\n@@ -1193,7 +1193,7 @@ class CustomMeanMetric(keras.metrics.Mean):\n     super(CustomMeanMetric, self).update_state(*args)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class MetricTest(tf.test.TestCase, parameterized.TestCase):\n \n   def _save_model_dir(self, dirname='saved_model'):\n@@ -1214,7 +1214,7 @@ class MetricTest(tf.test.TestCase, parameterized.TestCase):\n                                  shape=(1, 5),\n                                  test_sample_weight=True):\n     with self.cached_session():\n-      model = testing_utils.get_model_from_layers(\n+      model = test_utils.get_model_from_layers(\n           [keras.layers.Layer()], input_shape=[3], model_type='functional')\n       model.saved_metric = metric\n       model.save(save_dir, save_format='tf')\n@@ -1340,7 +1340,7 @@ class MetricTest(tf.test.TestCase, parameterized.TestCase):\n       self._test_metric_save_and_load(\n           metric, self._save_model_dir(), 1, test_sample_weight=False)\n \n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_with_all_model_types\n   def test_custom_metric_model(self):\n     # TODO(b/134519980): Issue with `model.fit` if the model call function uses\n     # a `tf.function` in graph mode.\n@@ -1357,7 +1357,7 @@ class MetricTest(tf.test.TestCase, parameterized.TestCase):\n       del y_true, y_pred\n       return 0\n \n-    model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n+    model = test_utils.get_small_mlp(1, 4, input_dim=3)\n     model.compile(loss='mse', optimizer='SGD',\n                   metrics=[CustomMetric(), zero_metric])\n     model.fit(x, y)\n\n@@ -22,16 +22,15 @@ import numpy as np\n \n import keras\n from keras import backend\n-from keras import combinations\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.engine import sequential\n from keras.feature_column import dense_features\n from keras.optimizers.optimizer_v2 import gradient_descent\n from keras.saving import saving_utils\n \n \n-class TraceModelCallTest(keras_parameterized.TestCase):\n+class TraceModelCallTest(test_combinations.TestCase):\n \n   def _assert_all_close(self, expected, actual):\n     if not tf.executing_eagerly():\n@@ -41,11 +40,11 @@ class TraceModelCallTest(keras_parameterized.TestCase):\n     else:\n       self.assertAllClose(expected, actual)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_trace_model_outputs(self):\n-    input_dim = 5 if testing_utils.get_model_type() == 'functional' else None\n-    model = testing_utils.get_small_mlp(10, 3, input_dim)\n+    input_dim = 5 if test_utils.get_model_type() == 'functional' else None\n+    model = test_utils.get_small_mlp(10, 3, input_dim)\n     inputs = tf.ones((8, 5))\n \n     if input_dim is None:\n@@ -62,15 +61,15 @@ class TraceModelCallTest(keras_parameterized.TestCase):\n \n     self._assert_all_close(expected_outputs, signature_outputs)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_trace_model_outputs_after_fitting(self):\n-    input_dim = 5 if testing_utils.get_model_type() == 'functional' else None\n-    model = testing_utils.get_small_mlp(10, 3, input_dim)\n+    input_dim = 5 if test_utils.get_model_type() == 'functional' else None\n+    model = test_utils.get_small_mlp(10, 3, input_dim)\n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(\n         x=np.random.random((8, 5)).astype(np.float32),\n         y=np.random.random((8, 3)).astype(np.float32),\n@@ -87,8 +86,8 @@ class TraceModelCallTest(keras_parameterized.TestCase):\n \n     self._assert_all_close(expected_outputs, signature_outputs)\n \n-  @keras_parameterized.run_with_all_model_types(exclude_models='sequential')\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types(exclude_models='sequential')\n+  @test_combinations.run_all_keras_modes\n   def test_trace_multi_io_model_outputs(self):\n     input_dim = 5\n     num_classes = 3\n@@ -102,21 +101,21 @@ class TraceModelCallTest(keras_parameterized.TestCase):\n     branch_a = [input_a, dense]\n     branch_b = [input_b, dense, dense2, dropout]\n \n-    model = testing_utils.get_multi_io_model(branch_a, branch_b)\n+    model = test_utils.get_multi_io_model(branch_a, branch_b)\n \n     input_a_ts = tf.constant(\n         np.random.random((10, input_dim)).astype(np.float32))\n     input_b_ts = tf.constant(\n         np.random.random((10, input_dim)).astype(np.float32))\n \n-    if testing_utils.get_model_type() == 'subclass':\n+    if test_utils.get_model_type() == 'subclass':\n       with self.assertRaisesRegex(ValueError, '.*input shape is not availabl*'):\n         saving_utils.trace_model_call(model)\n \n     model.compile(\n         optimizer='sgd',\n         loss='mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(x=[np.random.random((8, input_dim)).astype(np.float32),\n                  np.random.random((8, input_dim)).astype(np.float32)],\n               y=[np.random.random((8, num_classes)).astype(np.float32),\n@@ -128,7 +127,7 @@ class TraceModelCallTest(keras_parameterized.TestCase):\n     # ConcreteFunction. For some reason V1 models defines the inputs as a list,\n     # while V2 models sets the inputs as a tuple.\n     if (not tf.executing_eagerly() and\n-        testing_utils.get_model_type() != 'functional'):\n+        test_utils.get_model_type() != 'functional'):\n       signature_outputs = fn([input_a_ts, input_b_ts])\n     else:\n       signature_outputs = fn((input_a_ts, input_b_ts))\n@@ -142,7 +141,8 @@ class TraceModelCallTest(keras_parameterized.TestCase):\n       expected_outputs = {'output_1': outputs[0], 'output_2': outputs[1]}\n     self._assert_all_close(expected_outputs, signature_outputs)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_trace_features_layer(self):\n     columns = [tf.feature_column.numeric_column('x')]\n     model = sequential.Sequential([dense_features.DenseFeatures(columns)])\n@@ -162,9 +162,10 @@ class TraceModelCallTest(keras_parameterized.TestCase):\n     fn = saving_utils.trace_model_call(model)\n     self.assertAllClose({'output_1': [[1., 2.]]}, fn(model_input))\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_specify_input_signature(self):\n-    model = testing_utils.get_small_sequential_mlp(10, 3, None)\n+    model = test_utils.get_small_sequential_mlp(10, 3, None)\n     inputs = tf.ones((8, 5))\n \n     with self.assertRaisesRegex(ValueError, '.*input shape is not availabl*'):\n@@ -179,7 +180,8 @@ class TraceModelCallTest(keras_parameterized.TestCase):\n       expected_outputs = {'output_1': model(inputs)}\n     self._assert_all_close(expected_outputs, signature_outputs)\n \n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=['graph', 'eager']))\n   def test_subclassed_model_with_input_signature(self):\n \n     class Model(keras.Model):\n@@ -203,15 +205,15 @@ class TraceModelCallTest(keras_parameterized.TestCase):\n     signature_outputs = fn([x, y])\n     self._assert_all_close(expected_outputs, signature_outputs)\n \n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n   def test_model_with_fixed_input_dim(self):\n     \"\"\"Ensure that the batch_dim is removed when saving.\n \n     When serving or retraining, it is important to reset the batch dim.\n     This can be an issue inside of tf.function. See b/132783590 for context.\n     \"\"\"\n-    model = testing_utils.get_small_mlp(10, 3, 5)\n+    model = test_utils.get_small_mlp(10, 3, 5)\n \n     loss_object = keras.losses.MeanSquaredError()\n     optimizer = gradient_descent.SGD()\n@@ -301,9 +303,9 @@ class BasicAutographedMetricModel(keras.models.Model):\n     return self._layer(inputs)\n \n \n-@keras_parameterized.run_with_all_model_types\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class ModelSaveTest(keras_parameterized.TestCase):\n+@test_combinations.run_with_all_model_types\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class ModelSaveTest(test_combinations.TestCase):\n \n   def test_model_save_preserves_autograph(self):\n     model = BasicAutographedMetricModel()\n@@ -331,10 +333,10 @@ class ModelSaveTest(keras_parameterized.TestCase):\n \n   def test_model_save(self):\n     input_dim = 5\n-    model = testing_utils.get_small_mlp(10, 3, input_dim)\n+    model = test_utils.get_small_mlp(10, 3, input_dim)\n     inputs = tf.ones((8, 5))\n \n-    if testing_utils.get_model_type() == 'subclass':\n+    if test_utils.get_model_type() == 'subclass':\n       model._set_inputs(inputs)\n \n     save_dir = os.path.join(self.get_temp_dir(), 'saved_model')\n@@ -352,7 +354,7 @@ class ModelSaveTest(keras_parameterized.TestCase):\n                                           {input_name: np.ones((8, 5))}))\n \n \n-class ExtractModelMetricsTest(keras_parameterized.TestCase):\n+class ExtractModelMetricsTest(test_combinations.TestCase):\n \n   def test_extract_model_metrics(self):\n     # saving_utils.extract_model_metrics is used in V1 only API\n@@ -394,7 +396,7 @@ class ExtractModelMetricsTest(keras_parameterized.TestCase):\n       self.assertEqual(set(extract_metric_names), set(extract_metrics.keys()))\n \n \n-class UnbuiltModelSavingErrorMessageTest(keras_parameterized.TestCase):\n+class UnbuiltModelSavingErrorMessageTest(test_combinations.TestCase):\n \n   def setUp(self):\n     super(UnbuiltModelSavingErrorMessageTest, self).setUp()\n\n@@ -0,0 +1,14 @@\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Run doctests for tensorflow.\"\"\"\n+\"\"\"Run doctests for Keras.\"\"\"\n \n import doctest\n import re\n\n@@ -17,7 +17,7 @@\n import doctest\n \n from absl.testing import parameterized\n-from keras.utils import keras_doctest_lib\n+from keras.testing_infra import keras_doctest_lib\n import tensorflow.compat.v2 as tf\n \n \n\n@@ -1,4 +1,4 @@\n-# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -13,6 +13,7 @@\n # limitations under the License.\n # ==============================================================================\n \"\"\"Utilities for unit-testing Keras.\"\"\"\n+# pylint: disable=g-bad-import-order\n \n import tensorflow.compat.v2 as tf\n \n@@ -24,13 +25,15 @@ import unittest\n from absl.testing import parameterized\n \n import keras\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n \n try:\n   import h5py  # pylint:disable=g-import-not-at-top\n except ImportError:\n   h5py = None\n \n+KERAS_MODEL_TYPES = ['functional', 'subclass', 'sequential']\n+\n \n class TestCase(tf.test.TestCase, parameterized.TestCase):\n \n@@ -45,7 +48,7 @@ def run_with_all_saved_model_formats(\n   \"\"\"Execute the decorated test with all Keras saved model formats).\n \n   This decorator is intended to be applied either to individual test methods in\n-  a `keras_parameterized.TestCase` class, or directly to a test class that\n+  a `test_combinations.TestCase` class, or directly to a test class that\n   extends it. Doing so will cause the contents of the individual test\n   method (or all test methods in the class) to be executed multiple times - once\n   for each Keras saved model format.\n@@ -64,11 +67,11 @@ def run_with_all_saved_model_formats(\n   For example, consider the following unittest:\n \n   ```python\n-  class MyTests(testing_utils.KerasTestCase):\n+  class MyTests(test_utils.KerasTestCase):\n \n-    @testing_utils.run_with_all_saved_model_formats\n+    @test_utils.run_with_all_saved_model_formats\n     def test_foo(self):\n-      save_format = testing_utils.get_save_format()\n+      save_format = test_utils.get_save_format()\n       saved_model_dir = '/tmp/saved_model/'\n       model = keras.models.Sequential()\n       model.add(keras.layers.Dense(2, input_shape=(3,)))\n@@ -88,11 +91,11 @@ def run_with_all_saved_model_formats(\n   We can also annotate the whole class if we want this to apply to all tests in\n   the class:\n   ```python\n-  @testing_utils.run_with_all_saved_model_formats\n-  class MyTests(testing_utils.KerasTestCase):\n+  @test_utils.run_with_all_saved_model_formats\n+  class MyTests(test_utils.KerasTestCase):\n \n     def test_foo(self):\n-      save_format = testing_utils.get_save_format()\n+      save_format = test_utils.get_save_format()\n       saved_model_dir = '/tmp/saved_model/'\n       model = keras.models.Sequential()\n       model.add(keras.layers.Dense(2, input_shape=(3,)))\n@@ -152,17 +155,17 @@ def run_with_all_saved_model_formats(\n \n \n def _test_h5_saved_model_format(f, test_or_class, *args, **kwargs):\n-  with testing_utils.saved_model_format_scope('h5'):\n+  with test_utils.saved_model_format_scope('h5'):\n     f(test_or_class, *args, **kwargs)\n \n \n def _test_tf_saved_model_format(f, test_or_class, *args, **kwargs):\n-  with testing_utils.saved_model_format_scope('tf'):\n+  with test_utils.saved_model_format_scope('tf'):\n     f(test_or_class, *args, **kwargs)\n \n \n def _test_tf_saved_model_format_no_traces(f, test_or_class, *args, **kwargs):\n-  with testing_utils.saved_model_format_scope('tf', save_traces=False):\n+  with test_utils.saved_model_format_scope('tf', save_traces=False):\n     f(test_or_class, *args, **kwargs)\n \n \n@@ -181,7 +184,7 @@ def run_with_all_model_types(\n   \"\"\"Execute the decorated test with all Keras model types.\n \n   This decorator is intended to be applied either to individual test methods in\n-  a `keras_parameterized.TestCase` class, or directly to a test class that\n+  a `test_combinations.TestCase` class, or directly to a test class that\n   extends it. Doing so will cause the contents of the individual test\n   method (or all test methods in the class) to be executed multiple times - once\n   for each Keras model type.\n@@ -198,12 +201,12 @@ def run_with_all_model_types(\n   For example, consider the following unittest:\n \n   ```python\n-  class MyTests(testing_utils.KerasTestCase):\n+  class MyTests(test_utils.KerasTestCase):\n \n-    @testing_utils.run_with_all_model_types(\n+    @test_utils.run_with_all_model_types(\n       exclude_models = ['sequential'])\n     def test_foo(self):\n-      model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n+      model = test_utils.get_small_mlp(1, 4, input_dim=3)\n       optimizer = RMSPropOptimizer(learning_rate=0.001)\n       loss = 'mse'\n       metrics = ['mae']\n@@ -227,11 +230,11 @@ def run_with_all_model_types(\n   We can also annotate the whole class if we want this to apply to all tests in\n   the class:\n   ```python\n-  @testing_utils.run_with_all_model_types(exclude_models = ['sequential'])\n-  class MyTests(testing_utils.KerasTestCase):\n+  @test_utils.run_with_all_model_types(exclude_models = ['sequential'])\n+  class MyTests(test_utils.KerasTestCase):\n \n     def test_foo(self):\n-      model = testing_utils.get_small_mlp(1, 4, input_dim=3)\n+      model = test_utils.get_small_mlp(1, 4, input_dim=3)\n       optimizer = RMSPropOptimizer(learning_rate=0.001)\n       loss = 'mse'\n       metrics = ['mae']\n@@ -292,17 +295,17 @@ def run_with_all_model_types(\n \n \n def _test_functional_model_type(f, test_or_class, *args, **kwargs):\n-  with testing_utils.model_type_scope('functional'):\n+  with test_utils.model_type_scope('functional'):\n     f(test_or_class, *args, **kwargs)\n \n \n def _test_subclass_model_type(f, test_or_class, *args, **kwargs):\n-  with testing_utils.model_type_scope('subclass'):\n+  with test_utils.model_type_scope('subclass'):\n     f(test_or_class, *args, **kwargs)\n \n \n def _test_sequential_model_type(f, test_or_class, *args, **kwargs):\n-  with testing_utils.model_type_scope('sequential'):\n+  with test_utils.model_type_scope('sequential'):\n     f(test_or_class, *args, **kwargs)\n \n \n@@ -314,7 +317,7 @@ def run_all_keras_modes(test_or_class=None,\n   \"\"\"Execute the decorated test with all keras execution modes.\n \n   This decorator is intended to be applied either to individual test methods in\n-  a `keras_parameterized.TestCase` class, or directly to a test class that\n+  a `test_combinations.TestCase` class, or directly to a test class that\n   extends it. Doing so will cause the contents of the individual test\n   method (or all test methods in the class) to be executed multiple times -\n   once executing in legacy graph mode, once running eagerly and with\n@@ -330,17 +333,17 @@ def run_all_keras_modes(test_or_class=None,\n   For example, consider the following unittest:\n \n   ```python\n-  class MyTests(testing_utils.KerasTestCase):\n+  class MyTests(test_utils.KerasTestCase):\n \n-    @testing_utils.run_all_keras_modes\n+    @test_utils.run_all_keras_modes\n     def test_foo(self):\n-      model = testing_utils.get_small_functional_mlp(1, 4, input_dim=3)\n+      model = test_utils.get_small_functional_mlp(1, 4, input_dim=3)\n       optimizer = RMSPropOptimizer(learning_rate=0.001)\n       loss = 'mse'\n       metrics = ['mae']\n       model.compile(\n           optimizer, loss, metrics=metrics,\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n \n       inputs = np.zeros((10, 3))\n       targets = np.zeros((10, 4))\n@@ -412,20 +415,20 @@ def run_all_keras_modes(test_or_class=None,\n \n def _v1_session_test(f, test_or_class, config, *args, **kwargs):\n   with tf.compat.v1.get_default_graph().as_default():\n-    with testing_utils.run_eagerly_scope(False):\n+    with test_utils.run_eagerly_scope(False):\n       with test_or_class.test_session(config=config):\n         f(test_or_class, *args, **kwargs)\n \n \n def _v2_eager_test(f, test_or_class, *args, **kwargs):\n   with tf.__internal__.eager_context.eager_mode():\n-    with testing_utils.run_eagerly_scope(True):\n+    with test_utils.run_eagerly_scope(True):\n       f(test_or_class, *args, **kwargs)\n \n \n def _v2_function_test(f, test_or_class, *args, **kwargs):\n   with tf.__internal__.eager_context.eager_mode():\n-    with testing_utils.run_eagerly_scope(False):\n+    with test_utils.run_eagerly_scope(False):\n       f(test_or_class, *args, **kwargs)\n \n \n@@ -446,7 +449,7 @@ def _test_or_class_decorator(test_or_class, single_method_decorator):\n   Args:\n     test_or_class: A test method (that may have already been decorated with a\n       parameterized decorator, or a test class that extends\n-      keras_parameterized.TestCase\n+      test_combinations.TestCase\n     single_method_decorator:\n       A parameterized decorator intended for a single test method.\n   Returns:\n@@ -473,3 +476,85 @@ def _test_or_class_decorator(test_or_class, single_method_decorator):\n     return _decorate_test_or_class(test_or_class)\n \n   return _decorate_test_or_class\n+\n+\n+def keras_mode_combinations(mode=None, run_eagerly=None):\n+  \"\"\"Returns the default test combinations for tf.keras tests.\n+\n+  Note that if tf2 is enabled, then v1 session test will be skipped.\n+\n+  Args:\n+    mode: List of modes to run the tests. The valid options are 'graph' and\n+      'eager'. Default to ['graph', 'eager'] if not specified. If a empty list\n+      is provide, then the test will run under the context based on tf's\n+      version, eg graph for v1 and eager for v2.\n+    run_eagerly: List of `run_eagerly` value to be run with the tests.\n+      Default to [True, False] if not specified. Note that for `graph` mode,\n+      run_eagerly value will only be False.\n+\n+  Returns:\n+    A list contains all the combinations to be used to generate test cases.\n+  \"\"\"\n+  if mode is None:\n+    mode = ['eager'] if tf.__internal__.tf2.enabled() else ['graph', 'eager']\n+  if run_eagerly is None:\n+    run_eagerly = [True, False]\n+  result = []\n+  if 'eager' in mode:\n+    result += tf.__internal__.test.combinations.combine(mode=['eager'], run_eagerly=run_eagerly)\n+  if 'graph' in mode:\n+    result += tf.__internal__.test.combinations.combine(mode=['graph'], run_eagerly=[False])\n+  return result\n+\n+\n+def keras_model_type_combinations():\n+  return tf.__internal__.test.combinations.combine(model_type=KERAS_MODEL_TYPES)\n+\n+\n+class KerasModeCombination(tf.__internal__.test.combinations.TestCombination):\n+  \"\"\"Combination for Keras test mode.\n+\n+  It by default includes v1_session, v2_eager and v2_tf_function.\n+  \"\"\"\n+\n+  def context_managers(self, kwargs):\n+    run_eagerly = kwargs.pop('run_eagerly', None)\n+\n+    if run_eagerly is not None:\n+      return [test_utils.run_eagerly_scope(run_eagerly)]\n+    else:\n+      return []\n+\n+  def parameter_modifiers(self):\n+    return [tf.__internal__.test.combinations.OptionalParameter('run_eagerly')]\n+\n+\n+class KerasModelTypeCombination(tf.__internal__.test.combinations.TestCombination):\n+  \"\"\"Combination for Keras model types when doing model test.\n+\n+  It by default includes 'functional', 'subclass', 'sequential'.\n+\n+  Various methods in `testing_utils` to get models will auto-generate a model\n+  of the currently active Keras model type. This allows unittests to confirm\n+  the equivalence between different Keras models.\n+  \"\"\"\n+\n+  def context_managers(self, kwargs):\n+    model_type = kwargs.pop('model_type', None)\n+    if model_type in KERAS_MODEL_TYPES:\n+      return [test_utils.model_type_scope(model_type)]\n+    else:\n+      return []\n+\n+  def parameter_modifiers(self):\n+    return [tf.__internal__.test.combinations.OptionalParameter('model_type')]\n+\n+\n+_defaults = tf.__internal__.test.combinations.generate.keywords['test_combinations']\n+generate = functools.partial(\n+    tf.__internal__.test.combinations.generate,\n+    test_combinations=_defaults +\n+    (KerasModeCombination(), KerasModelTypeCombination()))\n+combine = tf.__internal__.test.combinations.combine\n+times = tf.__internal__.test.combinations.times\n+NamedObject = tf.__internal__.test.combinations.NamedObject\n\n@@ -12,34 +12,179 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for Keras testing_utils.\"\"\"\n+\"\"\"Tests for Keras test_utils.\"\"\"\n \n import tensorflow.compat.v2 as tf\n \n import unittest\n-\n from absl.testing import parameterized\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras import models as keras_models\n+from keras.testing_infra import test_utils\n+from keras.testing_infra import test_combinations\n \n \n-class KerasParameterizedTest(keras_parameterized.TestCase):\n+class CombinationsTest(tf.test.TestCase):\n+\n+  def test_run_all_keras_modes(self):\n+    test_params = []\n+\n+    class ExampleTest(parameterized.TestCase):\n+\n+      def runTest(self):\n+        pass\n+\n+      @test_combinations.generate(test_combinations.keras_mode_combinations())\n+      def testBody(self):\n+        mode = \"eager\" if tf.executing_eagerly() else \"graph\"\n+        should_run_eagerly = test_utils.should_run_eagerly()\n+        test_params.append((mode, should_run_eagerly))\n+\n+    e = ExampleTest()\n+    if not tf.__internal__.tf2.enabled():\n+      e.testBody_test_mode_graph_runeagerly_False()\n+    e.testBody_test_mode_eager_runeagerly_True()\n+    e.testBody_test_mode_eager_runeagerly_False()\n+\n+    if not tf.__internal__.tf2.enabled():\n+      self.assertLen(test_params, 3)\n+      self.assertAllEqual(test_params, [\n+          (\"graph\", False),\n+          (\"eager\", True),\n+          (\"eager\", False),\n+      ])\n+\n+      ts = unittest.makeSuite(ExampleTest)\n+      res = unittest.TestResult()\n+      ts.run(res)\n+      self.assertLen(test_params, 6)\n+    else:\n+      self.assertLen(test_params, 2)\n+      self.assertAllEqual(test_params, [\n+          (\"eager\", True),\n+          (\"eager\", False),\n+      ])\n+\n+      ts = unittest.makeSuite(ExampleTest)\n+      res = unittest.TestResult()\n+      ts.run(res)\n+      self.assertLen(test_params, 4)\n+\n+  def test_generate_keras_mode_eager_only(self):\n+    result = test_combinations.keras_mode_combinations(mode=[\"eager\"])\n+    self.assertLen(result, 2)\n+    self.assertEqual(result[0], {\"mode\": \"eager\", \"run_eagerly\": True})\n+    self.assertEqual(result[1], {\"mode\": \"eager\", \"run_eagerly\": False})\n+\n+  def test_generate_keras_mode_skip_run_eagerly(self):\n+    result = test_combinations.keras_mode_combinations(run_eagerly=[False])\n+    if tf.__internal__.tf2.enabled():\n+      self.assertLen(result, 1)\n+      self.assertEqual(result[0], {\"mode\": \"eager\", \"run_eagerly\": False})\n+    else:\n+      self.assertLen(result, 2)\n+      self.assertEqual(result[0], {\"mode\": \"eager\", \"run_eagerly\": False})\n+      self.assertEqual(result[1], {\"mode\": \"graph\", \"run_eagerly\": False})\n+\n+  def test_run_all_keras_model_types(self):\n+    model_types = []\n+    models = []\n+\n+    class ExampleTest(parameterized.TestCase):\n+\n+      def runTest(self):\n+        pass\n+\n+      @test_combinations.generate(\n+          test_combinations.keras_model_type_combinations())\n+      def testBody(self):\n+        model_types.append(test_utils.get_model_type())\n+        models.append(test_utils.get_small_mlp(1, 4, input_dim=3))\n+\n+    e = ExampleTest()\n+    e.testBody_test_modeltype_functional()\n+    e.testBody_test_modeltype_subclass()\n+    e.testBody_test_modeltype_sequential()\n+\n+    self.assertLen(model_types, 3)\n+    self.assertAllEqual(model_types, [\n+        \"functional\",\n+        \"subclass\",\n+        \"sequential\"\n+    ])\n+\n+    # Validate that the models are what they should be\n+    self.assertTrue(models[0]._is_graph_network)\n+    self.assertFalse(models[1]._is_graph_network)\n+    self.assertNotIsInstance(models[0], keras_models.Sequential)\n+    self.assertNotIsInstance(models[1], keras_models.Sequential)\n+    self.assertIsInstance(models[2], keras_models.Sequential)\n+\n+    ts = unittest.makeSuite(ExampleTest)\n+    res = unittest.TestResult()\n+    ts.run(res)\n+\n+    self.assertLen(model_types, 6)\n+\n+  def test_combine_combinations(self):\n+    test_cases = []\n+\n+    @test_combinations.generate(test_combinations.times(\n+        test_combinations.keras_mode_combinations(),\n+        test_combinations.keras_model_type_combinations()))\n+    class ExampleTest(parameterized.TestCase):\n+\n+      def runTest(self):\n+        pass\n+\n+      @parameterized.named_parameters(dict(testcase_name=\"_arg\",\n+                                           arg=True))\n+      def testBody(self, arg):\n+        del arg\n+        mode = \"eager\" if tf.executing_eagerly() else \"graph\"\n+        should_run_eagerly = test_utils.should_run_eagerly()\n+        test_cases.append((mode, should_run_eagerly,\n+                           test_utils.get_model_type()))\n+\n+    ts = unittest.makeSuite(ExampleTest)\n+    res = unittest.TestResult()\n+    ts.run(res)\n+\n+    expected_combinations = [\n+        (\"eager\", False, \"functional\"),\n+        (\"eager\", False, \"sequential\"),\n+        (\"eager\", False, \"subclass\"),\n+        (\"eager\", True, \"functional\"),\n+        (\"eager\", True, \"sequential\"),\n+        (\"eager\", True, \"subclass\"),\n+    ]\n+\n+    if not tf.__internal__.tf2.enabled():\n+      expected_combinations.extend([\n+          (\"graph\", False, \"functional\"),\n+          (\"graph\", False, \"sequential\"),\n+          (\"graph\", False, \"subclass\"),\n+      ])\n+\n+    self.assertAllEqual(sorted(test_cases), expected_combinations)\n+\n+\n+class KerasParameterizedTest(test_combinations.TestCase):\n \n   def test_run_with_all_model_types(self):\n     model_types = []\n     models = []\n \n-    class ExampleTest(keras_parameterized.TestCase):\n+    class ExampleTest(test_combinations.TestCase):\n \n       def runTest(self):\n         pass\n \n-      @keras_parameterized.run_with_all_model_types\n+      @test_combinations.run_with_all_model_types\n       def testBody(self):\n-        model_types.append(testing_utils.get_model_type())\n-        models.append(testing_utils.get_small_mlp(1, 4, input_dim=3))\n+        model_types.append(test_utils.get_model_type())\n+        models.append(test_utils.get_small_mlp(1, 4, input_dim=3))\n \n     e = ExampleTest()\n     e.testBody_functional()\n@@ -70,19 +215,19 @@ class KerasParameterizedTest(keras_parameterized.TestCase):\n     model_types = []\n     models = []\n \n-    class ExampleTest(keras_parameterized.TestCase):\n+    class ExampleTest(test_combinations.TestCase):\n \n       def runTest(self):\n         pass\n \n-      @keras_parameterized.run_with_all_model_types\n+      @test_combinations.run_with_all_model_types\n       @parameterized.named_parameters(\n           [dict(testcase_name=\"_0\", with_brackets=True),\n            dict(testcase_name=\"_1\", with_brackets=False)])\n       def testBody(self, with_brackets):\n         with_brackets = \"with_brackets\" if with_brackets else \"without_brackets\"\n-        model_types.append((with_brackets, testing_utils.get_model_type()))\n-        models.append(testing_utils.get_small_mlp(1, 4, input_dim=3))\n+        model_types.append((with_brackets, test_utils.get_model_type()))\n+        models.append(test_utils.get_small_mlp(1, 4, input_dim=3))\n \n     e = ExampleTest()\n     e.testBody_0_functional()\n@@ -119,15 +264,15 @@ class KerasParameterizedTest(keras_parameterized.TestCase):\n     model_types = []\n     models = []\n \n-    class ExampleTest(keras_parameterized.TestCase):\n+    class ExampleTest(test_combinations.TestCase):\n \n       def runTest(self):\n         pass\n \n-      @keras_parameterized.run_with_all_model_types(exclude_models=\"sequential\")\n+      @test_combinations.run_with_all_model_types(exclude_models=\"sequential\")\n       def testBody(self):\n-        model_types.append(testing_utils.get_model_type())\n-        models.append(testing_utils.get_small_mlp(1, 4, input_dim=3))\n+        model_types.append(test_utils.get_model_type())\n+        models.append(test_utils.get_small_mlp(1, 4, input_dim=3))\n \n     e = ExampleTest()\n     if hasattr(e, \"testBody_functional\"):\n@@ -159,16 +304,16 @@ class KerasParameterizedTest(keras_parameterized.TestCase):\n     model_types = []\n     models = []\n \n-    class ExampleTest(keras_parameterized.TestCase):\n+    class ExampleTest(test_combinations.TestCase):\n \n       def runTest(self):\n         pass\n \n-      @keras_parameterized.run_with_all_model_types(\n+      @test_combinations.run_with_all_model_types(\n           exclude_models=[\"sequential\", \"functional\"])\n       def testBody(self):\n-        model_types.append(testing_utils.get_model_type())\n-        models.append(testing_utils.get_small_mlp(1, 4, input_dim=3))\n+        model_types.append(test_utils.get_model_type())\n+        models.append(test_utils.get_small_mlp(1, 4, input_dim=3))\n \n     e = ExampleTest()\n     if hasattr(e, \"testBody_functional\"):\n@@ -196,15 +341,15 @@ class KerasParameterizedTest(keras_parameterized.TestCase):\n   def test_run_all_keras_modes(self):\n     l = []\n \n-    class ExampleTest(keras_parameterized.TestCase):\n+    class ExampleTest(test_combinations.TestCase):\n \n       def runTest(self):\n         pass\n \n-      @keras_parameterized.run_all_keras_modes()\n+      @test_combinations.run_all_keras_modes()\n       def testBody(self):\n         mode = \"eager\" if tf.executing_eagerly() else \"graph\"\n-        should_run_eagerly = testing_utils.should_run_eagerly()\n+        should_run_eagerly = test_utils.should_run_eagerly()\n         l.append((mode, should_run_eagerly))\n \n     e = ExampleTest()\n@@ -240,19 +385,19 @@ class KerasParameterizedTest(keras_parameterized.TestCase):\n   def test_run_all_keras_modes_extra_params(self):\n     l = []\n \n-    class ExampleTest(keras_parameterized.TestCase):\n+    class ExampleTest(test_combinations.TestCase):\n \n       def runTest(self):\n         pass\n \n-      @keras_parameterized.run_all_keras_modes()\n+      @test_combinations.run_all_keras_modes()\n       @parameterized.named_parameters(\n           [dict(testcase_name=\"_0\", with_brackets=True),\n            dict(testcase_name=\"_1\", with_brackets=False)])\n       def testBody(self, with_brackets):\n         mode = \"eager\" if tf.executing_eagerly() else \"graph\"\n         with_brackets = \"with_brackets\" if with_brackets else \"without_brackets\"\n-        should_run_eagerly = testing_utils.should_run_eagerly()\n+        should_run_eagerly = test_utils.should_run_eagerly()\n         l.append((with_brackets, mode, should_run_eagerly))\n \n     e = ExampleTest()\n@@ -290,15 +435,15 @@ class KerasParameterizedTest(keras_parameterized.TestCase):\n   def test_run_all_keras_modes_always_skip_v1(self):\n     l = []\n \n-    class ExampleTest(keras_parameterized.TestCase):\n+    class ExampleTest(test_combinations.TestCase):\n \n       def runTest(self):\n         pass\n \n-      @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+      @test_combinations.run_all_keras_modes(always_skip_v1=True)\n       def testBody(self):\n         mode = \"eager\" if tf.executing_eagerly() else \"graph\"\n-        should_run_eagerly = testing_utils.should_run_eagerly()\n+        should_run_eagerly = test_utils.should_run_eagerly()\n         l.append((mode, should_run_eagerly))\n \n     e = ExampleTest()\n@@ -319,17 +464,17 @@ class KerasParameterizedTest(keras_parameterized.TestCase):\n   def test_run_all_keras_modes_with_all_model_types(self):\n     l = []\n \n-    class ExampleTest(keras_parameterized.TestCase):\n+    class ExampleTest(test_combinations.TestCase):\n \n       def runTest(self):\n         pass\n \n-      @keras_parameterized.run_with_all_model_types\n-      @keras_parameterized.run_all_keras_modes\n+      @test_combinations.run_with_all_model_types\n+      @test_combinations.run_all_keras_modes\n       def testBody(self):\n         mode = \"eager\" if tf.executing_eagerly() else \"graph\"\n-        should_run_eagerly = testing_utils.should_run_eagerly()\n-        l.append((mode, should_run_eagerly, testing_utils.get_model_type()))\n+        should_run_eagerly = test_utils.should_run_eagerly()\n+        l.append((mode, should_run_eagerly, test_utils.get_model_type()))\n \n     e = ExampleTest()\n     e.testBody_v2_eager_functional()\n@@ -372,17 +517,17 @@ class KerasParameterizedTest(keras_parameterized.TestCase):\n   def test_run_all_model_types_with_all_keras_modes(self):\n     l = []\n \n-    class ExampleTest(keras_parameterized.TestCase):\n+    class ExampleTest(test_combinations.TestCase):\n \n       def runTest(self):\n         pass\n \n-      @keras_parameterized.run_all_keras_modes\n-      @keras_parameterized.run_with_all_model_types\n+      @test_combinations.run_all_keras_modes\n+      @test_combinations.run_with_all_model_types\n       def testBody(self):\n         mode = \"eager\" if tf.executing_eagerly() else \"graph\"\n-        should_run_eagerly = testing_utils.should_run_eagerly()\n-        l.append((mode, should_run_eagerly, testing_utils.get_model_type()))\n+        should_run_eagerly = test_utils.should_run_eagerly()\n+        l.append((mode, should_run_eagerly, test_utils.get_model_type()))\n \n     e = ExampleTest()\n     e.testBody_functional_v2_eager()\n@@ -425,9 +570,9 @@ class KerasParameterizedTest(keras_parameterized.TestCase):\n   def test_run_all_keras_modes_with_all_model_types_annotate_class(self):\n     l = []\n \n-    @keras_parameterized.run_with_all_model_types\n-    @keras_parameterized.run_all_keras_modes\n-    class ExampleTest(keras_parameterized.TestCase):\n+    @test_combinations.run_with_all_model_types\n+    @test_combinations.run_all_keras_modes\n+    class ExampleTest(test_combinations.TestCase):\n \n       def runTest(self):\n         pass\n@@ -436,8 +581,8 @@ class KerasParameterizedTest(keras_parameterized.TestCase):\n                                            arg=True))\n       def testBody(self, arg):\n         mode = \"eager\" if tf.executing_eagerly() else \"graph\"\n-        should_run_eagerly = testing_utils.should_run_eagerly()\n-        l.append((mode, should_run_eagerly, testing_utils.get_model_type()))\n+        should_run_eagerly = test_utils.should_run_eagerly()\n+        l.append((mode, should_run_eagerly, test_utils.get_model_type()))\n \n     e = ExampleTest()\n     e.testBody_arg_v2_eager_functional()\n@@ -480,19 +625,19 @@ class KerasParameterizedTest(keras_parameterized.TestCase):\n   def test_run_all_keras_modes_with_all_model_types_annotate_class_2(self):\n     l = []\n \n-    @keras_parameterized.run_with_all_model_types\n-    class ExampleTest(keras_parameterized.TestCase):\n+    @test_combinations.run_with_all_model_types\n+    class ExampleTest(test_combinations.TestCase):\n \n       def runTest(self):\n         pass\n \n-      @keras_parameterized.run_all_keras_modes\n+      @test_combinations.run_all_keras_modes\n       @parameterized.named_parameters(dict(testcase_name=\"_arg\",\n                                            arg=True))\n       def testBody(self, arg):\n         mode = \"eager\" if tf.executing_eagerly() else \"graph\"\n-        should_run_eagerly = testing_utils.should_run_eagerly()\n-        l.append((mode, should_run_eagerly, testing_utils.get_model_type()))\n+        should_run_eagerly = test_utils.should_run_eagerly()\n+        l.append((mode, should_run_eagerly, test_utils.get_model_type()))\n \n     e = ExampleTest()\n     e.testBody_arg_v2_eager_functional()\n@@ -532,13 +677,13 @@ class KerasParameterizedTest(keras_parameterized.TestCase):\n \n     self.assertLen(l, len(expected_combinations) * 2)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   @parameterized.named_parameters(dict(testcase_name=\"argument\",\n                                        arg=True))\n   def test_run_all_keras_modes_extra_params_2(self, arg):\n     self.assertEqual(arg, True)\n \n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_with_all_model_types\n   @parameterized.named_parameters(dict(testcase_name=\"argument\",\n                                        arg=True))\n   def test_run_with_all_model_types_extra_params_2(self, arg):\n\n@@ -36,7 +36,7 @@ from keras.utils import tf_contextlib\n from keras.utils import tf_inspect\n import numpy as np\n import tensorflow.compat.v2 as tf\n-from tensorflow.python.framework import test_util\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n \n \n def string_test(actual, expected):\n@@ -76,7 +76,7 @@ def get_test_data(train_samples,\n           (x[train_samples:], y[train_samples:]))\n \n \n-@test_util.disable_cudnn_autotune\n+@tf_test_utils.disable_cudnn_autotune\n def layer_test(layer_cls,\n                kwargs=None,\n                input_shape=None,\n\n@@ -18,13 +18,13 @@ import tensorflow.compat.v2 as tf\n \n import numpy as np\n from keras import Input\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras import layers\n from keras import losses\n from keras import Model\n from keras.optimizers import optimizer_v2\n from keras import Sequential\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.training.rmsprop import RMSPropOptimizer\n \n@@ -53,7 +53,7 @@ def get_ctl_train_step(model):\n # not part of the training model.\n \n \n-class TestAddLossCorrectness(keras_parameterized.TestCase):\n+class TestAddLossCorrectness(test_combinations.TestCase):\n \n   def setUp(self):\n     super(TestAddLossCorrectness, self).setUp()\n@@ -61,25 +61,25 @@ class TestAddLossCorrectness(keras_parameterized.TestCase):\n     self.y = np.array([[0.5], [2.], [3.5]], dtype='float32')\n     self.w = np.array([[1.25], [0.5], [1.25]], dtype='float32')\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_loss_on_model_fit(self):\n     inputs = Input(shape=(1,))\n     targets = Input(shape=(1,))\n-    outputs = testing_utils.Bias()(inputs)\n+    outputs = test_utils.Bias()(inputs)\n     model = Model([inputs, targets], outputs)\n     model.add_loss(MAE()(targets, outputs))\n     model.add_loss(tf.reduce_mean(mae(targets, outputs)))\n     model.compile(\n         optimizer_v2.gradient_descent.SGD(0.05),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     history = model.fit([self.x, self.y], batch_size=3, epochs=5)\n     self.assertAllClose(history.history['loss'], [2., 1.8, 1.6, 1.4, 1.2], 1e-3)\n \n-  @keras_parameterized.run_with_all_model_types(exclude_models=['sequential'])\n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_with_all_model_types(exclude_models=['sequential'])\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_loss_callable_on_model_fit(self):\n-    model = testing_utils.get_model_from_layers([testing_utils.Bias()],\n+    model = test_utils.get_model_from_layers([test_utils.Bias()],\n                                              input_shape=(1,))\n \n     def callable_loss():\n@@ -88,17 +88,17 @@ class TestAddLossCorrectness(keras_parameterized.TestCase):\n     model.add_loss(callable_loss)\n     model.compile(\n         optimizer_v2.gradient_descent.SGD(0.1),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     history = model.fit(self.x, batch_size=3, epochs=5)\n     self.assertAllClose(history.history['loss'], [0., -.1, -.2, -.3, -.4], 1e-3)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_loss_on_model_ctl(self):\n     def get_model_and_train_step():\n       inputs = Input(shape=(1,))\n       targets = Input(shape=(1,))\n-      outputs = testing_utils.Bias()(inputs)\n+      outputs = test_utils.Bias()(inputs)\n       model = Model([inputs, targets], outputs)\n       model.add_loss(MAE()(targets, outputs))\n       model.add_loss(tf.reduce_mean(mae(targets, outputs)))\n@@ -112,12 +112,12 @@ class TestAddLossCorrectness(keras_parameterized.TestCase):\n     loss = [train_step(self.x, self.y) for _ in range(5)]\n     self.assertAllClose(loss, [2., 1.8, 1.6, 1.4, 1.2], 1e-3)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_loss_callable_on_model_ctl(self):\n     def get_model_and_train_step():\n       inputs = Input(shape=(1,))\n       targets = Input(shape=(1,))\n-      outputs = testing_utils.Bias()(inputs)\n+      outputs = test_utils.Bias()(inputs)\n       model = Model([inputs, targets], outputs)\n \n       def callable_loss():\n@@ -134,29 +134,29 @@ class TestAddLossCorrectness(keras_parameterized.TestCase):\n     loss = [train_step(self.x, self.y) for _ in range(5)]\n     self.assertAllClose(loss, [0., -0.05, -0.1, -0.15, -0.2], 1e-3)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_loss_with_sample_weight_on_model_fit(self):\n     inputs = Input(shape=(1,))\n     targets = Input(shape=(1,))\n     sw = Input(shape=(1,))\n-    outputs = testing_utils.Bias()(inputs)\n+    outputs = test_utils.Bias()(inputs)\n     model = Model([inputs, targets, sw], outputs)\n     model.add_loss(MAE()(targets, outputs, sw))\n     model.add_loss(3 * tf.reduce_mean(sw * mae(targets, outputs)))\n     model.compile(\n         optimizer_v2.gradient_descent.SGD(0.025),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     history = model.fit([self.x, self.y, self.w], batch_size=3, epochs=5)\n     self.assertAllClose(history.history['loss'], [4., 3.6, 3.2, 2.8, 2.4], 1e-3)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_loss_with_sample_weight_on_model_ctl(self):\n     def get_model_and_train_step():\n       inputs = Input(shape=(1,))\n       targets = Input(shape=(1,))\n       sw = Input(shape=(1,))\n-      outputs = testing_utils.Bias()(inputs)\n+      outputs = test_utils.Bias()(inputs)\n       model = Model([inputs, targets, sw], outputs)\n       model.add_loss(MAE()(targets, outputs, sw))\n       model.add_loss(tf.reduce_mean(sw * mae(targets, outputs)))\n@@ -170,14 +170,14 @@ class TestAddLossCorrectness(keras_parameterized.TestCase):\n     loss = [train_step(self.x, self.y, self.w) for _ in range(5)]\n     self.assertAllClose(loss, [2., 1.8, 1.6, 1.4, 1.2], 1e-3)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_loss_with_sample_weight_in_model_call(self):\n \n     class MyModel(Model):\n \n       def __init__(self):\n         super(MyModel, self).__init__()\n-        self.bias = testing_utils.Bias()\n+        self.bias = test_utils.Bias()\n \n       def call(self, inputs):\n         outputs = self.bias(inputs[0])\n@@ -189,7 +189,7 @@ class TestAddLossCorrectness(keras_parameterized.TestCase):\n     model.predict([self.x, self.y, self.w])\n     model.compile(\n         optimizer_v2.gradient_descent.SGD(0.05),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     history = model.fit([self.x, self.y, self.w], batch_size=3, epochs=5)\n     self.assertEqual(len(model.losses), 2)\n@@ -198,14 +198,14 @@ class TestAddLossCorrectness(keras_parameterized.TestCase):\n     eval_out = model.evaluate([self.x, self.y, self.w])\n     self.assertAlmostEqual(eval_out, 1.0, 3)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_loss_with_sample_weight_in_layer_call(self):\n \n     class MyLayer(layers.Layer):\n \n       def __init__(self):\n         super(MyLayer, self).__init__()\n-        self.bias = testing_utils.Bias()\n+        self.bias = test_utils.Bias()\n \n       def call(self, inputs):\n         out = self.bias(inputs[0])\n@@ -222,7 +222,7 @@ class TestAddLossCorrectness(keras_parameterized.TestCase):\n     model.predict([self.x, self.y, self.w])\n     model.compile(\n         optimizer_v2.gradient_descent.SGD(0.05),\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     history = model.fit([self.x, self.y, self.w], batch_size=3, epochs=5)\n     self.assertAllClose(history.history['loss'], [2., 1.8, 1.6, 1.4, 1.2], 1e-3)\n@@ -233,7 +233,7 @@ class TestAddLossCorrectness(keras_parameterized.TestCase):\n     output = model.test_on_batch([self.x, self.y, self.w])\n     self.assertAlmostEqual(output, 1.0, 3)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_loss_on_layer(self):\n \n     class MyLayer(layers.Layer):\n@@ -250,12 +250,12 @@ class TestAddLossCorrectness(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     loss = model.train_on_batch(np.ones((2, 3)), np.ones((2, 3)))\n     self.assertEqual(loss, 2 * 3)\n \n-  @keras_parameterized.run_all_keras_modes\n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n   def test_activity_regularizer(self):\n     loss = {}\n     for reg in [None, 'l2']:\n@@ -273,7 +273,7 @@ class TestAddLossCorrectness(keras_parameterized.TestCase):\n               use_bias=False),\n       ]\n \n-      model = testing_utils.get_model_from_layers(\n+      model = test_utils.get_model_from_layers(\n           model_layers, input_shape=(10,))\n \n       x = np.ones((10, 10), 'float32')\n@@ -283,13 +283,13 @@ class TestAddLossCorrectness(keras_parameterized.TestCase):\n       model.compile(\n           optimizer,\n           'binary_crossentropy',\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n       model.fit(x, y, batch_size=2, epochs=5)\n       loss[reg] = model.evaluate(x, y)\n     self.assertLess(loss[None], loss['l2'])\n \n-  @keras_parameterized.run_all_keras_modes\n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_all_keras_modes\n+  @test_combinations.run_with_all_model_types\n   def test_activity_regularizer_loss_value(self):\n     layer = layers.Dense(\n         1,\n@@ -297,17 +297,17 @@ class TestAddLossCorrectness(keras_parameterized.TestCase):\n         bias_initializer='ones',\n         activity_regularizer='l2')\n \n-    model = testing_utils.get_model_from_layers([layer], input_shape=(10,))\n+    model = test_utils.get_model_from_layers([layer], input_shape=(10,))\n \n     x = np.ones((10, 10), 'float32')\n     optimizer = RMSPropOptimizer(learning_rate=0.001)\n     model.compile(\n         optimizer,\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     loss = model.test_on_batch(x)\n     self.assertAlmostEqual(0.01, loss, places=4)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_activity_regularizer_batch_independent(self):\n     inputs = layers.Input(shape=(10,))\n     x = layers.Dense(10, activation='relu', activity_regularizer='l2')(inputs)\n@@ -317,13 +317,13 @@ class TestAddLossCorrectness(keras_parameterized.TestCase):\n     optimizer = RMSPropOptimizer(learning_rate=0.001)\n     model.compile(\n         optimizer,\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     loss_small_batch = model.test_on_batch(np.ones((10, 10), 'float32'))\n     loss_big_batch = model.test_on_batch(np.ones((20, 10), 'float32'))\n     self.assertAlmostEqual(loss_small_batch, loss_big_batch, places=4)\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_with_shared_layer(self):\n \n     class LayerWithLoss(layers.Layer):\n@@ -340,7 +340,7 @@ class TestAddLossCorrectness(keras_parameterized.TestCase):\n     self.assertEqual(len(m2.losses), 2)\n     self.assertAllClose(m2.losses, [6, 12])\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_with_shared_nested_layer(self):\n \n     class LayerWithLoss(layers.Layer):\n@@ -366,7 +366,7 @@ class TestAddLossCorrectness(keras_parameterized.TestCase):\n     self.assertEqual(len(m2.losses), 2)\n     self.assertAllClose(m2.losses, [6, 12])\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_clear_losses(self):\n \n     class LayerWithSharedNestedLossLayer(layers.Layer):\n@@ -417,34 +417,34 @@ class TestAddLossCorrectness(keras_parameterized.TestCase):\n       self.assertEqual(len(model.get_losses_for(x4)), 2)\n       self.assertEqual(len(model.get_losses_for(None)), 1)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_invalid_constant_input(self):\n     inputs = Input(shape=(1,))\n-    outputs = testing_utils.Bias()(inputs)\n+    outputs = test_utils.Bias()(inputs)\n     model = Model(inputs, outputs)\n     with self.assertRaisesRegex(\n         ValueError,\n         'Expected a symbolic Tensors or a callable for the loss value'):\n       model.add_loss(1.)\n \n-  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @test_combinations.run_all_keras_modes(always_skip_v1=True)\n   def test_invalid_variable_input(self):\n     inputs = Input(shape=(1,))\n-    outputs = testing_utils.Bias()(inputs)\n+    outputs = test_utils.Bias()(inputs)\n     model = Model(inputs, outputs)\n     with self.assertRaisesRegex(\n         ValueError,\n         'Expected a symbolic Tensors or a callable for the loss value'):\n       model.add_loss(model.weights[0])\n \n-  @keras_parameterized.run_all_keras_modes\n+  @test_combinations.run_all_keras_modes\n   def test_add_entropy_loss_on_functional_model(self):\n     inputs = Input(shape=(1,))\n     targets = Input(shape=(1,))\n-    outputs = testing_utils.Bias()(inputs)\n+    outputs = test_utils.Bias()(inputs)\n     model = Model([inputs, targets], outputs)\n     model.add_loss(losses.binary_crossentropy(targets, outputs))\n-    model.compile('sgd', run_eagerly=testing_utils.should_run_eagerly())\n+    model.compile('sgd', run_eagerly=test_utils.should_run_eagerly())\n     with tf.compat.v1.test.mock.patch.object(logging, 'warning') as mock_log:\n       model.fit([self.x, self.y], batch_size=3, epochs=5)\n       self.assertNotIn('Gradients do not exist for variables',\n\n@@ -19,7 +19,6 @@ import os\n \n from absl import flags\n from keras import callbacks\n-from keras import testing_utils\n from keras.distribute import distribute_strategy_test\n from keras.engine import base_layer\n from keras.engine import sequential as sequential_model_lib\n@@ -29,6 +28,7 @@ from keras.layers import core as layer_lib\n from keras.layers import pooling as pool_layer_lib\n from keras.layers import regularization as regularization_layer_lib\n from keras.layers import reshaping as reshaping_layer_lib\n+from keras.testing_infra import test_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n@@ -36,7 +36,7 @@ from tensorboard.plugins.histogram import summary_v2 as histogram_summary_v2\n from tensorboard.plugins.image import summary_v2 as image_summary_v2\n from tensorboard.plugins.scalar import summary_v2 as scalar_summary_v2\n from tensorflow.python.eager.context import set_soft_device_placement  # pylint: disable=g-direct-tensorflow-import\n-from tensorflow.python.framework import test_util  # pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n \n NUM_CLASSES = 4\n \n@@ -154,7 +154,7 @@ def mnist_model(input_shape, enable_histograms=True):\n   return model\n \n \n-@testing_utils.run_v2_only\n+@test_utils.run_v2_only\n class AutoOutsideCompilationWithKerasTest(tf.test.TestCase):\n \n   def setUp(self):\n@@ -184,7 +184,7 @@ class AutoOutsideCompilationWithKerasTest(tf.test.TestCase):\n     # TODO(https://github.com/tensorflow/tensorboard/issues/2885): remove this\n     #   if histogram summaries are supported fully on non-MLIR bridge or\n     #   non-MLIR bridge is no longer run.\n-    enable_histograms = test_util.is_mlir_bridge_enabled()\n+    enable_histograms = tf_test_utils.is_mlir_bridge_enabled()\n     strategy = get_tpu_strategy()\n \n     with strategy.scope():\n@@ -217,7 +217,7 @@ class AutoOutsideCompilationWithKerasTest(tf.test.TestCase):\n     # TODO(https://github.com/tensorflow/tensorboard/issues/2885): remove this\n     #   if histogram summaries are supported fully on non-MLIR bridge or\n     #   non-MLIR bridge is no longer run.\n-    enable_histograms = test_util.is_mlir_bridge_enabled()\n+    enable_histograms = tf_test_utils.is_mlir_bridge_enabled()\n     strategy = get_tpu_strategy()\n     with strategy.scope():\n       model = CustomModel(enable_histograms=enable_histograms)\n\n@@ -22,7 +22,7 @@ import numpy as np\n \n import keras\n from tensorflow.python.framework import convert_to_constants\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from tensorflow.python.saved_model.load import load\n from tensorflow.python.saved_model.save import save\n \n@@ -90,7 +90,7 @@ class VariablesToConstantsTest(tf.test.TestCase):\n     for expected, actual in zip(expected_value, actual_value):\n       np.testing.assert_almost_equal(expected.numpy(), actual.numpy())\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def testKerasModel(self):\n     \"\"\"Test a basic Keras model with Variables.\"\"\"\n     input_data = {\"x\": tf.constant(1., shape=[1, 1])}\n@@ -113,7 +113,7 @@ class VariablesToConstantsTest(tf.test.TestCase):\n     root, output_func = self._freezeModel(to_save)\n     self._testConvertedFunction(root, root.f, output_func, input_data)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def testKerasLSTM(self):\n     \"\"\"Test a Keras LSTM containing dynamic_rnn ops.\"\"\"\n     input_data = {\n@@ -135,7 +135,7 @@ class VariablesToConstantsTest(tf.test.TestCase):\n     root, output_func = self._freezeModel(to_save)\n     self._testConvertedFunction(root, root.f, output_func, input_data)\n \n-  @testing_utils.run_v2_only\n+  @test_utils.run_v2_only\n   def testEmbeddings(self):\n     \"\"\"Test model with embeddings.\"\"\"\n     input_data = {\n\n@@ -20,8 +20,8 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n \n \n class LayerWithLosses(keras.layers.Layer):\n@@ -62,7 +62,7 @@ class LayerWithTrainingArg(keras.layers.Layer):\n \n def add_loss_step(defun):\n   optimizer = keras.optimizers.optimizer_v2.adam.Adam()\n-  model = testing_utils.get_model_from_layers([LayerWithLosses()],\n+  model = test_utils.get_model_from_layers([LayerWithLosses()],\n                                            input_shape=(10,))\n \n   def train_step(x):\n@@ -83,11 +83,10 @@ def add_loss_step(defun):\n \n def batch_norm_step(defun):\n   optimizer = keras.optimizers.optimizer_v2.adadelta.Adadelta()\n-  model = testing_utils.get_model_from_layers([\n+  model = test_utils.get_model_from_layers([\n       keras.layers.BatchNormalization(momentum=0.9),\n       keras.layers.Dense(1, kernel_initializer='zeros', activation='softmax')\n-  ],\n-                                              input_shape=(10,))\n+  ], input_shape=(10,))\n \n   def train_step(x, y):\n     with tf.GradientTape() as tape:\n@@ -106,11 +105,10 @@ def batch_norm_step(defun):\n \n def add_metric_step(defun):\n   optimizer = keras.optimizers.optimizer_v2.rmsprop.RMSprop()\n-  model = testing_utils.get_model_from_layers([\n+  model = test_utils.get_model_from_layers([\n       LayerWithMetrics(),\n       keras.layers.Dense(1, kernel_initializer='zeros', activation='softmax')\n-  ],\n-                                              input_shape=(10,))\n+  ], input_shape=(10,))\n \n   def train_step(x, y):\n     with tf.GradientTape() as tape:\n@@ -133,8 +131,8 @@ def add_metric_step(defun):\n   return metrics\n \n \n-@keras_parameterized.run_with_all_model_types\n-class CustomTrainingLoopTest(keras_parameterized.TestCase):\n+@test_combinations.run_with_all_model_types\n+class CustomTrainingLoopTest(test_combinations.TestCase):\n \n   @parameterized.named_parameters(('add_loss_step', add_loss_step),\n                                   ('add_metric_step', add_metric_step),\n@@ -147,7 +145,7 @@ class CustomTrainingLoopTest(keras_parameterized.TestCase):\n   @parameterized.named_parameters(('eager', False), ('defun', True))\n   def test_training_arg_propagation(self, defun):\n \n-    model = testing_utils.get_model_from_layers([LayerWithTrainingArg()],\n+    model = test_utils.get_model_from_layers([LayerWithTrainingArg()],\n                                              input_shape=(1,))\n \n     def train_step(x):\n\n@@ -14,16 +14,15 @@\n #,============================================================================\n \"\"\"Tests for `get_config` backwards compatibility.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n-from keras import keras_parameterized\n from keras.engine import sequential\n from keras.engine import training\n+from keras.testing_infra import test_combinations\n from keras.tests import get_config_samples\n+import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class TestGetConfigBackwardsCompatible(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class TestGetConfigBackwardsCompatible(test_combinations.TestCase):\n \n   def test_functional_dnn(self):\n     model = training.Model.from_config(get_config_samples.FUNCTIONAL_DNN)\n\n@@ -22,14 +22,14 @@ import random\n import numpy as np\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.layers.legacy_rnn import rnn_cell_impl as rnn_cell\n from keras.legacy_tf_layers import base as base_layer\n from keras.utils import np_utils\n \n \n-class KerasIntegrationTest(keras_parameterized.TestCase):\n+class KerasIntegrationTest(test_combinations.TestCase):\n \n   def _save_and_reload_model(self, model):\n     self.temp_dir = self.get_temp_dir()\n@@ -47,20 +47,20 @@ class KerasIntegrationTest(keras_parameterized.TestCase):\n     return model\n \n \n-@keras_parameterized.run_with_all_model_types\n-@keras_parameterized.run_all_keras_modes\n-class VectorClassificationIntegrationTest(keras_parameterized.TestCase):\n+@test_combinations.run_with_all_model_types\n+@test_combinations.run_all_keras_modes\n+class VectorClassificationIntegrationTest(test_combinations.TestCase):\n \n   def test_vector_classification(self):\n     np.random.seed(1337)\n-    (x_train, y_train), _ = testing_utils.get_test_data(\n+    (x_train, y_train), _ = test_utils.get_test_data(\n         train_samples=100,\n         test_samples=0,\n         input_shape=(10,),\n         num_classes=2)\n     y_train = np_utils.to_categorical(y_train)\n \n-    model = testing_utils.get_model_from_layers(\n+    model = test_utils.get_model_from_layers(\n         [keras.layers.Dense(16, activation='relu'),\n          keras.layers.Dropout(0.1),\n          keras.layers.Dense(y_train.shape[-1], activation='softmax')],\n@@ -69,7 +69,7 @@ class VectorClassificationIntegrationTest(keras_parameterized.TestCase):\n         loss='categorical_crossentropy',\n         optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n         metrics=['acc'],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(x_train, y_train, epochs=10, batch_size=10,\n                         validation_data=(x_train, y_train),\n                         verbose=2)\n@@ -83,14 +83,14 @@ class VectorClassificationIntegrationTest(keras_parameterized.TestCase):\n     # Test that Sequential models that feature internal updates\n     # and internal losses can be shared.\n     np.random.seed(1337)\n-    (x_train, y_train), _ = testing_utils.get_test_data(\n+    (x_train, y_train), _ = test_utils.get_test_data(\n         train_samples=100,\n         test_samples=0,\n         input_shape=(10,),\n         num_classes=2)\n     y_train = np_utils.to_categorical(y_train)\n \n-    base_model = testing_utils.get_model_from_layers(\n+    base_model = test_utils.get_model_from_layers(\n         [keras.layers.Dense(16,\n                             activation='relu',\n                             kernel_regularizer=keras.regularizers.l2(1e-5),\n@@ -105,7 +105,7 @@ class VectorClassificationIntegrationTest(keras_parameterized.TestCase):\n         loss='categorical_crossentropy',\n         optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n         metrics=['acc'],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     self.assertLen(model.losses, 2)\n     if not tf.executing_eagerly():\n       self.assertLen(model.get_updates_for(x), 2)\n@@ -119,7 +119,7 @@ class VectorClassificationIntegrationTest(keras_parameterized.TestCase):\n     self.assertEqual(predictions.shape, (x_train.shape[0], 2))\n \n \n-@keras_parameterized.run_all_keras_modes\n+@test_combinations.run_all_keras_modes\n class SequentialIntegrationTest(KerasIntegrationTest):\n \n   def test_sequential_save_and_pop(self):\n@@ -130,7 +130,7 @@ class SequentialIntegrationTest(KerasIntegrationTest):\n     # - pop its last layer and add a new layer instead\n     # - continue training\n     np.random.seed(1337)\n-    (x_train, y_train), _ = testing_utils.get_test_data(\n+    (x_train, y_train), _ = test_utils.get_test_data(\n         train_samples=100,\n         test_samples=0,\n         input_shape=(10,),\n@@ -145,7 +145,7 @@ class SequentialIntegrationTest(KerasIntegrationTest):\n         loss='categorical_crossentropy',\n         optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n         metrics=['acc'],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(x_train, y_train, epochs=1, batch_size=10,\n               validation_data=(x_train, y_train),\n               verbose=2)\n@@ -158,7 +158,7 @@ class SequentialIntegrationTest(KerasIntegrationTest):\n         loss='categorical_crossentropy',\n         optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n         metrics=['acc'],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(x_train, y_train, epochs=10, batch_size=10,\n                         validation_data=(x_train, y_train),\n                         verbose=2)\n@@ -171,13 +171,13 @@ class SequentialIntegrationTest(KerasIntegrationTest):\n \n \n # See b/122473407\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class TimeseriesClassificationIntegrationTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class TimeseriesClassificationIntegrationTest(test_combinations.TestCase):\n \n-  @keras_parameterized.run_with_all_model_types\n+  @test_combinations.run_with_all_model_types\n   def test_timeseries_classification(self):\n     np.random.seed(1337)\n-    (x_train, y_train), _ = testing_utils.get_test_data(\n+    (x_train, y_train), _ = test_utils.get_test_data(\n         train_samples=100,\n         test_samples=0,\n         input_shape=(4, 10),\n@@ -188,13 +188,13 @@ class TimeseriesClassificationIntegrationTest(keras_parameterized.TestCase):\n         keras.layers.LSTM(5, return_sequences=True),\n         keras.layers.GRU(y_train.shape[-1], activation='softmax')\n     ]\n-    model = testing_utils.get_model_from_layers(\n+    model = test_utils.get_model_from_layers(\n         layers, input_shape=x_train.shape[1:])\n     model.compile(\n         loss='categorical_crossentropy',\n         optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n         metrics=['acc'],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(x_train, y_train, epochs=15, batch_size=10,\n                         validation_data=(x_train, y_train),\n                         verbose=2)\n@@ -206,7 +206,7 @@ class TimeseriesClassificationIntegrationTest(keras_parameterized.TestCase):\n \n   def test_timeseries_classification_sequential_tf_rnn(self):\n     np.random.seed(1337)\n-    (x_train, y_train), _ = testing_utils.get_test_data(\n+    (x_train, y_train), _ = test_utils.get_test_data(\n         train_samples=100,\n         test_samples=0,\n         input_shape=(4, 10),\n@@ -224,7 +224,7 @@ class TimeseriesClassificationIntegrationTest(keras_parameterized.TestCase):\n           loss='categorical_crossentropy',\n           optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n           metrics=['acc'],\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n \n     history = model.fit(x_train, y_train, epochs=15, batch_size=10,\n                         validation_data=(x_train, y_train),\n@@ -236,13 +236,13 @@ class TimeseriesClassificationIntegrationTest(keras_parameterized.TestCase):\n     self.assertEqual(predictions.shape, (x_train.shape[0], 2))\n \n \n-@keras_parameterized.run_with_all_model_types\n-@keras_parameterized.run_all_keras_modes\n-class ImageClassificationIntegrationTest(keras_parameterized.TestCase):\n+@test_combinations.run_with_all_model_types\n+@test_combinations.run_all_keras_modes\n+class ImageClassificationIntegrationTest(test_combinations.TestCase):\n \n   def test_image_classification(self):\n     np.random.seed(1337)\n-    (x_train, y_train), _ = testing_utils.get_test_data(\n+    (x_train, y_train), _ = test_utils.get_test_data(\n         train_samples=100,\n         test_samples=0,\n         input_shape=(10, 10, 3),\n@@ -257,13 +257,13 @@ class ImageClassificationIntegrationTest(keras_parameterized.TestCase):\n         keras.layers.Flatten(),\n         keras.layers.Dense(y_train.shape[-1], activation='softmax')\n     ]\n-    model = testing_utils.get_model_from_layers(\n+    model = test_utils.get_model_from_layers(\n         layers, input_shape=x_train.shape[1:])\n     model.compile(\n         loss='categorical_crossentropy',\n         optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n         metrics=['acc'],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(x_train, y_train, epochs=10, batch_size=10,\n                         validation_data=(x_train, y_train),\n                         verbose=2)\n@@ -274,8 +274,8 @@ class ImageClassificationIntegrationTest(keras_parameterized.TestCase):\n     self.assertEqual(predictions.shape, (x_train.shape[0], 2))\n \n \n-@keras_parameterized.run_all_keras_modes\n-class ActivationV2IntegrationTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class ActivationV2IntegrationTest(test_combinations.TestCase):\n   \"\"\"Tests activation function V2 in model exporting and loading.\n \n   This test is to verify in TF 2.x, when 'tf.nn.softmax' is used as an\n@@ -285,7 +285,7 @@ class ActivationV2IntegrationTest(keras_parameterized.TestCase):\n \n   def test_serialization_v2_model(self):\n     np.random.seed(1337)\n-    (x_train, y_train), _ = testing_utils.get_test_data(\n+    (x_train, y_train), _ = test_utils.get_test_data(\n         train_samples=100,\n         test_samples=0,\n         input_shape=(10,),\n@@ -307,7 +307,7 @@ class ActivationV2IntegrationTest(keras_parameterized.TestCase):\n         loss='categorical_crossentropy',\n         optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n         metrics=['accuracy'],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(x_train, y_train, epochs=2, batch_size=10,\n               validation_data=(x_train, y_train),\n               verbose=2)\n\n@@ -23,7 +23,7 @@ import sys\n \n from absl import flags\n from absl.testing import absltest\n-from keras.utils import keras_doctest_lib\n+from keras.testing_infra import keras_doctest_lib\n import numpy as np\n import tensorflow as tf\n import tensorflow.compat.v2 as tf\n\n@@ -24,14 +24,14 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras.optimizers import optimizer_v1\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.tests import model_architectures\n \n \n-@keras_parameterized.run_with_all_saved_model_formats\n-class TestModelArchitectures(keras_parameterized.TestCase):\n+@test_combinations.run_with_all_saved_model_formats\n+class TestModelArchitectures(test_combinations.TestCase):\n \n   def _save_model_dir(self, dirname='saved_model'):\n     temp_dir = self.get_temp_dir()\n@@ -72,7 +72,7 @@ class TestModelArchitectures(keras_parameterized.TestCase):\n \n   @parameterized.named_parameters(*model_architectures.ALL_MODELS)\n   def test_basic_saving_and_loading(self, model_fn):\n-    save_format = testing_utils.get_save_format()\n+    save_format = test_utils.get_save_format()\n     custom_objects = self.get_custom_objects()\n     if 'subclassed_in_functional' in model_fn.__name__:\n       subclass_custom_objects = {\n\n@@ -21,8 +21,8 @@ import os\n import numpy as np\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.tests import model_subclassing_test_util as model_util\n \n try:\n@@ -31,21 +31,21 @@ except ImportError:\n   h5py = None\n \n \n-@keras_parameterized.run_all_keras_modes\n-class ModelSubclassCompiledTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class ModelSubclassCompiledTest(test_combinations.TestCase):\n \n   def test_single_io_workflow_with_np_arrays(self):\n     num_classes = 2\n     num_samples = 100\n     input_dim = 50\n \n-    model = testing_utils.SmallSubclassMLP(\n+    model = test_utils.SmallSubclassMLP(\n         num_hidden=32, num_classes=num_classes, use_dp=True, use_bn=True)\n     model.compile(\n         loss='mse',\n         optimizer='rmsprop',\n         metrics=['acc', keras.metrics.CategoricalAccuracy()],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x = np.ones((num_samples, input_dim))\n     y = np.zeros((num_samples, num_classes))\n@@ -64,7 +64,7 @@ class ModelSubclassCompiledTest(keras_parameterized.TestCase):\n         loss='mse',\n         optimizer='rmsprop',\n         metrics=['acc'],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x1 = np.ones((num_samples, input_dim))\n     x2 = np.ones((num_samples, input_dim))\n@@ -80,12 +80,12 @@ class ModelSubclassCompiledTest(keras_parameterized.TestCase):\n     input_dim = 50\n \n     with self.cached_session():\n-      model = testing_utils.SmallSubclassMLP(\n+      model = test_utils.SmallSubclassMLP(\n           num_hidden=32, num_classes=num_classes, use_dp=True, use_bn=True)\n       model.compile(\n           loss='mse',\n           optimizer='rmsprop',\n-          run_eagerly=testing_utils.should_run_eagerly())\n+          run_eagerly=test_utils.should_run_eagerly())\n \n       x = np.ones((num_samples, input_dim), dtype=np.float32)\n       y = np.zeros((num_samples, num_classes), dtype=np.float32)\n@@ -118,7 +118,7 @@ class ModelSubclassCompiledTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='mse',\n         optimizer='rmsprop',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch([x1, x2], [y1, y2])\n \n     self.assertEqual(model.built, True)\n@@ -149,7 +149,7 @@ class ModelSubclassCompiledTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='mse',\n         optimizer='rmsprop',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     y_ref = model.predict(x)\n \n     model.train_on_batch(x, y)\n@@ -182,7 +182,7 @@ class ModelSubclassCompiledTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='mse',\n         optimizer='rmsprop',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     loss = model.train_on_batch(x, y)\n     self.assertGreater(loss, 0.1)\n \n@@ -204,7 +204,7 @@ class ModelSubclassCompiledTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='mse',\n         optimizer='rmsprop',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit([x1, x2], [y1, y2], epochs=2, batch_size=32, verbose=0)\n     model.fit({'input_1': x1, 'input_2': x2},\n               {'output_1': y1, 'output_2': y2},\n@@ -217,7 +217,7 @@ class ModelSubclassCompiledTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='mse',\n         optimizer='rmsprop',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.train_on_batch([x1, x2], [y1, y2])\n     model.train_on_batch({'input_1': x1, 'input_2': x2},\n                          {'output_1': y1, 'output_2': y2})\n@@ -239,7 +239,7 @@ class ModelSubclassCompiledTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='mse',\n         optimizer='rmsprop',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.evaluate([x1, x2], [y1, y2])\n     model.test_on_batch([x1, x2], [y1, y2])\n \n@@ -266,7 +266,7 @@ class ModelSubclassCompiledTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='mse',\n         optimizer='rmsprop',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit([x1, x2], [y1, y2], epochs=2, batch_size=32, verbose=0)\n     y_ref_1, y_ref_2 = model.predict([x1, x2])\n \n@@ -306,7 +306,7 @@ class ModelSubclassCompiledTest(keras_parameterized.TestCase):\n         loss='mse',\n         optimizer='rmsprop',\n         metrics=['acc'],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x = np.ones((num_samples, input_dim))\n     y = np.zeros((num_samples, num_classes))\n@@ -330,7 +330,7 @@ class ModelSubclassCompiledTest(keras_parameterized.TestCase):\n         loss='mse',\n         optimizer='rmsprop',\n         metrics=['acc'],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x = np.ones((num_samples, input_dim))\n     y = np.zeros((num_samples, num_classes))\n@@ -355,7 +355,7 @@ class ModelSubclassCompiledTest(keras_parameterized.TestCase):\n         loss='mse',\n         optimizer='rmsprop',\n         metrics=['acc'],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x = np.ones((num_samples, input_dim))\n     y = np.zeros((num_samples, num_classes))\n@@ -390,7 +390,7 @@ class ModelSubclassCompiledTest(keras_parameterized.TestCase):\n         loss='mse',\n         optimizer='rmsprop',\n         metrics=['acc'],\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     x = np.ones((num_samples, input_dim))\n     y = np.zeros((num_samples, num_classes))\n@@ -429,7 +429,7 @@ class ModelSubclassCompiledTest(keras_parameterized.TestCase):\n     model.compile(\n         loss='mse',\n         optimizer='rmsprop',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     loss = model.train_on_batch(x, y)\n     self.assertGreater(loss, 0.1)\n \n\n@@ -23,10 +23,9 @@ from absl.testing import parameterized\n import numpy as np\n \n import keras\n-from tensorflow.python.framework import test_util\n-from keras import combinations\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.tests import model_subclassing_test_util as model_util\n from tensorflow.python.training.tracking import data_structures\n \n@@ -36,8 +35,8 @@ except ImportError:\n   h5py = None\n \n \n-@keras_parameterized.run_all_keras_modes\n-class ModelSubclassingTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class ModelSubclassingTest(test_combinations.TestCase):\n \n   def test_custom_build(self):\n     class DummyModel(keras.Model):\n@@ -89,7 +88,7 @@ class ModelSubclassingTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n     model.fit(np.ones((10, 10)), np.ones((10, 1)), batch_size=2, epochs=2)\n     self.assertLen(model.layers, 2)\n     self.assertLen(model.trainable_variables, 4)\n@@ -111,7 +110,7 @@ class ModelSubclassingTest(keras_parameterized.TestCase):\n     model.compile(\n         'sgd',\n         'mse',\n-        run_eagerly=testing_utils.should_run_eagerly())\n+        run_eagerly=test_utils.should_run_eagerly())\n \n     data = tf.data.Dataset.from_tensor_slices(({\n         'a': np.ones((32, 10)),\n@@ -123,7 +122,7 @@ class ModelSubclassingTest(keras_parameterized.TestCase):\n     num_classes = 2\n     input_dim = 50\n \n-    model = testing_utils.SmallSubclassMLP(\n+    model = test_utils.SmallSubclassMLP(\n         num_hidden=32, num_classes=num_classes, use_dp=True, use_bn=True)\n \n     self.assertFalse(model.built, 'Model should not have been built')\n@@ -200,7 +199,7 @@ class ModelSubclassingTest(keras_parameterized.TestCase):\n     input_dim = 50\n     batch_size = None\n \n-    model = testing_utils.SmallSubclassMLP(\n+    model = test_utils.SmallSubclassMLP(\n         num_hidden=32, num_classes=num_classes, use_dp=True, use_bn=True)\n \n     self.assertFalse(model.built, 'Model should not have been built')\n@@ -217,7 +216,7 @@ class ModelSubclassingTest(keras_parameterized.TestCase):\n     input_dim = tf.compat.v1.Dimension(50)\n     batch_size = tf.compat.v1.Dimension(None)\n \n-    model = testing_utils.SmallSubclassMLP(\n+    model = test_utils.SmallSubclassMLP(\n         num_hidden=32, num_classes=num_classes, use_dp=True, use_bn=True)\n \n     self.assertFalse(model.built, 'Model should not have been built')\n@@ -326,7 +325,7 @@ class ModelSubclassingTest(keras_parameterized.TestCase):\n         self.contents += msg + '\\n'\n \n     # Single-io\n-    model = testing_utils.SmallSubclassMLP(\n+    model = test_utils.SmallSubclassMLP(\n         num_hidden=32, num_classes=4, use_bn=True, use_dp=True)\n     model(np.ones((3, 4)))  # need to build model first\n     print_fn = ToString()\n@@ -342,7 +341,7 @@ class ModelSubclassingTest(keras_parameterized.TestCase):\n     self.assertIn('Trainable params: 587', print_fn.contents)\n \n     # Single-io with unused layer\n-    model = testing_utils.SmallSubclassMLP(\n+    model = test_utils.SmallSubclassMLP(\n         num_hidden=32, num_classes=4, use_bn=True, use_dp=True)\n     model.unused_layer = keras.layers.Dense(10)\n     model(np.ones((3, 4)))  # need to build model first\n@@ -484,7 +483,7 @@ class GraphSpecificModelSubclassingTests(tf.test.TestCase):\n     input_dim = 50\n \n     with tf.Graph().as_default(), self.cached_session():\n-      model = testing_utils.SmallSubclassMLP(\n+      model = test_utils.SmallSubclassMLP(\n           num_hidden=32, num_classes=num_classes, use_dp=True, use_bn=True)\n       model.compile(loss='mse', optimizer='rmsprop')\n \n@@ -603,7 +602,7 @@ class GraphSpecificModelSubclassingTests(tf.test.TestCase):\n       _ = model.evaluate([x1, x2], [y1, y2], verbose=0)\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class CustomCallSignatureTests(tf.test.TestCase, parameterized.TestCase):\n \n   def test_no_inputs_in_signature(self):\n@@ -668,8 +667,8 @@ class CustomCallSignatureTests(tf.test.TestCase, parameterized.TestCase):\n     if not tf.executing_eagerly():\n       self.assertLen(model.inputs, 1)\n \n-  @test_util.assert_no_new_tensors\n-  @test_util.assert_no_garbage_created\n+  @tf_test_utils.assert_no_new_tensors\n+  @tf_test_utils.assert_no_garbage_created\n   def test_training_no_default(self):\n     if not tf.executing_eagerly():\n       return\n\n@@ -15,7 +15,7 @@\n \"\"\"Keras models for use in Model subclassing tests.\"\"\"\n \n import keras\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n \n \n # pylint: disable=missing-docstring,not-callable\n@@ -49,7 +49,7 @@ def get_multi_io_subclass_model(use_bn=False, use_dp=False, num_classes=(2, 3)):\n   branch_b.append(keras.layers.Dense(num_classes[1], activation='softmax'))\n \n   model = (\n-      testing_utils._MultiIOSubclassModel(   # pylint: disable=protected-access\n+      test_utils._MultiIOSubclassModel(   # pylint: disable=protected-access\n           branch_a, branch_b, name='test_model'))\n   return model\n \n@@ -64,7 +64,7 @@ class NestedTestModel1(keras.Model):\n     self.dense1 = keras.layers.Dense(32, activation='relu')\n     self.dense2 = keras.layers.Dense(num_classes, activation='relu')\n     self.bn = keras.layers.BatchNormalization()\n-    self.test_net = testing_utils.SmallSubclassMLP(\n+    self.test_net = test_utils.SmallSubclassMLP(\n         num_hidden=32, num_classes=4, use_bn=True, use_dp=True)\n \n   def call(self, inputs):\n\n@@ -17,7 +17,7 @@\n import tensorflow.compat.v2 as tf\n \n import os\n-from tensorflow.python.framework import test_util\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n from keras.layers import core\n from keras.optimizers.optimizer_v2 import adam\n \n@@ -47,7 +47,7 @@ class MemoryTests(tf.test.TestCase):\n     super(MemoryTests, self).setUp()\n     self._model = _ModelWithOptimizerUsingDefun()\n \n-  @test_util.assert_no_garbage_created\n+  @tf_test_utils.assert_no_garbage_created\n   def DISABLED_test_no_reference_cycles(self):\n     x = tf.constant([[3., 4.]])\n     y = tf.constant([2.])\n\n@@ -17,8 +17,7 @@\n import tensorflow.compat.v2 as tf\n \n import json\n-from keras import combinations\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras.engine import input_layer\n from keras.engine import sequential\n from keras.engine import training\n@@ -26,8 +25,8 @@ from keras.layers import core\n from keras.saving.saved_model import json_utils\n \n \n-@combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n-class SerializationTests(keras_parameterized.TestCase):\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class SerializationTests(test_combinations.TestCase):\n \n   def test_serialize_dense(self):\n     dense = core.Dense(3)\n\n@@ -18,11 +18,11 @@ import tensorflow.compat.v2 as tf\n \n import numpy as np\n \n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras import layers\n from keras import metrics\n from keras.optimizers import optimizer_v2\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n \n \n class Bias(layers.Layer):\n@@ -48,7 +48,7 @@ def get_multi_io_temporal_model():\n \n   branch_a = [inp_1, x, out_1]\n   branch_b = [inp_2, x, out_2]\n-  return testing_utils.get_multi_io_model(branch_a, branch_b)\n+  return test_utils.get_multi_io_model(branch_a, branch_b)\n \n \n def get_compiled_multi_io_model_temporal(sample_weight_mode):\n@@ -59,7 +59,7 @@ def get_compiled_multi_io_model_temporal(sample_weight_mode):\n       metrics=[metrics.MeanAbsoluteError(name='mae')],\n       weighted_metrics=[metrics.MeanAbsoluteError(name='mae_2')],\n       sample_weight_mode=sample_weight_mode,\n-      run_eagerly=testing_utils.should_run_eagerly())\n+      run_eagerly=test_utils.should_run_eagerly())\n   return model\n \n \n@@ -96,9 +96,9 @@ def run_with_different_sample_weight_mode_inputs(fn, partial_sw=True):\n     # fn(model)\n \n \n-@keras_parameterized.run_with_all_model_types(exclude_models=['sequential'])\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class TestMetricsCorrectnessMultiIOTemporal(keras_parameterized.TestCase):\n+@test_combinations.run_with_all_model_types(exclude_models=['sequential'])\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class TestMetricsCorrectnessMultiIOTemporal(test_combinations.TestCase):\n \n   def custom_generator_multi_io_temporal(self, sample_weights=None):\n     \"\"\"Generator for getting data for temporal multi io model.\n\n@@ -19,8 +19,7 @@ import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n import numpy\n-from keras import combinations\n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras.engine import sequential\n from keras.engine import training\n from keras.layers import core\n@@ -60,9 +59,10 @@ class HasList(training.Model):\n     return bn(x) / aggregation\n \n \n-class ListTests(keras_parameterized.TestCase):\n+class ListTests(test_combinations.TestCase):\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testTracking(self):\n     with self.test_session():\n       model = HasList()\n@@ -163,7 +163,8 @@ class ListTests(keras_parameterized.TestCase):\n     m2(m2.null_input())\n     self.assertLen(m2.trainable_variables, 6)\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testUpdatesForwarded(self):\n     model = HasList()\n     model_input = tf.ones([32, 2])\n@@ -175,7 +176,8 @@ class ListTests(keras_parameterized.TestCase):\n       self.assertEqual(set(model.layers_with_updates[0].updates),\n                        set(model.updates))\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testLossesForwarded(self):\n     model = HasList()\n     model_input = tf.ones([32, 2])\n@@ -197,7 +199,8 @@ class ListTests(keras_parameterized.TestCase):\n     model.l2.append(second_layer)\n     self.assertEqual([first_layer, second_layer], model.layers)\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testTensorConversion(self):\n \n     class ListToTensor(training.Model):\n@@ -248,9 +251,10 @@ class HasMapping(training.Model):\n     return self.layer_dict[\"output\"](x) / aggregation\n \n \n-class MappingTests(keras_parameterized.TestCase):\n+class MappingTests(test_combinations.TestCase):\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testTracking(self):\n     with self.test_session():\n       model = HasMapping()\n@@ -398,9 +402,10 @@ class HasTuple(training.Model):\n     return bn(x) / aggregation\n \n \n-class TupleTests(keras_parameterized.TestCase):\n+class TupleTests(test_combinations.TestCase):\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testTracking(self):\n     with self.test_session():\n       model = HasTuple()\n@@ -481,7 +486,8 @@ class TupleTests(keras_parameterized.TestCase):\n     model(model_input)\n     self.assertEmpty(model.updates)\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testLossesForwarded(self):\n     model = HasTuple()\n     model_input = tf.ones([32, 2])\n@@ -509,7 +515,8 @@ class TupleTests(keras_parameterized.TestCase):\n     self.assertEqual(2, d[(second_layer,)])\n     self.assertEqual([first_layer, second_layer], model.layers)\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testTensorConversion(self):\n \n     class TupleToTensor(training.Model):\n@@ -527,7 +534,7 @@ class TupleTests(keras_parameterized.TestCase):\n         self.evaluate(tf.raw_ops.Pack(values=TupleToTensor().l)))\n \n \n-class InterfaceTests(keras_parameterized.TestCase):\n+class InterfaceTests(test_combinations.TestCase):\n \n   def testNoDependency(self):\n     root = tf.Module()\n@@ -551,7 +558,8 @@ class InterfaceTests(keras_parameterized.TestCase):\n     nodeps = NoDependencyModel()\n     self.assertEqual([nodeps], util.list_objects(nodeps))\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testDictionariesBasic(self):\n     a = training.Model()\n     b = training.Model()\n@@ -572,7 +580,8 @@ class InterfaceTests(keras_parameterized.TestCase):\n     with self.cached_session():\n       checkpoint.restore(save_path).assert_consumed().initialize_or_restore()\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testNoDepList(self):\n     a = training.Model()\n     a.l1 = data_structures.NoDependency([])\n\n@@ -19,10 +19,9 @@ import tensorflow.compat.v2 as tf\n import os\n import weakref\n from tensorflow.python.eager import context\n-from tensorflow.python.framework import test_util\n-from keras import combinations\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.engine import input_layer\n from keras.engine import sequential\n from keras.engine import training\n@@ -80,9 +79,9 @@ class InterfaceTests(tf.test.TestCase):\n       checkpoint.save(os.path.join(self.get_temp_dir(), \"ckpt\"))\n \n \n-class CheckpointingTests(keras_parameterized.TestCase):\n+class CheckpointingTests(test_combinations.TestCase):\n \n-  @test_util.run_in_graph_and_eager_modes(assert_no_eager_garbage=True)\n+  @tf_test_utils.run_in_graph_and_eager_modes(assert_no_eager_garbage=True)\n   def testNamingWithOptimizer(self):\n     input_value = tf.constant([[3.]])\n     model = MyModel()\n@@ -174,7 +173,8 @@ class CheckpointingTests(keras_parameterized.TestCase):\n         len([key + suffix for key in expected_slot_keys]),\n         len(serialized_slot_keys))\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testSaveRestore(self):\n     with self.test_session():\n       model = MyModel()\n@@ -309,7 +309,8 @@ class CheckpointingTests(keras_parameterized.TestCase):\n             self.assertEqual(training_continuation + 1,\n                              session.run(root.save_counter))\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testAgnosticUsage(self):\n     \"\"\"Graph/eager agnostic usage.\"\"\"\n     # Does create garbage when executing eagerly due to ops.Graph() creation.\n@@ -324,7 +325,7 @@ class CheckpointingTests(keras_parameterized.TestCase):\n         gradients = tape.gradient(loss, variables)\n         return optimizer.apply_gradients(zip(gradients, variables))\n       for training_continuation in range(3):\n-        with testing_utils.device(should_use_gpu=True):\n+        with test_utils.device(should_use_gpu=True):\n           model = MyModel()\n           root = tf.train.Checkpoint(\n               optimizer=optimizer, model=model)\n@@ -344,7 +345,7 @@ class CheckpointingTests(keras_parameterized.TestCase):\n           self.assertEqual(training_continuation + 1,\n                            self.evaluate(root.save_counter))\n \n-  @combinations.generate(combinations.combine(mode=[\"eager\"]))\n+  @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n   def testPartialRestoreWarningObject(self):\n     optimizer = adam.Adam(0.0)\n     original_root = tf.train.Checkpoint(v1=tf.Variable(2.),\n@@ -370,14 +371,15 @@ class CheckpointingTests(keras_parameterized.TestCase):\n     self.assertIn(\"expect_partial()\", messages)\n \n   # pylint: disable=cell-var-from-loop\n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testWithDefun(self):\n     with self.test_session():\n       num_training_steps = 2\n       checkpoint_directory = self.get_temp_dir()\n       checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n       for training_continuation in range(3):\n-        with testing_utils.device(should_use_gpu=True):\n+        with test_utils.device(should_use_gpu=True):\n           model = MyModel()\n           # Don't actually train so we can test variable values\n           optimizer = adam.Adam(0.)\n@@ -412,7 +414,7 @@ class CheckpointingTests(keras_parameterized.TestCase):\n                            self.evaluate(root.save_counter))\n   # pylint: enable=cell-var-from-loop\n \n-  @combinations.generate(combinations.combine(mode=[\"eager\"]))\n+  @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n   def testAnonymousVarsInInit(self):\n \n     class Model(training.Model):\n@@ -441,7 +443,8 @@ class CheckpointingTests(keras_parameterized.TestCase):\n       optimizer.apply_gradients(\n           [(g, v) for g, v in zip(grad, model.vars)])\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testDeferredSlotRestoration(self):\n     with self.test_session():\n       checkpoint_directory = self.get_temp_dir()\n@@ -546,7 +549,8 @@ class CheckpointingTests(keras_parameterized.TestCase):\n         graph.finalize()\n         obj.restore(save_path)\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def test_sequential(self):\n     with self.test_session():\n       model = sequential.Sequential()\n@@ -579,13 +583,14 @@ class CheckpointingTests(keras_parameterized.TestCase):\n       self.assertAllEqual([1., 2., 3., 4., 5.],\n                           self.evaluate(deferred_second_dense.bias))\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def test_initialize_if_not_restoring(self):\n     with self.test_session():\n       checkpoint_directory = self.get_temp_dir()\n       checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n       optimizer_only_prefix = os.path.join(checkpoint_directory, \"opt\")\n-      with testing_utils.device(should_use_gpu=True):\n+      with test_utils.device(should_use_gpu=True):\n         model = MyModel()\n         optimizer = adam.Adam(0.001)\n         root = tf.train.Checkpoint(\n@@ -621,7 +626,7 @@ class CheckpointingTests(keras_parameterized.TestCase):\n       del train_fn\n \n       # Restore into a graph with the optimizer\n-      with testing_utils.device(should_use_gpu=True):\n+      with test_utils.device(should_use_gpu=True):\n         model = MyModel()\n         optimizer = adam.Adam(0.001)\n         root = tf.train.Checkpoint(\n@@ -645,7 +650,7 @@ class CheckpointingTests(keras_parameterized.TestCase):\n       del train_fn1\n \n       # Make sure initialization doesn't clobber later restores\n-      with testing_utils.device(should_use_gpu=True):\n+      with test_utils.device(should_use_gpu=True):\n         model = MyModel()\n         optimizer = adam.Adam(0.001, beta_1=1.0)\n         root = tf.train.Checkpoint(\n@@ -683,8 +688,8 @@ class _ManualScope(tf.Module):\n     return tf.compat.v1.get_variable(name=\"in_manual_scope\", shape=[])\n \n \n-@combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n-class TemplateTests(keras_parameterized.TestCase):\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class TemplateTests(test_combinations.TestCase):\n \n   def test_trackable_save_restore(self):\n     with self.test_session():\n@@ -737,7 +742,7 @@ class TemplateTests(keras_parameterized.TestCase):\n       self.assertAllEqual([14.], self.evaluate(var2))\n \n \n-class CheckpointCompatibilityTests(keras_parameterized.TestCase):\n+class CheckpointCompatibilityTests(test_combinations.TestCase):\n \n   def _initialized_model(self):\n     input_value = tf.constant([[3.]])\n@@ -792,10 +797,11 @@ class CheckpointCompatibilityTests(keras_parameterized.TestCase):\n             save_path=checkpoint_prefix,\n             global_step=root.optimizer.iterations)\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testLoadFromNameBasedSaver(self):\n     \"\"\"Save a name-based checkpoint, load it using the object-based API.\"\"\"\n-    with testing_utils.device(should_use_gpu=True):\n+    with test_utils.device(should_use_gpu=True):\n       with self.test_session():\n         save_path = self._write_name_based_checkpoint()\n         root = self._initialized_model()\n\n@@ -19,10 +19,9 @@ import tensorflow.compat.v2 as tf\n import functools\n import os\n from tensorflow.python.eager import context\n-from tensorflow.python.framework import test_util\n-from keras import combinations\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.engine import training\n from keras.layers import core\n from tensorflow.python.training.tracking import util as trackable_utils\n@@ -52,9 +51,9 @@ class MyModel(training.Model):\n     return ret\n \n \n-class CheckpointingTests(keras_parameterized.TestCase):\n+class CheckpointingTests(test_combinations.TestCase):\n \n-  @test_util.run_in_graph_and_eager_modes(assert_no_eager_garbage=True)\n+  @tf_test_utils.run_in_graph_and_eager_modes(assert_no_eager_garbage=True)\n   def testNamingWithOptimizer(self):\n     input_value = tf.constant([[3.]])\n     model = MyModel()\n@@ -162,7 +161,8 @@ class CheckpointingTests(keras_parameterized.TestCase):\n             optimizer_node.slot_variables[0]\n             .slot_variable_node_id].attributes[0].checkpoint_key)\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testSaveRestore(self):\n     with self.test_session():\n       model = MyModel()\n@@ -363,7 +363,8 @@ class CheckpointingTests(keras_parameterized.TestCase):\n             self.assertEqual(training_continuation + 1,\n                              session.run(root.save_counter))\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testAgnosticUsage(self):\n     \"\"\"Graph/eager agnostic usage.\"\"\"\n     # Does create garbage when executing eagerly due to ops.Graph() creation.\n@@ -371,7 +372,7 @@ class CheckpointingTests(keras_parameterized.TestCase):\n       num_training_steps = 10\n       checkpoint_directory = self.get_temp_dir()\n       for training_continuation in range(3):\n-        with testing_utils.device(should_use_gpu=True):\n+        with test_utils.device(should_use_gpu=True):\n           model = MyModel()\n           optimizer = tf.compat.v1.train.AdamOptimizer(0.001)\n           root = tf.train.Checkpoint(\n@@ -397,14 +398,15 @@ class CheckpointingTests(keras_parameterized.TestCase):\n                            self.evaluate(root.save_counter))\n \n   # pylint: disable=cell-var-from-loop\n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testWithDefun(self):\n     with self.test_session():\n       num_training_steps = 2\n       checkpoint_directory = self.get_temp_dir()\n       checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n       for training_continuation in range(3):\n-        with testing_utils.device(should_use_gpu=True):\n+        with test_utils.device(should_use_gpu=True):\n           model = MyModel()\n           # Don't actually train so we can test variable values\n           optimizer = tf.compat.v1.train.AdamOptimizer(0.)\n@@ -441,7 +443,7 @@ class CheckpointingTests(keras_parameterized.TestCase):\n                            self.evaluate(root.save_counter))\n   # pylint: enable=cell-var-from-loop\n \n-  @combinations.generate(combinations.combine(mode=[\"eager\"]))\n+  @test_combinations.generate(test_combinations.combine(mode=[\"eager\"]))\n   def testAnonymousVarsInInit(self):\n \n     class Model(training.Model):\n@@ -470,13 +472,14 @@ class CheckpointingTests(keras_parameterized.TestCase):\n       optimizer.apply_gradients(\n           [(g, v) for g, v in zip(grad, model.vars)])\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def test_initialize_if_not_restoring(self):\n     with self.test_session():\n       checkpoint_directory = self.get_temp_dir()\n       checkpoint_prefix = os.path.join(checkpoint_directory, \"ckpt\")\n       optimizer_only_prefix = os.path.join(checkpoint_directory, \"opt\")\n-      with testing_utils.device(should_use_gpu=True):\n+      with test_utils.device(should_use_gpu=True):\n         model = MyModel()\n         optimizer = tf.compat.v1.train.AdamOptimizer(0.001)\n         root = tf.train.Checkpoint(\n@@ -503,7 +506,7 @@ class CheckpointingTests(keras_parameterized.TestCase):\n         optimizer_save_path = optimizer_checkpoint.save(optimizer_only_prefix)\n \n       # Restore into a graph with the optimizer\n-      with testing_utils.device(should_use_gpu=True):\n+      with test_utils.device(should_use_gpu=True):\n         model = MyModel()\n         optimizer = tf.compat.v1.train.AdamOptimizer(0.001)\n         root = tf.train.Checkpoint(\n@@ -525,7 +528,7 @@ class CheckpointingTests(keras_parameterized.TestCase):\n           status.assert_consumed()\n \n       # Make sure initialization doesn't clobber later restores\n-      with testing_utils.device(should_use_gpu=True):\n+      with test_utils.device(should_use_gpu=True):\n         model = MyModel()\n         optimizer = tf.compat.v1.train.AdamOptimizer(0.001, beta1=1.0)\n         root = tf.train.Checkpoint(\n@@ -550,7 +553,7 @@ class CheckpointingTests(keras_parameterized.TestCase):\n         self.assertEqual(42., self.evaluate(optimizer.variables()[0]))\n \n \n-class CheckpointCompatibilityTests(keras_parameterized.TestCase):\n+class CheckpointCompatibilityTests(test_combinations.TestCase):\n \n   def _initialized_model(self):\n     input_value = tf.constant([[3.]])\n@@ -605,10 +608,11 @@ class CheckpointCompatibilityTests(keras_parameterized.TestCase):\n             sess=session, save_path=checkpoint_prefix,\n             global_step=root.optimizer_step)\n \n-  @combinations.generate(combinations.combine(mode=[\"graph\", \"eager\"]))\n+  @test_combinations.generate(\n+      test_combinations.combine(mode=[\"graph\", \"eager\"]))\n   def testLoadFromNameBasedSaver(self):\n     \"\"\"Save a name-based checkpoint, load it using the object-based API.\"\"\"\n-    with testing_utils.device(should_use_gpu=True):\n+    with test_utils.device(should_use_gpu=True):\n       with self.test_session():\n         save_path = self._write_name_based_checkpoint()\n         root = self._initialized_model()\n\n@@ -30,7 +30,6 @@ from keras.utils.generic_utils import deserialize_keras_object\n from keras.utils.generic_utils import get_custom_objects\n from keras.utils.generic_utils import Progbar\n from keras.utils.generic_utils import serialize_keras_object\n-from keras.utils.keras_doctest_lib import KerasDoctestOutputChecker\n from keras.utils.layer_utils import get_source_inputs\n from keras.utils.np_utils import normalize\n from keras.utils.np_utils import to_categorical\n\n@@ -22,8 +22,8 @@ import numpy as np\n import scipy.sparse\n \n import keras\n-from keras import keras_parameterized\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.engine import input_layer\n from keras.layers import core\n from keras.layers import Dense\n@@ -114,7 +114,7 @@ def get_model_from_layers_with_input(layers,\n   if model_input is not None and input_shape is not None:\n     raise ValueError(\"Cannot specify a model_input and an input shape.\")\n \n-  model_type = testing_utils.get_model_type()\n+  model_type = test_utils.get_model_type()\n   if model_type == \"subclass\":\n     return _SubclassModel(layers, model_input)\n \n@@ -145,21 +145,21 @@ def get_model_from_layers_with_input(layers,\n \n \n def get_test_mode_kwargs():\n-  run_eagerly = testing_utils.should_run_eagerly()\n+  run_eagerly = test_utils.should_run_eagerly()\n   return {\n       \"run_eagerly\": run_eagerly,\n   }\n \n \n-@keras_parameterized.run_with_all_model_types\n-@keras_parameterized.run_all_keras_modes\n-class CompositeTensorInternalTest(keras_parameterized.TestCase):\n+@test_combinations.run_with_all_model_types\n+@test_combinations.run_all_keras_modes\n+class CompositeTensorInternalTest(test_combinations.TestCase):\n \n   def test_internal_ragged_tensors(self):\n     # Create a model that accepts an input, converts it to Ragged, and\n     # converts the ragged tensor back to a dense tensor.\n     layers = [ToRagged(padding=0), ToDense(default_value=-1)]\n-    model = testing_utils.get_model_from_layers(layers, input_shape=(None,))\n+    model = test_utils.get_model_from_layers(layers, input_shape=(None,))\n \n     # Define some input data with additional padding.\n     input_data = np.array([[1, 0, 0], [2, 3, 0]])\n@@ -171,7 +171,7 @@ class CompositeTensorInternalTest(keras_parameterized.TestCase):\n     # Create a model that accepts an input, converts it to Sparse, and\n     # converts the sparse tensor back to a dense tensor.\n     layers = [ToSparse(), ToDense(default_value=-1)]\n-    model = testing_utils.get_model_from_layers(layers, input_shape=(None,))\n+    model = test_utils.get_model_from_layers(layers, input_shape=(None,))\n \n     # Define some input data with additional padding.\n     input_data = np.array([[1, 0, 0], [2, 3, 0]])\n@@ -185,7 +185,7 @@ class CompositeTensorInternalTest(keras_parameterized.TestCase):\n     # for this test, as ToSparse() doesn't support gradient propagation through\n     # the layer.) TODO(b/124796939): Investigate this.\n     layers = [core.Dense(2), ToRagged(padding=0), ToDense(default_value=-1)]\n-    model = testing_utils.get_model_from_layers(layers, input_shape=(1,))\n+    model = test_utils.get_model_from_layers(layers, input_shape=(1,))\n \n     input_data = np.random.rand(1024, 1)\n     expected_data = np.concatenate((input_data * 3, input_data * .5), axis=-1)\n@@ -198,16 +198,16 @@ class CompositeTensorInternalTest(keras_parameterized.TestCase):\n     self.assertNotEqual(history.history[\"loss\"][-1], history.history[\"loss\"][0])\n \n \n-@keras_parameterized.run_with_all_model_types\n-@keras_parameterized.run_all_keras_modes\n-class CompositeTensorOutputTest(keras_parameterized.TestCase):\n+@test_combinations.run_with_all_model_types\n+@test_combinations.run_all_keras_modes\n+class CompositeTensorOutputTest(test_combinations.TestCase):\n \n   def test_ragged_tensor_outputs(self):\n     # Create a model that accepts an input, converts it to Ragged, and\n     # converts the ragged tensor back to a dense tensor.\n     layers = [ToRagged(padding=0)]\n-    model = testing_utils.get_model_from_layers(layers, input_shape=(None,))\n-    model._run_eagerly = testing_utils.should_run_eagerly()\n+    model = test_utils.get_model_from_layers(layers, input_shape=(None,))\n+    model._run_eagerly = test_utils.should_run_eagerly()\n \n     # Define some input data with additional padding.\n     input_data = np.array([[1, 0, 0], [2, 3, 0]])\n@@ -220,8 +220,8 @@ class CompositeTensorOutputTest(keras_parameterized.TestCase):\n     # Create a model that accepts an input, converts it to Ragged, and\n     # converts the ragged tensor back to a dense tensor.\n     layers = [ToRagged(padding=0)]\n-    model = testing_utils.get_model_from_layers(layers, input_shape=(None,))\n-    model._run_eagerly = testing_utils.should_run_eagerly()\n+    model = test_utils.get_model_from_layers(layers, input_shape=(None,))\n+    model._run_eagerly = test_utils.should_run_eagerly()\n \n     # Define some input data with additional padding.\n     input_data = np.array([[1, 0, 0], [2, 3, 0], [4, 0, 0], [5, 6, 0]])\n@@ -234,8 +234,8 @@ class CompositeTensorOutputTest(keras_parameterized.TestCase):\n     # Create a model that accepts an input, converts it to Ragged, and\n     # converts the ragged tensor back to a dense tensor.\n     layers = [ToSparse()]\n-    model = testing_utils.get_model_from_layers(layers, input_shape=(None,))\n-    model._run_eagerly = testing_utils.should_run_eagerly()\n+    model = test_utils.get_model_from_layers(layers, input_shape=(None,))\n+    model._run_eagerly = test_utils.should_run_eagerly()\n \n     # Define some input data with additional padding.\n     input_data = np.array([[1, 0, 0], [2, 3, 0]])\n@@ -253,8 +253,8 @@ class CompositeTensorOutputTest(keras_parameterized.TestCase):\n     # Create a model that accepts an input, converts it to Ragged, and\n     # converts the ragged tensor back to a dense tensor.\n     layers = [ToSparse()]\n-    model = testing_utils.get_model_from_layers(layers, input_shape=(None,))\n-    model._run_eagerly = testing_utils.should_run_eagerly()\n+    model = test_utils.get_model_from_layers(layers, input_shape=(None,))\n+    model._run_eagerly = test_utils.should_run_eagerly()\n \n     # Define some input data with additional padding.\n     input_data = np.array([[1, 0, 0], [2, 3, 0], [4, 0, 0], [5, 6, 0]])\n@@ -274,7 +274,7 @@ def get_input_name(use_dict):\n   # Define the input name.\n   if not use_dict:\n     return None  # This is the same as not setting 'name'.\n-  elif testing_utils.get_model_type() == \"subclass\":\n+  elif test_utils.get_model_type() == \"subclass\":\n     return \"input_1\"  # Subclass models don\"t support input names.\n   else:\n     return \"test_input_name\"\n@@ -306,14 +306,14 @@ def prepare_inputs(data, use_dict, use_dataset, action, input_name):\n   return (input_data, expected_output)\n \n \n-@keras_parameterized.run_with_all_model_types\n-@keras_parameterized.run_all_keras_modes\n+@test_combinations.run_with_all_model_types\n+@test_combinations.run_all_keras_modes\n @parameterized.named_parameters(\n-    *testing_utils.generate_combinations_with_testcase_name(\n+    *test_utils.generate_combinations_with_testcase_name(\n         use_dict=[True, False],\n         use_dataset=[True, False],\n         action=[\"predict\", \"evaluate\", \"fit\"]))\n-class SparseTensorInputTest(keras_parameterized.TestCase):\n+class SparseTensorInputTest(test_combinations.TestCase):\n \n   def test_sparse_tensors(self, use_dict, use_dataset, action):\n     data = [(tf.SparseTensor([[0, 0, 0], [1, 0, 0], [1, 0, 1]],\n@@ -354,9 +354,9 @@ class SparseTensorInputTest(keras_parameterized.TestCase):\n         _ = model.fit(input_data, expected_output, shuffle=False, **kwargs)\n \n \n-@keras_parameterized.run_with_all_model_types\n-@keras_parameterized.run_all_keras_modes\n-class ScipySparseTensorInputTest(keras_parameterized.TestCase,\n+@test_combinations.run_with_all_model_types\n+@test_combinations.run_all_keras_modes\n+class ScipySparseTensorInputTest(test_combinations.TestCase,\n                                  tf.test.TestCase):\n \n   def test_sparse_scipy_predict_inputs_via_input_layer_args(self):\n@@ -408,7 +408,7 @@ class ScipySparseTensorInputTest(keras_parameterized.TestCase,\n     # Create a model that accepts a sparse input and converts the sparse tensor\n     # back to a dense tensor. Scipy sparse matrices are limited to 2D, so use\n     # a one-dimensional shape; note also that scipy's default dtype is int64.\n-    if testing_utils.get_model_type() == \"subclass\":\n+    if test_utils.get_model_type() == \"subclass\":\n       input_name = \"input_1\"  # Subclass models don\"t support input names.\n     else:\n       input_name = \"test_input_name\"\n@@ -439,7 +439,7 @@ class ScipySparseTensorInputTest(keras_parameterized.TestCase,\n     # Create a model that accepts a sparse input and converts the sparse tensor\n     # back to a dense tensor. Scipy sparse matrices are limited to 2D, so use\n     # a one-dimensional shape; note also that scipy's default dtype is int64.\n-    if testing_utils.get_model_type() == \"subclass\":\n+    if test_utils.get_model_type() == \"subclass\":\n       input_name = \"input_1\"  # Subclass models don\"t support input names.\n     else:\n       input_name = \"test_input_name\"\n@@ -471,14 +471,14 @@ class ScipySparseTensorInputTest(keras_parameterized.TestCase,\n     self.assertAllEqual(1.0, output_2[-1])\n \n \n-@keras_parameterized.run_with_all_model_types\n-@keras_parameterized.run_all_keras_modes\n+@test_combinations.run_with_all_model_types\n+@test_combinations.run_all_keras_modes\n @parameterized.named_parameters(\n-    *testing_utils.generate_combinations_with_testcase_name(\n+    *test_utils.generate_combinations_with_testcase_name(\n         use_dict=[True, False],\n         use_dataset=[True, False],\n         action=[\"predict\", \"evaluate\", \"fit\"]))\n-class RaggedTensorInputTest(keras_parameterized.TestCase,\n+class RaggedTensorInputTest(test_combinations.TestCase,\n                             tf.test.TestCase):\n \n   def test_ragged_input(self, use_dict, use_dataset, action):\n@@ -518,12 +518,12 @@ class RaggedTensorInputTest(keras_parameterized.TestCase,\n         _ = model.fit(input_data, expected_output, shuffle=False)\n \n \n-@keras_parameterized.run_with_all_model_types\n-@keras_parameterized.run_all_keras_modes\n+@test_combinations.run_with_all_model_types\n+@test_combinations.run_all_keras_modes\n @parameterized.named_parameters(\n-    *testing_utils.generate_combinations_with_testcase_name(\n+    *test_utils.generate_combinations_with_testcase_name(\n         use_dict=[True, False], use_dataset=[True, False]))\n-class RaggedTensorInputValidationTest(keras_parameterized.TestCase,\n+class RaggedTensorInputValidationTest(test_combinations.TestCase,\n                                       tf.test.TestCase):\n \n   def test_ragged_tensor_input_with_one_none_dimension(self, use_dict,\n@@ -586,9 +586,9 @@ class RaggedTensorInputValidationTest(keras_parameterized.TestCase,\n       self.assertAllEqual(expected_output, result)\n \n \n-@keras_parameterized.run_with_all_model_types()\n-@keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n-class CompositeTensorModelPredictTest(keras_parameterized.TestCase):\n+@test_combinations.run_with_all_model_types()\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n+class CompositeTensorModelPredictTest(test_combinations.TestCase):\n \n   def _normalize_shape(self, shape):\n     if not isinstance(shape, tuple):\n\n@@ -18,8 +18,8 @@ import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n from tensorflow.python.distribute.cluster_resolver import SimpleClusterResolver\n-from keras import combinations\n-from keras import testing_utils\n+from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.distribute import multi_worker_testing_utils\n from keras.engine import data_adapter\n from keras.engine import sequential\n@@ -29,7 +29,7 @@ from keras.utils import dataset_creator\n from tensorflow.python.training.server_lib import ClusterSpec\n \n \n-@testing_utils.run_v2_only\n+@test_utils.run_v2_only\n class DatasetCreatorTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_dataset_creator(self):\n@@ -63,7 +63,8 @@ class DatasetCreatorTest(tf.test.TestCase, parameterized.TestCase):\n \n     return dataset_fn\n \n-  @combinations.generate(combinations.combine(use_input_options=[True, False]))\n+  @test_combinations.generate(\n+      test_combinations.combine(use_input_options=[True, False]))\n   def test_dataset_creator_model_fit_without_strategy(self, use_input_options):\n     model = sequential.Sequential([core_layers.Dense(10)])\n     model.compile(gradient_descent.SGD(), loss=\"mse\")\n@@ -82,7 +83,8 @@ class DatasetCreatorTest(tf.test.TestCase, parameterized.TestCase):\n     return tf.distribute.experimental.ParameterServerStrategy(\n         SimpleClusterResolver(ClusterSpec(cluster_def), rpc_layer=\"grpc\"))\n \n-  @combinations.generate(combinations.combine(use_input_options=[True, False]))\n+  @test_combinations.generate(\n+      test_combinations.combine(use_input_options=[True, False]))\n   def test_dataset_creator_usage_in_parameter_server_model_fit(\n       self, use_input_options):\n     strategy = self._get_parameter_server_strategy()\n\n@@ -18,12 +18,12 @@ import builtins\n from pathlib import Path\n import sys\n \n-from keras import keras_parameterized\n+from keras.testing_infra import test_combinations\n from keras.utils import io_utils\n import tensorflow.compat.v2 as tf\n \n \n-class TestIOUtils(keras_parameterized.TestCase):\n+class TestIOUtils(test_combinations.TestCase):\n \n   def test_ask_to_proceed_with_overwrite(self):\n     with tf.compat.v1.test.mock.patch.object(builtins, 'input') as mock_log:\n\n@@ -14,12 +14,12 @@\n # ==============================================================================\n \"\"\"Tests for losses_utils.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n-from keras import combinations\n+from keras.testing_infra import test_combinations\n from keras.utils import losses_utils\n+import tensorflow.compat.v2 as tf\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class RemoveSqueezableTest(tf.test.TestCase):\n   \"\"\"Test remove_squeezable_dimensions\"\"\"\n \n\n@@ -17,11 +17,11 @@\n import tensorflow.compat.v2 as tf\n \n from absl.testing import parameterized\n-from keras import combinations\n+from keras.testing_infra import test_combinations\n from keras.utils import metrics_utils\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class RaggedSizeOpTest(tf.test.TestCase, parameterized.TestCase):\n \n   @parameterized.parameters([\n@@ -240,7 +240,7 @@ class RaggedSizeOpTest(tf.test.TestCase, parameterized.TestCase):\n           metrics_utils.ragged_assert_compatible_and_get_flat_values([x, y])\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class FilterTopKTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_one_dimensional(self):\n\n@@ -16,7 +16,7 @@\n \n from absl.testing import parameterized\n import keras\n-from keras import combinations\n+from keras.testing_infra import test_combinations\n from keras.utils import tf_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n@@ -27,7 +27,7 @@ except ImportError:\n   attr = None\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+@test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class TestIsSymbolicTensor(tf.test.TestCase, parameterized.TestCase):\n \n   def test_default_behavior(self):\n\n@@ -17,17 +17,17 @@\n import abc\n \n import keras\n-from keras import keras_parameterized\n from keras.engine import base_layer\n from keras.engine import base_layer_v1\n from keras.engine import training\n from keras.engine import training_v1\n+from keras.testing_infra import test_combinations\n import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-@keras_parameterized.run_all_keras_modes\n-class SplitUtilsTest(keras_parameterized.TestCase):\n+@test_combinations.run_all_keras_modes\n+class SplitUtilsTest(test_combinations.TestCase):\n \n   def _check_model_class(self, model_class):\n     if tf.compat.v1.executing_eagerly_outside_functions():\n\n@@ -21,7 +21,7 @@ import tensorflow.compat.v2 as tf\n import numpy as np\n \n import keras\n-from keras import testing_utils\n+from keras.testing_infra import test_utils\n from keras.wrappers import scikit_learn\n \n INPUT_DIM = 5\n@@ -48,7 +48,7 @@ def build_fn_clf(hidden_dim):\n \n def assert_classification_works(clf):\n   np.random.seed(42)\n-  (x_train, y_train), (x_test, _) = testing_utils.get_test_data(\n+  (x_train, y_train), (x_test, _) = test_utils.get_test_data(\n       train_samples=TRAIN_SAMPLES,\n       test_samples=TEST_SAMPLES,\n       input_shape=(INPUT_DIM,),\n@@ -84,7 +84,7 @@ def build_fn_reg(hidden_dim):\n \n def assert_regression_works(reg):\n   np.random.seed(42)\n-  (x_train, y_train), (x_test, _) = testing_utils.get_test_data(\n+  (x_train, y_train), (x_test, _) = test_utils.get_test_data(\n       train_samples=TRAIN_SAMPLES,\n       test_samples=TEST_SAMPLES,\n       input_shape=(INPUT_DIM,),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
