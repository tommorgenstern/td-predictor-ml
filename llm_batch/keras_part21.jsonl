{"custom_id": "keras#32f483fe33fc9ff5474067b4a284616def479d8f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 214 | Contributors (this commit): 5 | Commits (past 90d): 21 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -13,7 +13,7 @@ def mean_absolute_error(y_true, y_pred):\n     return T.abs_(y_pred - y_true).mean(axis=-1)\n \n def mean_absolute_percentage_error(y_true, y_pred):\n-    return T.abs_((y_true - y_pred) / y_true).mean() * 100\n+    return T.abs_((y_true - y_pred) / y_true).mean(axis=-1) * 100\n \n def mean_squared_logarithmic_error(y_true, y_pred):\n     return T.sqr(T.log(T.clip(y_pred, epsilon, np.inf) + 1.) - T.log(T.clip(y_true, epsilon, np.inf) + 1.)).mean(axis=-1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#be75548ca354fc9113bba16b8c83ae8fd1b031ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 12 | Methods Changed: 11 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 32 | Churn Cumulative: 2425 | Contributors (this commit): 17 | Commits (past 90d): 91 | Contributors (cumulative): 20 | DMM Complexity: 0.8846153846153846\n\nDIFF:\n@@ -106,6 +106,9 @@ class Graph(Layer):\n         self.input_order = [] # strings\n         self.outputs = {} # layer-like\n         self.output_order = [] # strings\n+        self.input_config = [] # dicts\n+        self.output_config = [] # dicts\n+        self.node_config = [] # dicts\n \n         self.params = []\n         self.regularizers = []\n@@ -142,8 +145,10 @@ class Graph(Layer):\n         layer.input = ndim_tensor(ndim)\n         layer.input.name = name\n         self.inputs[name] = layer\n+        self.output_config.append({'name':name, 'ndim':ndim})\n \n     def add_node(self, layer, name, input=None, inputs=[], merge_mode='concat'):\n+        if hasattr(layer, 'set_name'):\n             layer.set_name(name)\n         if name in self.namespace:\n             raise Exception('Duplicate node identifier: ' + name)\n@@ -165,6 +170,7 @@ class Graph(Layer):\n \n         self.namespace.add(name)\n         self.nodes[name] = layer\n+        self.node_config.append({'name':name, 'input':input, 'inputs':inputs, 'merge_mode':merge_mode})\n         params, regularizers, constraints = layer.get_params()\n         self.params += params\n         self.regularizers += regularizers\n@@ -190,9 +196,14 @@ class Graph(Layer):\n             self.outputs[name] = merge\n         self.namespace.add(name)\n         self.output_order.append(name)\n+        self.output_config.append({'name':name, 'input':input, 'inputs':inputs, 'merge_mode':merge_mode})\n \n     def get_config(self):\n-        pass\n+        return {\"name\":self.__class__.__name__,\n+            \"input_config\":self.input_config,\n+            \"output_config\":self.output_config,\n+            \"node_config\":self.node_config,\n+            \"nodes\":[self.nodes[c[\"name\"]].get_config() for c in self.node_config]}\n \n \n \n\n@@ -10,7 +10,7 @@ from . import objectives\n from . import regularizers\n from . import constraints\n from . import callbacks as cbks\n-import time, copy\n+import time, copy, pprint\n from .utils.generic_utils import Progbar, printv\n from .layers import containers\n from six.moves import range\n@@ -293,6 +293,13 @@ class Sequential(Model, containers.Sequential):\n         self._test_with_acc = theano.function(test_ins, [test_loss, test_accuracy],\n             allow_input_downcast=True, mode=theano_mode)\n \n+    def train(self, X, y, accuracy=False, sample_weight=None):\n+        warnings.warn('The \"train\" method is deprecated, use \"train_on_batch\" instead.')\n+        return self.train_on_batch(X, y, accuracy, sample_weight)\n+\n+    def test(self, X, y, accuracy=False):\n+        warnings.warn('The \"test\" method is deprecated, use \"test_on_batch\" instead.')\n+        return self.test_on_batch(X, y, accuracy)\n \n     def train_on_batch(self, X, y, accuracy=False, sample_weight=None):\n         X = standardize_X(X)\n@@ -576,3 +583,10 @@ class Graph(Model, containers.Graph):\n         weights = [g['param_{}'.format(p)] for p in range(g.attrs['nb_params'])]\n         self.set_weights(weights)\n         f.close()\n+\n+    def get_config(self, verbose=1):\n+        config = super(Graph, self).get_config()\n+        if verbose:\n+            pp = pprint.PrettyPrinter(indent=4)\n+            pp.pprint(config)\n+        return config\n\n@@ -63,6 +63,7 @@ class TestRegularizers(unittest.TestCase):\n         loss = graph.evaluate({'input1':X_test, 'output1':y_test})\n         print(loss)\n         assert(loss < 1.4)\n+        graph.get_config(verbose=1)\n \n     def test_1o_2i(self):\n         print('test a non-sequential graph with 2 inputs and 1 output')\n@@ -86,6 +87,7 @@ class TestRegularizers(unittest.TestCase):\n         loss = graph.evaluate({'input1':X_test, 'input2':X2_test, 'output1':y_test})\n         print(loss)\n         assert(loss < 1.4)\n+        graph.get_config(verbose=1)\n \n     def test_2o_1i_weights(self):\n         print('test a non-sequential graph with 1 input and 2 outputs')\n@@ -149,6 +151,7 @@ class TestRegularizers(unittest.TestCase):\n \n         loss = seq.evaluate(X_test, y_test, show_accuracy=True)\n         pred = seq.predict(X_test)\n+        seq.get_config(verbose=1)\n \n if __name__ == '__main__':\n     print('Test graph model')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dab55518bacc70c2f44ca11787c346b821343316", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 180 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -73,7 +73,7 @@ if not data_augmentation:\n     X_test = X_test.astype(\"float32\")\n     X_train /= 255\n     X_test /= 255\n-    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=10)\n+    model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch)\n     score = model.evaluate(X_test, Y_test, batch_size=batch_size)\n     print('Test score:', score)\n \n@@ -105,14 +105,14 @@ else:\n         # batch train with realtime data augmentation\n         progbar = generic_utils.Progbar(X_train.shape[0])\n         for X_batch, Y_batch in datagen.flow(X_train, Y_train):\n-            loss = model.train(X_batch, Y_batch)\n+            loss = model.train_on_batch(X_batch, Y_batch)\n             progbar.add(X_batch.shape[0], values=[(\"train loss\", loss)])\n \n         print(\"Testing...\")\n         # test time!\n         progbar = generic_utils.Progbar(X_test.shape[0])\n         for X_batch, Y_batch in datagen.flow(X_test, Y_test):\n-            score = model.test(X_batch, Y_batch)\n+            score = model.test_on_batch(X_batch, Y_batch)\n             progbar.add(X_batch.shape[0], values=[(\"test loss\", score)])\n \n             \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#53a05b6e4c1e7ee2c6d13eda9826f5bc9a321391", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1944 | Contributors (this commit): 17 | Commits (past 90d): 80 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -207,7 +207,7 @@ class Model(object):\n                     for batch_out in enumerate(batch_outs):\n                         outs.append(0.)\n                 for i, batch_out in enumerate(batch_outs):\n-                    outs[i] += batch_out\n+                    outs[i] += batch_out * len(batch_ids)\n             else:\n                 if batch_index == 0:\n                     outs.append(0.)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8bd8ae1daa432bce9881214c4d326ac8a38e2046", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 216 | Contributors (this commit): 5 | Commits (past 90d): 22 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -13,7 +13,7 @@ def mean_absolute_error(y_true, y_pred):\n     return T.abs_(y_pred - y_true).mean(axis=-1)\n \n def mean_absolute_percentage_error(y_true, y_pred):\n-    return T.abs_((y_true - y_pred) / y_true).mean(axis=-1) * 100\n+    return T.abs_((y_true - y_pred) / T.clip(T.abs_(y_true), epsilon, np.inf)).mean(axis=-1) * 100.\n \n def mean_squared_logarithmic_error(y_true, y_pred):\n     return T.sqr(T.log(T.clip(y_pred, epsilon, np.inf) + 1.) - T.log(T.clip(y_true, epsilon, np.inf) + 1.)).mean(axis=-1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e014b9435f48b0508b4554049c43b99749ef5f20", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 21 | Churn Cumulative: 1265 | Contributors (this commit): 12 | Commits (past 90d): 62 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -208,6 +208,27 @@ class Dropout(MaskedLayer):\n             \"p\":self.p}\n \n \n+class GaussianNoise(MaskedLayer):\n+    '''\n+        Corruption process with GaussianNoise\n+    '''\n+    def __init__(self, sigma):\n+        super(GaussianNoise, self).__init__()\n+        self.sigma = sigma\n+\n+    def get_output(self, train=False):\n+        X = self.get_input(train)\n+        if train or self.sigma == 0:\n+            return X\n+        else:\n+            return X + srng.normal(size=X.shape, avg=0.0, std=self.sigma,\n+                             dtype=theano.config.floatX)\n+\n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+            \"sigma\":self.sigma}\n+\n+\n class Activation(MaskedLayer):\n     '''\n         Apply an activation function to an output.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8995b50a96a118b1db3410915bb8107a51795b35", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 35 | Files Changed: 2 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 43 | Churn Cumulative: 2134 | Contributors (this commit): 15 | Commits (past 90d): 97 | Contributors (cumulative): 18 | DMM Complexity: 0.0\n\nDIFF:\n@@ -494,34 +494,6 @@ class AutoEncoder(Layer):\n                 \"tie_weights\":self.tie_weights}\n \n \n-class DenoisingAutoEncoder(AutoEncoder):\n-    '''\n-        A denoising autoencoder model that inherits the base features from autoencoder\n-    '''\n-    def __init__(self, encoder=None, decoder=None, output_reconstruction=True, tie_weights=False, weights=None, corruption_level=0.3):\n-        super(DenoisingAutoEncoder, self).__init__(encoder, decoder, output_reconstruction, tie_weights, weights)\n-        self.corruption_level = corruption_level\n-\n-    def _corrupt_input(self, X):\n-        \"\"\"\n-            http://deeplearning.net/tutorial/dA.html\n-        \"\"\"\n-        return X * srng.binomial(size=X.shape, n=1,\n-                             p=1-self.corruption_level,\n-                             dtype=theano.config.floatX)\n-\n-    def get_input(self, train=False):\n-        uncorrupted_input = super(DenoisingAutoEncoder, self).get_input(train)\n-        return self._corrupt_input(uncorrupted_input)\n-\n-    def get_config(self):\n-        return {\"name\":self.__class__.__name__,\n-                \"encoder_config\":self.encoder.get_config(),\n-                \"decoder_config\":self.decoder.get_config(),\n-                \"corruption_level\":self.corruption_level,\n-                \"output_reconstruction\":self.output_reconstruction,\n-                \"tie_weights\":self.tie_weights}\n-\n \n class MaxoutDense(Layer):\n     '''\n\n@@ -71,7 +71,7 @@ class SimpleRNN(Recurrent):\n         '''\n         return self.activation(x_t + mask_tm1 * T.dot(h_tm1, u))\n \n-    def get_output(self, train):\n+    def get_output(self, train=False):\n         X = self.get_input(train) # shape: (nb_samples, time (padded with zeros), input_dim)\n         # new shape: (time, nb_samples, input_dim) -> because theano.scan iterates over main dimension\n         padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n@@ -149,7 +149,7 @@ class SimpleDeepRNN(Recurrent):\n             o += mask_tmi*self.inner_activation(T.dot(h_tmi, U_tmi))\n         return self.activation(o)\n \n-    def get_output(self, train):\n+    def get_output(self, train=False):\n         X = self.get_input(train)\n         padded_mask = self.get_padded_shuffled_mask(train, X, pad=self.depth)\n         X = X.dimshuffle((1, 0, 2)) \n@@ -263,7 +263,7 @@ class GRU(Recurrent):\n         h_t = z * h_mask_tm1 + (1 - z) * hh_t\n         return h_t\n \n-    def get_output(self, train):\n+    def get_output(self, train=False):\n         X = self.get_input(train) \n         padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n         X = X.dimshuffle((1, 0, 2)) \n@@ -379,7 +379,7 @@ class LSTM(Recurrent):\n         h_t = o_t * self.activation(c_t)\n         return h_t, c_t\n \n-    def get_output(self, train):\n+    def get_output(self, train=False):\n         X = self.get_input(train) \n         padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n         X = X.dimshuffle((1, 0, 2))\n@@ -493,7 +493,7 @@ class JZS1(Recurrent):\n         h_t = hh_t * z + h_mask_tm1 * (1 - z)\n         return h_t\n \n-    def get_output(self, train):\n+    def get_output(self, train=False):\n         X = self.get_input(train) \n         padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n         X = X.dimshuffle((1, 0, 2))\n@@ -601,7 +601,7 @@ class JZS2(Recurrent):\n         h_t = hh_t * z + h_mask_tm1 * (1 - z)\n         return h_t\n \n-    def get_output(self, train):\n+    def get_output(self, train=False):\n         X = self.get_input(train)\n         padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n         X = X.dimshuffle((1, 0, 2)) \n@@ -702,7 +702,7 @@ class JZS3(Recurrent):\n         h_t = hh_t * z + h_mask_tm1 * (1 - z)\n         return h_t\n \n-    def get_output(self, train):\n+    def get_output(self, train=False):\n         X = self.get_input(train)\n         padded_mask = self.get_padded_shuffled_mask(train, X, pad=1)\n         X = X.dimshuffle((1, 0, 2)) \n@@ -734,3 +734,4 @@ class JZS3(Recurrent):\n \n \n \n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#63f9a7955d81894a643bc292bcd522b3a391676a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 718 | Contributors (this commit): 4 | Commits (past 90d): 20 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -174,7 +174,7 @@ class ModelCheckpoint(Callback):\n     def __init__(self, filepath, monitor='val_loss', verbose=0, save_best_only=False):\n         super(Callback, self).__init__()\n         \n-        self.monitor\n+        self.monitor = monitor\n         self.verbose = verbose\n         self.filepath = filepath\n         self.save_best_only = save_best_only\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ddd5f47640126dae27d3a99d5663e8efbd27da89", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 1305 | Contributors (this commit): 12 | Commits (past 90d): 64 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -125,11 +125,13 @@ class Merge(object):\n         self.regularizers = []\n         self.constraints = []\n         for m in self.models:\n-            self.regularizers += m.regularizers\n-            for i in range(len(m.params)):\n-                if not m.params[i] in self.params:\n-                    self.params.append(m.params[i])\n-                    self.constraints.append(m.constraints[i])\n+            params, regs, consts = m.get_params()\n+            self.regularizers += regs\n+            # params and constraints have the same size\n+            for p, c in zip(params, consts):\n+                if not p in self.params:\n+                    self.params.append(p)\n+                    self.constraints.append(c)\n \n     def get_params(self):\n         return self.params, self.regularizers, self.constraints\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c315b0d7a95b6677452300bdefd526b444951819", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 2 | Churn Cumulative: 1307 | Contributors (this commit): 12 | Commits (past 90d): 65 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -218,7 +218,7 @@ class GaussianNoise(MaskedLayer):\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        if train or self.sigma == 0:\n+        if not train or self.sigma == 0:\n             return X\n         else:\n             return X + srng.normal(size=X.shape, avg=0.0, std=self.sigma,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fe3c4d73eb2ffd998d5f328d2ded796da9e30631", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 166 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,7 @@ import numpy as np\n np.random.seed(1337)\n from keras.models import Graph, Sequential\n from keras.layers import containers\n-from keras.layers.core import Dense\n+from keras.layers.core import Dense, Activation\n from keras.utils.test_utils import get_test_data\n \n X = np.random.random((100, 32))\n@@ -17,7 +17,7 @@ y2 = np.random.random((100, 4))\n (X2_train, y2_train), (X2_test, y2_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(32,),\n     classification=False, output_shape=(4,))\n \n-class TestRegularizers(unittest.TestCase):\n+class TestGraph(unittest.TestCase):\n     def test_1o_1i(self):\n         print('test a non-sequential graph with 1 input and 1 output')\n         graph = Graph()\n@@ -46,7 +46,8 @@ class TestRegularizers(unittest.TestCase):\n         graph.add_input(name='input1', ndim=2)\n \n         graph.add_node(Dense(32, 16), name='dense1', input='input1')\n-        graph.add_node(Dense(32, 4), name='dense2', input='input1')\n+        graph.add_node(Dense(32, 4), name='dense2-0', input='input1')\n+        graph.add_node(Activation('relu'), name='dense2', input='dense2-0')\n \n         graph.add_node(Dense(4, 16), name='dense3', input='dense2')\n         graph.add_node(Dense(16, 4), name='dense4', inputs=['dense1', 'dense3'], merge_mode='sum')\n@@ -153,6 +154,7 @@ class TestRegularizers(unittest.TestCase):\n         pred = seq.predict(X_test)\n         seq.get_config(verbose=1)\n \n+\n if __name__ == '__main__':\n     print('Test graph model')\n     unittest.main()\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b1d9448908f4d65b1948c4cfd513fd0368810ea6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 19 | Files Changed: 1 | Hunks: 10 | Methods Changed: 7 | Complexity Δ (Sum/Max): -9/0 | Churn Δ: 38 | Churn Cumulative: 1345 | Contributors (this commit): 12 | Commits (past 90d): 66 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -113,19 +113,19 @@ class MaskedLayer(Layer):\n \n \n class Merge(object): \n-    def __init__(self, models, mode='sum'):\n-        ''' Merge the output of a list of models into a single tensor.\n+    def __init__(self, layers, mode='sum'):\n+        ''' Merge the output of a list of layers or containers into a single tensor.\n             mode: {'sum', 'concat'}\n         '''\n-        if len(models) < 2:\n-            raise Exception(\"Please specify two or more input models to merge\")\n+        if len(layers) < 2:\n+            raise Exception(\"Please specify two or more input layers (or containers) to merge\")\n         self.mode = mode\n-        self.models = models\n+        self.layers = layers\n         self.params = []\n         self.regularizers = []\n         self.constraints = []\n-        for m in self.models:\n-            params, regs, consts = m.get_params()\n+        for l in self.layers:\n+            params, regs, consts = l.get_params()\n             self.regularizers += regs\n             # params and constraints have the same size\n             for p, c in zip(params, consts):\n@@ -138,20 +138,20 @@ class Merge(object):\n \n     def get_output(self, train=False):\n         if self.mode == 'sum':\n-            s = self.models[0].get_output(train)\n-            for i in range(1, len(self.models)):\n-                s += self.models[i].get_output(train)\n+            s = self.layers[0].get_output(train)\n+            for i in range(1, len(self.layers)):\n+                s += self.layers[i].get_output(train)\n             return s\n         elif self.mode == 'concat':\n-            inputs = [self.models[i].get_output(train) for i in range(len(self.models))]\n+            inputs = [self.layers[i].get_output(train) for i in range(len(self.layers))]\n             return T.concatenate(inputs, axis=-1)\n         else:\n             raise Exception('Unknown merge mode')\n \n     def get_input(self, train=False):\n         res = []\n-        for i in range(len(self.models)):\n-            o = self.models[i].get_input(train)\n+        for i in range(len(self.layers)):\n+            o = self.layers[i].get_input(train)\n             if not type(o) == list:\n                 o = [o]\n             for output in o:\n@@ -171,19 +171,19 @@ class Merge(object):\n \n     def get_weights(self):\n         weights = []\n-        for m in self.models:\n-            weights += m.get_weights()\n+        for l in self.layers:\n+            weights += l.get_weights()\n         return weights\n \n     def set_weights(self, weights):\n-        for i in range(len(self.models)):\n-            nb_param = len(self.models[i].params)\n-            self.models[i].set_weights(weights[:nb_param])\n+        for i in range(len(self.layers)):\n+            nb_param = len(self.layers[i].params)\n+            self.layers[i].set_weights(weights[:nb_param])\n             weights = weights[nb_param:]\n \n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n-            \"models\":[m.get_config() for m in self.models],\n+            \"layers\":[l.get_config() for l in self.layers],\n             \"mode\":self.mode}\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#35bcd5a45a6599dcd4965bb93936e48cbed60788", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 183 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -3,6 +3,7 @@ from __future__ import print_function\n \n import numpy as np\n import pandas as pd\n+np.random.seed(1337) # for reproducibility\n \n from keras.models import Sequential\n from keras.layers.core import Dense, Dropout, Activation\n@@ -17,7 +18,7 @@ from sklearn.preprocessing import StandardScaler\n     This demonstrates how to reach a score of 0.4890 (local validation)\n     on the Kaggle Otto challenge, with a deep net using Keras.\n \n-    Compatible Python 2.7-3.4 \n+    Compatible Python 2.7-3.4. Requires Scikit-Learn and Pandas.\n \n     Recommended to run on GPU: \n         Command: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python kaggle_otto_nn.py\n@@ -35,8 +36,6 @@ from sklearn.preprocessing import StandardScaler\n     Get the data from Kaggle: https://www.kaggle.com/c/otto-group-product-classification-challenge/data\n '''\n \n-np.random.seed(1337) # for reproducibility\n-\n def load_data(path, train=True):\n     df = pd.read_csv(path)\n     X = df.values.copy()\n@@ -121,4 +120,3 @@ print(\"Generating submission...\")\n \n proba = model.predict_proba(X_test)\n make_submission(proba, ids, encoder, fname='keras-otto.csv')\n-\n\n@@ -20,7 +20,7 @@ from keras.utils import np_utils\n     arXiv:1504.00941v2 [cs.NE] 7 Apr 201\n     http://arxiv.org/pdf/1504.00941v2.pdf\n \n-    Optimizer is replaced with RMSprop which give more stable and steady\n+    Optimizer is replaced with RMSprop which yields more stable and steady\n     improvement.\n \n     0.80 train/test accuracy and 0.55 train/test loss after 70 epochs\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#aeb954dd49cdc72b520c35f177c31407e9bc9d06", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 10 | Churn Cumulative: 335 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -136,16 +136,22 @@ class Graph(Layer):\n                 when it has exactly one input and one output.')\n         return self.outputs[self.output_order[0]].get_output(train)\n \n-    def add_input(self, name, ndim=2):\n+    def add_input(self, name, ndim=2, dtype='float'):\n         if name in self.namespace:\n             raise Exception('Duplicate node identifier: ' + name)\n         self.namespace.add(name)\n         self.input_order.append(name)\n         layer = Layer() # empty layer\n+        if dtype == 'float':\n             layer.input = ndim_tensor(ndim)\n+        else:\n+            if ndim == 2:\n+                layer.input = T.imatrix()\n+            else:\n+                raise Exception('Type \"int\" can only be used with ndim==2.')\n         layer.input.name = name\n         self.inputs[name] = layer\n-        self.output_config.append({'name':name, 'ndim':ndim})\n+        self.output_config.append({'name':name, 'ndim':ndim, 'dtype':dtype})\n \n     def add_node(self, layer, name, input=None, inputs=[], merge_mode='concat'):\n         if hasattr(layer, 'set_name'):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ab8f7da83fc1e8859df543b386484cfb7e491723", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 21 | Files Changed: 2 | Hunks: 2 | Methods Changed: 6 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 44 | Churn Cumulative: 1389 | Contributors (this commit): 12 | Commits (past 90d): 68 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -210,27 +210,6 @@ class Dropout(MaskedLayer):\n             \"p\":self.p}\n \n \n-class GaussianNoise(MaskedLayer):\n-    '''\n-        Corruption process with GaussianNoise\n-    '''\n-    def __init__(self, sigma):\n-        super(GaussianNoise, self).__init__()\n-        self.sigma = sigma\n-\n-    def get_output(self, train=False):\n-        X = self.get_input(train)\n-        if not train or self.sigma == 0:\n-            return X\n-        else:\n-            return X + srng.normal(size=X.shape, avg=0.0, std=self.sigma,\n-                             dtype=theano.config.floatX)\n-\n-    def get_config(self):\n-        return {\"name\":self.__class__.__name__,\n-            \"sigma\":self.sigma}\n-\n-\n class Activation(MaskedLayer):\n     '''\n         Apply an activation function to an output.\n\n@@ -0,0 +1,23 @@\n+from __future__ import absolute_import\n+from .core import srng, MaskedLayer\n+import theano\n+\n+class GaussianNoise(MaskedLayer):\n+    '''\n+        Corruption process with GaussianNoise\n+    '''\n+    def __init__(self, sigma):\n+        super(GaussianNoise, self).__init__()\n+        self.sigma = sigma\n+\n+    def get_output(self, train=False):\n+        X = self.get_input(train)\n+        if not train or self.sigma == 0:\n+            return X\n+        else:\n+            return X + srng.normal(size=X.shape, avg=0.0, std=self.sigma,\n+                             dtype=theano.config.floatX)\n+\n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+            \"sigma\":self.sigma}\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d4f39c8f53bc10c241046641d03a1fe5411467c3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 342 | Contributors (this commit): 3 | Commits (past 90d): 12 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -168,9 +168,12 @@ class Graph(Layer):\n         if inputs:\n             to_merge = []\n             for n in inputs:\n-                if n not in self.nodes:\n-                    raise Exception('Unknown identifier: ' + n)\n+                if n in self.nodes:\n                     to_merge.append(self.nodes[n])\n+                elif n in self.inputs:\n+                    to_merge.append(self.inputs[n])\n+                else:\n+                    raise Exception('Unknown identifier: ' + n)\n             merge = Merge(to_merge, mode=merge_mode)\n             layer.set_previous(merge)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7094be1b44a21f4c10879adc16adf8e967f4e6bf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 2128 | Contributors (this commit): 17 | Commits (past 90d): 85 | Contributors (cumulative): 18 | DMM Complexity: None\n\nDIFF:\n@@ -506,12 +506,12 @@ class Graph(Model, containers.Graph):\n \n     def train_on_batch(self, data):\n         # data is a dictionary mapping output and input names to arrays\n-        ins = [data[name] for name in self.input_order] + [data[name] for name in self.output_order]\n+        ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order]\n         return self._train(*ins)\n \n     def test_on_batch(self, data):\n         # data is a dictionary mapping input names to arrays\n-        ins = [data[name] for name in self.input_order] + [data[name] for name in self.output_order]\n+        ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order]\n         return self._test(*ins)\n \n     def predict_on_batch(self, data):\n@@ -521,14 +521,14 @@ class Graph(Model, containers.Graph):\n \n     def fit(self, data, batch_size=128, nb_epoch=100, verbose=1, callbacks=[],\n             validation_split=0., validation_data=None, shuffle=True):\n-        ins = [data[name] for name in self.input_order] + [data[name] for name in self.output_order]\n+        ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order]\n \n         val_f = None\n         val_ins = None\n         if validation_data or validation_split:\n             val_f = self._test\n         if validation_data:\n-            val_ins = [validation_data[name] for name in self.input_order] + [validation_data[name] for name in self.output_order]\n+            val_ins = [validation_data[name] for name in self.input_order] + [standardize_y(validation_data[name]) for name in self.output_order]\n \n         f = self._train\n         out_labels = self.output_order\n@@ -538,7 +538,7 @@ class Graph(Model, containers.Graph):\n         return history\n \n     def evaluate(self, data, batch_size=128, verbose=0):\n-        ins = [data[name] for name in self.input_order] + [data[name] for name in self.output_order]\n+        ins = [data[name] for name in self.input_order] + [standardize_y(data[name]) for name in self.output_order]\n         outs = self._test_loop(self._test, ins, batch_size, verbose)\n         return outs[0]\n \n\n@@ -10,12 +10,12 @@ from keras.utils.test_utils import get_test_data\n X = np.random.random((100, 32))\n X2 = np.random.random((100, 32))\n y = np.random.random((100, 4))\n-y2 = np.random.random((100, 4))\n+y2 = np.random.random((100,))\n \n (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(32,),\n     classification=False, output_shape=(4,))\n (X2_train, y2_train), (X2_test, y2_test) = get_test_data(nb_train=1000, nb_test=200, input_shape=(32,),\n-    classification=False, output_shape=(4,))\n+    classification=False, output_shape=(1,))\n \n class TestGraph(unittest.TestCase):\n     def test_1o_1i(self):\n@@ -97,7 +97,7 @@ class TestGraph(unittest.TestCase):\n \n         graph.add_node(Dense(32, 16), name='dense1', input='input1')\n         graph.add_node(Dense(32, 4), name='dense2', input='input1')\n-        graph.add_node(Dense(16, 4), name='dense3', input='dense1')\n+        graph.add_node(Dense(16, 1), name='dense3', input='dense1')\n \n         graph.add_output(name='output1', input='dense2')\n         graph.add_output(name='output2', input='dense3')\n@@ -119,7 +119,7 @@ class TestGraph(unittest.TestCase):\n         graph.add_input(name='input1', ndim=2)\n         graph.add_node(Dense(32, 16), name='dense1', input='input1')\n         graph.add_node(Dense(32, 4), name='dense2', input='input1')\n-        graph.add_node(Dense(16, 4), name='dense3', input='dense1')\n+        graph.add_node(Dense(16, 1), name='dense3', input='dense1')\n         graph.add_output(name='output1', input='dense2')\n         graph.add_output(name='output2', input='dense3')\n         graph.compile('rmsprop', {'output1':'mse', 'output2':'mse'})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c354b9be5c463dbc0e56f137521603ef5a3cb2e5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 22 | Churn Cumulative: 45 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -21,3 +21,25 @@ class GaussianNoise(MaskedLayer):\n     def get_config(self):\n         return {\"name\":self.__class__.__name__,\n             \"sigma\":self.sigma}\n+\n+class GaussianDropout(Layer):\n+    '''\n+        Multiplicative Gaussian Noise\n+        Reference: \n+            Dropout: A Simple Way to Prevent Neural Networks from Overfitting\n+            Srivastava, Hinton, et al. 2014\n+    '''\n+    def __init__(self, p):\n+        super(GaussianDropout,self).__init__()\n+        self.p = p\n+\n+    def get_output(self, train):\n+        X = self.get_input(train)\n+        if train:\n+            # self.p refers to drop probability rather than retain probability (as in paper) to match Dropout layer syntax\n+            X *= srng.normal(size=X.shape, avg = 1., std=T.sqrt(self.p/(1-self.p)), dtype=theano.config.floatX)\n+        return X\n+\n+    def get_config(self):\n+        return {\"name\":self.__class__.__name__,\n+            \"p\":self.p}\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0cae63accbff19eb727d8da83c6e92e46b8b02b9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 47 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -37,7 +37,7 @@ class GaussianDropout(Layer):\n         X = self.get_input(train)\n         if train:\n             # self.p refers to drop probability rather than retain probability (as in paper) to match Dropout layer syntax\n-            X *= srng.normal(size=X.shape, avg = 1., std=T.sqrt(self.p/(1-self.p)), dtype=theano.config.floatX)\n+            X *= srng.normal(size=X.shape, avg=1.0, std=T.sqrt(self.p / (1.0 - self.p)), dtype=theano.config.floatX)\n         return X\n \n     def get_config(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8213e515d3e942c08d7094935a447a48e18467db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 48 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -28,6 +28,7 @@ class GaussianDropout(Layer):\n         Reference: \n             Dropout: A Simple Way to Prevent Neural Networks from Overfitting\n             Srivastava, Hinton, et al. 2014\n+            http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf\n     '''\n     def __init__(self, p):\n         super(GaussianDropout,self).__init__()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7ff158de41f99a9422309ba3f0aad4804a3a58f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 51 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -23,8 +23,8 @@ from keras.utils import np_utils\n     Optimizer is replaced with RMSprop which yields more stable and steady\n     improvement.\n \n-    0.80 train/test accuracy and 0.55 train/test loss after 70 epochs\n-    (it's still underfitting at that point, though).\n+    Reaches 0.93 train/test accuracy after 900 epochs (which roughly corresponds\n+    to 1687500 steps in the original paper.)\n '''\n \n batch_size = 32\n@@ -64,7 +64,7 @@ model.add(Activation('softmax'))\n rmsprop = RMSprop(lr=learning_rate)\n model.compile(loss='categorical_crossentropy', optimizer=rmsprop)\n \n-model.fit(X_train, Y_train, batch_size=16, nb_epoch=nb_epochs,\n+model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epochs,\n           show_accuracy=True, verbose=1, validation_data=(X_test, Y_test))\n \n scores = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)\n@@ -79,7 +79,7 @@ model.add(Activation('softmax'))\n rmsprop = RMSprop(lr=learning_rate)\n model.compile(loss='categorical_crossentropy', optimizer=rmsprop)\n \n-model.fit(X_train, Y_train, batch_size=16, nb_epoch=nb_epochs,\n+model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epochs,\n           show_accuracy=True, verbose=1, validation_data=(X_test, Y_test))\n \n scores = model.evaluate(X_test, Y_test, show_accuracy=True, verbose=0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#07f2253ff23d26126185fdb81e81d357fc6a7b71", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 278 | Lines Deleted: 213 | Files Changed: 2 | Hunks: 2 | Methods Changed: 5 | Complexity Δ (Sum/Max): 10/10 | Churn Δ: 491 | Churn Cumulative: 520 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,278 @@\n+from __future__ import absolute_import\n+from __future__ import print_function\n+import unittest\n+import numpy as np\n+np.random.seed(1337)\n+\n+from keras.models import Sequential\n+from keras.layers.core import Dense, Activation, Merge\n+from keras.utils import np_utils\n+from keras.utils.test_utils import get_test_data\n+\n+input_dim = 32\n+nb_hidden = 16\n+nb_class = 4\n+batch_size = 64\n+nb_epoch = 1\n+\n+train_samples = 5000\n+test_samples = 1000\n+\n+(X_train, y_train), (X_test, y_test) = get_test_data(nb_train=train_samples, nb_test=test_samples, input_shape=(input_dim,),\n+    classification=True, nb_class=4)\n+y_test = np_utils.to_categorical(y_test)\n+y_train = np_utils.to_categorical(y_train)\n+print(X_train.shape)\n+print(y_train.shape)\n+\n+class TestSequential(unittest.TestCase):\n+    def test_sequential(self):\n+        print('Test sequential')\n+        model = Sequential()\n+        model.add(Dense(input_dim, nb_hidden))\n+        model.add(Activation('relu'))\n+        model.add(Dense(nb_hidden, nb_class))\n+        model.add(Activation('softmax'))\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=1, validation_data=(X_test, y_test))\n+        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=2, validation_data=(X_test, y_test))\n+        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=2, validation_split=0.1)\n+        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=1, validation_split=0.1)\n+        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, shuffle=False)\n+\n+        model.train_on_batch(X_train[:32], y_train[:32])\n+\n+        loss = model.evaluate(X_train, y_train, verbose=0)\n+        print('loss:', loss)\n+        if loss > 0.6:\n+            raise Exception('Score too low, learning issue.')\n+        preds = model.predict(X_test, verbose=0)\n+        classes = model.predict_classes(X_test, verbose=0)\n+        probas = model.predict_proba(X_test, verbose=0)\n+        print(model.get_config(verbose=1))\n+\n+        print('test weight saving')\n+        model.save_weights('temp.h5', overwrite=True)\n+        model = Sequential()\n+        model.add(Dense(input_dim, nb_hidden))\n+        model.add(Activation('relu'))\n+        model.add(Dense(nb_hidden, nb_class))\n+        model.add(Activation('softmax'))\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+        model.load_weights('temp.h5')\n+\n+        nloss = model.evaluate(X_train, y_train, verbose=0)\n+        print(nloss)\n+        assert(loss == nloss)\n+\n+\n+    def test_merge_sum(self):\n+        print('Test merge: sum')\n+        left = Sequential()\n+        left.add(Dense(input_dim, nb_hidden))\n+        left.add(Activation('relu'))\n+\n+        right = Sequential()\n+        right.add(Dense(input_dim, nb_hidden))\n+        right.add(Activation('relu'))\n+\n+        model = Sequential()\n+        model.add(Merge([left, right], mode='sum'))\n+\n+        model.add(Dense(nb_hidden, nb_class))\n+        model.add(Activation('softmax'))\n+\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test], y_test))\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test], y_test))\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n+\n+        loss = model.evaluate([X_train, X_train], y_train, verbose=0)\n+        print('loss:', loss)\n+        if loss > 0.7:\n+            raise Exception('Score too low, learning issue.')\n+        preds = model.predict([X_test, X_test], verbose=0)\n+        classes = model.predict_classes([X_test, X_test], verbose=0)\n+        probas = model.predict_proba([X_test, X_test], verbose=0)\n+        print(model.get_config(verbose=1))\n+\n+        print('test weight saving')\n+        model.save_weights('temp.h5', overwrite=True)\n+        left = Sequential()\n+        left.add(Dense(input_dim, nb_hidden))\n+        left.add(Activation('relu'))\n+        right = Sequential()\n+        right.add(Dense(input_dim, nb_hidden))\n+        right.add(Activation('relu'))\n+        model = Sequential()\n+        model.add(Merge([left, right], mode='sum'))\n+        model.add(Dense(nb_hidden, nb_class))\n+        model.add(Activation('softmax'))\n+        model.load_weights('temp.h5')\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+        nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n+        print(nloss)\n+        assert(loss == nloss)\n+\n+\n+    def test_merge_concat(self):\n+        print('Test merge: concat')\n+        left = Sequential()\n+        left.add(Dense(input_dim, nb_hidden))\n+        left.add(Activation('relu'))\n+\n+        right = Sequential()\n+        right.add(Dense(input_dim, nb_hidden))\n+        right.add(Activation('relu'))\n+\n+        model = Sequential()\n+        model.add(Merge([left, right], mode='concat'))\n+\n+        model.add(Dense(nb_hidden * 2, nb_class))\n+        model.add(Activation('softmax'))\n+\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test], y_test))\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test], y_test))\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+        model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n+\n+        loss = model.evaluate([X_train, X_train], y_train, verbose=0)\n+        print('loss:', loss)\n+        if loss > 0.6:\n+            raise Exception('Score too low, learning issue.')\n+        preds = model.predict([X_test, X_test], verbose=0)\n+        classes = model.predict_classes([X_test, X_test], verbose=0)\n+        probas = model.predict_proba([X_test, X_test], verbose=0)\n+        print(model.get_config(verbose=1))\n+\n+        print('test weight saving')\n+        model.save_weights('temp.h5', overwrite=True)\n+        left = Sequential()\n+        left.add(Dense(input_dim, nb_hidden))\n+        left.add(Activation('relu'))\n+\n+        right = Sequential()\n+        right.add(Dense(input_dim, nb_hidden))\n+        right.add(Activation('relu'))\n+\n+        model = Sequential()\n+        model.add(Merge([left, right], mode='concat'))\n+\n+        model.add(Dense(nb_hidden * 2, nb_class))\n+        model.add(Activation('softmax'))\n+\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+        model.load_weights('temp.h5')\n+\n+        nloss = model.evaluate([X_train, X_train], y_train, verbose=0)\n+        print(nloss)\n+        assert(loss == nloss)\n+\n+\n+    def test_merge_recursivity(self):\n+        print('Test merge recursivity')\n+\n+        left = Sequential()\n+        left.add(Dense(input_dim, nb_hidden))\n+        left.add(Activation('relu'))\n+\n+        right = Sequential()\n+        right.add(Dense(input_dim, nb_hidden))\n+        right.add(Activation('relu'))\n+\n+        righter = Sequential()\n+        righter.add(Dense(input_dim, nb_hidden))\n+        righter.add(Activation('relu'))\n+\n+        intermediate = Sequential()\n+        intermediate.add(Merge([left, right], mode='sum'))\n+        intermediate.add(Dense(nb_hidden, nb_hidden))\n+        intermediate.add(Activation('relu'))\n+\n+        model = Sequential()\n+        model.add(Merge([intermediate, righter], mode='sum'))\n+\n+        model.add(Dense(nb_hidden, nb_class))\n+        model.add(Activation('softmax'))\n+\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+        model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test, X_test], y_test))\n+        model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test, X_test], y_test))\n+        model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n+        model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n+        model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+        model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n+\n+        loss = model.evaluate([X_train, X_train, X_train], y_train, verbose=0)\n+        print('loss:', loss)\n+        if loss > 0.6:\n+            raise Exception('Score too low, learning issue.')\n+        preds = model.predict([X_test, X_test, X_test], verbose=0)\n+        classes = model.predict_classes([X_test, X_test, X_test], verbose=0)\n+        probas = model.predict_proba([X_test, X_test, X_test], verbose=0)\n+        print(model.get_config(verbose=1))\n+\n+        model.save_weights('temp.h5', overwrite=True)\n+        model.load_weights('temp.h5')\n+\n+        nloss = model.evaluate([X_train, X_train, X_train], y_train, verbose=0)\n+        print(nloss)\n+        assert(loss == nloss)\n+\n+\n+    def test_merge_overlap(self):\n+        print('Test merge overlap')\n+        left = Sequential()\n+        left.add(Dense(input_dim, nb_hidden))\n+        left.add(Activation('relu'))\n+\n+        model = Sequential()\n+        model.add(Merge([left, left], mode='sum'))\n+\n+        model.add(Dense(nb_hidden, nb_class))\n+        model.add(Activation('softmax'))\n+\n+        model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=1, validation_data=(X_test, y_test))\n+        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=2, validation_data=(X_test, y_test))\n+        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=2, validation_split=0.1)\n+        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=1, validation_split=0.1)\n+        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n+        model.fit(X_train, y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1, shuffle=False)\n+\n+        model.train_on_batch(X_train[:32], y_train[:32])\n+\n+        loss = model.evaluate(X_train, y_train, verbose=0)\n+        print('loss:', loss)\n+        if loss > 0.6:\n+            raise Exception('Score too low, learning issue.')\n+        preds = model.predict(X_test, verbose=0)\n+        classes = model.predict_classes(X_test, verbose=0)\n+        probas = model.predict_proba(X_test, verbose=0)\n+        print(model.get_config(verbose=1))\n+\n+        model.save_weights('temp.h5', overwrite=True)\n+        model.load_weights('temp.h5')\n+\n+        nloss = model.evaluate(X_train, y_train, verbose=0)\n+        print(nloss)\n+        assert(loss == nloss)\n+\n+\n+\n+if __name__ == '__main__':\n+    print('Test Sequential model')\n+    unittest.main()\n\n@@ -1,213 +0,0 @@\n-from __future__ import absolute_import\n-from __future__ import print_function\n-from keras.datasets import mnist\n-from keras.models import Sequential\n-from keras.layers.core import Dense, Activation, Merge\n-from keras.utils import np_utils\n-import numpy as np\n-\n-nb_classes = 10\n-batch_size = 128\n-nb_epoch = 1\n-\n-max_train_samples = 5000\n-max_test_samples = 1000\n-\n-np.random.seed(1337) # for reproducibility\n-\n-# the data, shuffled and split between tran and test sets\n-(X_train, y_train), (X_test, y_test) = mnist.load_data()\n-\n-X_train = X_train.reshape(60000,784)[:max_train_samples]\n-X_test = X_test.reshape(10000,784)[:max_test_samples]\n-X_train = X_train.astype(\"float32\")\n-X_test = X_test.astype(\"float32\")\n-X_train /= 255\n-X_test /= 255\n-\n-# convert class vectors to binary class matrices\n-Y_train = np_utils.to_categorical(y_train, nb_classes)[:max_train_samples]\n-Y_test = np_utils.to_categorical(y_test, nb_classes)[:max_test_samples]\n-\n-#########################\n-# sequential model test #\n-#########################\n-print('Test sequential')\n-model = Sequential()\n-model.add(Dense(784, 50))\n-model.add(Activation('relu'))\n-model.add(Dense(50, 10))\n-model.add(Activation('softmax'))\n-\n-model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=(X_test, Y_test))\n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=(X_test, Y_test))\n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n-\n-score = model.evaluate(X_train, Y_train, verbose=0)\n-print('score:', score)\n-if score < 0.25:\n-    raise Exception('Score too low, learning issue.')\n-preds = model.predict(X_test, verbose=0)\n-classes = model.predict_classes(X_test, verbose=0)\n-\n-model.get_config(verbose=1)\n-\n-###################\n-# merge test: sum #\n-###################\n-print('Test merge: sum')\n-left = Sequential()\n-left.add(Dense(784, 50))\n-left.add(Activation('relu'))\n-\n-right = Sequential()\n-right.add(Dense(784, 50))\n-right.add(Activation('relu'))\n-\n-model = Sequential()\n-model.add(Merge([left, right], mode='sum'))\n-\n-model.add(Dense(50, 10))\n-model.add(Activation('softmax'))\n-\n-model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-model.fit([X_train, X_train], Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test], Y_test))\n-model.fit([X_train, X_train], Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test], Y_test))\n-model.fit([X_train, X_train], Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n-model.fit([X_train, X_train], Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n-model.fit([X_train, X_train], Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-model.fit([X_train, X_train], Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n-\n-score = model.evaluate([X_train, X_train], Y_train, verbose=0)\n-print('score:', score)\n-if score < 0.22:\n-    raise Exception('Score too low, learning issue.')\n-preds = model.predict([X_test, X_test], verbose=0)\n-classes = model.predict_classes([X_test, X_test], verbose=0)\n-\n-model.get_config(verbose=1)\n-\n-###################\n-# merge test: concat #\n-###################\n-print('Test merge: concat')\n-left = Sequential()\n-left.add(Dense(784, 50))\n-left.add(Activation('relu'))\n-\n-right = Sequential()\n-right.add(Dense(784, 50))\n-right.add(Activation('relu'))\n-\n-model = Sequential()\n-model.add(Merge([left, right], mode='concat'))\n-\n-model.add(Dense(50*2, 10))\n-model.add(Activation('softmax'))\n-\n-model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-model.fit([X_train, X_train], Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test], Y_test))\n-model.fit([X_train, X_train], Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test], Y_test))\n-model.fit([X_train, X_train], Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n-model.fit([X_train, X_train], Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n-model.fit([X_train, X_train], Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-model.fit([X_train, X_train], Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n-\n-score = model.evaluate([X_train, X_train], Y_train, verbose=0)\n-print('score:', score)\n-if score < 0.22:\n-    raise Exception('Score too low, learning issue.')\n-preds = model.predict([X_test, X_test], verbose=0)\n-classes = model.predict_classes([X_test, X_test], verbose=0)\n-\n-model.get_config(verbose=1)\n-\n-##########################\n-# test merge recursivity #\n-##########################\n-print('Test merge recursivity')\n-\n-left = Sequential()\n-left.add(Dense(784, 50))\n-left.add(Activation('relu'))\n-\n-right = Sequential()\n-right.add(Dense(784, 50))\n-right.add(Activation('relu'))\n-\n-righter = Sequential()\n-righter.add(Dense(784, 50))\n-righter.add(Activation('relu'))\n-\n-intermediate = Sequential()\n-intermediate.add(Merge([left, right], mode='sum'))\n-intermediate.add(Dense(50, 50))\n-intermediate.add(Activation('relu'))\n-\n-model = Sequential()\n-model.add(Merge([intermediate, righter], mode='sum'))\n-\n-model.add(Dense(50, 10))\n-model.add(Activation('softmax'))\n-\n-model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-model.fit([X_train, X_train, X_train], Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=([X_test, X_test, X_test], Y_test))\n-model.fit([X_train, X_train, X_train], Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=([X_test, X_test, X_test], Y_test))\n-model.fit([X_train, X_train, X_train], Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n-model.fit([X_train, X_train, X_train], Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n-model.fit([X_train, X_train, X_train], Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-model.fit([X_train, X_train, X_train], Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n-\n-score = model.evaluate([X_train, X_train, X_train], Y_train, verbose=0)\n-print('score:', score)\n-if score < 0.19:\n-    raise Exception('Score too low, learning issue.')\n-preds = model.predict([X_test, X_test, X_test], verbose=0)\n-classes = model.predict_classes([X_test, X_test, X_test], verbose=0)\n-\n-model.get_config(verbose=1)\n-\n-model.save_weights('temp.h5')\n-model.load_weights('temp.h5')\n-\n-score = model.evaluate([X_train, X_train, X_train], Y_train, verbose=0)\n-print('score:', score)\n-\n-######################\n-# test merge overlap #\n-######################\n-print('Test merge overlap')\n-left = Sequential()\n-left.add(Dense(784, 50))\n-left.add(Activation('relu'))\n-\n-model = Sequential()\n-model.add(Merge([left, left], mode='sum'))\n-\n-model.add(Dense(50, 10))\n-model.add(Activation('softmax'))\n-\n-model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n-\n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_data=(X_test, Y_test))\n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_data=(X_test, Y_test))\n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=True, verbose=0, validation_split=0.1)\n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, show_accuracy=False, verbose=0, validation_split=0.1)\n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0)\n-model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n-\n-score = model.evaluate(X_train, Y_train, verbose=0)\n-print('score:', score)\n-if score < 0.22:\n-    raise Exception('Score too low, learning issue.')\n-preds = model.predict(X_test, verbose=0)\n-classes = model.predict_classes(X_test, verbose=0)\n-\n-model.get_config(verbose=1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
