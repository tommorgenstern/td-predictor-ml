{"custom_id": "scrapy#2dfeff7eca2e1482638595c48e04506ff002e3f7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 305 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -39,19 +39,19 @@ class ImagesPipelineTestCase(unittest.TestCase):\n     def tearDown(self):\n         rmtree(self.tempdir)\n \n-    def test_image_path(self):\n-        image_path = self.pipeline.file_path\n-        self.assertEqual(image_path(Request(\"https://dev.mydeco.com/mydeco.gif\")),\n+    def test_file_path(self):\n+        file_path = self.pipeline.file_path\n+        self.assertEqual(file_path(Request(\"https://dev.mydeco.com/mydeco.gif\")),\n                          'full/3fd165099d8e71b8a48b2683946e64dbfad8b52d.jpg')\n-        self.assertEqual(image_path(Request(\"http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.jpg\")),\n+        self.assertEqual(file_path(Request(\"http://www.maddiebrown.co.uk///catalogue-items//image_54642_12175_95307.jpg\")),\n                          'full/0ffcd85d563bca45e2f90becd0ca737bc58a00b2.jpg')\n-        self.assertEqual(image_path(Request(\"https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.gif\")),\n+        self.assertEqual(file_path(Request(\"https://dev.mydeco.com/two/dirs/with%20spaces%2Bsigns.gif\")),\n                          'full/b250e3a74fff2e4703e310048a5b13eba79379d2.jpg')\n-        self.assertEqual(image_path(Request(\"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg\")),\n+        self.assertEqual(file_path(Request(\"http://www.dfsonline.co.uk/get_prod_image.php?img=status_0907_mdm.jpg\")),\n                          'full/4507be485f38b0da8a0be9eb2e1dfab8a19223f2.jpg')\n-        self.assertEqual(image_path(Request(\"http://www.dorma.co.uk/images/product_details/2532/\")),\n+        self.assertEqual(file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532/\")),\n                          'full/97ee6f8a46cbbb418ea91502fd24176865cf39b2.jpg')\n-        self.assertEqual(image_path(Request(\"http://www.dorma.co.uk/images/product_details/2532\")),\n+        self.assertEqual(file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532\")),\n                          'full/244e0dd7d96a3b7b01f54eded250c9e272577aa1.jpg')\n \n     def test_thumbnail_name(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d8ca8f83bb783e3faa5a747630c0de0559808091", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 556 | Contributors (this commit): 7 | Commits (past 90d): 13 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -45,6 +45,10 @@ class FilesPipelineTestCase(unittest.TestCase):\n                          'full/97ee6f8a46cbbb418ea91502fd24176865cf39b2')\n         self.assertEqual(file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532\")),\n                          'full/244e0dd7d96a3b7b01f54eded250c9e272577aa1')\n+        self.assertEqual(file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532\"),\n+                                   response=Response(\"http://www.dorma.co.uk/images/product_details/2532\"),\n+                                   info=object()),\n+                         'full/244e0dd7d96a3b7b01f54eded250c9e272577aa1')\n \n     def test_fs_store(self):\n         assert isinstance(self.pipeline.store, FSFilesStore)\n\n@@ -8,7 +8,7 @@ from shutil import rmtree\n from twisted.trial import unittest\n \n from scrapy.item import Item, Field\n-from scrapy.http import Request\n+from scrapy.http import Request, Response\n from scrapy.settings import Settings\n from scrapy.contrib.pipeline.images import ImagesPipeline\n \n@@ -53,6 +53,10 @@ class ImagesPipelineTestCase(unittest.TestCase):\n                          'full/97ee6f8a46cbbb418ea91502fd24176865cf39b2.jpg')\n         self.assertEqual(file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532\")),\n                          'full/244e0dd7d96a3b7b01f54eded250c9e272577aa1.jpg')\n+        self.assertEqual(file_path(Request(\"http://www.dorma.co.uk/images/product_details/2532\"),\n+                                   response=Response(\"http://www.dorma.co.uk/images/product_details/2532\"),\n+                                   info=object()),\n+                         'full/244e0dd7d96a3b7b01f54eded250c9e272577aa1.jpg')\n \n     def test_thumbnail_name(self):\n         thumb_path = self.pipeline.thumb_path\n@@ -65,6 +69,10 @@ class ImagesPipelineTestCase(unittest.TestCase):\n                          'thumbs/50/0329ad83ebb8e93ea7c7906d46e9ed55f7349a50.jpg')\n         self.assertEqual(thumb_path(Request(\"file:///tmp/some.name/foo\"), name),\n                          'thumbs/50/850233df65a5b83361798f532f1fc549cd13cbe9.jpg')\n+        self.assertEqual(thumb_path(Request(\"file:///tmp/some.name/foo\"), name,\n+                                    response=Response(\"file:///tmp/some.name/foo\"),\n+                                    info=object()),\n+                         'thumbs/50/850233df65a5b83361798f532f1fc549cd13cbe9.jpg')\n \n     def test_convert_image(self):\n         SIZE = (100, 100)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#87559a1240b33ccdb0f043f4587b86f7f36e10f5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 390 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -255,7 +255,7 @@ class FilesPipeline(MediaPipeline):\n         return [Request(x) for x in item.get(self.FILES_URLS_FIELD, [])]\n \n     def file_downloaded(self, response, request, info):\n-        path = self.file_path(request)\n+        path = self.file_path(request, response=response, info=info)\n         buf = StringIO(response.body)\n         self.store.persist_file(path, buf, info)\n         checksum = md5sum(buf)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#71c59e1c948f2e9e3dad32a8aaabf1ddd795fd39", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 93 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -13,17 +13,18 @@ class AjaxCrawlableMiddleware(object):\n     For more info see https://developers.google.com/webmasters/ajax-crawling/docs/getting-started.\n     \"\"\"\n \n-    # XXX: Google parses at least first 100k bytes; scrapy's redirect\n-    # middleware parses first 4k. 4k turns out to be insufficient\n-    # for this middleware, and parsing 100k could be slow.\n-    _lookup_bytes = 32768\n-\n     enabled_setting = 'AJAXCRAWLABLE_ENABLED'\n \n     def __init__(self, settings):\n         if not settings.getbool(self.enabled_setting):\n             raise NotConfigured\n \n+        # XXX: Google parses at least first 100k bytes; scrapy's redirect\n+        # middleware parses first 4k. 4k turns out to be insufficient\n+        # for this middleware, and parsing 100k could be slow.\n+        # We use something in between (32K) by default.\n+        self.lookup_bytes = settings.getint('AJAXCRAWLABLE_MAXSIZE', 32768)\n+\n     @classmethod\n     def from_crawler(cls, crawler):\n         return cls(crawler.settings)\n@@ -57,7 +58,7 @@ class AjaxCrawlableMiddleware(object):\n         Return True if a page without hash fragment could be \"AJAX crawlable\"\n         according to https://developers.google.com/webmasters/ajax-crawling/docs/getting-started.\n         \"\"\"\n-        body = response.body_as_unicode()[:self._lookup_bytes]\n+        body = response.body_as_unicode()[:self.lookup_bytes]\n         return _has_ajaxcrawlable_meta(body)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#503ab58f59886a2306a69b2662cd2219b80ae3d1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 10 | Churn Cumulative: 103 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -75,7 +75,17 @@ def _has_ajaxcrawlable_meta(text):\n     >>> _has_ajaxcrawlable_meta('<html></html>')\n     False\n     \"\"\"\n+\n+    # Stripping scripts and comments is slow (about 20x slower than\n+    # just checking if a string is in text); this is a quick fail-fast\n+    # path that should work for most pages.\n+    if 'fragment' not in text:\n+        return False\n+    if 'content' not in text:\n+        return False\n+\n     text = _script_re.sub(u'', text)\n     text = _noscript_re.sub(u'', text)\n     text = html.remove_comments(html.remove_entities(text))\n     return _ajax_crawlable_re.search(text) is not None\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#84a3a9daac5536fa8f76927935b5235fafba9b0e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 62 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -2,7 +2,7 @@ import unittest\n \n from scrapy.contrib.downloadermiddleware.ajaxcrawlable import AjaxCrawlableMiddleware\n from scrapy.spider import BaseSpider\n-from scrapy.http import Request, HtmlResponse\n+from scrapy.http import Request, HtmlResponse, Response\n from scrapy.utils.test import get_crawler\n \n __doctests__ = ['scrapy.contrib.downloadermiddleware.ajaxcrawlable']\n@@ -26,6 +26,12 @@ class AjaxCrawlableMiddlewareTest(unittest.TestCase):\n         resp2 = self.mw.process_response(req, resp, self.spider)\n         self.assertEqual(resp, resp2)\n \n+    def test_binary_response(self):\n+        req = Request('http://example.com/')\n+        resp = Response('http://example.com/', body=b'foobar\\x00\\x01\\x02', request=req)\n+        resp2 = self.mw.process_response(req, resp, self.spider)\n+        self.assertIs(resp, resp2)\n+\n     def test_ajax_crawlable(self):\n         req, resp = self._req_resp(\n             'http://example.com/',\n@@ -49,4 +55,4 @@ class AjaxCrawlableMiddlewareTest(unittest.TestCase):\n     def test_noncrawlable_body(self):\n         req, resp = self._req_resp('http://example.com/', {}, {'body': '<html></html>'})\n         resp2 = self.mw.process_response(req, resp, self.spider)\n-        assert resp2 is resp\n+        self.assertIs(resp, resp2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#84aa7599a44e78253b42a72e634601076d027b66", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 291 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -53,13 +53,13 @@ def flatten(x):\n \n def unique(list_, key=lambda x: x):\n     \"\"\"efficient function to uniquify a list preserving item order\"\"\"\n-    seen = {}\n+    seen = set()\n     result = []\n     for item in list_:\n         seenkey = key(item)\n         if seenkey in seen:\n             continue\n-        seen[seenkey] = 1\n+        seen.add(seenkey)\n         result.append(item)\n     return result\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ee46ec892061f01656e294441b2836a2f5b722c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 107 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -13,10 +13,8 @@ class AjaxCrawlableMiddleware(object):\n     For more info see https://developers.google.com/webmasters/ajax-crawling/docs/getting-started.\n     \"\"\"\n \n-    enabled_setting = 'AJAXCRAWLABLE_ENABLED'\n-\n     def __init__(self, settings):\n-        if not settings.getbool(self.enabled_setting):\n+        if not settings.getbool('AJAXCRAWLABLE_ENABLED'):\n             raise NotConfigured\n \n         # XXX: Google parses at least first 100k bytes; scrapy's redirect\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2b7fea26a5ab8f0bb080a4301d35515a17b9e072", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 73 | Lines Deleted: 3 | Files Changed: 5 | Hunks: 10 | Methods Changed: 8 | Complexity Δ (Sum/Max): 9/5 | Churn Δ: 76 | Churn Cumulative: 1013 | Contributors (this commit): 5 | Commits (past 90d): 10 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,7 +17,9 @@ class Headers(CaselessDict):\n \n     def normvalue(self, value):\n         \"\"\"Headers must not be unicode\"\"\"\n-        if not hasattr(value, '__iter__'):\n+        if value is None:\n+            value = []\n+        elif not hasattr(value, '__iter__'):\n             value = [value]\n         return [x.encode(self.encoding) if isinstance(x, unicode) else x \\\n             for x in value]\n\n@@ -1,5 +1,5 @@\n from __future__ import print_function\n-import sys, time, random, urllib, os\n+import sys, time, random, urllib, os, json\n from subprocess import Popen, PIPE\n from twisted.web.server import Site, NOT_DONE_YET\n from twisted.web.resource import Resource\n@@ -119,6 +119,16 @@ class Raw(LeafResource):\n         request.finish()\n \n \n+class Echo(LeafResource):\n+\n+    def render_GET(self, request):\n+        output = {\n+            'headers': dict(request.requestHeaders.getAllRawHeaders()),\n+            'body': request.content.read(),\n+        }\n+        return json.dumps(output)\n+\n+\n class Partial(LeafResource):\n \n     def render_GET(self, request):\n@@ -156,6 +166,7 @@ class Root(Resource):\n         self.putChild(\"partial\", Partial())\n         self.putChild(\"drop\", Drop())\n         self.putChild(\"raw\", Raw())\n+        self.putChild(\"echo\", Echo())\n \n     def getChild(self, name, request):\n         return self\n\n@@ -132,3 +132,22 @@ class BrokenStartRequestsSpider(FollowAllSpider):\n         self.seedsseen.append(response.meta.get('seed'))\n         for req in super(BrokenStartRequestsSpider, self).parse(response):\n             yield req\n+\n+\n+class SingleRequestSpider(MetaSpider):\n+\n+    seed = None\n+\n+    def start_requests(self):\n+        if isinstance(self.seed, Request):\n+            yield self.seed.replace(callback=self.parse, errback=self.on_error)\n+        else:\n+            yield Request(self.seed, callback=self.parse, errback=self.on_error)\n+\n+    def parse(self, response):\n+        self.meta.setdefault('responses', []).append(response)\n+        if 'next' in response.meta:\n+            return response.meta['next']\n+\n+    def on_error(self, failure):\n+        self.meta['failure'] = failure\n\n@@ -1,9 +1,11 @@\n+import json\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n from scrapy.utils.test import get_crawler, get_testlog\n from scrapy.tests.spiders import FollowAllSpider, DelaySpider, SimpleSpider, \\\n-    BrokenStartRequestsSpider\n+    BrokenStartRequestsSpider, SingleRequestSpider\n from scrapy.tests.mockserver import MockServer\n+from scrapy.http import Request\n \n \n def docrawl(spider, settings=None):\n@@ -158,3 +160,31 @@ with multiples lines\n         log = get_testlog()\n         self.assertEqual(log.count(\"Retrying\"), 2)\n         self.assertEqual(log.count(\"Gave up retrying\"), 1)\n+\n+    @defer.inlineCallbacks\n+    def test_referer_header(self):\n+        \"\"\"Referer header is set by RefererMiddleware unless it is already set\"\"\"\n+        req0 = Request('http://localhost:8998/echo?headers=1&body=0', dont_filter=1)\n+        req1 = req0.replace()\n+        req2 = req0.replace(headers={'Referer': None})\n+        req3 = req0.replace(headers={'Referer': 'http://example.com'})\n+        req0.meta['next'] = req1\n+        req1.meta['next'] = req2\n+        req2.meta['next'] = req3\n+        spider = SingleRequestSpider(seed=req0)\n+        yield docrawl(spider)\n+        # basic asserts in case of weird communication errors\n+        self.assertIn('responses', spider.meta)\n+        self.assertNotIn('failures', spider.meta)\n+        # start requests doesn't set Referer header\n+        echo0 = json.loads(spider.meta['responses'][2].body)\n+        self.assertNotIn('Referer', echo0['headers'])\n+        # following request sets Referer to start request url\n+        echo1 = json.loads(spider.meta['responses'][1].body)\n+        self.assertEqual(echo1['headers'].get('Referer'), [req0.url])\n+        # next request avoids Referer header\n+        echo2 = json.loads(spider.meta['responses'][2].body)\n+        self.assertNotIn('Referer', echo2['headers'])\n+        # last request explicitly sets a Referer header\n+        echo3 = json.loads(spider.meta['responses'][3].body)\n+        self.assertEqual(echo3['headers'].get('Referer'), ['http://example.com'])\n\n@@ -119,3 +119,11 @@ class HeadersTest(unittest.TestCase):\n         h1.setlistdefault('header2', ['value2', 'value3'])\n         self.assertEqual(h1.getlist('header1'), ['value1'])\n         self.assertEqual(h1.getlist('header2'), ['value2', 'value3'])\n+\n+    def test_none_value(self):\n+        h1 = Headers()\n+        h1['foo'] = 'bar'\n+        h1['foo'] = None\n+        h1.setdefault('foo', 'bar')\n+        self.assertEqual(h1.get('foo'), None)\n+        self.assertEqual(h1.getlist('foo'), [])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#95dc46f2bc608c118ac5b54baff33e0becd5f416", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 32 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -17,8 +17,8 @@ IGNORED_EXTENSIONS = [\n     'm4a',\n \n     # office suites\n-    'xls', 'ppt', 'doc', 'docx', 'odt', 'ods', 'odg', 'odp',\n+    'xls', 'xlsx', 'ppt', 'pptx', 'doc', 'docx', 'odt', 'ods', 'odg', 'odp',\n \n     # other\n-    'css', 'pdf', 'doc', 'exe', 'bin', 'rss', 'zip', 'rar',\n+    'css', 'pdf', 'exe', 'bin', 'rss', 'zip', 'rar',\n ]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#09c3f536939e3a23b388a009a0fee6d41999e7b6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 462 | Contributors (this commit): 7 | Commits (past 90d): 5 | Contributors (cumulative): 12 | DMM Complexity: 0.5\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import functools\n import operator\n import unittest\n from itertools import count\n@@ -174,12 +175,14 @@ class UtilsPythonTestCase(unittest.TestCase):\n                 pass\n \n         a = A(1, 2, 3)\n+        partial_f1 = functools.partial(f1, None)\n         cal = Callable()\n \n         self.assertEqual(get_func_args(f1), ['a', 'b', 'c'])\n         self.assertEqual(get_func_args(f2), ['a', 'b', 'c'])\n         self.assertEqual(get_func_args(A), ['a', 'b', 'c'])\n         self.assertEqual(get_func_args(a.method), ['a', 'b', 'c'])\n+        self.assertEqual(get_func_args(partial_f1), ['a', 'b', 'c'])\n         self.assertEqual(get_func_args(cal), ['a', 'b', 'c'])\n         self.assertEqual(get_func_args(object), [])\n \n\n@@ -10,7 +10,7 @@ import re\n import inspect\n import weakref\n import errno\n-from functools import wraps\n+from functools import partial, wraps\n from sgmllib import SGMLParser\n \n \n@@ -156,6 +156,8 @@ def get_func_args(func, stripself=False):\n         return get_func_args(func.__func__, True)\n     elif inspect.ismethoddescriptor(func):\n         return []\n+    elif isinstance(func, partial):\n+        return get_func_args(func.func)\n     elif hasattr(func, '__call__'):\n         if inspect.isroutine(func):\n             return []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#589fd037d9a156733580bc4b3f065c86aa54af23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 11 | Churn Cumulative: 473 | Contributors (this commit): 7 | Commits (past 90d): 7 | Contributors (cumulative): 12 | DMM Complexity: 0.8\n\nDIFF:\n@@ -175,14 +175,18 @@ class UtilsPythonTestCase(unittest.TestCase):\n                 pass\n \n         a = A(1, 2, 3)\n-        partial_f1 = functools.partial(f1, None)\n         cal = Callable()\n+        partial_f1 = functools.partial(f1, None)\n+        partial_f2 = functools.partial(f1, b=None)\n+        partial_f3 = functools.partial(partial_f2, None)\n \n         self.assertEqual(get_func_args(f1), ['a', 'b', 'c'])\n         self.assertEqual(get_func_args(f2), ['a', 'b', 'c'])\n         self.assertEqual(get_func_args(A), ['a', 'b', 'c'])\n         self.assertEqual(get_func_args(a.method), ['a', 'b', 'c'])\n-        self.assertEqual(get_func_args(partial_f1), ['a', 'b', 'c'])\n+        self.assertEqual(get_func_args(partial_f1), ['b', 'c'])\n+        self.assertEqual(get_func_args(partial_f2), ['a', 'c'])\n+        self.assertEqual(get_func_args(partial_f3), ['c'])\n         self.assertEqual(get_func_args(cal), ['a', 'b', 'c'])\n         self.assertEqual(get_func_args(object), [])\n \n\n@@ -157,7 +157,8 @@ def get_func_args(func, stripself=False):\n     elif inspect.ismethoddescriptor(func):\n         return []\n     elif isinstance(func, partial):\n-        return get_func_args(func.func)\n+        return [x for x in get_func_args(func.func)[len(func.args):]\n+                if not (func.keywords and x in func.keywords)]\n     elif hasattr(func, '__call__'):\n         if inspect.isroutine(func):\n             return []\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#22e107194bb35f83066c06e176417791f9129b4c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 24 | Churn Cumulative: 762 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n import unittest\n+from functools import partial\n \n from scrapy.contrib.loader import ItemLoader\n from scrapy.contrib.loader.processor import Join, Identity, TakeFirst, \\\n@@ -332,6 +333,29 @@ class BasicItemLoaderTest(unittest.TestCase):\n         item = il.load_item()\n         self.assertEqual(item['name'], u'Mart')\n \n+    def test_partial_processor(self):\n+        def join(values, sep=None, loader_context=None, ignored=None):\n+            if sep is not None:\n+                return sep.join(values)\n+            elif loader_context and 'sep' in loader_context:\n+                return loader_context['sep'].join(values)\n+            else:\n+                return ''.join(values)\n+\n+        class TestItemLoader(NameItemLoader):\n+            name_out = Compose(partial(join, sep='+'))\n+            url_out = Compose(partial(join, loader_context={'sep': '.'}))\n+            summary_out = Compose(partial(join, ignored='foo'))\n+\n+        il = TestItemLoader()\n+        il.add_value('name', [u'rabbit', u'hole'])\n+        il.add_value('url', [u'rabbit', u'hole'])\n+        il.add_value('summary', [u'rabbit', u'hole'])\n+        item = il.load_item()\n+        self.assertEqual(item['name'], u'rabbit+hole')\n+        self.assertEqual(item['url'], u'rabbit.hole')\n+        self.assertEqual(item['summary'], u'rabbithole')\n+\n \n class ProcessorsTest(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#439a141aaeaa3c48dd1f662edab29bc52e31e35f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 14 | Churn Cumulative: 27 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,3 +9,17 @@ def attribute(obj, oldattr, newattr, version='0.12'):\n     warnings.warn(\"%s.%s attribute is deprecated and will be no longer supported \"\n         \"in Scrapy %s, use %s.%s attribute instead\" % \\\n         (cname, oldattr, version, cname, newattr), ScrapyDeprecationWarning, stacklevel=3)\n+\n+\n+def warn_when_subclassed(mro_len, message, category=ScrapyDeprecationWarning):\n+    \"\"\"\n+    Return a metaclass that causes classes to\n+    issue a warning when they are subclassed.\n+    \"\"\"\n+    class Metaclass(type):\n+        def __init__(cls, name, bases, clsdict):\n+            if len(cls.mro()) > mro_len:\n+                warnings.warn(message, category, stacklevel=2)\n+            super(Metaclass, cls).__init__(name, bases, clsdict)\n+    return Metaclass\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a27d91f0a6eb9fd9f2c3ee1d65f131feaf901bd3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 148 | Lines Deleted: 140 | Files Changed: 49 | Hunks: 141 | Methods Changed: 51 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 288 | Churn Cumulative: 8349 | Contributors (this commit): 27 | Commits (past 90d): 77 | Contributors (cumulative): 191 | DMM Complexity: None\n\nDIFF:\n@@ -7,11 +7,11 @@ usage:\n \n \"\"\"\n \n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.http import Request\n \n \n-class QPSSpider(BaseSpider):\n+class QPSSpider(Spider):\n \n     name = 'qps'\n     benchurl = 'http://localhost:8880/'\n\n@@ -3,7 +3,7 @@ from w3lib.url import is_url\n \n from scrapy.command import ScrapyCommand\n from scrapy.http import Request\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.exceptions import UsageError\n from scrapy.utils.spider import create_spider_for_request\n \n@@ -54,6 +54,6 @@ class Command(ScrapyCommand):\n             spider = crawler.spiders.create(opts.spider)\n         else:\n             spider = create_spider_for_request(crawler.spiders, request, \\\n-                default_spider=BaseSpider('default'))\n+                default_spider=Spider('default'))\n         crawler.crawl(spider, [request])\n         self.crawler_process.start()\n\n@@ -9,7 +9,7 @@ import copy\n \n from scrapy.http import Request, HtmlResponse\n from scrapy.utils.spider import iterate_spider_output\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n \n def identity(x):\n     return x\n@@ -27,7 +27,7 @@ class Rule(object):\n         else:\n             self.follow = follow\n \n-class CrawlSpider(BaseSpider):\n+class CrawlSpider(Spider):\n \n     rules = ()\n \n\n@@ -4,7 +4,7 @@ for scraping from an XML feed.\n \n See documentation in docs/topics/spiders.rst\n \"\"\"\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.item import BaseItem\n from scrapy.http import Request\n from scrapy.utils.iterators import xmliter, csviter\n@@ -13,7 +13,7 @@ from scrapy.selector import Selector\n from scrapy.exceptions import NotConfigured, NotSupported\n \n \n-class XMLFeedSpider(BaseSpider):\n+class XMLFeedSpider(Spider):\n     \"\"\"\n     This class intends to be the base class for spiders that scrape\n     from XML feeds.\n@@ -92,7 +92,7 @@ class XMLFeedSpider(BaseSpider):\n         for (prefix, uri) in self.namespaces:\n             selector.register_namespace(prefix, uri)\n \n-class CSVFeedSpider(BaseSpider):\n+class CSVFeedSpider(Spider):\n     \"\"\"Spider for parsing CSV feeds.\n     It receives a CSV file in a response; iterates through each of its rows,\n     and calls parse_row with a dict containing each field's data.\n\n@@ -1,7 +1,7 @@\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.utils.spider import iterate_spider_output\n \n-class InitSpider(BaseSpider):\n+class InitSpider(Spider):\n     \"\"\"Base Spider with initialization facilities\"\"\"\n \n     def start_requests(self):\n\n@@ -1,12 +1,12 @@\n import re\n \n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.http import Request, XmlResponse\n from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n from scrapy.utils.gz import gunzip, is_gzipped\n from scrapy import log\n \n-class SitemapSpider(BaseSpider):\n+class SitemapSpider(Spider):\n \n     sitemap_urls = ()\n     sitemap_rules = [('', 'parse')]\n\n@@ -11,7 +11,7 @@ from twisted.python import threadable\n from w3lib.url import any_to_uri\n \n from scrapy.item import BaseItem\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.selector import Selector\n from scrapy.utils.spider import create_spider_for_request\n from scrapy.utils.misc import load_object\n@@ -24,7 +24,7 @@ from scrapy.exceptions import IgnoreRequest\n \n class Shell(object):\n \n-    relevant_classes = (BaseSpider, Request, Response, BaseItem,\n+    relevant_classes = (Spider, Request, Response, BaseItem,\n                         Selector, Settings)\n \n     def __init__(self, crawler, update_vars=None, code=None):\n@@ -67,7 +67,7 @@ class Shell(object):\n         if spider is None:\n             spider = create_spider_for_request(self.crawler.spiders,\n                                                request,\n-                                               BaseSpider('default'),\n+                                               Spider('default'),\n                                                log_multiple=True)\n         spider.set_crawler(self.crawler)\n         self.crawler.engine.open_spider(spider, close_if_idle=False)\n\n@@ -3,14 +3,14 @@ Base class for Scrapy spiders\n \n See documentation in docs/topics/spiders.rst\n \"\"\"\n-\n from scrapy import log\n from scrapy.http import Request\n from scrapy.utils.trackref import object_ref\n from scrapy.utils.url import url_is_from_spider\n+from scrapy.utils.deprecate import warn_when_subclassed\n \n \n-class BaseSpider(object_ref):\n+class Spider(object_ref):\n     \"\"\"Base class for scrapy spiders. All spiders must inherit from this\n     class.\n     \"\"\"\n@@ -65,6 +65,14 @@ class BaseSpider(object_ref):\n     __repr__ = __str__\n \n \n+class BaseSpider(Spider):\n+    __metaclass__ = warn_when_subclassed(\n+        4,  # == len([object, object_ref, Spider, BaseSpider])\n+        \"scrapy.spider.BaseSpider was deprecated. \"\n+        \"Please inherit from scrapy.spider.Spider.\"\n+    )\n+\n+\n class ObsoleteClass(object):\n     def __init__(self, message):\n         self.message = message\n\n@@ -5,13 +5,13 @@ Some spiders used for testing and benchmarking\n import time\n from urllib import urlencode\n \n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.http import Request\n from scrapy.item import Item\n from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n \n \n-class MetaSpider(BaseSpider):\n+class MetaSpider(Spider):\n \n     name = 'meta'\n \n\n@@ -130,9 +130,9 @@ class RunSpiderCommandTest(CommandTest):\n         with open(fname, 'w') as f:\n             f.write(\"\"\"\n from scrapy import log\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n \n-class MySpider(BaseSpider):\n+class MySpider(Spider):\n     name = 'myspider'\n \n     def start_requests(self):\n@@ -153,7 +153,7 @@ class MySpider(BaseSpider):\n         with open(fname, 'w') as f:\n             f.write(\"\"\"\n from scrapy import log\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n \"\"\")\n         p = self.proc('runspider', fname)\n         log = p.stderr.read()\n@@ -184,10 +184,10 @@ class ParseCommandTest(CommandTest):\n         with open(fname, 'w') as f:\n             f.write(\"\"\"\n from scrapy import log\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.item import Item\n \n-class MySpider(BaseSpider):\n+class MySpider(Spider):\n     name = '{0}'\n \n     def parse(self, response):\n\n@@ -2,7 +2,7 @@ from unittest import TextTestRunner\n \n from twisted.trial import unittest\n \n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.http import Request\n from scrapy.item import Item, Field\n from scrapy.contracts import ContractsManager\n@@ -22,7 +22,7 @@ class ResponseMock(object):\n     url = 'http://scrapy.org'\n \n \n-class TestSpider(BaseSpider):\n+class TestSpider(Spider):\n     name = 'demo_spider'\n \n     def returns_request(self, response):\n\n@@ -6,7 +6,7 @@ from twisted.trial import unittest\n from twisted.internet import defer\n from w3lib.url import path_to_file_uri\n \n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.contrib.feedexport import IFeedStorage, FileFeedStorage, FTPFeedStorage, S3FeedStorage, StdoutFeedStorage\n from scrapy.utils.test import assert_aws_environ\n \n@@ -38,7 +38,7 @@ class FileFeedStorageTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def _assert_stores(self, storage, path):\n-        spider = BaseSpider(\"default\")\n+        spider = Spider(\"default\")\n         file = storage.open(spider)\n         file.write(\"content\")\n         yield storage.store(file)\n@@ -59,7 +59,7 @@ class FTPFeedStorageTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def _assert_stores(self, storage, path):\n-        spider = BaseSpider(\"default\")\n+        spider = Spider(\"default\")\n         file = storage.open(spider)\n         file.write(\"content\")\n         yield storage.store(file)\n@@ -81,7 +81,7 @@ class S3FeedStorageTest(unittest.TestCase):\n         from boto import connect_s3\n         storage = S3FeedStorage(uri)\n         verifyObject(IFeedStorage, storage)\n-        file = storage.open(BaseSpider(\"default\"))\n+        file = storage.open(Spider(\"default\"))\n         file.write(\"content\")\n         yield storage.store(file)\n         u = urlparse.urlparse(uri)\n@@ -94,7 +94,7 @@ class StdoutFeedStorageTest(unittest.TestCase):\n     def test_store(self):\n         out = StringIO()\n         storage = StdoutFeedStorage('stdout:', _stdout=out)\n-        file = storage.open(BaseSpider(\"default\"))\n+        file = storage.open(Spider(\"default\"))\n         file.write(\"content\")\n         yield storage.store(file)\n         self.assertEqual(out.getvalue(), \"content\")\n\n@@ -3,7 +3,7 @@ from datetime import datetime\n from twisted.trial import unittest\n \n from scrapy.contrib.spiderstate import SpiderState\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n \n \n class SpiderStateTest(unittest.TestCase):\n@@ -11,7 +11,7 @@ class SpiderStateTest(unittest.TestCase):\n     def test_store_load(self):\n         jobdir = self.mktemp()\n         os.mkdir(jobdir)\n-        spider = BaseSpider(name='default')\n+        spider = Spider(name='default')\n         dt = datetime.now()\n \n         ss = SpiderState(jobdir)\n@@ -20,7 +20,7 @@ class SpiderStateTest(unittest.TestCase):\n         spider.state['dt'] = dt\n         ss.spider_closed(spider)\n \n-        spider2 = BaseSpider(name='default')\n+        spider2 = Spider(name='default')\n         ss2 = SpiderState(jobdir)\n         ss2.spider_opened(spider2)\n         self.assertEqual(spider.state, {'one': 1, 'dt': dt})\n@@ -29,7 +29,7 @@ class SpiderStateTest(unittest.TestCase):\n     def test_state_attribute(self):\n         # state attribute must be present if jobdir is not set, to provide a\n         # consistent interface\n-        spider = BaseSpider(name='default')\n+        spider = Spider(name='default')\n         ss = SpiderState()\n         ss.spider_opened(spider)\n         self.assertEqual(spider.state, {})\n\n@@ -22,7 +22,7 @@ from scrapy.core.downloader.handlers.http11 import HTTP11DownloadHandler\n from scrapy.core.downloader.handlers.s3 import S3DownloadHandler\n from scrapy.core.downloader.handlers.ftp import FTPDownloadHandler\n \n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.http import Request\n from scrapy.settings import Settings\n from scrapy import optional_features\n@@ -45,11 +45,11 @@ class FileTestCase(unittest.TestCase):\n \n         request = Request(path_to_file_uri(self.tmpname + '^'))\n         assert request.url.upper().endswith('%5E')\n-        return self.download_request(request, BaseSpider('foo')).addCallback(_test)\n+        return self.download_request(request, Spider('foo')).addCallback(_test)\n \n     def test_non_existent(self):\n         request = Request('file://%s' % self.mktemp())\n-        d = self.download_request(request, BaseSpider('foo'))\n+        d = self.download_request(request, Spider('foo'))\n         return self.assertFailure(d, IOError)\n \n \n@@ -87,35 +87,35 @@ class HttpTestCase(unittest.TestCase):\n \n     def test_download(self):\n         request = Request(self.getURL('file'))\n-        d = self.download_request(request, BaseSpider('foo'))\n+        d = self.download_request(request, Spider('foo'))\n         d.addCallback(lambda r: r.body)\n         d.addCallback(self.assertEquals, \"0123456789\")\n         return d\n \n     def test_download_head(self):\n         request = Request(self.getURL('file'), method='HEAD')\n-        d = self.download_request(request, BaseSpider('foo'))\n+        d = self.download_request(request, Spider('foo'))\n         d.addCallback(lambda r: r.body)\n         d.addCallback(self.assertEquals, '')\n         return d\n \n     def test_redirect_status(self):\n         request = Request(self.getURL('redirect'))\n-        d = self.download_request(request, BaseSpider('foo'))\n+        d = self.download_request(request, Spider('foo'))\n         d.addCallback(lambda r: r.status)\n         d.addCallback(self.assertEquals, 302)\n         return d\n \n     def test_redirect_status_head(self):\n         request = Request(self.getURL('redirect'), method='HEAD')\n-        d = self.download_request(request, BaseSpider('foo'))\n+        d = self.download_request(request, Spider('foo'))\n         d.addCallback(lambda r: r.status)\n         d.addCallback(self.assertEquals, 302)\n         return d\n \n     @defer.inlineCallbacks\n     def test_timeout_download_from_spider(self):\n-        spider = BaseSpider('foo')\n+        spider = Spider('foo')\n         meta = {'download_timeout': 0.2}\n         # client connects but no data is received\n         request = Request(self.getURL('wait'), meta=meta)\n@@ -132,7 +132,7 @@ class HttpTestCase(unittest.TestCase):\n             self.assertEquals(request.headers, {})\n \n         request = Request(self.getURL('host'))\n-        return self.download_request(request, BaseSpider('foo')).addCallback(_test)\n+        return self.download_request(request, Spider('foo')).addCallback(_test)\n \n     def test_host_header_seted_in_request_headers(self):\n         def _test(response):\n@@ -140,9 +140,9 @@ class HttpTestCase(unittest.TestCase):\n             self.assertEquals(request.headers.get('Host'), 'example.com')\n \n         request = Request(self.getURL('host'), headers={'Host': 'example.com'})\n-        return self.download_request(request, BaseSpider('foo')).addCallback(_test)\n+        return self.download_request(request, Spider('foo')).addCallback(_test)\n \n-        d = self.download_request(request, BaseSpider('foo'))\n+        d = self.download_request(request, Spider('foo'))\n         d.addCallback(lambda r: r.body)\n         d.addCallback(self.assertEquals, 'example.com')\n         return d\n@@ -150,7 +150,7 @@ class HttpTestCase(unittest.TestCase):\n     def test_payload(self):\n         body = '1'*100 # PayloadResource requires body length to be 100\n         request = Request(self.getURL('payload'), method='POST', body=body)\n-        d = self.download_request(request, BaseSpider('foo'))\n+        d = self.download_request(request, Spider('foo'))\n         d.addCallback(lambda r: r.body)\n         d.addCallback(self.assertEquals, body)\n         return d\n@@ -211,7 +211,7 @@ class HttpProxyTestCase(unittest.TestCase):\n \n         http_proxy = self.getURL('')\n         request = Request('http://example.com', meta={'proxy': http_proxy})\n-        return self.download_request(request, BaseSpider('foo')).addCallback(_test)\n+        return self.download_request(request, Spider('foo')).addCallback(_test)\n \n     def test_download_with_proxy_https_noconnect(self):\n         def _test(response):\n@@ -221,7 +221,7 @@ class HttpProxyTestCase(unittest.TestCase):\n \n         http_proxy = '%s?noconnect' % self.getURL('')\n         request = Request('https://example.com', meta={'proxy': http_proxy})\n-        return self.download_request(request, BaseSpider('foo')).addCallback(_test)\n+        return self.download_request(request, Spider('foo')).addCallback(_test)\n \n     def test_download_without_proxy(self):\n         def _test(response):\n@@ -230,7 +230,7 @@ class HttpProxyTestCase(unittest.TestCase):\n             self.assertEquals(response.body, '/path/to/resource')\n \n         request = Request(self.getURL('path/to/resource'))\n-        return self.download_request(request, BaseSpider('foo')).addCallback(_test)\n+        return self.download_request(request, Spider('foo')).addCallback(_test)\n \n \n class DeprecatedHttpProxyTestCase(unittest.TestCase):\n@@ -270,7 +270,7 @@ class S3TestCase(unittest.TestCase):\n                 self.AWS_SECRET_ACCESS_KEY, \\\n                 httpdownloadhandler=HttpDownloadHandlerMock)\n         self.download_request = s3reqh.download_request\n-        self.spider = BaseSpider('foo')\n+        self.spider = Spider('foo')\n \n     def test_request_signing1(self):\n         # gets an object from the johnsmith bucket.\n\n@@ -2,7 +2,7 @@ from twisted.trial.unittest import TestCase\n from twisted.python.failure import Failure\n \n from scrapy.http import Request, Response\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.core.downloader.middleware import DownloaderMiddlewareManager\n from scrapy.utils.test import get_crawler\n \n@@ -13,7 +13,7 @@ class ManagerTestCase(TestCase):\n \n     def setUp(self):\n         self.crawler = get_crawler(self.settings_dict)\n-        self.spider = BaseSpider('foo')\n+        self.spider = Spider('foo')\n         self.spider.set_crawler(self.crawler)\n         self.mwman = DownloaderMiddlewareManager.from_crawler(self.crawler)\n         # some mw depends on stats collector\n\n@@ -1,14 +1,14 @@\n from unittest import TestCase\n \n from scrapy.http import Response, Request\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.contrib.downloadermiddleware.cookies import CookiesMiddleware\n \n \n class CookiesMiddlewareTest(TestCase):\n \n     def setUp(self):\n-        self.spider = BaseSpider('foo')\n+        self.spider = Spider('foo')\n         self.mw = CookiesMiddleware()\n \n     def tearDown(self):\n\n@@ -1,7 +1,7 @@\n from unittest import TestCase, main\n from scrapy.http import Response, XmlResponse\n from scrapy.contrib_exp.downloadermiddleware.decompression import DecompressionMiddleware\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.tests import get_testdata\n from scrapy.utils.test import assert_samelines\n \n@@ -22,7 +22,7 @@ class DecompressionMiddlewareTest(TestCase):\n \n     def setUp(self):\n         self.mw = DecompressionMiddleware()\n-        self.spider = BaseSpider('foo')\n+        self.spider = Spider('foo')\n \n     def test_known_compression_formats(self):\n         for fmt in self.test_formats:\n\n@@ -2,7 +2,7 @@ from unittest import TestCase\n \n from scrapy.contrib.downloadermiddleware.defaultheaders import DefaultHeadersMiddleware\n from scrapy.http import Request\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.utils.test import get_crawler\n \n \n@@ -10,7 +10,7 @@ class TestDefaultHeadersMiddleware(TestCase):\n \n     def get_defaults_spider_mw(self):\n         crawler = get_crawler()\n-        spider = BaseSpider('foo')\n+        spider = Spider('foo')\n         spider.set_crawler(crawler)\n         defaults = dict([(k, [v]) for k, v in \\\n             crawler.settings.get('DEFAULT_REQUEST_HEADERS').iteritems()])\n\n@@ -1,7 +1,7 @@\n import unittest\n \n from scrapy.contrib.downloadermiddleware.downloadtimeout import DownloadTimeoutMiddleware\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.http import Request\n from scrapy.utils.test import get_crawler\n \n@@ -10,7 +10,7 @@ class DownloadTimeoutMiddlewareTest(unittest.TestCase):\n \n     def get_request_spider_mw(self):\n         crawler = get_crawler()\n-        spider = BaseSpider('foo')\n+        spider = Spider('foo')\n         spider.set_crawler(crawler)\n         request = Request('http://scrapytest.org/')\n         return request, spider, DownloadTimeoutMiddleware.from_crawler(crawler)\n\n@@ -2,9 +2,9 @@ import unittest\n \n from scrapy.http import Request\n from scrapy.contrib.downloadermiddleware.httpauth import HttpAuthMiddleware\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n \n-class TestSpider(BaseSpider):\n+class TestSpider(Spider):\n     http_user = 'foo'\n     http_pass = 'bar'\n \n\n@@ -7,7 +7,7 @@ import email.utils\n from contextlib import contextmanager\n \n from scrapy.http import Response, HtmlResponse, Request\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.settings import Settings\n from scrapy.exceptions import IgnoreRequest\n from scrapy.utils.test import get_crawler\n@@ -24,7 +24,7 @@ class _BaseTest(unittest.TestCase):\n         self.today = email.utils.formatdate()\n         self.tomorrow = email.utils.formatdate(time.time() + 86400)\n         self.crawler = get_crawler()\n-        self.spider = BaseSpider('example.com')\n+        self.spider = Spider('example.com')\n         self.tmpdir = tempfile.mkdtemp()\n         self.request = Request('http://www.example.com',\n                                headers={'User-Agent': 'test'})\n\n@@ -3,7 +3,7 @@ from os.path import join, abspath, dirname\n from cStringIO import StringIO\n from gzip import GzipFile\n \n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.http import Response, Request, HtmlResponse\n from scrapy.contrib.downloadermiddleware.httpcompression import HttpCompressionMiddleware\n from scrapy.tests import tests_datadir\n@@ -22,7 +22,7 @@ FORMAT = {\n class HttpCompressionTest(TestCase):\n \n     def setUp(self):\n-        self.spider = BaseSpider('foo')\n+        self.spider = Spider('foo')\n         self.mw = HttpCompressionMiddleware()\n \n     def _getresponse(self, coding):\n\n@@ -5,9 +5,9 @@ from twisted.trial.unittest import TestCase, SkipTest\n from scrapy.contrib.downloadermiddleware.httpproxy import HttpProxyMiddleware\n from scrapy.exceptions import NotConfigured\n from scrapy.http import Response, Request\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n \n-spider = BaseSpider('foo')\n+spider = Spider('foo')\n \n class TestDefaultHeadersMiddleware(TestCase):\n \n\n@@ -1,7 +1,7 @@\n import unittest\n \n from scrapy.contrib.downloadermiddleware.redirect import RedirectMiddleware, MetaRefreshMiddleware\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.exceptions import IgnoreRequest\n from scrapy.http import Request, Response, HtmlResponse\n from scrapy.utils.test import get_crawler\n@@ -11,7 +11,7 @@ class RedirectMiddlewareTest(unittest.TestCase):\n \n     def setUp(self):\n         crawler = get_crawler()\n-        self.spider = BaseSpider('foo')\n+        self.spider = Spider('foo')\n         self.mw = RedirectMiddleware.from_crawler(crawler)\n \n     def test_priority_adjust(self):\n@@ -124,7 +124,7 @@ class MetaRefreshMiddlewareTest(unittest.TestCase):\n \n     def setUp(self):\n         crawler = get_crawler()\n-        self.spider = BaseSpider('foo')\n+        self.spider = Spider('foo')\n         self.mw = MetaRefreshMiddleware.from_crawler(crawler)\n \n     def _body(self, interval=5, url='http://example.org/newpage'):\n\n@@ -6,7 +6,7 @@ from twisted.internet.error import TimeoutError as ServerTimeoutError, \\\n from scrapy import optional_features\n from scrapy.contrib.downloadermiddleware.retry import RetryMiddleware\n from scrapy.xlib.tx import ResponseFailed\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.http import Request, Response\n from scrapy.utils.test import get_crawler\n \n@@ -14,7 +14,7 @@ from scrapy.utils.test import get_crawler\n class RetryTest(unittest.TestCase):\n     def setUp(self):\n         crawler = get_crawler()\n-        self.spider = BaseSpider('foo')\n+        self.spider = Spider('foo')\n         self.mw = RetryMiddleware.from_crawler(crawler)\n         self.mw.max_retry_times = 2\n \n\n@@ -2,7 +2,7 @@ from unittest import TestCase\n \n from scrapy.contrib.downloadermiddleware.stats import DownloaderStats\n from scrapy.http import Request, Response\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.utils.test import get_crawler\n \n \n@@ -10,7 +10,7 @@ class TestDownloaderStats(TestCase):\n \n     def setUp(self):\n         self.crawler = get_crawler()\n-        self.spider = BaseSpider('scrapytest.org')\n+        self.spider = Spider('scrapytest.org')\n         self.mw = DownloaderStats(self.crawler.stats)\n \n         self.crawler.stats.open_spider(self.spider)\n\n@@ -1,6 +1,6 @@\n from unittest import TestCase\n \n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.http import Request\n from scrapy.contrib.downloadermiddleware.useragent import UserAgentMiddleware\n from scrapy.utils.test import get_crawler\n@@ -10,7 +10,7 @@ class UserAgentMiddlewareTest(TestCase):\n \n     def get_spider_and_mw(self, default_useragent):\n         crawler = get_crawler({'USER_AGENT': default_useragent})\n-        spider = BaseSpider('foo')\n+        spider = Spider('foo')\n         spider.set_crawler(crawler)\n         return spider, UserAgentMiddleware.from_crawler(crawler)\n \n\n@@ -21,7 +21,7 @@ from scrapy import signals\n from scrapy.utils.test import get_crawler\n from scrapy.xlib.pydispatch import dispatcher\n from scrapy.tests import tests_datadir\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.item import Item, Field\n from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor\n from scrapy.http import Request\n@@ -32,7 +32,7 @@ class TestItem(Item):\n     url = Field()\n     price = Field()\n \n-class TestSpider(BaseSpider):\n+class TestSpider(Spider):\n     name = \"scrapytest.org\"\n     allowed_domains = [\"scrapytest.org\", \"localhost\"]\n \n\n@@ -4,7 +4,7 @@ from twisted.python import log as txlog, failure\n from twisted.trial import unittest\n \n from scrapy import log\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.settings import default_settings\n \n class LogTest(unittest.TestCase):\n@@ -41,7 +41,7 @@ class ScrapyFileLogObserverTest(unittest.TestCase):\n         self.assertEqual(self.logged(), \"[scrapy] INFO: Hello\")\n \n     def test_msg_spider(self):\n-        spider = BaseSpider(\"myspider\")\n+        spider = Spider(\"myspider\")\n         log.msg(\"Hello\", spider=spider)\n         self.assertEqual(self.logged(), \"[myspider] INFO: Hello\")\n \n@@ -58,7 +58,7 @@ class ScrapyFileLogObserverTest(unittest.TestCase):\n         self.assertEqual(self.logged(), \"[scrapy] NOLEVEL: Hello\")\n \n     def test_msg_level_spider(self):\n-        spider = BaseSpider(\"myspider\")\n+        spider = Spider(\"myspider\")\n         log.msg(\"Hello\", spider=spider, level=log.WARNING)\n         self.assertEqual(self.logged(), \"[myspider] WARNING: Hello\")\n \n\n@@ -1,6 +1,6 @@\n import unittest\n \n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.http import Request, Response\n from scrapy.item import Item, Field\n from scrapy.logformatter import LogFormatter\n@@ -18,7 +18,7 @@ class LoggingContribTest(unittest.TestCase):\n \n     def setUp(self):\n         self.formatter = LogFormatter()\n-        self.spider = BaseSpider('default')\n+        self.spider = Spider('default')\n \n     def test_crawled(self):\n         req = Request(\"http://www.example.com\")\n\n@@ -6,7 +6,7 @@ from twisted.internet.defer import Deferred, inlineCallbacks\n from twisted.python import log as txlog\n \n from scrapy.http import Request, Response\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.utils.request import request_fingerprint\n from scrapy.contrib.pipeline.media import MediaPipeline\n from scrapy.utils.signal import disconnect_all\n@@ -24,7 +24,7 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n     pipeline_class = MediaPipeline\n \n     def setUp(self):\n-        self.spider = BaseSpider('media.com')\n+        self.spider = Spider('media.com')\n         self.pipe = self.pipeline_class(download_func=_mocked_download_func)\n         self.pipe.open_spider(self.spider)\n         self.info = self.pipe.spiderinfo\n\n@@ -2,7 +2,7 @@ import unittest\n \n from scrapy.settings import Settings\n from scrapy.utils.test import get_crawler\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n \n class SettingsTest(unittest.TestCase):\n \n\n@@ -5,15 +5,15 @@ from cStringIO import StringIO\n \n from twisted.trial import unittest\n \n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.http import Response, TextResponse, XmlResponse, HtmlResponse\n from scrapy.contrib.spiders.init import InitSpider\n from scrapy.contrib.spiders import CrawlSpider, XMLFeedSpider, CSVFeedSpider, SitemapSpider\n \n \n-class BaseSpiderTest(unittest.TestCase):\n+class SpiderTest(unittest.TestCase):\n \n-    spider_class = BaseSpider\n+    spider_class = Spider\n \n     def setUp(self):\n         warnings.simplefilter(\"always\")\n@@ -43,12 +43,12 @@ class BaseSpiderTest(unittest.TestCase):\n         self.assertRaises(ValueError, self.spider_class, somearg='foo')\n \n \n-class InitSpiderTest(BaseSpiderTest):\n+class InitSpiderTest(SpiderTest):\n \n     spider_class = InitSpider\n \n \n-class XMLFeedSpiderTest(BaseSpiderTest):\n+class XMLFeedSpiderTest(SpiderTest):\n \n     spider_class = XMLFeedSpider\n \n@@ -92,17 +92,17 @@ class XMLFeedSpiderTest(BaseSpiderTest):\n             ], iterator)\n \n \n-class CSVFeedSpiderTest(BaseSpiderTest):\n+class CSVFeedSpiderTest(SpiderTest):\n \n     spider_class = CSVFeedSpider\n \n \n-class CrawlSpiderTest(BaseSpiderTest):\n+class CrawlSpiderTest(SpiderTest):\n \n     spider_class = CrawlSpider\n \n \n-class SitemapSpiderTest(BaseSpiderTest):\n+class SitemapSpiderTest(SpiderTest):\n \n     spider_class = SitemapSpider\n \n\n@@ -1,4 +1,4 @@\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n \n-class Spider0(BaseSpider):\n+class Spider0(Spider):\n     allowed_domains = [\"scrapy1.org\", \"scrapy3.org\"]\n\n@@ -1,5 +1,5 @@\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n \n-class Spider1(BaseSpider):\n+class Spider1(Spider):\n     name = \"spider1\"\n     allowed_domains = [\"scrapy1.org\", \"scrapy3.org\"]\n\n@@ -1,5 +1,5 @@\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n \n-class Spider2(BaseSpider):\n+class Spider2(Spider):\n     name = \"spider2\"\n     allowed_domains = [\"scrapy2.org\", \"scrapy3.org\"]\n\n@@ -1,6 +1,6 @@\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n \n-class Spider3(BaseSpider):\n+class Spider3(Spider):\n     name = \"spider3\"\n     allowed_domains = ['spider3.com']\n \n\n@@ -1,6 +1,6 @@\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n \n-class Spider4(BaseSpider):\n+class Spider4(Spider):\n     name = \"spider4\"\n \n     @classmethod\n\n@@ -2,7 +2,7 @@ from unittest import TestCase\n \n from scrapy.contrib.spidermiddleware.depth import DepthMiddleware\n from scrapy.http import Response, Request\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.statscol import StatsCollector\n from scrapy.utils.test import get_crawler\n \n@@ -10,7 +10,7 @@ from scrapy.utils.test import get_crawler\n class TestDepthMiddleware(TestCase):\n \n     def setUp(self):\n-        self.spider = BaseSpider('scrapytest.org')\n+        self.spider = Spider('scrapytest.org')\n \n         self.stats = StatsCollector(get_crawler())\n         self.stats.open_spider(self.spider)\n\n@@ -1,7 +1,7 @@\n from unittest import TestCase\n \n from scrapy.http import Response, Request\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.contrib.spidermiddleware.httperror import HttpErrorMiddleware, HttpError\n from scrapy.settings import Settings\n \n@@ -9,7 +9,7 @@ from scrapy.settings import Settings\n class TestHttpErrorMiddleware(TestCase):\n \n     def setUp(self):\n-        self.spider = BaseSpider('foo')\n+        self.spider = Spider('foo')\n         self.mw = HttpErrorMiddleware(Settings({}))\n         self.req = Request('http://scrapytest.org')\n \n@@ -47,7 +47,7 @@ class TestHttpErrorMiddlewareSettings(TestCase):\n     \"\"\"Similar test, but with settings\"\"\"\n \n     def setUp(self):\n-        self.spider = BaseSpider('foo')\n+        self.spider = Spider('foo')\n         self.mw = HttpErrorMiddleware(Settings({'HTTPERROR_ALLOWED_CODES': (402,)}))\n         self.req = Request('http://scrapytest.org')\n \n@@ -89,7 +89,7 @@ class TestHttpErrorMiddlewareSettings(TestCase):\n class TestHttpErrorMiddlewareHandleAll(TestCase):\n \n     def setUp(self):\n-        self.spider = BaseSpider('foo')\n+        self.spider = Spider('foo')\n         self.mw = HttpErrorMiddleware(Settings({'HTTPERROR_ALLOW_ALL': True}))\n         self.req = Request('http://scrapytest.org')\n \n\n@@ -1,7 +1,7 @@\n from unittest import TestCase\n \n from scrapy.http import Response, Request\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.contrib.spidermiddleware.offsite import OffsiteMiddleware\n \n \n@@ -13,7 +13,7 @@ class TestOffsiteMiddleware(TestCase):\n         self.mw.spider_opened(self.spider)\n \n     def _get_spider(self):\n-        return BaseSpider('foo', allowed_domains=['scrapytest.org', 'scrapy.org'])\n+        return Spider('foo', allowed_domains=['scrapytest.org', 'scrapy.org'])\n \n     def test_process_spider_output(self):\n         res = Response('http://scrapytest.org')\n@@ -33,7 +33,7 @@ class TestOffsiteMiddleware(TestCase):\n class TestOffsiteMiddleware2(TestOffsiteMiddleware):\n \n     def _get_spider(self):\n-        return BaseSpider('foo', allowed_domains=None)\n+        return Spider('foo', allowed_domains=None)\n \n     def test_process_spider_output(self):\n         res = Response('http://scrapytest.org')\n@@ -44,5 +44,5 @@ class TestOffsiteMiddleware2(TestOffsiteMiddleware):\n class TestOffsiteMiddleware3(TestOffsiteMiddleware2):\n \n     def _get_spider(self):\n-        return BaseSpider('foo')\n+        return Spider('foo')\n \n\n@@ -1,14 +1,14 @@\n from unittest import TestCase\n \n from scrapy.http import Response, Request\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.contrib.spidermiddleware.referer import RefererMiddleware\n \n \n class TestRefererMiddleware(TestCase):\n \n     def setUp(self):\n-        self.spider = BaseSpider('foo')\n+        self.spider = Spider('foo')\n         self.mw = RefererMiddleware()\n \n     def test_process_spider_output(self):\n\n@@ -2,7 +2,7 @@ from unittest import TestCase\n \n from scrapy.contrib.spidermiddleware.urllength import UrlLengthMiddleware\n from scrapy.http import Response, Request\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n \n \n class TestUrlLengthMiddleware(TestCase):\n@@ -15,7 +15,7 @@ class TestUrlLengthMiddleware(TestCase):\n         reqs = [short_url_req, long_url_req]\n \n         mw = UrlLengthMiddleware(maxlength=25)\n-        spider = BaseSpider('foo')\n+        spider = Spider('foo')\n         out = list(mw.process_spider_output(res, reqs, spider))\n         self.assertEquals(out, [short_url_req])\n \n\n@@ -1,6 +1,6 @@\n import unittest\n \n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.statscol import StatsCollector, DummyStatsCollector\n from scrapy.utils.test import get_crawler\n \n@@ -8,7 +8,7 @@ class StatsCollectorTest(unittest.TestCase):\n \n     def setUp(self):\n         self.crawler = get_crawler()\n-        self.spider = BaseSpider('foo')\n+        self.spider = Spider('foo')\n \n     def test_collector(self):\n         stats = StatsCollector(self.crawler)\n\n@@ -1,7 +1,7 @@\n import unittest\n \n from scrapy.http import Request\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.utils.reqser import request_to_dict, request_from_dict\n \n class RequestSerializationTest(unittest.TestCase):\n@@ -67,7 +67,7 @@ class RequestSerializationTest(unittest.TestCase):\n         self.assertRaises(ValueError, request_to_dict, r)\n \n \n-class TestSpider(BaseSpider):\n+class TestSpider(Spider):\n     name = 'test'\n     def parse_item(self, response):\n         pass\n\n@@ -6,7 +6,7 @@ from decimal import Decimal\n from twisted.internet import defer\n \n from scrapy.utils.serialize import SpiderReferencer, ScrapyJSONEncoder, ScrapyJSONDecoder\n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.http import Request, Response\n \n \n@@ -21,8 +21,8 @@ class CrawlerMock(object):\n class BaseTestCase(unittest.TestCase):\n \n     def setUp(self):\n-        self.spider1 = BaseSpider('name1')\n-        self.spider2 = BaseSpider('name2')\n+        self.spider1 = Spider('name1')\n+        self.spider2 = Spider('name2')\n         open_spiders = set([self.spider1, self.spider2])\n         crawler = CrawlerMock(open_spiders)\n         self.spref = SpiderReferencer(crawler)\n@@ -43,7 +43,7 @@ class SpiderReferencerTestCase(BaseTestCase):\n         sp1 = self.spref.get_spider_from_reference(ref1)\n         sp2 = self.spref.get_spider_from_reference(ref2)\n         sp1_ = self.spref.get_spider_from_reference(ref1)\n-        assert isinstance(sp1, BaseSpider)\n+        assert isinstance(sp1, Spider)\n         assert sp1 is not sp2\n         assert sp1 is sp1_\n \n\n@@ -1,6 +1,6 @@\n import unittest\n \n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.utils.url import url_is_from_any_domain, url_is_from_spider, canonicalize_url\n \n __doctests__ = ['scrapy.utils.url']\n@@ -26,14 +26,14 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertFalse(url_is_from_any_domain(url+'.testdomain.com', ['testdomain.com']))\n \n     def test_url_is_from_spider(self):\n-        spider = BaseSpider(name='example.com')\n+        spider = Spider(name='example.com')\n         self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', spider))\n         self.assertTrue(url_is_from_spider('http://sub.example.com/some/page.html', spider))\n         self.assertFalse(url_is_from_spider('http://www.example.org/some/page.html', spider))\n         self.assertFalse(url_is_from_spider('http://www.example.net/some/page.html', spider))\n \n     def test_url_is_from_spider_class_attributes(self):\n-        class MySpider(BaseSpider):\n+        class MySpider(Spider):\n             name = 'example.com'\n         self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', MySpider))\n         self.assertTrue(url_is_from_spider('http://sub.example.com/some/page.html', MySpider))\n@@ -41,7 +41,7 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertFalse(url_is_from_spider('http://www.example.net/some/page.html', MySpider))\n \n     def test_url_is_from_spider_with_allowed_domains(self):\n-        spider = BaseSpider(name='example.com', allowed_domains=['example.org', 'example.net'])\n+        spider = Spider(name='example.com', allowed_domains=['example.org', 'example.net'])\n         self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', spider))\n         self.assertTrue(url_is_from_spider('http://sub.example.com/some/page.html', spider))\n         self.assertTrue(url_is_from_spider('http://example.com/some/page.html', spider))\n@@ -49,14 +49,14 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertTrue(url_is_from_spider('http://www.example.net/some/page.html', spider))\n         self.assertFalse(url_is_from_spider('http://www.example.us/some/page.html', spider))\n \n-        spider = BaseSpider(name='example.com', allowed_domains=set(('example.com', 'example.net')))\n+        spider = Spider(name='example.com', allowed_domains=set(('example.com', 'example.net')))\n         self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', spider))\n \n-        spider = BaseSpider(name='example.com', allowed_domains=('example.com', 'example.net'))\n+        spider = Spider(name='example.com', allowed_domains=('example.com', 'example.net'))\n         self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', spider))\n \n     def test_url_is_from_spider_with_allowed_domains_class_attributes(self):\n-        class MySpider(BaseSpider):\n+        class MySpider(Spider):\n             name = 'example.com'\n             allowed_domains = ('example.org', 'example.net')\n         self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', MySpider))\n\n@@ -5,7 +5,7 @@ import json\n \n from twisted.internet import defer\n \n-from scrapy.spider import BaseSpider\n+from scrapy.spider import Spider\n from scrapy.http import Request, Response\n from scrapy.item import BaseItem\n \n@@ -42,7 +42,7 @@ class SpiderReferencer(object):\n \n     def encode_references(self, obj):\n         \"\"\"Look for Spider objects and replace them with spider references\"\"\"\n-        if isinstance(obj, BaseSpider):\n+        if isinstance(obj, Spider):\n             return self.get_reference_from_spider(obj)\n         elif isinstance(obj, dict):\n             d = {}\n\n@@ -14,11 +14,11 @@ def iter_spider_classes(module):\n     \"\"\"\n     # this needs to be imported here until get rid of the spider manager\n     # singleton in scrapy.spider.spiders\n-    from scrapy.spider import BaseSpider\n+    from scrapy.spider import Spider\n \n     for obj in vars(module).itervalues():\n         if inspect.isclass(obj) and \\\n-           issubclass(obj, BaseSpider) and \\\n+           issubclass(obj, Spider) and \\\n            obj.__module__ == module.__name__ and \\\n            getattr(obj, 'name', None):\n             yield obj\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
