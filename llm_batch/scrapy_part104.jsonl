{"custom_id": "scrapy#ef7a0972720b5ea1ff168af95aeaf61c23c9a770", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 18 | Files Changed: 4 | Hunks: 17 | Methods Changed: 11 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 36 | Churn Cumulative: 470 | Contributors (this commit): 1 | Commits (past 90d): 14 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -6,14 +6,14 @@ class EngineStatusResource(JsonResource):\n \n     ws_name = 'enginestatus'\n \n-    def __init__(self, spider_name=None, _manager=crawler):\n+    def __init__(self, spider_name=None, _crawler=crawler):\n         JsonResource.__init__(self)\n         self._spider_name = spider_name\n         self.isLeaf = spider_name is not None\n-        self._manager = _manager\n+        self._crawler = _crawler\n \n     def render_GET(self, txrequest):\n-        status = get_engine_status(self._manager.engine)\n+        status = get_engine_status(self._crawler.engine)\n         if self._spider_name is None:\n             return status\n         for sp, st in status['spiders'].items():\n@@ -21,4 +21,4 @@ class EngineStatusResource(JsonResource):\n                 return st\n \n     def getChild(self, name, txrequest):\n-        return EngineStatusResource(name, self._manager)\n+        return EngineStatusResource(name, self._crawler)\n\n@@ -4,10 +4,10 @@ from cStringIO import StringIO\n from scrapy.utils.jsonrpc import jsonrpc_client_call, jsonrpc_server_call, \\\n     JsonRpcError, jsonrpc_errors\n from scrapy.utils.serialize import ScrapyJSONDecoder\n-from scrapy.tests.test_utils_serialize import ExecutionMangerStub\n+from scrapy.tests.test_utils_serialize import CrawlerMock\n from scrapy.utils.py26 import json\n \n-class urllib_stub(object):\n+class urllib_mock(object):\n     def __init__(self, result=None, error=None):\n         response = {}\n         if result:\n@@ -33,11 +33,11 @@ class TestTarget(object):\n class JsonRpcUtilsTestCase(unittest.TestCase):\n \n     def setUp(self):\n-        crawler = ExecutionMangerStub([])\n+        crawler = CrawlerMock([])\n         self.json_decoder = ScrapyJSONDecoder(crawler=crawler)\n \n     def test_jsonrpc_client_call_request(self):\n-        ul = urllib_stub(1)\n+        ul = urllib_mock(1)\n         jsonrpc_client_call('url', 'test', 'one', 2, _urllib=ul)\n         req = json.loads(ul.request)\n         assert 'id' in req\n@@ -47,12 +47,12 @@ class JsonRpcUtilsTestCase(unittest.TestCase):\n         self.assertEqual(req['params'], ['one', 2])\n \n     def test_jsonrpc_client_call_response(self):\n-        ul = urllib_stub()\n+        ul = urllib_mock()\n         # must return result or error\n         self.assertRaises(ValueError, jsonrpc_client_call, 'url', 'test', _urllib=ul)\n-        ul = urllib_stub(result={'one': 1})\n+        ul = urllib_mock(result={'one': 1})\n         self.assertEquals(jsonrpc_client_call('url', 'test', _urllib=ul), {'one': 1})\n-        ul = urllib_stub(error={'code': 123, 'message': 'hello', 'data': 'some data'})\n+        ul = urllib_mock(error={'code': 123, 'message': 'hello', 'data': 'some data'})\n \n         raised = False\n         try:\n\n@@ -10,13 +10,13 @@ from scrapy.spider import BaseSpider\n from scrapy.http import Request, Response\n \n \n-class ExecutionEngineStub(object):\n+class _EngineMock(object):\n     def __init__(self, open_spiders):\n         self.open_spiders = open_spiders\n \n-class ExecutionMangerStub(object):\n+class CrawlerMock(object):\n     def __init__(self, open_spiders):\n-        self.engine = ExecutionEngineStub(open_spiders)\n+        self.engine = _EngineMock(open_spiders)\n \n class BaseTestCase(unittest.TestCase):\n \n@@ -24,8 +24,8 @@ class BaseTestCase(unittest.TestCase):\n         self.spider1 = BaseSpider('name1')\n         self.spider2 = BaseSpider('name2')\n         open_spiders = set([self.spider1, self.spider2])\n-        manager = ExecutionMangerStub(open_spiders)\n-        self.spref = SpiderReferencer(manager)\n+        crawler = CrawlerMock(open_spiders)\n+        self.spref = SpiderReferencer(crawler)\n         self.encoder = ScrapyJSONEncoder(spref=self.spref)\n         self.decoder = ScrapyJSONDecoder(spref=self.spref)\n \n\n@@ -22,7 +22,7 @@ class SpiderReferencer(object):\n     def __init__(self, crawler=None):\n         if crawler is None:\n             from scrapy.project import crawler\n-        self.manager = crawler\n+        self.crawler = crawler\n \n     def get_reference_from_spider(self, spider):\n         return 'spider:%x:%s' % (id(spider), spider.name)\n@@ -35,7 +35,7 @@ class SpiderReferencer(object):\n         m = self.spider_ref_re.search(ref)\n         if m:\n             spid, spname = m.groups()\n-            for spider in self.manager.engine.open_spiders:\n+            for spider in self.crawler.engine.open_spiders:\n                 if \"%x\" % id(spider) == spid or spider.name == spname:\n                     return spider\n             raise RuntimeError(\"Spider not running: %s\" % ref)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#40b590cad33e76c35902866f97b63ffa41a89f06", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 34 | Files Changed: 3 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/7 | Churn Δ: 64 | Churn Cumulative: 286 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -8,6 +8,7 @@ import os\n import cPickle as pickle\n \n from scrapy.conf import default_settings\n+from scrapy.utils.conf import set_scrapy_settings_envvar\n \n import_ = lambda x: __import__(x, {}, {}, [''])\n \n@@ -51,12 +52,16 @@ class Settings(object):\n \n class EnvironmentSettings(Settings):\n \n+    ENVVAR = 'SCRAPY_SETTINGS_MODULE'\n+\n     def __init__(self):\n         super(EnvironmentSettings, self).__init__()\n         self.defaults = {}\n         self.disabled = os.environ.get('SCRAPY_SETTINGS_DISABLED', False)\n-        settings_module_path = os.environ.get('SCRAPY_SETTINGS_MODULE', \\\n-            'scrapy_settings')\n+        if self.ENVVAR not in os.environ:\n+            project = os.environ.get('SCRAPY_PROJECT', 'default')\n+            set_scrapy_settings_envvar(project)\n+        settings_module_path = os.environ.get(self.ENVVAR, 'scrapy_settings')\n         self.set_settings_module(settings_module_path)\n \n         # XXX: find a better solution for this hack\n\n@@ -1,32 +0,0 @@\n-\"\"\"\n-Scrapy command-line tool\n-\"\"\"\n-\n-import sys, os\n-from ConfigParser import RawConfigParser\n-\n-def closest_scrapy_cfg(path='.', prevpath=None):\n-    if path == prevpath:\n-        return ''\n-    path = os.path.abspath(path)\n-    cfgfile = os.path.join(path, 'scrapy.cfg')\n-    if os.path.exists(cfgfile):\n-        return cfgfile\n-    return closest_scrapy_cfg(os.path.dirname(path), path)\n-\n-def main():\n-    scrapy_cfg = closest_scrapy_cfg()\n-    cfg_sources = [scrapy_cfg, os.path.expanduser('~/.scrapy.cfg'), '/etc/scrapy.cfg']\n-    cfg = RawConfigParser()\n-    cfg.read(cfg_sources)\n-    if cfg.has_option('default', 'settings'):\n-        os.environ['SCRAPY_SETTINGS_MODULE'] = cfg.get('default', 'settings')\n-        projdir = os.path.dirname(scrapy_cfg)\n-        if projdir not in sys.path:\n-            sys.path.append(projdir)\n-\n-    from scrapy.cmdline import execute\n-    execute()\n-\n-if __name__ == '__main__':\n-    main()\n\n@@ -1,3 +1,6 @@\n+import sys\n+import os\n+from ConfigParser import RawConfigParser\n from operator import itemgetter\n \n def build_component_list(base, custom):\n@@ -11,3 +14,23 @@ def build_component_list(base, custom):\n     compdict.update(custom)\n     return [k for k, v in sorted(compdict.items(), key=itemgetter(1)) \\\n         if v is not None]\n+\n+def closest_scrapy_cfg(path='.', prevpath=None):\n+    if path == prevpath:\n+        return ''\n+    path = os.path.abspath(path)\n+    cfgfile = os.path.join(path, 'scrapy.cfg')\n+    if os.path.exists(cfgfile):\n+        return cfgfile\n+    return closest_scrapy_cfg(os.path.dirname(path), path)\n+\n+def set_scrapy_settings_envvar(project='default', set_syspath=True):\n+    scrapy_cfg = closest_scrapy_cfg()\n+    cfg_sources = [scrapy_cfg, os.path.expanduser('~/.scrapy.cfg'), '/etc/scrapy.cfg']\n+    cfg = RawConfigParser()\n+    cfg.read(cfg_sources)\n+    if cfg.has_option(project, 'settings'):\n+        os.environ['SCRAPY_SETTINGS_MODULE'] = cfg.get(project, 'settings')\n+        projdir = os.path.dirname(scrapy_cfg)\n+        if set_syspath and projdir not in sys.path:\n+            sys.path.append(projdir)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#59d18cf99e899f8ec0b005e65a429ac5be2b9cb3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 244 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -88,7 +88,7 @@ class MediaPipeline(object):\n     def download(self, request, info):\n         \"\"\"Defines how to download the media request\"\"\"\n         request.priority = self.DOWNLOAD_PRIORITY\n-        return crawler.engine.download(request, info.spider)\n+        return self.crawler.engine.download(request, info.spider)\n \n     def media_to_download(self, request, info):\n         \"\"\"Check request before starting download\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a82a4be3ab49aa6fd25c05d6a78898c00a98a6a3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 339 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -1,3 +1,15 @@\n+\"\"\"\n+Scrapy engine tests\n+\n+This starts a testing web server (using twisted.server.Site) and then crawls it\n+with the Scrapy crawler.\n+\n+To view the testing web server in a brwoser you can run start it by running\n+this module with the ``runserver`` argument::\n+\n+    python test_engine.py runserver\n+\"\"\"\n+\n import sys, os, re, urlparse\n \n from twisted.internet import reactor, defer\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e95f7f63f9e922524e28d43f54876abd91f7e01a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 343 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -4,8 +4,8 @@ Scrapy engine tests\n This starts a testing web server (using twisted.server.Site) and then crawls it\n with the Scrapy crawler.\n \n-To view the testing web server in a brwoser you can run start it by running\n-this module with the ``runserver`` argument::\n+To view the testing web server in a brwoser you can start it by running this\n+module with the ``runserver`` argument::\n \n     python test_engine.py runserver\n \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#747f090f941bd676cc0ebb4afee04427d7723a0e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 3 | Churn Cumulative: 3 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -41,7 +41,8 @@ def add_missing_blockingCallFromThread():\n \n try:\n     import twisted\n-    if twisted.__version__ < '8.0.0':\n+    from twisted.python.versions import Version\n+    if twisted.version < Version(\"twisted\", 8, 0, 0):\n         add_missing_blockingCallFromThread()\n except ImportError:\n     pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f6c11af4c2741313fc671d49555864315b64ce73", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 95 | Lines Deleted: 90 | Files Changed: 7 | Hunks: 8 | Methods Changed: 18 | Complexity Δ (Sum/Max): 0/17 | Churn Δ: 185 | Churn Cumulative: 890 | Contributors (this commit): 5 | Commits (past 90d): 38 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -10,7 +10,7 @@ from scrapy.shell import Shell\n class Command(ScrapyCommand):\n \n     requires_project = False\n-    default_settings = {'QUEUE_CLASS': 'scrapy.core.queue.KeepAliveExecutionQueue'}\n+    default_settings = {'QUEUE_CLASS': 'scrapy.queue.KeepAliveExecutionQueue'}\n \n     def syntax(self):\n         return \"[url|file]\"\n\n@@ -175,7 +175,7 @@ MEMUSAGE_WARNING_MB = 0\n \n NEWSPIDER_MODULE = ''\n \n-QUEUE_CLASS = 'scrapy.core.queue.ExecutionQueue'\n+QUEUE_CLASS = 'scrapy.queue.ExecutionQueue'\n \n RANDOMIZE_DOWNLOAD_DELAY = True\n \n\n@@ -1,4 +1,4 @@\n-from scrapy.core.queue import ExecutionQueue\n+from scrapy.queue import ExecutionQueue\n from scrapy.utils.sqlite import JsonSqlitePriorityQueue\n from scrapy.conf import settings\n \n\n@@ -4,7 +4,7 @@ from twisted.internet import threads\n from boto.sqs.connection import SQSConnection\n from boto.sqs import regions\n \n-from scrapy.core.queue import ExecutionQueue\n+from scrapy.queue import ExecutionQueue\n from scrapy.utils.py26 import json\n from scrapy.conf import settings\n \n\n@@ -1,86 +1,5 @@\n-from twisted.internet import defer\n+from scrapy.queue import *\n \n-from scrapy.http import Request\n-from scrapy.utils.misc import arg_to_iter\n-from scrapy import log\n-\n-\n-class ExecutionQueue(object):\n-\n-    polling_delay = 5\n-\n-    def __init__(self, _spiders):\n-        self.spider_requests = []\n-        self._spiders = _spiders\n-\n-    def _append_next(self):\n-        \"\"\"Called when there are no more items left in self.spider_requests.\n-        This method is meant to be overriden in subclasses to add new (spider,\n-        requests) tuples to self.spider_requests. It can return a Deferred.\n-        \"\"\"\n-        pass\n-\n-    def get_next(self):\n-        \"\"\"Return a tuple (spider, requests) containing a list of Requests and\n-        the Spider which will be used to crawl those Requests. If there aren't\n-        any more spiders to crawl it must return (None, []).\n-\n-        This method can return a deferred.\n-        \"\"\"\n-        if self.spider_requests:\n-            return self._get_next_now()\n-        d = defer.maybeDeferred(self._append_next)\n-        d.addCallback(lambda _: self._get_next_now())\n-        return d\n-\n-    def _get_next_now(self):\n-        try:\n-            return self.spider_requests.pop(0)\n-        except IndexError:\n-            return (None, [])\n-\n-    def is_finished(self):\n-        \"\"\"Return True if the queue is empty and there won't be any more\n-        spiders to crawl (this is for one-shot runs). If it returns ``False``\n-        Scrapy will keep polling this queue for new requests to scrape\n-        \"\"\"\n-        return not bool(self.spider_requests)\n-\n-    def append_spider(self, spider):\n-        \"\"\"Append a Spider to crawl\"\"\"\n-        requests = spider.start_requests()\n-        self.spider_requests.append((spider, requests))\n-\n-    def append_request(self, request, spider=None, **kwargs):\n-        if spider is None:\n-            spider = self._spiders.create_for_request(request, **kwargs)\n-        if spider:\n-            self.spider_requests.append((spider, [request]))\n-\n-    def append_url(self, url, spider=None, **kwargs):\n-        \"\"\"Append a URL to crawl with the given spider. If the spider is not\n-        given, a spider will be looked up based on the URL\n-        \"\"\"\n-        if spider is None:\n-            spider = self._spiders.create_for_request(Request(url), **kwargs)\n-        if spider:\n-            requests = arg_to_iter(spider.make_requests_from_url(url))\n-            self.spider_requests.append((spider, requests))\n-\n-    def append_spider_name(self, spider_name, **spider_kwargs):\n-        \"\"\"Append a spider to crawl given its name and optional arguments,\n-        which are used to instantiate it. The SpiderManager is used to lookup\n-        the spider\n-        \"\"\"\n-        try:\n-            spider = self._spiders.create(spider_name, **spider_kwargs)\n-        except KeyError:\n-            log.msg('Unable to find spider: %s' % spider_name, log.ERROR)\n-        else:\n-            self.append_spider(spider)\n-\n-\n-class KeepAliveExecutionQueue(ExecutionQueue):\n-\n-    def is_finished(self):\n-        return False\n+import warnings\n+warnings.warn(\"scrapy.core.queue is deprecated and will be removed in Scrapy 0.11, use scrapy.queue instead\", \\\n+    DeprecationWarning, stacklevel=2)\n\n@@ -0,0 +1,86 @@\n+from twisted.internet import defer\n+\n+from scrapy.http import Request\n+from scrapy.utils.misc import arg_to_iter\n+from scrapy import log\n+\n+\n+class ExecutionQueue(object):\n+\n+    polling_delay = 5\n+\n+    def __init__(self, _spiders):\n+        self.spider_requests = []\n+        self._spiders = _spiders\n+\n+    def _append_next(self):\n+        \"\"\"Called when there are no more items left in self.spider_requests.\n+        This method is meant to be overriden in subclasses to add new (spider,\n+        requests) tuples to self.spider_requests. It can return a Deferred.\n+        \"\"\"\n+        pass\n+\n+    def get_next(self):\n+        \"\"\"Return a tuple (spider, requests) containing a list of Requests and\n+        the Spider which will be used to crawl those Requests. If there aren't\n+        any more spiders to crawl it must return (None, []).\n+\n+        This method can return a deferred.\n+        \"\"\"\n+        if self.spider_requests:\n+            return self._get_next_now()\n+        d = defer.maybeDeferred(self._append_next)\n+        d.addCallback(lambda _: self._get_next_now())\n+        return d\n+\n+    def _get_next_now(self):\n+        try:\n+            return self.spider_requests.pop(0)\n+        except IndexError:\n+            return (None, [])\n+\n+    def is_finished(self):\n+        \"\"\"Return True if the queue is empty and there won't be any more\n+        spiders to crawl (this is for one-shot runs). If it returns ``False``\n+        Scrapy will keep polling this queue for new requests to scrape\n+        \"\"\"\n+        return not bool(self.spider_requests)\n+\n+    def append_spider(self, spider):\n+        \"\"\"Append a Spider to crawl\"\"\"\n+        requests = spider.start_requests()\n+        self.spider_requests.append((spider, requests))\n+\n+    def append_request(self, request, spider=None, **kwargs):\n+        if spider is None:\n+            spider = self._spiders.create_for_request(request, **kwargs)\n+        if spider:\n+            self.spider_requests.append((spider, [request]))\n+\n+    def append_url(self, url, spider=None, **kwargs):\n+        \"\"\"Append a URL to crawl with the given spider. If the spider is not\n+        given, a spider will be looked up based on the URL\n+        \"\"\"\n+        if spider is None:\n+            spider = self._spiders.create_for_request(Request(url), **kwargs)\n+        if spider:\n+            requests = arg_to_iter(spider.make_requests_from_url(url))\n+            self.spider_requests.append((spider, requests))\n+\n+    def append_spider_name(self, spider_name, **spider_kwargs):\n+        \"\"\"Append a spider to crawl given its name and optional arguments,\n+        which are used to instantiate it. The SpiderManager is used to lookup\n+        the spider\n+        \"\"\"\n+        try:\n+            spider = self._spiders.create(spider_name, **spider_kwargs)\n+        except KeyError:\n+            log.msg('Unable to find spider: %s' % spider_name, log.ERROR)\n+        else:\n+            self.append_spider(spider)\n+\n+\n+class KeepAliveExecutionQueue(ExecutionQueue):\n+\n+    def is_finished(self):\n+        return False\n\n@@ -1,6 +1,6 @@\n import unittest\n \n-from scrapy.core.queue import ExecutionQueue, KeepAliveExecutionQueue\n+from scrapy.queue import ExecutionQueue, KeepAliveExecutionQueue\n from scrapy.spider import BaseSpider\n from scrapy.http import Request\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d1e260a8d40a3a639526cbfaaa861a525649c0db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 31 | Files Changed: 3 | Hunks: 12 | Methods Changed: 8 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 36 | Churn Cumulative: 1875 | Contributors (this commit): 4 | Commits (past 90d): 25 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -9,10 +9,8 @@ from time import time\n from twisted.internet import reactor, defer\n from twisted.python.failure import Failure\n \n-from scrapy import log\n+from scrapy import log, signals\n from scrapy.stats import stats\n-from scrapy.conf import settings\n-from scrapy import signals\n from scrapy.core.downloader import Downloader\n from scrapy.core.scraper import Scraper\n from scrapy.exceptions import IgnoreRequest, DontCloseSpider\n@@ -23,25 +21,17 @@ from scrapy.utils.defer import mustbe_deferred\n \n class ExecutionEngine(object):\n \n-    def __init__(self, crawler):\n-        self.crawler = crawler\n-        self.configured = False\n+    def __init__(self, settings, spider_closed_callback):\n+        self.settings = settings\n         self.closing = {} # dict (spider -> reason) of spiders being closed\n         self.closing_dfds = {} # dict (spider -> deferred) of spiders being closed\n         self.running = False\n-        self.killed = False\n         self.paused = False\n         self._next_request_calls = {}\n         self._crawled_logline = load_object(settings['LOG_FORMATTER_CRAWLED'])\n-\n-    def configure(self, spider_closed_callback):\n-        \"\"\"\n-        Configure execution engine with the given scheduling policy and downloader.\n-        \"\"\"\n         self.scheduler = load_object(settings['SCHEDULER'])()\n         self.downloader = Downloader()\n-        self.scraper = Scraper(self, self.crawler.settings)\n-        self.configured = True\n+        self.scraper = Scraper(self, self.settings)\n         self._spider_closed_callback = spider_closed_callback\n \n     @defer.inlineCallbacks\n@@ -59,13 +49,6 @@ class ExecutionEngine(object):\n         dfd = self._close_all_spiders()\n         return dfd.addBoth(lambda _: self._finish_stopping_engine())\n \n-    def kill(self):\n-        \"\"\"Forces shutdown without waiting for pending transfers to finish.\n-        stop() must have been called first\n-        \"\"\"\n-        assert not self.running, \"Call engine.stop() before engine.kill()\"\n-        self.killed = True\n-\n     def pause(self):\n         \"\"\"Pause the execution engine\"\"\"\n         self.paused = True\n@@ -256,8 +239,6 @@ class ExecutionEngine(object):\n         dfd.addErrback(log.err, \"Unhandled error in scraper.close_spider()\", \\\n             spider=spider)\n         dfd.addBoth(lambda _: self._finish_closing_spider(spider))\n-        if self.killed:\n-            return self._finish_closing_spider(spider)\n         return dfd\n \n     def _close_all_spiders(self):\n@@ -277,9 +258,6 @@ class ExecutionEngine(object):\n         dfd.addBoth(lambda _: stats.close_spider(spider, reason=reason))\n         dfd.addErrback(log.err, \"Unhandled error in stats.close_spider()\",\n             spider=spider)\n-        dfd.addBoth(lambda _: self.crawler.spiders.close_spider(spider))\n-        dfd.addErrback(log.err, \"Unhandled error in spiders.close_spider()\",\n-            spider=spider)\n         dfd.addBoth(lambda _: log.msg(\"Spider closed (%s)\" % reason, spider=spider))\n         dfd.addBoth(lambda _: self.closing_dfds.pop(spider).callback(spider))\n         dfd.addBoth(lambda _: self._spider_closed_callback(spider))\n\n@@ -30,13 +30,12 @@ class Crawler(object):\n         if self.configured:\n             return\n         self.configured = True\n-        self.engine = ExecutionEngine(self)\n         self.extensions = ExtensionManager.from_settings(self.settings)\n         spman_cls = load_object(self.settings['SPIDER_MANAGER_CLASS'])\n         self.spiders = spman_cls.from_settings(self.settings)\n         queue_cls = load_object(self.settings['QUEUE_CLASS'])\n         self.queue = queue_cls(self.spiders)\n-        self.engine.configure(self._spider_closed)\n+        self.engine = ExecutionEngine(self.settings, self._spider_closed)\n \n     @defer.inlineCallbacks\n     def _start_next_spider(self):\n\n@@ -62,6 +62,3 @@ class SpiderManager(object):\n     def list(self):\n         \"\"\"Returns list of spiders available.\"\"\"\n         return self._spiders.keys()\n-\n-    def close_spider(self, spider):\n-        pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ad18d4a70ea7f5ab48c306d4a4850c3de9459f46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 14 | Files Changed: 6 | Hunks: 10 | Methods Changed: 10 | Complexity Δ (Sum/Max): 3/5 | Churn Δ: 45 | Churn Cumulative: 2829 | Contributors (this commit): 6 | Commits (past 90d): 57 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -152,7 +152,7 @@ ITEM_PIPELINES = []\n \n LOG_ENABLED = True\n LOG_ENCODING = 'utf-8'\n-LOG_FORMATTER_CRAWLED = 'scrapy.contrib.logformatter.crawled_logline'\n+LOG_FORMATTER = 'scrapy.logformatter.LogFormatter'\n LOG_STDOUT = False\n LOG_LEVEL = 'DEBUG'\n LOG_FILE = None\n\n@@ -1,7 +0,0 @@\n-\"\"\"Functions for logging diferent actions\"\"\"\n-\n-def crawled_logline(request, response):\n-    referer = request.headers.get('Referer')\n-    flags = ' %s' % str(response.flags) if response.flags else ''\n-    return \"Crawled (%d) %s (referer: %s)%s\" % (response.status, \\\n-        request, referer, flags)\n\n@@ -28,7 +28,6 @@ class ExecutionEngine(object):\n         self.running = False\n         self.paused = False\n         self._next_request_calls = {}\n-        self._crawled_logline = load_object(settings['LOG_FORMATTER_CRAWLED'])\n         self.scheduler = load_object(settings['SCHEDULER'])()\n         self.downloader = Downloader()\n         self.scraper = Scraper(self, self.settings)\n@@ -156,7 +155,7 @@ class ExecutionEngine(object):\n             assert isinstance(response, (Response, Request))\n             if isinstance(response, Response):\n                 response.request = request # tie request to response received\n-                log.msg(self._crawled_logline(request, response), \\\n+                log.msg(log.formatter.crawled(request, response, spider), \\\n                     level=log.DEBUG, spider=spider)\n                 return response\n             elif isinstance(response, Request):\n\n@@ -168,8 +168,8 @@ class Scraper(object):\n                 spider=spider)\n             self.engine.crawl(request=output, spider=spider)\n         elif isinstance(output, BaseItem):\n-            log.msg(\"Scraped %s in <%s>\" % (output, request.url), level=log.DEBUG, \\\n-                spider=spider)\n+            log.msg(log.formatter.scraped(output, request, response, spider), \\\n+                level=log.DEBUG, spider=spider)\n             self.sites[spider].itemproc_size += 1\n             dfd = send_catch_log_deferred(signal=signals.item_scraped, \\\n                 item=output, spider=spider, response=response)\n@@ -204,13 +204,14 @@ class Scraper(object):\n         if isinstance(output, Failure):\n             ex = output.value\n             if isinstance(ex, DropItem):\n-                log.msg(\"Dropped %s - %s\" % (item, str(ex)), level=log.WARNING, spider=spider)\n+                log.msg(log.formatter.dropped(item, ex, spider), \\\n+                    level=log.WARNING, spider=spider)\n                 return send_catch_log_deferred(signal=signals.item_dropped, \\\n                     item=item, spider=spider, exception=output.value)\n             else:\n                 log.err(output, 'Error processing %s' % item, spider=spider)\n         else:\n-            log.msg(\"Passed %s\" % item, log.INFO, spider=spider)\n+            log.msg(log.formatter.passed(item, spider), log.INFO, spider=spider)\n             return send_catch_log_deferred(signal=signals.item_passed, \\\n                 item=item, spider=spider, output=output)\n \n\n@@ -10,6 +10,7 @@ from twisted.python import log\n \n from scrapy.conf import settings\n from scrapy.utils.python import unicode_to_str\n+from scrapy.utils.misc import load_object\n  \n # Logging levels\n DEBUG = logging.DEBUG\n@@ -117,3 +118,5 @@ def err(_stuff=None, _why=None, **kw):\n     kw.setdefault('system', 'scrapy')\n     kw['logLevel'] = kw.pop('level', ERROR)\n     log.err(_stuff, _why, **kw)\n+\n+formatter = load_object(settings['LOG_FORMATTER'])()\n\n@@ -0,0 +1,21 @@\n+\n+class LogFormatter(object):\n+    \"\"\"Class for generating log messages for different actions. All methods\n+    must return a plain string which doesn't include the log level or the\n+    timestamp\n+    \"\"\"\n+\n+    def crawled(self, request, response, spider):\n+        referer = request.headers.get('Referer')\n+        flags = ' %s' % str(response.flags) if response.flags else ''\n+        return \"Crawled (%d) %s (referer: %s)%s\" % (response.status, \\\n+            request, referer, flags)\n+\n+    def scraped(self, item, request, response, spider):\n+        return \"Scraped %s in <%s>\" % (item, request.url)\n+\n+    def dropped(self, item, exception, spider):\n+        return \"Dropped %s - %s\" % (item, str(exception))\n+\n+    def passed(self, item, spider):\n+        return \"Passed %s\" % item\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#648f700ed11ec9ab45cf91fae940e4f8b8aeb0fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 13 | Churn Cumulative: 13 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,20 +1,25 @@\n import unittest\n \n+from scrapy.spider import BaseSpider\n from scrapy.http import Request, Response\n-from scrapy.contrib.logformatter import crawled_logline\n+from scrapy.logformatter import LogFormatter\n \n \n class LoggingContribTest(unittest.TestCase):\n \n-    def test_crawled_logline(self):\n+    def setUp(self):\n+        self.formatter = LogFormatter()\n+        self.spider = BaseSpider('default')\n+\n+    def test_crawled(self):\n         req = Request(\"http://www.example.com\")\n         res = Response(\"http://www.example.com\")\n-        self.assertEqual(crawled_logline(req, res),\n+        self.assertEqual(self.formatter.crawled(req, res, self.spider),\n             \"Crawled (200) <GET http://www.example.com> (referer: None)\")\n \n         req = Request(\"http://www.example.com\", headers={'referer': 'http://example.com'})\n         res = Response(\"http://www.example.com\", flags=['cached'])\n-        self.assertEqual(crawled_logline(req, res),\n+        self.assertEqual(self.formatter.crawled(req, res, self.spider),\n             \"Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']\")\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e14cc2c12ac3c517f03ab54689d3292f24b8116c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 25/25 | Churn Δ: 26 | Churn Cumulative: 26 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,8 +1,14 @@\n #!/usr/bin/env python\n \"\"\"\n-Example script to control and monitor Scrapy using its web service. It only\n-provides a reduced functionality as its main purpose is to illustrate how to\n-write a web service client. Feel free to improve or write you own.\n+Example script to control a Scrapy server using its JSON-RPC web service.\n+\n+It only provides a reduced functionality as its main purpose is to illustrate\n+how to write a web service client. Feel free to improve or write you own.\n+\n+Also, keep in mind that the JSON-RPC API is not stable. The recommended way for\n+controlling a Scrapy server is through the execution queue (see the \"queue\"\n+command).\n+\n \"\"\"\n \n import sys, optparse, urllib\n@@ -19,7 +25,6 @@ def get_commands():\n         'list-available': cmd_list_available,\n         'list-running': cmd_list_running,\n         'list-resources': cmd_list_resources,\n-        'list-extensions': cmd_list_extensions,\n         'get-global-stats': cmd_get_global_stats,\n         'get-spider-stats': cmd_get_spider_stats,\n     }\n@@ -32,20 +37,20 @@ def cmd_help(args, opts):\n \n def cmd_run(args, opts):\n     \"\"\"run <spider_name> - schedule spider for running\"\"\"\n-    jsonrpc_call(opts, 'manager/queue', 'append_spider_name', args[0])\n+    jsonrpc_call(opts, 'crawler/queue', 'append_spider_name', args[0])\n \n def cmd_stop(args, opts):\n     \"\"\"stop <spider> - stop a running spider\"\"\"\n-    jsonrpc_call(opts, 'manager/engine', 'close_spider', args[0])\n+    jsonrpc_call(opts, 'crawler/engine', 'close_spider', args[0])\n \n def cmd_list_running(args, opts):\n     \"\"\"list-running - list running spiders\"\"\"\n-    for x in json_get(opts, 'manager/engine/open_spiders'):\n+    for x in json_get(opts, 'crawler/engine/open_spiders'):\n         print x\n \n def cmd_list_available(args, opts):\n     \"\"\"list-available - list name of available spiders\"\"\"\n-    for x in jsonrpc_call(opts, 'spiders', 'list'):\n+    for x in jsonrpc_call(opts, 'crawler/spiders', 'list'):\n         print x\n \n def cmd_list_resources(args, opts):\n@@ -53,11 +58,6 @@ def cmd_list_resources(args, opts):\n     for x in json_get(opts, '')['resources']:\n         print x\n \n-def cmd_list_extensions(args, opts):\n-    \"\"\"list-extensions - list enabled extensions\"\"\"\n-    for x in jsonrpc_call(opts, 'extensions/enabled', 'keys'):\n-        print x\n-\n def cmd_get_spider_stats(args, opts):\n     \"\"\"get-spider-stats <spider> - get stats of a running spider\"\"\"\n     stats = jsonrpc_call(opts, 'stats', 'get_stats', args[0])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#aab38be4983821abc90d124be19a9c881999a87e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 308 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -8,6 +8,7 @@ import logging\n \n from twisted.python import log\n \n+import scrapy\n from scrapy.conf import settings\n from scrapy.utils.python import unicode_to_str\n from scrapy.utils.misc import load_object\n@@ -103,7 +104,8 @@ def start(logfile=None, loglevel=None, logstdout=None):\n             logstdout = settings.getbool('LOG_STDOUT')\n         sflo = ScrapyFileLogObserver(file, loglevel, settings['LOG_ENCODING'])\n         log.startLoggingWithObserver(sflo.emit, setStdout=logstdout)\n-        msg(\"Started project: %s\" % settings['BOT_NAME'])\n+        msg(\"Scrapy %s started (bot: %s)\" % (scrapy.__version__, \\\n+            settings['BOT_NAME']))\n \n def msg(message, level=INFO, **kw):\n     if 'component' in kw:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e7b3247a180e228176282af89c1b4def5e9290f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 121 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -74,7 +74,7 @@ if len(sys.argv) > 1 and sys.argv[1] == 'bdist_wininst':\n     for file_info in data_files:\n         file_info[0] = '\\\\PURELIB\\\\%s' % file_info[0]\n \n-scripts = ['bin/scrapy', 'bin/scrapy-ws.py']\n+scripts = ['bin/scrapy']\n if os.name == 'nt':\n     scripts.append('extras/scrapy.bat')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4eb0383dd26abe80dda772649dcca897fd100aaa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 15 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -94,7 +94,7 @@ class ScrapyCommand(object):\n                 sys.exit(2)\n \n         if opts.version:\n-            print scrapy.__version__\n+            print \"Scrapy %s\" % scrapy.__version__\n             sys.exit()\n \n         if opts.logfile:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ffad8e08e727162e05f3f791b615d2b67bb6725a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 26 | Churn Cumulative: 219 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -57,25 +57,29 @@ class ExecutionQueue(object):\n         if spider:\n             self.spider_requests.append((spider, [request]))\n \n-    def append_url(self, url, spider=None, **kwargs):\n+    def append_url(self, url=None, spider=None, **kwargs):\n         \"\"\"Append a URL to crawl with the given spider. If the spider is not\n         given, a spider will be looked up based on the URL\n         \"\"\"\n+        if url is None:\n+            raise ValueError(\"A url is required\")\n         if spider is None:\n             spider = self._spiders.create_for_request(Request(url), **kwargs)\n         if spider:\n             requests = arg_to_iter(spider.make_requests_from_url(url))\n             self.spider_requests.append((spider, requests))\n \n-    def append_spider_name(self, spider_name, **spider_kwargs):\n+    def append_spider_name(self, name=None, **spider_kwargs):\n         \"\"\"Append a spider to crawl given its name and optional arguments,\n         which are used to instantiate it. The SpiderManager is used to lookup\n         the spider\n         \"\"\"\n+        if name is None:\n+            raise ValueError(\"A spider name is required\")\n         try:\n-            spider = self._spiders.create(spider_name, **spider_kwargs)\n+            spider = self._spiders.create(name, **spider_kwargs)\n         except KeyError:\n-            log.msg('Unable to find spider: %s' % spider_name, log.ERROR)\n+            log.msg('Unable to find spider: %s' % name, log.ERROR)\n         else:\n             self.append_spider(spider)\n \n\n@@ -71,6 +71,14 @@ class ExecutionQueueTest(unittest.TestCase):\n         self._assert_request_urls(self.queue.spider_requests[0][1], \\\n             ['http://www.example.com/asd/make1', 'http://www.example.com/asd/make2'])\n \n+    def test_append_url_kwarg(self):\n+        spider = TestSpider()\n+        url = 'http://www.example.com/asd'\n+        self.queue.append_url(url=url, spider=spider)\n+        self.assert_(self.queue.spider_requests[0][0] is spider)\n+        self._assert_request_urls(self.queue.spider_requests[0][1], \\\n+            ['http://www.example.com/asd/make1', 'http://www.example.com/asd/make2'])\n+\n     def test_append_url2(self):\n         url = 'http://www.example.com/asd'\n         self.queue.append_url(url, arg='123')\n@@ -86,6 +94,12 @@ class ExecutionQueueTest(unittest.TestCase):\n         self.assert_(spider.name == 'test123')\n         self.assert_(spider.arg == '123')\n \n+    def test_append_spider_name_kwarg(self):\n+        self.queue.append_spider_name(name='test123', arg='123')\n+        spider = self.queue.spider_requests[0][0]\n+        self.assert_(spider.name == 'test123')\n+        self.assert_(spider.arg == '123')\n+\n     def _assert_request_urls(self, requests, urls):\n         assert all(isinstance(x, Request) for x in requests)\n         self.assertEqual([x.url for x in requests], urls)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#88c99e0b838acbc502d0cde149a04d5e6c81c835", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 5 | Churn Cumulative: 248 | Contributors (this commit): 1 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -36,6 +36,9 @@ class JsonRpcUtilsTestCase(unittest.TestCase):\n         crawler = CrawlerMock([])\n         self.json_decoder = ScrapyJSONDecoder(crawler=crawler)\n \n+    def test_jsonrpc_client_call_args_kwargs_raises(self):\n+        self.assertRaises(ValueError, jsonrpc_client_call, 'url', 'test', 'one', kw=123)\n+\n     def test_jsonrpc_client_call_request(self):\n         ul = urllib_mock(1)\n         jsonrpc_client_call('url', 'test', 'one', 2, _urllib=ul)\n\n@@ -31,6 +31,8 @@ class JsonRpcError(Exception):\n def jsonrpc_client_call(url, method, *args, **kwargs):\n     \"\"\"Execute a JSON-RPC call on the given url\"\"\"\n     _urllib = kwargs.pop('_urllib', urllib)\n+    if args and kwargs:\n+        raise ValueError(\"Pass *args or **kwargs but not both to jsonrpc_client_call\")\n     req = {'jsonrpc': '2.0', 'method': method, 'params': args or kwargs, 'id': 1}\n     res = json.loads(_urllib.urlopen(url, json.dumps(req)).read())\n     if 'result' in res:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#783887457a374f6028415496d87b98e29a2d72da", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 0 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 0 | Churn Cumulative: 0 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3234d76b8d8a9d49ce8d34e166400bfd17582dcb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 42 | Contributors (this commit): 1 | Commits (past 90d): 4 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,9 +3,10 @@ SpiderManager is the class which locates and manages all website-specific\n spiders\n \"\"\"\n \n-from scrapy import log\n+from scrapy import log, signals\n from scrapy.utils.misc import walk_modules\n from scrapy.utils.spider import iter_spider_classes\n+from scrapy.xlib.pydispatch import dispatcher\n \n \n class SpiderManager(object):\n@@ -16,6 +17,7 @@ class SpiderManager(object):\n         for name in self.spider_modules:\n             for module in walk_modules(name):\n                 self._load_spiders(module)\n+        dispatcher.connect(self.close_spider, signals.spider_closed)\n \n     def _load_spiders(self, module):\n         for spcls in iter_spider_classes(module):\n@@ -62,3 +64,6 @@ class SpiderManager(object):\n     def list(self):\n         \"\"\"Returns list of spiders available.\"\"\"\n         return self._spiders.keys()\n+\n+    def close_spider(self, spider, reason):\n+        pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#35fd1a26600f84fbccd551bcdead7cbc7e451f67", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 44 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -29,7 +29,7 @@ class SpiderManager(object):\n \n     def create(self, spider_name, **spider_kwargs):\n         \"\"\"Returns a Spider instance for the given spider name, using the given\n-        spider arguments. If the sipder name is not found, it raises a\n+        spider arguments. If the spider name is not found, it raises a\n         KeyError.\n         \"\"\"\n         return self._spiders[spider_name](**spider_kwargs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0ae4c3aa82ce8c35ba6da97a4ebd6518dd32e2a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 48 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -66,4 +66,6 @@ class SpiderManager(object):\n         return self._spiders.keys()\n \n     def close_spider(self, spider, reason):\n-        pass\n+        closed = getattr(spider, 'closed', None)\n+        if callable(closed):\n+            return closed(reason)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d4941a06194572f1897730c22ded10aa6adcbc1d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 17 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -110,7 +110,7 @@ class ScrapyCommand(object):\n \n         if opts.pidfile:\n             with open(opts.pidfile, \"w\") as f:\n-                f.write(str(os.getpid()))\n+                f.write(str(os.getpid()) + os.linesep)\n \n     def run(self, args, opts):\n         \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#616ecc5a1a6966c1dd7ff055e993496ebe3d6f18", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 12 | Files Changed: 4 | Hunks: 13 | Methods Changed: 7 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 42 | Churn Cumulative: 140 | Contributors (this commit): 1 | Commits (past 90d): 14 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -10,6 +10,7 @@ from optparse import OptionGroup\n \n import scrapy\n from scrapy.conf import settings\n+from scrapy.utils.conf import arglist_to_dict\n \n class ScrapyCommand(object):\n \n@@ -74,7 +75,7 @@ class ScrapyCommand(object):\n             help=\"write lsprof profiling stats to FILE\")\n         group.add_option(\"--pidfile\", dest=\"pidfile\", metavar=\"FILE\", \\\n             help=\"write process ID to FILE\")\n-        group.add_option(\"--set\", dest=\"set\", action=\"append\", default=[], \\\n+        group.add_option(\"--set\", dest=\"set\", action=\"append\", default=[], metavar=\"NAME=VALUE\", \\\n             help=\"set/override setting (may be repeated)\")\n         group.add_option(\"--settings\", dest=\"settings\", metavar=\"MODULE\",\n             help=\"python path to the Scrapy project settings\")\n@@ -84,13 +85,10 @@ class ScrapyCommand(object):\n         if opts.settings:\n             settings.set_settings_module(opts.settings)\n \n-        for setting in opts.set:\n-            if '=' in setting:\n-                name, val = setting.split('=', 1)\n-                settings.overrides[name] = val\n-            else:\n-                sys.stderr.write(\"%s: invalid argument --set %s - proper format \" \\\n-                    \"is --set SETTING=VALUE'\\n\" % (sys.argv[0], setting))\n+        try:\n+            settings.overrides.update(arglist_to_dict(opts.set))\n+        except ValueError:\n+            sys.stderr.write(\"Invalid --set value, use --set NAME=VALUE\\n\")\n             sys.exit(2)\n \n         if opts.version:\n\n@@ -1,8 +1,11 @@\n+import sys\n+\n from scrapy import log\n from scrapy.command import ScrapyCommand\n from scrapy.conf import settings\n from scrapy.http import Request\n from scrapy.utils.url import is_url\n+from scrapy.utils.conf import arglist_to_dict\n \n from collections import defaultdict\n \n@@ -20,11 +23,18 @@ class Command(ScrapyCommand):\n         ScrapyCommand.add_options(self, parser)\n         parser.add_option(\"--spider\", dest=\"spider\", default=None, \\\n             help=\"always use this spider when arguments are urls\")\n+        parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\", \\\n+            help=\"set spider argument (may be repeated)\")\n         parser.add_option(\"-n\", \"--nofollow\", dest=\"nofollow\", action=\"store_true\", \\\n             help=\"don't follow links (for use with URLs only)\")\n \n     def process_options(self, args, opts):\n         ScrapyCommand.process_options(self, args, opts)\n+        try:\n+            opts.spargs = arglist_to_dict(opts.spargs)\n+        except ValueError:\n+            sys.stderr.write(\"Invalid --arg value, use --arg NAME=VALUE\\n\")\n+            sys.exit(2)\n         if opts.nofollow:\n             settings.overrides['CRAWLSPIDER_FOLLOW_LINKS'] = False\n \n@@ -32,18 +42,18 @@ class Command(ScrapyCommand):\n         q = self.crawler.queue\n         urls, names = self._split_urls_and_names(args)\n         for name in names:\n-            q.append_spider_name(name)\n+            q.append_spider_name(name, **opts.spargs)\n \n         if opts.spider:\n             try:\n-                spider = self.crawler.spiders.create(opts.spider)\n+                spider = self.crawler.spiders.create(opts.spider, **opts.spargs)\n                 for url in urls:\n                     q.append_url(url, spider)\n             except KeyError:\n                 log.msg('Unable to find spider: %s' % opts.spider, log.ERROR)\n         else:\n             for name, urls in self._group_urls_by_spider(urls):\n-                spider = self.crawler.spiders.create(name)\n+                spider = self.crawler.spiders.create(name, **opts.spargs)\n                 for url in urls:\n                     q.append_url(url, spider)\n         self.crawler.start()\n\n@@ -1,6 +1,6 @@\n import unittest\n \n-from scrapy.utils.conf import build_component_list\n+from scrapy.utils.conf import build_component_list, arglist_to_dict\n \n class UtilsConfTestCase(unittest.TestCase):\n \n@@ -13,5 +13,10 @@ class UtilsConfTestCase(unittest.TestCase):\n         custom = ['a', 'b', 'c']\n         self.assertEqual(build_component_list(base, custom), custom)\n \n+    def test_arglist_to_dict(self):\n+        self.assertEqual(arglist_to_dict(['arg1=val1', 'arg2=val2']),\n+            {'arg1': 'val1', 'arg2': 'val2'})\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -15,6 +15,11 @@ def build_component_list(base, custom):\n     return [k for k, v in sorted(compdict.items(), key=itemgetter(1)) \\\n         if v is not None]\n \n+def arglist_to_dict(arglist):\n+    \"\"\"Convert a list of arguments like ['arg1=val1', 'arg2=val2', ...] to a\n+    dict\"\"\"\n+    return dict(x.split('=', 1) for x in arglist)\n+\n def closest_scrapy_cfg(path='.', prevpath=None):\n     if path == prevpath:\n         return ''\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
