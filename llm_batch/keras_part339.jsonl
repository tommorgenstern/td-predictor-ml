{"custom_id": "keras#06db77cff463cd0e736a488d8406590e17d0c149", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 288 | Lines Deleted: 112 | Files Changed: 2 | Hunks: 97 | Methods Changed: 29 | Complexity Δ (Sum/Max): 21/11 | Churn Δ: 400 | Churn Cumulative: 4141 | Contributors (this commit): 5 | Commits (past 90d): 14 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -21,7 +21,6 @@ import numpy as np\n from keras import backend\n from keras.engine import base_layer\n from keras.engine import base_preprocessing_layer\n-from keras.engine.input_spec import InputSpec\n from keras.preprocessing import image as image_preprocessing\n from keras.utils import control_flow_util\n from tensorflow.python.ops import stateless_random_ops\n@@ -40,8 +39,8 @@ _RESIZE_METHODS = {\n     'mitchellcubic': ResizeMethod.MITCHELLCUBIC\n }\n \n-H_AXIS = 1\n-W_AXIS = 2\n+H_AXIS = -3\n+W_AXIS = -2\n \n \n def check_fill_mode_and_interpolation(fill_mode, interpolation):\n@@ -59,7 +58,7 @@ class Resizing(base_layer.Layer):\n   \"\"\"Image resizing layer.\n \n   Resize the batched image input to target height and width. The input should\n-  be a 4D tensor in `\"channels_last\"` format.\n+  be a 4D (batched) or 3D (unbatched) tensor in `\"channels_last\"` format.\n \n   Args:\n     height: Integer, the height of the output shape.\n@@ -86,7 +85,6 @@ class Resizing(base_layer.Layer):\n     self.interpolation = interpolation\n     self.crop_to_aspect_ratio = crop_to_aspect_ratio\n     self._interpolation_method = get_interpolation(interpolation)\n-    self.input_spec = InputSpec(ndim=4)\n     super(Resizing, self).__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('Resizing').set(True)\n \n@@ -105,8 +103,9 @@ class Resizing(base_layer.Layer):\n \n   def compute_output_shape(self, input_shape):\n     input_shape = tf.TensorShape(input_shape).as_list()\n-    return tf.TensorShape(\n-        [input_shape[0], self.target_height, self.target_width, input_shape[3]])\n+    input_shape[H_AXIS] = self.target_height\n+    input_shape[W_AXIS] = self.target_width\n+    return tf.TensorShape(input_shape)\n \n   def get_config(self):\n     config = {\n@@ -124,12 +123,12 @@ class CenterCrop(base_layer.Layer):\n   \"\"\"Crop the central portion of the images to target height and width.\n \n   Input shape:\n-    4D tensor with shape:\n-    `(samples, height, width, channels)`, in `\"channels_last\"` format.\n+    3D (unbatched) or 4D (batched) tensor with shape:\n+    `(..., height, width, channels)`, in `\"channels_last\"` format.\n \n   Output shape:\n-    4D tensor with shape:\n-    `(samples, target_height, target_width, channels)`.\n+    3D (unbatched) or 4D (batched) tensor with shape:\n+    `(..., target_height, target_width, channels)`.\n \n   If the input height/width is even and the target height/width is odd (or\n   inversely), the input image is left-padded by 1 pixel.\n@@ -142,12 +141,13 @@ class CenterCrop(base_layer.Layer):\n   def __init__(self, height, width, **kwargs):\n     self.target_height = height\n     self.target_width = width\n-    self.input_spec = InputSpec(ndim=4)\n     super(CenterCrop, self).__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('CenterCrop').set(True)\n \n   def call(self, inputs):\n+    inputs = tf.convert_to_tensor(inputs)\n     inputs_shape = tf.compat.v1.shape(inputs)\n+    unbatched = inputs.shape.rank == 3\n     img_hd = inputs_shape[H_AXIS]\n     img_wd = inputs_shape[W_AXIS]\n     img_hd_diff = img_hd - self.target_height\n@@ -166,16 +166,20 @@ class CenterCrop(base_layer.Layer):\n     with tf.control_dependencies(checks):\n       bbox_h_start = tf.cast(img_hd_diff / 2, tf.int32)\n       bbox_w_start = tf.cast(img_wd_diff / 2, tf.int32)\n+      if unbatched:\n+        bbox_begin = tf.stack([bbox_h_start, bbox_w_start, 0])\n+        bbox_size = tf.stack([self.target_height, self.target_width, -1])\n+      else:\n         bbox_begin = tf.stack([0, bbox_h_start, bbox_w_start, 0])\n-      bbox_size = tf.stack(\n-          [-1, self.target_height, self.target_width, -1])\n+        bbox_size = tf.stack([-1, self.target_height, self.target_width, -1])\n       outputs = tf.slice(inputs, bbox_begin, bbox_size)\n       return outputs\n \n   def compute_output_shape(self, input_shape):\n     input_shape = tf.TensorShape(input_shape).as_list()\n-    return tf.TensorShape(\n-        [input_shape[0], self.target_height, self.target_width, input_shape[3]])\n+    input_shape[H_AXIS] = self.target_height\n+    input_shape[W_AXIS] = self.target_width\n+    return tf.TensorShape(input_shape)\n \n   def get_config(self):\n     config = {\n@@ -198,12 +202,12 @@ class RandomCrop(base_layer.Layer):\n   set `training` to True when calling the layer.\n \n   Input shape:\n-    4D tensor with shape:\n-    `(samples, height, width, channels)`, in `\"channels_last\"` format.\n+    3D (unbatched) or 4D (batched) tensor with shape:\n+    `(..., height, width, channels)`, in `\"channels_last\"` format.\n \n   Output shape:\n-    4D tensor with shape:\n-    `(samples, target_height, target_width, channels)`.\n+    3D (unbatched) or 4D (batched) tensor with shape:\n+    `(..., target_height, target_width, channels)`.\n \n   Args:\n     height: Integer, the height of the output shape.\n@@ -216,7 +220,6 @@ class RandomCrop(base_layer.Layer):\n     self.width = width\n     self.seed = seed\n     self._rng = make_generator(self.seed)\n-    self.input_spec = InputSpec(ndim=4)\n     super(RandomCrop, self).__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomCrop').set(True)\n \n@@ -224,18 +227,23 @@ class RandomCrop(base_layer.Layer):\n     if training is None:\n       training = backend.learning_phase()\n \n+    inputs = tf.convert_to_tensor(inputs)\n+    unbatched = inputs.shape.rank == 3\n+\n     def random_cropped_inputs():\n       \"\"\"Cropped inputs with stateless random ops.\"\"\"\n-      input_shape = tf.compat.v1.shape(inputs)\n-      crop_size = tf.stack(\n-          [input_shape[0], self.height, self.width, input_shape[3]])\n+      shape = tf.compat.v1.shape(inputs)\n+      if unbatched:\n+        crop_size = tf.stack([self.height, self.width, shape[-1]])\n+      else:\n+        crop_size = tf.stack([shape[0], self.height, self.width, shape[-1]])\n       check = tf.Assert(\n-          tf.reduce_all(input_shape >= crop_size),\n+          tf.reduce_all(shape >= crop_size),\n           [self.height, self.width])\n       with tf.control_dependencies([check]):\n-        limit = input_shape - crop_size + 1\n+        limit = shape - crop_size + 1\n         offset = stateless_random_ops.stateless_random_uniform(\n-            tf.compat.v1.shape(input_shape),\n+            tf.compat.v1.shape(shape),\n             dtype=crop_size.dtype,\n             maxval=crop_size.dtype.max,\n             seed=self._rng.make_seeds()[:, 0]) % limit\n@@ -265,6 +273,10 @@ class RandomCrop(base_layer.Layer):\n       img_wd_diff = resized_width - self.width\n       bbox_h_start = tf.cast(img_hd_diff / 2, tf.int32)\n       bbox_w_start = tf.cast(img_wd_diff / 2, tf.int32)\n+      if unbatched:\n+        bbox_begin = tf.stack([bbox_h_start, bbox_w_start, 0])\n+        bbox_size = tf.stack([self.height, self.width, -1])\n+      else:\n         bbox_begin = tf.stack([0, bbox_h_start, bbox_w_start, 0])\n         bbox_size = tf.stack([-1, self.height, self.width, -1])\n       outputs = tf.slice(resized_inputs, bbox_begin, bbox_size)\n@@ -272,16 +284,19 @@ class RandomCrop(base_layer.Layer):\n \n     output = control_flow_util.smart_cond(training, random_cropped_inputs,\n                                           resize_and_center_cropped_inputs)\n-    original_shape = inputs.shape.as_list()\n-    batch_size, num_channels = original_shape[0], original_shape[3]\n-    output_shape = [batch_size] + [self.height, self.width] + [num_channels]\n+    input_shape = inputs.shape.as_list()\n+    if unbatched:\n+      output_shape = [self.height, self.width, input_shape[-1]]\n+    else:\n+      output_shape = [input_shape[0], self.height, self.width, input_shape[-1]]\n     output.set_shape(output_shape)\n     return output\n \n   def compute_output_shape(self, input_shape):\n     input_shape = tf.TensorShape(input_shape).as_list()\n-    return tf.TensorShape(\n-        [input_shape[0], self.height, self.width, input_shape[3]])\n+    input_shape[H_AXIS] = self.height\n+    input_shape[W_AXIS] = self.width\n+    return tf.TensorShape(input_shape)\n \n   def get_config(self):\n     config = {\n@@ -356,12 +371,12 @@ class RandomFlip(base_layer.Layer):\n   with `training=True` to flip the input.\n \n   Input shape:\n-    4D tensor with shape:\n-    `(samples, height, width, channels)`, in `\"channels_last\"` format.\n+    3D (unbatched) or 4D (batched) tensor with shape:\n+    `(..., height, width, channels)`, in `\"channels_last\"` format.\n \n   Output shape:\n-    4D tensor with shape:\n-    `(samples, height, width, channels)`, in `\"channels_last\"` format.\n+    3D (unbatched) or 4D (batched) tensor with shape:\n+    `(..., height, width, channels)`, in `\"channels_last\"` format.\n \n   Attributes:\n     mode: String indicating which flip mode to use. Can be `\"horizontal\"`,\n@@ -392,7 +407,6 @@ class RandomFlip(base_layer.Layer):\n                        'argument {arg}'.format(name=self.name, arg=mode))\n     self.seed = seed\n     self._rng = make_generator(self.seed)\n-    self.input_spec = InputSpec(ndim=4)\n \n   def call(self, inputs, training=True):\n     if training is None:\n@@ -467,12 +481,12 @@ class RandomTranslation(base_layer.Layer):\n       when `fill_mode=\"constant\"`.\n \n   Input shape:\n-    4D tensor with shape: `(samples, height, width, channels)`,\n-      in `\"channels_last\"` format.\n+    3D (unbatched) or 4D (batched) tensor with shape:\n+    `(..., height, width, channels)`,  in `\"channels_last\"` format.\n \n   Output shape:\n-    4D tensor with shape: `(samples, height, width, channels)`,\n-      in `\"channels_last\"` format.\n+    3D (unbatched) or 4D (batched) tensor with shape:\n+    `(..., height, width, channels)`,  in `\"channels_last\"` format.\n   \"\"\"\n \n   def __init__(self,\n@@ -518,7 +532,6 @@ class RandomTranslation(base_layer.Layer):\n     self.interpolation = interpolation\n     self.seed = seed\n     self._rng = make_generator(self.seed)\n-    self.input_spec = InputSpec(ndim=4)\n     super(RandomTranslation, self).__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomTranslation').set(\n         True)\n@@ -527,13 +540,20 @@ class RandomTranslation(base_layer.Layer):\n     if training is None:\n       training = backend.learning_phase()\n \n+    inputs = tf.convert_to_tensor(inputs)\n+    original_shape = inputs.shape\n+    unbatched = inputs.shape.rank == 3\n+    # The transform op only accepts rank 4 inputs, so if we have an unbatched\n+    # image, we need to temporarily expand dims to a batch.\n+    if unbatched:\n+      inputs = tf.expand_dims(inputs, 0)\n+\n     def random_translated_inputs():\n       \"\"\"Translated inputs with random ops.\"\"\"\n       inputs_shape = tf.compat.v1.shape(inputs)\n       batch_size = inputs_shape[0]\n-      h_axis, w_axis = H_AXIS, W_AXIS\n-      img_hd = tf.cast(inputs_shape[h_axis], tf.float32)\n-      img_wd = tf.cast(inputs_shape[w_axis], tf.float32)\n+      img_hd = tf.cast(inputs_shape[H_AXIS], tf.float32)\n+      img_wd = tf.cast(inputs_shape[W_AXIS], tf.float32)\n       height_translate = self._rng.uniform(\n           shape=[batch_size, 1],\n           minval=self.height_lower,\n@@ -558,7 +578,9 @@ class RandomTranslation(base_layer.Layer):\n \n     output = control_flow_util.smart_cond(training, random_translated_inputs,\n                                           lambda: inputs)\n-    output.set_shape(inputs.shape)\n+    if unbatched:\n+      output = tf.squeeze(output, 0)\n+    output.set_shape(original_shape)\n     return output\n \n   def compute_output_shape(self, input_shape):\n@@ -621,10 +643,8 @@ def transform(images,\n \n   Args:\n     images: A tensor of shape\n-      `(num_images, num_rows, num_columns, num_channels)`\n-      (NHWC), `(num_rows, num_columns, num_channels)` (HWC), or `(num_rows,\n-      num_columns)` (HW). The rank must be statically known (the shape is not\n-      `TensorShape(None)`.\n+      `(num_images, num_rows, num_columns, num_channels)` (NHWC). The rank must\n+      be statically known (the shape is not `TensorShape(None)`).\n     transforms: Projective transform matrix/matrices. A vector of length 8 or\n       tensor of size N x 8. If one row of transforms is [a0, a1, a2, b0, b1, b2,\n       c0, c1], then it maps the *output* point `(x, y)` to a transformed *input*\n@@ -751,12 +771,12 @@ class RandomRotation(base_layer.Layer):\n   rotations at inference time, set `training` to True when calling the layer.\n \n   Input shape:\n-    4D tensor with shape:\n-    `(samples, height, width, channels)`, in `\"channels_last\"` format.\n+    3D (unbatched) or 4D (batched) tensor with shape:\n+    `(..., height, width, channels)`, in `\"channels_last\"` format\n \n   Output shape:\n-    4D tensor with shape:\n-    `(samples, height, width, channels)`, in `\"channels_last\"` format.\n+    3D (unbatched) or 4D (batched) tensor with shape:\n+    `(..., height, width, channels)`, in `\"channels_last\"` format\n \n   Attributes:\n     factor: a float represented as fraction of 2 Pi, or a tuple of size 2\n@@ -807,7 +827,6 @@ class RandomRotation(base_layer.Layer):\n     self.interpolation = interpolation\n     self.seed = seed\n     self._rng = make_generator(self.seed)\n-    self.input_spec = InputSpec(ndim=4)\n     super(RandomRotation, self).__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomRotation').set(\n         True)\n@@ -816,6 +835,14 @@ class RandomRotation(base_layer.Layer):\n     if training is None:\n       training = backend.learning_phase()\n \n+    inputs = tf.convert_to_tensor(inputs)\n+    original_shape = inputs.shape\n+    unbatched = inputs.shape.rank == 3\n+    # The transform op only accepts rank 4 inputs, so if we have an unbatched\n+    # image, we need to temporarily expand dims to a batch.\n+    if unbatched:\n+      inputs = tf.expand_dims(inputs, 0)\n+\n     def random_rotated_inputs():\n       \"\"\"Rotated inputs with random ops.\"\"\"\n       inputs_shape = tf.compat.v1.shape(inputs)\n@@ -835,7 +862,9 @@ class RandomRotation(base_layer.Layer):\n \n     output = control_flow_util.smart_cond(training, random_rotated_inputs,\n                                           lambda: inputs)\n-    output.set_shape(inputs.shape)\n+    if unbatched:\n+      output = tf.squeeze(output, 0)\n+    output.set_shape(original_shape)\n     return output\n \n   def compute_output_shape(self, input_shape):\n@@ -898,12 +927,12 @@ class RandomZoom(base_layer.Layer):\n   TensorShape([32, 224, 224, 3])\n \n   Input shape:\n-    4D tensor with shape: `(samples, height, width, channels)`,\n-      in `\"channels_last\"` format.\n+    3D (unbatched) or 4D (batched) tensor with shape:\n+    `(..., height, width, channels)`, in `\"channels_last\"` format.\n \n   Output shape:\n-    4D tensor with shape: `(samples, height, width, channels)`,\n-      in `\"channels_last\"` format.\n+    3D (unbatched) or 4D (batched) tensor with shape:\n+    `(..., height, width, channels)`, in `\"channels_last\"` format.\n   \"\"\"\n \n   def __init__(self,\n@@ -946,7 +975,6 @@ class RandomZoom(base_layer.Layer):\n     self.interpolation = interpolation\n     self.seed = seed\n     self._rng = make_generator(self.seed)\n-    self.input_spec = InputSpec(ndim=4)\n     super(RandomZoom, self).__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomZoom').set(True)\n \n@@ -954,6 +982,14 @@ class RandomZoom(base_layer.Layer):\n     if training is None:\n       training = backend.learning_phase()\n \n+    inputs = tf.convert_to_tensor(inputs)\n+    original_shape = inputs.shape\n+    unbatched = inputs.shape.rank == 3\n+    # The transform op only accepts rank 4 inputs, so if we have an unbatched\n+    # image, we need to temporarily expand dims to a batch.\n+    if unbatched:\n+      inputs = tf.expand_dims(inputs, 0)\n+\n     def random_zoomed_inputs():\n       \"\"\"Zoomed inputs with random ops.\"\"\"\n       inputs_shape = tf.compat.v1.shape(inputs)\n@@ -983,7 +1019,9 @@ class RandomZoom(base_layer.Layer):\n \n     output = control_flow_util.smart_cond(training, random_zoomed_inputs,\n                                           lambda: inputs)\n-    output.set_shape(inputs.shape)\n+    if unbatched:\n+      output = tf.squeeze(output, 0)\n+    output.set_shape(original_shape)\n     return output\n \n   def compute_output_shape(self, input_shape):\n@@ -1056,12 +1094,12 @@ class RandomContrast(base_layer.Layer):\n   `(x - mean) * contrast_factor + mean`.\n \n   Input shape:\n-    4D tensor with shape:\n-    `(samples, height, width, channels)`, in `\"channels_last\"` format.\n+    3D (unbatched) or 4D (batched) tensor with shape:\n+    `(..., height, width, channels)`, in `\"channels_last\"` format.\n \n   Output shape:\n-    4D tensor with shape:\n-    `(samples, height, width, channels)`, in `\"channels_last\"` format.\n+    3D (unbatched) or 4D (batched) tensor with shape:\n+    `(..., height, width, channels)`, in `\"channels_last\"` format.\n \n   Attributes:\n     factor: a positive float represented as fraction of value, or a tuple of\n@@ -1083,7 +1121,6 @@ class RandomContrast(base_layer.Layer):\n                        ' got {}'.format(factor))\n     self.seed = seed\n     self._rng = make_generator(self.seed)\n-    self.input_spec = InputSpec(ndim=4)\n     super(RandomContrast, self).__init__(**kwargs)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomContrast').set(\n         True)\n@@ -1119,7 +1156,8 @@ class RandomHeight(base_layer.Layer):\n   \"\"\"Randomly vary the height of a batch of images during training.\n \n   Adjusts the height of a batch of images by a random factor. The input\n-  should be a 4D tensor in the `\"channels_last\"` image data format.\n+  should be a 3D (unbatched) or 4D (batched) tensor in the `\"channels_last\"`\n+  image data format.\n \n   By default, this layer is inactive during inference.\n \n@@ -1138,11 +1176,12 @@ class RandomHeight(base_layer.Layer):\n     seed: Integer. Used to create a random seed.\n \n   Input shape:\n-    4D tensor with shape: `(samples, height, width, channels)`\n-      (in `\"channels_last\"` format.).\n+    3D (unbatched) or 4D (batched) tensor with shape:\n+    `(..., height, width, channels)`, in `\"channels_last\"` format.\n \n   Output shape:\n-    4D tensor with shape: `(samples, random_height, width, channels)`.\n+    3D (unbatched) or 4D (batched) tensor with shape:\n+    `(..., random_height, width, channels)`.\n   \"\"\"\n \n   def __init__(self,\n@@ -1166,7 +1205,6 @@ class RandomHeight(base_layer.Layer):\n                        'got {}'.format(factor))\n     self.interpolation = interpolation\n     self._interpolation_method = get_interpolation(interpolation)\n-    self.input_spec = InputSpec(ndim=4)\n     self.seed = seed\n     self._rng = make_generator(self.seed)\n     super(RandomHeight, self).__init__(**kwargs)\n@@ -1189,8 +1227,8 @@ class RandomHeight(base_layer.Layer):\n       adjusted_size = tf.stack([adjusted_height, img_wd])\n       output = tf.image.resize(\n           images=inputs, size=adjusted_size, method=self._interpolation_method)\n-      original_shape = inputs.shape.as_list()\n-      output_shape = [original_shape[0]] + [None] + original_shape[2:4]\n+      output_shape = inputs.shape.as_list()\n+      output_shape[H_AXIS] = None\n       output.set_shape(output_shape)\n       return output\n \n@@ -1199,8 +1237,8 @@ class RandomHeight(base_layer.Layer):\n \n   def compute_output_shape(self, input_shape):\n     input_shape = tf.TensorShape(input_shape).as_list()\n-    return tf.TensorShape(\n-        [input_shape[0], None, input_shape[2], input_shape[3]])\n+    input_shape[H_AXIS] = None\n+    return tf.TensorShape(input_shape)\n \n   def get_config(self):\n     config = {\n@@ -1217,7 +1255,8 @@ class RandomWidth(base_layer.Layer):\n   \"\"\"Randomly vary the width of a batch of images during training.\n \n   Adjusts the width of a batch of images by a random factor. The input\n-  should be a 4D tensor in the `\"channels_last\"` image data format.\n+  should be a 3D (unbatched) or 4D (batched) tensor in the `\"channels_last\"`\n+  image data format.\n \n   By default, this layer is inactive during inference.\n \n@@ -1236,11 +1275,12 @@ class RandomWidth(base_layer.Layer):\n     seed: Integer. Used to create a random seed.\n \n   Input shape:\n-    4D tensor with shape: `(samples, height, width, channels)`\n-      (in `\"channels_last\"` format.).\n+    3D (unbatched) or 4D (batched) tensor with shape:\n+    `(..., height, width, channels)`, in `\"channels_last\"` format.\n \n   Output shape:\n-    4D tensor with shape: `(samples, height, random_width, channels)`.\n+    3D (unbatched) or 4D (batched) tensor with shape:\n+    `(..., random_height, width, channels)`.\n   \"\"\"\n \n   def __init__(self,\n@@ -1263,7 +1303,6 @@ class RandomWidth(base_layer.Layer):\n                        'got {}'.format(factor))\n     self.interpolation = interpolation\n     self._interpolation_method = get_interpolation(interpolation)\n-    self.input_spec = InputSpec(ndim=4)\n     self.seed = seed\n     self._rng = make_generator(self.seed)\n     super(RandomWidth, self).__init__(**kwargs)\n@@ -1286,8 +1325,8 @@ class RandomWidth(base_layer.Layer):\n       adjusted_size = tf.stack([img_hd, adjusted_width])\n       output = tf.image.resize(\n           images=inputs, size=adjusted_size, method=self._interpolation_method)\n-      original_shape = inputs.shape.as_list()\n-      output_shape = original_shape[0:2] + [None] + [original_shape[3]]\n+      output_shape = inputs.shape.as_list()\n+      output_shape[W_AXIS] = None\n       output.set_shape(output_shape)\n       return output\n \n@@ -1296,8 +1335,8 @@ class RandomWidth(base_layer.Layer):\n \n   def compute_output_shape(self, input_shape):\n     input_shape = tf.TensorShape(input_shape).as_list()\n-    return tf.TensorShape(\n-        [input_shape[0], input_shape[1], None, input_shape[3]])\n+    input_shape[W_AXIS] = None\n+    return tf.TensorShape(input_shape)\n \n   def get_config(self):\n     config = {\n\n@@ -153,6 +153,18 @@ class ResizingTest(keras_parameterized.TestCase):\n       expected_output = np.reshape(expected_output, (1, 4, 2, 1))\n       self.assertAllEqual(expected_output, output_image)\n \n+  def test_unbatched_image(self):\n+    with testing_utils.use_gpu():\n+      input_image = np.reshape(np.arange(0, 16), (4, 4, 1)).astype('float32')\n+      layer = image_preprocessing.Resizing(2, 2, interpolation='nearest')\n+      output_image = layer(input_image)\n+      expected_output = np.asarray([\n+          [5, 7],\n+          [13, 15],\n+      ]).astype('float32')\n+      expected_output = np.reshape(expected_output, (2, 2, 1))\n+      self.assertAllEqual(expected_output, output_image)\n+\n   @parameterized.named_parameters(\n       ('batch_crop_to_aspect_ratio', True, True),\n       ('batch_dont_crop_to_aspect_ratio', False, True),\n@@ -256,6 +268,18 @@ class CenterCropTest(keras_parameterized.TestCase):\n     layer_1 = image_preprocessing.CenterCrop.from_config(config)\n     self.assertEqual(layer_1.name, layer.name)\n \n+  def test_unbatched_image(self):\n+    with testing_utils.use_gpu():\n+      input_image = np.reshape(np.arange(0, 16), (4, 4, 1)).astype('float32')\n+      layer = image_preprocessing.CenterCrop(2, 2)\n+      output_image = layer(input_image)\n+      expected_output = np.asarray([\n+          [5, 6],\n+          [9, 10],\n+      ]).astype('float32')\n+      expected_output = np.reshape(expected_output, (2, 2, 1))\n+      self.assertAllEqual(expected_output, output_image)\n+\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class RandomCropTest(keras_parameterized.TestCase):\n@@ -352,6 +376,19 @@ class RandomCropTest(keras_parameterized.TestCase):\n     layer_1 = image_preprocessing.RandomCrop.from_config(config)\n     self.assertEqual(layer_1.name, layer.name)\n \n+  def test_unbatched_image(self):\n+    np.random.seed(1337)\n+    inp = np.random.random((16, 16, 3))\n+    mock_offset = [2, 2, 0]\n+    with tf.compat.v1.test.mock.patch.object(\n+        stateless_random_ops,\n+        'stateless_random_uniform',\n+        return_value=mock_offset):\n+      with testing_utils.use_gpu():\n+        layer = image_preprocessing.RandomCrop(8, 8)\n+        actual_output = layer(inp, training=1)\n+        self.assertAllClose(inp[2:10, 2:10, :], actual_output)\n+\n \n class RescalingTest(keras_parameterized.TestCase):\n \n@@ -385,6 +422,13 @@ class RescalingTest(keras_parameterized.TestCase):\n     layer_1 = image_preprocessing.Rescaling.from_config(config)\n     self.assertEqual(layer_1.name, layer.name)\n \n+  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  def test_unbatched_image(self):\n+    layer = image_preprocessing.Rescaling(scale=1. / 127.5, offset=-1)\n+    inputs = tf.random.uniform((4, 5, 3))\n+    outputs = layer(inputs)\n+    self.assertAllClose(outputs.numpy(), inputs.numpy() * (1. / 127.5) - 1)\n+\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class RandomFlipTest(keras_parameterized.TestCase):\n@@ -475,6 +519,21 @@ class RandomFlipTest(keras_parameterized.TestCase):\n     layer_1 = image_preprocessing.RandomFlip.from_config(config)\n     self.assertEqual(layer_1.name, layer.name)\n \n+  def test_random_flip_unbatched_image(self):\n+    with CustomObjectScope({'RandomFlip': image_preprocessing.RandomFlip}):\n+      input_image = np.random.random((4, 4, 1)).astype(np.float32)\n+      expected_output = np.flip(input_image, axis=0)\n+      # mock_random = np.reshape([0.], [1, 1, 1])\n+      with tf.compat.v1.test.mock.patch.object(\n+          stateless_random_ops,\n+          'stateless_random_uniform',\n+          return_value=0.,\n+      ):\n+        with self.cached_session():\n+          layer = image_preprocessing.RandomFlip('vertical')\n+          actual_output = layer(input_image, training=1)\n+          self.assertAllClose(expected_output, actual_output)\n+\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class RandomContrastTest(keras_parameterized.TestCase):\n@@ -558,6 +617,23 @@ class RandomContrastTest(keras_parameterized.TestCase):\n     layer_1 = image_preprocessing.RandomContrast.from_config(config)\n     self.assertEqual(layer_1.name, layer.name)\n \n+  def test_unbatched_image(self):\n+    np.random.seed(1337)\n+    mock_random = 0.2\n+    inp = np.random.random((4, 4, 1))\n+    inp_mean = np.mean(inp, axis=0, keepdims=True)\n+    inp_mean = np.mean(inp_mean, axis=1, keepdims=True)\n+    expected_output = (inp - inp_mean) * mock_random + inp_mean\n+    with tf.compat.v1.test.mock.patch.object(\n+        stateless_random_ops,\n+        'stateless_random_uniform',\n+        return_value=mock_random,\n+    ):\n+      with testing_utils.use_gpu():\n+        layer = image_preprocessing.RandomContrast((0.2, 0.5))\n+        actual_output = layer(inp, training=True)\n+        self.assertAllClose(expected_output, actual_output)\n+\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class RandomTranslationTest(keras_parameterized.TestCase):\n@@ -590,15 +666,13 @@ class RandomTranslationTest(keras_parameterized.TestCase):\n         layer = image_preprocessing.RandomTranslation(\n             height_factor=(-.2, -.2), width_factor=0.)\n         output_image = layer(input_image)\n-        # pyformat: disable\n         expected_output = np.asarray([\n             [5, 6, 7, 8, 9],\n             [10, 11, 12, 13, 14],\n             [15, 16, 17, 18, 19],\n             [20, 21, 22, 23, 24],\n-            [20, 21, 22, 23, 24]\n+            [20, 21, 22, 23, 24],\n         ]).astype(dtype)\n-        # pyformat: enable\n         expected_output = np.reshape(expected_output, (1, 5, 5, 1))\n         self.assertAllEqual(expected_output, output_image)\n \n@@ -610,15 +684,13 @@ class RandomTranslationTest(keras_parameterized.TestCase):\n         layer = image_preprocessing.RandomTranslation(\n             height_factor=(-.2, -.2), width_factor=0., fill_mode='constant')\n         output_image = layer(input_image)\n-        # pyformat: disable\n         expected_output = np.asarray([\n             [5, 6, 7, 8, 9],\n             [10, 11, 12, 13, 14],\n             [15, 16, 17, 18, 19],\n             [20, 21, 22, 23, 24],\n-            [0, 0, 0, 0, 0]\n+            [0, 0, 0, 0, 0],\n         ]).astype(dtype)\n-        # pyformat: enable\n         expected_output = np.reshape(expected_output, (1, 5, 5, 1))\n         self.assertAllEqual(expected_output, output_image)\n \n@@ -630,15 +702,13 @@ class RandomTranslationTest(keras_parameterized.TestCase):\n         layer = image_preprocessing.RandomTranslation(\n             height_factor=(.2, .2), width_factor=0.)\n         output_image = layer(input_image)\n-        # pyformat: disable\n         expected_output = np.asarray([\n             [0, 1, 2, 3, 4],\n             [0, 1, 2, 3, 4],\n             [5, 6, 7, 8, 9],\n             [10, 11, 12, 13, 14],\n-            [15, 16, 17, 18, 19]\n+            [15, 16, 17, 18, 19],\n         ]).astype(dtype)\n-        # pyformat: enable\n         expected_output = np.reshape(expected_output, (1, 5, 5, 1))\n         self.assertAllEqual(expected_output, output_image)\n \n@@ -673,15 +743,13 @@ class RandomTranslationTest(keras_parameterized.TestCase):\n         layer = image_preprocessing.RandomTranslation(\n             height_factor=(.2, .2), width_factor=0., fill_mode='constant')\n         output_image = layer(input_image)\n-        # pyformat: disable\n         expected_output = np.asarray([\n             [0, 0, 0, 0, 0],\n             [0, 1, 2, 3, 4],\n             [5, 6, 7, 8, 9],\n             [10, 11, 12, 13, 14],\n-            [15, 16, 17, 18, 19]\n+            [15, 16, 17, 18, 19],\n         ]).astype(dtype)\n-        # pyformat: enable\n         expected_output = np.reshape(expected_output, (1, 5, 5, 1))\n         self.assertAllEqual(expected_output, output_image)\n \n@@ -693,15 +761,13 @@ class RandomTranslationTest(keras_parameterized.TestCase):\n         layer = image_preprocessing.RandomTranslation(\n             height_factor=0., width_factor=(-.2, -.2))\n         output_image = layer(input_image)\n-        # pyformat: disable\n         expected_output = np.asarray([\n             [1, 2, 3, 4, 4],\n             [6, 7, 8, 9, 9],\n             [11, 12, 13, 14, 14],\n             [16, 17, 18, 19, 19],\n-            [21, 22, 23, 24, 24]\n+            [21, 22, 23, 24, 24],\n         ]).astype(dtype)\n-        # pyformat: enable\n         expected_output = np.reshape(expected_output, (1, 5, 5, 1))\n         self.assertAllEqual(expected_output, output_image)\n \n@@ -713,15 +779,13 @@ class RandomTranslationTest(keras_parameterized.TestCase):\n         layer = image_preprocessing.RandomTranslation(\n             height_factor=0., width_factor=(-.2, -.2), fill_mode='constant')\n         output_image = layer(input_image)\n-        # pyformat: disable\n         expected_output = np.asarray([\n             [1, 2, 3, 4, 0],\n             [6, 7, 8, 9, 0],\n             [11, 12, 13, 14, 0],\n             [16, 17, 18, 19, 0],\n-            [21, 22, 23, 24, 0]\n+            [21, 22, 23, 24, 0],\n         ]).astype(dtype)\n-        # pyformat: enable\n         expected_output = np.reshape(expected_output, (1, 5, 5, 1))\n         self.assertAllEqual(expected_output, output_image)\n \n@@ -742,6 +806,23 @@ class RandomTranslationTest(keras_parameterized.TestCase):\n     layer_1 = image_preprocessing.RandomTranslation.from_config(config)\n     self.assertEqual(layer_1.name, layer.name)\n \n+  def test_unbatched_image(self):\n+    with testing_utils.use_gpu():\n+      input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(np.int64)\n+      # Shifting by -.2 * 5 = 1 pixel.\n+      layer = image_preprocessing.RandomTranslation(\n+          height_factor=(-.2, -.2), width_factor=0.)\n+      output_image = layer(input_image)\n+      expected_output = np.asarray([\n+          [5, 6, 7, 8, 9],\n+          [10, 11, 12, 13, 14],\n+          [15, 16, 17, 18, 19],\n+          [20, 21, 22, 23, 24],\n+          [20, 21, 22, 23, 24],\n+      ]).astype(np.int64)\n+      expected_output = np.reshape(expected_output, (5, 5, 1))\n+      self.assertAllEqual(expected_output, output_image)\n+\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class RandomTransformTest(keras_parameterized.TestCase):\n@@ -1159,6 +1240,22 @@ class RandomRotationTest(keras_parameterized.TestCase):\n     layer_1 = image_preprocessing.RandomRotation.from_config(config)\n     self.assertEqual(layer_1.name, layer.name)\n \n+  def test_unbatched_image(self):\n+    with testing_utils.use_gpu():\n+      input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(np.float32)\n+      # 180 rotation.\n+      layer = image_preprocessing.RandomRotation(factor=(0.5, 0.5))\n+      output_image = layer(input_image)\n+      expected_output = np.asarray([\n+          [24, 23, 22, 21, 20],\n+          [19, 18, 17, 16, 15],\n+          [14, 13, 12, 11, 10],\n+          [9, 8, 7, 6, 5],\n+          [4, 3, 2, 1, 0],\n+      ]).astype(np.float32)\n+      expected_output = np.reshape(expected_output, (5, 5, 1))\n+      self.assertAllClose(expected_output, output_image)\n+\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class RandomZoomTest(keras_parameterized.TestCase):\n@@ -1196,15 +1293,13 @@ class RandomZoomTest(keras_parameterized.TestCase):\n         layer = image_preprocessing.RandomZoom((-.5, -.5), (-.5, -.5),\n                                                interpolation='nearest')\n         output_image = layer(np.expand_dims(input_image, axis=0))\n-        # pyformat: disable\n         expected_output = np.asarray([\n             [6, 7, 7, 8, 8],\n             [11, 12, 12, 13, 13],\n             [11, 12, 12, 13, 13],\n             [16, 17, 17, 18, 18],\n-            [16, 17, 17, 18, 18]\n+            [16, 17, 17, 18, 18],\n         ]).astype(dtype)\n-        # pyformat: enable\n         expected_output = np.reshape(expected_output, (1, 5, 5, 1))\n         self.assertAllEqual(expected_output, output_image)\n \n@@ -1216,15 +1311,13 @@ class RandomZoomTest(keras_parameterized.TestCase):\n                                                fill_mode='constant',\n                                                interpolation='nearest')\n         output_image = layer(np.expand_dims(input_image, axis=0))\n-        # pyformat: disable\n         expected_output = np.asarray([\n             [0, 0, 0, 0, 0],\n             [0, 5, 7, 9, 0],\n             [0, 10, 12, 14, 0],\n             [0, 20, 22, 24, 0],\n-            [0, 0, 0, 0, 0]\n+            [0, 0, 0, 0, 0],\n         ]).astype(dtype)\n-        # pyformat: enable\n         expected_output = np.reshape(expected_output, (1, 5, 5, 1))\n         self.assertAllEqual(expected_output, output_image)\n \n@@ -1236,15 +1329,13 @@ class RandomZoomTest(keras_parameterized.TestCase):\n                                                fill_mode='constant',\n                                                interpolation='nearest')\n         output_image = layer(np.expand_dims(input_image, axis=0))\n-        # pyformat: disable\n         expected_output = np.asarray([\n             [0, 0, 0, 0, 0],\n             [0, 6, 7, 9, 0],\n             [0, 11, 12, 14, 0],\n             [0, 21, 22, 24, 0],\n-            [0, 0, 0, 0, 0]\n+            [0, 0, 0, 0, 0],\n         ]).astype(dtype)\n-        # pyformat: enable\n         expected_output = np.reshape(expected_output, (1, 5, 5, 1))\n         self.assertAllEqual(expected_output, output_image)\n \n@@ -1264,6 +1355,22 @@ class RandomZoomTest(keras_parameterized.TestCase):\n     layer_1 = image_preprocessing.RandomZoom.from_config(config)\n     self.assertEqual(layer_1.name, layer.name)\n \n+  def test_unbatched_image(self):\n+    with testing_utils.use_gpu():\n+      input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(np.int64)\n+      layer = image_preprocessing.RandomZoom((-.5, -.5), (-.5, -.5),\n+                                             interpolation='nearest')\n+      output_image = layer(input_image)\n+      expected_output = np.asarray([\n+          [6, 7, 7, 8, 8],\n+          [11, 12, 12, 13, 13],\n+          [11, 12, 12, 13, 13],\n+          [16, 17, 17, 18, 18],\n+          [16, 17, 17, 18, 18],\n+      ]).astype(np.int64)\n+      expected_output = np.reshape(expected_output, (5, 5, 1))\n+      self.assertAllEqual(expected_output, output_image)\n+\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class RandomHeightTest(keras_parameterized.TestCase):\n@@ -1359,6 +1466,21 @@ class RandomHeightTest(keras_parameterized.TestCase):\n     layer_1 = image_preprocessing.RandomHeight.from_config(config)\n     self.assertEqual(layer_1.name, layer.name)\n \n+  def test_unbatched_image(self):\n+    # need (maxval - minval) * rnd + minval = 0.6\n+    mock_factor = 0\n+    with tf.compat.v1.test.mock.patch.object(\n+        gen_stateful_random_ops, 'stateful_uniform', return_value=mock_factor):\n+      with tf.compat.v1.test.mock.patch.object(\n+          gen_stateless_random_ops_v2,\n+          'stateless_random_uniform_v2',\n+          return_value=mock_factor):\n+        with testing_utils.use_gpu():\n+          img = np.random.random((5, 8, 3))\n+          layer = image_preprocessing.RandomHeight(.4)\n+          img_out = layer(img, training=True)\n+          self.assertEqual(img_out.shape[0], 3)\n+\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class RandomWidthTest(keras_parameterized.TestCase):\n@@ -1453,6 +1575,21 @@ class RandomWidthTest(keras_parameterized.TestCase):\n     layer_1 = image_preprocessing.RandomWidth.from_config(config)\n     self.assertEqual(layer_1.name, layer.name)\n \n+  def test_unbatched_image(self):\n+    # need (maxval - minval) * rnd + minval = 0.6\n+    mock_factor = 0\n+    with tf.compat.v1.test.mock.patch.object(\n+        gen_stateful_random_ops, 'stateful_uniform', return_value=mock_factor):\n+      with tf.compat.v1.test.mock.patch.object(\n+          gen_stateless_random_ops_v2,\n+          'stateless_random_uniform_v2',\n+          return_value=mock_factor):\n+        with testing_utils.use_gpu():\n+          img = np.random.random((8, 5, 3))\n+          layer = image_preprocessing.RandomWidth(.4)\n+          img_out = layer(img, training=True)\n+          self.assertEqual(img_out.shape[1], 3)\n+\n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class LearningPhaseTest(keras_parameterized.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#069e95a5c8075555b3bd2895bdbf34ed68bb3bba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 4 | Files Changed: 4 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 3915 | Contributors (this commit): 5 | Commits (past 90d): 37 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -97,19 +97,20 @@ class CategoryEncoding(base_layer.Layer):\n           `(..., num_tokens)`.\n         - `\"count\"`: Like `\"multi_hot\"`, but the int array contains a count of\n           the number of times the token at that index appeared in the sample.\n+      For all output modes, currently only output up to rank 2 is supported.\n     sparse: Boolean. If true, returns a `SparseTensor` instead of a dense\n       `Tensor`. Defaults to `False`.\n \n   Call arguments:\n-    inputs: A 2D tensor `(samples, timesteps)`.\n-    count_weights: A 2D tensor in the same shape as `inputs` indicating the\n+    inputs: A 1D or 2D tensor of integer inputs.\n+    count_weights: A tensor in the same shape as `inputs` indicating the\n       weight for each sample value when summing up in `count` mode. Not used in\n-      `\"multi_hot\"` mode.\n+      `\"multi_hot\"` or `\"one_hot\"` modes.\n   \"\"\"\n \n   def __init__(self,\n                num_tokens=None,\n-               output_mode=MULTI_HOT,\n+               output_mode=\"multi_hot\",\n                sparse=False,\n                **kwargs):\n     # max_tokens is an old name for the num_tokens arg we continue to support\n\n@@ -95,6 +95,8 @@ class IntegerLookup(index_lookup.IndexLookup):\n           number of times the token at that index appeared in the sample.\n         - `\"tf_idf\"`: As `\"multi_hot\"`, but the TF-IDF algorithm is applied to\n           find the value in each token slot.\n+      For `\"int\"` output, any shape of input and output is supported. For all\n+      other output modes, currently only output up to rank 2 is supported.\n     pad_to_max_tokens: Only applicable when `output_mode` is `\"multi_hot\"`,\n       `\"count\"`, or `\"tf_idf\"`. If True, the output will have its feature axis\n       padded to `max_tokens` even if the number of unique tokens in the\n\n@@ -94,6 +94,8 @@ class StringLookup(index_lookup.IndexLookup):\n           number of times the token at that index appeared in the sample.\n         - `\"tf_idf\"`: As `\"multi_hot\"`, but the TF-IDF algorithm is applied to\n           find the value in each token slot.\n+      For `\"int\"` output, any shape of input and output is supported. For all\n+      other output modes, currently only output up to rank 2 is supported.\n     pad_to_max_tokens: Only applicable when `output_mode` is `\"multi_hot\"`,\n       `\"count\"`, or `\"tf_idf\"`. If True, the output will have its feature axis\n       padded to `max_tokens` even if the number of unique tokens in the\n\n@@ -136,6 +136,9 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n           batch item.\n         - `\"tf_idf\"`: Like `\"multi_hot\"`, but the TF-IDF algorithm is applied to\n           find the value in each token slot.\n+      For `\"int\"` output, any shape of input and output is supported. For all\n+      other output modes, currently only rank 1 inputs (and rank 2 outputs after\n+      splitting) are supported.\n     output_sequence_length: Only valid in INT mode. If set, the output will have\n       its time dimension padded or truncated to exactly `output_sequence_length`\n       values, resulting in a tensor of shape\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#db0904b1fa12ef5c38a90bee0dd8a32698528fca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 7094 | Contributors (this commit): 92 | Commits (past 90d): 16 | Contributors (cumulative): 92 | DMM Complexity: None\n\nDIFF:\n@@ -695,9 +695,8 @@ class Callback:\n \n     Args:\n         batch: Integer, index of batch within the current epoch.\n-        logs: Dict, contains the return value of `model.train_step`. Typically,\n-          the values of the `Model`'s metrics are returned.  Example:\n-          `{'loss': 0.2, 'accuracy': 0.7}`.\n+        logs: Dict. Currently no data is passed to this argument for this method\n+          but that may change in the future.\n     \"\"\"\n     # For backwards compatibility.\n     self.on_batch_begin(batch, logs=logs)\n@@ -736,9 +735,8 @@ class Callback:\n \n     Args:\n         batch: Integer, index of batch within the current epoch.\n-        logs: Dict, contains the return value of `model.test_step`. Typically,\n-          the values of the `Model`'s metrics are returned.  Example:\n-          `{'loss': 0.2, 'accuracy': 0.7}`.\n+        logs: Dict. Currently no data is passed to this argument for this method\n+          but that may change in the future.\n     \"\"\"\n \n   @doc_controls.for_subclass_implementers\n@@ -773,9 +771,8 @@ class Callback:\n \n     Args:\n         batch: Integer, index of batch within the current epoch.\n-        logs: Dict, contains the return value of `model.predict_step`,\n-          it typically returns a dict with a key 'outputs' containing\n-          the model's outputs.\n+        logs: Dict. Currently no data is passed to this argument for this method\n+          but that may change in the future.\n     \"\"\"\n \n   @doc_controls.for_subclass_implementers\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6372035857e7055051e066fb3a2e723590e556d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 37 | Churn Cumulative: 5118 | Contributors (this commit): 4 | Commits (past 90d): 21 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -626,6 +626,9 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n   def call(self, inputs):\n     if isinstance(inputs, (list, tuple, np.ndarray)):\n       inputs = tf.convert_to_tensor(inputs)\n+    # In all cases, we should uprank scalar input to a single sample.\n+    if inputs.shape.rank == 0:\n+      inputs = self._expand_dims(inputs, -1)\n \n     if not self.max_tokens and self._vocab_size is None:\n       raise ValueError(\"You must set the layer's vocabulary before calling it. \"\n@@ -665,20 +668,11 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n         return self._encode_output(lookup_result)\n \n   def _encode_output(self, lookup_result):\n-    def expand_dims(inputs, axis):\n-      if tf_utils.is_sparse(inputs):\n-        return tf.sparse.expand_dims(inputs, axis)\n-      else:\n-        return tf.compat.v1.expand_dims(inputs, axis)\n-\n     original_shape = lookup_result.shape\n-    # In all cases, we should uprank scalar input to a single sample.\n-    if lookup_result.shape.rank == 0:\n-      lookup_result = expand_dims(lookup_result, -1)\n     # One hot will unprank only if the final output dimension is not already 1.\n     if self.output_mode == ONE_HOT:\n       if lookup_result.shape[-1] != 1:\n-        lookup_result = expand_dims(lookup_result, -1)\n+        lookup_result = self._expand_dims(lookup_result, -1)\n \n     # TODO(b/190445202): remove output rank restriction.\n     if lookup_result.shape.rank > 2:\n@@ -708,6 +702,12 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n   def _convert_to_ndarray(self, x):\n     return np.array(x) if isinstance(x, (list, tuple)) else x\n \n+  def _expand_dims(self, inputs, axis):\n+    if tf_utils.is_sparse(inputs):\n+      return tf.sparse.expand_dims(inputs, axis)\n+    else:\n+      return tf.compat.v1.expand_dims(inputs, axis)\n+\n   def _oov_start_index(self):\n     return 1 if self.mask_token is not None and self.output_mode == INT else 0\n \n\n@@ -888,6 +888,23 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n     output_dataset = model.predict(input_array)\n     self.assertAllEqual(expected_output, output_dataset)\n \n+  def test_one_hot_output_rank_zero_no_oov(self):\n+    \"\"\"Check binary output when pad_to_max_tokens=False.\"\"\"\n+    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n+    input_data = tf.constant(\"earth\")\n+    expected_output = [1, 0, 0, 0]\n+\n+    layer = index_lookup.IndexLookup(\n+        max_tokens=None,\n+        num_oov_indices=0,\n+        mask_token=\"\",\n+        oov_token=\"[OOV]\",\n+        output_mode=index_lookup.ONE_HOT,\n+        dtype=tf.string)\n+    layer.set_vocabulary(vocab_data)\n+    output_data = layer(input_data)\n+    self.assertAllEqual(expected_output, output_data)\n+\n   def test_one_hot_output_shape(self):\n     inputs = keras.Input(batch_size=16, shape=(1,), dtype=tf.string)\n     layer = index_lookup.IndexLookup(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2d551d31915906120f3f6a3ed3c4de94ba4bb288", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 27 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 37 | Churn Cumulative: 5155 | Contributors (this commit): 4 | Commits (past 90d): 23 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -626,9 +626,6 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n   def call(self, inputs):\n     if isinstance(inputs, (list, tuple, np.ndarray)):\n       inputs = tf.convert_to_tensor(inputs)\n-    # In all cases, we should uprank scalar input to a single sample.\n-    if inputs.shape.rank == 0:\n-      inputs = self._expand_dims(inputs, -1)\n \n     if not self.max_tokens and self._vocab_size is None:\n       raise ValueError(\"You must set the layer's vocabulary before calling it. \"\n@@ -668,11 +665,20 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n         return self._encode_output(lookup_result)\n \n   def _encode_output(self, lookup_result):\n+    def expand_dims(inputs, axis):\n+      if tf_utils.is_sparse(inputs):\n+        return tf.sparse.expand_dims(inputs, axis)\n+      else:\n+        return tf.compat.v1.expand_dims(inputs, axis)\n+\n     original_shape = lookup_result.shape\n+    # In all cases, we should uprank scalar input to a single sample.\n+    if lookup_result.shape.rank == 0:\n+      lookup_result = expand_dims(lookup_result, -1)\n     # One hot will unprank only if the final output dimension is not already 1.\n     if self.output_mode == ONE_HOT:\n       if lookup_result.shape[-1] != 1:\n-        lookup_result = self._expand_dims(lookup_result, -1)\n+        lookup_result = expand_dims(lookup_result, -1)\n \n     # TODO(b/190445202): remove output rank restriction.\n     if lookup_result.shape.rank > 2:\n@@ -702,12 +708,6 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n   def _convert_to_ndarray(self, x):\n     return np.array(x) if isinstance(x, (list, tuple)) else x\n \n-  def _expand_dims(self, inputs, axis):\n-    if tf_utils.is_sparse(inputs):\n-      return tf.sparse.expand_dims(inputs, axis)\n-    else:\n-      return tf.compat.v1.expand_dims(inputs, axis)\n-\n   def _oov_start_index(self):\n     return 1 if self.mask_token is not None and self.output_mode == INT else 0\n \n\n@@ -888,23 +888,6 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n     output_dataset = model.predict(input_array)\n     self.assertAllEqual(expected_output, output_dataset)\n \n-  def test_one_hot_output_rank_zero_no_oov(self):\n-    \"\"\"Check binary output when pad_to_max_tokens=False.\"\"\"\n-    vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-    input_data = tf.constant(\"earth\")\n-    expected_output = [1, 0, 0, 0]\n-\n-    layer = index_lookup.IndexLookup(\n-        max_tokens=None,\n-        num_oov_indices=0,\n-        mask_token=\"\",\n-        oov_token=\"[OOV]\",\n-        output_mode=index_lookup.ONE_HOT,\n-        dtype=tf.string)\n-    layer.set_vocabulary(vocab_data)\n-    output_data = layer(input_data)\n-    self.assertAllEqual(expected_output, output_data)\n-\n   def test_one_hot_output_shape(self):\n     inputs = keras.Input(batch_size=16, shape=(1,), dtype=tf.string)\n     layer = index_lookup.IndexLookup(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
