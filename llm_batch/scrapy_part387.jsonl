{"custom_id": "scrapy#53916630723a86277836053e1c54fe50655f0bd5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 987 | Lines Deleted: 1280 | Files Changed: 143 | Hunks: 826 | Methods Changed: 529 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2267 | Churn Cumulative: 99145 | Contributors (this commit): 330 | Commits (past 90d): 156 | Contributors (cumulative): 2381 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n import logging\n-from typing import TYPE_CHECKING, Any, List\n+from typing import TYPE_CHECKING, Any\n \n from scrapy.exceptions import NotConfigured\n from scrapy.utils.conf import build_component_list\n@@ -20,7 +20,7 @@ class AddonManager:\n \n     def __init__(self, crawler: Crawler) -> None:\n         self.crawler: Crawler = crawler\n-        self.addons: List[Any] = []\n+        self.addons: list[Any] = []\n \n     def load_settings(self, settings: Settings) -> None:\n         \"\"\"Load add-ons and configurations from a settings object and apply them.\n\n@@ -6,7 +6,7 @@ import inspect\n import os\n import sys\n from importlib.metadata import entry_points\n-from typing import TYPE_CHECKING, Callable, Dict, Iterable, List, Optional, Tuple, Type\n+from typing import TYPE_CHECKING, Optional\n \n import scrapy\n from scrapy.commands import BaseRunSpiderCommand, ScrapyCommand, ScrapyHelpFormatter\n@@ -17,6 +17,8 @@ from scrapy.utils.project import get_project_settings, inside_project\n from scrapy.utils.python import garbage_collect\n \n if TYPE_CHECKING:\n+    from collections.abc import Callable, Iterable\n+\n     # typing.ParamSpec requires Python 3.10\n     from typing_extensions import ParamSpec\n \n@@ -28,7 +30,7 @@ if TYPE_CHECKING:\n class ScrapyArgumentParser(argparse.ArgumentParser):\n     def _parse_optional(\n         self, arg_string: str\n-    ) -> Optional[Tuple[Optional[argparse.Action], str, Optional[str]]]:\n+    ) -> Optional[tuple[Optional[argparse.Action], str, Optional[str]]]:\n         # if starts with -: it means that is a parameter not a argument\n         if arg_string[:2] == \"-:\":\n             return None\n@@ -36,7 +38,7 @@ class ScrapyArgumentParser(argparse.ArgumentParser):\n         return super()._parse_optional(arg_string)\n \n \n-def _iter_command_classes(module_name: str) -> Iterable[Type[ScrapyCommand]]:\n+def _iter_command_classes(module_name: str) -> Iterable[type[ScrapyCommand]]:\n     # TODO: add `name` attribute to commands and merge this function with\n     # scrapy.utils.spider.iter_spider_classes\n     for module in walk_modules(module_name):\n@@ -50,8 +52,8 @@ def _iter_command_classes(module_name: str) -> Iterable[Type[ScrapyCommand]]:\n                 yield obj\n \n \n-def _get_commands_from_module(module: str, inproject: bool) -> Dict[str, ScrapyCommand]:\n-    d: Dict[str, ScrapyCommand] = {}\n+def _get_commands_from_module(module: str, inproject: bool) -> dict[str, ScrapyCommand]:\n+    d: dict[str, ScrapyCommand] = {}\n     for cmd in _iter_command_classes(module):\n         if inproject or not cmd.requires_project:\n             cmdname = cmd.__module__.split(\".\")[-1]\n@@ -61,8 +63,8 @@ def _get_commands_from_module(module: str, inproject: bool) -> Dict[str, ScrapyC\n \n def _get_commands_from_entry_points(\n     inproject: bool, group: str = \"scrapy.commands\"\n-) -> Dict[str, ScrapyCommand]:\n-    cmds: Dict[str, ScrapyCommand] = {}\n+) -> dict[str, ScrapyCommand]:\n+    cmds: dict[str, ScrapyCommand] = {}\n     if sys.version_info >= (3, 10):\n         eps = entry_points(group=group)\n     else:\n@@ -78,7 +80,7 @@ def _get_commands_from_entry_points(\n \n def _get_commands_dict(\n     settings: BaseSettings, inproject: bool\n-) -> Dict[str, ScrapyCommand]:\n+) -> dict[str, ScrapyCommand]:\n     cmds = _get_commands_from_module(\"scrapy.commands\", inproject)\n     cmds.update(_get_commands_from_entry_points(inproject))\n     cmds_module = settings[\"COMMANDS_MODULE\"]\n@@ -87,7 +89,7 @@ def _get_commands_dict(\n     return cmds\n \n \n-def _pop_command_name(argv: List[str]) -> Optional[str]:\n+def _pop_command_name(argv: list[str]) -> Optional[str]:\n     i = 0\n     for arg in argv[1:]:\n         if not arg.startswith(\"-\"):\n@@ -146,7 +148,7 @@ def _run_print_help(\n \n \n def execute(\n-    argv: Optional[List[str]] = None, settings: Optional[Settings] = None\n+    argv: Optional[list[str]] = None, settings: Optional[Settings] = None\n ) -> None:\n     if argv is None:\n         argv = sys.argv\n@@ -189,7 +191,7 @@ def execute(\n     sys.exit(cmd.exitcode)\n \n \n-def _run_command(cmd: ScrapyCommand, args: List[str], opts: argparse.Namespace) -> None:\n+def _run_command(cmd: ScrapyCommand, args: list[str], opts: argparse.Namespace) -> None:\n     if opts.profile:\n         _run_command_profiled(cmd, args, opts)\n     else:\n@@ -197,7 +199,7 @@ def _run_command(cmd: ScrapyCommand, args: List[str], opts: argparse.Namespace)\n \n \n def _run_command_profiled(\n-    cmd: ScrapyCommand, args: List[str], opts: argparse.Namespace\n+    cmd: ScrapyCommand, args: list[str], opts: argparse.Namespace\n ) -> None:\n     if opts.profile:\n         sys.stderr.write(f\"scrapy: writing cProfile stats to {opts.profile!r}\\n\")\n\n@@ -8,7 +8,7 @@ import argparse\n import builtins\n import os\n from pathlib import Path\n-from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Optional\n+from typing import TYPE_CHECKING, Any, Optional\n \n from twisted.python import failure\n \n@@ -16,6 +16,8 @@ from scrapy.exceptions import UsageError\n from scrapy.utils.conf import arglist_to_dict, feed_process_params_from_cli\n \n if TYPE_CHECKING:\n+    from collections.abc import Iterable\n+\n     from scrapy.crawler import Crawler, CrawlerProcess\n \n \n@@ -24,7 +26,7 @@ class ScrapyCommand:\n     crawler_process: Optional[CrawlerProcess] = None\n \n     # default settings to be used for this command instead of global defaults\n-    default_settings: Dict[str, Any] = {}\n+    default_settings: dict[str, Any] = {}\n \n     exitcode: int = 0\n \n@@ -97,7 +99,7 @@ class ScrapyCommand:\n         )\n         group.add_argument(\"--pdb\", action=\"store_true\", help=\"enable pdb on failure\")\n \n-    def process_options(self, args: List[str], opts: argparse.Namespace) -> None:\n+    def process_options(self, args: list[str], opts: argparse.Namespace) -> None:\n         try:\n             self.settings.setdict(arglist_to_dict(opts.set), priority=\"cmdline\")\n         except ValueError:\n@@ -122,7 +124,7 @@ class ScrapyCommand:\n         if opts.pdb:\n             failure.startDebugMode()\n \n-    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n+    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n         \"\"\"\n         Entry point for running commands\n         \"\"\"\n@@ -167,7 +169,7 @@ class BaseRunSpiderCommand(ScrapyCommand):\n             help=\"format to use for dumping items\",\n         )\n \n-    def process_options(self, args: List[str], opts: argparse.Namespace) -> None:\n+    def process_options(self, args: list[str], opts: argparse.Namespace) -> None:\n         super().process_options(args, opts)\n         try:\n             opts.spargs = arglist_to_dict(opts.spargs)\n@@ -207,7 +209,7 @@ class ScrapyHelpFormatter(argparse.HelpFormatter):\n         parts = self.format_part_strings(builtins.list(part_strings))\n         return super()._join_parts(parts)\n \n-    def format_part_strings(self, part_strings: List[str]) -> List[str]:\n+    def format_part_strings(self, part_strings: list[str]) -> list[str]:\n         \"\"\"\n         Underline and title case command line help message headers.\n         \"\"\"\n\n@@ -4,7 +4,7 @@ import argparse\n import subprocess  # nosec\n import sys\n import time\n-from typing import TYPE_CHECKING, Any, Iterable, List\n+from typing import TYPE_CHECKING, Any\n from urllib.parse import urlencode\n \n import scrapy\n@@ -13,6 +13,8 @@ from scrapy.http import Response, TextResponse\n from scrapy.linkextractors import LinkExtractor\n \n if TYPE_CHECKING:\n+    from collections.abc import Iterable\n+\n     from scrapy import Request\n \n \n@@ -26,7 +28,7 @@ class Command(ScrapyCommand):\n     def short_desc(self) -> str:\n         return \"Run quick benchmark test\"\n \n-    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n+    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n         with _BenchServer():\n             assert self.crawler_process\n             self.crawler_process.crawl(_BenchSpider, total=100000)\n\n@@ -1,7 +1,6 @@\n import argparse\n import time\n from collections import defaultdict\n-from typing import List\n from unittest import TextTestResult as _TextTestResult\n from unittest import TextTestRunner\n \n@@ -69,7 +68,7 @@ class Command(ScrapyCommand):\n             help=\"print contract tests for all spiders\",\n         )\n \n-    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n+    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n         # load contracts\n         contracts = build_component_list(self.settings.getwithbase(\"SPIDER_CONTRACTS\"))\n         conman = ContractsManager(load_object(c) for c in contracts)\n\n@@ -1,6 +1,6 @@\n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, List, cast\n+from typing import TYPE_CHECKING, cast\n \n from twisted.python.failure import Failure\n \n@@ -20,7 +20,7 @@ class Command(BaseRunSpiderCommand):\n     def short_desc(self) -> str:\n         return \"Run a spider\"\n \n-    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n+    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n         if len(args) < 1:\n             raise UsageError()\n         elif len(args) > 1:\n\n@@ -1,7 +1,6 @@\n import argparse\n import os\n import sys\n-from typing import List\n \n from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n@@ -27,7 +26,7 @@ class Command(ScrapyCommand):\n         sys.stderr.write(msg + os.linesep)\n         self.exitcode = 1\n \n-    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n+    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n         if len(args) != 1:\n             raise UsageError()\n \n\n@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n import sys\n-from typing import TYPE_CHECKING, Dict, List, Type\n+from typing import TYPE_CHECKING\n \n from w3lib.url import is_url\n \n@@ -48,7 +48,7 @@ class Command(ScrapyCommand):\n             help=\"do not handle HTTP 3xx status codes and print response as-is\",\n         )\n \n-    def _print_headers(self, headers: Dict[bytes, List[bytes]], prefix: bytes) -> None:\n+    def _print_headers(self, headers: dict[bytes, list[bytes]], prefix: bytes) -> None:\n         for key, values in headers.items():\n             for value in values:\n                 self._print_bytes(prefix + b\" \" + key + b\": \" + value)\n@@ -65,7 +65,7 @@ class Command(ScrapyCommand):\n     def _print_bytes(self, bytes_: bytes) -> None:\n         sys.stdout.buffer.write(bytes_ + b\"\\n\")\n \n-    def run(self, args: List[str], opts: Namespace) -> None:\n+    def run(self, args: list[str], opts: Namespace) -> None:\n         if len(args) != 1 or not is_url(args[0]):\n             raise UsageError()\n         request = Request(\n@@ -81,7 +81,7 @@ class Command(ScrapyCommand):\n         else:\n             request.meta[\"handle_httpstatus_all\"] = True\n \n-        spidercls: Type[Spider] = DefaultSpider\n+        spidercls: type[Spider] = DefaultSpider\n         assert self.crawler_process\n         spider_loader = self.crawler_process.spider_loader\n         if opts.spider:\n\n@@ -4,7 +4,7 @@ import shutil\n import string\n from importlib import import_module\n from pathlib import Path\n-from typing import List, Optional, Union, cast\n+from typing import Optional, Union, cast\n from urllib.parse import urlparse\n \n import scrapy\n@@ -87,7 +87,7 @@ class Command(ScrapyCommand):\n             help=\"If the spider already exists, overwrite it with the template\",\n         )\n \n-    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n+    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n         if opts.list:\n             self._list_templates()\n             return\n\n@@ -1,6 +1,6 @@\n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, List\n+from typing import TYPE_CHECKING\n \n from scrapy.commands import ScrapyCommand\n \n@@ -15,7 +15,7 @@ class Command(ScrapyCommand):\n     def short_desc(self) -> str:\n         return \"List available spiders\"\n \n-    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n+    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n         assert self.crawler_process\n         for s in sorted(self.crawler_process.spider_loader.list()):\n             print(s)\n\n@@ -5,20 +5,7 @@ import functools\n import inspect\n import json\n import logging\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    AsyncGenerator,\n-    Coroutine,\n-    Dict,\n-    Iterable,\n-    List,\n-    Optional,\n-    Tuple,\n-    TypeVar,\n-    Union,\n-    overload,\n-)\n+from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union, overload\n \n from itemadapter import ItemAdapter, is_item\n from twisted.internet.defer import Deferred, maybeDeferred\n@@ -35,6 +22,8 @@ from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.spider import spidercls_for_request\n \n if TYPE_CHECKING:\n+    from collections.abc import AsyncGenerator, Coroutine, Iterable\n+\n     from twisted.python.failure import Failure\n \n     from scrapy.http.request import CallbackT\n@@ -50,8 +39,8 @@ class Command(BaseRunSpiderCommand):\n     requires_project = True\n \n     spider = None\n-    items: Dict[int, List[Any]] = {}\n-    requests: Dict[int, List[Request]] = {}\n+    items: dict[int, list[Any]] = {}\n+    requests: dict[int, list[Request]] = {}\n \n     first_response = None\n \n@@ -166,11 +155,11 @@ class Command(BaseRunSpiderCommand):\n             return d\n         return arg_to_iter(deferred_from_coro(result))\n \n-    def add_items(self, lvl: int, new_items: List[Any]) -> None:\n+    def add_items(self, lvl: int, new_items: list[Any]) -> None:\n         old_items = self.items.get(lvl, [])\n         self.items[lvl] = old_items + new_items\n \n-    def add_requests(self, lvl: int, new_reqs: List[Request]) -> None:\n+    def add_requests(self, lvl: int, new_reqs: list[Request]) -> None:\n         old_reqs = self.requests.get(lvl, [])\n         self.requests[lvl] = old_reqs + new_reqs\n \n@@ -219,7 +208,7 @@ class Command(BaseRunSpiderCommand):\n         depth: int,\n         spider: Spider,\n         callback: CallbackT,\n-    ) -> Tuple[List[Any], List[Request], argparse.Namespace, int, Spider, CallbackT]:\n+    ) -> tuple[list[Any], list[Request], argparse.Namespace, int, Spider, CallbackT]:\n         items, requests = [], []\n         for x in spider_output:\n             if is_item(x):\n@@ -232,7 +221,7 @@ class Command(BaseRunSpiderCommand):\n         self,\n         response: Response,\n         callback: CallbackT,\n-        cb_kwargs: Optional[Dict[str, Any]] = None,\n+        cb_kwargs: Optional[dict[str, Any]] = None,\n     ) -> Deferred[Any]:\n         cb_kwargs = cb_kwargs or {}\n         d = maybeDeferred(self.iterate_spider_output, callback(response, **cb_kwargs))\n@@ -285,10 +274,10 @@ class Command(BaseRunSpiderCommand):\n \n     def scraped_data(\n         self,\n-        args: Tuple[\n-            List[Any], List[Request], argparse.Namespace, int, Spider, CallbackT\n+        args: tuple[\n+            list[Any], list[Request], argparse.Namespace, int, Spider, CallbackT\n         ],\n-    ) -> List[Any]:\n+    ) -> list[Any]:\n         items, requests, opts, depth, spider, callback = args\n         if opts.pipelines:\n             itemproc = self.pcrawler.engine.scraper.itemproc\n@@ -345,7 +334,7 @@ class Command(BaseRunSpiderCommand):\n     def prepare_request(\n         self, spider: Spider, request: Request, opts: argparse.Namespace\n     ) -> Request:\n-        def callback(response: Response, **cb_kwargs: Any) -> Deferred[List[Any]]:\n+        def callback(response: Response, **cb_kwargs: Any) -> Deferred[list[Any]]:\n             # memorize first request\n             if not self.first_response:\n                 self.first_response = response\n@@ -376,7 +365,7 @@ class Command(BaseRunSpiderCommand):\n         request.callback = callback\n         return request\n \n-    def process_options(self, args: List[str], opts: argparse.Namespace) -> None:\n+    def process_options(self, args: list[str], opts: argparse.Namespace) -> None:\n         super().process_options(args, opts)\n \n         self.process_request_meta(opts)\n@@ -404,7 +393,7 @@ class Command(BaseRunSpiderCommand):\n                     print_help=False,\n                 )\n \n-    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n+    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n         # parse arguments\n         if not len(args) == 1 or not is_url(args[0]):\n             raise UsageError()\n\n@@ -4,7 +4,7 @@ import argparse\n import sys\n from importlib import import_module\n from pathlib import Path\n-from typing import TYPE_CHECKING, List, Union\n+from typing import TYPE_CHECKING, Union\n \n from scrapy.commands import BaseRunSpiderCommand\n from scrapy.exceptions import UsageError\n@@ -41,7 +41,7 @@ class Command(BaseRunSpiderCommand):\n     def long_desc(self) -> str:\n         return \"Run the spider defined in the given file\"\n \n-    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n+    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n         if len(args) != 1:\n             raise UsageError()\n         filename = Path(args[0])\n\n@@ -1,6 +1,5 @@\n import argparse\n import json\n-from typing import List\n \n from scrapy.commands import ScrapyCommand\n from scrapy.settings import BaseSettings\n@@ -46,7 +45,7 @@ class Command(ScrapyCommand):\n             help=\"print setting value, interpreted as a list\",\n         )\n \n-    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n+    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n         assert self.crawler_process\n         settings = self.crawler_process.settings\n         if opts.get:\n\n@@ -7,7 +7,7 @@ See documentation in docs/topics/shell.rst\n from __future__ import annotations\n \n from threading import Thread\n-from typing import TYPE_CHECKING, Any, Dict, List, Type\n+from typing import TYPE_CHECKING, Any\n \n from scrapy import Spider\n from scrapy.commands import ScrapyCommand\n@@ -56,13 +56,13 @@ class Command(ScrapyCommand):\n             help=\"do not handle HTTP 3xx status codes and print response as-is\",\n         )\n \n-    def update_vars(self, vars: Dict[str, Any]) -> None:\n+    def update_vars(self, vars: dict[str, Any]) -> None:\n         \"\"\"You can use this function to update the Scrapy objects that will be\n         available in the shell\n         \"\"\"\n         pass\n \n-    def run(self, args: List[str], opts: Namespace) -> None:\n+    def run(self, args: list[str], opts: Namespace) -> None:\n         url = args[0] if args else None\n         if url:\n             # first argument may be a local file\n@@ -71,7 +71,7 @@ class Command(ScrapyCommand):\n         assert self.crawler_process\n         spider_loader = self.crawler_process.spider_loader\n \n-        spidercls: Type[Spider] = DefaultSpider\n+        spidercls: type[Spider] = DefaultSpider\n         if opts.spider:\n             spidercls = spider_loader.load(opts.spider)\n         elif url:\n\n@@ -6,14 +6,14 @@ from importlib.util import find_spec\n from pathlib import Path\n from shutil import copy2, copystat, ignore_patterns, move\n from stat import S_IWUSR as OWNER_WRITE_PERMISSION\n-from typing import List, Tuple, Union\n+from typing import Union\n \n import scrapy\n from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n from scrapy.utils.template import render_templatefile, string_camelcase\n \n-TEMPLATES_TO_RENDER: Tuple[Tuple[str, ...], ...] = (\n+TEMPLATES_TO_RENDER: tuple[tuple[str, ...], ...] = (\n     (\"scrapy.cfg\",),\n     (\"${project_name}\", \"settings.py.tmpl\"),\n     (\"${project_name}\", \"items.py.tmpl\"),\n@@ -86,7 +86,7 @@ class Command(ScrapyCommand):\n         copystat(src, dst)\n         _make_writable(dst)\n \n-    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n+    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n         if len(args) not in (1, 2):\n             raise UsageError()\n \n@@ -107,9 +107,7 @@ class Command(ScrapyCommand):\n             return\n \n         self._copytree(Path(self.templates_dir), project_dir.resolve())\n-        # On 3.8 shutil.move doesn't fully support Path args, but it supports our use case\n-        # See https://bugs.python.org/issue32689\n-        move(project_dir / \"module\", project_dir / project_name)  # type: ignore[arg-type]\n+        move(project_dir / \"module\", project_dir / project_name)\n         for paths in TEMPLATES_TO_RENDER:\n             tplfile = Path(\n                 project_dir,\n\n@@ -1,5 +1,4 @@\n import argparse\n-from typing import List\n \n import scrapy\n from scrapy.commands import ScrapyCommand\n@@ -25,7 +24,7 @@ class Command(ScrapyCommand):\n             help=\"also display twisted/python/platform info (useful for bug reports)\",\n         )\n \n-    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n+    def run(self, args: list[str], opts: argparse.Namespace) -> None:\n         if opts.verbose:\n             versions = scrapy_components_versions()\n             width = max(len(n) for (n, _) in versions)\n\n@@ -2,22 +2,11 @@ from __future__ import annotations\n \n import re\n import sys\n+from collections.abc import AsyncGenerator, Iterable\n from functools import wraps\n from inspect import getmembers\n from types import CoroutineType\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    AsyncGenerator,\n-    Callable,\n-    Dict,\n-    Iterable,\n-    List,\n-    Optional,\n-    Tuple,\n-    Type,\n-    cast,\n-)\n+from typing import TYPE_CHECKING, Any, Optional, cast\n from unittest import TestCase, TestResult\n \n from scrapy.http import Request, Response\n@@ -25,6 +14,8 @@ from scrapy.utils.python import get_spec\n from scrapy.utils.spider import iterate_spider_output\n \n if TYPE_CHECKING:\n+    from collections.abc import Callable\n+\n     from twisted.python.failure import Failure\n \n     from scrapy import Spider\n@@ -33,13 +24,13 @@ if TYPE_CHECKING:\n class Contract:\n     \"\"\"Abstract class for contracts\"\"\"\n \n-    request_cls: Optional[Type[Request]] = None\n+    request_cls: Optional[type[Request]] = None\n     name: str\n \n     def __init__(self, method: Callable, *args: Any):\n         self.testcase_pre = _create_testcase(method, f\"@{self.name} pre-hook\")\n         self.testcase_post = _create_testcase(method, f\"@{self.name} post-hook\")\n-        self.args: Tuple[Any, ...] = args\n+        self.args: tuple[Any, ...] = args\n \n     def add_pre_hook(self, request: Request, results: TestResult) -> Request:\n         if hasattr(self, \"pre_process\"):\n@@ -47,7 +38,7 @@ class Contract:\n             assert cb is not None\n \n             @wraps(cb)\n-            def wrapper(response: Response, **cb_kwargs: Any) -> List[Any]:\n+            def wrapper(response: Response, **cb_kwargs: Any) -> list[Any]:\n                 try:\n                     results.startTest(self.testcase_pre)\n                     self.pre_process(response)\n@@ -76,7 +67,7 @@ class Contract:\n             assert cb is not None\n \n             @wraps(cb)\n-            def wrapper(response: Response, **cb_kwargs: Any) -> List[Any]:\n+            def wrapper(response: Response, **cb_kwargs: Any) -> list[Any]:\n                 cb_result = cb(response, **cb_kwargs)\n                 if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n                     raise TypeError(\"Contracts don't support async callbacks\")\n@@ -98,18 +89,18 @@ class Contract:\n \n         return request\n \n-    def adjust_request_args(self, args: Dict[str, Any]) -> Dict[str, Any]:\n+    def adjust_request_args(self, args: dict[str, Any]) -> dict[str, Any]:\n         return args\n \n \n class ContractsManager:\n-    contracts: Dict[str, Type[Contract]] = {}\n+    contracts: dict[str, type[Contract]] = {}\n \n-    def __init__(self, contracts: Iterable[Type[Contract]]):\n+    def __init__(self, contracts: Iterable[type[Contract]]):\n         for contract in contracts:\n             self.contracts[contract.name] = contract\n \n-    def tested_methods_from_spidercls(self, spidercls: Type[Spider]) -> List[str]:\n+    def tested_methods_from_spidercls(self, spidercls: type[Spider]) -> list[str]:\n         is_method = re.compile(r\"^\\s*@\", re.MULTILINE).search\n         methods = []\n         for key, value in getmembers(spidercls):\n@@ -118,8 +109,8 @@ class ContractsManager:\n \n         return methods\n \n-    def extract_contracts(self, method: Callable) -> List[Contract]:\n-        contracts: List[Contract] = []\n+    def extract_contracts(self, method: Callable) -> list[Contract]:\n+        contracts: list[Contract] = []\n         assert method.__doc__ is not None\n         for line in method.__doc__.split(\"\\n\"):\n             line = line.strip()\n@@ -137,8 +128,8 @@ class ContractsManager:\n \n     def from_spider(\n         self, spider: Spider, results: TestResult\n-    ) -> List[Optional[Request]]:\n-        requests: List[Optional[Request]] = []\n+    ) -> list[Optional[Request]]:\n+        requests: list[Optional[Request]] = []\n         for method in self.tested_methods_from_spidercls(type(spider)):\n             bound_method = spider.__getattribute__(method)\n             try:\n\n@@ -1,5 +1,5 @@\n import json\n-from typing import Any, Callable, Dict, List, Optional\n+from typing import Any, Callable, Optional\n \n from itemadapter import ItemAdapter, is_item\n \n@@ -16,7 +16,7 @@ class UrlContract(Contract):\n \n     name = \"url\"\n \n-    def adjust_request_args(self, args: Dict[str, Any]) -> Dict[str, Any]:\n+    def adjust_request_args(self, args: dict[str, Any]) -> dict[str, Any]:\n         args[\"url\"] = self.args[0]\n         return args\n \n@@ -30,7 +30,7 @@ class CallbackKeywordArgumentsContract(Contract):\n \n     name = \"cb_kwargs\"\n \n-    def adjust_request_args(self, args: Dict[str, Any]) -> Dict[str, Any]:\n+    def adjust_request_args(self, args: dict[str, Any]) -> dict[str, Any]:\n         args[\"cb_kwargs\"] = json.loads(\" \".join(self.args))\n         return args\n \n@@ -44,7 +44,7 @@ class MetadataContract(Contract):\n \n     name = \"meta\"\n \n-    def adjust_request_args(self, args: Dict[str, Any]) -> Dict[str, Any]:\n+    def adjust_request_args(self, args: dict[str, Any]) -> dict[str, Any]:\n         args[\"meta\"] = json.loads(\" \".join(self.args))\n         return args\n \n@@ -63,7 +63,7 @@ class ReturnsContract(Contract):\n     \"\"\"\n \n     name = \"returns\"\n-    object_type_verifiers: Dict[Optional[str], Callable[[Any], bool]] = {\n+    object_type_verifiers: dict[Optional[str], Callable[[Any], bool]] = {\n         \"request\": lambda x: isinstance(x, Request),\n         \"requests\": lambda x: isinstance(x, Request),\n         \"item\": is_item,\n@@ -90,7 +90,7 @@ class ReturnsContract(Contract):\n         except IndexError:\n             self.max_bound = float(\"inf\")\n \n-    def post_process(self, output: List[Any]) -> None:\n+    def post_process(self, output: list[Any]) -> None:\n         occurrences = 0\n         for x in output:\n             if self.obj_type_verifier(x):\n@@ -116,7 +116,7 @@ class ScrapesContract(Contract):\n \n     name = \"scrapes\"\n \n-    def post_process(self, output: List[Any]) -> None:\n+    def post_process(self, output: list[Any]) -> None:\n         for x in output:\n             if is_item(x):\n                 missing = [arg for arg in self.args if arg not in ItemAdapter(x)]\n\n@@ -5,18 +5,7 @@ import warnings\n from collections import deque\n from datetime import datetime\n from time import time\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    Deque,\n-    Dict,\n-    Optional,\n-    Set,\n-    Tuple,\n-    TypeVar,\n-    Union,\n-    cast,\n-)\n+from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union, cast\n \n from twisted.internet import task\n from twisted.internet.defer import Deferred\n@@ -55,9 +44,9 @@ class Slot:\n         self.randomize_delay: bool = randomize_delay\n         self.throttle = throttle\n \n-        self.active: Set[Request] = set()\n-        self.queue: Deque[Tuple[Request, Deferred[Response]]] = deque()\n-        self.transferring: Set[Request] = set()\n+        self.active: set[Request] = set()\n+        self.queue: deque[tuple[Request, Deferred[Response]]] = deque()\n+        self.transferring: set[Request] = set()\n         self.lastseen: float = 0\n         self.latercall = None\n \n@@ -95,7 +84,7 @@ class Slot:\n \n def _get_concurrency_delay(\n     concurrency: int, spider: Spider, settings: BaseSettings\n-) -> Tuple[int, float]:\n+) -> tuple[int, float]:\n     delay: float = settings.getfloat(\"DOWNLOAD_DELAY\")\n     if hasattr(spider, \"download_delay\"):\n         delay = spider.download_delay\n@@ -112,8 +101,8 @@ class Downloader:\n     def __init__(self, crawler: Crawler):\n         self.settings: BaseSettings = crawler.settings\n         self.signals: SignalManager = crawler.signals\n-        self.slots: Dict[str, Slot] = {}\n-        self.active: Set[Request] = set()\n+        self.slots: dict[str, Slot] = {}\n+        self.active: set[Request] = set()\n         self.handlers: DownloadHandlers = DownloadHandlers(crawler)\n         self.total_concurrency: int = self.settings.getint(\"CONCURRENT_REQUESTS\")\n         self.domain_concurrency: int = self.settings.getint(\n@@ -126,7 +115,7 @@ class Downloader:\n         )\n         self._slot_gc_loop: task.LoopingCall = task.LoopingCall(self._slot_gc)\n         self._slot_gc_loop.start(60)\n-        self.per_slot_settings: Dict[str, Dict[str, Any]] = self.settings.getdict(\n+        self.per_slot_settings: dict[str, dict[str, Any]] = self.settings.getdict(\n             \"DOWNLOAD_SLOTS\", {}\n         )\n \n@@ -146,7 +135,7 @@ class Downloader:\n     def needs_backout(self) -> bool:\n         return len(self.active) >= self.total_concurrency\n \n-    def _get_slot(self, request: Request, spider: Spider) -> Tuple[str, Slot]:\n+    def _get_slot(self, request: Request, spider: Spider) -> tuple[str, Slot]:\n         key = self.get_slot_key(request)\n         if key not in self.slots:\n             slot_settings = self.per_slot_settings.get(key, {})\n\n@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n import warnings\n-from typing import TYPE_CHECKING, Any, List, Optional\n+from typing import TYPE_CHECKING, Any, Optional\n \n from OpenSSL import SSL\n from twisted.internet._sslverify import _setAcceptableProtocols\n@@ -154,10 +154,10 @@ class AcceptableProtocolsContextFactory:\n     negotiation.\n     \"\"\"\n \n-    def __init__(self, context_factory: Any, acceptable_protocols: List[bytes]):\n+    def __init__(self, context_factory: Any, acceptable_protocols: list[bytes]):\n         verifyObject(IPolicyForHTTPS, context_factory)\n         self._wrapped_context_factory: Any = context_factory\n-        self._acceptable_protocols: List[bytes] = acceptable_protocols\n+        self._acceptable_protocols: list[bytes] = acceptable_protocols\n \n     def creatorForNetloc(self, hostname: bytes, port: int) -> ClientTLSOptions:\n         options: ClientTLSOptions = self._wrapped_context_factory.creatorForNetloc(\n\n@@ -3,18 +3,8 @@\n from __future__ import annotations\n \n import logging\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    Callable,\n-    Dict,\n-    Generator,\n-    Optional,\n-    Protocol,\n-    Type,\n-    Union,\n-    cast,\n-)\n+from collections.abc import Callable\n+from typing import TYPE_CHECKING, Any, Optional, Protocol, Union, cast\n \n from twisted.internet import defer\n \n@@ -25,6 +15,8 @@ from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.python import without_none_values\n \n if TYPE_CHECKING:\n+    from collections.abc import Generator\n+\n     from twisted.internet.defer import Deferred\n \n     from scrapy.crawler import Crawler\n@@ -43,16 +35,16 @@ class DownloadHandlerProtocol(Protocol):\n class DownloadHandlers:\n     def __init__(self, crawler: Crawler):\n         self._crawler: Crawler = crawler\n-        self._schemes: Dict[str, Union[str, Callable[..., Any]]] = (\n+        self._schemes: dict[str, Union[str, Callable[..., Any]]] = (\n             {}\n         )  # stores acceptable schemes on instancing\n-        self._handlers: Dict[str, DownloadHandlerProtocol] = (\n+        self._handlers: dict[str, DownloadHandlerProtocol] = (\n             {}\n         )  # stores instanced handlers for schemes\n-        self._notconfigured: Dict[str, str] = {}  # remembers failed handlers\n-        handlers: Dict[str, Union[str, Callable[..., Any]]] = without_none_values(\n+        self._notconfigured: dict[str, str] = {}  # remembers failed handlers\n+        handlers: dict[str, Union[str, Callable[..., Any]]] = without_none_values(\n             cast(\n-                Dict[str, Union[str, Callable[..., Any]]],\n+                dict[str, Union[str, Callable[..., Any]]],\n                 crawler.settings.getwithbase(\"DOWNLOAD_HANDLERS\"),\n             )\n         )\n@@ -81,7 +73,7 @@ class DownloadHandlers:\n     ) -> Optional[DownloadHandlerProtocol]:\n         path = self._schemes[scheme]\n         try:\n-            dhcls: Type[DownloadHandlerProtocol] = load_object(path)\n+            dhcls: type[DownloadHandlerProtocol] = load_object(path)\n             if skip_lazy and getattr(dhcls, \"lazy\", True):\n                 return None\n             dh = build_from_crawler(\n\n@@ -1,6 +1,6 @@\n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Any, Dict\n+from typing import TYPE_CHECKING, Any\n \n from w3lib.url import parse_data_uri\n \n@@ -20,7 +20,7 @@ class DataURIDownloadHandler:\n         uri = parse_data_uri(request.url)\n         respcls = responsetypes.from_mimetype(uri.media_type)\n \n-        resp_kwargs: Dict[str, Any] = {}\n+        resp_kwargs: dict[str, Any] = {}\n         if issubclass(respcls, TextResponse) and uri.media_type.split(\"/\")[0] == \"text\":\n             charset = uri.media_type_parameters.get(\"charset\")\n             resp_kwargs[\"encoding\"] = charset\n\n@@ -32,7 +32,7 @@ from __future__ import annotations\n \n import re\n from io import BytesIO\n-from typing import TYPE_CHECKING, Any, BinaryIO, Dict, Optional\n+from typing import TYPE_CHECKING, Any, BinaryIO, Optional\n from urllib.parse import unquote\n \n from twisted.internet.protocol import ClientCreator, Protocol\n@@ -79,7 +79,7 @@ _CODE_RE = re.compile(r\"\\d+\")\n class FTPDownloadHandler:\n     lazy = False\n \n-    CODE_MAPPING: Dict[str, int] = {\n+    CODE_MAPPING: dict[str, int] = {\n         \"550\": 404,\n         \"default\": 503,\n     }\n\n@@ -1,9 +1,8 @@\n-\"\"\"Download handlers for http and https schemes\n-\"\"\"\n+\"\"\"Download handlers for http and https schemes\"\"\"\n \n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Type\n+from typing import TYPE_CHECKING\n \n from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.python import to_unicode\n@@ -27,10 +26,10 @@ class HTTP10DownloadHandler:\n     lazy = False\n \n     def __init__(self, settings: BaseSettings, crawler: Crawler):\n-        self.HTTPClientFactory: Type[ScrapyHTTPClientFactory] = load_object(\n+        self.HTTPClientFactory: type[ScrapyHTTPClientFactory] = load_object(\n             settings[\"DOWNLOADER_HTTPCLIENTFACTORY\"]\n         )\n-        self.ClientContextFactory: Type[ScrapyClientContextFactory] = load_object(\n+        self.ClientContextFactory: type[ScrapyClientContextFactory] = load_object(\n             settings[\"DOWNLOADER_CLIENTCONTEXTFACTORY\"]\n         )\n         self._settings: BaseSettings = settings\n\n@@ -8,7 +8,7 @@ import re\n from contextlib import suppress\n from io import BytesIO\n from time import time\n-from typing import TYPE_CHECKING, Any, List, Optional, Tuple, TypedDict, TypeVar, Union\n+from typing import TYPE_CHECKING, Any, Optional, TypedDict, TypeVar, Union\n from urllib.parse import urldefrag, urlunparse\n \n from twisted.internet import ssl\n@@ -52,7 +52,7 @@ _T = TypeVar(\"_T\")\n class _ResultT(TypedDict):\n     txresponse: TxResponse\n     body: bytes\n-    flags: Optional[List[str]]\n+    flags: Optional[list[str]]\n     certificate: Optional[ssl.Certificate]\n     ip_address: Union[ipaddress.IPv4Address, ipaddress.IPv6Address, None]\n     failure: NotRequired[Optional[Failure]]\n@@ -143,10 +143,10 @@ class TunnelingTCP4ClientEndpoint(TCP4ClientEndpoint):\n         reactor: ReactorBase,\n         host: str,\n         port: int,\n-        proxyConf: Tuple[str, int, Optional[bytes]],\n+        proxyConf: tuple[str, int, Optional[bytes]],\n         contextFactory: IPolicyForHTTPS,\n         timeout: float = 30,\n-        bindAddress: Optional[Tuple[str, int]] = None,\n+        bindAddress: Optional[tuple[str, int]] = None,\n     ):\n         proxyHost, proxyPort, self._proxyAuthHeader = proxyConf\n         super().__init__(reactor, proxyHost, proxyPort, timeout, bindAddress)\n@@ -254,14 +254,14 @@ class TunnelingAgent(Agent):\n         self,\n         *,\n         reactor: ReactorBase,\n-        proxyConf: Tuple[str, int, Optional[bytes]],\n+        proxyConf: tuple[str, int, Optional[bytes]],\n         contextFactory: IPolicyForHTTPS,\n         connectTimeout: Optional[float] = None,\n         bindAddress: Optional[bytes] = None,\n         pool: Optional[HTTPConnectionPool] = None,\n     ):\n         super().__init__(reactor, contextFactory, connectTimeout, bindAddress, pool)\n-        self._proxyConf: Tuple[str, int, Optional[bytes]] = proxyConf\n+        self._proxyConf: tuple[str, int, Optional[bytes]] = proxyConf\n         self._contextFactory: IPolicyForHTTPS = contextFactory\n \n     def _getEndpoint(self, uri: URI) -> TunnelingTCP4ClientEndpoint:\n@@ -621,7 +621,7 @@ class _ResponseReader(Protocol):\n         self._crawler: Crawler = crawler\n \n     def _finish_response(\n-        self, flags: Optional[List[str]] = None, failure: Optional[Failure] = None\n+        self, flags: Optional[list[str]] = None, failure: Optional[Failure] = None\n     ) -> None:\n         self._finished.callback(\n             {\n\n@@ -1,6 +1,6 @@\n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Any, Optional, Type\n+from typing import TYPE_CHECKING, Any, Optional\n \n from scrapy.core.downloader.handlers.http import HTTPDownloadHandler\n from scrapy.exceptions import NotConfigured\n@@ -29,7 +29,7 @@ class S3DownloadHandler:\n         aws_access_key_id: Optional[str] = None,\n         aws_secret_access_key: Optional[str] = None,\n         aws_session_token: Optional[str] = None,\n-        httpdownloadhandler: Type[HTTPDownloadHandler] = HTTPDownloadHandler,\n+        httpdownloadhandler: type[HTTPDownloadHandler] = HTTPDownloadHandler,\n         **kw: Any,\n     ):\n         if not is_botocore_available():\n\n@@ -6,7 +6,8 @@ See documentation in docs/topics/downloader-middleware.rst\n \n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Any, Callable, Generator, List, Union, cast\n+from collections.abc import Callable\n+from typing import TYPE_CHECKING, Any, Union, cast\n \n from twisted.internet.defer import Deferred, inlineCallbacks\n \n@@ -17,6 +18,8 @@ from scrapy.utils.conf import build_component_list\n from scrapy.utils.defer import deferred_from_coro, mustbe_deferred\n \n if TYPE_CHECKING:\n+    from collections.abc import Generator\n+\n     from twisted.python.failure import Failure\n \n     from scrapy import Spider\n@@ -27,7 +30,7 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n     component_name = \"downloader middleware\"\n \n     @classmethod\n-    def _get_mwlist_from_settings(cls, settings: BaseSettings) -> List[Any]:\n+    def _get_mwlist_from_settings(cls, settings: BaseSettings) -> list[Any]:\n         return build_component_list(settings.getwithbase(\"DOWNLOADER_MIDDLEWARES\"))\n \n     def _add_middleware(self, mw: Any) -> None:\n\n@@ -1,5 +1,5 @@\n import logging\n-from typing import Any, Dict\n+from typing import Any\n \n from OpenSSL import SSL\n from service_identity.exceptions import CertificateError\n@@ -21,7 +21,7 @@ METHOD_TLSv11 = \"TLSv1.1\"\n METHOD_TLSv12 = \"TLSv1.2\"\n \n \n-openssl_methods: Dict[str, int] = {\n+openssl_methods: dict[str, int] = {\n     METHOD_TLS: SSL.SSLv23_METHOD,  # protocol negotiation (recommended)\n     METHOD_TLSv10: SSL.TLSv1_METHOD,  # TLS 1.0 only\n     METHOD_TLSv11: SSL.TLSv1_1_METHOD,  # TLS 1.1 only\n\n@@ -2,7 +2,7 @@ from __future__ import annotations\n \n import re\n from time import time\n-from typing import TYPE_CHECKING, Optional, Tuple\n+from typing import TYPE_CHECKING, Optional\n from urllib.parse import ParseResult, urldefrag, urlparse, urlunparse\n \n from twisted.internet import defer\n@@ -18,7 +18,7 @@ if TYPE_CHECKING:\n     from scrapy import Request\n \n \n-def _parsed_url_args(parsed: ParseResult) -> Tuple[bytes, bytes, bytes, int, bytes]:\n+def _parsed_url_args(parsed: ParseResult) -> tuple[bytes, bytes, bytes, int, bytes]:\n     # Assume parsed is urlparse-d from Request.url,\n     # which was passed via safe_url_string and is ascii-only.\n     path_str = urlunparse((\"\", \"\", parsed.path or \"/\", parsed.params, parsed.query, \"\"))\n@@ -33,7 +33,7 @@ def _parsed_url_args(parsed: ParseResult) -> Tuple[bytes, bytes, bytes, int, byt\n     return scheme, netloc, host, port, path\n \n \n-def _parse(url: str) -> Tuple[bytes, bytes, bytes, int, bytes]:\n+def _parse(url: str) -> tuple[bytes, bytes, bytes, int, bytes]:\n     \"\"\"Return tuple of (scheme, netloc, host, port, path),\n     all in bytes except for port which is int.\n     Assume url is from Request.url, which was passed via safe_url_string\n\n@@ -9,20 +9,7 @@ from __future__ import annotations\n \n import logging\n from time import time\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    Callable,\n-    Generator,\n-    Iterable,\n-    Iterator,\n-    Optional,\n-    Set,\n-    Type,\n-    TypeVar,\n-    Union,\n-    cast,\n-)\n+from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union, cast\n \n from itemadapter import is_item\n from twisted.internet.defer import Deferred, inlineCallbacks, succeed\n@@ -42,6 +29,8 @@ from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.reactor import CallLaterOnce\n \n if TYPE_CHECKING:\n+    from collections.abc import Callable, Generator, Iterable, Iterator\n+\n     from scrapy.core.scheduler import BaseScheduler\n     from scrapy.core.scraper import _HandleOutputDeferred\n     from scrapy.crawler import Crawler\n@@ -63,7 +52,7 @@ class Slot:\n         scheduler: BaseScheduler,\n     ) -> None:\n         self.closing: Optional[Deferred[None]] = None\n-        self.inprogress: Set[Request] = set()\n+        self.inprogress: set[Request] = set()\n         self.start_requests: Optional[Iterator[Request]] = iter(start_requests)\n         self.close_if_idle: bool = close_if_idle\n         self.nextcall: CallLaterOnce[None] = nextcall\n@@ -106,10 +95,10 @@ class ExecutionEngine:\n         self.spider: Optional[Spider] = None\n         self.running: bool = False\n         self.paused: bool = False\n-        self.scheduler_cls: Type[BaseScheduler] = self._get_scheduler_class(\n+        self.scheduler_cls: type[BaseScheduler] = self._get_scheduler_class(\n             crawler.settings\n         )\n-        downloader_cls: Type[Downloader] = load_object(self.settings[\"DOWNLOADER\"])\n+        downloader_cls: type[Downloader] = load_object(self.settings[\"DOWNLOADER\"])\n         self.downloader: Downloader = downloader_cls(crawler)\n         self.scraper = Scraper(crawler)\n         self._spider_closed_callback: Callable[[Spider], Optional[Deferred[None]]] = (\n@@ -117,10 +106,10 @@ class ExecutionEngine:\n         )\n         self.start_time: Optional[float] = None\n \n-    def _get_scheduler_class(self, settings: BaseSettings) -> Type[BaseScheduler]:\n+    def _get_scheduler_class(self, settings: BaseSettings) -> type[BaseScheduler]:\n         from scrapy.core.scheduler import BaseScheduler\n \n-        scheduler_cls: Type[BaseScheduler] = load_object(settings[\"SCHEDULER\"])\n+        scheduler_cls: type[BaseScheduler] = load_object(settings[\"SCHEDULER\"])\n         if not issubclass(scheduler_cls, BaseScheduler):\n             raise TypeError(\n                 f\"The provided scheduler class ({settings['SCHEDULER']})\"\n\n@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n from collections import deque\n-from typing import TYPE_CHECKING, Deque, Dict, List, Optional, Tuple\n+from typing import TYPE_CHECKING, Optional\n \n from twisted.internet import defer\n from twisted.internet.defer import Deferred\n@@ -26,7 +26,7 @@ if TYPE_CHECKING:\n     from scrapy.spiders import Spider\n \n \n-ConnectionKeyT = Tuple[bytes, bytes, int]\n+ConnectionKeyT = tuple[bytes, bytes, int]\n \n \n class H2ConnectionPool:\n@@ -36,11 +36,11 @@ class H2ConnectionPool:\n \n         # Store a dictionary which is used to get the respective\n         # H2ClientProtocolInstance using the  key as Tuple(scheme, hostname, port)\n-        self._connections: Dict[ConnectionKeyT, H2ClientProtocol] = {}\n+        self._connections: dict[ConnectionKeyT, H2ClientProtocol] = {}\n \n         # Save all requests that arrive before the connection is established\n-        self._pending_requests: Dict[\n-            ConnectionKeyT, Deque[Deferred[H2ClientProtocol]]\n+        self._pending_requests: dict[\n+            ConnectionKeyT, deque[Deferred[H2ClientProtocol]]\n         ] = {}\n \n     def get_connection(\n@@ -68,7 +68,7 @@ class H2ConnectionPool:\n     ) -> Deferred[H2ClientProtocol]:\n         self._pending_requests[key] = deque()\n \n-        conn_lost_deferred: Deferred[List[BaseException]] = Deferred()\n+        conn_lost_deferred: Deferred[list[BaseException]] = Deferred()\n         conn_lost_deferred.addCallback(self._remove_connection, key)\n \n         factory = H2ClientFactory(uri, self.settings, conn_lost_deferred)\n@@ -94,7 +94,7 @@ class H2ConnectionPool:\n         return conn\n \n     def _remove_connection(\n-        self, errors: List[BaseException], key: ConnectionKeyT\n+        self, errors: list[BaseException], key: ConnectionKeyT\n     ) -> None:\n         self._connections.pop(key)\n \n\n@@ -4,7 +4,7 @@ import ipaddress\n import itertools\n import logging\n from collections import deque\n-from typing import TYPE_CHECKING, Any, Deque, Dict, List, Optional, Union\n+from typing import TYPE_CHECKING, Any, Optional, Union\n \n from h2.config import H2Configuration\n from h2.connection import H2Connection\n@@ -91,7 +91,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         self,\n         uri: URI,\n         settings: Settings,\n-        conn_lost_deferred: Deferred[List[BaseException]],\n+        conn_lost_deferred: Deferred[list[BaseException]],\n     ) -> None:\n         \"\"\"\n         Arguments:\n@@ -102,7 +102,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n             conn_lost_deferred -- Deferred fires with the reason: Failure to notify\n                 that connection was lost\n         \"\"\"\n-        self._conn_lost_deferred: Deferred[List[BaseException]] = conn_lost_deferred\n+        self._conn_lost_deferred: Deferred[list[BaseException]] = conn_lost_deferred\n \n         config = H2Configuration(client_side=True, header_encoding=\"utf-8\")\n         self.conn = H2Connection(config=config)\n@@ -113,19 +113,19 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         self._stream_id_generator = itertools.count(start=1, step=2)\n \n         # Streams are stored in a dictionary keyed off their stream IDs\n-        self.streams: Dict[int, Stream] = {}\n+        self.streams: dict[int, Stream] = {}\n \n         # If requests are received before connection is made we keep\n         # all requests in a pool and send them as the connection is made\n-        self._pending_request_stream_pool: Deque[Stream] = deque()\n+        self._pending_request_stream_pool: deque[Stream] = deque()\n \n         # Save an instance of errors raised which lead to losing the connection\n         # We pass these instances to the streams ResponseFailed() failure\n-        self._conn_lost_errors: List[BaseException] = []\n+        self._conn_lost_errors: list[BaseException] = []\n \n         # Some meta data of this connection\n         # initialized when connection is successfully made\n-        self.metadata: Dict[str, Any] = {\n+        self.metadata: dict[str, Any] = {\n             # Peer certificate instance\n             \"certificate\": None,\n             # Address of the server we are connected to which\n@@ -250,7 +250,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         self.conn.initiate_connection()\n         self._write_to_transport()\n \n-    def _lose_connection_with_error(self, errors: List[BaseException]) -> None:\n+    def _lose_connection_with_error(self, errors: list[BaseException]) -> None:\n         \"\"\"Helper function to lose the connection with the error sent as a\n         reason\"\"\"\n         self._conn_lost_errors += errors\n@@ -353,7 +353,7 @@ class H2ClientProtocol(Protocol, TimeoutMixin):\n         self._pending_request_stream_pool.clear()\n         self.conn.close_connection()\n \n-    def _handle_events(self, events: List[Event]) -> None:\n+    def _handle_events(self, events: list[Event]) -> None:\n         \"\"\"Private method which acts as a bridge between the events\n         received from the HTTP/2 data and IH2EventsHandler\n \n@@ -442,7 +442,7 @@ class H2ClientFactory(Factory):\n         self,\n         uri: URI,\n         settings: Settings,\n-        conn_lost_deferred: Deferred[List[BaseException]],\n+        conn_lost_deferred: Deferred[list[BaseException]],\n     ) -> None:\n         self.uri = uri\n         self.settings = settings\n@@ -451,5 +451,5 @@ class H2ClientFactory(Factory):\n     def buildProtocol(self, addr: IAddress) -> H2ClientProtocol:\n         return H2ClientProtocol(self.uri, self.settings, self.conn_lost_deferred)\n \n-    def acceptableProtocols(self) -> List[bytes]:\n+    def acceptableProtocols(self) -> list[bytes]:\n         return [PROTOCOL_NAME]\n\n@@ -3,7 +3,7 @@ from __future__ import annotations\n import logging\n from enum import Enum\n from io import BytesIO\n-from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple\n+from typing import TYPE_CHECKING, Any, Optional\n \n from h2.errors import ErrorCodes\n from h2.exceptions import H2Error, ProtocolError, StreamClosedError\n@@ -113,7 +113,7 @@ class Stream:\n \n         # Metadata of an HTTP/2 connection stream\n         # initialized when stream is instantiated\n-        self.metadata: Dict[str, Any] = {\n+        self.metadata: dict[str, Any] = {\n             \"request_content_length\": (\n                 0 if self._request.body is None else len(self._request.body)\n             ),\n@@ -134,7 +134,7 @@ class Stream:\n         # Private variable used to build the response\n         # this response is then converted to appropriate Response class\n         # passed to the response deferred callback\n-        self._response: Dict[str, Any] = {\n+        self._response: dict[str, Any] = {\n             # Data received frame by frame from the server is appended\n             # and passed to the response Deferred when completely received.\n             \"body\": BytesIO(),\n@@ -196,7 +196,7 @@ class Stream:\n             == f'{self._protocol.metadata[\"ip_address\"]}:{self._protocol.metadata[\"uri\"].port}'\n         )\n \n-    def _get_request_headers(self) -> List[Tuple[str, str]]:\n+    def _get_request_headers(self) -> list[tuple[str, str]]:\n         url = urlparse_cached(self._request)\n \n         path = url.path\n@@ -349,7 +349,7 @@ class Stream:\n             self._response[\"flow_controlled_size\"], self.stream_id\n         )\n \n-    def receive_headers(self, headers: List[HeaderTuple]) -> None:\n+    def receive_headers(self, headers: list[HeaderTuple]) -> None:\n         for name, value in headers:\n             self._response[\"headers\"].appendlist(name, value)\n \n@@ -382,7 +382,7 @@ class Stream:\n     def close(\n         self,\n         reason: StreamCloseReason,\n-        errors: Optional[List[BaseException]] = None,\n+        errors: Optional[list[BaseException]] = None,\n         from_protocol: bool = False,\n     ) -> None:\n         \"\"\"Based on the reason sent we will handle each case.\"\"\"\n\n@@ -4,7 +4,7 @@ import json\n import logging\n from abc import abstractmethod\n from pathlib import Path\n-from typing import TYPE_CHECKING, Any, List, Optional, Type, cast\n+from typing import TYPE_CHECKING, Any, Optional, cast\n \n # working around https://github.com/sphinx-doc/sphinx/issues/10400\n from twisted.internet.defer import Deferred  # noqa: TC002\n@@ -182,18 +182,18 @@ class Scheduler(BaseScheduler):\n         self,\n         dupefilter: BaseDupeFilter,\n         jobdir: Optional[str] = None,\n-        dqclass: Optional[Type[BaseQueue]] = None,\n-        mqclass: Optional[Type[BaseQueue]] = None,\n+        dqclass: Optional[type[BaseQueue]] = None,\n+        mqclass: Optional[type[BaseQueue]] = None,\n         logunser: bool = False,\n         stats: Optional[StatsCollector] = None,\n-        pqclass: Optional[Type[ScrapyPriorityQueue]] = None,\n+        pqclass: Optional[type[ScrapyPriorityQueue]] = None,\n         crawler: Optional[Crawler] = None,\n     ):\n         self.df: BaseDupeFilter = dupefilter\n         self.dqdir: Optional[str] = self._dqdir(jobdir)\n-        self.pqclass: Optional[Type[ScrapyPriorityQueue]] = pqclass\n-        self.dqclass: Optional[Type[BaseQueue]] = dqclass\n-        self.mqclass: Optional[Type[BaseQueue]] = mqclass\n+        self.pqclass: Optional[type[ScrapyPriorityQueue]] = pqclass\n+        self.dqclass: Optional[type[BaseQueue]] = dqclass\n+        self.mqclass: Optional[type[BaseQueue]] = mqclass\n         self.logunser: bool = logunser\n         self.stats: Optional[StatsCollector] = stats\n         self.crawler: Optional[Crawler] = crawler\n@@ -364,13 +364,13 @@ class Scheduler(BaseScheduler):\n             return str(dqdir)\n         return None\n \n-    def _read_dqs_state(self, dqdir: str) -> List[int]:\n+    def _read_dqs_state(self, dqdir: str) -> list[int]:\n         path = Path(dqdir, \"active.json\")\n         if not path.exists():\n             return []\n         with path.open(encoding=\"utf-8\") as f:\n-            return cast(List[int], json.load(f))\n+            return cast(list[int], json.load(f))\n \n-    def _write_dqs_state(self, dqdir: str, state: List[int]) -> None:\n+    def _write_dqs_state(self, dqdir: str, state: list[int]) -> None:\n         with Path(dqdir, \"active.json\").open(\"w\", encoding=\"utf-8\") as f:\n             json.dump(state, f)\n\n@@ -5,23 +5,8 @@ from __future__ import annotations\n \n import logging\n from collections import deque\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    AsyncIterable,\n-    Deque,\n-    Generator,\n-    Iterable,\n-    Iterator,\n-    List,\n-    Optional,\n-    Set,\n-    Tuple,\n-    Type,\n-    TypeVar,\n-    Union,\n-    cast,\n-)\n+from collections.abc import AsyncIterable, Iterator\n+from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union, cast\n \n from itemadapter import is_item\n from twisted.internet.defer import Deferred, inlineCallbacks\n@@ -47,6 +32,8 @@ from scrapy.utils.misc import load_object, warn_on_generator_with_return_value\n from scrapy.utils.spider import iterate_spider_output\n \n if TYPE_CHECKING:\n+    from collections.abc import Generator, Iterable\n+\n     from scrapy.crawler import Crawler\n \n \n@@ -54,12 +41,12 @@ logger = logging.getLogger(__name__)\n \n \n _T = TypeVar(\"_T\")\n-_ParallelResult = List[Tuple[bool, Iterator[Any]]]\n+_ParallelResult = list[tuple[bool, Iterator[Any]]]\n \n if TYPE_CHECKING:\n     # parameterized Deferreds require Twisted 21.7.0\n     _HandleOutputDeferred = Deferred[Union[_ParallelResult, None]]\n-    QueueTuple = Tuple[Union[Response, Failure], Request, _HandleOutputDeferred]\n+    QueueTuple = tuple[Union[Response, Failure], Request, _HandleOutputDeferred]\n \n \n class Slot:\n@@ -69,8 +56,8 @@ class Slot:\n \n     def __init__(self, max_active_size: int = 5000000):\n         self.max_active_size = max_active_size\n-        self.queue: Deque[QueueTuple] = deque()\n-        self.active: Set[Request] = set()\n+        self.queue: deque[QueueTuple] = deque()\n+        self.active: set[Request] = set()\n         self.active_size: int = 0\n         self.itemproc_size: int = 0\n         self.closing: Optional[Deferred[Spider]] = None\n@@ -113,7 +100,7 @@ class Scraper:\n         self.spidermw: SpiderMiddlewareManager = SpiderMiddlewareManager.from_crawler(\n             crawler\n         )\n-        itemproc_cls: Type[ItemPipelineManager] = load_object(\n+        itemproc_cls: type[ItemPipelineManager] = load_object(\n             crawler.settings[\"ITEM_PROCESSOR\"]\n         )\n         self.itemproc: ItemPipelineManager = itemproc_cls.from_crawler(crawler)\n\n@@ -7,22 +7,10 @@ See documentation in docs/topics/spider-middleware.rst\n from __future__ import annotations\n \n import logging\n+from collections.abc import AsyncIterable, Callable, Iterable\n from inspect import isasyncgenfunction, iscoroutine\n from itertools import islice\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    AsyncIterable,\n-    Callable,\n-    Generator,\n-    Iterable,\n-    List,\n-    Optional,\n-    Tuple,\n-    TypeVar,\n-    Union,\n-    cast,\n-)\n+from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union, cast\n \n from twisted.internet.defer import Deferred, inlineCallbacks\n from twisted.python.failure import Failure\n@@ -42,6 +30,8 @@ from scrapy.utils.defer import (\n from scrapy.utils.python import MutableAsyncChain, MutableChain\n \n if TYPE_CHECKING:\n+    from collections.abc import Generator\n+\n     from scrapy.settings import BaseSettings\n \n \n@@ -66,7 +56,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         self.downgrade_warning_done = False\n \n     @classmethod\n-    def _get_mwlist_from_settings(cls, settings: BaseSettings) -> List[Any]:\n+    def _get_mwlist_from_settings(cls, settings: BaseSettings) -> list[Any]:\n         return build_component_list(settings.getwithbase(\"SPIDER_MIDDLEWARES\"))\n \n     def _add_middleware(self, mw: Any) -> None:\n@@ -349,7 +339,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n     @staticmethod\n     def _get_async_method_pair(\n         mw: Any, methodname: str\n-    ) -> Union[None, Callable, Tuple[Callable, Callable]]:\n+    ) -> Union[None, Callable, tuple[Callable, Callable]]:\n         normal_method: Optional[Callable] = getattr(mw, methodname, None)\n         methodname_async = methodname + \"_async\"\n         async_method: Optional[Callable] = getattr(mw, methodname_async, None)\n\n@@ -4,18 +4,7 @@ import logging\n import pprint\n import signal\n import warnings\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    Dict,\n-    Generator,\n-    Optional,\n-    Set,\n-    Type,\n-    TypeVar,\n-    Union,\n-    cast,\n-)\n+from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union, cast\n \n from twisted.internet.defer import (\n     Deferred,\n@@ -53,6 +42,8 @@ from scrapy.utils.reactor import (\n )\n \n if TYPE_CHECKING:\n+    from collections.abc import Generator\n+\n     from scrapy.utils.request import RequestFingerprinter\n \n \n@@ -64,8 +55,8 @@ _T = TypeVar(\"_T\")\n class Crawler:\n     def __init__(\n         self,\n-        spidercls: Type[Spider],\n-        settings: Union[None, Dict[str, Any], Settings] = None,\n+        spidercls: type[Spider],\n+        settings: Union[None, dict[str, Any], Settings] = None,\n         init_reactor: bool = False,\n     ):\n         if isinstance(spidercls, Spider):\n@@ -74,7 +65,7 @@ class Crawler:\n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n \n-        self.spidercls: Type[Spider] = spidercls\n+        self.spidercls: type[Spider] = spidercls\n         self.settings: Settings = settings.copy()\n         self.spidercls.update_settings(self.settings)\n         self._update_root_log_handler()\n@@ -112,7 +103,7 @@ class Crawler:\n         self.__remove_handler = lambda: logging.root.removeHandler(handler)\n         self.signals.connect(self.__remove_handler, signals.engine_stopped)\n \n-        lf_cls: Type[LogFormatter] = load_object(self.settings[\"LOG_FORMATTER\"])\n+        lf_cls: type[LogFormatter] = load_object(self.settings[\"LOG_FORMATTER\"])\n         self.logformatter = lf_cls.from_crawler(self)\n \n         self.request_fingerprinter = build_from_crawler(\n@@ -256,18 +247,18 @@ class CrawlerRunner:\n         verifyClass(ISpiderLoader, loader_cls)\n         return loader_cls.from_settings(settings.frozencopy())\n \n-    def __init__(self, settings: Union[Dict[str, Any], Settings, None] = None):\n+    def __init__(self, settings: Union[dict[str, Any], Settings, None] = None):\n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n         self.settings = settings\n         self.spider_loader = self._get_spider_loader(settings)\n-        self._crawlers: Set[Crawler] = set()\n-        self._active: Set[Deferred[None]] = set()\n+        self._crawlers: set[Crawler] = set()\n+        self._active: set[Deferred[None]] = set()\n         self.bootstrap_failed = False\n \n     def crawl(\n         self,\n-        crawler_or_spidercls: Union[Type[Spider], str, Crawler],\n+        crawler_or_spidercls: Union[type[Spider], str, Crawler],\n         *args: Any,\n         **kwargs: Any,\n     ) -> Deferred[None]:\n@@ -314,7 +305,7 @@ class CrawlerRunner:\n         return d.addBoth(_done)\n \n     def create_crawler(\n-        self, crawler_or_spidercls: Union[Type[Spider], str, Crawler]\n+        self, crawler_or_spidercls: Union[type[Spider], str, Crawler]\n     ) -> Crawler:\n         \"\"\"\n         Return a :class:`~scrapy.crawler.Crawler` object.\n@@ -335,11 +326,11 @@ class CrawlerRunner:\n             return crawler_or_spidercls\n         return self._create_crawler(crawler_or_spidercls)\n \n-    def _create_crawler(self, spidercls: Union[str, Type[Spider]]) -> Crawler:\n+    def _create_crawler(self, spidercls: Union[str, type[Spider]]) -> Crawler:\n         if isinstance(spidercls, str):\n             spidercls = self.spider_loader.load(spidercls)\n         # temporary cast until self.spider_loader is typed\n-        return Crawler(cast(Type[Spider], spidercls), self.settings)\n+        return Crawler(cast(type[Spider], spidercls), self.settings)\n \n     def stop(self) -> Deferred[Any]:\n         \"\"\"\n@@ -387,7 +378,7 @@ class CrawlerProcess(CrawlerRunner):\n \n     def __init__(\n         self,\n-        settings: Union[Dict[str, Any], Settings, None] = None,\n+        settings: Union[dict[str, Any], Settings, None] = None,\n         install_root_handler: bool = True,\n     ):\n         super().__init__(settings)\n@@ -416,14 +407,14 @@ class CrawlerProcess(CrawlerRunner):\n         )\n         reactor.callFromThread(self._stop_reactor)\n \n-    def _create_crawler(self, spidercls: Union[Type[Spider], str]) -> Crawler:\n+    def _create_crawler(self, spidercls: Union[type[Spider], str]) -> Crawler:\n         if isinstance(spidercls, str):\n             spidercls = self.spider_loader.load(spidercls)\n         init_reactor = not self._initialized_reactor\n         self._initialized_reactor = True\n         # temporary cast until self.spider_loader is typed\n         return Crawler(\n-            cast(Type[Spider], spidercls), self.settings, init_reactor=init_reactor\n+            cast(type[Spider], spidercls), self.settings, init_reactor=init_reactor\n         )\n \n     def start(\n\n@@ -2,7 +2,7 @@ from __future__ import annotations\n \n import logging\n from collections import defaultdict\n-from typing import TYPE_CHECKING, Any, DefaultDict, Iterable, Optional, Sequence, Union\n+from typing import TYPE_CHECKING, Any, Optional, Union\n \n from tldextract import TLDExtract\n \n@@ -13,6 +13,7 @@ from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_unicode\n \n if TYPE_CHECKING:\n+    from collections.abc import Iterable, Sequence\n     from http.cookiejar import Cookie\n \n     # typing.Self requires Python 3.11\n@@ -39,7 +40,7 @@ class CookiesMiddleware:\n     \"\"\"This middleware enables working with sites that need cookies\"\"\"\n \n     def __init__(self, debug: bool = False):\n-        self.jars: DefaultDict[Any, CookieJar] = defaultdict(CookieJar)\n+        self.jars: defaultdict[Any, CookieJar] = defaultdict(CookieJar)\n         self.debug: bool = debug\n \n     @classmethod\n\n@@ -6,11 +6,13 @@ See documentation in docs/topics/downloader-middleware.rst\n \n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Iterable, Tuple, Union\n+from typing import TYPE_CHECKING, Union\n \n from scrapy.utils.python import without_none_values\n \n if TYPE_CHECKING:\n+    from collections.abc import Iterable\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n@@ -20,8 +22,8 @@ if TYPE_CHECKING:\n \n \n class DefaultHeadersMiddleware:\n-    def __init__(self, headers: Iterable[Tuple[str, str]]):\n-        self._headers: Iterable[Tuple[str, str]] = headers\n+    def __init__(self, headers: Iterable[tuple[str, str]]):\n+        self._headers: Iterable[tuple[str, str]] = headers\n \n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n\n@@ -3,7 +3,7 @@ from __future__ import annotations\n import warnings\n from itertools import chain\n from logging import getLogger\n-from typing import TYPE_CHECKING, Any, Dict, List, Optional, Tuple, Union\n+from typing import TYPE_CHECKING, Any, Optional, Union\n \n from scrapy import Request, Spider, signals\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n@@ -28,7 +28,7 @@ if TYPE_CHECKING:\n \n logger = getLogger(__name__)\n \n-ACCEPTED_ENCODINGS: List[bytes] = [b\"gzip\", b\"deflate\"]\n+ACCEPTED_ENCODINGS: list[bytes] = [b\"gzip\", b\"deflate\"]\n \n try:\n     try:\n@@ -140,7 +140,7 @@ class HttpCompressionMiddleware:\n                 respcls = responsetypes.from_args(\n                     headers=response.headers, url=response.url, body=decoded_body\n                 )\n-                kwargs: Dict[str, Any] = {\"body\": decoded_body}\n+                kwargs: dict[str, Any] = {\"body\": decoded_body}\n                 if issubclass(respcls, TextResponse):\n                     # force recalculating the encoding until we make sure the\n                     # responsetypes guessing is reliable\n@@ -152,23 +152,23 @@ class HttpCompressionMiddleware:\n         return response\n \n     def _handle_encoding(\n-        self, body: bytes, content_encoding: List[bytes], max_size: int\n-    ) -> Tuple[bytes, List[bytes]]:\n+        self, body: bytes, content_encoding: list[bytes], max_size: int\n+    ) -> tuple[bytes, list[bytes]]:\n         to_decode, to_keep = self._split_encodings(content_encoding)\n         for encoding in to_decode:\n             body = self._decode(body, encoding, max_size)\n         return body, to_keep\n \n     def _split_encodings(\n-        self, content_encoding: List[bytes]\n-    ) -> Tuple[List[bytes], List[bytes]]:\n-        to_keep: List[bytes] = [\n+        self, content_encoding: list[bytes]\n+    ) -> tuple[list[bytes], list[bytes]]:\n+        to_keep: list[bytes] = [\n             encoding.strip().lower()\n             for encoding in chain.from_iterable(\n                 encodings.split(b\",\") for encodings in content_encoding\n             )\n         ]\n-        to_decode: List[bytes] = []\n+        to_decode: list[bytes] = []\n         while to_keep:\n             encoding = to_keep.pop()\n             if encoding not in ACCEPTED_ENCODINGS:\n\n@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n import base64\n-from typing import TYPE_CHECKING, Dict, Optional, Tuple, Union\n+from typing import TYPE_CHECKING, Optional, Union\n from urllib.parse import unquote, urlunparse\n from urllib.request import (  # type: ignore[attr-defined]\n     _parse_proxy,\n@@ -25,7 +25,7 @@ if TYPE_CHECKING:\n class HttpProxyMiddleware:\n     def __init__(self, auth_encoding: Optional[str] = \"latin-1\"):\n         self.auth_encoding: Optional[str] = auth_encoding\n-        self.proxies: Dict[str, Tuple[Optional[bytes], str]] = {}\n+        self.proxies: dict[str, tuple[Optional[bytes], str]] = {}\n         for type_, url in getproxies().items():\n             try:\n                 self.proxies[type_] = self._get_proxy(url, type_)\n@@ -47,7 +47,7 @@ class HttpProxyMiddleware:\n         )\n         return base64.b64encode(user_pass)\n \n-    def _get_proxy(self, url: str, orig_type: str) -> Tuple[Optional[bytes], str]:\n+    def _get_proxy(self, url: str, orig_type: str) -> tuple[Optional[bytes], str]:\n         proxy_type, user, password, hostport = _parse_proxy(url)\n         proxy_url = urlunparse((proxy_type or orig_type, hostport, \"\", \"\", \"\", \"\"))\n \n\n@@ -3,7 +3,7 @@ from __future__ import annotations\n import logging\n import re\n import warnings\n-from typing import TYPE_CHECKING, Set\n+from typing import TYPE_CHECKING\n \n from scrapy import Request, Spider, signals\n from scrapy.exceptions import IgnoreRequest\n@@ -31,7 +31,7 @@ class OffsiteMiddleware:\n \n     def __init__(self, stats: StatsCollector):\n         self.stats = stats\n-        self.domains_seen: Set[str] = set()\n+        self.domains_seen: set[str] = set()\n \n     def spider_opened(self, spider: Spider) -> None:\n         self.host_regex: re.Pattern[str] = self.get_host_regex(spider)\n\n@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n import logging\n-from typing import TYPE_CHECKING, Any, List, Union, cast\n+from typing import TYPE_CHECKING, Any, Union, cast\n from urllib.parse import urljoin\n \n from w3lib.url import safe_url_string\n@@ -180,7 +180,7 @@ class MetaRefreshMiddleware(BaseRedirectMiddleware):\n \n     def __init__(self, settings: BaseSettings):\n         super().__init__(settings)\n-        self._ignore_tags: List[str] = settings.getlist(\"METAREFRESH_IGNORE_TAGS\")\n+        self._ignore_tags: list[str] = settings.getlist(\"METAREFRESH_IGNORE_TAGS\")\n         self._maxdelay: int = settings.getint(\"METAREFRESH_MAXDELAY\")\n \n     def process_response(\n\n@@ -7,14 +7,14 @@ RETRY_TIMES - how many times to retry a failed page\n RETRY_HTTP_CODES - which HTTP response codes to retry\n \n Failed pages are collected on the scraping process and rescheduled at the end,\n-once the spider has finished crawling all regular (non failed) pages.\n+once the spider has finished crawling all regular (non-failed) pages.\n \"\"\"\n \n from __future__ import annotations\n \n import warnings\n from logging import Logger, getLogger\n-from typing import TYPE_CHECKING, Any, Optional, Tuple, Type, Union\n+from typing import TYPE_CHECKING, Any, Optional, Union\n \n from scrapy.exceptions import NotConfigured, ScrapyDeprecationWarning\n from scrapy.settings import BaseSettings, Settings\n@@ -35,7 +35,7 @@ if TYPE_CHECKING:\n retry_logger = getLogger(__name__)\n \n \n-def backwards_compatibility_getattr(self: Any, name: str) -> Tuple[Any, ...]:\n+def backwards_compatibility_getattr(self: Any, name: str) -> tuple[Any, ...]:\n     if name == \"EXCEPTIONS_TO_RETRY\":\n         warnings.warn(\n             \"Attribute RetryMiddleware.EXCEPTIONS_TO_RETRY is deprecated. \"\n@@ -60,7 +60,7 @@ def get_retry_request(\n     request: Request,\n     *,\n     spider: Spider,\n-    reason: Union[str, Exception, Type[Exception]] = \"unspecified\",\n+    reason: Union[str, Exception, type[Exception]] = \"unspecified\",\n     max_retry_times: Optional[int] = None,\n     priority_adjust: Optional[int] = None,\n     logger: Logger = retry_logger,\n@@ -187,7 +187,7 @@ class RetryMiddleware(metaclass=BackwardsCompatibilityMetaclass):\n     def _retry(\n         self,\n         request: Request,\n-        reason: Union[str, Exception, Type[Exception]],\n+        reason: Union[str, Exception, type[Exception]],\n         spider: Spider,\n     ) -> Optional[Request]:\n         max_retry_times = request.meta.get(\"max_retry_times\", self.max_retry_times)\n\n@@ -7,7 +7,7 @@ enable this middleware and enable the ROBOTSTXT_OBEY setting.\n from __future__ import annotations\n \n import logging\n-from typing import TYPE_CHECKING, Dict, Optional, TypeVar, Union\n+from typing import TYPE_CHECKING, Optional, TypeVar, Union\n \n from twisted.internet.defer import Deferred, maybeDeferred\n \n@@ -45,7 +45,7 @@ class RobotsTxtMiddleware:\n             \"ROBOTSTXT_USER_AGENT\", None\n         )\n         self.crawler: Crawler = crawler\n-        self._parsers: Dict[\n+        self._parsers: dict[\n             str, Union[RobotParser, Deferred[Optional[RobotParser]], None]\n         ] = {}\n         self._parserimpl: RobotParser = load_object(\n\n@@ -1,6 +1,6 @@\n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Dict, List, Tuple, Union\n+from typing import TYPE_CHECKING, Union\n \n from twisted.web import http\n \n@@ -19,7 +19,7 @@ if TYPE_CHECKING:\n \n \n def get_header_size(\n-    headers: Dict[str, Union[List[Union[str, bytes]], Tuple[Union[str, bytes], ...]]]\n+    headers: dict[str, Union[list[Union[str, bytes]], tuple[Union[str, bytes], ...]]]\n ) -> int:\n     size = 0\n     for key, value in headers.items():\n\n@@ -2,7 +2,7 @@ from __future__ import annotations\n \n import logging\n from pathlib import Path\n-from typing import TYPE_CHECKING, Optional, Set\n+from typing import TYPE_CHECKING, Optional\n \n from scrapy.utils.job import job_dir\n from scrapy.utils.request import (\n@@ -56,7 +56,7 @@ class RFPDupeFilter(BaseDupeFilter):\n         self.fingerprinter: RequestFingerprinterProtocol = (\n             fingerprinter or RequestFingerprinter()\n         )\n-        self.fingerprints: Set[str] = set()\n+        self.fingerprints: set[str] = set()\n         self.logdupes = True\n         self.debug = debug\n         self.logger = logging.getLogger(__name__)\n\n@@ -6,9 +6,10 @@ import csv\n import marshal\n import pickle  # nosec\n import pprint\n+from collections.abc import Callable, Iterable, Mapping\n from io import BytesIO, TextIOWrapper\n from json import JSONEncoder\n-from typing import Any, Callable, Dict, Iterable, Mapping, Optional, Tuple, Union\n+from typing import Any, Optional, Union\n from xml.sax.saxutils import XMLGenerator  # nosec\n from xml.sax.xmlreader import AttributesImpl  # nosec\n \n@@ -32,10 +33,10 @@ __all__ = [\n \n class BaseItemExporter:\n     def __init__(self, *, dont_fail: bool = False, **kwargs: Any):\n-        self._kwargs: Dict[str, Any] = kwargs\n+        self._kwargs: dict[str, Any] = kwargs\n         self._configure(kwargs, dont_fail=dont_fail)\n \n-    def _configure(self, options: Dict[str, Any], dont_fail: bool = False) -> None:\n+    def _configure(self, options: dict[str, Any], dont_fail: bool = False) -> None:\n         \"\"\"Configure the exporter by popping options from the ``options`` dict.\n         If dont_fail is set, it won't raise an exception on unexpected options\n         (useful for using with keyword arguments in subclasses ``__init__`` methods)\n@@ -66,7 +67,7 @@ class BaseItemExporter:\n \n     def _get_serialized_fields(\n         self, item: Any, default_value: Any = None, include_empty: Optional[bool] = None\n-    ) -> Iterable[Tuple[str, Any]]:\n+    ) -> Iterable[tuple[str, Any]]:\n         \"\"\"Return the fields to export as an iterable of tuples\n         (name, serialized_value)\n         \"\"\"\n@@ -339,7 +340,7 @@ class PythonItemExporter(BaseItemExporter):\n     .. _msgpack: https://pypi.org/project/msgpack/\n     \"\"\"\n \n-    def _configure(self, options: Dict[str, Any], dont_fail: bool = False) -> None:\n+    def _configure(self, options: dict[str, Any], dont_fail: bool = False) -> None:\n         super()._configure(options, dont_fail)\n         if not self.encoding:\n             self.encoding = \"utf-8\"\n@@ -363,10 +364,10 @@ class PythonItemExporter(BaseItemExporter):\n             return to_unicode(value, encoding=self.encoding)\n         return value\n \n-    def _serialize_item(self, item: Any) -> Iterable[Tuple[Union[str, bytes], Any]]:\n+    def _serialize_item(self, item: Any) -> Iterable[tuple[Union[str, bytes], Any]]:\n         for key, value in ItemAdapter(item).items():\n             yield key, self._serialize_value(value)\n \n-    def export_item(self, item: Any) -> Dict[Union[str, bytes], Any]:  # type: ignore[override]\n-        result: Dict[Union[str, bytes], Any] = dict(self._get_serialized_fields(item))\n+    def export_item(self, item: Any) -> dict[Union[str, bytes], Any]:  # type: ignore[override]\n+        result: dict[Union[str, bytes], Any] = dict(self._get_serialized_fields(item))\n         return result\n\n@@ -6,7 +6,7 @@ See documentation in docs/topics/extensions.rst\n \n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Any, List\n+from typing import TYPE_CHECKING, Any\n \n from scrapy.middleware import MiddlewareManager\n from scrapy.utils.conf import build_component_list\n@@ -19,5 +19,5 @@ class ExtensionManager(MiddlewareManager):\n     component_name = \"extension\"\n \n     @classmethod\n-    def _get_mwlist_from_settings(cls, settings: Settings) -> List[Any]:\n+    def _get_mwlist_from_settings(cls, settings: Settings) -> list[Any]:\n         return build_component_list(settings.getwithbase(\"EXTENSIONS\"))\n\n@@ -8,7 +8,7 @@ from __future__ import annotations\n \n import logging\n from collections import defaultdict\n-from typing import TYPE_CHECKING, Any, DefaultDict, Dict\n+from typing import TYPE_CHECKING, Any\n \n from scrapy import Request, Spider, signals\n from scrapy.exceptions import NotConfigured\n@@ -30,7 +30,7 @@ class CloseSpider:\n     def __init__(self, crawler: Crawler):\n         self.crawler: Crawler = crawler\n \n-        self.close_on: Dict[str, Any] = {\n+        self.close_on: dict[str, Any] = {\n             \"timeout\": crawler.settings.getfloat(\"CLOSESPIDER_TIMEOUT\"),\n             \"itemcount\": crawler.settings.getint(\"CLOSESPIDER_ITEMCOUNT\"),\n             \"pagecount\": crawler.settings.getint(\"CLOSESPIDER_PAGECOUNT\"),\n@@ -44,7 +44,7 @@ class CloseSpider:\n         if not any(self.close_on.values()):\n             raise NotConfigured\n \n-        self.counter: DefaultDict[str, int] = defaultdict(int)\n+        self.counter: defaultdict[str, int] = defaultdict(int)\n \n         if self.close_on.get(\"errorcount\"):\n             crawler.signals.connect(self.error_count, signal=signals.spider_error)\n\n@@ -10,25 +10,11 @@ import logging\n import re\n import sys\n import warnings\n+from collections.abc import Callable\n from datetime import datetime, timezone\n from pathlib import Path, PureWindowsPath\n from tempfile import NamedTemporaryFile\n-from typing import (\n-    IO,\n-    TYPE_CHECKING,\n-    Any,\n-    Callable,\n-    Dict,\n-    Iterable,\n-    List,\n-    Optional,\n-    Protocol,\n-    Tuple,\n-    Type,\n-    TypeVar,\n-    Union,\n-    cast,\n-)\n+from typing import IO, TYPE_CHECKING, Any, Optional, Protocol, TypeVar, Union, cast\n from urllib.parse import unquote, urlparse\n \n from twisted.internet.defer import Deferred, DeferredList, maybeDeferred\n@@ -50,6 +36,8 @@ from scrapy.utils.misc import build_from_crawler, load_object\n from scrapy.utils.python import without_none_values\n \n if TYPE_CHECKING:\n+    from collections.abc import Iterable\n+\n     from _typeshed import OpenBinaryMode\n     from twisted.python.failure import Failure\n \n@@ -70,7 +58,7 @@ except ImportError:\n \n logger = logging.getLogger(__name__)\n \n-UriParamsCallableT = Callable[[Dict[str, Any], Spider], Optional[Dict[str, Any]]]\n+UriParamsCallableT = Callable[[dict[str, Any], Spider], Optional[dict[str, Any]]]\n \n _StorageT = TypeVar(\"_StorageT\", bound=\"FeedStorageProtocol\")\n \n@@ -79,7 +67,7 @@ def build_storage(\n     builder: Callable[..., _StorageT],\n     uri: str,\n     *args: Any,\n-    feed_options: Optional[Dict[str, Any]] = None,\n+    feed_options: Optional[dict[str, Any]] = None,\n     preargs: Iterable[Any] = (),\n     **kwargs: Any,\n ) -> _StorageT:\n@@ -96,10 +84,10 @@ class ItemFilter:\n     :type feed_options: dict\n     \"\"\"\n \n-    feed_options: Optional[Dict[str, Any]]\n-    item_classes: Tuple[type, ...]\n+    feed_options: Optional[dict[str, Any]]\n+    item_classes: tuple[type, ...]\n \n-    def __init__(self, feed_options: Optional[Dict[str, Any]]) -> None:\n+    def __init__(self, feed_options: Optional[dict[str, Any]]) -> None:\n         self.feed_options = feed_options\n         if feed_options is not None:\n             self.item_classes = tuple(\n@@ -141,7 +129,7 @@ class IFeedStorage(Interface):\n class FeedStorageProtocol(Protocol):\n     \"\"\"Reimplementation of ``IFeedStorage`` that can be used in type hints.\"\"\"\n \n-    def __init__(self, uri: str, *, feed_options: Optional[Dict[str, Any]] = None):\n+    def __init__(self, uri: str, *, feed_options: Optional[dict[str, Any]] = None):\n         \"\"\"Initialize the storage with the parameters given in the URI and the\n         feed-specific options (see :setting:`FEEDS`)\"\"\"\n \n@@ -176,7 +164,7 @@ class StdoutFeedStorage:\n         uri: str,\n         _stdout: Optional[IO[bytes]] = None,\n         *,\n-        feed_options: Optional[Dict[str, Any]] = None,\n+        feed_options: Optional[dict[str, Any]] = None,\n     ):\n         if not _stdout:\n             _stdout = sys.stdout.buffer\n@@ -198,7 +186,7 @@ class StdoutFeedStorage:\n \n @implementer(IFeedStorage)\n class FileFeedStorage:\n-    def __init__(self, uri: str, *, feed_options: Optional[Dict[str, Any]] = None):\n+    def __init__(self, uri: str, *, feed_options: Optional[dict[str, Any]] = None):\n         self.path: str = file_uri_to_path(uri)\n         feed_options = feed_options or {}\n         self.write_mode: OpenBinaryMode = (\n@@ -225,7 +213,7 @@ class S3FeedStorage(BlockingFeedStorage):\n         acl: Optional[str] = None,\n         endpoint_url: Optional[str] = None,\n         *,\n-        feed_options: Optional[Dict[str, Any]] = None,\n+        feed_options: Optional[dict[str, Any]] = None,\n         session_token: Optional[str] = None,\n         region_name: Optional[str] = None,\n     ):\n@@ -291,7 +279,7 @@ class S3FeedStorage(BlockingFeedStorage):\n         crawler: Crawler,\n         uri: str,\n         *,\n-        feed_options: Optional[Dict[str, Any]] = None,\n+        feed_options: Optional[dict[str, Any]] = None,\n     ) -> Self:\n         return build_storage(\n             cls,\n@@ -307,7 +295,7 @@ class S3FeedStorage(BlockingFeedStorage):\n \n     def _store_in_thread(self, file: IO[bytes]) -> None:\n         file.seek(0)\n-        kwargs: Dict[str, Any]\n+        kwargs: dict[str, Any]\n         if IS_BOTO3_AVAILABLE:\n             kwargs = {\"ExtraArgs\": {\"ACL\": self.acl}} if self.acl else {}\n             self.s3_client.upload_fileobj(\n@@ -354,7 +342,7 @@ class FTPFeedStorage(BlockingFeedStorage):\n         uri: str,\n         use_active_mode: bool = False,\n         *,\n-        feed_options: Optional[Dict[str, Any]] = None,\n+        feed_options: Optional[dict[str, Any]] = None,\n     ):\n         u = urlparse(uri)\n         if not u.hostname:\n@@ -373,7 +361,7 @@ class FTPFeedStorage(BlockingFeedStorage):\n         crawler: Crawler,\n         uri: str,\n         *,\n-        feed_options: Optional[Dict[str, Any]] = None,\n+        feed_options: Optional[dict[str, Any]] = None,\n     ) -> Self:\n         return build_storage(\n             cls,\n@@ -405,9 +393,9 @@ class FeedSlot:\n         batch_id: int,\n         uri_template: str,\n         filter: ItemFilter,\n-        feed_options: Dict[str, Any],\n+        feed_options: dict[str, Any],\n         spider: Spider,\n-        exporters: Dict[str, Type[BaseItemExporter]],\n+        exporters: dict[str, type[BaseItemExporter]],\n         settings: BaseSettings,\n         crawler: Crawler,\n     ):\n@@ -422,9 +410,9 @@ class FeedSlot:\n         self.uri: str = uri\n         self.filter: ItemFilter = filter\n         # exporter params\n-        self.feed_options: Dict[str, Any] = feed_options\n+        self.feed_options: dict[str, Any] = feed_options\n         self.spider: Spider = spider\n-        self.exporters: Dict[str, Type[BaseItemExporter]] = exporters\n+        self.exporters: dict[str, type[BaseItemExporter]] = exporters\n         self.settings: BaseSettings = settings\n         self.crawler: Crawler = crawler\n         # flags\n@@ -460,7 +448,7 @@ class FeedSlot:\n             self._exporting = True\n \n     def _get_instance(\n-        self, objcls: Type[BaseItemExporter], *args: Any, **kwargs: Any\n+        self, objcls: type[BaseItemExporter], *args: Any, **kwargs: Any\n     ) -> BaseItemExporter:\n         return build_from_crawler(objcls, self.crawler, *args, **kwargs)\n \n@@ -483,7 +471,7 @@ _FeedSlot = create_deprecated_class(\n \n \n class FeedExporter:\n-    _pending_deferreds: List[Deferred[None]] = []\n+    _pending_deferreds: list[Deferred[None]] = []\n \n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n@@ -497,8 +485,8 @@ class FeedExporter:\n         self.crawler: Crawler = crawler\n         self.settings: Settings = crawler.settings\n         self.feeds = {}\n-        self.slots: List[FeedSlot] = []\n-        self.filters: Dict[str, ItemFilter] = {}\n+        self.slots: list[FeedSlot] = []\n+        self.filters: dict[str, ItemFilter] = {}\n \n         if not self.settings[\"FEEDS\"] and not self.settings[\"FEED_URI\"]:\n             raise NotConfigured\n@@ -530,10 +518,10 @@ class FeedExporter:\n             )\n             self.filters[uri] = self._load_filter(feed_options)\n \n-        self.storages: Dict[str, Type[FeedStorageProtocol]] = self._load_components(\n+        self.storages: dict[str, type[FeedStorageProtocol]] = self._load_components(\n             \"FEED_STORAGES\"\n         )\n-        self.exporters: Dict[str, Type[BaseItemExporter]] = self._load_components(\n+        self.exporters: dict[str, type[BaseItemExporter]] = self._load_components(\n             \"FEED_EXPORTERS\"\n         )\n         for uri, feed_options in self.feeds.items():\n@@ -631,7 +619,7 @@ class FeedExporter:\n         self,\n         batch_id: int,\n         uri: str,\n-        feed_options: Dict[str, Any],\n+        feed_options: dict[str, Any],\n         spider: Spider,\n         uri_template: str,\n     ) -> FeedSlot:\n@@ -696,9 +684,9 @@ class FeedExporter:\n                 slots.append(slot)\n         self.slots = slots\n \n-    def _load_components(self, setting_prefix: str) -> Dict[str, Any]:\n+    def _load_components(self, setting_prefix: str) -> dict[str, Any]:\n         conf = without_none_values(\n-            cast(Dict[str, str], self.settings.getwithbase(setting_prefix))\n+            cast(dict[str, str], self.settings.getwithbase(setting_prefix))\n         )\n         d = {}\n         for k, v in conf.items():\n@@ -732,7 +720,7 @@ class FeedExporter:\n                 return False\n         return True\n \n-    def _storage_supported(self, uri: str, feed_options: Dict[str, Any]) -> bool:\n+    def _storage_supported(self, uri: str, feed_options: dict[str, Any]) -> bool:\n         scheme = urlparse(uri).scheme\n         if scheme in self.storages or PureWindowsPath(uri).drive:\n             try:\n@@ -748,7 +736,7 @@ class FeedExporter:\n         return False\n \n     def _get_storage(\n-        self, uri: str, feed_options: Dict[str, Any]\n+        self, uri: str, feed_options: dict[str, Any]\n     ) -> FeedStorageProtocol:\n         \"\"\"Fork of create_instance specific to feed storage classes\n \n@@ -759,7 +747,7 @@ class FeedExporter:\n         crawler = getattr(self, \"crawler\", None)\n \n         def build_instance(\n-            builder: Type[FeedStorageProtocol], *preargs: Any\n+            builder: type[FeedStorageProtocol], *preargs: Any\n         ) -> FeedStorageProtocol:\n             return build_storage(\n                 builder, uri, feed_options=feed_options, preargs=preargs\n@@ -784,7 +772,7 @@ class FeedExporter:\n         spider: Spider,\n         uri_params_function: Union[str, UriParamsCallableT, None],\n         slot: Optional[FeedSlot] = None,\n-    ) -> Dict[str, Any]:\n+    ) -> dict[str, Any]:\n         params = {}\n         for k in dir(spider):\n             params[k] = getattr(spider, k)\n@@ -800,9 +788,9 @@ class FeedExporter:\n         new_params = uripar_function(params, spider)\n         return new_params if new_params is not None else params\n \n-    def _load_filter(self, feed_options: Dict[str, Any]) -> ItemFilter:\n+    def _load_filter(self, feed_options: dict[str, Any]) -> ItemFilter:\n         # load the item filter if declared else load the default filter class\n-        item_filter_class: Type[ItemFilter] = load_object(\n+        item_filter_class: type[ItemFilter] = load_object(\n             feed_options.get(\"item_filter\", ItemFilter)\n         )\n         return item_filter_class(feed_options)\n\n@@ -9,7 +9,7 @@ from importlib import import_module\n from pathlib import Path\n from time import time\n from types import ModuleType\n-from typing import IO, TYPE_CHECKING, Any, Callable, Dict, List, Optional, Union, cast\n+from typing import IO, TYPE_CHECKING, Any, Optional, Union, cast\n from weakref import WeakKeyDictionary\n \n from w3lib.http import headers_dict_to_raw, headers_raw_to_dict\n@@ -22,6 +22,8 @@ from scrapy.utils.python import to_bytes, to_unicode\n from scrapy.utils.request import RequestFingerprinter\n \n if TYPE_CHECKING:\n+    from collections.abc import Callable\n+\n     # typing.Concatenate requires Python 3.10\n     from typing_extensions import Concatenate\n \n@@ -35,8 +37,8 @@ logger = logging.getLogger(__name__)\n \n class DummyPolicy:\n     def __init__(self, settings: BaseSettings):\n-        self.ignore_schemes: List[str] = settings.getlist(\"HTTPCACHE_IGNORE_SCHEMES\")\n-        self.ignore_http_codes: List[int] = [\n+        self.ignore_schemes: list[str] = settings.getlist(\"HTTPCACHE_IGNORE_SCHEMES\")\n+        self.ignore_http_codes: list[int] = [\n             int(x) for x in settings.getlist(\"HTTPCACHE_IGNORE_HTTP_CODES\")\n         ]\n \n@@ -62,18 +64,18 @@ class RFC2616Policy:\n \n     def __init__(self, settings: BaseSettings):\n         self.always_store: bool = settings.getbool(\"HTTPCACHE_ALWAYS_STORE\")\n-        self.ignore_schemes: List[str] = settings.getlist(\"HTTPCACHE_IGNORE_SCHEMES\")\n+        self.ignore_schemes: list[str] = settings.getlist(\"HTTPCACHE_IGNORE_SCHEMES\")\n         self._cc_parsed: WeakKeyDictionary[\n-            Union[Request, Response], Dict[bytes, Optional[bytes]]\n+            Union[Request, Response], dict[bytes, Optional[bytes]]\n         ] = WeakKeyDictionary()\n-        self.ignore_response_cache_controls: List[bytes] = [\n+        self.ignore_response_cache_controls: list[bytes] = [\n             to_bytes(cc)\n             for cc in settings.getlist(\"HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS\")\n         ]\n \n     def _parse_cachecontrol(\n         self, r: Union[Request, Response]\n-    ) -> Dict[bytes, Optional[bytes]]:\n+    ) -> dict[bytes, Optional[bytes]]:\n         if r not in self._cc_parsed:\n             cch = r.headers.get(b\"Cache-Control\", b\"\")\n             assert cch is not None\n@@ -189,7 +191,7 @@ class RFC2616Policy:\n         if b\"ETag\" in cachedresponse.headers:\n             request.headers[b\"If-None-Match\"] = cachedresponse.headers[b\"ETag\"]\n \n-    def _get_max_age(self, cc: Dict[bytes, Optional[bytes]]) -> Optional[int]:\n+    def _get_max_age(self, cc: dict[bytes, Optional[bytes]]) -> Optional[int]:\n         try:\n             return max(0, int(cc[b\"max-age\"]))  # type: ignore[arg-type]\n         except (KeyError, ValueError):\n@@ -298,7 +300,7 @@ class DbmCacheStorage:\n         self.db[f\"{key}_data\"] = pickle.dumps(data, protocol=4)\n         self.db[f\"{key}_time\"] = str(time())\n \n-    def _read_data(self, spider: Spider, request: Request) -> Optional[Dict[str, Any]]:\n+    def _read_data(self, spider: Spider, request: Request) -> Optional[dict[str, Any]]:\n         key = self._fingerprinter.fingerprint(request).hex()\n         db = self.db\n         tkey = f\"{key}_time\"\n@@ -309,7 +311,7 @@ class DbmCacheStorage:\n         if 0 < self.expiration_secs < time() - float(ts):\n             return None  # expired\n \n-        return cast(Dict[str, Any], pickle.loads(db[f\"{key}_data\"]))  # nosec\n+        return cast(dict[str, Any], pickle.loads(db[f\"{key}_data\"]))  # nosec\n \n \n class FilesystemCacheStorage:\n@@ -385,7 +387,7 @@ class FilesystemCacheStorage:\n         key = self._fingerprinter.fingerprint(request).hex()\n         return str(Path(self.cachedir, spider.name, key[0:2], key))\n \n-    def _read_meta(self, spider: Spider, request: Request) -> Optional[Dict[str, Any]]:\n+    def _read_meta(self, spider: Spider, request: Request) -> Optional[dict[str, Any]]:\n         rpath = Path(self._get_request_path(spider, request))\n         metapath = rpath / \"pickled_meta\"\n         if not metapath.exists():\n@@ -394,10 +396,10 @@ class FilesystemCacheStorage:\n         if 0 < self.expiration_secs < time() - mtime:\n             return None  # expired\n         with self._open(metapath, \"rb\") as f:\n-            return cast(Dict[str, Any], pickle.load(f))  # nosec\n+            return cast(dict[str, Any], pickle.load(f))  # nosec\n \n \n-def parse_cachecontrol(header: bytes) -> Dict[bytes, Optional[bytes]]:\n+def parse_cachecontrol(header: bytes) -> dict[bytes, Optional[bytes]]:\n     \"\"\"Parse Cache-Control header\n \n     https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9\n\n@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n import logging\n-from typing import TYPE_CHECKING, Optional, Tuple, Union\n+from typing import TYPE_CHECKING, Optional, Union\n \n from twisted.internet import task\n \n@@ -81,7 +81,7 @@ class LogStats:\n \n     def calculate_final_stats(\n         self, spider: Spider\n-    ) -> Union[Tuple[None, None], Tuple[float, float]]:\n+    ) -> Union[tuple[None, None], tuple[float, float]]:\n         start_time = self.stats.get_value(\"start_time\")\n         finished_time = self.stats.get_value(\"finished_time\")\n \n\n@@ -11,7 +11,7 @@ import socket\n import sys\n from importlib import import_module\n from pprint import pformat\n-from typing import TYPE_CHECKING, List\n+from typing import TYPE_CHECKING\n \n from twisted.internet import task\n \n@@ -42,7 +42,7 @@ class MemoryUsage:\n \n         self.crawler: Crawler = crawler\n         self.warned: bool = False\n-        self.notify_mails: List[str] = crawler.settings.getlist(\"MEMUSAGE_NOTIFY_MAIL\")\n+        self.notify_mails: list[str] = crawler.settings.getlist(\"MEMUSAGE_NOTIFY_MAIL\")\n         self.limit: int = crawler.settings.getint(\"MEMUSAGE_LIMIT_MB\") * 1024 * 1024\n         self.warning: int = crawler.settings.getint(\"MEMUSAGE_WARNING_MB\") * 1024 * 1024\n         self.check_interval: float = crawler.settings.getfloat(\n@@ -66,7 +66,7 @@ class MemoryUsage:\n     def engine_started(self) -> None:\n         assert self.crawler.stats\n         self.crawler.stats.set_value(\"memusage/startup\", self.get_virtual_size())\n-        self.tasks: List[task.LoopingCall] = []\n+        self.tasks: list[task.LoopingCall] = []\n         tsk = task.LoopingCall(self.update)\n         self.tasks.append(tsk)\n         tsk.start(self.check_interval, now=True)\n@@ -141,7 +141,7 @@ class MemoryUsage:\n                 self.crawler.stats.set_value(\"memusage/warning_notified\", 1)\n             self.warned = True\n \n-    def _send_report(self, rcpts: List[str], subject: str) -> None:\n+    def _send_report(self, rcpts: list[str], subject: str) -> None:\n         \"\"\"send notification mail with some additional useful info\"\"\"\n         assert self.crawler.engine\n         assert self.crawler.stats\n\n@@ -3,7 +3,7 @@ from __future__ import annotations\n import logging\n from datetime import datetime, timezone\n from json import JSONEncoder\n-from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union\n+from typing import TYPE_CHECKING, Any, Optional, Union\n \n from twisted.internet import task\n \n@@ -29,8 +29,8 @@ class PeriodicLog:\n         self,\n         stats: StatsCollector,\n         interval: float = 60.0,\n-        ext_stats: Dict[str, Any] = {},\n-        ext_delta: Dict[str, Any] = {},\n+        ext_stats: dict[str, Any] = {},\n+        ext_delta: dict[str, Any] = {},\n         ext_timing_enabled: bool = False,\n     ):\n         self.stats: StatsCollector = stats\n@@ -39,11 +39,11 @@ class PeriodicLog:\n         self.task: Optional[task.LoopingCall] = None\n         self.encoder: JSONEncoder = ScrapyJSONEncoder(sort_keys=True, indent=4)\n         self.ext_stats_enabled: bool = bool(ext_stats)\n-        self.ext_stats_include: List[str] = ext_stats.get(\"include\", [])\n-        self.ext_stats_exclude: List[str] = ext_stats.get(\"exclude\", [])\n+        self.ext_stats_include: list[str] = ext_stats.get(\"include\", [])\n+        self.ext_stats_exclude: list[str] = ext_stats.get(\"exclude\", [])\n         self.ext_delta_enabled: bool = bool(ext_delta)\n-        self.ext_delta_include: List[str] = ext_delta.get(\"include\", [])\n-        self.ext_delta_exclude: List[str] = ext_delta.get(\"exclude\", [])\n+        self.ext_delta_include: list[str] = ext_delta.get(\"include\", [])\n+        self.ext_delta_exclude: list[str] = ext_delta.get(\"exclude\", [])\n         self.ext_timing_enabled: bool = ext_timing_enabled\n \n     @classmethod\n@@ -52,7 +52,7 @@ class PeriodicLog:\n         if not interval:\n             raise NotConfigured\n         try:\n-            ext_stats: Optional[Dict[str, Any]] = crawler.settings.getdict(\n+            ext_stats: Optional[dict[str, Any]] = crawler.settings.getdict(\n                 \"PERIODIC_LOG_STATS\"\n             )\n         except (TypeError, ValueError):\n@@ -62,7 +62,7 @@ class PeriodicLog:\n                 else None\n             )\n         try:\n-            ext_delta: Optional[Dict[str, Any]] = crawler.settings.getdict(\n+            ext_delta: Optional[dict[str, Any]] = crawler.settings.getdict(\n                 \"PERIODIC_LOG_DELTA\"\n             )\n         except (TypeError, ValueError):\n@@ -93,14 +93,14 @@ class PeriodicLog:\n \n     def spider_opened(self, spider: Spider) -> None:\n         self.time_prev: datetime = datetime.now(tz=timezone.utc)\n-        self.delta_prev: Dict[str, Union[int, float]] = {}\n-        self.stats_prev: Dict[str, Union[int, float]] = {}\n+        self.delta_prev: dict[str, Union[int, float]] = {}\n+        self.stats_prev: dict[str, Union[int, float]] = {}\n \n         self.task = task.LoopingCall(self.log)\n         self.task.start(self.interval)\n \n     def log(self) -> None:\n-        data: Dict[str, Any] = {}\n+        data: dict[str, Any] = {}\n         if self.ext_timing_enabled:\n             data.update(self.log_timing())\n         if self.ext_delta_enabled:\n@@ -109,8 +109,8 @@ class PeriodicLog:\n             data.update(self.log_crawler_stats())\n         logger.info(self.encoder.encode(data))\n \n-    def log_delta(self) -> Dict[str, Any]:\n-        num_stats: Dict[str, Union[int, float]] = {\n+    def log_delta(self) -> dict[str, Any]:\n+        num_stats: dict[str, Union[int, float]] = {\n             k: v\n             for k, v in self.stats._stats.items()\n             if isinstance(v, (int, float))\n@@ -120,7 +120,7 @@ class PeriodicLog:\n         self.delta_prev = num_stats\n         return {\"delta\": delta}\n \n-    def log_timing(self) -> Dict[str, Any]:\n+    def log_timing(self) -> dict[str, Any]:\n         now = datetime.now(tz=timezone.utc)\n         time = {\n             \"log_interval\": self.interval,\n@@ -132,7 +132,7 @@ class PeriodicLog:\n         self.time_prev = now\n         return {\"time\": time}\n \n-    def log_crawler_stats(self) -> Dict[str, Any]:\n+    def log_crawler_stats(self) -> dict[str, Any]:\n         stats = {\n             k: v\n             for k, v in self.stats._stats.items()\n@@ -141,7 +141,7 @@ class PeriodicLog:\n         return {\"stats\": stats}\n \n     def param_allowed(\n-        self, stat_name: str, include: List[str], exclude: List[str]\n+        self, stat_name: str, include: list[str], exclude: list[str]\n     ) -> bool:\n         if not include and not exclude:\n             return True\n\n@@ -6,7 +6,7 @@ from bz2 import BZ2File\n from gzip import GzipFile\n from io import IOBase\n from lzma import LZMAFile\n-from typing import IO, Any, BinaryIO, Dict, List, cast\n+from typing import IO, Any, BinaryIO, cast\n \n from scrapy.utils.misc import load_object\n \n@@ -24,7 +24,7 @@ class GzipPlugin:\n     See :py:class:`gzip.GzipFile` for more info about parameters.\n     \"\"\"\n \n-    def __init__(self, file: BinaryIO, feed_options: Dict[str, Any]) -> None:\n+    def __init__(self, file: BinaryIO, feed_options: dict[str, Any]) -> None:\n         self.file = file\n         self.feed_options = feed_options\n         compress_level = self.feed_options.get(\"gzip_compresslevel\", 9)\n@@ -56,7 +56,7 @@ class Bz2Plugin:\n     See :py:class:`bz2.BZ2File` for more info about parameters.\n     \"\"\"\n \n-    def __init__(self, file: BinaryIO, feed_options: Dict[str, Any]) -> None:\n+    def __init__(self, file: BinaryIO, feed_options: dict[str, Any]) -> None:\n         self.file = file\n         self.feed_options = feed_options\n         compress_level = self.feed_options.get(\"bz2_compresslevel\", 9)\n@@ -88,7 +88,7 @@ class LZMAPlugin:\n     See :py:class:`lzma.LZMAFile` for more info about parameters.\n     \"\"\"\n \n-    def __init__(self, file: BinaryIO, feed_options: Dict[str, Any]) -> None:\n+    def __init__(self, file: BinaryIO, feed_options: dict[str, Any]) -> None:\n         self.file = file\n         self.feed_options = feed_options\n \n@@ -126,7 +126,7 @@ class PostProcessingManager(IOBase):\n     \"\"\"\n \n     def __init__(\n-        self, plugins: List[Any], file: IO[bytes], feed_options: Dict[str, Any]\n+        self, plugins: list[Any], file: IO[bytes], feed_options: dict[str, Any]\n     ) -> None:\n         self.plugins = self._load_plugins(plugins)\n         self.file = file\n@@ -156,7 +156,7 @@ class PostProcessingManager(IOBase):\n     def writable(self) -> bool:\n         return True\n \n-    def _load_plugins(self, plugins: List[Any]) -> List[Any]:\n+    def _load_plugins(self, plugins: list[Any]) -> list[Any]:\n         plugins = [load_object(plugin) for plugin in plugins]\n         return plugins\n \n\n@@ -6,7 +6,7 @@ Use STATSMAILER_RCPTS setting to enable and give the recipient mail address\n \n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, List, Optional\n+from typing import TYPE_CHECKING, Optional\n \n from scrapy import Spider, signals\n from scrapy.exceptions import NotConfigured\n@@ -23,14 +23,14 @@ if TYPE_CHECKING:\n \n \n class StatsMailer:\n-    def __init__(self, stats: StatsCollector, recipients: List[str], mail: MailSender):\n+    def __init__(self, stats: StatsCollector, recipients: list[str], mail: MailSender):\n         self.stats: StatsCollector = stats\n-        self.recipients: List[str] = recipients\n+        self.recipients: list[str] = recipients\n         self.mail: MailSender = mail\n \n     @classmethod\n     def from_crawler(cls, crawler: Crawler) -> Self:\n-        recipients: List[str] = crawler.settings.getlist(\"STATSMAILER_RCPTS\")\n+        recipients: list[str] = crawler.settings.getlist(\"STATSMAILER_RCPTS\")\n         if not recipients:\n             raise NotConfigured\n         mail: MailSender = MailSender.from_settings(crawler.settings)\n\n@@ -10,7 +10,7 @@ import binascii\n import logging\n import os\n import pprint\n-from typing import TYPE_CHECKING, Any, Dict, List\n+from typing import TYPE_CHECKING, Any\n \n from twisted.internet import protocol\n from twisted.internet.tcp import Port\n@@ -45,7 +45,7 @@ class TelnetConsole(protocol.ServerFactory):\n \n         self.crawler: Crawler = crawler\n         self.noisy: bool = False\n-        self.portrange: List[int] = [\n+        self.portrange: list[int] = [\n             int(x) for x in crawler.settings.getlist(\"TELNETCONSOLE_PORT\")\n         ]\n         self.host: str = crawler.settings[\"TELNETCONSOLE_HOST\"]\n@@ -98,10 +98,10 @@ class TelnetConsole(protocol.ServerFactory):\n \n         return telnet.TelnetTransport(telnet.AuthenticatingTelnetProtocol, Portal())\n \n-    def _get_telnet_vars(self) -> Dict[str, Any]:\n+    def _get_telnet_vars(self) -> dict[str, Any]:\n         # Note: if you add entries here also update topics/telnetconsole.rst\n         assert self.crawler.engine\n-        telnet_vars: Dict[str, Any] = {\n+        telnet_vars: dict[str, Any] = {\n             \"engine\": self.crawler.engine,\n             \"spider\": self.crawler.engine.spider,\n             \"slot\": self.crawler.engine.slot,\n\n@@ -1,7 +1,7 @@\n from __future__ import annotations\n \n import logging\n-from typing import TYPE_CHECKING, Optional, Tuple\n+from typing import TYPE_CHECKING, Optional\n \n from scrapy import Request, Spider, signals\n from scrapy.exceptions import NotConfigured\n@@ -90,7 +90,7 @@ class AutoThrottle:\n \n     def _get_slot(\n         self, request: Request, spider: Spider\n-    ) -> Tuple[Optional[str], Optional[Slot]]:\n+    ) -> tuple[Optional[str], Optional[Slot]]:\n         key: Optional[str] = request.meta.get(\"download_slot\")\n         if key is None:\n             return None, None\n\n@@ -5,22 +5,14 @@ import time\n from http.cookiejar import Cookie\n from http.cookiejar import CookieJar as _CookieJar\n from http.cookiejar import CookiePolicy, DefaultCookiePolicy\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    Dict,\n-    Iterator,\n-    List,\n-    Optional,\n-    Sequence,\n-    Tuple,\n-    cast,\n-)\n+from typing import TYPE_CHECKING, Any, Optional, cast\n \n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_unicode\n \n if TYPE_CHECKING:\n+    from collections.abc import Iterator, Sequence\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n@@ -83,7 +75,7 @@ class CookieJar:\n             self.jar.clear_expired_cookies()\n \n     @property\n-    def _cookies(self) -> Dict[str, Dict[str, Dict[str, Cookie]]]:\n+    def _cookies(self) -> dict[str, dict[str, dict[str, Cookie]]]:\n         return self.jar._cookies  # type: ignore[attr-defined,no-any-return]\n \n     def clear_session_cookies(self) -> None:\n@@ -118,7 +110,7 @@ class CookieJar:\n         self.jar.set_cookie_if_ok(cookie, WrappedRequest(request))  # type: ignore[arg-type]\n \n \n-def potential_domain_matches(domain: str) -> List[str]:\n+def potential_domain_matches(domain: str) -> list[str]:\n     \"\"\"Potential domain matches for a cookie\n \n     >>> potential_domain_matches('www.example.com')\n@@ -200,7 +192,7 @@ class WrappedRequest:\n         value = self.request.headers.get(name, default)\n         return to_unicode(value, errors=\"replace\") if value is not None else None\n \n-    def header_items(self) -> List[Tuple[str, List[str]]]:\n+    def header_items(self) -> list[tuple[str, list[str]]]:\n         return [\n             (\n                 to_unicode(k, errors=\"replace\"),\n@@ -220,7 +212,7 @@ class WrappedResponse:\n     def info(self) -> Self:\n         return self\n \n-    def get_all(self, name: str, default: Any = None) -> List[str]:\n+    def get_all(self, name: str, default: Any = None) -> list[str]:\n         return [\n             to_unicode(v, errors=\"replace\") for v in self.response.headers.getlist(name)\n         ]\n\n@@ -1,18 +1,7 @@\n from __future__ import annotations\n \n from collections.abc import Mapping\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    AnyStr,\n-    Dict,\n-    Iterable,\n-    List,\n-    Optional,\n-    Tuple,\n-    Union,\n-    cast,\n-)\n+from typing import TYPE_CHECKING, Any, AnyStr, Optional, Union, cast\n \n from w3lib.http import headers_dict_to_raw\n \n@@ -20,6 +9,8 @@ from scrapy.utils.datatypes import CaseInsensitiveDict, CaselessDict\n from scrapy.utils.python import to_unicode\n \n if TYPE_CHECKING:\n+    from collections.abc import Iterable\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n@@ -34,17 +25,17 @@ class Headers(CaselessDict):\n \n     def __init__(\n         self,\n-        seq: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n+        seq: Union[Mapping[AnyStr, Any], Iterable[tuple[AnyStr, Any]], None] = None,\n         encoding: str = \"utf-8\",\n     ):\n         self.encoding: str = encoding\n         super().__init__(seq)\n \n     def update(  # type: ignore[override]\n-        self, seq: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]]]\n+        self, seq: Union[Mapping[AnyStr, Any], Iterable[tuple[AnyStr, Any]]]\n     ) -> None:\n         seq = seq.items() if isinstance(seq, Mapping) else seq\n-        iseq: Dict[bytes, List[bytes]] = {}\n+        iseq: dict[bytes, list[bytes]] = {}\n         for k, v in seq:\n             iseq.setdefault(self.normkey(k), []).extend(self.normvalue(v))\n         super().update(iseq)\n@@ -53,7 +44,7 @@ class Headers(CaselessDict):\n         \"\"\"Normalize key to bytes\"\"\"\n         return self._tobytes(key.title())\n \n-    def normvalue(self, value: Union[_RawValueT, Iterable[_RawValueT]]) -> List[bytes]:\n+    def normvalue(self, value: Union[_RawValueT, Iterable[_RawValueT]]) -> list[bytes]:\n         \"\"\"Normalize values to bytes\"\"\"\n         _value: Iterable[_RawValueT]\n         if value is None:\n@@ -78,19 +69,19 @@ class Headers(CaselessDict):\n \n     def __getitem__(self, key: AnyStr) -> Optional[bytes]:\n         try:\n-            return cast(List[bytes], super().__getitem__(key))[-1]\n+            return cast(list[bytes], super().__getitem__(key))[-1]\n         except IndexError:\n             return None\n \n     def get(self, key: AnyStr, def_val: Any = None) -> Optional[bytes]:\n         try:\n-            return cast(List[bytes], super().get(key, def_val))[-1]\n+            return cast(list[bytes], super().get(key, def_val))[-1]\n         except IndexError:\n             return None\n \n-    def getlist(self, key: AnyStr, def_val: Any = None) -> List[bytes]:\n+    def getlist(self, key: AnyStr, def_val: Any = None) -> list[bytes]:\n         try:\n-            return cast(List[bytes], super().__getitem__(key))\n+            return cast(list[bytes], super().__getitem__(key))\n         except KeyError:\n             if def_val is not None:\n                 return self.normvalue(def_val)\n@@ -109,10 +100,10 @@ class Headers(CaselessDict):\n         lst.extend(self.normvalue(value))\n         self[key] = lst\n \n-    def items(self) -> Iterable[Tuple[bytes, List[bytes]]]:  # type: ignore[override]\n+    def items(self) -> Iterable[tuple[bytes, list[bytes]]]:  # type: ignore[override]\n         return ((k, self.getlist(k)) for k in self.keys())\n \n-    def values(self) -> List[Optional[bytes]]:  # type: ignore[override]\n+    def values(self) -> list[Optional[bytes]]:  # type: ignore[override]\n         return [\n             self[k] for k in self.keys()  # pylint: disable=consider-using-dict-items\n         ]\n\n@@ -12,14 +12,8 @@ from typing import (\n     TYPE_CHECKING,\n     Any,\n     AnyStr,\n-    Dict,\n-    Iterable,\n-    List,\n-    Mapping,\n     NoReturn,\n     Optional,\n-    Tuple,\n-    Type,\n     TypedDict,\n     TypeVar,\n     Union,\n@@ -36,7 +30,7 @@ from scrapy.utils.trackref import object_ref\n from scrapy.utils.url import escape_ajax\n \n if TYPE_CHECKING:\n-    from collections.abc import Callable\n+    from collections.abc import Callable, Iterable, Mapping\n \n     from twisted.python.failure import Failure\n \n@@ -57,7 +51,7 @@ class VerboseCookie(TypedDict):\n     secure: NotRequired[bool]\n \n \n-CookiesT = Union[Dict[str, str], List[VerboseCookie]]\n+CookiesT = Union[dict[str, str], list[VerboseCookie]]\n \n \n RequestTypeVar = TypeVar(\"RequestTypeVar\", bound=\"Request\")\n@@ -92,7 +86,7 @@ class Request(object_ref):\n     executed by the Downloader, thus generating a :class:`Response`.\n     \"\"\"\n \n-    attributes: Tuple[str, ...] = (\n+    attributes: tuple[str, ...] = (\n         \"url\",\n         \"callback\",\n         \"method\",\n@@ -120,16 +114,16 @@ class Request(object_ref):\n         url: str,\n         callback: Optional[CallbackT] = None,\n         method: str = \"GET\",\n-        headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n+        headers: Union[Mapping[AnyStr, Any], Iterable[tuple[AnyStr, Any]], None] = None,\n         body: Optional[Union[bytes, str]] = None,\n         cookies: Optional[CookiesT] = None,\n-        meta: Optional[Dict[str, Any]] = None,\n+        meta: Optional[dict[str, Any]] = None,\n         encoding: str = \"utf-8\",\n         priority: int = 0,\n         dont_filter: bool = False,\n         errback: Optional[Callable[[Failure], Any]] = None,\n-        flags: Optional[List[str]] = None,\n-        cb_kwargs: Optional[Dict[str, Any]] = None,\n+        flags: Optional[list[str]] = None,\n+        cb_kwargs: Optional[dict[str, Any]] = None,\n     ) -> None:\n         self._encoding: str = encoding  # this one has to be set first\n         self.method: str = str(method).upper()\n@@ -152,20 +146,20 @@ class Request(object_ref):\n         self.headers: Headers = Headers(headers or {}, encoding=encoding)\n         self.dont_filter: bool = dont_filter\n \n-        self._meta: Optional[Dict[str, Any]] = dict(meta) if meta else None\n-        self._cb_kwargs: Optional[Dict[str, Any]] = (\n+        self._meta: Optional[dict[str, Any]] = dict(meta) if meta else None\n+        self._cb_kwargs: Optional[dict[str, Any]] = (\n             dict(cb_kwargs) if cb_kwargs else None\n         )\n-        self.flags: List[str] = [] if flags is None else list(flags)\n+        self.flags: list[str] = [] if flags is None else list(flags)\n \n     @property\n-    def cb_kwargs(self) -> Dict[str, Any]:\n+    def cb_kwargs(self) -> dict[str, Any]:\n         if self._cb_kwargs is None:\n             self._cb_kwargs = {}\n         return self._cb_kwargs\n \n     @property\n-    def meta(self) -> Dict[str, Any]:\n+    def meta(self) -> dict[str, Any]:\n         if self._meta is None:\n             self._meta = {}\n         return self._meta\n@@ -207,14 +201,14 @@ class Request(object_ref):\n \n     @overload\n     def replace(\n-        self, *args: Any, cls: Type[RequestTypeVar], **kwargs: Any\n+        self, *args: Any, cls: type[RequestTypeVar], **kwargs: Any\n     ) -> RequestTypeVar: ...\n \n     @overload\n     def replace(self, *args: Any, cls: None = None, **kwargs: Any) -> Self: ...\n \n     def replace(\n-        self, *args: Any, cls: Optional[Type[Request]] = None, **kwargs: Any\n+        self, *args: Any, cls: Optional[type[Request]] = None, **kwargs: Any\n     ) -> Request:\n         \"\"\"Create a new Request with the same attributes except for those given new values\"\"\"\n         for x in self.attributes:\n@@ -261,7 +255,7 @@ class Request(object_ref):\n         request_kwargs.update(kwargs)\n         return cls(**request_kwargs)\n \n-    def to_dict(self, *, spider: Optional[scrapy.Spider] = None) -> Dict[str, Any]:\n+    def to_dict(self, *, spider: Optional[scrapy.Spider] = None) -> dict[str, Any]:\n         \"\"\"Return a dictionary containing the Request's data.\n \n         Use :func:`~scrapy.utils.request.request_from_dict` to convert back into a :class:`~scrapy.Request` object.\n\n@@ -7,17 +7,8 @@ See documentation in docs/topics/request-response.rst\n \n from __future__ import annotations\n \n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    Dict,\n-    Iterable,\n-    List,\n-    Optional,\n-    Tuple,\n-    Union,\n-    cast,\n-)\n+from collections.abc import Iterable\n+from typing import TYPE_CHECKING, Any, Optional, Union, cast\n from urllib.parse import urlencode, urljoin, urlsplit, urlunsplit\n \n from lxml.html import FormElement  # nosec\n@@ -31,6 +22,7 @@ from scrapy.http.request import Request\n from scrapy.utils.python import is_listlike, to_bytes\n \n if TYPE_CHECKING:\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n@@ -38,8 +30,8 @@ if TYPE_CHECKING:\n \n \n FormdataVType = Union[str, Iterable[str]]\n-FormdataKVType = Tuple[str, FormdataVType]\n-FormdataType = Optional[Union[Dict[str, FormdataVType], List[FormdataKVType]]]\n+FormdataKVType = tuple[str, FormdataVType]\n+FormdataType = Optional[Union[dict[str, FormdataVType], list[FormdataKVType]]]\n \n \n class FormRequest(Request):\n@@ -74,7 +66,7 @@ class FormRequest(Request):\n         formid: Optional[str] = None,\n         formnumber: int = 0,\n         formdata: FormdataType = None,\n-        clickdata: Optional[Dict[str, Union[str, int]]] = None,\n+        clickdata: Optional[dict[str, Union[str, int]]] = None,\n         dont_click: bool = False,\n         formxpath: Optional[str] = None,\n         formcss: Optional[str] = None,\n@@ -168,8 +160,8 @@ def _get_inputs(\n     form: FormElement,\n     formdata: FormdataType,\n     dont_click: bool,\n-    clickdata: Optional[Dict[str, Union[str, int]]],\n-) -> List[FormdataKVType]:\n+    clickdata: Optional[dict[str, Union[str, int]]],\n+) -> list[FormdataKVType]:\n     \"\"\"Return a list of key-value pairs for the inputs found in the given form.\"\"\"\n     try:\n         formdata_keys = dict(formdata or ()).keys()\n@@ -187,7 +179,7 @@ def _get_inputs(\n         '  not(re:test(., \"^(?:checkbox|radio)$\", \"i\")))]]',\n         namespaces={\"re\": \"http://exslt.org/regular-expressions\"},\n     )\n-    values: List[FormdataKVType] = [\n+    values: list[FormdataKVType] = [\n         (k, \"\" if v is None else v)\n         for k, v in (_value(e) for e in inputs)\n         if k and k not in formdata_keys\n@@ -205,7 +197,7 @@ def _get_inputs(\n \n def _value(\n     ele: Union[InputElement, SelectElement, TextareaElement]\n-) -> Tuple[Optional[str], Union[None, str, MultipleSelectOptions]]:\n+) -> tuple[Optional[str], Union[None, str, MultipleSelectOptions]]:\n     n = ele.name\n     v = ele.value\n     if ele.tag == \"select\":\n@@ -215,7 +207,7 @@ def _value(\n \n def _select_value(\n     ele: SelectElement, n: Optional[str], v: Union[None, str, MultipleSelectOptions]\n-) -> Tuple[Optional[str], Union[None, str, MultipleSelectOptions]]:\n+) -> tuple[Optional[str], Union[None, str, MultipleSelectOptions]]:\n     multiple = ele.multiple\n     if v is None and not multiple:\n         # Match browser behaviour on simple select tag without options selected\n@@ -226,8 +218,8 @@ def _select_value(\n \n \n def _get_clickable(\n-    clickdata: Optional[Dict[str, Union[str, int]]], form: FormElement\n-) -> Optional[Tuple[str, str]]:\n+    clickdata: Optional[dict[str, Union[str, int]]], form: FormElement\n+) -> Optional[tuple[str, str]]:\n     \"\"\"\n     Returns the clickable element specified in clickdata,\n     if the latter is given. If not, it returns the first\n\n@@ -10,7 +10,7 @@ from __future__ import annotations\n import copy\n import json\n import warnings\n-from typing import TYPE_CHECKING, Any, Dict, Optional, Tuple, Type, overload\n+from typing import TYPE_CHECKING, Any, Optional, overload\n \n from scrapy.http.request import Request, RequestTypeVar\n \n@@ -20,14 +20,14 @@ if TYPE_CHECKING:\n \n \n class JsonRequest(Request):\n-    attributes: Tuple[str, ...] = Request.attributes + (\"dumps_kwargs\",)\n+    attributes: tuple[str, ...] = Request.attributes + (\"dumps_kwargs\",)\n \n     def __init__(\n-        self, *args: Any, dumps_kwargs: Optional[Dict[str, Any]] = None, **kwargs: Any\n+        self, *args: Any, dumps_kwargs: Optional[dict[str, Any]] = None, **kwargs: Any\n     ) -> None:\n         dumps_kwargs = copy.deepcopy(dumps_kwargs) if dumps_kwargs is not None else {}\n         dumps_kwargs.setdefault(\"sort_keys\", True)\n-        self._dumps_kwargs: Dict[str, Any] = dumps_kwargs\n+        self._dumps_kwargs: dict[str, Any] = dumps_kwargs\n \n         body_passed = kwargs.get(\"body\", None) is not None\n         data: Any = kwargs.pop(\"data\", None)\n@@ -47,19 +47,19 @@ class JsonRequest(Request):\n         )\n \n     @property\n-    def dumps_kwargs(self) -> Dict[str, Any]:\n+    def dumps_kwargs(self) -> dict[str, Any]:\n         return self._dumps_kwargs\n \n     @overload\n     def replace(\n-        self, *args: Any, cls: Type[RequestTypeVar], **kwargs: Any\n+        self, *args: Any, cls: type[RequestTypeVar], **kwargs: Any\n     ) -> RequestTypeVar: ...\n \n     @overload\n     def replace(self, *args: Any, cls: None = None, **kwargs: Any) -> Self: ...\n \n     def replace(\n-        self, *args: Any, cls: Optional[Type[Request]] = None, **kwargs: Any\n+        self, *args: Any, cls: Optional[type[Request]] = None, **kwargs: Any\n     ) -> Request:\n         body_passed = kwargs.get(\"body\", None) is not None\n         data: Any = kwargs.pop(\"data\", None)\n\n@@ -7,22 +7,7 @@ See documentation in docs/topics/request-response.rst\n \n from __future__ import annotations\n \n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    AnyStr,\n-    Callable,\n-    Dict,\n-    Iterable,\n-    List,\n-    Mapping,\n-    Optional,\n-    Tuple,\n-    Type,\n-    TypeVar,\n-    Union,\n-    overload,\n-)\n+from typing import TYPE_CHECKING, Any, AnyStr, Optional, TypeVar, Union, overload\n from urllib.parse import urljoin\n \n from scrapy.exceptions import NotSupported\n@@ -32,6 +17,7 @@ from scrapy.link import Link\n from scrapy.utils.trackref import object_ref\n \n if TYPE_CHECKING:\n+    from collections.abc import Callable, Iterable, Mapping\n     from ipaddress import IPv4Address, IPv6Address\n \n     from twisted.internet.ssl import Certificate\n@@ -52,7 +38,7 @@ class Response(object_ref):\n     downloaded (by the Downloader) and fed to the Spiders for processing.\n     \"\"\"\n \n-    attributes: Tuple[str, ...] = (\n+    attributes: tuple[str, ...] = (\n         \"url\",\n         \"status\",\n         \"headers\",\n@@ -74,9 +60,9 @@ class Response(object_ref):\n         self,\n         url: str,\n         status: int = 200,\n-        headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n+        headers: Union[Mapping[AnyStr, Any], Iterable[tuple[AnyStr, Any]], None] = None,\n         body: bytes = b\"\",\n-        flags: Optional[List[str]] = None,\n+        flags: Optional[list[str]] = None,\n         request: Optional[Request] = None,\n         certificate: Optional[Certificate] = None,\n         ip_address: Union[IPv4Address, IPv6Address, None] = None,\n@@ -87,13 +73,13 @@ class Response(object_ref):\n         self._set_body(body)\n         self._set_url(url)\n         self.request: Optional[Request] = request\n-        self.flags: List[str] = [] if flags is None else list(flags)\n+        self.flags: list[str] = [] if flags is None else list(flags)\n         self.certificate: Optional[Certificate] = certificate\n         self.ip_address: Union[IPv4Address, IPv6Address, None] = ip_address\n         self.protocol: Optional[str] = protocol\n \n     @property\n-    def cb_kwargs(self) -> Dict[str, Any]:\n+    def cb_kwargs(self) -> dict[str, Any]:\n         try:\n             return self.request.cb_kwargs  # type: ignore[union-attr]\n         except AttributeError:\n@@ -103,7 +89,7 @@ class Response(object_ref):\n             )\n \n     @property\n-    def meta(self) -> Dict[str, Any]:\n+    def meta(self) -> dict[str, Any]:\n         try:\n             return self.request.meta  # type: ignore[union-attr]\n         except AttributeError:\n@@ -149,14 +135,14 @@ class Response(object_ref):\n \n     @overload\n     def replace(\n-        self, *args: Any, cls: Type[ResponseTypeVar], **kwargs: Any\n+        self, *args: Any, cls: type[ResponseTypeVar], **kwargs: Any\n     ) -> ResponseTypeVar: ...\n \n     @overload\n     def replace(self, *args: Any, cls: None = None, **kwargs: Any) -> Self: ...\n \n     def replace(\n-        self, *args: Any, cls: Optional[Type[Response]] = None, **kwargs: Any\n+        self, *args: Any, cls: Optional[type[Response]] = None, **kwargs: Any\n     ) -> Response:\n         \"\"\"Create a new Response with the same attributes except for those given new values\"\"\"\n         for x in self.attributes:\n@@ -200,16 +186,16 @@ class Response(object_ref):\n         url: Union[str, Link],\n         callback: Optional[CallbackT] = None,\n         method: str = \"GET\",\n-        headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n+        headers: Union[Mapping[AnyStr, Any], Iterable[tuple[AnyStr, Any]], None] = None,\n         body: Optional[Union[bytes, str]] = None,\n         cookies: Optional[CookiesT] = None,\n-        meta: Optional[Dict[str, Any]] = None,\n+        meta: Optional[dict[str, Any]] = None,\n         encoding: Optional[str] = \"utf-8\",\n         priority: int = 0,\n         dont_filter: bool = False,\n         errback: Optional[Callable[[Failure], Any]] = None,\n-        cb_kwargs: Optional[Dict[str, Any]] = None,\n-        flags: Optional[List[str]] = None,\n+        cb_kwargs: Optional[dict[str, Any]] = None,\n+        flags: Optional[list[str]] = None,\n     ) -> Request:\n         \"\"\"\n         Return a :class:`~.Request` instance to follow a link ``url``.\n@@ -253,16 +239,16 @@ class Response(object_ref):\n         urls: Iterable[Union[str, Link]],\n         callback: Optional[CallbackT] = None,\n         method: str = \"GET\",\n-        headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n+        headers: Union[Mapping[AnyStr, Any], Iterable[tuple[AnyStr, Any]], None] = None,\n         body: Optional[Union[bytes, str]] = None,\n         cookies: Optional[CookiesT] = None,\n-        meta: Optional[Dict[str, Any]] = None,\n+        meta: Optional[dict[str, Any]] = None,\n         encoding: Optional[str] = \"utf-8\",\n         priority: int = 0,\n         dont_filter: bool = False,\n         errback: Optional[Callable[[Failure], Any]] = None,\n-        cb_kwargs: Optional[Dict[str, Any]] = None,\n-        flags: Optional[List[str]] = None,\n+        cb_kwargs: Optional[dict[str, Any]] = None,\n+        flags: Optional[list[str]] = None,\n     ) -> Iterable[Request]:\n         \"\"\"\n         .. versionadded:: 2.0\n\n@@ -8,21 +8,9 @@ See documentation in docs/topics/request-response.rst\n from __future__ import annotations\n \n import json\n+from collections.abc import Iterable\n from contextlib import suppress\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    AnyStr,\n-    Callable,\n-    Dict,\n-    Iterable,\n-    List,\n-    Mapping,\n-    Optional,\n-    Tuple,\n-    Union,\n-    cast,\n-)\n+from typing import TYPE_CHECKING, Any, AnyStr, Optional, Union, cast\n from urllib.parse import urljoin\n \n import parsel\n@@ -41,6 +29,8 @@ from scrapy.utils.python import memoizemethod_noargs, to_unicode\n from scrapy.utils.response import get_base_url\n \n if TYPE_CHECKING:\n+    from collections.abc import Callable, Mapping\n+\n     from twisted.python.failure import Failure\n \n     from scrapy.http.request import CallbackT, CookiesT, Request\n@@ -54,7 +44,7 @@ class TextResponse(Response):\n     _DEFAULT_ENCODING = \"ascii\"\n     _cached_decoded_json = _NONE\n \n-    attributes: Tuple[str, ...] = Response.attributes + (\"encoding\",)\n+    attributes: tuple[str, ...] = Response.attributes + (\"encoding\",)\n \n     def __init__(self, *args: Any, **kwargs: Any):\n         self._encoding: Optional[str] = kwargs.pop(\"encoding\", None)\n@@ -183,16 +173,16 @@ class TextResponse(Response):\n         url: Union[str, Link, parsel.Selector],\n         callback: Optional[CallbackT] = None,\n         method: str = \"GET\",\n-        headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n+        headers: Union[Mapping[AnyStr, Any], Iterable[tuple[AnyStr, Any]], None] = None,\n         body: Optional[Union[bytes, str]] = None,\n         cookies: Optional[CookiesT] = None,\n-        meta: Optional[Dict[str, Any]] = None,\n+        meta: Optional[dict[str, Any]] = None,\n         encoding: Optional[str] = None,\n         priority: int = 0,\n         dont_filter: bool = False,\n         errback: Optional[Callable[[Failure], Any]] = None,\n-        cb_kwargs: Optional[Dict[str, Any]] = None,\n-        flags: Optional[List[str]] = None,\n+        cb_kwargs: Optional[dict[str, Any]] = None,\n+        flags: Optional[list[str]] = None,\n     ) -> Request:\n         \"\"\"\n         Return a :class:`~.Request` instance to follow a link ``url``.\n@@ -236,16 +226,16 @@ class TextResponse(Response):\n         urls: Union[Iterable[Union[str, Link]], parsel.SelectorList, None] = None,\n         callback: Optional[CallbackT] = None,\n         method: str = \"GET\",\n-        headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n+        headers: Union[Mapping[AnyStr, Any], Iterable[tuple[AnyStr, Any]], None] = None,\n         body: Optional[Union[bytes, str]] = None,\n         cookies: Optional[CookiesT] = None,\n-        meta: Optional[Dict[str, Any]] = None,\n+        meta: Optional[dict[str, Any]] = None,\n         encoding: Optional[str] = None,\n         priority: int = 0,\n         dont_filter: bool = False,\n         errback: Optional[Callable[[Failure], Any]] = None,\n-        cb_kwargs: Optional[Dict[str, Any]] = None,\n-        flags: Optional[List[str]] = None,\n+        cb_kwargs: Optional[dict[str, Any]] = None,\n+        flags: Optional[list[str]] = None,\n         css: Optional[str] = None,\n         xpath: Optional[str] = None,\n     ) -> Iterable[Request]:\n\n@@ -7,27 +7,21 @@ See documentation in docs/topics/item.rst\n from __future__ import annotations\n \n from abc import ABCMeta\n+from collections.abc import MutableMapping\n from copy import deepcopy\n from pprint import pformat\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    Dict,\n-    Iterator,\n-    KeysView,\n-    MutableMapping,\n-    NoReturn,\n-    Tuple,\n-)\n+from typing import TYPE_CHECKING, Any, NoReturn\n \n from scrapy.utils.trackref import object_ref\n \n if TYPE_CHECKING:\n+    from collections.abc import Iterator, KeysView\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n \n-class Field(Dict[str, Any]):\n+class Field(dict[str, Any]):\n     \"\"\"Container of field metadata\"\"\"\n \n \n@@ -38,7 +32,7 @@ class ItemMeta(ABCMeta):\n     \"\"\"\n \n     def __new__(\n-        mcs, class_name: str, bases: Tuple[type, ...], attrs: Dict[str, Any]\n+        mcs, class_name: str, bases: tuple[type, ...], attrs: dict[str, Any]\n     ) -> ItemMeta:\n         classcell = attrs.pop(\"__classcell__\", None)\n         new_bases = tuple(base._class for base in bases if hasattr(base, \"_class\"))\n@@ -83,10 +77,10 @@ class Item(MutableMapping[str, Any], object_ref, metaclass=ItemMeta):\n     :ref:`tracked <topics-leaks-trackrefs>` to debug memory leaks.\n     \"\"\"\n \n-    fields: Dict[str, Field]\n+    fields: dict[str, Field]\n \n     def __init__(self, *args: Any, **kwargs: Any):\n-        self._values: Dict[str, Any] = {}\n+        self._values: dict[str, Any] = {}\n         if args or kwargs:  # avoid creating dict for most common case\n             for k, v in dict(*args, **kwargs).items():\n                 self[k] = v\n\n@@ -6,8 +6,13 @@ This package contains a collection of Link Extractors.\n For more info see docs/topics/link-extractors.rst\n \"\"\"\n \n-import re\n-from typing import Iterable, Pattern\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING\n+\n+if TYPE_CHECKING:\n+    from collections.abc import Iterable\n+    from re import Pattern\n \n # common file extensions that are not followed if they occur in links\n IGNORED_EXTENSIONS = [\n\n@@ -6,20 +6,10 @@ from __future__ import annotations\n \n import logging\n import operator\n+import re\n+from collections.abc import Callable, Iterable\n from functools import partial\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    Callable,\n-    Iterable,\n-    List,\n-    Optional,\n-    Pattern,\n-    Set,\n-    Tuple,\n-    Union,\n-    cast,\n-)\n+from typing import TYPE_CHECKING, Any, Optional, Union, cast\n from urllib.parse import urljoin, urlparse\n \n from lxml import etree  # nosec\n@@ -28,13 +18,14 @@ from w3lib.html import strip_html5_whitespace\n from w3lib.url import canonicalize_url, safe_url_string\n \n from scrapy.link import Link\n-from scrapy.linkextractors import IGNORED_EXTENSIONS, _is_valid_url, _matches, re\n+from scrapy.linkextractors import IGNORED_EXTENSIONS, _is_valid_url, _matches\n from scrapy.utils.misc import arg_to_iter, rel_has_nofollow\n from scrapy.utils.python import unique as unique_list\n from scrapy.utils.response import get_base_url\n from scrapy.utils.url import url_has_any_extension, url_is_from_any_domain\n \n if TYPE_CHECKING:\n+\n     from lxml.html import HtmlElement  # nosec\n \n     from scrapy import Selector\n@@ -98,7 +89,7 @@ class LxmlParserLinkExtractor:\n \n     def _iter_links(\n         self, document: HtmlElement\n-    ) -> Iterable[Tuple[HtmlElement, str, str]]:\n+    ) -> Iterable[tuple[HtmlElement, str, str]]:\n         for el in document.iter(etree.Element):\n             if not self.scan_tag(_nons(el.tag)):\n                 continue\n@@ -114,8 +105,8 @@ class LxmlParserLinkExtractor:\n         response_url: str,\n         response_encoding: str,\n         base_url: str,\n-    ) -> List[Link]:\n-        links: List[Link] = []\n+    ) -> list[Link]:\n+        links: list[Link] = []\n         # hacky way to get the underlying lxml parsed document\n         for el, attr, attr_val in self._iter_links(selector.root):\n             # pseudo lxml.html.HtmlElement.make_links_absolute(base_url)\n@@ -145,26 +136,26 @@ class LxmlParserLinkExtractor:\n             links.append(link)\n         return self._deduplicate_if_needed(links)\n \n-    def extract_links(self, response: TextResponse) -> List[Link]:\n+    def extract_links(self, response: TextResponse) -> list[Link]:\n         base_url = get_base_url(response)\n         return self._extract_links(\n             response.selector, response.url, response.encoding, base_url\n         )\n \n-    def _process_links(self, links: List[Link]) -> List[Link]:\n+    def _process_links(self, links: list[Link]) -> list[Link]:\n         \"\"\"Normalize and filter extracted links\n \n         The subclass should override it if necessary\n         \"\"\"\n         return self._deduplicate_if_needed(links)\n \n-    def _deduplicate_if_needed(self, links: List[Link]) -> List[Link]:\n+    def _deduplicate_if_needed(self, links: list[Link]) -> list[Link]:\n         if self.unique:\n             return unique_list(links, key=self.link_key)\n         return links\n \n \n-_RegexT = Union[str, Pattern[str]]\n+_RegexT = Union[str, re.Pattern[str]]\n _RegexOrSeveralT = Union[_RegexT, Iterable[_RegexT]]\n \n \n@@ -197,13 +188,13 @@ class LxmlLinkExtractor:\n             strip=strip,\n             canonicalized=not canonicalize,\n         )\n-        self.allow_res: List[Pattern[str]] = self._compile_regexes(allow)\n-        self.deny_res: List[Pattern[str]] = self._compile_regexes(deny)\n+        self.allow_res: list[re.Pattern[str]] = self._compile_regexes(allow)\n+        self.deny_res: list[re.Pattern[str]] = self._compile_regexes(deny)\n \n-        self.allow_domains: Set[str] = set(arg_to_iter(allow_domains))\n-        self.deny_domains: Set[str] = set(arg_to_iter(deny_domains))\n+        self.allow_domains: set[str] = set(arg_to_iter(allow_domains))\n+        self.deny_domains: set[str] = set(arg_to_iter(deny_domains))\n \n-        self.restrict_xpaths: Tuple[str, ...] = tuple(arg_to_iter(restrict_xpaths))\n+        self.restrict_xpaths: tuple[str, ...] = tuple(arg_to_iter(restrict_xpaths))\n         self.restrict_xpaths += tuple(\n             map(self._csstranslator.css_to_xpath, arg_to_iter(restrict_css))\n         )\n@@ -211,11 +202,11 @@ class LxmlLinkExtractor:\n         if deny_extensions is None:\n             deny_extensions = IGNORED_EXTENSIONS\n         self.canonicalize: bool = canonicalize\n-        self.deny_extensions: Set[str] = {\".\" + e for e in arg_to_iter(deny_extensions)}\n-        self.restrict_text: List[Pattern[str]] = self._compile_regexes(restrict_text)\n+        self.deny_extensions: set[str] = {\".\" + e for e in arg_to_iter(deny_extensions)}\n+        self.restrict_text: list[re.Pattern[str]] = self._compile_regexes(restrict_text)\n \n     @staticmethod\n-    def _compile_regexes(value: Optional[_RegexOrSeveralT]) -> List[Pattern[str]]:\n+    def _compile_regexes(value: Optional[_RegexOrSeveralT]) -> list[re.Pattern[str]]:\n         return [\n             x if isinstance(x, re.Pattern) else re.compile(x)\n             for x in arg_to_iter(value)\n@@ -257,7 +248,7 @@ class LxmlLinkExtractor:\n         denied = (regex.search(url) for regex in self.deny_res) if self.deny_res else []\n         return any(allowed) and not any(denied)\n \n-    def _process_links(self, links: List[Link]) -> List[Link]:\n+    def _process_links(self, links: list[Link]) -> list[Link]:\n         links = [x for x in links if self._link_allowed(x)]\n         if self.canonicalize:\n             for link in links:\n@@ -265,10 +256,10 @@ class LxmlLinkExtractor:\n         links = self.link_extractor._process_links(links)\n         return links\n \n-    def _extract_links(self, *args: Any, **kwargs: Any) -> List[Link]:\n+    def _extract_links(self, *args: Any, **kwargs: Any) -> list[Link]:\n         return self.link_extractor._extract_links(*args, **kwargs)\n \n-    def extract_links(self, response: TextResponse) -> List[Link]:\n+    def extract_links(self, response: TextResponse) -> list[Link]:\n         \"\"\"Returns a list of :class:`~scrapy.link.Link` objects from the\n         specified :class:`response <scrapy.http.Response>`.\n \n\n@@ -2,7 +2,7 @@ from __future__ import annotations\n \n import logging\n import os\n-from typing import TYPE_CHECKING, Any, Dict, Optional, Tuple, TypedDict, Union\n+from typing import TYPE_CHECKING, Any, Optional, TypedDict, Union\n \n from twisted.python.failure import Failure\n \n@@ -31,7 +31,7 @@ DOWNLOADERRORMSG_LONG = \"Error downloading %(request)s: %(errmsg)s\"\n class LogFormatterResult(TypedDict):\n     level: int\n     msg: str\n-    args: Union[Dict[str, Any], Tuple[Any, ...]]\n+    args: Union[dict[str, Any], tuple[Any, ...]]\n \n \n class LogFormatter:\n@@ -181,7 +181,7 @@ class LogFormatter:\n \n         .. versionadded:: 2.0\n         \"\"\"\n-        args: Dict[str, Any] = {\"request\": request}\n+        args: dict[str, Any] = {\"request\": request}\n         if errmsg:\n             msg = DOWNLOADERRORMSG_LONG\n             args[\"errmsg\"] = errmsg\n\n@@ -14,18 +14,7 @@ from email.mime.nonmultipart import MIMENonMultipart\n from email.mime.text import MIMEText\n from email.utils import formatdate\n from io import BytesIO\n-from typing import (\n-    IO,\n-    TYPE_CHECKING,\n-    Any,\n-    Callable,\n-    Dict,\n-    List,\n-    Optional,\n-    Sequence,\n-    Tuple,\n-    Union,\n-)\n+from typing import IO, TYPE_CHECKING, Any, Optional, Union\n \n from twisted import version as twisted_version\n from twisted.internet import ssl\n@@ -36,6 +25,8 @@ from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.python import to_bytes\n \n if TYPE_CHECKING:\n+    from collections.abc import Callable, Sequence\n+\n     # imports twisted.internet.reactor\n     from twisted.mail.smtp import ESMTPSenderFactory\n     from twisted.python.failure import Failure\n@@ -95,11 +86,11 @@ class MailSender:\n \n     def send(\n         self,\n-        to: Union[str, List[str]],\n+        to: Union[str, list[str]],\n         subject: str,\n         body: str,\n-        cc: Union[str, List[str], None] = None,\n-        attachs: Sequence[Tuple[str, str, IO[Any]]] = (),\n+        cc: Union[str, list[str], None] = None,\n+        attachs: Sequence[tuple[str, str, IO[Any]]] = (),\n         mimetype: str = \"text/plain\",\n         charset: Optional[str] = None,\n         _callback: Optional[Callable[..., None]] = None,\n@@ -164,7 +155,7 @@ class MailSender:\n         return dfd\n \n     def _sent_ok(\n-        self, result: Any, to: List[str], cc: List[str], subject: str, nattachs: int\n+        self, result: Any, to: list[str], cc: list[str], subject: str, nattachs: int\n     ) -> None:\n         logger.info(\n             \"Mail sent OK: To=%(mailto)s Cc=%(mailcc)s \"\n@@ -180,8 +171,8 @@ class MailSender:\n     def _sent_failed(\n         self,\n         failure: Failure,\n-        to: List[str],\n-        cc: List[str],\n+        to: list[str],\n+        cc: list[str],\n         subject: str,\n         nattachs: int,\n     ) -> Failure:\n@@ -200,7 +191,7 @@ class MailSender:\n         )\n         return failure\n \n-    def _sendmail(self, to_addrs: List[str], msg: bytes) -> Deferred[Any]:\n+    def _sendmail(self, to_addrs: list[str], msg: bytes) -> Deferred[Any]:\n         from twisted.internet import reactor\n \n         msg_io = BytesIO(msg)\n@@ -218,11 +209,11 @@ class MailSender:\n         return d\n \n     def _create_sender_factory(\n-        self, to_addrs: List[str], msg: IO[bytes], d: Deferred[Any]\n+        self, to_addrs: list[str], msg: IO[bytes], d: Deferred[Any]\n     ) -> ESMTPSenderFactory:\n         from twisted.mail.smtp import ESMTPSenderFactory\n \n-        factory_keywords: Dict[str, Any] = {\n+        factory_keywords: dict[str, Any] = {\n             \"heloFallback\": True,\n             \"requireAuthentication\": False,\n             \"requireTransportSecurity\": self.smtptls,\n\n@@ -3,26 +3,15 @@ from __future__ import annotations\n import logging\n import pprint\n from collections import defaultdict, deque\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    Callable,\n-    Deque,\n-    Dict,\n-    Iterable,\n-    List,\n-    Optional,\n-    Tuple,\n-    TypeVar,\n-    Union,\n-    cast,\n-)\n+from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union, cast\n \n from scrapy.exceptions import NotConfigured\n from scrapy.utils.defer import process_chain, process_parallel\n from scrapy.utils.misc import build_from_crawler, build_from_settings, load_object\n \n if TYPE_CHECKING:\n+    from collections.abc import Callable, Iterable\n+\n     from twisted.internet.defer import Deferred\n \n     # typing.Concatenate and typing.ParamSpec require Python 3.10\n@@ -51,14 +40,14 @@ class MiddlewareManager:\n         self.middlewares = middlewares\n         # Only process_spider_output and process_spider_exception can be None.\n         # Only process_spider_output can be a tuple, and only until _async compatibility methods are removed.\n-        self.methods: Dict[\n-            str, Deque[Union[None, Callable, Tuple[Callable, Callable]]]\n+        self.methods: dict[\n+            str, deque[Union[None, Callable, tuple[Callable, Callable]]]\n         ] = defaultdict(deque)\n         for mw in middlewares:\n             self._add_middleware(mw)\n \n     @classmethod\n-    def _get_mwlist_from_settings(cls, settings: Settings) -> List[Any]:\n+    def _get_mwlist_from_settings(cls, settings: Settings) -> list[Any]:\n         raise NotImplementedError\n \n     @classmethod\n@@ -107,7 +96,7 @@ class MiddlewareManager:\n \n     def _process_parallel(\n         self, methodname: str, obj: _T, *args: Any\n-    ) -> Deferred[List[_T2]]:\n+    ) -> Deferred[list[_T2]]:\n         methods = cast(\n             \"Iterable[Callable[Concatenate[_T, _P], _T2]]\", self.methods[methodname]\n         )\n@@ -119,8 +108,8 @@ class MiddlewareManager:\n         )\n         return process_chain(methods, obj, *args)\n \n-    def open_spider(self, spider: Spider) -> Deferred[List[None]]:\n+    def open_spider(self, spider: Spider) -> Deferred[list[None]]:\n         return self._process_parallel(\"open_spider\", spider)\n \n-    def close_spider(self, spider: Spider) -> Deferred[List[None]]:\n+    def close_spider(self, spider: Spider) -> Deferred[list[None]]:\n         return self._process_parallel(\"close_spider\", spider)\n\n@@ -6,7 +6,7 @@ See documentation in docs/item-pipeline.rst\n \n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Any, List\n+from typing import TYPE_CHECKING, Any\n \n from scrapy.middleware import MiddlewareManager\n from scrapy.utils.conf import build_component_list\n@@ -23,7 +23,7 @@ class ItemPipelineManager(MiddlewareManager):\n     component_name = \"item pipeline\"\n \n     @classmethod\n-    def _get_mwlist_from_settings(cls, settings: Settings) -> List[Any]:\n+    def _get_mwlist_from_settings(cls, settings: Settings) -> list[Any]:\n         return build_component_list(settings.getwithbase(\"ITEM_PIPELINES\"))\n \n     def _add_middleware(self, pipe: Any) -> None:\n\n@@ -21,15 +21,9 @@ from typing import (\n     IO,\n     TYPE_CHECKING,\n     Any,\n-    Callable,\n-    DefaultDict,\n-    Dict,\n-    List,\n     NoReturn,\n     Optional,\n     Protocol,\n-    Set,\n-    Type,\n     TypedDict,\n     Union,\n     cast,\n@@ -53,6 +47,7 @@ from scrapy.utils.python import to_bytes\n from scrapy.utils.request import referer_str\n \n if TYPE_CHECKING:\n+    from collections.abc import Callable\n     from os import PathLike\n \n     from twisted.python.failure import Failure\n@@ -104,8 +99,8 @@ class FilesStoreProtocol(Protocol):\n         path: str,\n         buf: BytesIO,\n         info: MediaPipeline.SpiderInfo,\n-        meta: Optional[Dict[str, Any]] = None,\n-        headers: Optional[Dict[str, str]] = None,\n+        meta: Optional[dict[str, Any]] = None,\n+        headers: Optional[dict[str, str]] = None,\n     ) -> Optional[Deferred[Any]]: ...\n \n     def stat_file(\n@@ -120,7 +115,7 @@ class FSFilesStore:\n             basedir = basedir.split(\"://\", 1)[1]\n         self.basedir: str = basedir\n         self._mkdir(Path(self.basedir))\n-        self.created_directories: DefaultDict[MediaPipeline.SpiderInfo, Set[str]] = (\n+        self.created_directories: defaultdict[MediaPipeline.SpiderInfo, set[str]] = (\n             defaultdict(set)\n         )\n \n@@ -129,8 +124,8 @@ class FSFilesStore:\n         path: Union[str, PathLike[str]],\n         buf: BytesIO,\n         info: MediaPipeline.SpiderInfo,\n-        meta: Optional[Dict[str, Any]] = None,\n-        headers: Optional[Dict[str, str]] = None,\n+        meta: Optional[dict[str, Any]] = None,\n+        headers: Optional[dict[str, str]] = None,\n     ) -> None:\n         absolute_path = self._get_filesystem_path(path)\n         self._mkdir(absolute_path.parent, info)\n@@ -157,7 +152,7 @@ class FSFilesStore:\n     def _mkdir(\n         self, dirname: Path, domain: Optional[MediaPipeline.SpiderInfo] = None\n     ) -> None:\n-        seen: Set[str] = self.created_directories[domain] if domain else set()\n+        seen: set[str] = self.created_directories[domain] if domain else set()\n         if str(dirname) not in seen:\n             if not dirname.exists():\n                 dirname.mkdir(parents=True)\n@@ -201,7 +196,7 @@ class S3FilesStore:\n     def stat_file(\n         self, path: str, info: MediaPipeline.SpiderInfo\n     ) -> Deferred[StatInfo]:\n-        def _onsuccess(boto_key: Dict[str, Any]) -> StatInfo:\n+        def _onsuccess(boto_key: dict[str, Any]) -> StatInfo:\n             checksum = boto_key[\"ETag\"].strip('\"')\n             last_modified = boto_key[\"LastModified\"]\n             modified_stamp = time.mktime(last_modified.timetuple())\n@@ -209,10 +204,10 @@ class S3FilesStore:\n \n         return self._get_boto_key(path).addCallback(_onsuccess)\n \n-    def _get_boto_key(self, path: str) -> Deferred[Dict[str, Any]]:\n+    def _get_boto_key(self, path: str) -> Deferred[dict[str, Any]]:\n         key_name = f\"{self.prefix}{path}\"\n         return cast(\n-            \"Deferred[Dict[str, Any]]\",\n+            \"Deferred[dict[str, Any]]\",\n             deferToThread(\n                 self.s3_client.head_object, Bucket=self.bucket, Key=key_name  # type: ignore[attr-defined]\n             ),\n@@ -223,8 +218,8 @@ class S3FilesStore:\n         path: str,\n         buf: BytesIO,\n         info: MediaPipeline.SpiderInfo,\n-        meta: Optional[Dict[str, Any]] = None,\n-        headers: Optional[Dict[str, str]] = None,\n+        meta: Optional[dict[str, Any]] = None,\n+        headers: Optional[dict[str, str]] = None,\n     ) -> Deferred[Any]:\n         \"\"\"Upload file to S3 storage\"\"\"\n         key_name = f\"{self.prefix}{path}\"\n@@ -242,7 +237,7 @@ class S3FilesStore:\n             **extra,\n         )\n \n-    def _headers_to_botocore_kwargs(self, headers: Dict[str, Any]) -> Dict[str, Any]:\n+    def _headers_to_botocore_kwargs(self, headers: dict[str, Any]) -> dict[str, Any]:\n         \"\"\"Convert headers to botocore keyword arguments.\"\"\"\n         # This is required while we need to support both boto and botocore.\n         mapping = CaseInsensitiveDict(\n@@ -274,7 +269,7 @@ class S3FilesStore:\n                 \"X-Amz-Website-Redirect-Location\": \"WebsiteRedirectLocation\",\n             }\n         )\n-        extra: Dict[str, Any] = {}\n+        extra: dict[str, Any] = {}\n         for key, value in headers.items():\n             try:\n                 kwarg = mapping[key]\n@@ -332,7 +327,7 @@ class GCSFilesStore:\n             deferToThread(self.bucket.get_blob, blob_path).addCallback(_onsuccess),\n         )\n \n-    def _get_content_type(self, headers: Optional[Dict[str, str]]) -> str:\n+    def _get_content_type(self, headers: Optional[dict[str, str]]) -> str:\n         if headers and \"Content-Type\" in headers:\n             return headers[\"Content-Type\"]\n         return \"application/octet-stream\"\n@@ -345,8 +340,8 @@ class GCSFilesStore:\n         path: str,\n         buf: BytesIO,\n         info: MediaPipeline.SpiderInfo,\n-        meta: Optional[Dict[str, Any]] = None,\n-        headers: Optional[Dict[str, str]] = None,\n+        meta: Optional[dict[str, Any]] = None,\n+        headers: Optional[dict[str, str]] = None,\n     ) -> Deferred[Any]:\n         blob_path = self._get_blob_path(path)\n         blob = self.bucket.blob(blob_path)\n@@ -385,8 +380,8 @@ class FTPFilesStore:\n         path: str,\n         buf: BytesIO,\n         info: MediaPipeline.SpiderInfo,\n-        meta: Optional[Dict[str, Any]] = None,\n-        headers: Optional[Dict[str, str]] = None,\n+        meta: Optional[dict[str, Any]] = None,\n+        headers: Optional[dict[str, str]] = None,\n     ) -> Deferred[Any]:\n         path = f\"{self.basedir}/{path}\"\n         return deferToThread(\n@@ -443,7 +438,7 @@ class FilesPipeline(MediaPipeline):\n \n     MEDIA_NAME: str = \"file\"\n     EXPIRES: int = 90\n-    STORE_SCHEMES: Dict[str, Type[FilesStoreProtocol]] = {\n+    STORE_SCHEMES: dict[str, type[FilesStoreProtocol]] = {\n         \"\": FSFilesStore,\n         \"file\": FSFilesStore,\n         \"s3\": S3FilesStore,\n@@ -457,7 +452,7 @@ class FilesPipeline(MediaPipeline):\n         self,\n         store_uri: Union[str, PathLike[str]],\n         download_func: Optional[Callable[[Request, Spider], Response]] = None,\n-        settings: Union[Settings, Dict[str, Any], None] = None,\n+        settings: Union[Settings, dict[str, Any], None] = None,\n     ):\n         store_uri = _to_string(store_uri)\n         if not store_uri:\n@@ -486,7 +481,7 @@ class FilesPipeline(MediaPipeline):\n \n     @classmethod\n     def from_settings(cls, settings: Settings) -> Self:\n-        s3store: Type[S3FilesStore] = cast(Type[S3FilesStore], cls.STORE_SCHEMES[\"s3\"])\n+        s3store: type[S3FilesStore] = cast(type[S3FilesStore], cls.STORE_SCHEMES[\"s3\"])\n         s3store.AWS_ACCESS_KEY_ID = settings[\"AWS_ACCESS_KEY_ID\"]\n         s3store.AWS_SECRET_ACCESS_KEY = settings[\"AWS_SECRET_ACCESS_KEY\"]\n         s3store.AWS_SESSION_TOKEN = settings[\"AWS_SESSION_TOKEN\"]\n@@ -496,14 +491,14 @@ class FilesPipeline(MediaPipeline):\n         s3store.AWS_VERIFY = settings[\"AWS_VERIFY\"]\n         s3store.POLICY = settings[\"FILES_STORE_S3_ACL\"]\n \n-        gcs_store: Type[GCSFilesStore] = cast(\n-            Type[GCSFilesStore], cls.STORE_SCHEMES[\"gs\"]\n+        gcs_store: type[GCSFilesStore] = cast(\n+            type[GCSFilesStore], cls.STORE_SCHEMES[\"gs\"]\n         )\n         gcs_store.GCS_PROJECT_ID = settings[\"GCS_PROJECT_ID\"]\n         gcs_store.POLICY = settings[\"FILES_STORE_GCS_ACL\"] or None\n \n-        ftp_store: Type[FTPFilesStore] = cast(\n-            Type[FTPFilesStore], cls.STORE_SCHEMES[\"ftp\"]\n+        ftp_store: type[FTPFilesStore] = cast(\n+            type[FTPFilesStore], cls.STORE_SCHEMES[\"ftp\"]\n         )\n         ftp_store.FTP_USERNAME = settings[\"FTP_USER\"]\n         ftp_store.FTP_PASSWORD = settings[\"FTP_PASSWORD\"]\n@@ -660,7 +655,7 @@ class FilesPipeline(MediaPipeline):\n     # Overridable Interface\n     def get_media_requests(\n         self, item: Any, info: MediaPipeline.SpiderInfo\n-    ) -> List[Request]:\n+    ) -> list[Request]:\n         urls = ItemAdapter(item).get(self.files_urls_field, [])\n         return [Request(u, callback=NO_CALLBACK) for u in urls]\n \n@@ -680,7 +675,7 @@ class FilesPipeline(MediaPipeline):\n         return checksum\n \n     def item_completed(\n-        self, results: List[FileInfoOrError], item: Any, info: MediaPipeline.SpiderInfo\n+        self, results: list[FileInfoOrError], item: Any, info: MediaPipeline.SpiderInfo\n     ) -> Any:\n         with suppress(KeyError):\n             ItemAdapter(item)[self.files_result_field] = [x for ok, x in results if ok]\n\n@@ -11,19 +11,7 @@ import hashlib\n import warnings\n from contextlib import suppress\n from io import BytesIO\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    Callable,\n-    Dict,\n-    Iterable,\n-    List,\n-    Optional,\n-    Tuple,\n-    Type,\n-    Union,\n-    cast,\n-)\n+from typing import TYPE_CHECKING, Any, Optional, Union, cast\n \n from itemadapter import ItemAdapter\n \n@@ -42,6 +30,7 @@ from scrapy.settings import Settings\n from scrapy.utils.python import get_func_args, to_bytes\n \n if TYPE_CHECKING:\n+    from collections.abc import Callable, Iterable\n     from os import PathLike\n \n     from PIL import Image\n@@ -79,7 +68,7 @@ class ImagesPipeline(FilesPipeline):\n     MIN_WIDTH: int = 0\n     MIN_HEIGHT: int = 0\n     EXPIRES: int = 90\n-    THUMBS: Dict[str, Tuple[int, int]] = {}\n+    THUMBS: dict[str, tuple[int, int]] = {}\n     DEFAULT_IMAGES_URLS_FIELD = \"image_urls\"\n     DEFAULT_IMAGES_RESULT_FIELD = \"images\"\n \n@@ -87,7 +76,7 @@ class ImagesPipeline(FilesPipeline):\n         self,\n         store_uri: Union[str, PathLike[str]],\n         download_func: Optional[Callable[[Request, Spider], Response]] = None,\n-        settings: Union[Settings, Dict[str, Any], None] = None,\n+        settings: Union[Settings, dict[str, Any], None] = None,\n     ):\n         try:\n             from PIL import Image\n@@ -127,7 +116,7 @@ class ImagesPipeline(FilesPipeline):\n         self.min_height: int = settings.getint(\n             resolve(\"IMAGES_MIN_HEIGHT\"), self.MIN_HEIGHT\n         )\n-        self.thumbs: Dict[str, Tuple[int, int]] = settings.get(\n+        self.thumbs: dict[str, tuple[int, int]] = settings.get(\n             resolve(\"IMAGES_THUMBS\"), self.THUMBS\n         )\n \n@@ -135,7 +124,7 @@ class ImagesPipeline(FilesPipeline):\n \n     @classmethod\n     def from_settings(cls, settings: Settings) -> Self:\n-        s3store: Type[S3FilesStore] = cast(Type[S3FilesStore], cls.STORE_SCHEMES[\"s3\"])\n+        s3store: type[S3FilesStore] = cast(type[S3FilesStore], cls.STORE_SCHEMES[\"s3\"])\n         s3store.AWS_ACCESS_KEY_ID = settings[\"AWS_ACCESS_KEY_ID\"]\n         s3store.AWS_SECRET_ACCESS_KEY = settings[\"AWS_SECRET_ACCESS_KEY\"]\n         s3store.AWS_SESSION_TOKEN = settings[\"AWS_SESSION_TOKEN\"]\n@@ -145,14 +134,14 @@ class ImagesPipeline(FilesPipeline):\n         s3store.AWS_VERIFY = settings[\"AWS_VERIFY\"]\n         s3store.POLICY = settings[\"IMAGES_STORE_S3_ACL\"]\n \n-        gcs_store: Type[GCSFilesStore] = cast(\n-            Type[GCSFilesStore], cls.STORE_SCHEMES[\"gs\"]\n+        gcs_store: type[GCSFilesStore] = cast(\n+            type[GCSFilesStore], cls.STORE_SCHEMES[\"gs\"]\n         )\n         gcs_store.GCS_PROJECT_ID = settings[\"GCS_PROJECT_ID\"]\n         gcs_store.POLICY = settings[\"IMAGES_STORE_GCS_ACL\"] or None\n \n-        ftp_store: Type[FTPFilesStore] = cast(\n-            Type[FTPFilesStore], cls.STORE_SCHEMES[\"ftp\"]\n+        ftp_store: type[FTPFilesStore] = cast(\n+            type[FTPFilesStore], cls.STORE_SCHEMES[\"ftp\"]\n         )\n         ftp_store.FTP_USERNAME = settings[\"FTP_USER\"]\n         ftp_store.FTP_PASSWORD = settings[\"FTP_PASSWORD\"]\n@@ -202,7 +191,7 @@ class ImagesPipeline(FilesPipeline):\n         info: MediaPipeline.SpiderInfo,\n         *,\n         item: Any = None,\n-    ) -> Iterable[Tuple[str, Image.Image, BytesIO]]:\n+    ) -> Iterable[tuple[str, Image.Image, BytesIO]]:\n         path = self.file_path(request, response=response, info=info, item=item)\n         orig_image = self._Image.open(BytesIO(response.body))\n \n@@ -246,9 +235,9 @@ class ImagesPipeline(FilesPipeline):\n     def convert_image(\n         self,\n         image: Image.Image,\n-        size: Optional[Tuple[int, int]] = None,\n+        size: Optional[tuple[int, int]] = None,\n         response_body: Optional[BytesIO] = None,\n-    ) -> Tuple[Image.Image, BytesIO]:\n+    ) -> tuple[Image.Image, BytesIO]:\n         if response_body is None:\n             warnings.warn(\n                 f\"{self.__class__.__name__}.convert_image() method called in a deprecated way, \"\n@@ -288,12 +277,12 @@ class ImagesPipeline(FilesPipeline):\n \n     def get_media_requests(\n         self, item: Any, info: MediaPipeline.SpiderInfo\n-    ) -> List[Request]:\n+    ) -> list[Request]:\n         urls = ItemAdapter(item).get(self.images_urls_field, [])\n         return [Request(u, callback=NO_CALLBACK) for u in urls]\n \n     def item_completed(\n-        self, results: List[FileInfoOrError], item: Any, info: MediaPipeline.SpiderInfo\n+        self, results: list[FileInfoOrError], item: Any, info: MediaPipeline.SpiderInfo\n     ) -> Any:\n         with suppress(KeyError):\n             ItemAdapter(item)[self.images_result_field] = [x for ok, x in results if ok]\n\n@@ -7,15 +7,9 @@ from collections import defaultdict\n from typing import (\n     TYPE_CHECKING,\n     Any,\n-    Callable,\n-    DefaultDict,\n-    Dict,\n-    List,\n     Literal,\n     NoReturn,\n     Optional,\n-    Set,\n-    Tuple,\n     TypedDict,\n     TypeVar,\n     Union,\n@@ -33,6 +27,8 @@ from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import arg_to_iter\n \n if TYPE_CHECKING:\n+    from collections.abc import Callable\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n@@ -52,7 +48,7 @@ class FileInfo(TypedDict):\n     status: str\n \n \n-FileInfoOrError = Union[Tuple[Literal[True], FileInfo], Tuple[Literal[False], Failure]]\n+FileInfoOrError = Union[tuple[Literal[True], FileInfo], tuple[Literal[False], Failure]]\n \n \n logger = logging.getLogger(__name__)\n@@ -67,16 +63,16 @@ class MediaPipeline(ABC):\n     class SpiderInfo:\n         def __init__(self, spider: Spider):\n             self.spider: Spider = spider\n-            self.downloading: Set[bytes] = set()\n-            self.downloaded: Dict[bytes, Union[FileInfo, Failure]] = {}\n-            self.waiting: DefaultDict[bytes, List[Deferred[FileInfo]]] = defaultdict(\n+            self.downloading: set[bytes] = set()\n+            self.downloaded: dict[bytes, Union[FileInfo, Failure]] = {}\n+            self.waiting: defaultdict[bytes, list[Deferred[FileInfo]]] = defaultdict(\n                 list\n             )\n \n     def __init__(\n         self,\n         download_func: Optional[Callable[[Request, Spider], Response]] = None,\n-        settings: Union[Settings, Dict[str, Any], None] = None,\n+        settings: Union[Settings, dict[str, Any], None] = None,\n     ):\n         self.download_func = download_func\n \n@@ -129,12 +125,12 @@ class MediaPipeline(ABC):\n \n     def process_item(\n         self, item: Any, spider: Spider\n-    ) -> Deferred[List[FileInfoOrError]]:\n+    ) -> Deferred[list[FileInfoOrError]]:\n         info = self.spiderinfo\n         requests = arg_to_iter(self.get_media_requests(item, info))\n         dlist = [self._process_request(r, info, item) for r in requests]\n         dfd = cast(\n-            \"Deferred[List[FileInfoOrError]]\", DeferredList(dlist, consumeErrors=True)\n+            \"Deferred[list[FileInfoOrError]]\", DeferredList(dlist, consumeErrors=True)\n         )\n         return dfd.addCallback(self.item_completed, item, info)\n \n@@ -252,7 +248,7 @@ class MediaPipeline(ABC):\n         raise NotImplementedError()\n \n     @abstractmethod\n-    def get_media_requests(self, item: Any, info: SpiderInfo) -> List[Request]:\n+    def get_media_requests(self, item: Any, info: SpiderInfo) -> list[Request]:\n         \"\"\"Returns the media requests to download\"\"\"\n         raise NotImplementedError()\n \n@@ -276,7 +272,7 @@ class MediaPipeline(ABC):\n         raise NotImplementedError()\n \n     def item_completed(\n-        self, results: List[FileInfoOrError], item: Any, info: SpiderInfo\n+        self, results: list[FileInfoOrError], item: Any, info: SpiderInfo\n     ) -> Any:\n         \"\"\"Called per item when all media requests has been processed\"\"\"\n         if self.LOG_FAILED_RESULTS:\n\n@@ -2,23 +2,15 @@ from __future__ import annotations\n \n import hashlib\n import logging\n-from typing import (\n-    TYPE_CHECKING,\n-    Dict,\n-    Iterable,\n-    List,\n-    Optional,\n-    Protocol,\n-    Tuple,\n-    Type,\n-    cast,\n-)\n+from typing import TYPE_CHECKING, Optional, Protocol, cast\n \n from scrapy import Request\n from scrapy.core.downloader import Downloader\n from scrapy.utils.misc import build_from_crawler\n \n if TYPE_CHECKING:\n+    from collections.abc import Iterable\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n@@ -87,7 +79,7 @@ class ScrapyPriorityQueue:\n     def from_crawler(\n         cls,\n         crawler: Crawler,\n-        downstream_queue_cls: Type[QueueProtocol],\n+        downstream_queue_cls: type[QueueProtocol],\n         key: str,\n         startprios: Iterable[int] = (),\n     ) -> Self:\n@@ -96,14 +88,14 @@ class ScrapyPriorityQueue:\n     def __init__(\n         self,\n         crawler: Crawler,\n-        downstream_queue_cls: Type[QueueProtocol],\n+        downstream_queue_cls: type[QueueProtocol],\n         key: str,\n         startprios: Iterable[int] = (),\n     ):\n         self.crawler: Crawler = crawler\n-        self.downstream_queue_cls: Type[QueueProtocol] = downstream_queue_cls\n+        self.downstream_queue_cls: type[QueueProtocol] = downstream_queue_cls\n         self.key: str = key\n-        self.queues: Dict[int, QueueProtocol] = {}\n+        self.queues: dict[int, QueueProtocol] = {}\n         self.curprio: Optional[int] = None\n         self.init_prios(startprios)\n \n@@ -160,8 +152,8 @@ class ScrapyPriorityQueue:\n         # Protocols can't declare optional members\n         return cast(Request, queue.peek())  # type: ignore[attr-defined]\n \n-    def close(self) -> List[int]:\n-        active: List[int] = []\n+    def close(self) -> list[int]:\n+        active: list[int] = []\n         for p, q in self.queues.items():\n             active.append(p)\n             q.close()\n@@ -176,7 +168,7 @@ class DownloaderInterface:\n         assert crawler.engine\n         self.downloader: Downloader = crawler.engine.downloader\n \n-    def stats(self, possible_slots: Iterable[str]) -> List[Tuple[int, str]]:\n+    def stats(self, possible_slots: Iterable[str]) -> list[tuple[int, str]]:\n         return [(self._active_downloads(slot), slot) for slot in possible_slots]\n \n     def get_slot_key(self, request: Request) -> str:\n@@ -199,18 +191,18 @@ class DownloaderAwarePriorityQueue:\n     def from_crawler(\n         cls,\n         crawler: Crawler,\n-        downstream_queue_cls: Type[QueueProtocol],\n+        downstream_queue_cls: type[QueueProtocol],\n         key: str,\n-        startprios: Optional[Dict[str, Iterable[int]]] = None,\n+        startprios: Optional[dict[str, Iterable[int]]] = None,\n     ) -> Self:\n         return cls(crawler, downstream_queue_cls, key, startprios)\n \n     def __init__(\n         self,\n         crawler: Crawler,\n-        downstream_queue_cls: Type[QueueProtocol],\n+        downstream_queue_cls: type[QueueProtocol],\n         key: str,\n-        slot_startprios: Optional[Dict[str, Iterable[int]]] = None,\n+        slot_startprios: Optional[dict[str, Iterable[int]]] = None,\n     ):\n         if crawler.settings.getint(\"CONCURRENT_REQUESTS_PER_IP\") != 0:\n             raise ValueError(\n@@ -229,11 +221,11 @@ class DownloaderAwarePriorityQueue:\n             )\n \n         self._downloader_interface: DownloaderInterface = DownloaderInterface(crawler)\n-        self.downstream_queue_cls: Type[QueueProtocol] = downstream_queue_cls\n+        self.downstream_queue_cls: type[QueueProtocol] = downstream_queue_cls\n         self.key: str = key\n         self.crawler: Crawler = crawler\n \n-        self.pqueues: Dict[str, ScrapyPriorityQueue] = {}  # slot -> priority queue\n+        self.pqueues: dict[str, ScrapyPriorityQueue] = {}  # slot -> priority queue\n         for slot, startprios in (slot_startprios or {}).items():\n             self.pqueues[slot] = self.pqfactory(slot, startprios)\n \n@@ -281,7 +273,7 @@ class DownloaderAwarePriorityQueue:\n         queue = self.pqueues[slot]\n         return queue.peek()\n \n-    def close(self) -> Dict[str, List[int]]:\n+    def close(self) -> dict[str, list[int]]:\n         active = {slot: queue.close() for slot, queue in self.pqueues.items()}\n         self.pqueues.clear()\n         return active\n\n@@ -1,6 +1,6 @@\n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Any, List, Optional, Sequence, Type\n+from typing import TYPE_CHECKING, Any, Optional\n \n from twisted.internet import defer\n from twisted.internet.base import ReactorBase, ThreadedResolver\n@@ -16,6 +16,8 @@ from zope.interface.declarations import implementer, provider\n from scrapy.utils.datatypes import LocalCache\n \n if TYPE_CHECKING:\n+    from collections.abc import Sequence\n+\n     from twisted.internet.defer import Deferred\n \n     # typing.Self requires Python 3.11\n@@ -82,7 +84,7 @@ class _CachingResolutionReceiver:\n     def __init__(self, resolutionReceiver: IResolutionReceiver, hostName: str):\n         self.resolutionReceiver: IResolutionReceiver = resolutionReceiver\n         self.hostName: str = hostName\n-        self.addresses: List[IAddress] = []\n+        self.addresses: list[IAddress] = []\n \n     def resolutionBegan(self, resolution: IHostResolution) -> None:\n         self.resolutionReceiver.resolutionBegan(resolution)\n@@ -126,7 +128,7 @@ class CachingHostnameResolver:\n         resolutionReceiver: IResolutionReceiver,\n         hostName: str,\n         portNumber: int = 0,\n-        addressTypes: Optional[Sequence[Type[IAddress]]] = None,\n+        addressTypes: Optional[Sequence[type[IAddress]]] = None,\n         transportSemantics: str = \"TCP\",\n     ) -> IHostResolution:\n         try:\n\n@@ -3,15 +3,20 @@ This module implements a class which returns the appropriate Response class\n based on different criteria.\n \"\"\"\n \n+from __future__ import annotations\n+\n from io import StringIO\n from mimetypes import MimeTypes\n from pkgutil import get_data\n-from typing import Dict, Mapping, Optional, Type, Union\n+from typing import TYPE_CHECKING, Optional, Union\n \n from scrapy.http import Response\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import binary_is_text, to_bytes, to_unicode\n \n+if TYPE_CHECKING:\n+    from collections.abc import Mapping\n+\n \n class ResponseTypes:\n     CLASSES = {\n@@ -32,7 +37,7 @@ class ResponseTypes:\n     }\n \n     def __init__(self) -> None:\n-        self.classes: Dict[str, Type[Response]] = {}\n+        self.classes: dict[str, type[Response]] = {}\n         self.mimetypes: MimeTypes = MimeTypes()\n         mimedata = get_data(\"scrapy\", \"mime.types\")\n         if not mimedata:\n@@ -43,7 +48,7 @@ class ResponseTypes:\n         for mimetype, cls in self.CLASSES.items():\n             self.classes[mimetype] = load_object(cls)\n \n-    def from_mimetype(self, mimetype: str) -> Type[Response]:\n+    def from_mimetype(self, mimetype: str) -> type[Response]:\n         \"\"\"Return the most appropriate Response class for the given mimetype\"\"\"\n         if mimetype is None:\n             return Response\n@@ -54,7 +59,7 @@ class ResponseTypes:\n \n     def from_content_type(\n         self, content_type: Union[str, bytes], content_encoding: Optional[bytes] = None\n-    ) -> Type[Response]:\n+    ) -> type[Response]:\n         \"\"\"Return the most appropriate Response class from an HTTP Content-Type\n         header\"\"\"\n         if content_encoding:\n@@ -66,7 +71,7 @@ class ResponseTypes:\n \n     def from_content_disposition(\n         self, content_disposition: Union[str, bytes]\n-    ) -> Type[Response]:\n+    ) -> type[Response]:\n         try:\n             filename = (\n                 to_unicode(content_disposition, encoding=\"latin-1\", errors=\"replace\")\n@@ -78,7 +83,7 @@ class ResponseTypes:\n         except IndexError:\n             return Response\n \n-    def from_headers(self, headers: Mapping[bytes, bytes]) -> Type[Response]:\n+    def from_headers(self, headers: Mapping[bytes, bytes]) -> type[Response]:\n         \"\"\"Return the most appropriate Response class by looking at the HTTP\n         headers\"\"\"\n         cls = Response\n@@ -91,14 +96,14 @@ class ResponseTypes:\n             cls = self.from_content_disposition(headers[b\"Content-Disposition\"])\n         return cls\n \n-    def from_filename(self, filename: str) -> Type[Response]:\n+    def from_filename(self, filename: str) -> type[Response]:\n         \"\"\"Return the most appropriate Response class from a file name\"\"\"\n         mimetype, encoding = self.mimetypes.guess_type(filename)\n         if mimetype and not encoding:\n             return self.from_mimetype(mimetype)\n         return Response\n \n-    def from_body(self, body: bytes) -> Type[Response]:\n+    def from_body(self, body: bytes) -> type[Response]:\n         \"\"\"Try to guess the appropriate response based on the body content.\n         This method is a bit magic and could be improved in the future, but\n         it's not meant to be used except for special cases where response types\n@@ -122,7 +127,7 @@ class ResponseTypes:\n         url: Optional[str] = None,\n         filename: Optional[str] = None,\n         body: Optional[bytes] = None,\n-    ) -> Type[Response]:\n+    ) -> type[Response]:\n         \"\"\"Guess the most appropriate Response class based on\n         the given arguments.\"\"\"\n         cls = Response\n\n@@ -2,7 +2,7 @@\n XPath selectors based on lxml\n \"\"\"\n \n-from typing import Any, Optional, Type, Union\n+from typing import Any, Optional, Union\n \n from parsel import Selector as _ParselSelector\n \n@@ -23,7 +23,7 @@ def _st(response: Optional[TextResponse], st: Optional[str]) -> str:\n \n \n def _response_from_text(text: Union[str, bytes], st: Optional[str]) -> TextResponse:\n-    rt: Type[TextResponse] = XmlResponse if st == \"xml\" else HtmlResponse\n+    rt: type[TextResponse] = XmlResponse if st == \"xml\" else HtmlResponse\n     return rt(url=\"about:blank\", encoding=\"utf-8\", body=to_bytes(text, \"utf-8\"))\n \n \n\n@@ -2,22 +2,10 @@ from __future__ import annotations\n \n import copy\n import json\n+from collections.abc import Iterable, Iterator, Mapping, MutableMapping\n from importlib import import_module\n from pprint import pformat\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    Dict,\n-    Iterable,\n-    Iterator,\n-    List,\n-    Mapping,\n-    MutableMapping,\n-    Optional,\n-    Tuple,\n-    Union,\n-    cast,\n-)\n+from typing import TYPE_CHECKING, Any, Optional, Union, cast\n \n from scrapy.settings import default_settings\n \n@@ -37,7 +25,7 @@ if TYPE_CHECKING:\n     _SettingsInputT = Union[SupportsItems[_SettingsKeyT, Any], str, None]\n \n \n-SETTINGS_PRIORITIES: Dict[str, int] = {\n+SETTINGS_PRIORITIES: dict[str, int] = {\n     \"default\": 0,\n     \"command\": 10,\n     \"addon\": 15,\n@@ -192,8 +180,8 @@ class BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n         return float(self.get(name, default))\n \n     def getlist(\n-        self, name: _SettingsKeyT, default: Optional[List[Any]] = None\n-    ) -> List[Any]:\n+        self, name: _SettingsKeyT, default: Optional[list[Any]] = None\n+    ) -> list[Any]:\n         \"\"\"\n         Get a setting value as a list. If the setting original type is a list, a\n         copy of it will be returned. If it's a string it will be split by \",\".\n@@ -213,8 +201,8 @@ class BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n         return list(value)\n \n     def getdict(\n-        self, name: _SettingsKeyT, default: Optional[Dict[Any, Any]] = None\n-    ) -> Dict[Any, Any]:\n+        self, name: _SettingsKeyT, default: Optional[dict[Any, Any]] = None\n+    ) -> dict[Any, Any]:\n         \"\"\"\n         Get a setting value as a dictionary. If the setting original type is a\n         dictionary, a copy of it will be returned. If it is a string it will be\n@@ -238,8 +226,8 @@ class BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n     def getdictorlist(\n         self,\n         name: _SettingsKeyT,\n-        default: Union[Dict[Any, Any], List[Any], Tuple[Any], None] = None,\n-    ) -> Union[Dict[Any, Any], List[Any]]:\n+        default: Union[dict[Any, Any], list[Any], tuple[Any], None] = None,\n+    ) -> Union[dict[Any, Any], list[Any]]:\n         \"\"\"Get a setting value as either a :class:`dict` or a :class:`list`.\n \n         If the setting is already a dict or a list, a copy of it will be\n@@ -412,7 +400,7 @@ class BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n         \"\"\"\n         self._assert_mutability()\n         if isinstance(values, str):\n-            values = cast(Dict[_SettingsKeyT, Any], json.loads(values))\n+            values = cast(dict[_SettingsKeyT, Any], json.loads(values))\n         if values is not None:\n             if isinstance(values, BaseSettings):\n                 for name, value in values.items():\n@@ -477,7 +465,7 @@ class BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n     def __len__(self) -> int:\n         return len(self.attributes)\n \n-    def _to_dict(self) -> Dict[_SettingsKeyT, Any]:\n+    def _to_dict(self) -> dict[_SettingsKeyT, Any]:\n         return {\n             self._get_key(k): (v._to_dict() if isinstance(v, BaseSettings) else v)\n             for k, v in self.items()\n@@ -490,7 +478,7 @@ class BaseSettings(MutableMapping[_SettingsKeyT, Any]):\n             else str(key_value)\n         )\n \n-    def copy_to_dict(self) -> Dict[_SettingsKeyT, Any]:\n+    def copy_to_dict(self) -> dict[_SettingsKeyT, Any]:\n         \"\"\"\n         Make a copy of current settings and convert to a dict.\n \n@@ -553,7 +541,7 @@ class Settings(BaseSettings):\n         self.update(values, priority)\n \n \n-def iter_default_settings() -> Iterable[Tuple[str, Any]]:\n+def iter_default_settings() -> Iterable[tuple[str, Any]]:\n     \"\"\"Return the default settings as an iterator of (name, value) tuples\"\"\"\n     for name in dir(default_settings):\n         if name.isupper():\n@@ -562,7 +550,7 @@ def iter_default_settings() -> Iterable[Tuple[str, Any]]:\n \n def overridden_settings(\n     settings: Mapping[_SettingsKeyT, Any]\n-) -> Iterable[Tuple[str, Any]]:\n+) -> Iterable[tuple[str, Any]]:\n     \"\"\"Return an iterable of the settings that have been overridden\"\"\"\n     for name, defvalue in iter_default_settings():\n         value = settings[name]\n\n@@ -8,7 +8,7 @@ from __future__ import annotations\n \n import os\n import signal\n-from typing import Any, Callable, Dict, Optional, Tuple, Union\n+from typing import TYPE_CHECKING, Any, Optional, Union\n \n from itemadapter import is_item\n from twisted.internet import defer, threads\n@@ -27,25 +27,28 @@ from scrapy.utils.misc import load_object\n from scrapy.utils.reactor import is_asyncio_reactor_installed, set_asyncio_event_loop\n from scrapy.utils.response import open_in_browser\n \n+if TYPE_CHECKING:\n+    from collections.abc import Callable\n+\n \n class Shell:\n-    relevant_classes: Tuple[type, ...] = (Crawler, Spider, Request, Response, Settings)\n+    relevant_classes: tuple[type, ...] = (Crawler, Spider, Request, Response, Settings)\n \n     def __init__(\n         self,\n         crawler: Crawler,\n-        update_vars: Optional[Callable[[Dict[str, Any]], None]] = None,\n+        update_vars: Optional[Callable[[dict[str, Any]], None]] = None,\n         code: Optional[str] = None,\n     ):\n         self.crawler: Crawler = crawler\n-        self.update_vars: Callable[[Dict[str, Any]], None] = update_vars or (\n+        self.update_vars: Callable[[dict[str, Any]], None] = update_vars or (\n             lambda x: None\n         )\n         self.item_class: type = load_object(crawler.settings[\"DEFAULT_ITEM_CLASS\"])\n         self.spider: Optional[Spider] = None\n         self.inthread: bool = not threadable.isInIOThread()\n         self.code: Optional[str] = code\n-        self.vars: Dict[str, Any] = {}\n+        self.vars: dict[str, Any] = {}\n \n     def start(\n         self,\n\n@@ -1,6 +1,6 @@\n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Any, List, Tuple\n+from typing import TYPE_CHECKING, Any\n \n from pydispatch import dispatcher\n \n@@ -40,7 +40,7 @@ class SignalManager:\n         kwargs.setdefault(\"sender\", self.sender)\n         dispatcher.disconnect(receiver, signal, **kwargs)\n \n-    def send_catch_log(self, signal: Any, **kwargs: Any) -> List[Tuple[Any, Any]]:\n+    def send_catch_log(self, signal: Any, **kwargs: Any) -> list[tuple[Any, Any]]:\n         \"\"\"\n         Send a signal, catch exceptions and log them.\n \n@@ -52,7 +52,7 @@ class SignalManager:\n \n     def send_catch_log_deferred(\n         self, signal: Any, **kwargs: Any\n-    ) -> Deferred[List[Tuple[Any, Any]]]:\n+    ) -> Deferred[list[tuple[Any, Any]]]:\n         \"\"\"\n         Like :meth:`send_catch_log` but supports returning\n         :class:`~twisted.internet.defer.Deferred` objects from signal handlers.\n\n@@ -3,7 +3,7 @@ from __future__ import annotations\n import traceback\n import warnings\n from collections import defaultdict\n-from typing import TYPE_CHECKING, DefaultDict, Dict, List, Tuple, Type\n+from typing import TYPE_CHECKING\n \n from zope.interface import implementer\n \n@@ -29,10 +29,10 @@ class SpiderLoader:\n     \"\"\"\n \n     def __init__(self, settings: BaseSettings):\n-        self.spider_modules: List[str] = settings.getlist(\"SPIDER_MODULES\")\n+        self.spider_modules: list[str] = settings.getlist(\"SPIDER_MODULES\")\n         self.warn_only: bool = settings.getbool(\"SPIDER_LOADER_WARN_ONLY\")\n-        self._spiders: Dict[str, Type[Spider]] = {}\n-        self._found: DefaultDict[str, List[Tuple[str, str]]] = defaultdict(list)\n+        self._spiders: dict[str, type[Spider]] = {}\n+        self._found: defaultdict[str, list[tuple[str, str]]] = defaultdict(list)\n         self._load_all_spiders()\n \n     def _check_name_duplicates(self) -> None:\n@@ -80,7 +80,7 @@ class SpiderLoader:\n     def from_settings(cls, settings: BaseSettings) -> Self:\n         return cls(settings)\n \n-    def load(self, spider_name: str) -> Type[Spider]:\n+    def load(self, spider_name: str) -> type[Spider]:\n         \"\"\"\n         Return the Spider class for the given spider name. If the spider\n         name is not found, raise a KeyError.\n@@ -90,7 +90,7 @@ class SpiderLoader:\n         except KeyError:\n             raise KeyError(f\"Spider not found: {spider_name}\")\n \n-    def find_by_request(self, request: Request) -> List[str]:\n+    def find_by_request(self, request: Request) -> list[str]:\n         \"\"\"\n         Return the list of spider names that can handle the given request.\n         \"\"\"\n@@ -98,7 +98,7 @@ class SpiderLoader:\n             name for name, cls in self._spiders.items() if cls.handles_request(request)\n         ]\n \n-    def list(self) -> List[str]:\n+    def list(self) -> list[str]:\n         \"\"\"\n         Return a list with the names of all spiders available in the project.\n         \"\"\"\n\n@@ -7,11 +7,13 @@ See documentation in docs/topics/spider-middleware.rst\n from __future__ import annotations\n \n import logging\n-from typing import TYPE_CHECKING, Any, AsyncIterable, Iterable\n+from typing import TYPE_CHECKING, Any\n \n from scrapy.http import Request, Response\n \n if TYPE_CHECKING:\n+    from collections.abc import AsyncIterable, Iterable\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n\n@@ -7,11 +7,13 @@ See documentation in docs/topics/spider-middleware.rst\n from __future__ import annotations\n \n import logging\n-from typing import TYPE_CHECKING, Any, Iterable, List, Optional\n+from typing import TYPE_CHECKING, Any, Optional\n \n from scrapy.exceptions import IgnoreRequest\n \n if TYPE_CHECKING:\n+    from collections.abc import Iterable\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n@@ -39,7 +41,7 @@ class HttpErrorMiddleware:\n \n     def __init__(self, settings: BaseSettings):\n         self.handle_httpstatus_all: bool = settings.getbool(\"HTTPERROR_ALLOW_ALL\")\n-        self.handle_httpstatus_list: List[int] = settings.getlist(\n+        self.handle_httpstatus_list: list[int] = settings.getlist(\n             \"HTTPERROR_ALLOWED_CODES\"\n         )\n \n\n@@ -9,7 +9,7 @@ from __future__ import annotations\n import logging\n import re\n import warnings\n-from typing import TYPE_CHECKING, Any, AsyncIterable, Iterable, Set\n+from typing import TYPE_CHECKING, Any\n \n from scrapy import Spider, signals\n from scrapy.exceptions import ScrapyDeprecationWarning\n@@ -23,6 +23,8 @@ warnings.warn(\n )\n \n if TYPE_CHECKING:\n+    from collections.abc import AsyncIterable, Iterable\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n@@ -109,7 +111,7 @@ class OffsiteMiddleware:\n \n     def spider_opened(self, spider: Spider) -> None:\n         self.host_regex: re.Pattern[str] = self.get_host_regex(spider)\n-        self.domains_seen: Set[str] = set()\n+        self.domains_seen: set[str] = set()\n \n \n class URLWarning(Warning):\n\n@@ -6,18 +6,7 @@ originated it.\n from __future__ import annotations\n \n import warnings\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    AsyncIterable,\n-    Dict,\n-    Iterable,\n-    Optional,\n-    Tuple,\n-    Type,\n-    Union,\n-    cast,\n-)\n+from typing import TYPE_CHECKING, Any, Optional, Union, cast\n from urllib.parse import urlparse\n \n from w3lib.url import safe_url_string\n@@ -30,6 +19,8 @@ from scrapy.utils.python import to_unicode\n from scrapy.utils.url import strip_url\n \n if TYPE_CHECKING:\n+    from collections.abc import AsyncIterable, Iterable\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n@@ -37,7 +28,7 @@ if TYPE_CHECKING:\n     from scrapy.settings import BaseSettings\n \n \n-LOCAL_SCHEMES: Tuple[str, ...] = (\n+LOCAL_SCHEMES: tuple[str, ...] = (\n     \"about\",\n     \"blob\",\n     \"data\",\n@@ -56,7 +47,7 @@ POLICY_SCRAPY_DEFAULT = \"scrapy-default\"\n \n \n class ReferrerPolicy:\n-    NOREFERRER_SCHEMES: Tuple[str, ...] = LOCAL_SCHEMES\n+    NOREFERRER_SCHEMES: tuple[str, ...] = LOCAL_SCHEMES\n     name: str\n \n     def referrer(self, response_url: str, request_url: str) -> Optional[str]:\n@@ -291,11 +282,11 @@ class DefaultReferrerPolicy(NoReferrerWhenDowngradePolicy):\n     using ``file://`` or ``s3://`` scheme.\n     \"\"\"\n \n-    NOREFERRER_SCHEMES: Tuple[str, ...] = LOCAL_SCHEMES + (\"file\", \"s3\")\n+    NOREFERRER_SCHEMES: tuple[str, ...] = LOCAL_SCHEMES + (\"file\", \"s3\")\n     name: str = POLICY_SCRAPY_DEFAULT\n \n \n-_policy_classes: Dict[str, Type[ReferrerPolicy]] = {\n+_policy_classes: dict[str, type[ReferrerPolicy]] = {\n     p.name: p\n     for p in (\n         NoReferrerPolicy,\n@@ -316,14 +307,14 @@ _policy_classes[\"\"] = NoReferrerWhenDowngradePolicy\n \n def _load_policy_class(\n     policy: str, warning_only: bool = False\n-) -> Optional[Type[ReferrerPolicy]]:\n+) -> Optional[type[ReferrerPolicy]]:\n     \"\"\"\n     Expect a string for the path to the policy class,\n     otherwise try to interpret the string as a standard value\n     from https://www.w3.org/TR/referrer-policy/#referrer-policies\n     \"\"\"\n     try:\n-        return cast(Type[ReferrerPolicy], load_object(policy))\n+        return cast(type[ReferrerPolicy], load_object(policy))\n     except ValueError:\n         tokens = [token.strip() for token in policy.lower().split(\",\")]\n         # https://www.w3.org/TR/referrer-policy/#parse-referrer-policy-from-header\n@@ -341,7 +332,7 @@ def _load_policy_class(\n \n class RefererMiddleware:\n     def __init__(self, settings: Optional[BaseSettings] = None):\n-        self.default_policy: Type[ReferrerPolicy] = DefaultReferrerPolicy\n+        self.default_policy: type[ReferrerPolicy] = DefaultReferrerPolicy\n         if settings is not None:\n             settings_policy = _load_policy_class(settings.get(\"REFERRER_POLICY\"))\n             assert settings_policy\n\n@@ -7,12 +7,14 @@ See documentation in docs/topics/spider-middleware.rst\n from __future__ import annotations\n \n import logging\n-from typing import TYPE_CHECKING, Any, AsyncIterable, Iterable\n+from typing import TYPE_CHECKING, Any\n \n from scrapy.exceptions import NotConfigured\n from scrapy.http import Request, Response\n \n if TYPE_CHECKING:\n+    from collections.abc import AsyncIterable, Iterable\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n\n@@ -7,7 +7,7 @@ See documentation in docs/topics/spiders.rst\n from __future__ import annotations\n \n import logging\n-from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Optional, cast\n+from typing import TYPE_CHECKING, Any, Optional, cast\n \n from scrapy import signals\n from scrapy.http import Request, Response\n@@ -15,6 +15,8 @@ from scrapy.utils.trackref import object_ref\n from scrapy.utils.url import url_is_from_spider\n \n if TYPE_CHECKING:\n+    from collections.abc import Iterable\n+\n     from twisted.internet.defer import Deferred\n \n     # typing.Self requires Python 3.11\n@@ -32,7 +34,7 @@ class Spider(object_ref):\n     \"\"\"\n \n     name: str\n-    custom_settings: Optional[Dict[_SettingsKeyT, Any]] = None\n+    custom_settings: Optional[dict[_SettingsKeyT, Any]] = None\n \n     def __init__(self, name: Optional[str] = None, **kwargs: Any):\n         if name is not None:\n@@ -41,7 +43,7 @@ class Spider(object_ref):\n             raise ValueError(f\"{type(self).__name__} must have a name\")\n         self.__dict__.update(kwargs)\n         if not hasattr(self, \"start_urls\"):\n-            self.start_urls: List[str] = []\n+            self.start_urls: list[str] = []\n \n     @property\n     def logger(self) -> SpiderLoggerAdapter:\n\n@@ -8,22 +8,8 @@ See documentation in docs/topics/spiders.rst\n from __future__ import annotations\n \n import copy\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    AsyncIterable,\n-    Awaitable,\n-    Callable,\n-    Dict,\n-    Iterable,\n-    List,\n-    Optional,\n-    Sequence,\n-    Set,\n-    TypeVar,\n-    Union,\n-    cast,\n-)\n+from collections.abc import AsyncIterable, Awaitable, Callable\n+from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union, cast\n \n from twisted.python.failure import Failure\n \n@@ -35,6 +21,8 @@ from scrapy.utils.asyncgen import collect_asyncgen\n from scrapy.utils.spider import iterate_spider_output\n \n if TYPE_CHECKING:\n+    from collections.abc import Iterable, Sequence\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n@@ -43,7 +31,7 @@ if TYPE_CHECKING:\n \n \n _T = TypeVar(\"_T\")\n-ProcessLinksT = Callable[[List[Link]], List[Link]]\n+ProcessLinksT = Callable[[list[Link]], list[Link]]\n ProcessRequestT = Callable[[Request, Response], Optional[Request]]\n \n \n@@ -75,7 +63,7 @@ class Rule:\n         self,\n         link_extractor: Optional[LinkExtractor] = None,\n         callback: Union[CallbackT, str, None] = None,\n-        cb_kwargs: Optional[Dict[str, Any]] = None,\n+        cb_kwargs: Optional[dict[str, Any]] = None,\n         follow: Optional[bool] = None,\n         process_links: Union[ProcessLinksT, str, None] = None,\n         process_request: Union[ProcessRequestT, str, None] = None,\n@@ -84,7 +72,7 @@ class Rule:\n         self.link_extractor: LinkExtractor = link_extractor or _default_link_extractor\n         self.callback: Union[CallbackT, str, None] = callback\n         self.errback: Union[Callable[[Failure], Any], str, None] = errback\n-        self.cb_kwargs: Dict[str, Any] = cb_kwargs or {}\n+        self.cb_kwargs: dict[str, Any] = cb_kwargs or {}\n         self.process_links: Union[ProcessLinksT, str] = process_links or _identity\n         self.process_request: Union[ProcessRequestT, str] = (\n             process_request or _identity_process_request\n@@ -105,7 +93,7 @@ class Rule:\n \n class CrawlSpider(Spider):\n     rules: Sequence[Rule] = ()\n-    _rules: List[Rule]\n+    _rules: list[Rule]\n     _follow_links: bool\n \n     def __init__(self, *a: Any, **kw: Any):\n@@ -139,9 +127,9 @@ class CrawlSpider(Spider):\n     def _requests_to_follow(self, response: Response) -> Iterable[Optional[Request]]:\n         if not isinstance(response, HtmlResponse):\n             return\n-        seen: Set[Link] = set()\n+        seen: set[Link] = set()\n         for rule_index, rule in enumerate(self._rules):\n-            links: List[Link] = [\n+            links: list[Link] = [\n                 lnk\n                 for lnk in rule.link_extractor.extract_links(response)\n                 if lnk not in seen\n@@ -170,7 +158,7 @@ class CrawlSpider(Spider):\n         self,\n         response: Response,\n         callback: Optional[CallbackT],\n-        cb_kwargs: Dict[str, Any],\n+        cb_kwargs: dict[str, Any],\n         follow: bool = True,\n     ) -> AsyncIterable[Any]:\n         if callback:\n\n@@ -5,7 +5,9 @@ for scraping from an XML feed.\n See documentation in docs/topics/spiders.rst\n \"\"\"\n \n-from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, Any, Optional\n \n from scrapy.exceptions import NotConfigured, NotSupported\n from scrapy.http import Response, TextResponse\n@@ -14,6 +16,9 @@ from scrapy.spiders import Spider\n from scrapy.utils.iterators import csviter, xmliter_lxml\n from scrapy.utils.spider import iterate_spider_output\n \n+if TYPE_CHECKING:\n+    from collections.abc import Iterable, Sequence\n+\n \n class XMLFeedSpider(Spider):\n     \"\"\"\n@@ -27,7 +32,7 @@ class XMLFeedSpider(Spider):\n \n     iterator: str = \"iternodes\"\n     itertag: str = \"item\"\n-    namespaces: Sequence[Tuple[str, str]] = ()\n+    namespaces: Sequence[tuple[str, str]] = ()\n \n     def process_results(\n         self, response: Response, results: Iterable[Any]\n@@ -118,7 +123,7 @@ class CSVFeedSpider(Spider):\n     quotechar: Optional[str] = (\n         None  # When this is None, python's csv module's default quotechar is used\n     )\n-    headers: Optional[List[str]] = None\n+    headers: Optional[list[str]] = None\n \n     def process_results(\n         self, response: Response, results: Iterable[Any]\n@@ -130,7 +135,7 @@ class CSVFeedSpider(Spider):\n         \"\"\"This method has the same purpose as the one in XMLFeedSpider\"\"\"\n         return response\n \n-    def parse_row(self, response: Response, row: Dict[str, str]) -> Any:\n+    def parse_row(self, response: Response, row: dict[str, str]) -> Any:\n         \"\"\"This method must be overridden with your custom spider functionality\"\"\"\n         raise NotImplementedError\n \n\n@@ -1,6 +1,7 @@\n from __future__ import annotations\n \n-from typing import TYPE_CHECKING, Any, Iterable, Optional, cast\n+from collections.abc import Iterable\n+from typing import TYPE_CHECKING, Any, Optional, cast\n \n from scrapy import Request\n from scrapy.spiders import Spider\n\n@@ -2,18 +2,7 @@ from __future__ import annotations\n \n import logging\n import re\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    Dict,\n-    Iterable,\n-    List,\n-    Optional,\n-    Sequence,\n-    Tuple,\n-    Union,\n-    cast,\n-)\n+from typing import TYPE_CHECKING, Any, Optional, Union, cast\n \n from scrapy.http import Request, Response, XmlResponse\n from scrapy.spiders import Spider\n@@ -22,6 +11,8 @@ from scrapy.utils.gz import gunzip, gzip_magic_number\n from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n \n if TYPE_CHECKING:\n+    from collections.abc import Iterable, Sequence\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n@@ -34,7 +25,7 @@ logger = logging.getLogger(__name__)\n class SitemapSpider(Spider):\n     sitemap_urls: Sequence[str] = ()\n     sitemap_rules: Sequence[\n-        Tuple[Union[re.Pattern[str], str], Union[str, CallbackT]]\n+        tuple[Union[re.Pattern[str], str], Union[str, CallbackT]]\n     ] = [(\"\", \"parse\")]\n     sitemap_follow: Sequence[Union[re.Pattern[str], str]] = [\"\"]\n     sitemap_alternate_links: bool = False\n@@ -54,20 +45,20 @@ class SitemapSpider(Spider):\n \n     def __init__(self, *a: Any, **kw: Any):\n         super().__init__(*a, **kw)\n-        self._cbs: List[Tuple[re.Pattern[str], CallbackT]] = []\n+        self._cbs: list[tuple[re.Pattern[str], CallbackT]] = []\n         for r, c in self.sitemap_rules:\n             if isinstance(c, str):\n                 c = cast(\"CallbackT\", getattr(self, c))\n             self._cbs.append((regex(r), c))\n-        self._follow: List[re.Pattern[str]] = [regex(x) for x in self.sitemap_follow]\n+        self._follow: list[re.Pattern[str]] = [regex(x) for x in self.sitemap_follow]\n \n     def start_requests(self) -> Iterable[Request]:\n         for url in self.sitemap_urls:\n             yield Request(url, self._parse_sitemap)\n \n     def sitemap_filter(\n-        self, entries: Iterable[Dict[str, Any]]\n-    ) -> Iterable[Dict[str, Any]]:\n+        self, entries: Iterable[dict[str, Any]]\n+    ) -> Iterable[dict[str, Any]]:\n         \"\"\"This method can be used to filter sitemap entries by their\n         attributes, for example, you can filter locs with lastmod greater\n         than a given date (see docs).\n@@ -142,7 +133,7 @@ def regex(x: Union[re.Pattern[str], str]) -> re.Pattern[str]:\n     return x\n \n \n-def iterloc(it: Iterable[Dict[str, Any]], alt: bool = False) -> Iterable[str]:\n+def iterloc(it: Iterable[dict[str, Any]], alt: bool = False) -> Iterable[str]:\n     for d in it:\n         yield d[\"loc\"]\n \n\n@@ -7,13 +7,14 @@ from __future__ import annotations\n import marshal\n import pickle  # nosec\n from pathlib import Path\n-from typing import TYPE_CHECKING, Any, Callable, Optional, Type, Union\n+from typing import TYPE_CHECKING, Any, Optional, Union\n \n from queuelib import queue\n \n from scrapy.utils.request import request_from_dict\n \n if TYPE_CHECKING:\n+    from collections.abc import Callable\n     from os import PathLike\n \n     # typing.Self requires Python 3.11\n@@ -23,7 +24,7 @@ if TYPE_CHECKING:\n     from scrapy.crawler import Crawler\n \n \n-def _with_mkdir(queue_class: Type[queue.BaseQueue]) -> Type[queue.BaseQueue]:\n+def _with_mkdir(queue_class: type[queue.BaseQueue]) -> type[queue.BaseQueue]:\n     class DirectoriesCreated(queue_class):  # type: ignore[valid-type,misc]\n         def __init__(self, path: Union[str, PathLike], *args: Any, **kwargs: Any):\n             dirname = Path(path).parent\n@@ -35,10 +36,10 @@ def _with_mkdir(queue_class: Type[queue.BaseQueue]) -> Type[queue.BaseQueue]:\n \n \n def _serializable_queue(\n-    queue_class: Type[queue.BaseQueue],\n+    queue_class: type[queue.BaseQueue],\n     serialize: Callable[[Any], bytes],\n     deserialize: Callable[[bytes], Any],\n-) -> Type[queue.BaseQueue]:\n+) -> type[queue.BaseQueue]:\n     class SerializableQueue(queue_class):  # type: ignore[valid-type,misc]\n         def push(self, obj: Any) -> None:\n             s = serialize(obj)\n@@ -71,8 +72,8 @@ def _serializable_queue(\n \n \n def _scrapy_serialization_queue(\n-    queue_class: Type[queue.BaseQueue],\n-) -> Type[queue.BaseQueue]:\n+    queue_class: type[queue.BaseQueue],\n+) -> type[queue.BaseQueue]:\n     class ScrapyRequestQueue(queue_class):  # type: ignore[valid-type,misc]\n         def __init__(self, crawler: Crawler, key: str):\n             self.spider = crawler.spider\n@@ -110,8 +111,8 @@ def _scrapy_serialization_queue(\n \n \n def _scrapy_non_serialization_queue(\n-    queue_class: Type[queue.BaseQueue],\n-) -> Type[queue.BaseQueue]:\n+    queue_class: type[queue.BaseQueue],\n+) -> type[queue.BaseQueue]:\n     class ScrapyRequestQueue(queue_class):  # type: ignore[valid-type,misc]\n         @classmethod\n         def from_crawler(cls, crawler: Crawler, *args: Any, **kwargs: Any) -> Self:\n\n@@ -6,7 +6,7 @@ from __future__ import annotations\n \n import logging\n import pprint\n-from typing import TYPE_CHECKING, Any, Dict, Optional\n+from typing import TYPE_CHECKING, Any, Optional\n \n if TYPE_CHECKING:\n     from scrapy import Spider\n@@ -16,7 +16,7 @@ if TYPE_CHECKING:\n logger = logging.getLogger(__name__)\n \n \n-StatsT = Dict[str, Any]\n+StatsT = dict[str, Any]\n \n \n class StatsCollector:\n@@ -71,7 +71,7 @@ class StatsCollector:\n class MemoryStatsCollector(StatsCollector):\n     def __init__(self, crawler: Crawler):\n         super().__init__(crawler)\n-        self.spider_stats: Dict[str, StatsT] = {}\n+        self.spider_stats: dict[str, StatsT] = {}\n \n     def _persist_stats(self, stats: StatsT, spider: Spider) -> None:\n         self.spider_stats[spider.name] = stats\n\n@@ -1,9 +1,10 @@\n-from typing import AsyncGenerator, AsyncIterable, Iterable, List, TypeVar, Union\n+from collections.abc import AsyncGenerator, AsyncIterable, Iterable\n+from typing import TypeVar, Union\n \n _T = TypeVar(\"_T\")\n \n \n-async def collect_asyncgen(result: AsyncIterable[_T]) -> List[_T]:\n+async def collect_asyncgen(result: AsyncIterable[_T]) -> list[_T]:\n     results = []\n     async for x in result:\n         results.append(x)\n\n@@ -1,35 +1,29 @@\n+from __future__ import annotations\n+\n import numbers\n import os\n import sys\n import warnings\n+from collections.abc import Iterable\n from configparser import ConfigParser\n from operator import itemgetter\n from pathlib import Path\n-from typing import (\n-    Any,\n-    Callable,\n-    Collection,\n-    Dict,\n-    Iterable,\n-    List,\n-    Mapping,\n-    MutableMapping,\n-    Optional,\n-    Union,\n-    cast,\n-)\n+from typing import TYPE_CHECKING, Any, Callable, Optional, Union, cast\n \n from scrapy.exceptions import ScrapyDeprecationWarning, UsageError\n from scrapy.settings import BaseSettings\n from scrapy.utils.deprecate import update_classpath\n from scrapy.utils.python import without_none_values\n \n+if TYPE_CHECKING:\n+    from collections.abc import Collection, Mapping, MutableMapping\n+\n \n def build_component_list(\n     compdict: MutableMapping[Any, Any],\n     custom: Any = None,\n     convert: Callable[[Any], Any] = update_classpath,\n-) -> List[Any]:\n+) -> list[Any]:\n     \"\"\"Compose a component list from a { class: order } dictionary.\"\"\"\n \n     def _check_components(complist: Collection[Any]) -> None:\n@@ -39,7 +33,7 @@ def build_component_list(\n                 \"please update your settings\"\n             )\n \n-    def _map_keys(compdict: Mapping[Any, Any]) -> Union[BaseSettings, Dict[Any, Any]]:\n+    def _map_keys(compdict: Mapping[Any, Any]) -> Union[BaseSettings, dict[Any, Any]]:\n         if isinstance(compdict, BaseSettings):\n             compbs = BaseSettings()\n             for k, v in compdict.items():\n@@ -84,7 +78,7 @@ def build_component_list(\n     return [k for k, v in sorted(compdict.items(), key=itemgetter(1))]\n \n \n-def arglist_to_dict(arglist: List[str]) -> Dict[str, str]:\n+def arglist_to_dict(arglist: list[str]) -> dict[str, str]:\n     \"\"\"Convert a list of arguments like ['arg1=val1', 'arg2=val2', ...] to a\n     dict\n     \"\"\"\n@@ -130,7 +124,7 @@ def get_config(use_closest: bool = True) -> ConfigParser:\n     return cfg\n \n \n-def get_sources(use_closest: bool = True) -> List[str]:\n+def get_sources(use_closest: bool = True) -> list[str]:\n     xdg_config_home = (\n         os.environ.get(\"XDG_CONFIG_HOME\") or Path(\"~/.config\").expanduser()\n     )\n@@ -146,8 +140,8 @@ def get_sources(use_closest: bool = True) -> List[str]:\n \n \n def feed_complete_default_values_from_settings(\n-    feed: Dict[str, Any], settings: BaseSettings\n-) -> Dict[str, Any]:\n+    feed: dict[str, Any], settings: BaseSettings\n+) -> dict[str, Any]:\n     out = feed.copy()\n     out.setdefault(\"batch_item_count\", settings.getint(\"FEED_EXPORT_BATCH_ITEM_COUNT\"))\n     out.setdefault(\"encoding\", settings[\"FEED_EXPORT_ENCODING\"])\n@@ -164,17 +158,17 @@ def feed_complete_default_values_from_settings(\n \n def feed_process_params_from_cli(\n     settings: BaseSettings,\n-    output: List[str],\n+    output: list[str],\n     output_format: Optional[str] = None,\n-    overwrite_output: Optional[List[str]] = None,\n-) -> Dict[str, Dict[str, Any]]:\n+    overwrite_output: Optional[list[str]] = None,\n+) -> dict[str, dict[str, Any]]:\n     \"\"\"\n     Receives feed export params (from the 'crawl' or 'runspider' commands),\n     checks for inconsistencies in their quantities and returns a dictionary\n     suitable to be used as the FEEDS setting.\n     \"\"\"\n     valid_output_formats: Iterable[str] = without_none_values(\n-        cast(Dict[str, str], settings.getwithbase(\"FEED_EXPORTERS\"))\n+        cast(dict[str, str], settings.getwithbase(\"FEED_EXPORTERS\"))\n     ).keys()\n \n     def check_valid_format(output_format: str) -> None:\n@@ -223,7 +217,7 @@ def feed_process_params_from_cli(\n             \"URIs are specified\"\n         )\n \n-    result: Dict[str, Dict[str, Any]] = {}\n+    result: dict[str, dict[str, Any]] = {}\n     for element in output:\n         try:\n             feed_uri, feed_format = element.rsplit(\":\", 1)\n\n@@ -1,12 +1,18 @@\n+from __future__ import annotations\n+\n+from collections.abc import Callable\n from functools import wraps\n-from typing import Any, Callable, Dict, Iterable, Optional\n+from typing import TYPE_CHECKING, Any, Optional\n+\n+if TYPE_CHECKING:\n+    from collections.abc import Iterable\n \n EmbedFuncT = Callable[..., None]\n-KnownShellsT = Dict[str, Callable[..., EmbedFuncT]]\n+KnownShellsT = dict[str, Callable[..., EmbedFuncT]]\n \n \n def _embed_ipython_shell(\n-    namespace: Dict[str, Any] = {}, banner: str = \"\"\n+    namespace: dict[str, Any] = {}, banner: str = \"\"\n ) -> EmbedFuncT:\n     \"\"\"Start an IPython Shell\"\"\"\n     try:\n@@ -21,7 +27,7 @@ def _embed_ipython_shell(\n         )\n \n     @wraps(_embed_ipython_shell)\n-    def wrapper(namespace: Dict[str, Any] = namespace, banner: str = \"\") -> None:\n+    def wrapper(namespace: dict[str, Any] = namespace, banner: str = \"\") -> None:\n         config = load_default_config()\n         # Always use .instance() to ensure _instance propagation to all parents\n         # this is needed for <TAB> completion works well for new imports\n@@ -37,26 +43,26 @@ def _embed_ipython_shell(\n \n \n def _embed_bpython_shell(\n-    namespace: Dict[str, Any] = {}, banner: str = \"\"\n+    namespace: dict[str, Any] = {}, banner: str = \"\"\n ) -> EmbedFuncT:\n     \"\"\"Start a bpython shell\"\"\"\n     import bpython\n \n     @wraps(_embed_bpython_shell)\n-    def wrapper(namespace: Dict[str, Any] = namespace, banner: str = \"\") -> None:\n+    def wrapper(namespace: dict[str, Any] = namespace, banner: str = \"\") -> None:\n         bpython.embed(locals_=namespace, banner=banner)\n \n     return wrapper\n \n \n def _embed_ptpython_shell(\n-    namespace: Dict[str, Any] = {}, banner: str = \"\"\n+    namespace: dict[str, Any] = {}, banner: str = \"\"\n ) -> EmbedFuncT:\n     \"\"\"Start a ptpython shell\"\"\"\n     import ptpython.repl\n \n     @wraps(_embed_ptpython_shell)\n-    def wrapper(namespace: Dict[str, Any] = namespace, banner: str = \"\") -> None:\n+    def wrapper(namespace: dict[str, Any] = namespace, banner: str = \"\") -> None:\n         print(banner)\n         ptpython.repl.embed(locals=namespace)\n \n@@ -64,7 +70,7 @@ def _embed_ptpython_shell(\n \n \n def _embed_standard_shell(\n-    namespace: Dict[str, Any] = {}, banner: str = \"\"\n+    namespace: dict[str, Any] = {}, banner: str = \"\"\n ) -> EmbedFuncT:\n     \"\"\"Start a standard python shell\"\"\"\n     import code\n@@ -79,7 +85,7 @@ def _embed_standard_shell(\n         readline.parse_and_bind(\"tab:complete\")\n \n     @wraps(_embed_standard_shell)\n-    def wrapper(namespace: Dict[str, Any] = namespace, banner: str = \"\") -> None:\n+    def wrapper(namespace: dict[str, Any] = namespace, banner: str = \"\") -> None:\n         code.interact(banner=banner, local=namespace)\n \n     return wrapper\n@@ -114,7 +120,7 @@ def get_shell_embed_func(\n \n \n def start_python_console(\n-    namespace: Optional[Dict[str, Any]] = None,\n+    namespace: Optional[dict[str, Any]] = None,\n     banner: str = \"\",\n     shells: Optional[Iterable[str]] = None,\n ) -> None:\n\n@@ -1,12 +1,17 @@\n+from __future__ import annotations\n+\n import argparse\n import warnings\n from http.cookies import SimpleCookie\n from shlex import split\n-from typing import Any, Dict, List, NoReturn, Optional, Sequence, Tuple, Union\n+from typing import TYPE_CHECKING, Any, NoReturn, Optional, Union\n from urllib.parse import urlparse\n \n from w3lib.http import basic_auth_header\n \n+if TYPE_CHECKING:\n+    from collections.abc import Sequence\n+\n \n class DataAction(argparse.Action):\n     def __call__(\n@@ -51,9 +56,9 @@ for argument in safe_to_ignore_arguments:\n \n def _parse_headers_and_cookies(\n     parsed_args: argparse.Namespace,\n-) -> Tuple[List[Tuple[str, bytes]], Dict[str, str]]:\n-    headers: List[Tuple[str, bytes]] = []\n-    cookies: Dict[str, str] = {}\n+) -> tuple[list[tuple[str, bytes]], dict[str, str]]:\n+    headers: list[tuple[str, bytes]] = []\n+    cookies: dict[str, str] = {}\n     for header in parsed_args.headers or ():\n         name, val = header.split(\":\", 1)\n         name = name.strip()\n@@ -73,7 +78,7 @@ def _parse_headers_and_cookies(\n \n def curl_to_request_kwargs(\n     curl_command: str, ignore_unknown_options: bool = True\n-) -> Dict[str, Any]:\n+) -> dict[str, Any]:\n     \"\"\"Convert a cURL command syntax to Request kwargs.\n \n     :param str curl_command: string containing the curl command\n@@ -107,7 +112,7 @@ def curl_to_request_kwargs(\n \n     method = parsed_args.method or \"GET\"\n \n-    result: Dict[str, Any] = {\"method\": method.upper(), \"url\": url}\n+    result: dict[str, Any] = {\"method\": method.upper(), \"url\": url}\n \n     headers, cookies = _parse_headers_and_cookies(parsed_args)\n \n\n@@ -10,23 +10,15 @@ from __future__ import annotations\n import collections\n import warnings\n import weakref\n+from collections import OrderedDict\n from collections.abc import Mapping\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    AnyStr,\n-    Iterable,\n-    Optional,\n-    OrderedDict,\n-    Sequence,\n-    Tuple,\n-    TypeVar,\n-    Union,\n-)\n+from typing import TYPE_CHECKING, Any, AnyStr, Optional, TypeVar, Union\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n \n if TYPE_CHECKING:\n+    from collections.abc import Iterable, Sequence\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n@@ -52,7 +44,7 @@ class CaselessDict(dict):\n \n     def __init__(\n         self,\n-        seq: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n+        seq: Union[Mapping[AnyStr, Any], Iterable[tuple[AnyStr, Any]], None] = None,\n     ):\n         super().__init__()\n         if seq:\n@@ -92,7 +84,7 @@ class CaselessDict(dict):\n         return dict.setdefault(self, self.normkey(key), self.normvalue(def_val))  # type: ignore[arg-type]\n \n     # doesn't fully implement MutableMapping.update()\n-    def update(self, seq: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]]]) -> None:  # type: ignore[override]\n+    def update(self, seq: Union[Mapping[AnyStr, Any], Iterable[tuple[AnyStr, Any]]]) -> None:  # type: ignore[override]\n         seq = seq.items() if isinstance(seq, Mapping) else seq\n         iseq = ((self.normkey(k), self.normvalue(v)) for k, v in seq)\n         super().update(iseq)\n\n@@ -2,7 +2,7 @@ from __future__ import annotations\n \n import warnings\n from functools import wraps\n-from typing import TYPE_CHECKING, Any, Callable, TypeVar\n+from typing import TYPE_CHECKING, Any, TypeVar\n \n from twisted.internet.defer import Deferred, maybeDeferred\n from twisted.internet.threads import deferToThread\n@@ -10,6 +10,8 @@ from twisted.internet.threads import deferToThread\n from scrapy.exceptions import ScrapyDeprecationWarning\n \n if TYPE_CHECKING:\n+    from collections.abc import Callable\n+\n     # typing.ParamSpec requires Python 3.10\n     from typing_extensions import ParamSpec\n \n\n@@ -8,28 +8,10 @@ import asyncio\n import inspect\n import warnings\n from asyncio import Future\n+from collections.abc import Awaitable, Coroutine, Iterable, Iterator\n from functools import wraps\n from types import CoroutineType\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    AsyncIterable,\n-    AsyncIterator,\n-    Awaitable,\n-    Callable,\n-    Coroutine,\n-    Dict,\n-    Generic,\n-    Iterable,\n-    Iterator,\n-    List,\n-    Optional,\n-    Tuple,\n-    TypeVar,\n-    Union,\n-    cast,\n-    overload,\n-)\n+from typing import TYPE_CHECKING, Any, Generic, Optional, TypeVar, Union, cast, overload\n \n from twisted.internet import defer\n from twisted.internet.defer import Deferred, DeferredList, ensureDeferred\n@@ -40,6 +22,8 @@ from scrapy.exceptions import IgnoreRequest, ScrapyDeprecationWarning\n from scrapy.utils.reactor import _get_asyncio_event_loop, is_asyncio_reactor_installed\n \n if TYPE_CHECKING:\n+    from collections.abc import AsyncIterable, AsyncIterator, Callable\n+\n     from twisted.python.failure import Failure\n \n     # typing.Concatenate and typing.ParamSpec require Python 3.10\n@@ -47,6 +31,7 @@ if TYPE_CHECKING:\n \n     _P = ParamSpec(\"_P\")\n \n+\n _T = TypeVar(\"_T\")\n _T2 = TypeVar(\"_T2\")\n \n@@ -134,7 +119,7 @@ def parallel(\n     callable: Callable[Concatenate[_T, _P], _T2],\n     *args: _P.args,\n     **named: _P.kwargs,\n-) -> Deferred[List[Tuple[bool, Iterator[_T2]]]]:\n+) -> Deferred[list[tuple[bool, Iterator[_T2]]]]:\n     \"\"\"Execute a callable over the objects in the given iterable, in parallel,\n     using no more than ``count`` concurrent calls.\n \n@@ -145,7 +130,7 @@ def parallel(\n     return DeferredList([coop.coiterate(work) for _ in range(count)])\n \n \n-class _AsyncCooperatorAdapter(Iterator[Deferred], Generic[_T]):\n+class _AsyncCooperatorAdapter(Iterator, Generic[_T]):\n     \"\"\"A class that wraps an async iterable into a normal iterator suitable\n     for using in Cooperator.coiterate(). As it's only needed for parallel_async(),\n     it calls the callable directly in the callback, instead of providing a more\n@@ -200,10 +185,10 @@ class _AsyncCooperatorAdapter(Iterator[Deferred], Generic[_T]):\n     ):\n         self.aiterator: AsyncIterator[_T] = aiterable.__aiter__()\n         self.callable: Callable[Concatenate[_T, _P], Optional[Deferred[Any]]] = callable\n-        self.callable_args: Tuple[Any, ...] = callable_args\n-        self.callable_kwargs: Dict[str, Any] = callable_kwargs\n+        self.callable_args: tuple[Any, ...] = callable_args\n+        self.callable_kwargs: dict[str, Any] = callable_kwargs\n         self.finished: bool = False\n-        self.waiting_deferreds: List[Deferred[Any]] = []\n+        self.waiting_deferreds: list[Deferred[Any]] = []\n         self.anext_deferred: Optional[Deferred[_T]] = None\n \n     def _callback(self, result: _T) -> None:\n@@ -255,13 +240,13 @@ def parallel_async(\n     callable: Callable[Concatenate[_T, _P], Optional[Deferred[Any]]],\n     *args: _P.args,\n     **named: _P.kwargs,\n-) -> Deferred[List[Tuple[bool, Iterator[Deferred[Any]]]]]:\n+) -> Deferred[list[tuple[bool, Iterator[Deferred[Any]]]]]:\n     \"\"\"Like ``parallel`` but for async iterators\"\"\"\n     coop = Cooperator()\n     work: Iterator[Deferred[Any]] = _AsyncCooperatorAdapter(\n         async_iterable, callable, *args, **named\n     )\n-    dl: Deferred[List[Tuple[bool, Iterator[Deferred[Any]]]]] = DeferredList(\n+    dl: Deferred[list[tuple[bool, Iterator[Deferred[Any]]]]] = DeferredList(\n         [coop.coiterate(work) for _ in range(count)]\n     )\n     return dl\n@@ -311,15 +296,15 @@ def process_parallel(\n     input: _T,\n     *a: _P.args,\n     **kw: _P.kwargs,\n-) -> Deferred[List[_T2]]:\n+) -> Deferred[list[_T2]]:\n     \"\"\"Return a Deferred with the output of all successful calls to the given\n     callbacks\n     \"\"\"\n     dfds = [defer.succeed(input).addCallback(x, *a, **kw) for x in callbacks]\n-    d: Deferred[List[Tuple[bool, _T2]]] = DeferredList(\n+    d: Deferred[list[tuple[bool, _T2]]] = DeferredList(\n         dfds, fireOnOneErrback=True, consumeErrors=True\n     )\n-    d2: Deferred[List[_T2]] = d.addCallback(lambda r: [x[1] for x in r])\n+    d2: Deferred[list[_T2]] = d.addCallback(lambda r: [x[1] for x in r])\n     d2.addErrback(lambda f: f.value.subFailure)\n     return d2\n \n\n@@ -2,7 +2,7 @@\n \n import inspect\n import warnings\n-from typing import Any, Dict, List, Optional, Tuple, Type, overload\n+from typing import Any, Optional, overload\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n \n@@ -20,8 +20,8 @@ def attribute(obj: Any, oldattr: str, newattr: str, version: str = \"0.12\") -> No\n def create_deprecated_class(\n     name: str,\n     new_class: type,\n-    clsdict: Optional[Dict[str, Any]] = None,\n-    warn_category: Type[Warning] = ScrapyDeprecationWarning,\n+    clsdict: Optional[dict[str, Any]] = None,\n+    warn_category: type[Warning] = ScrapyDeprecationWarning,\n     warn_once: bool = True,\n     old_class_path: Optional[str] = None,\n     new_class_path: Optional[str] = None,\n@@ -59,14 +59,14 @@ def create_deprecated_class(\n         warned_on_subclass: bool = False\n \n         def __new__(\n-            metacls, name: str, bases: Tuple[type, ...], clsdict_: Dict[str, Any]\n+            metacls, name: str, bases: tuple[type, ...], clsdict_: dict[str, Any]\n         ) -> type:\n             cls = super().__new__(metacls, name, bases, clsdict_)\n             if metacls.deprecated_class is None:\n                 metacls.deprecated_class = cls\n             return cls\n \n-        def __init__(cls, name: str, bases: Tuple[type, ...], clsdict_: Dict[str, Any]):\n+        def __init__(cls, name: str, bases: tuple[type, ...], clsdict_: dict[str, Any]):\n             meta = cls.__class__\n             old = meta.deprecated_class\n             if old in bases and not (warn_once and meta.warned_on_subclass):\n@@ -134,7 +134,7 @@ def _clspath(cls: type, forced: Optional[str] = None) -> str:\n     return f\"{cls.__module__}.{cls.__name__}\"\n \n \n-DEPRECATION_RULES: List[Tuple[str, str]] = []\n+DEPRECATION_RULES: list[tuple[str, str]] = []\n \n \n @overload\n\n@@ -4,13 +4,13 @@ from __future__ import annotations\n \n # used in global tests code\n from time import time  # noqa: F401\n-from typing import TYPE_CHECKING, Any, List, Tuple\n+from typing import TYPE_CHECKING, Any\n \n if TYPE_CHECKING:\n     from scrapy.core.engine import ExecutionEngine\n \n \n-def get_engine_status(engine: ExecutionEngine) -> List[Tuple[str, Any]]:\n+def get_engine_status(engine: ExecutionEngine) -> list[tuple[str, Any]]:\n     \"\"\"Return a report of the current engine status\"\"\"\n     tests = [\n         \"time()-engine.start_time\",\n@@ -29,7 +29,7 @@ def get_engine_status(engine: ExecutionEngine) -> List[Tuple[str, Any]]:\n         \"engine.scraper.slot.needs_backout()\",\n     ]\n \n-    checks: List[Tuple[str, Any]] = []\n+    checks: list[tuple[str, Any]] = []\n     for test in tests:\n         try:\n             checks += [(test, eval(test))]  # nosec\n\n@@ -1,19 +1,10 @@\n+from __future__ import annotations\n+\n import csv\n import logging\n import re\n from io import StringIO\n-from typing import (\n-    Any,\n-    Callable,\n-    Dict,\n-    Iterator,\n-    List,\n-    Literal,\n-    Optional,\n-    Union,\n-    cast,\n-    overload,\n-)\n+from typing import TYPE_CHECKING, Any, Literal, Optional, Union, cast, overload\n from warnings import warn\n \n from lxml import etree  # nosec\n@@ -23,6 +14,9 @@ from scrapy.http import Response, TextResponse\n from scrapy.selector import Selector\n from scrapy.utils.python import re_rsearch\n \n+if TYPE_CHECKING:\n+    from collections.abc import Callable, Iterator\n+\n logger = logging.getLogger(__name__)\n \n \n@@ -59,7 +53,7 @@ def xmliter(obj: Union[Response, str, bytes], nodename: str) -> Iterator[Selecto\n     )\n     header_end_idx = re_rsearch(HEADER_END_RE, text)\n     header_end = text[header_end_idx[1] :].strip() if header_end_idx else \"\"\n-    namespaces: Dict[str, str] = {}\n+    namespaces: dict[str, str] = {}\n     if header_end:\n         for tagname in reversed(re.findall(END_TAG_RE, header_end)):\n             assert header_end_idx\n@@ -162,10 +156,10 @@ class _StreamReader:\n def csviter(\n     obj: Union[Response, str, bytes],\n     delimiter: Optional[str] = None,\n-    headers: Optional[List[str]] = None,\n+    headers: Optional[list[str]] = None,\n     encoding: Optional[str] = None,\n     quotechar: Optional[str] = None,\n-) -> Iterator[Dict[str, str]]:\n+) -> Iterator[dict[str, str]]:\n     \"\"\"Returns an iterator of dictionaries from the given csv object\n \n     obj can be:\n@@ -191,7 +185,7 @@ def csviter(\n \n     lines = StringIO(_body_or_str(obj, unicode=True))\n \n-    kwargs: Dict[str, Any] = {}\n+    kwargs: dict[str, Any] = {}\n     if delimiter:\n         kwargs[\"delimiter\"] = delimiter\n     if quotechar:\n\n@@ -2,20 +2,10 @@ from __future__ import annotations\n \n import logging\n import sys\n+from collections.abc import MutableMapping\n from logging.config import dictConfig\n from types import TracebackType\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    Dict,\n-    List,\n-    MutableMapping,\n-    Optional,\n-    Tuple,\n-    Type,\n-    Union,\n-    cast,\n-)\n+from typing import TYPE_CHECKING, Any, Optional, Union, cast\n \n from twisted.python import log as twisted_log\n from twisted.python.failure import Failure\n@@ -25,6 +15,7 @@ from scrapy.settings import Settings, _SettingsKeyT\n from scrapy.utils.versions import scrapy_components_versions\n \n if TYPE_CHECKING:\n+\n     from scrapy.crawler import Crawler\n     from scrapy.logformatter import LogFormatterResult\n \n@@ -34,7 +25,7 @@ logger = logging.getLogger(__name__)\n \n def failure_to_exc_info(\n     failure: Failure,\n-) -> Optional[Tuple[Type[BaseException], BaseException, Optional[TracebackType]]]:\n+) -> Optional[tuple[type[BaseException], BaseException, Optional[TracebackType]]]:\n     \"\"\"Extract exc_info from Failure instances\"\"\"\n     if isinstance(failure, Failure):\n         assert failure.type\n@@ -48,7 +39,7 @@ def failure_to_exc_info(\n \n \n class TopLevelFormatter(logging.Filter):\n-    \"\"\"Keep only top level loggers's name (direct children from root) from\n+    \"\"\"Keep only top level loggers' name (direct children from root) from\n     records.\n \n     This filter will replace Scrapy loggers' names with 'scrapy'. This mimics\n@@ -59,8 +50,8 @@ class TopLevelFormatter(logging.Filter):\n     ``loggers`` list where it should act.\n     \"\"\"\n \n-    def __init__(self, loggers: Optional[List[str]] = None):\n-        self.loggers: List[str] = loggers or []\n+    def __init__(self, loggers: Optional[list[str]] = None):\n+        self.loggers: list[str] = loggers or []\n \n     def filter(self, record: logging.LogRecord) -> bool:\n         if any(record.name.startswith(logger + \".\") for logger in self.loggers):\n@@ -89,7 +80,7 @@ DEFAULT_LOGGING = {\n \n \n def configure_logging(\n-    settings: Union[Settings, Dict[_SettingsKeyT, Any], None] = None,\n+    settings: Union[Settings, dict[_SettingsKeyT, Any], None] = None,\n     install_root_handler: bool = True,\n ) -> None:\n     \"\"\"\n@@ -240,7 +231,7 @@ class LogCounterHandler(logging.Handler):\n \n def logformatter_adapter(\n     logkws: LogFormatterResult,\n-) -> Tuple[int, str, Union[Dict[str, Any], Tuple[Any, ...]]]:\n+) -> tuple[int, str, Union[dict[str, Any], tuple[Any, ...]]]:\n     \"\"\"\n     Helper that takes the dictionary output from the methods in LogFormatter\n     and adapts it into a tuple of positional arguments for logger.log calls,\n@@ -251,7 +242,7 @@ def logformatter_adapter(\n     message = logkws.get(\"msg\") or \"\"\n     # NOTE: This also handles 'args' being an empty dict, that case doesn't\n     # play well in logger.log calls\n-    args = cast(Dict[str, Any], logkws) if not logkws.get(\"args\") else logkws[\"args\"]\n+    args = cast(dict[str, Any], logkws) if not logkws.get(\"args\") else logkws[\"args\"]\n \n     return (level, message, args)\n \n@@ -259,7 +250,7 @@ def logformatter_adapter(\n class SpiderLoggerAdapter(logging.LoggerAdapter):\n     def process(\n         self, msg: str, kwargs: MutableMapping[str, Any]\n-    ) -> Tuple[str, MutableMapping[str, Any]]:\n+    ) -> tuple[str, MutableMapping[str, Any]]:\n         \"\"\"Method that augments logging with additional 'extra' data\"\"\"\n         if isinstance(kwargs.get(\"extra\"), MutableMapping):\n             kwargs[\"extra\"].update(self.extra)\n\n@@ -9,31 +9,19 @@ import os\n import re\n import warnings\n from collections import deque\n+from collections.abc import Iterable\n from contextlib import contextmanager\n from functools import partial\n from importlib import import_module\n from pkgutil import iter_modules\n-from typing import (\n-    IO,\n-    TYPE_CHECKING,\n-    Any,\n-    Callable,\n-    Deque,\n-    Iterable,\n-    Iterator,\n-    List,\n-    Optional,\n-    Type,\n-    TypeVar,\n-    Union,\n-    cast,\n-)\n+from typing import IO, TYPE_CHECKING, Any, Optional, TypeVar, Union, cast\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.item import Item\n from scrapy.utils.datatypes import LocalWeakReferencedCache\n \n if TYPE_CHECKING:\n+    from collections.abc import Callable, Iterator\n     from types import ModuleType\n \n     from scrapy import Spider\n@@ -91,7 +79,7 @@ def load_object(path: Union[str, Callable[..., Any]]) -> Any:\n     return obj\n \n \n-def walk_modules(path: str) -> List[ModuleType]:\n+def walk_modules(path: str) -> list[ModuleType]:\n     \"\"\"Loads a module and all its submodules from the given module path and\n     returns them. If *any* module throws an exception while importing, that\n     exception is thrown back.\n@@ -99,7 +87,7 @@ def walk_modules(path: str) -> List[ModuleType]:\n     For example: walk_modules('scrapy.utils')\n     \"\"\"\n \n-    mods: List[ModuleType] = []\n+    mods: list[ModuleType] = []\n     mod = import_module(path)\n     mods.append(mod)\n     if hasattr(mod, \"__path__\"):\n@@ -186,7 +174,7 @@ def create_instance(objcls, settings, crawler, *args, **kwargs):\n \n \n def build_from_crawler(\n-    objcls: Type[T], crawler: Crawler, /, *args: Any, **kwargs: Any\n+    objcls: type[T], crawler: Crawler, /, *args: Any, **kwargs: Any\n ) -> T:\n     \"\"\"Construct a class instance using its ``from_crawler`` constructor.\n \n@@ -209,7 +197,7 @@ def build_from_crawler(\n \n \n def build_from_settings(\n-    objcls: Type[T], settings: BaseSettings, /, *args: Any, **kwargs: Any\n+    objcls: type[T], settings: BaseSettings, /, *args: Any, **kwargs: Any\n ) -> T:\n     \"\"\"Construct a class instance using its ``from_settings`` constructor.\n \n@@ -250,7 +238,7 @@ def walk_callable(node: ast.AST) -> Iterable[ast.AST]:\n     \"\"\"Similar to ``ast.walk``, but walks only function body and skips nested\n     functions defined within the node.\n     \"\"\"\n-    todo: Deque[ast.AST] = deque([node])\n+    todo: deque[ast.AST] = deque([node])\n     walked_func_def = False\n     while todo:\n         node = todo.popleft()\n\n@@ -1,13 +1,14 @@\n import signal\n+from collections.abc import Callable\n from types import FrameType\n-from typing import Any, Callable, Dict, Optional, Union\n+from typing import Any, Optional, Union\n \n # copy of _HANDLER from typeshed/stdlib/signal.pyi\n SignalHandlerT = Union[\n     Callable[[int, Optional[FrameType]], Any], int, signal.Handlers, None\n ]\n \n-signal_names: Dict[int, str] = {}\n+signal_names: dict[int, str] = {}\n for signame in dir(signal):\n     if signame.startswith(\"SIG\") and not signame.startswith(\"SIG_\"):\n         signum = getattr(signal, signame)\n\n@@ -1,5 +1,3 @@\n-from __future__ import annotations\n-\n import os\n import warnings\n from importlib import import_module\n\n@@ -4,36 +4,22 @@ This module contains essential stuff that should've come with Python itself ;)\n \n from __future__ import annotations\n \n-import collections.abc\n import gc\n import inspect\n import re\n import sys\n import weakref\n+from collections.abc import AsyncIterable, Iterable, Mapping\n from functools import partial, wraps\n from itertools import chain\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    AsyncIterable,\n-    AsyncIterator,\n-    Callable,\n-    Dict,\n-    Iterable,\n-    Iterator,\n-    List,\n-    Mapping,\n-    Optional,\n-    Pattern,\n-    Tuple,\n-    TypeVar,\n-    Union,\n-    overload,\n-)\n+from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union, overload\n \n from scrapy.utils.asyncgen import as_async_generator\n \n if TYPE_CHECKING:\n+    from collections.abc import AsyncIterator, Callable, Iterator\n+    from re import Pattern\n+\n     # typing.Concatenate and typing.ParamSpec require Python 3.10\n     from typing_extensions import Concatenate, ParamSpec\n \n@@ -44,7 +30,7 @@ _KT = TypeVar(\"_KT\")\n _VT = TypeVar(\"_VT\")\n \n \n-def flatten(x: Iterable[Any]) -> List[Any]:\n+def flatten(x: Iterable[Any]) -> list[Any]:\n     \"\"\"flatten(sequence) -> list\n \n     Returns a single, flat list which contains all elements retrieved\n@@ -99,10 +85,10 @@ def is_listlike(x: Any) -> bool:\n     return hasattr(x, \"__iter__\") and not isinstance(x, (str, bytes))\n \n \n-def unique(list_: Iterable[_T], key: Callable[[_T], Any] = lambda x: x) -> List[_T]:\n+def unique(list_: Iterable[_T], key: Callable[[_T], Any] = lambda x: x) -> list[_T]:\n     \"\"\"efficient function to uniquify a list preserving item order\"\"\"\n     seen = set()\n-    result: List[_T] = []\n+    result: list[_T] = []\n     for item in list_:\n         seenkey = key(item)\n         if seenkey in seen:\n@@ -147,7 +133,7 @@ def to_bytes(\n \n def re_rsearch(\n     pattern: Union[str, Pattern[str]], text: str, chunk_size: int = 1024\n-) -> Optional[Tuple[int, int]]:\n+) -> Optional[tuple[int, int]]:\n     \"\"\"\n     This function does a reverse search in a text using a regular expression\n     given in the attribute 'pattern'.\n@@ -161,7 +147,7 @@ def re_rsearch(\n     the start position of the match, and the ending (regarding the entire text).\n     \"\"\"\n \n-    def _chunk_iter() -> Iterable[Tuple[str, int]]:\n+    def _chunk_iter() -> Iterable[tuple[str, int]]:\n         offset = len(text)\n         while True:\n             offset -= chunk_size * 1024\n@@ -215,12 +201,12 @@ def binary_is_text(data: bytes) -> bool:\n     return all(c not in _BINARYCHARS for c in data)\n \n \n-def get_func_args(func: Callable[..., Any], stripself: bool = False) -> List[str]:\n+def get_func_args(func: Callable[..., Any], stripself: bool = False) -> list[str]:\n     \"\"\"Return the argument name list of a callable object\"\"\"\n     if not callable(func):\n         raise TypeError(f\"func must be callable, got '{type(func).__name__}'\")\n \n-    args: List[str] = []\n+    args: list[str] = []\n     try:\n         sig = inspect.signature(func)\n     except ValueError:\n@@ -245,7 +231,7 @@ def get_func_args(func: Callable[..., Any], stripself: bool = False) -> List[str\n     return args\n \n \n-def get_spec(func: Callable[..., Any]) -> Tuple[List[str], Dict[str, Any]]:\n+def get_spec(func: Callable[..., Any]) -> tuple[list[str], dict[str, Any]]:\n     \"\"\"Returns (args, kwargs) tuple for a function\n     >>> import re\n     >>> get_spec(re.match)\n@@ -274,7 +260,7 @@ def get_spec(func: Callable[..., Any]) -> Tuple[List[str], Dict[str, Any]]:\n     else:\n         raise TypeError(f\"{type(func)} is not callable\")\n \n-    defaults: Tuple[Any, ...] = spec.defaults or ()\n+    defaults: tuple[Any, ...] = spec.defaults or ()\n \n     firstdefault = len(spec.args) - len(defaults)\n     args = spec.args[:firstdefault]\n@@ -283,7 +269,7 @@ def get_spec(func: Callable[..., Any]) -> Tuple[List[str], Dict[str, Any]]:\n \n \n def equal_attributes(\n-    obj1: Any, obj2: Any, attributes: Optional[List[Union[str, Callable[[Any], Any]]]]\n+    obj1: Any, obj2: Any, attributes: Optional[list[Union[str, Callable[[Any], Any]]]]\n ) -> bool:\n     \"\"\"Compare two objects attributes\"\"\"\n     # not attributes given return False by default\n@@ -303,7 +289,7 @@ def equal_attributes(\n \n \n @overload\n-def without_none_values(iterable: Mapping[_KT, _VT]) -> Dict[_KT, _VT]: ...\n+def without_none_values(iterable: Mapping[_KT, _VT]) -> dict[_KT, _VT]: ...\n \n \n @overload\n@@ -312,13 +298,13 @@ def without_none_values(iterable: Iterable[_KT]) -> Iterable[_KT]: ...\n \n def without_none_values(\n     iterable: Union[Mapping[_KT, _VT], Iterable[_KT]]\n-) -> Union[Dict[_KT, _VT], Iterable[_KT]]:\n+) -> Union[dict[_KT, _VT], Iterable[_KT]]:\n     \"\"\"Return a copy of ``iterable`` with all ``None`` entries removed.\n \n     If ``iterable`` is a mapping, return a dictionary where all pairs that have\n     value ``None`` have been removed.\n     \"\"\"\n-    if isinstance(iterable, collections.abc.Mapping):\n+    if isinstance(iterable, Mapping):\n         return {k: v for k, v in iterable.items() if v is not None}\n     else:\n         # the iterable __init__ must take another iterable\n\n@@ -3,18 +3,7 @@ from __future__ import annotations\n import asyncio\n import sys\n from contextlib import suppress\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    Callable,\n-    Dict,\n-    Generic,\n-    List,\n-    Optional,\n-    Tuple,\n-    Type,\n-    TypeVar,\n-)\n+from typing import TYPE_CHECKING, Any, Generic, Optional, TypeVar\n from warnings import catch_warnings, filterwarnings, warn\n \n from twisted.internet import asyncioreactor, error\n@@ -25,6 +14,7 @@ from scrapy.utils.misc import load_object\n \n if TYPE_CHECKING:\n     from asyncio import AbstractEventLoop, AbstractEventLoopPolicy\n+    from collections.abc import Callable\n \n     from twisted.internet.protocol import ServerFactory\n     from twisted.internet.tcp import Port\n@@ -37,7 +27,7 @@ if TYPE_CHECKING:\n _T = TypeVar(\"_T\")\n \n \n-def listen_tcp(portrange: List[int], host: str, factory: ServerFactory) -> Port:  # type: ignore[return]\n+def listen_tcp(portrange: list[int], host: str, factory: ServerFactory) -> Port:  # type: ignore[return]\n     \"\"\"Like reactor.listenTCP but tries different ports in a range.\"\"\"\n     from twisted.internet import reactor\n \n@@ -62,8 +52,8 @@ class CallLaterOnce(Generic[_T]):\n \n     def __init__(self, func: Callable[_P, _T], *a: _P.args, **kw: _P.kwargs):\n         self._func: Callable[_P, _T] = func\n-        self._a: Tuple[Any, ...] = a\n-        self._kw: Dict[str, Any] = kw\n+        self._a: tuple[Any, ...] = a\n+        self._kw: dict[str, Any] = kw\n         self._call: Optional[DelayedCall] = None\n \n     def schedule(self, delay: float = 0) -> None:\n@@ -142,7 +132,7 @@ def _get_asyncio_event_loop() -> AbstractEventLoop:\n def set_asyncio_event_loop(event_loop_path: Optional[str]) -> AbstractEventLoop:\n     \"\"\"Sets and returns the event loop with specified import path.\"\"\"\n     if event_loop_path is not None:\n-        event_loop_class: Type[AbstractEventLoop] = load_object(event_loop_path)\n+        event_loop_class: type[AbstractEventLoop] = load_object(event_loop_path)\n         event_loop = event_loop_class()\n         asyncio.set_event_loop(event_loop)\n     else:\n\n@@ -8,18 +8,7 @@ from __future__ import annotations\n import hashlib\n import json\n import warnings\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    Dict,\n-    Iterable,\n-    List,\n-    Optional,\n-    Protocol,\n-    Tuple,\n-    Type,\n-    Union,\n-)\n+from typing import TYPE_CHECKING, Any, Optional, Protocol, Union\n from urllib.parse import urlunparse\n from weakref import WeakKeyDictionary\n \n@@ -33,6 +22,8 @@ from scrapy.utils.misc import load_object\n from scrapy.utils.python import to_bytes, to_unicode\n \n if TYPE_CHECKING:\n+    from collections.abc import Iterable\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n@@ -47,7 +38,7 @@ def _serialize_headers(headers: Iterable[bytes], request: Request) -> Iterable[b\n \n \n _fingerprint_cache: WeakKeyDictionary[\n-    Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], bytes]\n+    Request, dict[tuple[Optional[tuple[bytes, ...]], bool], bytes]\n ]\n _fingerprint_cache = WeakKeyDictionary()\n \n@@ -88,7 +79,7 @@ def fingerprint(\n     If you want to include them, set the keep_fragments argument to True\n     (for instance when handling requests with a headless browser).\n     \"\"\"\n-    processed_include_headers: Optional[Tuple[bytes, ...]] = None\n+    processed_include_headers: Optional[tuple[bytes, ...]] = None\n     if include_headers:\n         processed_include_headers = tuple(\n             to_bytes(h.lower()) for h in sorted(include_headers)\n@@ -98,7 +89,7 @@ def fingerprint(\n     if cache_key not in cache:\n         # To decode bytes reliably (JSON does not support bytes), regardless of\n         # character encoding, we use bytes.hex()\n-        headers: Dict[str, List[str]] = {}\n+        headers: dict[str, list[str]] = {}\n         if processed_include_headers:\n             for header in processed_include_headers:\n                 if header in request.headers:\n@@ -194,13 +185,13 @@ def referer_str(request: Request) -> Optional[str]:\n     return to_unicode(referrer, errors=\"replace\")\n \n \n-def request_from_dict(d: Dict[str, Any], *, spider: Optional[Spider] = None) -> Request:\n+def request_from_dict(d: dict[str, Any], *, spider: Optional[Spider] = None) -> Request:\n     \"\"\"Create a :class:`~scrapy.Request` object from a dict.\n \n     If a spider is given, it will try to resolve the callbacks looking at the\n     spider for methods with the same name.\n     \"\"\"\n-    request_cls: Type[Request] = load_object(d[\"_class\"]) if \"_class\" in d else Request\n+    request_cls: type[Request] = load_object(d[\"_class\"]) if \"_class\" in d else Request\n     kwargs = {key: value for key, value in d.items() if key in request_cls.attributes}\n     if d.get(\"callback\") and spider:\n         kwargs[\"callback\"] = _get_method(spider, d[\"callback\"])\n\n@@ -9,7 +9,7 @@ import os\n import re\n import tempfile\n import webbrowser\n-from typing import TYPE_CHECKING, Any, Callable, Iterable, Tuple, Union\n+from typing import TYPE_CHECKING, Any, Union\n from weakref import WeakKeyDictionary\n \n from twisted.web import http\n@@ -18,6 +18,8 @@ from w3lib import html\n from scrapy.utils.python import to_bytes, to_unicode\n \n if TYPE_CHECKING:\n+    from collections.abc import Callable, Iterable\n+\n     from scrapy.http import Response, TextResponse\n \n _baseurl_cache: WeakKeyDictionary[Response, str] = WeakKeyDictionary()\n@@ -34,14 +36,14 @@ def get_base_url(response: TextResponse) -> str:\n \n \n _metaref_cache: WeakKeyDictionary[\n-    Response, Union[Tuple[None, None], Tuple[float, str]]\n+    Response, Union[tuple[None, None], tuple[float, str]]\n ] = WeakKeyDictionary()\n \n \n def get_meta_refresh(\n     response: TextResponse,\n     ignore_tags: Iterable[str] = (\"script\", \"noscript\"),\n-) -> Union[Tuple[None, None], Tuple[float, str]]:\n+) -> Union[tuple[None, None], tuple[float, str]]:\n     \"\"\"Parse the http-equiv refresh parameter from the given response\"\"\"\n     if response not in _metaref_cache:\n         text = response.text[0:4096]\n\n@@ -2,10 +2,9 @@\n \n from __future__ import annotations\n \n-import collections.abc\n import logging\n+from collections.abc import Sequence\n from typing import Any as TypingAny\n-from typing import List, Tuple\n \n from pydispatch.dispatcher import (\n     Anonymous,\n@@ -30,19 +29,15 @@ def send_catch_log(\n     sender: TypingAny = Anonymous,\n     *arguments: TypingAny,\n     **named: TypingAny,\n-) -> List[Tuple[TypingAny, TypingAny]]:\n+) -> list[tuple[TypingAny, TypingAny]]:\n     \"\"\"Like pydispatcher.robust.sendRobust but it also logs errors and returns\n     Failures instead of exceptions.\n     \"\"\"\n     dont_log = named.pop(\"dont_log\", ())\n-    dont_log = (\n-        tuple(dont_log)\n-        if isinstance(dont_log, collections.abc.Sequence)\n-        else (dont_log,)\n-    )\n+    dont_log = tuple(dont_log) if isinstance(dont_log, Sequence) else (dont_log,)\n     dont_log += (StopDownload,)\n     spider = named.get(\"spider\", None)\n-    responses: List[Tuple[TypingAny, TypingAny]] = []\n+    responses: list[tuple[TypingAny, TypingAny]] = []\n     for receiver in liveReceivers(getAllReceivers(sender, signal)):\n         result: TypingAny\n         try:\n@@ -76,7 +71,7 @@ def send_catch_log_deferred(\n     sender: TypingAny = Anonymous,\n     *arguments: TypingAny,\n     **named: TypingAny,\n-) -> Deferred[List[Tuple[TypingAny, TypingAny]]]:\n+) -> Deferred[list[tuple[TypingAny, TypingAny]]]:\n     \"\"\"Like send_catch_log but supports returning deferreds on signal handlers.\n     Returns a deferred that gets fired once all signal handlers deferreds were\n     fired.\n@@ -94,14 +89,14 @@ def send_catch_log_deferred(\n \n     dont_log = named.pop(\"dont_log\", None)\n     spider = named.get(\"spider\", None)\n-    dfds: List[Deferred[Tuple[TypingAny, TypingAny]]] = []\n+    dfds: list[Deferred[tuple[TypingAny, TypingAny]]] = []\n     for receiver in liveReceivers(getAllReceivers(sender, signal)):\n         d: Deferred[TypingAny] = maybeDeferred_coro(\n             robustApply, receiver, signal=signal, sender=sender, *arguments, **named\n         )\n         d.addErrback(logerror, receiver)\n         # TODO https://pylint.readthedocs.io/en/latest/user_guide/messages/warning/cell-var-from-loop.html\n-        d2: Deferred[Tuple[TypingAny, TypingAny]] = d.addBoth(\n+        d2: Deferred[tuple[TypingAny, TypingAny]] = d.addBoth(\n             lambda result: (\n                 receiver,  # pylint: disable=cell-var-from-loop  # noqa: B023\n                 result,\n@@ -109,7 +104,7 @@ def send_catch_log_deferred(\n         )\n         dfds.append(d2)\n     dl = DeferredList(dfds)\n-    d3: Deferred[List[Tuple[TypingAny, TypingAny]]] = dl.addCallback(\n+    d3: Deferred[list[tuple[TypingAny, TypingAny]]] = dl.addCallback(\n         lambda out: [x[1] for x in out]\n     )\n     return d3\n\n@@ -5,11 +5,16 @@ Note: The main purpose of this module is to provide support for the\n SitemapSpider, its API is subject to change without notice.\n \"\"\"\n \n-from typing import Any, Dict, Iterable, Iterator, Optional, Union\n+from __future__ import annotations\n+\n+from typing import TYPE_CHECKING, Any, Optional, Union\n from urllib.parse import urljoin\n \n import lxml.etree  # nosec\n \n+if TYPE_CHECKING:\n+    from collections.abc import Iterable, Iterator\n+\n \n class Sitemap:\n     \"\"\"Class to parse Sitemap (type=urlset) and Sitemap Index\n@@ -23,9 +28,9 @@ class Sitemap:\n         rt = self._root.tag\n         self.type = self._root.tag.split(\"}\", 1)[1] if \"}\" in rt else rt\n \n-    def __iter__(self) -> Iterator[Dict[str, Any]]:\n+    def __iter__(self) -> Iterator[dict[str, Any]]:\n         for elem in self._root.getchildren():\n-            d: Dict[str, Any] = {}\n+            d: dict[str, Any] = {}\n             for el in elem.getchildren():\n                 tag = el.tag\n                 name = tag.split(\"}\", 1)[1] if \"}\" in tag else tag\n\n@@ -2,24 +2,14 @@ from __future__ import annotations\n \n import inspect\n import logging\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    AsyncGenerator,\n-    Iterable,\n-    Literal,\n-    Optional,\n-    Type,\n-    TypeVar,\n-    Union,\n-    overload,\n-)\n+from typing import TYPE_CHECKING, Any, Literal, Optional, TypeVar, Union, overload\n \n from scrapy.spiders import Spider\n from scrapy.utils.defer import deferred_from_coro\n from scrapy.utils.misc import arg_to_iter\n \n if TYPE_CHECKING:\n+    from collections.abc import AsyncGenerator, Iterable\n     from types import CoroutineType, ModuleType\n \n     from twisted.internet.defer import Deferred\n@@ -58,7 +48,7 @@ def iterate_spider_output(\n     return arg_to_iter(deferred_from_coro(result))\n \n \n-def iter_spider_classes(module: ModuleType) -> Iterable[Type[Spider]]:\n+def iter_spider_classes(module: ModuleType) -> Iterable[type[Spider]]:\n     \"\"\"Return an iterator over all spider classes defined in the given module\n     that can be instantiated (i.e. which have name)\n     \"\"\"\n@@ -80,10 +70,10 @@ def iter_spider_classes(module: ModuleType) -> Iterable[Type[Spider]]:\n def spidercls_for_request(\n     spider_loader: SpiderLoader,\n     request: Request,\n-    default_spidercls: Type[Spider],\n+    default_spidercls: type[Spider],\n     log_none: bool = ...,\n     log_multiple: bool = ...,\n-) -> Type[Spider]: ...\n+) -> type[Spider]: ...\n \n \n @overload\n@@ -93,7 +83,7 @@ def spidercls_for_request(\n     default_spidercls: Literal[None],\n     log_none: bool = ...,\n     log_multiple: bool = ...,\n-) -> Optional[Type[Spider]]: ...\n+) -> Optional[type[Spider]]: ...\n \n \n @overload\n@@ -103,16 +93,16 @@ def spidercls_for_request(\n     *,\n     log_none: bool = ...,\n     log_multiple: bool = ...,\n-) -> Optional[Type[Spider]]: ...\n+) -> Optional[type[Spider]]: ...\n \n \n def spidercls_for_request(\n     spider_loader: SpiderLoader,\n     request: Request,\n-    default_spidercls: Optional[Type[Spider]] = None,\n+    default_spidercls: Optional[type[Spider]] = None,\n     log_none: bool = False,\n     log_multiple: bool = False,\n-) -> Optional[Type[Spider]]:\n+) -> Optional[type[Spider]]:\n     \"\"\"Return a spider class that handles the given Request.\n \n     This will look for the spiders that can handle the given request (using\n\n@@ -9,17 +9,7 @@ import os\n from importlib import import_module\n from pathlib import Path\n from posixpath import split\n-from typing import (\n-    TYPE_CHECKING,\n-    Any,\n-    Awaitable,\n-    Dict,\n-    List,\n-    Optional,\n-    Tuple,\n-    Type,\n-    TypeVar,\n-)\n+from typing import TYPE_CHECKING, Any, Optional, TypeVar\n from unittest import TestCase, mock\n \n from twisted.trial.unittest import SkipTest\n@@ -29,6 +19,8 @@ from scrapy.crawler import Crawler\n from scrapy.utils.boto import is_botocore_available\n \n if TYPE_CHECKING:\n+    from collections.abc import Awaitable\n+\n     from twisted.internet.defer import Deferred\n     from twisted.web.client import Response as TxResponse\n \n@@ -48,7 +40,7 @@ def skip_if_no_boto() -> None:\n \n def get_gcs_content_and_delete(\n     bucket: Any, path: str\n-) -> Tuple[bytes, List[Dict[str, str]], Any]:\n+) -> tuple[bytes, list[dict[str, str]], Any]:\n     from google.cloud import storage\n \n     client = storage.Client(project=os.environ.get(\"GCS_PROJECT_ID\"))\n@@ -75,7 +67,7 @@ def get_ftp_content_and_delete(\n     ftp.login(username, password)\n     if use_active_mode:\n         ftp.set_pasv(False)\n-    ftp_data: List[bytes] = []\n+    ftp_data: list[bytes] = []\n \n     def buffer_data(data: bytes) -> None:\n         ftp_data.append(data)\n@@ -92,8 +84,8 @@ class TestSpider(Spider):\n \n \n def get_crawler(\n-    spidercls: Optional[Type[Spider]] = None,\n-    settings_dict: Optional[Dict[str, Any]] = None,\n+    spidercls: Optional[type[Spider]] = None,\n+    settings_dict: Optional[dict[str, Any]] = None,\n     prevent_warnings: bool = True,\n ) -> Crawler:\n     \"\"\"Return an unconfigured Crawler object. If settings_dict is given, it\n@@ -103,7 +95,7 @@ def get_crawler(\n     from scrapy.crawler import CrawlerRunner\n \n     # Set by default settings that prevent deprecation warnings.\n-    settings: Dict[str, Any] = {}\n+    settings: dict[str, Any] = {}\n     settings.update(settings_dict or {})\n     runner = CrawlerRunner(settings)\n     crawler = runner.create_crawler(spidercls or TestSpider)\n@@ -118,7 +110,7 @@ def get_pythonpath() -> str:\n     return str(Path(scrapy_path).parent) + os.pathsep + os.environ.get(\"PYTHONPATH\", \"\")\n \n \n-def get_testenv() -> Dict[str, str]:\n+def get_testenv() -> dict[str, str]:\n     \"\"\"Return a OS environment dict suitable to fork processes that need to import\n     this installation of Scrapy, instead of a system installed one.\n     \"\"\"\n@@ -143,7 +135,7 @@ def get_from_asyncio_queue(value: _T) -> Awaitable[_T]:\n     return getter\n \n \n-def mock_google_cloud_storage() -> Tuple[Any, Any, Any]:\n+def mock_google_cloud_storage() -> tuple[Any, Any, Any]:\n     \"\"\"Creates autospec mocks for google-cloud-storage Client, Bucket and Blob\n     classes and set their proper return values.\n     \"\"\"\n\n@@ -2,13 +2,15 @@ from __future__ import annotations\n \n import os\n import sys\n-from typing import TYPE_CHECKING, Iterable, List, Optional, Tuple, cast\n+from typing import TYPE_CHECKING, Optional, cast\n \n from twisted.internet.defer import Deferred\n from twisted.internet.error import ProcessTerminated\n from twisted.internet.protocol import ProcessProtocol\n \n if TYPE_CHECKING:\n+    from collections.abc import Iterable\n+\n     from twisted.python.failure import Failure\n \n \n@@ -36,8 +38,8 @@ class ProcessTest:\n         return pp.deferred\n \n     def _process_finished(\n-        self, pp: TestProcessProtocol, cmd: List[str], check_code: bool\n-    ) -> Tuple[int, bytes, bytes]:\n+        self, pp: TestProcessProtocol, cmd: list[str], check_code: bool\n+    ) -> tuple[int, bytes, bytes]:\n         if pp.exitcode and check_code:\n             msg = f\"process {cmd} exit with code {pp.exitcode}\"\n             msg += f\"\\n>>> stdout <<<\\n{pp.out.decode()}\"\n\n@@ -9,19 +9,23 @@ and no performance penalty at all when disabled (as object_ref becomes just an\n alias to object in that case).\n \"\"\"\n \n+from __future__ import annotations\n+\n from collections import defaultdict\n from operator import itemgetter\n from time import time\n-from typing import TYPE_CHECKING, Any, DefaultDict, Iterable\n+from typing import TYPE_CHECKING, Any\n from weakref import WeakKeyDictionary\n \n if TYPE_CHECKING:\n+    from collections.abc import Iterable\n+\n     # typing.Self requires Python 3.11\n     from typing_extensions import Self\n \n \n NoneType = type(None)\n-live_refs: DefaultDict[type, WeakKeyDictionary] = defaultdict(WeakKeyDictionary)\n+live_refs: defaultdict[type, WeakKeyDictionary] = defaultdict(WeakKeyDictionary)\n \n \n class object_ref:\n@@ -29,7 +33,7 @@ class object_ref:\n \n     __slots__ = ()\n \n-    def __new__(cls, *args: Any, **kwargs: Any) -> \"Self\":\n+    def __new__(cls, *args: Any, **kwargs: Any) -> Self:\n         obj = object.__new__(cls)\n         live_refs[cls][obj] = time()\n         return obj\n\n@@ -6,8 +6,10 @@ Some of the functions that used to be imported from this module have been moved\n to the w3lib.url module. Always import those from there instead.\n \"\"\"\n \n+from __future__ import annotations\n+\n import re\n-from typing import TYPE_CHECKING, Iterable, Optional, Type, Union, cast\n+from typing import TYPE_CHECKING, Optional, Union, cast\n from urllib.parse import ParseResult, urldefrag, urlparse, urlunparse\n \n # scrapy.utils.url was moved to w3lib.url and import * ensures this\n@@ -18,6 +20,8 @@ from w3lib.url import _safe_chars, _unquotepath  # noqa: F401\n from scrapy.utils.python import to_unicode\n \n if TYPE_CHECKING:\n+    from collections.abc import Iterable\n+\n     from scrapy import Spider\n \n \n@@ -33,7 +37,7 @@ def url_is_from_any_domain(url: UrlT, domains: Iterable[str]) -> bool:\n     return any((host == d) or (host.endswith(f\".{d}\")) for d in domains)\n \n \n-def url_is_from_spider(url: UrlT, spider: Type[\"Spider\"]) -> bool:\n+def url_is_from_spider(url: UrlT, spider: type[Spider]) -> bool:\n     \"\"\"Return True if the url belongs to the given spider\"\"\"\n     return url_is_from_any_domain(\n         url, [spider.name] + list(getattr(spider, \"allowed_domains\", []))\n\n@@ -1,6 +1,5 @@\n import platform\n import sys\n-from typing import List, Tuple\n \n import cryptography\n import cssselect\n@@ -13,7 +12,7 @@ import scrapy\n from scrapy.utils.ssl import get_openssl_version\n \n \n-def scrapy_components_versions() -> List[Tuple[str, str]]:\n+def scrapy_components_versions() -> list[tuple[str, str]]:\n     lxml_version = \".\".join(map(str, lxml.etree.LXML_VERSION))\n     libxml2_version = \".\".join(map(str, lxml.etree.LIBXML_VERSION))\n \n\n@@ -6,12 +6,12 @@ version = (Path(__file__).parent / \"scrapy/VERSION\").read_text(\"ascii\").strip()\n \n \n install_requires = [\n-    \"Twisted>=18.9.0\",\n-    \"cryptography>=36.0.0\",\n+    \"Twisted>=21.7.0\",\n+    \"cryptography>=37.0.0\",\n     \"cssselect>=0.9.1\",\n     \"itemloaders>=1.0.1\",\n     \"parsel>=1.5.0\",\n-    \"pyOpenSSL>=21.0.0\",\n+    \"pyOpenSSL>=22.0.0\",\n     \"queuelib>=1.4.2\",\n     \"service_identity>=18.1.0\",\n     \"w3lib>=1.17.0\",\n@@ -20,7 +20,7 @@ install_requires = [\n     \"itemadapter>=0.1.0\",\n     \"packaging\",\n     \"tldextract\",\n-    \"lxml>=4.4.1\",\n+    \"lxml>=4.6.0\",\n     \"defusedxml>=0.7.1\",\n ]\n extras_require = {\n@@ -58,7 +58,6 @@ setup(\n         \"Operating System :: OS Independent\",\n         \"Programming Language :: Python\",\n         \"Programming Language :: Python :: 3\",\n-        \"Programming Language :: Python :: 3.8\",\n         \"Programming Language :: Python :: 3.9\",\n         \"Programming Language :: Python :: 3.10\",\n         \"Programming Language :: Python :: 3.11\",\n@@ -69,7 +68,7 @@ setup(\n         \"Topic :: Software Development :: Libraries :: Application Frameworks\",\n         \"Topic :: Software Development :: Libraries :: Python Modules\",\n     ],\n-    python_requires=\">=3.8\",\n+    python_requires=\">=3.9\",\n     install_requires=install_requires,\n     extras_require=extras_require,\n )\n\n@@ -1,7 +1,7 @@\n \"\"\"DBM-like dummy module\"\"\"\n \n-import collections\n-from typing import Any, DefaultDict\n+from collections import defaultdict\n+from typing import Any\n \n \n class DummyDB(dict):\n@@ -14,7 +14,7 @@ class DummyDB(dict):\n error = KeyError\n \n \n-_DATABASES: DefaultDict[Any, DummyDB] = collections.defaultdict(DummyDB)\n+_DATABASES: defaultdict[Any, DummyDB] = defaultdict(DummyDB)\n \n \n def open(file, flag=\"r\", mode=0o666):\n\n@@ -9,7 +9,7 @@ from pathlib import Path\n from shutil import rmtree\n from subprocess import PIPE, Popen\n from tempfile import mkdtemp\n-from typing import TYPE_CHECKING, Dict\n+from typing import TYPE_CHECKING\n from urllib.parse import urlencode\n \n from OpenSSL import SSL\n@@ -37,7 +37,7 @@ def getarg(request, name, default=None, type=None):\n     return default\n \n \n-def get_mockserver_env() -> Dict[str, str]:\n+def get_mockserver_env() -> dict[str, str]:\n     \"\"\"Return a OS environment dict suitable to run mockserver processes.\"\"\"\n \n     tests_path = Path(__file__).parent.parent\n\n@@ -1,5 +1,5 @@\n import itertools\n-from typing import Any, Dict\n+from typing import Any\n from unittest.mock import patch\n \n from twisted.internet.defer import inlineCallbacks\n@@ -17,7 +17,7 @@ class SimpleAddon:\n         pass\n \n \n-def get_addon_cls(config: Dict[str, Any]) -> type:\n+def get_addon_cls(config: dict[str, Any]) -> type:\n     class AddonWithConfig:\n         def update_settings(self, settings: BaseSettings):\n             settings.update(config, priority=\"addon\")\n\n@@ -1,3 +1,5 @@\n+from __future__ import annotations\n+\n import argparse\n import inspect\n import json\n@@ -13,7 +15,7 @@ from shutil import copytree, rmtree\n from stat import S_IWRITE as ANYONE_WRITE_PERMISSION\n from tempfile import TemporaryFile, mkdtemp\n from threading import Timer\n-from typing import Dict, Iterator, Optional, Union\n+from typing import TYPE_CHECKING, Optional, Union\n from unittest import skipIf\n \n from pytest import mark\n@@ -27,6 +29,9 @@ from scrapy.utils.python import to_unicode\n from scrapy.utils.test import get_testenv\n from tests.test_crawler import ExceptionSpider, NoRequestsSpider\n \n+if TYPE_CHECKING:\n+    from collections.abc import Iterator\n+\n \n class CommandSettings(unittest.TestCase):\n     def setUp(self):\n@@ -194,7 +199,7 @@ class StartprojectTest(ProjectTest):\n \n def get_permissions_dict(\n     path: Union[str, os.PathLike], renamings=None, ignore=None\n-) -> Dict[str, str]:\n+) -> dict[str, str]:\n     def get_permissions(path: Path) -> str:\n         return oct(path.stat().st_mode)\n \n\n@@ -6,7 +6,6 @@ import subprocess\n import sys\n import warnings\n from pathlib import Path\n-from typing import List\n \n import pytest\n from packaging.version import parse as parse_version\n@@ -651,7 +650,7 @@ class ScriptRunnerMixin:\n     script_dir: Path\n     cwd = os.getcwd()\n \n-    def get_script_args(self, script_name: str, *script_args: str) -> List[str]:\n+    def get_script_args(self, script_name: str, *script_args: str) -> list[str]:\n         script_path = self.script_dir / script_name\n         return [sys.executable, str(script_path)] + list(script_args)\n \n\n@@ -4,7 +4,7 @@ import shutil\n import sys\n from pathlib import Path\n from tempfile import mkdtemp, mkstemp\n-from typing import Optional, Type\n+from typing import Optional\n from unittest import SkipTest, mock\n \n from testfixtures import LogCapture\n@@ -218,7 +218,7 @@ class DuplicateHeaderResource(resource.Resource):\n \n class HttpTestCase(unittest.TestCase):\n     scheme = \"http\"\n-    download_handler_cls: Type = HTTPDownloadHandler\n+    download_handler_cls: type = HTTPDownloadHandler\n \n     # only used for HTTPS tests\n     keyfile = \"keys/localhost.key\"\n@@ -428,7 +428,7 @@ class HttpTestCase(unittest.TestCase):\n class Http10TestCase(HttpTestCase):\n     \"\"\"HTTP 1.0 test case\"\"\"\n \n-    download_handler_cls: Type = HTTP10DownloadHandler\n+    download_handler_cls: type = HTTP10DownloadHandler\n \n     def test_protocol(self):\n         request = Request(self.getURL(\"host\"), method=\"GET\")\n@@ -445,7 +445,7 @@ class Https10TestCase(Http10TestCase):\n class Http11TestCase(HttpTestCase):\n     \"\"\"HTTP 1.1 test case\"\"\"\n \n-    download_handler_cls: Type = HTTP11DownloadHandler\n+    download_handler_cls: type = HTTP11DownloadHandler\n \n     def test_download_without_maxsize_limit(self):\n         request = Request(self.getURL(\"file\"))\n@@ -645,7 +645,7 @@ class Https11InvalidDNSPattern(Https11TestCase):\n \n class Https11CustomCiphers(unittest.TestCase):\n     scheme = \"https\"\n-    download_handler_cls: Type = HTTP11DownloadHandler\n+    download_handler_cls: type = HTTP11DownloadHandler\n \n     keyfile = \"keys/localhost.key\"\n     certfile = \"keys/localhost.crt\"\n@@ -740,7 +740,7 @@ class UriResource(resource.Resource):\n \n \n class HttpProxyTestCase(unittest.TestCase):\n-    download_handler_cls: Type = HTTPDownloadHandler\n+    download_handler_cls: type = HTTPDownloadHandler\n     expected_http_proxy_request_body = b\"http://example.com\"\n \n     def setUp(self):\n@@ -783,14 +783,14 @@ class HttpProxyTestCase(unittest.TestCase):\n \n \n class Http10ProxyTestCase(HttpProxyTestCase):\n-    download_handler_cls: Type = HTTP10DownloadHandler\n+    download_handler_cls: type = HTTP10DownloadHandler\n \n     def test_download_with_proxy_https_noconnect(self):\n         raise unittest.SkipTest(\"noconnect is not supported in HTTP10DownloadHandler\")\n \n \n class Http11ProxyTestCase(HttpProxyTestCase):\n-    download_handler_cls: Type = HTTP11DownloadHandler\n+    download_handler_cls: type = HTTP11DownloadHandler\n \n     @defer.inlineCallbacks\n     def test_download_with_proxy_https_timeout(self):\n@@ -845,7 +845,7 @@ class S3AnonTestCase(unittest.TestCase):\n \n \n class S3TestCase(unittest.TestCase):\n-    download_handler_cls: Type = S3DownloadHandler\n+    download_handler_cls: type = S3DownloadHandler\n \n     # test use same example keys than amazon developer guide\n     # http://s3.amazonaws.com/awsdocs/S3/20060301/s3-dg-20060301.pdf\n\n@@ -8,7 +8,7 @@ import string\n from ipaddress import IPv4Address\n from pathlib import Path\n from tempfile import mkdtemp\n-from typing import TYPE_CHECKING, Dict\n+from typing import TYPE_CHECKING\n from unittest import mock, skipIf\n from urllib.parse import urlencode\n \n@@ -152,7 +152,7 @@ class QueryParams(LeafResource):\n         request.setHeader(\"Content-Type\", \"application/json; charset=UTF-8\")\n         request.setHeader(\"Content-Encoding\", \"UTF-8\")\n \n-        query_params: Dict[str, str] = {}\n+        query_params: dict[str, str] = {}\n         assert request.args is not None\n         for k, v in request.args.items():\n             query_params[str(k, \"utf-8\")] = str(v[0], \"utf-8\")\n\n@@ -3,7 +3,7 @@ import re\n import unittest\n import warnings\n import xmlrpc.client\n-from typing import Any, Dict, List\n+from typing import Any\n from unittest import mock\n from urllib.parse import parse_qs, unquote_to_bytes\n \n@@ -23,8 +23,8 @@ from scrapy.utils.python import to_bytes, to_unicode\n class RequestTest(unittest.TestCase):\n     request_class = Request\n     default_method = \"GET\"\n-    default_headers: Dict[bytes, List[bytes]] = {}\n-    default_meta: Dict[str, Any] = {}\n+    default_headers: dict[bytes, list[bytes]] = {}\n+    default_meta: dict[str, Any] = {}\n \n     def test_init(self):\n         # Request requires url in the __init__ method\n\n@@ -1,7 +1,7 @@\n import shutil\n from pathlib import Path\n from tempfile import mkdtemp\n-from typing import Optional, Set\n+from typing import Optional\n \n from testfixtures import LogCapture\n from twisted.internet import defer\n@@ -57,7 +57,7 @@ class FileDownloadCrawlTestCase(TestCase):\n     store_setting_key = \"FILES_STORE\"\n     media_key = \"files\"\n     media_urls_key = \"file_urls\"\n-    expected_checksums: Optional[Set[str]] = {\n+    expected_checksums: Optional[set[str]] = {\n         \"5547178b89448faf0015a13f904c936e\",\n         \"c2281c83670e31d8aaab7cb642b824db\",\n         \"ed3f6538dc15d4d9179dae57319edc5f\",\n\n@@ -7,7 +7,6 @@ from io import BytesIO\n from pathlib import Path\n from shutil import rmtree\n from tempfile import mkdtemp\n-from typing import Dict, List\n from unittest import mock\n from urllib.parse import urlparse\n \n@@ -309,11 +308,11 @@ class FilesPipelineTestCaseFieldsDataClass(\n class FilesPipelineTestAttrsItem:\n     name = attr.ib(default=\"\")\n     # default fields\n-    file_urls: List[str] = attr.ib(default=lambda: [])\n-    files: List[Dict[str, str]] = attr.ib(default=lambda: [])\n+    file_urls: list[str] = attr.ib(default=lambda: [])\n+    files: list[dict[str, str]] = attr.ib(default=lambda: [])\n     # overridden fields\n-    custom_file_urls: List[str] = attr.ib(default=lambda: [])\n-    custom_files: List[Dict[str, str]] = attr.ib(default=lambda: [])\n+    custom_file_urls: list[str] = attr.ib(default=lambda: [])\n+    custom_files: list[dict[str, str]] = attr.ib(default=lambda: [])\n \n \n class FilesPipelineTestCaseFieldsAttrsItem(\n\n@@ -5,7 +5,7 @@ import random\n import warnings\n from shutil import rmtree\n from tempfile import mkdtemp\n-from typing import Dict, List, Optional\n+from typing import Optional\n from unittest.mock import patch\n \n import attr\n@@ -406,11 +406,11 @@ class ImagesPipelineTestCaseFieldsDataClass(\n class ImagesPipelineTestAttrsItem:\n     name = attr.ib(default=\"\")\n     # default fields\n-    image_urls: List[str] = attr.ib(default=lambda: [])\n-    images: List[Dict[str, str]] = attr.ib(default=lambda: [])\n+    image_urls: list[str] = attr.ib(default=lambda: [])\n+    images: list[dict[str, str]] = attr.ib(default=lambda: [])\n     # overridden fields\n-    custom_image_urls: List[str] = attr.ib(default=lambda: [])\n-    custom_images: List[Dict[str, str]] = attr.ib(default=lambda: [])\n+    custom_image_urls: list[str] = attr.ib(default=lambda: [])\n+    custom_images: list[dict[str, str]] = attr.ib(default=lambda: [])\n \n \n class ImagesPipelineTestCaseFieldsAttrsItem(\n\n@@ -1,5 +1,3 @@\n-from typing import List\n-\n from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n@@ -64,7 +62,7 @@ class KeywordArgumentsSpider(MockServerSpider):\n         },\n     }\n \n-    checks: List[bool] = []\n+    checks: list[bool] = []\n \n     def start_requests(self):\n         data = {\"key\": \"value\", \"number\": 123, \"callback\": \"some_callback\"}\n\n@@ -1,4 +1,4 @@\n-from typing import Dict, Optional\n+from typing import Optional\n from unittest import TestCase\n from urllib.parse import urljoin\n \n@@ -20,7 +20,7 @@ URLS = [urljoin(\"https://example.org\", p) for p in PATHS]\n \n class MinimalScheduler:\n     def __init__(self) -> None:\n-        self.requests: Dict[bytes, Request] = {}\n+        self.requests: dict[bytes, Request] = {}\n \n     def has_pending_requests(self) -> bool:\n         return bool(self.requests)\n\n@@ -105,9 +105,10 @@ class BaseSettingsTest(unittest.TestCase):\n \n     def test_set_calls_settings_attributes_methods_on_update(self):\n         attr = SettingsAttribute(\"value\", 10)\n-        with mock.patch.object(attr, \"__setattr__\") as mock_setattr, mock.patch.object(\n-            attr, \"set\"\n-        ) as mock_set:\n+        with (\n+            mock.patch.object(attr, \"__setattr__\") as mock_setattr,\n+            mock.patch.object(attr, \"set\") as mock_set,\n+        ):\n             self.settings.attributes = {\"TEST_OPTION\": attr}\n \n             for priority in (0, 10, 20):\n\n@@ -1,4 +1,4 @@\n-import collections.abc\n+from collections.abc import AsyncIterator, Iterable\n from typing import Optional, Union\n from unittest import mock\n \n@@ -147,7 +147,7 @@ class BaseAsyncSpiderMiddlewareTestCase(SpiderMiddlewareTestCase):\n             result = yield self._get_middleware_result(\n                 *mw_classes, start_index=start_index\n             )\n-        self.assertIsInstance(result, collections.abc.Iterable)\n+        self.assertIsInstance(result, Iterable)\n         result_list = list(result)\n         self.assertEqual(len(result_list), self.RESULT_COUNT)\n         self.assertIsInstance(result_list[0], self.ITEM_TYPE)\n@@ -161,7 +161,7 @@ class BaseAsyncSpiderMiddlewareTestCase(SpiderMiddlewareTestCase):\n             result = yield self._get_middleware_result(\n                 *mw_classes, start_index=start_index\n             )\n-        self.assertIsInstance(result, collections.abc.AsyncIterator)\n+        self.assertIsInstance(result, AsyncIterator)\n         result_list = yield deferred_from_coro(collect_asyncgen(result))\n         self.assertEqual(len(result_list), self.RESULT_COUNT)\n         self.assertIsInstance(result_list[0], self.ITEM_TYPE)\n\n@@ -1,5 +1,4 @@\n import logging\n-from typing import Set\n from unittest import TestCase\n \n from testfixtures import LogCapture\n@@ -17,7 +16,7 @@ from tests.spiders import MockServerSpider\n \n class _HttpErrorSpider(MockServerSpider):\n     name = \"httperror\"\n-    bypass_status_codes: Set[int] = set()\n+    bypass_status_codes: set[int] = set()\n \n     def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n\n@@ -1,5 +1,5 @@\n import warnings\n-from typing import Any, Dict, List, Optional, Tuple\n+from typing import Any, Optional\n from unittest import TestCase\n from urllib.parse import urlparse\n \n@@ -32,10 +32,10 @@ from scrapy.spiders import Spider\n \n \n class TestRefererMiddleware(TestCase):\n-    req_meta: Dict[str, Any] = {}\n-    resp_headers: Dict[str, str] = {}\n-    settings: Dict[str, Any] = {}\n-    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n+    req_meta: dict[str, Any] = {}\n+    resp_headers: dict[str, str] = {}\n+    settings: dict[str, Any] = {}\n+    scenarii: list[tuple[str, str, Optional[bytes]]] = [\n         (\"http://scrapytest.org\", \"http://scrapytest.org/\", b\"http://scrapytest.org\"),\n     ]\n \n@@ -65,7 +65,7 @@ class MixinDefault:\n     with some additional filtering of s3://\n     \"\"\"\n \n-    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n+    scenarii: list[tuple[str, str, Optional[bytes]]] = [\n         (\"https://example.com/\", \"https://scrapy.org/\", b\"https://example.com/\"),\n         (\"http://example.com/\", \"http://scrapy.org/\", b\"http://example.com/\"),\n         (\"http://example.com/\", \"https://scrapy.org/\", b\"http://example.com/\"),\n@@ -86,7 +86,7 @@ class MixinDefault:\n \n \n class MixinNoReferrer:\n-    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n+    scenarii: list[tuple[str, str, Optional[bytes]]] = [\n         (\"https://example.com/page.html\", \"https://example.com/\", None),\n         (\"http://www.example.com/\", \"https://scrapy.org/\", None),\n         (\"http://www.example.com/\", \"http://scrapy.org/\", None),\n@@ -96,7 +96,7 @@ class MixinNoReferrer:\n \n \n class MixinNoReferrerWhenDowngrade:\n-    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n+    scenarii: list[tuple[str, str, Optional[bytes]]] = [\n         # TLS to TLS: send non-empty referrer\n         (\n             \"https://example.com/page.html\",\n@@ -178,7 +178,7 @@ class MixinNoReferrerWhenDowngrade:\n \n \n class MixinSameOrigin:\n-    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n+    scenarii: list[tuple[str, str, Optional[bytes]]] = [\n         # Same origin (protocol, host, port): send referrer\n         (\n             \"https://example.com/page.html\",\n@@ -247,7 +247,7 @@ class MixinSameOrigin:\n \n \n class MixinOrigin:\n-    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n+    scenarii: list[tuple[str, str, Optional[bytes]]] = [\n         # TLS or non-TLS to TLS or non-TLS: referrer origin is sent (yes, even for downgrades)\n         (\n             \"https://example.com/page.html\",\n@@ -271,7 +271,7 @@ class MixinOrigin:\n \n \n class MixinStrictOrigin:\n-    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n+    scenarii: list[tuple[str, str, Optional[bytes]]] = [\n         # TLS or non-TLS to TLS or non-TLS: referrer origin is sent but not for downgrades\n         (\n             \"https://example.com/page.html\",\n@@ -299,7 +299,7 @@ class MixinStrictOrigin:\n \n \n class MixinOriginWhenCrossOrigin:\n-    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n+    scenarii: list[tuple[str, str, Optional[bytes]]] = [\n         # Same origin (protocol, host, port): send referrer\n         (\n             \"https://example.com/page.html\",\n@@ -406,7 +406,7 @@ class MixinOriginWhenCrossOrigin:\n \n \n class MixinStrictOriginWhenCrossOrigin:\n-    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n+    scenarii: list[tuple[str, str, Optional[bytes]]] = [\n         # Same origin (protocol, host, port): send referrer\n         (\n             \"https://example.com/page.html\",\n@@ -518,7 +518,7 @@ class MixinStrictOriginWhenCrossOrigin:\n \n \n class MixinUnsafeUrl:\n-    scenarii: List[Tuple[str, str, Optional[bytes]]] = [\n+    scenarii: list[tuple[str, str, Optional[bytes]]] = [\n         # TLS to TLS: send referrer\n         (\n             \"https://example.com/sekrit.html\",\n@@ -968,8 +968,8 @@ class TestPolicyHeaderPrecedence004(\n \n class TestReferrerOnRedirect(TestRefererMiddleware):\n     settings = {\"REFERRER_POLICY\": \"scrapy.spidermiddlewares.referer.UnsafeUrlPolicy\"}\n-    scenarii: List[\n-        Tuple[str, str, Tuple[Tuple[int, str], ...], Optional[bytes], Optional[bytes]]\n+    scenarii: list[\n+        tuple[str, str, tuple[tuple[int, str], ...], Optional[bytes], Optional[bytes]]\n     ] = [  # type: ignore[assignment]\n         (\n             \"http://scrapytest.org/1\",  # parent\n\n@@ -1,8 +1,7 @@\n import copy\n import unittest\n import warnings\n-from collections.abc import Mapping, MutableMapping\n-from typing import Iterator\n+from collections.abc import Iterator, Mapping, MutableMapping\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request\n\n@@ -1,10 +1,12 @@\n+from __future__ import annotations\n+\n import json\n import logging\n import re\n import sys\n import unittest\n from io import StringIO\n-from typing import Any, Dict, Mapping, MutableMapping\n+from typing import TYPE_CHECKING, Any\n from unittest import TestCase\n \n import pytest\n@@ -21,6 +23,9 @@ from scrapy.utils.log import (\n from scrapy.utils.test import get_crawler\n from tests.spiders import LogSpider\n \n+if TYPE_CHECKING:\n+    from collections.abc import Mapping, MutableMapping\n+\n \n class FailureToExcInfoTest(unittest.TestCase):\n     def test_failure(self):\n@@ -133,7 +138,7 @@ class StreamLoggerTest(unittest.TestCase):\n     ),\n )\n def test_spider_logger_adapter_process(\n-    base_extra: Mapping[str, Any], log_extra: MutableMapping, expected_extra: Dict\n+    base_extra: Mapping[str, Any], log_extra: MutableMapping, expected_extra: dict\n ):\n     logger = logging.getLogger(\"test\")\n     spider_logger_adapter = SpiderLoggerAdapter(logger, base_extra)\n\n@@ -2,7 +2,7 @@ import json\n import unittest\n import warnings\n from hashlib import sha1\n-from typing import Dict, Optional, Tuple, Union\n+from typing import Optional, Union\n from weakref import WeakKeyDictionary\n \n from scrapy.http import Request\n@@ -57,11 +57,11 @@ class FingerprintTest(unittest.TestCase):\n \n     function: staticmethod = staticmethod(fingerprint)\n     cache: Union[\n-        \"WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], bytes]]\",\n-        \"WeakKeyDictionary[Request, Dict[Tuple[Optional[Tuple[bytes, ...]], bool], str]]\",\n+        \"WeakKeyDictionary[Request, dict[tuple[Optional[tuple[bytes, ...]], bool], bytes]]\",\n+        \"WeakKeyDictionary[Request, dict[tuple[Optional[tuple[bytes, ...]], bool], str]]\",\n     ] = _fingerprint_cache\n     default_cache_key = (None, False)\n-    known_hashes: Tuple[Tuple[Request, Union[bytes, str], Dict], ...] = (\n+    known_hashes: tuple[tuple[Request, Union[bytes, str], dict], ...] = (\n         (\n             Request(\"http://example.org\"),\n             b\"xs\\xd7\\x0c3uj\\x15\\xfe\\xd7d\\x9b\\xa9\\t\\xe0d\\xbf\\x9cXD\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
