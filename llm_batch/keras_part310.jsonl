{"custom_id": "keras#d592b6ca19fb1c9c561862f9a968b52e6e073a27", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1990 | Lines Deleted: 1632 | Files Changed: 45 | Hunks: 210 | Methods Changed: 245 | Complexity Δ (Sum/Max): 27/63 | Churn Δ: 3622 | Churn Cumulative: 64103 | Contributors (this commit): 166 | Commits (past 90d): 96 | Contributors (cumulative): 289 | DMM Complexity: 0.5502392344497608\n\nDIFF:\n@@ -205,7 +205,7 @@ def MobileNetV2(input_shape=None,\n         raise ValueError('input_tensor: ', input_tensor,\n                          'is not type input_tensor')\n     if is_input_t_tensor:\n-      if backend.image_data_format == 'channels_first':\n+      if backend.image_data_format() == 'channels_first':\n         if backend.int_shape(input_tensor)[1] != input_shape[1]:\n           raise ValueError('input_shape: ', input_shape, 'and input_tensor: ',\n                            input_tensor,\n\n@@ -78,11 +78,6 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n \n   Optionally loads weights pre-trained on ImageNet.\n \n-  Note: each Keras Application expects a specific kind of input preprocessing.\n-  For MobileNetV3, call\n-  `tf.keras.applications.mobilenet_v3.preprocess_input` on your\n-  inputs before passing them to the model.\n-\n   Arguments:\n     input_shape: Optional shape tuple, to be specified if you would\n       like to use a model with an input image resolution that is not\n@@ -137,6 +132,10 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n       on the \"top\" layer. Ignored unless `include_top=True`. Set\n       `classifier_activation=None` to return the logits of the \"top\" layer.\n \n+  Call arguments:\n+    inputs: A floating point `numpy.array` or a `tf.Tensor`, 4D with 3 color\n+      channels, with values in the range [0, 255].\n+\n   Returns:\n     A `keras.Model` instance.\n \n@@ -185,7 +184,7 @@ def MobileNetV3(stack_fn,\n         raise ValueError('input_tensor: ', input_tensor,\n                          'is not type input_tensor')\n     if is_input_t_tensor:\n-      if backend.image_data_format == 'channels_first':\n+      if backend.image_data_format() == 'channels_first':\n         if backend.int_shape(input_tensor)[1] != input_shape[1]:\n           raise ValueError('input_shape: ', input_shape, 'and input_tensor: ',\n                            input_tensor,\n@@ -556,6 +555,24 @@ def _inverted_res_block(x, expansion, filters, kernel_size, stride, se_ratio,\n \n @keras_export('keras.applications.mobilenet_v3.preprocess_input')\n def preprocess_input(x, data_format=None):  # pylint: disable=unused-argument\n+  \"\"\"A placeholder method for backward compatibility.\n+\n+  The preprocessing logic has been included in the mobilenet_v3 model\n+  implementation. Users are no longer required to call this method to normalize\n+  the input data. This method does nothing and only kept as a placeholder to\n+  align the API surface between old and new version of model.\n+\n+  Args:\n+    x: A floating point `numpy.array` or a `tf.Tensor`.\n+    data_format: Optional data format of the image tensor/array. Defaults to\n+      None, in which case the global setting\n+      `tf.keras.backend.image_data_format()` is used (unless you changed it,\n+      it defaults to \"channels_last\").{mode}\n+\n+  Returns:\n+    Unchanged `numpy.array` or `tf.Tensor`.\n+  \"\"\"\n+\n   return x\n \n \n@@ -564,8 +581,4 @@ def decode_predictions(preds, top=5):\n   return imagenet_utils.decode_predictions(preds, top=top)\n \n \n-preprocess_input.__doc__ = imagenet_utils.PREPROCESS_INPUT_DOC.format(\n-    mode='',\n-    ret=imagenet_utils.PREPROCESS_INPUT_RET_DOC_TF,\n-    error=imagenet_utils.PREPROCESS_INPUT_ERROR_DOC)\n decode_predictions.__doc__ = imagenet_utils.decode_predictions.__doc__\n\n@@ -84,8 +84,8 @@ def compare_two_inputs_op_to_numpy(keras_op,\n       backend.variable(input_a, dtype=dtype),\n       backend.variable(input_b, dtype=dtype), *keras_args, **keras_kwargs)\n   keras_output = backend.eval(keras_output)\n-  np_output = np_op(input_a.astype(dtype), input_b.astype(dtype),\n-                    *np_args, **np_kwargs)\n+  np_output = np_op(\n+      input_a.astype(dtype), input_b.astype(dtype), *np_args, **np_kwargs)\n   try:\n     np.testing.assert_allclose(keras_output, np_output, atol=1e-4)\n   except AssertionError:\n@@ -416,19 +416,31 @@ class BackendLinearAlgebraTest(tf.test.TestCase, parameterized.TestCase):\n         (backend.argmax, np.argmax),\n     ]\n     for keras_op, np_op in ops_to_test:\n-      compare_single_input_op_to_numpy(keras_op, np_op, input_shape=(4, 7, 5),\n+      compare_single_input_op_to_numpy(\n+          keras_op,\n+          np_op,\n+          input_shape=(4, 7, 5),\n           keras_kwargs={'axis': 1},\n           np_kwargs={'axis': 1})\n-      compare_single_input_op_to_numpy(keras_op, np_op, input_shape=(4, 7, 5),\n+      compare_single_input_op_to_numpy(\n+          keras_op,\n+          np_op,\n+          input_shape=(4, 7, 5),\n           keras_kwargs={'axis': -1},\n           np_kwargs={'axis': -1})\n       if 'keepdims' in tf_inspect.getargspec(keras_op).args:\n-        compare_single_input_op_to_numpy(keras_op, np_op,\n+        compare_single_input_op_to_numpy(\n+            keras_op,\n+            np_op,\n             input_shape=(4, 7, 5),\n-                                         keras_kwargs={'axis': 1,\n-                                                       'keepdims': True},\n-                                         np_kwargs={'axis': 1,\n-                                                    'keepdims': True})\n+            keras_kwargs={\n+                'axis': 1,\n+                'keepdims': True\n+            },\n+            np_kwargs={\n+                'axis': 1,\n+                'keepdims': True\n+            })\n \n   def test_elementwise_ops(self):\n     ops_to_test = [\n@@ -448,9 +460,8 @@ class BackendLinearAlgebraTest(tf.test.TestCase, parameterized.TestCase):\n         (backend.log, np.log),\n     ]\n     for keras_op, np_op in ops_to_test:\n-      compare_single_input_op_to_numpy(keras_op, np_op,\n-                                       input_shape=(4, 7),\n-                                       negative_values=False)\n+      compare_single_input_op_to_numpy(\n+          keras_op, np_op, input_shape=(4, 7), negative_values=False)\n \n     compare_single_input_op_to_numpy(\n         backend.clip,\n@@ -480,9 +491,8 @@ class BackendLinearAlgebraTest(tf.test.TestCase, parameterized.TestCase):\n         (backend.minimum, np.minimum),\n     ]\n     for keras_op, np_op in ops_to_test:\n-      compare_two_inputs_op_to_numpy(keras_op, np_op,\n-                                     input_shape_a=(4, 7),\n-                                     input_shape_b=(4, 7))\n+      compare_two_inputs_op_to_numpy(\n+          keras_op, np_op, input_shape_a=(4, 7), input_shape_b=(4, 7))\n \n   def test_relu(self):\n     x = tf.convert_to_tensor([[-4, 0], [2, 7]], 'float32')\n@@ -704,19 +714,14 @@ class BackendShapeOpsTest(tf.test.TestCase):\n         shape[2] += padding[1][0] + padding[1][1]\n         shape[3] += padding[2][0] + padding[2][1]\n         y = np.zeros(tuple(shape))\n-        y[:,\n-          padding[0][0]:-padding[0][1],\n-          padding[1][0]:-padding[1][1],\n-          padding[2][0]:-padding[2][1],\n-          :] = x\n+        y[:, padding[0][0]:-padding[0][1], padding[1][0]:-padding[1][1],\n+          padding[2][0]:-padding[2][1], :] = x\n       else:\n         shape[2] += padding[0][0] + padding[0][1]\n         shape[3] += padding[1][0] + padding[1][1]\n         shape[4] += padding[2][0] + padding[2][1]\n         y = np.zeros(tuple(shape))\n-        y[:, :,\n-          padding[0][0]:-padding[0][1],\n-          padding[1][0]:-padding[1][1],\n+        y[:, :, padding[0][0]:-padding[0][1], padding[1][0]:-padding[1][1],\n           padding[2][0]:-padding[2][1]] = x\n       return y\n \n@@ -744,18 +749,14 @@ class BackendNNOpsTest(tf.test.TestCase, parameterized.TestCase):\n   def test_bias_add(self):\n     keras_op = backend.bias_add\n     np_op = np.add\n-    compare_two_inputs_op_to_numpy(keras_op, np_op,\n-                                   input_shape_a=(4, 7),\n-                                   input_shape_b=(7,))\n-    compare_two_inputs_op_to_numpy(keras_op, np_op,\n-                                   input_shape_a=(4, 3, 7),\n-                                   input_shape_b=(7,))\n-    compare_two_inputs_op_to_numpy(keras_op, np_op,\n-                                   input_shape_a=(4, 3, 5, 7),\n-                                   input_shape_b=(7,))\n-    compare_two_inputs_op_to_numpy(keras_op, np_op,\n-                                   input_shape_a=(4, 3, 5, 2, 7),\n-                                   input_shape_b=(7,))\n+    compare_two_inputs_op_to_numpy(\n+        keras_op, np_op, input_shape_a=(4, 7), input_shape_b=(7,))\n+    compare_two_inputs_op_to_numpy(\n+        keras_op, np_op, input_shape_a=(4, 3, 7), input_shape_b=(7,))\n+    compare_two_inputs_op_to_numpy(\n+        keras_op, np_op, input_shape_a=(4, 3, 5, 7), input_shape_b=(7,))\n+    compare_two_inputs_op_to_numpy(\n+        keras_op, np_op, input_shape_a=(4, 3, 5, 2, 7), input_shape_b=(7,))\n \n     with self.assertRaises((ValueError, tf.errors.InvalidArgumentError)):\n       x = backend.variable((3, 4))\n@@ -778,12 +779,10 @@ class BackendNNOpsTest(tf.test.TestCase, parameterized.TestCase):\n         b = b.reshape((1, b.shape[0], 1, 1))\n       return x + b\n \n-    compare_two_inputs_op_to_numpy(keras_op, np_op,\n-                                   input_shape_a=(4, 3, 7),\n-                                   input_shape_b=(3,))\n-    compare_two_inputs_op_to_numpy(keras_op, np_op,\n-                                   input_shape_a=(4, 3, 5, 7),\n-                                   input_shape_b=(3,))\n+    compare_two_inputs_op_to_numpy(\n+        keras_op, np_op, input_shape_a=(4, 3, 7), input_shape_b=(3,))\n+    compare_two_inputs_op_to_numpy(\n+        keras_op, np_op, input_shape_a=(4, 3, 5, 7), input_shape_b=(3,))\n \n   def test_pool2d(self):\n     val = np.random.random((10, 3, 10, 10))\n@@ -838,8 +837,6 @@ class BackendNNOpsTest(tf.test.TestCase, parameterized.TestCase):\n       y = backend.pool2d(x, (2, 2), strides=(2, 2), pool_mode='other')\n \n   def test_pool3d(self):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest('Pooling with 3D tensors is not supported in ROCm')\n     val = np.random.random((10, 3, 10, 10, 10))\n     x = backend.variable(val)\n     y = backend.pool3d(\n@@ -929,18 +926,16 @@ class BackendNNOpsTest(tf.test.TestCase, parameterized.TestCase):\n           kernel_sizes = (kernel_size,) * dim\n           strides = (stride,) * dim\n \n-          output_shape = tuple([(i - kernel_size + stride) // stride\n-                                for i in input_spatial_shape])\n+          output_shape = tuple([\n+              (i - kernel_size + stride) // stride for i in input_spatial_shape\n+          ])\n \n           kernel_shape = (np.prod(output_shape),\n-                          np.prod(kernel_sizes) * channels_in,\n-                          filters)\n+                          np.prod(kernel_sizes) * channels_in, filters)\n \n           kernel = np.random.normal(\n-              0,\n-              1,\n-              output_shape + (channels_in, np.prod(kernel_sizes), filters)\n-          )\n+              0, 1,\n+              output_shape + (channels_in, np.prod(kernel_sizes), filters))\n \n           kernel_cf = np.reshape(kernel, kernel_shape)\n           kernel_cf = backend.variable(kernel_cf)\n@@ -948,14 +943,14 @@ class BackendNNOpsTest(tf.test.TestCase, parameterized.TestCase):\n           conv_cf = backend.local_conv(inputs_cf, kernel_cf, kernel_sizes,\n                                        strides, output_shape, 'channels_first')\n \n-          inputs_cl = np.transpose(inputs, [0, 2] + list(range(3, dim + 2)) +\n-                                   [1])\n+          inputs_cl = np.transpose(inputs,\n+                                   [0, 2] + list(range(3, dim + 2)) + [1])\n           inputs_cl = backend.variable(inputs_cl)\n \n           kernel_cl = np.reshape(\n-              np.transpose(kernel, list(range(dim)) + [dim + 1, dim, dim + 2]),\n-              kernel_shape\n-          )\n+              np.transpose(kernel,\n+                           list(range(dim)) + [dim + 1, dim, dim + 2]),\n+              kernel_shape)\n           kernel_cl = backend.variable(kernel_cl)\n \n           conv_cl = backend.local_conv(inputs_cl, kernel_cl, kernel_sizes,\n@@ -966,18 +961,13 @@ class BackendNNOpsTest(tf.test.TestCase, parameterized.TestCase):\n \n           self.assertAllCloseAccordingToType(\n               conv_cf,\n-              np.transpose(conv_cl,\n-                           [0, dim + 1] + list(range(1, dim + 1))),\n-              atol=1e-5\n-          )\n+              np.transpose(conv_cl, [0, dim + 1] + list(range(1, dim + 1))),\n+              atol=1e-5)\n \n   @parameterized.named_parameters(\n       ('local_conv1d', (5, 6), (3,), (1,), (3,)),\n       ('local_conv2d', (4, 5, 6), (3, 3), (1, 1), (2, 3)))\n-  def test_local_conv_1d_and_2d(self,\n-                                input_shape,\n-                                kernel_sizes,\n-                                strides,\n+  def test_local_conv_1d_and_2d(self, input_shape, kernel_sizes, strides,\n                                 output_shape):\n     filters = 3\n     batch_size = 2\n@@ -985,9 +975,9 @@ class BackendNNOpsTest(tf.test.TestCase, parameterized.TestCase):\n     inputs = np.random.normal(0, 1, (batch_size,) + input_shape)\n     inputs = backend.variable(inputs)\n \n-    kernel = np.random.normal(0, 1, (np.prod(output_shape),\n-                                     np.prod(kernel_sizes) * input_shape[-1],\n-                                     filters))\n+    kernel = np.random.normal(0, 1,\n+                              (np.prod(output_shape), np.prod(kernel_sizes) *\n+                               input_shape[-1], filters))\n     kernel = backend.variable(kernel)\n \n     local_conv = backend.local_conv(inputs, kernel, kernel_sizes, strides,\n@@ -1216,12 +1206,33 @@ class BackendNNOpsTest(tf.test.TestCase, parameterized.TestCase):\n     mask = backend.variable(np_mask)\n \n     kwargs_list = [\n-        {'go_backwards': False, 'mask': None},\n-        {'go_backwards': False, 'mask': None, 'unroll': True},\n-        {'go_backwards': True, 'mask': None},\n-        {'go_backwards': True, 'mask': None, 'unroll': True},\n-        {'go_backwards': False, 'mask': mask},\n-        {'go_backwards': False, 'mask': mask, 'unroll': True},\n+        {\n+            'go_backwards': False,\n+            'mask': None\n+        },\n+        {\n+            'go_backwards': False,\n+            'mask': None,\n+            'unroll': True\n+        },\n+        {\n+            'go_backwards': True,\n+            'mask': None\n+        },\n+        {\n+            'go_backwards': True,\n+            'mask': None,\n+            'unroll': True\n+        },\n+        {\n+            'go_backwards': False,\n+            'mask': mask\n+        },\n+        {\n+            'go_backwards': False,\n+            'mask': mask,\n+            'unroll': True\n+        },\n     ]\n     for i, kwargs in enumerate(kwargs_list):\n       last_output, outputs, new_states = backend.rnn(rnn_fn, inputs,\n@@ -1310,12 +1321,33 @@ class BackendNNOpsTest(tf.test.TestCase, parameterized.TestCase):\n     mask = backend.variable(np_mask)\n \n     kwargs_list = [\n-        {'go_backwards': False, 'mask': None},\n-        {'go_backwards': False, 'mask': None, 'unroll': True},\n-        {'go_backwards': True, 'mask': None},\n-        {'go_backwards': True, 'mask': None, 'unroll': True},\n-        {'go_backwards': False, 'mask': mask},\n-        {'go_backwards': False, 'mask': mask, 'unroll': True},\n+        {\n+            'go_backwards': False,\n+            'mask': None\n+        },\n+        {\n+            'go_backwards': False,\n+            'mask': None,\n+            'unroll': True\n+        },\n+        {\n+            'go_backwards': True,\n+            'mask': None\n+        },\n+        {\n+            'go_backwards': True,\n+            'mask': None,\n+            'unroll': True\n+        },\n+        {\n+            'go_backwards': False,\n+            'mask': mask\n+        },\n+        {\n+            'go_backwards': False,\n+            'mask': mask,\n+            'unroll': True\n+        },\n     ]\n     for i, kwargs in enumerate(kwargs_list):\n       last_output, outputs, new_states = backend.rnn(rnn_fn, inputs,\n@@ -1385,8 +1417,8 @@ class BackendNNOpsTest(tf.test.TestCase, parameterized.TestCase):\n     def step_function(inputs, states):\n       return inputs, [s + 1 for s in states]\n \n-    inputs_vals = np.random.random((num_samples, num_timesteps,\n-                                    state_and_io_size))\n+    inputs_vals = np.random.random(\n+        (num_samples, num_timesteps, state_and_io_size))\n     initial_state_vals = np.random.random((num_samples, state_and_io_size))\n     # masking of two last timesteps for second sample only\n     mask_vals = np.ones((num_samples, num_timesteps))\n@@ -1776,18 +1808,21 @@ class TestCTC(tf.test.TestCase):\n     depth = 6\n     seq_len_0 = 5\n     input_prob_matrix_0 = np.asarray(\n-        [[0.30999, 0.309938, 0.0679938, 0.0673362, 0.0708352, 0.173908],\n+        [\n+            [0.30999, 0.309938, 0.0679938, 0.0673362, 0.0708352, 0.173908],\n             [0.215136, 0.439699, 0.0370931, 0.0393967, 0.0381581, 0.230517],\n             [0.199959, 0.489485, 0.0233221, 0.0251417, 0.0233289, 0.238763],\n             [0.279611, 0.452966, 0.0204795, 0.0209126, 0.0194803, 0.20655],\n             [0.51286, 0.288951, 0.0243026, 0.0220788, 0.0219297, 0.129878],\n             # Random entry added in at time=5\n-         [0.155251, 0.164444, 0.173517, 0.176138, 0.169979, 0.160671]],\n+            [0.155251, 0.164444, 0.173517, 0.176138, 0.169979, 0.160671]\n+        ],\n         dtype=np.float32)\n \n     # len max_time_steps array of batch_size x depth matrices\n-    inputs = ([input_prob_matrix_0[t, :][np.newaxis, :]\n-               for t in range(seq_len_0)] +  # Pad to max_time_steps = 8\n+    inputs = (\n+        [input_prob_matrix_0[t, :][np.newaxis, :] for t in range(seq_len_0)\n+        ] +  # Pad to max_time_steps = 8\n         2 * [np.zeros((1, depth), dtype=np.float32)])\n \n     inputs = backend.variable(np.asarray(inputs).transpose((1, 0, 2)))\n@@ -1795,10 +1830,12 @@ class TestCTC(tf.test.TestCase):\n     # batch_size length vector of sequence_lengths\n     input_length = backend.variable(np.array([seq_len_0], dtype=np.int32))\n     # batch_size length vector of negative log probabilities\n-    log_prob_truth = np.array([\n+    log_prob_truth = np.array(\n+        [\n             -3.5821197,  # output beam 0\n             -3.777835  # output beam 1\n-    ], np.float32)[np.newaxis, :]\n+        ],\n+        np.float32)[np.newaxis, :]\n \n     decode_truth = [\n         np.array([1, 0, -1, -1, -1, -1, -1]),\n@@ -1857,9 +1894,9 @@ class TestCTC(tf.test.TestCase):\n \n       labels = np.asarray([[0, 1, 2, 1, 0]])\n       inputs = np.asarray(\n-          [[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553], [\n-              0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436\n-          ], [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688],\n+          [[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553],\n+            [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436],\n+            [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688],\n             [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533],\n             [0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107]]\n           ],\n\n@@ -21,6 +21,7 @@ from __future__ import print_function\n import tensorflow as tf\n \n import functools\n+import numpy as np\n import six\n from keras.benchmarks.layer_benchmarks import layer_benchmarks_test_base\n \n@@ -36,6 +37,16 @@ def _get_metadata(name):\n   }\n \n \n+def _get_input_data(inputs):\n+  if \"input_shape\" in inputs:\n+    return tf.ones(inputs[\"input_shape\"])\n+  elif \"input\" in inputs:\n+    return inputs[\"input\"]\n+  else:\n+    raise ValueError(\"Please specificy either `input_shape` or `input`\"\n+                     \"for the benchmark test\")\n+\n+\n def _generate_benchmark_params(*params_list):\n   benchmark_params = []\n   for params in params_list:\n@@ -65,19 +76,25 @@ class KerasLayerBenchmarks(six.with_metaclass(\n   _benchmark_parameters = _generate_benchmark_params([\n       (\"Conv2D_small_shape\", tf.keras.layers.Conv2D,\n        {\"filters\": 1, \"kernel_size\": 1, \"activation\": \"relu\"},\n-       (1, 1, 1, 1), 10000),\n+       {\"input_shape\": (1, 1, 1, 1)}, 10),\n       (\"Conv2D_normal_shape\", tf.keras.layers.Conv2D,\n        {\"filters\": 1, \"kernel_size\": 1, \"activation\": \"relu\"},\n-       (64, 28, 28, 3), 10000),\n+       {\"input_shape\": (64, 28, 28, 3)}, 10),\n       (\"LSTM_small_shape\", tf.keras.layers.LSTM,\n-       {\"units\": 1}, (1, 1, 1), 10000),\n+       {\"units\": 1}, {\"input_shape\": (1, 1, 1)}, 10),\n       (\"LSTM_normal_shape\", tf.keras.layers.LSTM,\n-       {\"units\": 4}, (32, 10, 8), 10000),\n+       {\"units\": 4}, {\"input_shape\": (32, 10, 8)}, 10),\n+      (\"Embedding_small_shape\", tf.keras.layers.Embedding,\n+       {\"input_dim\": 1, \"output_dim\": 1, \"input_length\": 1},\n+       {\"input\": np.random.randint(1, size=(1, 1))}, 10),\n+      (\"Embedding_normal_shape\", tf.keras.layers.Embedding,\n+       {\"input_dim\": 1000, \"output_dim\": 64, \"input_length\": 10},\n+       {\"input\": np.random.randint(1000, size=(32, 10))}, 10),\n   ])\n \n-  def benchmark_layer_call(self, layer_cls, layer_args, input_shape, num_iters):\n+  def benchmark_layer_call(self, layer_cls, layer_args, inputs, num_iters):\n     layer = layer_cls(**layer_args)\n-    x = tf.ones(input_shape)\n+    x = _get_input_data(inputs)\n \n     fn = functools.partial(layer, x)\n     name = _get_benchmark_name(self._get_name())\n@@ -86,9 +103,9 @@ class KerasLayerBenchmarks(six.with_metaclass(\n     self.run_report(fn, num_iters, metadata)\n \n   def benchmark_layer_call_with_function(\n-      self, layer_cls, layer_args, input_shape, num_iters):\n+      self, layer_cls, layer_args, inputs, num_iters):\n     layer = layer_cls(**layer_args)\n-    x = tf.ones(input_shape)\n+    x = _get_input_data(inputs)\n     layer.call = tf.function(layer.call)\n \n     fn = functools.partial(layer, x)\n@@ -98,22 +115,25 @@ class KerasLayerBenchmarks(six.with_metaclass(\n     self.run_report(fn, num_iters, metadata)\n \n   def benchmark_layer_call_with_xla(\n-      self, layer_cls, layer_args, input_shape, num_iters):\n+      self, layer_cls, layer_args, inputs, num_iters):\n+    name = _get_benchmark_name(self._get_name())\n+    # TODO(b/173461426)\n+    if layer_cls is tf.keras.layers.Embedding and name[-1] == \"GPU\":\n+      return\n     layer = layer_cls(**layer_args)\n-    x = tf.ones(input_shape)\n+    x = _get_input_data(inputs)\n     layer.call = tf.function(\n         layer.call, jit_compile=True)\n \n     fn = functools.partial(layer, x)\n-    name = _get_benchmark_name(self._get_name())\n     metadata = {\"implementation\": name[0] + \".layer.call.xla\"}\n     metadata.update(_get_metadata(name))\n     self.run_report(fn, num_iters, metadata)\n \n   def benchmark_layer_call_backward(\n-      self, layer_cls, layer_args, input_shape, num_iters):\n+      self, layer_cls, layer_args, inputs, num_iters):\n     layer = layer_cls(**layer_args)\n-    x = tf.ones(input_shape)\n+    x = _get_input_data(inputs)\n \n     fn = functools.partial(_layer_call_backward, layer, x)\n     name = _get_benchmark_name(self._get_name())\n@@ -122,9 +142,9 @@ class KerasLayerBenchmarks(six.with_metaclass(\n     self.run_report(fn, num_iters, metadata)\n \n   def benchmark_layer_call_backward_with_function(\n-      self, layer_cls, layer_args, input_shape, num_iters):\n+      self, layer_cls, layer_args, inputs, num_iters):\n     layer = layer_cls(**layer_args)\n-    x = tf.ones(input_shape)\n+    x = _get_input_data(inputs)\n     layer.call = tf.function(layer.call)\n \n     fn = functools.partial(_layer_call_backward, layer, x)\n@@ -141,26 +161,35 @@ class KerasLayerBenchmarksBackwardXLA(six.with_metaclass(\n   _benchmark_parameters = _generate_benchmark_params([\n       (\"Conv2D_small_shape\", tf.keras.layers.Conv2D,\n        {\"filters\": 1, \"kernel_size\": 1, \"activation\": \"relu\"},\n-       (1, 1, 1, 1), 10000),\n+       {\"input_shape\": (1, 1, 1, 1)}, 10000),\n       (\"Conv2D_normal_shape\", tf.keras.layers.Conv2D,\n        {\"filters\": 1, \"kernel_size\": 1, \"activation\": \"relu\"},\n-       (64, 28, 28, 3), 10000),\n+       {\"input_shape\": (64, 28, 28, 3)}, 10000),\n       # TODO(b/153480400)\n       # (\"LSTM_small_shape\", tf.keras.layers.LSTM,\n-      #  {\"units\": 1}, (1, 1, 1), 10000),\n+      #  {\"units\": 1}, {\"input_shape\": (1, 1, 1)}, 10000),\n       # (\"LSTM_normal_shape\", tf.keras.layers.LSTM,\n-      #  {\"units\": 4}, (32, 10, 8), 10000),\n+      #  {\"units\": 4}, {\"input_shape\": (32, 10, 8)}, 10000),\n+      (\"Embedding_small_shape\", tf.keras.layers.Embedding,\n+       {\"input_dim\": 1, \"output_dim\": 1, \"input_length\": 1},\n+       {\"input\": np.random.randint(1, size=(1, 1))}, 10),\n+      (\"Embedding_normal_shape\", tf.keras.layers.Embedding,\n+       {\"input_dim\": 1000, \"output_dim\": 64, \"input_length\": 10},\n+       {\"input\": np.random.randint(1000, size=(32, 10))}, 10),\n   ])\n \n   def benchmark_layer_call_backward_with_xla(\n-      self, layer_cls, layer_args, input_shape, num_iters):\n+      self, layer_cls, layer_args, inputs, num_iters):\n+    name = _get_benchmark_name(self._get_name())\n+    # TODO(b/173461426)\n+    if layer_cls is tf.keras.layers.Embedding and name[-1] == \"GPU\":\n+      return\n     layer = layer_cls(**layer_args)\n-    x = tf.ones(input_shape)\n+    x = _get_input_data(inputs)\n     layer.call = tf.function(\n         layer.call, jit_compile=True)\n \n     fn = functools.partial(_layer_call_backward, layer, x)\n-    name = _get_benchmark_name(self._get_name())\n     metadata = {\"implementation\": name[0] + \".layer.call.backward.xla\"}\n     metadata.update(_get_metadata(name))\n     self.run_report(fn, num_iters, metadata)\n\n@@ -1155,12 +1155,14 @@ class ModelCheckpoint(Callback):\n   ```\n \n   Arguments:\n-      filepath: string or `PathLike`, path to save the model file. `filepath`\n+      filepath: string or `PathLike`, path to save the model file. e.g.\n+        filepath = os.path.join(working_dir, 'ckpt', file_name). `filepath`\n         can contain named formatting options, which will be filled the value of\n         `epoch` and keys in `logs` (passed in `on_epoch_end`). For example: if\n         `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`, then the model\n         checkpoints will be saved with the epoch number and the validation loss\n-        in the filename.\n+        in the filename. The directory of the filepath should not be reused by\n+        any other callbacks to avoid conflicts.\n       monitor: The metric name to monitor. Typically the metrics are set by the\n         `Model.compile` method. Note:\n \n@@ -1556,7 +1558,7 @@ class BackupAndRestore(Callback):\n   ...     if epoch == 4:\n   ...       raise RuntimeError('Interrupting!')\n   >>> callback = tf.keras.callbacks.experimental.BackupAndRestore(\n-  ... backup_dir=\"/tmp\")\n+  ... backup_dir=\"/tmp/backup\")\n   >>> model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])\n   >>> model.compile(tf.keras.optimizers.SGD(), loss='mse')\n   >>> try:\n@@ -1573,11 +1575,13 @@ class BackupAndRestore(Callback):\n   6\n \n   Arguments:\n-      backup_dir: String, path to save the model file. This is the directory in\n-        which the system stores temporary files to recover the model from jobs\n-        terminated unexpectedly. The directory cannot be reused elsewhere to\n-        store other checkpoints, e.g. by BackupAndRestore callback of another\n-        training, or by another callback (ModelCheckpoint) of the same training.\n+      backup_dir: String, path to store the checkpoint.\n+        e.g. backup_dir = os.path.join(working_dir, 'backup')\n+        This is the directory in which the system stores temporary files to\n+        recover the model from jobs terminated unexpectedly. The directory\n+        cannot be reused elsewhere to store other files, e.g. by\n+        BackupAndRestore callback of another training, or by another callback\n+        (ModelCheckpoint) of the same training.\n   \"\"\"\n \n   def __init__(self, backup_dir):\n@@ -1974,7 +1978,8 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n \n   Arguments:\n       log_dir: the path of the directory where to save the log files to be\n-        parsed by TensorBoard.\n+        parsed by TensorBoard. e.g. log_dir = os.path.join(working_dir, 'logs')\n+        This directory should not be reused by any other callbacks.\n       histogram_freq: frequency (in epochs) at which to compute activation and\n         weight histograms for the layers of the model. If set to 0, histograms\n         won't be computed. Validation data (or split) must be specified for\n\n@@ -604,7 +604,7 @@ class ListsOfScalarsDataAdapter(DataAdapter):\n   def _is_list_of_scalars(inp):\n     if isinstance(inp, (float, int, str, bytes, bytearray)):\n       return True\n-    if isinstance(inp, (list, tuple)):\n+    if isinstance(inp, (list, tuple)) and inp:\n       return ListsOfScalarsDataAdapter._is_list_of_scalars(inp[0])\n     return False\n \n\n@@ -1089,6 +1089,7 @@ class ListsOfScalarsDataAdapterTest(DataAdapterTestBase):\n     self.assertFalse(self.adapter_cls.can_handle(self.dataset_input))\n     self.assertFalse(self.adapter_cls.can_handle(self.generator_input))\n     self.assertFalse(self.adapter_cls.can_handle(self.sequence_input))\n+    self.assertFalse(self.adapter_cls.can_handle([]))\n \n \n class TestUtils(keras_parameterized.TestCase):\n\n@@ -173,6 +173,10 @@ class Sequential(functional.Functional):\n       origin_layer = layer._keras_history[0]\n       if isinstance(origin_layer, input_layer.InputLayer):\n         layer = origin_layer\n+        logging.warning(\n+            'Please add `keras.layers.InputLayer` instead of `keras.Input` to '\n+            'Sequential model. `keras.Input` is intended to be used by '\n+            'Functional model.')\n \n     if isinstance(layer, tf.Module):\n       if not isinstance(layer, base_layer.Layer):\n\n@@ -118,7 +118,9 @@ def _forward_over_back_hessian(f, params, use_pfor, dtype=None):\n   \"\"\"\n   return _vectorize_parameters(\n       functools.partial(_hvp, f, params),\n-      params, use_pfor=use_pfor, dtype=dtype)\n+      params,\n+      use_pfor=use_pfor,\n+      dtype=dtype)\n \n \n def _test_gradients(testcase,\n@@ -173,7 +175,10 @@ class ForwardpropTest(tf.test.TestCase, parameterized.TestCase):\n                   2. / tf.size(v, out_type=tf.float32),\n                   dtype=tf.float32), v.shape))\n     _test_gradients(\n-        self, layer, [input_value], atol=atol,\n+        self,\n+        layer,\n+        [input_value],\n+        atol=atol,\n         # These are linear, so second-order is pretty boring.\n         order=2)\n \n@@ -189,8 +194,10 @@ class ForwardpropTest(tf.test.TestCase, parameterized.TestCase):\n       input_value = tf.constant(value, dtype=tf.float32)\n       layer.build(input_value.shape)\n       _test_gradients(\n-          self, functools.partial(layer, training=training), [input_value],\n-          order=2, atol=1e-3)\n+          self,\n+          functools.partial(layer, training=training), [input_value],\n+          order=2,\n+          atol=1e-3)\n \n   @parameterized.named_parameters([\n       (\"NonFused\", [[0.1], [0.2], [-0.3]],\n@@ -205,8 +212,8 @@ class ForwardpropTest(tf.test.TestCase, parameterized.TestCase):\n         input_value = tf.constant(value, dtype=tf.float32)\n         tape.watch(input_value)\n         output = layer(input_value, training=training)\n-      jac_back = tape.jacobian(\n-          output, [input_value] + layer.trainable_variables)\n+      jac_back = tape.jacobian(output,\n+                               [input_value] + layer.trainable_variables)\n       jac_forward = _jacfwd(\n           lambda *args: layer(args[0], training=training),  # pylint:disable=cell-var-from-loop\n           [input_value] + layer.trainable_variables)\n@@ -218,12 +225,6 @@ class ForwardpropTest(tf.test.TestCase, parameterized.TestCase):\n                                    (\"NoFunction\", lambda f: f)])\n   def testVariablesHVP(self, decorator):\n \n-    if tf.test.is_built_with_rocm():\n-      # TODO(rocm)\n-      # This test was recently added and has never passed on the\n-      # ROCm platform. Remove this skip once the test is passing again\n-      self.skipTest(\"NoFunction decorator test fails on the ROCm platform\")\n-\n     class _Model(tf.Module):\n \n       def __init__(self):\n@@ -240,6 +241,7 @@ class ForwardpropTest(tf.test.TestCase, parameterized.TestCase):\n         return self._second_dense(x)\n \n     model = _Model()\n+\n     def _loss():\n       input_value = tf.constant([[-0.5, 1.], [0.5, -1.]])\n       target = tf.constant([[-1.], [2.]])\n@@ -251,8 +253,8 @@ class ForwardpropTest(tf.test.TestCase, parameterized.TestCase):\n         loss = _loss()\n       vector = tape.gradient(loss, model.trainable_variables)\n       variable_input_fn = lambda unused_variables: _loss()\n-      forward_over_back_hvp, = _hvp(\n-          variable_input_fn, [model.trainable_variables], [vector])\n+      forward_over_back_hvp, = _hvp(variable_input_fn,\n+                                    [model.trainable_variables], [vector])\n       with tf.GradientTape(persistent=True) as tape:\n         tape.watch(model.trainable_variables)\n         loss = _loss()\n@@ -260,6 +262,7 @@ class ForwardpropTest(tf.test.TestCase, parameterized.TestCase):\n       back_over_back_hvp = tape.gradient(\n           first_grads, model.trainable_variables, output_gradients=vector)\n       return forward_over_back_hvp, back_over_back_hvp\n+\n     self.assertAllClose(*_compute_hvps(), rtol=1e-5, atol=1e-5)\n \n   def testEmbeddingLayerInFunction(self):\n@@ -288,9 +291,7 @@ class ForwardpropTest(tf.test.TestCase, parameterized.TestCase):\n \n class HessianTests(tf.test.TestCase, parameterized.TestCase):\n \n-  @parameterized.named_parameters(\n-      [(\"PFor\", True),\n-       (\"MapFn\", False)])\n+  @parameterized.named_parameters([(\"PFor\", True), (\"MapFn\", False)])\n   def testHessianOfVariables(self, use_pfor):\n     model = tf.keras.layers.Dense(1)\n     model.build([None, 2])\n\n@@ -32,14 +32,16 @@ from keras import testing_utils\n class GlobalPoolingTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_globalpooling_1d(self):\n-    testing_utils.layer_test(keras.layers.pooling.GlobalMaxPooling1D,\n-                             input_shape=(3, 4, 5))\n-    testing_utils.layer_test(keras.layers.pooling.GlobalMaxPooling1D,\n+    testing_utils.layer_test(\n+        keras.layers.pooling.GlobalMaxPooling1D, input_shape=(3, 4, 5))\n+    testing_utils.layer_test(\n+        keras.layers.pooling.GlobalMaxPooling1D,\n         kwargs={'data_format': 'channels_first'},\n         input_shape=(3, 4, 5))\n     testing_utils.layer_test(\n         keras.layers.pooling.GlobalAveragePooling1D, input_shape=(3, 4, 5))\n-    testing_utils.layer_test(keras.layers.pooling.GlobalAveragePooling1D,\n+    testing_utils.layer_test(\n+        keras.layers.pooling.GlobalAveragePooling1D,\n         kwargs={'data_format': 'channels_first'},\n         input_shape=(3, 4, 5))\n \n@@ -55,9 +57,9 @@ class GlobalPoolingTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllClose(output[0], model_input[0, 0, :])\n \n   def test_globalpooling_1d_with_ragged(self):\n-    ragged_data = tf.ragged.constant([\n-        [[1.0, 1.0], [2.0, 2.0], [3.0, 3.0]],\n-        [[1.0, 1.0], [2.0, 2.0]]], ragged_rank=1)\n+    ragged_data = tf.ragged.constant(\n+        [[[1.0, 1.0], [2.0, 2.0], [3.0, 3.0]], [[1.0, 1.0], [2.0, 2.0]]],\n+        ragged_rank=1)\n     dense_data = ragged_data.to_tensor()\n \n     inputs = keras.Input(shape=(None, 2), dtype='float32', ragged=True)\n@@ -74,9 +76,10 @@ class GlobalPoolingTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllEqual(output_ragged, output_dense)\n \n   def test_globalpooling_2d_with_ragged(self):\n-    ragged_data = tf.ragged.constant([\n-        [[[1.0], [1.0]], [[2.0], [2.0]], [[3.0], [3.0]]],\n-        [[[1.0], [1.0]], [[2.0], [2.0]]]], ragged_rank=1)\n+    ragged_data = tf.ragged.constant(\n+        [[[[1.0], [1.0]], [[2.0], [2.0]], [[3.0], [3.0]]],\n+         [[[1.0], [1.0]], [[2.0], [2.0]]]],\n+        ragged_rank=1)\n     dense_data = ragged_data.to_tensor()\n \n     inputs = keras.Input(shape=(None, 2, 1), dtype='float32', ragged=True)\n@@ -92,9 +95,10 @@ class GlobalPoolingTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllEqual(output_ragged, output_dense)\n \n   def test_globalpooling_3d_with_ragged(self):\n-    ragged_data = tf.ragged.constant([\n-        [[[[1.0]], [[1.0]]], [[[2.0]], [[2.0]]], [[[3.0]], [[3.0]]]],\n-        [[[[1.0]], [[1.0]]], [[[2.0]], [[2.0]]]]], ragged_rank=1)\n+    ragged_data = tf.ragged.constant(\n+        [[[[[1.0]], [[1.0]]], [[[2.0]], [[2.0]]], [[[3.0]], [[3.0]]]],\n+         [[[[1.0]], [[1.0]]], [[[2.0]], [[2.0]]]]],\n+        ragged_rank=1)\n \n     inputs = keras.Input(shape=(None, 2, 1, 1), dtype='float32', ragged=True)\n     out = keras.layers.GlobalAveragePooling3D()(inputs)\n@@ -160,15 +164,19 @@ class Pooling2DTest(tf.test.TestCase, parameterized.TestCase):\n   def test_averagepooling_2d(self):\n     testing_utils.layer_test(\n         keras.layers.AveragePooling2D,\n-        kwargs={'strides': (2, 2),\n+        kwargs={\n+            'strides': (2, 2),\n             'padding': 'same',\n-                'pool_size': (2, 2)},\n+            'pool_size': (2, 2)\n+        },\n         input_shape=(3, 5, 6, 4))\n     testing_utils.layer_test(\n         keras.layers.AveragePooling2D,\n-        kwargs={'strides': (2, 2),\n+        kwargs={\n+            'strides': (2, 2),\n             'padding': 'valid',\n-                'pool_size': (3, 3)},\n+            'pool_size': (3, 3)\n+        },\n         input_shape=(3, 5, 6, 4))\n \n     # This part of the test can only run on GPU but doesn't appear\n@@ -192,14 +200,14 @@ class Pooling2DTest(tf.test.TestCase, parameterized.TestCase):\n class Pooling3DTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_maxpooling_3d(self):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest('Pooling with 3D tensors is not supported in ROCm')\n     pool_size = (3, 3, 3)\n     testing_utils.layer_test(\n         keras.layers.MaxPooling3D,\n-        kwargs={'strides': 2,\n+        kwargs={\n+            'strides': 2,\n             'padding': 'valid',\n-                'pool_size': pool_size},\n+            'pool_size': pool_size\n+        },\n         input_shape=(3, 11, 12, 10, 4))\n     testing_utils.layer_test(\n         keras.layers.MaxPooling3D,\n@@ -212,14 +220,14 @@ class Pooling3DTest(tf.test.TestCase, parameterized.TestCase):\n         input_shape=(3, 4, 11, 12, 10))\n \n   def test_averagepooling_3d(self):\n-    if tf.test.is_built_with_rocm():\n-      self.skipTest('Pooling with 3D tensors is not supported in ROCm')\n     pool_size = (3, 3, 3)\n     testing_utils.layer_test(\n         keras.layers.AveragePooling3D,\n-        kwargs={'strides': 2,\n+        kwargs={\n+            'strides': 2,\n             'padding': 'valid',\n-                'pool_size': pool_size},\n+            'pool_size': pool_size\n+        },\n         input_shape=(3, 11, 12, 10, 4))\n     testing_utils.layer_test(\n         keras.layers.AveragePooling3D,\n@@ -240,8 +248,10 @@ class Pooling1DTest(tf.test.TestCase, parameterized.TestCase):\n       for stride in [1, 2]:\n         testing_utils.layer_test(\n             keras.layers.MaxPooling1D,\n-            kwargs={'strides': stride,\n-                    'padding': padding},\n+            kwargs={\n+                'strides': stride,\n+                'padding': padding\n+            },\n             input_shape=(3, 5, 4))\n     testing_utils.layer_test(\n         keras.layers.MaxPooling1D,\n@@ -253,8 +263,10 @@ class Pooling1DTest(tf.test.TestCase, parameterized.TestCase):\n       for stride in [1, 2]:\n         testing_utils.layer_test(\n             keras.layers.AveragePooling1D,\n-            kwargs={'strides': stride,\n-                    'padding': padding},\n+            kwargs={\n+                'strides': stride,\n+                'padding': padding\n+            },\n             input_shape=(3, 5, 4))\n \n     testing_utils.layer_test(\n\n@@ -52,24 +52,36 @@ class ResizingTest(keras_parameterized.TestCase):\n           expected_output_shape=(None, expected_height, expected_width,\n                                  channels))\n \n-  @parameterized.named_parameters(\n-      ('down_sample_bilinear_2_by_2', {'interpolation': 'bilinear'}, 2, 2),\n-      ('down_sample_bilinear_3_by_2', {'interpolation': 'bilinear'}, 3, 2),\n-      ('down_sample_nearest_2_by_2', {'interpolation': 'nearest'}, 2, 2),\n-      ('down_sample_nearest_3_by_2', {'interpolation': 'nearest'}, 3, 2),\n-      ('down_sample_area_2_by_2', {'interpolation': 'area'}, 2, 2),\n-      ('down_sample_area_3_by_2', {'interpolation': 'area'}, 3, 2))\n+  @parameterized.named_parameters(('down_sample_bilinear_2_by_2', {\n+      'interpolation': 'bilinear'\n+  }, 2, 2), ('down_sample_bilinear_3_by_2', {\n+      'interpolation': 'bilinear'\n+  }, 3, 2), ('down_sample_nearest_2_by_2', {\n+      'interpolation': 'nearest'\n+  }, 2, 2), ('down_sample_nearest_3_by_2', {\n+      'interpolation': 'nearest'\n+  }, 3, 2), ('down_sample_area_2_by_2', {\n+      'interpolation': 'area'\n+  }, 2, 2), ('down_sample_area_3_by_2', {\n+      'interpolation': 'area'\n+  }, 3, 2))\n   def test_down_sampling(self, kwargs, expected_height, expected_width):\n     with CustomObjectScope({'Resizing': image_preprocessing.Resizing}):\n       self._run_test(kwargs, expected_height, expected_width)\n \n-  @parameterized.named_parameters(\n-      ('up_sample_bilinear_10_by_12', {'interpolation': 'bilinear'}, 10, 12),\n-      ('up_sample_bilinear_12_by_12', {'interpolation': 'bilinear'}, 12, 12),\n-      ('up_sample_nearest_10_by_12', {'interpolation': 'nearest'}, 10, 12),\n-      ('up_sample_nearest_12_by_12', {'interpolation': 'nearest'}, 12, 12),\n-      ('up_sample_area_10_by_12', {'interpolation': 'area'}, 10, 12),\n-      ('up_sample_area_12_by_12', {'interpolation': 'area'}, 12, 12))\n+  @parameterized.named_parameters(('up_sample_bilinear_10_by_12', {\n+      'interpolation': 'bilinear'\n+  }, 10, 12), ('up_sample_bilinear_12_by_12', {\n+      'interpolation': 'bilinear'\n+  }, 12, 12), ('up_sample_nearest_10_by_12', {\n+      'interpolation': 'nearest'\n+  }, 10, 12), ('up_sample_nearest_12_by_12', {\n+      'interpolation': 'nearest'\n+  }, 12, 12), ('up_sample_area_10_by_12', {\n+      'interpolation': 'area'\n+  }, 10, 12), ('up_sample_area_12_by_12', {\n+      'interpolation': 'area'\n+  }, 12, 12))\n   def test_up_sampling(self, kwargs, expected_height, expected_width):\n     with CustomObjectScope({'Resizing': image_preprocessing.Resizing}):\n       self._run_test(kwargs, expected_height, expected_width)\n@@ -108,8 +120,9 @@ class ResizingTest(keras_parameterized.TestCase):\n         expected_output = np.reshape(expected_output, (1, 4, 4, 1))\n         self.assertAllEqual(expected_output, output_image)\n \n-  @parameterized.named_parameters(\n-      ('reshape_bilinear_10_by_4', {'interpolation': 'bilinear'}, 10, 4))\n+  @parameterized.named_parameters(('reshape_bilinear_10_by_4', {\n+      'interpolation': 'bilinear'\n+  }, 10, 4))\n   def test_reshaping(self, kwargs, expected_height, expected_width):\n     with CustomObjectScope({'Resizing': image_preprocessing.Resizing}):\n       self._run_test(kwargs, expected_height, expected_width)\n@@ -147,8 +160,8 @@ class CenterCropTest(keras_parameterized.TestCase):\n     kwargs = {'height': expected_height, 'width': expected_width}\n     input_images = np.random.random(\n         (num_samples, orig_height, orig_width, channels)).astype(np.float32)\n-    expected_output = get_numpy_center_crop(\n-        input_images, expected_height, expected_width)\n+    expected_output = get_numpy_center_crop(input_images, expected_height,\n+                                            expected_width)\n     with testing_utils.use_gpu():\n       testing_utils.layer_test(\n           image_preprocessing.CenterCrop,\n@@ -159,29 +172,25 @@ class CenterCropTest(keras_parameterized.TestCase):\n           expected_output_shape=(None, expected_height, expected_width,\n                                  channels))\n \n-  @parameterized.named_parameters(\n-      ('center_crop_3_by_4', 3, 4),\n+  @parameterized.named_parameters(('center_crop_3_by_4', 3, 4),\n                                   ('center_crop_3_by_2', 3, 2))\n   def test_center_crop_aligned(self, expected_height, expected_width):\n     with CustomObjectScope({'CenterCrop': image_preprocessing.CenterCrop}):\n       self._run_test(expected_height, expected_width)\n \n-  @parameterized.named_parameters(\n-      ('center_crop_4_by_5', 4, 5),\n+  @parameterized.named_parameters(('center_crop_4_by_5', 4, 5),\n                                   ('center_crop_4_by_3', 4, 3))\n   def test_center_crop_mis_aligned(self, expected_height, expected_width):\n     with CustomObjectScope({'CenterCrop': image_preprocessing.CenterCrop}):\n       self._run_test(expected_height, expected_width)\n \n-  @parameterized.named_parameters(\n-      ('center_crop_4_by_6', 4, 6),\n+  @parameterized.named_parameters(('center_crop_4_by_6', 4, 6),\n                                   ('center_crop_3_by_2', 3, 2))\n   def test_center_crop_half_mis_aligned(self, expected_height, expected_width):\n     with CustomObjectScope({'CenterCrop': image_preprocessing.CenterCrop}):\n       self._run_test(expected_height, expected_width)\n \n-  @parameterized.named_parameters(\n-      ('center_crop_5_by_12', 5, 12),\n+  @parameterized.named_parameters(('center_crop_5_by_12', 5, 12),\n                                   ('center_crop_10_by_8', 10, 8),\n                                   ('center_crop_10_by_12', 10, 12))\n   def test_invalid_center_crop(self, expected_height, expected_width):\n@@ -214,8 +223,7 @@ class RandomCropTest(keras_parameterized.TestCase):\n           expected_output_shape=(None, expected_height, expected_width,\n                                  channels))\n \n-  @parameterized.named_parameters(\n-      ('random_crop_5_by_12', 5, 12),\n+  @parameterized.named_parameters(('random_crop_5_by_12', 5, 12),\n                                   ('random_crop_10_by_8', 10, 8),\n                                   ('random_crop_10_by_12', 10, 12))\n   def test_invalid_random_crop(self, expected_height, expected_width):\n@@ -224,18 +232,14 @@ class RandomCropTest(keras_parameterized.TestCase):\n         self._run_test(expected_height, expected_width)\n \n   def test_training_with_mock(self):\n-    if tf.test.is_built_with_rocm():\n-      # TODO(rocm):\n-      # re-enable this test once ROCm adds support for\n-      # the StatefulUniformFullInt Op (on the GPU)\n-      self.skipTest('Feature not supported on ROCm')\n     np.random.seed(1337)\n     height, width = 3, 4\n     height_offset = np.random.randint(low=0, high=3)\n     width_offset = np.random.randint(low=0, high=5)\n     mock_offset = [0, height_offset, width_offset, 0]\n     with tf.compat.v1.test.mock.patch.object(\n-        stateless_random_ops, 'stateless_random_uniform',\n+        stateless_random_ops,\n+        'stateless_random_uniform',\n         return_value=mock_offset):\n       with testing_utils.use_gpu():\n         layer = image_preprocessing.RandomCrop(height, width)\n@@ -245,15 +249,9 @@ class RandomCropTest(keras_parameterized.TestCase):\n                               width_offset:(width_offset + width), :]\n         self.assertAllClose(expected_output, actual_output)\n \n-  @parameterized.named_parameters(\n-      ('random_crop_4_by_6', 4, 6),\n+  @parameterized.named_parameters(('random_crop_4_by_6', 4, 6),\n                                   ('random_crop_3_by_2', 3, 2))\n   def test_random_crop_output_shape(self, expected_height, expected_width):\n-    if tf.test.is_built_with_rocm():\n-      # TODO(rocm):\n-      # re-enable this test once ROCm adds support for\n-      # the StatefulUniformFullInt Op (on the GPU)\n-      self.skipTest('Feature not supported on ROCm')\n     with CustomObjectScope({'RandomCrop': image_preprocessing.RandomCrop}):\n       self._run_test(expected_height, expected_width)\n \n@@ -279,8 +277,7 @@ class RandomCropTest(keras_parameterized.TestCase):\n     with testing_utils.use_gpu():\n       layer = image_preprocessing.RandomCrop(height, width)\n       actual_output = layer(inp, training=0)\n-      resized_inp = tf.image.resize(\n-          inp, size=[5, 3])\n+      resized_inp = tf.image.resize(inp, size=[5, 3])\n       expected_output = resized_inp[:, 1:4, :, :]\n       self.assertAllClose(expected_output, actual_output)\n \n@@ -422,11 +419,7 @@ class RandomFlipTest(keras_parameterized.TestCase):\n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n class RandomContrastTest(keras_parameterized.TestCase):\n \n-  def _run_test(self,\n-                lower,\n-                upper,\n-                expected_output=None,\n-                mock_random=None):\n+  def _run_test(self, lower, upper, expected_output=None, mock_random=None):\n     np.random.seed(1337)\n     num_samples = 2\n     orig_height = 5\n@@ -448,8 +441,7 @@ class RandomContrastTest(keras_parameterized.TestCase):\n         actual_output = layer(inp, training=True)\n         self.assertAllClose(expected_output, actual_output)\n \n-  @parameterized.named_parameters(\n-      ('random_contrast_2_by_5', 0.2, 0.5),\n+  @parameterized.named_parameters(('random_contrast_2_by_5', 0.2, 0.5),\n                                   ('random_contrast_2_by_13', 0.2, 1.3),\n                                   ('random_contrast_5_by_2', 0.5, 0.2))\n   def test_random_contrast(self, lower, upper):\n@@ -457,8 +449,7 @@ class RandomContrastTest(keras_parameterized.TestCase):\n         {'RandomContrast': image_preprocessing.RandomContrast}):\n       self._run_test(lower, upper)\n \n-  @parameterized.named_parameters(\n-      ('random_contrast_amplitude_2', 0.2),\n+  @parameterized.named_parameters(('random_contrast_amplitude_2', 0.2),\n                                   ('random_contrast_amplitude_5', 0.5))\n   def test_random_contrast_amplitude(self, amplitude):\n     with CustomObjectScope(\n@@ -998,8 +989,10 @@ class RandomTransformTest(keras_parameterized.TestCase):\n     # pyformat: enable\n     transform_matrix = np.asarray([[1., 0., 0., 0., 1., -1., 0., 0.]])\n     self._run_random_transform_with_mock(\n-        transform_matrix, expected_output,\n-        mode='constant', interpolation='nearest')\n+        transform_matrix,\n+        expected_output,\n+        mode='constant',\n+        interpolation='nearest')\n \n     # Test up shift by 1.\n     # pyformat: disable\n@@ -1012,8 +1005,10 @@ class RandomTransformTest(keras_parameterized.TestCase):\n     # pyformat: enable\n     transform_matrix = np.asarray([[1., 0., 0., 0., 1., 1., 0., 0.]])\n     self._run_random_transform_with_mock(\n-        transform_matrix, expected_output,\n-        mode='constant', interpolation='nearest')\n+        transform_matrix,\n+        expected_output,\n+        mode='constant',\n+        interpolation='nearest')\n \n     # Test left shift by 1.\n     # pyformat: disable\n@@ -1026,8 +1021,10 @@ class RandomTransformTest(keras_parameterized.TestCase):\n     # pyformat: enable\n     transform_matrix = np.asarray([[1., 0., 1., 0., 1., 0., 0., 0.]])\n     self._run_random_transform_with_mock(\n-        transform_matrix, expected_output,\n-        mode='constant', interpolation='nearest')\n+        transform_matrix,\n+        expected_output,\n+        mode='constant',\n+        interpolation='nearest')\n \n     # Test right shift by 1.\n     # pyformat: disable\n@@ -1040,8 +1037,10 @@ class RandomTransformTest(keras_parameterized.TestCase):\n     # pyformat: enable\n     transform_matrix = np.asarray([[1., 0., -1., 0., 1., 0., 0., 0.]])\n     self._run_random_transform_with_mock(\n-        transform_matrix, expected_output,\n-        mode='constant', interpolation='nearest')\n+        transform_matrix,\n+        expected_output,\n+        mode='constant',\n+        interpolation='nearest')\n \n \n @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n@@ -1189,8 +1188,7 @@ class RandomZoomTest(keras_parameterized.TestCase):\n         self.assertAllEqual(expected_output, output_image)\n \n   def test_random_zoom_inference(self):\n-    with CustomObjectScope(\n-        {'RandomZoom': image_preprocessing.RandomZoom}):\n+    with CustomObjectScope({'RandomZoom': image_preprocessing.RandomZoom}):\n       input_images = np.random.random((2, 5, 8, 3)).astype(np.float32)\n       expected_output = input_images\n       with testing_utils.use_gpu():\n@@ -1235,7 +1233,8 @@ class RandomHeightTest(keras_parameterized.TestCase):\n     with tf.compat.v1.test.mock.patch.object(\n         gen_stateful_random_ops, 'stateful_uniform', return_value=mock_factor):\n       with tf.compat.v1.test.mock.patch.object(\n-          gen_stateless_random_ops_v2, 'stateless_random_uniform_v2',\n+          gen_stateless_random_ops_v2,\n+          'stateless_random_uniform_v2',\n           return_value=mock_factor):\n         with testing_utils.use_gpu():\n           img = np.random.random((12, 5, 8, 3))\n@@ -1250,8 +1249,8 @@ class RandomHeightTest(keras_parameterized.TestCase):\n         layer = image_preprocessing.RandomHeight(factor=(1., 1.))\n         # Return type of RandomHeight() is float32 if `interpolation` is not\n         # set to `ResizeMethod.NEAREST_NEIGHBOR`; cast `layer` to desired dtype.\n-        output_image = tf.cast(layer(np.expand_dims(input_image, axis=0)),\n-                                     dtype=dtype)\n+        output_image = tf.cast(\n+            layer(np.expand_dims(input_image, axis=0)), dtype=dtype)\n         # pyformat: disable\n         expected_output = np.asarray([\n             [0, 1, 2],\n@@ -1329,7 +1328,8 @@ class RandomWidthTest(keras_parameterized.TestCase):\n     with tf.compat.v1.test.mock.patch.object(\n         gen_stateful_random_ops, 'stateful_uniform', return_value=mock_factor):\n       with tf.compat.v1.test.mock.patch.object(\n-          gen_stateless_random_ops_v2, 'stateless_random_uniform_v2',\n+          gen_stateless_random_ops_v2,\n+          'stateless_random_uniform_v2',\n           return_value=mock_factor):\n         with testing_utils.use_gpu():\n           img = np.random.random((12, 8, 5, 3))\n@@ -1344,8 +1344,8 @@ class RandomWidthTest(keras_parameterized.TestCase):\n         layer = image_preprocessing.RandomWidth(factor=(1., 1.))\n         # Return type of RandomWidth() is float32 if `interpolation` is not\n         # set to `ResizeMethod.NEAREST_NEIGHBOR`; cast `layer` to desired dtype.\n-        output_image = tf.cast(layer(np.expand_dims(input_image, axis=0)),\n-                                     dtype=dtype)\n+        output_image = tf.cast(\n+            layer(np.expand_dims(input_image, axis=0)), dtype=dtype)\n         # pyformat: disable\n         expected_output = np.asarray([\n             [0, 0.25, 0.75, 1],\n\n@@ -53,8 +53,8 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n   logic but is not intended to be exported as part of the Keras API.\n \n   If desired, the user can call this layer's `adapt()` method on a data set,\n-  which will analyze the data set, determine the frequency of individual string\n-  values, and create a vocabulary from them. This vocabulary can have\n+  which will analyze the data set, determine the frequency of individual\n+  hashable values, and create a vocabulary from them. This vocabulary can have\n   unlimited size or be capped, depending on the configuration options for this\n   layer; if there are more unique values in the input than the maximum\n   vocabulary size, the most frequent terms will be used to create the\n@@ -390,7 +390,7 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     layer, this method will either replace it\n \n     Arguments:\n-      vocab: An array of string tokens.\n+      vocab: An array of hashable tokens.\n \n     Raises:\n       ValueError: If there are too many inputs, the inputs do not match, or\n\n@@ -240,10 +240,14 @@ class StringLookup(index_lookup.IndexLookup):\n     if self._table_handler.vocab_size() == 0:\n       return []\n \n-    keys, values = self._table_handler.data()\n+    if self.invert:\n+      ids, strings = self._table_handler.data()\n+    else:\n+      strings, ids = self._table_handler.data()\n+\n     # This is required because the MutableHashTable doesn't preserve insertion\n     # order, but we rely on the order of the array to assign indices.\n-    return [x.decode(self.encoding) for _, x in sorted(zip(values, keys))]\n+    return [x.decode(self.encoding) for _, x in sorted(zip(ids, strings))]\n \n   def set_vocabulary(self, vocab):\n     if isinstance(vocab, str):\n\n@@ -197,6 +197,12 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n     self.assertAllEqual(expected_vocab, layer_vocab)\n     self.assertIsInstance(layer_vocab[0], six.text_type)\n \n+    inverse_layer = get_layer_class()(vocabulary=layer.get_vocabulary(),\n+                                      invert=True)\n+    layer_vocab = inverse_layer.get_vocabulary()\n+    self.assertAllEqual(expected_vocab, layer_vocab)\n+    self.assertIsInstance(layer_vocab[0], six.text_type)\n+\n   def test_int_output_explicit_vocab_from_file(self):\n     vocab_list = [\"earth\", \"wind\", \"and\", \"fire\"]\n     vocab_path = self._write_to_temp_file(\"vocab_file\", vocab_list)\n\n@@ -266,7 +266,9 @@ class RNN(Layer):\n   Call arguments:\n     inputs: Input tensor.\n     mask: Binary tensor of shape `[batch_size, timesteps]` indicating whether\n-      a given timestep should be masked.\n+      a given timestep should be masked. An individual `True` entry indicates\n+      that the corresponding timestep should be utilized, while a `False`\n+      entry indicates that the corresponding timestep should be ignored.\n     training: Python boolean indicating whether the layer should behave in\n       training mode or in inference mode. This argument is passed to the cell\n       when calling it. This is for use with cells that use dropout.\n@@ -1483,7 +1485,9 @@ class SimpleRNN(RNN):\n   Call arguments:\n     inputs: A 3D tensor, with shape `[batch, timesteps, feature]`.\n     mask: Binary tensor of shape `[batch, timesteps]` indicating whether\n-      a given timestep should be masked.\n+      a given timestep should be masked. An individual `True` entry indicates\n+      that the corresponding timestep should be utilized, while a `False` entry\n+      indicates that the corresponding timestep should be ignored.\n     training: Python boolean indicating whether the layer should behave in\n       training mode or in inference mode. This argument is passed to the cell\n       when calling it. This is only relevant if `dropout` or\n@@ -2028,7 +2032,9 @@ class GRU(RNN):\n   Call arguments:\n     inputs: A 3D tensor.\n     mask: Binary tensor of shape `(samples, timesteps)` indicating whether\n-      a given timestep should be masked.\n+      a given timestep should be masked. An individual `True` entry indicates\n+      that the corresponding timestep should be utilized, while a `False`\n+      entry indicates that the corresponding timestep should be ignored.\n     training: Python boolean indicating whether the layer should behave in\n       training mode or in inference mode. This argument is passed to the cell\n       when calling it. This is only relevant if `dropout` or\n@@ -2702,7 +2708,9 @@ class LSTM(RNN):\n   Call arguments:\n     inputs: A 3D tensor.\n     mask: Binary tensor of shape `(samples, timesteps)` indicating whether\n-      a given timestep should be masked.\n+      a given timestep should be masked. An individual `True` entry indicates\n+      that the corresponding timestep should be utilized, while a `False`\n+      entry indicates that the corresponding timestep should be ignored.\n     training: Python boolean indicating whether the layer should behave in\n       training mode or in inference mode. This argument is passed to the cell\n       when calling it. This is only relevant if `dropout` or\n\n@@ -322,6 +322,9 @@ class GRU(recurrent.DropoutRNNCellMixin, recurrent.GRU):\n     inputs: A 3D tensor, with shape `[batch, timesteps, feature]`.\n     mask: Binary tensor of shape `[samples, timesteps]` indicating whether\n       a given timestep should be masked  (optional, defaults to `None`).\n+      An individual `True` entry indicates that the corresponding timestep\n+      should be utilized, while a `False` entry indicates that the\n+      corresponding timestep should be ignored.\n     training: Python boolean indicating whether the layer should behave in\n       training mode or in inference mode. This argument is passed to the cell\n       when calling it. This is only relevant if `dropout` or\n@@ -558,7 +561,9 @@ def standard_gru(inputs, init_h, kernel, recurrent_kernel, bias, mask,\n     bias: Weights for cell kernel bias and recurrent bias. The bias contains the\n       combined input_bias and recurrent_bias.\n     mask: Binary tensor of shape `(samples, timesteps)` indicating whether\n-      a given timestep should be masked.\n+      a given timestep should be masked. An individual `True` entry indicates\n+      that the corresponding timestep should be utilized, while a `False` entry\n+      indicates that the corresponding timestep should be ignored.\n     time_major: Boolean, whether the inputs are in the format of\n       [time, batch, feature] or [batch, time, feature].\n     go_backwards: Boolean (default False). If True, process the input sequence\n@@ -728,6 +733,9 @@ def gru_with_backend_selection(inputs, init_h, kernel, recurrent_kernel, bias,\n     bias: Weights for cell kernel bias and recurrent bias. Only recurrent bias\n       is used in this case.\n     mask: Boolean tensor for mask out the steps within sequence.\n+      An individual `True` entry indicates that the corresponding timestep\n+      should be utilized, while a `False` entry indicates that the corresponding\n+      timestep should be ignored.\n     time_major: Boolean, whether the inputs are in the format of\n       [time, batch, feature] or [batch, time, feature].\n     go_backwards: Boolean (default False). If True, process the input sequence\n@@ -1050,6 +1058,9 @@ class LSTM(recurrent.DropoutRNNCellMixin, recurrent.LSTM):\n     inputs: A 3D tensor with shape `[batch, timesteps, feature]`.\n     mask: Binary tensor of shape `[batch, timesteps]` indicating whether\n       a given timestep should be masked (optional, defaults to `None`).\n+      An individual `True` entry indicates that the corresponding timestep\n+      should be utilized, while a `False` entry indicates that the corresponding\n+      timestep should be ignored.\n     training: Python boolean indicating whether the layer should behave in\n       training mode or in inference mode. This argument is passed to the cell\n       when calling it. This is only relevant if `dropout` or\n@@ -1336,6 +1347,9 @@ def standard_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias,\n     bias: weights for cell kernel bias and recurrent bias. Only recurrent bias\n       is used in this case.\n     mask: Boolean tensor for mask out the steps within sequence.\n+      An individual `True` entry indicates that the corresponding timestep\n+      should be utilized, while a `False` entry indicates that the corresponding\n+      timestep should be ignored.\n     time_major: boolean, whether the inputs are in the format of\n       [time, batch, feature] or [batch, time, feature].\n     go_backwards: Boolean (default False). If True, process the input sequence\n@@ -1408,6 +1422,9 @@ def gpu_lstm(inputs, init_h, init_c, kernel, recurrent_kernel, bias, mask,\n     bias: Weights for cell kernel bias and recurrent bias. Only recurrent bias\n       is used in this case.\n     mask: Boolean tensor for mask out the steps within sequence.\n+      An individual `True` entry indicates that the corresponding timestep\n+      should be utilized, while a `False` entry indicates that the corresponding\n+      timestep should be ignored.\n     time_major: Boolean, whether the inputs are in the format of [time, batch,\n       feature] or [batch, time, feature].\n     go_backwards: Boolean (default False). If True, process the input sequence\n@@ -1538,6 +1555,9 @@ def lstm_with_backend_selection(inputs, init_h, init_c, kernel,\n     bias: Weights for cell kernel bias and recurrent bias. Only recurrent bias\n       is used in this case.\n     mask: Boolean tensor for mask out the steps within sequence.\n+      An individual `True` entry indicates that the corresponding timestep\n+      should be utilized, while a `False` entry indicates that the corresponding\n+      timestep should be ignored.\n     time_major: Boolean, whether the inputs are in the format of\n       [time, batch, feature] or [batch, time, feature].\n     go_backwards: Boolean (default False). If True, process the input sequence\n\n@@ -31,6 +31,7 @@ import six\n from tensorflow.python.autograph.core import ag_ctx\n from tensorflow.python.autograph.impl import api as autograph\n from tensorflow.python.eager import def_function\n+from keras import activations\n from keras import backend as K\n from keras.engine import base_layer\n from keras.engine import base_layer_utils\n@@ -1828,7 +1829,17 @@ class RecallAtPrecision(SensitivitySpecificityBase):\n \n @keras_export('keras.metrics.AUC')\n class AUC(Metric):\n-  \"\"\"Computes the approximate AUC (Area under the curve) via a Riemann sum.\n+  \"\"\"Approximates the AUC (Area under the curve) of the ROC or PR curves.\n+\n+  The AUC (Area under the curve) of the ROC (Receiver operating\n+  characteristic; default) or PR (Precision Recall) curves are quality measures\n+  of binary classifiers. Unlike the accuracy, and like cross-entropy\n+  losses, ROC-AUC and PR-AUC evaluate all the operational points of a model.\n+\n+  This classes approximates AUCs using a Riemann sum: During the metric\n+  accumulation phrase, predictions are accumulated within predefined buckets\n+  by value. The AUC is then computed by interpolating per-bucket averages. These\n+  buckets define the evaluated operational points.\n \n   This metric creates four local variables, `true_positives`, `true_negatives`,\n   `false_positives` and `false_negatives` that are used to compute the AUC.\n@@ -1846,11 +1857,11 @@ class AUC(Metric):\n   dramatically depending on `num_thresholds`. The `thresholds` parameter can be\n   used to manually specify thresholds which split the predictions more evenly.\n \n-  For best results, `predictions` should be distributed approximately uniformly\n-  in the range [0, 1] and not peaked around 0 or 1. The quality of the AUC\n-  approximation may be poor if this is not the case. Setting `summation_method`\n-  to 'minoring' or 'majoring' can help quantify the error in the approximation\n-  by providing lower or upper bound estimate of the AUC.\n+  For a best approximation of the real AUC, `predictions` should be distributed\n+  approximately uniformly in the range [0, 1] (if `from_logits=False`). The\n+  quality of the AUC approximation may be poor if this is not the case. Setting\n+  `summation_method` to 'minoring' or 'majoring' can help quantify the error in\n+  the approximation by providing lower or upper bound estimate of the AUC.\n \n   If `sample_weight` is `None`, weights default to 1.\n   Use `sample_weight` of 0 to mask values.\n@@ -1896,6 +1907,10 @@ class AUC(Metric):\n       label, whereas label_weights depends only on the index of that label\n       before flattening; therefore `label_weights` should not be used for\n       multi-class data.\n+    from_logits: boolean indicating whether the predictions (`y_pred` in\n+      `update_state`) are probabilities or sigmoid logits. As a rule of thumb,\n+      when using a keras loss, the `from_logits` constructor argument of the\n+      loss should match the AUC `from_logits` constructor argument.\n \n   Standalone usage:\n \n@@ -1917,7 +1932,15 @@ class AUC(Metric):\n   Usage with `compile()` API:\n \n   ```python\n-  model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.AUC()])\n+  # Reports the AUC of a model outputing a probability.\n+  model.compile(optimizer='sgd',\n+                loss=tf.keras.losses.BinaryCrossentropy(),\n+                metrics=[tf.keras.metrics.AUC()])\n+\n+  # Reports the AUC of a model outputing a logit.\n+  model.compile(optimizer='sgd',\n+                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n+                metrics=[tf.keras.metrics.AUC(from_logits=True)])\n   ```\n   \"\"\"\n \n@@ -1930,7 +1953,8 @@ class AUC(Metric):\n                thresholds=None,\n                multi_label=False,\n                num_labels=None,\n-               label_weights=None):\n+               label_weights=None,\n+               from_logits=False):\n     # Validate configurations.\n     if isinstance(curve, metrics_utils.AUCCurve) and curve not in list(\n         metrics_utils.AUCCurve):\n@@ -1990,6 +2014,8 @@ class AUC(Metric):\n     else:\n       self.label_weights = None\n \n+    self._from_logits = from_logits\n+\n     self._built = False\n     if self.multi_label:\n       if num_labels:\n@@ -2089,6 +2115,10 @@ class AUC(Metric):\n     # multi_label is False. Otherwise the averaging of individual label AUCs is\n     # handled in AUC.result\n     label_weights = None if self.multi_label else self.label_weights\n+\n+    if self._from_logits:\n+      y_pred = activations.sigmoid(y_pred)\n+\n     with tf.control_dependencies(deps):\n       return metrics_utils.update_confusion_matrix_variables(\n           {\n\n@@ -1137,6 +1137,8 @@ class AUCTest(tf.test.TestCase, parameterized.TestCase):\n   def setup(self):\n     self.num_thresholds = 3\n     self.y_pred = tf.constant([0, 0.5, 0.3, 0.9], dtype=tf.float32)\n+    epsilon = 1e-12\n+    self.y_pred_logits = -tf.math.log(1.0 / (self.y_pred + epsilon) - 1.0)\n     self.y_true = tf.constant([0, 0, 1, 1])\n     self.sample_weight = [1, 2, 3, 4]\n \n@@ -1259,6 +1261,20 @@ class AUCTest(tf.test.TestCase, parameterized.TestCase):\n     expected_result = (0.75 * 1 + 0.25 * 0)\n     self.assertAllClose(self.evaluate(result), expected_result, 1e-3)\n \n+  def test_unweighted_from_logits(self):\n+    self.setup()\n+    auc_obj = metrics.AUC(num_thresholds=self.num_thresholds, from_logits=True)\n+    self.evaluate(tf.compat.v1.variables_initializer(auc_obj.variables))\n+    result = auc_obj(self.y_true, self.y_pred_logits)\n+\n+    # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]\n+    # recall = [2/2, 1/(1+1), 0] = [1, 0.5, 0]\n+    # fp_rate = [2/2, 0, 0] = [1, 0, 0]\n+    # heights = [(1 + 0.5)/2, (0.5 + 0)/2] = [0.75, 0.25]\n+    # widths = [(1 - 0), (0 - 0)] = [1, 0]\n+    expected_result = (0.75 * 1 + 0.25 * 0)\n+    self.assertAllClose(self.evaluate(result), expected_result, 1e-3)\n+\n   def test_manual_thresholds(self):\n     self.setup()\n     # Verify that when specified, thresholds are used instead of num_thresholds.\n@@ -1413,6 +1429,10 @@ class MultiAUCTest(tf.test.TestCase, parameterized.TestCase):\n     self.y_pred = tf.constant(\n         np.array([[0, 0.5, 0.3, 0.9], [0.1, 0.2, 0.3, 0.4]]).T,\n         dtype=tf.float32)\n+\n+    epsilon = 1e-12\n+    self.y_pred_logits = -tf.math.log(1.0 / (self.y_pred + epsilon) - 1.0)\n+\n     self.y_true_good = tf.constant(\n         np.array([[0, 0, 1, 1], [0, 0, 1, 1]]).T)\n     self.y_true_bad = tf.constant(\n@@ -1498,6 +1518,21 @@ class MultiAUCTest(tf.test.TestCase, parameterized.TestCase):\n       expected_result = (0.875 + 1.0) / 2.0\n       self.assertAllClose(self.evaluate(result), expected_result, 1e-3)\n \n+  def test_unweighted_from_logits(self):\n+    with self.test_session():\n+      self.setup()\n+      auc_obj = metrics.AUC(\n+          num_thresholds=self.num_thresholds,\n+          multi_label=True,\n+          from_logits=True)\n+      self.evaluate(tf.compat.v1.variables_initializer(auc_obj.variables))\n+      result = auc_obj(self.y_true_good, self.y_pred_logits)\n+\n+      # tpr = [[1, 1, 0.5, 0.5, 0], [1, 1, 0, 0, 0]]\n+      # fpr = [[1, 0.5, 0, 0, 0], [1, 0, 0, 0, 0]]\n+      expected_result = (0.875 + 1.0) / 2.0\n+      self.assertAllClose(self.evaluate(result), expected_result, 1e-3)\n+\n   def test_sample_weight_flat(self):\n     self.setup()\n     auc_obj = metrics.AUC(num_thresholds=self.num_thresholds, multi_label=False)\n@@ -1567,6 +1602,23 @@ class MultiAUCTest(tf.test.TestCase, parameterized.TestCase):\n     expected_result = 1.0 - (3.0 / 32.0)\n     self.assertAllClose(self.evaluate(result), expected_result, 1e-3)\n \n+  def test_unweighted_flat_from_logits(self):\n+    self.setup()\n+    auc_obj = metrics.AUC(\n+        num_thresholds=self.num_thresholds, multi_label=False, from_logits=True)\n+    self.evaluate(tf.compat.v1.variables_initializer(auc_obj.variables))\n+    result = auc_obj(self.y_true_good, self.y_pred_logits)\n+\n+    # tp = [4, 4, 1, 1, 0]\n+    # fp = [4, 1, 0, 0, 0]\n+    # fn = [0, 0, 3, 3, 4]\n+    # tn = [0, 3, 4, 4, 4]\n+\n+    # tpr = [1, 1, 0.25, 0.25, 0]\n+    # fpr = [1, 0.25, 0, 0, 0]\n+    expected_result = 1.0 - (3.0 / 32.0)\n+    self.assertAllClose(self.evaluate(result), expected_result, 1e-3)\n+\n   def test_manual_thresholds(self):\n     with self.test_session():\n       self.setup()\n\n@@ -2255,6 +2255,29 @@ class ResetStatesTest(keras_parameterized.TestCase):\n       self.assertEqual(self.evaluate(auc_obj.false_negatives[1]), 25.)\n       self.assertEqual(self.evaluate(auc_obj.true_negatives[1]), 25.)\n \n+  def test_reset_states_auc_from_logits(self):\n+    auc_obj = metrics.AUC(num_thresholds=3, from_logits=True)\n+\n+    model_layers = [layers.Dense(1, kernel_initializer='ones', use_bias=False)]\n+    model = testing_utils.get_model_from_layers(model_layers, input_shape=(4,))\n+    model.compile(\n+        loss='mae',\n+        metrics=[auc_obj],\n+        optimizer='rmsprop',\n+        run_eagerly=testing_utils.should_run_eagerly())\n+\n+    x = np.concatenate((np.ones((25, 4)), -np.ones((25, 4)), -np.ones(\n+        (25, 4)), np.ones((25, 4))))\n+    y = np.concatenate((np.ones((25, 1)), np.zeros((25, 1)), np.ones(\n+        (25, 1)), np.zeros((25, 1))))\n+\n+    for _ in range(2):\n+      model.evaluate(x, y)\n+      self.assertEqual(self.evaluate(auc_obj.true_positives[1]), 25.)\n+      self.assertEqual(self.evaluate(auc_obj.false_positives[1]), 25.)\n+      self.assertEqual(self.evaluate(auc_obj.false_negatives[1]), 25.)\n+      self.assertEqual(self.evaluate(auc_obj.true_negatives[1]), 25.)\n+\n   def test_reset_states_auc_manual_thresholds(self):\n     auc_obj = metrics.AUC(thresholds=[0.5])\n     model = _get_model([auc_obj])\n\n@@ -151,7 +151,7 @@ class Adadelta(optimizer_v2.OptimizerV2):\n     config = super(Adadelta, self).get_config()\n     config.update({\n         'learning_rate': self._serialize_hyperparameter('learning_rate'),\n-        'decay': self._serialize_hyperparameter('decay'),\n+        'decay': self._initial_decay,\n         'rho': self._serialize_hyperparameter('rho'),\n         'epsilon': self.epsilon,\n     })\n\n@@ -153,7 +153,7 @@ class Adagrad(optimizer_v2.OptimizerV2):\n     config = super(Adagrad, self).get_config()\n     config.update({\n         'learning_rate': self._serialize_hyperparameter('learning_rate'),\n-        'decay': self._serialize_hyperparameter('decay'),\n+        'decay': self._initial_decay,\n         'initial_accumulator_value': self._initial_accumulator_value,\n         'epsilon': self.epsilon,\n     })\n\n@@ -238,7 +238,7 @@ class Adam(optimizer_v2.OptimizerV2):\n     config = super(Adam, self).get_config()\n     config.update({\n         'learning_rate': self._serialize_hyperparameter('learning_rate'),\n-        'decay': self._serialize_hyperparameter('decay'),\n+        'decay': self._initial_decay,\n         'beta_1': self._serialize_hyperparameter('beta_1'),\n         'beta_2': self._serialize_hyperparameter('beta_2'),\n         'epsilon': self.epsilon,\n@@ -462,7 +462,7 @@ class NonFusedAdam(optimizer_v2.OptimizerV2):\n     config = super(NonFusedAdam, self).get_config()\n     config.update({\n         'learning_rate': self._serialize_hyperparameter('learning_rate'),\n-        'decay': self._serialize_hyperparameter('decay'),\n+        'decay': self._initial_decay,\n         'beta_1': self._serialize_hyperparameter('beta_1'),\n         'beta_2': self._serialize_hyperparameter('beta_2'),\n         'epsilon': self.epsilon,\n\n@@ -175,7 +175,7 @@ class Adamax(optimizer_v2.OptimizerV2):\n     config = super(Adamax, self).get_config()\n     config.update({\n         'learning_rate': self._serialize_hyperparameter('learning_rate'),\n-        'decay': self._serialize_hyperparameter('decay'),\n+        'decay': self._initial_decay,\n         'beta_1': self._serialize_hyperparameter('beta_1'),\n         'beta_2': self._serialize_hyperparameter('beta_2'),\n         'epsilon': self.epsilon,\n\n@@ -232,7 +232,7 @@ class Ftrl(optimizer_v2.OptimizerV2):\n         'learning_rate':\n             self._serialize_hyperparameter('learning_rate'),\n         'decay':\n-            self._serialize_hyperparameter('decay'),\n+            self._initial_decay,\n         'initial_accumulator_value':\n             self._initial_accumulator_value,\n         'learning_rate_power':\n\n@@ -184,7 +184,7 @@ class SGD(optimizer_v2.OptimizerV2):\n     config = super(SGD, self).get_config()\n     config.update({\n         \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n-        \"decay\": self._serialize_hyperparameter(\"decay\"),\n+        \"decay\": self._initial_decay,\n         \"momentum\": self._serialize_hyperparameter(\"momentum\"),\n         \"nesterov\": self.nesterov,\n     })\n\n@@ -209,7 +209,7 @@ class Nadam(optimizer_v2.OptimizerV2):\n     config = super(Nadam, self).get_config()\n     config.update({\n         'learning_rate': self._serialize_hyperparameter('learning_rate'),\n-        'decay': self._serialize_hyperparameter('decay'),\n+        'decay': self._initial_decay,\n         'beta_1': self._serialize_hyperparameter('beta_1'),\n         'beta_2': self._serialize_hyperparameter('beta_2'),\n         'epsilon': self.epsilon,\n\n@@ -281,8 +281,11 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n   If you intend to create your own optimization algorithm, simply inherit from\n   this class and override the following methods:\n \n-    - `_resource_apply_dense` (update variable given gradient tensor is dense)\n-    - `_resource_apply_sparse` (update variable given gradient tensor is sparse)\n+    - `_resource_apply_dense` (update variable given gradient tensor is a dense\n+      `tf.Tensor`)\n+    - `_resource_apply_sparse` (update variable given gradient tensor is a\n+      sparse `tf.IndexedSlices`. The most common way for this to happen\n+      is if you are taking the gradient through a `tf.gather`.)\n     - `_create_slots`\n       (if your optimizer algorithm requires additional variables)\n     - `get_config`\n@@ -986,7 +989,7 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n       lr_t = tf.cast(lr_t(local_step), var_dtype)\n     if self._initial_decay > 0.:\n       local_step = tf.cast(self.iterations, var_dtype)\n-      decay_t = self._get_hyper(\"decay\", var_dtype)\n+      decay_t = tf.cast(self._initial_decay, var_dtype)\n       lr_t = lr_t / (1. + decay_t * local_step)\n     return lr_t\n \n\n@@ -897,6 +897,19 @@ class OptimizerWithFunctionTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllClose([0., 1.], fn(), atol=1e-4)\n     self.assertAllClose([-1, 0.], fn(), atol=1e-4)\n \n+  def testBasicWithConstantDecay(self):\n+    var = tf.Variable([1.0, 2.0], dtype=tf.float32)\n+    loss = lambda: 3 * var\n+    opt = adam.Adam(learning_rate=1.0)\n+\n+    @tf.function\n+    def fn():\n+      opt.minimize(loss, [var])\n+      return var\n+\n+    self.assertAllClose([0., 1.], fn(), atol=1e-4)\n+    self.assertAllClose([-1, 0.], fn(), atol=1e-4)\n+\n   def testVarKeyWithVarCreatedInEager(self):\n     a = tf.Variable([1., 2.], name='var')\n     b = tf.Variable([1.], name='var')\n\n@@ -285,7 +285,7 @@ class RMSprop(optimizer_v2.OptimizerV2):\n     config = super(RMSprop, self).get_config()\n     config.update({\n         \"learning_rate\": self._serialize_hyperparameter(\"learning_rate\"),\n-        \"decay\": self._serialize_hyperparameter(\"decay\"),\n+        \"decay\": self._initial_decay,\n         \"rho\": self._serialize_hyperparameter(\"rho\"),\n         \"momentum\": self._serialize_hyperparameter(\"momentum\"),\n         \"epsilon\": self.epsilon,\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Keras data preprocessing utils.\"\"\"\n+\"\"\"Provides keras data preprocessing utils to pre-process tf.data.Datasets before they are fed to the model.\"\"\"\n # pylint: disable=g-import-not-at-top\n from __future__ import absolute_import\n from __future__ import division\n\n@@ -179,7 +179,9 @@ def load_model_from_hdf5(filepath, custom_objects=None, compile=True):  # pylint\n     model_config = f.attrs.get('model_config')\n     if model_config is None:\n       raise ValueError('No model found in config file.')\n-    model_config = json_utils.decode(model_config.decode('utf-8'))\n+    if hasattr(model_config, 'decode'):\n+      model_config = model_config.decode('utf-8')\n+    model_config = json_utils.decode(model_config)\n     model = model_config_lib.model_from_config(model_config,\n                                                custom_objects=custom_objects)\n \n@@ -189,11 +191,13 @@ def load_model_from_hdf5(filepath, custom_objects=None, compile=True):  # pylint\n     if compile:\n       # instantiate optimizer\n       training_config = f.attrs.get('training_config')\n+      if hasattr(training_config, 'decode'):\n+        training_config = training_config.decode('utf-8')\n       if training_config is None:\n         logging.warning('No training configuration found in the save file, so '\n                         'the model was *not* compiled. Compile it manually.')\n         return model\n-      training_config = json_utils.decode(training_config.decode('utf-8'))\n+      training_config = json_utils.decode(training_config)\n \n       # Compile model.\n       model.compile(**saving_utils.compile_args_from_training_config(\n@@ -659,11 +663,15 @@ def load_weights_from_hdf5_group(f, layers):\n           and weights file.\n   \"\"\"\n   if 'keras_version' in f.attrs:\n-    original_keras_version = f.attrs['keras_version'].decode('utf8')\n+    original_keras_version = f.attrs['keras_version']\n+    if hasattr(original_keras_version, 'decode'):\n+      original_keras_version = original_keras_version.decode('utf8')\n   else:\n     original_keras_version = '1'\n   if 'backend' in f.attrs:\n-    original_backend = f.attrs['backend'].decode('utf8')\n+    original_backend = f.attrs['backend']\n+    if hasattr(original_backend, 'decode'):\n+      original_backend = original_backend.decode('utf8')\n   else:\n     original_backend = None\n \n@@ -730,11 +738,15 @@ def load_weights_from_hdf5_group_by_name(\n           and weights file and skip_match=False.\n   \"\"\"\n   if 'keras_version' in f.attrs:\n-    original_keras_version = f.attrs['keras_version'].decode('utf8')\n+    original_keras_version = f.attrs['keras_version']\n+    if hasattr(original_keras_version, 'decode'):\n+      original_keras_version = original_keras_version.decode('utf8')\n   else:\n     original_keras_version = '1'\n   if 'backend' in f.attrs:\n-    original_backend = f.attrs['backend'].decode('utf8')\n+    original_backend = f.attrs['backend']\n+    if hasattr(original_backend, 'decode'):\n+      original_backend = original_backend.decode('utf8')\n   else:\n     original_backend = None\n \n@@ -849,13 +861,18 @@ def load_attributes_from_hdf5_group(group, name):\n       data: Attributes data.\n   \"\"\"\n   if name in group.attrs:\n-    data = [n.decode('utf8') for n in group.attrs[name]]\n+    data = [\n+        n.decode('utf8') if hasattr(n, 'decode') else n\n+        for n in group.attrs[name]\n+    ]\n   else:\n     data = []\n     chunk_id = 0\n     while '%s%d' % (name, chunk_id) in group.attrs:\n-      data.extend(\n-          [n.decode('utf8') for n in group.attrs['%s%d' % (name, chunk_id)]])\n+      data.extend([\n+          n.decode('utf8') if hasattr(n, 'decode') else n\n+          for n in group.attrs['%s%d' % (name, chunk_id)]\n+      ])\n       chunk_id += 1\n   return data\n \n\n@@ -1,1316 +0,0 @@\n-# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-#,============================================================================\n-\"\"\"Tests for model saving in the HDF5 format.\"\"\"\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-\n-import os\n-import shutil\n-import tempfile\n-import uuid\n-\n-from absl.testing import parameterized\n-import numpy as np\n-\n-import keras\n-from keras import combinations\n-from keras import keras_parameterized\n-from keras import optimizer_v1\n-from keras import optimizers\n-from keras import testing_utils\n-from keras.engine import training\n-from keras.saving import hdf5_format\n-from tensorflow.python.platform import tf_logging as logging\n-\n-try:\n-  import h5py  # pylint:disable=g-import-not-at-top\n-except ImportError:\n-  h5py = None\n-\n-\n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-class TestWeightSavingAndLoading(tf.test.TestCase, parameterized.TestCase):\n-\n-  @keras_parameterized.run_with_all_weight_formats\n-  def test_weight_loading(self):\n-    temp_dir = self.get_temp_dir()\n-    self.addCleanup(shutil.rmtree, temp_dir)\n-    saved_model_dir = os.path.join(temp_dir, 'saved_model')\n-    save_format = testing_utils.get_save_format()\n-    with self.cached_session():\n-      a = keras.layers.Input(shape=(2,))\n-      x = keras.layers.Dense(3)(a)\n-      b = keras.layers.Dense(1)(x)\n-      model = keras.models.Model(a, b)\n-\n-      x = np.random.random((3, 2))\n-      ref_y = model.predict(x)\n-      weights = model.get_weights()\n-      model.set_weights(weights)\n-      y = model.predict(x)\n-      self.assertAllClose(ref_y, y)\n-\n-      with self.assertRaises(ValueError):\n-        model.set_weights(weights[1:])\n-      with self.assertRaises(ValueError):\n-        model.set_weights(weights[::-1])\n-\n-      model.save_weights(saved_model_dir, save_format=save_format)\n-      model.load_weights(saved_model_dir)\n-      y = model.predict(x)\n-      self.assertAllClose(ref_y, y)\n-\n-  def test_weight_preprocessing(self):\n-    input_dim = 3\n-    output_dim = 3\n-    size = 2\n-    cases = [\n-        [\n-            (keras.layers.Bidirectional(keras.layers.SimpleRNN(2))),\n-            [np.random.random((2, 1)), np.random.random((2, 1))],\n-            (None, 3, 2),\n-        ],\n-        [\n-            (keras.layers.TimeDistributed(keras.layers.Dense(1))),\n-            [np.random.random((2, 1)), np.random.random((1,))],\n-            (None, 3, 2),\n-        ],\n-        [\n-            (keras.layers.Conv1D(output_dim, size, use_bias=False)),\n-            [np.random.random((output_dim, input_dim, size, 1))],\n-            (None, 4, input_dim),\n-        ],\n-        [\n-            (keras.layers.Conv2D(output_dim, size,\n-                                 use_bias=False, data_format='channels_first')),\n-            [np.random.random((output_dim, input_dim, size, size))],\n-            (None, input_dim, 4, 4),\n-        ],\n-        [\n-            (keras.layers.Conv2DTranspose(output_dim, size,\n-                                          use_bias=False,\n-                                          data_format='channels_first')),\n-            [np.random.random((output_dim, input_dim, size, size))],\n-            (None, input_dim, 4, 4),\n-        ],\n-        [\n-            (keras.layers.Conv2DTranspose(output_dim, size,\n-                                          use_bias=False,\n-                                          data_format='channels_last')),\n-            [np.random.random((size, size, input_dim, output_dim))],\n-            (None, 4, 4, input_dim),\n-        ],\n-        [\n-            (keras.layers.Conv3D(output_dim, size,\n-                                 use_bias=False, data_format='channels_first')),\n-            [np.random.random((output_dim, input_dim, size, size, size))],\n-            (None, input_dim, 4, 4, 4),\n-        ],\n-        [\n-            (keras.layers.GRUV1(output_dim)),\n-            [np.random.random((input_dim, output_dim)),\n-             np.random.random((output_dim, output_dim)),\n-             np.random.random((output_dim,)),\n-             np.random.random((input_dim, output_dim)),\n-             np.random.random((output_dim, output_dim)),\n-             np.random.random((output_dim,)),\n-             np.random.random((input_dim, output_dim)),\n-             np.random.random((output_dim, output_dim)),\n-             np.random.random((output_dim,))],\n-            (None, 4, input_dim),\n-        ],\n-        [\n-            (keras.layers.LSTMV1(output_dim)),\n-            [np.random.random((input_dim, output_dim)),\n-             np.random.random((output_dim, output_dim)),\n-             np.random.random((output_dim,)),\n-             np.random.random((input_dim, output_dim)),\n-             np.random.random((output_dim, output_dim)),\n-             np.random.random((output_dim,)),\n-             np.random.random((input_dim, output_dim)),\n-             np.random.random((output_dim, output_dim)),\n-             np.random.random((output_dim,)),\n-             np.random.random((input_dim, output_dim)),\n-             np.random.random((output_dim, output_dim)),\n-             np.random.random((output_dim,))],\n-            (None, 4, input_dim),\n-        ],\n-    ]\n-    for layer, weights, input_shape in cases:\n-      layer.build(input_shape)\n-      _ = hdf5_format.preprocess_weights_for_loading(\n-          layer, weights, original_keras_version='1')\n-\n-    model = keras.models.Sequential([keras.layers.Dense(2, input_dim=2)])\n-    _ = hdf5_format.preprocess_weights_for_loading(\n-        model, model.weights, original_keras_version='1')\n-\n-    x = keras.Input((2,))\n-    y = keras.layers.Dense(2)(x)\n-    model = keras.models.Model(x, y)\n-    _ = hdf5_format.preprocess_weights_for_loading(\n-        model, model.weights, original_keras_version='1')\n-\n-  @parameterized.named_parameters(\n-      ('gru', keras.layers.GRU, {\n-          'units': 2,\n-          'input_shape': (3, 5)\n-      }),\n-      ('gru_with_reset_after', keras.layers.GRU, {\n-          'units': 2,\n-          'input_shape': (3, 5),\n-          'reset_after': True\n-      }),\n-      ('lstm', keras.layers.LSTM, {\n-          'units': 2,\n-          'input_shape': (3, 5)\n-      }),\n-      ('cudnngru', keras.layers.CuDNNGRU, {\n-          'units': 2,\n-          'input_shape': (3, 5)\n-      }),\n-      ('cudnnlstm', keras.layers.CuDNNLSTM, {\n-          'units': 2,\n-          'input_shape': (3, 5)\n-      }))\n-  def test_preprocess_weights_for_loading_rnn_should_be_idempotent(\n-      self, layer_class, layer_args):\n-    with self.cached_session():\n-      layer = layer_class(**layer_args)\n-      layer.build(input_shape=layer_args.get('input_shape'))\n-      weights1 = layer.get_weights()\n-      weights2 = hdf5_format.preprocess_weights_for_loading(\n-          layer, weights1)\n-      _ = [\n-          self.assertAllClose(x, y, rtol=1e-05)\n-          for (x, y) in zip(weights1, weights2)\n-      ]\n-\n-  def test_sequential_weight_loading(self):\n-    if h5py is None:\n-      return\n-\n-    temp_dir = self.get_temp_dir()\n-    self.addCleanup(shutil.rmtree, temp_dir)\n-    h5_path = os.path.join(temp_dir, 'test.h5')\n-\n-    num_hidden = 5\n-    input_dim = 3\n-    batch_size = 5\n-    num_classes = 2\n-\n-    with self.cached_session():\n-      model = keras.models.Sequential()\n-      model.add(keras.layers.Dense(num_hidden, input_dim=input_dim))\n-      model.add(keras.layers.Dense(num_classes))\n-\n-      x = np.random.random((batch_size, input_dim))\n-      ref_y = model.predict(x)\n-\n-      model.save_weights(h5_path)\n-\n-      model = keras.models.Sequential()\n-      model.add(keras.layers.Dense(num_hidden, input_dim=input_dim))\n-      model.add(keras.layers.Dense(num_classes))\n-      model.load_weights(h5_path)\n-      y = model.predict(x)\n-\n-      self.assertAllClose(y, ref_y)\n-\n-  @keras_parameterized.run_with_all_saved_model_formats\n-  def test_nested_model_weight_loading(self):\n-    save_format = testing_utils.get_save_format()\n-    temp_dir = self.get_temp_dir()\n-    self.addCleanup(shutil.rmtree, temp_dir)\n-    saved_model_dir = os.path.join(temp_dir, 'saved_model')\n-\n-    batch_size = 5\n-    shape = (None, None, 3)\n-\n-    with self.cached_session():\n-      def gen_model():\n-\n-        def seq_model():\n-          model = keras.models.Sequential([\n-              keras.layers.Conv2D(3, 1, input_shape=shape),\n-              keras.layers.BatchNormalization()])\n-          return model\n-\n-        x = inner_inputs = keras.layers.Input((None, None, 3))\n-        x = seq_model()(x)\n-        x = seq_model()(x)\n-        inner_model = keras.models.Model(inner_inputs, x)\n-\n-        inputs = keras.layers.Input(shape)\n-        return keras.models.Model(inputs, inner_model(inputs))\n-\n-      model = gen_model()\n-      x = np.random.random((batch_size, 1, 1, 3))\n-      ref_y = model.predict(x)\n-\n-      model.save_weights(saved_model_dir, save_format=save_format)\n-\n-      model = gen_model()\n-      model.load_weights(saved_model_dir)\n-      y = model.predict(x)\n-\n-      self.assertAllClose(y, ref_y)\n-\n-  def test_sequential_weight_loading_group_name_with_incorrect_length(self):\n-    if h5py is None:\n-      return\n-\n-    temp_dir = self.get_temp_dir()\n-    self.addCleanup(shutil.rmtree, temp_dir)\n-    h5_path = os.path.join(temp_dir, 'test.h5')\n-\n-    num_hidden = 5\n-    input_dim = 3\n-    num_classes = 2\n-    with self.cached_session():\n-      ref_model = keras.models.Sequential()\n-      ref_model.add(keras.layers.Dense(num_hidden, input_dim=input_dim,\n-                                       name='d1'))\n-      ref_model.add(keras.layers.Dense(num_classes, name='d2'))\n-      ref_model.compile(loss=keras.losses.MSE,\n-                        optimizer='rmsprop',\n-                        metrics=[keras.metrics.categorical_accuracy])\n-\n-      f_ref_model = h5py.File(h5_path, 'w')\n-      hdf5_format.save_weights_to_hdf5_group(f_ref_model, ref_model.layers)\n-\n-      f_model = h5py.File(h5_path, 'r')\n-      model = keras.models.Sequential()\n-      model.add(keras.layers.Dense(num_hidden, use_bias=False,\n-                                   input_dim=input_dim, name='d1'))\n-      model.add(keras.layers.Dense(num_classes, name='d2'))\n-      model.compile(loss=keras.losses.MSE,\n-                    optimizer='rmsprop',\n-                    metrics=[keras.metrics.categorical_accuracy])\n-      with self.assertRaisesRegex(\n-          ValueError, r'Layer #0 \\(named \\\"d1\\\"\\) expects 1 '\n-          r'weight\\(s\\), but the saved weights have 2 '\n-          r'element\\(s\\)\\.'):\n-        hdf5_format.load_weights_from_hdf5_group_by_name(f_model, model.layers)\n-\n-      hdf5_format.load_weights_from_hdf5_group_by_name(\n-          f_model, model.layers, skip_mismatch=True)\n-      self.assertAllClose(keras.backend.get_value(ref_model.layers[1].kernel),\n-                          keras.backend.get_value(model.layers[1].kernel))\n-\n-  def test_sequential_weight_loading_group_name_with_incorrect_shape(self):\n-    if h5py is None:\n-      return\n-\n-    temp_dir = self.get_temp_dir()\n-    self.addCleanup(shutil.rmtree, temp_dir)\n-    h5_path = os.path.join(temp_dir, 'test.h5')\n-\n-    num_hidden = 5\n-    input_dim = 3\n-    num_classes = 2\n-    with tf.Graph().as_default(), self.cached_session():\n-      ref_model = keras.models.Sequential()\n-      ref_model.add(keras.layers.Dense(num_hidden, input_dim=input_dim,\n-                                       name='d1'))\n-      ref_model.add(keras.layers.Dense(num_classes, name='d2'))\n-      ref_model.compile(loss=keras.losses.MSE,\n-                        optimizer=optimizer_v1.RMSprop(lr=0.0001),\n-                        metrics=[keras.metrics.categorical_accuracy])\n-\n-      f_ref_model = h5py.File(h5_path, 'w')\n-      keras.backend.set_value(ref_model.layers[1].bias, [3.5] * num_classes)\n-      hdf5_format.save_weights_to_hdf5_group(f_ref_model, ref_model.layers)\n-\n-      f_model = h5py.File(h5_path, 'r')\n-      model = keras.models.Sequential()\n-      model.add(keras.layers.Dense(num_hidden + 5, input_dim=input_dim,\n-                                   name='d1'))\n-      model.add(keras.layers.Dense(num_classes, name='d2'))\n-      model.compile(loss=keras.losses.MSE,\n-                    optimizer=optimizer_v1.RMSprop(lr=0.0001),\n-                    metrics=[keras.metrics.categorical_accuracy])\n-      with self.assertRaisesRegex(\n-          ValueError, r'Layer #0 \\(named \"d1\"\\), weight '\n-          r'<tf\\.Variable \\'d1_1\\/kernel:0\\' '\n-          r'shape=\\(3, 10\\) dtype=float32> has '\n-          r'shape \\(3, 10\\), but the saved weight has '\n-          r'shape \\(3, 5\\)\\.'):\n-        hdf5_format.load_weights_from_hdf5_group_by_name(f_model, model.layers)\n-\n-      hdf5_format.load_weights_from_hdf5_group_by_name(\n-          f_model, model.layers, skip_mismatch=True)\n-      self.assertAllClose([3.5] * num_classes,\n-                          keras.backend.get_value(model.layers[1].bias))\n-\n-\n-@keras_parameterized.run_with_all_saved_model_formats\n-class TestWholeModelSaving(keras_parameterized.TestCase):\n-\n-  def _save_model_dir(self, dirname='saved_model'):\n-    temp_dir = self.get_temp_dir()\n-    self.addCleanup(shutil.rmtree, temp_dir, ignore_errors=True)\n-    return os.path.join(temp_dir, dirname)\n-\n-  def _assert_same_weights_and_metrics(self, model, loaded_model):\n-    \"\"\"Checks that the loaded weights and metrics are the same as the original.\n-\n-    Args:\n-      model: original model\n-      loaded_model: loaded model\n-    \"\"\"\n-    self.assertAllClose(model.weights, loaded_model.weights)\n-\n-    if loaded_model.optimizer:\n-      if testing_utils.get_save_format() == 'tf':\n-        # TODO(b/153110928): Keras TF format doesn't restore optimizer weights\n-        # currently.\n-        return\n-      self.assertAllClose(model.optimizer.weights,\n-                          loaded_model.optimizer.weights)\n-\n-    # In V1/Graph mode, the model isn't built, so the metrics are not loaded\n-    # immediately (requires model to be called on some data before building\n-    # metrics).\n-    check_metrics = tf.__internal__.tf2.enabled() and tf.executing_eagerly()\n-\n-    if check_metrics:\n-      self.assertAllEqual([m.name for m in model.metrics],\n-                          [m.name for m in loaded_model.metrics])\n-\n-  @keras_parameterized.run_with_all_model_types\n-  @keras_parameterized.run_all_keras_modes\n-  def test_save_and_load(self):\n-    saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n-    save_kwargs = testing_utils.get_save_kwargs()\n-\n-    if ((save_format == 'h5' or not save_kwargs.get('save_traces', True)) and\n-        testing_utils.get_model_type() == 'subclass'):\n-      # HDF5 format currently does not allow saving subclassed models.\n-      # When saving with `save_traces=False`, the subclassed model must have a\n-      # get_config/from_config, which the autogenerated model does not have.\n-      return\n-\n-    with self.cached_session():\n-      model = testing_utils.get_model_from_layers(\n-          [keras.layers.Dense(2),\n-           keras.layers.RepeatVector(3),\n-           keras.layers.TimeDistributed(keras.layers.Dense(3))],\n-          input_shape=(3,))\n-      model.compile(\n-          loss=keras.losses.MSE,\n-          optimizer=keras.optimizer_v2.rmsprop.RMSprop(lr=0.0001),\n-          metrics=[\n-              keras.metrics.categorical_accuracy,\n-              keras.metrics.CategoricalCrossentropy(\n-                  name='cce', label_smoothing=tf.constant(0.2)),\n-          ],\n-          weighted_metrics=[\n-              keras.metrics.categorical_crossentropy,\n-              keras.metrics.CategoricalCrossentropy(\n-                  name='cce', label_smoothing=tf.constant(0.2)),\n-          ],\n-          sample_weight_mode='temporal')\n-\n-      x = np.random.random((1, 3))\n-      y = np.random.random((1, 3, 3))\n-      model.train_on_batch(x, y)\n-\n-      out = model.predict(x)\n-      keras.models.save_model(\n-          model, saved_model_dir, save_format=save_format,\n-          **save_kwargs)\n-\n-      loaded_model = keras.models.load_model(saved_model_dir)\n-      self._assert_same_weights_and_metrics(model, loaded_model)\n-\n-      out2 = loaded_model.predict(x)\n-      self.assertAllClose(out, out2, atol=1e-05)\n-\n-      eval_out = model.evaluate(x, y)\n-      eval_out2 = loaded_model.evaluate(x, y)\n-      self.assertArrayNear(eval_out, eval_out2, 0.001)\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def test_sequential_model_saving_without_input_shape(self):\n-    saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n-    with self.cached_session():\n-      model = keras.models.Sequential()\n-      model.add(keras.layers.Dense(2))\n-      model.add(keras.layers.RepeatVector(3))\n-      model.add(keras.layers.TimeDistributed(keras.layers.Dense(3)))\n-      model.compile(\n-          loss=keras.losses.MSE,\n-          optimizer='rmsprop',\n-          metrics=[\n-              keras.metrics.categorical_accuracy,\n-              keras.metrics.CategoricalAccuracy(name='cat_acc')\n-          ],\n-          weighted_metrics=[\n-              keras.metrics.categorical_accuracy,\n-              keras.metrics.CategoricalAccuracy(name='cat_acc2')\n-          ],\n-          sample_weight_mode='temporal')\n-      x = np.random.random((1, 3))\n-      y = np.random.random((1, 3, 3))\n-      model.train_on_batch(x, y)\n-\n-      out = model.predict(x)\n-      model.save(saved_model_dir, save_format=save_format)\n-\n-      new_model = keras.models.load_model(saved_model_dir)\n-\n-      self._assert_same_weights_and_metrics(model, new_model)\n-\n-      out2 = new_model.predict(x)\n-      self.assertAllClose(out, out2, atol=1e-05)\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def test_sequential_model_saving_without_compile(self):\n-    saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n-    with self.cached_session():\n-      model = keras.models.Sequential()\n-      model.add(keras.layers.Dense(2, input_shape=(3,)))\n-      model.add(keras.layers.RepeatVector(3))\n-      model.add(keras.layers.TimeDistributed(keras.layers.Dense(3)))\n-\n-      x = np.random.random((1, 3))\n-      out = model.predict(x)\n-\n-      # Save the model without any compilation or training.\n-      keras.models.save_model(model, saved_model_dir, save_format=save_format)\n-\n-      new_model = keras.models.load_model(saved_model_dir)\n-      self._assert_same_weights_and_metrics(model, new_model)\n-\n-      out2 = new_model.predict(x)\n-      self.assertAllClose(out, out2, atol=1e-05)\n-\n-  def test_sequential_model_saving_2(self):\n-    saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n-\n-    with tf.Graph().as_default(), self.cached_session():\n-      # test with custom optimizer, loss\n-\n-      class CustomOp(optimizer_v1.RMSprop):\n-        pass\n-\n-      def custom_loss(y_true, y_pred):\n-        return keras.losses.mse(y_true, y_pred)\n-\n-      model = keras.models.Sequential()\n-      model.add(keras.layers.Dense(2, input_shape=(3,)))\n-      model.add(keras.layers.Dense(3))\n-      model.compile(loss=custom_loss, optimizer=CustomOp(), metrics=['acc'])\n-\n-      x = np.random.random((1, 3))\n-      y = np.random.random((1, 3))\n-      model.train_on_batch(x, y)\n-\n-      out = model.predict(x)\n-      keras.models.save_model(model, saved_model_dir, save_format=save_format)\n-\n-      new_model = keras.models.load_model(\n-          saved_model_dir,\n-          custom_objects={'CustomOp': CustomOp,\n-                          'custom_loss': custom_loss})\n-      self._assert_same_weights_and_metrics(model, new_model)\n-\n-      out2 = new_model.predict(x)\n-      self.assertAllClose(out, out2, atol=1e-05)\n-\n-  def test_saving_without_compilation(self):\n-    saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n-    model = keras.models.Sequential()\n-    model.add(keras.layers.Dense(2, input_shape=(3,)))\n-    model.add(keras.layers.Dense(3))\n-    model.compile(loss='mse', optimizer='sgd', metrics=['acc'])\n-\n-    keras.models.save_model(model, saved_model_dir, save_format=save_format)\n-    model = keras.models.load_model(saved_model_dir)\n-\n-  def test_saving_with_tf_optimizer(self):\n-    saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n-\n-    model = keras.models.Sequential()\n-    model.add(keras.layers.Dense(2, input_shape=(3,)))\n-    model.add(keras.layers.Dense(3))\n-    model.compile(loss='mse',\n-                  optimizer=tf.compat.v1.train.AdadeltaOptimizer(0.1),\n-                  metrics=['acc'])\n-\n-    keras.models.save_model(model, saved_model_dir, save_format=save_format)\n-    model = keras.models.load_model(saved_model_dir)\n-\n-  def test_saving_right_after_compilation(self):\n-    saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n-    with self.cached_session():\n-      model = keras.models.Sequential()\n-      model.add(keras.layers.Dense(2, input_shape=(3,)))\n-      model.add(keras.layers.Dense(3))\n-      model.compile(loss='mse', optimizer='sgd', metrics=['acc'])\n-      if not tf.compat.v1.executing_eagerly_outside_functions():\n-        model._make_train_function()\n-      keras.models.save_model(model, saved_model_dir, save_format=save_format)\n-      model = keras.models.load_model(saved_model_dir)\n-\n-  def test_saving_lambda_numpy_array_arguments(self):\n-    saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n-\n-    if h5py is None:\n-      self.skipTest('h5py required to run this test')\n-\n-    mean = np.random.random((4, 2, 3))\n-    std = np.abs(np.random.random((4, 2, 3))) + 1e-5\n-    inputs = keras.layers.Input(shape=(4, 2, 3))\n-    output = keras.layers.Lambda(lambda image, mu, std: (image - mu) / std,\n-                                 arguments={'mu': mean, 'std': std})(inputs)\n-    model = keras.models.Model(inputs, output)\n-    model.compile(loss='mse', optimizer='sgd', metrics=['acc'])\n-\n-    keras.models.save_model(model, saved_model_dir, save_format=save_format)\n-\n-    model = keras.models.load_model(saved_model_dir)\n-\n-    self.assertAllClose(mean, model.layers[1].arguments['mu'])\n-    self.assertAllClose(std, model.layers[1].arguments['std'])\n-\n-  def test_saving_model_with_long_layer_names(self):\n-    saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n-    with self.cached_session():\n-      # This layer name will make the `layers_name` HDF5 attribute blow\n-      # out of proportion. Note that it fits into the internal HDF5\n-      # attribute memory limit on its own but because h5py converts\n-      # the list of layer names into numpy array, which uses the same\n-      # amount of memory for every item, it increases the memory\n-      # requirements substantially.\n-      x = keras.Input(shape=(2,), name='input_' + ('x' * (2**15)))\n-      f = x\n-      for i in range(4):\n-        f = keras.layers.Dense(2, name='dense_%d' % (i,))(f)\n-      model = keras.Model(inputs=[x], outputs=[f])\n-      model.compile(\n-          'adam', loss=keras.losses.MeanSquaredError(), metrics=['acc'])\n-\n-      x = np.random.random((1, 2))\n-      y = np.random.random((1, 2))\n-      model.train_on_batch(x, y)\n-      out = model.predict(x)\n-\n-      keras.models.save_model(model, saved_model_dir, save_format=save_format)\n-      model = keras.models.load_model(saved_model_dir)\n-\n-      if save_format in ['tf', 'tensorflow']:\n-        return\n-      # Check that the HDF5 files contains chunked array\n-      # of layer names.\n-      with h5py.File(saved_model_dir, 'r') as h5file:\n-        num_names_arrays = len([attr for attr in h5file['model_weights'].attrs\n-                                if attr.startswith('layer_names')])\n-      # The chunking of layer names array should have happened.\n-      self.assertGreater(num_names_arrays, 0)\n-      out2 = model.predict(x)\n-      self.assertAllClose(out, out2, atol=1e-05)\n-\n-  def test_saving_model_with_long_weights_names(self):\n-    saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n-\n-    with self.cached_session():\n-      x = keras.Input(shape=(2,), name='nested_model_input')\n-      f = x\n-      for i in range(4):\n-        f = keras.layers.Dense(2, name='nested_model_dense_%d' % (i,))(f)\n-      # This layer name will make the `weights_name`\n-      # HDF5 attribute blow out of proportion.\n-      f = keras.layers.Dense(2, name='nested_model_output' + ('x' * (2**14)))(f)\n-      nested_model = keras.Model(inputs=[x], outputs=[f], name='nested_model')\n-\n-      x = keras.Input(shape=(2,), name='outer_model_input')\n-      f = nested_model(x)\n-      f = keras.layers.Dense(2, name='outer_model_output')(f)\n-\n-      model = keras.Model(inputs=[x], outputs=[f])\n-      model.compile(loss='mse', optimizer='adam', metrics=['acc'])\n-\n-      x = np.random.random((1, 2))\n-      y = np.random.random((1, 2))\n-      model.train_on_batch(x, y)\n-      out = model.predict(x)\n-\n-      keras.models.save_model(model, saved_model_dir, save_format=save_format)\n-      model = keras.models.load_model(saved_model_dir)\n-\n-      if save_format in ['h5', 'hdf5', 'keras']:\n-        # Check that the HDF5 files contains chunked array\n-        # of weight names.\n-        with h5py.File(saved_model_dir, 'r') as h5file:\n-          num_weight_arrays = len(\n-              [attr for attr in h5file['model_weights']['nested_model'].attrs\n-               if attr.startswith('weight_names')])\n-        # The chunking of layer names array should have happened.\n-        self.assertGreater(num_weight_arrays, 0)\n-      out2 = model.predict(x)\n-      self.assertAllClose(out, out2, atol=1e-05)\n-\n-  def test_model_saving_to_pre_created_h5py_file(self):\n-    saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n-    with tf.Graph().as_default(), self.cached_session():\n-      inputs = keras.Input(shape=(3,))\n-      x = keras.layers.Dense(2)(inputs)\n-      outputs = keras.layers.Dense(3)(x)\n-\n-      model = keras.Model(inputs, outputs)\n-      model.compile(\n-          loss=keras.losses.MSE,\n-          optimizer=optimizer_v1.Adam(),\n-          metrics=[\n-              keras.metrics.categorical_accuracy,\n-              keras.metrics.CategoricalAccuracy()\n-          ])\n-      x = np.random.random((1, 3))\n-      y = np.random.random((1, 3))\n-      model.train_on_batch(x, y)\n-\n-      out = model.predict(x)\n-\n-      keras.models.save_model(model, saved_model_dir, save_format=save_format)\n-      loaded_model = keras.models.load_model(saved_model_dir)\n-      out1 = loaded_model.predict(x)\n-      self.assertAllClose(out, out1, atol=1e-05)\n-      if save_format in ['tf', 'tensorflow']:\n-        return\n-\n-      # Test h5 format specifically\n-      fd, fname = tempfile.mkstemp('.h5')\n-      with h5py.File(fname, mode='r+') as h5file:\n-        keras.models.save_model(model, h5file)\n-        loaded_model = keras.models.load_model(h5file)\n-        out2 = loaded_model.predict(x)\n-      self.assertAllClose(out, out2, atol=1e-05)\n-\n-      # Test non-default options in h5\n-      with h5py.File('_', driver='core',\n-                     backing_store=False) as h5file:\n-        keras.models.save_model(model, h5file)\n-        loaded_model = keras.models.load_model(h5file)\n-        out2 = loaded_model.predict(x)\n-      self.assertAllClose(out, out2, atol=1e-05)\n-\n-      # Cleanup\n-      os.close(fd)\n-      os.remove(fname)\n-\n-  def test_model_saving_to_new_dir_path(self):\n-    saved_model_dir = os.path.join(self._save_model_dir(), 'newdir',\n-                                   'saved_model')\n-    save_format = testing_utils.get_save_format()\n-\n-    with self.cached_session():\n-      model = keras.models.Sequential()\n-      model.add(keras.layers.Dense(2, input_shape=(3,)))\n-      model.add(keras.layers.RepeatVector(3))\n-      model.add(keras.layers.TimeDistributed(keras.layers.Dense(3)))\n-\n-      x = np.random.random((1, 3))\n-      out = model.predict(x)\n-\n-      keras.models.save_model(model, saved_model_dir, save_format=save_format)\n-\n-      new_model = keras.models.load_model(saved_model_dir)\n-      self._assert_same_weights_and_metrics(model, new_model)\n-\n-      out2 = new_model.predict(x)\n-      self.assertAllClose(out, out2, atol=1e-05)\n-\n-  def test_model_raise_exception_with_failed_saving(self):\n-    if h5py is None:\n-      self.skipTest('h5py required to run this test')\n-\n-    saved_model_dir = self._save_model_dir()\n-    saved_model_path = os.path.join(saved_model_dir, 'saved_model.h5')\n-\n-    with self.cached_session():\n-      model = keras.models.Sequential()\n-      model.add(keras.layers.Dense(2, input_shape=(3,)))\n-      model.add(keras.layers.RepeatVector(3))\n-      model.add(keras.layers.TimeDistributed(keras.layers.Dense(3)))\n-\n-      with self.assertRaisesRegex(OSError, 'Unable to create file'):\n-        with h5py.File(saved_model_path, 'w'):\n-          keras.models.save_model(model, saved_model_path)\n-\n-  def test_saving_constant_initializer_with_numpy(self):\n-    saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n-\n-    model = keras.models.Sequential()\n-    model.add(\n-        keras.layers.Dense(\n-            2,\n-            input_shape=(3,),\n-            kernel_initializer=keras.initializers.Constant(np.ones((3, 2)))))\n-    model.add(keras.layers.Dense(3))\n-    model.compile(loss='mse', optimizer='sgd', metrics=['acc'])\n-    keras.models.save_model(model, saved_model_dir, save_format=save_format)\n-    model = keras.models.load_model(saved_model_dir)\n-\n-  def test_saving_group_naming_h5py(self):\n-    # Test saving model with layer which name is prefix to a previous layer\n-    # name.\n-\n-    temp_dir = self.get_temp_dir()\n-    self.addCleanup(shutil.rmtree, temp_dir)\n-    h5_path = os.path.join(temp_dir, 'test.h5')\n-\n-    input_layer = keras.layers.Input((None, None, 3), name='test_input')\n-    x = keras.layers.Conv2D(1, 1, name='conv1/conv')(input_layer)\n-    x = keras.layers.Activation('relu', name='conv1')(x)\n-    model = keras.models.Model(inputs=input_layer, outputs=x)\n-\n-    model.save_weights(h5_path)\n-    model.load_weights(h5_path)\n-\n-  def test_primitive_attrs_contain_no_extraneous_strings(self):\n-    if h5py is None:\n-      self.skipTest('h5py required to run this test')\n-\n-    saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n-    model = keras.models.Sequential()\n-    model.add(keras.layers.Dense(1, input_shape=[2]))\n-    model.save(saved_model_dir, save_format=save_format)\n-    if save_format in ['tf', 'tensorflow']:\n-      return\n-\n-    h5file = h5py.File(saved_model_dir, 'r')\n-    self.assertRegex(h5file.attrs['keras_version'], r'^[\\d]+\\.[\\d]+\\.[\\S]+$')\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def test_functional_model_with_custom_loss_and_metric(self):\n-    def _make_model():\n-      inputs = keras.Input(shape=(4,))\n-      x = keras.layers.Dense(8, activation='relu')(inputs)\n-      outputs = keras.layers.Dense(3, activation='softmax')(x)\n-      model = keras.Model(inputs=inputs, outputs=outputs)\n-      custom_loss = keras.layers.Lambda(lambda x: keras.backend.sum(x * x))(x)\n-      model.add_loss(custom_loss)\n-      model.add_metric(custom_loss, aggregation='mean', name='custom_loss')\n-      return model\n-\n-    saved_model_dir = self._save_model_dir()\n-    save_format = testing_utils.get_save_format()\n-\n-    with self.cached_session():\n-      model = _make_model()\n-      model.compile(\n-          loss=keras.losses.SparseCategoricalCrossentropy(),\n-          optimizer=optimizers.gradient_descent_v2.SGD(),\n-          metrics=[keras.metrics.SparseCategoricalCrossentropy()])\n-      x = np.random.normal(size=(32, 4))\n-      y = np.random.randint(0, 3, size=32)\n-      model.train_on_batch(x, y)\n-      evaluation_results = model.evaluate(x, y)\n-      # Save and reload model.\n-      model.save(saved_model_dir, save_format=save_format)\n-      del model  # Prevent misuse.\n-      loaded_model = keras.models.load_model(saved_model_dir)\n-      loaded_model_eval_results = loaded_model.evaluate(x, y)\n-      # Assert all evaluation results are the same.\n-      self.assertAllClose(evaluation_results, loaded_model_eval_results, 1e-9)\n-      # Check correctness of the loss calculation.\n-      self.assertAllGreater(evaluation_results, 0.)\n-      evaluation_results = dict(\n-          zip(loaded_model.metrics_names, evaluation_results))\n-      self.assertNear(\n-          evaluation_results['sparse_categorical_crossentropy'] +\n-          evaluation_results['custom_loss'], evaluation_results['loss'], 1e-6)\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def test_save_uncompiled_model_with_optimizer(self):\n-    with self.cached_session() as session:\n-      saved_model_dir = self._save_model_dir()\n-      save_format = testing_utils.get_save_format()\n-      model = keras.models.Sequential([keras.layers.Dense(1, input_shape=(3,))])\n-      # Set the model's optimizer but don't compile. This can happen if the\n-      # model is trained with a custom training loop.\n-      model.optimizer = keras.optimizer_v2.rmsprop.RMSprop(lr=0.0001)\n-      if not tf.executing_eagerly():\n-        session.run([v.initializer for v in model.variables])\n-      model.save(saved_model_dir, save_format=save_format)\n-\n-      if save_format in ['tf', 'tensorflow']:\n-        loaded = keras.models.load_model(saved_model_dir)\n-        self.assertIsInstance(loaded.optimizer,\n-                              keras.optimizer_v2.optimizer_v2.OptimizerV2)\n-\n-  @combinations.generate(combinations.combine(mode=['eager']))\n-  def test_functional_model_with_getitem_op_layer(self):\n-    inp = keras.Input(shape=(8))\n-\n-    out = inp[:]\n-    model = keras.Model(\n-        inputs=[inp],\n-        outputs=out)\n-    batch_size = 7\n-    x = tf.stack([\n-        tf.range(8) for _ in range(batch_size)])\n-    args = [x]\n-    expected = x[:]\n-\n-    self.assertAllEqual(model(args), expected)\n-    self.assertAllEqual(model.predict(args, batch_size=batch_size), expected)\n-\n-    # Make sure it can be successfully saved and loaded\n-    save_format = testing_utils.get_save_format()\n-    saved_model_dir = self._save_model_dir()\n-    keras.models.save_model(model, saved_model_dir, save_format=save_format)\n-\n-    loaded_model = keras.models.load_model(saved_model_dir)\n-\n-    self.assertAllEqual(loaded_model(args), expected)\n-    self.assertAllEqual(loaded_model.predict(args, batch_size=batch_size),\n-                        expected)\n-\n-\n-# Factory functions to create models that will be serialized inside a Network.\n-def _make_graph_network(input_size, output_size):\n-  inputs = keras.Input(input_size)\n-  x = keras.layers.Dense(8, activation='relu')(inputs)\n-  y = keras.layers.Dense(output_size)(x)\n-  return keras.Model(inputs=inputs, outputs=y)\n-\n-\n-def _make_sequential(input_size, output_size):\n-  del input_size\n-  return keras.Sequential([\n-      keras.layers.Dense(8, activation='relu'),\n-      keras.layers.Dense(output_size),\n-  ])\n-\n-\n-def _make_sequential_built(input_size, output_size):\n-  model = _make_sequential(input_size, output_size)\n-  model.build((None, input_size))\n-  return model\n-\n-\n-def _make_sequential_graph_network(input_size, output_size):\n-  return keras.Sequential([\n-      keras.layers.InputLayer(input_size),\n-      keras.layers.Dense(8, activation='relu'),\n-      keras.layers.Dense(output_size),\n-  ])\n-\n-\n-def _make_sequential_input_shape(input_size, output_size):\n-  return keras.Sequential([\n-      keras.layers.Dense(8, activation='relu', input_shape=(input_size,)),\n-      keras.layers.Dense(output_size),\n-  ])\n-\n-\n-class _make_subclassed(keras.Model):  # pylint: disable=invalid-name\n-\n-  def __init__(self, input_size, output_size):\n-    super(_make_subclassed, self).__init__()\n-    self._config = {'input_size': input_size, 'output_size': output_size}\n-    self._hidden_layer = keras.layers.Dense(8, activation='relu', name='hidden')\n-    self._logits_layer = keras.layers.Dense(output_size, name='logits')\n-\n-  def call(self, inputs):\n-    x = self._hidden_layer(inputs)\n-    return self._logits_layer(x)\n-\n-  def get_config(self):\n-    return self._config\n-\n-  @classmethod\n-  def from_config(cls, config):\n-    return cls(**config)\n-\n-\n-class _make_subclassed_built(_make_subclassed):  # pylint: disable=invalid-name\n-\n-  def __init__(self, input_size, output_size):\n-    super(_make_subclassed_built, self).__init__(input_size, output_size)\n-    self.build((None, input_size))\n-\n-\n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-class TestWholeModelSavingWithNesting(tf.test.TestCase, parameterized.TestCase):\n-  \"\"\"Tests saving a whole model that contains other models.\"\"\"\n-\n-  @parameterized.named_parameters([\n-      ('graph_network', _make_graph_network),\n-      ('sequential', _make_sequential),\n-      ('sequential_built', _make_sequential_built),\n-      ('sequential_graph_network', _make_sequential_graph_network),\n-      ('sequential_input_shape', _make_sequential_input_shape),\n-      ('subclassed', _make_subclassed),\n-      ('subclassed_built', _make_subclassed_built),\n-  ])\n-  def test_functional(self, model_fn):\n-    \"\"\"Tests serializing a model that uses a nested model to share weights.\"\"\"\n-    if h5py is None:\n-      self.skipTest('h5py required to run this test')\n-\n-    def _make_model():\n-      inputs = (keras.Input(shape=(4,), name='examples'),\n-                keras.Input(shape=(4,), name='neighbors'))\n-      base_model = model_fn(inputs[0].shape.as_list()[-1], 2)\n-      outputs = keras.layers.add([base_model(inputs[0]), base_model(inputs[1])])\n-      return keras.Model(inputs=inputs, outputs=outputs)\n-\n-    with self.cached_session():\n-      x = (np.random.normal(size=(16, 4)).astype(np.float32),\n-           np.random.normal(size=(16, 4)).astype(np.float32))\n-      model = _make_model()\n-      predictions = model(x)\n-      # Save and reload.\n-      model_path = os.path.join(self.get_temp_dir(), 'model.h5')\n-      model.save(model_path)\n-      del model\n-      loaded_model = keras.models.load_model(\n-          model_path,\n-          custom_objects={\n-              '_make_subclassed': _make_subclassed,\n-              '_make_subclassed_built': _make_subclassed_built,\n-          },\n-          compile=False)\n-      self.assertAllClose(loaded_model(x), predictions, 1e-9)\n-\n-\n-class SubclassedModel(training.Model):\n-\n-  def __init__(self):\n-    super(SubclassedModel, self).__init__()\n-    self.x_layer = keras.layers.Dense(3)\n-    self.b_layer = keras.layers.Dense(1)\n-\n-  def call(self, a):\n-    return self.b_layer(self.x_layer(a))\n-\n-\n-class TestWeightSavingAndLoadingTFFormat(tf.test.TestCase, parameterized.TestCase):\n-\n-  def test_keras_optimizer_warning(self):\n-    graph = tf.Graph()\n-    with graph.as_default(), self.session(graph):\n-      model = keras.models.Sequential()\n-      model.add(keras.layers.Dense(2, input_shape=(3,)))\n-      model.add(keras.layers.Dense(3))\n-      model.compile(loss='mse', optimizer=optimizer_v1.Adam(), metrics=['acc'])\n-      if not tf.compat.v1.executing_eagerly_outside_functions():\n-        model._make_train_function()\n-      temp_dir = self.get_temp_dir()\n-      prefix = os.path.join(temp_dir, 'ckpt')\n-      with tf.compat.v1.test.mock.patch.object(logging, 'warning') as mock_log:\n-        model.save_weights(prefix)\n-        self.assertRegex(str(mock_log.call_args), 'Keras optimizer')\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def test_tensorflow_format_overwrite(self):\n-    with self.cached_session() as session:\n-      model = SubclassedModel()\n-      temp_dir = self.get_temp_dir()\n-      prefix = os.path.join(temp_dir, 'ckpt')\n-\n-      x = tf.constant(np.random.random((3, 2)), dtype=tf.float32)\n-      executing_eagerly = tf.executing_eagerly()\n-      model(x)  # pylint: disable=not-callable\n-      if not executing_eagerly:\n-        session.run([v.initializer for v in model.variables])\n-      model.save_weights(prefix, save_format='tensorflow')\n-      model.save_weights(prefix, save_format='tensorflow', overwrite=True)\n-      with self.assertRaises(EOFError):\n-        # Indirectly tests that the user is prompted\n-        model.save_weights(prefix, save_format='tensorflow', overwrite=False)\n-\n-  def test_no_default_session(self):\n-    with tf.Graph().as_default():\n-      self.assertFalse(tf.compat.v1.get_default_session())\n-      data = np.random.random((1000, 32)).astype(np.float32)\n-      labels = np.random.random((1000, 10)).astype(np.float32)\n-\n-      model = keras.models.Sequential([\n-          keras.layers.Dense(10, activation='softmax'),\n-          keras.layers.Dense(10, activation='softmax')])\n-\n-      model.compile(optimizer=tf.compat.v1.train.RMSPropOptimizer(0.001),\n-                    loss='categorical_crossentropy',\n-                    metrics=['accuracy'])\n-\n-      model.fit(data, labels)\n-      fname = os.path.join(self.get_temp_dir(), 'weights', 'ckpt')\n-      model.save_weights(fname)\n-      model.load_weights(fname)\n-\n-  def test_no_graph_pollution(self):\n-    with tf.compat.v1.get_default_graph().as_default():\n-      graph = tf.Graph()\n-      with graph.as_default(), self.session(graph) as session:\n-        model = SubclassedModel()\n-        temp_dir = self.get_temp_dir()\n-        prefix = os.path.join(temp_dir, 'ckpt')\n-\n-        x = tf.constant(np.random.random((3, 2)), dtype=tf.float32)\n-        model(x)  # pylint: disable=not-callable\n-        session.run([v.initializer for v in model.variables])\n-        model.save_weights(prefix, save_format='tensorflow')\n-        op_count = len(graph.get_operations())\n-        model.save_weights(prefix, save_format='tensorflow')\n-        self.assertLen(graph.get_operations(), op_count)\n-\n-        model.load_weights(prefix)\n-        op_count = len(graph.get_operations())\n-        model.load_weights(prefix)\n-        self.assertLen(graph.get_operations(), op_count)\n-\n-  def _weight_loading_test_template(self, make_model_fn):\n-    with self.cached_session():\n-      model = make_model_fn()\n-      model.compile(\n-          loss='mse',\n-          optimizer=tf.compat.v1.train.RMSPropOptimizer(0.1),\n-          metrics=['acc', keras.metrics.CategoricalAccuracy()])\n-      temp_dir = self.get_temp_dir()\n-      prefix = os.path.join(temp_dir, 'ckpt')\n-      train_x = np.random.random((3, 2))\n-      train_y = np.random.random((3,))\n-      x = tf.constant(train_x, dtype=tf.float32)\n-\n-      model.train_on_batch(train_x, train_y)\n-      model.save_weights(prefix, save_format='tf')\n-      ref_y_before_train = model.predict(train_x)\n-      model.train_on_batch(train_x, train_y)\n-      ref_y_after_train = model.predict(train_x)\n-      for v in model.variables:\n-        self.evaluate(\n-            v.assign(tf.random.normal(shape=tf.compat.v1.shape(v))))\n-\n-      self.addCleanup(shutil.rmtree, temp_dir)\n-\n-      model.load_weights(prefix)\n-      self.assertAllClose(ref_y_before_train, self.evaluate(model(x)))\n-\n-      # Test restore-on-create if this is a subclassed Model (graph Networks\n-      # will have already created their variables).\n-      load_model = make_model_fn()\n-      load_model.load_weights(prefix)\n-      self.assertAllClose(\n-          ref_y_before_train,\n-          self.evaluate(load_model(x)))\n-      load_model = make_model_fn()\n-      load_model.load_weights(prefix)\n-      # We need to run some of the restore ops for predict(), but not all\n-      # variables have been created yet (optimizer slot variables). Tests\n-      # incremental restore.\n-      load_model.predict(train_x)\n-      load_model.compile(\n-          loss='mse',\n-          optimizer=tf.compat.v1.train.RMSPropOptimizer(0.1),\n-          metrics=['acc', keras.metrics.CategoricalAccuracy()])\n-      load_model.train_on_batch(train_x, train_y)\n-      self.assertAllClose(ref_y_after_train, self.evaluate(load_model(x)))\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def test_weight_loading_graph_model(self):\n-    def _make_graph_model():\n-      a = keras.layers.Input(shape=(2,))\n-      x = keras.layers.Dense(3)(a)\n-      b = keras.layers.Dense(1)(x)\n-      return keras.models.Model(a, b)\n-\n-    self._weight_loading_test_template(_make_graph_model)\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def test_weight_loading_subclassed_model(self):\n-    self._weight_loading_test_template(SubclassedModel)\n-\n-  def _new_layer_weight_loading_test_template(\n-      self, first_model_fn, second_model_fn):\n-    with self.cached_session() as session:\n-      model = first_model_fn()\n-      temp_dir = self.get_temp_dir()\n-      prefix = os.path.join(temp_dir, 'ckpt')\n-\n-      x = tf.constant(np.random.random((3, 2)), dtype=tf.float32)\n-      executing_eagerly = tf.executing_eagerly()\n-      ref_y_tensor = model(x)\n-      if not executing_eagerly:\n-        session.run([v.initializer for v in model.variables])\n-      ref_y = self.evaluate(ref_y_tensor)\n-      model.save_weights(prefix)\n-      self.assertEqual(\n-          prefix,\n-          tf.train.latest_checkpoint(temp_dir))\n-      for v in model.variables:\n-        self.evaluate(\n-            v.assign(tf.random.normal(shape=tf.compat.v1.shape(v))))\n-\n-      self.addCleanup(shutil.rmtree, temp_dir)\n-\n-      second_model = second_model_fn()\n-      status = second_model.load_weights(prefix)\n-      second_model(x)\n-      status.run_restore_ops()\n-      second_model.save_weights(prefix)\n-      # Check that the second model's checkpoint loads into the original model\n-      status = model.load_weights(prefix)\n-      status.run_restore_ops(session)\n-      y = self.evaluate(model(x))\n-      self.assertAllClose(ref_y, y)\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def test_weight_loading_graph_model_added_layer(self):\n-    def _save_graph_model():\n-      a = keras.layers.Input(shape=(2,))\n-      x = keras.layers.Dense(3, name='first')(a)\n-      b = keras.layers.Dense(1, name='second')(x)\n-      return keras.models.Model(a, b)\n-    def _restore_graph_model():\n-      a = keras.layers.Input(shape=(2,))\n-      x = keras.layers.Dense(3, name='first')(a)\n-      y = keras.layers.Dense(1, name='second')(x)\n-      b = keras.layers.Dense(3, name='secondjr')(y)\n-      return keras.models.Model(a, b)\n-\n-    self._new_layer_weight_loading_test_template(\n-        _save_graph_model, _restore_graph_model)\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def test_weight_loading_graph_model_added_no_weight_layer(self):\n-    def _save_graph_model():\n-      a = keras.layers.Input(shape=(2,))\n-      x = keras.layers.Dense(3, name='first')(a)\n-      b = keras.layers.Dense(1, name='second')(x)\n-      return keras.models.Model(a, b)\n-    def _restore_graph_model():\n-      a = keras.layers.Input(shape=(2,))\n-      x = keras.layers.Dense(3, name='first')(a)\n-      b = keras.layers.Dense(1, name='second')(x)\n-      y = keras.layers.Dropout(rate=0.1)(b)\n-      return keras.models.Model(a, y)\n-\n-    self._new_layer_weight_loading_test_template(\n-        _save_graph_model, _restore_graph_model)\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def test_weight_loading_subclassed_model_added_layer(self):\n-\n-    class SubclassedModelRestore(training.Model):\n-\n-      def __init__(self):\n-        super(SubclassedModelRestore, self).__init__()\n-        self.x_layer = keras.layers.Dense(3)\n-        self.y_layer = keras.layers.Dense(3)\n-        self.b_layer = keras.layers.Dense(1)\n-\n-      def call(self, a):\n-        return self.b_layer(self.y_layer(self.x_layer(a)))\n-\n-    self._new_layer_weight_loading_test_template(\n-        SubclassedModel, SubclassedModelRestore)\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def test_incompatible_checkpoint(self):\n-    save_path = tf.train.Checkpoint().save(\n-        os.path.join(self.get_temp_dir(), 'ckpt'))\n-    m = DummySubclassModel()\n-    with self.assertRaisesRegex(AssertionError, 'Nothing to load'):\n-      m.load_weights(save_path)\n-    m.dense = keras.layers.Dense(2)\n-    m.dense(tf.constant([[1.]]))\n-    with self.assertRaisesRegex(AssertionError,\n-                                'Nothing except the root object matched'):\n-      m.load_weights(save_path)\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def test_directory_passed(self):\n-    with self.cached_session():\n-      m = DummySubclassModel()\n-      v = m.add_weight(name='v', shape=[])\n-      self.evaluate(v.assign(42.))\n-      prefix = os.path.join(self.get_temp_dir(), str(uuid.uuid4()), 'ckpt/')\n-      m.save_weights(prefix)\n-      self.evaluate(v.assign(2.))\n-      m.load_weights(prefix)\n-      self.assertEqual(42., self.evaluate(v))\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def test_relative_path(self):\n-    with self.cached_session():\n-      m = DummySubclassModel()\n-      v = m.add_weight(name='v', shape=[])\n-      os.chdir(self.get_temp_dir())\n-\n-      prefix = 'ackpt'\n-      self.evaluate(v.assign(42.))\n-      m.save_weights(prefix)\n-      self.assertTrue(tf.io.gfile.exists('ackpt.index'))\n-      self.evaluate(v.assign(1.))\n-      m.load_weights(prefix)\n-      self.assertEqual(42., self.evaluate(v))\n-\n-      prefix = 'subdir/ackpt'\n-      self.evaluate(v.assign(43.))\n-      m.save_weights(prefix)\n-      self.assertTrue(tf.io.gfile.exists('subdir/ackpt.index'))\n-      self.evaluate(v.assign(2.))\n-      m.load_weights(prefix)\n-      self.assertEqual(43., self.evaluate(v))\n-\n-      prefix = 'ackpt/'\n-      self.evaluate(v.assign(44.))\n-      m.save_weights(prefix)\n-      self.assertTrue(tf.io.gfile.exists('ackpt/.index'))\n-      self.evaluate(v.assign(3.))\n-      m.load_weights(prefix)\n-      self.assertEqual(44., self.evaluate(v))\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def test_nonexistent_prefix_directory(self):\n-    with self.cached_session():\n-      m = DummySubclassModel()\n-      v = m.add_weight(name='v', shape=[])\n-      self.evaluate(v.assign(42.))\n-      prefix = os.path.join(self.get_temp_dir(), str(uuid.uuid4()), 'bckpt')\n-      m.save_weights(prefix)\n-      self.evaluate(v.assign(2.))\n-      m.load_weights(prefix)\n-      self.assertEqual(42., self.evaluate(v))\n-\n-\n-class DummySubclassModel(training.Model):\n-  pass\n-\n-\n-if __name__ == '__main__':\n-  tf.test.main()\n\n@@ -21,14 +21,19 @@ from __future__ import print_function\n import tensorflow as tf\n \n import os\n+import shutil\n import sys\n+import tempfile\n \n from absl.testing import parameterized\n import numpy as np\n \n import keras\n from keras import combinations\n+from keras import keras_parameterized\n from keras import losses\n+from keras import optimizer_v1\n+from keras import optimizers\n from keras import testing_utils\n from keras.engine import sequential\n from keras.feature_column import dense_features\n@@ -39,6 +44,7 @@ from keras.saving import save\n from keras.utils import generic_utils\n from tensorflow.python.saved_model import loader_impl\n \n+\n if sys.version_info >= (3, 6):\n   import pathlib  # pylint:disable=g-import-not-at-top\n try:\n@@ -306,5 +312,652 @@ class TestSaveModel(tf.test.TestCase, parameterized.TestCase):\n       _ = save.load_model(filepath, compile=True)\n \n \n+@keras_parameterized.run_with_all_saved_model_formats\n+class TestWholeModelSaving(keras_parameterized.TestCase):\n+\n+  def _save_model_dir(self, dirname='saved_model'):\n+    temp_dir = self.get_temp_dir()\n+    self.addCleanup(shutil.rmtree, temp_dir, ignore_errors=True)\n+    return os.path.join(temp_dir, dirname)\n+\n+  def _assert_same_weights_and_metrics(self, model, loaded_model):\n+    \"\"\"Checks that the loaded weights and metrics are the same as the original.\n+\n+    Args:\n+      model: original model\n+      loaded_model: loaded model\n+    \"\"\"\n+    self.assertAllClose(model.weights, loaded_model.weights)\n+\n+    if loaded_model.optimizer:\n+      if testing_utils.get_save_format() == 'tf':\n+        # TODO(b/153110928): Keras TF format doesn't restore optimizer weights\n+        # currently.\n+        return\n+      self.assertAllClose(model.optimizer.weights,\n+                          loaded_model.optimizer.weights)\n+\n+    # In V1/Graph mode, the model isn't built, so the metrics are not loaded\n+    # immediately (requires model to be called on some data before building\n+    # metrics).\n+    check_metrics = tf.__internal__.tf2.enabled() and tf.executing_eagerly()\n+\n+    if check_metrics:\n+      self.assertAllEqual([m.name for m in model.metrics],\n+                          [m.name for m in loaded_model.metrics])\n+\n+  @keras_parameterized.run_with_all_model_types\n+  @keras_parameterized.run_all_keras_modes\n+  def test_save_and_load(self):\n+    saved_model_dir = self._save_model_dir()\n+    save_format = testing_utils.get_save_format()\n+    save_kwargs = testing_utils.get_save_kwargs()\n+\n+    if ((save_format == 'h5' or not save_kwargs.get('save_traces', True)) and\n+        testing_utils.get_model_type() == 'subclass'):\n+      # HDF5 format currently does not allow saving subclassed models.\n+      # When saving with `save_traces=False`, the subclassed model must have a\n+      # get_config/from_config, which the autogenerated model does not have.\n+      return\n+\n+    with self.cached_session():\n+      model = testing_utils.get_model_from_layers(\n+          [keras.layers.Dense(2),\n+           keras.layers.RepeatVector(3),\n+           keras.layers.TimeDistributed(keras.layers.Dense(3))],\n+          input_shape=(3,))\n+      model.compile(\n+          loss=keras.losses.MSE,\n+          optimizer=keras.optimizer_v2.rmsprop.RMSprop(lr=0.0001),\n+          metrics=[\n+              keras.metrics.categorical_accuracy,\n+              keras.metrics.CategoricalCrossentropy(\n+                  name='cce', label_smoothing=tf.constant(0.2)),\n+          ],\n+          weighted_metrics=[\n+              keras.metrics.categorical_crossentropy,\n+              keras.metrics.CategoricalCrossentropy(\n+                  name='cce', label_smoothing=tf.constant(0.2)),\n+          ],\n+          sample_weight_mode='temporal')\n+\n+      x = np.random.random((1, 3))\n+      y = np.random.random((1, 3, 3))\n+      model.train_on_batch(x, y)\n+\n+      out = model.predict(x)\n+      keras.models.save_model(\n+          model, saved_model_dir, save_format=save_format,\n+          **save_kwargs)\n+\n+      loaded_model = keras.models.load_model(saved_model_dir)\n+      self._assert_same_weights_and_metrics(model, loaded_model)\n+\n+      out2 = loaded_model.predict(x)\n+      self.assertAllClose(out, out2, atol=1e-05)\n+\n+      eval_out = model.evaluate(x, y)\n+      eval_out2 = loaded_model.evaluate(x, y)\n+      self.assertArrayNear(eval_out, eval_out2, 0.001)\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_sequential_model_saving_without_input_shape(self):\n+    saved_model_dir = self._save_model_dir()\n+    save_format = testing_utils.get_save_format()\n+    with self.cached_session():\n+      model = keras.models.Sequential()\n+      model.add(keras.layers.Dense(2))\n+      model.add(keras.layers.RepeatVector(3))\n+      model.add(keras.layers.TimeDistributed(keras.layers.Dense(3)))\n+      model.compile(\n+          loss=keras.losses.MSE,\n+          optimizer='rmsprop',\n+          metrics=[\n+              keras.metrics.categorical_accuracy,\n+              keras.metrics.CategoricalAccuracy(name='cat_acc')\n+          ],\n+          weighted_metrics=[\n+              keras.metrics.categorical_accuracy,\n+              keras.metrics.CategoricalAccuracy(name='cat_acc2')\n+          ],\n+          sample_weight_mode='temporal')\n+      x = np.random.random((1, 3))\n+      y = np.random.random((1, 3, 3))\n+      model.train_on_batch(x, y)\n+\n+      out = model.predict(x)\n+      model.save(saved_model_dir, save_format=save_format)\n+\n+      new_model = keras.models.load_model(saved_model_dir)\n+\n+      self._assert_same_weights_and_metrics(model, new_model)\n+\n+      out2 = new_model.predict(x)\n+      self.assertAllClose(out, out2, atol=1e-05)\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_sequential_model_saving_without_compile(self):\n+    saved_model_dir = self._save_model_dir()\n+    save_format = testing_utils.get_save_format()\n+    with self.cached_session():\n+      model = keras.models.Sequential()\n+      model.add(keras.layers.Dense(2, input_shape=(3,)))\n+      model.add(keras.layers.RepeatVector(3))\n+      model.add(keras.layers.TimeDistributed(keras.layers.Dense(3)))\n+\n+      x = np.random.random((1, 3))\n+      out = model.predict(x)\n+\n+      # Save the model without any compilation or training.\n+      keras.models.save_model(model, saved_model_dir, save_format=save_format)\n+\n+      new_model = keras.models.load_model(saved_model_dir)\n+      self._assert_same_weights_and_metrics(model, new_model)\n+\n+      out2 = new_model.predict(x)\n+      self.assertAllClose(out, out2, atol=1e-05)\n+\n+  def test_sequential_model_saving_2(self):\n+    saved_model_dir = self._save_model_dir()\n+    save_format = testing_utils.get_save_format()\n+\n+    with tf.Graph().as_default(), self.cached_session():\n+      # test with custom optimizer, loss\n+\n+      class CustomOp(optimizer_v1.RMSprop):\n+        pass\n+\n+      def custom_loss(y_true, y_pred):\n+        return keras.losses.mse(y_true, y_pred)\n+\n+      model = keras.models.Sequential()\n+      model.add(keras.layers.Dense(2, input_shape=(3,)))\n+      model.add(keras.layers.Dense(3))\n+      model.compile(loss=custom_loss, optimizer=CustomOp(), metrics=['acc'])\n+\n+      x = np.random.random((1, 3))\n+      y = np.random.random((1, 3))\n+      model.train_on_batch(x, y)\n+\n+      out = model.predict(x)\n+      keras.models.save_model(model, saved_model_dir, save_format=save_format)\n+\n+      new_model = keras.models.load_model(\n+          saved_model_dir,\n+          custom_objects={'CustomOp': CustomOp,\n+                          'custom_loss': custom_loss})\n+      self._assert_same_weights_and_metrics(model, new_model)\n+\n+      out2 = new_model.predict(x)\n+      self.assertAllClose(out, out2, atol=1e-05)\n+\n+  def test_saving_without_compilation(self):\n+    saved_model_dir = self._save_model_dir()\n+    save_format = testing_utils.get_save_format()\n+    model = keras.models.Sequential()\n+    model.add(keras.layers.Dense(2, input_shape=(3,)))\n+    model.add(keras.layers.Dense(3))\n+    model.compile(loss='mse', optimizer='sgd', metrics=['acc'])\n+\n+    keras.models.save_model(model, saved_model_dir, save_format=save_format)\n+    model = keras.models.load_model(saved_model_dir)\n+\n+  def test_saving_with_tf_optimizer(self):\n+    saved_model_dir = self._save_model_dir()\n+    save_format = testing_utils.get_save_format()\n+\n+    model = keras.models.Sequential()\n+    model.add(keras.layers.Dense(2, input_shape=(3,)))\n+    model.add(keras.layers.Dense(3))\n+    model.compile(loss='mse',\n+                  optimizer=tf.compat.v1.train.AdadeltaOptimizer(0.1),\n+                  metrics=['acc'])\n+\n+    keras.models.save_model(model, saved_model_dir, save_format=save_format)\n+    model = keras.models.load_model(saved_model_dir)\n+\n+  def test_saving_right_after_compilation(self):\n+    saved_model_dir = self._save_model_dir()\n+    save_format = testing_utils.get_save_format()\n+    with self.cached_session():\n+      model = keras.models.Sequential()\n+      model.add(keras.layers.Dense(2, input_shape=(3,)))\n+      model.add(keras.layers.Dense(3))\n+      model.compile(loss='mse', optimizer='sgd', metrics=['acc'])\n+      if not tf.compat.v1.executing_eagerly_outside_functions():\n+        model._make_train_function()\n+      keras.models.save_model(model, saved_model_dir, save_format=save_format)\n+      model = keras.models.load_model(saved_model_dir)\n+\n+  def test_saving_lambda_numpy_array_arguments(self):\n+    saved_model_dir = self._save_model_dir()\n+    save_format = testing_utils.get_save_format()\n+\n+    if h5py is None:\n+      self.skipTest('h5py required to run this test')\n+\n+    mean = np.random.random((4, 2, 3))\n+    std = np.abs(np.random.random((4, 2, 3))) + 1e-5\n+    inputs = keras.layers.Input(shape=(4, 2, 3))\n+    output = keras.layers.Lambda(lambda image, mu, std: (image - mu) / std,\n+                                 arguments={'mu': mean, 'std': std})(inputs)\n+    model = keras.models.Model(inputs, output)\n+    model.compile(loss='mse', optimizer='sgd', metrics=['acc'])\n+\n+    keras.models.save_model(model, saved_model_dir, save_format=save_format)\n+\n+    model = keras.models.load_model(saved_model_dir)\n+\n+    self.assertAllClose(mean, model.layers[1].arguments['mu'])\n+    self.assertAllClose(std, model.layers[1].arguments['std'])\n+\n+  def test_saving_model_with_long_layer_names(self):\n+    saved_model_dir = self._save_model_dir()\n+    save_format = testing_utils.get_save_format()\n+    with self.cached_session():\n+      # This layer name will make the `layers_name` HDF5 attribute blow\n+      # out of proportion. Note that it fits into the internal HDF5\n+      # attribute memory limit on its own but because h5py converts\n+      # the list of layer names into numpy array, which uses the same\n+      # amount of memory for every item, it increases the memory\n+      # requirements substantially.\n+      x = keras.Input(shape=(2,), name='input_' + ('x' * (2**15)))\n+      f = x\n+      for i in range(4):\n+        f = keras.layers.Dense(2, name='dense_%d' % (i,))(f)\n+      model = keras.Model(inputs=[x], outputs=[f])\n+      model.compile(\n+          'adam', loss=keras.losses.MeanSquaredError(), metrics=['acc'])\n+\n+      x = np.random.random((1, 2))\n+      y = np.random.random((1, 2))\n+      model.train_on_batch(x, y)\n+      out = model.predict(x)\n+\n+      keras.models.save_model(model, saved_model_dir, save_format=save_format)\n+      model = keras.models.load_model(saved_model_dir)\n+\n+      if save_format in ['tf', 'tensorflow']:\n+        return\n+      # Check that the HDF5 files contains chunked array\n+      # of layer names.\n+      with h5py.File(saved_model_dir, 'r') as h5file:\n+        num_names_arrays = len([attr for attr in h5file['model_weights'].attrs\n+                                if attr.startswith('layer_names')])\n+      # The chunking of layer names array should have happened.\n+      self.assertGreater(num_names_arrays, 0)\n+      out2 = model.predict(x)\n+      self.assertAllClose(out, out2, atol=1e-05)\n+\n+  def test_saving_model_with_long_weights_names(self):\n+    saved_model_dir = self._save_model_dir()\n+    save_format = testing_utils.get_save_format()\n+\n+    with self.cached_session():\n+      x = keras.Input(shape=(2,), name='nested_model_input')\n+      f = x\n+      for i in range(4):\n+        f = keras.layers.Dense(2, name='nested_model_dense_%d' % (i,))(f)\n+      # This layer name will make the `weights_name`\n+      # HDF5 attribute blow out of proportion.\n+      f = keras.layers.Dense(2, name='nested_model_output' + ('x' * (2**14)))(f)\n+      nested_model = keras.Model(inputs=[x], outputs=[f], name='nested_model')\n+\n+      x = keras.Input(shape=(2,), name='outer_model_input')\n+      f = nested_model(x)\n+      f = keras.layers.Dense(2, name='outer_model_output')(f)\n+\n+      model = keras.Model(inputs=[x], outputs=[f])\n+      model.compile(loss='mse', optimizer='adam', metrics=['acc'])\n+\n+      x = np.random.random((1, 2))\n+      y = np.random.random((1, 2))\n+      model.train_on_batch(x, y)\n+      out = model.predict(x)\n+\n+      keras.models.save_model(model, saved_model_dir, save_format=save_format)\n+      model = keras.models.load_model(saved_model_dir)\n+\n+      if save_format in ['h5', 'hdf5', 'keras']:\n+        # Check that the HDF5 files contains chunked array\n+        # of weight names.\n+        with h5py.File(saved_model_dir, 'r') as h5file:\n+          num_weight_arrays = len(\n+              [attr for attr in h5file['model_weights']['nested_model'].attrs\n+               if attr.startswith('weight_names')])\n+        # The chunking of layer names array should have happened.\n+        self.assertGreater(num_weight_arrays, 0)\n+      out2 = model.predict(x)\n+      self.assertAllClose(out, out2, atol=1e-05)\n+\n+  def test_model_saving_to_pre_created_h5py_file(self):\n+    saved_model_dir = self._save_model_dir()\n+    save_format = testing_utils.get_save_format()\n+    with tf.Graph().as_default(), self.cached_session():\n+      inputs = keras.Input(shape=(3,))\n+      x = keras.layers.Dense(2)(inputs)\n+      outputs = keras.layers.Dense(3)(x)\n+\n+      model = keras.Model(inputs, outputs)\n+      model.compile(\n+          loss=keras.losses.MSE,\n+          optimizer=optimizer_v1.Adam(),\n+          metrics=[\n+              keras.metrics.categorical_accuracy,\n+              keras.metrics.CategoricalAccuracy()\n+          ])\n+      x = np.random.random((1, 3))\n+      y = np.random.random((1, 3))\n+      model.train_on_batch(x, y)\n+\n+      out = model.predict(x)\n+\n+      keras.models.save_model(model, saved_model_dir, save_format=save_format)\n+      loaded_model = keras.models.load_model(saved_model_dir)\n+      out1 = loaded_model.predict(x)\n+      self.assertAllClose(out, out1, atol=1e-05)\n+      if save_format in ['tf', 'tensorflow']:\n+        return\n+\n+      # Test h5 format specifically\n+      fd, fname = tempfile.mkstemp('.h5')\n+      with h5py.File(fname, mode='r+') as h5file:\n+        keras.models.save_model(model, h5file)\n+        loaded_model = keras.models.load_model(h5file)\n+        out2 = loaded_model.predict(x)\n+      self.assertAllClose(out, out2, atol=1e-05)\n+\n+      # Test non-default options in h5\n+      with h5py.File(\n+          '_', driver='core', mode='w', backing_store=False) as h5file:\n+        keras.models.save_model(model, h5file)\n+        loaded_model = keras.models.load_model(h5file)\n+        out2 = loaded_model.predict(x)\n+      self.assertAllClose(out, out2, atol=1e-05)\n+\n+      # Cleanup\n+      os.close(fd)\n+      os.remove(fname)\n+\n+  def test_model_saving_to_new_dir_path(self):\n+    saved_model_dir = os.path.join(self._save_model_dir(), 'newdir',\n+                                   'saved_model')\n+    save_format = testing_utils.get_save_format()\n+\n+    with self.cached_session():\n+      model = keras.models.Sequential()\n+      model.add(keras.layers.Dense(2, input_shape=(3,)))\n+      model.add(keras.layers.RepeatVector(3))\n+      model.add(keras.layers.TimeDistributed(keras.layers.Dense(3)))\n+\n+      x = np.random.random((1, 3))\n+      out = model.predict(x)\n+\n+      keras.models.save_model(model, saved_model_dir, save_format=save_format)\n+\n+      new_model = keras.models.load_model(saved_model_dir)\n+      self._assert_same_weights_and_metrics(model, new_model)\n+\n+      out2 = new_model.predict(x)\n+      self.assertAllClose(out, out2, atol=1e-05)\n+\n+  def test_model_raise_exception_with_failed_saving(self):\n+    if h5py is None:\n+      self.skipTest('h5py required to run this test')\n+\n+    saved_model_dir = self._save_model_dir()\n+    saved_model_path = os.path.join(saved_model_dir, 'saved_model.h5')\n+\n+    with self.cached_session():\n+      model = keras.models.Sequential()\n+      model.add(keras.layers.Dense(2, input_shape=(3,)))\n+      model.add(keras.layers.RepeatVector(3))\n+      model.add(keras.layers.TimeDistributed(keras.layers.Dense(3)))\n+\n+      with self.assertRaisesRegex(OSError, 'Unable to create file'):\n+        with h5py.File(saved_model_path, 'w'):\n+          keras.models.save_model(model, saved_model_path)\n+\n+  def test_saving_constant_initializer_with_numpy(self):\n+    saved_model_dir = self._save_model_dir()\n+    save_format = testing_utils.get_save_format()\n+\n+    model = keras.models.Sequential()\n+    model.add(\n+        keras.layers.Dense(\n+            2,\n+            input_shape=(3,),\n+            kernel_initializer=keras.initializers.Constant(np.ones((3, 2)))))\n+    model.add(keras.layers.Dense(3))\n+    model.compile(loss='mse', optimizer='sgd', metrics=['acc'])\n+    keras.models.save_model(model, saved_model_dir, save_format=save_format)\n+    model = keras.models.load_model(saved_model_dir)\n+\n+  def test_saving_group_naming_h5py(self):\n+    # Test saving model with layer which name is prefix to a previous layer\n+    # name.\n+\n+    temp_dir = self.get_temp_dir()\n+    self.addCleanup(shutil.rmtree, temp_dir)\n+    h5_path = os.path.join(temp_dir, 'test.h5')\n+\n+    input_layer = keras.layers.Input((None, None, 3), name='test_input')\n+    x = keras.layers.Conv2D(1, 1, name='conv1/conv')(input_layer)\n+    x = keras.layers.Activation('relu', name='conv1')(x)\n+    model = keras.models.Model(inputs=input_layer, outputs=x)\n+\n+    model.save_weights(h5_path)\n+    model.load_weights(h5_path)\n+\n+  def test_primitive_attrs_contain_no_extraneous_strings(self):\n+    if h5py is None:\n+      self.skipTest('h5py required to run this test')\n+\n+    saved_model_dir = self._save_model_dir()\n+    save_format = testing_utils.get_save_format()\n+    model = keras.models.Sequential()\n+    model.add(keras.layers.Dense(1, input_shape=[2]))\n+    model.save(saved_model_dir, save_format=save_format)\n+    if save_format in ['tf', 'tensorflow']:\n+      return\n+\n+    h5file = h5py.File(saved_model_dir, 'r')\n+    self.assertRegex(h5file.attrs['keras_version'], r'^[\\d]+\\.[\\d]+\\.[\\S]+$')\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_functional_model_with_custom_loss_and_metric(self):\n+    def _make_model():\n+      inputs = keras.Input(shape=(4,))\n+      x = keras.layers.Dense(8, activation='relu')(inputs)\n+      outputs = keras.layers.Dense(3, activation='softmax')(x)\n+      model = keras.Model(inputs=inputs, outputs=outputs)\n+      custom_loss = keras.layers.Lambda(lambda x: keras.backend.sum(x * x))(x)\n+      model.add_loss(custom_loss)\n+      model.add_metric(custom_loss, aggregation='mean', name='custom_loss')\n+      return model\n+\n+    saved_model_dir = self._save_model_dir()\n+    save_format = testing_utils.get_save_format()\n+\n+    with self.cached_session():\n+      model = _make_model()\n+      model.compile(\n+          loss=keras.losses.SparseCategoricalCrossentropy(),\n+          optimizer=optimizers.gradient_descent_v2.SGD(),\n+          metrics=[keras.metrics.SparseCategoricalCrossentropy()])\n+      x = np.random.normal(size=(32, 4))\n+      y = np.random.randint(0, 3, size=32)\n+      model.train_on_batch(x, y)\n+      evaluation_results = model.evaluate(x, y)\n+      # Save and reload model.\n+      model.save(saved_model_dir, save_format=save_format)\n+      del model  # Prevent misuse.\n+      loaded_model = keras.models.load_model(saved_model_dir)\n+      loaded_model_eval_results = loaded_model.evaluate(x, y)\n+      # Assert all evaluation results are the same.\n+      self.assertAllClose(evaluation_results, loaded_model_eval_results, 1e-9)\n+      # Check correctness of the loss calculation.\n+      self.assertAllGreater(evaluation_results, 0.)\n+      evaluation_results = dict(\n+          zip(loaded_model.metrics_names, evaluation_results))\n+      self.assertNear(\n+          evaluation_results['sparse_categorical_crossentropy'] +\n+          evaluation_results['custom_loss'], evaluation_results['loss'], 1e-6)\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_save_uncompiled_model_with_optimizer(self):\n+    with self.cached_session() as session:\n+      saved_model_dir = self._save_model_dir()\n+      save_format = testing_utils.get_save_format()\n+      model = keras.models.Sequential([keras.layers.Dense(1, input_shape=(3,))])\n+      # Set the model's optimizer but don't compile. This can happen if the\n+      # model is trained with a custom training loop.\n+      model.optimizer = keras.optimizer_v2.rmsprop.RMSprop(lr=0.0001)\n+      if not tf.executing_eagerly():\n+        session.run([v.initializer for v in model.variables])\n+      model.save(saved_model_dir, save_format=save_format)\n+\n+      if save_format in ['tf', 'tensorflow']:\n+        loaded = keras.models.load_model(saved_model_dir)\n+        self.assertIsInstance(loaded.optimizer,\n+                              keras.optimizer_v2.optimizer_v2.OptimizerV2)\n+\n+  @combinations.generate(combinations.combine(mode=['eager']))\n+  def test_functional_model_with_getitem_op_layer(self):\n+    inp = keras.Input(shape=(8))\n+\n+    out = inp[:]\n+    model = keras.Model(\n+        inputs=[inp],\n+        outputs=out)\n+    batch_size = 7\n+    x = tf.stack([\n+        tf.range(8) for _ in range(batch_size)])\n+    args = [x]\n+    expected = x[:]\n+\n+    self.assertAllEqual(model(args), expected)\n+    self.assertAllEqual(model.predict(args, batch_size=batch_size), expected)\n+\n+    # Make sure it can be successfully saved and loaded\n+    save_format = testing_utils.get_save_format()\n+    saved_model_dir = self._save_model_dir()\n+    keras.models.save_model(model, saved_model_dir, save_format=save_format)\n+\n+    loaded_model = keras.models.load_model(saved_model_dir)\n+\n+    self.assertAllEqual(loaded_model(args), expected)\n+    self.assertAllEqual(loaded_model.predict(args, batch_size=batch_size),\n+                        expected)\n+\n+\n+# Factory functions to create models that will be serialized inside a Network.\n+def _make_graph_network(input_size, output_size):\n+  inputs = keras.Input(input_size)\n+  x = keras.layers.Dense(8, activation='relu')(inputs)\n+  y = keras.layers.Dense(output_size)(x)\n+  return keras.Model(inputs=inputs, outputs=y)\n+\n+\n+def _make_sequential(input_size, output_size):\n+  del input_size\n+  return keras.Sequential([\n+      keras.layers.Dense(8, activation='relu'),\n+      keras.layers.Dense(output_size),\n+  ])\n+\n+\n+def _make_sequential_built(input_size, output_size):\n+  model = _make_sequential(input_size, output_size)\n+  model.build((None, input_size))\n+  return model\n+\n+\n+def _make_sequential_graph_network(input_size, output_size):\n+  return keras.Sequential([\n+      keras.layers.InputLayer(input_size),\n+      keras.layers.Dense(8, activation='relu'),\n+      keras.layers.Dense(output_size),\n+  ])\n+\n+\n+def _make_sequential_input_shape(input_size, output_size):\n+  return keras.Sequential([\n+      keras.layers.Dense(8, activation='relu', input_shape=(input_size,)),\n+      keras.layers.Dense(output_size),\n+  ])\n+\n+\n+class _make_subclassed(keras.Model):  # pylint: disable=invalid-name\n+\n+  def __init__(self, input_size, output_size):\n+    super(_make_subclassed, self).__init__()\n+    self._config = {'input_size': input_size, 'output_size': output_size}\n+    self._hidden_layer = keras.layers.Dense(8, activation='relu', name='hidden')\n+    self._logits_layer = keras.layers.Dense(output_size, name='logits')\n+\n+  def call(self, inputs):\n+    x = self._hidden_layer(inputs)\n+    return self._logits_layer(x)\n+\n+  def get_config(self):\n+    return self._config\n+\n+  @classmethod\n+  def from_config(cls, config):\n+    return cls(**config)\n+\n+\n+class _make_subclassed_built(_make_subclassed):  # pylint: disable=invalid-name\n+\n+  def __init__(self, input_size, output_size):\n+    super(_make_subclassed_built, self).__init__(input_size, output_size)\n+    self.build((None, input_size))\n+\n+\n+@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+class TestWholeModelSavingWithNesting(tf.test.TestCase, parameterized.TestCase):\n+  \"\"\"Tests saving a whole model that contains other models.\"\"\"\n+\n+  @parameterized.named_parameters([\n+      ('graph_network', _make_graph_network),\n+      ('sequential', _make_sequential),\n+      ('sequential_built', _make_sequential_built),\n+      ('sequential_graph_network', _make_sequential_graph_network),\n+      ('sequential_input_shape', _make_sequential_input_shape),\n+      ('subclassed', _make_subclassed),\n+      ('subclassed_built', _make_subclassed_built),\n+  ])\n+  def test_functional(self, model_fn):\n+    \"\"\"Tests serializing a model that uses a nested model to share weights.\"\"\"\n+    if h5py is None:\n+      self.skipTest('h5py required to run this test')\n+\n+    def _make_model():\n+      inputs = (keras.Input(shape=(4,), name='examples'),\n+                keras.Input(shape=(4,), name='neighbors'))\n+      base_model = model_fn(inputs[0].shape.as_list()[-1], 2)\n+      outputs = keras.layers.add([base_model(inputs[0]), base_model(inputs[1])])\n+      return keras.Model(inputs=inputs, outputs=outputs)\n+\n+    with self.cached_session():\n+      x = (np.random.normal(size=(16, 4)).astype(np.float32),\n+           np.random.normal(size=(16, 4)).astype(np.float32))\n+      model = _make_model()\n+      predictions = model(x)\n+      # Save and reload.\n+      model_path = os.path.join(self.get_temp_dir(), 'model.h5')\n+      model.save(model_path)\n+      del model\n+      loaded_model = keras.models.load_model(\n+          model_path,\n+          custom_objects={\n+              '_make_subclassed': _make_subclassed,\n+              '_make_subclassed_built': _make_subclassed_built,\n+          },\n+          compile=False)\n+      self.assertAllClose(loaded_model(x), predictions, 1e-9)\n+\n+\n if __name__ == '__main__':\n   tf.test.main()\n\n@@ -0,0 +1,668 @@\n+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#,============================================================================\n+\"\"\"Tests for model saving in the HDF5 format.\"\"\"\n+\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import tensorflow as tf\n+\n+import os\n+import shutil\n+import uuid\n+\n+from absl.testing import parameterized\n+import numpy as np\n+\n+import keras\n+from keras import combinations\n+from keras import keras_parameterized\n+from keras import optimizer_v1\n+from keras import testing_utils\n+from keras.engine import training\n+from keras.saving import hdf5_format\n+from tensorflow.python.platform import tf_logging as logging\n+\n+try:\n+  import h5py  # pylint:disable=g-import-not-at-top\n+except ImportError:\n+  h5py = None\n+\n+\n+@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+class TestWeightSavingAndLoading(tf.test.TestCase, parameterized.TestCase):\n+\n+  @keras_parameterized.run_with_all_weight_formats\n+  def test_weight_loading(self):\n+    temp_dir = self.get_temp_dir()\n+    self.addCleanup(shutil.rmtree, temp_dir)\n+    saved_model_dir = os.path.join(temp_dir, 'saved_model')\n+    save_format = testing_utils.get_save_format()\n+    with self.cached_session():\n+      a = keras.layers.Input(shape=(2,))\n+      x = keras.layers.Dense(3)(a)\n+      b = keras.layers.Dense(1)(x)\n+      model = keras.models.Model(a, b)\n+\n+      x = np.random.random((3, 2))\n+      ref_y = model.predict(x)\n+      weights = model.get_weights()\n+      model.set_weights(weights)\n+      y = model.predict(x)\n+      self.assertAllClose(ref_y, y)\n+\n+      with self.assertRaises(ValueError):\n+        model.set_weights(weights[1:])\n+      with self.assertRaises(ValueError):\n+        model.set_weights(weights[::-1])\n+\n+      model.save_weights(saved_model_dir, save_format=save_format)\n+      model.load_weights(saved_model_dir)\n+      y = model.predict(x)\n+      self.assertAllClose(ref_y, y)\n+\n+  def test_weight_preprocessing(self):\n+    input_dim = 3\n+    output_dim = 3\n+    size = 2\n+    cases = [\n+        [\n+            (keras.layers.Bidirectional(keras.layers.SimpleRNN(2))),\n+            [np.random.random((2, 1)), np.random.random((2, 1))],\n+            (None, 3, 2),\n+        ],\n+        [\n+            (keras.layers.TimeDistributed(keras.layers.Dense(1))),\n+            [np.random.random((2, 1)), np.random.random((1,))],\n+            (None, 3, 2),\n+        ],\n+        [\n+            (keras.layers.Conv1D(output_dim, size, use_bias=False)),\n+            [np.random.random((output_dim, input_dim, size, 1))],\n+            (None, 4, input_dim),\n+        ],\n+        [\n+            (keras.layers.Conv2D(output_dim, size,\n+                                 use_bias=False, data_format='channels_first')),\n+            [np.random.random((output_dim, input_dim, size, size))],\n+            (None, input_dim, 4, 4),\n+        ],\n+        [\n+            (keras.layers.Conv2DTranspose(output_dim, size,\n+                                          use_bias=False,\n+                                          data_format='channels_first')),\n+            [np.random.random((output_dim, input_dim, size, size))],\n+            (None, input_dim, 4, 4),\n+        ],\n+        [\n+            (keras.layers.Conv2DTranspose(output_dim, size,\n+                                          use_bias=False,\n+                                          data_format='channels_last')),\n+            [np.random.random((size, size, input_dim, output_dim))],\n+            (None, 4, 4, input_dim),\n+        ],\n+        [\n+            (keras.layers.Conv3D(output_dim, size,\n+                                 use_bias=False, data_format='channels_first')),\n+            [np.random.random((output_dim, input_dim, size, size, size))],\n+            (None, input_dim, 4, 4, 4),\n+        ],\n+        [\n+            (keras.layers.GRUV1(output_dim)),\n+            [np.random.random((input_dim, output_dim)),\n+             np.random.random((output_dim, output_dim)),\n+             np.random.random((output_dim,)),\n+             np.random.random((input_dim, output_dim)),\n+             np.random.random((output_dim, output_dim)),\n+             np.random.random((output_dim,)),\n+             np.random.random((input_dim, output_dim)),\n+             np.random.random((output_dim, output_dim)),\n+             np.random.random((output_dim,))],\n+            (None, 4, input_dim),\n+        ],\n+        [\n+            (keras.layers.LSTMV1(output_dim)),\n+            [np.random.random((input_dim, output_dim)),\n+             np.random.random((output_dim, output_dim)),\n+             np.random.random((output_dim,)),\n+             np.random.random((input_dim, output_dim)),\n+             np.random.random((output_dim, output_dim)),\n+             np.random.random((output_dim,)),\n+             np.random.random((input_dim, output_dim)),\n+             np.random.random((output_dim, output_dim)),\n+             np.random.random((output_dim,)),\n+             np.random.random((input_dim, output_dim)),\n+             np.random.random((output_dim, output_dim)),\n+             np.random.random((output_dim,))],\n+            (None, 4, input_dim),\n+        ],\n+    ]\n+    for layer, weights, input_shape in cases:\n+      layer.build(input_shape)\n+      _ = hdf5_format.preprocess_weights_for_loading(\n+          layer, weights, original_keras_version='1')\n+\n+    model = keras.models.Sequential([keras.layers.Dense(2, input_dim=2)])\n+    _ = hdf5_format.preprocess_weights_for_loading(\n+        model, model.weights, original_keras_version='1')\n+\n+    x = keras.Input((2,))\n+    y = keras.layers.Dense(2)(x)\n+    model = keras.models.Model(x, y)\n+    _ = hdf5_format.preprocess_weights_for_loading(\n+        model, model.weights, original_keras_version='1')\n+\n+  @parameterized.named_parameters(\n+      ('gru', keras.layers.GRU, {\n+          'units': 2,\n+          'input_shape': (3, 5)\n+      }),\n+      ('gru_with_reset_after', keras.layers.GRU, {\n+          'units': 2,\n+          'input_shape': (3, 5),\n+          'reset_after': True\n+      }),\n+      ('lstm', keras.layers.LSTM, {\n+          'units': 2,\n+          'input_shape': (3, 5)\n+      }),\n+      ('cudnngru', keras.layers.CuDNNGRU, {\n+          'units': 2,\n+          'input_shape': (3, 5)\n+      }),\n+      ('cudnnlstm', keras.layers.CuDNNLSTM, {\n+          'units': 2,\n+          'input_shape': (3, 5)\n+      }))\n+  def test_preprocess_weights_for_loading_rnn_should_be_idempotent(\n+      self, layer_class, layer_args):\n+    with self.cached_session():\n+      layer = layer_class(**layer_args)\n+      layer.build(input_shape=layer_args.get('input_shape'))\n+      weights1 = layer.get_weights()\n+      weights2 = hdf5_format.preprocess_weights_for_loading(\n+          layer, weights1)\n+      _ = [\n+          self.assertAllClose(x, y, rtol=1e-05)\n+          for (x, y) in zip(weights1, weights2)\n+      ]\n+\n+  def test_sequential_weight_loading(self):\n+    if h5py is None:\n+      return\n+\n+    temp_dir = self.get_temp_dir()\n+    self.addCleanup(shutil.rmtree, temp_dir)\n+    h5_path = os.path.join(temp_dir, 'test.h5')\n+\n+    num_hidden = 5\n+    input_dim = 3\n+    batch_size = 5\n+    num_classes = 2\n+\n+    with self.cached_session():\n+      model = keras.models.Sequential()\n+      model.add(keras.layers.Dense(num_hidden, input_dim=input_dim))\n+      model.add(keras.layers.Dense(num_classes))\n+\n+      x = np.random.random((batch_size, input_dim))\n+      ref_y = model.predict(x)\n+\n+      model.save_weights(h5_path)\n+\n+      model = keras.models.Sequential()\n+      model.add(keras.layers.Dense(num_hidden, input_dim=input_dim))\n+      model.add(keras.layers.Dense(num_classes))\n+      model.load_weights(h5_path)\n+      y = model.predict(x)\n+\n+      self.assertAllClose(y, ref_y)\n+\n+  @keras_parameterized.run_with_all_saved_model_formats(\n+      exclude_formats=['tf_no_traces'])\n+  def test_nested_model_weight_loading(self):\n+    save_format = testing_utils.get_save_format()\n+    temp_dir = self.get_temp_dir()\n+    self.addCleanup(shutil.rmtree, temp_dir)\n+    saved_model_dir = os.path.join(temp_dir, 'saved_model')\n+\n+    batch_size = 5\n+    shape = (None, None, 3)\n+\n+    with self.cached_session():\n+      def gen_model():\n+\n+        def seq_model():\n+          model = keras.models.Sequential([\n+              keras.layers.Conv2D(3, 1, input_shape=shape),\n+              keras.layers.BatchNormalization()])\n+          return model\n+\n+        x = inner_inputs = keras.layers.Input((None, None, 3))\n+        x = seq_model()(x)\n+        x = seq_model()(x)\n+        inner_model = keras.models.Model(inner_inputs, x)\n+\n+        inputs = keras.layers.Input(shape)\n+        return keras.models.Model(inputs, inner_model(inputs))\n+\n+      model = gen_model()\n+      x = np.random.random((batch_size, 1, 1, 3))\n+      ref_y = model.predict(x)\n+\n+      model.save_weights(saved_model_dir, save_format=save_format)\n+\n+      model = gen_model()\n+      model.load_weights(saved_model_dir)\n+      y = model.predict(x)\n+\n+      self.assertAllClose(y, ref_y)\n+\n+  def test_sequential_weight_loading_group_name_with_incorrect_length(self):\n+    if h5py is None:\n+      return\n+\n+    temp_dir = self.get_temp_dir()\n+    self.addCleanup(shutil.rmtree, temp_dir)\n+    h5_path = os.path.join(temp_dir, 'test.h5')\n+\n+    num_hidden = 5\n+    input_dim = 3\n+    num_classes = 2\n+    with self.cached_session():\n+      ref_model = keras.models.Sequential()\n+      ref_model.add(keras.layers.Dense(num_hidden, input_dim=input_dim,\n+                                       name='d1'))\n+      ref_model.add(keras.layers.Dense(num_classes, name='d2'))\n+      ref_model.compile(loss=keras.losses.MSE,\n+                        optimizer='rmsprop',\n+                        metrics=[keras.metrics.categorical_accuracy])\n+\n+      f_ref_model = h5py.File(h5_path, 'w')\n+      hdf5_format.save_weights_to_hdf5_group(f_ref_model, ref_model.layers)\n+\n+      f_model = h5py.File(h5_path, 'r')\n+      model = keras.models.Sequential()\n+      model.add(keras.layers.Dense(num_hidden, use_bias=False,\n+                                   input_dim=input_dim, name='d1'))\n+      model.add(keras.layers.Dense(num_classes, name='d2'))\n+      model.compile(loss=keras.losses.MSE,\n+                    optimizer='rmsprop',\n+                    metrics=[keras.metrics.categorical_accuracy])\n+      with self.assertRaisesRegex(\n+          ValueError, r'Layer #0 \\(named \\\"d1\\\"\\) expects 1 '\n+          r'weight\\(s\\), but the saved weights have 2 '\n+          r'element\\(s\\)\\.'):\n+        hdf5_format.load_weights_from_hdf5_group_by_name(f_model, model.layers)\n+\n+      hdf5_format.load_weights_from_hdf5_group_by_name(\n+          f_model, model.layers, skip_mismatch=True)\n+      self.assertAllClose(keras.backend.get_value(ref_model.layers[1].kernel),\n+                          keras.backend.get_value(model.layers[1].kernel))\n+\n+  def test_sequential_weight_loading_group_name_with_incorrect_shape(self):\n+    if h5py is None:\n+      return\n+\n+    temp_dir = self.get_temp_dir()\n+    self.addCleanup(shutil.rmtree, temp_dir)\n+    h5_path = os.path.join(temp_dir, 'test.h5')\n+\n+    num_hidden = 5\n+    input_dim = 3\n+    num_classes = 2\n+    with tf.Graph().as_default(), self.cached_session():\n+      ref_model = keras.models.Sequential()\n+      ref_model.add(keras.layers.Dense(num_hidden, input_dim=input_dim,\n+                                       name='d1'))\n+      ref_model.add(keras.layers.Dense(num_classes, name='d2'))\n+      ref_model.compile(loss=keras.losses.MSE,\n+                        optimizer=optimizer_v1.RMSprop(lr=0.0001),\n+                        metrics=[keras.metrics.categorical_accuracy])\n+\n+      f_ref_model = h5py.File(h5_path, 'w')\n+      keras.backend.set_value(ref_model.layers[1].bias, [3.5] * num_classes)\n+      hdf5_format.save_weights_to_hdf5_group(f_ref_model, ref_model.layers)\n+\n+      f_model = h5py.File(h5_path, 'r')\n+      model = keras.models.Sequential()\n+      model.add(keras.layers.Dense(num_hidden + 5, input_dim=input_dim,\n+                                   name='d1'))\n+      model.add(keras.layers.Dense(num_classes, name='d2'))\n+      model.compile(loss=keras.losses.MSE,\n+                    optimizer=optimizer_v1.RMSprop(lr=0.0001),\n+                    metrics=[keras.metrics.categorical_accuracy])\n+      with self.assertRaisesRegex(\n+          ValueError, r'Layer #0 \\(named \"d1\"\\), weight '\n+          r'<tf\\.Variable \\'d1_1\\/kernel:0\\' '\n+          r'shape=\\(3, 10\\) dtype=float32> has '\n+          r'shape \\(3, 10\\), but the saved weight has '\n+          r'shape \\(3, 5\\)\\.'):\n+        hdf5_format.load_weights_from_hdf5_group_by_name(f_model, model.layers)\n+\n+      hdf5_format.load_weights_from_hdf5_group_by_name(\n+          f_model, model.layers, skip_mismatch=True)\n+      self.assertAllClose([3.5] * num_classes,\n+                          keras.backend.get_value(model.layers[1].bias))\n+\n+\n+class SubclassedModel(training.Model):\n+\n+  def __init__(self):\n+    super(SubclassedModel, self).__init__()\n+    self.x_layer = keras.layers.Dense(3)\n+    self.b_layer = keras.layers.Dense(1)\n+\n+  def call(self, a):\n+    return self.b_layer(self.x_layer(a))\n+\n+\n+class TestWeightSavingAndLoadingTFFormat(tf.test.TestCase, parameterized.TestCase):\n+\n+  def test_keras_optimizer_warning(self):\n+    graph = tf.Graph()\n+    with graph.as_default(), self.session(graph):\n+      model = keras.models.Sequential()\n+      model.add(keras.layers.Dense(2, input_shape=(3,)))\n+      model.add(keras.layers.Dense(3))\n+      model.compile(loss='mse', optimizer=optimizer_v1.Adam(), metrics=['acc'])\n+      if not tf.compat.v1.executing_eagerly_outside_functions():\n+        model._make_train_function()\n+      temp_dir = self.get_temp_dir()\n+      prefix = os.path.join(temp_dir, 'ckpt')\n+      with tf.compat.v1.test.mock.patch.object(logging, 'warning') as mock_log:\n+        model.save_weights(prefix)\n+        self.assertRegex(str(mock_log.call_args), 'Keras optimizer')\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_tensorflow_format_overwrite(self):\n+    with self.cached_session() as session:\n+      model = SubclassedModel()\n+      temp_dir = self.get_temp_dir()\n+      prefix = os.path.join(temp_dir, 'ckpt')\n+\n+      x = tf.constant(np.random.random((3, 2)), dtype=tf.float32)\n+      executing_eagerly = tf.executing_eagerly()\n+      model(x)  # pylint: disable=not-callable\n+      if not executing_eagerly:\n+        session.run([v.initializer for v in model.variables])\n+      model.save_weights(prefix, save_format='tensorflow')\n+      model.save_weights(prefix, save_format='tensorflow', overwrite=True)\n+      with self.assertRaises(EOFError):\n+        # Indirectly tests that the user is prompted\n+        model.save_weights(prefix, save_format='tensorflow', overwrite=False)\n+\n+  def test_no_default_session(self):\n+    with tf.Graph().as_default():\n+      self.assertFalse(tf.compat.v1.get_default_session())\n+      data = np.random.random((1000, 32)).astype(np.float32)\n+      labels = np.random.random((1000, 10)).astype(np.float32)\n+\n+      model = keras.models.Sequential([\n+          keras.layers.Dense(10, activation='softmax'),\n+          keras.layers.Dense(10, activation='softmax')])\n+\n+      model.compile(optimizer=tf.compat.v1.train.RMSPropOptimizer(0.001),\n+                    loss='categorical_crossentropy',\n+                    metrics=['accuracy'])\n+\n+      model.fit(data, labels)\n+      fname = os.path.join(self.get_temp_dir(), 'weights', 'ckpt')\n+      model.save_weights(fname)\n+      model.load_weights(fname)\n+\n+  def test_no_graph_pollution(self):\n+    with tf.compat.v1.get_default_graph().as_default():\n+      graph = tf.Graph()\n+      with graph.as_default(), self.session(graph) as session:\n+        model = SubclassedModel()\n+        temp_dir = self.get_temp_dir()\n+        prefix = os.path.join(temp_dir, 'ckpt')\n+\n+        x = tf.constant(np.random.random((3, 2)), dtype=tf.float32)\n+        model(x)  # pylint: disable=not-callable\n+        session.run([v.initializer for v in model.variables])\n+        model.save_weights(prefix, save_format='tensorflow')\n+        op_count = len(graph.get_operations())\n+        model.save_weights(prefix, save_format='tensorflow')\n+        self.assertLen(graph.get_operations(), op_count)\n+\n+        model.load_weights(prefix)\n+        op_count = len(graph.get_operations())\n+        model.load_weights(prefix)\n+        self.assertLen(graph.get_operations(), op_count)\n+\n+  def _weight_loading_test_template(self, make_model_fn):\n+    with self.cached_session():\n+      model = make_model_fn()\n+      model.compile(\n+          loss='mse',\n+          optimizer=tf.compat.v1.train.RMSPropOptimizer(0.1),\n+          metrics=['acc', keras.metrics.CategoricalAccuracy()])\n+      temp_dir = self.get_temp_dir()\n+      prefix = os.path.join(temp_dir, 'ckpt')\n+      train_x = np.random.random((3, 2))\n+      train_y = np.random.random((3,))\n+      x = tf.constant(train_x, dtype=tf.float32)\n+\n+      model.train_on_batch(train_x, train_y)\n+      model.save_weights(prefix, save_format='tf')\n+      ref_y_before_train = model.predict(train_x)\n+      model.train_on_batch(train_x, train_y)\n+      ref_y_after_train = model.predict(train_x)\n+      for v in model.variables:\n+        self.evaluate(\n+            v.assign(tf.random.normal(shape=tf.compat.v1.shape(v))))\n+\n+      self.addCleanup(shutil.rmtree, temp_dir)\n+\n+      model.load_weights(prefix)\n+      self.assertAllClose(ref_y_before_train, self.evaluate(model(x)))\n+\n+      # Test restore-on-create if this is a subclassed Model (graph Networks\n+      # will have already created their variables).\n+      load_model = make_model_fn()\n+      load_model.load_weights(prefix)\n+      self.assertAllClose(\n+          ref_y_before_train,\n+          self.evaluate(load_model(x)))\n+      load_model = make_model_fn()\n+      load_model.load_weights(prefix)\n+      # We need to run some of the restore ops for predict(), but not all\n+      # variables have been created yet (optimizer slot variables). Tests\n+      # incremental restore.\n+      load_model.predict(train_x)\n+      load_model.compile(\n+          loss='mse',\n+          optimizer=tf.compat.v1.train.RMSPropOptimizer(0.1),\n+          metrics=['acc', keras.metrics.CategoricalAccuracy()])\n+      load_model.train_on_batch(train_x, train_y)\n+      self.assertAllClose(ref_y_after_train, self.evaluate(load_model(x)))\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_weight_loading_graph_model(self):\n+    def _make_graph_model():\n+      a = keras.layers.Input(shape=(2,))\n+      x = keras.layers.Dense(3)(a)\n+      b = keras.layers.Dense(1)(x)\n+      return keras.models.Model(a, b)\n+\n+    self._weight_loading_test_template(_make_graph_model)\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_weight_loading_subclassed_model(self):\n+    self._weight_loading_test_template(SubclassedModel)\n+\n+  def _new_layer_weight_loading_test_template(\n+      self, first_model_fn, second_model_fn):\n+    with self.cached_session() as session:\n+      model = first_model_fn()\n+      temp_dir = self.get_temp_dir()\n+      prefix = os.path.join(temp_dir, 'ckpt')\n+\n+      x = tf.constant(np.random.random((3, 2)), dtype=tf.float32)\n+      executing_eagerly = tf.executing_eagerly()\n+      ref_y_tensor = model(x)\n+      if not executing_eagerly:\n+        session.run([v.initializer for v in model.variables])\n+      ref_y = self.evaluate(ref_y_tensor)\n+      model.save_weights(prefix)\n+      self.assertEqual(\n+          prefix,\n+          tf.train.latest_checkpoint(temp_dir))\n+      for v in model.variables:\n+        self.evaluate(\n+            v.assign(tf.random.normal(shape=tf.compat.v1.shape(v))))\n+\n+      self.addCleanup(shutil.rmtree, temp_dir)\n+\n+      second_model = second_model_fn()\n+      status = second_model.load_weights(prefix)\n+      second_model(x)\n+      status.run_restore_ops()\n+      second_model.save_weights(prefix)\n+      # Check that the second model's checkpoint loads into the original model\n+      status = model.load_weights(prefix)\n+      status.run_restore_ops(session)\n+      y = self.evaluate(model(x))\n+      self.assertAllClose(ref_y, y)\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_weight_loading_graph_model_added_layer(self):\n+    def _save_graph_model():\n+      a = keras.layers.Input(shape=(2,))\n+      x = keras.layers.Dense(3, name='first')(a)\n+      b = keras.layers.Dense(1, name='second')(x)\n+      return keras.models.Model(a, b)\n+    def _restore_graph_model():\n+      a = keras.layers.Input(shape=(2,))\n+      x = keras.layers.Dense(3, name='first')(a)\n+      y = keras.layers.Dense(1, name='second')(x)\n+      b = keras.layers.Dense(3, name='secondjr')(y)\n+      return keras.models.Model(a, b)\n+\n+    self._new_layer_weight_loading_test_template(\n+        _save_graph_model, _restore_graph_model)\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_weight_loading_graph_model_added_no_weight_layer(self):\n+    def _save_graph_model():\n+      a = keras.layers.Input(shape=(2,))\n+      x = keras.layers.Dense(3, name='first')(a)\n+      b = keras.layers.Dense(1, name='second')(x)\n+      return keras.models.Model(a, b)\n+    def _restore_graph_model():\n+      a = keras.layers.Input(shape=(2,))\n+      x = keras.layers.Dense(3, name='first')(a)\n+      b = keras.layers.Dense(1, name='second')(x)\n+      y = keras.layers.Dropout(rate=0.1)(b)\n+      return keras.models.Model(a, y)\n+\n+    self._new_layer_weight_loading_test_template(\n+        _save_graph_model, _restore_graph_model)\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_weight_loading_subclassed_model_added_layer(self):\n+\n+    class SubclassedModelRestore(training.Model):\n+\n+      def __init__(self):\n+        super(SubclassedModelRestore, self).__init__()\n+        self.x_layer = keras.layers.Dense(3)\n+        self.y_layer = keras.layers.Dense(3)\n+        self.b_layer = keras.layers.Dense(1)\n+\n+      def call(self, a):\n+        return self.b_layer(self.y_layer(self.x_layer(a)))\n+\n+    self._new_layer_weight_loading_test_template(\n+        SubclassedModel, SubclassedModelRestore)\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_incompatible_checkpoint(self):\n+    save_path = tf.train.Checkpoint().save(\n+        os.path.join(self.get_temp_dir(), 'ckpt'))\n+    m = DummySubclassModel()\n+    with self.assertRaisesRegex(AssertionError, 'Nothing to load'):\n+      m.load_weights(save_path)\n+    m.dense = keras.layers.Dense(2)\n+    m.dense(tf.constant([[1.]]))\n+    with self.assertRaisesRegex(AssertionError,\n+                                'Nothing except the root object matched'):\n+      m.load_weights(save_path)\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_directory_passed(self):\n+    with self.cached_session():\n+      m = DummySubclassModel()\n+      v = m.add_weight(name='v', shape=[])\n+      self.evaluate(v.assign(42.))\n+      prefix = os.path.join(self.get_temp_dir(), str(uuid.uuid4()), 'ckpt/')\n+      m.save_weights(prefix)\n+      self.evaluate(v.assign(2.))\n+      m.load_weights(prefix)\n+      self.assertEqual(42., self.evaluate(v))\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_relative_path(self):\n+    with self.cached_session():\n+      m = DummySubclassModel()\n+      v = m.add_weight(name='v', shape=[])\n+      os.chdir(self.get_temp_dir())\n+\n+      prefix = 'ackpt'\n+      self.evaluate(v.assign(42.))\n+      m.save_weights(prefix)\n+      self.assertTrue(tf.io.gfile.exists('ackpt.index'))\n+      self.evaluate(v.assign(1.))\n+      m.load_weights(prefix)\n+      self.assertEqual(42., self.evaluate(v))\n+\n+      prefix = 'subdir/ackpt'\n+      self.evaluate(v.assign(43.))\n+      m.save_weights(prefix)\n+      self.assertTrue(tf.io.gfile.exists('subdir/ackpt.index'))\n+      self.evaluate(v.assign(2.))\n+      m.load_weights(prefix)\n+      self.assertEqual(43., self.evaluate(v))\n+\n+      prefix = 'ackpt/'\n+      self.evaluate(v.assign(44.))\n+      m.save_weights(prefix)\n+      self.assertTrue(tf.io.gfile.exists('ackpt/.index'))\n+      self.evaluate(v.assign(3.))\n+      m.load_weights(prefix)\n+      self.assertEqual(44., self.evaluate(v))\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_nonexistent_prefix_directory(self):\n+    with self.cached_session():\n+      m = DummySubclassModel()\n+      v = m.add_weight(name='v', shape=[])\n+      self.evaluate(v.assign(42.))\n+      prefix = os.path.join(self.get_temp_dir(), str(uuid.uuid4()), 'bckpt')\n+      m.save_weights(prefix)\n+      self.evaluate(v.assign(2.))\n+      m.load_weights(prefix)\n+      self.assertEqual(42., self.evaluate(v))\n+\n+\n+class DummySubclassModel(training.Model):\n+  pass\n+\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -30,3 +30,22 @@ KERAS_CACHE_KEY = 'keras_serialized_attributes'\n \n # Name of Keras metadata file stored in the SavedModel.\n SAVED_METADATA_PATH = 'keras_metadata.pb'\n+\n+# Names of SavedObject Keras identifiers.\n+INPUT_LAYER_IDENTIFIER = '_tf_keras_input_layer'\n+LAYER_IDENTIFIER = '_tf_keras_layer'\n+METRIC_IDENTIFIER = '_tf_keras_metric'\n+MODEL_IDENTIFIER = '_tf_keras_model'\n+NETWORK_IDENTIFIER = '_tf_keras_network'\n+RNN_LAYER_IDENTIFIER = '_tf_keras_rnn_layer'\n+SEQUENTIAL_IDENTIFIER = '_tf_keras_sequential'\n+\n+KERAS_OBJECT_IDENTIFIERS = (\n+    INPUT_LAYER_IDENTIFIER,\n+    LAYER_IDENTIFIER,\n+    METRIC_IDENTIFIER,\n+    MODEL_IDENTIFIER,\n+    NETWORK_IDENTIFIER,\n+    RNN_LAYER_IDENTIFIER,\n+    SEQUENTIAL_IDENTIFIER,\n+)\n\n@@ -31,12 +31,14 @@ import collections.abc as collections_abc\n import json\n import numpy as np\n import wrapt\n+from tensorflow.python.framework import type_spec\n \n \n class Encoder(json.JSONEncoder):\n   \"\"\"JSON encoder and decoder that handles TensorShapes and tuples.\"\"\"\n \n   def default(self, obj):\n+    \"\"\"Encodes objects for types that aren't handled by the default encoder.\"\"\"\n     if isinstance(obj, tf.TensorShape):\n       items = obj.as_list() if obj.rank is not None else None\n       return {'class_name': 'TensorShape', 'items': items}\n@@ -67,6 +69,9 @@ def _decode_helper(obj):\n   if isinstance(obj, dict) and 'class_name' in obj:\n     if obj['class_name'] == 'TensorShape':\n       return tf.TensorShape(obj['items'])\n+    elif obj['class_name'] == 'TypeSpec':\n+      return type_spec.lookup(obj['type_spec'])._deserialize(  # pylint: disable=protected-access\n+          _decode_helper(obj['serialized']))\n     elif obj['class_name'] == '__tuple__':\n       return tuple(_decode_helper(i) for i in obj['items'])\n     elif obj['class_name'] == '__ellipsis__':\n@@ -124,4 +129,15 @@ def get_json_type(obj):\n   if isinstance(obj, wrapt.ObjectProxy):\n     return obj.__wrapped__\n \n+  if isinstance(obj, tf.TypeSpec):\n+    try:\n+      type_spec_name = type_spec.get_name(type(obj))\n+      return {'class_name': 'TypeSpec', 'type_spec': type_spec_name,\n+              'serialized': obj._serialize()}  # pylint: disable=protected-access\n+    except ValueError:\n+      raise ValueError('Unable to serialize {} to JSON, because the TypeSpec '\n+                       'class {} has not been registered.'\n+                       .format(obj, type(obj)))\n+\n   raise TypeError('Not JSON Serializable:', obj)\n+\n\n@@ -49,6 +49,18 @@ class JsonUtilsTest(tf.test.TestCase):\n     self.assertAllEqual(loaded['key1'], (3, 5))\n     self.assertAllEqual(loaded['key2'], [(1, (3, 4)), (1,)])\n \n+  def test_encode_decode_type_spec(self):\n+    spec = tf.TensorSpec((1, 5), tf.float32)\n+    string = json_utils.Encoder().encode(spec)\n+    loaded = json_utils.decode(string)\n+    self.assertEqual(spec, loaded)\n+\n+    invalid_type_spec = {'class_name': 'TypeSpec', 'type_spec': 'Invalid Type',\n+                         'serialized': None}\n+    string = json_utils.Encoder().encode(invalid_type_spec)\n+    with self.assertRaisesRegexp(ValueError, 'No TypeSpec has been registered'):\n+      loaded = json_utils.decode(string)\n+\n \n if __name__ == '__main__':\n   tf.test.main()\n\n@@ -34,7 +34,7 @@ class LayerSavedModelSaver(base_serialization.SavedModelSaver):\n \n   @property\n   def object_identifier(self):\n-    return '_tf_keras_layer'\n+    return constants.LAYER_IDENTIFIER\n \n   @property\n   def python_properties(self):\n@@ -128,7 +128,7 @@ class InputLayerSavedModelSaver(base_serialization.SavedModelSaver):\n \n   @property\n   def object_identifier(self):\n-    return '_tf_keras_input_layer'\n+    return constants.INPUT_LAYER_IDENTIFIER\n \n   @property\n   def python_properties(self):\n@@ -154,7 +154,7 @@ class RNNSavedModelSaver(LayerSavedModelSaver):\n \n   @property\n   def object_identifier(self):\n-    return '_tf_keras_rnn_layer'\n+    return constants.RNN_LAYER_IDENTIFIER\n \n   def _get_serialized_attributes_internal(self, serialization_cache):\n     objects, functions = (\n\n@@ -85,12 +85,6 @@ PUBLIC_ATTRIBUTES = CommonEndpoints.all_functions.union(\n PUBLIC_ATTRIBUTES.add(constants.KERAS_ATTR)\n \n \n-KERAS_OBJECT_IDENTIFIERS = (\n-    '_tf_keras_layer', '_tf_keras_input_layer', '_tf_keras_network',\n-    '_tf_keras_model', '_tf_keras_sequential', '_tf_keras_metric',\n-    '_tf_keras_rnn_layer')\n-\n-\n def load(path, compile=True, options=None):  # pylint: disable=redefined-builtin\n   \"\"\"Loads Keras objects from a SavedModel.\n \n@@ -189,7 +183,7 @@ def _read_legacy_metadata(object_graph_def, metadata):\n   node_paths = _generate_object_paths(object_graph_def)\n   for node_id, proto in enumerate(object_graph_def.nodes):\n     if (proto.WhichOneof('kind') == 'user_object' and\n-        proto.user_object.identifier in KERAS_OBJECT_IDENTIFIERS):\n+        proto.user_object.identifier in constants.KERAS_OBJECT_IDENTIFIERS):\n       metadata.nodes.add(\n           node_id=node_id,\n           node_path=node_paths[node_id],\n@@ -340,7 +334,7 @@ class KerasObjectLoader(object):\n       if (child_proto.user_object.identifier in\n           revived_types.registered_identifiers()):\n         setter = revived_types.get_setter(child_proto.user_object)\n-      elif obj_child._object_identifier in KERAS_OBJECT_IDENTIFIERS:\n+      elif obj_child._object_identifier in constants.KERAS_OBJECT_IDENTIFIERS:\n         setter = _revive_setter\n       else:\n         setter = setattr\n@@ -377,7 +371,7 @@ class KerasObjectLoader(object):\n     # time by creating objects multiple times).\n     metric_list = []\n     for node_metadata in self._metadata.nodes:\n-      if node_metadata.identifier == '_tf_keras_metric':\n+      if node_metadata.identifier == constants.METRIC_IDENTIFIER:\n         metric_list.append(node_metadata)\n         continue\n \n@@ -425,12 +419,12 @@ class KerasObjectLoader(object):\n \n   def _revive_from_config(self, identifier, metadata, node_id):\n     \"\"\"Revives a layer/model from config, or returns None.\"\"\"\n-    if identifier == '_tf_keras_metric':\n+    if identifier == constants.METRIC_IDENTIFIER:\n       obj = self._revive_metric_from_config(metadata)\n     else:\n       obj = (\n           self._revive_graph_network(metadata, node_id) or\n-          self._revive_layer_from_config(metadata, node_id))\n+          self._revive_layer_or_model_from_config(metadata, node_id))\n \n     if obj is None:\n       return None, None\n@@ -476,8 +470,8 @@ class KerasObjectLoader(object):\n       self._models_to_reconstruct.append(node_id)\n     return model\n \n-  def _revive_layer_from_config(self, metadata, node_id):\n-    \"\"\"Revives a layer from config, or returns None if infeasible.\"\"\"\n+  def _revive_layer_or_model_from_config(self, metadata, node_id):\n+    \"\"\"Revives a layer/custom model from config; returns None if infeasible.\"\"\"\n     # Check that the following requirements are met for reviving from config:\n     #    1. Object can be deserialized from config.\n     #    2. If the object needs to be built, then the build input shape can be\n@@ -515,6 +509,12 @@ class KerasObjectLoader(object):\n       obj._set_dtype_policy(metadata['dtype'])\n     if metadata.get('stateful') is not None:\n       obj.stateful = metadata['stateful']\n+    # Restore model save spec for subclassed models. (layers do not store a\n+    # SaveSpec)\n+    if isinstance(obj, training_lib.Model):\n+      save_spec = metadata.get('save_spec')\n+      if save_spec is not None:\n+        obj._set_save_spec(save_spec)\n     # pylint: enable=protected-access\n \n     build_input_shape = metadata.get('build_input_shape')\n@@ -523,7 +523,6 @@ class KerasObjectLoader(object):\n     if not built:\n       # If the layer cannot be built, revive a custom layer instead.\n       return None\n-\n     return obj\n \n   def _revive_metric_from_config(self, metadata):\n@@ -914,11 +913,12 @@ def revive_custom_object(identifier, metadata):\n     model_class = training_lib_v1.Model\n \n   revived_classes = {\n-      '_tf_keras_layer': (RevivedLayer, base_layer.Layer),\n-      '_tf_keras_input_layer': (RevivedInputLayer, input_layer.InputLayer),\n-      '_tf_keras_network': (RevivedNetwork, functional_lib.Functional),\n-      '_tf_keras_model': (RevivedNetwork, model_class),\n-      '_tf_keras_sequential': (RevivedNetwork, models_lib.Sequential),\n+      constants.INPUT_LAYER_IDENTIFIER: (\n+          RevivedInputLayer, input_layer.InputLayer),\n+      constants.LAYER_IDENTIFIER: (RevivedLayer, base_layer.Layer),\n+      constants.MODEL_IDENTIFIER: (RevivedNetwork, model_class),\n+      constants.NETWORK_IDENTIFIER: (RevivedNetwork, functional_lib.Functional),\n+      constants.SEQUENTIAL_IDENTIFIER: (RevivedNetwork, models_lib.Sequential),\n   }\n   parent_classes = revived_classes.get(identifier, None)\n \n\n@@ -18,6 +18,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n+from keras.saving.saved_model import constants\n from keras.saving.saved_model import layer_serialization\n from keras.utils import generic_utils\n from tensorflow.python.training.tracking import data_structures\n@@ -28,7 +29,7 @@ class MetricSavedModelSaver(layer_serialization.LayerSavedModelSaver):\n \n   @property\n   def object_identifier(self):\n-    return '_tf_keras_metric'\n+    return constants.METRIC_IDENTIFIER\n \n   def _python_properties_internal(self):\n     metadata = dict(\n\n@@ -29,13 +29,14 @@ class ModelSavedModelSaver(layer_serialization.LayerSavedModelSaver):\n \n   @property\n   def object_identifier(self):\n-    return '_tf_keras_model'\n+    return constants.MODEL_IDENTIFIER\n \n   def _python_properties_internal(self):\n     metadata = super(ModelSavedModelSaver, self)._python_properties_internal()\n     # Network stateful property is dependent on the child layers.\n     metadata.pop('stateful')\n     metadata['is_graph_network'] = self.obj._is_graph_network  # pylint: disable=protected-access\n+    metadata['save_spec'] = self.obj._get_save_spec(dynamic_batch=False)  # pylint: disable=protected-access\n \n     metadata.update(\n         saving_utils.model_metadata(\n@@ -63,4 +64,4 @@ class SequentialSavedModelSaver(ModelSavedModelSaver):\n \n   @property\n   def object_identifier(self):\n-    return '_tf_keras_sequential'\n+    return constants.SEQUENTIAL_IDENTIFIER\n\n@@ -18,6 +18,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n+from keras.saving.saved_model import constants\n from keras.saving.saved_model import model_serialization\n \n \n@@ -27,4 +28,4 @@ class NetworkSavedModelSaver(model_serialization.ModelSavedModelSaver):\n \n   @property\n   def object_identifier(self):\n-    return '_tf_keras_network'\n+    return constants.NETWORK_IDENTIFIER\n\n@@ -357,6 +357,15 @@ class TestModelRevive(ReviveTestBase):\n     self.assertAllClose(model.test_on_batch(x, y_true),\n                         revived.test_on_batch(x, y_true))\n \n+  def test_revived_model_has_save_spec(self):\n+    model = SubclassedModelWithConfig(2, 3)\n+    model.predict(np.random.random((5, 10)).astype(np.float32))\n+    model.save(self.path, save_format='tf')\n+    revived = keras_load.load(self.path, compile=True)\n+    self.assertAllEqual(\n+        model._get_save_spec(dynamic_batch=False),\n+        revived._get_save_spec(dynamic_batch=False))\n+\n \n if __name__ == '__main__':\n   tf.compat.v1.enable_eager_execution()\n\n@@ -335,7 +335,7 @@ class ModelSubclassingTest(keras_parameterized.TestCase):\n     model(np.ones((3, 4)))  # need to build model first\n     print_fn = ToString()\n     model.summary(print_fn=print_fn)\n-    self.assertTrue('Trainable params: 356' in print_fn.contents)\n+    self.assertIn('Trainable params: 356', print_fn.contents)\n \n     # Multi-io\n     model = model_util.get_multi_io_subclass_model(\n@@ -343,7 +343,17 @@ class ModelSubclassingTest(keras_parameterized.TestCase):\n     model([np.ones((3, 4)), np.ones((3, 4))])  # need to build model first\n     print_fn = ToString()\n     model.summary(print_fn=print_fn)\n-    self.assertTrue('Trainable params: 587' in print_fn.contents)\n+    self.assertIn('Trainable params: 587', print_fn.contents)\n+\n+    # Single-io with unused layer\n+    model = testing_utils.SmallSubclassMLP(\n+        num_hidden=32, num_classes=4, use_bn=True, use_dp=True)\n+    model.unused_layer = keras.layers.Dense(10)\n+    model(np.ones((3, 4)))  # need to build model first\n+    print_fn = ToString()\n+    model.summary(print_fn=print_fn)\n+    self.assertIn('Trainable params: 356', print_fn.contents)\n+    self.assertIn('0 (unused)', print_fn.contents)\n \n   def test_no_dependency(self):\n     class Foo(keras.Model):\n\n@@ -207,7 +207,13 @@ def print_summary(model, line_length=None, positions=None, print_fn=None):\n       output_shape = '?'\n     name = layer.name\n     cls_name = layer.__class__.__name__\n-    fields = [name + ' (' + cls_name + ')', output_shape, layer.count_params()]\n+    if not layer.built and not getattr(layer, '_is_graph_network', False):\n+      # If a subclassed model has a layer that is not called in Model.call, the\n+      # layer will not be built and we cannot call layer.count_params().\n+      params = '0 (unused)'\n+    else:\n+      params = layer.count_params()\n+    fields = [name + ' (' + cls_name + ')', output_shape, params]\n     print_row(fields, positions)\n \n   def print_layer_summary_with_connections(layer):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
