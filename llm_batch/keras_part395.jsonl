{"custom_id": "keras#08f735dafc8b99a63fe4af0950728e0887029c69", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 48 | Churn Cumulative: 7741 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -1766,16 +1766,56 @@ _USE_GENERATOR_FOR_RNG = False\n _SEED_GENERATOR = threading.local()\n \n \n-def use_generator_for_rng():\n+@keras_export('keras.backend.experimental.is_tf_random_generator_enabled',\n+              v1=[])\n+def is_tf_random_generator_enabled():\n+  \"\"\"Check whether `tf.random.Generator` is used for RNG in Keras.\n+\n+  Compared to existing TF stateful random ops, `tf.random.Generator` uses\n+  `tf.Variable` and stateless random ops to generate random numbers,\n+  which leads to better reproducibility in distributed training.\n+  Note enabling it might introduce some breakage to existing code,\n+  by producing differently-seeded random number sequences\n+  and breaking tests that rely on specific random numbers being generated.\n+  To disable the\n+  usage of `tf.random.Generator`, please use\n+  `tf.keras.backend.experimental.disable_random_generator`.\n+\n+  We expect the `tf.random.Generator` code path to become the default, and will\n+  remove the legacy stateful random ops such as `tf.random.uniform` in the\n+  future (see the\n+  [TF RNG guide](https://www.tensorflow.org/guide/random_numbers)).\n+\n+  This API will also be removed in a future release as well, together with\n+  `tf.keras.backend.experimental.enable_tf_random_generator()` and\n+  `tf.keras.backend.experimental.disable_tf_random_generator()`\n+\n+  Returns:\n+    boolean: whether `tf.random.Generator` is used for random number generation\n+      in Keras.\n+  \"\"\"\n   return _USE_GENERATOR_FOR_RNG\n \n \n-def enabled_generator_for_rng():\n+@keras_export('keras.backend.experimental.enable_tf_random_generator', v1=[])\n+def enable_tf_random_generator():\n+  \"\"\"Enable the `tf.random.Generator` as the RNG for Keras.\n+\n+  See `tf.keras.backend.experimental.is_tf_random_generator_enabled` for more\n+  details.\n+  \"\"\"\n+\n   global _USE_GENERATOR_FOR_RNG\n   _USE_GENERATOR_FOR_RNG = True\n \n \n-def disable_generator_for_rng():\n+@keras_export('keras.backend.experimental.disable_tf_random_generator', v1=[])\n+def disable_tf_random_generator():\n+  \"\"\"Disable the `tf.random.Generator` as the RNG for Keras.\n+\n+  See `tf.keras.backend.experimental.is_tf_random_generator_enabled` for more\n+  details.\n+  \"\"\"\n   global _USE_GENERATOR_FOR_RNG\n   _USE_GENERATOR_FOR_RNG = False\n \n@@ -1808,7 +1848,7 @@ class RandomGenerator(tf.__internal__.tracking.AutoTrackable):\n       return\n \n     if (tf.compat.v1.executing_eagerly_outside_functions() and\n-        (use_generator_for_rng() or self._force_generator)):\n+        (is_tf_random_generator_enabled() or self._force_generator)):\n       # In the case of V2, we use tf.random.Generator to create all the random\n       # numbers and seeds.\n       from keras.utils import tf_utils  # pylint: disable=g-import-not-at-top\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8fb895d30ac526be9dfc1d5bb30ef6c42c56d032", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 206 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 20/19 | Churn Δ: 206 | Churn Cumulative: 741 | Contributors (this commit): 1 | Commits (past 90d): 15 | Contributors (cumulative): 2 | DMM Complexity: 0.3300970873786408\n\nDIFF:\n@@ -13,12 +13,14 @@ from keras.optimizer_experimental import adadelta as adadelta_new\n from keras.optimizer_experimental import adagrad as adagrad_new\n from keras.optimizer_experimental import adam as adam_new\n from keras.optimizer_experimental import optimizer_lib\n+from keras.optimizer_experimental import rmsprop as rmsprop_new\n from keras.optimizer_experimental import sgd as sgd_new\n from keras.optimizer_v2 import adadelta as adadelta_old\n from keras.optimizer_v2 import adagrad as adagrad_old\n from keras.optimizer_v2 import adam as adam_old\n from keras.optimizer_v2 import gradient_descent as sgd_old\n from keras.optimizer_v2 import learning_rate_schedule\n+from keras.optimizer_v2 import rmsprop as rmsprop_old\n from keras.utils import losses_utils\n import numpy as np\n import tensorflow.compat.v2 as tf\n@@ -47,6 +49,8 @@ adagrad_new_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentaladagrad\", lambda: adagrad_new.Adagrad(0.002))\n adam_new_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentaladam\", lambda: adam_new.Adam(0.002))\n+rmsprop_new_fn = tf.__internal__.test.combinations.NamedObject(\n+    \"experimentalrmsprop\", lambda: rmsprop_new.RMSprop(0.002))\n sgd_new_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentalsgdaverage\",\n     lambda: sgd_new.SGD(  # pylint: disable=g-long-lambda\n@@ -58,6 +62,7 @@ OPTIMIZER_FN = [\n     adadelta_new_fn,\n     adagrad_new_fn,\n     adam_new_fn,\n+    rmsprop_new_fn,\n     sgd_new_fn,\n ]\n \n@@ -303,6 +308,9 @@ class OptimizerRegressionTest(tf.test.TestCase, parameterized.TestCase):\n   def testAdagrad(self):\n     self._compare_numerical(adagrad_old.Adagrad(), adagrad_new.Adagrad())\n \n+  def testRMSprop(self):\n+    self._compare_numerical(rmsprop_new.RMSprop(), rmsprop_old.RMSprop())\n+\n   @parameterized.product(nesterov=[True, False])\n   def testSgd(self, nesterov):\n     self._compare_numerical(\n\n@@ -0,0 +1,198 @@\n+# Copyright 2021 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"RMSprop optimizer implementation.\"\"\"\n+\n+from keras.optimizer_experimental import optimizer\n+from keras.utils import generic_utils\n+import tensorflow.compat.v2 as tf\n+\n+\n+@generic_utils.register_keras_serializable()\n+class RMSprop(optimizer.Optimizer):\n+  r\"\"\"Optimizer that implements the RMSprop algorithm.\n+\n+  The gist of RMSprop is to:\n+\n+  - Maintain a moving (discounted) average of the square of gradients\n+  - Divide the gradient by the root of this average\n+\n+  This implementation of RMSprop uses plain momentum, not Nesterov momentum.\n+\n+  The centered version additionally maintains a moving average of the\n+  gradients, and uses that average to estimate the variance.\n+\n+  Attributes:\n+    learning_rate: Initial value for the learning rate:\n+      either a floating point value,\n+      or a `tf.keras.optimizers.schedules.LearningRateSchedule` instance.\n+      Defaults to 0.001.\n+    rho: float, defaults to 0.9. Discounting factor for the old gradients.\n+    momentum: float, defaults to 0.0. If not 0.0., the optimizer tracks the\n+      momentum value, with a decay rate equals to `1 - momentum`.\n+    epsilon: A small constant for numerical stability. This epsilon is\n+      \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n+      Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n+      1e-7.\n+    centered: Boolean. If `True`, gradients are normalized by the estimated\n+      variance of the gradient; if False, by the uncentered second moment.\n+      Setting this to `True` may help with training, but is slightly more\n+      expensive in terms of computation and memory. Defaults to `False`.\n+    gradients_clip_option: an instance of\n+      `optimizer_experimental.GradientsClipOption`, for attributes related to\n+      gradients clipping, such as clipnorm and clipvalue. Default to None\n+      (not applying gradients clipping).\n+    ema_option: an instance of `optimizer_experimental.EMAOption`, for\n+      attributes related to exponenatial moving average, such as `use_ema` (a\n+      boolean field indicates if EMA is used) and EMA momentum. Default to None\n+      (not applying EMA).\n+    jit_compile: Bool, default to False. If True, the optimizer will use XLA\n+      acceleration. `jit_compile` can only be False when using Parameter\n+      Server Strategy.\n+    name: Optional name prefix for the operations created when applying\n+      gradients.  Defaults to `\"RMSprop\"`.\n+    **kwargs: keyword arguments only used for backward compatibility with\n+      `optimizer_v2.OptimizerV2`. Any new code using\n+      `optimizer_experimental.Optimizer` should leave this parameter empty.\n+\n+  Usage:\n+\n+  >>> opt = tf.keras.optimizers.RMSprop(learning_rate=0.1)\n+  >>> var1 = tf.Variable(10.0)\n+  >>> loss = lambda: (var1 ** 2) / 2.0    # d(loss) / d(var1) = var1\n+  >>> step_count = opt.minimize(loss, [var1]).numpy()\n+  >>> var1.numpy()\n+  9.683772\n+\n+  Reference:\n+    - [Hinton, 2012](\n+      http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\n+\n+  \"\"\"\n+\n+  def __init__(self,\n+               learning_rate=0.001,\n+               rho=0.9,\n+               momentum=0.0,\n+               epsilon=1e-7,\n+               centered=False,\n+               gradients_clip_option=None,\n+               ema_option=None,\n+               jit_compile=False,\n+               name='RMSprop',\n+               **kwargs):\n+    super(RMSprop, self).__init__(\n+        gradients_clip_option=gradients_clip_option,\n+        ema_option=ema_option,\n+        jit_compile=jit_compile,\n+        name=name,\n+        **kwargs)\n+    self._learning_rate = self._build_learning_rate(learning_rate)\n+    self.rho = rho\n+    self.momentum = momentum\n+    self.epsilon = epsilon\n+    self.centered = centered\n+\n+  def build(self, var_list):\n+    super().build(var_list)\n+    if hasattr(self, '_built') and self._built:\n+      return\n+    self._built = True\n+\n+    self._velocities = []\n+    for var in var_list:\n+      self._velocities.append(\n+          self.add_variable_from_reference(var, 'velocity'))\n+\n+    self._momentums = []\n+    if self.momentum > 0:\n+      for var in var_list:\n+        self._momentums.append(\n+            self.add_variable_from_reference(var, 'momentum'))\n+\n+    self._average_gradients = []\n+    if self.centered:\n+      for var in var_list:\n+        self._average_gradients.append(\n+            self.add_variable_from_reference(var, 'average gradient'))\n+\n+  def update_step(self, gradient, variable, params=None):\n+    \"\"\"Update step given gradient and the associated model variable.\"\"\"\n+    if self._var_key(variable) not in self._index_dict:\n+      raise KeyError(f'Optimizer cannot recognize variable {variable.name}, '\n+                     f'this usually means you are calling an optimizer '\n+                     f'previously used on a different model. Please try '\n+                     f'creating a new optimizer instance.')\n+    lr = tf.cast(self.learning_rate, variable.dtype)\n+\n+    var_key = self._var_key(variable)\n+    velocity = self._velocities[self._index_dict[var_key]]\n+    momentum = None\n+    if self.momentum > 0:\n+      momentum = self._momentums[self._index_dict[var_key]]\n+    average_grad = None\n+    if self.centered:\n+      average_grad = self._average_grad[self._index_dict[var_key]]\n+\n+    rho = self.rho\n+\n+    if isinstance(gradient, tf.IndexedSlices):\n+      # Sparse gradients.\n+      velocity.assign(rho * velocity)\n+      velocity.scatter_add(tf.IndexedSlices(\n+          tf.square(gradient.values) * (1 - rho), gradient.indices))\n+      if self.centered:\n+        average_grad.assign(rho * average_grad)\n+        average_grad.scatter_add(\n+            tf.IndexedSlices(\n+                tf.square(gradient.values) * (1 - rho), gradient.indices))\n+        velocity.assign_add(-tf.square(average_grad))\n+      velocity_value = tf.gather(velocity, gradient.indices)\n+      transformed_grad = tf.IndexedSlices(\n+          gradient.values / (tf.sqrt(velocity_value) + self.epsilon),\n+          gradient.indices)\n+\n+      if self.momentum > 0:\n+        momentum.assign(self.momentum * momentum)\n+        momentum.scatter_add(transformed_grad)\n+        variable.assign_add(-lr * momentum)\n+      else:\n+        variable.scatter_add(\n+            tf.IndexedSlices(-lr * transformed_grad.values,\n+                             transformed_grad.indices))\n+    else:\n+      # Dense gradients.\n+      velocity.assign(rho * velocity + (1 - rho) * tf.square(gradient))\n+      if self.centered:\n+        average_grad.assign(rho * average_grad +\n+                            (1 - rho) * tf.square(gradient))\n+        velocity.assign_add(-tf.square(average_grad))\n+      transformed_grad = gradient / (tf.sqrt(velocity) + self.epsilon)\n+      if self.momentum > 0:\n+        momentum.assign(self.momentum * momentum + transformed_grad)\n+        variable.assign_add(-lr * momentum)\n+      else:\n+        variable.assign_add(-lr * transformed_grad)\n+\n+  def get_config(self):\n+    config = super(RMSprop, self).get_config()\n+\n+    config.update({\n+        'learning_rate': self._serialize_hyperparameter(self._learning_rate),\n+        'rho': self.rho,\n+        'momentum': self.momentum,\n+        'epsilon': self.epsilon,\n+        'centered': self.centered,\n+    })\n+    return config\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fbf0cfc74642c6187c69e2f75cfda6576256ffb0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 93 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 8 | Complexity Δ (Sum/Max): 6/5 | Churn Δ: 97 | Churn Cumulative: 20696 | Contributors (this commit): 120 | Commits (past 90d): 27 | Contributors (cumulative): 127 | DMM Complexity: 1.0\n\nDIFF:\n@@ -839,13 +839,67 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     # Run forward pass.\n     with tf.GradientTape() as tape:\n       y_pred = self(x, training=True)\n-      loss = self.compiled_loss(\n-          y, y_pred, sample_weight, regularization_losses=self.losses)\n+      loss = self.compute_loss(x, y, y_pred, sample_weight)\n     self._validate_target_and_loss(y, loss)\n     # Run backwards pass.\n     self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n     return self.compute_metrics(x, y, y_pred, sample_weight)\n \n+  def compute_loss(self, x=None, y=None, y_pred=None, sample_weight=None):\n+    \"\"\"Compute the total loss, validate it, and return it.\n+\n+    Subclasses can optionally override this method to provide custom loss\n+    computation logic.\n+\n+    Example:\n+    ```python\n+    class MyModel(tf.keras.Model):\n+\n+      def __init__(self, *args, **kwargs):\n+        super(MyModel, self).__init__(*args, **kwargs)\n+        self.loss_tracker = tf.keras.metrics.Mean(name='loss')\n+\n+      def compute_loss(self, x, y, y_pred, sample_weight):\n+        loss = tf.reduce_mean(tf.math.squared_difference(y_pred, y))\n+        loss += tf.add_n(self.losses)\n+        self.loss_tracker.update_state(loss)\n+        return loss\n+\n+      def reset_metrics(self):\n+        self.loss_tracker.reset_states()\n+\n+      @property\n+      def metrics(self):\n+        return [self.loss_tracker]\n+\n+    tensors = tf.random.uniform((10, 10)), tf.random.uniform((10,))\n+    dataset = tf.data.Dataset.from_tensor_slices(tensors).repeat().batch(1)\n+\n+    inputs = tf.keras.layers.Input(shape=(10,), name='my_input')\n+    outputs = tf.keras.layers.Dense(10)(inputs)\n+    model = MyModel(inputs, outputs)\n+    model.add_loss(tf.reduce_sum(outputs))\n+\n+    optimizer = tf.keras.optimizers.SGD()\n+    model.compile(optimizer, loss='mse', steps_per_execution=10)\n+    model.fit(dataset, epochs=2, steps_per_epoch=10)\n+    print('My custom loss: ', model.loss_tracker.result().numpy())\n+    ```\n+\n+    Args:\n+      x: Input data.\n+      y: Target data.\n+      y_pred: Predictions returned by the model (output of `model(x)`)\n+      sample_weight: Sample weights for weighting the loss function.\n+\n+    Returns:\n+      The total loss as a `tf.Tensor`, or `None` if no loss results (which is\n+      the case when called by `Model.test_step`).\n+    \"\"\"\n+    del x  # The default implementation does not use `x`.\n+    return self.compiled_loss(\n+        y, y_pred, sample_weight, regularization_losses=self.losses)\n+\n   def compute_metrics(self, x, y, y_pred, sample_weight):\n     \"\"\"Update metric states and collect all metrics to be returned.\n \n@@ -1395,8 +1449,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n     y_pred = self(x, training=False)\n     # Updates stateful loss metrics.\n-    self.compiled_loss(\n-        y, y_pred, sample_weight, regularization_losses=self.losses)\n+    self.compute_loss(x, y, y_pred, sample_weight)\n     return self.compute_metrics(x, y, y_pred, sample_weight)\n \n   def make_test_function(self, force=False):\n\n@@ -1772,6 +1772,42 @@ class TrainingTest(keras_parameterized.TestCase):\n     self.assertNotEqual(self.evaluate(initial_result),\n                         self.evaluate(after_fit_result))\n \n+  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  def test_custom_compute_loss(self):\n+\n+    class MyModel(training_module.Model):\n+\n+      def __init__(self, *args, **kwargs):\n+        super(MyModel, self).__init__(*args, **kwargs)\n+        self.loss_metric = metrics_module.Mean(name='loss')\n+\n+      def compute_loss(self, x, y, y_pred, sample_weight):\n+        loss = tf.reduce_mean(tf.math.squared_difference(y_pred, y))\n+        loss += tf.add_n(self.losses)\n+        self.loss_metric.update_state(loss)\n+        return loss\n+\n+      def reset_metrics(self):\n+        self.loss_metric.reset_states()\n+\n+      @property\n+      def metrics(self):\n+        return [self.loss_metric]\n+\n+    tensors = tf.random.uniform((10, 10)), tf.random.uniform((10,))\n+    dataset = tf.data.Dataset.from_tensor_slices(tensors).repeat().batch(1)\n+\n+    inputs = layers_module.Input(shape=(10,), name='my_input')\n+    outputs = layers_module.Dense(10)(inputs)\n+    model = MyModel(inputs, outputs)\n+    model.add_loss(tf.reduce_sum(outputs))\n+\n+    optimizer = optimizer_v2.gradient_descent.SGD()\n+    model.compile(optimizer, loss='mse', steps_per_execution=10)\n+    history = model.fit(dataset, epochs=2, steps_per_epoch=10)\n+    self.assertLen(history.history['loss'], 2)\n+    self.assertAllClose(history.history['loss'][1], model.loss_metric.result())\n+\n \n class TestExceptionsAndWarnings(keras_parameterized.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f1dff04a07c0c25198eb97e7018cc9f7f9a047b3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 202 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -125,7 +125,7 @@ class RMSprop(optimizer.Optimizer):\n     if self.centered:\n       for var in var_list:\n         self._average_gradients.append(\n-            self.add_variable_from_reference(var, 'average gradient'))\n+            self.add_variable_from_reference(var, 'average_gradient'))\n \n   def update_step(self, gradient, variable, params=None):\n     \"\"\"Update step given gradient and the associated model variable.\"\"\"\n@@ -143,7 +143,7 @@ class RMSprop(optimizer.Optimizer):\n       momentum = self._momentums[self._index_dict[var_key]]\n     average_grad = None\n     if self.centered:\n-      average_grad = self._average_grad[self._index_dict[var_key]]\n+      average_grad = self._average_gradients[self._index_dict[var_key]]\n \n     rho = self.rho\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a1937132e8fdd49ca1baf4a4417bce3deb093ee2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 16368 | Contributors (this commit): 118 | Commits (past 90d): 20 | Contributors (cumulative): 118 | DMM Complexity: None\n\nDIFF:\n@@ -1628,9 +1628,6 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     See the discussion of `Unpacking behavior for iterator-like inputs` for\n     `Model.fit`.\n \n-    `Model.evaluate` is not yet supported with\n-    `tf.distribute.experimental.ParameterServerStrategy`.\n-\n     Returns:\n         Scalar test loss (if the model has a single output and no metrics)\n         or list of scalars (if the model has multiple outputs\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#72978209d20c416610c32e195466ccc98c2cf370", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 12 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 24 | Churn Cumulative: 1703 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -49,18 +49,18 @@ WEIGHTS_HASHES = {\n     \"x120\": (\"5cafc461b78897d5e4f24e68cb406d18e75f31105ef620e7682b611bb355eb3a\", \"36174ddd0299db04a42631d028abcb1cc7afec2b705e42bd28fcd325e5d596bf\"),\n     \"x160\": (\"8093f57a5824b181fb734ea21ae34b1f7ee42c5298e63cf6d587c290973195d2\", \"9d1485050bdf19531ffa1ed7827c75850e0f2972118a996b91aa9264b088fd43\"),\n     \"x320\": (\"91fb3e6f4e9e44b3687e80977f7f4412ee9937c0c704232664fc83e4322ea01e\", \"9db7eacc37b85c98184070e1a172e6104c00846f44bcd4e727da9e50d9692398\"),\n-    \"y002\": (\"\", \"\"),\n-    \"y004\": (\"\", \"\"),\n-    \"y006\": (\"\", \"\"),\n-    \"y008\": (\"\", \"\"),\n-    \"y016\": (\"\", \"\"),\n-    \"y032\": (\"\", \"\"),\n-    \"y040\": (\"\", \"\"),\n-    \"y064\": (\"\", \"\"),\n-    \"y080\": (\"\", \"\"),\n-    \"y120\": (\"\", \"\"),\n-    \"y160\": (\"\", \"\"),\n-    \"y320\": (\"\", \"\")\n+    \"y002\": (\"1e8091c674532b1a61c04f6393a9c570113e0197f22bd1b98cc4c4fe800c6465\", \"f63221f63d625b8e201221499682587bfe29d33f50a4c4f4d53be00f66c0f12c\"),\n+    \"y004\": (),\n+    \"y006\": (\"98942e07b273da500ff9699a1f88aca78dfad4375faabb0bab784bb0dace80a9\", \"b70261cba4e60013c99d130cc098d2fce629ff978a445663b6fa4f8fc099a2be\"),\n+    \"y008\": (\"1b099377cc9a4fb183159a6f9b24bc998e5659d25a449f40c90cbffcbcfdcae4\", \"b11f5432a216ee640fe9be6e32939defa8d08b8d136349bf3690715a98752ca1\"),\n+    \"y016\": (\"b7ce1f5e223f0941c960602de922bcf846288ce7a4c33b2a4f2e4ac4b480045b\", \"d7404f50205e82d793e219afb9eb2bfeb781b6b2d316a6128c6d7d7dacab7f57\"),\n+    \"y032\": (\"6a6a545cf3549973554c9b94f0cd40e25f229fffb1e7f7ac779a59dcbee612bd\", \"eb3ac1c45ec60f4f031c3f5180573422b1cf7bebc26c004637517372f68f8937\"),\n+    \"y040\": (\"98d00118b335162bbffe8f1329e54e5c8e75ee09b2a5414f97b0ddfc56e796f6\", \"b5be2a5e5f072ecdd9c0b8a437cd896df0efa1f6a1f77e41caa8719b7dfcb05d\"),\n+    \"y064\": (\"65c948c7a18aaecaad2d1bd4fd978987425604ba6669ef55a1faa0069a2804b7\", \"885c4b7ed7ea339daca7dafa1a62cb7d41b1068897ef90a5a3d71b4a2e2db31a\"),\n+    \"y080\": (\"7a2c62da2982e369a4984d3c7c3b32d6f8d3748a71cb37a31156c436c37f3e95\", \"3d119577e1e3bf8d153b895e8ea9e4ec150ff2d92abdca711b6e949c3fd7115d\"),\n+    \"y120\": (\"a96ab0d27d3ae35a422ee7df0d789069b3e3217a99334e0ce861a96595bc5986\", \"4a6fa387108380b730b71feea2ad80b5224b5ea9dc21dc156c93fe3c6186485c\"),\n+    \"y160\": (\"45067240ffbc7ca2591313fee2f80dbdda6d66ec1a7451446f9a6d00d8f7ac6e\", \"ead1e6b568be8f34447ec8941299a9df4368736ba9a8205de5427fa20a1fb316\"),\n+    \"y320\": (\"b05e173e4ae635cfa22d06392ee3741284d17dadfee68f2aa6fd8cb2b7561112\", \"cad78f74a586e24c61d38be17f3ae53bb9674380174d2585da1a526b8c20e1fd\")\n }\n \n # The widths and depths are deduced from a quantized linear function. For\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b08a1dadc5e982f55adc3c824305cecf2a1b75ec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 25 | Churn Cumulative: 1018 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -549,13 +549,16 @@ def sync_to_numpy_or_python_type(tensors):\n     tensors are converted to Numpy arrays.\n   \"\"\"\n   if isinstance(tensors, tf.distribute.experimental.coordinator.RemoteValue):\n-    return tensors.fetch()\n+    tensors = tensors.fetch()\n \n   def _to_single_numpy_or_python_type(t):\n+    # Don't turn ragged or sparse tensors to NumPy.\n     if isinstance(t, tf.Tensor):\n-      x = t.numpy()\n-      return x.item() if np.ndim(x) == 0 else x\n-    return t  # Don't turn ragged or sparse tensors to NumPy.\n+      t = t.numpy()\n+    # Strings, ragged and sparse tensors don't have .item(). Return them as-is.\n+    if not isinstance(t, (np.ndarray, np.generic)):\n+      return t\n+    return t.item() if np.ndim(t) == 0 else t\n \n   return tf.nest.map_structure(_to_single_numpy_or_python_type, tensors)\n \n\n@@ -309,5 +309,19 @@ class TestGetTensorSpec(parameterized.TestCase):\n       self.assertEqual(result.shape.as_list(), expected_shape)\n \n \n+class TestSyncToNumpyOrPythonType(parameterized.TestCase):\n+\n+  @parameterized.parameters([\n+      (0.5,),\n+      (b'string value',),\n+  ])\n+  def test_types(self, value):\n+    if not tf.executing_eagerly():\n+      self.skipTest('`sync_to_numpy_or_python_type` only works in eager')\n+    tensor = tf.constant(value)\n+\n+    self.assertEqual(tf_utils.sync_to_numpy_or_python_type(\n+        tensor), value)\n+\n if __name__ == '__main__':\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8ef5b2deaf1bc535230c9fd154b2a2f32203c357", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 7743 | Contributors (this commit): 10 | Commits (past 90d): 6 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -2147,7 +2147,7 @@ def moving_average_update(x, value, momentum):\n   if tf.__internal__.tf2.enabled():\n     momentum = tf.cast(momentum, x.dtype)\n     value = tf.cast(value, x.dtype)\n-    return x.assign(x * momentum + value * (1 - momentum))\n+    return x.assign_sub((x - value) * (1 - momentum))\n   else:\n     return tf.__internal__.train.assign_moving_average(\n         x, value, momentum, zero_debias=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#98d67e5dd0d8252c2876cf27982df5c8fef02881", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1705 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -50,7 +50,7 @@ WEIGHTS_HASHES = {\n     \"x160\": (\"8093f57a5824b181fb734ea21ae34b1f7ee42c5298e63cf6d587c290973195d2\", \"9d1485050bdf19531ffa1ed7827c75850e0f2972118a996b91aa9264b088fd43\"),\n     \"x320\": (\"91fb3e6f4e9e44b3687e80977f7f4412ee9937c0c704232664fc83e4322ea01e\", \"9db7eacc37b85c98184070e1a172e6104c00846f44bcd4e727da9e50d9692398\"),\n     \"y002\": (\"1e8091c674532b1a61c04f6393a9c570113e0197f22bd1b98cc4c4fe800c6465\", \"f63221f63d625b8e201221499682587bfe29d33f50a4c4f4d53be00f66c0f12c\"),\n-    \"y004\": (),\n+    \"y004\": (\"752fdbad21c78911bf1dcb8c513e5a0e14697b068e5d9e73525dbaa416d18d8e\", \"45e6ba8309a17a77e67afc05228454b2e0ee6be0dae65edc0f31f1da10cc066b\"),\n     \"y006\": (\"98942e07b273da500ff9699a1f88aca78dfad4375faabb0bab784bb0dace80a9\", \"b70261cba4e60013c99d130cc098d2fce629ff978a445663b6fa4f8fc099a2be\"),\n     \"y008\": (\"1b099377cc9a4fb183159a6f9b24bc998e5659d25a449f40c90cbffcbcfdcae4\", \"b11f5432a216ee640fe9be6e32939defa8d08b8d136349bf3690715a98752ca1\"),\n     \"y016\": (\"b7ce1f5e223f0941c960602de922bcf846288ce7a4c33b2a4f2e4ac4b480045b\", \"d7404f50205e82d793e219afb9eb2bfeb781b6b2d316a6128c6d7d7dacab7f57\"),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4383429e8e5e7d41198c5ce4f5f63c38f26794b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1707 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -237,7 +237,7 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n   RegNet models expect their inputs to be float or uint8 tensors of pixels with \n   values in the [0-255] range.\n \n-  The naming of models is as follows: `RegNet{block_type}{flops}` where \n+  The naming of models is as follows: `RegNet<block_type><flops>` where \n   `block_type` is one of `(Y, Z)` and `flops` signifies hundred million \n   floating point operations. For example RegNetY64 corresponds to RegNet with \n   Y block and 6.4 giga flops (64 hundred million flops). \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c7f21519fa96d5a588ff4e4e176e12e96539f701", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 164 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -125,7 +125,7 @@ class ApplicationsLoadWeightTest(tf.test.TestCase, parameterized.TestCase):\n       self.assertShapeEqual(model.output_shape, (None, _IMAGENET_CLASSES))\n       x = _get_elephant(model.input_shape[1:3])\n       x = app_module.preprocess_input(x)\n-      preds = model.predict(x)\n+      preds = model(x).numpy()\n       names = [p[1] for p in app_module.decode_predictions(preds)[0]]\n       # Test correct label is in top 3 (weak correctness test).\n       self.assertIn('African_elephant', names[:3])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#012fd63aba9f4dfb66b8055a1cd94a9b93987d28", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 85 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 10/9 | Churn Δ: 86 | Churn Cumulative: 10875 | Contributors (this commit): 102 | Commits (past 90d): 22 | Contributors (cumulative): 110 | DMM Complexity: 1.0\n\nDIFF:\n@@ -25,6 +25,7 @@ import re\n import sys\n import time\n \n+\n from keras import backend\n from keras.distribute import distributed_file_utils\n from keras.distribute import worker_training_state\n@@ -40,6 +41,7 @@ import numpy as np\n import tensorflow.compat.v2 as tf\n \n from tensorflow.python.platform import tf_logging as logging\n+from tensorflow.python.util import deprecation  # pylint: disable=g-direct-tensorflow-import\n from tensorflow.python.util.tf_export import keras_export\n from tensorflow.tools.docs import doc_controls\n \n@@ -1596,7 +1598,7 @@ class ModelCheckpoint(Callback):\n       return file_path_with_largest_file_name\n \n \n-@keras_export('keras.callbacks.experimental.BackupAndRestore', v1=[])\n+@keras_export('keras.callbacks.BackupAndRestore', v1=[])\n class BackupAndRestore(Callback):\n   \"\"\"Callback to back up and restore the training state.\n \n@@ -1719,6 +1721,21 @@ class BackupAndRestore(Callback):\n     self._training_state.back_up(epoch)\n \n \n+@keras_export('keras.callbacks.experimental.BackupAndRestore', v1=[])\n+@deprecation.deprecated_endpoints(\n+    'keras.callbacks.experimental.BackupAndRestore')\n+class BackupAndRestoreExperimental(BackupAndRestore):\n+\n+  __doc__ = BackupAndRestore.__doc__\n+\n+  def __init__(self, *args, **kwargs):\n+    logging.warning(\n+        '`tf.keras.callbacks.experimental.BackupAndRestore` endpoint is '\n+        'deprecated and will be removed in a future release. Please use '\n+        '`tf.keras.callbacks.BackupAndRestore`.')\n+    super(BackupAndRestoreExperimental, self).__init__(*args, **kwargs)\n+\n+\n @keras_export('keras.callbacks.EarlyStopping')\n class EarlyStopping(Callback):\n   \"\"\"Stop training when a monitored metric has stopped improving.\n\n@@ -34,6 +34,7 @@ import keras\n from keras import keras_parameterized\n from keras import testing_utils\n from keras.callbacks import BackupAndRestore\n+from keras.callbacks import BackupAndRestoreExperimental\n from keras.engine import sequential\n from keras.layers import Activation\n from keras.layers import Dense\n@@ -356,6 +357,72 @@ class KerasCallbacksTest(keras_parameterized.TestCase):\n       pass\n     self.assertEqual(model._train_counter.numpy(), 13)\n \n+  def _test_backup_and_restore_callback_with(self, cls):\n+    if not tf.compat.v1.executing_eagerly():\n+      self.skipTest('BackupAndRestore only available when execution is enabled')\n+\n+    class InterruptingCallback(keras.callbacks.Callback):\n+      \"\"\"A callback to intentionally introduce interruption to training.\"\"\"\n+\n+      def on_epoch_end(self, epoch, log=None):\n+        if epoch == 15:\n+          raise RuntimeError('Interruption')\n+\n+    model = keras.Sequential([keras.layers.Dense(10)])\n+    optimizer = gradient_descent.SGD()\n+    model.compile(optimizer, loss='mse')\n+\n+    x = tf.random.uniform((24, 10))\n+    y = tf.random.uniform((24,))\n+    dataset = tf.data.Dataset.from_tensor_slices((x, y)).repeat().batch(2)\n+\n+    backup_callback = cls(backup_dir=self.get_temp_dir())\n+    try:\n+      model.fit(\n+          dataset,\n+          epochs=20,\n+          steps_per_epoch=5,\n+          callbacks=[backup_callback, InterruptingCallback()])\n+    except RuntimeError:\n+      logging.warning('***Handling interruption***')\n+      # This continues at the epoch where it left off.\n+      model.fit(\n+          dataset, epochs=20, steps_per_epoch=5, callbacks=[backup_callback])\n+\n+  def test_experimental_backup_and_restore(self):\n+    \"\"\"Ensure the legacy endpoint of `BackupAndRestore` gives warning.\"\"\"\n+\n+    warning_messages = []\n+\n+    def warning(msg):\n+      warning_messages.append(msg)\n+\n+    with tf.compat.v1.test.mock.patch.object(logging, 'warning', warning):\n+      self._test_backup_and_restore_callback_with(BackupAndRestoreExperimental)\n+\n+    warning_msg = ('`tf.keras.callbacks.experimental.BackupAndRestore` '\n+                   'endpoint is deprecated')\n+    self.assertIn(warning_msg, '\\n'.join(warning_messages))\n+    warning_msg = ('***Handling interruption***')\n+    self.assertIn(warning_msg, '\\n'.join(warning_messages))\n+\n+  def test_backup_and_restore(self):\n+    \"\"\"Ensure the public endpoint of `BackupAndRestore` is working.\"\"\"\n+\n+    warning_messages = []\n+\n+    def warning(msg):\n+      warning_messages.append(msg)\n+\n+    with tf.compat.v1.test.mock.patch.object(logging, 'warning', warning):\n+      self._test_backup_and_restore_callback_with(BackupAndRestore)\n+\n+    warning_msg = ('`tf.keras.callbacks.experimental.BackupAndRestore` '\n+                   'endpoint is deprecated')\n+    self.assertNotIn(warning_msg, '\\n'.join(warning_messages))\n+    warning_msg = ('***Handling interruption***')\n+    self.assertIn(warning_msg, '\\n'.join(warning_messages))\n+\n   @keras_parameterized.run_all_keras_modes\n   def test_callback_warning(self):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#36cb38e8e3cce6cee8d3dd168e1b83cadc82aa24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 91 | Lines Deleted: 28 | Files Changed: 3 | Hunks: 37 | Methods Changed: 36 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 119 | Churn Cumulative: 3250 | Contributors (this commit): 6 | Commits (past 90d): 9 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,12 +23,15 @@ import threading\n \n from absl.testing import parameterized\n from keras import combinations\n+from keras import models\n from keras import regularizers\n+from keras.engine import base_layer\n from keras.engine import input_layer as input_layer_module\n from keras.engine import training as training_module\n from keras.layers import core\n from keras.legacy_tf_layers import core as core_layers\n from keras.legacy_tf_layers import variable_scope_shim\n+\n import numpy\n import tensorflow as tf\n \n@@ -802,7 +805,7 @@ class VariableScopeMultithreadedTest(tf.test.TestCase):\n       thread.join()\n \n \n-class CompatV1TemplateScaleByY(variable_scope_shim.VariableScopeLayer):\n+class CompatV1TemplateScaleByY(base_layer.Layer):\n \n   def __init__(self, **kwargs):\n     super().__init__(**kwargs)\n@@ -816,7 +819,8 @@ class CompatV1TemplateScaleByY(variable_scope_shim.VariableScopeLayer):\n     self.scale_by_y = tf.compat.v1.make_template(\n         \"scale_by_y\", my_op, scalar_name=\"y\")\n \n-  def forward_pass(self, inputs):\n+  @variable_scope_shim.track_tf1_style_variables\n+  def call(self, inputs):\n     with tf.compat.v1.variable_scope(\"foo\"):\n       return self.scale_by_y(inputs)\n \n@@ -841,13 +845,14 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n   def test_get_variable(self):\n     # Test the shim when using `get_variable` (and regularizers) directly\n \n-    class WrappedDenseLayer(variable_scope_shim.VariableScopeLayer):\n+    class WrappedDenseLayer(base_layer.Layer):\n \n       def __init__(self, units, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n         self.units = units\n \n-      def forward_pass(self, inputs, training=None):\n+      @variable_scope_shim.track_tf1_style_variables\n+      def call(self, inputs, training=None):\n         out = inputs\n         with tf.compat.v1.variable_scope(\"dense_one\"):\n           # The weights are created with a `regularizer`,\n@@ -901,13 +906,14 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n   def test_compat_v1_layer(self):\n     # Test the shim when using `compat.v1` layers\n \n-    class WrappedDenseLayer(variable_scope_shim.VariableScopeLayer):\n+    class WrappedDenseLayer(base_layer.Layer):\n \n       def __init__(self, units, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n         self.units = units\n \n-      def forward_pass(self, inputs, training=None):\n+      @variable_scope_shim.track_tf1_style_variables\n+      def call(self, inputs, training=None):\n         out = core_layers.dense(\n             inputs, self.units, name=\"dense_one\",\n             kernel_initializer=tf.compat.v1.ones_initializer(),\n@@ -941,13 +947,14 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n \n   def test_shim_exporting(self):\n \n-    class WrappedDenseLayer(variable_scope_shim.VariableScopeLayer):\n+    class WrappedDenseLayer(base_layer.Layer):\n \n       def __init__(self, units, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n         self.units = units\n \n-      def forward_pass(self, inputs, training=None):\n+      @variable_scope_shim.track_tf1_style_variables\n+      def call(self, inputs, training=None):\n         out = core_layers.dense(\n             inputs,\n             self.units,\n@@ -967,8 +974,19 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n     layer(tf.ones(shape=(5, 5)))\n \n     tmp_dir = self.get_temp_dir()\n+\n+    # Try exporting the layer directly\n     tf.saved_model.save(layer, tmp_dir)\n \n+    # Try exporting the layer nested in a functional model\n+    # This is where saving reflection gets tricky due to\n+    # trying to replace the passed training arg in training=True\n+    # and training=False modes\n+    inp = input_layer_module.Input(shape=(5, 5))\n+    outs = layer(inp)\n+    model = models.Model(inp, outs)\n+    tf.saved_model.save(model, tmp_dir)\n+\n   def test_variable_store_scope_get_variable(self):\n     # Test the module shim when using `get_variable` (and regularizers) directly\n \n@@ -1146,13 +1164,14 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n   def test_shim_nesting(self):\n     # Test that nesting the shim in itself works\n \n-    class NestedLayer(variable_scope_shim.VariableScopeLayer):\n+    class NestedLayer(base_layer.Layer):\n \n       def __init__(self, units, name, *args, **kwargs):\n         super().__init__(*args, name=name, **kwargs)\n         self.units = units\n \n-      def forward_pass(self, inputs):\n+      @variable_scope_shim.track_tf1_style_variables\n+      def call(self, inputs):\n         out = inputs\n         with tf.compat.v1.variable_scope(self.name):\n           # The weights are created with a `regularizer`,\n@@ -1170,7 +1189,7 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n           out = tf.compat.v1.nn.bias_add(out, bias)\n         return out\n \n-    class WrappedDenseLayer(variable_scope_shim.VariableScopeLayer):\n+    class WrappedDenseLayer(base_layer.Layer):\n \n       def __init__(self, units, **kwargs):\n         super().__init__(**kwargs)\n@@ -1178,7 +1197,8 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n         self.dense_layer_a = None\n         self.dense_layer_b = None\n \n-      def forward_pass(self, inputs):\n+      @variable_scope_shim.track_tf1_style_variables\n+      def call(self, inputs):\n         # Only create the nested tf.variable/module/layer/model if it has not\n         # already been created!\n         if not self.dense_layer_a:\n@@ -1277,14 +1297,15 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n     # that were not created by get_variable. These variables/modules/layers\n     # need to be tracked separately\n \n-    class WrappedDenseLayer(variable_scope_shim.VariableScopeLayer):\n+    class WrappedDenseLayer(base_layer.Layer):\n \n       def __init__(self, units, **kwargs):\n         super().__init__(**kwargs)\n         self.units = units\n         self._dense_model = None\n \n-      def forward_pass(self, inputs):\n+      @variable_scope_shim.track_tf1_style_variables\n+      def call(self, inputs):\n         dense_layer = core.Dense(\n             self.units, name=\"dense\",\n             kernel_initializer=tf.compat.v1.ones_initializer(),\n@@ -1301,14 +1322,15 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n     # Test the shim when embedding a Keras model inside of it\n     # And assigning the model to an attribute\n \n-    class WrappedDenseLayer(variable_scope_shim.VariableScopeLayer):\n+    class WrappedDenseLayer(base_layer.Layer):\n \n       def __init__(self, units, **kwargs):\n         super().__init__(**kwargs)\n         self.units = units\n         self._dense_model = None\n \n-      def forward_pass(self, inputs):\n+      @variable_scope_shim.track_tf1_style_variables\n+      def call(self, inputs):\n         if not self._dense_model:\n           inp = input_layer_module.Input(shape=inputs.shape)\n           dense_layer = core.Dense(\n@@ -1386,13 +1408,14 @@ class TF1VariableScopeLayerTest(tf.test.TestCase, parameterized.TestCase):\n   def test_training_arg(self):\n     # Test the shim when passing in a Keras `training` arg\n \n-    class TrainingCheckLayer(variable_scope_shim.VariableScopeLayer):\n+    class TrainingCheckLayer(base_layer.Layer):\n \n       def __init__(self, units, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n         self.units = units\n \n-      def forward_pass(self, inputs, training=None):\n+      @variable_scope_shim.track_tf1_style_variables\n+      def call(self, inputs, training=None):\n         if training:\n           out = core_layers.dense(inputs, self.units, name=\"dense_training\")\n         else:\n@@ -1431,7 +1454,7 @@ class GetOrCreateLayerTest(tf.test.TestCase, parameterized.TestCase):\n   @combinations.generate(combinations.combine(mode=[\"eager\"]))\n   def test_get_or_create_layer_with_regularizer_eager(self):\n \n-    class NestedLayer(variable_scope_shim.VariableScopeLayer):\n+    class NestedLayer(base_layer.Layer):\n \n       def __init__(self, units, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n@@ -1445,7 +1468,8 @@ class GetOrCreateLayerTest(tf.test.TestCase, parameterized.TestCase):\n         model = training_module.Model(inputs=inp, outputs=dense_layer(inp))\n         return model\n \n-      def forward_pass(self, inputs):\n+      @variable_scope_shim.track_tf1_style_variables\n+      def call(self, inputs):\n         # enter a variable scope to check module key naming\n         with tf.compat.v1.variable_scope(\"test_scope\"):\n           model = variable_scope_shim.get_or_create_layer(\n@@ -1476,7 +1500,7 @@ class GetOrCreateLayerTest(tf.test.TestCase, parameterized.TestCase):\n   @combinations.generate(combinations.combine(mode=[\"eager\"]))\n   def test_get_or_create_layer_no_regularizer_eager(self):\n \n-    class NestedLayer(variable_scope_shim.VariableScopeLayer):\n+    class NestedLayer(base_layer.Layer):\n \n       def __init__(self, units, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n@@ -1490,7 +1514,8 @@ class GetOrCreateLayerTest(tf.test.TestCase, parameterized.TestCase):\n         model = training_module.Model(inputs=inp, outputs=dense_layer(inp))\n         return model\n \n-      def forward_pass(self, inputs):\n+      @variable_scope_shim.track_tf1_style_variables\n+      def call(self, inputs):\n         # enter a variable scope to check module key naming\n         with tf.compat.v1.variable_scope(\"test_scope\"):\n           model = variable_scope_shim.get_or_create_layer(\n@@ -1521,7 +1546,7 @@ class GetOrCreateLayerTest(tf.test.TestCase, parameterized.TestCase):\n   @combinations.generate(combinations.combine(mode=[\"eager\"]))\n   def test_get_or_create_layer_tf_function(self):\n \n-    class NestedLayer(variable_scope_shim.VariableScopeLayer):\n+    class NestedLayer(base_layer.Layer):\n \n       def __init__(self, units, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n@@ -1535,7 +1560,8 @@ class GetOrCreateLayerTest(tf.test.TestCase, parameterized.TestCase):\n         model = training_module.Model(inputs=inp, outputs=dense_layer(inp))\n         return model\n \n-      def forward_pass(self, inputs):\n+      @variable_scope_shim.track_tf1_style_variables\n+      def call(self, inputs):\n         model = variable_scope_shim.get_or_create_layer(\n             \"dense_model\", self.build_model)\n         return model(inputs)\n\n@@ -398,7 +398,7 @@ class LayerCallCollection:\n     # Get the input argument name from the args.\n     arg_spec = tf_inspect.getfullargspec(self.layer_call_method)\n     args = arg_spec.args\n-    if tf_inspect.ismethod(self.layer_call_method):\n+    if utils.call_is_method(self.layer_call_method):\n       args = args[1:]\n     self._input_arg_name = args[0] if args else 'inputs'\n \n@@ -497,7 +497,7 @@ class LayerCallCollection:\n \n       # Set new training arg index\n       self._training_arg_index = len(args) - 1\n-      if tf_inspect.ismethod(call_fn):\n+      if utils.call_is_method(call_fn):\n         self._training_arg_index -= 1\n \n       def wrap_with_training_arg(*args, **kwargs):\n\n@@ -14,11 +14,12 @@\n # ==============================================================================\n \"\"\"Utility functions shared between SavedModel saving/loading implementations.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n \n+import inspect as _inspect\n import itertools\n import threading\n import types\n+\n from keras import backend\n from keras.engine import base_layer_utils\n from keras.utils import control_flow_util\n@@ -26,6 +27,8 @@ from keras.utils import tf_contextlib\n from keras.utils import tf_inspect\n from keras.utils.generic_utils import LazyLoader\n \n+import tensorflow.compat.v2 as tf\n+\n \n # pylint:disable=g-inconsistent-quotes\n training_lib = LazyLoader(\n@@ -217,7 +220,7 @@ def get_training_arg_index(call_fn):\n   else:\n     # Try to find 'training' in the list of args or kwargs.\n     arg_list = argspec.args\n-    if tf_inspect.ismethod(call_fn):\n+    if call_is_method(call_fn):\n       arg_list = arg_list[1:]\n \n     if 'training' in arg_list:\n@@ -227,6 +230,40 @@ def get_training_arg_index(call_fn):\n     return None\n \n \n+def call_is_method(call_fn):\n+  \"\"\"Check if call_fn is a method regardless of pre/post-binding decoration.\n+\n+  E.g. decoration after instance creation/method binding\n+  self.call = @tf.function(self.call).\n+\n+  Or decoration in the class definition before binding:\n+  class Foo(Layer):\n+    @decorator\n+    def call(self, ...):\n+      pass\n+\n+  Args:\n+    call_fn: The fn to check\n+\n+  Returns:\n+    True if the fn is a bound method, or a bound method that was decorated\n+    after binding. Else False.\n+  \"\"\"\n+  # tf_inspect checks if a call_fn is either an undecorated bound method,\n+  # or a bound method that was decorated after the method was bound\n+  # to an instance. E.g. in the case of\n+  # self.call = @tf.function(self.call).\n+  #\n+  # _inspect checks if the method is bound, and returns true even if the method\n+  # was decorated before binding occurred.\n+  # e.g. this would happen in the case of\n+  # class Foo(Layer):\n+  #   @decorator\n+  #   def call(self, ...):\n+  #     pass\n+  return tf_inspect.ismethod(call_fn) or _inspect.ismethod(call_fn)\n+\n+\n def set_training_arg(training, index, args, kwargs):\n   if index is None or index < 0 or len(args) <= index:  # index is invalid\n     kwargs['training'] = training\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
