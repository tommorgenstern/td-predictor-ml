{"custom_id": "keras#37fd456a5cd44db6ca23975de6fe151c9713c58f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 43 | Files Changed: 1 | Hunks: 22 | Methods Changed: 7 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 86 | Churn Cumulative: 8039 | Contributors (this commit): 43 | Commits (past 90d): 63 | Contributors (cumulative): 43 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,9 +12,7 @@ from .. import backend as K\n from .. import activations, initializations, regularizers, constraints\n from ..regularizers import ActivityRegularizer\n \n-import marshal\n-import types\n-import sys\n+import inspect\n \n \n class Layer(object):\n@@ -1501,25 +1499,20 @@ class Lambda(Layer):\n             Takes one argument: the output of previous layer\n         output_shape: Expected output shape from function.\n             Could be a tuple or a function of the shape of the input\n+        arguments: optional dictionary of keyword arguments to be passed\n+            to the function.\n     '''\n-    def __init__(self, function, output_shape=None, **kwargs):\n+    def __init__(self, function, output_shape=None, arguments={}, **kwargs):\n         super(Lambda, self).__init__(**kwargs)\n-        py3 = sys.version_info[0] == 3\n-        if py3:\n-            self.function = marshal.dumps(function.__code__)\n-        else:\n-            assert hasattr(function, 'func_code'), ('The Lambda layer \"function\"'\n-                                                    ' argument must be a Python function.')\n-            self.function = marshal.dumps(function.func_code)\n+        self.function = function\n+        self.arguments = arguments\n         if output_shape is None:\n             self._output_shape = None\n         elif type(output_shape) in {tuple, list}:\n             self._output_shape = tuple(output_shape)\n         else:\n-            if py3:\n-                self._output_shape = marshal.dumps(output_shape.__code__)\n-            else:\n-                self._output_shape = marshal.dumps(output_shape.func_code)\n+            assert hasattr(output_shape, '__call__'), 'In Lambda, `output_shape` must be a list, a tuple, or a function.'\n+            self._output_shape = output_shape\n         super(Lambda, self).__init__()\n \n     @property\n@@ -1528,26 +1521,34 @@ class Lambda(Layer):\n             # if TensorFlow, we can infer the output shape directly:\n             if K._BACKEND == 'tensorflow':\n                 # we assume output shape is not dependent on train/test mode\n-                x = self.get_input()\n+                x = self.get_output()\n                 return K.int_shape(x)\n             # otherwise, we default to the input shape\n             return self.input_shape\n-        elif type(self._output_shape) == tuple:\n+        elif type(self._output_shape) in {tuple, list}:\n             nb_samples = self.input_shape[0] if self.input_shape else None\n-            return (nb_samples, ) + self._output_shape\n+            return (nb_samples,) + tuple(self._output_shape)\n         else:\n-            output_shape_func = marshal.loads(self._output_shape)\n-            output_shape_func = types.FunctionType(output_shape_func, globals())\n-            shape = output_shape_func(self.input_shape)\n+            shape = self._output_shape(self.input_shape)\n             if type(shape) not in {list, tuple}:\n                 raise Exception('output_shape function must return a tuple')\n             return tuple(shape)\n \n     def get_output(self, train=False):\n         X = self.get_input(train)\n-        func = marshal.loads(self.function)\n-        func = types.FunctionType(func, globals())\n-        return func(X)\n+        arguments = self.arguments\n+        arg_spec = inspect.getargspec(self.function)\n+        if 'train' in arg_spec.args:\n+            arguments['train'] = train\n+        return self.function(X, **arguments)\n+\n+    def get_config(self):\n+        # note: not serializable at the moment.\n+        config = {'function': self.function,\n+                  'output_shape': self._output_shape,\n+                  'arguments': self.arguments}\n+        base_config = super(Lambda, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n \n \n class MaskedLambda(MaskedLayer, Lambda):\n@@ -1567,8 +1568,10 @@ class LambdaMerge(Lambda):\n             list of outputs from input layers\n         output_shape - Expected output shape from function.\n             Could be a tuple or a function of list of input shapes\n+        arguments: optional dictionary of keyword arguments to be passed\n+            to the function.\n     '''\n-    def __init__(self, layers, function, output_shape=None):\n+    def __init__(self, layers, function, output_shape=None, arguments={}):\n         if len(layers) < 2:\n             raise Exception('Please specify two or more input layers '\n                             '(or containers) to merge.')\n@@ -1577,6 +1580,7 @@ class LambdaMerge(Lambda):\n         self.regularizers = []\n         self.constraints = []\n         self.updates = []\n+        self.arguments = arguments\n         for l in self.layers:\n             params, regs, consts, updates = l.get_params()\n             self.regularizers += regs\n@@ -1586,20 +1590,14 @@ class LambdaMerge(Lambda):\n                 if p not in self.trainable_weights:\n                     self.trainable_weights.append(p)\n                     self.constraints.append(c)\n-        py3 = sys.version_info[0] == 3\n-        if py3:\n-            self.function = marshal.dumps(function.__code__)\n-        else:\n-            self.function = marshal.dumps(function.func_code)\n+        self.function = function\n         if output_shape is None:\n             self._output_shape = None\n         elif type(output_shape) in {tuple, list}:\n             self._output_shape = tuple(output_shape)\n         else:\n-            if py3:\n-                self._output_shape = marshal.dumps(output_shape.__code__)\n-            else:\n-                self._output_shape = marshal.dumps(output_shape.func_code)\n+            assert hasattr(output_shape, '__call__'), 'In LambdaMerge, `output_shape` must be a list, a tuple, or a function.'\n+            self._output_shape = output_shape\n         super(Lambda, self).__init__()\n \n     @property\n@@ -1607,24 +1605,24 @@ class LambdaMerge(Lambda):\n         input_shapes = [layer.output_shape for layer in self.layers]\n         if self._output_shape is None:\n             return input_shapes[0]\n-        elif type(self._output_shape) == tuple:\n+        elif type(self._output_shape) in {tuple, list}:\n             return (input_shapes[0][0],) + self._output_shape\n         else:\n-            output_shape_func = marshal.loads(self._output_shape)\n-            output_shape_func = types.FunctionType(output_shape_func, globals())\n-            shape = output_shape_func(input_shapes)\n+            shape = self._output_shape(input_shapes)\n             if type(shape) not in {list, tuple}:\n-                raise Exception('output_shape function must return a tuple.')\n+                raise Exception('In LambdaMerge, the `output_shape` function must return a tuple.')\n             return tuple(shape)\n \n     def get_params(self):\n         return self.trainable_weights, self.regularizers, self.constraints, self.updates\n \n     def get_output(self, train=False):\n-        func = marshal.loads(self.function)\n-        func = types.FunctionType(func, globals())\n         inputs = [layer.get_output(train) for layer in self.layers]\n-        return func(inputs)\n+        arguments = self.arguments\n+        arg_spec = inspect.getargspec(self.function)\n+        if 'train' in arg_spec.args:\n+            arguments['train'] = train\n+        return self.function(inputs, **arguments)\n \n     def get_input(self, train=False):\n         res = []\n@@ -1660,10 +1658,12 @@ class LambdaMerge(Lambda):\n             weights = weights[nb_param:]\n \n     def get_config(self):\n+        # note: not serializable at the moment.\n         config = {'name': self.__class__.__name__,\n                   'layers': [l.get_config() for l in self.layers],\n                   'function': self.function,\n-                  'output_shape': self._output_shape}\n+                  'output_shape': self._output_shape,\n+                  'arguments': self.arguments}\n         base_config = super(LambdaMerge, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#80a831de1a954d23b5b9f7b9526398e10a1512e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 396 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -511,7 +511,7 @@ def test_lambda():\n     g = Graph()\n     g.add_input(name='input_a', input_shape=(2,))\n     g.add_input(name='input_b', input_shape=(2,))\n-    g.add_node(Lambda(difference),\n+    g.add_node(Lambda(difference, output_shape=(2,)),\n                inputs=['input_a', 'input_b'],\n                merge_mode='join',\n                name='d')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ecd414d7164613177725c81d023e6a66701de470", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 61 | Lines Deleted: 42 | Files Changed: 2 | Hunks: 11 | Methods Changed: 6 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 103 | Churn Cumulative: 3945 | Contributors (this commit): 21 | Commits (past 90d): 38 | Contributors (cumulative): 25 | DMM Complexity: 0.0\n\nDIFF:\n@@ -250,9 +250,9 @@ class SimpleRNN(Recurrent):\n         self.init = initializations.get(init)\n         self.inner_init = initializations.get(inner_init)\n         self.activation = activations.get(activation)\n-        self.W_regularizer = W_regularizer\n-        self.U_regularizer = U_regularizer\n-        self.b_regularizer = b_regularizer\n+        self.W_regularizer = regularizers.get(W_regularizer)\n+        self.U_regularizer = regularizers.get(U_regularizer)\n+        self.b_regularizer = regularizers.get(b_regularizer)\n         self.dropout_W, self.dropout_U = dropout_W, dropout_U\n         super(SimpleRNN, self).__init__(**kwargs)\n \n@@ -272,16 +272,16 @@ class SimpleRNN(Recurrent):\n                                  name='{}_U'.format(self.name))\n         self.b = K.zeros((self.output_dim,), name='{}_b'.format(self.name))\n \n-        def append_regulariser(input_regulariser, param, regularizers_list):\n-            regulariser = regularizers.get(input_regulariser)\n-            if regulariser:\n-                regulariser.set_param(param)\n-                regularizers_list.append(regulariser)\n-\n         self.regularizers = []\n-        append_regulariser(self.W_regularizer, self.W, self.regularizers)\n-        append_regulariser(self.U_regularizer, self.U, self.regularizers)\n-        append_regulariser(self.b_regularizer, self.b, self.regularizers)\n+        if self.W_regularizer:\n+            self.W_regularizer.set_param(self.W)\n+            self.regularizers.append(self.W_regularizer)\n+        if self.U_regularizer:\n+            self.W_regularizer.set_param(self.U)\n+            self.regularizers.append(self.U_regularizer)\n+        if self.b_regularizer:\n+            self.W_regularizer.set_param(self.b)\n+            self.regularizers.append(self.b_regularizer)\n \n         self.trainable_weights = [self.W, self.U, self.b]\n \n@@ -380,9 +380,9 @@ class GRU(Recurrent):\n         self.inner_init = initializations.get(inner_init)\n         self.activation = activations.get(activation)\n         self.inner_activation = activations.get(inner_activation)\n-        self.W_regularizer = W_regularizer\n-        self.U_regularizer = U_regularizer\n-        self.b_regularizer = b_regularizer\n+        self.W_regularizer = regularizers.get(W_regularizer)\n+        self.U_regularizer = regularizers.get(U_regularizer)\n+        self.b_regularizer = regularizers.get(b_regularizer)\n         self.dropout_W, self.dropout_U = dropout_W, dropout_U\n         super(GRU, self).__init__(**kwargs)\n \n@@ -409,19 +409,22 @@ class GRU(Recurrent):\n                                    name='{}_U_h'.format(self.name))\n         self.b_h = K.zeros((self.output_dim,), name='{}_b_h'.format(self.name))\n \n-        def append_regulariser(input_regulariser, param, regularizers_list):\n-            regulariser = regularizers.get(input_regulariser)\n-            if regulariser:\n-                regulariser.set_param(param)\n-                regularizers_list.append(regulariser)\n-\n         self.regularizers = []\n-        for W in [self.W_z, self.W_r, self.W_h]:\n-            append_regulariser(self.W_regularizer, W, self.regularizers)\n-        for U in [self.U_z, self.U_r, self.U_h]:\n-            append_regulariser(self.U_regularizer, U, self.regularizers)\n-        for b in [self.b_z, self.b_r, self.b_h]:\n-            append_regulariser(self.b_regularizer, b, self.regularizers)\n+        if self.W_regularizer:\n+            self.W_regularizer.set_param(K.concatenate([self.W_z,\n+                                                        self.W_r,\n+                                                        self.W_h]))\n+            self.regularizers.append(self.W_regularizer)\n+        if self.U_regularizer:\n+            self.U_regularizer.set_param(K.concatenate([self.U_z,\n+                                                        self.U_r,\n+                                                        self.U_h]))\n+            self.regularizers.append(self.U_regularizer)\n+        if self.b_regularizer:\n+            self.b_regularizer.set_param(K.concatenate([self.b_z,\n+                                                        self.b_r,\n+                                                        self.b_h]))\n+            self.regularizers.append(self.b_regularizer)\n \n         self.trainable_weights = [self.W_z, self.U_z, self.b_z,\n                                   self.W_r, self.U_r, self.b_r,\n@@ -552,9 +555,9 @@ class LSTM(Recurrent):\n         self.forget_bias_init = initializations.get(forget_bias_init)\n         self.activation = activations.get(activation)\n         self.inner_activation = activations.get(inner_activation)\n-        self.W_regularizer = W_regularizer\n-        self.U_regularizer = U_regularizer\n-        self.b_regularizer = b_regularizer\n+        self.W_regularizer = regularizers.get(W_regularizer)\n+        self.U_regularizer = regularizers.get(U_regularizer)\n+        self.b_regularizer = regularizers.get(b_regularizer)\n         self.dropout_W, self.dropout_U = dropout_W, dropout_U\n         super(LSTM, self).__init__(**kwargs)\n \n@@ -594,19 +597,25 @@ class LSTM(Recurrent):\n                                    name='{}_U_o'.format(self.name))\n         self.b_o = K.zeros((self.output_dim,), name='{}_b_o'.format(self.name))\n \n-        def append_regulariser(input_regulariser, param, regularizers_list):\n-            regulariser = regularizers.get(input_regulariser)\n-            if regulariser:\n-                regulariser.set_param(param)\n-                regularizers_list.append(regulariser)\n-\n         self.regularizers = []\n-        for W in [self.W_i, self.W_f, self.W_c, self.W_o]:\n-            append_regulariser(self.W_regularizer, W, self.regularizers)\n-        for U in [self.U_i, self.U_f, self.U_c, self.U_o]:\n-            append_regulariser(self.U_regularizer, U, self.regularizers)\n-        for b in [self.b_i, self.b_f, self.b_c, self.b_o]:\n-            append_regulariser(self.b_regularizer, b, self.regularizers)\n+        if self.W_regularizer:\n+            self.W_regularizer.set_param(K.concatenate([self.W_i,\n+                                                        self.W_f,\n+                                                        self.W_c,\n+                                                        self.W_o]))\n+            self.regularizers.append(self.W_regularizer)\n+        if self.U_regularizer:\n+            self.U_regularizer.set_param(K.concatenate([self.U_i,\n+                                                        self.U_f,\n+                                                        self.U_c,\n+                                                        self.U_o]))\n+            self.regularizers.append(self.U_regularizer)\n+        if self.b_regularizer:\n+            self.b_regularizer.set_param(K.concatenate([self.b_i,\n+                                                        self.b_f,\n+                                                        self.b_c,\n+                                                        self.b_o]))\n+            self.regularizers.append(self.b_regularizer)\n \n         self.trainable_weights = [self.W_i, self.U_i, self.b_i,\n                                   self.W_c, self.U_c, self.b_c,\n\n@@ -5,6 +5,7 @@ from numpy.testing import assert_allclose\n from keras.layers import recurrent, embeddings\n from keras.models import Sequential\n from keras.layers.core import Masking\n+from keras import regularizers\n \n from keras import backend as K\n from keras.models import Sequential, model_from_json\n@@ -108,6 +109,15 @@ def _runner(layer_class):\n \n     assert_allclose(out7, out6, atol=1e-5)\n \n+    # check regularizers\n+    layer = layer_class(output_dim, return_sequences=ret_seq, weights=None,\n+                        batch_input_shape=(nb_samples, timesteps, embedding_dim),\n+                        W_regularizer=regularizers.WeightRegularizer(l1=0.01),\n+                        U_regularizer=regularizers.WeightRegularizer(l1=0.01),\n+                        b_regularizer='l2')\n+    layer.input = K.variable(np.ones((nb_samples, timesteps, embedding_dim)))\n+    out = K.eval(layer.get_output(train=True))\n+\n \n def test_SimpleRNN():\n     _runner(recurrent.SimpleRNN)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e72bb9506ab89ac250509cbc6ce0e7ee58e581fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 4 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 496 | Contributors (this commit): 5 | Commits (past 90d): 9 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -66,7 +66,7 @@ batch_size = 128\n nb_classes = 10\n nb_epoch = 40\n \n-# the data, shuffled and split between tran and test sets\n+# the data, shuffled and split between train and test sets\n (X_train, y_train), (X_test, y_test) = mnist.load_data()\n \n X_train = X_train.reshape(60000, 784)\n\n@@ -29,7 +29,7 @@ nb_pool = 2\n # convolution kernel size\n nb_conv = 3\n \n-# the data, shuffled and split between tran and test sets\n+# the data, shuffled and split between train and test sets\n (X_train, y_train), (X_test, y_test) = mnist.load_data()\n \n X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n\n@@ -20,7 +20,7 @@ batch_size = 128\n nb_classes = 10\n nb_epoch = 20\n \n-# the data, shuffled and split between tran and test sets\n+# the data, shuffled and split between train and test sets\n (X_train, y_train), (X_test, y_test) = mnist.load_data()\n \n X_train = X_train.reshape(60000, 784)\n\n@@ -77,7 +77,7 @@ def compute_accuracy(predictions, labels):\n     return labels[predictions.ravel() < 0.5].mean()\n \n \n-# the data, shuffled and split between tran and test sets\n+# the data, shuffled and split between train and test sets\n (X_train, y_train), (X_test, y_test) = mnist.load_data()\n X_train = X_train.reshape(60000, 784)\n X_test = X_test.reshape(10000, 784)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a67034ee7c91c1a4131c9809cdd2cb0c5e23e7f0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 555 | Contributors (this commit): 10 | Commits (past 90d): 6 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -281,7 +281,7 @@ class SReLU(MaskedLayer):\n     def get_config(self):\n         return {'name': self.__class__.__name__,\n                 'input_shape': self.input_shape,\n-                't_left_init': self.t_left_init,\n-                'a_left_init': self.a_left_init,\n-                't_right_init': self.t_right_init,\n-                'a_right_init': self.a_right_init}\n+                't_left_init': self.t_left_init.__name__,\n+                'a_left_init': self.a_left_init.__name__,\n+                't_right_init': self.t_right_init.__name__,\n+                'a_right_init': self.a_right_init.__name__}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#775664fdb8aa19eb89aebb05e64514f58caf5b61", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 556 | Contributors (this commit): 10 | Commits (past 90d): 7 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -280,7 +280,6 @@ class SReLU(MaskedLayer):\n \n     def get_config(self):\n         return {'name': self.__class__.__name__,\n-                'input_shape': self.input_shape,\n                 't_left_init': self.t_left_init.__name__,\n                 'a_left_init': self.a_left_init.__name__,\n                 't_right_init': self.t_right_init.__name__,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2dcafafcf9c2b5319a03997b7b7007a43f5e72f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 8 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 402 | Contributors (this commit): 9 | Commits (past 90d): 13 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -11,14 +11,13 @@ from scipy import linalg\n \n from os import listdir\n from os.path import isfile, join\n-import random\n import math\n from six.moves import range\n import threading\n \n \n def random_rotation(x, rg, fill_mode='nearest', cval=0.):\n-    angle = random.uniform(-rg, rg)\n+    angle = np.random.uniform(-rg, rg)\n     x = ndimage.interpolation.rotate(x, angle,\n                                      axes=(1, 2),\n                                      reshape=False,\n@@ -31,9 +30,9 @@ def random_shift(x, wrg, hrg, fill_mode='nearest', cval=0.):\n     shift_x = shift_y = 0\n \n     if wrg:\n-        shift_x = random.uniform(-wrg, wrg) * x.shape[2]\n+        shift_x = np.random.uniform(-wrg, wrg) * x.shape[2]\n     if hrg:\n-        shift_y = random.uniform(-hrg, hrg) * x.shape[1]\n+        shift_y = np.random.uniform(-hrg, hrg) * x.shape[1]\n     x = ndimage.interpolation.shift(x, (0, shift_y, shift_x),\n                                     order=0,\n                                     mode=fill_mode,\n@@ -59,7 +58,7 @@ def random_barrel_transform(x, intensity):\n \n \n def random_shear(x, intensity, fill_mode='nearest', cval=0.):\n-    shear = random.uniform(-intensity, intensity)\n+    shear = np.random.uniform(-intensity, intensity)\n     shear_matrix = np.array([[1.0, -math.sin(shear), 0.0],\n                             [0.0, math.cos(shear), 0.0],\n                             [0.0, 0.0, 1.0]])\n@@ -76,8 +75,8 @@ def random_channel_shift(x, rg):\n \n \n def random_zoom(x, rg, fill_mode='nearest', cval=0.):\n-    zoom_w = random.uniform(1.-rg, 1.)\n-    zoom_h = random.uniform(1.-rg, 1.)\n+    zoom_w = np.random.uniform(1.-rg, 1.)\n+    zoom_h = np.random.uniform(1.-rg, 1.)\n     x = ndimage.interpolation.zoom(x, zoom=(1., zoom_w, zoom_h),\n                                    mode=fill_mode,\n                                    cval=cval)\n@@ -253,10 +252,10 @@ class ImageDataGenerator(object):\n         if self.width_shift_range or self.height_shift_range:\n             x = random_shift(x, self.width_shift_range, self.height_shift_range)\n         if self.horizontal_flip:\n-            if random.random() < 0.5:\n+            if np.random.random() < 0.5:\n                 x = horizontal_flip(x)\n         if self.vertical_flip:\n-            if random.random() < 0.5:\n+            if np.random.random() < 0.5:\n                 x = vertical_flip(x)\n         if self.shear_range:\n             x = random_shear(x, self.shear_range)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1145fec39ff7f665f7bd352261549f8738de815e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 61 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 10 | Methods Changed: 11 | Complexity Δ (Sum/Max): 9/5 | Churn Δ: 64 | Churn Cumulative: 2508 | Contributors (this commit): 21 | Commits (past 90d): 70 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,6 +15,9 @@ def set_epsilon(e):\n \n \n def floatx():\n+    '''Returns the default float type, as a string\n+    (e.g. 'float32', 'float64').\n+    '''\n     return _FLOATX\n \n \n\n@@ -7,6 +7,12 @@ from .common import _FLOATX, _EPSILON\n # INTERNAL UTILS\n \n _SESSION = None\n+_LEARNING_PHASE = tf.placeholder(dtype='uint8')  # 0 = test, 1 = train\n+\n+\n+def learning_phase():\n+    # False = test, True = train\n+    return _LEARNING_PHASE\n \n \n def get_session():\n@@ -36,8 +42,11 @@ def variable(value, dtype=_FLOATX, name=None):\n def placeholder(shape=None, ndim=None, dtype=_FLOATX, name=None):\n     if not shape:\n         if ndim:\n-            shape = [None for _ in range(ndim)]\n-    return tf.placeholder(dtype, shape=shape, name=name)\n+            shape = tuple([None for _ in range(ndim)])\n+    x = tf.placeholder(dtype, shape=shape, name=name)\n+    x._keras_shape = shape\n+    x._uses_learning_phase = False\n+    return x\n \n \n def shape(x):\n@@ -54,6 +63,10 @@ def ndim(x):\n     return len(x.get_shape())\n \n \n+def dtype(x):\n+    return x.dtype.name\n+\n+\n def eval(x):\n     '''Run a graph.\n     '''\n@@ -534,6 +547,21 @@ def switch(condition, then_expression, else_expression):\n                                            lambda: else_expression)\n \n \n+def in_train_phase(x, expression):\n+    x = tf.python.control_flow_ops.cond(_LEARNING_PHASE,\n+                                        lambda: x,\n+                                        lambda: expression)\n+    x._uses_learning_phase = True\n+    return x\n+\n+\n+def in_test_phase(x, expression):\n+    x = tf.python.control_flow_ops.cond(_LEARNING_PHASE,\n+                                        lambda: expression,\n+                                        lambda: x)\n+    x._uses_learning_phase = True\n+    return x\n+\n # NN OPERATIONS\n \n def relu(x, alpha=0., max_value=None):\n\n@@ -10,6 +10,12 @@ from .common import _FLOATX, _EPSILON\n \n # INTERNAL UTILS\n theano.config.floatX = _FLOATX\n+_LEARNING_PHASE = T.scalar(dtype='uint8')  # 0 = test, 1 = train\n+\n+\n+def learning_phase():\n+    # False = test, True = train\n+    return _LEARNING_PHASE\n \n \n def _on_gpu():\n@@ -43,9 +49,14 @@ def placeholder(shape=None, ndim=None, dtype=_FLOATX, name=None):\n         raise Exception('Specify either a shape or ndim value.')\n     if shape is not None:\n         ndim = len(shape)\n+    else:\n+        shape = tuple([None for _ in range(ndim)])\n \n     broadcast = (False,) * ndim\n-    return T.TensorType(dtype, broadcast)(name)\n+    x = T.TensorType(dtype, broadcast)(name)\n+    x._keras_shape = shape\n+    x._uses_learning_phase = False\n+    return x\n \n \n def shape(x):\n@@ -61,6 +72,10 @@ def ndim(x):\n     return x.ndim\n \n \n+def dtype(x):\n+    return x.dtype\n+\n+\n def eval(x):\n     '''Run a graph.\n     '''\n@@ -571,6 +586,18 @@ def switch(condition, then_expression, else_expression):\n     return T.switch(condition, then_expression, else_expression)\n \n \n+def in_train_phase(x, expression):\n+    x = T.switch(_LEARNING_PHASE, x, expression)\n+    x._uses_learning_phase = True\n+    return x\n+\n+\n+def in_test_phase(x, expression):\n+    x = T.switch(_LEARNING_PHASE, expression, x)\n+    x._uses_learning_phase = True\n+    return x\n+\n+\n # NN OPERATIONS\n \n def relu(x, alpha=0., max_value=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9ac50e0050e8ae69bf68a5c69250a84db1020a5d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 5164 | Contributors (this commit): 41 | Commits (past 90d): 37 | Contributors (cumulative): 41 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1160,6 +1160,8 @@ class Sequential(Model, containers.Sequential):\n                 samples_seen += batch_size\n \n                 # epoch finished\n+                if samples_seen > samples_per_epoch:\n+                    warnings.warn('Epoch contained duplicated samples which might affect learning results. Set \"samples_per_epoch\" correctly to avoid this warning.')\n                 if samples_seen >= samples_per_epoch and do_validation:\n                     if val_gen:\n                         val_outs = self.evaluate_generator(validation_data,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3bd7d11170b47ad478f61bcb502fc63fe804abc9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 5166 | Contributors (this commit): 41 | Commits (past 90d): 37 | Contributors (cumulative): 41 | DMM Complexity: None\n\nDIFF:\n@@ -1161,7 +1161,7 @@ class Sequential(Model, containers.Sequential):\n \n                 # epoch finished\n                 if samples_seen > samples_per_epoch:\n-                    warnings.warn('Epoch contained duplicated samples which might affect learning results. Set \"samples_per_epoch\" correctly to avoid this warning.')\n+                    warnings.warn('Epoch contained too many samples which might affect learning results. Set \"samples_per_epoch\" correctly to avoid this warning.')\n                 if samples_seen >= samples_per_epoch and do_validation:\n                     if val_gen:\n                         val_outs = self.evaluate_generator(validation_data,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#404a30df88f243d0cad71821914a2025f96b94e9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 10 | Churn Cumulative: 5176 | Contributors (this commit): 42 | Commits (past 90d): 38 | Contributors (cumulative): 42 | DMM Complexity: None\n\nDIFF:\n@@ -111,24 +111,24 @@ def standardize_weights(y, sample_weight=None, class_weight=None,\n         if sample_weight_mode != 'temporal':\n             raise Exception('\"sample_weight_mode '\n                             'should be None or \"temporal\".')\n-        if y.ndim < 3:\n+        if len(y.shape) < 3:\n             raise Exception('Timestep-wise sample weighting (use of '\n                             'sample_weight_mode=\"temporal\") is restricted to '\n                             'outputs that are at least 3D, i.e. that have '\n                             'a time dimension.')\n-        if sample_weight is not None and sample_weight.ndim != 2:\n+        if sample_weight is not None and len(sample_weight.shape) != 2:\n             raise Exception('In order to use timestep-wise sample weighting, '\n                             'you should pass a 2D sample_weight array.')\n     else:\n-        if sample_weight is not None and sample_weight.ndim != 1:\n+        if sample_weight is not None and len(sample_weight.shape) != 1:\n             raise Exception('In order to use timestep-wise sample weights, '\n                             'you should specify sample_weight_mode=\"temporal\" '\n                             'in compile(). If you just mean to use '\n                             'sample-wise weights, make sure your '\n                             'sample_weight array is 1D.')\n     if sample_weight is not None:\n-        assert sample_weight.ndim <= y.ndim\n-        assert y.shape[:sample_weight.ndim] == sample_weight.shape\n+        assert len(sample_weight.shape) <= len(y.shape)\n+        assert y.shape[:len(sample_weight.shape)] == sample_weight.shape\n         return sample_weight\n     elif isinstance(class_weight, dict):\n         if len(y.shape) > 2:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6543b675096725a6ca1d05254d645d636c30d28d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 52 | Files Changed: 1 | Hunks: 8 | Methods Changed: 1 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 62 | Churn Cumulative: 1428 | Contributors (this commit): 15 | Commits (past 90d): 37 | Contributors (cumulative): 15 | DMM Complexity: 0.0\n\nDIFF:\n@@ -12,21 +12,6 @@ from .common import _FLOATX, _EPSILON\n theano.config.floatX = _FLOATX\n \n \n-def _on_gpu():\n-    '''Return whether the session is set to\n-    run on GPU or not (i.e. on CPU).\n-    '''\n-    return theano.config.device[:3] == 'gpu' or theano.sandbox.cuda.cuda_enabled\n-\n-\n-if _on_gpu():\n-    '''Import cuDNN only if running on GPU:\n-    not having Cuda installed should not\n-    prevent from running the present code.\n-    '''\n-    from theano.sandbox.cuda import dnn\n-\n-\n # VARIABLE MANIPULATION\n \n def variable(value, dtype=_FLOATX, name=None):\n@@ -644,10 +629,7 @@ def l2_normalize(x, axis):\n \n def conv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='th',\n            image_shape=None, filter_shape=None):\n-    '''\n-    Run on cuDNN if available.\n-    border_mode: string, \"same\" or \"valid\".\n-    '''\n+\n     if dim_ordering not in {'th', 'tf'}:\n         raise Exception('Unknown dim_ordering ' + str(dim_ordering))\n \n@@ -667,30 +649,8 @@ def conv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='th',\n             filter_shape = (filter_shape[3], filter_shape[2],\n                             filter_shape[0], filter_shape[1])\n \n-    if _on_gpu() and dnn.dnn_available():\n     if border_mode == 'same':\n-            np_kernel = kernel.eval()\n-            assert strides[0] <= np_kernel.shape[2], 'strides should be smaller than the convolution window.'\n-            assert strides[1] <= np_kernel.shape[3], 'strides should be smaller than the convolution window.'\n-            conv_out = dnn.dnn_conv(img=x,\n-                                    kerns=kernel,\n-                                    border_mode='full')\n-            shift_x = (np_kernel.shape[2] - strides[0]) // 2\n-            shift_y = (np_kernel.shape[3] - strides[1]) // 2\n-            expected_width = (x.shape[2] + strides[0] - 1) // strides[0]\n-            expected_height = (x.shape[3] + strides[1] - 1) // strides[1]\n-\n-            conv_out = conv_out[:, :,\n-                                shift_x: shift_x + expected_width,\n-                                shift_y: shift_y + expected_height]\n-        else:\n-            conv_out = dnn.dnn_conv(img=x,\n-                                    kerns=kernel,\n-                                    border_mode=border_mode,\n-                                    subsample=strides)\n-    else:\n-        if border_mode == 'same':\n-            th_border_mode = 'full'\n+        th_border_mode = 'half'\n         np_kernel = kernel.eval()\n         assert strides[0] <= np_kernel.shape[2], 'strides should be smaller than the convolution window.'\n         assert strides[1] <= np_kernel.shape[3], 'strides should be smaller than the convolution window.'\n@@ -699,20 +659,18 @@ def conv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='th',\n     else:\n         raise Exception('Border mode not supported: ' + str(border_mode))\n \n-        conv_out = T.nnet.conv.conv2d(x, kernel,\n+    conv_out = T.nnet.conv2d(x, kernel,\n                              border_mode=th_border_mode,\n                              subsample=strides,\n-                                      image_shape=image_shape,\n+                             input_shape=image_shape,\n                              filter_shape=filter_shape)\n-        if border_mode == 'same':\n-            shift_x = (np_kernel.shape[2] - strides[0]) // 2\n-            shift_y = (np_kernel.shape[3] - strides[1]) // 2\n-            expected_width = (x.shape[2] + strides[0] - 1) // strides[0]\n-            expected_height = (x.shape[3] + strides[1] - 1) // strides[1]\n     \n-            conv_out = conv_out[:, :,\n-                                shift_x: shift_x + expected_width,\n-                                shift_y: shift_y + expected_height]\n+    if border_mode == 'same':\n+        if np_kernel.shape[2] % 2 == 0:\n+            conv_out = conv_out[:,:,:-1,:]\n+        if np_kernel.shape[3] % 2 == 0:\n+            conv_out = conv_out[:,:,:,:-1]\n+\n     if dim_ordering == 'tf':\n         conv_out = conv_out.dimshuffle((0, 2, 3, 1))\n     return conv_out\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3dd27c61fb1a503372aff9ef39be0a84030b8b9d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 21 | Churn Cumulative: 1449 | Contributors (this commit): 15 | Commits (past 90d): 38 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -629,7 +629,9 @@ def l2_normalize(x, axis):\n \n def conv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='th',\n            image_shape=None, filter_shape=None):\n-\n+    '''\n+    border_mode: string, \"same\" or \"valid\".\n+    '''\n     if dim_ordering not in {'th', 'tf'}:\n         raise Exception('Unknown dim_ordering ' + str(dim_ordering))\n \n@@ -659,6 +661,17 @@ def conv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='th',\n     else:\n         raise Exception('Border mode not supported: ' + str(border_mode))\n \n+    # Theano might not accept like longs\n+    def int_or_none(value):\n+        try: return int(value)\n+        except TypeError: return None\n+\n+    if image_shape is not None:\n+        image_shape = tuple(int_or_none(v) for v in image_shape)\n+\n+    if filter_shape is not None:\n+        filter_shape = tuple(int_or_none(v) for v in filter_shape)\n+\n     conv_out = T.nnet.conv2d(x, kernel,\n                              border_mode=th_border_mode,\n                              subsample=strides,\n@@ -666,10 +679,12 @@ def conv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='th',\n                              filter_shape=filter_shape)\n \n     if border_mode == 'same':\n+        slices = [slice(None)] * 4\n         if np_kernel.shape[2] % 2 == 0:\n-            conv_out = conv_out[:,:,:-1,:]\n+            slices[2] = slice((x.shape[2]+strides[0]-1) // strides[0])\n         if np_kernel.shape[3] % 2 == 0:\n-            conv_out = conv_out[:,:,:,:-1]\n+            slices[3] = slice((x.shape[3]+strides[1]-1) // strides[1])\n+        conv_out = conv_out[slices]\n \n     if dim_ordering == 'tf':\n         conv_out = conv_out.dimshuffle((0, 2, 3, 1))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#63c099714b7cc299aa44cc5d19a6fbc1df180b0e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1455 | Contributors (this commit): 15 | Commits (past 90d): 39 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -663,8 +663,10 @@ def conv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='th',\n \n     # Theano might not accept like longs\n     def int_or_none(value):\n-        try: return int(value)\n-        except TypeError: return None\n+        try:\n+            return int(value)\n+        except TypeError:\n+            return None\n \n     if image_shape is not None:\n         image_shape = tuple(int_or_none(v) for v in image_shape)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0f4c7864ba8c144ba626979d47087e5d99946cef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1461 | Contributors (this commit): 15 | Commits (past 90d): 40 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -681,12 +681,10 @@ def conv2d(x, kernel, strides=(1, 1), border_mode='valid', dim_ordering='th',\n                              filter_shape=filter_shape)\n \n     if border_mode == 'same':\n-        slices = [slice(None)] * 4\n         if np_kernel.shape[2] % 2 == 0:\n-            slices[2] = slice((x.shape[2]+strides[0]-1) // strides[0])\n+            conv_out = conv_out[:,:,:(x.shape[2]+strides[0]-1) // strides[0],:]\n         if np_kernel.shape[3] % 2 == 0:\n-            slices[3] = slice((x.shape[3]+strides[1]-1) // strides[1])\n-        conv_out = conv_out[slices]\n+            conv_out = conv_out[:,:,:,:(x.shape[3]+strides[1]-1) // strides[1]]\n \n     if dim_ordering == 'tf':\n         conv_out = conv_out.dimshuffle((0, 2, 3, 1))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#677e9ee8ba5f2770fa42a3ddd6c25a3f2d4d19c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 194 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -20,7 +20,13 @@ import random\n import sys\n \n path = get_file('nietzsche.txt', origin=\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\")\n+\n+try: \n     text = open(path).read().lower()\n+except UnicodeDecodeError:\n+    import codecs\n+    text = codecs.open(path, encoding='utf-8').read().lower()\n+\n print('corpus length:', len(text))\n \n chars = set(text)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a19db7b672ea8d43e8ed398cb1cbf5733a514418", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 16 | Files Changed: 4 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/4 | Churn Δ: 48 | Churn Cumulative: 11102 | Contributors (this commit): 57 | Commits (past 90d): 123 | Contributors (cumulative): 80 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -93,6 +93,20 @@ def dot(x, y):\n     return tf.matmul(x, y)\n \n \n+def batch_dot(x, y, axes=None):\n+    if ndim(x) != 3 or ndim(y) != 3:\n+        raise Exception('batch_dot supports only 3D tensor. ' +\n+                        'Input1 dim is {}, Input2 ndim is {}.'.format(\n+                            ndim(x), ndim(y)))\n+    if axes:\n+        adj_x = None if axes[0][0] == 2 else True\n+        adj_y = True if axes[1][0] == 2 else None\n+    else:\n+        adj_x = None\n+        adj_y = None\n+    return tf.batch_matmul(x, y, adj_x=adj_x, adj_y=adj_y)\n+\n+\n def transpose(x):\n     return tf.transpose(x)\n \n\n@@ -111,6 +111,17 @@ def dot(x, y):\n     return T.dot(x, y)\n \n \n+def batch_dot(x, y, axes=None):\n+    if ndim(x) != 3 or ndim(y) != 3:\n+        raise Exception('batch_dot supports only 3D tensor. ' +\n+                        'Input1 dim is {}, Input2 ndim is {}.'.format(\n+                            ndim(x), ndim(y)))\n+    if axes is None:\n+        # behaves like tf.batch_matmul as default\n+        axes = [(2,), (1,)]\n+    return T.batched_tensordot(x, y, axes=axes)\n+\n+\n def transpose(x):\n     return T.transpose(x)\n \n\n@@ -423,9 +423,6 @@ class Merge(Layer):\n         dot_axes: axis or axes to use in `dot` mode\n             (see [the Numpy documentation](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.tensordot.html) for more details).\n \n-    # TensorFlow warning\n-        `dot` mode only works with Theano for the time being.\n-\n     # Examples\n \n     ```python\n@@ -464,9 +461,6 @@ class Merge(Layer):\n                                 'be merged using ' + mode + ' mode. ' +\n                                 'Layer shapes: %s' % ([l.output_shape for l in layers]))\n         if mode in {'cos', 'dot'}:\n-            if K._BACKEND != 'theano':\n-                raise Exception('\"' + mode + '\" merge mode will only work with Theano.')\n-\n             if len(layers) > 2:\n                 raise Exception(mode + ' merge takes exactly 2 layers')\n             shape1 = layers[0].output_shape\n@@ -583,23 +577,18 @@ class Merge(Layer):\n                 s *= self.layers[i].get_output(train)\n             return s\n         elif self.mode == 'dot':\n-            if K._BACKEND != 'theano':\n-                raise Exception('\"dot\" merge mode will only work with Theano.')\n-            from theano import tensor as T\n             l1 = self.layers[0].get_output(train)\n             l2 = self.layers[1].get_output(train)\n-            output = T.batched_tensordot(l1, l2, self.dot_axes)\n+            output = K.tensordot(l1, l2, self.dot_axes)  # T.batched_tensordot(l1, l2, self.dot_axes)\n             output_shape = list(self.output_shape)\n-            output_shape[0] = l1.shape[0]\n-            output = output.reshape(tuple(output_shape))\n+            output_shape[0] = -1\n+            output = K.reshape(output, (tuple(output_shape)))\n             return output\n         elif self.mode == 'cos':\n-            if K._BACKEND != 'theano':\n-                raise Exception('\"cos\" merge mode will only work with Theano.')\n-            from theano import tensor as T\n             l1 = self.layers[0].get_output(train)\n             l2 = self.layers[1].get_output(train)\n-            output = T.batched_tensordot(l1, l2, self.dot_axes) / T.sqrt(T.batched_tensordot(l1, l1, self.dot_axes) * T.batched_tensordot(l2, l2, self.dot_axes))\n+            output = K.tensordot(l1, l2, self.dot_axes) / K.sqrt(\n+                K.tensordot(l1, l1, self.dot_axes) * K.tensordot(l2, l2, self.dot_axes))\n             output = output.dimshuffle((0, 'x'))\n             return output\n         else:\n\n@@ -40,6 +40,8 @@ class TestBackend(object):\n \n     def test_linear_operations(self):\n         check_two_tensor_operation('dot', (4, 2), (2, 4))\n+        check_two_tensor_operation('batch_dot', (4, 2, 3), (4, 5, 3),\n+                                   axes=((2,), (2,)))\n         check_single_tensor_operation('transpose', (4, 2))\n \n     def test_shape_operations(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
