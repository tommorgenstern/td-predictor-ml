{"custom_id": "keras#7cc2c050bda816c6a072a26c398e983607f60d85", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 224 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -107,14 +107,14 @@ class KerasPremadeModelsTest(tf.test.TestCase, parameterized.TestCase):\n       model.compile(opt, 'mse')\n       if use_dataset_creator:\n         x = dataset_creator.DatasetCreator(dataset_fn)\n-        hist = model.fit(x, epochs=5, steps_per_epoch=INPUT_SIZE)\n+        hist = model.fit(x, epochs=3, steps_per_epoch=INPUT_SIZE)\n       else:\n         if data_fn == 'numpy':\n           inputs, output = get_numpy()\n-          hist = model.fit(inputs, output, epochs=5)\n+          hist = model.fit(inputs, output, epochs=3)\n         else:\n-          hist = model.fit(get_dataset(), epochs=5)\n-        self.assertLess(hist.history['loss'][4], 0.2)\n+          hist = model.fit(get_dataset(), epochs=3)\n+        self.assertLess(hist.history['loss'][2], 0.2)\n \n   @tf.__internal__.distribute.combinations.generate(\n       strategy_combinations_eager_data_fn())\n@@ -140,14 +140,14 @@ class KerasPremadeModelsTest(tf.test.TestCase, parameterized.TestCase):\n \n       if use_dataset_creator:\n         x = dataset_creator.DatasetCreator(dataset_fn)\n-        hist = wide_deep_model.fit(x, epochs=5, steps_per_epoch=INPUT_SIZE)\n+        hist = wide_deep_model.fit(x, epochs=3, steps_per_epoch=INPUT_SIZE)\n       else:\n         if data_fn == 'numpy':\n           inputs, output = get_numpy()\n-          hist = wide_deep_model.fit(inputs, output, epochs=5)\n+          hist = wide_deep_model.fit(inputs, output, epochs=3)\n         else:\n-          hist = wide_deep_model.fit(get_dataset(), epochs=5)\n-      self.assertLess(hist.history['loss'][4], 0.2)\n+          hist = wide_deep_model.fit(get_dataset(), epochs=3)\n+      self.assertLess(hist.history['loss'][2], 0.2)\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3c63251dde6ea9df2787952a9687fbf14366e5de", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 125 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -21,11 +21,14 @@ from keras.layers import deserialize as deserialize_layer\n from keras.models import Model\n from keras.utils import generic_utils\n import tensorflow.compat.v2 as tf\n+# pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.util.tf_export import keras_export\n \n # pylint: disable=g-classes-have-attributes\n \n \n @generic_utils.register_keras_serializable()\n+@keras_export(\"keras.models.experimental.SharpnessAwareMinimization\", v1=[])\n class SharpnessAwareMinimization(Model):\n   \"\"\"Sharpness aware minimization (SAM) training flow.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3a3d7b6c59e8f46eeeac617c484cd0d11a3c5c58", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 23 | Churn Cumulative: 1188 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -97,6 +97,29 @@ class Normalization(base_preprocessing_layer.PreprocessingLayer):\n   array([[-1.4142135 ],\n          [-0.70710677],\n          [ 0.        ]], dtype=float32)>\n+\n+  Using it in the invert manner for denormalizing the inputs with calculating a mean and variance for each index on the last axis.\n+\n+  >>> adapt_data = np.array([[0., 7., 4.],\n+  ...                        [2., 9., 6.],\n+  ...                        [0., 7., 4.],\n+  ...                        [2., 9., 6.]], dtype='float32')\n+  >>> input_data = np.array([[1., 2., 3.]], dtype='float32')\n+  >>> layer = tf.keras.layers.Normalization(axis=-1, invert=True)\n+  >>> layer.adapt(adapt_data)\n+  >>> layer(input_data)\n+  <tf.Tensor: shape=(1, 3), dtype=float32, numpy=\n+  array([2., 10., 8.], dtype=float32)>\n+\n+  Using it in the invert manner for denormalizing the inputs with passing the mean and variance directly.\n+  \n+  >>> input_data = np.array([[-1.4142135], [-0.70710677], [0.]], dtype='float32')\n+  >>> layer = tf.keras.layers.Normalization(mean=3., variance=2., invert=True)\n+  >>> layer(input_data)\n+  <tf.Tensor: shape=(3, 1), dtype=float32, numpy=\n+  array([[1. ],\n+         [2. ],\n+         [3. ]], dtype=float32)>  \n   \"\"\"\n \n   def __init__(self, axis=-1, mean=None, variance=None, invert=False, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c15adf9944558b25fc74e14827c9720fbf00be5f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 872 | Contributors (this commit): 8 | Commits (past 90d): 3 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -107,7 +107,7 @@ class LossesContainer(Container):\n     self._user_losses = losses\n     self._user_loss_weights = loss_weights\n \n-    self._losses = copy.copy(losses)\n+    self._losses = losses\n     self._loss_weights = loss_weights\n     self._per_output_metrics = None  # Per-output losses become metrics.\n     self._loss_metric = metrics_mod.Mean(name='loss')  # Total loss.\n@@ -309,7 +309,7 @@ class MetricsContainer(Container):\n     self._user_metrics = metrics\n     self._user_weighted_metrics = weighted_metrics\n \n-    self._metrics = copy.copy(metrics)\n+    self._metrics = metrics\n     self._weighted_metrics = weighted_metrics\n     self._built = False\n \n@@ -706,6 +706,7 @@ def map_missing_dict_keys(y_pred, struct):\n   \"\"\"Replaces missing dict keys in `struct` with `None` placeholders.\"\"\"\n   if not isinstance(y_pred, dict) or not isinstance(struct, dict):\n     return struct\n+  struct = copy.copy(struct)\n   for k in y_pred.keys():\n     if k not in struct:\n       struct[k] = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e61cbc52fd3b0170769c120e9b8dabc8c4205322", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 311 | Lines Deleted: 134 | Files Changed: 3 | Hunks: 73 | Methods Changed: 44 | Complexity Δ (Sum/Max): 26/23 | Churn Δ: 445 | Churn Cumulative: 3539 | Contributors (this commit): 17 | Commits (past 90d): 10 | Contributors (cumulative): 28 | DMM Complexity: 0.4\n\nDIFF:\n@@ -15,13 +15,17 @@\n # ==============================================================================\n \"\"\"Tests for ClusterCoordinator and Keras models.\"\"\"\n \n+from absl.testing import parameterized\n+\n import keras\n from keras.distribute import multi_worker_testing_utils\n+from keras.distribute import strategy_combinations\n from keras.engine import base_layer\n+import numpy as np\n import tensorflow.compat.v2 as tf\n \n \n-class ShardedVariableTest(tf.test.TestCase):\n+class ShardedVariableTest(tf.test.TestCase, parameterized.TestCase):\n \n   @classmethod\n   def setUpClass(cls):\n@@ -132,30 +136,133 @@ class ShardedVariableTest(tf.test.TestCase):\n     self.assertTrue(hasattr(auc.true_positives, 'variables'))\n     self.assertTrue(hasattr(fp.accumulator, 'variables'))\n \n-  def test_saved_model(self):\n+  @tf.__internal__.distribute.combinations.generate(\n+      tf.__internal__.test.combinations.combine(\n+          shard_config=[[2, 2], [2, 3], [3, 2], [2, 1], [1, 1], [1, 2], [1, 3]],\n+          model_type=['dense', 'embedding'],\n+      ))\n+  def test_saved_model_combined(self, shard_config, model_type):\n+    \"\"\"Test saving and loading models with various fixed numbers of shards.\n \n-    def create_model():\n-      inputs = keras.layers.Input(shape=(4,))\n-      outputs = keras.layers.Dense(2)(inputs)\n+    Args:\n+      shard_config: The number of shards to use per variable before and after\n+        loading. For example, [1, 3] means to create and save the model with 1\n+        shard (i.e., no variable partitioning), and load it into 3 shards per\n+        variable.\n+      model_type: Either 'dense' or 'embedding', which simple model to test.\n+    \"\"\"\n+\n+    def create_embedding_model():\n+      inputs = keras.layers.Input(shape=(6,))\n+      embedding = keras.layers.Embedding(output_dim=2, input_dim=6)\n+      outputs = embedding(inputs)\n       model = keras.Model(inputs, outputs)\n       model.compile(optimizer='adam', loss='mean_squared_error')\n       return model\n \n-    with self.strategy.scope():\n-      model = create_model()\n+    def create_dense_model():\n+      inputs = keras.layers.Input(shape=(6,))\n+      outputs = keras.layers.Dense(6)(inputs)\n+      model = keras.Model(inputs, outputs)\n+      model.compile(optimizer='adam', loss='mean_squared_error')\n+      return model\n+\n+    # Maybe create new strategy with different number of shards\n+    if shard_config[0] > 2:\n+      strategy = tf.distribute.experimental.ParameterServerStrategy(\n+          multi_worker_testing_utils.make_parameter_server_cluster(3, 3),\n+          variable_partitioner=tf.distribute.experimental.partitioners\n+          .FixedShardsPartitioner(shard_config[0]))\n+    elif shard_config[0] == 2:\n+      strategy = self.strategy\n+    else:\n+      # Just one shard, so use default strategy\n+      strategy = tf.distribute.get_strategy()\n+\n+    x = tf.cast(tf.expand_dims(tf.range(6), 0), tf.float32)\n+    with strategy.scope():\n+      model = (\n+          create_dense_model()\n+          if model_type == 'dense' else create_embedding_model())\n+      expect = model(x)\n+\n+    # Dense layers have two variables (kernel and bias), embedding layers have 1\n+    n_expected_variables = shard_config[0] * (2 if model_type == 'dense' else 1)\n+    self.assertLen(model.variables, n_expected_variables)\n+    model_weights = [v.numpy() for v in model.variables]\n \n-    inputs = tf.random.normal(shape=(8, 4))\n-    expect = model(inputs)\n     saved_dir = self.get_temp_dir()\n     model.save(saved_dir)\n \n-    loaded_model = keras.models.load_model(saved_dir)\n-    got = loaded_model(inputs)\n-    self.assertAllClose(got, expect)\n-    self.assertGreater(len(model.variables), len(loaded_model.variables))\n+    if shard_config[1] > 2:\n+      strategy2 = tf.distribute.experimental.ParameterServerStrategy(\n+          multi_worker_testing_utils.make_parameter_server_cluster(3, 3),\n+          variable_partitioner=tf.distribute.experimental.partitioners\n+          .FixedShardsPartitioner(shard_config[1]))\n+    elif shard_config[1] == 2:\n+      strategy2 = self.strategy\n+    else:\n+      # Just one shard, so use default strategy\n+      strategy2 = tf.distribute.get_strategy()\n \n+    with strategy2.scope():\n+      loaded_model = keras.models.load_model(saved_dir)\n+      got = loaded_model(x)\n+\n+      self.assertAllClose(got, expect)\n+      n_expected_variables = shard_config[1] * (2\n+                                                if model_type == 'dense' else 1)\n+      self.assertLen(loaded_model.variables, n_expected_variables)\n+      loaded_model_weights = [v.numpy() for v in loaded_model.variables]\n+      self.assertAllClose(\n+          np.concatenate([w.flatten() for w in model_weights]),\n+          np.concatenate([w.flatten() for w in loaded_model_weights]))\n+\n+  @tf.__internal__.distribute.combinations.generate(\n+      tf.__internal__.test.combinations.combine(\n+          distribution=strategy_combinations.strategies_minus_tpu,\n+          model_type=['dense', 'embedding'],\n+      ))\n+  def test_saved_model_load_non_pss(self, model_type, distribution):\n+\n+    def create_embedding_model():\n+      inputs = keras.layers.Input(shape=(6,))\n+      embedding = keras.layers.Embedding(output_dim=2, input_dim=6)\n+      outputs = embedding(inputs)\n+      model = keras.Model(inputs, outputs)\n+      model.compile(optimizer='adam', loss='mean_squared_error')\n+      return model\n+\n+    def create_dense_model():\n+      inputs = keras.layers.Input(shape=(6,))\n+      outputs = keras.layers.Dense(6)(inputs)\n+      model = keras.Model(inputs, outputs)\n+      model.compile(optimizer='adam', loss='mean_squared_error')\n+      return model\n+\n+    x = tf.cast(tf.expand_dims(tf.range(6), 0), tf.float32)\n     with self.strategy.scope():\n-      keras.models.load_model(saved_dir)\n+      model = (\n+          create_dense_model()\n+          if model_type == 'dense' else create_embedding_model())\n+      expect = model(x)\n+\n+    model_weights = [v.numpy() for v in model.variables]\n+\n+    saved_dir = self.get_temp_dir()\n+    model.save(saved_dir)\n+\n+    with distribution.scope():\n+      loaded_model = keras.models.load_model(saved_dir)\n+      got = loaded_model(x)\n+\n+      self.assertAllClose(got, expect)\n+      n_expected_variables = 2 if model_type == 'dense' else 1\n+      self.assertLen(loaded_model.variables, n_expected_variables)\n+      loaded_model_weights = [v.numpy() for v in loaded_model.variables]\n+      self.assertAllClose(\n+          np.concatenate([w.flatten() for w in model_weights]),\n+          np.concatenate([w.flatten() for w in loaded_model_weights]))\n \n   def test_slot_variable_checkpointing(self):\n \n@@ -258,10 +365,56 @@ class ShardedVariableTest(tf.test.TestCase):\n     post_ckpt_slots = []\n     for slot in new_ckpt.opt.get_slot_names():\n       post_ckpt_slots.extend(\n-          tf.concat(list(new_ckpt.opt.get_slot(var, slot)),\n-                    axis=0).numpy())\n+          tf.concat(list(new_ckpt.opt.get_slot(var, slot)), axis=0).numpy())\n     self.assertAllClose(pre_ckpt_slots, post_ckpt_slots)\n \n+\n+class ShardedVariableMixedPartitioningTest(tf.test.TestCase):\n+\n+  def test_saved_model_min_size_partitioner(self):\n+\n+    # set min_shard_bytes such that Dense kernel is split into 2 and bias into 1\n+    partitioner = tf.distribute.experimental.partitioners.MinSizePartitioner(\n+        min_shard_bytes=(6 * 6 * 4) // 2, max_shards=2)\n+\n+    cluster_resolver = multi_worker_testing_utils.make_parameter_server_cluster(\n+        3, 2)\n+    strategy = tf.distribute.experimental.ParameterServerStrategy(\n+        cluster_resolver, variable_partitioner=partitioner)\n+\n+    def create_dense_model():\n+      inputs = keras.layers.Input(shape=(6,))\n+      outputs = keras.layers.Dense(6)(inputs)\n+      model = keras.Model(inputs, outputs)\n+      model.compile(optimizer='adam', loss='mean_squared_error')\n+      return model\n+\n+    x = tf.cast(tf.expand_dims(tf.range(6), 0), tf.float32)\n+    with strategy.scope():\n+      model = create_dense_model()\n+      expect = model(x)\n+\n+    # 2 kernel variables, 1 bias\n+    self.assertLen(model.variables, 3)\n+\n+    saved_dir = self.get_temp_dir()\n+    model.save(saved_dir)\n+\n+    # set min_shard_bytes such that Dense kernel is split into 3 and bias into 1\n+    partitioner2 = tf.distribute.experimental.partitioners.MinSizePartitioner(\n+        min_shard_bytes=(6 * 6 * 4) // 3, max_shards=3)\n+    strategy2 = tf.distribute.experimental.ParameterServerStrategy(\n+        cluster_resolver, variable_partitioner=partitioner2)\n+\n+    with strategy2.scope():\n+      loaded_model = keras.models.load_model(saved_dir)\n+      got = loaded_model(x)\n+\n+      self.assertAllClose(got, expect)\n+      # 3 kernel variables, 1 bias\n+      self.assertLen(loaded_model.variables, 4)\n+\n+\n if __name__ == '__main__':\n   tf.compat.v1.enable_v2_behavior()\n   tf.test.main()\n\n@@ -16,6 +16,7 @@\n \n import re\n import types\n+import warnings\n \n from keras import backend\n from keras import regularizers\n@@ -42,34 +43,19 @@ from google.protobuf import message\n # TODO(b/134426265): Switch back to single-quotes to match the rest of the file\n # once the issue with copybara is fixed.\n # pylint:disable=g-inconsistent-quotes\n-models_lib = LazyLoader(\"models_lib\", globals(),\n-                        \"keras.models\")\n-base_layer = LazyLoader(\n-    \"base_layer\", globals(),\n-    \"keras.engine.base_layer\")\n-layers_module = LazyLoader(\n-    \"layers_module\", globals(),\n-    \"keras.layers\")\n-input_layer = LazyLoader(\n-    \"input_layer\", globals(),\n-    \"keras.engine.input_layer\")\n-functional_lib = LazyLoader(\n-    \"functional_lib\", globals(),\n-    \"keras.engine.functional\")\n-training_lib = LazyLoader(\n-    \"training_lib\", globals(),\n-    \"keras.engine.training\")\n-training_lib_v1 = LazyLoader(\n-    \"training_lib_v1\", globals(),\n-    \"keras.engine.training_v1\")\n-metrics = LazyLoader(\"metrics\", globals(),\n-                     \"keras.metrics\")\n-base_rnn = LazyLoader(\n-    \"base_rnn\", globals(),\n-    \"keras.layers.rnn.base_rnn\")\n+models_lib = LazyLoader('models_lib', globals(), 'keras.models')\n+base_layer = LazyLoader('base_layer', globals(), 'keras.engine.base_layer')\n+layers_module = LazyLoader('layers_module', globals(), 'keras.layers')\n+input_layer = LazyLoader('input_layer', globals(), 'keras.engine.input_layer')\n+functional_lib = LazyLoader('functional_lib', globals(),\n+                            'keras.engine.functional')\n+training_lib = LazyLoader('training_lib', globals(), 'keras.engine.training')\n+training_lib_v1 = LazyLoader('training_lib_v1', globals(),\n+                             'keras.engine.training_v1')\n+metrics = LazyLoader('metrics', globals(), 'keras.metrics')\n+base_rnn = LazyLoader('base_rnn', globals(), 'keras.layers.rnn.base_rnn')\n # pylint:enable=g-inconsistent-quotes\n \n-\n PUBLIC_ATTRIBUTES = CommonEndpoints.all_functions.union(\n     CommonEndpoints.all_checkpointable_objects)\n PUBLIC_ATTRIBUTES.add(constants.KERAS_ATTR)\n@@ -92,9 +78,8 @@ def load(path, compile=True, options=None):  # pylint: disable=redefined-builtin\n   Args:\n     path: Path to SavedModel.\n     compile: If true, compile the model after loading it.\n-    options: Optional `tf.saved_model.LoadOptions` object that specifies\n-      options for loading from SavedModel.\n-\n+    options: Optional `tf.saved_model.LoadOptions` object that specifies options\n+      for loading from SavedModel.\n \n   Returns:\n     Object loaded from SavedModel.\n@@ -138,6 +123,8 @@ def load(path, compile=True, options=None):  # pylint: disable=redefined-builtin\n   nodes_to_load = {'root': None}\n   for node_id, loaded_node in keras_loader.loaded_nodes.items():\n     nodes_to_load[keras_loader.get_path(node_id)] = loaded_node\n+  with warnings.catch_warnings():\n+    warnings.filterwarnings('ignore', message='Trying to load ShardedVariables')\n     loaded = tf.__internal__.saved_model.load_partial(\n         path, nodes_to_load, options=options)\n \n@@ -154,8 +141,9 @@ def load(path, compile=True, options=None):  # pylint: disable=redefined-builtin\n     training_config = model._serialized_attributes['metadata'].get(\n         'training_config', None)\n     if training_config is not None:\n-      model.compile(**saving_utils.compile_args_from_training_config(\n-          training_config), from_serialized=True)\n+      model.compile(\n+          **saving_utils.compile_args_from_training_config(training_config),\n+          from_serialized=True)\n       saving_utils.try_build_compiled_arguments(model)\n       if isinstance(model.optimizer, optimizer_v2.OptimizerV2):\n         if model.optimizer.get_slot_names():\n@@ -182,7 +170,8 @@ def _update_to_current_version(metadata):\n   for node in metadata.nodes:\n     if node.version.producer == 1 and node.identifier in [\n         constants.MODEL_IDENTIFIER, constants.SEQUENTIAL_IDENTIFIER,\n-        constants.NETWORK_IDENTIFIER]:\n+        constants.NETWORK_IDENTIFIER\n+    ]:\n       node_metadata = json_utils.decode(node.metadata)\n       save_spec = node_metadata.get('save_spec')\n \n@@ -240,13 +229,14 @@ def _is_graph_network(layer):\n   if isinstance(layer, RevivedNetwork):\n     return False\n   elif isinstance(layer, functional_lib.Functional):\n-    return (layer._is_graph_network or\n-            isinstance(layer, models_lib.Sequential))\n+    return layer._is_graph_network or isinstance(layer, models_lib.Sequential)\n   return False\n \n \n class KerasObjectLoader:\n-  \"\"\"Loader that recreates Keras objects (e.g. layers, models).\n+  \"\"\"Loader that recreates Keras objects (e.g.\n+\n+  layers, models).\n \n   Layers and models are revived from either the config or SavedModel following\n   these rules:\n@@ -268,8 +258,9 @@ class KerasObjectLoader:\n     self._metadata = {x.node_id: x for x in metadata.nodes}\n     self._proto = object_graph_def\n \n-    self._node_paths = {node_data.node_id: node_data.node_path\n-                        for node_data in metadata.nodes}\n+    self._node_paths = {\n+        node_data.node_id: node_data.node_path for node_data in metadata.nodes\n+    }\n     self.loaded_nodes = {}  # Maps node path -> loaded node\n \n     # Store all node ids that have already been traversed when tracking nodes\n@@ -350,8 +341,8 @@ class KerasObjectLoader:\n \n       if not isinstance(obj_child, tf.__internal__.tracking.Trackable):\n         continue\n-      if (child_proto.user_object.identifier in\n-          tf.__internal__.saved_model.load.registered_identifiers()):\n+      if (child_proto.user_object.identifier\n+          in tf.__internal__.saved_model.load.registered_identifiers()):\n         setter = tf.__internal__.saved_model.load.get_setter(\n             child_proto.user_object)\n       elif obj_child._object_identifier in constants.KERAS_OBJECT_IDENTIFIERS:\n@@ -381,8 +372,7 @@ class KerasObjectLoader:\n \n       child_path = '{}.{}'.format(parent_path, child_name)\n       self._node_paths[child_id] = child_path\n-      self._add_children_recreated_from_config(\n-          obj_child, child_proto, child_id)\n+      self._add_children_recreated_from_config(obj_child, child_proto, child_id)\n       self.loaded_nodes[child_id] = obj_child, setter\n \n   def load_layers(self, compile=True):  # pylint: disable=redefined-builtin\n@@ -462,8 +452,8 @@ class KerasObjectLoader:\n       return None, None\n \n     setter = self._config_node_setter(_revive_setter)\n-    self._add_children_recreated_from_config(\n-        obj, self._proto.nodes[node_id], node_id)\n+    self._add_children_recreated_from_config(obj, self._proto.nodes[node_id],\n+                                             node_id)\n     return obj, setter\n \n   def _revive_graph_network(self, identifier, metadata, node_id):\n@@ -478,8 +468,7 @@ class KerasObjectLoader:\n     if generic_utils.get_registered_object(class_name) is not None:\n       return None\n     model_is_functional_or_sequential = (\n-        metadata.get('is_graph_network', False) or\n-        class_name == 'Sequential' or\n+        metadata.get('is_graph_network', False) or class_name == 'Sequential' or\n         class_name == 'Functional')\n     if not model_is_functional_or_sequential:\n       return None\n@@ -495,8 +484,7 @@ class KerasObjectLoader:\n       # Uses the custom class name, since the config does not have one.\n       model = models_lib.Sequential(name=class_name)\n     else:\n-      model = models_lib.Functional(\n-          inputs=[], outputs=[], name=config['name'])\n+      model = models_lib.Functional(inputs=[], outputs=[], name=config['name'])\n \n     # Record this model and its layers. This will later be used to reconstruct\n     # the model.\n@@ -675,7 +663,8 @@ class KerasObjectLoader:\n           set(self.model_layer_dependencies.keys()) - all_initialized_models)\n       uninitialized_model_names = [\n           self.model_layer_dependencies[model_id][0].name\n-          for model_id in uninitialized_model_ids]\n+          for model_id in uninitialized_model_ids\n+      ]\n       raise ValueError(f'Error loading model(s) in the SavedModel format. '\n                        f'The following model(s) could not be initialized: '\n                        f'{uninitialized_model_names}')\n@@ -692,11 +681,14 @@ class KerasObjectLoader:\n     elif isinstance(model, models_lib.Sequential):\n       if not layers or not isinstance(layers[0], input_layer.InputLayer):\n         if config['layers'][0]['class_name'] == 'InputLayer':\n-          layers.insert(0, input_layer.InputLayer.from_config(\n-              config['layers'][0]['config']))\n+          layers.insert(\n+              0,\n+              input_layer.InputLayer.from_config(config['layers'][0]['config']))\n         elif 'batch_input_shape' in config['layers'][0]['config']:\n           batch_input_shape = config['layers'][0]['config']['batch_input_shape']\n-          layers.insert(0, input_layer.InputLayer(\n+          layers.insert(\n+              0,\n+              input_layer.InputLayer(\n                   input_shape=batch_input_shape[1:],\n                   batch_size=batch_input_shape[0],\n                   dtype=layers[0].dtype,\n@@ -799,10 +791,12 @@ class KerasObjectLoader:\n \n   def _config_node_setter(self, setter):\n     \"\"\"Creates edges for nodes that are recreated from config.\"\"\"\n+\n     def setattr_wrapper(obj, name, value):\n       # Avoid overwriting attributes of objects recreated from the config.\n       if obj._lookup_dependency(name) is None:  # pylint: disable=protected-access\n         setter(obj, name, value)\n+\n     return setattr_wrapper\n \n \n@@ -813,11 +807,10 @@ def _finalize_saved_model_layers(layers):\n   # and not the config)\n   for layer in layers:\n     layer.built = True\n-    layer_call = getattr(_get_keras_attr(layer),\n-                         'call_and_return_conditional_losses', None)\n+    layer_call = getattr(\n+        _get_keras_attr(layer), 'call_and_return_conditional_losses', None)\n     if layer_call and layer_call.concrete_functions:\n-      layer.call = utils.use_wrapped_call(\n-          layer, layer_call, return_method=True)\n+      layer.call = utils.use_wrapped_call(layer, layer_call, return_method=True)\n       expects_training_arg = layer._serialized_attributes['metadata'][\n           'expects_training_arg']\n       if 'training' in layer_call.function_spec.arg_names:\n@@ -852,7 +845,8 @@ def _finalize_saved_model_layers(layers):\n         # V1 models require calling _set_inputs to set the `.inputs` attr.\n         # Skip this step when there are multiple tensor inputs (this behavior\n         # is not well supported in V1 models).\n-        if not any(isinstance(x, tf.TensorSpec)\n+        if not any(\n+            isinstance(x, tf.TensorSpec)\n             for x in tf.nest.flatten([args, kwargs])):\n           layer._set_inputs(inputs)\n \n@@ -866,8 +860,8 @@ def _finalize_saved_model_layers(layers):\n   # pylint: enable=protected-access\n \n \n-def _unable_to_call_layer_due_to_serialization_issue(\n-    layer, *unused_args, **unused_kwargs):\n+def _unable_to_call_layer_due_to_serialization_issue(layer, *unused_args,\n+                                                     **unused_kwargs):\n   \"\"\"Replaces the `layer.call` if the layer was not fully serialized.\n \n   Keras Model/Layer serialization is relatively relaxed because SavedModels\n@@ -923,8 +917,7 @@ def _finalize_config_layers(layers):\n     _restore_layer_metrics(layer)\n \n     # Restore RNN layer states.\n-    if (isinstance(layer, base_rnn.RNN) and\n-        layer.stateful and\n+    if (isinstance(layer, base_rnn.RNN) and layer.stateful and\n         hasattr(_get_keras_attr(layer), 'states')):\n       layer.states = getattr(_get_keras_attr(layer), 'states', None)\n       for variable in tf.nest.flatten(layer.states):\n@@ -935,8 +928,8 @@ def _finalize_config_layers(layers):\n \n \n def _finalize_metric(metric):\n-  metric.update_state = types.MethodType(metrics_utils.update_state_wrapper(\n-      metric.keras_api.update_state), metric)\n+  metric.update_state = types.MethodType(\n+      metrics_utils.update_state_wrapper(metric.keras_api.update_state), metric)\n   metric.result = metric.keras_api.result\n \n \n@@ -957,8 +950,8 @@ def _restore_layer_activation_loss(layer):\n   \"\"\"Restore actiation loss from SavedModel.\"\"\"\n   # Use wrapped activity regularizer function if the layer's activity\n   # regularizer wasn't created during initialization.\n-  activity_regularizer = getattr(_get_keras_attr(layer),\n-                                 'activity_regularizer_fn', None)\n+  activity_regularizer = getattr(\n+      _get_keras_attr(layer), 'activity_regularizer_fn', None)\n   if activity_regularizer and not layer.activity_regularizer:\n     try:\n       layer.activity_regularizer = activity_regularizer\n@@ -976,8 +969,8 @@ def revive_custom_object(identifier, metadata):\n     model_class = training_lib_v1.Model\n \n   revived_classes = {\n-      constants.INPUT_LAYER_IDENTIFIER: (\n-          RevivedInputLayer, input_layer.InputLayer),\n+      constants.INPUT_LAYER_IDENTIFIER:\n+          (RevivedInputLayer, input_layer.InputLayer),\n       constants.LAYER_IDENTIFIER: (RevivedLayer, base_layer.Layer),\n       constants.MODEL_IDENTIFIER: (RevivedNetwork, model_class),\n       constants.NETWORK_IDENTIFIER: (RevivedNetwork, functional_lib.Functional),\n@@ -1015,9 +1008,7 @@ class RevivedLayer:\n   @classmethod\n   def _init_from_metadata(cls, metadata):\n     \"\"\"Create revived layer from metadata stored in the SavedModel proto.\"\"\"\n-    init_args = dict(\n-        name=metadata['name'],\n-        trainable=metadata['trainable'])\n+    init_args = dict(name=metadata['name'], trainable=metadata['trainable'])\n     if metadata.get('dtype') is not None:\n       init_args['dtype'] = metadata['dtype']\n     if metadata.get('batch_input_shape') is not None:\n@@ -1116,12 +1107,14 @@ def recursively_deserialize_keras_object(config, module_objects=None):\n       return generic_utils.deserialize_keras_object(\n           config, module_objects=module_objects)\n     else:\n-      return {key: recursively_deserialize_keras_object(config[key],\n-                                                        module_objects)\n-              for key in config}\n+      return {\n+          key: recursively_deserialize_keras_object(config[key], module_objects)\n+          for key in config\n+      }\n   elif isinstance(config, (tuple, list)):\n-    return [recursively_deserialize_keras_object(x, module_objects)\n-            for x in config]\n+    return [\n+        recursively_deserialize_keras_object(x, module_objects) for x in config\n+    ]\n   else:\n     raise ValueError(\n         f'Unable to decode Keras layer config. Config should be a dictionary, '\n@@ -1132,12 +1125,13 @@ def infer_inputs_from_restored_call_function(fn):\n   \"\"\"Returns TypeSpec of inputs from a restored call function.\n \n   Args:\n-    fn: Restored layer call function. It is assumed that `fn` has at least\n-        one concrete function and that the inputs are in the first argument.\n+    fn: Restored layer call function. It is assumed that `fn` has at least one\n+      concrete function and that the inputs are in the first argument.\n \n   Returns:\n     TypeSpec of call function inputs in the form of (args, kwargs)\n   \"\"\"\n+\n   def common_spec(x, y):\n     if not isinstance(x, tf.TypeSpec):\n       # Doesn't particularly matter what is returned in this case because the\n@@ -1205,5 +1199,5 @@ def _maybe_add_serialized_attributes(layer, metadata):\n \n \n def _get_keras_attr(layer):\n-  return getattr(layer, '_serialized_attributes', {}).get(constants.KERAS_ATTR,\n-                                                          None)\n+  return getattr(layer, '_serialized_attributes',\n+                 {}).get(constants.KERAS_ATTR, None)\n\n@@ -39,27 +39,18 @@ from keras.utils.generic_utils import LazyLoader\n import tensorflow.compat.v1.logging as logging\n import tensorflow.compat.v2 as tf\n \n-\n # To avoid circular dependencies between keras/engine and keras/saving,\n # code in keras/saving must delay imports.\n \n # TODO(b/134426265): Switch back to single-quotes to match the rest of the file\n # once the issue with copybara is fixed.\n # pylint:disable=g-inconsistent-quotes\n-base_layer = LazyLoader(\n-    \"base_layer\", globals(),\n-    \"keras.engine.base_layer\")\n-metrics = LazyLoader(\"metrics\", globals(),\n-                     \"keras.metrics\")\n-input_layer = LazyLoader(\n-    \"input_layer\", globals(),\n-    \"keras.engine.input_layer\")\n-training_lib = LazyLoader(\n-    \"training_lib\", globals(),\n-    \"keras.engine.training\")\n-sequential_lib = LazyLoader(\n-    \"sequential_lib\", globals(),\n-    \"keras.engine.sequential\")\n+base_layer = LazyLoader('base_layer', globals(), 'keras.engine.base_layer')\n+metrics = LazyLoader('metrics', globals(), 'keras.metrics')\n+input_layer = LazyLoader('input_layer', globals(), 'keras.engine.input_layer')\n+training_lib = LazyLoader('training_lib', globals(), 'keras.engine.training')\n+sequential_lib = LazyLoader('sequential_lib', globals(),\n+                            'keras.engine.sequential')\n # pylint:enable=g-inconsistent-quotes\n \n \n@@ -74,6 +65,10 @@ def should_skip_serialization(layer):\n   return False\n \n \n+def _filter_shards(variables):\n+  return [var for var in variables if not hasattr(var, '_sharded_container')]\n+\n+\n def wrap_layer_objects(layer, serialization_cache):\n   \"\"\"Returns extra trackable objects to attach to the serialized layer.\n \n@@ -104,17 +99,26 @@ def wrap_layer_objects(layer, serialization_cache):\n       wrapped_loss = _wrap_unconditional_loss(loss_fn, len(keras_loss_cache))\n       keras_loss_cache[loss_fn] = wrapped_loss\n       wrapped_loss_functions.append(wrapped_loss)\n-  wrapped_layer_losses = [keras_loss_cache[fn]\n-                          for fn in layer._callable_losses[:]]  # pylint: disable=protected-access\n+  wrapped_layer_losses = [\n+      keras_loss_cache[fn] for fn in layer._callable_losses[:]  # pylint: disable=protected-access\n+  ]\n \n   layer_metrics = tf.__internal__.tracking.wrap(\n       {m.name: m for m in layer._metrics})  # pylint: disable=protected-access\n+\n+  # Avoid duplicate creation of shard Variables on loading.\n+  # `layer.variables` will return the shard Variables rather than the\n+  # ShardedVariables (b/224541446), but Keras loading will create new\n+  # ShardedVariables (and thus shard Variables) from Keras metadata if needed.\n+  # There's no need to also save the shard Variables here, so filter them out.\n+  variables = _filter_shards(layer.variables)\n+  trainable_variables = _filter_shards(layer.trainable_variables)\n+  non_trainable_variables = _filter_shards(layer.non_trainable_variables)\n   return dict(\n-      variables=tf.__internal__.tracking.wrap(layer.variables),\n-      trainable_variables=tf.__internal__.tracking.wrap(\n-          layer.trainable_variables),\n+      variables=tf.__internal__.tracking.wrap(variables),\n+      trainable_variables=tf.__internal__.tracking.wrap(trainable_variables),\n       non_trainable_variables=tf.__internal__.tracking.wrap(\n-          layer.non_trainable_variables),\n+          non_trainable_variables),\n       layers=tf.__internal__.tracking.wrap(utils.list_all_layers(layer)),\n       metrics=tf.__internal__.tracking.wrap(layer.metrics),\n       regularization_losses=tf.__internal__.tracking.wrap(\n@@ -141,8 +145,10 @@ def wrap_layer_functions(layer, serialization_cache):\n   # model.pop(), don't use saved functions.\n   if (isinstance(layer, keras_load.RevivedLayer) and\n       not isinstance(layer, sequential_lib.Sequential)):\n-    return {fn_name: getattr(layer.keras_api, fn_name, None)\n-            for fn_name in serialized_attributes.LayerAttributes.all_functions}\n+    return {\n+        fn_name: getattr(layer.keras_api, fn_name, None)\n+        for fn_name in serialized_attributes.LayerAttributes.all_functions\n+    }\n \n   # Reset the losses of the layer and its children. The call function in each\n   # child layer is replaced with tf.functions.\n@@ -169,15 +175,17 @@ def wrap_layer_functions(layer, serialization_cache):\n       # function, `match_layer_training_arg` should be set to False.\n       match_layer_training_arg=False)\n \n-  fns = {'call_and_return_conditional_losses': call_fn_with_losses,\n-         '__call__': call_fn}\n+  fns = {\n+      'call_and_return_conditional_losses': call_fn_with_losses,\n+      '__call__': call_fn\n+  }\n \n   if layer._activity_regularizer is not None:  # pylint: disable=protected-access\n     fns['activity_regularizer_fn'] = _wrap_activity_regularizer(layer)\n     fns['call_and_return_all_conditional_losses'] = (\n         call_collection.add_function(\n-            _append_activity_regularizer_loss(\n-                layer, call_fn_with_losses, fns['activity_regularizer_fn']),\n+            _append_activity_regularizer_loss(layer, call_fn_with_losses,\n+                                              fns['activity_regularizer_fn']),\n             '{}_layer_call_and_return_all_conditional_losses'.format(\n                 layer.name),\n             match_layer_training_arg=False))\n@@ -313,8 +321,10 @@ def _reset_layer_losses(parent_layer):\n   \"\"\"Resets losses of layer and its sublayers, and returns original losses.\"\"\"\n   losses_dict = {}\n   for layer in utils.list_all_layers_and_sublayers(parent_layer):\n-    losses_dict[layer] = {'losses': layer._losses[:],\n-                          'eager_losses': layer._eager_losses[:]}\n+    losses_dict[layer] = {\n+        'losses': layer._losses[:],\n+        'eager_losses': layer._eager_losses[:]\n+    }\n     with utils.no_automatic_dependency_tracking_scope(layer):\n       layer._losses = []\n       layer._eager_losses = []\n@@ -326,6 +336,8 @@ def _restore_layer_losses(losses_dict):\n     with utils.no_automatic_dependency_tracking_scope(layer):\n       layer._losses = losses_dict[layer]['losses']\n       layer._eager_losses = losses_dict[layer]['eager_losses']\n+\n+\n # pylint: enable=protected-access\n \n \n@@ -336,6 +348,7 @@ class LayerTracingContext(threading.local):\n     self.enable_call_tracing = False\n     self.trace_queue = []\n \n+\n _thread_local_data = LayerTracingContext()\n \n \n@@ -429,8 +442,10 @@ class LayerCallCollection:\n         if spec.shape == tf.TensorShape(None):\n           return None, None\n         return spec\n-      input_signature = [tf.nest.map_structure(\n-          to_tensor_spec_or_none, layer.input_spec)]\n+\n+      input_signature = [\n+          tf.nest.map_structure(to_tensor_spec_or_none, layer.input_spec)\n+      ]\n \n       return input_signature, {}\n     else:\n@@ -450,6 +465,7 @@ class LayerCallCollection:\n       # TODO(kathywu): Replace arguments with broader shapes defined in the\n       # input signature.\n       if self._expects_training_arg:\n+\n         def trace_with_training(value, fn=fn):\n           utils.set_training_arg(value, self._training_arg_index, args, kwargs)\n           add_trace_to_queue(fn, args, kwargs, value)\n@@ -465,18 +481,27 @@ class LayerCallCollection:\n               is not None)\n     else:\n       return self.layer._call_arg_was_passed(  # pylint: disable=protected-access\n-          'training', args, kwargs, inputs_in_args=True)\n+          'training',\n+          args,\n+          kwargs,\n+          inputs_in_args=True)\n \n   def get_training_arg_value(self, args, kwargs):\n     if not self.layer._expects_training_arg and self._expects_training_arg:  # pylint: disable=protected-access\n       return utils.get_training_arg(self._training_arg_index, args, kwargs)\n     else:\n       return self.layer._get_call_arg_value(  # pylint: disable=protected-access\n-          'training', args, kwargs, inputs_in_args=True)\n+          'training',\n+          args,\n+          kwargs,\n+          inputs_in_args=True)\n \n   def get_input_arg_value(self, args, kwargs):\n     return self.layer._get_call_arg_value(  # pylint: disable=protected-access\n-        self._input_arg_name, args, kwargs, inputs_in_args=True)\n+        self._input_arg_name,\n+        args,\n+        kwargs,\n+        inputs_in_args=True)\n \n   def _maybe_wrap_with_training_arg(self, call_fn, match_layer_training_arg):\n     \"\"\"Wraps call function with added training argument if necessary.\"\"\"\n@@ -629,6 +654,7 @@ def _wrap_call_and_conditional_losses(layer):\n   \"\"\"\n   # Create function that generates both outputs and losses\n   layer_call = _get_layer_call_method(layer)\n+\n   def call_and_return_conditional_losses(*args, **kwargs):\n     \"\"\"Returns layer (call_output, conditional losses) tuple.\"\"\"\n     call_output = layer_call(*args, **kwargs)\n@@ -648,30 +674,34 @@ def _extract_outputs_from_fn(layer, call_and_return_conditional_losses):\n   \"\"\"Returns a function that returns only call function outputs.\"\"\"\n   if isinstance(layer, keras_load.RevivedLayer):\n     return layer.keras_api.__call__  # pylint: disable=protected-access\n+\n   def call(inputs, *args, **kwargs):\n     return call_and_return_conditional_losses(inputs, *args, **kwargs)[0]\n+\n   return _create_call_fn_decorator(layer, call)\n \n \n-def _append_activity_regularizer_loss(\n-    layer, call_fn_with_losses, activity_regularizer_fn):\n+def _append_activity_regularizer_loss(layer, call_fn_with_losses,\n+                                      activity_regularizer_fn):\n   \"\"\"Appends activity regularizer loss to losses returned by the wrapped fn.\"\"\"\n+\n   def fn(inputs, *args, **kwargs):\n     outputs, losses = call_fn_with_losses(inputs, *args, **kwargs)\n     losses.append(activity_regularizer_fn(outputs))\n     return outputs, losses\n+\n   return _create_call_fn_decorator(layer, fn)\n \n \n def _create_call_fn_decorator(layer, wrapped_call):\n   call_fn = _get_layer_call_method(layer)\n   fn, arg_spec = utils.maybe_add_training_arg(\n-      call_fn, wrapped_call, layer._expects_training_arg,  # pylint: disable=protected-access\n+      call_fn,\n+      wrapped_call,\n+      layer._expects_training_arg,  # pylint: disable=protected-access\n       default_training_value=False)\n   return tf.__internal__.decorator.make_decorator(\n-      target=call_fn,\n-      decorator_func=fn,\n-      decorator_argspec=arg_spec)\n+      target=call_fn, decorator_func=fn, decorator_argspec=arg_spec)\n \n \n def _wrap_unconditional_loss(loss_fn, index):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a404a31621cc5a61d21304ed49b474e0c47779be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 7 | Churn Cumulative: 1098 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 14 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -549,6 +549,8 @@ def get_tensor_spec(t, dynamic_batch=False, name=None):\n   elif (hasattr(t, '_keras_history') and\n         hasattr(t._keras_history[0], '_type_spec')):\n     return t._keras_history[0]._type_spec\n+  elif isinstance(t, keras_tensor.KerasTensor):\n+    spec = t.type_spec\n   elif hasattr(t, 'shape') and hasattr(t, 'dtype'):\n     spec = tf.TensorSpec(shape=t.shape, dtype=t.dtype, name=name)\n   else:\n\n@@ -331,6 +331,11 @@ class TestGetTensorSpec(parameterized.TestCase):\n     else:\n       self.assertEqual(result.shape.as_list(), expected_shape)\n \n+  def test_with_keras_tensor_with_ragged_spec(self):\n+    t = keras.engine.keras_tensor.KerasTensor(\n+        tf.RaggedTensorSpec(shape=(None, None, 1)))\n+    self.assertIsInstance(tf_utils.get_tensor_spec(t), tf.RaggedTensorSpec)\n+\n \n class TestSyncToNumpyOrPythonType(parameterized.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c277bcb0417f5c682f945627bd9d78a5763c38cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 680 | Contributors (this commit): 1 | Commits (past 90d): 11 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -363,7 +363,7 @@ def _create_dvariable(layout_map, object_path, variable):\n   Returns:\n     A new tf.Variable with correct layout information.\n   \"\"\"\n-  # TODO(scottzhu): Revisit this in future and see if we can just reuse the\n+  # TODO(b/228209108): Revisit this in future and see if we can just reuse the\n   # LazyInitVariable rather than creating a new tf.Variable instance.\n   layout = layout_map[object_path]\n   if layout is None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bb14ef91bd6d9931c43b678579df70c96353e4d1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 3984 | Contributors (this commit): 8 | Commits (past 90d): 23 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1433,6 +1433,9 @@ class RandomContrast(BaseImageAugmentationLayer):\n     output.set_shape(image.shape)\n     return output\n \n+  def augment_label(self, label, transformation=None):\n+    return label\n+\n   def compute_output_shape(self, input_shape):\n     return input_shape\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8e7edd2a646b526c38e3541ca3e5000b9acfe353", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 93 | Lines Deleted: 26 | Files Changed: 2 | Hunks: 13 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 119 | Churn Cumulative: 6807 | Contributors (this commit): 9 | Commits (past 90d): 36 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1028,7 +1028,7 @@ def get_rotation_matrix(angles, image_height, image_width, name=None):\n @keras_export('keras.layers.RandomRotation',\n               'keras.layers.experimental.preprocessing.RandomRotation',\n               v1=[])\n-class RandomRotation(base_layer.BaseRandomLayer):\n+class RandomRotation(BaseImageAugmentationLayer):\n   \"\"\"A preprocessing layer which randomly rotates images during training.\n \n   This layer will apply random rotations to each image, filling empty space\n@@ -1105,41 +1105,36 @@ class RandomRotation(base_layer.BaseRandomLayer):\n     self.interpolation = interpolation\n     self.seed = seed\n \n-  def call(self, inputs, training=True):\n-    inputs = utils.ensure_tensor(inputs, self.compute_dtype)\n-\n-    def random_rotated_inputs(inputs):\n-      \"\"\"Rotated inputs with random ops.\"\"\"\n-      original_shape = inputs.shape\n-      unbatched = inputs.shape.rank == 3\n-      # The transform op only accepts rank 4 inputs, so if we have an unbatched\n-      # image, we need to temporarily expand dims to a batch.\n-      if unbatched:\n-        inputs = tf.expand_dims(inputs, 0)\n-      inputs_shape = tf.shape(inputs)\n-      batch_size = inputs_shape[0]\n-      img_hd = tf.cast(inputs_shape[H_AXIS], tf.float32)\n-      img_wd = tf.cast(inputs_shape[W_AXIS], tf.float32)\n+  def get_random_transformation(self,\n+                                image=None,\n+                                label=None,\n+                                bounding_box=None):\n     min_angle = self.lower * 2. * np.pi\n     max_angle = self.upper * 2. * np.pi\n-      angles = self._random_generator.random_uniform(\n-          shape=[batch_size], minval=min_angle, maxval=max_angle)\n+    angle = self._random_generator.random_uniform(\n+        shape=[1], minval=min_angle, maxval=max_angle)\n+    return {'angle': angle}\n+\n+  def augment_image(self, image, transformation=None):\n+    if transformation is None:\n+      transformation = self.get_random_transformation()\n+    image = utils.ensure_tensor(image, self.compute_dtype)\n+    original_shape = image.shape\n+    image = tf.expand_dims(image, 0)\n+    image_shape = tf.shape(image)\n+    img_hd = tf.cast(image_shape[H_AXIS], tf.float32)\n+    img_wd = tf.cast(image_shape[W_AXIS], tf.float32)\n+    angle = transformation['angle']\n     output = transform(\n-          inputs,\n-          get_rotation_matrix(angles, img_hd, img_wd),\n+        image,\n+        get_rotation_matrix(angle, img_hd, img_wd),\n         fill_mode=self.fill_mode,\n         fill_value=self.fill_value,\n         interpolation=self.interpolation)\n-      if unbatched:\n     output = tf.squeeze(output, 0)\n     output.set_shape(original_shape)\n     return output\n \n-    if training:\n-      return random_rotated_inputs(inputs)\n-    else:\n-      return inputs\n-\n   def compute_output_shape(self, input_shape):\n     return input_shape\n \n@@ -1281,6 +1276,8 @@ class RandomZoom(BaseImageAugmentationLayer):\n \n   def augment_image(self, image, transformation=None):\n     image = utils.ensure_tensor(image, self.compute_dtype)\n+    if transformation is None:\n+      transformation = self.get_random_transformation()\n     original_shape = image.shape\n     image = tf.expand_dims(image, 0)\n     image_shape = tf.shape(image)\n@@ -1427,6 +1424,8 @@ class RandomContrast(BaseImageAugmentationLayer):\n     return {'contrast_factor': contrast_factor}\n \n   def augment_image(self, image, transformation=None):\n+    if transformation is None:\n+      transformation = self.get_random_transformation()\n     contrast_factor = transformation['contrast_factor']\n     output = tf.image.adjust_contrast(image, contrast_factor=contrast_factor)\n     output = tf.clip_by_value(output, 0, 255)\n@@ -1520,6 +1519,8 @@ class RandomBrightness(BaseImageAugmentationLayer):\n     self._seed = seed\n \n   def augment_image(self, image, transformation=None):\n+    if transformation is None:\n+      transformation = self.get_random_transformation()\n     return self._brightness_adjust(image, transformation['rgb_delta'])\n \n   def get_random_transformation(self,\n\n@@ -690,6 +690,23 @@ class RandomContrastTest(test_combinations.TestCase):\n         actual_output = layer(inp, training=True)\n         self.assertAllClose(expected_output, actual_output)\n \n+  def test_augment_image(self):\n+    np.random.seed(1337)\n+    mock_random = 0.2\n+    inp = np.random.random((4, 4, 1))\n+    inp_mean = np.mean(inp, axis=0, keepdims=True)\n+    inp_mean = np.mean(inp_mean, axis=1, keepdims=True)\n+    expected_output = (inp - inp_mean) * mock_random + inp_mean\n+    with tf.compat.v1.test.mock.patch.object(\n+        stateless_random_ops,\n+        'stateless_random_uniform',\n+        return_value=mock_random,\n+    ):\n+      with test_utils.use_gpu():\n+        layer = image_preprocessing.RandomContrast((0.2, 0.5))\n+        actual_output = layer.augment_image(inp)\n+        self.assertAllClose(expected_output, actual_output)\n+\n   @test_utils.run_v2_only\n   def test_output_dtypes(self):\n     inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n@@ -759,6 +776,23 @@ class RandomBrightnessTest(test_combinations.TestCase):\n     self.assertLessEqual(tf.math.reduce_max(diff), 0)\n     self.assertLess(tf.math.reduce_mean(diff), 0)\n \n+  def test_augment_image(self):\n+    # Always scale up, but randomly between 0 ~ 255\n+    layer = image_preprocessing.RandomBrightness([0, 1.0])\n+    image = np.random.randint(0, 255, size=(224, 224, 3))\n+    output = layer.augment_image(image, transformation=None)\n+    diff = output - image\n+    self.assertGreaterEqual(tf.math.reduce_min(diff), 0)\n+    self.assertGreater(tf.math.reduce_mean(diff), 0)\n+\n+    # Always scale down, but randomly between 0 ~ 255\n+    layer = image_preprocessing.RandomBrightness([-1.0, 0.0])\n+    image = np.random.randint(0, 255, size=(224, 224, 3))\n+    output = layer.augment_image(image)\n+    diff = output - image\n+    self.assertLessEqual(tf.math.reduce_max(diff), 0)\n+    self.assertLess(tf.math.reduce_mean(diff), 0)\n+\n   @test_utils.run_v2_only\n   def test_scale_output(self):\n     layer = image_preprocessing.RandomBrightness([0, 1.0], seed=1337)\n@@ -1454,6 +1488,22 @@ class RandomRotationTest(test_combinations.TestCase):\n       expected_output = np.reshape(expected_output, (5, 5, 1))\n       self.assertAllClose(expected_output, output_image)\n \n+  def test_augment_image(self):\n+    with test_utils.use_gpu():\n+      input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(np.float32)\n+      # 180 rotation.\n+      layer = image_preprocessing.RandomRotation(factor=(0.5, 0.5))\n+      output_image = layer.augment_image(input_image, transformation=None)\n+      expected_output = np.asarray([\n+          [24, 23, 22, 21, 20],\n+          [19, 18, 17, 16, 15],\n+          [14, 13, 12, 11, 10],\n+          [9, 8, 7, 6, 5],\n+          [4, 3, 2, 1, 0],\n+      ]).astype(np.float32)\n+      expected_output = np.reshape(expected_output, (5, 5, 1))\n+      self.assertAllClose(expected_output, output_image)\n+\n   @test_utils.run_v2_only\n   def test_output_dtypes(self):\n     inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n@@ -1576,6 +1626,22 @@ class RandomZoomTest(test_combinations.TestCase):\n       expected_output = np.reshape(expected_output, (5, 5, 1))\n       self.assertAllEqual(expected_output, output_image)\n \n+  def test_augment_image(self):\n+    with test_utils.use_gpu():\n+      input_image = np.reshape(np.arange(0, 25), (5, 5, 1)).astype(np.int64)\n+      layer = image_preprocessing.RandomZoom((-.5, -.5), (-.5, -.5),\n+                                             interpolation='nearest')\n+      output_image = layer.augment_image(input_image)\n+      expected_output = np.asarray([\n+          [6, 7, 7, 8, 8],\n+          [11, 12, 12, 13, 13],\n+          [11, 12, 12, 13, 13],\n+          [16, 17, 17, 18, 18],\n+          [16, 17, 17, 18, 18],\n+      ]).astype(np.int64)\n+      expected_output = np.reshape(expected_output, (5, 5, 1))\n+      self.assertAllEqual(expected_output, output_image)\n+\n   @test_utils.run_v2_only\n   def test_output_dtypes(self):\n     inputs = np.array([[[1], [2]], [[3], [4]]], dtype='float64')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
