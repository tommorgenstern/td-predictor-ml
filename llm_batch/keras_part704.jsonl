{"custom_id": "keras#9345cba2bb79bcda2a47529afeda5c358ea8eb6c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 36 | Churn Cumulative: 1297 | Contributors (this commit): 6 | Commits (past 90d): 10 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -35,12 +35,7 @@ def ignore_files(_, filenames):\n     return [f for f in filenames if f.endswith(\"_test.py\")]\n \n \n-def build(root_path):\n-    if os.path.exists(build_directory):\n-        raise ValueError(f\"Directory already exists: {build_directory}\")\n-\n-    whl_path = None\n-    try:\n+def copy_source_to_build_directory(root_path):\n     # Copy sources (`keras_core/` directory and setup files) to build\n     # directory\n     os.chdir(root_path)\n@@ -52,12 +47,16 @@ def build(root_path):\n         shutil.copy(fname, os.path.join(f\"{build_directory}\", fname))\n     os.chdir(build_directory)\n \n+\n+def run_namex_conversion():\n     # Restructure the codebase so that source files live in `keras_core/src`\n     namex.convert_codebase(package, code_directory=\"src\")\n \n     # Generate API __init__.py files in `keras_core/`\n     namex.generate_api_files(package, code_directory=\"src\", verbose=True)\n \n+\n+def create_legacy_directory():\n     # Make keras_core/_tf_keras/ by copying keras_core/\n     tf_keras_dirpath = os.path.join(package, \"_tf_keras\")\n     os.makedirs(tf_keras_dirpath)\n@@ -127,23 +126,23 @@ def build(root_path):\n                             f\"keras_core.{legacy_submodule}\",\n                             f\"keras_core._tf_keras.{legacy_submodule}\",\n                         )\n-                        legacy_contents = (\n-                            core_api_contents + \"\\n\" + legacy_contents\n-                        )\n+                    legacy_contents = core_api_contents + \"\\n\" + legacy_contents\n                 with open(tf_keras_fpath, \"w\") as f:\n                     f.write(legacy_contents)\n \n     # Delete keras_core/_legacy/\n     shutil.rmtree(os.path.join(package, \"_legacy\"))\n \n-        # Make sure to export the __version__ string\n-        from keras_core.src.version import __version__  # noqa: E402\n \n+def export_version_string(__version__):\n+    # Make sure to export the __version__ string\n     with open(os.path.join(package, \"__init__.py\")) as f:\n         init_contents = f.read()\n     with open(os.path.join(package, \"__init__.py\"), \"w\") as f:\n         f.write(init_contents + \"\\n\\n\" + f'__version__ = \"{__version__}\"\\n')\n \n+\n+def build_and_save_output(root_path, __version__):\n     # Build the package\n     os.system(\"python3 -m build\")\n \n@@ -161,6 +160,21 @@ def build(root_path):\n         if __version__ in fname and fname.endswith(\".whl\"):\n             whl_path = os.path.abspath(os.path.join(dist_directory, fname))\n     print(f\"Build successful. Wheel file available at {whl_path}\")\n+\n+\n+def build(root_path):\n+    if os.path.exists(build_directory):\n+        raise ValueError(f\"Directory already exists: {build_directory}\")\n+\n+    whl_path = None\n+    try:\n+        copy_source_to_build_directory(root_path)\n+        run_namex_conversion()\n+        create_legacy_directory()\n+        from keras_core.src.version import __version__  # noqa: E402\n+\n+        export_version_string(__version__)\n+        build_and_save_output(root_path, __version__)\n     finally:\n         # Clean up: remove the build directory (no longer needed)\n         shutil.rmtree(build_directory)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b14d0c21eb02be2c45ca459bc2031c820d4d2c3c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1303 | Contributors (this commit): 6 | Commits (past 90d): 11 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -156,17 +156,18 @@ def build_and_save_output(root_path, __version__):\n         shutil.copy(fpath, dist_directory)\n \n     # Find the .whl file path\n+    whl_path = None\n     for fname in os.listdir(dist_directory):\n         if __version__ in fname and fname.endswith(\".whl\"):\n             whl_path = os.path.abspath(os.path.join(dist_directory, fname))\n     print(f\"Build successful. Wheel file available at {whl_path}\")\n+    return whl_path\n \n \n def build(root_path):\n     if os.path.exists(build_directory):\n         raise ValueError(f\"Directory already exists: {build_directory}\")\n \n-    whl_path = None\n     try:\n         copy_source_to_build_directory(root_path)\n         run_namex_conversion()\n@@ -174,11 +175,10 @@ def build(root_path):\n         from keras_core.src.version import __version__  # noqa: E402\n \n         export_version_string(__version__)\n-        build_and_save_output(root_path, __version__)\n+        return build_and_save_output(root_path, __version__)\n     finally:\n         # Clean up: remove the build directory (no longer needed)\n         shutil.rmtree(build_directory)\n-    return whl_path\n \n \n def install_whl(whl_fpath):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c0359821ad12478ffd447a73fe23158e6eed2b76", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 686 | Contributors (this commit): 9 | Commits (past 90d): 17 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -582,6 +582,7 @@ def squeeze(x, axis=None):\n \n \n def transpose(x, axes=None):\n+    x = convert_to_tensor(x)\n     return jnp.transpose(x, axes=axes)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5a00e8b2b0140c5f62eb2e899330fbc45d5f5e7d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 25 | Churn Cumulative: 787 | Contributors (this commit): 7 | Commits (past 90d): 19 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -421,6 +421,18 @@ def shape(x):\n     return backend.core.shape(x)\n \n \n+class Cast(Operation):\n+    def __init__(self, dtype):\n+        super().__init__()\n+        self.dtype = backend.standardize_dtype(dtype)\n+\n+    def call(self, x):\n+        return backend.core.cast(x, self.dtype)\n+\n+    def compute_output_spec(self, x):\n+        return backend.KerasTensor(shape=x.shape, dtype=self.dtype)\n+\n+\n @keras_core_export(\"keras_core.ops.cast\")\n def cast(x, dtype):\n     \"\"\"Cast a tensor to the desired dtype.\n@@ -440,7 +452,7 @@ def cast(x, dtype):\n     dtype = backend.standardize_dtype(dtype)\n \n     if any_symbolic_tensors((x,)):\n-        return backend.KerasTensor(shape=x.shape, dtype=dtype)\n+        return Cast(dtype=dtype)(x)\n     return backend.core.cast(x, dtype)\n \n \n\n@@ -330,3 +330,14 @@ class CoreOpsCorrectnessTest(testing.TestCase):\n         for o, o_e in zip(out, out_ex):\n             o = ops.convert_to_numpy(o)\n             self.assertAllClose(o, o_e)\n+\n+    def test_cast(self):\n+        x = ops.ones((2,), dtype=\"float32\")\n+        y = ops.cast(x, \"float16\")\n+        self.assertIn(\"float16\", str(y.dtype))\n+\n+        x = ops.KerasTensor((2,), dtype=\"float32\")\n+        y = ops.cast(x, \"float16\")\n+        self.assertEqual(\"float16\", y.dtype)\n+        self.assertEqual(x.shape, y.shape)\n+        self.assertTrue(hasattr(y, \"_keras_history\"))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a1b9c93a26b268072e58ff2d976869d42f1e9ab8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 3 | Methods Changed: 5 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 27 | Churn Cumulative: 2860 | Contributors (this commit): 14 | Commits (past 90d): 44 | Contributors (cumulative): 18 | DMM Complexity: 0.0\n\nDIFF:\n@@ -419,9 +419,18 @@ class Functional(Function, Model):\n         config[\"output_layers\"] = map_tensors(self._outputs_struct)\n         return copy.deepcopy(config)\n \n-    @classmethod\n-    def _from_config(cls, config, custom_objects=None):\n-        \"\"\"Instantiates a Model from its config (output of `get_config()`).\"\"\"\n+\n+def functional_from_config(cls, config, custom_objects=None):\n+    \"\"\"Instantiates a Functional model from its config (from `get_config()`).\n+\n+    Args:\n+        cls: Class of the model, e.g. a custom subclass of `Model`.\n+        config: Output of `get_config()` for the original model instance.\n+        custom_objects: Optional dict of custom objects.\n+\n+    Returns:\n+        An instance of `cls`.\n+    \"\"\"\n     # Layer instances created during\n     # the graph reconstruction process\n     created_layers = {}\n@@ -531,9 +540,7 @@ class Functional(Function, Model):\n     def get_tensor(layer_name, node_index, tensor_index):\n         assert layer_name in created_layers\n         layer = created_layers[layer_name]\n-            layer_output_tensors = layer._inbound_nodes[\n-                node_index\n-            ].output_tensors\n+        layer_output_tensors = layer._inbound_nodes[node_index].output_tensors\n         return layer_output_tensors[tensor_index]\n \n     def map_tensors(tensors):\n\n@@ -480,9 +480,11 @@ class Model(Trainer, Layer):\n         if is_functional_config and revivable_as_functional:\n             # Revive Functional model\n             # (but not Functional subclasses with a custom __init__)\n-            if cls == Model:\n-                cls = Functional\n-            return cls._from_config(config, custom_objects=custom_objects)\n+            from keras_core.models.functional import functional_from_config\n+\n+            return functional_from_config(\n+                cls, config, custom_objects=custom_objects\n+            )\n \n         # Either the model has a custom __init__, or the config\n         # does not contain all the information necessary to\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4d88a29725bae75399845bb72fc941f7d5c57287", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 26 | Files Changed: 5 | Hunks: 19 | Methods Changed: 18 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 69 | Churn Cumulative: 8042 | Contributors (this commit): 15 | Commits (past 90d): 63 | Contributors (cumulative): 39 | DMM Complexity: 1.0\n\nDIFF:\n@@ -83,7 +83,7 @@ def gelu(x, approximate=True):\n     return jnn.gelu(x, approximate)\n \n \n-def softmax(x, axis=None):\n+def softmax(x, axis=-1):\n     x = convert_to_tensor(x)\n     return jnn.softmax(x, axis=axis)\n \n\n@@ -74,26 +74,27 @@ def gelu(x, approximate=True):\n     return tf.nn.gelu(x, approximate)\n \n \n-def softmax(x, axis=None):\n+def softmax(x, axis=-1):\n     logits = x\n     if axis is None:\n         # Unlike numpy, tf will handle axis=None as axis=-1.\n         # We need this workaround for the reduction on every dim.\n-        logits_exp = tf.exp(logits)\n-        output = logits_exp / tf.reduce_sum(logits_exp, keepdims=True)\n+        output = tf.reshape(x, [-1])\n+        output = tf.nn.softmax(output, axis=-1)\n+        output = tf.reshape(output, tf.shape(x))\n     else:\n         output = tf.nn.softmax(x, axis=axis)\n     output._keras_logits = logits\n     return output\n \n \n-def log_softmax(x, axis=None):\n+def log_softmax(x, axis=-1):\n     if axis is None:\n         # Unlike numpy, tf will handle axis=None as axis=-1.\n         # We need this workaround for the reduction on every dim.\n-        logits = x\n-        logits_exp = tf.exp(logits)\n-        return logits - tf.math.log(tf.reduce_sum(logits_exp, keepdims=True))\n+        output = tf.reshape(x, [-1])\n+        output = tf.nn.log_softmax(output, axis=-1)\n+        return tf.reshape(output, tf.shape(x))\n     return tf.nn.log_softmax(x, axis=axis)\n \n \n\n@@ -89,24 +89,26 @@ def gelu(x, approximate=True):\n     return tnn.gelu(x)\n \n \n-def softmax(x, axis=None):\n-    logits = convert_to_tensor(x)\n+def softmax(x, axis=-1):\n+    x = convert_to_tensor(x)\n     if axis is None:\n         # Unlike numpy, PyTorch will handle axis=None as axis=-1.\n         # We need this workaround for the reduction on every dim.\n-        logits_exp = torch.exp(logits)\n-        return logits_exp / torch.sum(logits_exp)\n-    return tnn.softmax(logits, dim=axis)\n+        output = torch.reshape(x, [-1])\n+        output = tnn.softmax(output, dim=-1)\n+        return torch.reshape(output, x.shape)\n+    return tnn.softmax(x, dim=axis)\n \n \n-def log_softmax(x, axis=None):\n-    logits = convert_to_tensor(x)\n+def log_softmax(x, axis=-1):\n+    x = convert_to_tensor(x)\n     if axis is None:\n         # Unlike numpy, PyTorch will handle axis=None as axis=-1.\n         # We need this workaround for the reduction on every dim.\n-        logits_exp = torch.exp(logits)\n-        return logits - torch.log(torch.sum(logits_exp))\n-    return tnn.log_softmax(logits, dim=axis)\n+        output = torch.reshape(x, [-1])\n+        output = tnn.log_softmax(output, dim=-1)\n+        return torch.reshape(output, x.shape)\n+    return tnn.log_softmax(x, dim=axis)\n \n \n def _compute_padding_length(\n\n@@ -443,9 +443,9 @@ def gelu(x, approximate=True):\n \n \n class Softmax(Operation):\n-    def __init__(self, axis=None):\n+    def __init__(self, axis=-1):\n         super().__init__()\n-        self.axis = axis if axis is not None else -1\n+        self.axis = axis\n \n     def call(self, x):\n         return backend.nn.softmax(x, axis=self.axis)\n@@ -455,16 +455,16 @@ class Softmax(Operation):\n \n \n @keras_core_export([\"keras_core.ops.softmax\", \"keras_core.ops.nn.softmax\"])\n-def softmax(x, axis=None):\n+def softmax(x, axis=-1):\n     if any_symbolic_tensors((x,)):\n         return Softmax(axis).symbolic_call(x)\n     return backend.nn.softmax(x, axis=axis)\n \n \n class LogSoftmax(Operation):\n-    def __init__(self, axis=None):\n+    def __init__(self, axis=-1):\n         super().__init__()\n-        self.axis = axis if axis is not None else -1\n+        self.axis = axis\n \n     def call(self, x):\n         return backend.nn.log_softmax(x, axis=self.axis)\n@@ -479,7 +479,7 @@ class LogSoftmax(Operation):\n         \"keras_core.ops.nn.log_softmax\",\n     ]\n )\n-def log_softmax(x, axis=None):\n+def log_softmax(x, axis=-1):\n     if any_symbolic_tensors((x,)):\n         return LogSoftmax(axis).symbolic_call(x)\n     return backend.nn.log_softmax(x, axis=axis)\n\n@@ -688,7 +688,7 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n     def test_softmax(self):\n         x = np.array([[1, 2, 3], [1, 2, 3]], dtype=np.float32)\n         self.assertAllClose(\n-            knn.softmax(x),\n+            knn.softmax(x, axis=None),  # Reduce on all axes.\n             [[0.045015, 0.122364, 0.33262], [0.045015, 0.122364, 0.33262]],\n         )\n         self.assertAllClose(\n@@ -702,11 +702,18 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n                 [0.09003057, 0.24472848, 0.66524094],\n             ],\n         )\n+        self.assertAllClose(\n+            knn.softmax(x),  # Default axis should be -1.\n+            [\n+                [0.09003057, 0.24472848, 0.66524094],\n+                [0.09003057, 0.24472848, 0.66524094],\n+            ],\n+        )\n \n     def test_log_softmax(self):\n         x = np.array([[1, 2, 3], [1, 2, 3]], dtype=np.float32)\n         self.assertAllClose(\n-            knn.log_softmax(x),\n+            knn.log_softmax(x, axis=None),  # Reduce on all axes.\n             [\n                 [-3.100753, -2.100753, -1.100753],\n                 [-3.100753, -2.100753, -1.100753],\n@@ -726,6 +733,13 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n                 [-2.407606, -1.407606, -0.407606],\n             ],\n         )\n+        self.assertAllClose(\n+            knn.log_softmax(x),  # Default axis should be -1.\n+            [\n+                [-2.407606, -1.407606, -0.407606],\n+                [-2.407606, -1.407606, -0.407606],\n+            ],\n+        )\n \n     def test_max_pool(self):\n         # Test 1D max pooling.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f8aa455b92781c1cb12dfc45f764d6da46bd0e73", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 109 | Files Changed: 8 | Hunks: 31 | Methods Changed: 31 | Complexity Δ (Sum/Max): -8/1 | Churn Δ: 152 | Churn Cumulative: 11581 | Contributors (this commit): 19 | Commits (past 90d): 116 | Contributors (cumulative): 60 | DMM Complexity: 0.022222222222222223\n\nDIFF:\n@@ -48,11 +48,6 @@ def silu(x):\n     return jnn.silu(x)\n \n \n-def swish(x):\n-    x = convert_to_tensor(x)\n-    return jnn.swish(x)\n-\n-\n def log_sigmoid(x):\n     x = convert_to_tensor(x)\n     return jnn.log_sigmoid(x)\n@@ -83,7 +78,7 @@ def gelu(x, approximate=True):\n     return jnn.gelu(x, approximate)\n \n \n-def softmax(x, axis=-1):\n+def softmax(x, axis=None):\n     x = convert_to_tensor(x)\n     return jnn.softmax(x, axis=axis)\n \n\n@@ -40,10 +40,6 @@ def silu(x):\n     return x * (1.0 / (1.0 + np.exp(-x)))\n \n \n-def swish(x):\n-    return x * (1.0 / (1.0 + np.exp(-x)))\n-\n-\n def log_sigmoid(x):\n     return np.log(1.0 / (1.0 + np.exp(-x)))\n \n\n@@ -41,10 +41,6 @@ def silu(x, beta=1.0):\n     return tf.nn.silu(x, beta=beta)\n \n \n-def swish(x):\n-    return x * sigmoid(x)\n-\n-\n def log_sigmoid(x):\n     return tf.math.log_sigmoid(x)\n \n@@ -74,27 +70,26 @@ def gelu(x, approximate=True):\n     return tf.nn.gelu(x, approximate)\n \n \n-def softmax(x, axis=-1):\n+def softmax(x, axis=None):\n     logits = x\n     if axis is None:\n         # Unlike numpy, tf will handle axis=None as axis=-1.\n         # We need this workaround for the reduction on every dim.\n-        output = tf.reshape(x, [-1])\n-        output = tf.nn.softmax(output, axis=-1)\n-        output = tf.reshape(output, tf.shape(x))\n+        logits_exp = tf.exp(logits)\n+        output = logits_exp / tf.reduce_sum(logits_exp, keepdims=True)\n     else:\n         output = tf.nn.softmax(x, axis=axis)\n     output._keras_logits = logits\n     return output\n \n \n-def log_softmax(x, axis=-1):\n+def log_softmax(x, axis=None):\n     if axis is None:\n         # Unlike numpy, tf will handle axis=None as axis=-1.\n         # We need this workaround for the reduction on every dim.\n-        output = tf.reshape(x, [-1])\n-        output = tf.nn.log_softmax(output, axis=-1)\n-        return tf.reshape(output, tf.shape(x))\n+        logits = x\n+        logits_exp = tf.exp(logits)\n+        return logits - tf.math.log(tf.reduce_sum(logits_exp, keepdims=True))\n     return tf.nn.log_softmax(x, axis=axis)\n \n \n\n@@ -51,11 +51,6 @@ def silu(x, beta=1.0):\n     return x * sigmoid(beta * x)\n \n \n-def swish(x):\n-    x = convert_to_tensor(x)\n-    return silu(x, beta=1)\n-\n-\n def log_sigmoid(x):\n     x = convert_to_tensor(x)\n     return tnn.logsigmoid(x)\n@@ -89,26 +84,24 @@ def gelu(x, approximate=True):\n     return tnn.gelu(x)\n \n \n-def softmax(x, axis=-1):\n-    x = convert_to_tensor(x)\n+def softmax(x, axis=None):\n+    logits = convert_to_tensor(x)\n     if axis is None:\n         # Unlike numpy, PyTorch will handle axis=None as axis=-1.\n         # We need this workaround for the reduction on every dim.\n-        output = torch.reshape(x, [-1])\n-        output = tnn.softmax(output, dim=-1)\n-        return torch.reshape(output, x.shape)\n-    return tnn.softmax(x, dim=axis)\n+        logits_exp = torch.exp(logits)\n+        return logits_exp / torch.sum(logits_exp)\n+    return tnn.softmax(logits, dim=axis)\n \n \n-def log_softmax(x, axis=-1):\n-    x = convert_to_tensor(x)\n+def log_softmax(x, axis=None):\n+    logits = convert_to_tensor(x)\n     if axis is None:\n         # Unlike numpy, PyTorch will handle axis=None as axis=-1.\n         # We need this workaround for the reduction on every dim.\n-        output = torch.reshape(x, [-1])\n-        output = tnn.log_softmax(output, dim=-1)\n-        return torch.reshape(output, x.shape)\n-    return tnn.log_softmax(x, dim=axis)\n+        logits_exp = torch.exp(logits)\n+        return logits - torch.log(torch.sum(logits_exp))\n+    return tnn.log_softmax(logits, dim=axis)\n \n \n def _compute_padding_length(\n\n@@ -419,18 +419,9 @@ class Functional(Function, Model):\n         config[\"output_layers\"] = map_tensors(self._outputs_struct)\n         return copy.deepcopy(config)\n \n-\n-def functional_from_config(cls, config, custom_objects=None):\n-    \"\"\"Instantiates a Functional model from its config (from `get_config()`).\n-\n-    Args:\n-        cls: Class of the model, e.g. a custom subclass of `Model`.\n-        config: Output of `get_config()` for the original model instance.\n-        custom_objects: Optional dict of custom objects.\n-\n-    Returns:\n-        An instance of `cls`.\n-    \"\"\"\n+    @classmethod\n+    def _from_config(cls, config, custom_objects=None):\n+        \"\"\"Instantiates a Model from its config (output of `get_config()`).\"\"\"\n         # Layer instances created during\n         # the graph reconstruction process\n         created_layers = {}\n@@ -540,7 +531,9 @@ def functional_from_config(cls, config, custom_objects=None):\n         def get_tensor(layer_name, node_index, tensor_index):\n             assert layer_name in created_layers\n             layer = created_layers[layer_name]\n-        layer_output_tensors = layer._inbound_nodes[node_index].output_tensors\n+            layer_output_tensors = layer._inbound_nodes[\n+                node_index\n+            ].output_tensors\n             return layer_output_tensors[tensor_index]\n \n         def map_tensors(tensors):\n\n@@ -480,11 +480,9 @@ class Model(Trainer, Layer):\n         if is_functional_config and revivable_as_functional:\n             # Revive Functional model\n             # (but not Functional subclasses with a custom __init__)\n-            from keras_core.models.functional import functional_from_config\n-\n-            return functional_from_config(\n-                cls, config, custom_objects=custom_objects\n-            )\n+            if cls == Model:\n+                cls = Functional\n+            return cls._from_config(config, custom_objects=custom_objects)\n \n         # Either the model has a custom __init__, or the config\n         # does not contain all the information necessary to\n\n@@ -186,7 +186,14 @@ class Silu(Operation):\n         return KerasTensor(x.shape, dtype=x.dtype)\n \n \n-@keras_core_export([\"keras_core.ops.silu\", \"keras_core.ops.nn.silu\"])\n+@keras_core_export(\n+    [\n+        \"keras_core.ops.silu\",\n+        \"keras_core.ops.nn.silu\",\n+        \"keras_core.ops.swish\",\n+        \"keras_core.ops.nn.swish\",\n+    ]\n+)\n def silu(x):\n     \"\"\"Sigmoid-weighted linear unit activation function.\n \n@@ -211,21 +218,6 @@ def silu(x):\n     return backend.nn.silu(x)\n \n \n-class Swish(Operation):\n-    def call(self, x):\n-        return backend.nn.swish(x)\n-\n-    def compute_output_spec(self, x):\n-        return KerasTensor(x.shape, dtype=x.dtype)\n-\n-\n-@keras_core_export([\"keras_core.ops.swish\", \"keras_core.ops.nn.swish\"])\n-def swish(x):\n-    if any_symbolic_tensors((x,)):\n-        return Swish().symbolic_call(x)\n-    return backend.nn.swish(x)\n-\n-\n class LogSigmoid(Operation):\n     def call(self, x):\n         return backend.nn.log_sigmoid(x)\n@@ -443,9 +435,9 @@ def gelu(x, approximate=True):\n \n \n class Softmax(Operation):\n-    def __init__(self, axis=-1):\n+    def __init__(self, axis=None):\n         super().__init__()\n-        self.axis = axis\n+        self.axis = axis if axis is not None else -1\n \n     def call(self, x):\n         return backend.nn.softmax(x, axis=self.axis)\n@@ -455,16 +447,16 @@ class Softmax(Operation):\n \n \n @keras_core_export([\"keras_core.ops.softmax\", \"keras_core.ops.nn.softmax\"])\n-def softmax(x, axis=-1):\n+def softmax(x, axis=None):\n     if any_symbolic_tensors((x,)):\n         return Softmax(axis).symbolic_call(x)\n     return backend.nn.softmax(x, axis=axis)\n \n \n class LogSoftmax(Operation):\n-    def __init__(self, axis=-1):\n+    def __init__(self, axis=None):\n         super().__init__()\n-        self.axis = axis\n+        self.axis = axis if axis is not None else -1\n \n     def call(self, x):\n         return backend.nn.log_softmax(x, axis=self.axis)\n@@ -479,7 +471,7 @@ class LogSoftmax(Operation):\n         \"keras_core.ops.nn.log_softmax\",\n     ]\n )\n-def log_softmax(x, axis=-1):\n+def log_softmax(x, axis=None):\n     if any_symbolic_tensors((x,)):\n         return LogSoftmax(axis).symbolic_call(x)\n     return backend.nn.log_softmax(x, axis=axis)\n\n@@ -34,10 +34,6 @@ class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n         x = KerasTensor([None, 2, 3])\n         self.assertEqual(knn.silu(x).shape, (None, 2, 3))\n \n-    def test_swish(self):\n-        x = KerasTensor([None, 2, 3])\n-        self.assertEqual(knn.swish(x).shape, (None, 2, 3))\n-\n     def test_log_sigmoid(self):\n         x = KerasTensor([None, 2, 3])\n         self.assertEqual(knn.log_sigmoid(x).shape, (None, 2, 3))\n@@ -328,10 +324,6 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor([1, 2, 3])\n         self.assertEqual(knn.silu(x).shape, (1, 2, 3))\n \n-    def test_swish(self):\n-        x = KerasTensor([1, 2, 3])\n-        self.assertEqual(knn.swish(x).shape, (1, 2, 3))\n-\n     def test_log_sigmoid(self):\n         x = KerasTensor([1, 2, 3])\n         self.assertEqual(knn.log_sigmoid(x).shape, (1, 2, 3))\n@@ -633,12 +625,6 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             [-0.26894143, 0, 0.7310586, 1.7615942, 2.8577223],\n         )\n \n-    def test_swish(self):\n-        x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n-        self.assertAllClose(\n-            knn.swish(x), [-0.26894143, 0.0, 0.7310586, 1.7615943, 2.8577223]\n-        )\n-\n     def test_log_sigmoid(self):\n         x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n         self.assertAllClose(\n@@ -688,7 +674,7 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n     def test_softmax(self):\n         x = np.array([[1, 2, 3], [1, 2, 3]], dtype=np.float32)\n         self.assertAllClose(\n-            knn.softmax(x, axis=None),  # Reduce on all axes.\n+            knn.softmax(x),\n             [[0.045015, 0.122364, 0.33262], [0.045015, 0.122364, 0.33262]],\n         )\n         self.assertAllClose(\n@@ -702,18 +688,11 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n                 [0.09003057, 0.24472848, 0.66524094],\n             ],\n         )\n-        self.assertAllClose(\n-            knn.softmax(x),  # Default axis should be -1.\n-            [\n-                [0.09003057, 0.24472848, 0.66524094],\n-                [0.09003057, 0.24472848, 0.66524094],\n-            ],\n-        )\n \n     def test_log_softmax(self):\n         x = np.array([[1, 2, 3], [1, 2, 3]], dtype=np.float32)\n         self.assertAllClose(\n-            knn.log_softmax(x, axis=None),  # Reduce on all axes.\n+            knn.log_softmax(x),\n             [\n                 [-3.100753, -2.100753, -1.100753],\n                 [-3.100753, -2.100753, -1.100753],\n@@ -733,13 +712,6 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n                 [-2.407606, -1.407606, -0.407606],\n             ],\n         )\n-        self.assertAllClose(\n-            knn.log_softmax(x),  # Default axis should be -1.\n-            [\n-                [-2.407606, -1.407606, -0.407606],\n-                [-2.407606, -1.407606, -0.407606],\n-            ],\n-        )\n \n     def test_max_pool(self):\n         # Test 1D max pooling.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#14162fe777e6733112a9149c8aa31fec786dd615", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 61 | Lines Deleted: 35 | Files Changed: 7 | Hunks: 22 | Methods Changed: 23 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 96 | Churn Cumulative: 11146 | Contributors (this commit): 19 | Commits (past 90d): 120 | Contributors (cumulative): 57 | DMM Complexity: 0.9375\n\nDIFF:\n@@ -78,7 +78,7 @@ def gelu(x, approximate=True):\n     return jnn.gelu(x, approximate)\n \n \n-def softmax(x, axis=None):\n+def softmax(x, axis=-1):\n     x = convert_to_tensor(x)\n     return jnn.softmax(x, axis=axis)\n \n\n@@ -70,26 +70,27 @@ def gelu(x, approximate=True):\n     return tf.nn.gelu(x, approximate)\n \n \n-def softmax(x, axis=None):\n+def softmax(x, axis=-1):\n     logits = x\n     if axis is None:\n         # Unlike numpy, tf will handle axis=None as axis=-1.\n         # We need this workaround for the reduction on every dim.\n-        logits_exp = tf.exp(logits)\n-        output = logits_exp / tf.reduce_sum(logits_exp, keepdims=True)\n+        output = tf.reshape(x, [-1])\n+        output = tf.nn.softmax(output, axis=-1)\n+        output = tf.reshape(output, tf.shape(x))\n     else:\n         output = tf.nn.softmax(x, axis=axis)\n     output._keras_logits = logits\n     return output\n \n \n-def log_softmax(x, axis=None):\n+def log_softmax(x, axis=-1):\n     if axis is None:\n         # Unlike numpy, tf will handle axis=None as axis=-1.\n         # We need this workaround for the reduction on every dim.\n-        logits = x\n-        logits_exp = tf.exp(logits)\n-        return logits - tf.math.log(tf.reduce_sum(logits_exp, keepdims=True))\n+        output = tf.reshape(x, [-1])\n+        output = tf.nn.log_softmax(output, axis=-1)\n+        return tf.reshape(output, tf.shape(x))\n     return tf.nn.log_softmax(x, axis=axis)\n \n \n\n@@ -84,24 +84,26 @@ def gelu(x, approximate=True):\n     return tnn.gelu(x)\n \n \n-def softmax(x, axis=None):\n-    logits = convert_to_tensor(x)\n+def softmax(x, axis=-1):\n+    x = convert_to_tensor(x)\n     if axis is None:\n         # Unlike numpy, PyTorch will handle axis=None as axis=-1.\n         # We need this workaround for the reduction on every dim.\n-        logits_exp = torch.exp(logits)\n-        return logits_exp / torch.sum(logits_exp)\n-    return tnn.softmax(logits, dim=axis)\n+        output = torch.reshape(x, [-1])\n+        output = tnn.softmax(output, dim=-1)\n+        return torch.reshape(output, x.shape)\n+    return tnn.softmax(x, dim=axis)\n \n \n-def log_softmax(x, axis=None):\n-    logits = convert_to_tensor(x)\n+def log_softmax(x, axis=-1):\n+    x = convert_to_tensor(x)\n     if axis is None:\n         # Unlike numpy, PyTorch will handle axis=None as axis=-1.\n         # We need this workaround for the reduction on every dim.\n-        logits_exp = torch.exp(logits)\n-        return logits - torch.log(torch.sum(logits_exp))\n-    return tnn.log_softmax(logits, dim=axis)\n+        output = torch.reshape(x, [-1])\n+        output = tnn.log_softmax(output, dim=-1)\n+        return torch.reshape(output, x.shape)\n+    return tnn.log_softmax(x, dim=axis)\n \n \n def _compute_padding_length(\n\n@@ -419,9 +419,18 @@ class Functional(Function, Model):\n         config[\"output_layers\"] = map_tensors(self._outputs_struct)\n         return copy.deepcopy(config)\n \n-    @classmethod\n-    def _from_config(cls, config, custom_objects=None):\n-        \"\"\"Instantiates a Model from its config (output of `get_config()`).\"\"\"\n+\n+def functional_from_config(cls, config, custom_objects=None):\n+    \"\"\"Instantiates a Functional model from its config (from `get_config()`).\n+\n+    Args:\n+        cls: Class of the model, e.g. a custom subclass of `Model`.\n+        config: Output of `get_config()` for the original model instance.\n+        custom_objects: Optional dict of custom objects.\n+\n+    Returns:\n+        An instance of `cls`.\n+    \"\"\"\n     # Layer instances created during\n     # the graph reconstruction process\n     created_layers = {}\n@@ -531,9 +540,7 @@ class Functional(Function, Model):\n     def get_tensor(layer_name, node_index, tensor_index):\n         assert layer_name in created_layers\n         layer = created_layers[layer_name]\n-            layer_output_tensors = layer._inbound_nodes[\n-                node_index\n-            ].output_tensors\n+        layer_output_tensors = layer._inbound_nodes[node_index].output_tensors\n         return layer_output_tensors[tensor_index]\n \n     def map_tensors(tensors):\n\n@@ -480,9 +480,11 @@ class Model(Trainer, Layer):\n         if is_functional_config and revivable_as_functional:\n             # Revive Functional model\n             # (but not Functional subclasses with a custom __init__)\n-            if cls == Model:\n-                cls = Functional\n-            return cls._from_config(config, custom_objects=custom_objects)\n+            from keras_core.models.functional import functional_from_config\n+\n+            return functional_from_config(\n+                cls, config, custom_objects=custom_objects\n+            )\n \n         # Either the model has a custom __init__, or the config\n         # does not contain all the information necessary to\n\n@@ -435,9 +435,9 @@ def gelu(x, approximate=True):\n \n \n class Softmax(Operation):\n-    def __init__(self, axis=None):\n+    def __init__(self, axis=-1):\n         super().__init__()\n-        self.axis = axis if axis is not None else -1\n+        self.axis = axis\n \n     def call(self, x):\n         return backend.nn.softmax(x, axis=self.axis)\n@@ -447,16 +447,16 @@ class Softmax(Operation):\n \n \n @keras_core_export([\"keras_core.ops.softmax\", \"keras_core.ops.nn.softmax\"])\n-def softmax(x, axis=None):\n+def softmax(x, axis=-1):\n     if any_symbolic_tensors((x,)):\n         return Softmax(axis).symbolic_call(x)\n     return backend.nn.softmax(x, axis=axis)\n \n \n class LogSoftmax(Operation):\n-    def __init__(self, axis=None):\n+    def __init__(self, axis=-1):\n         super().__init__()\n-        self.axis = axis if axis is not None else -1\n+        self.axis = axis\n \n     def call(self, x):\n         return backend.nn.log_softmax(x, axis=self.axis)\n@@ -471,7 +471,7 @@ class LogSoftmax(Operation):\n         \"keras_core.ops.nn.log_softmax\",\n     ]\n )\n-def log_softmax(x, axis=None):\n+def log_softmax(x, axis=-1):\n     if any_symbolic_tensors((x,)):\n         return LogSoftmax(axis).symbolic_call(x)\n     return backend.nn.log_softmax(x, axis=axis)\n\n@@ -674,7 +674,7 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n     def test_softmax(self):\n         x = np.array([[1, 2, 3], [1, 2, 3]], dtype=np.float32)\n         self.assertAllClose(\n-            knn.softmax(x),\n+            knn.softmax(x, axis=None),  # Reduce on all axes.\n             [[0.045015, 0.122364, 0.33262], [0.045015, 0.122364, 0.33262]],\n         )\n         self.assertAllClose(\n@@ -688,11 +688,18 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n                 [0.09003057, 0.24472848, 0.66524094],\n             ],\n         )\n+        self.assertAllClose(\n+            knn.softmax(x),  # Default axis should be -1.\n+            [\n+                [0.09003057, 0.24472848, 0.66524094],\n+                [0.09003057, 0.24472848, 0.66524094],\n+            ],\n+        )\n \n     def test_log_softmax(self):\n         x = np.array([[1, 2, 3], [1, 2, 3]], dtype=np.float32)\n         self.assertAllClose(\n-            knn.log_softmax(x),\n+            knn.log_softmax(x, axis=None),  # Reduce on all axes.\n             [\n                 [-3.100753, -2.100753, -1.100753],\n                 [-3.100753, -2.100753, -1.100753],\n@@ -712,6 +719,13 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n                 [-2.407606, -1.407606, -0.407606],\n             ],\n         )\n+        self.assertAllClose(\n+            knn.log_softmax(x),  # Default axis should be -1.\n+            [\n+                [-2.407606, -1.407606, -0.407606],\n+                [-2.407606, -1.407606, -0.407606],\n+            ],\n+        )\n \n     def test_max_pool(self):\n         # Test 1D max pooling.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#73b22e0f68ca19afc5f71e416ecdfb50080770f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 80 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 83 | Churn Cumulative: 640 | Contributors (this commit): 6 | Commits (past 90d): 15 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -57,7 +57,7 @@ class RandomFlip(TFDataLayer):\n         if self.mode == HORIZONTAL or self.mode == HORIZONTAL_AND_VERTICAL:\n             flipped_outputs = self.backend.numpy.where(\n                 self.backend.random.uniform(\n-                    shape=(batch_size,), seed=seed_generator\n+                    shape=(batch_size, 1, 1, 1), seed=seed_generator\n                 )\n                 <= 0.5,\n                 self.backend.numpy.flip(flipped_outputs, axis=-2),\n@@ -66,7 +66,7 @@ class RandomFlip(TFDataLayer):\n         if self.mode == VERTICAL or self.mode == HORIZONTAL_AND_VERTICAL:\n             flipped_outputs = self.backend.numpy.where(\n                 self.backend.random.uniform(\n-                    shape=(batch_size,), seed=seed_generator\n+                    shape=(batch_size, 1, 1, 1), seed=seed_generator\n                 )\n                 <= 0.5,\n                 self.backend.numpy.flip(flipped_outputs, axis=-3),\n\n@@ -15,7 +15,7 @@ class MockedRandomFlip(layers.RandomFlip):\n         unbatched = len(inputs.shape) == 3\n         batch_size = 1 if unbatched else self.backend.shape(inputs)[0]\n         mocked_value = self.backend.numpy.full(\n-            (batch_size,), 0.1, dtype=\"float32\"\n+            (batch_size, 1, 1, 1), 0.1, dtype=\"float32\"\n         )\n         with unittest.mock.patch.object(\n             self.backend.random,\n@@ -48,6 +48,7 @@ class RandomFlipTest(testing.TestCase, parameterized.TestCase):\n     def test_random_flip_horizontal(self):\n         run_training_check = False if backend.backend() == \"numpy\" else True\n         utils.set_random_seed(0)\n+        # Test 3D input: shape (1*2*3)\n         self.run_layer_test(\n             MockedRandomFlip,\n             init_kwargs={\n@@ -59,10 +60,33 @@ class RandomFlipTest(testing.TestCase, parameterized.TestCase):\n             supports_masking=False,\n             run_training_check=run_training_check,\n         )\n+        # Test 4D input: shape (2*1*2*3)\n+        self.run_layer_test(\n+            MockedRandomFlip,\n+            init_kwargs={\n+                \"mode\": \"horizontal\",\n+                \"seed\": 42,\n+            },\n+            input_data=np.asarray(\n+                [\n+                    [[[2, 3, 4], [5, 6, 7]]],\n+                    [[[2, 3, 4], [5, 6, 7]]],\n+                ]\n+            ),\n+            expected_output=backend.convert_to_tensor(\n+                [\n+                    [[[5, 6, 7], [2, 3, 4]]],\n+                    [[[5, 6, 7], [2, 3, 4]]],\n+                ]\n+            ),\n+            supports_masking=False,\n+            run_training_check=run_training_check,\n+        )\n \n     def test_random_flip_vertical(self):\n         run_training_check = False if backend.backend() == \"numpy\" else True\n         utils.set_random_seed(0)\n+        # Test 3D input: shape (2*1*3)\n         self.run_layer_test(\n             MockedRandomFlip,\n             init_kwargs={\n@@ -76,8 +100,37 @@ class RandomFlipTest(testing.TestCase, parameterized.TestCase):\n             supports_masking=False,\n             run_training_check=run_training_check,\n         )\n+        # Test 4D input: shape (2*2*1*3)\n+        self.run_layer_test(\n+            MockedRandomFlip,\n+            init_kwargs={\n+                \"mode\": \"vertical\",\n+                \"seed\": 42,\n+            },\n+            input_data=np.asarray(\n+                [\n+                    [\n+                        [[2, 3, 4]],\n+                        [[5, 6, 7]],\n+                    ],\n+                    [\n+                        [[2, 3, 4]],\n+                        [[5, 6, 7]],\n+                    ],\n+                ]\n+            ),\n+            expected_output=backend.convert_to_tensor(\n+                [\n+                    [[[5, 6, 7]], [[2, 3, 4]]],\n+                    [[[5, 6, 7]], [[2, 3, 4]]],\n+                ]\n+            ),\n+            supports_masking=False,\n+            run_training_check=run_training_check,\n+        )\n \n     def test_tf_data_compatibility(self):\n+        # Test 3D input: shape (2, 1, 3)\n         layer = layers.RandomFlip(\"vertical\", seed=42)\n         input_data = np.array([[[2, 3, 4]], [[5, 6, 7]]])\n         expected_output = np.array([[[5, 6, 7]], [[2, 3, 4]]])\n@@ -85,3 +138,27 @@ class RandomFlipTest(testing.TestCase, parameterized.TestCase):\n         for output in ds.take(1):\n             output = output.numpy()\n         self.assertAllClose(output, expected_output)\n+        # Test 4D input: shape (2, 2, 1, 3)\n+        layer = layers.RandomFlip(\"vertical\", seed=42)\n+        input_data = np.array(\n+            [\n+                [\n+                    [[2, 3, 4]],\n+                    [[5, 6, 7]],\n+                ],\n+                [\n+                    [[2, 3, 4]],\n+                    [[5, 6, 7]],\n+                ],\n+            ]\n+        )\n+        expected_output = np.array(\n+            [\n+                [[[5, 6, 7]], [[2, 3, 4]]],\n+                [[[5, 6, 7]], [[2, 3, 4]]],\n+            ]\n+        )\n+        ds = tf.data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n+        for output in ds.take(1):\n+            output = output.numpy()\n+        self.assertAllClose(output, expected_output)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e0194197e7ac114c2c374a1f457c04ee8a2de0f5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 0 | Files Changed: 12 | Hunks: 24 | Methods Changed: 12 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 30 | Churn Cumulative: 6999 | Contributors (this commit): 13 | Commits (past 90d): 79 | Contributors (cumulative): 47 | DMM Complexity: 0.6896551724137931\n\nDIFF:\n@@ -50,6 +50,7 @@ class Adadelta(optimizer.Optimizer):\n         ema_momentum=0.99,\n         ema_overwrite_frequency=None,\n         name=\"adadelta\",\n+        **kwargs,\n     ):\n         super().__init__(\n             learning_rate=learning_rate,\n@@ -61,6 +62,7 @@ class Adadelta(optimizer.Optimizer):\n             ema_momentum=ema_momentum,\n             ema_overwrite_frequency=ema_overwrite_frequency,\n             name=name,\n+            **kwargs,\n         )\n         self.rho = rho\n         self.epsilon = epsilon\n\n@@ -57,6 +57,7 @@ class Adafactor(optimizer.Optimizer):\n         ema_momentum=0.99,\n         ema_overwrite_frequency=None,\n         name=\"adafactor\",\n+        **kwargs,\n     ):\n         super().__init__(\n             learning_rate=learning_rate,\n@@ -68,6 +69,7 @@ class Adafactor(optimizer.Optimizer):\n             use_ema=use_ema,\n             ema_momentum=ema_momentum,\n             ema_overwrite_frequency=ema_overwrite_frequency,\n+            **kwargs,\n         )\n         self.beta_2_decay = beta_2_decay\n         self.epsilon_1 = epsilon_1\n\n@@ -45,6 +45,7 @@ class Adagrad(optimizer.Optimizer):\n         ema_momentum=0.99,\n         ema_overwrite_frequency=None,\n         name=\"adagrad\",\n+        **kwargs,\n     ):\n         super().__init__(\n             learning_rate=learning_rate,\n@@ -56,6 +57,7 @@ class Adagrad(optimizer.Optimizer):\n             ema_momentum=ema_momentum,\n             ema_overwrite_frequency=ema_overwrite_frequency,\n             name=name,\n+            **kwargs,\n         )\n         self.initial_accumulator_value = initial_accumulator_value\n         self.epsilon = epsilon\n\n@@ -55,6 +55,7 @@ class Adam(optimizer.Optimizer):\n         ema_momentum=0.99,\n         ema_overwrite_frequency=None,\n         name=\"adam\",\n+        **kwargs,\n     ):\n         super().__init__(\n             learning_rate=learning_rate,\n@@ -66,6 +67,7 @@ class Adam(optimizer.Optimizer):\n             use_ema=use_ema,\n             ema_momentum=ema_momentum,\n             ema_overwrite_frequency=ema_overwrite_frequency,\n+            **kwargs,\n         )\n         self.beta_1 = beta_1\n         self.beta_2 = beta_2\n\n@@ -64,6 +64,7 @@ class Adamax(optimizer.Optimizer):\n         ema_momentum=0.99,\n         ema_overwrite_frequency=None,\n         name=\"adamax\",\n+        **kwargs,\n     ):\n         super().__init__(\n             learning_rate=learning_rate,\n@@ -75,6 +76,7 @@ class Adamax(optimizer.Optimizer):\n             use_ema=use_ema,\n             ema_momentum=ema_momentum,\n             ema_overwrite_frequency=ema_overwrite_frequency,\n+            **kwargs,\n         )\n         self.beta_1 = beta_1\n         self.beta_2 = beta_2\n\n@@ -65,6 +65,7 @@ class AdamW(adam.Adam):\n         ema_momentum=0.99,\n         ema_overwrite_frequency=None,\n         name=\"adamw\",\n+        **kwargs,\n     ):\n         super().__init__(\n             learning_rate=learning_rate,\n@@ -80,6 +81,7 @@ class AdamW(adam.Adam):\n             use_ema=use_ema,\n             ema_momentum=ema_momentum,\n             ema_overwrite_frequency=ema_overwrite_frequency,\n+            **kwargs,\n         )\n \n         if self.weight_decay is None:\n\n@@ -22,9 +22,17 @@ class BaseOptimizer:\n         ema_momentum=0.99,\n         ema_overwrite_frequency=None,\n         name=None,\n+        **kwargs,\n     ):\n         self._lock = False\n \n+        if kwargs.pop(\"decay\", None) is not None:\n+            warnings.warn(\n+                \"Argument `decay` is no longer supported and will be ignored.\"\n+            )\n+        if kwargs:\n+            raise ValueError(f\"Argument(s) not recognized: {kwargs}\")\n+\n         self.name = name\n         self.weight_decay = weight_decay\n         self.clipnorm = clipnorm\n\n@@ -92,6 +92,7 @@ class Ftrl(optimizer.Optimizer):\n         ema_momentum=0.99,\n         ema_overwrite_frequency=None,\n         name=\"ftrl\",\n+        **kwargs,\n     ):\n         super().__init__(\n             learning_rate=learning_rate,\n@@ -103,6 +104,7 @@ class Ftrl(optimizer.Optimizer):\n             use_ema=use_ema,\n             ema_momentum=ema_momentum,\n             ema_overwrite_frequency=ema_overwrite_frequency,\n+            **kwargs,\n         )\n \n         if initial_accumulator_value < 0.0:\n\n@@ -54,6 +54,7 @@ class Lion(optimizer.Optimizer):\n         ema_momentum=0.99,\n         ema_overwrite_frequency=None,\n         name=\"lion\",\n+        **kwargs,\n     ):\n         super().__init__(\n             learning_rate=learning_rate,\n@@ -65,6 +66,7 @@ class Lion(optimizer.Optimizer):\n             use_ema=use_ema,\n             ema_momentum=ema_momentum,\n             ema_overwrite_frequency=ema_overwrite_frequency,\n+            **kwargs,\n         )\n         self.beta_1 = beta_1\n         self.beta_2 = beta_2\n\n@@ -50,6 +50,7 @@ class Nadam(optimizer.Optimizer):\n         ema_momentum=0.99,\n         ema_overwrite_frequency=None,\n         name=\"nadam\",\n+        **kwargs,\n     ):\n         super().__init__(\n             learning_rate=learning_rate,\n@@ -61,6 +62,7 @@ class Nadam(optimizer.Optimizer):\n             use_ema=use_ema,\n             ema_momentum=ema_momentum,\n             ema_overwrite_frequency=ema_overwrite_frequency,\n+            **kwargs,\n         )\n         self.beta_1 = beta_1\n         self.beta_2 = beta_2\n\n@@ -65,6 +65,7 @@ class RMSprop(optimizer.Optimizer):\n         ema_momentum=0.99,\n         ema_overwrite_frequency=100,\n         name=\"rmsprop\",\n+        **kwargs,\n     ):\n         super().__init__(\n             learning_rate=learning_rate,\n@@ -76,6 +77,7 @@ class RMSprop(optimizer.Optimizer):\n             ema_momentum=ema_momentum,\n             ema_overwrite_frequency=ema_overwrite_frequency,\n             name=name,\n+            **kwargs,\n         )\n         self.rho = rho\n         self.momentum = momentum\n\n@@ -53,6 +53,7 @@ class SGD(optimizer.Optimizer):\n         ema_momentum=0.99,\n         ema_overwrite_frequency=None,\n         name=\"SGD\",\n+        **kwargs,\n     ):\n         super().__init__(\n             learning_rate=learning_rate,\n@@ -64,6 +65,7 @@ class SGD(optimizer.Optimizer):\n             use_ema=use_ema,\n             ema_momentum=ema_momentum,\n             ema_overwrite_frequency=ema_overwrite_frequency,\n+            **kwargs,\n         )\n         if not isinstance(momentum, float) or momentum < 0 or momentum > 1:\n             raise ValueError(\"`momentum` must be a float between [0, 1].\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
