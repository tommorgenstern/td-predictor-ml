{"custom_id": "keras#ce4ecc6cc29fcc8a75512f845988994cb07a02ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 190 | Files Changed: 3 | Hunks: 3 | Methods Changed: 9 | Complexity Δ (Sum/Max): -13/0 | Churn Δ: 191 | Churn Cumulative: 1115 | Contributors (this commit): 5 | Commits (past 90d): 15 | Contributors (cumulative): 10 | DMM Complexity: 0.0\n\nDIFF:\n@@ -42,7 +42,7 @@ else:\n \n     class keras_core_export:\n         def __init__(self, path):\n-            pass\n+            self.path = path\n \n         def __call__(self, symbol):\n             register_internal_serializable(self.path, symbol)\n\n@@ -1,144 +0,0 @@\n-from keras_core import operations as ops\n-from keras_core.optimizers import optimizer\n-\n-\n-class Adam(optimizer.Optimizer):\n-    \"\"\"Optimizer that implements the Adam algorithm.\n-\n-    Adam optimization is a stochastic gradient descent method that is based on\n-    adaptive estimation of first-order and second-order moments.\n-\n-    According to\n-    [Kingma et al., 2014](http://arxiv.org/abs/1412.6980),\n-    the method is \"*computationally\n-    efficient, has little memory requirement, invariant to diagonal rescaling of\n-    gradients, and is well suited for problems that are large in terms of\n-    data/parameters*\".\n-\n-    Args:\n-      learning_rate: A `Tensor`, floating point value, a schedule that is a\n-        `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n-        that takes no arguments and returns the actual value to use. The\n-        learning rate. Defaults to `0.001`.\n-      beta_1: A float value or a constant float tensor, or a callable\n-        that takes no arguments and returns the actual value to use. The\n-        exponential decay rate for the 1st moment estimates. Defaults to `0.9`.\n-      beta_2: A float value or a constant float tensor, or a callable\n-        that takes no arguments and returns the actual value to use. The\n-        exponential decay rate for the 2nd moment estimates. Defaults to\n-        `0.999`.\n-      epsilon: A small constant for numerical stability. This epsilon is\n-        \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n-        Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n-        `1e-7`.\n-      amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm from\n-        the paper \"On the Convergence of Adam and beyond\". Defaults to `False`.\n-      {{base_optimizer_keyword_args}}\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        learning_rate=0.001,\n-        beta_1=0.9,\n-        beta_2=0.999,\n-        epsilon=1e-7,\n-        amsgrad=False,\n-        weight_decay=None,\n-        clipnorm=None,\n-        clipvalue=None,\n-        global_clipnorm=None,\n-        use_ema=False,\n-        ema_momentum=0.99,\n-        ema_overwrite_frequency=None,\n-        name=\"Adam\",\n-        **kwargs\n-    ):\n-        super().__init__(\n-            learning_rate=learning_rate\n-            name=name,\n-            weight_decay=weight_decay,\n-            clipnorm=clipnorm,\n-            clipvalue=clipvalue,\n-            global_clipnorm=global_clipnorm,\n-            use_ema=use_ema,\n-            ema_momentum=ema_momentum,\n-            ema_overwrite_frequency=ema_overwrite_frequency,\n-            **kwargs\n-        )\n-        self.beta_1 = beta_1\n-        self.beta_2 = beta_2\n-        self.epsilon = epsilon\n-        self.amsgrad = amsgrad\n-\n-    def build(self, var_list):\n-        \"\"\"Initialize optimizer variables.\n-\n-        Adam optimizer has 3 types of variables: momentums, velocities and\n-        velocity_hat (only set when amsgrad is applied),\n-\n-        Args:\n-          var_list: list of model variables to build Adam variables on.\n-        \"\"\"\n-        if self.built:\n-            return\n-        super().build(var_list)\n-        self._momentums = []\n-        self._velocities = []\n-        for var in var_list:\n-            self._momentums.append(\n-                self.add_variable_from_reference(\n-                    model_variable=var, variable_name=\"m\"\n-                )\n-            )\n-            self._velocities.append(\n-                self.add_variable_from_reference(\n-                    model_variable=var, variable_name=\"v\"\n-                )\n-            )\n-        if self.amsgrad:\n-            self._velocity_hats = []\n-            for var in var_list:\n-                self._velocity_hats.append(\n-                    self.add_variable_from_reference(\n-                        model_variable=var, variable_name=\"vhat\"\n-                    )\n-                )\n-\n-    def update_step(self, gradient, variable, learning_rate):\n-        \"\"\"Update step given gradient and the associated model variable.\"\"\"\n-        beta_1_power = None\n-        beta_2_power = None\n-        lr = ops.cast(self.learning_rate, variable.dtype)\n-        local_step = ops.cast(self.iterations + 1, variable.dtype)\n-        beta_1_power = ops.power(ops.cast(self.beta_1, variable.dtype), local_step)\n-        beta_2_power = ops.power(ops.cast(self.beta_2, variable.dtype), local_step)\n-\n-        m = self._momentums[self._get_variable_index(variable)]\n-        v = self._velocities[self._get_variable_index(variable)]\n-\n-        alpha = lr * ops.sqrt(1 - beta_2_power) / (1 - beta_1_power)\n-\n-        m.assign(m + (gradient - m) * (1 - self.beta_1))\n-        v.assign(v + (ops.square(gradient) - v) * (1 - self.beta_2))\n-        if self.amsgrad:\n-            v_hat = self._velocity_hats[self._get_variable_index(variable)]\n-            v_hat.assign(ops.maximum(v_hat, v))\n-            v = v_hat\n-        variable.assign(m - (m * alpha) / (ops.sqrt(v) + self.epsilon))\n-\n-    def get_config(self):\n-        config = super().get_config()\n-        config.update(\n-            {\n-                \"beta_1\": self.beta_1,\n-                \"beta_2\": self.beta_2,\n-                \"epsilon\": self.epsilon,\n-                \"amsgrad\": self.amsgrad,\n-            }\n-        )\n-        return config\n-\n-\n-Adam.__doc__ = Adam.__doc__.replace(\n-    \"{{base_optimizer_keyword_args}}\", optimizer.base_optimizer_keyword_args\n-)\n\\ No newline at end of file\n\n@@ -1,45 +0,0 @@\n-from keras_core import backend\n-from keras_core import operations as ops\n-from keras_core import testing\n-from keras_core.optimizers.adam import Adam\n-import numpy as np\n-\n-class AdamTest(testing.TestCase):\n-    def test_config(self):\n-        # TODO\n-        pass\n-\n-    def test_weight_decay(self):\n-        grads, var1, var2, var3 = (\n-            np.zeros(()),\n-            backend.Variable(2.0),\n-            backend.Variable(2.0, name=\"exclude\"),\n-            backend.Variable(2.0),\n-        )\n-        optimizer_1 = Adam(learning_rate=1, weight_decay=0.004)\n-        optimizer_1.apply_gradients(zip([grads], [var1]))\n-\n-        optimizer_2 = Adam(learning_rate=1, weight_decay=0.004)\n-        optimizer_2.exclude_from_weight_decay(var_names=[\"exclude\"])\n-        optimizer_2.apply_gradients(zip([grads, grads], [var1, var2]))\n-\n-        optimizer_3 = Adam(learning_rate=1, weight_decay=0.004)\n-        optimizer_3.exclude_from_weight_decay(var_list=[var3])\n-        optimizer_3.apply_gradients(zip([grads, grads], [var1, var3]))\n-\n-        self.assertAlmostEqual(var1, 1.9760959)\n-        self.assertAlmostEqual(var2, 2.0)\n-        self.assertAlmostEqual(var3, 2.0)\n-\n-    def test_clip_norm(self):\n-        optimizer = Adam(clipnorm=1)\n-        grad = [np.array([100.0, 100.0])]\n-        clipped_grad = optimizer._clip_gradients(grad)\n-        self.assertAllClose(clipped_grad[0], [2**0.5 / 2, 2**0.5 / 2])\n-\n-    def test_clip_value(self):\n-        optimizer = Adam(clipvalue=1)\n-        grad = [np.array([100.0, 100.0])]\n-        clipped_grad = optimizer._clip_gradients(grad)\n-        self.assertAllClose(clipped_grad[0], [1.0, 1.0])\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#295f9a5f5ff9d19020da8799eefcd8e7410f979d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 400 | Lines Deleted: 73 | Files Changed: 10 | Hunks: 43 | Methods Changed: 30 | Complexity Δ (Sum/Max): 43/38 | Churn Δ: 473 | Churn Cumulative: 4029 | Contributors (this commit): 7 | Commits (past 90d): 125 | Contributors (cumulative): 30 | DMM Complexity: 0.38735177865612647\n\nDIFF:\n@@ -85,7 +85,7 @@ class KerasTensor:\n     def __repr__(self):\n         return (\n             f\"<KerasTensor shape={self.shape}, dtype={self.dtype}, \"\n-            \"name={self.name}>\"\n+            f\"name={self.name}>\"\n         )\n \n     def __iter__(self):\n\n@@ -127,8 +127,10 @@ class Layer(Operation):\n         if config:\n             if \"input_shape\" in config:\n                 self.build(config[\"input_shape\"])\n+                self._build_shapes_dict = config\n             elif \"shapes_dict\" in config:\n                 self.build(**config[\"shapes_dict\"])\n+                self._build_shapes_dict = config[\"shapes_dict\"]\n \n     def add_variable(\n         self,\n\n@@ -1,3 +1,4 @@\n+import copy\n import inspect\n import warnings\n \n@@ -33,11 +34,8 @@ class Functional(Function, Model):\n         return Function.__new__(cls)\n \n     @tracking.no_automatic_dependency_tracking\n+    @python_utils.default\n     def __init__(self, inputs, outputs, name=None, **kwargs):\n-        # This is used by the Model class, since we have some logic to swap the\n-        # class in the __new__ method, which will lead to __init__ get invoked\n-        # twice. Using the skip_init to skip one of the invocation of __init__\n-        # to avoid any side effects\n         if isinstance(inputs, dict):\n             for k, v in inputs.items():\n                 if not isinstance(v, backend.KerasTensor):\n@@ -87,7 +85,7 @@ class Functional(Function, Model):\n                 f\"Unrecognized type for `outputs`: {outputs} (of type {type(outputs)})\"\n             )\n \n-        super().__init__(inputs, outputs, name=name, **kwargs)\n+        Function.__init__(self, inputs, outputs, name=name, **kwargs)\n         self._layers = self.layers\n         self.built = True\n \n@@ -108,9 +106,10 @@ class Functional(Function, Model):\n             masks = self._flatten_to_reference_inputs(mask)\n             for x, mask in zip(inputs, masks):\n                 x._keras_mask = mask\n-        return self._run_through_graph(\n+        outputs = self._run_through_graph(\n             inputs, operation_fn=lambda op: operation_fn(op, training=training)\n         )\n+        return unpack_singleton(outputs)\n \n     def compute_output_spec(self, inputs, training=None, mask=None):\n         # From Function\n@@ -188,35 +187,202 @@ class Functional(Function, Model):\n         # Symbolic only. TODO\n         raise NotImplementedError\n \n-    @python_utils.default\n     def get_config(self):\n-        # Prepare base arguments\n+        if not functional_like_constructor(self.__class__):\n+            # Subclassed networks are not serializable\n+            # (unless serialization is implemented by\n+            # the author of the subclassed network).\n+            return Model.get_config()\n+\n         config = {\n             \"name\": self.name,\n             \"trainable\": self.trainable,\n         }\n-        # Check whether the class has a constructor compatible with a Functional\n-        # model or if it has a custom constructor.\n-        if functional_like_constructor(self.__class__):\n-            # Only return a Functional config if the constructor is the same\n-            # as that of a Functional model. This excludes subclassed Functional\n-            # models with a custom __init__.\n-            config = {**config, **get_functional_config(self)}\n+        # Build a map from a layer unique name (make_node_key)\n+        # to the index of the nodes that are saved in the config.\n+        # Only nodes in network_nodes are saved.\n+        node_reindexing_map = {}\n+        for operation in self.operations:\n+            if issubclass(operation.__class__, Functional):\n+                # Functional models start with a pre-existing node\n+                # linking their input to output.\n+                kept_nodes = 1\n             else:\n-            # Try to autogenerate config\n-            xtra_args = set(config.keys())\n-            if getattr(self, \"_auto_get_config\", False):\n-                config.update(self._auto_config.config)\n-            # Remove args non explicitly supported\n-            argspec = inspect.getfullargspec(self.__init__)\n-            if argspec.varkw != \"kwargs\":\n-                for key in xtra_args - xtra_args.intersection(argspec.args[1:]):\n-                    config.pop(key, None)\n-        return config\n+                kept_nodes = 0\n+            for original_node_index, node in enumerate(\n+                operation._inbound_nodes\n+            ):\n+                node_key = make_node_key(operation, original_node_index)\n+                if node_key in self._nodes:\n+                    # i.e. we mark it to be saved\n+                    node_reindexing_map[node_key] = kept_nodes\n+                    kept_nodes += 1\n+\n+        # serialize and save the layers in layer_configs\n+        layer_configs = []\n+        for operation in self.operations:  # From the earliest layers on.\n+            filtered_inbound_nodes = []\n+            for original_node_index, node in enumerate(\n+                operation._inbound_nodes\n+            ):\n+                node_key = make_node_key(operation, original_node_index)\n+                if node_key in self._nodes:\n+                    # The node is relevant to the model:\n+                    # add to filtered_inbound_nodes.\n+                    node_data = serialize_node(node, node_reindexing_map)\n+                    if node_data is not None:\n+                        filtered_inbound_nodes.append(node_data)\n+\n+            layer_config = serialization_lib.serialize_keras_object(operation)\n+            layer_config[\"name\"] = operation.name\n+            layer_config[\"inbound_nodes\"] = filtered_inbound_nodes\n+            layer_configs.append(layer_config)\n+        config[\"layers\"] = layer_configs\n+\n+        # Gather info about inputs and outputs.\n+        model_inputs = []\n+        for tensor in self._inputs:\n+            operation = tensor._keras_history[0]\n+            node_index = tensor._keras_history[1]\n+            tensor_index = tensor._keras_history[2]\n+            node_key = make_node_key(operation, node_index)\n+            if node_key not in self._nodes:\n+                continue\n+            new_node_index = node_reindexing_map[node_key]\n+            model_inputs.append([operation.name, new_node_index, tensor_index])\n+        config[\"input_layers\"] = model_inputs\n+        model_outputs = []\n+        for tensor in self._outputs:\n+            operation = tensor._keras_history[0]\n+            node_index = tensor._keras_history[1]\n+            tensor_index = tensor._keras_history[2]\n+            node_key = make_node_key(operation, node_index)\n+            if node_key not in self._nodes:\n+                continue\n+            new_node_index = node_reindexing_map[node_key]\n+            model_outputs.append([operation.name, new_node_index, tensor_index])\n+        config[\"output_layers\"] = model_outputs\n+        return copy.deepcopy(config)\n \n     @classmethod\n-    def from_config(self):\n-        raise NotImplementedError\n+    def from_config(cls, config, custom_objects=None):\n+        \"\"\"Instantiates a Model from its config (output of `get_config()`).\"\"\"\n+        # Layer instances created during\n+        # the graph reconstruction process\n+        created_layers = {}\n+\n+        # Dictionary mapping layer instances to\n+        # node data that specifies a layer call.\n+        # It acts as a queue that maintains any unprocessed\n+        # layer call until it becomes possible to process it\n+        # (i.e. until the input tensors to the call all exist).\n+        unprocessed_nodes = {}\n+\n+        def add_unprocessed_node(layer, node_data):\n+            \"\"\"Add node to layer list\n+\n+            Arg:\n+                layer: layer object\n+                node_data: Node data specifying layer call\n+            \"\"\"\n+            if layer not in unprocessed_nodes:\n+                unprocessed_nodes[layer] = [node_data]\n+            else:\n+                unprocessed_nodes[layer].append(node_data)\n+\n+        def process_node(layer, node_data):\n+            \"\"\"Reconstruct node by linking to inbound layers\n+\n+            Args:\n+                layer: Layer to process\n+                node_data: List of layer configs\n+            \"\"\"\n+            args, kwargs = deserialize_node(node_data, created_layers)\n+            # Call layer on its inputs, thus creating the node\n+            # and building the layer if needed.\n+            layer(*args, **kwargs)\n+\n+        def process_layer(layer_data):\n+            \"\"\"Deserializes a layer, then call it on appropriate inputs.\n+\n+            Args:\n+                layer_data: layer config dict.\n+            \"\"\"\n+            layer_name = layer_data[\"name\"]\n+\n+            # Instantiate layer.\n+            layer = serialization_lib.deserialize_keras_object(\n+                layer_data, custom_objects=custom_objects\n+            )\n+            created_layers[layer_name] = layer\n+\n+            # Gather layer inputs.\n+            inbound_nodes_data = layer_data[\"inbound_nodes\"]\n+            for node_data in inbound_nodes_data:\n+                # We don't process nodes (i.e. make layer calls)\n+                # on the fly because the inbound node may not yet exist,\n+                # in case of layer shared at different topological depths\n+                # (e.g. a model such as A(B(A(B(x)))))\n+                add_unprocessed_node(layer, node_data)\n+\n+        # First, we create all layers and enqueue nodes to be processed\n+        for layer_data in config[\"layers\"]:\n+            process_layer(layer_data)\n+\n+        # Then we process nodes in order of layer depth.\n+        # Nodes that cannot yet be processed (if the inbound node\n+        # does not yet exist) are re-enqueued, and the process\n+        # is repeated until all nodes are processed.\n+        while unprocessed_nodes:\n+            for layer_data in config[\"layers\"]:\n+                layer = created_layers[layer_data[\"name\"]]\n+\n+                # Process all nodes in layer, if not yet processed\n+                if layer in unprocessed_nodes:\n+                    node_data_list = unprocessed_nodes[layer]\n+\n+                    # Process nodes in order\n+                    node_index = 0\n+                    while node_index < len(node_data_list):\n+                        node_data = node_data_list[node_index]\n+                        try:\n+                            process_node(layer, node_data)\n+\n+                        # If the node does not have all inbound layers\n+                        # available, stop processing and continue later\n+                        except IndexError:\n+                            break\n+\n+                        node_index += 1\n+\n+                    # If not all nodes processed then store unprocessed nodes\n+                    if node_index < len(node_data_list):\n+                        unprocessed_nodes[layer] = node_data_list[node_index:]\n+                    # If all nodes processed remove the layer\n+                    else:\n+                        del unprocessed_nodes[layer]\n+\n+        # Create lits of input and output tensors and return new class\n+        name = config.get(\"name\")\n+        input_tensors = []\n+        output_tensors = []\n+        for layer_data in config[\"input_layers\"]:\n+            layer_name, node_index, tensor_index = layer_data\n+            assert layer_name in created_layers\n+            layer = created_layers[layer_name]\n+            layer_output_tensors = layer._inbound_nodes[\n+                node_index\n+            ].output_tensors\n+            input_tensors.append(layer_output_tensors[tensor_index])\n+        for layer_data in config[\"output_layers\"]:\n+            layer_name, node_index, tensor_index = layer_data\n+            assert layer_name in created_layers\n+            layer = created_layers[layer_name]\n+            layer_output_tensors = layer._inbound_nodes[\n+                node_index\n+            ].output_tensors\n+            output_tensors.append(layer_output_tensors[tensor_index])\n+        return cls(inputs=input_tensors, outputs=output_tensors, name=name)\n \n \n def operation_fn(operation, training):\n@@ -239,5 +405,86 @@ def functional_like_constructor(cls):\n     return False\n \n \n-def get_functional_config(network):\n-    raise NotImplementedError\n+def unpack_singleton(x):\n+    if len(x) == 1:\n+        return x[0]\n+    return x\n+\n+\n+def serialize_node(node, node_reindexing_map):\n+    if not node.input_tensors:\n+        # Does not need to be serialized.\n+        return\n+\n+    args = node.arguments.args\n+    kwargs = node.arguments.kwargs\n+    return {\n+        \"args\": serialization_lib.serialize_keras_object(args),\n+        \"kwargs\": serialization_lib.serialize_keras_object(kwargs),\n+    }\n+\n+\n+def deserialize_node(node_data, created_layers):\n+    \"\"\"Return (args, kwargs) for calling the node layer.\"\"\"\n+    if not node_data:\n+        return [], {}\n+\n+    if isinstance(node_data, list):\n+        # Legacy case.\n+        input_tensors = []\n+        for input_data in node_data:\n+            inbound_layer_name = input_data[0]\n+            inbound_node_index = input_data[1]\n+            inbound_tensor_index = input_data[2]\n+            if len(input_data) == 3:\n+                kwargs = {}\n+            elif len(input_data) == 4:\n+                kwargs = input_data[3]\n+            else:\n+                raise ValueError(\n+                    \"Cannot deserialize the model (invalid config data?)\"\n+                )\n+            inbound_layer = created_layers[inbound_layer_name]\n+\n+            # Raise an error if the corresponding layer node\n+            # has not yet been created\n+            if len(inbound_layer._inbound_nodes) <= inbound_node_index:\n+                raise IndexError(\n+                    \"Layer node index out of bounds.\\n\"\n+                    f\"inbound_layer = {inbound_layer}\\n\"\n+                    f\"inbound_layer._inbound_nodes = {inbound_layer._inbound_nodes}\\n\"\n+                    f\"inbound_node_index = {inbound_node_index}\"\n+                )\n+            inbound_node = inbound_layer._inbound_nodes[inbound_node_index]\n+            input_tensors.append(\n+                inbound_node.output_tensors[inbound_tensor_index]\n+            )\n+        return [unpack_singleton(input_tensors)], kwargs\n+\n+    args = serialization_lib.deserialize_keras_object(node_data[\"args\"])\n+    kwargs = serialization_lib.deserialize_keras_object(node_data[\"kwargs\"])\n+\n+    def convert_revived_tensor(x):\n+        if isinstance(x, backend.KerasTensor):\n+            history = x._pre_serialization_keras_history\n+            if history is None:\n+                return x\n+            layer = created_layers.get(history[0], None)\n+            if layer is None:\n+                raise ValueError(f\"Unknown layer: {history[0]}\")\n+            inbound_node_index = history[1]\n+            inbound_tensor_index = history[1]\n+            if len(layer._inbound_nodes) <= inbound_node_index:\n+                raise ValueError(\n+                    \"Layer node index out of bounds.\\n\"\n+                    f\"inbound_layer = {layer}\\n\"\n+                    f\"inbound_layer._inbound_nodes = {layer._inbound_nodes}\\n\"\n+                    f\"inbound_node_index = {inbound_node_index}\"\n+                )\n+            inbound_node = layer._inbound_nodes[inbound_node_index]\n+            return inbound_node.output_tensors[inbound_tensor_index]\n+        return x\n+\n+    args = nest.map_structure(convert_revived_tensor, args)\n+    kwargs = nest.map_structure(convert_revived_tensor, kwargs)\n+    return args, kwargs\n\n@@ -4,7 +4,8 @@ from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n from keras_core.layers.core.input_layer import Input\n-from keras_core.models.functional import Functional\n+from keras_core.models import Functional\n+from keras_core.models import Model\n \n \n class FunctionalTest(testing.TestCase):\n@@ -14,9 +15,13 @@ class FunctionalTest(testing.TestCase):\n         x = input_a + input_b\n         x = layers.Dense(5)(x)\n         outputs = layers.Dense(4)(x)\n-        model = Functional([input_a, input_b], outputs)\n+        model = Functional([input_a, input_b], outputs, name=\"basic\")\n         model.summary()\n \n+        self.assertEqual(model.name, \"basic\")\n+        self.assertTrue(isinstance(model, Functional))\n+        self.assertTrue(isinstance(model, Model))\n+\n         # Eager call\n         in_val = [np.random.random((2, 3)), np.random.random((2, 3))]\n         out_val = model(in_val)\n@@ -156,8 +161,44 @@ class FunctionalTest(testing.TestCase):\n         self.assertEqual(out_val.shape, (2, 3, 3))\n \n     def test_serialization(self):\n-        # TODO\n-        pass\n+        # Test basic model\n+        inputs = Input(shape=(3,), batch_size=2)\n+        outputs = layers.Dense(3)(inputs)\n+        model = Functional(inputs, outputs)\n+        self.run_class_serialization_test(model)\n+\n+        # Test multi-io model\n+        input_a = Input(shape=(3,), batch_size=2, name=\"input_a\")\n+        input_b = Input(shape=(3,), batch_size=2, name=\"input_b\")\n+        xa = layers.Dense(5, name=\"middle_a\")(input_a)\n+        xb = layers.Dense(5, name=\"middle_b\")(input_b)\n+        output_a = layers.Dense(4, name=\"output_a\")(xa)\n+        output_b = layers.Dense(4, name=\"output_b\")(xb)\n+        model = Functional(\n+            [input_a, input_b], [output_a, output_b], name=\"func\"\n+        )\n+        self.run_class_serialization_test(model)\n+\n+        # Test model that includes floating ops\n+        input_a = Input(shape=(3,), batch_size=2, name=\"input_a\")\n+        input_b = Input(shape=(3,), batch_size=2, name=\"input_b\")\n+        x = input_a + input_b\n+        x = layers.Dense(5, name=\"middle\")(x)\n+        output_a = layers.Dense(4, name=\"output_a\")(x)\n+        output_b = layers.Dense(4, name=\"output_b\")(x)\n+        model = Functional(\n+            [input_a, input_b], [output_a, output_b], name=\"func\"\n+        )\n+        self.run_class_serialization_test(model)\n+\n+        # Test model with dict i/o\n+        input_a = Input(shape=(3,), batch_size=2, name=\"a\")\n+        input_b = Input(shape=(3,), batch_size=2, name=\"b\")\n+        x = input_a + input_b\n+        x = layers.Dense(5)(x)\n+        outputs = layers.Dense(4)(x)\n+        model = Functional({\"a\": input_a, \"b\": input_b}, outputs)\n+        self.run_class_serialization_test(model)\n \n     def test_add_loss(self):\n         # TODO\n\n@@ -1,6 +1,7 @@\n from keras_core import backend\n from keras_core.api_export import keras_core_export\n from keras_core.layers.layer import Layer\n+from keras_core.utils import python_utils\n from keras_core.utils import summary_utils\n \n if backend.backend() == \"tensorflow\":\n@@ -33,15 +34,21 @@ class Model(Trainer, Layer):\n     def __new__(cls, *args, **kwargs):\n         # Signature detection\n         if functional_init_arguments(args, kwargs):\n-            # Functional model\n             from keras_core.models import functional\n \n             return functional.Functional(*args, **kwargs)\n-        return Layer.__new__(cls)\n+        return super().__new__(cls)\n \n-    def __init__(self, trainable=True, name=None, dtype=None):\n+    def __init__(self, *args, **kwargs):\n         Trainer.__init__(self)\n-        Layer.__init__(self, trainable=trainable, name=name, dtype=dtype)\n+        from keras_core.models import functional\n+\n+        if isinstance(self, functional.Functional) and python_utils.is_default(\n+            self.__init__\n+        ):\n+            functional.Functional.__init__(self, *args, **kwargs)\n+        else:\n+            Layer.__init__(self, *args, **kwargs)\n \n     def call(self, inputs, training=False):\n         raise NotImplementedError\n\n@@ -23,6 +23,10 @@ class Function(Operation):\n         self._operations = operations\n         self._operations_by_depth = operations_by_depth\n \n+    @property\n+    def operations(self):\n+        return self._operations[:]\n+\n     @property\n     def inputs(self):\n         return self._inputs\n@@ -134,8 +138,8 @@ class Function(Operation):\n                         )\n \n \n-def make_node_key(op_name, node_index):\n-    return op_name + \"_ib-\" + str(node_index)\n+def make_node_key(op, node_index):\n+    return str(id(op)) + \"_ib-\" + str(node_index)\n \n \n def map_graph(inputs, outputs):\n@@ -156,9 +160,7 @@ def map_graph(inputs, outputs):\n     # Nodes are ordered from inputs -> outputs.\n     nodes_in_decreasing_depth, operation_indices = _build_map(outputs)\n     network_nodes = {\n-        make_node_key(\n-            str(id(node.operation)), node.operation._inbound_nodes.index(node)\n-        )\n+        make_node_key(node.operation, node.operation._inbound_nodes.index(node))\n         for node in nodes_in_decreasing_depth\n     }\n \n@@ -195,7 +197,7 @@ def map_graph(inputs, outputs):\n             operations_depths[input_operation] = 0\n             operation_indices[input_operation] = -1\n             nodes_depths[input_operation._inbound_nodes[0]] = 0\n-            network_nodes.add(make_node_key(input_operation.name, 0))\n+            network_nodes.add(make_node_key(input_operation, 0))\n \n     # Build a dict {depth: list of nodes with this depth}\n     nodes_by_depth = collections.defaultdict(list)\n\n@@ -139,6 +139,19 @@ def serialize_keras_object(obj):\n             \"class_name\": \"__bytes__\",\n             \"config\": {\"value\": obj.decode(\"utf-8\")},\n         }\n+    if isinstance(obj, backend.KerasTensor):\n+        history = getattr(obj, \"_keras_history\", None)\n+        if history:\n+            history = list(history)\n+            history[0] = history[0].name\n+        return {\n+            \"class_name\": \"__keras_tensor__\",\n+            \"config\": {\n+                \"shape\": obj.shape,\n+                \"dtype\": obj.dtype,\n+                \"keras_history\": history,\n+            },\n+        }\n     if isinstance(obj, tf.TensorShape):\n         return obj.as_list() if obj._dims is not None else None\n     if isinstance(obj, (tf.Tensor, jax.numpy.ndarray)):\n@@ -203,12 +216,6 @@ def serialize_keras_object(obj):\n         obj.__class__, inner_config\n     )\n \n-    # TODO(nkovela): Add TF ops dispatch handler serialization for\n-    # ops.EagerTensor that contains nested numpy array.\n-    # Target: NetworkConstructionTest.test_constant_initializer_with_numpy\n-    if isinstance(inner_config, str) and inner_config == \"op_dispatch_handler\":\n-        return obj\n-\n     if config_with_public_class is not None:\n         get_build_and_compile_config(obj, config_with_public_class)\n         record_object_after_serialization(obj, config_with_public_class)\n@@ -564,6 +571,13 @@ def deserialize_keras_object(\n     custom_objects = custom_objects or {}\n \n     # Special cases:\n+    if class_name == \"__keras_tensor__\":\n+        obj = backend.KerasTensor(\n+            inner_config[\"shape\"], dtype=inner_config[\"dtype\"]\n+        )\n+        obj._pre_serialization_keras_history = inner_config[\"keras_history\"]\n+        return obj\n+\n     if class_name == \"__tensor__\":\n         return backend.convert_to_tensor(\n             inner_config[\"value\"], dtype=inner_config[\"dtype\"]\n\n@@ -168,7 +168,9 @@ class SerializationLibTest(testing.TestCase):\n     # def test_safe_mode_scope(self):\n     #     lmbda = keras_core.layers.Lambda(lambda x: x**2)\n     #     with serialization_lib.SafeModeScope(safe_mode=True):\n-    #         with self.assertRaisesRegex(ValueError, \"arbitrary code execution\"):\n+    #         with self.assertRaisesRegex(\n+    #             ValueError, \"arbitrary code execution\"\n+    #         ):\n     #             self.roundtrip(lmbda)\n     #     with serialization_lib.SafeModeScope(safe_mode=False):\n     #         _, new_lmbda, _ = self.roundtrip(lmbda)\n@@ -192,25 +194,26 @@ class SerializationLibTest(testing.TestCase):\n         self.assertIs(model.layers[2], model.layers[3].layer)\n         self.assertIs(new_model.layers[2], new_model.layers[3].layer)\n \n-    # TODO\n-    # def test_functional_subclass(self):\n-    #     class PlainFunctionalSubclass(keras_core.Model):\n-    #         pass\n+    def test_functional_subclass(self):\n+        class PlainFunctionalSubclass(keras_core.Model):\n+            pass\n \n-    #     inputs = keras_core.Input((2,), batch_size=3)\n-    #     outputs = keras_core.layers.Dense(1)(inputs)\n-    #     model = PlainFunctionalSubclass(inputs, outputs)\n-    #     x = ops.random.normal((2, 2))\n-    #     y1 = model(x)\n-    #     _, new_model, _ = self.roundtrip(\n-    #         model,\n-    #         custom_objects={\"PlainFunctionalSubclass\": PlainFunctionalSubclass},\n-    #     )\n-    #     new_model.set_weights(model.get_weights())\n-    #     y2 = new_model(x)\n-    #     self.assertAllClose(y1, y2, atol=1e-5)\n+        inputs = keras_core.Input((2,), batch_size=3)\n+        outputs = keras_core.layers.Dense(1)(inputs)\n+        model = PlainFunctionalSubclass(inputs, outputs)\n+        x = ops.random.normal((2, 2))\n+        y1 = model(x)\n+        _, new_model, _ = self.roundtrip(\n+            model,\n+            custom_objects={\"PlainFunctionalSubclass\": PlainFunctionalSubclass},\n+        )\n+        new_model.set_weights(model.get_weights())\n+        y2 = new_model(x)\n+        self.assertAllClose(y1, y2, atol=1e-5)\n+        # TODO\n         # self.assertIsInstance(new_model, PlainFunctionalSubclass)\n \n+        # TODO\n         # class FunctionalSubclassWCustomInit(keras_core.Model):\n         #     def __init__(self, num_units=1, **kwargs):\n         #         inputs = keras_core.Input((2,), batch_size=3)\n\n@@ -6,6 +6,8 @@ from tensorflow import nest\n \n \n class TestCase(unittest.TestCase):\n+    maxDiff = None\n+\n     def assertAllClose(self, x1, x2, atol=1e-7, rtol=1e-7):\n         np.testing.assert_allclose(x1, x2, atol=atol, rtol=rtol)\n \n@@ -23,20 +25,29 @@ class TestCase(unittest.TestCase):\n         # get_config roundtrip\n         cls = instance.__class__\n         config = instance.get_config()\n+        config_json = json.dumps(config, sort_keys=True, indent=4)\n         ref_dir = dir(instance)[:]\n         with custom_object_scope(custom_objects):\n             revived_instance = cls.from_config(config)\n         revived_config = revived_instance.get_config()\n-        self.assertEqual(config, revived_config)\n+        revived_config_json = json.dumps(\n+            revived_config, sort_keys=True, indent=4\n+        )\n+        self.assertEqual(config_json, revived_config_json)\n         self.assertEqual(ref_dir, dir(revived_instance))\n \n         # serialization roundtrip\n         serialized = serialize_keras_object(instance)\n-        json_str = json.dumps(serialized)\n+        serialized_json = json.dumps(serialized, sort_keys=True, indent=4)\n         with custom_object_scope(custom_objects):\n-            revived_instance = deserialize_keras_object(json.loads(json_str))\n+            revived_instance = deserialize_keras_object(\n+                json.loads(serialized_json)\n+            )\n         revived_config = revived_instance.get_config()\n-        self.assertEqual(config, revived_config)\n+        revived_config_json = json.dumps(\n+            revived_config, sort_keys=True, indent=4\n+        )\n+        self.assertEqual(config_json, revived_config_json)\n         self.assertEqual(ref_dir, dir(revived_instance))\n \n     def run_layer_test(\n\n@@ -250,11 +250,11 @@ class Trainer:\n \n     def get_compile_config(self):\n         # TODO\n-        raise NotImplementedError\n+        pass\n \n     def compile_from_config(self):\n         # TODO\n-        raise NotImplementedError\n+        return {}\n \n     def _should_eval(self, epoch, validation_freq):\n         epoch = epoch + 1  # one-index the user-facing epoch.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7fc2f3320ebd6be75995253d18ea782f2f4f6bae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 100 | Lines Deleted: 34 | Files Changed: 3 | Hunks: 12 | Methods Changed: 10 | Complexity Δ (Sum/Max): 13/7 | Churn Δ: 134 | Churn Cumulative: 1462 | Contributors (this commit): 5 | Commits (past 90d): 45 | Contributors (cumulative): 8 | DMM Complexity: 0.463768115942029\n\nDIFF:\n@@ -11,7 +11,6 @@ from keras_core.models.model import Model\n from keras_core.operations.function import Function\n from keras_core.operations.function import make_node_key\n from keras_core.saving import serialization_lib\n-from keras_core.utils import python_utils\n from keras_core.utils import tracking\n \n \n@@ -29,12 +28,7 @@ class Functional(Function, Model):\n     Symbolic add_loss\n     \"\"\"\n \n-    def __new__(cls, *args, **kwargs):\n-        # Skip Model.__new__.\n-        return Function.__new__(cls)\n-\n     @tracking.no_automatic_dependency_tracking\n-    @python_utils.default\n     def __init__(self, inputs, outputs, name=None, **kwargs):\n         if isinstance(inputs, dict):\n             for k, v in inputs.items():\n@@ -192,7 +186,7 @@ class Functional(Function, Model):\n             # Subclassed networks are not serializable\n             # (unless serialization is implemented by\n             # the author of the subclassed network).\n-            return Model.get_config()\n+            return Model.get_config(self)\n \n         config = {\n             \"name\": self.name,\n@@ -266,6 +260,55 @@ class Functional(Function, Model):\n \n     @classmethod\n     def from_config(cls, config, custom_objects=None):\n+        functional_config_keys = [\n+            \"name\",\n+            \"layers\",\n+            \"input_layers\",\n+            \"output_layers\",\n+        ]\n+        is_functional_config = all(\n+            key in config for key in functional_config_keys\n+        )\n+        argspec = inspect.getfullargspec(cls.__init__)\n+        functional_init_args = inspect.getfullargspec(Functional.__init__).args[\n+            1:\n+        ]\n+        revivable_as_functional = (\n+            cls in {Functional, Model}\n+            or argspec.args[1:] == functional_init_args\n+            or (argspec.varargs == \"args\" and argspec.varkw == \"kwargs\")\n+        )\n+        if is_functional_config and revivable_as_functional:\n+            # Revive Functional model\n+            # (but not Functional subclasses with a custom __init__)\n+            return cls._from_config(config, custom_objects=custom_objects)\n+\n+        # Either the model has a custom __init__, or the config\n+        # does not contain all the information necessary to\n+        # revive a Functional model. This happens when the user creates\n+        # subclassed models where `get_config()` is returning\n+        # insufficient information to be considered a Functional model.\n+        # In this case, we fall back to provide all config into the\n+        # constructor of the class.\n+        try:\n+            return cls(**config)\n+        except TypeError as e:\n+            raise TypeError(\n+                \"Unable to revive model from config. When overriding \"\n+                \"the `get_config()` method, make sure that the \"\n+                \"returned config contains all items used as arguments \"\n+                f\"in the  constructor to {cls}, \"\n+                \"which is the default behavior. \"\n+                \"You can override this default behavior by defining a \"\n+                \"`from_config(cls, config)` class method to specify \"\n+                \"how to create an \"\n+                f\"instance of {cls.__name__} from its config.\\n\\n\"\n+                f\"Received config={config}\\n\\n\"\n+                f\"Error encountered during deserialization: {e}\"\n+            )\n+\n+    @classmethod\n+    def _from_config(cls, config, custom_objects=None):\n         \"\"\"Instantiates a Model from its config (output of `get_config()`).\"\"\"\n         # Layer instances created during\n         # the graph reconstruction process\n\n@@ -1,7 +1,6 @@\n from keras_core import backend\n from keras_core.api_export import keras_core_export\n from keras_core.layers.layer import Layer\n-from keras_core.utils import python_utils\n from keras_core.utils import summary_utils\n \n if backend.backend() == \"tensorflow\":\n@@ -32,8 +31,8 @@ class Model(Trainer, Layer):\n     \"\"\"\n \n     def __new__(cls, *args, **kwargs):\n-        # Signature detection\n-        if functional_init_arguments(args, kwargs):\n+        # Signature detection for usage of `Model` as a `Functional`\n+        if functional_init_arguments(args, kwargs) and cls == Model:\n             from keras_core.models import functional\n \n             return functional.Functional(*args, **kwargs)\n@@ -43,9 +42,10 @@ class Model(Trainer, Layer):\n         Trainer.__init__(self)\n         from keras_core.models import functional\n \n-        if isinstance(self, functional.Functional) and python_utils.is_default(\n-            self.__init__\n-        ):\n+        # Signature detection for usage of a `Model` subclass\n+        # as a `Functional` subclass\n+        if functional_init_arguments(args, kwargs):\n+            inject_functional_model_class(self.__class__)\n             functional.Functional.__init__(self, *args, **kwargs)\n         else:\n             Layer.__init__(self, *args, **kwargs)\n@@ -166,3 +166,24 @@ def functional_init_arguments(args, kwargs):\n         or (len(args) == 1 and \"outputs\" in kwargs)\n         or (\"inputs\" in kwargs and \"outputs\" in kwargs)\n     )\n+\n+\n+def inject_functional_model_class(cls):\n+    \"\"\"Inject `Functional` into the hierarchy of this class if needed.\"\"\"\n+    from keras_core.models import functional\n+\n+    if cls == Model:\n+        return functional.Functional\n+    # In case there is any multiple inheritance, we stop injecting the\n+    # class if keras model is not in its class hierarchy.\n+    if cls == object:\n+        return object\n+\n+    cls.__bases__ = tuple(\n+        inject_functional_model_class(base) for base in cls.__bases__\n+    )\n+    # Trigger any `__new__` class swapping that needed to happen on `Functional`\n+    # but did not because functional was not in the class hierarchy.\n+    cls.__new__(cls)\n+\n+    return cls\n\n@@ -210,29 +210,31 @@ class SerializationLibTest(testing.TestCase):\n         new_model.set_weights(model.get_weights())\n         y2 = new_model(x)\n         self.assertAllClose(y1, y2, atol=1e-5)\n-        # TODO\n-        # self.assertIsInstance(new_model, PlainFunctionalSubclass)\n+        self.assertIsInstance(new_model, PlainFunctionalSubclass)\n \n-        # TODO\n-        # class FunctionalSubclassWCustomInit(keras_core.Model):\n-        #     def __init__(self, num_units=1, **kwargs):\n-        #         inputs = keras_core.Input((2,), batch_size=3)\n-        #         outputs = keras_core.layers.Dense(num_units)(inputs)\n-        #         super().__init__(inputs, outputs)\n+        class FunctionalSubclassWCustomInit(keras_core.Model):\n+            def __init__(self, num_units=2):\n+                inputs = keras_core.Input((2,), batch_size=3)\n+                outputs = keras_core.layers.Dense(num_units)(inputs)\n+                super().__init__(inputs, outputs)\n+                self.num_units = num_units\n \n-        # model = FunctionalSubclassWCustomInit(num_units=2)\n-        # x = ops.random.normal((2, 2))\n-        # y1 = model(x)\n-        # _, new_model, _ = self.roundtrip(\n-        #     model,\n-        #     custom_objects={\n-        #         \"FunctionalSubclassWCustomInit\": FunctionalSubclassWCustomInit\n-        #     },\n-        # )\n-        # new_model.set_weights(model.get_weights())\n-        # y2 = new_model(x)\n-        # self.assertAllClose(y1, y2, atol=1e-5)\n-        # self.assertIsInstance(new_model, FunctionalSubclassWCustomInit)\n+            def get_config(self):\n+                return {\"num_units\": self.num_units}\n+\n+        model = FunctionalSubclassWCustomInit(num_units=3)\n+        x = ops.random.normal((2, 2))\n+        y1 = model(x)\n+        _, new_model, _ = self.roundtrip(\n+            model,\n+            custom_objects={\n+                \"FunctionalSubclassWCustomInit\": FunctionalSubclassWCustomInit\n+            },\n+        )\n+        new_model.set_weights(model.get_weights())\n+        y2 = new_model(x)\n+        self.assertAllClose(y1, y2, atol=1e-5)\n+        self.assertIsInstance(new_model, FunctionalSubclassWCustomInit)\n \n     def test_shared_object(self):\n         class MyLayer(keras_core.layers.Layer):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#036a2cddd6f99ebadd78e75a9722f99b22aebb05", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 121 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 15 | Complexity Δ (Sum/Max): 15/15 | Churn Δ: 121 | Churn Cumulative: 121 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,121 @@\n+import keras_core\n+from keras_core import testing\n+from keras_core.saving import object_registration\n+from keras_core.saving import serialization_lib\n+\n+\n+class TestObjectRegistration(testing.TestCase):\n+    def test_custom_object_scope(self):\n+        def custom_fn():\n+            pass\n+\n+        class CustomClass:\n+            pass\n+\n+        def check_get_in_thread():\n+            with object_registration.custom_object_scope(\n+                {\"CustomClass\": CustomClass, \"custom_fn\": custom_fn}\n+            ):\n+                actual_custom_fn = keras_core.activations.get(\"custom_fn\")\n+                self.assertEqual(actual_custom_fn, custom_fn)\n+                actual_custom_class = keras_core.regularizers.get(\"CustomClass\")\n+                self.assertEqual(actual_custom_class.__class__, CustomClass)\n+\n+            with object_registration.custom_object_scope(\n+                {\"CustomClass\": CustomClass, \"custom_fn\": custom_fn}\n+            ):\n+                actual_custom_fn = keras_core.activations.get(\"custom_fn\")\n+                self.assertEqual(actual_custom_fn, custom_fn)\n+                actual_custom_class = keras_core.regularizers.get(\"CustomClass\")\n+                self.assertEqual(actual_custom_class.__class__, CustomClass)\n+                checked_thread = self.checkedThread(check_get_in_thread)\n+                checked_thread.start()\n+                checked_thread.join()\n+\n+    def test_serialize_custom_class_with_default_name(self):\n+        @object_registration.register_keras_serializable()\n+        class TestClass:\n+            def __init__(self, value):\n+                self._value = value\n+\n+            def get_config(self):\n+                return {\"value\": self._value}\n+\n+            @classmethod\n+            def from_config(cls, config):\n+                return cls(**config)\n+\n+        serialized_name = \"Custom>TestClass\"\n+        inst = TestClass(value=10)\n+        class_name = object_registration._GLOBAL_CUSTOM_NAMES[TestClass]\n+        self.assertEqual(serialized_name, class_name)\n+\n+        config = serialization_lib.serialize_keras_object(inst)\n+        self.assertEqual(\"TestClass\", config[\"class_name\"])\n+        new_inst = serialization_lib.deserialize_keras_object(config)\n+        self.assertIsNot(inst, new_inst)\n+        self.assertIsInstance(new_inst, TestClass)\n+        self.assertEqual(10, new_inst._value)\n+\n+    def test_serialize_custom_class_with_custom_name(self):\n+        @object_registration.register_keras_serializable(\n+            \"TestPackage\", \"CustomName\"\n+        )\n+        class OtherTestClass:\n+            def __init__(self, val):\n+                self._val = val\n+\n+            def get_config(self):\n+                return {\"val\": self._val}\n+\n+            @classmethod\n+            def from_config(cls, config):\n+                return cls(**config)\n+\n+        serialized_name = \"TestPackage>CustomName\"\n+        inst = OtherTestClass(val=5)\n+        class_name = object_registration._GLOBAL_CUSTOM_NAMES[OtherTestClass]\n+        self.assertEqual(serialized_name, class_name)\n+        fn_class_name = object_registration.get_registered_name(OtherTestClass)\n+        self.assertEqual(fn_class_name, class_name)\n+\n+        cls = object_registration.get_registered_object(fn_class_name)\n+        self.assertEqual(OtherTestClass, cls)\n+\n+        config = keras_core.saving.serialize_keras_object(inst)\n+        self.assertEqual(\"OtherTestClass\", config[\"class_name\"])\n+        new_inst = keras_core.saving.deserialize_keras_object(config)\n+        self.assertIsNot(inst, new_inst)\n+        self.assertIsInstance(new_inst, OtherTestClass)\n+        self.assertEqual(5, new_inst._val)\n+\n+    def test_serialize_custom_function(self):\n+        @object_registration.register_keras_serializable()\n+        def my_fn():\n+            return 42\n+\n+        serialized_name = \"Custom>my_fn\"\n+        class_name = object_registration._GLOBAL_CUSTOM_NAMES[my_fn]\n+        self.assertEqual(serialized_name, class_name)\n+        fn_class_name = object_registration.get_registered_name(my_fn)\n+        self.assertEqual(fn_class_name, class_name)\n+\n+        config = keras_core.saving.serialize_keras_object(my_fn)\n+        fn = keras_core.saving.deserialize_keras_object(config)\n+        self.assertEqual(42, fn())\n+\n+        fn_2 = object_registration.get_registered_object(fn_class_name)\n+        self.assertEqual(42, fn_2())\n+\n+    def test_serialize_custom_class_without_get_config_fails(self):\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"Cannot register a class that does not have a get_config.*\",\n+        ):\n+\n+            @object_registration.register_keras_serializable(\n+                \"TestPackage\", \"TestClass\"\n+            )\n+            class TestClass:\n+                def __init__(self, value):\n+                    self._value = value\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
