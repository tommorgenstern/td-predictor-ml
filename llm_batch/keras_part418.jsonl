{"custom_id": "keras#08872b30c71c038954c9e72739687e78a60130e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 6 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -37,6 +37,7 @@ from keras.utils import tf_inspect\n import numpy as np\n import tensorflow.compat.v2 as tf\n from tensorflow.python.framework import test_util as tf_test_utils  # pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.util.tf_export import keras_export  # pylint: disable=g-direct-tensorflow-import\n \n \n def string_test(actual, expected):\n@@ -76,6 +77,7 @@ def get_test_data(train_samples,\n           (x[train_samples:], y[train_samples:]))\n \n \n+@keras_export('keras.__internal__.utils.layer_test', v1=[])\n @tf_test_utils.disable_cudnn_autotune\n def layer_test(layer_cls,\n                kwargs=None,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#91fad4e3552d25ea1de0ff5d9a344ce7c885ffaf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 18 | Churn Cumulative: 827 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -21,8 +21,6 @@ Reference:\n       https://arxiv.org/abs/1905.11946) (ICML 2019)\n \"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n import copy\n import math\n \n@@ -32,7 +30,10 @@ from keras.engine import training\n from keras.layers import VersionAwareLayers\n from keras.utils import data_utils\n from keras.utils import layer_utils\n-from tensorflow.python.util.tf_export import keras_export\n+\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export  # pylint: disable=g-direct-tensorflow-import\n \n \n BASE_WEIGHTS_PATH = 'https://storage.googleapis.com/keras-applications/'\n@@ -204,6 +205,10 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n \"\"\"\n \n \n+IMAGENET_MEAN_RGB = [0.485, 0.456, 0.406]\n+IMAGENET_STDDEV_RGB = [0.229, 0.224, 0.225]\n+\n+\n def EfficientNet(\n     width_coefficient,\n     depth_coefficient,\n@@ -319,6 +324,13 @@ def EfficientNet(\n   # Build stem\n   x = img_input\n   x = layers.Rescaling(1. / 255.)(x)\n+  if weights == 'imagenet':\n+    # Note that we use square value of STDDEV as the variance for the layer,\n+    # since normalization is computed as: result = (input - mean) / sqrt(var)\n+    x = layers.Normalization(axis=bn_axis,\n+                             mean=IMAGENET_MEAN_RGB,\n+                             variance=tf.math.square(IMAGENET_STDDEV_RGB))(x)\n+  else:\n     x = layers.Normalization(axis=bn_axis)(x)\n \n   x = layers.ZeroPadding2D(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#643778ae55fa47aac11b6ed0b88e041ecf39140f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 15 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 18 | Churn Cumulative: 845 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -21,6 +21,8 @@ Reference:\n       https://arxiv.org/abs/1905.11946) (ICML 2019)\n \"\"\"\n \n+import tensorflow.compat.v2 as tf\n+\n import copy\n import math\n \n@@ -30,10 +32,7 @@ from keras.engine import training\n from keras.layers import VersionAwareLayers\n from keras.utils import data_utils\n from keras.utils import layer_utils\n-\n-import tensorflow.compat.v2 as tf\n-\n-from tensorflow.python.util.tf_export import keras_export  # pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.util.tf_export import keras_export\n \n \n BASE_WEIGHTS_PATH = 'https://storage.googleapis.com/keras-applications/'\n@@ -205,10 +204,6 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n \"\"\"\n \n \n-IMAGENET_MEAN_RGB = [0.485, 0.456, 0.406]\n-IMAGENET_STDDEV_RGB = [0.229, 0.224, 0.225]\n-\n-\n def EfficientNet(\n     width_coefficient,\n     depth_coefficient,\n@@ -324,13 +319,6 @@ def EfficientNet(\n   # Build stem\n   x = img_input\n   x = layers.Rescaling(1. / 255.)(x)\n-  if weights == 'imagenet':\n-    # Note that we use square value of STDDEV as the variance for the layer,\n-    # since normalization is computed as: result = (input - mean) / sqrt(var)\n-    x = layers.Normalization(axis=bn_axis,\n-                             mean=IMAGENET_MEAN_RGB,\n-                             variance=tf.math.square(IMAGENET_STDDEV_RGB))(x)\n-  else:\n   x = layers.Normalization(axis=bn_axis)(x)\n \n   x = layers.ZeroPadding2D(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#09c282e183493a6c9f3b537e4af02097b71b5226", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 6 | Churn Cumulative: 1608 | Contributors (this commit): 12 | Commits (past 90d): 3 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -608,12 +608,6 @@ class KerasObjectLoader:\n \n     return False\n \n-  def _load_edges(self):\n-    \"\"\"Add edges for all nodes that are not waiting on initialization.\"\"\"\n-    for node_id, proto in enumerate(self._proto.nodes):\n-      if node_id not in self.model_layer_dependencies:\n-        self._add_object_graph_edges(proto, node_id)\n-\n   def get_path(self, node_id):\n     return self._node_paths[node_id]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a4005f4ffc5407d7c1af9cd6687d8839307c42ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 19 | Churn Cumulative: 16650 | Contributors (this commit): 121 | Commits (past 90d): 21 | Contributors (cumulative): 121 | DMM Complexity: 0.0\n\nDIFF:\n@@ -2011,7 +2011,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                          'information of where went wrong, or file a '\n                          'issue/bug to `tf.keras`.')\n       callbacks.on_predict_end()\n-    all_outputs = tf.__internal__.nest.map_structure_up_to(batch_outputs, concat, outputs)\n+    all_outputs = tf.__internal__.nest.map_structure_up_to(batch_outputs, potentially_variable_concat, outputs)\n \n     # If originally PSS strategy was used, then replace it back since predict\n     # is running under `OneDeviceStrategy` after the swap and once its done\n@@ -3258,6 +3258,23 @@ def concat(tensors, axis=0):\n   return tf.concat(tensors, axis=axis)\n \n \n+def potentially_variable_concat(tensors, axis=0):\n+  \"\"\"Concats `tensor`s along `axis`. When non-batch dimensions are variable, a RaggedTensor is returned instead.\"\"\"\n+  if isinstance(tensors[0], tf.SparseTensor):\n+    return tf.sparse.concat(axis=axis, sp_inputs=tensors)\n+  non_batch_shapes = tf.stack([tf.shape(tensor)[1:] for tensor in tensors])\n+  constant_dims = tf.math.reduce_all(non_batch_shapes == non_batch_shapes[:1], axis=0)\n+  if tf.math.reduce_all(constant_dims).numpy().item():  # All non-batch dimensions are constant\n+    return tf.concat(tensors, axis=axis)\n+  # First, identify constant inner dimensions by finding the rightmost dimension that is not constant\n+  constant_inner_dimensions = constant_dims.numpy().tolist()[::-1].index(False)\n+  if constant_inner_dimensions == 0:\n+    constant_inner_shape = None\n+  else:\n+    constant_inner_shape = tensors[0].shape[-constant_inner_dimensions:]\n+  return tf.ragged.constant([tensor.numpy() for tensor in tensors], inner_shape=constant_inner_shape).merge_dims(0, 1)\n+\n+\n def _get_verbosity(verbose, distribute_strategy):\n   \"\"\"Find the right verbosity value for 'auto'.\"\"\"\n   if verbose == 1 and distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3ad0f70a11b00a2fae9a8f6fc2ecd4ead202e6b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 16651 | Contributors (this commit): 121 | Commits (past 90d): 22 | Contributors (cumulative): 121 | DMM Complexity: None\n\nDIFF:\n@@ -3268,6 +3268,7 @@ def potentially_variable_concat(tensors, axis=0):\n     return tf.concat(tensors, axis=axis)\n   # First, identify constant inner dimensions by finding the rightmost dimension that is not constant\n   constant_inner_dimensions = constant_dims.numpy().tolist()[::-1].index(False)\n+  # If there are constant inner dimensions, define a constant inner shape\n   if constant_inner_dimensions == 0:\n     constant_inner_shape = None\n   else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e11ebd48eaade842210235cd7f4401ac45ea13a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 16657 | Contributors (this commit): 121 | Commits (past 90d): 23 | Contributors (cumulative): 121 | DMM Complexity: None\n\nDIFF:\n@@ -3258,14 +3258,14 @@ def concat(tensors, axis=0):\n   return tf.concat(tensors, axis=axis)\n \n \n-def potentially_variable_concat(tensors, axis=0):\n+def potentially_variable_concat(tensors):\n   \"\"\"Concats `tensor`s along `axis`. When non-batch dimensions are variable, a RaggedTensor is returned instead.\"\"\"\n   if isinstance(tensors[0], tf.SparseTensor):\n-    return tf.sparse.concat(axis=axis, sp_inputs=tensors)\n+    return tf.sparse.concat(axis=0, sp_inputs=tensors)\n   non_batch_shapes = tf.stack([tf.shape(tensor)[1:] for tensor in tensors])\n   constant_dims = tf.math.reduce_all(non_batch_shapes == non_batch_shapes[:1], axis=0)\n   if tf.math.reduce_all(constant_dims).numpy().item():  # All non-batch dimensions are constant\n-    return tf.concat(tensors, axis=axis)\n+    return tf.concat(tensors, axis=0)\n   # First, identify constant inner dimensions by finding the rightmost dimension that is not constant\n   constant_inner_dimensions = constant_dims.numpy().tolist()[::-1].index(False)\n   # If there are constant inner dimensions, define a constant inner shape\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9385ff85bdb7882265b3e345b84f301712dac4b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 5 | Methods Changed: 7 | Complexity Δ (Sum/Max): 7/5 | Churn Δ: 34 | Churn Cumulative: 1979 | Contributors (this commit): 11 | Commits (past 90d): 10 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -18,6 +18,7 @@\n import tensorflow.compat.v2 as tf\n from keras import backend\n from keras.engine import keras_tensor\n+from keras.utils import tf_utils\n from tensorflow.python.util.tf_export import keras_export\n \n \n@@ -111,10 +112,9 @@ def remove_squeezable_dimensions(\n     Tuple of `labels` and `predictions`, possibly with last dim squeezed.\n   \"\"\"\n   with backend.name_scope(name or 'remove_squeezable_dimensions'):\n-    # tf.is_tensor returns True if predictions is a tensor or composite tensor.\n-    if not tf.is_tensor(predictions):\n+    if not tf_utils.is_tensor_or_extension_type(predictions):\n       predictions = tf.convert_to_tensor(predictions)\n-    if not tf.is_tensor(labels):\n+    if not tf_utils.is_tensor_or_extension_type(labels):\n       labels = tf.convert_to_tensor(labels)\n     predictions_shape = predictions.shape\n     predictions_rank = predictions_shape.ndims\n\n@@ -466,6 +466,11 @@ def is_tensor_or_variable(x):\n   return tf.is_tensor(x) or isinstance(x, tf.Variable)\n \n \n+def is_tensor_or_extension_type(x):\n+  \"\"\"Returns true if 'x' is a TF-native type or an ExtensionType.\"\"\"\n+  return tf.is_tensor(x) or is_extension_type(x)\n+\n+\n def assert_no_legacy_layers(layers):\n   \"\"\"Prevent tf.layers.Layers from being used with Keras.\n \n\n@@ -228,6 +228,29 @@ class TestIsExtensionType(tf.test.TestCase):\n     self.assertFalse(tf_utils.is_extension_type(tensor))\n \n \n+class TestIsTensorOrExtensionType(tf.test.TestCase):\n+\n+  def test_is_tensor_or_extension_type_return_true_for_ragged_tensor(self):\n+    self.assertTrue(tf_utils.is_tensor_or_extension_type(\n+        tf.ragged.constant([[1, 2], [3]])))\n+\n+  def test_is_tensor_or_extension_type_return_true_for_sparse_tensor(self):\n+    self.assertTrue(tf_utils.is_tensor_or_extension_type(\n+        tf.sparse.from_dense([[1, 2], [3, 4]])))\n+\n+  def test_is_tensor_or_extension_type_return_true_for_dense_tensor(self):\n+    self.assertTrue(tf_utils.is_tensor_or_extension_type(\n+        tf.constant([[1, 2], [3, 4]])))\n+\n+  def test_is_tensor_or_extension_type_return_true_for_custom_ext_types(self):\n+    class DummyExtensionType(tf.experimental.ExtensionType):\n+      ...\n+    self.assertTrue(tf_utils.is_tensor_or_extension_type(DummyExtensionType()))\n+\n+  def test_is_tensor_or_extension_type_return_false_for_list(self):\n+    self.assertFalse(tf_utils.is_tensor_or_extension_type([1., 2., 3.]))\n+\n+\n class TestRandomSeedSetting(tf.test.TestCase):\n \n   def test_seeds(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7482fa2a3fe18af56cfaabbfd3efc57ca7cb58fc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 3120 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -1240,7 +1240,7 @@ class RandomHeight(base_layer.BaseRandomLayer):\n       lower bound. For instance, `factor=(0.2, 0.3)` results in an output with\n       height changed by a random amount in the range `[20%, 30%]`.\n       `factor=(-0.2, 0.3)` results in an output with height changed by a random\n-      amount in the range `[-20%, +30%]. `factor=0.2` results in an output with\n+      amount in the range `[-20%, +30%]`. `factor=0.2` results in an output with\n       height changed by a random amount in the range `[-20%, +20%]`.\n     interpolation: String, the interpolation method. Defaults to `\"bilinear\"`.\n       Supports `\"bilinear\"`, `\"nearest\"`, `\"bicubic\"`, `\"area\"`,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#844f60e3da1b99d2bc8bd5c48a411c8893a44227", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 18 | Churn Cumulative: 863 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -21,8 +21,6 @@ Reference:\n       https://arxiv.org/abs/1905.11946) (ICML 2019)\n \"\"\"\n \n-import tensorflow.compat.v2 as tf\n-\n import copy\n import math\n \n@@ -32,7 +30,10 @@ from keras.engine import training\n from keras.layers import VersionAwareLayers\n from keras.utils import data_utils\n from keras.utils import layer_utils\n-from tensorflow.python.util.tf_export import keras_export\n+\n+import tensorflow.compat.v2 as tf\n+\n+from tensorflow.python.util.tf_export import keras_export  # pylint: disable=g-direct-tensorflow-import\n \n \n BASE_WEIGHTS_PATH = 'https://storage.googleapis.com/keras-applications/'\n@@ -204,6 +205,9 @@ BASE_DOCSTRING = \"\"\"Instantiates the {name} architecture.\n \"\"\"\n \n \n+IMAGENET_STDDEV_RGB = [0.229, 0.224, 0.225]\n+\n+\n def EfficientNet(\n     width_coefficient,\n     depth_coefficient,\n@@ -320,6 +324,14 @@ def EfficientNet(\n   x = img_input\n   x = layers.Rescaling(1. / 255.)(x)\n   x = layers.Normalization(axis=bn_axis)(x)\n+  if weights == 'imagenet':\n+    # Note that the normaliztion layer uses square value of STDDEV as the\n+    # variance for the layer: result = (input - mean) / sqrt(var)\n+    # However, the orginal implemenetation uses (input - mean) / var to\n+    # normalize the input, we need to divide another sqrt(var) to match the\n+    # original implementation.\n+    # See https://github.com/tensorflow/tensorflow/issues/49930 for more details\n+    x = x / tf.math.sqrt(IMAGENET_STDDEV_RGB)\n \n   x = layers.ZeroPadding2D(\n       padding=imagenet_utils.correct_pad(x, 3),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9ccd953f077bf11d57bf855ba460b7b8f879e63a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 16668 | Contributors (this commit): 121 | Commits (past 90d): 24 | Contributors (cumulative): 121 | DMM Complexity: None\n\nDIFF:\n@@ -3258,8 +3258,15 @@ def concat(tensors, axis=0):\n   return tf.concat(tensors, axis=axis)\n \n \n-def potentially_variable_concat(tensors):\n-  \"\"\"Concats `tensor`s along `axis`. When non-batch dimensions are variable, a RaggedTensor is returned instead.\"\"\"\n+def potentially_ragged_concat(tensors):\n+  \"\"\"Concats `Tensor`s along their first dimension.\n+\n+  Args:\n+    tensors: `list` of `Tensor`s\n+\n+  Returns:\n+    `Tensor` if input shapes are compatible or `RaggedTensor` if not.\n+    \"\"\"\n   if isinstance(tensors[0], tf.SparseTensor):\n     return tf.sparse.concat(axis=0, sp_inputs=tensors)\n   non_batch_shapes = tf.stack([tf.shape(tensor)[1:] for tensor in tensors])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e7b63eca61b69d4c344831abae25bcc61d638450", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 44 | Churn Cumulative: 459 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -317,5 +317,49 @@ class ActivationV2IntegrationTest(test_combinations.TestCase):\n     loaded_model = keras.models.load_model(output_path)\n     self.assertEqual(model.summary(), loaded_model.summary())\n \n+@test_combinations.run_with_all_model_types\n+@test_combinations.run_all_keras_modes\n+class TokenClassificationIntegrationTest(test_combinations.TestCase):\n+  \"\"\"Tests a very simple token classification model.\n+\n+  The main purpose of this test is to verify that everything works as expected when\n+  input sequences have variable length, and batches are padded only to the maximum length\n+  of each batch. This is very common in NLP, and results in the sequence dimension varying\n+  with each batch step for both the features and the labels.\n+  \"\"\"\n+  def test_token_classification(self):\n+    np.random.seed(1337)\n+    data = [np.random.randint(low=0, high=16, size=random.randint(4, 16)) for _ in range(100)]\n+    labels = [np.random.randint(low=0, high=3, size=len(arr)) for arr in data]\n+    features_dataset = tf.data.Dataset.from_tensor_slices(data)\n+    labels_dataset = tf.data.Dataset.from_tensor_slices(labels)\n+    dataset = tf.data.Dataset.zip(features_dataset, labels_dataset)\n+    dataset = dataset.padded_batch(batch_size=10)  # Pads with 0 values by default\n+\n+    layers = [\n+        keras.layers.Embedding(16, 4),\n+        keras.layers.Conv1D(4, 5, padding='same', activation='relu'),\n+        keras.layers.Conv1D(8, 5, padding='same'),\n+        keras.layers.BatchNormalization(),\n+        keras.layers.Conv2D(3, 5, padding='same', activation='softmax'),\n+    ]\n+    model = test_utils.get_model_from_layers(\n+        layers, input_shape=(None,))\n+    model.compile(\n+        loss='categorical_crossentropy',\n+        optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n+        metrics=['acc'],\n+        run_eagerly=test_utils.should_run_eagerly())\n+    history = model.fit(dataset, epochs=10,\n+                        validation_data=dataset,\n+                        verbose=2)\n+    self.assertGreater(history.history['val_acc'][-1], 0.7)\n+    _, val_acc = model.evaluate(dataset)\n+    self.assertAlmostEqual(history.history['val_acc'][-1], val_acc)\n+    predictions = model.predict(dataset)\n+    self.assertTrue(isinstance(predictions, tf.RaggedTensor))\n+    self.assertEqual(predictions.shape[0], len(dataset) * 10)\n+    self.assertEqual(predictions.shape[-1], 3)\n+\n if __name__ == '__main__':\n   tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3b87e6c50be9cd8618aab5b154a72c20a6b97f9b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 246 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 16/15 | Churn Δ: 248 | Churn Cumulative: 270 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: 0.45535714285714285\n\nDIFF:\n@@ -0,0 +1,229 @@\n+# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"AdamW optimizer implementation.\"\"\"\n+\n+from keras.optimizers.optimizer_experimental import optimizer\n+from keras.utils import generic_utils\n+import tensorflow.compat.v2 as tf\n+# pylint: disable=g-direct-tensorflow-import\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@generic_utils.register_keras_serializable()\n+@keras_export('keras.optimizers.experimental.AdamW', v1=[])\n+class AdamW(optimizer.Optimizer):\n+  r\"\"\"Optimizer that implements the AdamW algorithm.\n+\n+  AdamW optimization is a stochastic gradient descent method that is based on\n+  adaptive estimation of first-order and second-order moments with an added\n+  method to decay weights per the techniques discussed in the paeper,\n+  'Decoupled Weight Decay Regularization' by\n+  [Loshchilov, Hutter et al., 2019](https://arxiv.org/abs/1711.05101).\n+\n+  According to\n+  [Kingma et al., 2014](http://arxiv.org/abs/1412.6980),\n+  the underying Adam method is \"*computationally\n+  efficient, has little memory requirement, invariant to diagonal rescaling of\n+  gradients, and is well suited for problems that are large in terms of\n+  data/parameters*\".\n+\n+  Attributes:\n+    learning_rate: A `tf.Tensor`, floating point value, a schedule that is a\n+      `tf.keras.optimizers.schedules.LearningRateSchedule`, or a callable\n+      that takes no arguments and returns the actual value to use. The\n+      learning rate. Defaults to 0.001.\n+    weight_decay: A `tf.Tensor`, floating point value. The weight decay.\n+      Defaults to 0.004.\n+    beta_1: A float value or a constant float tensor, or a callable\n+      that takes no arguments and returns the actual value to use. The\n+      exponential decay rate for the 1st moment estimates. Defaults to 0.9.\n+    beta_2: A float value or a constant float tensor, or a callable\n+      that takes no arguments and returns the actual value to use. The\n+      exponential decay rate for the 2nd moment estimates. Defaults to 0.999.\n+    epsilon: A small constant for numerical stability. This epsilon is\n+      \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n+      Section 2.1), not the epsilon in Algorithm 1 of the paper. Defaults to\n+      1e-7.\n+    amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm from\n+      the paper \"On the Convergence of Adam and beyond\". Defaults to `False`.\n+    clipnorm: see the `clipnorm` argument of `optimizer_experimental.Optimizer`.\n+    clipvalue: see the `clipvalue` argument of\n+      `optimizer_experimental.Optimizer`.\n+    global_clipnorm: see the `global_clipnorm` argument of\n+      `optimizer_experimental.Optimizer`.\n+    use_ema: see the `use_ema` argument of `optimizer_experimental.Optimizer`.\n+    ema_momentum: see the `ema_momentum` argument of\n+      `optimizer_experimental.Optimizer`.\n+    ema_overwrite_frequency: see the `ema_overwrite_frequency` argument of\n+      `optimizer_experimental.Optimizer`.\n+    jit_compile: see the `jit_compile` argument of\n+      `optimizer_experimental.Optimizer`.\n+    name: Optional name prefix for the operations created when applying\n+      gradients. Defaults to `\"Adam\"`.\n+    **kwargs: see the `**kwargs` argument of `optimizer_experimental.Optimizer`.\n+\n+  Reference:\n+    - [Loshchilov et al., 2019](https://arxiv.org/abs/1711.05101)\n+    - [Kingma et al., 2014](http://arxiv.org/abs/1412.6980) for `adam`\n+    - [Reddi et al., 2018](\n+        https://openreview.net/pdf?id=ryQu7f-RZ) for `amsgrad`.\n+\n+  Notes:\n+\n+  The default value of 1e-7 for epsilon might not be a good default in\n+  general. For example, when training an Inception network on ImageNet a\n+  current good choice is 1.0 or 0.1. Note that since Adam uses the\n+  formulation just before Section 2.1 of the Kingma and Ba paper rather than\n+  the formulation in Algorithm 1, the \"epsilon\" referred to here is \"epsilon\n+  hat\" in the paper.\n+\n+  The sparse implementation of this algorithm (used when the gradient is an\n+  IndexedSlices object, typically because of `tf.gather` or an embedding\n+  lookup in the forward pass) does apply momentum to variable slices even if\n+  they were not used in the forward pass (meaning they have a gradient equal\n+  to zero). Momentum decay (beta1) is also applied to the entire momentum\n+  accumulator. This means that the sparse behavior is equivalent to the dense\n+  behavior (in contrast to some momentum implementations which ignore momentum\n+  unless a variable slice was actually used).\n+  \"\"\"\n+\n+  def __init__(self,\n+               learning_rate=0.001,\n+               weight_decay=0.004,\n+               beta_1=0.9,\n+               beta_2=0.999,\n+               epsilon=1e-7,\n+               amsgrad=False,\n+               clipnorm=None,\n+               clipvalue=None,\n+               global_clipnorm=None,\n+               use_ema=False,\n+               ema_momentum=0.99,\n+               ema_overwrite_frequency=None,\n+               jit_compile=False,\n+               name='AdamW',\n+               **kwargs):\n+    super(AdamW, self).__init__(\n+        name=name,\n+        clipnorm=clipnorm,\n+        clipvalue=clipvalue,\n+        global_clipnorm=global_clipnorm,\n+        use_ema=use_ema,\n+        ema_momentum=ema_momentum,\n+        ema_overwrite_frequency=ema_overwrite_frequency,\n+        jit_compile=jit_compile,\n+        **kwargs)\n+    self._learning_rate = self._build_learning_rate(learning_rate)\n+    self.weight_decay = weight_decay\n+    self.beta_1 = beta_1\n+    self.beta_2 = beta_2\n+    self.epsilon = epsilon\n+    self.amsgrad = amsgrad\n+\n+    if self.weight_decay is None:\n+      raise ValueError('Missing value of `weight_decay` which is required and'\n+                       ' must be a float value.')\n+\n+  def build(self, var_list):\n+    \"\"\"Initialize optimizer variables.\n+\n+    AdamW optimizer has 3 types of variables: momentums, velocities and\n+    velocity_hat (only set when amsgrad is applied),\n+\n+    Args:\n+      var_list: list of model variables to build AdamW variables on.\n+    \"\"\"\n+    super().build(var_list)\n+    if hasattr(self, '_built') and self._built:\n+      return\n+    self._built = True\n+    self._momentums = []\n+    self._velocities = []\n+    for var in var_list:\n+      self._momentums.append(\n+          self.add_variable_from_reference(\n+              model_variable=var, variable_name='m'))\n+      self._velocities.append(\n+          self.add_variable_from_reference(\n+              model_variable=var, variable_name='v'))\n+    if self.amsgrad:\n+      self._velocity_hats = []\n+      for var in var_list:\n+        self._velocity_hats.append(\n+            self.add_variable_from_reference(\n+                model_variable=var, variable_name='vhat'))\n+\n+  def update_step(self, gradient, variable):\n+    \"\"\"Update step given gradient and the associated model variable.\"\"\"\n+    if self._var_key(variable) not in self._index_dict:\n+      raise KeyError(f'Optimizer cannot recognize variable {variable.name}, '\n+                     f'this usually means you are calling an optimizer '\n+                     f'previously used on a different model. Please try '\n+                     f'creating a new optimizer instance.')\n+    beta_1_power = None\n+    beta_2_power = None\n+    lr = tf.cast(self.learning_rate, variable.dtype)\n+    local_step = tf.cast(self.iterations + 1, variable.dtype)\n+    beta_1_power = tf.pow(tf.cast(self.beta_1, variable.dtype), local_step)\n+    beta_2_power = tf.pow(tf.cast(self.beta_2, variable.dtype), local_step)\n+\n+    var_key = self._var_key(variable)\n+    m = self._momentums[self._index_dict[var_key]]\n+    v = self._velocities[self._index_dict[var_key]]\n+\n+    alpha = (lr * tf.sqrt(1 - beta_2_power) / (1 - beta_1_power))\n+\n+    # Apply step weight decay\n+    if self.weight_decay != 0:\n+      wd = tf.cast(self.weight_decay, variable.dtype)\n+      variable.assign_sub(variable * (1 - lr * wd))\n+\n+    if isinstance(gradient, tf.IndexedSlices):\n+      # Sparse gradients.\n+      m.assign_add(-m * (1 - self.beta_1))\n+      m.scatter_add(\n+          tf.IndexedSlices(gradient.values * (1 - self.beta_1),\n+                           gradient.indices))\n+      v.assign_add(-v * (1 - self.beta_2))\n+      v.scatter_add(\n+          tf.IndexedSlices(\n+              tf.square(gradient.values) * (1 - self.beta_2), gradient.indices))\n+      if self.amsgrad:\n+        v_hat = self._velocity_hats[self._index_dict[var_key]]\n+        v_hat.assign(tf.maximum(v_hat, v))\n+        v = v_hat\n+      variable.assign_sub((m * alpha) / (tf.sqrt(v) + self.epsilon))\n+    else:\n+      # Dense gradients.\n+      m.assign_add((gradient - m) * (1 - self.beta_1))\n+      v.assign_add((tf.square(gradient) - v) * (1 - self.beta_2))\n+      if self.amsgrad:\n+        v_hat = self._velocity_hats[self._index_dict[var_key]]\n+        v_hat.assign(tf.maximum(v_hat, v))\n+        v = v_hat\n+      variable.assign_sub((m * alpha) / (tf.sqrt(v) + self.epsilon))\n+\n+  def get_config(self):\n+    config = super(AdamW, self).get_config()\n+\n+    config.update({\n+        'learning_rate': self._serialize_hyperparameter(self._learning_rate),\n+        'weight_decay': self.weight_decay,\n+        'beta_1': self.beta_1,\n+        'beta_2': self.beta_2,\n+        'epsilon': self.epsilon,\n+        'amsgrad': self.amsgrad,\n+    })\n+    return config\n\n@@ -13,6 +13,7 @@ from keras.optimizers import learning_rate_schedule\n from keras.optimizers.optimizer_experimental import adadelta as adadelta_new\n from keras.optimizers.optimizer_experimental import adagrad as adagrad_new\n from keras.optimizers.optimizer_experimental import adam as adam_new\n+from keras.optimizers.optimizer_experimental import adamw as adamw_new\n from keras.optimizers.optimizer_experimental import rmsprop as rmsprop_new\n from keras.optimizers.optimizer_experimental import sgd as sgd_new\n from keras.optimizers.optimizer_v2 import adadelta as adadelta_old\n@@ -41,22 +42,29 @@ STRATEGIES = [\n adadelta_new_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentaladadelta\",\n     lambda: adadelta_new.Adadelta(  # pylint: disable=g-long-lambda\n-        0.002, use_ema=True, ema_overwrite_frequency=None))\n+        0.002,\n+        use_ema=True,\n+        ema_overwrite_frequency=None))\n adagrad_new_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentaladagrad\", lambda: adagrad_new.Adagrad(0.002))\n adam_new_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentaladam\", lambda: adam_new.Adam(0.002))\n+adamw_new_fn = tf.__internal__.test.combinations.NamedObject(\n+    \"experimentaladamw\", lambda: adamw_new.AdamW(0.002, weight_decay=0.004))\n rmsprop_new_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentalrmsprop\", lambda: rmsprop_new.RMSprop(0.002))\n sgd_new_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentalsgdaverage\",\n     lambda: sgd_new.SGD(  # pylint: disable=g-long-lambda\n-        0.002, use_ema=True, ema_overwrite_frequency=1))\n+        0.002,\n+        use_ema=True,\n+        ema_overwrite_frequency=1))\n \n OPTIMIZER_FN = [\n     adadelta_new_fn,\n     adagrad_new_fn,\n     adam_new_fn,\n+    adamw_new_fn,\n     rmsprop_new_fn,\n     sgd_new_fn,\n ]\n@@ -163,6 +171,13 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n     with self.assertRaisesRegex(RuntimeError, \"Cannot set*\"):\n       optimizer.iterations = 2\n \n+  def testPassingMissingWDError(self):\n+    with self.assertRaises(ValueError):\n+      _ = adamw_new.AdamW(0.01, weight_decay=None)\n+\n+    with self.assertRaisesRegex(ValueError, \"Missing value of\"):\n+      _ = adamw_new.AdamW(0.01, weight_decay=None)\n+\n   def testMovingAverageOptimizer(self):\n     optimizer = sgd_new.SGD(\n         learning_rate=1,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b298bce746abdebbaa81e21383a3202656662607", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 16670 | Contributors (this commit): 121 | Commits (past 90d): 24 | Contributors (cumulative): 121 | DMM Complexity: None\n\nDIFF:\n@@ -2011,7 +2011,7 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n                          'information of where went wrong, or file a '\n                          'issue/bug to `tf.keras`.')\n       callbacks.on_predict_end()\n-    all_outputs = tf.__internal__.nest.map_structure_up_to(batch_outputs, potentially_variable_concat, outputs)\n+    all_outputs = tf.__internal__.nest.map_structure_up_to(batch_outputs, potentially_ragged_concat, outputs)\n \n     # If originally PSS strategy was used, then replace it back since predict\n     # is running under `OneDeviceStrategy` after the swap and once its done\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1eb343374c7ed05c73086471a292b64314c94918", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 16672 | Contributors (this commit): 121 | Commits (past 90d): 25 | Contributors (cumulative): 121 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3269,6 +3269,8 @@ def potentially_ragged_concat(tensors):\n     \"\"\"\n   if isinstance(tensors[0], tf.SparseTensor):\n     return tf.sparse.concat(axis=0, sp_inputs=tensors)\n+  elif isinstance(tensors[0], tf.RaggedTensor):\n+    return tf.concat(tensors, axis=0)\n   non_batch_shapes = tf.stack([tf.shape(tensor)[1:] for tensor in tensors])\n   constant_dims = tf.math.reduce_all(non_batch_shapes == non_batch_shapes[:1], axis=0)\n   if tf.math.reduce_all(constant_dims).numpy().item():  # All non-batch dimensions are constant\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#76ba9e70c2f6669eeb84a71437cd13daa3b15bf1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 11 | Churn Cumulative: 470 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -329,12 +329,15 @@ class TokenClassificationIntegrationTest(test_combinations.TestCase):\n   \"\"\"\n   def test_token_classification(self):\n     np.random.seed(1337)\n-    data = [np.random.randint(low=0, high=16, size=random.randint(4, 16)) for _ in range(100)]\n-    labels = [np.random.randint(low=0, high=3, size=len(arr)) for arr in data]\n+    def densify(x, y):\n+        return x.to_tensor(), y.to_tensor()\n+    data = tf.ragged.stack([np.random.randint(low=0, high=16, size=random.randint(4, 16)) for _ in range(100)])\n+    labels = tf.ragged.stack([np.random.randint(low=0, high=3, size=len(arr)) for arr in data])\n     features_dataset = tf.data.Dataset.from_tensor_slices(data)\n     labels_dataset = tf.data.Dataset.from_tensor_slices(labels)\n-    dataset = tf.data.Dataset.zip(features_dataset, labels_dataset)\n-    dataset = dataset.padded_batch(batch_size=10)  # Pads with 0 values by default\n+    dataset = tf.data.Dataset.zip((features_dataset, labels_dataset))\n+    dataset = dataset.batch(batch_size=10)\n+    dataset = dataset.map(densify)  # Pads with 0 values by default\n \n     layers = [\n         keras.layers.Embedding(16, 4),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#163ae9681e4ec19de40be444ebf4cdcbca759307", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 472 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -344,7 +344,7 @@ class TokenClassificationIntegrationTest(test_combinations.TestCase):\n         keras.layers.Conv1D(4, 5, padding='same', activation='relu'),\n         keras.layers.Conv1D(8, 5, padding='same'),\n         keras.layers.BatchNormalization(),\n-        keras.layers.Conv2D(3, 5, padding='same', activation='softmax'),\n+        keras.layers.Conv1D(3, 5, padding='same', activation='softmax'),\n     ]\n     model = test_utils.get_model_from_layers(\n         layers, input_shape=(None,))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5cc78246868cc1cae36bd14a2cef78b4ff486374", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 476 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -349,14 +349,14 @@ class TokenClassificationIntegrationTest(test_combinations.TestCase):\n     model = test_utils.get_model_from_layers(\n         layers, input_shape=(None,))\n     model.compile(\n-        loss='categorical_crossentropy',\n+        loss='sparse_categorical_crossentropy',\n         optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n         metrics=['acc'],\n         run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(dataset, epochs=10,\n                         validation_data=dataset,\n                         verbose=2)\n-    self.assertGreater(history.history['val_acc'][-1], 0.7)\n+    self.assertGreater(history.history['val_acc'][-1], 0.5)\n     _, val_acc = model.evaluate(dataset)\n     self.assertAlmostEqual(history.history['val_acc'][-1], val_acc)\n     predictions = model.predict(dataset)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#30cabb10f8db2f4dd278564e966415064dcc383c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 23 | Churn Cumulative: 499 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -317,22 +317,27 @@ class ActivationV2IntegrationTest(test_combinations.TestCase):\n     loaded_model = keras.models.load_model(output_path)\n     self.assertEqual(model.summary(), loaded_model.summary())\n \n-@test_combinations.run_with_all_model_types\n+@test_combinations.run_with_all_model_types(always_skip_v1=True)\n @test_combinations.run_all_keras_modes\n class TokenClassificationIntegrationTest(test_combinations.TestCase):\n   \"\"\"Tests a very simple token classification model.\n \n-  The main purpose of this test is to verify that everything works as expected when\n-  input sequences have variable length, and batches are padded only to the maximum length\n-  of each batch. This is very common in NLP, and results in the sequence dimension varying\n-  with each batch step for both the features and the labels.\n+  The main purpose of this test is to verify that everything works as expected\n+  when input sequences have variable length, and batches are padded only to the\n+  maximum length of each batch. This is very common in NLP, and results in the\n+  sequence dimension varying with each batch step for both the features\n+  and the labels.\n   \"\"\"\n   def test_token_classification(self):\n-    np.random.seed(1337)\n     def densify(x, y):\n         return x.to_tensor(), y.to_tensor()\n-    data = tf.ragged.stack([np.random.randint(low=0, high=16, size=random.randint(4, 16)) for _ in range(100)])\n-    labels = tf.ragged.stack([np.random.randint(low=0, high=3, size=len(arr)) for arr in data])\n+    np.random.seed(1337)\n+    keras.utils.set_random_seed(1337)\n+    data = tf.ragged.stack(\n+        [np.random.randint(low=0, high=16, size=random.randint(4, 16)) for _ in\n+         range(100)])\n+    labels = tf.ragged.stack(\n+        [np.random.randint(low=0, high=3, size=len(arr)) for arr in data])\n     features_dataset = tf.data.Dataset.from_tensor_slices(data)\n     labels_dataset = tf.data.Dataset.from_tensor_slices(labels)\n     dataset = tf.data.Dataset.zip((features_dataset, labels_dataset))\n@@ -350,7 +355,7 @@ class TokenClassificationIntegrationTest(test_combinations.TestCase):\n         layers, input_shape=(None,))\n     model.compile(\n         loss='sparse_categorical_crossentropy',\n-        optimizer=keras.optimizers.optimizer_v2.adam.Adam(0.005),\n+        optimizer='adam'\n         metrics=['acc'],\n         run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(dataset, epochs=10,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#13b58e2ddc38c95ae49f2564a9c41bc3efaaac9f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 35 | Churn Cumulative: 1819 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 5 | DMM Complexity: 0.9\n\nDIFF:\n@@ -3335,9 +3335,12 @@ def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n     Sparse top K categorical accuracy value.\n   \"\"\"\n   reshape_matches = False\n-  y_pred_rank = tf.convert_to_tensor(y_pred).shape.ndims\n-  y_true_org_shape = tf.convert_to_tensor(y_true).shape\n-  y_true_rank = y_true_org_shape.ndims\n+  y_true = tf.convert_to_tensor(y_true)\n+  y_pred = tf.convert_to_tensor(y_pred)\n+  y_true_rank = y_true.shape.ndims\n+  y_pred_rank = y_pred.shape.ndims\n+  y_true_org_shape = tf.shape(y_true)\n+\n   # Flatten y_pred to (batch_size, num_samples) and y_true to (num_samples,)\n   if (y_true_rank is not None) and (y_pred_rank is not None):\n     if y_pred_rank > 2:\n\n@@ -20,6 +20,7 @@ import math\n from keras import backend\n from keras import layers\n from keras import metrics\n+from keras import Model\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n import numpy as np\n@@ -770,6 +771,31 @@ class SparseTopKCategoricalAccuracyTest(tf.test.TestCase):\n     result = a_obj(y_true, y_pred, sample_weight=sample_weight)\n     self.assertAllClose(1.0, self.evaluate(result), atol=1e-5)\n \n+  def test_sparse_top_k_categorical_accuracy_mismatched_dims_dynamic(self):\n+\n+    if not tf.compat.v1.executing_eagerly():\n+      # Test will fail in v1 graph mode since the metric is not a normal layer.\n+      # It will aggregate the output by batch dim, which failed on v1 code.\n+      self.skipTest('v2 eager mode only')\n+\n+    class AccLayer(layers.Layer):\n+\n+      def build(self, _):\n+        self.acc = metrics.SparseTopKCategoricalAccuracy(k=1)\n+\n+      def call(self, y_true, y_pred):\n+        return self.acc(y_true, y_pred)\n+\n+    label = layers.Input(shape=[1])\n+    predict = layers.Input(shape=[3])\n+    metric_result = AccLayer()(label, predict)\n+    model = Model([label, predict], metric_result)\n+\n+    result = model.predict([tf.constant([[2], [1]]),\n+                            tf.constant([[0.1, 0.1, 0.8], [0.05, 0, 0.95]])],\n+                           steps=1)\n+    self.assertAllClose(result, 0.5)\n+\n \n @test_combinations.generate(test_combinations.combine(mode=['graph', 'eager']))\n class LogCoshErrorTest(tf.test.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cac9670fe1009c90378efa04a6383d977385d194", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 23 | Churn Cumulative: 17194 | Contributors (this commit): 121 | Commits (past 90d): 34 | Contributors (cumulative): 125 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3272,17 +3272,21 @@ def potentially_ragged_concat(tensors):\n   elif isinstance(tensors[0], tf.RaggedTensor):\n     return tf.concat(tensors, axis=0)\n   non_batch_shapes = tf.stack([tf.shape(tensor)[1:] for tensor in tensors])\n-  constant_dims = tf.math.reduce_all(non_batch_shapes == non_batch_shapes[:1], axis=0)\n-  if tf.math.reduce_all(constant_dims).numpy().item():  # All non-batch dimensions are constant\n+  constant_dims = tf.math.reduce_all(non_batch_shapes == non_batch_shapes[:1],\n+                                     axis=0)\n+  if tf.math.reduce_all(\n+          constant_dims).numpy().item():  # All non-batch dims are constant\n     return tf.concat(tensors, axis=0)\n-  # First, identify constant inner dimensions by finding the rightmost dimension that is not constant\n+  # First, identify constant inner dimensions by finding the\n+  # rightmost dimension that is not constant\n   constant_inner_dimensions = constant_dims.numpy().tolist()[::-1].index(False)\n   # If there are constant inner dimensions, define a constant inner shape\n   if constant_inner_dimensions == 0:\n     constant_inner_shape = None\n   else:\n     constant_inner_shape = tensors[0].shape[-constant_inner_dimensions:]\n-  return tf.ragged.constant([tensor.numpy() for tensor in tensors], inner_shape=constant_inner_shape).merge_dims(0, 1)\n+  return tf.ragged.constant([tensor.numpy() for tensor in tensors],\n+                            inner_shape=constant_inner_shape).merge_dims(0, 1)\n \n \n def _get_verbosity(verbose, distribute_strategy):\n\n@@ -26,7 +26,7 @@ from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n from keras.layers.legacy_rnn import rnn_cell_impl as rnn_cell\n from keras.legacy_tf_layers import base as base_layer\n-from keras.utils import np_utils\n+from keras.utils import np_utils, tf_utils\n \n \n class KerasIntegrationTest(test_combinations.TestCase):\n@@ -317,8 +317,8 @@ class ActivationV2IntegrationTest(test_combinations.TestCase):\n     loaded_model = keras.models.load_model(output_path)\n     self.assertEqual(model.summary(), loaded_model.summary())\n \n-@test_combinations.run_with_all_model_types(always_skip_v1=True)\n-@test_combinations.run_all_keras_modes\n+@test_combinations.run_with_all_model_types\n+@test_combinations.run_all_keras_modes(always_skip_v1=True)\n class TokenClassificationIntegrationTest(test_combinations.TestCase):\n   \"\"\"Tests a very simple token classification model.\n \n@@ -331,8 +331,7 @@ class TokenClassificationIntegrationTest(test_combinations.TestCase):\n   def test_token_classification(self):\n     def densify(x, y):\n         return x.to_tensor(), y.to_tensor()\n-    np.random.seed(1337)\n-    keras.utils.set_random_seed(1337)\n+    tf_utils.set_random_seed(1337)\n     data = tf.ragged.stack(\n         [np.random.randint(low=0, high=16, size=random.randint(4, 16)) for _ in\n          range(100)])\n@@ -355,7 +354,7 @@ class TokenClassificationIntegrationTest(test_combinations.TestCase):\n         layers, input_shape=(None,))\n     model.compile(\n         loss='sparse_categorical_crossentropy',\n-        optimizer='adam'\n+        optimizer='adam',\n         metrics=['acc'],\n         run_eagerly=test_utils.should_run_eagerly())\n     history = model.fit(dataset, epochs=10,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
