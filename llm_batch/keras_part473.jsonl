{"custom_id": "keras#d34dff6dbd5e3b9a5385cba6651169106a86fc67", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 40 | Churn Cumulative: 19625 | Contributors (this commit): 131 | Commits (past 90d): 25 | Contributors (cumulative): 131 | DMM Complexity: None\n\nDIFF:\n@@ -3663,16 +3663,46 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n \n \n def reduce_per_replica(values, strategy, reduction=\"first\"):\n-    \"\"\"Reduce PerReplica objects.\n+    \"\"\"Attempt to reduce the structure `values` to single values.\n+\n+    Given `values` (a `tf.Tensor` or a `PerReplica` structure),\n+    which represents the values across all the replicas, `reduce_per_replica`\n+    attempts to \"reduce\" those values and returns the corresponding structure\n+    that represents only single values.\n+\n+    Currently, `reduce_per_replica` is only used for reducing the metric results\n+    from `tf.distribute.Strategy.run()`. Depending on the underlying\n+    `Strategy` implementation, `values` may be a `PerReplica` object,\n+     which can be thought of as a collection of values across the replicas,\n+    or a `tf.Tensor`, if the strategy has already conducted the reduction\n+    for the downstream library.\n+\n+    There are three possible outcomes of reduction:\n+\n+    1) if the `values` is a structure of simple `tf.Tensor`s, meaning that\n+       reduction is not actually needed, `reduce_per_replica` returns the\n+       structure as-is.\n+    2) else, if `reduction=\"first\"`, then `reduce_per_replica`\n+       returns the values of the first replica. This is used in the case of\n+       training and evaluation, where `values` is expected to hold the same\n+       value across the replicas as a result of `Strategy`'s synchronization\n+       across the replicas.\n+       `reduce_per_replica` does not synchronize the values.\n+    3) else, if `reduction=\"concat\"`, then `reduce_per_replica`\n+       returns the concatenation of the values across the replicas, along the\n+       axis of dimension 0. This is used in the inference case (`predict()`).\n \n     Args:\n-      values: Structure of `PerReplica` objects or `Tensor`s. `Tensor`s are\n-        returned as-is.\n+      values: Structure of `PerReplica` objects or `tf.Tensor`s. `tf.Tensor`s\n+        are returned as-is.\n       strategy: `tf.distribute.Strategy` object.\n-      reduction: One of 'first', 'concat'.\n+      reduction: One of `\"first\"`, `\"concat\"`.\n \n     Returns:\n-      Structure of `Tensor`s.\n+      Structure of `Tensor`s, representing the result of reduction.\n+\n+    Raises:\n+      ValueError: if the reduction method is not supported.\n     \"\"\"\n \n     def _reduce(v):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dee67880be30eb1a5c2dfc7b4e60b32cd4789fbe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 8/5 | Churn Δ: 45 | Churn Cumulative: 708 | Contributors (this commit): 8 | Commits (past 90d): 13 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -46,8 +46,12 @@ Experimental. API is subject to change.\n \n import os\n \n+import requests\n import tensorflow.compat.v2 as tf\n \n+GCP_METADATA_HEADER = {\"Metadata-Flavor\": \"Google\"}\n+_GCE_METADATA_URL_ENV_VARIABLE = \"GCE_METADATA_IP\"\n+\n \n def _get_base_dirpath(strategy):\n     task_id = strategy.extended._task_id\n@@ -145,3 +149,31 @@ def remove_temp_dir_with_filepath(filepath, strategy):\n       strategy: The tf.distribute strategy object currently used.\n     \"\"\"\n     remove_temp_dirpath(os.path.dirname(filepath), strategy)\n+\n+\n+def _on_gcp():\n+    \"\"\"Detect whether the current running environment is on GCP.\"\"\"\n+    gce_metadata_endpoint = \"http://\" + os.environ.get(\n+        _GCE_METADATA_URL_ENV_VARIABLE, \"metadata.google.internal\"\n+    )\n+\n+    try:\n+        # Timeout in 5 seconds, in case the test environment has connectivity\n+        # issue. There is not default timeout, which means it might block\n+        # forever.\n+        response = requests.get(\n+            \"%s/computeMetadata/v1/%s\"\n+            % (gce_metadata_endpoint, \"instance/hostname\"),\n+            headers=GCP_METADATA_HEADER,\n+            timeout=5,\n+        )\n+        return response.status_code\n+    except requests.exceptions.RequestException:\n+        return False\n+\n+\n+def support_on_demand_checkpoint_callback():\n+    if _on_gcp() and not tf.config.list_physical_devices(\"TPU\"):\n+        return True\n+\n+    return False\n\n@@ -22,6 +22,19 @@ from keras import backend\n from keras.distribute import distributed_file_utils\n from keras.utils import mode_keys\n \n+# isort: off\n+from keras.distribute.distributed_file_utils import (\n+    support_on_demand_checkpoint_callback,\n+)  # noqa: E501\n+\n+\n+def _enable_preemption_checkpoint(preemption_checkpoint_arg, strategy):\n+    return (\n+        preemption_checkpoint_arg\n+        and isinstance(strategy, tf.distribute.MultiWorkerMirroredStrategy)\n+        and support_on_demand_checkpoint_callback()\n+    )\n+\n \n class WorkerTrainingState:\n     \"\"\"Training state management class.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f95c9050ae577d97f2a7cae84a3096bbc9a1d869", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 11 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 1/3 | Churn Δ: 43 | Churn Cumulative: 2527 | Contributors (this commit): 9 | Commits (past 90d): 13 | Contributors (cumulative): 16 | DMM Complexity: 0.8421052631578947\n\nDIFF:\n@@ -420,7 +420,7 @@ class BatchNormalizationBase(Layer):\n                 param_shape.insert(1, 1)\n                 for idx, x in enumerate(self.axis):\n                     self.axis[idx] = x + 1  # Account for added dimension\n-\n+        self._param_shape = param_shape\n         if self.scale:\n             self.gamma = self.add_weight(\n                 name=\"gamma\",\n@@ -434,10 +434,6 @@ class BatchNormalizationBase(Layer):\n             )\n         else:\n             self.gamma = None\n-            if self.fused:\n-                self._gamma_const = backend.constant(\n-                    1.0, dtype=self._param_dtype, shape=param_shape\n-                )\n \n         if self.center:\n             self.beta = self.add_weight(\n@@ -452,10 +448,6 @@ class BatchNormalizationBase(Layer):\n             )\n         else:\n             self.beta = None\n-            if self.fused:\n-                self._beta_const = backend.constant(\n-                    0.0, dtype=self._param_dtype, shape=param_shape\n-                )\n \n         try:\n             # Disable variable partitioning when creating the moving mean and\n@@ -582,8 +574,18 @@ class BatchNormalizationBase(Layer):\n \n     def _fused_batch_norm(self, inputs, training):\n         \"\"\"Returns the output of fused batch norm.\"\"\"\n-        beta = self.beta if self.center else self._beta_const\n-        gamma = self.gamma if self.scale else self._gamma_const\n+        if self.center:\n+            beta = self.beta\n+        else:\n+            beta = backend.constant(\n+                0.0, dtype=self._param_dtype, shape=self._param_shape\n+            )\n+        if self.scale:\n+            gamma = self.gamma\n+        else:\n+            gamma = backend.constant(\n+                1.0, dtype=self._param_dtype, shape=self._param_shape\n+            )\n \n         # TODO(b/129279393): Support zero batch input in non\n         # DistributionStrategy code as well.\n\n@@ -273,6 +273,25 @@ class BatchNormalizationTest(test_combinations.TestCase):\n         # variance is 2 * 0.5 == 1.\n         self.assertAllEqual(self.evaluate(layer.moving_variance), [1.0])\n \n+    @test_combinations.run_all_keras_modes\n+    def test_can_be_used_in_multiple_graphs(self):\n+        norm = keras.layers.BatchNormalization(\n+            scale=False, center=False, fused=True\n+        )\n+\n+        @tf.function\n+        def fn1(x):\n+            return norm(x, training=True)\n+\n+        @tf.function\n+        def fn2(x):\n+            return norm(x, training=True)\n+\n+        x = np.array([-1000.0, 1000.0]).reshape((2, 1, 1, 1))\n+        y = norm(fn2(fn1(x)), training=True)\n+        expected_y = np.array([-0.9995, 0.9995]).reshape((2, 1, 1, 1))\n+        self.assertAllClose(keras.backend.eval(y), expected_y)\n+\n \n class BatchNormalizationV1Test(test_combinations.TestCase):\n     @test_combinations.generate(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#76055673c562571c088f80184c70034221a6ee7e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 6 | Methods Changed: 4 | Complexity Δ (Sum/Max): 13/12 | Churn Δ: 25 | Churn Cumulative: 920 | Contributors (this commit): 9 | Commits (past 90d): 26 | Contributors (cumulative): 15 | DMM Complexity: 0.8333333333333334\n\nDIFF:\n@@ -548,7 +548,7 @@ class MultiHeadAttention(Layer):\n         attention_mask=None,\n         return_attention_scores=False,\n         training=None,\n-        use_causal_mask=False\n+        use_causal_mask=False,\n     ):\n         attention_mask = self._compute_attention_mask(\n             query,\n@@ -645,13 +645,16 @@ class MultiHeadAttention(Layer):\n         key_mask = getattr(key, \"_keras_mask\", None)\n         auto_mask = None\n         if query_mask is not None:\n+            query_mask = tf.cast(query_mask, tf.bool)  # defensive casting\n             # B = batch size, T = max query length\n             auto_mask = query_mask[:, :, tf.newaxis]  # shape is [B, T, 1]\n         if value_mask is not None:\n+            value_mask = tf.cast(value_mask, tf.bool)  # defensive casting\n             # B = batch size, S == max value length\n             mask = value_mask[:, tf.newaxis, :]  # shape is [B, 1, S]\n             auto_mask = mask if auto_mask is None else auto_mask & mask\n         if key_mask is not None:\n+            key_mask = tf.cast(key_mask, tf.bool)  # defensive casting\n             # B == batch size, S == max key length == max value length\n             mask = key_mask[:, tf.newaxis, :]  # shape is [B, 1, S]\n             auto_mask = mask if auto_mask is None else auto_mask & mask\n@@ -664,7 +667,7 @@ class MultiHeadAttention(Layer):\n             attention_mask = (\n                 auto_mask\n                 if attention_mask is None\n-                else attention_mask & auto_mask\n+                else tf.cast(attention_mask, bool) & auto_mask\n             )\n         return attention_mask\n \n\n@@ -371,6 +371,24 @@ class MultiHeadAttentionTest(test_combinations.TestCase):\n         )\n         self.assertAllClose(output, output_with_manual_mask)\n \n+    def test_masks_are_cast_to_bool(self):\n+        \"\"\"Test that the implicit and explicit masks are cast to bool.\"\"\"\n+        test_layer = keras.layers.MultiHeadAttention(num_heads=2, key_dim=2)\n+        query = np.array([[1, 2, 3, 0, 0], [3, 3, 1, 1, 2], [1, 0, 0, 0, 0]])\n+        masked_query = keras.layers.Embedding(4, 8, mask_zero=True)(query)\n+        masked_query._keras_mask = tf.cast(masked_query._keras_mask, tf.float32)\n+        value = np.array([[5, 4, 0], [3, 0, 0], [2, 1, 1]])\n+        masked_value = keras.layers.Embedding(6, 8, mask_zero=True)(value)\n+        masked_value._keras_mask = tf.cast(masked_value._keras_mask, tf.float32)\n+        float_mask = tf.constant([[[1.0]]])\n+        # if all works well, the following should not raise any exception:\n+        _ = test_layer(\n+            query=masked_query,\n+            value=masked_value,\n+            use_causal_mask=True,\n+            attention_mask=float_mask,\n+        )\n+\n \n class SubclassAttention(keras.layers.MultiHeadAttention):\n     def _build_attention(self, qkv_rank):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ba4269727d6f982ddbb6cbe1ed52765f51a264f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 866 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -306,7 +306,7 @@ class TestConvUtils(tf.test.TestCase, parameterized.TestCase):\n             ),\n         )\n \n-    def test_conv_kernel_mask_rect_kernel(self, *input_shape):\n+    def DISABLED_test_conv_kernel_mask_rect_kernel(self, *input_shape):\n         padding = \"valid\"\n         ndims = len(input_shape)\n         strides = (1,) * ndims\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#726e268c3d76f6e446dcb17e8424697ecc28a566", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 36 | Churn Cumulative: 711 | Contributors (this commit): 6 | Commits (past 90d): 14 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -20,6 +20,7 @@ from keras.dtensor import dtensor_api as dtensor\n from keras.optimizers.optimizer_experimental import adadelta\n from keras.optimizers.optimizer_experimental import adagrad\n from keras.optimizers.optimizer_experimental import adam\n+from keras.optimizers.optimizer_experimental import adamw\n from keras.optimizers.optimizer_experimental import optimizer as optimizer_lib\n from keras.optimizers.optimizer_experimental import rmsprop\n from keras.optimizers.optimizer_experimental import sgd\n@@ -241,6 +242,34 @@ class Adam(Optimizer, adam.Adam):\n         self.amsgrad = amsgrad\n \n \n+@keras_export(\"keras.dtensor.experimental.optimizers.AdamW\", v1=[])\n+class AdamW(Optimizer, adamw.AdamW):\n+    def __init__(\n+        self,\n+        learning_rate=0.001,\n+        weight_decay=0.004,\n+        beta_1=0.9,\n+        beta_2=0.999,\n+        epsilon=1e-7,\n+        amsgrad=False,\n+        name=\"AdamW\",\n+        mesh=None,\n+    ):\n+        Optimizer.__init__(self, name=name, mesh=mesh)\n+        self._learning_rate = self._build_learning_rate(learning_rate)\n+        self.weight_decay = weight_decay\n+        self.beta_1 = beta_1\n+        self.beta_2 = beta_2\n+        self.epsilon = epsilon\n+        self.amsgrad = amsgrad\n+\n+        if self.weight_decay is None:\n+            raise ValueError(\n+                \"Missing value of `weight_decay` which is required and\"\n+                \" must be a float value.\"\n+            )\n+\n+\n @keras_export(\"keras.dtensor.experimental.optimizers.RMSprop\", v1=[])\n class RMSprop(Optimizer, rmsprop.RMSprop):\n     def __init__(\n@@ -291,5 +320,6 @@ class SGD(Optimizer, sgd.SGD):\n Adadelta.__doc__ = Optimizer.__doc__ + adadelta.Adadelta.__doc__\n Adagrad.__doc__ = Optimizer.__doc__ + adagrad.Adagrad.__doc__\n Adam.__doc__ = Optimizer.__doc__ + adam.Adam.__doc__\n+AdamW.__doc__ = Optimizer.__doc__ + adamw.AdamW.__doc__\n RMSprop.__doc__ = Optimizer.__doc__ + rmsprop.RMSprop.__doc__\n SGD.__doc__ = Optimizer.__doc__ + sgd.SGD.__doc__\n\n@@ -88,6 +88,12 @@ class OptimizersTest(test_util.DTensorBaseTest):\n             {\"amsgrad\": True},\n             [\"Adam/m/Variable\", \"Adam/v/Variable\", \"Adam/vhat/Variable\"],\n         ),\n+        (\n+            \"AdamW\",\n+            optimizers.AdamW,\n+            {\"amsgrad\": True},\n+            [\"AdamW/m/Variable\", \"AdamW/v/Variable\", \"AdamW/vhat/Variable\"],\n+        ),\n         (\"Adagrad\", optimizers.Adagrad, {}, [\"Adagrad/accumulator/Variable\"]),\n         (\n             \"RMSprop\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1f28f313ef88a7cb3969b0c984a5a1fe2c9154ee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 653 | Contributors (this commit): 9 | Commits (past 90d): 8 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -409,7 +409,7 @@ class TruncatedNormal(tf.compat.v1.truncated_normal_initializer):\n     >>> a = initializer(shape=(2, 2))\n     >>> b = initializer(shape=(2, 2))\n     >>> tf.reduce_sum(a - b) == 0\n-    <tf.Tensor: shape=(), dtype=bool, numpy=False>\n+    <tf.Tensor: shape=(), dtype=bool, numpy=True>\n \n     @end_compatibility\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#22a3d41589fbe61703df64005f4f7fe9b3e47948", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 13 | Methods Changed: 7 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 58 | Churn Cumulative: 918 | Contributors (this commit): 7 | Commits (past 90d): 18 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,6 +14,8 @@\n # ==============================================================================\n \"\"\"AdamW optimizer implementation.\"\"\"\n \n+import re\n+\n import tensorflow.compat.v2 as tf\n \n from keras.optimizers.optimizer_experimental import optimizer\n@@ -129,7 +131,7 @@ class AdamW(optimizer.Optimizer):\n                 \" must be a float value.\"\n             )\n \n-    def build(self, var_list, exclude_from_weight_decay=None):\n+    def build(self, var_list):\n         \"\"\"Initialize optimizer variables.\n \n         AdamW optimizer has 3 types of variables: momentums, velocities and\n@@ -137,15 +139,11 @@ class AdamW(optimizer.Optimizer):\n \n         Args:\n           var_list: list of model variables to build AdamW variables on.\n-          exclude_from_weight_decay: list of model variables that will be\n-            excluded from weight decay.\n         \"\"\"\n         super().build(var_list)\n         if hasattr(self, \"_built\") and self._built:\n             return\n         self._built = True\n-        if not hasattr(self, \"_exclude_from_weight_decay\"):\n-            self._exclude_from_weight_decay = exclude_from_weight_decay or []\n         self._momentums = []\n         self._velocities = []\n         for var in var_list:\n@@ -168,6 +166,20 @@ class AdamW(optimizer.Optimizer):\n                     )\n                 )\n \n+    def _use_weight_decay(self, variable):\n+        exclude_from_weight_decay = getattr(\n+            self, \"_exclude_from_weight_decay\", []\n+        )\n+        exclude_from_weight_decay_names = getattr(\n+            self, \"_exclude_from_weight_decay_names\", []\n+        )\n+        if variable in exclude_from_weight_decay:\n+            return False\n+        for name in exclude_from_weight_decay_names:\n+            if re.search(name, variable.name) is not None:\n+                return False\n+        return True\n+\n     def update_step(self, gradient, variable):\n         \"\"\"Update step given gradient and the associated model variable.\"\"\"\n         beta_1_power = None\n@@ -184,12 +196,9 @@ class AdamW(optimizer.Optimizer):\n         alpha = lr * tf.sqrt(1 - beta_2_power) / (1 - beta_1_power)\n \n         # Apply step weight decay\n-        if (\n-            self.weight_decay != 0\n-            and variable not in self._exclude_from_weight_decay\n-        ):\n+        if self._use_weight_decay(variable):\n             wd = tf.cast(self.weight_decay, variable.dtype)\n-            variable.assign_sub(variable * wd)\n+            variable.assign_sub(variable * wd * lr)\n \n         if isinstance(gradient, tf.IndexedSlices):\n             # Sparse gradients.\n@@ -238,7 +247,21 @@ class AdamW(optimizer.Optimizer):\n         )\n         return config\n \n-    def exclude_from_weight_decay(self, var_list):\n+    def exclude_from_weight_decay(self, var_list=None, var_names=None):\n+        \"\"\"Exclude variables from weight decays.\n+\n+        This method must be called before the optimizer's `build` method is\n+        called. You can set specific variables to exclude out, or set a list of\n+        strings as the anchor words, if any of which appear in a variable's\n+        name, then the variable is excluded.\n+\n+        Args:\n+            var_list: A list of `tf.Variable`s to exclude from weight decay.\n+            var_names: A list of strings. If any string in `var_names` appear\n+                in the model variable's name, then this model variable is\n+                excluded from weight decay. For example, `var_names=['bias']`\n+                excludes all bias variables from weight decay.\n+        \"\"\"\n         if hasattr(self, \"_built\") and self._built:\n             raise ValueError(\n                 \"`exclude_from_weight_decay()` can only be configued before \"\n@@ -246,6 +269,7 @@ class AdamW(optimizer.Optimizer):\n             )\n \n         self._exclude_from_weight_decay = var_list or []\n+        self._exclude_from_weight_decay_names = var_names or []\n \n \n AdamW.__doc__ = AdamW.__doc__.replace(\n\n@@ -131,18 +131,18 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n         grads, var1, var2, var3 = (\n             tf.zeros(()),\n             tf.Variable(2.0),\n-            tf.Variable(2.0),\n+            tf.Variable(2.0, name=\"exclude\"),\n             tf.Variable(2.0),\n         )\n-        optimizer_1 = adamw_new.AdamW(learning_rate=0.001, weight_decay=0.004)\n+        optimizer_1 = adamw_new.AdamW(learning_rate=1, weight_decay=0.004)\n         optimizer_1.apply_gradients(zip([grads], [var1]))\n \n-        optimizer_2 = adamw_new.AdamW(learning_rate=0.001, weight_decay=0.004)\n-        optimizer_2.exclude_from_weight_decay([var2])\n+        optimizer_2 = adamw_new.AdamW(learning_rate=1, weight_decay=0.004)\n+        optimizer_2.exclude_from_weight_decay(var_names=[\"exclude\"])\n         optimizer_2.apply_gradients(zip([grads], [var2]))\n \n-        optimizer_3 = adamw_new.AdamW(learning_rate=0.001, weight_decay=0.004)\n-        optimizer_3.build([var3], exclude_from_weight_decay=[var3])\n+        optimizer_3 = adamw_new.AdamW(learning_rate=1, weight_decay=0.004)\n+        optimizer_3.exclude_from_weight_decay(var_list=[var3])\n         optimizer_3.apply_gradients(zip([grads], [var3]))\n \n         self.assertEqual(var1, 1.992)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b084f6e10a0dacca1c2664dcfe7e310738e5c64b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 13051 | Contributors (this commit): 68 | Commits (past 90d): 9 | Contributors (cumulative): 68 | DMM Complexity: 0.0\n\nDIFF:\n@@ -38,6 +38,7 @@ import numpy as np\n from keras import backend\n from keras.utils import data_utils\n from keras.utils import image_utils\n+from keras.utils import io_utils\n \n # isort: off\n from tensorflow.python.util.tf_export import keras_export\n@@ -596,9 +597,9 @@ class DirectoryIterator(BatchFromFilesMixin, Iterator):\n             self.classes[i : i + len(classes)] = classes\n             i += len(classes)\n \n-        print(\n-            \"Found %d images belonging to %d classes.\"\n-            % (self.samples, self.num_classes)\n+        io_utils.print_msg(\n+            f\"Found {self.samples} images belonging to \"\n+            f\"{self.num_classes} classes.\"\n         )\n         pool.close()\n         pool.join()\n@@ -995,9 +996,11 @@ class DataFrameIterator(BatchFromFilesMixin, Iterator):\n             \"validated\" if validate_filenames else \"non-validated\"\n         )\n         if class_mode in [\"input\", \"multi_output\", \"raw\", None]:\n-            print(f\"Found {self.samples} {validated_string} image filenames.\")\n+            io_utils.print_msg(\n+                f\"Found {self.samples} {validated_string} image filenames.\"\n+            )\n         else:\n-            print(\n+            io_utils.print_msg(\n                 f\"Found {self.samples} {validated_string} image filenames \"\n                 f\"belonging to {num_classes} classes.\"\n             )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#12c58e15d895580d949be296403055d25eee8ec1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 38 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 38 | Churn Cumulative: 15081 | Contributors (this commit): 17 | Commits (past 90d): 21 | Contributors (cumulative): 26 | DMM Complexity: 0.9090909090909091\n\nDIFF:\n@@ -4976,6 +4976,9 @@ def rnn(\n                 tf.compat.v1.get_default_graph()\n             )\n         ):\n+            if input_length is None:\n+                max_iterations = time_steps_t\n+            else:\n                 max_iterations = tf.reduce_max(input_length)\n         else:\n             max_iterations = None\n\n@@ -1721,6 +1721,41 @@ class BackendNNOpsTest(tf.test.TestCase, parameterized.TestCase):\n                 backend.eval(last_states[0]), expected_last_state\n             )\n \n+    def test_rnn_function_jit_compile_no_unroll_input_length_none(self):\n+        num_samples = 3\n+        num_timesteps = 4\n+\n+        def step_function(inputs, states):\n+            return inputs, [s + 1 for s in states]\n+\n+        inputs_vals = np.random.random((num_samples, num_timesteps, 5))\n+        initial_state_vals = np.random.random((num_samples, 6, 7))\n+        mask_vals = np.ones((num_samples, num_timesteps))\n+        mask_vals[0, -2:] = 0  # final two timesteps masked for first sample\n+\n+        expected_last_state = initial_state_vals.copy()\n+        expected_last_state[0] += num_timesteps - 2\n+        expected_last_state[1:] += num_timesteps\n+\n+        inputs = backend.variable(inputs_vals)\n+        initial_states = [backend.variable(initial_state_vals)]\n+        mask = backend.variable(mask_vals)\n+\n+        @tf.function(jit_compile=True)\n+        def fn():\n+            _, _, last_states = backend.rnn(\n+                step_function,\n+                inputs,\n+                initial_states,\n+                mask=mask,\n+                unroll=False,\n+                input_length=None,\n+            )\n+            return last_states\n+\n+        last_states = fn()\n+        self.assertAllClose(backend.eval(last_states[0]), expected_last_state)\n+\n     def test_batch_normalization(self):\n         g_val = np.random.random((3,))\n         b_val = np.random.random((3,))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1614aa06ee19092599860af6ae646da783708859", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 6 | Churn Cumulative: 2283 | Contributors (this commit): 8 | Commits (past 90d): 10 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -469,15 +469,9 @@ class TruncatedNormal(Initializer):\n         self.mean = mean\n         self.stddev = stddev\n         self.seed = seed\n-        if tf.compat.forward_compatible(2022, 6, 24):\n-            # Use the new stateless implementation after the forward compat date\n-            # is reached.\n         self._random_generator = backend.RandomGenerator(\n             seed, rng_type=\"stateless\"\n         )\n-        else:\n-            # TODO(scottzhu): Remove this after the forward compat date expires.\n-            self._random_generator = backend.RandomGenerator(seed)\n \n     def __call__(self, shape, dtype=None, **kwargs):\n         \"\"\"Returns a tensor object initialized to random normal values (truncated).\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5591008d0f9c037156c7721d0874ee95d45a7196", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 946 | Contributors (this commit): 8 | Commits (past 90d): 10 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -115,6 +115,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n     opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n     # `loss` is a callable that takes no argument and returns the value\n     # to minimize.\n+    var1 = tf.Variable(2.0)\n+    var2 = tf.Variable(5.0)\n     loss = lambda: 3 * var1 * var1 + 2 * var2 * var2\n     # In graph mode, returns op that minimizes the loss by updating the listed\n     # variables.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#738faf7537f35fd169c7c3aa974d7251cce35adf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 8 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1791 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -228,7 +228,7 @@ def Conv2DFixedPadding(filters, kernel_size, strides, name=None):\n \n \n def STEM(\n-    bn_momentum: float = 0.0,\n+    bn_momentum: float = 0.99,\n     bn_epsilon: float = 1e-5,\n     activation: str = \"relu\",\n     name=None,\n@@ -345,7 +345,7 @@ def BottleneckBlock(\n     filters: int,\n     strides: int,\n     use_projection: bool,\n-    bn_momentum: float = 0.0,\n+    bn_momentum: float = 0.99,\n     bn_epsilon: float = 1e-5,\n     activation: str = \"relu\",\n     se_ratio: float = 0.25,\n@@ -454,7 +454,7 @@ def BlockGroup(\n     num_repeats,\n     se_ratio: float = 0.25,\n     bn_epsilon: float = 1e-5,\n-    bn_momentum: float = 0.0,\n+    bn_momentum: float = 0.99,\n     activation: str = \"relu\",\n     survival_probability: float = 0.8,\n     name=None,\n@@ -526,7 +526,7 @@ def fixed_padding(inputs, kernel_size):\n def ResNetRS(\n     depth: int,\n     input_shape=None,\n-    bn_momentum=0.0,\n+    bn_momentum=0.99,\n     bn_epsilon=1e-5,\n     activation: str = \"relu\",\n     se_ratio=0.25,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e1d39b79e8e72fa5ed6dd2d3be9960c174f36bf6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 42 | Lines Deleted: 14 | Files Changed: 3 | Hunks: 14 | Methods Changed: 16 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 56 | Churn Cumulative: 17250 | Contributors (this commit): 115 | Commits (past 90d): 39 | Contributors (cumulative): 133 | DMM Complexity: 1.0\n\nDIFF:\n@@ -228,7 +228,7 @@ def Conv2DFixedPadding(filters, kernel_size, strides, name=None):\n \n \n def STEM(\n-    bn_momentum: float = 0.99,\n+    bn_momentum: float = 0.0,\n     bn_epsilon: float = 1e-5,\n     activation: str = \"relu\",\n     name=None,\n@@ -345,7 +345,7 @@ def BottleneckBlock(\n     filters: int,\n     strides: int,\n     use_projection: bool,\n-    bn_momentum: float = 0.99,\n+    bn_momentum: float = 0.0,\n     bn_epsilon: float = 1e-5,\n     activation: str = \"relu\",\n     se_ratio: float = 0.25,\n@@ -454,7 +454,7 @@ def BlockGroup(\n     num_repeats,\n     se_ratio: float = 0.25,\n     bn_epsilon: float = 1e-5,\n-    bn_momentum: float = 0.99,\n+    bn_momentum: float = 0.0,\n     activation: str = \"relu\",\n     survival_probability: float = 0.8,\n     name=None,\n@@ -526,7 +526,7 @@ def fixed_padding(inputs, kernel_size):\n def ResNetRS(\n     depth: int,\n     input_shape=None,\n-    bn_momentum=0.99,\n+    bn_momentum=0.0,\n     bn_epsilon=1e-5,\n     activation: str = \"relu\",\n     se_ratio=0.25,\n\n@@ -1781,9 +1781,14 @@ class BackupAndRestore(Callback):\n           the callback saves the checkpoint at the end of each epoch.\n           When set to an integer, the callback saves the checkpoint every\n           `save_freq` batches.\n+        delete_checkpoint: Boolean, default to True. This `BackupAndRestore`\n+          callback works by saving a checkpoint to back up the training state.\n+          If `delete_checkpoint=True`, the checkpoint will be deleted after\n+          training is finished. Use `False` if you'd like to keep the checkpoint\n+          for future usage.\n     \"\"\"\n \n-    def __init__(self, backup_dir, save_freq=\"epoch\"):\n+    def __init__(self, backup_dir, save_freq=\"epoch\", delete_checkpoint=True):\n         super().__init__()\n         self.backup_dir = backup_dir\n         self._supports_tf_logs = True\n@@ -1794,7 +1799,8 @@ class BackupAndRestore(Callback):\n             tf.distribute.TPUStrategy,\n             tf.distribute.experimental.ParameterServerStrategy,\n         )\n-        self._save_freq = save_freq\n+        self.save_freq = save_freq\n+        self.delete_checkpoint = delete_checkpoint\n         self._batches_count = 0\n         self._current_epoch = 0\n \n@@ -1831,27 +1837,27 @@ class BackupAndRestore(Callback):\n                 \"MirroredStrategy, MultiWorkerMirroredStrategy and TPUStrategy.\"\n             )\n         self.model._training_state = worker_training_state.WorkerTrainingState(\n-            self.model, self.backup_dir, self._save_freq\n+            self.model, self.backup_dir, self.save_freq\n         )\n         self._training_state = self.model._training_state\n         self._training_state.restore()\n \n     def on_train_batch_end(self, batch, logs=None):\n-        if self._save_freq != \"epoch\":\n+        if self.save_freq != \"epoch\":\n             self._batches_count += 1\n-            if self._batches_count >= self._save_freq:\n+            if self._batches_count >= self.save_freq:\n                 self._batches_count = 0\n                 self._training_state.back_up(\n                     epoch=self._current_epoch, batch=batch\n                 )\n \n     def _implements_train_batch_hooks(self):\n-        return self._save_freq != \"epoch\"\n+        return self.save_freq != \"epoch\"\n \n     def on_train_end(self, logs=None):\n-\n-        # On exit of training, delete the training state backup file that was\n-        # saved for the purpose of worker recovery.\n+        if self.delete_checkpoint:\n+            # On exit of training, delete the training state backup file saved\n+            # for the purpose of worker recovery unless the user opts out.\n             self._training_state.delete_backup()\n         # Clean up the training state.\n         del self._training_state\n@@ -1862,7 +1868,7 @@ class BackupAndRestore(Callback):\n \n     def on_epoch_end(self, epoch, logs=None):\n         # Back up the model and current epoch for possible future recovery.\n-        if self._save_freq == \"epoch\":\n+        if self.save_freq == \"epoch\":\n             self._training_state.back_up(epoch=epoch)\n \n \n\n@@ -651,6 +651,28 @@ class KerasCallbacksTest(test_combinations.TestCase):\n         warning_msg = \"***Handling interruption at Nth step***\"\n         self.assertIn(warning_msg, \"\\n\".join(warning_messages))\n \n+    def test_backup_and_restore_steps_clean_up(self):\n+        if not tf.executing_eagerly():\n+            self.skipTest(\n+                \"BackupAndRestore only available when eager execution is \"\n+                \"enabled.\"\n+            )\n+        path = self.get_temp_dir()\n+        callback = BackupAndRestore(path, delete_checkpoint=True)\n+        model = keras.Sequential([keras.layers.Dense(10)])\n+        optimizer = gradient_descent.SGD()\n+        model.compile(optimizer, loss=\"mse\")\n+\n+        x = tf.random.uniform((24, 10))\n+        y = tf.random.uniform((24,))\n+        dataset = tf.data.Dataset.from_tensor_slices((x, y)).batch(2)\n+        model.fit(dataset, epochs=1, callbacks=[callback])\n+        self.assertEmpty(os.listdir(path))\n+\n+        callback = BackupAndRestore(path, delete_checkpoint=False)\n+        model.fit(dataset, epochs=1, callbacks=[callback])\n+        self.assertNotEmpty(os.listdir(path))\n+\n     @test_combinations.run_all_keras_modes\n     def test_callback_warning(self):\n         class SleepCallback(keras.callbacks.Callback):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#492e5b6591a9e7413478c66a038e1a30b0a59326", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 5 | Methods Changed: 6 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 17 | Churn Cumulative: 9176 | Contributors (this commit): 108 | Commits (past 90d): 17 | Contributors (cumulative): 108 | DMM Complexity: 1.0\n\nDIFF:\n@@ -24,9 +24,6 @@ import os\n import re\n import sys\n import time\n-from typing import Iterable\n-from typing import Optional\n-from typing import Union\n \n import numpy as np\n import tensorflow.compat.v2 as tf\n@@ -937,7 +934,7 @@ class BaseLogger(Callback):\n             All others will be averaged in `on_epoch_end`.\n     \"\"\"\n \n-    def __init__(self, stateful_metrics: Optional[Iterable[str]] = None):\n+    def __init__(self, stateful_metrics=None):\n         super().__init__()\n         self.stateful_metrics = set(stateful_metrics or [])\n \n@@ -1015,7 +1012,7 @@ class ProgbarLogger(Callback):\n     def __init__(\n         self,\n         count_mode: str = \"samples\",\n-        stateful_metrics: Optional[Iterable[str]] = None,\n+        stateful_metrics=None\n     ):\n         # when we drop support for python 3.7, replace 'count_mode: str'\n         # with 'count_mode: Literal[\"samples\", \"steps\"]'\n@@ -1327,17 +1324,15 @@ class ModelCheckpoint(Callback):\n \n     def __init__(\n         self,\n-        filepath: Union[str, os.PathLike],\n+        filepath,\n         monitor: str = \"val_loss\",\n         verbose: int = 0,\n         save_best_only: bool = False,\n         save_weights_only: bool = False,\n         mode: str = \"auto\",\n-        save_freq: Union[int, str] = \"epoch\",\n-        options: Union[\n-            tf.train.CheckpointOptions, tf.saved_model.SaveOptions, None\n-        ] = None,\n-        initial_value_threshold: Optional[float] = None,\n+        save_freq=\"epoch\",\n+        options=None,\n+        initial_value_threshold=None,\n         **kwargs,\n     ):\n         super().__init__()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#95601be1ce933ed43ab71877a946b2e64e394d6c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 9182 | Contributors (this commit): 108 | Commits (past 90d): 18 | Contributors (cumulative): 108 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1009,11 +1009,7 @@ class ProgbarLogger(Callback):\n         ValueError: In case of invalid `count_mode`.\n     \"\"\"\n \n-    def __init__(\n-        self,\n-        count_mode: str = \"samples\",\n-        stateful_metrics=None\n-    ):\n+    def __init__(self, count_mode: str = \"samples\", stateful_metrics=None):\n         # when we drop support for python 3.7, replace 'count_mode: str'\n         # with 'count_mode: Literal[\"samples\", \"steps\"]'\n         super().__init__()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2dae4546039cc04510d7bd0849a5e427d16f995c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 9184 | Contributors (this commit): 108 | Commits (past 90d): 19 | Contributors (cumulative): 108 | DMM Complexity: None\n\nDIFF:\n@@ -1010,8 +1010,6 @@ class ProgbarLogger(Callback):\n     \"\"\"\n \n     def __init__(self, count_mode: str = \"samples\", stateful_metrics=None):\n-        # when we drop support for python 3.7, replace 'count_mode: str'\n-        # with 'count_mode: Literal[\"samples\", \"steps\"]'\n         super().__init__()\n         self._supports_tf_logs = True\n         if count_mode == \"samples\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b7f02816b5320855ae528971766fdcaad7134a9b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 430 | Lines Deleted: 136 | Files Changed: 6 | Hunks: 56 | Methods Changed: 27 | Complexity Δ (Sum/Max): 0/3 | Churn Δ: 566 | Churn Cumulative: 30183 | Contributors (this commit): 38 | Commits (past 90d): 61 | Contributors (cumulative): 82 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5440,6 +5440,41 @@ def softsign(x):\n     return tf.math.softsign(x)\n \n \n+def _get_logits(output, from_logits, op_type, fn_name):\n+    output_ = output\n+    from_logits_ = from_logits\n+\n+    has_keras_logits = hasattr(output, \"_keras_logits\")\n+    if has_keras_logits:\n+        output_ = output._keras_logits\n+        from_logits_ = True\n+\n+    from_expected_op_type = (\n+        not isinstance(output, (tf.__internal__.EagerTensor, tf.Variable))\n+        and output.op.type == op_type\n+    ) and not has_keras_logits\n+\n+    if from_expected_op_type:\n+        # When softmax activation function is used for output operation, we\n+        # use logits from the softmax function directly to compute loss in order\n+        # to prevent collapsing zero when training.\n+        # See b/117284466\n+        assert len(output.op.inputs) == 1\n+        output_ = output.op.inputs[0]\n+        from_logits_ = True\n+\n+    if from_logits and (has_keras_logits or from_expected_op_type):\n+        warnings.warn(\n+            f'\"`{fn_name}` received `from_logits=True`, but '\n+            f\"the `output` argument was produced by a {op_type} \"\n+            \"activation and thus does not represent logits. \"\n+            \"Was this intended?\",\n+            stacklevel=2,\n+        )\n+\n+    return output_, from_logits_\n+\n+\n @keras_export(\"keras.backend.categorical_crossentropy\")\n @tf.__internal__.dispatch.add_dispatch_support\n @doc_controls.do_not_generate_docs\n@@ -5490,39 +5525,14 @@ def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n     output = tf.convert_to_tensor(output)\n     target.shape.assert_is_compatible_with(output.shape)\n \n-    # Use logits whenever they are available. `softmax` and `sigmoid`\n-    # activations cache logits on the `output` Tensor.\n-    if hasattr(output, \"_keras_logits\"):\n-        output = output._keras_logits\n-        if from_logits:\n-            warnings.warn(\n-                '\"`categorical_crossentropy` received `from_logits=True`, but '\n-                \"the `output` argument was produced by a sigmoid or softmax \"\n-                \"activation and thus does not represent logits. \"\n-                \"Was this intended?\",\n-                stacklevel=2,\n+    output, from_logits = _get_logits(\n+        output, from_logits, \"Softmax\", \"categorical_crossentropy\"\n     )\n-        from_logits = True\n-\n     if from_logits:\n         return tf.nn.softmax_cross_entropy_with_logits(\n             labels=target, logits=output, axis=axis\n         )\n \n-    if (\n-        not isinstance(output, (tf.__internal__.EagerTensor, tf.Variable))\n-        and output.op.type == \"Softmax\"\n-    ) and not hasattr(output, \"_keras_history\"):\n-        # When softmax activation function is used for output operation, we\n-        # use logits from the softmax function directly to compute loss in order\n-        # to prevent collapsing zero when training.\n-        # See b/117284466\n-        assert len(output.op.inputs) == 1\n-        output = output.op.inputs[0]\n-        return tf.nn.softmax_cross_entropy_with_logits(\n-            labels=target, logits=output, axis=axis\n-        )\n-\n     # scale preds so that the class probas of each sample sum to 1\n     output = output / tf.reduce_sum(output, axis, True)\n     # Compute cross entropy from probabilities.\n@@ -5534,7 +5544,9 @@ def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n @keras_export(\"keras.backend.sparse_categorical_crossentropy\")\n @tf.__internal__.dispatch.add_dispatch_support\n @doc_controls.do_not_generate_docs\n-def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n+def sparse_categorical_crossentropy(\n+    target, output, from_logits=False, axis=-1, ignore_index=None\n+):\n     \"\"\"Categorical crossentropy with integer targets.\n \n     Args:\n@@ -5547,6 +5559,14 @@ def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n         axis: Int specifying the channels axis. `axis=-1` corresponds to data\n             format `channels_last`, and `axis=1` corresponds to data format\n             `channels_first`.\n+        ignore_index: Optional integer, the id of a label that will not be\n+            included in the entropy equation nor in gradient computation. This\n+            is useful in segmentation problems containing the *void* label\n+            (commonly -1 or 255) in its annotated segmentation maps.\n+            By default, all label ids are considered. If `ignore_index` is not\n+            `None` and the output is a tensor with `rank>=3`, then the valid\n+            entries will be averaged over the axes `range(1, output_rank-1)`,\n+            resulting in an output of shape `[batch]`.\n \n     Returns:\n         Output tensor.\n@@ -5557,36 +5577,17 @@ def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n     target = tf.convert_to_tensor(target)\n     output = tf.convert_to_tensor(output)\n \n-    # Use logits whenever they are available. `softmax` and `sigmoid`\n-    # activations cache logits on the `output` Tensor.\n-    if hasattr(output, \"_keras_logits\"):\n-        output = output._keras_logits\n-        if from_logits:\n-            warnings.warn(\n-                '\"`sparse_categorical_crossentropy` received '\n-                \"`from_logits=True`, but the `output` argument \"\n-                \"was produced by a sigmoid or softmax activation \"\n-                'and thus does not represent logits. Was this intended?\"',\n-                stacklevel=2,\n+    target = cast(target, \"int64\")\n+\n+    output, from_logits = _get_logits(\n+        output, from_logits, \"Softmax\", \"sparse_categorical_crossentropy\"\n     )\n-        from_logits = True\n-    elif (\n-        not from_logits\n-        and not isinstance(output, (tf.__internal__.EagerTensor, tf.Variable))\n-        and output.op.type == \"Softmax\"\n-    ) and not hasattr(output, \"_keras_history\"):\n-        # When softmax activation function is used for output operation, we\n-        # use logits from the softmax function directly to compute loss in order\n-        # to prevent collapsing zero when training.\n-        # See b/117284466\n-        assert len(output.op.inputs) == 1\n-        output = output.op.inputs[0]\n-        from_logits = True\n-    elif not from_logits:\n+    if not from_logits:\n         epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)\n         output = tf.clip_by_value(output, epsilon_, 1 - epsilon_)\n         output = tf.math.log(output)\n \n+    # Permute output so that the last axis contains the logits/probabilities.\n     if isinstance(output.shape, (tuple, list)):\n         output_rank = len(output.shape)\n     else:\n@@ -5606,8 +5607,6 @@ def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n             \"on an output tensor with unknown rank\".format(axis)\n         )\n \n-    target = cast(target, \"int64\")\n-\n     # Try to adjust the shape so that rank of labels = rank of logits - 1.\n     output_shape = tf.shape(output)\n     target_rank = target.shape.ndims\n@@ -5621,6 +5620,11 @@ def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n         target = flatten(target)\n         output = tf.reshape(output, [-1, output_shape[-1]])\n \n+    if ignore_index is not None:\n+        valid_mask = tf.not_equal(target, ignore_index)\n+        target = target[valid_mask]\n+        output = output[valid_mask]\n+\n     if py_any(_is_symbolic_tensor(v) for v in [target, output]):\n         with get_graph().as_default():\n             res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n@@ -5631,11 +5635,30 @@ def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n             labels=target, logits=output\n         )\n \n+    if ignore_index is not None:\n+        res_shape = cast(output_shape[:-1], \"int64\")\n+        valid_mask = tf.reshape(valid_mask, res_shape)\n+\n+        res = tf.scatter_nd(tf.where(valid_mask), res, res_shape)\n+\n+        if output_rank is not None and output_rank >= 3:\n+            # The output is a 2-dimensional (or higher) label map,\n+            # and some pixels might be zero. We reduce the loss among the\n+            # valid entries to prevent an artificial decrease of the loss\n+            # value when many of them are invalid.\n+            reduce_axis = list(range(1, output_rank - 1))\n+            res = tf.math.divide_no_nan(\n+                tf.reduce_sum(res, axis=reduce_axis),\n+                tf.reduce_sum(cast(valid_mask, res.dtype), axis=reduce_axis),\n+            )\n+\n+        return res\n+\n     if update_shape and output_rank >= 3:\n-        # If our output includes timesteps or spatial dimensions we need to\n-        # reshape\n-        return tf.reshape(res, output_shape[:-1])\n-    else:\n+        # If our output includes timesteps or\n+        # spatial dimensions we need to reshape\n+        res = tf.reshape(res, output_shape[:-1])\n+\n     return res\n \n \n@@ -5658,38 +5681,14 @@ def binary_crossentropy(target, output, from_logits=False):\n     target = tf.convert_to_tensor(target)\n     output = tf.convert_to_tensor(output)\n \n-    # Use logits whenever they are available. `softmax` and `sigmoid`\n-    # activations cache logits on the `output` Tensor.\n-    if hasattr(output, \"_keras_logits\"):\n-        output = output._keras_logits\n-        if from_logits:\n-            warnings.warn(\n-                '\"`binary_crossentropy` received `from_logits=True`, '\n-                \"but the `output` argument was produced by a sigmoid \"\n-                \"or softmax activation and thus \"\n-                'does not represent logits. Was this intended?\"',\n-                stacklevel=2,\n+    output, from_logits = _get_logits(\n+        output, from_logits, \"Sigmoid\", \"binary_crossentropy\"\n     )\n-        from_logits = True\n-\n     if from_logits:\n         return tf.nn.sigmoid_cross_entropy_with_logits(\n             labels=target, logits=output\n         )\n \n-    if (\n-        not isinstance(output, (tf.__internal__.EagerTensor, tf.Variable))\n-        and output.op.type == \"Sigmoid\"\n-    ) and not hasattr(output, \"_keras_history\"):\n-        # When sigmoid activation function is used for output operation, we\n-        # use logits from the sigmoid function directly to compute loss in order\n-        # to prevent collapsing zero when training.\n-        assert len(output.op.inputs) == 1\n-        output = output.op.inputs[0]\n-        return tf.nn.sigmoid_cross_entropy_with_logits(\n-            labels=target, logits=output\n-        )\n-\n     epsilon_ = _constant_to_tensor(epsilon(), output.dtype.base_dtype)\n     output = tf.clip_by_value(output, epsilon_, 1.0 - epsilon_)\n \n\n@@ -1969,6 +1969,74 @@ class BackendCrossEntropyLossesTest(tf.test.TestCase, parameterized.TestCase):\n         )\n         self.assertArrayNear(self.evaluate(result)[0], [0.002, 0, 0.17], 1e-3)\n \n+    @test_combinations.generate(\n+        test_combinations.combine(mode=[\"graph\", \"eager\"])\n+    )\n+    def test_sparse_categorical_crossentropy_loss_with_ignore_index(self):\n+        t = backend.constant([255, 1, 2, 2])\n+        p = backend.softmax(\n+            backend.constant(\n+                [\n+                    [1.8, 1.2, 0.5],\n+                    [0.2, 3.8, 0.8],\n+                    [1.1, 0.4, 3.4],\n+                    [1.3, 0.7, 3.8],\n+                ]\n+            )\n+        )\n+        result = backend.sparse_categorical_crossentropy(t, p, ignore_index=255)\n+        self.assertArrayNear(\n+            self.evaluate(result),\n+            [0.0, 0.07428224, 0.13980183, 0.11967831],\n+            1e-3,\n+        )\n+\n+        t = backend.constant([-1, 1, 2, 2])\n+        p = backend.constant(\n+            [\n+                [1.8, 1.2, 0.5],\n+                [0.2, 3.8, 0.8],\n+                [1.1, 0.4, 3.4],\n+                [1.3, 0.7, 3.8],\n+            ]\n+        )\n+        result = backend.sparse_categorical_crossentropy(\n+            t, p, ignore_index=-1, from_logits=True\n+        )\n+        self.assertArrayNear(\n+            self.evaluate(result),\n+            [0.0, 0.07428224, 0.13980183, 0.11967831],\n+            1e-3,\n+        )\n+\n+    @test_combinations.generate(\n+        test_combinations.combine(mode=[\"graph\", \"eager\"])\n+    )\n+    def test_sparse_cce_loss_with_ignore_index_for_segmentation(self):\n+        t = backend.constant(\n+            [\n+                [[0, 2], [-1, -1]],\n+                [[0, 2], [-1, -1]],\n+            ]\n+        )\n+        p = backend.constant(\n+            [\n+                [\n+                    [[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]],\n+                    [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]],\n+                ],\n+                [\n+                    [[1.0, 0.0, 0.0], [0.0, 0.5, 0.5]],\n+                    [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]],\n+                ],\n+            ]\n+        )\n+\n+        result = backend.sparse_categorical_crossentropy(t, p, ignore_index=-1)\n+        self.assertArrayNear(\n+            self.evaluate(result), [2.3841855e-07, 3.4657377e-01], 1e-3\n+        )\n+\n     @test_combinations.generate(test_combinations.combine(mode=[\"graph\"]))\n     def test_sparse_categorical_crossentropy_loss_with_unknown_rank_tensor(\n         self,\n\n@@ -2024,7 +2024,9 @@ def _ragged_tensor_categorical_crossentropy(\n     \"keras.losses.sparse_categorical_crossentropy\",\n )\n @tf.__internal__.dispatch.add_dispatch_support\n-def sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1):\n+def sparse_categorical_crossentropy(\n+    y_true, y_pred, from_logits=False, axis=-1, ignore_index=None\n+):\n     \"\"\"Computes the sparse categorical crossentropy loss.\n \n     Standalone usage:\n@@ -2036,6 +2038,20 @@ def sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1):\n     >>> loss.numpy()\n     array([0.0513, 2.303], dtype=float32)\n \n+    >>> y_true = [[[ 0,  2],\n+    ...            [-1, -1]],\n+    ...           [[ 0,  2],\n+    ...            [-1, -1]]]\n+    >>> y_pred = [[[[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]],\n+                   [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]]],\n+                  [[[1.0, 0.0, 0.0], [0.0, 0.5, 0.5]],\n+                   [[0.2, 0.5, 0.3], [0.0, 1.0, 0.0]]]]\n+    >>> loss = tf.keras.losses.sparse_categorical_crossentropy(\n+    ...   y_true, y_pred, ignore_index=-1)\n+    >>> assert loss.shape == (2,)\n+    >>> loss.numpy()\n+    array([2.3841855e-07, 3.4657377e-01], dtype=float32)\n+\n     Args:\n       y_true: Ground truth values.\n       y_pred: The predicted values.\n@@ -2043,6 +2059,14 @@ def sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1):\n         default, we assume that `y_pred` encodes a probability distribution.\n       axis: Defaults to -1. The dimension along which the entropy is\n         computed.\n+      ignore_index: Optional integer, the id of a label that will not be\n+        included in the entropy equation nor in gradient computation. This\n+        is useful in segmentation problems containing the *void* label\n+        (commonly -1 or 255) in its annotated segmentation maps.\n+        By default, all label ids are considered. If `ignore_index` is not\n+        `None` and the output is a tensor with `rank>=3`, then the valid\n+        entries will be averaged over the axes `range(1, output_rank-1)`,\n+        resulting in an output of shape `[batch]`.\n \n     Returns:\n       Sparse categorical crossentropy loss value.\n@@ -2050,13 +2074,17 @@ def sparse_categorical_crossentropy(y_true, y_pred, from_logits=False, axis=-1):\n     y_pred = tf.convert_to_tensor(y_pred)\n \n     return backend.sparse_categorical_crossentropy(\n-        y_true, y_pred, from_logits=from_logits, axis=axis\n+        y_true,\n+        y_pred,\n+        from_logits=from_logits,\n+        ignore_index=ignore_index,\n+        axis=axis,\n     )\n \n \n @dispatch.dispatch_for_types(sparse_categorical_crossentropy, tf.RaggedTensor)\n def _ragged_tensor_sparse_categorical_crossentropy(\n-    y_true, y_pred, from_logits=False, axis=-1\n+    y_true, y_pred, from_logits=False, axis=-1, ignore_index=None\n ):\n     \"\"\"Implements support for handling RaggedTensors.\n \n@@ -2071,7 +2099,10 @@ def _ragged_tensor_sparse_categorical_crossentropy(\n     the sum of the individual loss values divided by 3.\n     \"\"\"\n     fn = functools.partial(\n-        sparse_categorical_crossentropy, from_logits=from_logits, axis=axis\n+        sparse_categorical_crossentropy,\n+        from_logits=from_logits,\n+        ignore_index=ignore_index,\n+        axis=axis,\n     )\n     return _ragged_tensor_apply_loss(fn, y_true, y_pred, y_pred_extra_dim=True)\n \n\n@@ -161,6 +161,34 @@ class KerasLossesTest(tf.test.TestCase, parameterized.TestCase):\n             atol=1e-5,\n         )\n \n+    @test_combinations.generate(\n+        test_combinations.combine(mode=[\"graph\", \"eager\"])\n+    )\n+    def test_sparse_categorical_crossentropy_loss_with_ignore_index(self):\n+        ignore_index = 255\n+        target = backend.variable(np.random.randint(0, 1, (5, 1)))\n+        logits = backend.variable(np.random.random((5, 1)))\n+        softmax_output = backend.softmax(logits)\n+\n+        valid_entries = tf.reshape(\n+            tf.constant([0, 1, 0, 1, 1], target.dtype), (5, 1)\n+        )\n+        target.assign(\n+            target * valid_entries + (1 - valid_entries) * ignore_index\n+        )\n+\n+        output_from_logit = losses.sparse_categorical_crossentropy(\n+            target, logits, ignore_index=ignore_index, from_logits=True\n+        )\n+        output_from_softmax = losses.sparse_categorical_crossentropy(\n+            target, softmax_output, ignore_index=ignore_index\n+        )\n+        np.testing.assert_allclose(\n+            backend.eval(output_from_logit),\n+            backend.eval(output_from_softmax),\n+            atol=1e-5,\n+        )\n+\n     @test_combinations.generate(test_combinations.combine(mode=[\"graph\"]))\n     def test_sparse_categorical_crossentropy_loss_with_unknown_rank_tensor(\n         self,\n\n@@ -18,6 +18,7 @@\n \n import abc\n from typing import List\n+from typing import Optional\n from typing import Tuple\n from typing import Union\n \n@@ -2645,11 +2646,36 @@ class _IoUBase(base_metric.Metric):\n         `(num_classes, num_classes)` will be allocated.\n       name: (Optional) string name of the metric instance.\n       dtype: (Optional) data type of the metric result.\n+      ignore_index: Optional integer, the id of a label that will not be\n+        included in the metric computation. This is useful in segmentation\n+        problems containing the *void* label (commonly -1 or 255) in its\n+        annotated segmentation maps. By default, all label ids are considered.\n+      sparse_labels: Wether labels are encoded using natural numbers or\n+        probability distribution vectors. If `False`, the `tf.argmax` function\n+        will be used to determine each sample's most likely associated label.\n+      sparse_preds: Wether predictions are encoded using natural numbers or\n+        probability distribution vectors. If `False`, the `tf.argmax` function\n+        will be used to determine each sample's most likely associated label.\n+      axis: (Optional) Defaults to -1. The dimension containing the logits.\n+\n     \"\"\"\n \n-    def __init__(self, num_classes, name=None, dtype=None):\n+    def __init__(\n+        self,\n+        num_classes: int,\n+        name: Optional[str] = None,\n+        dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n+        ignore_index: Optional[int] = None,\n+        sparse_labels: bool = True,\n+        sparse_preds: bool = True,\n+        axis: int = -1,\n+    ):\n         super().__init__(name=name, dtype=dtype)\n         self.num_classes = num_classes\n+        self.ignore_index = ignore_index\n+        self.sparse_labels = sparse_labels\n+        self.sparse_preds = sparse_preds\n+        self.axis = axis\n \n         # Variable to accumulate the predictions in the confusion matrix.\n         self.total_cm = self.add_weight(\n@@ -2672,6 +2698,11 @@ class _IoUBase(base_metric.Metric):\n           Update op.\n         \"\"\"\n \n+        if not self.sparse_labels:\n+            y_true = tf.argmax(y_true, axis=self.axis)\n+        if not self.sparse_preds:\n+            y_pred = tf.argmax(y_pred, axis=self.axis)\n+\n         y_true = tf.cast(y_true, self._dtype)\n         y_pred = tf.cast(y_pred, self._dtype)\n \n@@ -2682,6 +2713,11 @@ class _IoUBase(base_metric.Metric):\n         if y_true.shape.ndims > 1:\n             y_true = tf.reshape(y_true, [-1])\n \n+        if self.ignore_index is not None:\n+            valid_mask = tf.not_equal(y_true, self.ignore_index)\n+            y_true = y_true[valid_mask]\n+            y_pred = y_pred[valid_mask]\n+\n         if sample_weight is not None:\n             sample_weight = tf.cast(sample_weight, self._dtype)\n             if sample_weight.shape.ndims > 1:\n@@ -2738,6 +2774,17 @@ class IoU(_IoUBase):\n         single id value should be provided.\n       name: (Optional) string name of the metric instance.\n       dtype: (Optional) data type of the metric result.\n+      ignore_index: Optional integer, the id of a label that will not be\n+        included in the metric computation. This is useful in segmentation\n+        problems containing the *void* label (commonly -1 or 255) in its\n+        annotated segmentation maps. By default, all label ids are considered.\n+      sparse_labels: Wether labels are encoded using natural numbers or\n+        probability distribution vectors. If `False`, the `tf.argmax` function\n+        will be used to determine each sample's most likely associated label.\n+      sparse_preds: Wether predictions are encoded using natural numbers or\n+        probability distribution vectors. If `False`, the `tf.argmax` function\n+        will be used to determine each sample's most likely associated label.\n+      axis: (Optional) Defaults to -1. The dimension containing the logits.\n \n     Standalone usage:\n \n@@ -2777,12 +2824,20 @@ class IoU(_IoUBase):\n         self,\n         num_classes: int,\n         target_class_ids: Union[List[int], Tuple[int, ...]],\n-        name=None,\n-        dtype=None,\n+        name: Optional[str] = None,\n+        dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n+        ignore_index: Optional[int] = None,\n+        sparse_labels: bool = True,\n+        sparse_preds: bool = True,\n+        axis: int = -1,\n     ):\n         super().__init__(\n             name=name,\n             num_classes=num_classes,\n+            ignore_index=ignore_index,\n+            sparse_labels=sparse_labels,\n+            sparse_preds=sparse_preds,\n+            axis=axis,\n             dtype=dtype,\n         )\n         if max(target_class_ids) >= num_classes:\n@@ -2828,6 +2883,10 @@ class IoU(_IoUBase):\n         config = {\n             \"num_classes\": self.num_classes,\n             \"target_class_ids\": self.target_class_ids,\n+            \"ignore_index\": self.ignore_index,\n+            \"sparse_labels\": self.sparse_labels,\n+            \"sparse_preds\": self.sparse_preds,\n+            \"axis\": self.axis,\n         }\n         base_config = super().get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n@@ -2983,6 +3042,17 @@ class MeanIoU(IoU):\n         [num_classes, num_classes] will be allocated.\n       name: (Optional) string name of the metric instance.\n       dtype: (Optional) data type of the metric result.\n+      ignore_index: Optional integer, the id of a label that will not be\n+        included in the metric computation. This is useful in segmentation\n+        problems containing the *void* label (commonly -1 or 255) in its\n+        annotated segmentation maps. By default, all label ids are considered.\n+      sparse_labels: Wether labels are encoded using natural numbers or\n+        probability distribution vectors. If `False`, the `tf.argmax` function\n+        will be used to determine each sample's most likely associated label.\n+      sparse_preds: Wether predictions are encoded using natural numbers or\n+        probability distribution vectors. If `False`, the `tf.argmax` function\n+        will be used to determine each sample's most likely associated label.\n+      axis: (Optional) Defaults to -1. The dimension containing the logits.\n \n     Standalone usage:\n \n@@ -3013,13 +3083,26 @@ class MeanIoU(IoU):\n     \"\"\"\n \n     @dtensor_utils.inject_mesh\n-    def __init__(self, num_classes, name=None, dtype=None):\n+    def __init__(\n+        self,\n+        num_classes: int,\n+        name: Optional[str] = None,\n+        dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n+        ignore_index: Optional[int] = None,\n+        sparse_labels: bool = True,\n+        sparse_preds: bool = True,\n+        axis: int = -1,\n+    ):\n         target_class_ids = list(range(num_classes))\n         super().__init__(\n             name=name,\n             num_classes=num_classes,\n             target_class_ids=target_class_ids,\n+            axis=axis,\n             dtype=dtype,\n+            ignore_index=ignore_index,\n+            sparse_labels=sparse_labels,\n+            sparse_preds=sparse_preds,\n         )\n \n     def get_config(self):\n@@ -3027,6 +3110,10 @@ class MeanIoU(IoU):\n             \"num_classes\": self.num_classes,\n             \"name\": self.name,\n             \"dtype\": self._dtype,\n+            \"ignore_index\": self.ignore_index,\n+            \"sparse_labels\": self.sparse_labels,\n+            \"sparse_preds\": self.sparse_preds,\n+            \"axis\": self.axis,\n         }\n \n \n@@ -3074,6 +3161,14 @@ class OneHotIoU(IoU):\n         single id value should be provided.\n       name: (Optional) string name of the metric instance.\n       dtype: (Optional) data type of the metric result.\n+      ignore_index: Optional integer, the id of a label that will not be\n+        included in the metric computation. This is useful in segmentation\n+        problems containing the *void* label (commonly -1 or 255) in its\n+        annotated segmentation maps. By default, all label ids are considered.\n+      sparse_preds: Wether predictions are encoded using natural numbers or\n+        probability distribution vectors. If `False`, the `tf.argmax` function\n+        will be used to determine each sample's most likely associated label.\n+      axis: (Optional) Defaults to -1. The dimension containing the logits.\n \n     Standalone usage:\n \n@@ -3111,32 +3206,31 @@ class OneHotIoU(IoU):\n         target_class_ids: Union[List[int], Tuple[int, ...]],\n         name=None,\n         dtype=None,\n+        ignore_index: Optional[int] = None,\n+        sparse_preds: bool = False,\n+        axis: int = -1,\n     ):\n         super().__init__(\n             num_classes=num_classes,\n             target_class_ids=target_class_ids,\n             name=name,\n             dtype=dtype,\n+            ignore_index=ignore_index,\n+            sparse_labels=False,\n+            sparse_preds=sparse_preds,\n+            axis=axis,\n         )\n \n-    def update_state(self, y_true, y_pred, sample_weight=None):\n-        \"\"\"Accumulates the confusion matrix statistics.\n-\n-        Args:\n-          y_true: The ground truth values.\n-          y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can\n-            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n-            and must be broadcastable to `y_true`.\n-\n-        Returns:\n-          Update op.\n-        \"\"\"\n-        # Select max hot-encoding channels to convert into all-class format\n-        y_true = tf.argmax(y_true, axis=-1, output_type=tf.int32)\n-        y_pred = tf.argmax(y_pred, axis=-1, output_type=tf.int32)\n-\n-        return super().update_state(y_true, y_pred, sample_weight)\n+    def get_config(self):\n+        return {\n+            \"num_classes\": self.num_classes,\n+            \"target_class_ids\": self.target_class_ids,\n+            \"name\": self.name,\n+            \"dtype\": self._dtype,\n+            \"ignore_index\": self.ignore_index,\n+            \"sparse_preds\": self.sparse_preds,\n+            \"axis\": self.axis,\n+        }\n \n \n @keras_export(\"keras.metrics.OneHotMeanIoU\")\n@@ -3181,6 +3275,14 @@ class OneHotMeanIoU(MeanIoU):\n         allocated to accumulate predictions from which the metric is calculated.\n       name: (Optional) string name of the metric instance.\n       dtype: (Optional) data type of the metric result.\n+      ignore_index: Optional integer, the id of a label that will not be\n+        included in the metric computation. This is useful in segmentation\n+        problems containing the *void* label (commonly -1 or 255) in its\n+        annotated segmentation maps. By default, all label ids are considered.\n+      sparse_preds: Wether predictions are encoded using natural numbers or\n+        probability distribution vectors. If `False`, the `tf.argmax` function\n+        will be used to determine each sample's most likely associated label.\n+      axis: (Optional) Defaults to -1. The dimension containing the logits.\n \n     Standalone usage:\n \n@@ -3215,33 +3317,31 @@ class OneHotMeanIoU(MeanIoU):\n     def __init__(\n         self,\n         num_classes: int,\n-        name=None,\n-        dtype=None,\n+        name: str = None,\n+        dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n+        ignore_index: Optional[int] = None,\n+        sparse_preds: bool = False,\n+        axis: int = -1,\n     ):\n         super().__init__(\n             num_classes=num_classes,\n+            axis=axis,\n             name=name,\n             dtype=dtype,\n+            ignore_index=ignore_index,\n+            sparse_labels=False,\n+            sparse_preds=sparse_preds,\n         )\n \n-    def update_state(self, y_true, y_pred, sample_weight=None):\n-        \"\"\"Accumulates the confusion matrix statistics.\n-\n-        Args:\n-          y_true: The ground truth values.\n-          y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can\n-            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n-            and must be broadcastable to `y_true`.\n-\n-        Returns:\n-          Update op.\n-        \"\"\"\n-        # Select max hot-encoding channels to convert into all-class format\n-        y_true = tf.argmax(y_true, axis=-1, output_type=tf.int32)\n-        y_pred = tf.argmax(y_pred, axis=-1, output_type=tf.int32)\n-\n-        return super().update_state(y_true, y_pred, sample_weight)\n+    def get_config(self):\n+        return {\n+            \"num_classes\": self.num_classes,\n+            \"name\": self.name,\n+            \"dtype\": self._dtype,\n+            \"ignore_index\": self.ignore_index,\n+            \"sparse_preds\": self.sparse_preds,\n+            \"axis\": self.axis,\n+        }\n \n \n @keras_export(\"keras.metrics.BinaryCrossentropy\")\n@@ -3319,6 +3419,8 @@ class CategoricalCrossentropy(base_metric.MeanMetricWrapper):\n         smoothed, meaning the confidence on label values are relaxed. e.g.\n         `label_smoothing=0.2` means that we will use a value of `0.1` for label\n         `0` and `0.9` for label `1`\"\n+      axis: (Optional) Defaults to -1. The dimension along which entropy is\n+        computed.\n \n     Standalone usage:\n \n@@ -3359,6 +3461,7 @@ class CategoricalCrossentropy(base_metric.MeanMetricWrapper):\n         dtype=None,\n         from_logits=False,\n         label_smoothing=0,\n+        axis=-1,\n     ):\n         super().__init__(\n             categorical_crossentropy,\n@@ -3366,6 +3469,7 @@ class CategoricalCrossentropy(base_metric.MeanMetricWrapper):\n             dtype=dtype,\n             from_logits=from_logits,\n             label_smoothing=label_smoothing,\n+            axis=axis,\n         )\n \n \n@@ -3389,7 +3493,11 @@ class SparseCategoricalCrossentropy(base_metric.MeanMetricWrapper):\n       dtype: (Optional) data type of the metric result.\n       from_logits: (Optional) Whether output is expected to be a logits tensor.\n         By default, we consider that output encodes a probability distribution.\n-      axis: (Optional) Defaults to -1. The dimension along which the metric is\n+      ignore_index: Optional integer, the id of a label that will not be\n+        included in the metric computation. This is useful in segmentation\n+        problems containing the *void* label (commonly -1 or 255) in its\n+        annotated segmentation maps. By default, all label ids are considered.\n+      axis: (Optional) Defaults to -1. The dimension along which entropy is\n         computed.\n \n     Standalone usage:\n@@ -3430,16 +3538,18 @@ class SparseCategoricalCrossentropy(base_metric.MeanMetricWrapper):\n     @dtensor_utils.inject_mesh\n     def __init__(\n         self,\n-        name=\"sparse_categorical_crossentropy\",\n-        dtype=None,\n-        from_logits=False,\n-        axis=-1,\n+        name: str = \"sparse_categorical_crossentropy\",\n+        dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n+        from_logits: bool = False,\n+        ignore_index: Optional[int] = None,\n+        axis: int = -1,\n     ):\n         super().__init__(\n             sparse_categorical_crossentropy,\n             name,\n             dtype=dtype,\n             from_logits=from_logits,\n+            ignore_index=ignore_index,\n             axis=axis,\n         )\n \n\n@@ -1282,6 +1282,44 @@ class MeanIoUTest(tf.test.TestCase):\n         expected_result = (1 / (2 + 2 - 1) + 1 / (2 + 2 - 1)) / 2\n         self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n \n+    def test_unweighted_ignore_index_255(self):\n+        y_pred = [0, 1, 1, 1]\n+        y_true = [0, 1, 2, 255]\n+\n+        m_obj = metrics.MeanIoU(num_classes=3, ignore_index=255)\n+        self.evaluate(tf.compat.v1.variables_initializer(m_obj.variables))\n+\n+        result = m_obj(y_true, y_pred)\n+\n+        # cm = [[1, 0, 0],\n+        #       [0, 1, 0],\n+        #       [0, 1, 0]]\n+        # sum_row = [1, 1, 1], sum_col = [1, 2, 0], true_positives = [1, 1, 0]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (\n+            1 / (1 + 1 - 1) + 1 / (2 + 1 - 1) + 0 / (0 + 1 - 0)\n+        ) / 3\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+    def test_unweighted_ignore_index_1(self):\n+        y_pred = [0, 1, 1, 1]\n+        y_true = [0, 1, 2, -1]\n+\n+        m_obj = metrics.MeanIoU(num_classes=3, ignore_index=-1)\n+        self.evaluate(tf.compat.v1.variables_initializer(m_obj.variables))\n+\n+        result = m_obj(y_true, y_pred)\n+\n+        # cm = [[1, 0, 0],\n+        #       [0, 1, 0],\n+        #       [0, 1, 0]]\n+        # sum_row = [1, 1, 1], sum_col = [1, 2, 0], true_positives = [1, 1, 0]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (\n+            1 / (1 + 1 - 1) + 1 / (2 + 1 - 1) + 0 / (0 + 1 - 0)\n+        ) / 3\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n     def test_weighted(self):\n         y_pred = tf.constant([0, 1, 0, 1], dtype=tf.float32)\n         y_true = tf.constant([0, 0, 1, 1])\n@@ -1302,6 +1340,26 @@ class MeanIoUTest(tf.test.TestCase):\n         ) / 2\n         self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n \n+    def test_weighted_ignore_index_1(self):\n+        y_pred = tf.constant([0, 1, 0, 1], dtype=tf.float32)\n+        y_true = tf.constant([0, 0, 1, -1])\n+        sample_weight = tf.constant([0.2, 0.3, 0.4, 0.1])\n+\n+        m_obj = metrics.MeanIoU(num_classes=2, ignore_index=-1)\n+        self.evaluate(tf.compat.v1.variables_initializer(m_obj.variables))\n+\n+        result = m_obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+        # cm = [[0.2, 0.3],\n+        #       [0.4, 0.0]]\n+        # sum_row = [0.6, 0.3], sum_col = [0.5, 0.4], true_positives = [0.2,\n+        # 0.0]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (\n+            0.2 / (0.6 + 0.5 - 0.2) + 0.0 / (0.3 + 0.4 - 0.0)\n+        ) / 2\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n     def test_multi_dim_input(self):\n         y_pred = tf.constant([[0, 1], [0, 1]], dtype=tf.float32)\n         y_true = tf.constant([[0, 0], [1, 1]])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
