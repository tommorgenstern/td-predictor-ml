{"custom_id": "keras#2b00f0f6d7c434d0ba839c75da70312f687acf42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 300 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -22,7 +22,9 @@ from tensorflow.python.util.tf_export import keras_export\n from tensorflow.python.util.tf_export import tf_export\n \n \n-@keras_export('keras.layers.InputSpec')\n+@keras_export('keras.layers.InputSpec',\n+              v1=['keras.layers.InputSpec',\n+                  'keras.__internal__.legacy.layers.InputSpec'])\n @tf_export(v1=['layers.InputSpec'])\n class InputSpec(object):\n   \"\"\"Specifies the rank, dtype and shape of every input to a layer.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1d12a9287cbe3f2d4cdde84918f0ef2086da9bee", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 12 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 1384 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -12,6 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n+# pylint: disable=g-classes-have-attributes\n \"\"\"Module implementing RNN Cells.\n \n This module provides a number of basic commonly used RNN cells, such as LSTM\n@@ -37,6 +38,7 @@ from keras.layers.legacy_rnn import rnn_cell_wrapper_impl\n from keras.legacy_tf_layers import base as base_layer\n from keras.utils import tf_utils\n from tensorflow.python.platform import tf_logging as logging\n+from tensorflow.python.util.tf_export import keras_export\n from tensorflow.python.util.tf_export import tf_export\n \n _BIAS_VARIABLE_NAME = \"bias\"\n@@ -166,6 +168,7 @@ def _zero_state_tensors(state_size, batch_size, dtype):\n   return tf.nest.map_structure(get_state_shape, state_size)\n \n \n+@keras_export(v1=[\"keras.__internal__.legacy.rnn_cell.RNNCell\"])\n @tf_export(v1=[\"nn.rnn_cell.RNNCell\"])\n class RNNCell(base_layer.Layer):\n   \"\"\"Abstract object representing an RNN cell.\n@@ -379,6 +382,7 @@ class LayerRNNCell(RNNCell):\n         self, inputs, state, scope=scope, *args, **kwargs)\n \n \n+@keras_export(v1=[\"keras.__internal__.legacy.rnn_cell.BasicRNNCell\"])\n @tf_export(v1=[\"nn.rnn_cell.BasicRNNCell\"])\n class BasicRNNCell(LayerRNNCell):\n   \"\"\"The most basic RNN cell.\n@@ -475,6 +479,7 @@ class BasicRNNCell(LayerRNNCell):\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n+@keras_export(v1=[\"keras.__internal__.legacy.rnn_cell.GRUCell\"])\n @tf_export(v1=[\"nn.rnn_cell.GRUCell\"])\n class GRUCell(LayerRNNCell):\n   \"\"\"Gated Recurrent Unit cell.\n@@ -614,6 +619,7 @@ class GRUCell(LayerRNNCell):\n _LSTMStateTuple = collections.namedtuple(\"LSTMStateTuple\", (\"c\", \"h\"))\n \n \n+@keras_export(v1=[\"keras.__internal__.legacy.rnn_cell.LSTMStateTuple\"])\n @tf_export(v1=[\"nn.rnn_cell.LSTMStateTuple\"])\n class LSTMStateTuple(_LSTMStateTuple):\n   \"\"\"Tuple used by LSTM Cells for `state_size`, `zero_state`, and output state.\n@@ -634,6 +640,7 @@ class LSTMStateTuple(_LSTMStateTuple):\n     return c.dtype\n \n \n+@keras_export(v1=[\"keras.__internal__.legacy.rnn_cell.BasicLSTMCell\"])\n @tf_export(v1=[\"nn.rnn_cell.BasicLSTMCell\"])\n class BasicLSTMCell(LayerRNNCell):\n   \"\"\"DEPRECATED: Please use `tf.compat.v1.nn.rnn_cell.LSTMCell` instead.\n@@ -804,6 +811,7 @@ class BasicLSTMCell(LayerRNNCell):\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n+@keras_export(v1=[\"keras.__internal__.legacy.rnn_cell.LSTMCell\"])\n @tf_export(v1=[\"nn.rnn_cell.LSTMCell\"])\n class LSTMCell(LayerRNNCell):\n   \"\"\"Long short-term memory unit (LSTM) recurrent network cell.\n@@ -1171,6 +1179,7 @@ class _RNNCellWrapperV1(RNNCell):\n                        \"instance.\")\n \n \n+@keras_export(v1=[\"keras.__internal__.legacy.rnn_cell.DropoutWrapper\"])\n @tf_export(v1=[\"nn.rnn_cell.DropoutWrapper\"])\n class DropoutWrapper(rnn_cell_wrapper_impl.DropoutWrapperBase,\n                      _RNNCellWrapperV1):\n@@ -1182,6 +1191,7 @@ class DropoutWrapper(rnn_cell_wrapper_impl.DropoutWrapperBase,\n   __init__.__doc__ = rnn_cell_wrapper_impl.DropoutWrapperBase.__init__.__doc__\n \n \n+@keras_export(v1=[\"keras.__internal__.legacy.rnn_cell.ResidualWrapper\"])\n @tf_export(v1=[\"nn.rnn_cell.ResidualWrapper\"])\n class ResidualWrapper(rnn_cell_wrapper_impl.ResidualWrapperBase,\n                       _RNNCellWrapperV1):\n@@ -1193,6 +1203,7 @@ class ResidualWrapper(rnn_cell_wrapper_impl.ResidualWrapperBase,\n   __init__.__doc__ = rnn_cell_wrapper_impl.ResidualWrapperBase.__init__.__doc__\n \n \n+@keras_export(v1=[\"keras.__internal__.legacy.rnn_cell.DeviceWrapper\"])\n @tf_export(v1=[\"nn.rnn_cell.DeviceWrapper\"])\n class DeviceWrapper(rnn_cell_wrapper_impl.DeviceWrapperBase,\n                     _RNNCellWrapperV1):\n@@ -1203,6 +1214,7 @@ class DeviceWrapper(rnn_cell_wrapper_impl.DeviceWrapperBase,\n   __init__.__doc__ = rnn_cell_wrapper_impl.DeviceWrapperBase.__init__.__doc__\n \n \n+@keras_export(v1=[\"keras.__internal__.legacy.rnn_cell.MultiRNNCell\"])\n @tf_export(v1=[\"nn.rnn_cell.MultiRNNCell\"])\n class MultiRNNCell(RNNCell):\n   \"\"\"RNN cell composed sequentially of multiple simple cells.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#953d0f4749429db3eec162fc4f6db72d4d977b42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1922 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -30,6 +30,7 @@ from keras import testing_utils\n from keras.engine import base_layer_utils\n from keras.layers import recurrent as rnn_v1\n from keras.layers import recurrent_v2 as rnn_v2\n+from keras.layers.legacy_rnn import rnn_cell_impl\n from keras.utils import generic_utils\n from tensorflow.python.training.tracking import util as trackable_util\n \n@@ -1276,7 +1277,7 @@ class RNNTest(keras_parameterized.TestCase):\n         recurrent_activation='sigmoid',\n         implementation=2)\n     tf_lstm_cell_output = _run_cell(\n-        tf.compat.v1.nn.rnn_cell.LSTMCell,\n+        rnn_cell_impl.LSTMCell,\n         use_peepholes=True,\n         initializer=tf.compat.v1.ones_initializer)\n     self.assertNotAllClose(first_implementation_output, no_peephole_output)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c83a18431259c1490e246798ae49dc22de39bd0e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 273 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -28,7 +28,7 @@ from tensorflow.core.example import feature_pb2\n from tensorflow.python.framework import test_util\n from keras.feature_column import dense_features\n from keras.feature_column import sequence_feature_column as ksfc\n-from keras.layers import core\n+from keras import backend\n from keras.layers import merge\n from keras.layers import recurrent\n \n@@ -92,7 +92,7 @@ class SequenceFeatureColumnIntegrationTest(tf.test.TestCase):\n     seq_input, _ = sequence_input_layer(features)\n     dense_input_layer = dense_features.DenseFeatures(ctx_cols)\n     ctx_input = dense_input_layer(features)\n-    ctx_input = core.RepeatVector(tf.compat.v1.shape(seq_input)[1])(ctx_input)\n+    ctx_input = backend.repeat(ctx_input, tf.compat.v1.shape(seq_input)[1])\n     concatenated_input = merge.concatenate([seq_input, ctx_input])\n \n     rnn_layer = recurrent.RNN(recurrent.SimpleRNNCell(10))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ab161eeb25ce45d61437b4d8c0fcf3e041a95311", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 7057 | Contributors (this commit): 90 | Commits (past 90d): 19 | Contributors (cumulative): 90 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1435,7 +1435,11 @@ class ModelCheckpoint(Callback):\n             self.model.save(filepath, overwrite=True, options=self._options)\n \n         self._maybe_remove_file()\n-      except IOError as e:\n+      except IsADirectoryError as e:  # h5py 3.x\n+        raise IOError('Please specify a non-directory filepath for '\n+                      'ModelCheckpoint. Filepath used is an existing '\n+                      'directory: {}'.format(filepath))\n+      except IOError as e:  # h5py 2.x\n         # `e.errno` appears to be `None` so checking the content of `e.args[0]`.\n         if 'is a directory' in str(e.args[0]).lower():\n           raise IOError('Please specify a non-directory filepath for '\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d16ddab6c6ee90d3931dac8c12a57500cb4da2db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 188 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -18,7 +18,6 @@\n import tensorflow.compat.v2 as tf\n \n import os\n-from absl import logging\n from absl.testing import parameterized\n import numpy as np\n import keras\n@@ -29,6 +28,7 @@ from keras.layers import core as core_layers\n from keras.layers.preprocessing import string_lookup\n from keras.optimizer_v2 import gradient_descent\n from keras.utils import dataset_creator\n+from tensorflow.python.platform import tf_logging as logging\n \n \n class DatasetCreatorModelFitTestBase(tf.test.TestCase, parameterized.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4a978914d2298db2c79baa4012af5ceff4a4e203", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 707 | Lines Deleted: 582 | Files Changed: 30 | Hunks: 117 | Methods Changed: 119 | Complexity Δ (Sum/Max): 112/62 | Churn Δ: 1289 | Churn Cumulative: 94101 | Contributors (this commit): 213 | Commits (past 90d): 213 | Contributors (cumulative): 368 | DMM Complexity: 0.9615384615384616\n\nDIFF:\n@@ -191,7 +191,7 @@ def selu(x):\n         `tf.keras.layers.AlphaDropout` (not regular dropout).\n \n   References:\n-      - [backendlambauer et al., 2017](https://arxiv.org/abs/1706.02515)\n+      - [Klambauer et al., 2017](https://arxiv.org/abs/1706.02515)\n   \"\"\"\n   return tf.nn.selu(x)\n \n\n@@ -5968,7 +5968,7 @@ def bias_add(x, bias, data_format=None):\n   if len(bias_shape) != 1 and len(bias_shape) != ndim(x) - 1:\n     raise ValueError(\n         'Unexpected bias dimensions %d, expect to be 1 or %d dimensions' %\n-        (len(bias_shape), ndim(x)))\n+        (len(bias_shape), ndim(x) - 1))\n \n   if len(bias_shape) == 1:\n     if data_format == 'channels_first':\n\n@@ -94,7 +94,7 @@ class KerasMetricsTest(tf.test.TestCase, parameterized.TestCase):\n   @tf.__internal__.distribute.combinations.generate(\n       tf.__internal__.test.combinations.combine(\n           distribution=strategy_combinations.all_strategies, mode=[\"eager\"]))\n-  @test_util.disable_mlir_bridge(\"TODO(b/68036682): Support dynamic padder\")\n+  @test_util.disable_mlir_bridge(\"TODO(b/168036682): Support dynamic padder\")\n   def test_update_keras_metrics_dynamic_shape(self, distribution):\n     with distribution.scope():\n       metric = metrics.Mean(\"test_metric\", dtype=np.float32)\n\n@@ -18,10 +18,13 @@\n import tensorflow.compat.v2 as tf\n from keras import callbacks as callbacks_lib\n from keras.distribute import dataset_creator_model_fit_test_base as test_base\n+from keras.distribute import strategy_combinations\n \n \n @tf.__internal__.distribute.combinations.generate(\n-    tf.__internal__.test.combinations.combine(strategy=[\"ParameterServerStrategy\"], mode=\"eager\"))\n+    tf.__internal__.test.combinations.combine(\n+        strategy=strategy_combinations.parameter_server_strategies_multi_worker,\n+        mode=\"eager\"))\n class DatasetCreatorModelFitParameterServerStrategyOnlyTest(\n     test_base.DatasetCreatorModelFitTestBase):\n \n@@ -101,6 +104,29 @@ class DatasetCreatorModelFitParameterServerStrategyOnlyTest(\n     files = tf.compat.v1.gfile.ListDirectory(log_dir)\n     self.assertGreaterEqual(len(files), 1)\n \n+  def testModelEvaluateWithDatasetInstance(self, strategy):\n+    with self.assertRaisesRegex(\n+        NotImplementedError,\n+        \"Only `tf.keras.utils.experimental.DatasetCreator` input is supported \"\n+        \"with `ParameterServerStrategy` at this time. Please see \"\n+        \"`tf.keras.utils.experimental.DatasetCreator` class docstring for more \"\n+        \"information.\"):\n+      self._model_evaluate(\n+          strategy,\n+          validation_data=tf.data.Dataset.from_tensor_slices([1, 1]))\n+\n+  def testModelEvaluateErrorOnBatchLevelCallbacks(self, strategy):\n+\n+    class BatchLevelCallback(callbacks_lib.Callback):\n+\n+      def on_train_batch_end(self, batch, logs=None):\n+        pass\n+\n+    with self.assertRaisesRegex(ValueError,\n+                                \"Batch-level `Callback`s are not supported\"):\n+      callbacks = [BatchLevelCallback()]\n+      self._model_evaluate(strategy, callbacks=callbacks)\n+\n \n if __name__ == \"__main__\":\n   tf.compat.v1.enable_v2_behavior()\n\n@@ -16,19 +16,27 @@\n \"\"\"Tests for `DatasetCreator` with `Model.fit` across usages and strategies.\"\"\"\n \n import tensorflow.compat.v2 as tf\n-from keras import callbacks as callbacks_lib\n+from tensorflow.python.framework import test_util\n from keras.distribute import dataset_creator_model_fit_test_base as test_base\n from keras.distribute import strategy_combinations\n \n \n+# TODO(rchao): Investigate why there cannot be single worker and multi worker\n+# PS strategies running in the same shard.\n @tf.__internal__.distribute.combinations.generate(\n     tf.__internal__.test.combinations.combine(\n         strategy=strategy_combinations.all_strategies +\n         strategy_combinations.multi_worker_mirrored_strategies +\n-        [\"ParameterServerStrategy\"],\n+        strategy_combinations.parameter_server_strategies_multi_worker,\n         mode=\"eager\"))\n class DatasetCreatorModelFitTest(test_base.DatasetCreatorModelFitTestBase):\n \n+  def setUp(self):\n+    super().setUp()\n+    if test_util.is_xla_enabled():\n+      self.skipTest(\"model.optimizer.iterations values is not as expected \"\n+                    \"with XLA: b/184384487\")\n+\n   def testModelFit(self, strategy):\n     model = self._model_fit(strategy)\n     self.assertEqual(model.optimizer.iterations, 100)\n@@ -53,12 +61,6 @@ class DatasetCreatorModelFitTest(test_base.DatasetCreatorModelFitTestBase):\n         \"`Model.fit` or `Model.evaluate`.\"):\n       self._model_fit(strategy, steps_per_epoch=None)\n \n-\n-@tf.__internal__.distribute.combinations.generate(\n-    tf.__internal__.test.combinations.combine(strategy=[\"ParameterServerStrategy\"], mode=\"eager\"))\n-class DatasetCreatorModelEvaluateParameterServerStrategyOnlyTest(\n-    test_base.DatasetCreatorModelFitTestBase):\n-\n   def testModelEvaluate(self, strategy):\n     self._model_evaluate(strategy)\n     self.assertGreaterEqual(self._metric.result(), 0.0)\n@@ -79,30 +81,6 @@ class DatasetCreatorModelEvaluateParameterServerStrategyOnlyTest(\n         \"`Model.fit` or `Model.evaluate`.\"):\n       self._model_evaluate(strategy, steps=None)\n \n-  def testModelEvaluateWithDatasetInstance(self, strategy):\n-    with self.assertRaisesRegex(\n-        NotImplementedError,\n-        \"Only `tf.keras.utils.experimental.DatasetCreator` input is supported \"\n-        \"with `ParameterServerStrategy` at this time. Please see \"\n-        \"`tf.keras.utils.experimental.DatasetCreator` class docstring for more \"\n-        \"information.\"\n-    ):\n-      self._model_evaluate(\n-          strategy,\n-          validation_data=tf.data.Dataset.from_tensor_slices([1, 1]))\n-\n-  def testModelFitErrorOnBatchLevelCallbacks(self, strategy):\n-\n-    class BatchLevelCallback(callbacks_lib.Callback):\n-\n-      def on_train_batch_end(self, batch, logs=None):\n-        pass\n-\n-    with self.assertRaisesRegex(ValueError,\n-                                \"Batch-level `Callback`s are not supported\"):\n-      callbacks = [BatchLevelCallback()]\n-      self._model_evaluate(strategy, callbacks=callbacks)\n-\n \n if __name__ == \"__main__\":\n   tf.compat.v1.enable_v2_behavior()\n\n@@ -22,7 +22,6 @@ from absl.testing import parameterized\n import numpy as np\n import keras\n from keras import callbacks as callbacks_lib\n-from keras.distribute import multi_worker_testing_utils\n from keras.engine import sequential\n from keras.layers import core as core_layers\n from keras.layers.preprocessing import string_lookup\n@@ -93,17 +92,6 @@ class DatasetCreatorModelFitTestBase(tf.test.TestCase, parameterized.TestCase):\n           raise RuntimeError(\"Unexpected last epoch: {}\".format(\n               self._prev_epoch))\n \n-    # TODO(b/182193218): Use ParameterServerStrategy as a proper strategy\n-    # combination.\n-    if strategy == \"ParameterServerStrategy\":\n-      gpu_devices = tf.config.list_physical_devices(\"GPU\")\n-      if len(gpu_devices) > 1:\n-        self.skipTest(\"b/178452835: Multi-GPUs not supported in \"\n-                      \"ParameterServerStrategy.\")\n-      strategy = tf.distribute.experimental.ParameterServerStrategy(\n-          multi_worker_testing_utils.make_parameter_server_cluster(3, 2),\n-          variable_partitioner=tf.distribute.experimental.partitioners.FixedShardsPartitioner(2))\n-\n     with strategy.scope():\n       model = sequential.Sequential([core_layers.Dense(10)])\n       if with_normalization_layer:\n\n@@ -240,16 +240,8 @@ class KPLCreatedInDatasetsFromFunctionTest(tf.test.TestCase,\n         return tf.data.Dataset.from_tensor_slices(\n             (x, y)).shuffle(10).repeat().batch(2).map(map_fn)\n \n-      def wrapped_dataset_fn(input_context):\n-        # TODO(b/186692679): Currently we need to remove the device scope\n-        # imposed in `distribute_datasets_from_function` lib so the\n-        # `StaticHashTable` is placed on the coordinator. Remove this workaround\n-        # once resolved.\n-        with tf.device(None):\n-          return dataset_fn(input_context)\n-\n       return self.coordinator.strategy.distribute_datasets_from_function(\n-          wrapped_dataset_fn)\n+          dataset_fn)\n \n     per_worker_distribute_dataset = self.coordinator.create_per_worker_dataset(\n         per_worker_dataset_fn)\n\n@@ -61,4 +61,14 @@ tpu_strategies = [\n     tf.__internal__.distribute.combinations.tpu_strategy,\n ]\n \n+parameter_server_strategies_single_worker = [\n+    tf.__internal__.distribute.combinations.parameter_server_strategy_1worker_2ps_cpu,\n+    tf.__internal__.distribute.combinations.parameter_server_strategy_1worker_2ps_1gpu,\n+]\n+\n+parameter_server_strategies_multi_worker = [\n+    tf.__internal__.distribute.combinations.parameter_server_strategy_3worker_2ps_cpu,\n+    tf.__internal__.distribute.combinations.parameter_server_strategy_3worker_2ps_1gpu,\n+]\n+\n all_strategies = strategies_minus_tpu + tpu_strategies\n\n@@ -459,9 +459,35 @@ class Layer(tf.Module, version_utils.LayerVersionSelector):\n     method to support masking.\n \n     Args:\n-        inputs: Input tensor, or list/tuple of input tensors.\n-        *args: Additional positional arguments. Currently unused.\n-        **kwargs: Additional keyword arguments. Currently unused.\n+      inputs: Input tensor, or dict/list/tuple of input tensors.\n+        The first positional `inputs` argument is subject to special rules:\n+        - `inputs` must be explicitly passed. A layer cannot have zero\n+          arguments, and `inputs` cannot be provided via the default value\n+          of a keyword argument.\n+        - NumPy array or Python scalar values in `inputs` get cast as tensors.\n+        - Keras mask metadata is only collected from `inputs`.\n+        - Layers are built (`build(input_shape)` method)\n+          using shape info from `inputs` only.\n+        - `input_spec` compatibility is only checked against `inputs`.\n+        - Mixed precision input casting is only applied to `inputs`.\n+          If a layer has tensor arguments in `*args` or `**kwargs`, their\n+          casting behavior in mixed precision should be handled manually.\n+        - The SavedModel input specification is generated using `inputs` only.\n+        - Integration with various ecosystem packages like TFMOT, TFLite,\n+          TF.js, etc is only supported for `inputs` and not for tensors in\n+          positional and keyword arguments.\n+      *args: Additional positional arguments. May contain tensors, although\n+        this is not recommended, for the reasons above.\n+      **kwargs: Additional keyword arguments. May contain tensors, although\n+        this is not recommended, for the reasons above.\n+        The following optional keyword arguments are reserved:\n+        - `training`: Boolean scalar tensor of Python boolean indicating\n+          whether the `call` is meant for training or inference.\n+        - `mask`: Boolean input mask. If the layer's `call()` method takes a\n+          `mask` argument, its default value will be set to the mask generated\n+          for `inputs` by the previous layer (if `input` did come from a layer\n+          that generated a corresponding mask, i.e. if it came from a Keras\n+          layer with masking support).\n \n     Returns:\n       A tensor or list/tuple of tensors.\n\n@@ -1596,11 +1596,6 @@ class IdentityLayer(base_layer.Layer):\n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n class DTypeTest(keras_parameterized.TestCase):\n \n-  # This class only have tests relating to layer.dtype. Tests for dtype policies\n-  # are in mixed_precision/keras_test.py\n-\n-  # TODO(reedwm): Maybe have a separate test file for input casting tests.\n-\n   def _const(self, dtype):\n     return tf.constant(1, dtype=dtype)\n \n\n@@ -1336,16 +1336,8 @@ class _ClusterCoordinatorDataHandler(DataHandler):\n \n     def per_worker_dataset_fn():\n \n-      def wrapped_dataset_fn(input_context):\n-        # TODO(b/186692679): Currently we need to remove the device scope\n-        # imposed in `distribute_datasets_from_function` lib so that any\n-        # `StaticHashTable` is placed on the coordinator. Remove this workaround\n-        # once resolved.\n-        with tf.device(None):\n-          return x(input_context)\n-\n       return strategy.distribute_datasets_from_function(\n-          wrapped_dataset_fn, options=x.input_options)\n+          x, options=x.input_options)\n \n     self._dataset = self._model._cluster_coordinator.create_per_worker_dataset(  # pylint: disable=protected-access\n         per_worker_dataset_fn)\n\n@@ -26,9 +26,9 @@ from google.protobuf import text_format\n from tensorflow.core.example import example_pb2\n from tensorflow.core.example import feature_pb2\n from tensorflow.python.framework import test_util\n+from keras import backend\n from keras.feature_column import dense_features\n from keras.feature_column import sequence_feature_column as ksfc\n-from keras import backend\n from keras.layers import merge\n from keras.layers import recurrent\n \n\n@@ -112,7 +112,7 @@ class TpuStrategyTest(tf.test.TestCase):\n         for i in dataset:\n           strategy.run(step_fn, args=(i,))\n \n-  @test_util.disable_mlir_bridge(\"TODO(b/68036682): Support dynamic padder\")\n+  @test_util.disable_mlir_bridge(\"TODO(b/168036682): Support dynamic padder\")\n   def test_train_and_serve(self):\n     strategy = get_tpu_strategy()\n     use_adapt = False\n\n@@ -402,20 +402,21 @@ class ReLU(Layer):\n     max_value: Float >= 0. Maximum activation value. Default to None, which\n       means unlimited.\n     negative_slope: Float >= 0. Negative slope coefficient. Default to 0.\n-    threshold: Float. Threshold value for thresholded activation. Default to 0.\n+    threshold: Float >= 0. Threshold value for thresholded activation. Default\n+      to 0.\n   \"\"\"\n \n   def __init__(self, max_value=None, negative_slope=0, threshold=0, **kwargs):\n     super(ReLU, self).__init__(**kwargs)\n     if max_value is not None and max_value < 0.:\n-      raise ValueError('max_value of Relu layer '\n-                       'cannot be negative value: ' + str(max_value))\n-    if negative_slope < 0.:\n-      raise ValueError('negative_slope of Relu layer '\n-                       'cannot be negative value: ' + str(negative_slope))\n-    if threshold is None:\n-      raise ValueError('threshold of Relu layer '\n-                       'cannot be None. Required a float')\n+      raise ValueError('max_value of a ReLU layer cannot be a negative '\n+                       'value. Got: %s' % max_value)\n+    if negative_slope is None or negative_slope < 0.:\n+      raise ValueError('negative_slope of a ReLU layer cannot be a negative '\n+                       'value. Got: %s' % negative_slope)\n+    if threshold is None or threshold < 0.:\n+      raise ValueError('threshold of a ReLU layer cannot be a negative '\n+                       'value. Got: %s' % threshold)\n \n     self.supports_masking = True\n     if max_value is not None:\n\n@@ -78,21 +78,53 @@ class AdvancedActivationsTest(keras_parameterized.TestCase):\n       # Test that we use `relu6` when appropriate in graph mode.\n       self.assertTrue('Relu6' in keras.layers.ReLU(max_value=6)(x).name)\n \n-  def test_relu_with_invalid_arg(self):\n+  def test_relu_with_invalid_max_value(self):\n     with self.assertRaisesRegex(\n-        ValueError, 'max_value of Relu layer cannot be negative value: -10'):\n-      testing_utils.layer_test(keras.layers.ReLU,\n+        ValueError, 'max_value of a ReLU layer cannot be a negative '\n+        'value. Got: -10'):\n+      testing_utils.layer_test(\n+          keras.layers.ReLU,\n           kwargs={'max_value': -10},\n           input_shape=(2, 3, 4),\n           supports_masking=True)\n+\n+  def test_relu_with_invalid_negative_slope(self):\n     with self.assertRaisesRegex(\n-        ValueError,\n-        'negative_slope of Relu layer cannot be negative value: -2'):\n-      with self.cached_session():\n+        ValueError, 'negative_slope of a ReLU layer cannot be a negative '\n+        'value. Got: None'):\n       testing_utils.layer_test(\n           keras.layers.ReLU,\n-            kwargs={'negative_slope': -2},\n-            input_shape=(2, 3, 4))\n+          kwargs={'negative_slope': None},\n+          input_shape=(2, 3, 4),\n+          supports_masking=True)\n+\n+    with self.assertRaisesRegex(\n+        ValueError, 'negative_slope of a ReLU layer cannot be a negative '\n+        'value. Got: -10'):\n+      testing_utils.layer_test(\n+          keras.layers.ReLU,\n+          kwargs={'negative_slope': -10},\n+          input_shape=(2, 3, 4),\n+          supports_masking=True)\n+\n+  def test_relu_with_invalid_threshold(self):\n+    with self.assertRaisesRegex(\n+        ValueError, 'threshold of a ReLU layer cannot be a negative '\n+        'value. Got: None'):\n+      testing_utils.layer_test(\n+          keras.layers.ReLU,\n+          kwargs={'threshold': None},\n+          input_shape=(2, 3, 4),\n+          supports_masking=True)\n+\n+    with self.assertRaisesRegex(\n+        ValueError, 'threshold of a ReLU layer cannot be a negative '\n+        'value. Got: -10'):\n+      testing_utils.layer_test(\n+          keras.layers.ReLU,\n+          kwargs={'threshold': -10},\n+          input_shape=(2, 3, 4),\n+          supports_masking=True)\n \n   @keras_parameterized.run_with_all_model_types\n   def test_layer_as_activation(self):\n@@ -126,7 +158,7 @@ class AdvancedActivationsTest(keras_parameterized.TestCase):\n           input_shape=(2, 3, 4),\n           supports_masking=True)\n \n-  def test_threshold_relu_with_invalid_alpha(self):\n+  def test_threshold_relu_with_invalid_theta(self):\n     with self.assertRaisesRegex(\n         ValueError, 'Theta of a Thresholded ReLU layer cannot '\n         'be None, requires a float. Got None'):\n\n@@ -54,7 +54,8 @@ class Conv(Layer):\n   Args:\n     rank: An integer, the rank of the convolution, e.g. \"2\" for 2D convolution.\n     filters: Integer, the dimensionality of the output space (i.e. the number\n-      of filters in the convolution).\n+      of filters in the convolution). Could be \"None\", eg in the case of\n+      depth wise convolution.\n     kernel_size: An integer or tuple/list of n integers, specifying the\n       length of the convolution window.\n     strides: An integer or tuple/list of n integers,\n@@ -131,6 +132,9 @@ class Conv(Layer):\n \n     if isinstance(filters, float):\n       filters = int(filters)\n+    if filters is not None and filters < 0:\n+      raise ValueError(f'Received a negative value for `filters`.'\n+                       f'Was expecting a positive value, got {filters}.')\n     self.filters = filters\n     self.groups = groups or 1\n     self.kernel_size = conv_utils.normalize_tuple(\n\n@@ -173,6 +173,9 @@ class Dropout(Layer):\n \n   def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n     super(Dropout, self).__init__(**kwargs)\n+    if isinstance(rate, (int, float)) and not 0 <= rate <= 1:\n+      raise ValueError(f'Invalid value {rate} received for '\n+                       f'`rate`, expected a value between 0 and 1.')\n     self.rate = rate\n     if isinstance(rate, (int, float)) and not rate:\n       keras_temporary_dropout_rate.get_cell().set(True)\n@@ -717,6 +720,8 @@ class RepeatVector(Layer):\n   def __init__(self, n, **kwargs):\n     super(RepeatVector, self).__init__(**kwargs)\n     self.n = n\n+    if not isinstance(n, int):\n+      raise TypeError(f'Expected an integer value for `n`, got {type(n)}.')\n     self.input_spec = InputSpec(ndim=2)\n \n   def compute_output_shape(self, input_shape):\n@@ -1143,6 +1148,9 @@ class Dense(Layer):\n         activity_regularizer=activity_regularizer, **kwargs)\n \n     self.units = int(units) if not isinstance(units, int) else units\n+    if self.units < 0:\n+      raise ValueError(f'Received an invalid value for `units`, expected '\n+                       f'a positive integer, got {units}.')\n     self.activation = activations.get(activation)\n     self.use_bias = use_bias\n     self.kernel_initializer = initializers.get(kernel_initializer)\n\n@@ -969,7 +969,7 @@ class GlobalAveragePooling1D(GlobalPooling1D):\n   def call(self, inputs, mask=None):\n     steps_axis = 1 if self.data_format == 'channels_last' else 2\n     if mask is not None:\n-      mask = tf.cast(mask, backend.floatx())\n+      mask = tf.cast(mask, inputs[0].dtype)\n       mask = tf.compat.v1.expand_dims(\n           mask, 2 if self.data_format == 'channels_last' else 1)\n       inputs *= mask\n\n@@ -22,11 +22,20 @@ import numpy as np\n import keras\n from keras import combinations\n from keras import testing_utils\n+from keras.mixed_precision import policy\n \n \n @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n class GlobalPoolingTest(tf.test.TestCase, parameterized.TestCase):\n \n+  @testing_utils.enable_v2_dtype_behavior\n+  def test_mixed_float16_policy(self):\n+    with policy.policy_scope('mixed_float16'):\n+      inputs1 = keras.Input(shape=(36, 512), dtype='float16')\n+      inputs2 = keras.Input(shape=(36,), dtype='bool')\n+      average_layer = keras.layers.pooling.GlobalAveragePooling1D()\n+      _ = average_layer(inputs1, inputs2)\n+\n   def test_globalpooling_1d(self):\n     testing_utils.layer_test(\n         keras.layers.pooling.GlobalMaxPooling1D, input_shape=(3, 4, 5))\n\n@@ -166,7 +166,6 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     self.invert = invert\n     self.max_tokens = max_tokens\n     self.num_oov_indices = num_oov_indices\n-    self.oov_token = oov_token\n     self.output_mode = output_mode\n     self.sparse = sparse\n     self.pad_to_max_tokens = pad_to_max_tokens\n@@ -188,15 +187,17 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n \n     restore_from_static_table = kwargs.pop(\"has_static_table\", False)\n \n-    # Make sure the mask token is truly of the dtype we want. We can ignore\n-    # strings here, because they have only one dtype.\n-    if mask_token is not None:\n+    # Make sure the mask token and oov token are truly of the dtype we want. We\n+    # can ignore strings here, because they have only one dtype.\n     dtype = kwargs[\"dtype\"]\n     if dtype == tf.int32:\n-        mask_token = np.int32(mask_token)\n+      mask_token = None if mask_token is None else np.int32(mask_token)\n+      oov_token = None if oov_token is None else np.int32(oov_token)\n     elif dtype == tf.int64:\n-        mask_token = np.int64(mask_token)\n+      mask_token = None if mask_token is None else np.int64(mask_token)\n+      oov_token = None if oov_token is None else np.int64(oov_token)\n     self.mask_token = mask_token\n+    self.oov_token = oov_token\n \n     if max_tokens is not None:\n       available_vocab_size = max_tokens - self._token_start_index()\n@@ -281,7 +282,7 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n           initializer, default_value=default_value)\n       self._table_handler = table_utils.TableHandler(\n           table=self._table,\n-          mask_token=self._mask_key,\n+          mask_token=self._mask_key if self.mask_token is not None else None,\n           mask_value=self._mask_value,\n           oov_tokens=oov_indices)\n \n\n@@ -67,7 +67,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n       `output_mode` is `\"int\"`, the token is included in vocabulary and mapped\n       to index 0. In other output modes, the token will not appear in the\n       vocabulary and instances of the mask token in the input will be dropped.\n-      If set to None, no mask term will be added. Defaults to 0.\n+      If set to None, no mask term will be added. Defaults to None.\n     oov_token: Only used when `invert` is True. The token to return for OOV\n       indices. Defaults to -1.\n     vocabulary: An optional list of integer tokens, or a path to a text file\n@@ -109,8 +109,8 @@ class IntegerLookup(index_lookup.IndexLookup):\n   >>> layer = IntegerLookup(vocabulary=vocab)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n-  array([[2, 4, 5],\n-         [5, 1, 3]])>\n+  array([[1, 3, 4],\n+         [4, 0, 2]])>\n \n   **Creating a lookup layer with an adapted vocabulary**\n \n@@ -121,19 +121,19 @@ class IntegerLookup(index_lookup.IndexLookup):\n   >>> layer = IntegerLookup()\n   >>> layer.adapt(data)\n   >>> layer.get_vocabulary()\n-  [0, -1, 42, 1138, 1000, 36, 12]\n+  [-1, 42, 1138, 1000, 36, 12]\n \n-  Note how the mask token 0 and the OOV token -1 have been added to the\n-  vocabulary. The remaining tokens are sorted by frequency (1138, which has\n-  2 occurrences, is first) then by inverse sort order.\n+  Note that the OOV token -1 have been added to the vocabulary. The remaining\n+  tokens are sorted by frequency (42, which has 2 occurrences, is first) then\n+  by inverse sort order.\n \n   >>> data = tf.constant([[12, 1138, 42], [42, 1000, 36]])\n   >>> layer = IntegerLookup()\n   >>> layer.adapt(data)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n-  array([[6, 3, 2],\n-         [2, 4, 5]])>\n+  array([[5, 2, 1],\n+         [1, 3, 4]])>\n \n \n   **Lookups with multiple OOV indices**\n@@ -148,12 +148,12 @@ class IntegerLookup(index_lookup.IndexLookup):\n   >>> layer = IntegerLookup(vocabulary=vocab, num_oov_indices=2)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n-  array([[3, 5, 6],\n-         [2, 1, 4]])>\n+  array([[2, 4, 5],\n+         [1, 0, 3]])>\n \n-  Note that the output for OOV token 37 is 2, while the output for OOV token\n-  1000 is 1. The in-vocab terms have their output index increased by 1 from\n-  earlier examples (12 maps to 3, etc) in order to make space for the extra OOV\n+  Note that the output for OOV token 37 is 1, while the output for OOV token\n+  1000 is 0. The in-vocab terms have their output index increased by 1 from\n+  earlier examples (12 maps to 2, etc) in order to make space for the extra OOV\n   token.\n \n   **Multi-hot output**\n@@ -226,16 +226,14 @@ class IntegerLookup(index_lookup.IndexLookup):\n   vocab in this example.)\n \n   >>> vocab = [12, 36, 1138, 42]\n-  >>> data = tf.constant([[2, 4, 5], [5, 1, 3]])\n+  >>> data = tf.constant([[1, 3, 4], [4, 0, 2]])\n   >>> layer = IntegerLookup(vocabulary=vocab, invert=True)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n   array([[  12, 1138,   42],\n          [  42,   -1,   36]])>\n \n-  Note that the first two indices correspond to the mask and oov token by\n-  default. This behavior can be disabled by setting `mask_token=None` and\n-  `num_oov_indices=0`.\n+  Note that the first index correspond to the oov token by default.\n \n \n   **Forward and inverse lookup pairs**\n@@ -257,13 +255,13 @@ class IntegerLookup(index_lookup.IndexLookup):\n   1000 was not in the vocabulary - it got represented as an OOV, and all OOV\n   tokens are returned as -1 in the inverse layer. Also, note that for the\n   inverse to work, you must have already set the forward layer vocabulary\n-  either directly or via `fit()` before calling `get_vocabulary()`.\n+  either directly or via `adapt()` before calling `get_vocabulary()`.\n   \"\"\"\n \n   def __init__(self,\n                max_tokens=None,\n                num_oov_indices=1,\n-               mask_token=0,\n+               mask_token=None,\n                oov_token=-1,\n                vocabulary=None,\n                invert=False,\n\n@@ -51,7 +51,7 @@ def _get_end_to_end_test_cases():\n               \"max_tokens\": None,\n               \"dtype\": tf.int64,\n           },\n-          \"expected_output\": [[2], [3], [4], [5], [5], [4], [2], [1]],\n+          \"expected_output\": [[1], [2], [3], [4], [4], [3], [1], [0]],\n           \"input_dtype\":\n               tf.int64\n       },)\n@@ -125,7 +125,7 @@ class CategoricalEncodingInputTest(\n         dense_shape=[3, 4])\n \n     expected_indices = [[0, 0], [1, 2]]\n-    expected_values = [5, 1]\n+    expected_values = [4, 0]\n     expected_dense_shape = [3, 4]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)\n@@ -142,7 +142,7 @@ class CategoricalEncodingInputTest(\n     vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)\n     input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 42]],\n                                               dtype=np.int64)\n-    expected_output = [[2, 3, 5], [5, 4, 2, 1]]\n+    expected_output = [[1, 2, 4], [4, 3, 1, 0]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)\n     layer = integer_lookup.IntegerLookup(max_tokens=None)\n@@ -188,7 +188,7 @@ class CategoricalEncodingMultiOOVTest(\n     vocab_data = np.array([10, 11, 12, 13], dtype=np.int64)\n     input_array = tf.ragged.constant([[10, 11, 13], [13, 12, 10, 133]],\n                                               dtype=np.int64)\n-    expected_output = [[3, 4, 6], [6, 5, 3, 2]]\n+    expected_output = [[2, 3, 5], [5, 4, 2, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64, ragged=True)\n     layer = integer_lookup.IntegerLookup(max_tokens=None, num_oov_indices=2)\n@@ -213,7 +213,7 @@ class CategoricalEncodingAdaptTest(\n \n     layer = integer_lookup.IntegerLookup()\n     layer.adapt(vocab_dataset)\n-    expected_vocabulary = [0, -1, 203, 1729]\n+    expected_vocabulary = [-1, 203, 1729]\n     self.assertAllEqual(expected_vocabulary, layer.get_vocabulary())\n \n   def test_ragged_adapt(self):\n@@ -222,7 +222,7 @@ class CategoricalEncodingAdaptTest(\n \n     layer = integer_lookup.IntegerLookup()\n     layer.adapt(vocab_dataset)\n-    expected_vocabulary = [0, -1, 203, 1729]\n+    expected_vocabulary = [-1, 203, 1729]\n     self.assertAllEqual(expected_vocabulary, layer.get_vocabulary())\n \n   def test_single_int_generator_dataset(self):\n@@ -248,7 +248,7 @@ class IntegerLookupOutputTest(keras_parameterized.TestCase,\n   def test_int_output(self):\n     vocab_data = [42, 1138, 725, 1729]\n     input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])\n-    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n+    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n     layer = integer_lookup.IntegerLookup()\n@@ -264,13 +264,13 @@ class IntegerLookupOutputTest(keras_parameterized.TestCase,\n     int_data = layer(input_data)\n     self.assertAllEqual(int_data.shape[1:], input_data.shape[1:])\n \n-  def test_int_output_no_reserved_zero(self):\n+  def test_int_output_with_mask(self):\n     vocab_data = [42, 1138, 725, 1729]\n     input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])\n-    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]\n+    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n-    layer = integer_lookup.IntegerLookup(max_tokens=None, mask_token=None)\n+    layer = integer_lookup.IntegerLookup(max_tokens=None, mask_token=0)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -280,7 +280,7 @@ class IntegerLookupOutputTest(keras_parameterized.TestCase,\n   def test_int_output_explicit_vocab(self):\n     vocab_data = [42, 1138, 725, 1729]\n     input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])\n-    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n+    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n     layer = integer_lookup.IntegerLookup(\n@@ -301,6 +301,7 @@ class IntegerLookupOutputTest(keras_parameterized.TestCase,\n     layer = integer_lookup.IntegerLookup(\n         vocabulary=vocab_data,\n         max_tokens=None,\n+        mask_token=0,\n     )\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -308,8 +309,8 @@ class IntegerLookupOutputTest(keras_parameterized.TestCase,\n     self.assertAllEqual(expected_output, output_dataset)\n \n   def test_inverse_output(self):\n-    vocab_data = [0, -1, 42, 1138, 725, 1729]\n-    input_array = np.array([[2, 3, 4, 5], [5, 4, 2, 1]])\n+    vocab_data = [-1, 42, 1138, 725, 1729]\n+    input_array = np.array([[1, 2, 3, 4], [4, 3, 1, 0]])\n     expected_output = np.array([[42, 1138, 725, 1729], [1729, 725, 42, -1]])\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n@@ -369,7 +370,7 @@ class IntegerLookupVocabularyTest(\n   def test_int_output_explicit_vocab(self):\n     vocab_data = [42, 1138, 725, 1729]\n     input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])\n-    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n+    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n     layer = integer_lookup.IntegerLookup(vocabulary=vocab_data)\n@@ -400,7 +401,7 @@ class IntegerLookupVocabularyTest(\n   def test_count_output(self):\n     vocab_data = [2, 3, 4, 5]\n     input_array = np.array([[2, 2, 3, 4], [0, 1, 5, 6]])\n-    expected_output = [[0, 2, 1, 1, 0], [2, 0, 0, 0, 1]]\n+    expected_output = [[0, 2, 1, 1, 0], [3, 0, 0, 0, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n     layer = integer_lookup.IntegerLookup(\n@@ -421,7 +422,7 @@ class IntegerLookupVocabularyTest(\n \n   def test_get_vocab_returns_int(self):\n     vocab_data = [42, 1138, 725, 1729]\n-    expected_vocab = [0, -1, 42, 1138, 725, 1729]\n+    expected_vocab = [-1, 42, 1138, 725, 1729]\n     layer = integer_lookup.IntegerLookup(vocabulary=vocab_data)\n     layer_vocab = layer.get_vocabulary()\n     self.assertAllEqual(expected_vocab, layer_vocab)\n@@ -432,7 +433,7 @@ class IntegerLookupVocabularyTest(\n     vocab_path = self._write_to_temp_file(\"vocab_file\", vocab_list)\n \n     input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])\n-    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n+    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n     layer = integer_lookup.IntegerLookup(vocabulary=vocab_path)\n@@ -445,7 +446,7 @@ class IntegerLookupVocabularyTest(\n     vocab_list = [42, 1138, 725, 1729]\n     vocab_path = self._write_to_temp_file(\"vocab_file\", vocab_list)\n \n-    input_array = np.array([[2, 3, 4, 5], [5, 4, 2, 1]])\n+    input_array = np.array([[1, 2, 3, 4], [4, 3, 1, 0]])\n     expected_output = [[42, 1138, 725, 1729], [1729, 725, 42, -1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n@@ -455,7 +456,7 @@ class IntegerLookupVocabularyTest(\n     output_dataset = model.predict(input_array)\n     self.assertAllEqual(expected_output, output_dataset)\n \n-  def test_int_output_inverted_vocab_from_file_nonstandard_mask(self):\n+  def test_int_output_inverted_vocab_from_file_with_mask(self):\n     vocab_list = [42, 1138, 725, 1729]\n     vocab_path = self._write_to_temp_file(\"vocab_file\", vocab_list)\n \n@@ -475,7 +476,7 @@ class IntegerLookupVocabularyTest(\n     vocab_path = self._write_to_temp_file(\"vocab_file\", vocab_list)\n \n     input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])\n-    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n+    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n     layer = integer_lookup.IntegerLookup()\n@@ -528,7 +529,7 @@ class IntegerLookupSavingTest(keras_parameterized.TestCase,\n   def test_vocabulary_persistence_across_saving(self):\n     vocab_data = [42, 1138, 725, 1729]\n     input_array = np.array([[42, 1138, 725, 1729], [1729, 725, 42, 203]])\n-    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n+    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]\n \n     # Build and validate a golden model.\n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n\n@@ -46,7 +46,7 @@ class StringLookup(index_lookup.IndexLookup):\n   (which can optionally occupy multiple indices in the vocabulary, as set\n   by `num_oov_indices`).\n   The position of these tokens in the vocabulary is fixed. When `output_mode` is\n-  `\"int\"`, the vocabulary will begin with the mask token at index 0, followed by\n+  `\"int\"`, the vocabulary will begin with the mask token (if set), followed by\n   OOV indices, followed by the rest of the vocabulary. When `output_mode` is\n   `\"multi_hot\"`, `\"count\"`, or `\"tf_idf\"` the vocabulary will begin with OOV\n   indices and instances of the mask token will be dropped.\n@@ -63,7 +63,7 @@ class StringLookup(index_lookup.IndexLookup):\n       `\"int\"`, the token is included in vocabulary and mapped to index 0. In\n       other output modes, the token will not appear in the vocabulary and\n       instances of the mask token in the input will be dropped. If set to None,\n-      no mask term will be added. Defaults to `\"\"`.\n+      no mask term will be added. Defaults to `None`.\n     oov_token: Only used when `invert` is True. The token to return for OOV\n       indices. Defaults to `\"[UNK]\"`.\n     vocabulary: An optional list of tokens, or a path to a text file containing\n@@ -104,8 +104,8 @@ class StringLookup(index_lookup.IndexLookup):\n   >>> layer = StringLookup(vocabulary=vocab)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n-  array([[2, 4, 5],\n-         [5, 1, 3]])>\n+  array([[1, 3, 4],\n+         [4, 0, 2]])>\n \n   **Creating a lookup layer with an adapted vocabulary**\n \n@@ -116,19 +116,19 @@ class StringLookup(index_lookup.IndexLookup):\n   >>> layer = StringLookup()\n   >>> layer.adapt(data)\n   >>> layer.get_vocabulary()\n-  ['', '[UNK]', 'd', 'z', 'c', 'b', 'a']\n+  ['[UNK]', 'd', 'z', 'c', 'b', 'a']\n \n-  Note how the mask token '' and the OOV token [UNK] have been added to the\n-  vocabulary. The remaining tokens are sorted by frequency ('d', which has\n-  2 occurrences, is first) then by inverse sort order.\n+  Note that the OOV token [UNK] has been added to the vocabulary. The remaining\n+  tokens are sorted by frequency ('d', which has 2 occurrences, is first) then\n+  by inverse sort order.\n \n   >>> data = tf.constant([[\"a\", \"c\", \"d\"], [\"d\", \"z\", \"b\"]])\n   >>> layer = StringLookup()\n   >>> layer.adapt(data)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n-  array([[6, 4, 2],\n-         [2, 3, 5]])>\n+  array([[5, 3, 1],\n+         [1, 2, 4]])>\n \n   **Lookups with multiple OOV indices**\n \n@@ -142,12 +142,12 @@ class StringLookup(index_lookup.IndexLookup):\n   >>> layer = StringLookup(vocabulary=vocab, num_oov_indices=2)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n-  array([[3, 5, 6],\n-         [1, 2, 4]])>\n+  array([[2, 4, 5],\n+         [0, 1, 3]])>\n \n-  Note that the output for OOV value 'm' is 1, while the output for OOV value\n-  'z' is 2. The in-vocab terms have their output index increased by 1 from\n-  earlier examples (a maps to 3, etc) in order to make space for the extra OOV\n+  Note that the output for OOV value 'm' is 0, while the output for OOV value\n+  'z' is 1. The in-vocab terms have their output index increased by 1 from\n+  earlier examples (a maps to 2, etc) in order to make space for the extra OOV\n   value.\n \n   **Multi-hot output**\n@@ -220,16 +220,14 @@ class StringLookup(index_lookup.IndexLookup):\n   vocab in this example.)\n \n   >>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n-  >>> data = tf.constant([[2, 4, 5], [5, 1, 3]])\n+  >>> data = tf.constant([[1, 3, 4], [4, 0, 2]])\n   >>> layer = StringLookup(vocabulary=vocab, invert=True)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 3), dtype=string, numpy=\n   array([[b'a', b'c', b'd'],\n          [b'd', b'[UNK]', b'b']], dtype=object)>\n \n-  Note that the first two indices correspond to the mask and oov token by\n-  default. This behavior can be disabled by setting `mask_token=None` and\n-  `num_oov_indices=0`.\n+  Note that the first index correspond to the oov token by default.\n \n \n   **Forward and inverse lookup pairs**\n@@ -251,13 +249,13 @@ class StringLookup(index_lookup.IndexLookup):\n   1000 was not in the vocabulary - it got represented as an OOV, and all OOV\n   values are returned as '[OOV}' in the inverse layer. Also, note that for the\n   inverse to work, you must have already set the forward layer vocabulary\n-  either directly or via fit() before calling get_vocabulary().\n+  either directly or via adapt() before calling get_vocabulary().\n   \"\"\"\n \n   def __init__(self,\n                max_tokens=None,\n                num_oov_indices=1,\n-               mask_token=\"\",\n+               mask_token=None,\n                oov_token=\"[UNK]\",\n                vocabulary=None,\n                encoding=None,\n\n@@ -44,7 +44,7 @@ def _get_end_to_end_test_cases():\n           \"kwargs\": {\n               \"max_tokens\": None,\n           },\n-          \"expected_output\": [[2], [3], [4], [5], [5], [4], [2], [1]],\n+          \"expected_output\": [[1], [2], [3], [4], [4], [3], [1], [0]],\n           \"input_dtype\":\n               tf.string\n       },\n@@ -124,7 +124,7 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n     input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n                             [\"fire\", \"and\", \"earth\", \"michigan\"]])\n-    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n+    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n     layer = string_lookup.StringLookup(vocabulary=vocab_data)\n@@ -140,7 +140,7 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n     expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n-    layer = string_lookup.StringLookup(vocabulary=vocab_data)\n+    layer = string_lookup.StringLookup(vocabulary=vocab_data, mask_token=\"\")\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n     output_data = model.predict(input_array)\n@@ -191,7 +191,7 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n \n   def test_get_vocab_returns_str(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-    expected_vocab = [\"\", \"[UNK]\", \"earth\", \"wind\", \"and\", \"fire\"]\n+    expected_vocab = [\"[UNK]\", \"earth\", \"wind\", \"and\", \"fire\"]\n     layer = string_lookup.StringLookup(vocabulary=vocab_data)\n     layer_vocab = layer.get_vocabulary()\n     self.assertAllEqual(expected_vocab, layer_vocab)\n@@ -209,7 +209,7 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n \n     input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n                             [\"fire\", \"and\", \"earth\", \"michigan\"]])\n-    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n+    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n     layer = string_lookup.StringLookup(vocabulary=vocab_path)\n@@ -224,7 +224,7 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n \n     input_array = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n                             [\"fire\", \"and\", \"earth\", \"michigan\"]])\n-    expected_output = [[2, 3, 4, 5], [5, 4, 2, 1]]\n+    expected_output = [[1, 2, 3, 4], [4, 3, 1, 0]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n     layer = string_lookup.StringLookup()\n@@ -254,7 +254,8 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n                                 [\"fire\", \"and\", \"earth\", \"\"]])\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n-    layer = string_lookup.StringLookup(vocabulary=vocab_data, invert=True)\n+    layer = string_lookup.StringLookup(\n+        vocabulary=vocab_data, invert=True, mask_token=\"\")\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n     output_data = model.predict(input_array)\n@@ -262,7 +263,7 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n \n   def test_inverse_layer_from_file(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n-    input_array = np.array([[2, 3, 4, 5], [5, 4, 2, 1]])\n+    input_array = np.array([[1, 2, 3, 4], [4, 3, 1, 0]])\n     expected_output = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n                                 [\"fire\", \"and\", \"earth\", \"[UNK]\"]])\n     vocab_path = self._write_to_temp_file(\"vocab_file\", vocab_data)\n@@ -274,7 +275,7 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n     output_data = model.predict(input_array)\n     self.assertAllEqual(expected_output, output_data)\n \n-  def test_inverse_layer_from_file_with_non_default_msk(self):\n+  def test_inverse_layer_from_file_with_mask(self):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n     input_array = np.array([[2, 3, 4, 5], [5, 4, 2, 0]])\n     expected_output = np.array([[\"earth\", \"wind\", \"and\", \"fire\"],\n@@ -329,7 +330,7 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n     input_array = tf.ragged.constant([[\"earth\", \"wind\", \"fire\"],\n                                                [\"fire\", \"and\", \"earth\",\n                                                 \"ohio\"]])\n-    expected_output = [[3, 4, 6], [6, 5, 3, 2]]\n+    expected_output = [[2, 3, 5], [5, 4, 2, 1]]\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string, ragged=True)\n     layer = string_lookup.StringLookup(num_oov_indices=2)\n\n@@ -313,6 +313,7 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n         max_tokens=max_tokens,\n         vocabulary=vocabulary,\n         pad_to_max_tokens=pad_to_max_tokens,\n+        mask_token=\"\",\n         output_mode=output_mode if output_mode is not None else INT,\n         vocabulary_size=vocabulary_size)\n \n\n@@ -1305,6 +1305,9 @@ class SimpleRNNCell(DropoutRNNCellMixin, Layer):\n                dropout=0.,\n                recurrent_dropout=0.,\n                **kwargs):\n+    if units < 0:\n+      raise ValueError(f'Received an invalid value for units, expected '\n+                       f'a positive integer, got {units}.')\n     # By default use cached variable under v2 mode, see b/143699808.\n     if tf.compat.v1.executing_eagerly_outside_functions():\n       self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n@@ -1743,6 +1746,9 @@ class GRUCell(DropoutRNNCellMixin, Layer):\n                recurrent_dropout=0.,\n                reset_after=False,\n                **kwargs):\n+    if units < 0:\n+      raise ValueError(f'Received an invalid value for units, expected '\n+                       f'a positive integer, got {units}.')\n     # By default use cached variable under v2 mode, see b/143699808.\n     if tf.compat.v1.executing_eagerly_outside_functions():\n       self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n@@ -2303,6 +2309,9 @@ class LSTMCell(DropoutRNNCellMixin, Layer):\n                dropout=0.,\n                recurrent_dropout=0.,\n                **kwargs):\n+    if units < 0:\n+      raise ValueError(f'Received an invalid value for units, expected '\n+                       f'a positive integer, got {units}.')\n     # By default use cached variable under v2 mode, see b/143699808.\n     if tf.compat.v1.executing_eagerly_outside_functions():\n       self._enable_caching_device = kwargs.pop('enable_caching_device', True)\n\n@@ -1982,7 +1982,7 @@ class AUC(Metric):\n       case, when multilabel data is passed to AUC, each label-prediction pair\n       is treated as an individual data point. Should be set to False for\n       multi-class data.\n-    num_labels: (Optional) The number of labels, used when `multi_label' is\n+    num_labels: (Optional) The number of labels, used when `multi_label` is\n       True. If `num_labels` is not specified, then state variables get created\n       on the first call to `update_state`.\n     label_weights: (Optional) list, array, or tensor of non-negative weights\n@@ -2006,7 +2006,7 @@ class AUC(Metric):\n   >>> m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])\n   >>> # threshold values are [0 - 1e-7, 0.5, 1 + 1e-7]\n   >>> # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]\n-  >>> # recall = [1, 0.5, 0], fp_rate = [1, 0, 0]\n+  >>> # tp_rate = recall = [1, 0.5, 0], fp_rate = [1, 0, 0]\n   >>> # auc = ((((1+0.5)/2)*(1-0)) + (((0.5+0)/2)*(0-0))) = 0.75\n   >>> m.result().numpy()\n   0.75\n@@ -2410,8 +2410,8 @@ class CosineSimilarity(MeanMetricWrapper):\n \n   Standalone usage:\n \n-  >>> # l2_norm(y_true) = [[0., 1.], [1./1.414], 1./1.414]]]\n-  >>> # l2_norm(y_pred) = [[1., 0.], [1./1.414], 1./1.414]]]\n+  >>> # l2_norm(y_true) = [[0., 1.], [1./1.414, 1./1.414]]\n+  >>> # l2_norm(y_pred) = [[1., 0.], [1./1.414, 1./1.414]]\n   >>> # l2_norm(y_true) . l2_norm(y_pred) = [[0., 0.], [0.5, 0.5]]\n   >>> # result = mean(sum(l2_norm(y_true) . l2_norm(y_pred), axis=1))\n   >>> #        = ((0. + 0.) +  (0.5 + 0.5)) / 2\n@@ -3362,7 +3362,7 @@ def accuracy(y_true, y_pred):\n   [y_pred, y_true], _ = \\\n       metrics_utils.ragged_assert_compatible_and_get_flat_values(\n           [y_pred, y_true])\n-  y_pred.shape.assert_is_compatible_with(y_true.shape)\n+  y_true.shape.assert_is_compatible_with(y_pred.shape)\n   if y_true.dtype != y_pred.dtype:\n     y_pred = tf.cast(y_pred, y_true.dtype)\n   return tf.cast(tf.equal(y_true, y_pred), backend.floatx())\n\n@@ -0,0 +1,433 @@\n+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests keras.layers.Layer works properly with mixed precision.\"\"\"\n+\n+import tensorflow.compat.v2 as tf\n+\n+import os\n+\n+from absl.testing import parameterized\n+import numpy as np\n+from keras import combinations\n+from keras import keras_parameterized\n+from keras import layers\n+from keras import models\n+from keras.engine import base_layer\n+from keras.engine import base_layer_utils\n+from keras.engine import input_spec\n+from keras.mixed_precision import get_layer_policy\n+from keras.mixed_precision import policy\n+from keras.mixed_precision import test_util as mp_test_util\n+from keras.optimizer_v2 import gradient_descent\n+\n+\n+class MultiplyLayerWithFunction(mp_test_util.MultiplyLayer):\n+  \"\"\"Same as MultiplyLayer, but _multiply is decorated with a tf.function.\"\"\"\n+\n+  @tf.function\n+  def _multiply(self, x, y):\n+    return super(MultiplyLayerWithFunction, self)._multiply(x, y)\n+\n+\n+# If called outside any strategy.scope() calls, this will return the default\n+# strategy.\n+default_strategy_fn = tf.distribute.get_strategy\n+\n+\n+def create_mirrored_strategy():\n+  \"\"\"Create a MirroredStrategy, using a GPU if it is available.\"\"\"\n+  if tf.config.list_logical_devices('GPU'):\n+    return tf.distribute.MirroredStrategy(['cpu:0', 'gpu:0'])\n+  else:\n+    return tf.distribute.MirroredStrategy(['cpu:0'])\n+\n+\n+def create_central_storage_strategy():\n+  \"\"\"Create a CentralStorageStrategy, using a GPU if it is available.\"\"\"\n+  compute_devices = ['cpu:0', 'gpu:0'] if (\n+      tf.config.list_logical_devices('GPU')) else ['cpu:0']\n+  return tf.distribute.experimental.CentralStorageStrategy(\n+      compute_devices, parameter_device='cpu:0')\n+\n+\n+TESTCASES = ({\n+    'testcase_name': 'base',\n+    'strategy_fn': default_strategy_fn\n+}, {\n+    'testcase_name': 'distribute',\n+    'strategy_fn': create_mirrored_strategy\n+})\n+\n+\n+@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+class LayerTest(keras_parameterized.TestCase):\n+  \"\"\"Test mixed precision with Keras layers.\"\"\"\n+\n+  @parameterized.named_parameters(*TESTCASES)\n+  def test_mixed_policies_(self, strategy_fn):\n+    strategy = strategy_fn()\n+    for dtype in 'float16', 'bfloat16':\n+      x = tf.constant([1.])\n+      policy_name = 'mixed_' + dtype\n+      with strategy.scope(), policy.policy_scope(policy_name):\n+        layer = mp_test_util.MultiplyLayer(assert_type=dtype)\n+        self.assertEqual(layer.dtype, tf.float32)\n+        self.assertEqual(get_layer_policy.get_layer_policy(layer).name,\n+                         policy_name)\n+        y = layer(x)\n+        self.assertEqual(layer.v.dtype, tf.float32)\n+        self.assertEqual(y.dtype, dtype)\n+        self.assertEqual(layer.dtype_policy.name, policy_name)\n+        self.assertIsInstance(layer.dtype_policy, policy.Policy)\n+        self.assertEqual(layer.compute_dtype, dtype)\n+        self.assertEqual(layer.dtype, tf.float32)\n+        self.assertEqual(layer.variable_dtype, tf.float32)\n+        self.assertEqual(get_layer_policy.get_layer_policy(layer).name,\n+                         policy_name)\n+        self.evaluate(tf.compat.v1.global_variables_initializer())\n+        self.assertEqual(self.evaluate(y), 1.)\n+\n+  def test_layer_with_int_variable(self):\n+    class LayerWithIntVar(base_layer.Layer):\n+\n+      def build(self, _):\n+        self.v = self.add_weight('v', dtype='int32', trainable=False)\n+\n+      def call(self, inputs):\n+        # Only float variables should be autocasted. This will fail if self.v is\n+        # autocasted to float32\n+        return tf.cast(inputs, 'int32') + self.v\n+\n+    x = tf.constant([1.])\n+    layer = LayerWithIntVar(dtype='mixed_float16')\n+    self.assertEqual(layer(x).dtype, 'int32')\n+\n+  @parameterized.named_parameters(*TESTCASES)\n+  def test_layer_with_non_autocast_variable(self, strategy_fn):\n+    x = tf.constant([1.])\n+    with strategy_fn().scope():\n+      with policy.policy_scope('mixed_float16'):\n+        layer = mp_test_util.MultiplyLayerWithoutAutoCast(\n+            assert_type=tf.float16)\n+        y = layer(x)\n+        self.assertEqual(layer.v.dtype, tf.float32)\n+        self.assertEqual(y.dtype, tf.float16)\n+        self.evaluate(tf.compat.v1.global_variables_initializer())\n+        self.assertEqual(self.evaluate(y), 1.)\n+\n+  @parameterized.named_parameters(*TESTCASES)\n+  def test_layer_calling_tf_function(self, strategy_fn):\n+    x = tf.constant([1.])\n+    with strategy_fn().scope():\n+      with policy.policy_scope('mixed_float16'):\n+        layer = MultiplyLayerWithFunction(assert_type=tf.float16)\n+        y = layer(x)\n+        self.assertEqual(layer.v.dtype, tf.float32)\n+        self.assertEqual(y.dtype, tf.float16)\n+        self.evaluate(tf.compat.v1.global_variables_initializer())\n+        self.assertEqual(self.evaluate(y), 1.)\n+\n+  @parameterized.named_parameters(*TESTCASES)\n+  def test_layer_regularizer_runs_in_var_dtype(self, strategy_fn):\n+    x = tf.constant([1.])\n+    with strategy_fn().scope():\n+      with policy.policy_scope('mixed_float16'):\n+        # Test on MultiplyLayer\n+        layer = mp_test_util.MultiplyLayer(\n+            assert_type=tf.float16,\n+            regularizer=mp_test_util.IdentityRegularizer())\n+        layer(x)\n+        (regularizer_loss,) = layer.losses\n+        self.assertEqual(regularizer_loss.dtype, tf.float32)\n+        self.evaluate(tf.compat.v1.global_variables_initializer())\n+        self.assertEqual(self.evaluate(regularizer_loss), 1.)\n+\n+        # Test on MultiplyLayerWithoutAutoCast\n+        layer = mp_test_util.MultiplyLayerWithoutAutoCast(\n+            assert_type=tf.float16,\n+            regularizer=mp_test_util.IdentityRegularizer())\n+        layer(x)\n+        (regularizer_loss,) = layer.losses\n+        self.assertEqual(regularizer_loss.dtype, tf.float32)\n+        self.evaluate(tf.compat.v1.global_variables_initializer())\n+        self.assertEqual(self.evaluate(regularizer_loss), 1.)\n+\n+  @parameterized.named_parameters(*TESTCASES)\n+  def test_passing_policy_to_layer(self, strategy_fn):\n+    x = tf.constant([1.], dtype=tf.float16)\n+    with strategy_fn().scope():\n+      # Passing a Policy to 'dtype' sets the policy for that layer.\n+      layer = mp_test_util.MultiplyLayer(\n+          assert_type=tf.float16, dtype=policy.Policy('mixed_float16'))\n+      # layer.dtype refers to the variable dtype\n+      self.assertEqual(layer.dtype, tf.float32)\n+      layer(x)\n+      self.assertEqual(layer.v.dtype, tf.float32)\n+      with policy.policy_scope('mixed_float16'):\n+        # Passing a Policy to dtype overrides the global Policy\n+        layer = mp_test_util.MultiplyLayer(\n+            assert_type=tf.float64, dtype=policy.Policy('float64'))\n+        self.assertEqual(layer.dtype_policy.name, 'float64')\n+        self.assertIsInstance(layer.dtype_policy, policy.Policy)\n+        self.assertEqual(layer.compute_dtype, tf.float64)\n+        self.assertEqual(layer.dtype, tf.float64)\n+        self.assertEqual(layer.variable_dtype, tf.float64)\n+        self.assertEqual(layer(x).dtype, tf.float64)\n+        self.assertEqual(layer.v.dtype, tf.float64)\n+\n+  @parameterized.named_parameters(*TESTCASES)\n+  def test_gradient(self, strategy_fn):\n+    x = tf.constant([1.])\n+    with strategy_fn().scope() as strategy:\n+      with policy.policy_scope('mixed_float16'):\n+        layer = mp_test_util.MultiplyLayer(assert_type=tf.float16)\n+        # Learning rate is small enough that if applied to a float16 variable,\n+        # the variable will not change. So this tests the learning rate is not\n+        # applied to a float16 value, but instead the float32 variable.\n+        opt = gradient_descent.SGD(2**-14)\n+\n+        def run_fn():\n+          with tf.GradientTape() as tape:\n+            y = layer(x)\n+            # Divide by num_replicas_in_sync, as the effective total loss is the\n+            # sum of each of the replica's losses.\n+            y /= strategy.num_replicas_in_sync\n+\n+          grad = tape.gradient(y, layer.v)\n+          return opt.apply_gradients([(grad, layer.v)])\n+\n+        op = strategy.experimental_run(run_fn)\n+        if not tf.executing_eagerly():\n+          self.evaluate(tf.compat.v1.global_variables_initializer())\n+          self.evaluate(op)\n+        # The gradient with respective to the variable is 1. Since the\n+        # variable is initialized with 1 and the learning rate is 2**-14, the\n+        # new variable value should be: init_val - gradient * learning_rate,\n+        # which is  1 - 1 * 2**-14\n+        self.assertEqual(self.evaluate(layer.v), 1 - 2**-14)\n+\n+  def _test_checkpointing_layer_weights(self, strategy_fn,\n+                                        mixed_prec_when_saving,\n+                                        mixed_prec_when_loading):\n+    # In this test, we potentially save with mixed precision enabled and load\n+    # with mixed precision disabled, or vice versa. This is possible because\n+    # variables are float32 regardless of whether mixed precision is enabled.\n+    save_policy = 'mixed_float16' if mixed_prec_when_saving else 'float32'\n+    load_policy = 'mixed_float16' if mixed_prec_when_loading else 'float32'\n+    save_input_dtype = 'float16' if mixed_prec_when_saving else 'float32'\n+    load_input_dtype = 'float16' if mixed_prec_when_loading else 'float32'\n+\n+    # Create a layer and save a checkpoint.\n+    x = tf.constant([1.])\n+    with strategy_fn().scope():\n+      with policy.policy_scope(save_policy):\n+        layer = mp_test_util.MultiplyLayer(assert_type=save_input_dtype)\n+        layer(x)  # Build layer\n+    layer.set_weights([np.array(100.)])\n+    self.assertEqual(self.evaluate(layer(x)), 100.)\n+    checkpoint = tf.train.Checkpoint(layer=layer)\n+    prefix = os.path.join(self.get_temp_dir(), 'ckpt')\n+    save_path = checkpoint.save(prefix)\n+\n+    # Create a new layer and restore the checkpoint.\n+    x = tf.constant([1.])\n+    with strategy_fn().scope():\n+      with policy.policy_scope(load_policy):\n+        layer = mp_test_util.MultiplyLayer(assert_type=load_input_dtype)\n+        layer(x)  # Build layer\n+    layer.set_weights([np.array(200.)])\n+    self.assertEqual(self.evaluate(layer(x)), 200.)\n+    checkpoint = tf.train.Checkpoint(layer=layer)\n+    checkpoint.restore(save_path).assert_consumed().run_restore_ops()\n+    self.assertEqual(layer.get_weights(), [100.])\n+    self.assertEqual(self.evaluate(layer(x)), 100.)\n+\n+  @parameterized.named_parameters(*TESTCASES)\n+  def test_checkpointing_layer_weights(self, strategy_fn):\n+    with self.test_session():\n+      self._test_checkpointing_layer_weights(\n+          strategy_fn, mixed_prec_when_saving=True,\n+          mixed_prec_when_loading=True)\n+      self._test_checkpointing_layer_weights(\n+          strategy_fn, mixed_prec_when_saving=True,\n+          mixed_prec_when_loading=False)\n+      self._test_checkpointing_layer_weights(\n+          strategy_fn, mixed_prec_when_saving=False,\n+          mixed_prec_when_loading=True)\n+\n+  @parameterized.named_parameters(*TESTCASES)\n+  def test_config(self, strategy_fn):\n+    x = tf.constant([1.], dtype=tf.float16)\n+    with strategy_fn().scope():\n+      for layer, dtype in (\n+          (mp_test_util.MultiplyLayer(), 'float32'),\n+          (mp_test_util.MultiplyLayer(dtype='float64'), 'float64'),\n+          (mp_test_util.MultiplyLayer(dtype=policy.Policy('float64')),\n+           'float64')):\n+        config = layer.get_config()\n+        self.assertEqual(config['dtype'], dtype)\n+        self.assertIsInstance(config['dtype'], str)\n+        layer = mp_test_util.MultiplyLayer.from_config(config)\n+        self.assertEqual(layer.dtype, dtype)\n+        self.assertEqual(layer(x).dtype, dtype)\n+        self.assertEqual(layer.v.dtype, dtype)\n+\n+      layer = mp_test_util.MultiplyLayer(dtype='mixed_float16')\n+      config = layer.get_config()\n+      self.assertEqual(config['dtype'],\n+                       {'class_name': 'Policy',\n+                        'config': {'name': 'mixed_float16'}})\n+      layer = mp_test_util.MultiplyLayer.from_config(config)\n+      self.assertEqual(layer.dtype, 'float32')\n+      self.assertEqual(layer(x).dtype, 'float16')\n+      self.assertEqual(layer.v.dtype, 'float32')\n+      config = layer.get_config()\n+      self.assertEqual(config['dtype'],\n+                       {'class_name': 'Policy',\n+                        'config': {'name': 'mixed_float16'}})\n+\n+      layer = mp_test_util.MultiplyLayer(dtype=policy.Policy('_infer'))\n+      config = layer.get_config()\n+      self.assertIsNone(config['dtype'])\n+      layer = mp_test_util.MultiplyLayer.from_config(config)\n+      # If a layer is serialized with the \"_infer\" policy, when deserialized\n+      # into TF 2 it will have the global policy instead of \"_infer\". This is\n+      # because \"_infer\" is serialized into None, and passing dtype=None in\n+      # TensorFlow 2 indicates to use the global policy.\n+      self.assertEqual(layer.dtype, 'float32')\n+      self.assertEqual(layer(x).dtype, 'float32')\n+      self.assertEqual(layer.v.dtype, 'float32')\n+\n+  @parameterized.named_parameters(*TESTCASES)\n+  def test_config_policy_v1(self, strategy_fn):\n+    x = tf.constant([1.], dtype=tf.float16)\n+    with strategy_fn().scope():\n+\n+      layer = mp_test_util.MultiplyLayer(dtype=policy.PolicyV1('mixed_float16',\n+                                                               loss_scale=None))\n+      config = layer.get_config()\n+      self.assertEqual(config['dtype'],\n+                       {'class_name': 'PolicyV1',\n+                        'config': {'name': 'mixed_float16',\n+                                   'loss_scale': None}})\n+      layer = mp_test_util.MultiplyLayer.from_config(config)\n+      self.assertEqual(layer.dtype, 'float32')\n+      self.assertEqual(layer(x).dtype, 'float16')\n+      self.assertEqual(layer.v.dtype, 'float32')\n+      # Restoring a PolicyV1 silently converts it to a Policy and drops the loss\n+      # scale.\n+      self.assertEqual(type(layer.dtype_policy), policy.Policy)\n+      config = layer.get_config()\n+      # The loss_scale is silently dropped\n+      self.assertEqual(config['dtype'],\n+                       {'class_name': 'Policy',\n+                        'config': {'name': 'mixed_float16'}})\n+\n+      layer = mp_test_util.MultiplyLayer(dtype=policy.PolicyV1('float64',\n+                                                               loss_scale=2.))\n+      config = layer.get_config()\n+      self.assertEqual(config['dtype'],\n+                       {'class_name': 'PolicyV1',\n+                        'config': {'name': 'float64',\n+                                   'loss_scale': {\n+                                       'class_name': 'FixedLossScale',\n+                                       'config': {'loss_scale_value': 2.0}}}})\n+      layer = mp_test_util.MultiplyLayer.from_config(config)\n+      self.assertEqual(layer.dtype, 'float64')\n+      self.assertEqual(layer(x).dtype, 'float64')\n+      self.assertEqual(layer.v.dtype, 'float64')\n+      self.assertEqual(type(layer.dtype_policy), policy.Policy)\n+      config = layer.get_config()\n+      self.assertEqual(config['dtype'], 'float64')\n+\n+      layer = mp_test_util.MultiplyLayer(dtype=policy.PolicyV1('_infer',\n+                                                               loss_scale=2.))\n+      config = layer.get_config()\n+      self.assertEqual(config['dtype'],\n+                       {'class_name': 'PolicyV1',\n+                        'config': {'name': '_infer',\n+                                   'loss_scale': {\n+                                       'class_name': 'FixedLossScale',\n+                                       'config': {'loss_scale_value': 2.0}}}})\n+      layer = mp_test_util.MultiplyLayer.from_config(config)\n+      self.assertEqual(layer.dtype, None)\n+      self.assertEqual(layer(x).dtype, 'float16')\n+      self.assertEqual(layer.v.dtype, 'float16')\n+      self.assertEqual(type(layer.dtype_policy), policy.Policy)\n+      config = layer.get_config()\n+      self.assertEqual(config['dtype'], 'float16')\n+\n+  def test_delete_variable(self):\n+    layer = base_layer.Layer(dtype='mixed_float16')\n+    layer.x = layer.add_weight('x')\n+    self.assertEqual(layer.trainable_weights, [layer.x])\n+    del layer.x\n+    self.assertEqual(layer.trainable_weights, [])\n+\n+  def test_build_and_call_layer_in_function(self):\n+    layer = mp_test_util.MultiplyLayer(dtype=policy.Policy('mixed_float16'))\n+    @tf.function\n+    def f():\n+      return layer(1.)\n+    y = f()\n+    self.evaluate(tf.compat.v1.global_variables_initializer())\n+    self.assertEqual(y.dtype, 'float16')\n+    self.assertEqual(layer.v.dtype, 'float32')\n+    self.assertEqual(self.evaluate(y), 1.)\n+\n+  def test_unsupported_strategy(self):\n+    strategy = create_central_storage_strategy()\n+    with strategy.scope(), self.assertRaisesRegex(\n+        ValueError, 'Mixed precision is not supported with the '\n+        'tf.distribute.Strategy: CentralStorageStrategy. Either '\n+        'stop using mixed precision by removing the use of the '\n+        '\"mixed_float16\" policy or use a different Strategy, e.g. '\n+        'a MirroredStrategy.'):\n+      mp_test_util.MultiplyLayer(dtype='mixed_float16')\n+    # Non-mixed policies are fine\n+    mp_test_util.MultiplyLayer(dtype=policy.Policy('float64'))\n+\n+  def test_input_spec_dtype(self):\n+    # Test the InputSpec's dtype is compared against the inputs before the layer\n+    # casts them, not after.\n+    layer = mp_test_util.MultiplyLayer(dtype='float64')\n+    layer.input_spec = input_spec.InputSpec(dtype='float16')\n+\n+    # Test passing Eager tensors\n+    x = tf.ones((2, 2), dtype='float16')\n+    layer(x)\n+    x = tf.ones((2, 2), dtype='float64')\n+    with self.assertRaisesRegex(\n+        ValueError, 'expected dtype=float16, found dtype=.*float64'):\n+      layer(x)\n+\n+    # Test passing symbolic tensors\n+    x = layers.Input((2,), dtype='float16')\n+    y = layer(x)\n+    model = models.Model(x, y)\n+    model(tf.ones((2, 2)))\n+\n+    x = layers.Input((2,), dtype='float64')\n+    with self.assertRaisesRegex(\n+        ValueError, 'expected dtype=float16, found dtype=.*float64'):\n+      # In TF2, the error is only raised when the model is run\n+      y = layer(x)\n+      model = models.Model(x, y)\n+      model(tf.ones((2, 2)))\n+\n+\n+if __name__ == '__main__':\n+  base_layer_utils.enable_v2_dtype_behavior()\n+  tf.test.main()\n\n@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests mixed precision works correctly with Keras layers and models.\"\"\"\n+\"\"\"Tests keras.Model works properly with mixed precision.\"\"\"\n \n import tensorflow.compat.v2 as tf\n \n@@ -28,7 +28,6 @@ from keras import layers\n from keras import models\n from keras import optimizer_v1\n from keras import testing_utils\n-from keras.engine import base_layer\n from keras.engine import base_layer_utils\n from keras.engine import input_spec\n from keras.engine import sequential\n@@ -42,40 +41,6 @@ from keras.saving import save\n from keras.utils import generic_utils\n \n \n-# Pylint's static analysis incorrectly believes many layers are non-callable, so\n-# we disable the lint error.\n-# pylint: disable=not-callable\n-\n-\n-class MultiplyLayerWithoutAutoCast(mp_test_util.MultiplyLayer):\n-  \"\"\"Same as MultiplyLayer, but does not use AutoCastVariables.\"\"\"\n-\n-  def build(self, _):\n-    dtype = self.dtype\n-    if dtype in ('float16', 'bfloat16'):\n-      dtype = 'float32'\n-    self.v = self.add_weight(\n-        'v', (),\n-        initializer='ones',\n-        dtype=dtype,\n-        experimental_autocast=False,\n-        regularizer=self._regularizer)\n-    self.built = True\n-\n-  def call(self, inputs):\n-    self.assert_input_types(inputs)\n-    assert self.v.dtype in (tf.float32, tf.float64)\n-    return self._multiply(inputs, tf.cast(self.v, inputs.dtype))\n-\n-\n-class MultiplyLayerWithFunction(mp_test_util.MultiplyLayer):\n-  \"\"\"Same as MultiplyLayer, but _multiply is decorated with a tf.function.\"\"\"\n-\n-  @tf.function\n-  def _multiply(self, x, y):\n-    return super(MultiplyLayerWithFunction, self)._multiply(x, y)\n-\n-\n # If called outside any strategy.scope() calls, this will return the default\n # strategy.\n default_strategy_fn = tf.distribute.get_strategy\n@@ -89,14 +54,6 @@ def create_mirrored_strategy():\n     return tf.distribute.MirroredStrategy(['cpu:0'])\n \n \n-def create_central_storage_strategy():\n-  \"\"\"Create a CentralStorageStrategy, using a GPU if it is available.\"\"\"\n-  compute_devices = ['cpu:0', 'gpu:0'] if (\n-      tf.config.list_logical_devices('GPU')) else ['cpu:0']\n-  return tf.distribute.experimental.CentralStorageStrategy(\n-      compute_devices, parameter_device='cpu:0')\n-\n-\n TESTCASES = ({\n     'testcase_name': 'base',\n     'strategy_fn': default_strategy_fn\n@@ -106,362 +63,6 @@ TESTCASES = ({\n })\n \n \n-@combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-class KerasLayerTest(keras_parameterized.TestCase):\n-  \"\"\"Test mixed precision with Keras layers.\"\"\"\n-\n-  @parameterized.named_parameters(*TESTCASES)\n-  def test_mixed_policies_(self, strategy_fn):\n-    strategy = strategy_fn()\n-    for dtype in 'float16', 'bfloat16':\n-      x = tf.constant([1.])\n-      policy_name = 'mixed_' + dtype\n-      with strategy.scope(), policy.policy_scope(policy_name):\n-        layer = mp_test_util.MultiplyLayer(assert_type=dtype)\n-        self.assertEqual(layer.dtype, tf.float32)\n-        self.assertEqual(get_layer_policy.get_layer_policy(layer).name,\n-                         policy_name)\n-        y = layer(x)\n-        self.assertEqual(layer.v.dtype, tf.float32)\n-        self.assertEqual(y.dtype, dtype)\n-        self.assertEqual(layer.dtype_policy.name, policy_name)\n-        self.assertIsInstance(layer.dtype_policy, policy.Policy)\n-        self.assertEqual(layer.compute_dtype, dtype)\n-        self.assertEqual(layer.dtype, tf.float32)\n-        self.assertEqual(layer.variable_dtype, tf.float32)\n-        self.assertEqual(get_layer_policy.get_layer_policy(layer).name,\n-                         policy_name)\n-        self.evaluate(tf.compat.v1.global_variables_initializer())\n-        self.assertEqual(self.evaluate(y), 1.)\n-\n-  def test_layer_with_int_variable(self):\n-    class LayerWithIntVar(base_layer.Layer):\n-\n-      def build(self, _):\n-        self.v = self.add_weight('v', dtype='int32', trainable=False)\n-\n-      def call(self, inputs):\n-        # Only float variables should be autocasted. This will fail if self.v is\n-        # autocasted to float32\n-        return tf.cast(inputs, 'int32') + self.v\n-\n-    x = tf.constant([1.])\n-    layer = LayerWithIntVar(dtype='mixed_float16')\n-    self.assertEqual(layer(x).dtype, 'int32')\n-\n-  @parameterized.named_parameters(*TESTCASES)\n-  def test_layer_with_non_autocast_variable(self, strategy_fn):\n-    x = tf.constant([1.])\n-    with strategy_fn().scope():\n-      with policy.policy_scope('mixed_float16'):\n-        layer = MultiplyLayerWithoutAutoCast(assert_type=tf.float16)\n-        y = layer(x)\n-        self.assertEqual(layer.v.dtype, tf.float32)\n-        self.assertEqual(y.dtype, tf.float16)\n-        self.evaluate(tf.compat.v1.global_variables_initializer())\n-        self.assertEqual(self.evaluate(y), 1.)\n-\n-  @parameterized.named_parameters(*TESTCASES)\n-  def test_layer_calling_tf_function(self, strategy_fn):\n-    x = tf.constant([1.])\n-    with strategy_fn().scope():\n-      with policy.policy_scope('mixed_float16'):\n-        layer = MultiplyLayerWithFunction(assert_type=tf.float16)\n-        y = layer(x)\n-        self.assertEqual(layer.v.dtype, tf.float32)\n-        self.assertEqual(y.dtype, tf.float16)\n-        self.evaluate(tf.compat.v1.global_variables_initializer())\n-        self.assertEqual(self.evaluate(y), 1.)\n-\n-  @parameterized.named_parameters(*TESTCASES)\n-  def test_layer_regularizer_runs_in_var_dtype(self, strategy_fn):\n-    x = tf.constant([1.])\n-    with strategy_fn().scope():\n-      with policy.policy_scope('mixed_float16'):\n-        # Test on MultiplyLayer\n-        layer = mp_test_util.MultiplyLayer(\n-            assert_type=tf.float16,\n-            regularizer=mp_test_util.IdentityRegularizer())\n-        layer(x)\n-        (regularizer_loss,) = layer.losses\n-        self.assertEqual(regularizer_loss.dtype, tf.float32)\n-        self.evaluate(tf.compat.v1.global_variables_initializer())\n-        self.assertEqual(self.evaluate(regularizer_loss), 1.)\n-\n-        # Test on MultiplyLayerWithoutAutoCast\n-        layer = MultiplyLayerWithoutAutoCast(\n-            assert_type=tf.float16,\n-            regularizer=mp_test_util.IdentityRegularizer())\n-        layer(x)\n-        (regularizer_loss,) = layer.losses\n-        self.assertEqual(regularizer_loss.dtype, tf.float32)\n-        self.evaluate(tf.compat.v1.global_variables_initializer())\n-        self.assertEqual(self.evaluate(regularizer_loss), 1.)\n-\n-  @parameterized.named_parameters(*TESTCASES)\n-  def test_passing_policy_to_layer(self, strategy_fn):\n-    x = tf.constant([1.], dtype=tf.float16)\n-    with strategy_fn().scope():\n-      # Passing a Policy to 'dtype' sets the policy for that layer.\n-      layer = mp_test_util.MultiplyLayer(\n-          assert_type=tf.float16, dtype=policy.Policy('mixed_float16'))\n-      # layer.dtype refers to the variable dtype\n-      self.assertEqual(layer.dtype, tf.float32)\n-      layer(x)\n-      self.assertEqual(layer.v.dtype, tf.float32)\n-      with policy.policy_scope('mixed_float16'):\n-        # Passing a Policy to dtype overrides the global Policy\n-        layer = mp_test_util.MultiplyLayer(\n-            assert_type=tf.float64, dtype=policy.Policy('float64'))\n-        self.assertEqual(layer.dtype_policy.name, 'float64')\n-        self.assertIsInstance(layer.dtype_policy, policy.Policy)\n-        self.assertEqual(layer.compute_dtype, tf.float64)\n-        self.assertEqual(layer.dtype, tf.float64)\n-        self.assertEqual(layer.variable_dtype, tf.float64)\n-        self.assertEqual(layer(x).dtype, tf.float64)\n-        self.assertEqual(layer.v.dtype, tf.float64)\n-\n-  @parameterized.named_parameters(*TESTCASES)\n-  def test_gradient(self, strategy_fn):\n-    x = tf.constant([1.])\n-    with strategy_fn().scope() as strategy:\n-      with policy.policy_scope('mixed_float16'):\n-        layer = mp_test_util.MultiplyLayer(assert_type=tf.float16)\n-        # Learning rate is small enough that if applied to a float16 variable,\n-        # the variable will not change. So this tests the learning rate is not\n-        # applied to a float16 value, but instead the float32 variable.\n-        opt = gradient_descent.SGD(2**-14)\n-\n-        def run_fn():\n-          with tf.GradientTape() as tape:\n-            y = layer(x)\n-            # Divide by num_replicas_in_sync, as the effective total loss is the\n-            # sum of each of the replica's losses.\n-            y /= strategy.num_replicas_in_sync\n-\n-          grad = tape.gradient(y, layer.v)\n-          return opt.apply_gradients([(grad, layer.v)])\n-\n-        op = strategy.experimental_run(run_fn)\n-        if not tf.executing_eagerly():\n-          self.evaluate(tf.compat.v1.global_variables_initializer())\n-          self.evaluate(op)\n-        # The gradient with respective to the variable is 1. Since the\n-        # variable is initialized with 1 and the learning rate is 2**-14, the\n-        # new variable value should be: init_val - gradient * learning_rate,\n-        # which is  1 - 1 * 2**-14\n-        self.assertEqual(self.evaluate(layer.v), 1 - 2**-14)\n-\n-  def _test_checkpointing_layer_weights(self, strategy_fn,\n-                                        mixed_prec_when_saving,\n-                                        mixed_prec_when_loading):\n-    # In this test, we potentially save with mixed precision enabled and load\n-    # with mixed precision disabled, or vice versa. This is possible because\n-    # variables are float32 regardless of whether mixed precision is enabled.\n-    save_policy = 'mixed_float16' if mixed_prec_when_saving else 'float32'\n-    load_policy = 'mixed_float16' if mixed_prec_when_loading else 'float32'\n-    save_input_dtype = 'float16' if mixed_prec_when_saving else 'float32'\n-    load_input_dtype = 'float16' if mixed_prec_when_loading else 'float32'\n-\n-    # Create a layer and save a checkpoint.\n-    x = tf.constant([1.])\n-    with strategy_fn().scope():\n-      with policy.policy_scope(save_policy):\n-        layer = mp_test_util.MultiplyLayer(assert_type=save_input_dtype)\n-        layer(x)  # Build layer\n-    layer.set_weights([np.array(100.)])\n-    self.assertEqual(self.evaluate(layer(x)), 100.)\n-    checkpoint = tf.train.Checkpoint(layer=layer)\n-    prefix = os.path.join(self.get_temp_dir(), 'ckpt')\n-    save_path = checkpoint.save(prefix)\n-\n-    # Create a new layer and restore the checkpoint.\n-    x = tf.constant([1.])\n-    with strategy_fn().scope():\n-      with policy.policy_scope(load_policy):\n-        layer = mp_test_util.MultiplyLayer(assert_type=load_input_dtype)\n-        layer(x)  # Build layer\n-    layer.set_weights([np.array(200.)])\n-    self.assertEqual(self.evaluate(layer(x)), 200.)\n-    checkpoint = tf.train.Checkpoint(layer=layer)\n-    checkpoint.restore(save_path).assert_consumed().run_restore_ops()\n-    self.assertEqual(layer.get_weights(), [100.])\n-    self.assertEqual(self.evaluate(layer(x)), 100.)\n-\n-  @parameterized.named_parameters(*TESTCASES)\n-  def test_checkpointing_layer_weights(self, strategy_fn):\n-    with self.test_session():\n-      self._test_checkpointing_layer_weights(\n-          strategy_fn, mixed_prec_when_saving=True,\n-          mixed_prec_when_loading=True)\n-      self._test_checkpointing_layer_weights(\n-          strategy_fn, mixed_prec_when_saving=True,\n-          mixed_prec_when_loading=False)\n-      self._test_checkpointing_layer_weights(\n-          strategy_fn, mixed_prec_when_saving=False,\n-          mixed_prec_when_loading=True)\n-\n-  @parameterized.named_parameters(*TESTCASES)\n-  def test_config(self, strategy_fn):\n-    x = tf.constant([1.], dtype=tf.float16)\n-    with strategy_fn().scope():\n-      for layer, dtype in (\n-          (mp_test_util.MultiplyLayer(), 'float32'),\n-          (mp_test_util.MultiplyLayer(dtype='float64'), 'float64'),\n-          (mp_test_util.MultiplyLayer(dtype=policy.Policy('float64')),\n-           'float64')):\n-        config = layer.get_config()\n-        self.assertEqual(config['dtype'], dtype)\n-        self.assertIsInstance(config['dtype'], str)\n-        layer = mp_test_util.MultiplyLayer.from_config(config)\n-        self.assertEqual(layer.dtype, dtype)\n-        self.assertEqual(layer(x).dtype, dtype)\n-        self.assertEqual(layer.v.dtype, dtype)\n-\n-      layer = mp_test_util.MultiplyLayer(dtype='mixed_float16')\n-      config = layer.get_config()\n-      self.assertEqual(config['dtype'],\n-                       {'class_name': 'Policy',\n-                        'config': {'name': 'mixed_float16'}})\n-      layer = mp_test_util.MultiplyLayer.from_config(config)\n-      self.assertEqual(layer.dtype, 'float32')\n-      self.assertEqual(layer(x).dtype, 'float16')\n-      self.assertEqual(layer.v.dtype, 'float32')\n-      config = layer.get_config()\n-      self.assertEqual(config['dtype'],\n-                       {'class_name': 'Policy',\n-                        'config': {'name': 'mixed_float16'}})\n-\n-      layer = mp_test_util.MultiplyLayer(dtype=policy.Policy('_infer'))\n-      config = layer.get_config()\n-      self.assertIsNone(config['dtype'])\n-      layer = mp_test_util.MultiplyLayer.from_config(config)\n-      # If a layer is serialized with the \"_infer\" policy, when deserialized\n-      # into TF 2 it will have the global policy instead of \"_infer\". This is\n-      # because \"_infer\" is serialized into None, and passing dtype=None in\n-      # TensorFlow 2 indicates to use the global policy.\n-      self.assertEqual(layer.dtype, 'float32')\n-      self.assertEqual(layer(x).dtype, 'float32')\n-      self.assertEqual(layer.v.dtype, 'float32')\n-\n-  @parameterized.named_parameters(*TESTCASES)\n-  def test_config_policy_v1(self, strategy_fn):\n-    x = tf.constant([1.], dtype=tf.float16)\n-    with strategy_fn().scope():\n-\n-      layer = mp_test_util.MultiplyLayer(dtype=policy.PolicyV1('mixed_float16',\n-                                                               loss_scale=None))\n-      config = layer.get_config()\n-      self.assertEqual(config['dtype'],\n-                       {'class_name': 'PolicyV1',\n-                        'config': {'name': 'mixed_float16',\n-                                   'loss_scale': None}})\n-      layer = mp_test_util.MultiplyLayer.from_config(config)\n-      self.assertEqual(layer.dtype, 'float32')\n-      self.assertEqual(layer(x).dtype, 'float16')\n-      self.assertEqual(layer.v.dtype, 'float32')\n-      # Restoring a PolicyV1 silently converts it to a Policy and drops the loss\n-      # scale.\n-      self.assertEqual(type(layer.dtype_policy), policy.Policy)\n-      config = layer.get_config()\n-      # The loss_scale is silently dropped\n-      self.assertEqual(config['dtype'],\n-                       {'class_name': 'Policy',\n-                        'config': {'name': 'mixed_float16'}})\n-\n-      layer = mp_test_util.MultiplyLayer(dtype=policy.PolicyV1('float64',\n-                                                               loss_scale=2.))\n-      config = layer.get_config()\n-      self.assertEqual(config['dtype'],\n-                       {'class_name': 'PolicyV1',\n-                        'config': {'name': 'float64',\n-                                   'loss_scale': {\n-                                       'class_name': 'FixedLossScale',\n-                                       'config': {'loss_scale_value': 2.0}}}})\n-      layer = mp_test_util.MultiplyLayer.from_config(config)\n-      self.assertEqual(layer.dtype, 'float64')\n-      self.assertEqual(layer(x).dtype, 'float64')\n-      self.assertEqual(layer.v.dtype, 'float64')\n-      self.assertEqual(type(layer.dtype_policy), policy.Policy)\n-      config = layer.get_config()\n-      self.assertEqual(config['dtype'], 'float64')\n-\n-      layer = mp_test_util.MultiplyLayer(dtype=policy.PolicyV1('_infer',\n-                                                               loss_scale=2.))\n-      config = layer.get_config()\n-      self.assertEqual(config['dtype'],\n-                       {'class_name': 'PolicyV1',\n-                        'config': {'name': '_infer',\n-                                   'loss_scale': {\n-                                       'class_name': 'FixedLossScale',\n-                                       'config': {'loss_scale_value': 2.0}}}})\n-      layer = mp_test_util.MultiplyLayer.from_config(config)\n-      self.assertEqual(layer.dtype, None)\n-      self.assertEqual(layer(x).dtype, 'float16')\n-      self.assertEqual(layer.v.dtype, 'float16')\n-      self.assertEqual(type(layer.dtype_policy), policy.Policy)\n-      config = layer.get_config()\n-      self.assertEqual(config['dtype'], 'float16')\n-\n-  def test_delete_variable(self):\n-    layer = base_layer.Layer(dtype='mixed_float16')\n-    layer.x = layer.add_weight('x')\n-    self.assertEqual(layer.trainable_weights, [layer.x])\n-    del layer.x\n-    self.assertEqual(layer.trainable_weights, [])\n-\n-  def test_build_and_call_layer_in_function(self):\n-    layer = mp_test_util.MultiplyLayer(dtype=policy.Policy('mixed_float16'))\n-    @tf.function\n-    def f():\n-      return layer(1.)\n-    y = f()\n-    self.evaluate(tf.compat.v1.global_variables_initializer())\n-    self.assertEqual(y.dtype, 'float16')\n-    self.assertEqual(layer.v.dtype, 'float32')\n-    self.assertEqual(self.evaluate(y), 1.)\n-\n-  def test_unsupported_strategy(self):\n-    strategy = create_central_storage_strategy()\n-    with strategy.scope(), self.assertRaisesRegex(\n-        ValueError, 'Mixed precision is not supported with the '\n-        'tf.distribute.Strategy: CentralStorageStrategy. Either '\n-        'stop using mixed precision by removing the use of the '\n-        '\"mixed_float16\" policy or use a different Strategy, e.g. '\n-        'a MirroredStrategy.'):\n-      mp_test_util.MultiplyLayer(dtype='mixed_float16')\n-    # Non-mixed policies are fine\n-    mp_test_util.MultiplyLayer(dtype=policy.Policy('float64'))\n-\n-  def test_input_spec_dtype(self):\n-    # Test the InputSpec's dtype is compared against the inputs before the layer\n-    # casts them, not after.\n-    layer = mp_test_util.MultiplyLayer(dtype='float64')\n-    layer.input_spec = input_spec.InputSpec(dtype='float16')\n-\n-    # Test passing Eager tensors\n-    x = tf.ones((2, 2), dtype='float16')\n-    layer(x)\n-    x = tf.ones((2, 2), dtype='float64')\n-    with self.assertRaisesRegex(\n-        ValueError, 'expected dtype=float16, found dtype=.*float64'):\n-      layer(x)\n-\n-    # Test passing symbolic tensors\n-    x = layers.Input((2,), dtype='float16')\n-    y = layer(x)\n-    model = models.Model(x, y)\n-    model(tf.ones((2, 2)))\n-\n-    x = layers.Input((2,), dtype='float64')\n-    with self.assertRaisesRegex(\n-        ValueError, 'expected dtype=float16, found dtype=.*float64'):\n-      # In TF2, the error is only raised when the model is run\n-      y = layer(x)\n-      model = models.Model(x, y)\n-      model(tf.ones((2, 2)))\n-\n-\n class KerasModelTest(keras_parameterized.TestCase):\n   \"\"\"Test mixed precision with Keras models.\"\"\"\n \n@@ -734,11 +335,11 @@ class KerasModelTest(keras_parameterized.TestCase):\n             assert_type=tf.float16,\n             regularizer=mp_test_util.IdentityRegularizer(),\n             use_operator=True)\n-        layer2 = MultiplyLayerWithoutAutoCast(\n+        layer2 = mp_test_util.MultiplyLayerWithoutAutoCast(\n             assert_type=tf.float16, use_operator=True)\n         layer3 = mp_test_util.MultiplyLayer(assert_type=tf.float16,\n                                             use_operator=False)\n-        layer4 = MultiplyLayerWithoutAutoCast(\n+        layer4 = mp_test_util.MultiplyLayerWithoutAutoCast(\n             assert_type=tf.float16,\n             regularizer=mp_test_util.IdentityRegularizer(),\n             use_operator=False)\n\n@@ -176,6 +176,27 @@ class MultiplyLayer(AssertTypeLayer):\n     return config\n \n \n+class MultiplyLayerWithoutAutoCast(MultiplyLayer):\n+  \"\"\"Same as MultiplyLayer, but does not use AutoCastVariables.\"\"\"\n+\n+  def build(self, _):\n+    dtype = self.dtype\n+    if dtype in ('float16', 'bfloat16'):\n+      dtype = 'float32'\n+    self.v = self.add_weight(\n+        'v', (),\n+        initializer='ones',\n+        dtype=dtype,\n+        experimental_autocast=False,\n+        regularizer=self._regularizer)\n+    self.built = True\n+\n+  def call(self, inputs):\n+    self.assert_input_types(inputs)\n+    assert self.v.dtype in (tf.float32, tf.float64)\n+    return self._multiply(inputs, tf.cast(self.v, inputs.dtype))\n+\n+\n class IdentityRegularizer(regularizers.Regularizer):\n \n   def __call__(self, x):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
