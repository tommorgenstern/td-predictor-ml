{"custom_id": "keras#c2268794ab638dd1c4f85b67b582ceb08acfe3c1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 6 | Hunks: 8 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 20988 | Contributors (this commit): 142 | Commits (past 90d): 86 | Contributors (cumulative): 199 | DMM Complexity: None\n\nDIFF:\n@@ -59,7 +59,7 @@ def normalize(x):\n \n \n kept_filters = []\n-for filter_index in range(0, 200):\n+for filter_index in range(200):\n     # we only scan through the first 200 filters,\n     # but there are actually 512 of them\n     print('Processing filter %d' % filter_index)\n\n@@ -260,7 +260,7 @@ class TextImageGenerator(keras.callbacks.Callback):\n         input_length = np.zeros([size, 1])\n         label_length = np.zeros([size, 1])\n         source_str = []\n-        for i in range(0, size):\n+        for i in range(size):\n             # Mix in some blank inputs.  This seems to be important for\n             # achieving translational invariance\n             if train and i > size - 4:\n@@ -372,7 +372,7 @@ class VizCallback(keras.callbacks.Callback):\n             word_batch = next(self.text_img_gen)[0]\n             num_proc = min(word_batch['the_input'].shape[0], num_left)\n             decoded_res = decode_batch(self.test_func, word_batch['the_input'][0:num_proc])\n-            for j in range(0, num_proc):\n+            for j in range(num_proc):\n                 edit_dist = editdistance.eval(decoded_res[j], word_batch['source_str'][j])\n                 mean_ed += float(edit_dist)\n                 mean_norm_ed += float(edit_dist) / len(word_batch['source_str'][j])\n\n@@ -512,7 +512,7 @@ def dot(x, y):\n         y_shape = int_shape(y)\n         if len(y_shape) > 2:\n             permutation = [len(y_shape) - 2]\n-            permutation += list(range(0, len(y_shape) - 2))\n+            permutation += list(range(len(y_shape) - 2))\n             permutation += [len(y_shape) - 1]\n             y = C.transpose(y, perm=permutation)\n         return C.times(x, y, len(y_shape) - 1)\n\n@@ -3732,11 +3732,11 @@ def ctc_label_dense_to_sparse(labels, label_lengths):\n                                      initializer=init, parallel_iterations=1)\n     dense_mask = dense_mask[:, 0, :]\n \n-    label_array = tf.reshape(tf.tile(tf.range(0, label_shape[1]), num_batches_tns),\n+    label_array = tf.reshape(tf.tile(tf.range(label_shape[1]), num_batches_tns),\n                              label_shape)\n     label_ind = tf.boolean_mask(label_array, dense_mask)\n \n-    batch_array = tf.transpose(tf.reshape(tf.tile(tf.range(0, label_shape[0]),\n+    batch_array = tf.transpose(tf.reshape(tf.tile(tf.range(label_shape[0]),\n                                                   max_num_labels_tns), reverse(label_shape, 0)))\n     batch_ind = tf.boolean_mask(batch_array, dense_mask)\n     indices = tf.transpose(tf.reshape(concatenate([batch_ind, label_ind], axis=0), [2, -1]))\n\n@@ -373,7 +373,7 @@ def _make_batches(size, batch_size):\n     \"\"\"\n     num_batches = int(np.ceil(size / float(batch_size)))\n     return [(i * batch_size, min(size, (i + 1) * batch_size))\n-            for i in range(0, num_batches)]\n+            for i in range(num_batches)]\n \n \n def _slice_arrays(arrays, start=None, stop=None):\n\n@@ -729,7 +729,7 @@ class TestBackend(object):\n         targets = np.random.randint(num_classes, size=batch_size, dtype='int32')\n \n         # (k == 0 or k > num_classes) does not raise an error but just return an unmeaningful tensor.\n-        for k in range(0, num_classes + 1):\n+        for k in range(num_classes + 1):\n             z_list = [b.eval(b.in_top_k(b.variable(predictions, dtype='float32'),\n                                         b.variable(targets, dtype='int32'), k))\n                       for b in [KTH, KTF]]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a45b0704a4ce75b85ae8d58210a816f2791105b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 53 | Files Changed: 4 | Hunks: 52 | Methods Changed: 28 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 106 | Churn Cumulative: 3382 | Contributors (this commit): 28 | Commits (past 90d): 11 | Contributors (cumulative): 38 | DMM Complexity: None\n\nDIFF:\n@@ -14,7 +14,7 @@ from keras.models import model_from_json, model_from_yaml\n \n input_dim = 16\n num_hidden = 8\n-num_class = 4\n+num_classes = 4\n batch_size = 32\n epochs = 1\n \n@@ -59,7 +59,7 @@ def test_merge_sum(in_tmpdir):\n \n     model = Sequential()\n     model.add(Merge([left, right], mode='sum'))\n-    model.add(Dense(num_class))\n+    model.add(Dense(num_classes))\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n@@ -85,7 +85,7 @@ def test_merge_sum(in_tmpdir):\n     right.add(Activation('relu'))\n     model = Sequential()\n     model.add(Merge([left, right], mode='sum'))\n-    model.add(Dense(num_class))\n+    model.add(Dense(num_classes))\n     model.add(Activation('softmax'))\n     model.load_weights(fname)\n     os.remove(fname)\n@@ -120,7 +120,7 @@ def test_merge_dot():\n \n     model = Sequential()\n     model.add(Merge([left, right], mode='dot', dot_axes=1))\n-    model.add(Dense(num_class))\n+    model.add(Dense(num_classes))\n     model.add(Activation('softmax'))\n \n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n@@ -135,7 +135,7 @@ def test_merge_dot():\n \n     model = Sequential()\n     model.add(Merge([left, right], mode='dot', dot_axes=[1, 1]))\n-    model.add(Dense(num_class))\n+    model.add(Dense(num_classes))\n     model.add(Activation('softmax'))\n \n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n@@ -155,7 +155,7 @@ def test_merge_concat(in_tmpdir):\n \n     model = Sequential(name='merged_branches')\n     model.add(Merge([left, right], mode='concat', name='merge'))\n-    model.add(Dense(num_class, name='final_dense'))\n+    model.add(Dense(num_classes, name='final_dense'))\n     model.add(Activation('softmax', name='softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n@@ -203,7 +203,7 @@ def test_merge_recursivity(in_tmpdir):\n \n     model = Sequential()\n     model.add(Merge([intermediate, righter], mode='sum'))\n-    model.add(Dense(num_class))\n+    model.add(Dense(num_classes))\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n@@ -247,7 +247,7 @@ def test_merge_overlap(in_tmpdir):\n \n     model = Sequential()\n     model.add(Merge([left, left], mode='sum'))\n-    model.add(Dense(num_class))\n+    model.add(Dense(num_classes))\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n\n@@ -20,7 +20,7 @@ from keras.utils import np_utils\n \n input_dim = 2\n num_hidden = 4\n-num_class = 2\n+num_classes = 2\n batch_size = 5\n train_samples = 20\n test_samples = 20\n@@ -33,7 +33,7 @@ def test_TerminateOnNaN():\n                                                          num_test=test_samples,\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n-                                                         num_classes=num_class)\n+                                                         num_classes=num_classes)\n \n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n@@ -43,7 +43,7 @@ def test_TerminateOnNaN():\n     for _ in range(5):\n         model.add(Dense(num_hidden, input_dim=input_dim, activation='relu',\n                         kernel_initializer=initializer))\n-    model.add(Dense(num_class, activation='linear'))\n+    model.add(Dense(num_classes, activation='linear'))\n     model.compile(loss='mean_squared_error',\n                   optimizer='rmsprop')\n \n@@ -81,7 +81,7 @@ def test_stop_training_csv(tmpdir):\n                                                          num_test=test_samples,\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n-                                                         num_classes=num_class)\n+                                                         num_classes=num_classes)\n \n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n@@ -89,7 +89,7 @@ def test_stop_training_csv(tmpdir):\n     model = Sequential()\n     for _ in range(5):\n         model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n-    model.add(Dense(num_class, activation='linear'))\n+    model.add(Dense(num_classes, activation='linear'))\n     model.compile(loss='mean_squared_error',\n                   optimizer='rmsprop')\n \n@@ -99,7 +99,7 @@ def test_stop_training_csv(tmpdir):\n         tot = 0\n         while 1:\n             if tot > 3 * len(X_train):\n-                yield np.ones([batch_size, input_dim]) * np.nan, np.ones([batch_size, num_class]) * np.nan\n+                yield np.ones([batch_size, input_dim]) * np.nan, np.ones([batch_size, num_classes]) * np.nan\n             else:\n                 yield (X_train[i * batch_size: (i + 1) * batch_size],\n                        y_train[i * batch_size: (i + 1) * batch_size])\n@@ -133,7 +133,7 @@ def test_ModelCheckpoint(tmpdir):\n                                                          num_test=test_samples,\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n-                                                         num_classes=num_class)\n+                                                         num_classes=num_classes)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n     # case 1\n@@ -143,7 +143,7 @@ def test_ModelCheckpoint(tmpdir):\n \n     model = Sequential()\n     model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n-    model.add(Dense(num_class, activation='softmax'))\n+    model.add(Dense(num_classes, activation='softmax'))\n     model.compile(loss='categorical_crossentropy',\n                   optimizer='rmsprop',\n                   metrics=['accuracy'])\n@@ -209,12 +209,12 @@ def test_EarlyStopping():\n                                                          num_test=test_samples,\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n-                                                         num_classes=num_class)\n+                                                         num_classes=num_classes)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n     model = Sequential()\n     model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n-    model.add(Dense(num_class, activation='softmax'))\n+    model.add(Dense(num_classes, activation='softmax'))\n     model.compile(loss='categorical_crossentropy',\n                   optimizer='rmsprop',\n                   metrics=['accuracy'])\n@@ -288,12 +288,12 @@ def test_LearningRateScheduler():\n                                                          num_test=test_samples,\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n-                                                         num_classes=num_class)\n+                                                         num_classes=num_classes)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n     model = Sequential()\n     model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n-    model.add(Dense(num_class, activation='softmax'))\n+    model.add(Dense(num_classes, activation='softmax'))\n     model.compile(loss='categorical_crossentropy',\n                   optimizer='sgd',\n                   metrics=['accuracy'])\n@@ -311,7 +311,7 @@ def test_ReduceLROnPlateau():\n                                                          num_test=test_samples,\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n-                                                         num_classes=num_class)\n+                                                         num_classes=num_classes)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n \n@@ -319,7 +319,7 @@ def test_ReduceLROnPlateau():\n         np.random.seed(1337)\n         model = Sequential()\n         model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n-        model.add(Dense(num_class, activation='softmax'))\n+        model.add(Dense(num_classes, activation='softmax'))\n \n         model.compile(loss='categorical_crossentropy',\n                       optimizer=optimizers.SGD(lr=0.1),\n@@ -350,7 +350,7 @@ def test_CSVLogger(tmpdir):\n                                                          num_test=test_samples,\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n-                                                         num_classes=num_class)\n+                                                         num_classes=num_classes)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n \n@@ -358,7 +358,7 @@ def test_CSVLogger(tmpdir):\n         np.random.seed(1337)\n         model = Sequential()\n         model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n-        model.add(Dense(num_class, activation='softmax'))\n+        model.add(Dense(num_classes, activation='softmax'))\n \n         model.compile(loss='categorical_crossentropy',\n                       optimizer=optimizers.SGD(lr=0.1),\n@@ -409,7 +409,7 @@ def test_TensorBoard(tmpdir):\n         num_test=test_samples,\n         input_shape=(input_dim,),\n         classification=True,\n-        num_classes=num_class)\n+        num_classes=num_classes)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n \n@@ -433,7 +433,7 @@ def test_TensorBoard(tmpdir):\n     inp = Input((input_dim,))\n     hidden = Dense(num_hidden, activation='relu')(inp)\n     hidden = Dropout(0.1)(hidden)\n-    output = Dense(num_class, activation='softmax')(hidden)\n+    output = Dense(num_classes, activation='softmax')(hidden)\n     model = Model(inputs=inp, outputs=output)\n     model.compile(loss='categorical_crossentropy',\n                   optimizer='sgd',\n@@ -483,7 +483,7 @@ def test_TensorBoard_histogram_freq_must_have_validation_data(tmpdir):\n         num_test=test_samples,\n         input_shape=(input_dim,),\n         classification=True,\n-        num_classes=num_class)\n+        num_classes=num_classes)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n \n@@ -507,7 +507,7 @@ def test_TensorBoard_histogram_freq_must_have_validation_data(tmpdir):\n     inp = Input((input_dim,))\n     hidden = Dense(num_hidden, activation='relu')(inp)\n     hidden = Dropout(0.1)(hidden)\n-    output = Dense(num_class, activation='softmax')(hidden)\n+    output = Dense(num_classes, activation='softmax')(hidden)\n     model = Model(inputs=inp, outputs=output)\n     model.compile(loss='categorical_crossentropy',\n                   optimizer='sgd',\n@@ -557,7 +557,7 @@ def test_TensorBoard_multi_input_output(tmpdir):\n         num_test=test_samples,\n         input_shape=(input_dim,),\n         classification=True,\n-        num_classes=num_class)\n+        num_classes=num_classes)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n \n@@ -583,8 +583,8 @@ def test_TensorBoard_multi_input_output(tmpdir):\n     inp = add([inp1, inp2])\n     hidden = Dense(num_hidden, activation='relu')(inp)\n     hidden = Dropout(0.1)(hidden)\n-    output1 = Dense(num_class, activation='softmax')(hidden)\n-    output2 = Dense(num_class, activation='softmax')(hidden)\n+    output1 = Dense(num_classes, activation='softmax')(hidden)\n+    output2 = Dense(num_classes, activation='softmax')(hidden)\n     model = Model(inputs=[inp1, inp2], outputs=[output1, output2])\n     model.compile(loss='categorical_crossentropy',\n                   optimizer='sgd',\n@@ -672,12 +672,12 @@ def test_CallbackValData():\n                                                          num_test=test_samples,\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n-                                                         num_classes=num_class)\n+                                                         num_classes=num_classes)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n     model = Sequential()\n     model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n-    model.add(Dense(num_class, activation='softmax'))\n+    model.add(Dense(num_classes, activation='softmax'))\n     model.compile(loss='categorical_crossentropy',\n                   optimizer='sgd',\n                   metrics=['accuracy'])\n@@ -721,12 +721,12 @@ def test_LambdaCallback():\n                                                          num_test=test_samples,\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n-                                                         num_classes=num_class)\n+                                                         num_classes=num_classes)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n     model = Sequential()\n     model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n-    model.add(Dense(num_class, activation='softmax'))\n+    model.add(Dense(num_classes, activation='softmax'))\n     model.compile(loss='categorical_crossentropy',\n                   optimizer='sgd',\n                   metrics=['accuracy'])\n@@ -759,13 +759,13 @@ def test_TensorBoard_with_ReduceLROnPlateau(tmpdir):\n                                                          num_test=test_samples,\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n-                                                         num_classes=num_class)\n+                                                         num_classes=num_classes)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n \n     model = Sequential()\n     model.add(Dense(num_hidden, input_dim=input_dim, activation='relu'))\n-    model.add(Dense(num_class, activation='softmax'))\n+    model.add(Dense(num_classes, activation='softmax'))\n     model.compile(loss='binary_crossentropy',\n                   optimizer='sgd',\n                   metrics=['accuracy'])\n\n@@ -17,7 +17,7 @@ from keras.engine.training import _make_batches\n \n input_dim = 16\n num_hidden = 8\n-num_class = 4\n+num_classes = 4\n batch_size = 32\n epochs = 1\n \n@@ -37,10 +37,10 @@ def in_tmpdir(tmpdir):\n def test_sequential_pop():\n     model = Sequential()\n     model.add(Dense(num_hidden, input_dim=input_dim))\n-    model.add(Dense(num_class))\n+    model.add(Dense(num_classes))\n     model.compile(loss='mse', optimizer='sgd')\n     x = np.random.random((batch_size, input_dim))\n-    y = np.random.random((batch_size, num_class))\n+    y = np.random.random((batch_size, num_classes))\n     model.fit(x, y, epochs=1)\n     model.pop()\n     assert len(model.layers) == 1\n@@ -87,9 +87,9 @@ def test_sequential_fit_generator():\n     model = Sequential()\n     model.add(Dense(num_hidden, input_shape=(input_dim,)))\n     model.add(Activation('relu'))\n-    model.add(Dense(num_class))\n+    model.add(Dense(num_classes))\n     model.pop()\n-    model.add(Dense(num_class))\n+    model.add(Dense(num_classes))\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n@@ -121,7 +121,7 @@ def test_sequential(in_tmpdir):\n     model = Sequential()\n     model.add(Dense(num_hidden, input_shape=(input_dim,)))\n     model.add(Activation('relu'))\n-    model.add(Dense(num_class))\n+    model.add(Dense(num_classes))\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n \n@@ -150,7 +150,7 @@ def test_sequential(in_tmpdir):\n     model = Sequential()\n     model.add(Dense(num_hidden, input_shape=(input_dim,)))\n     model.add(Activation('relu'))\n-    model.add(Dense(num_class))\n+    model.add(Dense(num_classes))\n     model.add(Activation('softmax'))\n     model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n     model.load_weights(fname)\n@@ -178,7 +178,7 @@ def test_nested_sequential(in_tmpdir):\n     inner = Sequential()\n     inner.add(Dense(num_hidden, input_shape=(input_dim,)))\n     inner.add(Activation('relu'))\n-    inner.add(Dense(num_class))\n+    inner.add(Dense(num_classes))\n \n     middle = Sequential()\n     middle.add(inner)\n@@ -207,7 +207,7 @@ def test_nested_sequential(in_tmpdir):\n     inner = Sequential()\n     inner.add(Dense(num_hidden, input_shape=(input_dim,)))\n     inner.add(Activation('relu'))\n-    inner.add(Dense(num_class))\n+    inner.add(Dense(num_classes))\n \n     middle = Sequential()\n     middle.add(inner)\n\n@@ -11,7 +11,7 @@ input_dim = 5\n hidden_dims = 5\n num_train = 100\n num_test = 50\n-num_class = 3\n+num_classes = 3\n batch_size = 32\n epochs = 1\n verbosity = 0\n@@ -21,7 +21,7 @@ loss = 'categorical_crossentropy'\n np.random.seed(42)\n (X_train, y_train), (X_test, y_test) = get_test_data(\n     num_train=num_train, num_test=num_test, input_shape=(input_dim,),\n-    classification=True, num_classes=num_class)\n+    classification=True, num_classes=num_classes)\n \n \n def build_fn_clf(hidden_dims):\n@@ -30,7 +30,7 @@ def build_fn_clf(hidden_dims):\n     model.add(Activation('relu'))\n     model.add(Dense(hidden_dims))\n     model.add(Activation('relu'))\n-    model.add(Dense(num_class))\n+    model.add(Dense(num_classes))\n     model.add(Activation('softmax'))\n     model.compile(optimizer='sgd', loss='categorical_crossentropy',\n                   metrics=['accuracy'])\n@@ -83,15 +83,15 @@ def assert_classification_works(clf):\n     preds = clf.predict(X_test, batch_size=batch_size)\n     assert preds.shape == (num_test, )\n     for prediction in np.unique(preds):\n-        assert prediction in range(num_class)\n+        assert prediction in range(num_classes)\n \n     proba = clf.predict_proba(X_test, batch_size=batch_size)\n-    assert proba.shape == (num_test, num_class)\n+    assert proba.shape == (num_test, num_classes)\n     assert np.allclose(np.sum(proba, axis=1), np.ones(num_test))\n \n \n def assert_string_classification_works(clf):\n-    string_classes = ['cls{}'.format(x) for x in range(num_class)]\n+    string_classes = ['cls{}'.format(x) for x in range(num_classes)]\n     str_y_train = np.array(string_classes)[y_train]\n \n     clf.fit(X_train, str_y_train, batch_size=batch_size, epochs=epochs)\n@@ -105,7 +105,7 @@ def assert_string_classification_works(clf):\n         assert prediction in string_classes\n \n     proba = clf.predict_proba(X_test, batch_size=batch_size)\n-    assert proba.shape == (num_test, num_class)\n+    assert proba.shape == (num_test, num_classes)\n     assert np.allclose(np.sum(proba, axis=1), np.ones(num_test))\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b9bda3700738e736c000e6ba6d33055ca99726e6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 21 | Churn Cumulative: 1283 | Contributors (this commit): 17 | Commits (past 90d): 7 | Contributors (cumulative): 17 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -96,6 +96,8 @@ EXCLUDE = {\n     'serialize',\n     'deserialize',\n     'get',\n+    'set_image_dim_ordering',\n+    'image_dim_ordering',\n }\n \n PAGES = [\n@@ -343,7 +345,11 @@ def get_function_signature(function, method=True):\n         signature = st[:-2] + ')'\n     else:\n         signature = st + ')'\n-    return signature\n+\n+    if not method:\n+        # Prepend the module name.\n+        signature = function.__module__ + '.' + signature\n+    return post_process_signature(signature)\n \n \n def get_class_signature(cls):\n@@ -354,14 +360,19 @@ def get_class_signature(cls):\n         # in case the class inherits from object and does not\n         # define __init__\n         class_signature = cls.__module__ + '.' + cls.__name__ + '()'\n+    return post_process_signature(class_signature)\n \n-    parts = class_signature.split('.')\n+\n+def post_process_signature(signature):\n+    parts = signature.split('.')\n     if len(parts) >= 4:\n         if parts[1] == 'layers':\n-            class_signature = 'keras.layers.' + '.'.join(parts[3:])\n+            signature = 'keras.layers.' + '.'.join(parts[3:])\n         if parts[1] == 'utils':\n-            class_signature = 'keras.utils.' + '.'.join(parts[3:])\n-    return class_signature\n+            signature = 'keras.utils.' + '.'.join(parts[3:])\n+        if parts[1] == 'backend':\n+            signature = 'keras.backend.' + '.'.join(parts[3:])\n+    return signature\n \n \n def class_to_docs_link(cls):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#16e869ae06a72418bc5acaa5fa13b94b94d83e2d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 15 | Files Changed: 5 | Hunks: 16 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 34 | Churn Cumulative: 3620 | Contributors (this commit): 28 | Commits (past 90d): 16 | Contributors (cumulative): 41 | DMM Complexity: None\n\nDIFF:\n@@ -7,6 +7,8 @@ from keras import layers\n import keras\n from keras.utils.np_utils import to_categorical\n \n+num_classes = 2\n+\n \n @keras_test\n def test_vector_classification():\n@@ -18,7 +20,7 @@ def test_vector_classification():\n                                                          num_test=200,\n                                                          input_shape=(20,),\n                                                          classification=True,\n-                                                         num_classes=2)\n+                                                         num_classes=num_classes)\n     y_train = to_categorical(y_train)\n     y_test = to_categorical(y_test)\n \n@@ -27,7 +29,7 @@ def test_vector_classification():\n         layers.Dense(16, input_shape=(x_train.shape[-1],), activation='relu'),\n         layers.Dense(8),\n         layers.Activation('relu'),\n-        layers.Dense(y_train.shape[-1], activation='softmax')\n+        layers.Dense(num_classes, activation='softmax')\n     ])\n     model.compile(loss='categorical_crossentropy',\n                   optimizer='rmsprop',\n@@ -47,13 +49,13 @@ def test_vector_classification_functional():\n                                                          num_test=200,\n                                                          input_shape=(20,),\n                                                          classification=True,\n-                                                         num_classes=2)\n+                                                         num_classes=num_classes)\n     # Test with functional API\n     inputs = layers.Input(shape=(x_train.shape[-1],))\n     x = layers.Dense(16, activation=keras.activations.relu)(inputs)\n     x = layers.Dense(8)(x)\n     x = layers.Activation('relu')(x)\n-    outputs = layers.Dense(y_train.shape[-1], activation='softmax')(x)\n+    outputs = layers.Dense(num_classes, activation='softmax')(x)\n     model = keras.models.Model(inputs, outputs)\n     model.compile(loss=keras.losses.sparse_categorical_crossentropy,\n                   optimizer=keras.optimizers.RMSprop(),\n@@ -73,12 +75,12 @@ def test_vector_regression():\n     (x_train, y_train), (x_test, y_test) = get_test_data(num_train=500,\n                                                          num_test=200,\n                                                          input_shape=(20,),\n-                                                         output_shape=(2,),\n+                                                         output_shape=(num_classes,),\n                                                          classification=False)\n \n     model = Sequential([\n         layers.Dense(16, input_shape=(x_train.shape[-1],), activation='tanh'),\n-        layers.Dense(y_train.shape[-1])\n+        layers.Dense(num_classes)\n     ])\n \n     model.compile(loss='hinge', optimizer='adagrad')\n\n@@ -40,7 +40,7 @@ def _get_test_data():\n                                                          num_test=test_samples,\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n-                                                         num_classes=4)\n+                                                         num_classes=num_classes)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n     return (x_train, y_train), (x_test, y_test)\n\n@@ -11,6 +11,8 @@ from keras.utils.test_utils import keras_test\n from keras.utils.np_utils import to_categorical\n from keras import backend as K\n \n+num_classes = 2\n+\n \n def get_test_data():\n     np.random.seed(1337)\n@@ -18,7 +20,7 @@ def get_test_data():\n                                                      num_test=200,\n                                                      input_shape=(10,),\n                                                      classification=True,\n-                                                     num_classes=2)\n+                                                     num_classes=num_classes)\n     y_train = to_categorical(y_train)\n     return x_train, y_train\n \n@@ -123,9 +125,9 @@ def test_tfoptimizer():\n     from tensorflow import train\n     optimizer = optimizers.TFOptimizer(train.AdamOptimizer())\n     model = Sequential()\n-    model.add(Dense(2, input_shape=(3,), kernel_constraint=constraints.MaxNorm(1)))\n+    model.add(Dense(num_classes, input_shape=(3,), kernel_constraint=constraints.MaxNorm(1)))\n     model.compile(loss='mean_squared_error', optimizer=optimizer)\n-    model.fit(np.random.random((5, 3)), np.random.random((5, 2)),\n+    model.fit(np.random.random((5, 3)), np.random.random((5, num_classes)),\n               epochs=1, batch_size=5, verbose=0)\n     # not supported\n     with pytest.raises(NotImplementedError):\n\n@@ -634,7 +634,7 @@ def test_TensorBoard_convnet(tmpdir):\n                                                          num_test=200,\n                                                          input_shape=input_shape,\n                                                          classification=True,\n-                                                         num_classes=4)\n+                                                         num_classes=num_classes)\n     y_train = np_utils.to_categorical(y_train)\n     y_test = np_utils.to_categorical(y_test)\n \n@@ -646,7 +646,7 @@ def test_TensorBoard_convnet(tmpdir):\n         Conv2D(filters=4, kernel_size=(3, 3),\n                activation='relu', padding='same'),\n         GlobalAveragePooling2D(),\n-        Dense(y_test.shape[-1], activation='softmax')\n+        Dense(num_classes, activation='softmax')\n     ])\n     model.compile(loss='categorical_crossentropy',\n                   optimizer='rmsprop',\n\n@@ -60,7 +60,7 @@ def _get_test_data():\n                                                          num_test=test_samples,\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n-                                                         num_classes=4)\n+                                                         num_classes=num_classes)\n     y_test = np_utils.to_categorical(y_test)\n     y_train = np_utils.to_categorical(y_train)\n     return (x_train, y_train), (x_test, y_test)\n@@ -276,8 +276,8 @@ def test_clone_functional_model():\n \n     input_a = keras.Input(shape=(4,))\n     input_b = keras.Input(shape=(4,))\n-    dense_1 = keras.layers.Dense(4,)\n-    dense_2 = keras.layers.Dense(4,)\n+    dense_1 = keras.layers.Dense(4)\n+    dense_2 = keras.layers.Dense(4)\n \n     x_a = dense_1(input_a)\n     x_a = keras.layers.Dropout(0.5)(x_a)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#46ebf30671b4e0f3fe8daf1e5a0694d6f99784b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 832 | Contributors (this commit): 21 | Commits (past 90d): 6 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -289,7 +289,7 @@ class Progbar(object):\n         self.seen_so_far = current\n \n         now = time.time()\n-        info = ' - %ds' % (now - self.start)\n+        info = ' - %.0fs' % (now - self.start)\n         if self.verbose == 1:\n             if (not force and (now - self.last_update) < self.interval and\n                     current < self.target):\n@@ -325,7 +325,7 @@ class Progbar(object):\n                 time_per_unit = 0\n             if self.target is not None and current <= self.target:\n                 eta = time_per_unit * (self.target - current)\n-                info = ' - ETA: %ds' % eta\n+                info = ' - ETA: %0.fs' % eta\n             for k in self.unique_values:\n                 info += ' - %s:' % k\n                 if isinstance(self.sum_values[k], list):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#decf35bcec7d6b147d338c5a49cd40047b634d97", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 416 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -3,10 +3,10 @@\n \n Model naming and structure follows TF-slim implementation (which has some additional\n layers and different number of filters from the original arXiv paper):\n-https://github.com/tensorflow/models/blob/master/slim/nets/inception_resnet_v2.py\n+https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_resnet_v2.py\n \n Pre-trained ImageNet weights are also converted from TF-slim, which can be found in:\n-https://github.com/tensorflow/models/tree/master/slim#pre-trained-models\n+https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models\n \n # Reference\n - [Inception-v4, Inception-ResNet and the Impact of\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#cc3328a7039ba126e1eee6feda8c075e553be133", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1289 | Contributors (this commit): 17 | Commits (past 90d): 8 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -281,7 +281,11 @@ PAGES = [\n     },\n     {\n         'page': 'utils.md',\n-        'all_module_functions': [utils],\n+        'functions': [utils.to_categorical,\n+                      utils.normalize,\n+                      utils.get_file,\n+                      utils.plot_model,\n+                      utils.multi_gpu_model],\n         'classes': [utils.CustomObjectScope,\n                     utils.HDF5Matrix,\n                     utils.Sequence],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1ea9fed96f48ff93a7bdffe3c02b636e08491645", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 8 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 37 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -17,11 +17,12 @@ import numpy as np\n import random\n from keras.datasets import mnist\n from keras.models import Model\n-from keras.layers import Dense, Dropout, Input, Lambda\n+from keras.layers import Input, Flatten, Dense, Dropout, Lambda\n from keras.optimizers import RMSprop\n from keras import backend as K\n \n num_classes = 10\n+epochs = 20\n \n \n def euclidean_distance(vects):\n@@ -62,11 +63,12 @@ def create_pairs(x, digit_indices):\n     return np.array(pairs), np.array(labels)\n \n \n-def create_base_network(input_dim):\n+def create_base_network(input_shape):\n     '''Base network to be shared (eq. to feature extraction).\n     '''\n-    input = Input(shape=(input_dim,))\n-    x = Dense(128, activation='relu')(input)\n+    input = Input(shape=input_shape)\n+    x = Flatten()(input)\n+    x = Dense(128, activation='relu')(x)\n     x = Dropout(0.1)(x)\n     x = Dense(128, activation='relu')(x)\n     x = Dropout(0.1)(x)\n@@ -89,14 +91,11 @@ def accuracy(y_true, y_pred):\n \n # the data, shuffled and split between train and test sets\n (x_train, y_train), (x_test, y_test) = mnist.load_data()\n-x_train = x_train.reshape(60000, 784)\n-x_test = x_test.reshape(10000, 784)\n x_train = x_train.astype('float32')\n x_test = x_test.astype('float32')\n x_train /= 255\n x_test /= 255\n-input_dim = 784\n-epochs = 20\n+input_shape = x_train.shape[1:]\n \n # create training+test positive and negative pairs\n digit_indices = [np.where(y_train == i)[0] for i in range(num_classes)]\n@@ -106,10 +105,10 @@ digit_indices = [np.where(y_test == i)[0] for i in range(num_classes)]\n te_pairs, te_y = create_pairs(x_test, digit_indices)\n \n # network definition\n-base_network = create_base_network(input_dim)\n+base_network = create_base_network(input_shape)\n \n-input_a = Input(shape=(input_dim,))\n-input_b = Input(shape=(input_dim,))\n+input_a = Input(shape=input_shape)\n+input_b = Input(shape=input_shape)\n \n # because we re-use the same instance `base_network`,\n # the weights of the network\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6b106ab4ec9a1c0eb3e24ae590ce63f84022ad40", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 104 | Lines Deleted: 182 | Files Changed: 2 | Hunks: 42 | Methods Changed: 8 | Complexity Δ (Sum/Max): -15/0 | Churn Δ: 286 | Churn Cumulative: 11900 | Contributors (this commit): 94 | Commits (past 90d): 46 | Contributors (cumulative): 109 | DMM Complexity: 0.0\n\nDIFF:\n@@ -45,9 +45,6 @@ _GRAPH_UID_DICTS = {}\n # Change its value via `manual_variable_initialization(value)`.\n _MANUAL_VAR_INIT = False\n \n-# This map is for converting the keras data format string to that of TF.\n-_DATA_FORMAT_MAP = {'channels_first': 'NCHW', 'channels_last': 'NHWC'}\n-\n # This list queries the devices.\n # We assume our devices don't change during our lifetime.\n _LOCAL_DEVICES = device_lib.list_local_devices()\n@@ -3043,26 +3040,6 @@ def _preprocess_deconv3d_output_shape(x, shape, data_format):\n     return shape\n \n \n-def _preprocess_deconv_output_shape(x, shape, data_format):\n-    \"\"\"Get the output_shape for the deconvolution.\n-\n-    # Arguments\n-        x: input tensor.\n-        shape: output shape.\n-        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n-\n-    # Returns\n-        The output shape.\n-    \"\"\"\n-    if data_format == 'channels_first':\n-        shape = (shape[0], shape[2], shape[3], shape[1])\n-\n-    if shape[0] is None:\n-        shape = (tf.shape(x)[0], ) + tuple(shape[1:])\n-        shape = tf.stack(list(shape))\n-    return shape\n-\n-\n def _preprocess_conv2d_input(x, data_format):\n     \"\"\"Transpose and cast the input before the conv2d.\n \n@@ -3075,13 +3052,13 @@ def _preprocess_conv2d_input(x, data_format):\n     \"\"\"\n     if dtype(x) == 'float64':\n         x = tf.cast(x, 'float32')\n+    tf_data_format = 'NHWC'\n     if data_format == 'channels_first':\n-        # TF uses the last dimension as channel dimension,\n-        # instead of the 2nd one.\n-        # TH input shape: (samples, input_depth, rows, cols)\n-        # TF input shape: (samples, rows, cols, input_depth)\n-        x = tf.transpose(x, (0, 2, 3, 1))\n-    return x\n+        if not _has_nchw_support():\n+            x = tf.transpose(x, (0, 2, 3, 1))  # NCHW -> NHWC\n+        else:\n+            tf_data_format = 'NCHW'\n+    return x, tf_data_format\n \n \n def _preprocess_conv3d_input(x, data_format):\n@@ -3096,43 +3073,13 @@ def _preprocess_conv3d_input(x, data_format):\n     \"\"\"\n     if dtype(x) == 'float64':\n         x = tf.cast(x, 'float32')\n+    tf_data_format = 'NDHWC'\n     if data_format == 'channels_first':\n+        if not _has_nchw_support():\n             x = tf.transpose(x, (0, 2, 3, 4, 1))\n-    return x\n-\n-\n-def _preprocess_conv2d_kernel(kernel, data_format):\n-    \"\"\"Transpose and cast the kernel before the conv2d.\n-\n-    # Arguments\n-        kernel: kernel tensor.\n-        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n-\n-    # Returns\n-        A tensor.\n-    \"\"\"\n-    if dtype(kernel) == 'float64':\n-        kernel = tf.cast(kernel, 'float32')\n-    if data_format == 'channels_first':\n-        kernel = tf.transpose(kernel, (2, 3, 1, 0))\n-    return kernel\n-\n-\n-def _preprocess_conv3d_kernel(kernel, data_format):\n-    \"\"\"Transpose and cast the kernel before the conv3d.\n-\n-    # Arguments\n-        kernel: kernel tensor.\n-        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n-\n-    # Returns\n-        A tensor.\n-    \"\"\"\n-    if dtype(kernel) == 'float64':\n-        kernel = tf.cast(kernel, 'float32')\n-    if data_format == 'channels_first':\n-        kernel = tf.transpose(kernel, (2, 3, 4, 1, 0))\n-    return kernel\n+        else:\n+            tf_data_format = 'NCDHW'\n+    return x, tf_data_format\n \n \n def _preprocess_padding(padding):\n@@ -3156,43 +3103,6 @@ def _preprocess_padding(padding):\n     return padding\n \n \n-def _postprocess_conv2d_output(x, data_format):\n-    \"\"\"Transpose and cast the output from conv2d if needed.\n-\n-    # Arguments\n-        x: A tensor.\n-        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n-\n-    # Returns\n-        A tensor.\n-    \"\"\"\n-\n-    if data_format == 'channels_first':\n-        x = tf.transpose(x, (0, 3, 1, 2))\n-\n-    if floatx() == 'float64':\n-        x = tf.cast(x, 'float64')\n-    return x\n-\n-\n-def _postprocess_conv3d_output(x, data_format):\n-    \"\"\"Transpose and cast the output from conv3d if needed.\n-\n-    # Arguments\n-        x: A tensor.\n-        data_format: string, `\"channels_last\"` or `\"channels_first\"`.\n-\n-    # Returns\n-        A tensor.\n-    \"\"\"\n-    if data_format == 'channels_first':\n-        x = tf.transpose(x, (0, 4, 1, 2, 3))\n-\n-    if floatx() == 'float64':\n-        x = tf.cast(x, 'float64')\n-    return x\n-\n-\n def conv1d(x, kernel, strides=1, padding='valid',\n            data_format=None, dilation_rate=1):\n     \"\"\"1D convolution.\n@@ -3251,16 +3161,10 @@ def conv2d(x, kernel, strides=(1, 1), padding='valid',\n     \"\"\"\n     if data_format is None:\n         data_format = image_data_format()\n-    if data_format not in _DATA_FORMAT_MAP:\n+    if data_format not in {'channels_first', 'channels_last'}:\n         raise ValueError('Unknown data_format ' + str(data_format))\n \n-    tf_data_format = _DATA_FORMAT_MAP[data_format]\n-\n-    nhwc_roundtrip = not _has_nchw_support() and tf_data_format == 'NCHW'\n-\n-    if nhwc_roundtrip:\n-        tf_data_format = 'NHWC'\n-        x = tf.transpose(x, (0, 2, 3, 1))  # NCHW -> NHWC\n+    x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n \n     padding = _preprocess_padding(padding)\n     x = tf.nn.convolution(\n@@ -3271,9 +3175,8 @@ def conv2d(x, kernel, strides=(1, 1), padding='valid',\n         padding=padding,\n         data_format=tf_data_format)\n \n-    if nhwc_roundtrip:\n-        x = tf.transpose(x, (0, 3, 1, 2))  # NCHW -> NHWC\n-\n+    if data_format == 'channels_first' and tf_data_format == 'NHWC':\n+        x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n     return x\n \n \n@@ -3304,14 +3207,28 @@ def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n     if isinstance(output_shape, (tuple, list)):\n         output_shape = tf.stack(output_shape)\n \n-    x = _preprocess_conv2d_input(x, data_format)\n-    output_shape = _preprocess_deconv_output_shape(x, output_shape, data_format)\n+    x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n+\n+    if data_format == 'channels_first' and tf_data_format == 'NHWC':\n+        output_shape = (output_shape[0],\n+                        output_shape[2],\n+                        output_shape[3],\n+                        output_shape[1])\n+    if output_shape[0] is None:\n+        output_shape = (tf.shape(x)[0],) + tuple(output_shape[1:])\n+        output_shape = tf.stack(list(output_shape))\n+\n     padding = _preprocess_padding(padding)\n+    if tf_data_format == 'NHWC':\n         strides = (1,) + strides + (1,)\n+    else:\n+        strides = (1, 1) + strides\n \n     x = tf.nn.conv2d_transpose(x, kernel, output_shape, strides,\n-                               padding=padding)\n-    x = _postprocess_conv2d_output(x, data_format)\n+                               padding=padding,\n+                               data_format=tf_data_format)\n+    if data_format == 'channels_first' and tf_data_format == 'NHWC':\n+        x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n     return x\n \n \n@@ -3340,15 +3257,21 @@ def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n     if data_format not in {'channels_first', 'channels_last'}:\n         raise ValueError('Unknown data_format ' + str(data_format))\n \n-    x = _preprocess_conv2d_input(x, data_format)\n+    x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n     padding = _preprocess_padding(padding)\n+    if tf_data_format == 'NHWC':\n         strides = (1,) + strides + (1,)\n+    else:\n+        strides = (1, 1) + strides\n \n     x = tf.nn.separable_conv2d(x, depthwise_kernel, pointwise_kernel,\n                                strides=strides,\n                                padding=padding,\n-                               rate=dilation_rate)\n-    return _postprocess_conv2d_output(x, data_format)\n+                               rate=dilation_rate,\n+                               data_format=tf_data_format)\n+    if data_format == 'channels_first' and tf_data_format == 'NHWC':\n+        x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n+    return x\n \n \n def depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n@@ -3375,15 +3298,21 @@ def depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n     if data_format not in {'channels_first', 'channels_last'}:\n         raise ValueError('Unknown data_format ' + str(data_format))\n \n-    x = _preprocess_conv2d_input(x, data_format)\n+    x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n     padding = _preprocess_padding(padding)\n+    if tf_data_format == 'NHWC':\n         strides = (1,) + strides + (1,)\n+    else:\n+        strides = (1, 1) + strides\n \n     x = tf.nn.depthwise_conv2d(x, depthwise_kernel,\n                                strides=strides,\n                                padding=padding,\n-                               rate=dilation_rate)\n-    return _postprocess_conv2d_output(x, data_format)\n+                               rate=dilation_rate,\n+                               data_format=tf_data_format)\n+    if data_format == 'channels_first' and tf_data_format == 'NHWC':\n+        x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n+    return x\n \n \n def conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n@@ -3411,10 +3340,7 @@ def conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n     if data_format not in {'channels_first', 'channels_last'}:\n         raise ValueError('Unknown data_format ' + str(data_format))\n \n-    # With 5d inputs, tf.nn.convolution only supports\n-    # data_format NDHWC, so we transpose the inputs\n-    # in case we are in data_format channels_first.\n-    x = _preprocess_conv3d_input(x, data_format)\n+    x, tf_data_format = _preprocess_conv3d_input(x, data_format)\n     padding = _preprocess_padding(padding)\n     x = tf.nn.convolution(\n         input=x,\n@@ -3422,8 +3348,10 @@ def conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n         dilation_rate=dilation_rate,\n         strides=strides,\n         padding=padding,\n-        data_format='NDHWC')\n-    return _postprocess_conv3d_output(x, data_format)\n+        data_format=tf_data_format)\n+    if data_format == 'channels_first' and tf_data_format == 'NDHWC':\n+        x = tf.transpose(x, (0, 4, 1, 2, 3))\n+    return x\n \n \n def conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n@@ -3453,14 +3381,30 @@ def conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n     if isinstance(output_shape, (tuple, list)):\n         output_shape = tf.stack(output_shape)\n \n-    x = _preprocess_conv3d_input(x, data_format)\n-    output_shape = _preprocess_deconv3d_output_shape(x, output_shape, data_format)\n+    x, tf_data_format = _preprocess_conv3d_input(x, data_format)\n+\n+    if data_format == 'channels_first' and tf_data_format == 'NDHWC':\n+        output_shape = (output_shape[0],\n+                        output_shape[2],\n+                        output_shape[3],\n+                        output_shape[4],\n+                        output_shape[1])\n+    if output_shape[0] is None:\n+        output_shape = (tf.shape(x)[0],) + tuple(output_shape[1:])\n+        output_shape = tf.stack(list(output_shape))\n+\n     padding = _preprocess_padding(padding)\n+    if tf_data_format == 'NDHWC':\n         strides = (1,) + strides + (1,)\n+    else:\n+        strides = (1, 1) + strides\n \n     x = tf.nn.conv3d_transpose(x, kernel, output_shape, strides,\n-                               padding=padding)\n-    return _postprocess_conv3d_output(x, data_format)\n+                               padding=padding,\n+                               data_format=tf_data_format)\n+    if data_format == 'channels_first' and tf_data_format == 'NDHWC':\n+        x = tf.transpose(x, (0, 4, 1, 2, 3))\n+    return x\n \n \n def pool2d(x, pool_size, strides=(1, 1),\n@@ -3488,20 +3432,29 @@ def pool2d(x, pool_size, strides=(1, 1),\n     if data_format not in {'channels_first', 'channels_last'}:\n         raise ValueError('Unknown data_format ' + str(data_format))\n \n+    x, tf_data_format = _preprocess_conv2d_input(x, data_format)\n     padding = _preprocess_padding(padding)\n+    if tf_data_format == 'NHWC':\n         strides = (1,) + strides + (1,)\n         pool_size = (1,) + pool_size + (1,)\n-\n-    x = _preprocess_conv2d_input(x, data_format)\n+    else:\n+        strides = (1, 1) + strides\n+        pool_size = (1, 1) + pool_size\n \n     if pool_mode == 'max':\n-        x = tf.nn.max_pool(x, pool_size, strides, padding=padding)\n+        x = tf.nn.max_pool(x, pool_size, strides,\n+                           padding=padding,\n+                           data_format=tf_data_format)\n     elif pool_mode == 'avg':\n-        x = tf.nn.avg_pool(x, pool_size, strides, padding=padding)\n+        x = tf.nn.avg_pool(x, pool_size, strides,\n+                           padding=padding,\n+                           data_format=tf_data_format)\n     else:\n         raise ValueError('Invalid pooling mode:', pool_mode)\n \n-    return _postprocess_conv2d_output(x, data_format)\n+    if data_format == 'channels_first' and tf_data_format == 'NHWC':\n+        x = tf.transpose(x, (0, 3, 1, 2))  # NHWC -> NCHW\n+    return x\n \n \n def pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n@@ -3528,20 +3481,29 @@ def pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n     if data_format not in {'channels_first', 'channels_last'}:\n         raise ValueError('Unknown data_format ' + str(data_format))\n \n+    x, tf_data_format = _preprocess_conv3d_input(x, data_format)\n     padding = _preprocess_padding(padding)\n+    if tf_data_format == 'NDHWC':\n         strides = (1,) + strides + (1,)\n         pool_size = (1,) + pool_size + (1,)\n-\n-    x = _preprocess_conv3d_input(x, data_format)\n+    else:\n+        strides = (1, 1) + strides\n+        pool_size = (1, 1) + pool_size\n \n     if pool_mode == 'max':\n-        x = tf.nn.max_pool3d(x, pool_size, strides, padding=padding)\n+        x = tf.nn.max_pool3d(x, pool_size, strides,\n+                             padding=padding,\n+                             data_format=tf_data_format)\n     elif pool_mode == 'avg':\n-        x = tf.nn.avg_pool3d(x, pool_size, strides, padding=padding)\n+        x = tf.nn.avg_pool3d(x, pool_size, strides,\n+                             padding=padding,\n+                             data_format=tf_data_format)\n     else:\n         raise ValueError('Invalid pooling mode:', pool_mode)\n \n-    return _postprocess_conv3d_output(x, data_format)\n+    if data_format == 'channels_first' and tf_data_format == 'NDHWC':\n+        x = tf.transpose(x, (0, 4, 1, 2, 3))\n+    return x\n \n \n def bias_add(x, bias, data_format=None):\n\n@@ -880,46 +880,6 @@ class TestBackend(object):\n             assert np.max(rand) == 1\n             assert np.min(rand) == 0\n \n-    '''need special handle for different backend'''\n-\n-    def test_internal_conv_utils(self):\n-        xshape = (5, 4, 3, 2)\n-        xval = np.random.random(xshape)\n-        xtf = KTF.variable(xval)\n-        ztf = KTF._preprocess_deconv_output_shape(xtf, xshape, 'channels_first')\n-        assert ztf == (5, 3, 2, 4)\n-\n-        for dtype in [None, 'float64']:\n-            xval = np.random.random((5, 4, 3, 2))\n-            xtf = KTF.variable(xval, dtype=dtype)\n-            ztf = KTF.eval(KTF._preprocess_conv2d_input(xtf, 'channels_first'))\n-            assert ztf.shape == (5, 3, 2, 4)\n-\n-            xval = np.random.random((6, 5, 4, 3, 2))\n-            xtf = KTF.variable(xval, dtype=dtype)\n-            ztf = KTF.eval(KTF._preprocess_conv3d_input(xtf, 'channels_first'))\n-            assert ztf.shape == (6, 4, 3, 2, 5)\n-\n-            xval = np.random.random((5, 4, 3, 2))\n-            xtf = KTF.variable(xval, dtype=dtype)\n-            ztf = KTF.eval(KTF._preprocess_conv2d_kernel(xtf, 'channels_first'))\n-            assert ztf.shape == (3, 2, 4, 5)\n-\n-            xval = np.random.random((6, 5, 4, 3, 2))\n-            xtf = KTF.variable(xval, dtype=dtype)\n-            ztf = KTF.eval(KTF._preprocess_conv3d_kernel(xtf, 'channels_first'))\n-            assert ztf.shape == (4, 3, 2, 5, 6)\n-\n-        xval = np.random.random((5, 4, 3, 2))\n-        xtf = KTF.variable(xval)\n-        ztf = KTF.eval(KTF._postprocess_conv2d_output(xtf, 'channels_first'))\n-        assert ztf.shape == (5, 2, 4, 3)\n-\n-        xval = np.random.random((6, 5, 4, 3, 2))\n-        xtf = KTF.variable(xval)\n-        ztf = KTF.eval(KTF._postprocess_conv3d_output(xtf, 'channels_first'))\n-        assert ztf.shape == (6, 2, 5, 4, 3)\n-\n     def test_pooling_invalid_use(self):\n         for (input_shape, pool_size) in zip([(5, 10, 12, 3), (5, 10, 12, 6, 3)], [(2, 2), (2, 2, 2)]):\n             for k in BACKENDS:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
