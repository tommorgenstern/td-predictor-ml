{"custom_id": "keras#0f97daf7a7fbbbfcd417d67546aca9d5d12277f8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 73 | Lines Deleted: 94 | Files Changed: 4 | Hunks: 24 | Methods Changed: 22 | Complexity Δ (Sum/Max): -12/2 | Churn Δ: 167 | Churn Cumulative: 1153 | Contributors (this commit): 3 | Commits (past 90d): 23 | Contributors (cumulative): 8 | DMM Complexity: 0.32\n\nDIFF:\n@@ -18,11 +18,11 @@ class MiniDense(Layer):\n         input_dim = input_shape[-1]\n         w_shape = (input_dim, self.units)\n         w_value = initializers.GlorotUniform()(w_shape)\n-        self.w = backend.Variable(w_value)\n+        self.w = backend.Variable(w_value, name=\"kernel\")\n \n         b_shape = (self.units,)\n         b_value = initializers.Zeros()(b_shape)\n-        self.b = backend.Variable(b_value)\n+        self.b = backend.Variable(b_value, name=\"bias\")\n \n     def call(self, inputs):\n         return ops.matmul(inputs, self.w) + self.b\n@@ -49,10 +49,10 @@ class MiniBatchNorm(Layer):\n     def build(self, input_shape):\n         shape = (input_shape[-1],)\n         self.mean = backend.Variable(\n-            initializers.Zeros()(shape), trainable=False\n+            initializers.Zeros()(shape), trainable=False, name=\"mean\"\n         )\n         self.variance = backend.Variable(\n-            initializers.GlorotUniform()(shape), trainable=False\n+            initializers.GlorotUniform()(shape), trainable=False, name=\"variance\"\n         )\n         self.beta = backend.Variable(initializers.Zeros()(shape))\n         self.gamma = backend.Variable(initializers.Ones()(shape))\n@@ -79,14 +79,14 @@ class MyModel(Layer):\n     def __init__(self, units, num_classes):\n         super().__init__()\n         self.dense1 = MiniDense(units)\n-        self.bn = MiniBatchNorm()\n-        self.dropout = MiniDropout(0.5)\n+        # self.bn = MiniBatchNorm()\n+        # self.dropout = MiniDropout(0.5)\n         self.dense2 = MiniDense(num_classes)\n \n     def call(self, x):\n         x = self.dense1(x)\n-        x = self.bn(x)\n-        x = self.dropout(x)\n+        # x = self.bn(x)\n+        # x = self.dropout(x)\n         return self.dense2(x)\n \n \n@@ -120,6 +120,7 @@ def compute_loss_and_updates(\n     y_pred, non_trainable_variables = model.stateless_call(\n         trainable_variables, non_trainable_variables, x\n     )\n+    \n     loss = loss_fn(y, y_pred)\n     return loss, non_trainable_variables\n \n@@ -155,10 +156,12 @@ for x, y in dataset:\n \n # Post-processing model state update\n for variable, value in zip(model.trainable_variables, trainable_variables):\n+    print(variable.name, np.sum(np.abs(variable - value)))\n     variable.assign(value)\n for variable, value in zip(\n     model.non_trainable_variables, non_trainable_variables\n ):\n+    print(variable.name, np.sum(np.abs(variable - value)))\n     variable.assign(value)\n \n print(\"Updated values\")\n\n@@ -52,11 +52,14 @@ class Variable(KerasVariable):\n         self.name = name or auto_name(self.__class__.__name__)\n         dtype = standardize_dtype(dtype)\n         self._value = jnp.array(value, dtype=dtype)\n+        self._dtype = dtype\n+        self._shape = tuple(self._value.shape)\n+        self._ndim = len(self._shape)\n         self.trainable = trainable\n \n     def assign(self, value):\n         value = convert_to_tensor(value, dtype=self.dtype)\n-        if value.shape != self.value.shape:\n+        if value.shape != self.shape:\n             raise ValueError(\n                 \"The shape of the target variable and \"\n                 \"the shape of the target value in \"\n@@ -68,7 +71,10 @@ class Variable(KerasVariable):\n             scope = get_stateless_scope()\n             scope.add_update((self, value))\n         else:\n-            # TODO: optimize by avoiding memory copies\n+            if isinstance(value, jnp.ndarray) and value.dtype == self.dtype:\n+                # Avoid a memory copy\n+                self._value = value\n+            else:\n                 self._value = jnp.array(value, dtype=self.dtype)\n \n     @property\n@@ -82,15 +88,15 @@ class Variable(KerasVariable):\n \n     @property\n     def dtype(self):\n-        return self.value.dtype.name\n+        return self._dtype\n \n     @property\n     def shape(self):\n-        return self.value.shape\n+        return self._shape\n \n     @property\n     def ndim(self):\n-        return self.value.ndim\n+        return self._ndim\n \n     def numpy(self):\n         return np.array(self.value)\n\n@@ -1,4 +1,5 @@\n import jax\n+import numpy as np\n \n from keras_core import backend\n from keras_core import callbacks as callbacks_module\n@@ -10,73 +11,6 @@ from keras_core.trainers.epoch_iterator import EpochIterator\n \n class Trainer(base_trainer.Trainer):\n \n-    def compute_loss_and_updates(\n-        self, trainable_variables, non_trainable_variables, data\n-    ):\n-        x, y, sample_weight = data_adapters_utils.unpack_x_y_sample_weight(data)\n-        y_pred, non_trainable_variables = self.stateless_call(\n-            trainable_variables, non_trainable_variables, x\n-        )\n-        loss = self.compute_loss(x, y, y_pred, sample_weight=sample_weight)\n-        return loss, (non_trainable_variables, y_pred)\n-    \n-    def _get_gradient_fn(self):\n-        return jax.value_and_grad(self.compute_loss_and_updates, has_aux=True)\n-    \n-    def compute_loss(self, x=None, y=None, y_pred=None, sample_weight=None):\n-        \"\"\"Stateless loss computation.\"\"\"\n-        del x  # The default implementation does not use `x`.\n-        losses = []\n-        if self._compile_loss is not None:\n-            loss = self._compile_loss(y, y_pred, sample_weight)\n-            if loss is not None:\n-                losses.append(loss)\n-        for l in self.losses:\n-            losses.append(l.astype(backend.floatx()))\n-        if len(losses) == 0:\n-            raise ValueError(\n-                \"No loss to compute. Provide a `loss` argument in `compile()`.\"\n-            )\n-        if len(losses) == 1:\n-            total_loss = losses[0]\n-        else:\n-            total_loss = sum(losses)\n-        return total_loss\n-\n-    def train_step(self, trainable_variables, non_trainable_variables, optimizer_variables, data):\n-        grad_fn = self._get_gradient_fn()\n-        (loss, (non_trainable_variables, y_pred)), grads = grad_fn(\n-            trainable_variables, non_trainable_variables, data\n-        )\n-        trainable_variables, optimizer_variables = self.optimizer.stateless_apply(\n-            grads, optimizer_variables\n-        )\n-        return trainable_variables, non_trainable_variables, optimizer_variables, y_pred, loss\n-\n-    def test_step(self, data):\n-        raise NotImplementedError\n-\n-    def predict_step(self, data):\n-        raise NotImplementedError\n-\n-    def make_train_function(self, force=False):\n-        if self.run_eagerly or not self.jit_compile:\n-            self.train_function = self.train_step\n-            return self.train_step\n-\n-        @jax.jit\n-        def train_fn(trainable_variables, non_trainable_variables, optimizer_variables, data):\n-            return self.train_step(trainable_variables, non_trainable_variables, optimizer_variables, data)\n-        \n-        self.train_function = train_fn\n-        return train_fn\n-\n-    def make_test_function(self, force=False):\n-        raise NotImplementedError\n-\n-    def make_predict_function(self, force=False):\n-        raise NotImplementedError\n-\n     def fit(\n         self,\n         x=None,\n@@ -152,7 +86,7 @@ class Trainer(base_trainer.Trainer):\n             )\n \n         self.stop_training = False\n-        self.make_train_function()\n+        # self.make_train_function()\n         callbacks.on_train_begin()\n         training_logs = None\n         logs = None\n@@ -163,6 +97,41 @@ class Trainer(base_trainer.Trainer):\n         non_trainable_variables = self.non_trainable_variables\n         optimizer_variables = self.optimizer.variables\n \n+\n+        def compute_loss_and_updates(\n+            trainable_variables, non_trainable_variables, x, y\n+        ):\n+            y_pred, non_trainable_variables = self.stateless_call(\n+                trainable_variables, non_trainable_variables, x\n+            )\n+            \n+            loss = self._compile_loss(y, y_pred)\n+            return loss, (y_pred, non_trainable_variables)\n+\n+\n+        grad_fn = jax.value_and_grad(compute_loss_and_updates, has_aux=True)\n+\n+\n+        @jax.jit\n+        def train_step(\n+            trainable_variables, non_trainable_variables, optimizer_variables, x, y\n+        ):\n+            (loss, (y_pred, non_trainable_variables)), grads = grad_fn(\n+                trainable_variables, non_trainable_variables, x, y\n+            )\n+            # trainable_variables, optimizer_variables = self.optimizer.stateless_apply(\n+            #     grads, optimizer_variables\n+            # )\n+            new_trainable_variables = []\n+            for grad, var in zip(grads, trainable_variables):\n+                new_trainable_variables.append(var - 0.0001 * grad)\n+            trainable_variables = new_trainable_variables\n+\n+            \n+            return loss, trainable_variables, non_trainable_variables, optimizer_variables\n+\n+        # counter = jax.numpy.ones(())\n+\n         for epoch in range(initial_epoch, epochs):\n             self.reset_metrics()\n             callbacks.on_epoch_begin(epoch)\n@@ -170,22 +139,24 @@ class Trainer(base_trainer.Trainer):\n                 # Callbacks\n                 callbacks.on_train_batch_begin(step)\n \n-                # Train step (in JAX context)\n-                trainable_variables, non_trainable_variables, optimizer_variables, y_pred, loss = self.train_function(trainable_variables, non_trainable_variables, optimizer_variables, data)\n+                first_var = trainable_variables[0]\n \n-                # Run variable updates (back in eager context)\n+\n+                # Train step\n                 x, y, sample_weight = data_adapters_utils.unpack_x_y_sample_weight(data)\n-                logs = self.compute_metrics(x, y, y_pred, sample_weight=sample_weight)\n-                # for variable, value in zip(self.trainable_variables, trainable_variables):\n-                #     variable.assign(value)\n-                # for variable, value in zip(\n-                #     self.non_trainable_variables, non_trainable_variables\n-                # ):\n-                #     variable.assign(value)\n-                # for variable, value in zip(self.optimizer.variables, optimizer_variables):\n-                #     variable.assign(value)\n+                (\n+                    loss,\n+                    trainable_variables,\n+                    non_trainable_variables,\n+                    optimizer_variables,\n+                ) = train_step(\n+                    trainable_variables, non_trainable_variables, optimizer_variables, x, y\n+                )\n+\n                 self._loss_tracker.update_state(loss)\n \n+                # print(np.sum(np.abs(trainable_variables[0] - first_var)))\n+\n                 # Callbacks\n                 callbacks.on_train_batch_end(step, logs)\n                 if self.stop_training:\n\n@@ -31,4 +31,3 @@ class TestStatelessScope(testing.TestCase):\n         # Updates can be reapplied.\n         var_out.assign(scope.get_current_value(var_out))\n         self.assertAllClose(var_out_value, 2 * np.ones((2,)))\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b55b1f2be4fcd08470672d235842679886583197", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 10 | Churn Cumulative: 26613 | Contributors (this commit): 144 | Commits (past 90d): 24 | Contributors (cumulative): 146 | DMM Complexity: 0.0\n\nDIFF:\n@@ -318,7 +318,7 @@ class ExactEvaluationTest(tf.test.TestCase, parameterized.TestCase):\n             model = MyModel()\n             model.compile(\n                 metrics=[build_metric()],\n-                loss=\"binary_crossentropy\",\n+                loss=\"mae\",\n                 pss_evaluation_shards=num_shards,\n             )\n \n@@ -337,7 +337,8 @@ class ExactEvaluationTest(tf.test.TestCase, parameterized.TestCase):\n             )\n \n         metric_name = \"custom_acc\" if custom_metric else \"accuracy\"\n-        expected_results = {metric_name: expected_acc}\n+        # Since outputs are always 0 or 1, MAE loss should == accuracy\n+        expected_results = {metric_name: expected_acc, \"loss\": expected_acc}\n \n         def kill_and_revive_in_thread(wait_secs=0.1):\n             def _kill_and_revive_fn():\n@@ -380,7 +381,6 @@ class ExactEvaluationTest(tf.test.TestCase, parameterized.TestCase):\n                 metric: val.numpy() for metric, val in eval_results.items()\n             }\n         for metric, val in eval_results.items():\n-            if \"loss\" not in metric:\n             self.assertIn(metric, expected_results)\n             self.assertAlmostEqual(val, expected_results[metric], places=5)\n \n\n@@ -1249,8 +1249,6 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         # metric variables, which will be used to update the metrics.\n         for shard_result in logs:\n             for metric in self.metrics:\n-                if metric.name == \"loss\":\n-                    continue\n                 if metric.name not in shard_result.keys():\n                     logging.log_first_n(\n                         logging.WARN,\n@@ -1883,6 +1881,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n             with tf_utils.with_metric_local_vars_scope():\n                 for metric in self.compiled_metrics.metrics:\n                     local_metrics.append(base_metric.clone_metric(metric))\n+                for metric in self.compiled_loss.metrics:\n+                    local_metrics.append(base_metric.clone_metric(metric))\n             dataset = input_ops.auto_shard_dataset(\n                 dataset, total_shards, shard_idx\n             )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#332fb2b4675342505fd6a290c22ee69c56ff2ca1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 121/121 | Churn Δ: 4 | Churn Cumulative: 9478 | Contributors (this commit): 34 | Commits (past 90d): 12 | Contributors (cumulative): 34 | DMM Complexity: None\n\nDIFF:\n@@ -303,7 +303,7 @@ class LossFunctionWrapper(Loss):\n class MeanSquaredError(LossFunctionWrapper):\n     \"\"\"Computes the mean of squares of errors between labels and predictions.\n \n-    `loss = square(y_true - y_pred)`\n+    `loss = mean(square(y_true - y_pred))`\n \n     Standalone usage:\n \n@@ -362,7 +362,7 @@ class MeanSquaredError(LossFunctionWrapper):\n class MeanAbsoluteError(LossFunctionWrapper):\n     \"\"\"Computes the mean of absolute difference between labels and predictions.\n \n-    `loss = abs(y_true - y_pred)`\n+    `loss = mean(abs(y_true - y_pred))`\n \n     Standalone usage:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#89b2ea5977009d387e144c558982b64e524d798f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 126 | Lines Deleted: 56 | Files Changed: 15 | Hunks: 53 | Methods Changed: 15 | Complexity Δ (Sum/Max): 9/6 | Churn Δ: 182 | Churn Cumulative: 4874 | Contributors (this commit): 3 | Commits (past 90d): 103 | Contributors (cumulative): 25 | DMM Complexity: 0.589041095890411\n\nDIFF:\n@@ -52,7 +52,9 @@ class MiniBatchNorm(Layer):\n             initializers.Zeros()(shape), trainable=False, name=\"mean\"\n         )\n         self.variance = backend.Variable(\n-            initializers.GlorotUniform()(shape), trainable=False, name=\"variance\"\n+            initializers.GlorotUniform()(shape),\n+            trainable=False,\n+            name=\"variance\",\n         )\n         self.beta = backend.Variable(initializers.Zeros()(shape))\n         self.gamma = backend.Variable(initializers.Ones()(shape))\n@@ -136,7 +138,7 @@ def train_step(\n         trainable_variables, non_trainable_variables, x, y\n     )\n     trainable_variables, optimizer_variables = optimizer.stateless_apply(\n-        grads, optimizer_variables\n+        grads, trainable_variables, optimizer_variables\n     )\n     return trainable_variables, non_trainable_variables, optimizer_variables\n \n\n@@ -1,7 +1,6 @@\n import json\n import os\n \n-from keras_core.backend.stateless_scope import StatelessScope\n from keras_core.backend.common import standardize_dtype\n from keras_core.backend.common import standardize_shape\n from keras_core.backend.config import epsilon\n@@ -13,6 +12,7 @@ from keras_core.backend.config import set_image_data_format\n from keras_core.backend.keras_tensor import KerasTensor\n from keras_core.backend.keras_tensor import any_symbolic_tensors\n from keras_core.backend.keras_tensor import is_keras_tensor\n+from keras_core.backend.stateless_scope import StatelessScope\n from keras_core.utils.io_utils import print_msg\n \n # Set Keras base dir path given KERAS_HOME env variable, if applicable.\n\n@@ -4,11 +4,11 @@ from jax import numpy as jnp\n from tensorflow import nest\n \n from keras_core.backend.common import KerasVariable\n+from keras_core.backend.common import standardize_dtype\n+from keras_core.backend.keras_tensor import KerasTensor\n from keras_core.backend.stateless_scope import StatelessScope\n from keras_core.backend.stateless_scope import get_stateless_scope\n from keras_core.backend.stateless_scope import in_stateless_scope\n-from keras_core.backend.common import standardize_dtype\n-from keras_core.backend.keras_tensor import KerasTensor\n from keras_core.utils.naming import auto_name\n \n DYNAMIC_SHAPES_OK = False  # Dynamic shapes NG\n\n@@ -1,5 +1,4 @@\n import jax\n-import numpy as np\n \n from keras_core import backend\n from keras_core import callbacks as callbacks_module\n@@ -10,7 +9,6 @@ from keras_core.trainers.epoch_iterator import EpochIterator\n \n \n class Trainer(base_trainer.Trainer):\n-\n     def fit(\n         self,\n         x=None,\n@@ -68,9 +66,16 @@ class Trainer(base_trainer.Trainer):\n         if not self.built:\n             # Build the model on one batch of data.\n             for _, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n-                x, y, _ = data_adapters_utils.unpack_x_y_sample_weight(data)\n+                (\n+                    x,\n+                    y,\n+                    sample_weight,\n+                ) = data_adapters_utils.unpack_x_y_sample_weight(data)\n                 # Build model\n-                self(x)\n+                y_pred = self(x)\n+                # Build metrics\n+                self.compute_metrics(x, y, y_pred, sample_weight)\n+                self.reset_metrics()\n                 break\n \n         # Container that configures and calls callbacks.\n@@ -96,7 +101,7 @@ class Trainer(base_trainer.Trainer):\n             self.optimizer.build(trainable_variables)\n         non_trainable_variables = self.non_trainable_variables\n         optimizer_variables = self.optimizer.variables\n-\n+        metrics_variables = self.metrics_variables\n \n         def compute_loss_and_updates(\n             trainable_variables, non_trainable_variables, x, y\n@@ -108,29 +113,56 @@ class Trainer(base_trainer.Trainer):\n             loss = self._compile_loss(y, y_pred)\n             return loss, (y_pred, non_trainable_variables)\n \n-\n         grad_fn = jax.value_and_grad(compute_loss_and_updates, has_aux=True)\n \n-\n         @jax.jit\n-        def train_step(\n-            trainable_variables, non_trainable_variables, optimizer_variables, x, y\n-        ):\n+        def train_step(state, data):\n+            (\n+                trainable_variables,\n+                non_trainable_variables,\n+                optimizer_variables,\n+                metrics_variables,\n+            ) = state\n+            x, y, sample_weight = data_adapters_utils.unpack_x_y_sample_weight(\n+                data\n+            )\n             (loss, (y_pred, non_trainable_variables)), grads = grad_fn(\n                 trainable_variables, non_trainable_variables, x, y\n             )\n-            # trainable_variables, optimizer_variables = self.optimizer.stateless_apply(\n-            #     grads, optimizer_variables\n-            # )\n-            new_trainable_variables = []\n-            for grad, var in zip(grads, trainable_variables):\n-                new_trainable_variables.append(var - 0.0001 * grad)\n-            trainable_variables = new_trainable_variables\n \n+            (\n+                trainable_variables,\n+                optimizer_variables,\n+            ) = self.optimizer.stateless_apply(\n+                grads, trainable_variables, optimizer_variables\n+            )\n \n-            return loss, trainable_variables, non_trainable_variables, optimizer_variables\n+            with backend.StatelessScope(\n+                state_mapping=[\n+                    (ref_v, v)\n+                    for ref_v, v in zip(\n+                        self.metrics_variables, metrics_variables\n+                    )\n+                ]\n+            ) as scope:\n+                logs = self.compute_metrics(x, y, y_pred, sample_weight)\n+                self._loss_tracker.update_state(loss)\n \n-        # counter = jax.numpy.ones(())\n+            new_metrics_variables = []\n+            for ref_v in self.metrics_variables:\n+                new_v = scope.get_current_value(ref_v)\n+                if new_v is None:\n+                    new_v = ref_v.value\n+                new_metrics_variables.append(new_v)\n+            metrics_variables = new_metrics_variables\n+\n+            state = (\n+                trainable_variables,\n+                non_trainable_variables,\n+                optimizer_variables,\n+                metrics_variables,\n+            )\n+            return logs, state\n \n         for epoch in range(initial_epoch, epochs):\n             self.reset_metrics()\n@@ -139,31 +171,41 @@ class Trainer(base_trainer.Trainer):\n                 # Callbacks\n                 callbacks.on_train_batch_begin(step)\n \n-                first_var = trainable_variables[0]\n-\n-\n                 # Train step\n-                x, y, sample_weight = data_adapters_utils.unpack_x_y_sample_weight(data)\n-                (\n-                    loss,\n+                state = (\n                     trainable_variables,\n                     non_trainable_variables,\n                     optimizer_variables,\n-                ) = train_step(\n-                    trainable_variables, non_trainable_variables, optimizer_variables, x, y\n+                    metrics_variables,\n                 )\n-\n-                self._loss_tracker.update_state(loss)\n-\n-                # print(np.sum(np.abs(trainable_variables[0] - first_var)))\n+                logs, state = train_step(state, data)\n+                (\n+                    trainable_variables,\n+                    non_trainable_variables,\n+                    optimizer_variables,\n+                    metrics_variables,\n+                ) = state\n \n                 # Callbacks\n                 callbacks.on_train_batch_end(step, logs)\n                 if self.stop_training:\n                     break\n \n+            # Update variable values\n+            # NOTE: doing this after each step would be a big performance bottleneck.\n+            for ref_v, v in zip(self.trainable_variables, trainable_variables):\n+                ref_v.assign(v)\n+            for ref_v, v in zip(\n+                self.non_trainable_variables, non_trainable_variables\n+            ):\n+                ref_v.assign(v)\n+            for ref_v, v in zip(self.optimizer.variables, optimizer_variables):\n+                ref_v.assign(v)\n+            for ref_v, v in zip(self.metrics_variables, metrics_variables):\n+                ref_v.assign(v)\n+\n             # Override with model metrics instead of last step logs\n-            epoch_logs = {\"loss\": self._loss_tracker.result()} # self._process_logs(self.get_metrics_result())\n+            epoch_logs = self._process_logs(self.get_metrics_result())\n \n             # Run validation.\n             if validation_data and self._should_eval(epoch, validation_freq):\n\n@@ -1,6 +1,6 @@\n import threading\n-from keras_core.backend.common import KerasVariable\n \n+from keras_core.backend.common import KerasVariable\n \n ### Stateless context manager\n \n\n@@ -1,7 +1,8 @@\n-from keras_core import testing\n+import numpy as np\n+\n from keras_core import backend\n from keras_core import operations as ops\n-import numpy as np\n+from keras_core import testing\n from keras_core.backend.stateless_scope import StatelessScope\n \n \n@@ -13,15 +14,17 @@ class TestStatelessScope(testing.TestCase):\n \n         value1 = ops.ones(shape=(2,))\n         value2 = ops.ones(shape=(2,))\n-        with StatelessScope(state_mapping=[(var1, value1), (var2, value2)]) as scope:\n+        with StatelessScope(\n+            state_mapping=[(var1, value1), (var2, value2)]\n+        ) as scope:\n             out = var1 + var2\n             var_out.assign(out)\n-            var_out_value = var_out + 0.\n+            var_out_value = var_out + 0.0\n             # Inside scope: new value is used.\n             self.assertAllClose(var_out_value, 2 * np.ones((2,)))\n \n         # Out of scope: old value is used.\n-        var_out_value = var_out + 0.\n+        var_out_value = var_out + 0.0\n         self.assertAllClose(var_out_value, np.zeros((2,)))\n \n         # Updates are tracked.\n\n@@ -3,9 +3,9 @@ from tensorflow.experimental import numpy as tfnp\n \n from keras_core.backend.common import KerasVariable\n from keras_core.backend.common import standardize_dtype\n+from keras_core.backend.keras_tensor import KerasTensor\n from keras_core.backend.stateless_scope import get_stateless_scope\n from keras_core.backend.stateless_scope import in_stateless_scope\n-from keras_core.backend.keras_tensor import KerasTensor\n from keras_core.utils.naming import auto_name\n \n DYNAMIC_SHAPES_OK = True\n\n@@ -17,7 +17,9 @@ class TFModel(layers.Dense, Trainer):\n         Trainer.__init__(self)\n \n \n-@pytest.mark.skipif(backend.backend() != \"tensorflow\", reason=\"Target the TF backend only.\")\n+@pytest.mark.skipif(\n+    backend.backend() != \"tensorflow\", reason=\"Target the TF backend only.\"\n+)\n class TestTFTrainer(testing.TestCase):\n     def _test_basic_flow(self, run_eagerly, jit_compile):\n         model = TFModel(units=3)\n\n@@ -1,9 +1,9 @@\n from keras_core.api_export import keras_core_export\n from keras_core.metrics.metric import Metric\n-from keras_core.metrics.regression_metrics import MeanSquaredError\n from keras_core.metrics.reduction_metrics import Mean\n-from keras_core.metrics.reduction_metrics import Sum\n from keras_core.metrics.reduction_metrics import MeanMetricWrapper\n+from keras_core.metrics.reduction_metrics import Sum\n+from keras_core.metrics.regression_metrics import MeanSquaredError\n \n \n @keras_core_export(\"keras_core.metrics.get\")\n\n@@ -1,7 +1,7 @@\n+from keras_core import backend\n from keras_core.api_export import keras_core_export\n from keras_core.layers.layer import Layer\n from keras_core.utils import summary_utils\n-from keras_core import backend\n \n if backend.backend() == \"tensorflow\":\n     from keras_core.backend.tensorflow.trainer import Trainer\n\n@@ -219,7 +219,7 @@ class Optimizer:\n                 if getattr(variable, \"constraint\", None) is not None:\n                     variable.assign(variable.constraint(variable))\n \n-    def stateless_apply(self, grads, optimizer_variables):\n+    def stateless_apply(self, grads, trainable_variables, optimizer_variables):\n         self._check_super_called()\n \n         if not self.built:\n@@ -235,9 +235,20 @@ class Optimizer:\n                 f\"Received list with length {len(optimizer_variables)}, but expected \"\n                 f\"{len(self.variables)} variables.\"\n             )\n+        if len(trainable_variables) != len(self._trainable_variables):\n+            raise ValueError(\n+                \"Argument `optimizer_variables` must be a list of tensors \"\n+                \"corresponding 1:1 to the trainable variables list that \"\n+                \"the optimizer was built with. \"\n+                f\"Received len(trainable_variables) == {len(trainable_variables)} \"\n+                \"whereas the optimizer was built with \"\n+                f\"{len(self._trainable_variables)} variables.\"\n+            )\n \n         # Gather variable mapping\n-        mapping = list(zip(self.variables, optimizer_variables))\n+        mapping = list(\n+            zip(self._trainable_variables, trainable_variables)\n+        ) + list(zip(self.variables, optimizer_variables))\n \n         # Call in stateless scope\n         with backend.StatelessScope(state_mapping=mapping) as scope:\n\n@@ -8,7 +8,8 @@ from keras_core.utils.naming import get_object_name\n \n \n class MetricsList(metrics_module.Metric):\n-    def __init__(self, metrics):\n+    def __init__(self, metrics, name=\"metrics_list\"):\n+        super().__init__(name=name)\n         self.metrics = metrics\n \n     def update_state(self, y_true, y_pred, sample_weight=None):\n@@ -112,7 +113,8 @@ def get_loss(identifier, y_true, y_pred):\n \n \n class CompileMetrics(metrics_module.Metric):\n-    def __init__(self, metrics, weighted_metrics):\n+    def __init__(self, metrics, weighted_metrics, name=\"compile_metric\"):\n+        super().__init__(name=name)\n         if metrics and not isinstance(metrics, (list, tuple, dict)):\n             raise ValueError(\n                 \"Expected `metrics` argument to be a list, tuple, or dict. \"\n\n@@ -1,6 +1,6 @@\n+import jax\n import numpy as np\n import tensorflow as tf\n-import jax\n \n from keras_core import backend\n from keras_core import testing\n\n@@ -1,8 +1,8 @@\n import math\n \n+import jax\n import numpy as np\n import tensorflow as tf\n-import jax\n \n try:\n     import pandas\n\n@@ -1,7 +1,8 @@\n import warnings\n+\n from keras_core import backend\n-from keras_core import operations as ops\n from keras_core import metrics as metrics_module\n+from keras_core import operations as ops\n from keras_core.trainers.compile_utils import CompileLoss\n from keras_core.trainers.compile_utils import CompileMetrics\n \n@@ -72,6 +73,13 @@ class Trainer:\n             metrics += [self._compile_metrics]\n         return metrics\n \n+    @property\n+    def metrics_variables(self):\n+        vars = []\n+        for metric in self.metrics:\n+            vars.extend(metric.variables)\n+        return vars\n+\n     def reset_metrics(self):\n         for m in self.metrics:\n             m.reset_state()\n@@ -144,7 +152,7 @@ class Trainer:\n         self._loss_tracker.update_state(total_loss)\n         return total_loss\n \n-    def compute_metrics(self, x, y, y_pred, sample_weight):\n+    def compute_metrics(self, x, y, y_pred, sample_weight=None):\n         \"\"\"Update metric states and collect all metrics to be returned.\n \n         Subclasses can optionally override this method to provide custom metric\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dbda705cda6c112adebec4b7d02b31c798a548cc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 616 | Lines Deleted: 494 | Files Changed: 2 | Hunks: 126 | Methods Changed: 115 | Complexity Δ (Sum/Max): 106/109 | Churn Δ: 1110 | Churn Cumulative: 7831 | Contributors (this commit): 3 | Commits (past 90d): 25 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -746,16 +746,12 @@ class CountNonzero(Operation):\n     def __init__(self, axis=None):\n         super().__init__()\n         if isinstance(axis, int):\n-            self.axis = [axis]\n+            self.axis = (axis,)\n         else:\n             self.axis = axis\n \n     def call(self, x):\n-        return backend.execute(\n-            \"count_nonzero\",\n-            x,\n-            axis=self.axis,\n-        )\n+        return backend.execute(\"count_nonzero\", x, axis=self.axis)\n \n     def compute_output_spec(self, x):\n         return KerasTensor(\n@@ -1883,16 +1879,7 @@ class Nonzero(Operation):\n     def call(self, x):\n         return backend.execute(\"nonzero\", x)\n \n-    def compute_output_spec(self, x):\n-        output = []\n-        for _ in range(len(x.shape)):\n-            output.append(KerasTensor([None]))\n-        return tuple(output)\n-\n-\n def nonzero(x):\n-    if any_symbolic_tensors((x,)):\n-        return Nonzero().symbolic_call(x)\n     return backend.execute(\"nonzero\", x)\n \n \n\n@@ -1,25 +1,219 @@\n+import pytest\n import numpy as np\n from tensorflow.python.ops.numpy_ops import np_config\n \n from keras_core import testing\n+from keras_core.backend import backend\n from keras_core.backend.keras_tensor import KerasTensor\n from keras_core.operations import numpy as knp\n \n np_config.enable_numpy_behavior()\n \n \n-class NumpyTwoInputOpsShapeTest(testing.TestCase):\n+@pytest.mark.skipif(\n+    backend() != \"tensorflow\",\n+    reason=\"Dynamic shapes are only supported in TensorFlow backend.\",\n+)\n+class NumpyTwoInputOpsDynamicShapeTest(testing.TestCase):\n     def test_add(self):\n-        x = KerasTensor((2, 3))\n-        y = KerasTensor((2, 3))\n-        self.assertEqual(knp.add(x, y).shape, (2, 3))\n-\n         x = KerasTensor((None, 3))\n         y = KerasTensor((2, None))\n         self.assertEqual(knp.add(x, y).shape, (2, 3))\n \n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.add(x, 2).shape, (2, 3))\n+    def test_subtract(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([2, None])\n+        self.assertEqual(knp.subtract(x, y).shape, (2, 3))\n+\n+    def test_multiply(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([2, None])\n+        self.assertEqual(knp.multiply(x, y).shape, (2, 3))\n+\n+    def test_matmul(self):\n+        x = KerasTensor([None, 3, 4])\n+        y = KerasTensor([3, None, 4, 5])\n+        self.assertEqual(knp.matmul(x, y).shape, (3, None, 3, 5))\n+\n+    def test_power(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([2, None])\n+        self.assertEqual(knp.power(x, y).shape, (2, 3))\n+\n+    def test_divide(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([2, None])\n+        self.assertEqual(knp.divide(x, y).shape, (2, 3))\n+\n+    def test_true_divide(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([2, None])\n+        self.assertEqual(knp.true_divide(x, y).shape, (2, 3))\n+\n+    def test_append(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([2, None])\n+        self.assertEqual(knp.append(x, y).shape, (None,))\n+\n+    def test_arctan2(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([2, None])\n+        self.assertEqual(knp.arctan2(x, y).shape, (2, 3))\n+\n+    def test_cross(self):\n+        x1 = KerasTensor([2, 3, 3])\n+        x2 = KerasTensor([1, 3, 2])\n+        y = KerasTensor([None, 1, 2])\n+        self.assertEqual(knp.cross(x1, y).shape, (2, 3, 3))\n+        self.assertEqual(knp.cross(x2, y).shape, (None, 3))\n+\n+    def test_full_like(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.full_like(x, KerasTensor([1, 3])).shape, (None, 3))\n+\n+        x = KerasTensor([None, 3, 3])\n+        self.assertEqual(knp.full_like(x, 2).shape, (None, 3, 3))\n+\n+    def test_greater(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([2, None])\n+        self.assertEqual(knp.greater(x, y).shape, (2, 3))\n+\n+    def test_greater_equal(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([2, None])\n+        self.assertEqual(knp.greater_equal(x, y).shape, (2, 3))\n+\n+    def test_isclose(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([2, None])\n+        self.assertEqual(knp.isclose(x, y).shape, (2, 3))\n+\n+    def test_less(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([2, None])\n+        self.assertEqual(knp.less(x, y).shape, (2, 3))\n+\n+    def test_less_equal(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([2, None])\n+        self.assertEqual(knp.less_equal(x, y).shape, (2, 3))\n+\n+    def test_linspace(self):\n+        start = KerasTensor([None, 3, 4])\n+        stop = KerasTensor([2, 3, 4])\n+        self.assertEqual(\n+            knp.linspace(start, stop, 10, axis=1).shape, (2, 10, 3, 4)\n+        )\n+\n+        start = KerasTensor([None, 3])\n+        stop = 2\n+        self.assertEqual(\n+            knp.linspace(start, stop, 10, axis=1).shape, (None, 10, 3)\n+        )\n+\n+    def test_logical_and(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([2, None])\n+        self.assertEqual(knp.logical_and(x, y).shape, (2, 3))\n+\n+    def test_logical_or(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([2, None])\n+        self.assertEqual(knp.logical_or(x, y).shape, (2, 3))\n+\n+    def test_logspace(self):\n+        start = KerasTensor([None, 3, 4])\n+        stop = KerasTensor([2, 3, 4])\n+        self.assertEqual(\n+            knp.logspace(start, stop, 10, axis=1).shape, (2, 10, 3, 4)\n+        )\n+\n+        start = KerasTensor([None, 3])\n+        stop = 2\n+        self.assertEqual(\n+            knp.logspace(start, stop, 10, axis=1).shape, (None, 10, 3)\n+        )\n+\n+    def test_maximum(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([2, None])\n+        self.assertEqual(knp.maximum(x, y).shape, (2, 3))\n+\n+    def test_minimum(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([2, None])\n+        self.assertEqual(knp.minimum(x, y).shape, (2, 3))\n+\n+    def test_mod(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([2, None])\n+        self.assertEqual(knp.mod(x, y).shape, (2, 3))\n+\n+    def test_not_equal(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([2, None])\n+        self.assertEqual(knp.not_equal(x, y).shape, (2, 3))\n+\n+    def test_outer(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([2, None])\n+        self.assertEqual(knp.outer(x, y).shape, (None, None))\n+\n+    def test_take(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.take(x, 1).shape, ())\n+        self.assertEqual(knp.take(x, [1, 2]).shape, (2,))\n+        self.assertEqual(\n+            knp.take(x, [[1, 2], [1, 2]], axis=1).shape, (None, 2, 2)\n+        )\n+\n+        x = KerasTensor([None, 3, 3])\n+        self.assertEqual(knp.take(x, 1, axis=1).shape, (None, 3))\n+        self.assertEqual(knp.take(x, [1, 2]).shape, (2,))\n+        self.assertEqual(\n+            knp.take(x, [[1, 2], [1, 2]], axis=1).shape, (None, 2, 2, 3)\n+        )\n+\n+    def test_take_along_axis(self):\n+        x = KerasTensor([None, 3])\n+        indices = KerasTensor([1, 3])\n+        self.assertEqual(knp.take_along_axis(x, indices, axis=0).shape, (1, 3))\n+        self.assertEqual(knp.take_along_axis(x, indices, axis=1).shape, (1, 3))\n+\n+        x = KerasTensor([None, 3, 3])\n+        indices = KerasTensor([1, 3, None])\n+        self.assertEqual(\n+            knp.take_along_axis(x, indices, axis=1).shape, (1, 3, 3)\n+        )\n+\n+    def test_tensordot(self):\n+        x = KerasTensor([None, 3, 4])\n+        y = KerasTensor([3, 4])\n+        self.assertEqual(knp.tensordot(x, y, axes=1).shape, (None, 3, 4))\n+        self.assertEqual(knp.tensordot(x, y, axes=[[0, 1], [1, 0]]).shape, (4,))\n+\n+    def test_vdot(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([None, 3])\n+        self.assertEqual(knp.vdot(x, y).shape, ())\n+\n+        x = KerasTensor([None, 3, 3])\n+        y = KerasTensor([None, 3, 3])\n+        self.assertEqual(knp.vdot(x, y).shape, ())\n+\n+    def test_where(self):\n+        condition = KerasTensor([2, None, 1])\n+        x = KerasTensor([None, 1])\n+        y = KerasTensor([None, 3])\n+        self.assertEqual(knp.where(condition, x, y).shape, (2, None, 3))\n+\n+\n+class NumpyTwoInputOpsStaticShapeTest(testing.TestCase):\n+    def test_add(self):\n+        x = KerasTensor((2, 3))\n+        y = KerasTensor((2, 3))\n+        self.assertEqual(knp.add(x, y).shape, (2, 3))\n \n         with self.assertRaises(ValueError):\n             x = KerasTensor([2, 3])\n@@ -31,10 +225,6 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.subtract(x, y).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([2, None])\n-        self.assertEqual(knp.subtract(x, y).shape, (2, 3))\n-\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.subtract(x, 2).shape, (2, 3))\n \n@@ -48,10 +238,6 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.multiply(x, y).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([2, None])\n-        self.assertEqual(knp.multiply(x, y).shape, (2, 3))\n-\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.multiply(x, 2).shape, (2, 3))\n \n@@ -65,10 +251,6 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         y = KerasTensor([3, 2])\n         self.assertEqual(knp.matmul(x, y).shape, (2, 2))\n \n-        x = KerasTensor([None, 3, 4])\n-        y = KerasTensor([3, None, 4, 5])\n-        self.assertEqual(knp.matmul(x, y).shape, (3, None, 3, 5))\n-\n         with self.assertRaises(ValueError):\n             x = KerasTensor([3, 4])\n             y = KerasTensor([2, 3, 4])\n@@ -79,10 +261,6 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.power(x, y).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([2, None])\n-        self.assertEqual(knp.power(x, y).shape, (2, 3))\n-\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.power(x, 2).shape, (2, 3))\n \n@@ -96,10 +274,6 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.divide(x, y).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([2, None])\n-        self.assertEqual(knp.divide(x, y).shape, (2, 3))\n-\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.divide(x, 2).shape, (2, 3))\n \n@@ -113,10 +287,6 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.true_divide(x, y).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([2, None])\n-        self.assertEqual(knp.true_divide(x, y).shape, (2, 3))\n-\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.true_divide(x, 2).shape, (2, 3))\n \n@@ -134,10 +304,6 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.append(x, y, axis=0).shape, (4, 3))\n \n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([2, None])\n-        self.assertEqual(knp.append(x, y).shape, (None,))\n-\n         with self.assertRaises(ValueError):\n             x = KerasTensor([2, 3])\n             y = KerasTensor([2, 3, 4])\n@@ -148,10 +314,6 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.arctan2(x, y).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([2, None])\n-        self.assertEqual(knp.arctan2(x, y).shape, (2, 3))\n-\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.arctan2(x, 2).shape, (2, 3))\n \n@@ -164,12 +326,9 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         x1 = KerasTensor([2, 3, 3])\n         x2 = KerasTensor([1, 3, 2])\n         y1 = KerasTensor([2, 3, 3])\n-        y2 = KerasTensor([None, 1, 2])\n-        y3 = KerasTensor([2, 3, 2])\n+        y2 = KerasTensor([2, 3, 2])\n         self.assertEqual(knp.cross(x1, y1).shape, (2, 3, 3))\n-        self.assertEqual(knp.cross(x1, y2).shape, (2, 3, 3))\n-        self.assertEqual(knp.cross(x2, y2).shape, (None, 3))\n-        self.assertEqual(knp.cross(x2, y3).shape, (2, 3))\n+        self.assertEqual(knp.cross(x2, y2).shape, (2, 3))\n \n         with self.assertRaises(ValueError):\n             x = KerasTensor([2, 3])\n@@ -185,21 +344,11 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.full_like(x, 2).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.full_like(x, KerasTensor([1, 3])).shape, (None, 3))\n-\n-        x = KerasTensor([None, 3, 3])\n-        self.assertEqual(knp.full_like(x, 2).shape, (None, 3, 3))\n-\n     def test_greater(self):\n         x = KerasTensor([2, 3])\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.greater(x, y).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([2, None])\n-        self.assertEqual(knp.greater(x, y).shape, (2, 3))\n-\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.greater(x, 2).shape, (2, 3))\n \n@@ -213,10 +362,6 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.greater_equal(x, y).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([2, None])\n-        self.assertEqual(knp.greater_equal(x, y).shape, (2, 3))\n-\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.greater_equal(x, 2).shape, (2, 3))\n \n@@ -230,10 +375,6 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.isclose(x, y).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([2, None])\n-        self.assertEqual(knp.isclose(x, y).shape, (2, 3))\n-\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.isclose(x, 2).shape, (2, 3))\n \n@@ -247,10 +388,6 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.less(x, y).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([2, None])\n-        self.assertEqual(knp.less(x, y).shape, (2, 3))\n-\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.less(x, 2).shape, (2, 3))\n \n@@ -264,10 +401,6 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.less_equal(x, y).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([2, None])\n-        self.assertEqual(knp.less_equal(x, y).shape, (2, 3))\n-\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.less_equal(x, 2).shape, (2, 3))\n \n@@ -281,18 +414,6 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         stop = KerasTensor([2, 3, 4])\n         self.assertEqual(knp.linspace(start, stop, 10).shape, (10, 2, 3, 4))\n \n-        start = KerasTensor([None, 3, 4])\n-        stop = KerasTensor([2, 3, 4])\n-        self.assertEqual(\n-            knp.linspace(start, stop, 10, axis=1).shape, (2, 10, 3, 4)\n-        )\n-\n-        start = KerasTensor([None, 3])\n-        stop = 2\n-        self.assertEqual(\n-            knp.linspace(start, stop, 10, axis=1).shape, (None, 10, 3)\n-        )\n-\n         with self.assertRaises(ValueError):\n             start = KerasTensor([2, 3])\n             stop = KerasTensor([2, 3, 4])\n@@ -303,10 +424,6 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.logical_and(x, y).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([2, None])\n-        self.assertEqual(knp.logical_and(x, y).shape, (2, 3))\n-\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.logical_and(x, 2).shape, (2, 3))\n \n@@ -320,10 +437,6 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.logical_or(x, y).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([2, None])\n-        self.assertEqual(knp.logical_or(x, y).shape, (2, 3))\n-\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.logical_or(x, 2).shape, (2, 3))\n \n@@ -337,18 +450,6 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         stop = KerasTensor([2, 3, 4])\n         self.assertEqual(knp.logspace(start, stop, 10).shape, (10, 2, 3, 4))\n \n-        start = KerasTensor([None, 3, 4])\n-        stop = KerasTensor([2, 3, 4])\n-        self.assertEqual(\n-            knp.logspace(start, stop, 10, axis=1).shape, (2, 10, 3, 4)\n-        )\n-\n-        start = KerasTensor([None, 3])\n-        stop = 2\n-        self.assertEqual(\n-            knp.logspace(start, stop, 10, axis=1).shape, (None, 10, 3)\n-        )\n-\n         with self.assertRaises(ValueError):\n             start = KerasTensor([2, 3])\n             stop = KerasTensor([2, 3, 4])\n@@ -359,10 +460,6 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.maximum(x, y).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([2, None])\n-        self.assertEqual(knp.maximum(x, y).shape, (2, 3))\n-\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.maximum(x, 2).shape, (2, 3))\n \n@@ -376,10 +473,6 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.minimum(x, y).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([2, None])\n-        self.assertEqual(knp.minimum(x, y).shape, (2, 3))\n-\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.minimum(x, 2).shape, (2, 3))\n \n@@ -393,10 +486,6 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.mod(x, y).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([2, None])\n-        self.assertEqual(knp.mod(x, y).shape, (2, 3))\n-\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.mod(x, 2).shape, (2, 3))\n \n@@ -410,10 +499,6 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.not_equal(x, y).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([2, None])\n-        self.assertEqual(knp.not_equal(x, y).shape, (2, 3))\n-\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.not_equal(x, 2).shape, (2, 3))\n \n@@ -431,10 +516,6 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         y = KerasTensor([4, 5])\n         self.assertEqual(knp.outer(x, y).shape, (6, 20))\n \n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([2, None])\n-        self.assertEqual(knp.outer(x, y).shape, (None, None))\n-\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.outer(x, 2).shape, (6, 1))\n \n@@ -444,37 +525,12 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n         self.assertEqual(knp.take(x, [1, 2]).shape, (2,))\n         self.assertEqual(knp.take(x, [[1, 2], [1, 2]], axis=1).shape, (2, 2, 2))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.take(x, 1).shape, ())\n-        self.assertEqual(knp.take(x, [1, 2]).shape, (2,))\n-        self.assertEqual(\n-            knp.take(x, [[1, 2], [1, 2]], axis=1).shape, (None, 2, 2)\n-        )\n-\n-        x = KerasTensor([None, 3, 3])\n-        self.assertEqual(knp.take(x, 1, axis=1).shape, (None, 3))\n-        self.assertEqual(knp.take(x, [1, 2]).shape, (2,))\n-        self.assertEqual(\n-            knp.take(x, [[1, 2], [1, 2]], axis=1).shape, (None, 2, 2, 3)\n-        )\n-\n     def test_take_along_axis(self):\n         x = KerasTensor([2, 3])\n         indices = KerasTensor([1, 3])\n         self.assertEqual(knp.take_along_axis(x, indices, axis=0).shape, (1, 3))\n         self.assertEqual(knp.take_along_axis(x, indices, axis=1).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        indices = KerasTensor([1, 3])\n-        self.assertEqual(knp.take_along_axis(x, indices, axis=0).shape, (1, 3))\n-        self.assertEqual(knp.take_along_axis(x, indices, axis=1).shape, (1, 3))\n-\n-        x = KerasTensor([None, 3, 3])\n-        indices = KerasTensor([1, 3, None])\n-        self.assertEqual(\n-            knp.take_along_axis(x, indices, axis=1).shape, (1, 3, 3)\n-        )\n-\n         with self.assertRaises(ValueError):\n             x = KerasTensor([2, 3])\n             indices = KerasTensor([1, 4])\n@@ -489,41 +545,24 @@ class NumpyTwoInputOpsShapeTest(testing.TestCase):\n             knp.tensordot(x, y, axes=[[1, 2], [0, 1]]).shape, (2, 4)\n         )\n \n-        x = KerasTensor([None, 3, 4])\n-        y = KerasTensor([3, 4])\n-        self.assertEqual(knp.tensordot(x, y, axes=1).shape, (None, 3, 4))\n-        self.assertEqual(knp.tensordot(x, y, axes=[[0, 1], [1, 0]]).shape, (4,))\n-\n     def test_vdot(self):\n         x = KerasTensor([2, 3])\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.vdot(x, y).shape, ())\n \n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([None, 3])\n-        self.assertEqual(knp.vdot(x, y).shape, ())\n-\n-        x = KerasTensor([None, 3, 3])\n-        y = KerasTensor([None, 3, 3])\n-        self.assertEqual(knp.vdot(x, y).shape, ())\n-\n     def test_where(self):\n         condition = KerasTensor([2, 3])\n         x = KerasTensor([2, 3])\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.where(condition, x, y).shape, (2, 3))\n \n-        condition = KerasTensor([2, None, 1])\n-        x = KerasTensor([None, 1])\n-        y = KerasTensor([None, 3])\n-        self.assertEqual(knp.where(condition, x, y).shape, (2, None, 3))\n \n-\n-class NumpyOneInputOpsShapeTest(testing.TestCase):\n+@pytest.mark.skipif(\n+    backend() != \"tensorflow\",\n+    reason=\"Dynamic shapes are only supported in TensorFlow backend.\",\n+)\n+class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n     def test_mean(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.mean(x).shape, ())\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.mean(x).shape, ())\n \n@@ -532,9 +571,6 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         self.assertEqual(knp.mean(x, axis=1, keepdims=True).shape, (None, 1, 3))\n \n     def test_all(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.all(x).shape, ())\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.all(x).shape, ())\n \n@@ -543,9 +579,6 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         self.assertEqual(knp.all(x, axis=1, keepdims=True).shape, (None, 1, 3))\n \n     def test_var(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.var(x).shape, ())\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.var(x).shape, ())\n \n@@ -554,9 +587,6 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         self.assertEqual(knp.var(x, axis=1, keepdims=True).shape, (None, 1, 3))\n \n     def test_sum(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.sum(x).shape, ())\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.sum(x).shape, ())\n \n@@ -565,9 +595,6 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         self.assertEqual(knp.sum(x, axis=1, keepdims=True).shape, (None, 1, 3))\n \n     def test_amax(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.amax(x).shape, ())\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.amax(x).shape, ())\n \n@@ -576,9 +603,6 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         self.assertEqual(knp.amax(x, axis=1, keepdims=True).shape, (None, 1, 3))\n \n     def test_amin(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.amin(x).shape, ())\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.amin(x).shape, ())\n \n@@ -587,37 +611,22 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         self.assertEqual(knp.amin(x, axis=1, keepdims=True).shape, (None, 1, 3))\n \n     def test_square(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.square(x).shape, (2, 3))\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.square(x).shape, (None, 3))\n \n     def test_negative(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.negative(x).shape, (2, 3))\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.negative(x).shape, (None, 3))\n \n     def test_abs(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.abs(x).shape, (2, 3))\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.abs(x).shape, (None, 3))\n \n     def test_absolute(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.absolute(x).shape, (2, 3))\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.absolute(x).shape, (None, 3))\n \n     def test_squeeze(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.squeeze(x).shape, (2, 3))\n-\n         x = KerasTensor([None, 1])\n         self.assertEqual(knp.squeeze(x).shape, (None,))\n         self.assertEqual(knp.squeeze(x, axis=1).shape, (None,))\n@@ -627,9 +636,6 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n             knp.squeeze(x, axis=0)\n \n     def test_transpose(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.transpose(x).shape, (3, 2))\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.transpose(x).shape, (3, None))\n \n@@ -637,30 +643,18 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         self.assertEqual(knp.transpose(x, (2, 0, 1)).shape, (3, None, 3))\n \n     def test_arccos(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.arccos(x).shape, (2, 3))\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.arccos(x).shape, (None, 3))\n \n     def test_arcsin(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.arcsin(x).shape, (2, 3))\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.arcsin(x).shape, (None, 3))\n \n     def test_arctan(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.arctan(x).shape, (2, 3))\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.arctan(x).shape, (None, 3))\n \n     def test_argmax(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.argmax(x).shape, ())\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.argmax(x).shape, ())\n \n@@ -668,9 +662,6 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         self.assertEqual(knp.argmax(x, axis=1).shape, (None, 3))\n \n     def test_argmin(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.argmin(x).shape, ())\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.argmin(x).shape, ())\n \n@@ -678,10 +669,6 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         self.assertEqual(knp.argmin(x, axis=1).shape, (None, 3))\n \n     def test_argsort(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.argsort(x).shape, (2, 3))\n-        self.assertEqual(knp.argsort(x, axis=None).shape, (6,))\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.argsort(x).shape, (None, 3))\n \n@@ -689,16 +676,10 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         self.assertEqual(knp.argsort(x, axis=1).shape, (None, 3, 3))\n \n     def test_array(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.array(x).shape, (2, 3))\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.array(x).shape, (None, 3))\n \n     def test_average(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.average(x).shape, ())\n-\n         x = KerasTensor([None, 3])\n         weights = KerasTensor([None, 3])\n         self.assertEqual(knp.average(x, weights=weights).shape, ())\n@@ -716,9 +697,6 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n             knp.average(x, weights=weights)\n \n     def test_broadcast_to(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.broadcast_to(x, (2, 2, 3)).shape, (2, 2, 3))\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.broadcast_to(x, (2, 3, 3)).shape, (2, 3, 3))\n \n@@ -727,25 +705,14 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n             knp.broadcast_to(x, (2, 2, 3))\n \n     def test_ceil(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.ceil(x).shape, (2, 3))\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.ceil(x).shape, (None, 3))\n \n     def test_clip(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.clip(x, 1, 2).shape, (2, 3))\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.clip(x, 1, 2).shape, (None, 3))\n \n     def test_concatenate(self):\n-        x = KerasTensor([2, 3])\n-        y = KerasTensor([2, 3])\n-        self.assertEqual(knp.concatenate([x, y]).shape, (4, 3))\n-        self.assertEqual(knp.concatenate([x, y], axis=1).shape, (2, 6))\n-\n         x = KerasTensor([None, 3])\n         y = KerasTensor([None, 3])\n         self.assertEqual(\n@@ -765,37 +732,22 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n             knp.concatenate([x, y], axis=1)\n \n     def test_conjugate(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.conjugate(x).shape, (2, 3))\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.conjugate(x).shape, (None, 3))\n \n     def test_conj(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.conj(x).shape, (2, 3))\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.conj(x).shape, (None, 3))\n \n     def test_copy(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.copy(x).shape, (2, 3))\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.copy(x).shape, (None, 3))\n \n     def test_cos(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.cos(x).shape, (2, 3))\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.cos(x).shape, (None, 3))\n \n     def test_count_nonzero(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.count_nonzero(x).shape, ())\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.count_nonzero(x).shape, ())\n \n@@ -803,9 +755,6 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         self.assertEqual(knp.count_nonzero(x, axis=1).shape, (None, 3))\n \n     def test_cumprod(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.cumprod(x).shape, (6,))\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.cumprod(x).shape, (None,))\n \n@@ -813,15 +762,402 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         self.assertEqual(knp.cumprod(x, axis=1).shape, (None, 3, 3))\n \n     def test_cumsum(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(knp.cumsum(x).shape, (6,))\n-\n         x = KerasTensor([None, 3])\n         self.assertEqual(knp.cumsum(x).shape, (None,))\n \n         x = KerasTensor([None, 3, 3])\n         self.assertEqual(knp.cumsum(x, axis=1).shape, (None, 3, 3))\n \n+    def test_diag(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.diag(x).shape, (None,))\n+        self.assertEqual(knp.diag(x, k=3).shape, (None,))\n+\n+        with self.assertRaises(ValueError):\n+            x = KerasTensor([2, 3, 4])\n+            knp.diag(x)\n+\n+    def test_diagonal(self):\n+        x = KerasTensor([None, 3, 3])\n+        self.assertEqual(knp.diagonal(x).shape, (3, None))\n+\n+    def test_exp(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.exp(x).shape, (None, 3))\n+\n+    def test_expand_dims(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.expand_dims(x, 0).shape, (1, None, 3))\n+        self.assertEqual(knp.expand_dims(x, 1).shape, (None, 1, 3))\n+        self.assertEqual(knp.expand_dims(x, -2).shape, (None, 1, 3))\n+\n+    def test_expm1(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.expm1(x).shape, (None, 3))\n+\n+    def test_flip(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.flip(x).shape, (None, 3))\n+\n+    def test_floor(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.floor(x).shape, (None, 3))\n+\n+    def test_get_item(self):\n+        x = KerasTensor([None, None])\n+        self.assertEqual(knp.get_item(x, 5).shape, (None,))\n+\n+    def test_hstack(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([None, 3])\n+        self.assertEqual(knp.hstack([x, y]).shape, (None, 6))\n+\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([None, None])\n+        self.assertEqual(knp.hstack([x, y]).shape, (None, None))\n+\n+    def test_imag(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.imag(x).shape, (None, 3))\n+\n+    def test_isfinite(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.isfinite(x).shape, (None, 3))\n+\n+    def test_isinf(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.isinf(x).shape, (None, 3))\n+\n+    def test_isnan(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.isnan(x).shape, (None, 3))\n+\n+    def test_log(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.log(x).shape, (None, 3))\n+\n+    def test_log10(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.log10(x).shape, (None, 3))\n+\n+    def test_log1p(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.log1p(x).shape, (None, 3))\n+\n+    def test_log2(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.log2(x).shape, (None, 3))\n+\n+    def test_logaddexp(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.logaddexp(x, x).shape, (None, 3))\n+\n+    def test_logical_not(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.logical_not(x).shape, (None, 3))\n+\n+    def test_max(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.max(x).shape, ())\n+\n+    def test_meshgrid(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([None, 3])\n+        self.assertEqual(knp.meshgrid(x, y)[0].shape, (None, None))\n+        self.assertEqual(knp.meshgrid(x, y)[1].shape, (None, None))\n+\n+        with self.assertRaises(ValueError):\n+            knp.meshgrid(x, y, indexing=\"kk\")\n+\n+    def test_moveaxis(self):\n+        x = KerasTensor([None, 3, 4, 5])\n+        self.assertEqual(knp.moveaxis(x, 0, -1).shape, (3, 4, 5, None))\n+        self.assertEqual(knp.moveaxis(x, -1, 0).shape, (5, None, 3, 4))\n+        self.assertEqual(\n+            knp.moveaxis(x, [0, 1], [-1, -2]).shape, (4, 5, 3, None)\n+        )\n+        self.assertEqual(knp.moveaxis(x, [0, 1], [1, 0]).shape, (3, None, 4, 5))\n+        self.assertEqual(\n+            knp.moveaxis(x, [0, 1], [-2, -1]).shape, (4, 5, None, 3)\n+        )\n+\n+    def test_ndim(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.ndim(x).shape, (2,))\n+\n+    def test_ones_like(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.ones_like(x).shape, (None, 3))\n+\n+    def test_pad(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.pad(x, 1).shape, (None, 5))\n+        self.assertEqual(knp.pad(x, (1, 2)).shape, (None, 6))\n+        self.assertEqual(knp.pad(x, ((1, 2), (3, 4))).shape, (None, 10))\n+\n+        x = KerasTensor([None, 3, 3])\n+        self.assertEqual(knp.pad(x, 1).shape, (None, 5, 5))\n+        self.assertEqual(knp.pad(x, (1, 2)).shape, (None, 6, 6))\n+        self.assertEqual(\n+            knp.pad(x, ((1, 2), (3, 4), (5, 6))).shape, (None, 10, 14)\n+        )\n+\n+    def test_prod(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.prod(x).shape, ())\n+        self.assertEqual(knp.prod(x, axis=0).shape, (3,))\n+        self.assertEqual(knp.prod(x, axis=1, keepdims=True).shape, (None, 1))\n+\n+    def test_ravel(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.ravel(x).shape, (None,))\n+\n+    def test_real(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.real(x).shape, (None, 3))\n+\n+    def test_reciprocal(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.reciprocal(x).shape, (None, 3))\n+\n+    def test_repeat(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.repeat(x, 2).shape, (None,))\n+        self.assertEqual(knp.repeat(x, 3, axis=1).shape, (None, 9))\n+        self.assertEqual(knp.repeat(x, [1, 2], axis=0).shape, (3, 3))\n+\n+    def test_reshape(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.reshape(x, (3, 2)).shape, (3, 2))\n+\n+    def test_roll(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.roll(x, 1).shape, (None, 3))\n+        self.assertEqual(knp.roll(x, 1, axis=1).shape, (None, 3))\n+        self.assertEqual(knp.roll(x, 1, axis=0).shape, (None, 3))\n+\n+    def test_round(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.round(x).shape, (None, 3))\n+\n+    def test_sign(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.sign(x).shape, (None, 3))\n+\n+    def test_sin(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.sin(x).shape, (None, 3))\n+\n+    def test_size(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.size(x).shape, ())\n+\n+    def test_sort(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.sort(x).shape, (None, 3))\n+        self.assertEqual(knp.sort(x, axis=1).shape, (None, 3))\n+        self.assertEqual(knp.sort(x, axis=0).shape, (None, 3))\n+\n+    def test_split(self):\n+        x = KerasTensor([None, 3, 3])\n+        self.assertEqual(knp.split(x, 2)[0].shape, (None, 3, 3))\n+        self.assertEqual(knp.split(x, 3, axis=1)[0].shape, (None, 1, 3))\n+        self.assertEqual(knp.split(x, [1, 3], axis=1)[1].shape, (None, 2, 3))\n+\n+    def test_sqrt(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.sqrt(x).shape, (None, 3))\n+\n+    def test_stack(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([None, 3])\n+        self.assertEqual(knp.stack([x, y]).shape, (2, None, 3))\n+        self.assertEqual(knp.stack([x, y], axis=-1).shape, (None, 3, 2))\n+\n+    def test_std(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.std(x).shape, ())\n+\n+        x = KerasTensor([None, 3, 3])\n+        self.assertEqual(knp.std(x, axis=1).shape, (None, 3))\n+        self.assertEqual(knp.std(x, axis=1, keepdims=True).shape, (None, 1, 3))\n+\n+    def test_swapaxes(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.swapaxes(x, 0, 1).shape, (3, None))\n+\n+    def test_tan(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.tan(x).shape, (None, 3))\n+\n+    def test_tile(self):\n+        x = KerasTensor([None, 3])\n+        self.assertEqual(knp.tile(x, [2]).shape, (None, 6))\n+        self.assertEqual(knp.tile(x, [1, 2]).shape, (None, 6))\n+        self.assertEqual(knp.tile(x, [2, 1, 2]).shape, (2, None, 6))\n+\n+    def test_trace(self):\n+        x = KerasTensor([None, 3, None, 5])\n+        self.assertEqual(knp.trace(x).shape, (None, 5))\n+        self.assertEqual(knp.trace(x, axis1=2, axis2=3).shape, (None, 3))\n+\n+    def test_tril(self):\n+        x = KerasTensor([None, 3, None, 5])\n+        self.assertEqual(knp.tril(x).shape, (None, 3, None, 5))\n+        self.assertEqual(knp.tril(x, k=1).shape, (None, 3, None, 5))\n+        self.assertEqual(knp.tril(x, k=-1).shape, (None, 3, None, 5))\n+\n+    def test_triu(self):\n+        x = KerasTensor([None, 3, None, 5])\n+        self.assertEqual(knp.triu(x).shape, (None, 3, None, 5))\n+        self.assertEqual(knp.triu(x, k=1).shape, (None, 3, None, 5))\n+        self.assertEqual(knp.triu(x, k=-1).shape, (None, 3, None, 5))\n+\n+    def test_vstack(self):\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([None, 3])\n+        self.assertEqual(knp.vstack([x, y]).shape, (None, 3))\n+\n+        x = KerasTensor([None, 3])\n+        y = KerasTensor([None, None])\n+        self.assertEqual(knp.vstack([x, y]).shape, (None, 3))\n+\n+\n+class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n+    def test_mean(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.mean(x).shape, ())\n+\n+    def test_all(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.all(x).shape, ())\n+\n+    def test_var(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.var(x).shape, ())\n+\n+    def test_sum(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.sum(x).shape, ())\n+\n+    def test_amax(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.amax(x).shape, ())\n+\n+    def test_amin(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.amin(x).shape, ())\n+\n+    def test_square(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.square(x).shape, (2, 3))\n+\n+    def test_negative(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.negative(x).shape, (2, 3))\n+\n+    def test_abs(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.abs(x).shape, (2, 3))\n+\n+    def test_absolute(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.absolute(x).shape, (2, 3))\n+\n+    def test_squeeze(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.squeeze(x).shape, (2, 3))\n+\n+    def test_transpose(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.transpose(x).shape, (3, 2))\n+\n+    def test_arccos(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.arccos(x).shape, (2, 3))\n+\n+    def test_arcsin(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.arcsin(x).shape, (2, 3))\n+\n+    def test_arctan(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.arctan(x).shape, (2, 3))\n+\n+    def test_argmax(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.argmax(x).shape, ())\n+\n+    def test_argmin(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.argmin(x).shape, ())\n+\n+    def test_argsort(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.argsort(x).shape, (2, 3))\n+        self.assertEqual(knp.argsort(x, axis=None).shape, (6,))\n+\n+    def test_array(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.array(x).shape, (2, 3))\n+\n+    def test_average(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.average(x).shape, ())\n+\n+    def test_broadcast_to(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.broadcast_to(x, (2, 2, 3)).shape, (2, 2, 3))\n+\n+        with self.assertRaises(ValueError):\n+            x = KerasTensor([3, 3])\n+            knp.broadcast_to(x, (2, 2, 3))\n+\n+    def test_ceil(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.ceil(x).shape, (2, 3))\n+\n+    def test_clip(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.clip(x, 1, 2).shape, (2, 3))\n+\n+    def test_concatenate(self):\n+        x = KerasTensor([2, 3])\n+        y = KerasTensor([2, 3])\n+        self.assertEqual(knp.concatenate([x, y]).shape, (4, 3))\n+        self.assertEqual(knp.concatenate([x, y], axis=1).shape, (2, 6))\n+\n+        with self.assertRaises(ValueError):\n+            self.assertEqual(knp.concatenate([x, y], axis=None).shape, (None,))\n+\n+    def test_conjugate(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.conjugate(x).shape, (2, 3))\n+\n+    def test_conj(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.conj(x).shape, (2, 3))\n+\n+    def test_copy(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.copy(x).shape, (2, 3))\n+\n+    def test_cos(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.cos(x).shape, (2, 3))\n+\n+    def test_count_nonzero(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.count_nonzero(x).shape, ())\n+\n+    def test_cumprod(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.cumprod(x).shape, (6,))\n+\n+    def test_cumsum(self):\n+        x = KerasTensor([2, 3])\n+        self.assertEqual(knp.cumsum(x).shape, (6,))\n+\n     def test_diag(self):\n         x = KerasTensor([3])\n         self.assertEqual(knp.diag(x).shape, (3, 3))\n@@ -833,10 +1169,6 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         self.assertEqual(knp.diag(x, k=3).shape, (2,))\n         self.assertEqual(knp.diag(x, k=-2).shape, (1,))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.diag(x).shape, (None,))\n-        self.assertEqual(knp.diag(x, k=3).shape, (None,))\n-\n         with self.assertRaises(ValueError):\n             x = KerasTensor([2, 3, 4])\n             knp.diag(x)\n@@ -849,9 +1181,6 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         x = KerasTensor([3, 5, 5])\n         self.assertEqual(knp.diagonal(x).shape, (5, 3))\n \n-        x = KerasTensor([None, 3, 3])\n-        self.assertEqual(knp.diagonal(x).shape, (3, None))\n-\n         with self.assertRaises(ValueError):\n             x = KerasTensor([3])\n             knp.diagonal(x)\n@@ -860,41 +1189,24 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.exp(x).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.exp(x).shape, (None, 3))\n-\n     def test_expand_dims(self):\n         x = KerasTensor([2, 3, 4])\n         self.assertEqual(knp.expand_dims(x, 0).shape, (1, 2, 3, 4))\n         self.assertEqual(knp.expand_dims(x, 1).shape, (2, 1, 3, 4))\n         self.assertEqual(knp.expand_dims(x, -2).shape, (2, 3, 1, 4))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.expand_dims(x, 0).shape, (1, None, 3))\n-        self.assertEqual(knp.expand_dims(x, 1).shape, (None, 1, 3))\n-        self.assertEqual(knp.expand_dims(x, -2).shape, (None, 1, 3))\n-\n     def test_expm1(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.expm1(x).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.expm1(x).shape, (None, 3))\n-\n     def test_flip(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.flip(x).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.flip(x).shape, (None, 3))\n-\n     def test_floor(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.floor(x).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.floor(x).shape, (None, 3))\n-\n     def test_get_item(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.get_item(x, 1).shape, (3,))\n@@ -909,99 +1221,55 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         )\n         self.assertEqual(knp.get_item(x, 0).shape, ())\n \n-        x = KerasTensor([None, None])\n-        self.assertEqual(knp.get_item(x, 5).shape, (None,))\n-\n     def test_hstack(self):\n         x = KerasTensor([2, 3])\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.hstack([x, y]).shape, (2, 6))\n \n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([None, 3])\n-        self.assertEqual(knp.hstack([x, y]).shape, (None, 6))\n-\n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([None, None])\n-        self.assertEqual(knp.hstack([x, y]).shape, (None, None))\n-\n     def test_imag(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.imag(x).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.imag(x).shape, (None, 3))\n-\n     def test_isfinite(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.isfinite(x).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.isfinite(x).shape, (None, 3))\n-\n     def test_isinf(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.isinf(x).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.isinf(x).shape, (None, 3))\n-\n     def test_isnan(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.isnan(x).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.isnan(x).shape, (None, 3))\n-\n     def test_log(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.log(x).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.log(x).shape, (None, 3))\n-\n     def test_log10(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.log10(x).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.log10(x).shape, (None, 3))\n-\n     def test_log1p(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.log1p(x).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.log1p(x).shape, (None, 3))\n-\n     def test_log2(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.log2(x).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.log2(x).shape, (None, 3))\n-\n     def test_logaddexp(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.logaddexp(x, x).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.logaddexp(x, x).shape, (None, 3))\n-\n     def test_logical_not(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.logical_not(x).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.logical_not(x).shape, (None, 3))\n-\n     def test_max(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.max(x).shape, ())\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.max(x).shape, ())\n-\n     def test_meshgrid(self):\n         x = KerasTensor([2, 3])\n         y = KerasTensor([2, 3, 4])\n@@ -1012,12 +1280,6 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         self.assertEqual(\n             knp.meshgrid(x, y, z, indexing=\"ij\")[0].shape, (6, 24, 120)\n         )\n-\n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([None, 3])\n-        self.assertEqual(knp.meshgrid(x, y)[0].shape, (None, None))\n-        self.assertEqual(knp.meshgrid(x, y)[1].shape, (None, None))\n-\n         with self.assertRaises(ValueError):\n             knp.meshgrid(x, y, indexing=\"kk\")\n \n@@ -1029,58 +1291,20 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         self.assertEqual(knp.moveaxis(x, [0, 1], [1, 0]).shape, (3, 2, 4, 5))\n         self.assertEqual(knp.moveaxis(x, [0, 1], [-2, -1]).shape, (4, 5, 2, 3))\n \n-        x = KerasTensor([None, 3, 4, 5])\n-        self.assertEqual(knp.moveaxis(x, 0, -1).shape, (3, 4, 5, None))\n-        self.assertEqual(knp.moveaxis(x, -1, 0).shape, (5, None, 3, 4))\n-        self.assertEqual(\n-            knp.moveaxis(x, [0, 1], [-1, -2]).shape, (4, 5, 3, None)\n-        )\n-        self.assertEqual(knp.moveaxis(x, [0, 1], [1, 0]).shape, (3, None, 4, 5))\n-        self.assertEqual(\n-            knp.moveaxis(x, [0, 1], [-2, -1]).shape, (4, 5, None, 3)\n-        )\n-\n     def test_ndim(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.ndim(x).shape, (2,))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.ndim(x).shape, (2,))\n-\n-    def test_nonzero(self):\n-        x = KerasTensor([2, 3])\n-        self.assertEqual(len(knp.nonzero(x)), 2)\n-        self.assertEqual(knp.nonzero(x)[0].shape, (None,))\n-\n-        x = KerasTensor([None, 3])\n-        self.assertEqual(len(knp.nonzero(x)), 2)\n-        self.assertEqual(knp.nonzero(x)[0].shape, (None,))\n-\n     def test_ones_like(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.ones_like(x).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.ones_like(x).shape, (None, 3))\n-\n     def test_pad(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.pad(x, 1).shape, (4, 5))\n         self.assertEqual(knp.pad(x, (1, 2)).shape, (5, 6))\n         self.assertEqual(knp.pad(x, ((1, 2), (3, 4))).shape, (5, 10))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.pad(x, 1).shape, (None, 5))\n-        self.assertEqual(knp.pad(x, (1, 2)).shape, (None, 6))\n-        self.assertEqual(knp.pad(x, ((1, 2), (3, 4))).shape, (None, 10))\n-\n-        x = KerasTensor([None, 3, 3])\n-        self.assertEqual(knp.pad(x, 1).shape, (None, 5, 5))\n-        self.assertEqual(knp.pad(x, (1, 2)).shape, (None, 6, 6))\n-        self.assertEqual(\n-            knp.pad(x, ((1, 2), (3, 4), (5, 6))).shape, (None, 10, 14)\n-        )\n-\n         with self.assertRaises(ValueError):\n             x = KerasTensor([2, 3])\n             knp.pad(x, ((1, 2), (3, 4), (5, 6)))\n@@ -1091,100 +1315,56 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         self.assertEqual(knp.prod(x, axis=0).shape, (3,))\n         self.assertEqual(knp.prod(x, axis=1).shape, (2,))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.prod(x).shape, ())\n-        self.assertEqual(knp.prod(x, axis=0).shape, (3,))\n-        self.assertEqual(knp.prod(x, axis=1, keepdims=True).shape, (None, 1))\n-\n     def test_ravel(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.ravel(x).shape, (6,))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.ravel(x).shape, (None,))\n-\n     def test_real(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.real(x).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.real(x).shape, (None, 3))\n-\n     def test_reciprocal(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.reciprocal(x).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.reciprocal(x).shape, (None, 3))\n-\n     def test_repeat(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.repeat(x, 2).shape, (12,))\n         self.assertEqual(knp.repeat(x, 3, axis=1).shape, (2, 9))\n         self.assertEqual(knp.repeat(x, [1, 2], axis=0).shape, (3, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.repeat(x, 2).shape, (None,))\n-        self.assertEqual(knp.repeat(x, 3, axis=1).shape, (None, 9))\n-        self.assertEqual(knp.repeat(x, [1, 2], axis=0).shape, (3, 3))\n-\n     def test_reshape(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.reshape(x, (3, 2)).shape, (3, 2))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.reshape(x, (3, 2)).shape, (3, 2))\n-\n     def test_roll(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.roll(x, 1).shape, (2, 3))\n         self.assertEqual(knp.roll(x, 1, axis=1).shape, (2, 3))\n         self.assertEqual(knp.roll(x, 1, axis=0).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.roll(x, 1).shape, (None, 3))\n-        self.assertEqual(knp.roll(x, 1, axis=1).shape, (None, 3))\n-        self.assertEqual(knp.roll(x, 1, axis=0).shape, (None, 3))\n-\n     def test_round(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.round(x).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.round(x).shape, (None, 3))\n-\n     def test_sign(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.sign(x).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.sign(x).shape, (None, 3))\n-\n     def test_sin(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.sin(x).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.sin(x).shape, (None, 3))\n-\n     def test_size(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.size(x).shape, ())\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.size(x).shape, ())\n-\n     def test_sort(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.sort(x).shape, (2, 3))\n         self.assertEqual(knp.sort(x, axis=1).shape, (2, 3))\n         self.assertEqual(knp.sort(x, axis=0).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.sort(x).shape, (None, 3))\n-        self.assertEqual(knp.sort(x, axis=1).shape, (None, 3))\n-        self.assertEqual(knp.sort(x, axis=0).shape, (None, 3))\n-\n     def test_split(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(len(knp.split(x, 2)), 2)\n@@ -1192,11 +1372,6 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         self.assertEqual(knp.split(x, 3, axis=1)[0].shape, (2, 1))\n         self.assertEqual(knp.split(x, [1, 3], axis=1)[1].shape, (2, 2))\n \n-        x = KerasTensor([None, 3, 3])\n-        self.assertEqual(knp.split(x, 2)[0].shape, (None, 3, 3))\n-        self.assertEqual(knp.split(x, 3, axis=1)[0].shape, (None, 1, 3))\n-        self.assertEqual(knp.split(x, [1, 3], axis=1)[1].shape, (None, 2, 3))\n-\n         with self.assertRaises(ValueError):\n             knp.split(x, 2, axis=1)\n \n@@ -1204,20 +1379,12 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.sqrt(x).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.sqrt(x).shape, (None, 3))\n-\n     def test_stack(self):\n         x = KerasTensor([2, 3])\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.stack([x, y]).shape, (2, 2, 3))\n         self.assertEqual(knp.stack([x, y], axis=-1).shape, (2, 3, 2))\n \n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([None, 3])\n-        self.assertEqual(knp.stack([x, y]).shape, (2, None, 3))\n-        self.assertEqual(knp.stack([x, y], axis=-1).shape, (None, 3, 2))\n-\n         with self.assertRaises(ValueError):\n             x = KerasTensor([2, 3])\n             y = KerasTensor([3, 3])\n@@ -1227,82 +1394,42 @@ class NumpyOneInputOpsShapeTest(testing.TestCase):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.std(x).shape, ())\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.std(x).shape, ())\n-\n-        x = KerasTensor([None, 3, 3])\n-        self.assertEqual(knp.std(x, axis=1).shape, (None, 3))\n-        self.assertEqual(knp.std(x, axis=1, keepdims=True).shape, (None, 1, 3))\n-\n     def test_swapaxes(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.swapaxes(x, 0, 1).shape, (3, 2))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.swapaxes(x, 0, 1).shape, (3, None))\n-\n     def test_tan(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.tan(x).shape, (2, 3))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.tan(x).shape, (None, 3))\n-\n     def test_tile(self):\n         x = KerasTensor([2, 3])\n         self.assertEqual(knp.tile(x, [2]).shape, (2, 6))\n         self.assertEqual(knp.tile(x, [1, 2]).shape, (2, 6))\n         self.assertEqual(knp.tile(x, [2, 1, 2]).shape, (2, 2, 6))\n \n-        x = KerasTensor([None, 3])\n-        self.assertEqual(knp.tile(x, [2]).shape, (None, 6))\n-        self.assertEqual(knp.tile(x, [1, 2]).shape, (None, 6))\n-        self.assertEqual(knp.tile(x, [2, 1, 2]).shape, (2, None, 6))\n-\n     def test_trace(self):\n         x = KerasTensor([2, 3, 4, 5])\n         self.assertEqual(knp.trace(x).shape, (4, 5))\n         self.assertEqual(knp.trace(x, axis1=2, axis2=3).shape, (2, 3))\n \n-        x = KerasTensor([None, 3, None, 5])\n-        self.assertEqual(knp.trace(x).shape, (None, 5))\n-        self.assertEqual(knp.trace(x, axis1=2, axis2=3).shape, (None, 3))\n-\n     def test_tril(self):\n         x = KerasTensor([2, 3, 4, 5])\n         self.assertEqual(knp.tril(x).shape, (2, 3, 4, 5))\n         self.assertEqual(knp.tril(x, k=1).shape, (2, 3, 4, 5))\n         self.assertEqual(knp.tril(x, k=-1).shape, (2, 3, 4, 5))\n \n-        x = KerasTensor([None, 3, None, 5])\n-        self.assertEqual(knp.tril(x).shape, (None, 3, None, 5))\n-        self.assertEqual(knp.tril(x, k=1).shape, (None, 3, None, 5))\n-        self.assertEqual(knp.tril(x, k=-1).shape, (None, 3, None, 5))\n-\n     def test_triu(self):\n         x = KerasTensor([2, 3, 4, 5])\n         self.assertEqual(knp.triu(x).shape, (2, 3, 4, 5))\n         self.assertEqual(knp.triu(x, k=1).shape, (2, 3, 4, 5))\n         self.assertEqual(knp.triu(x, k=-1).shape, (2, 3, 4, 5))\n \n-        x = KerasTensor([None, 3, None, 5])\n-        self.assertEqual(knp.triu(x).shape, (None, 3, None, 5))\n-        self.assertEqual(knp.triu(x, k=1).shape, (None, 3, None, 5))\n-        self.assertEqual(knp.triu(x, k=-1).shape, (None, 3, None, 5))\n-\n     def test_vstack(self):\n         x = KerasTensor([2, 3])\n         y = KerasTensor([2, 3])\n         self.assertEqual(knp.vstack([x, y]).shape, (4, 3))\n \n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([None, 3])\n-        self.assertEqual(knp.vstack([x, y]).shape, (None, 3))\n-\n-        x = KerasTensor([None, 3])\n-        y = KerasTensor([None, None])\n-        self.assertEqual(knp.vstack([x, y]).shape, (None, 3))\n-\n \n class NumpyTwoInputOpsCorretnessTest(testing.TestCase):\n     def test_add(self):\n@@ -2365,6 +2492,8 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n             np.meshgrid(x, y, z, indexing=\"ij\"),\n         )\n \n+        if backend() == \"tensorflow\":\n+            # Arguments to `jax.numpy.meshgrid` must be 1D now.\n             x = np.ones([1, 2, 3])\n             y = np.ones([4, 5, 6, 6])\n             z = np.ones([7, 8])\n@@ -2374,8 +2503,12 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n                 np.array(knp.meshgrid(x, y, z, indexing=\"ij\")),\n                 np.meshgrid(x, y, z, indexing=\"ij\"),\n             )\n-        self.assertAllClose(np.array(knp.Meshgrid()(x, y)), np.meshgrid(x, y))\n-        self.assertAllClose(np.array(knp.Meshgrid()(x, z)), np.meshgrid(x, z))\n+            self.assertAllClose(\n+                np.array(knp.Meshgrid()(x, y)), np.meshgrid(x, y)\n+            )\n+            self.assertAllClose(\n+                np.array(knp.Meshgrid()(x, z)), np.meshgrid(x, z)\n+            )\n             self.assertAllClose(\n                 np.array(knp.Meshgrid(indexing=\"ij\")(x, y, z)),\n                 np.meshgrid(x, y, z, indexing=\"ij\"),\n@@ -2507,16 +2640,16 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n             np.array(knp.repeat(x, 3, axis=1)), np.repeat(x, 3, axis=1)\n         )\n         self.assertAllClose(\n-            np.array(knp.repeat(x, [1, 2], axis=-1)),\n-            np.repeat(x, [1, 2], axis=-1),\n+            np.array(knp.repeat(x, np.array([1, 2]), axis=-1)),\n+            np.repeat(x, np.array([1, 2]), axis=-1),\n         )\n         self.assertAllClose(np.array(knp.Repeat(2)(x)), np.repeat(x, 2))\n         self.assertAllClose(\n             np.array(knp.Repeat(3, axis=1)(x)), np.repeat(x, 3, axis=1)\n         )\n         self.assertAllClose(\n-            np.array(knp.Repeat([1, 2], axis=0)(x)),\n-            np.repeat(x, [1, 2], axis=0),\n+            np.array(knp.Repeat(np.array([1, 2]), axis=0)(x)),\n+            np.repeat(x, np.array([1, 2]), axis=0),\n         )\n \n     def test_reshape(self):\n@@ -2703,7 +2836,8 @@ class NumpyArrayCreateOpsCorrectnessTest(testing.TestCase):\n             np.array(knp.full([2, 3], 0.1)), np.full([2, 3], 0.1)\n         )\n         self.assertAllClose(\n-            np.array(knp.full([2, 3], [1, 4, 5])), np.full([2, 3], [1, 4, 5])\n+            np.array(knp.full([2, 3], np.array([1, 4, 5]))),\n+            np.full([2, 3], np.array([1, 4, 5])),\n         )\n \n         self.assertAllClose(np.array(knp.Full()([2, 3], 0)), np.full([2, 3], 0))\n@@ -2711,7 +2845,8 @@ class NumpyArrayCreateOpsCorrectnessTest(testing.TestCase):\n             np.array(knp.Full()([2, 3], 0.1)), np.full([2, 3], 0.1)\n         )\n         self.assertAllClose(\n-            np.array(knp.Full()([2, 3], [1, 4, 5])), np.full([2, 3], [1, 4, 5])\n+            np.array(knp.Full()([2, 3], np.array([1, 4, 5]))),\n+            np.full([2, 3], np.array([1, 4, 5])),\n         )\n \n     def test_identity(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
