{"custom_id": "keras#097e46837cd48fe53e59971e8b5237e242797b0c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 977 | Contributors (this commit): 11 | Commits (past 90d): 7 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -49,6 +49,8 @@ class CallbackList(object):\n         self._t_enter_batch = time.time()\n \n     def on_batch_end(self, batch, logs={}):\n+        if not hasattr(self, '_t_enter_batch'):\n+            self._t_enter_batch = time.time()\n         self._delta_t_batch = time.time() - self._t_enter_batch\n         t_before_callbacks = time.time()\n         for callback in self.callbacks:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f9911c10b46df0ffaf93cc90a9c008f00026d23d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 12 | Files Changed: 4 | Hunks: 8 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 589 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -10,7 +10,6 @@ def load_data():\n     origin = \"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n     path = get_file(dirname, origin=origin, untar=True)\n \n-    nb_test_samples = 10000\n     nb_train_samples = 50000\n \n     X_train = np.zeros((nb_train_samples, 3, 32, 32), dtype=\"uint8\")\n\n@@ -2,12 +2,12 @@ from __future__ import absolute_import\n from six.moves import cPickle\n import gzip\n from .data_utils import get_file\n-import random\n from six.moves import zip\n import numpy as np\n \n \n-def load_data(path=\"imdb.pkl\", nb_words=None, skip_top=0, maxlen=None, test_split=0.2, seed=113,\n+def load_data(path=\"imdb.pkl\", nb_words=None, skip_top=0,\n+              maxlen=None, test_split=0.2, seed=113,\n               start_char=1, oov_char=2, index_from=3):\n \n     path = get_file(path, origin=\"https://s3.amazonaws.com/text-datasets/imdb.pkl\")\n\n@@ -19,5 +19,4 @@ def load_data(path=\"mnist.pkl.gz\"):\n         data = cPickle.load(f, encoding=\"bytes\")\n \n     f.close()\n-\n     return data  # (X_train, y_train), (X_test, y_test)\n\n@@ -1,18 +1,17 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n from .data_utils import get_file\n-import random\n from six.moves import cPickle\n from six.moves import zip\n import numpy as np\n \n \n-def load_data(path=\"reuters.pkl\", nb_words=None, skip_top=0, maxlen=None, test_split=0.2, seed=113,\n+def load_data(path=\"reuters.pkl\", nb_words=None, skip_top=0,\n+              maxlen=None, test_split=0.2, seed=113,\n               start_char=1, oov_char=2, index_from=3):\n \n     path = get_file(path, origin=\"https://s3.amazonaws.com/text-datasets/reuters.pkl\")\n     f = open(path, 'rb')\n-\n     X, labels = cPickle.load(f)\n     f.close()\n \n@@ -66,8 +65,3 @@ def get_word_index(path=\"reuters_word_index.pkl\"):\n     path = get_file(path, origin=\"https://s3.amazonaws.com/text-datasets/reuters_word_index.pkl\")\n     f = open(path, 'rb')\n     return cPickle.load(f)\n-\n-\n-if __name__ == \"__main__\":\n-    make_reuters_dataset()\n-    (X_train, y_train), (X_test, y_test) = load_data()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3f67168c44f8c7d34e5266a99bbdd90669a8eaa6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 1052 | Contributors (this commit): 5 | Commits (past 90d): 17 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -64,7 +64,7 @@ def test_sequential_fit_generator():\n     model.fit_generator(data_generator(True), len(X_train), nb_epoch, show_accuracy=True, validation_data=(X_test, y_test))\n \n     loss = model.evaluate(X_train, y_train, verbose=0)\n-    assert(loss < 0.8)\n+    assert(loss < 0.9)\n \n \n def test_sequential():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f9325e8fe5fe002f9720b7cb1c6d3614948a10ea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 3965 | Contributors (this commit): 27 | Commits (past 90d): 16 | Contributors (cumulative): 27 | DMM Complexity: None\n\nDIFF:\n@@ -836,7 +836,7 @@ class Sequential(Model, containers.Sequential):\n             while not _stop.is_set():\n                 try:\n                     if generator_queue.qsize() < max_queue_size:\n-                        generator_output = generator.next()\n+                        generator_output = next(generator)\n                         generator_queue.put(generator_output)\n                         i += 1\n                     else:\n@@ -1255,7 +1255,7 @@ class Graph(Model, containers.Graph):\n             while not _stop.is_set():\n                 try:\n                     if generator_queue.qsize() < max_queue_size:\n-                        generator_output = generator.next()\n+                        generator_output = next(generator)\n                         generator_queue.put(generator_output)\n                         i += 1\n                     else:\n@@ -1284,7 +1284,7 @@ class Graph(Model, containers.Graph):\n                 data, sample_weight = input_validation(generator_output)\n \n                 batch_logs = {}\n-                batch_size = len(data[data.keys()[0]])\n+                batch_size = len(data[list(data.keys())[0]])\n                 batch_logs['batch'] = batch_index\n                 batch_logs['size'] = batch_size\n                 callbacks.on_batch_begin(batch_index, batch_logs)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#93b01aff157253ed28fe67b0ffc5ff3308dc3eba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1060 | Contributors (this commit): 5 | Commits (past 90d): 18 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -139,7 +139,7 @@ def test_merge_sum():\n     model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n \n     loss = model.evaluate([X_test, X_test], y_test, verbose=0)\n-    assert(loss < 0.7)\n+    assert(loss < 0.8)\n \n     model.predict([X_test, X_test], verbose=0)\n     model.predict_classes([X_test, X_test], verbose=0)\n@@ -224,7 +224,7 @@ def test_merge_concat():\n     model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n \n     loss = model.evaluate([X_test, X_test], y_test, verbose=0)\n-    assert(loss < 0.7)\n+    assert(loss < 0.8)\n \n     model.predict([X_test, X_test], verbose=0)\n     model.predict_classes([X_test, X_test], verbose=0)\n@@ -287,7 +287,7 @@ def test_merge_recursivity():\n     model.fit([X_train, X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n \n     loss = model.evaluate([X_test, X_test, X_test], y_test, verbose=0)\n-    assert(loss < 0.7)\n+    assert(loss < 0.8)\n \n     model.predict([X_test, X_test, X_test], verbose=0)\n     model.predict_classes([X_test, X_test, X_test], verbose=0)\n@@ -375,7 +375,7 @@ def test_lambda():\n     model.fit([X_train, X_train], y_train, batch_size=batch_size, nb_epoch=nb_epoch, verbose=0, shuffle=False)\n \n     loss = model.evaluate([X_test, X_test], y_test, verbose=0)\n-    assert(loss < 0.7)\n+    assert(loss < 0.8)\n \n     model.predict([X_test, X_test], verbose=0)\n     model.predict_classes([X_test, X_test], verbose=0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e5d3abdf09d8c281ca8817b6292a044673ba3007", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 238 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -1,10 +1,17 @@\n-import pydot\n-# old pydot will not work with python3, must use one\n-# that works with python3 such as pydot2 or pydot\n import itertools\n from keras.layers.containers import Graph, Sequential\n from keras.layers.core import Merge\n \n+try:\n+    # pydot-ng is a fork of pydot that is better maintained\n+    import pydot_ng as pydot\n+except ImportError:\n+    # fall back on pydot if necessary\n+    import pydot\n+if not pydot.find_graphviz():\n+    raise RuntimeError(\"Failed to import pydot. You must install pydot\"\n+                       \" and graphviz for `pydotprint` to work.\")\n+\n \n def layer_typename(layer):\n     return type(layer).__module__ + \".\" + type(layer).__name__\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bdf084e35ef015eae45d3eed79758f652c717056", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 3 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 10156 | Contributors (this commit): 41 | Commits (past 90d): 154 | Contributors (cumulative): 52 | DMM Complexity: None\n\nDIFF:\n@@ -1018,7 +1018,7 @@ class TimeDistributedDense(MaskedLayer):\n         input_dim = self.input_shape[2]\n \n         self.W = self.init((input_dim, self.output_dim))\n-        self.b = K.zeros((self.output_dim))\n+        self.b = K.zeros((self.output_dim,))\n \n         self.params = [self.W, self.b]\n         self.regularizers = []\n\n@@ -50,12 +50,12 @@ class BatchNormalization(Layer):\n         input_shape = self.input_shape  # starts with samples axis\n         input_shape = input_shape[1:]\n \n-        self.gamma = self.init((input_shape))\n+        self.gamma = self.init(input_shape)\n         self.beta = K.zeros(input_shape)\n \n         self.params = [self.gamma, self.beta]\n         self.running_mean = K.zeros(input_shape)\n-        self.running_std = K.ones((input_shape))\n+        self.running_std = K.ones(input_shape)\n \n         # initialize self.updates: batch mean/std computation\n         X = self.get_input(train=True)\n\n@@ -205,7 +205,7 @@ class SimpleRNN(Recurrent):\n \n         self.W = self.init((input_dim, self.output_dim))\n         self.U = self.inner_init((self.output_dim, self.output_dim))\n-        self.b = K.zeros((self.output_dim))\n+        self.b = K.zeros((self.output_dim,))\n         self.params = [self.W, self.U, self.b]\n \n         if self.initial_weights is not None:\n@@ -391,19 +391,19 @@ class LSTM(Recurrent):\n \n         self.W_i = self.init((input_dim, self.output_dim))\n         self.U_i = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_i = K.zeros((self.output_dim))\n+        self.b_i = K.zeros((self.output_dim,))\n \n         self.W_f = self.init((input_dim, self.output_dim))\n         self.U_f = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_f = self.forget_bias_init((self.output_dim))\n+        self.b_f = self.forget_bias_init((self.output_dim,))\n \n         self.W_c = self.init((input_dim, self.output_dim))\n         self.U_c = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_c = K.zeros((self.output_dim))\n+        self.b_c = K.zeros((self.output_dim,))\n \n         self.W_o = self.init((input_dim, self.output_dim))\n         self.U_o = self.inner_init((self.output_dim, self.output_dim))\n-        self.b_o = K.zeros((self.output_dim))\n+        self.b_o = K.zeros((self.output_dim,))\n \n         self.params = [self.W_i, self.U_i, self.b_i,\n                        self.W_c, self.U_c, self.b_c,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#80096798fc3ecbd51e8e7a9109d6344d808d03a7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 1062 | Contributors (this commit): 5 | Commits (past 90d): 19 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -42,6 +42,8 @@ def _get_test_data():\n ####################\n \n def test_sequential_fit_generator():\n+    (X_train, y_train), (X_test, y_test) = _get_test_data()\n+\n     def data_generator(train):\n         if train:\n             max_batch_index = len(X_train) // batch_size\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dd58103a3cdee9cf57c3def74de22032c7765348", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 7 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 448 | Contributors (this commit): 2 | Commits (past 90d): 7 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -9,7 +9,7 @@ References:\n   \"End-To-End Memory Networks\",\n   http://arxiv.org/abs/1503.08895\n \n-Reaches 93% accuracy on task 'single_supporting_fact_10k' after 70 epochs.\n+Reaches 98.6% accuracy on task 'single_supporting_fact_10k' after 120 epochs.\n Time per epoch: 3s on CPU (core i7).\n '''\n \n@@ -153,12 +153,14 @@ input_encoder_m = Sequential()\n input_encoder_m.add(Embedding(input_dim=vocab_size,\n                               output_dim=64,\n                               input_length=story_maxlen))\n+input_encoder_m.add(Dropout(0.3))\n # output: (samples, story_maxlen, embedding_dim)\n # embed the question into a sequence of vectors\n question_encoder = Sequential()\n question_encoder.add(Embedding(input_dim=vocab_size,\n                                output_dim=64,\n                                input_length=query_maxlen))\n+question_encoder.add(Dropout(0.3))\n # output: (samples, query_maxlen, embedding_dim)\n # compute a 'match' between input sequence elements (which are vectors)\n # and the question vector sequence\n@@ -172,6 +174,7 @@ input_encoder_c = Sequential()\n input_encoder_c.add(Embedding(input_dim=vocab_size,\n                               output_dim=query_maxlen,\n                               input_length=story_maxlen))\n+input_encoder_c.add(Dropout(0.3))\n # output: (samples, story_maxlen, query_maxlen)\n # sum the match vector with the input vector:\n response = Sequential()\n@@ -185,9 +188,9 @@ answer = Sequential()\n answer.add(Merge([response, question_encoder], mode='concat', concat_axis=-1))\n # the original paper uses a matrix multiplication for this reduction step.\n # we choose to use a RNN instead.\n-answer.add(LSTM(64))\n+answer.add(LSTM(32))\n # one regularization layer -- more would probably be needed.\n-answer.add(Dropout(0.25))\n+answer.add(Dropout(0.3))\n answer.add(Dense(vocab_size))\n # we output a probability distribution over the vocabulary\n answer.add(Activation('softmax'))\n@@ -196,6 +199,6 @@ answer.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n # Note: you could use a Graph model to avoid repeat the input twice\n answer.fit([inputs_train, queries_train, inputs_train], answers_train,\n            batch_size=32,\n-           nb_epoch=70,\n+           nb_epoch=120,\n            show_accuracy=True,\n            validation_data=([inputs_test, queries_test, inputs_test], answers_test))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8d3b8ff62795733ba8adcedfe454728a6f2a1181", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 33 | Churn Cumulative: 1010 | Contributors (this commit): 11 | Commits (past 90d): 6 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,6 +8,7 @@ import warnings\n \n from collections import deque\n from .utils.generic_utils import Progbar\n+from keras import backend as K\n \n \n class CallbackList(object):\n@@ -301,23 +302,46 @@ class EarlyStopping(Callback):\n         patience: number of epochs with no improvement\n             after which training will be stopped.\n         verbose: verbosity mode.\n+        mode: one of {auto, min, max}. In 'min' mode,\n+            training will stop when the quantity\n+            monitored has stopped decreasing; in 'max'\n+            mode it will stopped when the quantity\n+            monitored has stopped increasing.\n     '''\n-    def __init__(self, monitor='val_loss', patience=0, verbose=0):\n+    def __init__(self, monitor='val_loss', patience=0, verbose=0, mode='auto'):\n         super(Callback, self).__init__()\n \n         self.monitor = monitor\n         self.patience = patience\n         self.verbose = verbose\n-        self.best = np.Inf\n         self.wait = 0\n \n+        if mode not in ['auto', 'min', 'max']:\n+            warnings.warn('EarlyStopping mode %s is unknown, '\n+                          'fallback to auto mode' % (self.mode), RuntimeWarning)\n+            mode = 'auto'\n+\n+        if mode == 'min':\n+            self.monitor_op = np.less\n+            self.best = np.Inf\n+        elif mode == 'max':\n+            self.monitor_op = np.greater\n+            self.best = -np.Inf\n+        else:\n+            if 'acc' in self.monitor:\n+                self.monitor_op = np.greater\n+                self.best = -np.Inf\n+            else:\n+                self.monitor_op = np.less\n+                self.best = np.Inf\n+\n     def on_epoch_end(self, epoch, logs={}):\n         current = logs.get(self.monitor)\n         if current is None:\n             warnings.warn('Early stopping requires %s available!' %\n                           (self.monitor), RuntimeWarning)\n \n-        if current < self.best:\n+        if self.monitor_op(current, self.best):\n             self.best = current\n             self.wait = 0\n         else:\n@@ -380,4 +404,5 @@ class LearningRateScheduler(Callback):\n         self.schedule = schedule\n \n     def on_epoch_begin(self, epoch, logs={}):\n-        self.model.optimizer.lr.set_value(self.schedule(epoch))\n+        assert hasattr(self.model.optimizer, 'lr'), 'Optimizer must have a \"lr\" attribute.'\n+        K.set_value(self.model.optimizer.lr, self.schedule(epoch))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e34f9e6debf3b39da77c72e8a4c75cf7ccd94ef9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 117 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 117 | Churn Cumulative: 117 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,117 @@\n+import pytest\n+import os\n+import numpy as np\n+np.random.seed(1337)\n+\n+from keras import callbacks\n+from keras.models import Graph, Sequential\n+from keras.layers.core import Dense\n+from keras.utils.test_utils import get_test_data\n+from keras import backend as K\n+\n+input_dim = 2\n+nb_hidden = 4\n+nb_class = 2\n+batch_size = 5\n+train_samples = 20\n+test_samples = 20\n+\n+\n+def test_ModelCheckpoint():\n+    filepath = 'checkpoint.h5'\n+    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=train_samples,\n+                                                         nb_test=test_samples,\n+                                                         input_shape=(input_dim,),\n+                                                         classification=True,\n+                                                         nb_class=nb_class)\n+    # case 1\n+    monitor = 'val_loss'\n+    save_best_only = False\n+    mode = 'auto'\n+\n+    model = Sequential()\n+    model.add(Dense(nb_hidden, input_dim=input_dim, activation='relu'))\n+    model.add(Dense(nb_class, activation='softmax'))\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    cbks = [callbacks.ModelCheckpoint(filepath, monitor=monitor,\n+                                      save_best_only=save_best_only, mode=mode)]\n+    model.fit(X_train, y_train, batch_size=batch_size, show_accuracy=True,\n+              validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=1)\n+    assert os.path.exists(filepath)\n+    os.remove(filepath)\n+\n+    # case 2\n+    mode = 'min'\n+    cbks = [callbacks.ModelCheckpoint(filepath, monitor=monitor,\n+                                      save_best_only=save_best_only, mode=mode)]\n+    model.fit(X_train, y_train, batch_size=batch_size, show_accuracy=True,\n+              validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=1)\n+    assert os.path.exists(filepath)\n+    os.remove(filepath)\n+\n+    # case 3\n+    mode = 'max'\n+    monitor = 'val_acc'\n+    cbks = [callbacks.ModelCheckpoint(filepath, monitor=monitor,\n+                                      save_best_only=save_best_only, mode=mode)]\n+    model.fit(X_train, y_train, batch_size=batch_size, show_accuracy=True,\n+              validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=1)\n+    assert os.path.exists(filepath)\n+    os.remove(filepath)\n+\n+    # case 4\n+    save_best_only = True\n+    cbks = [callbacks.ModelCheckpoint(filepath, monitor=monitor,\n+                                      save_best_only=save_best_only, mode=mode)]\n+    model.fit(X_train, y_train, batch_size=batch_size, show_accuracy=True,\n+              validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=1)\n+    assert os.path.exists(filepath)\n+    os.remove(filepath)\n+\n+\n+def test_EarlyStopping():\n+    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=train_samples,\n+                                                         nb_test=test_samples,\n+                                                         input_shape=(input_dim,),\n+                                                         classification=True,\n+                                                         nb_class=nb_class)\n+    model = Sequential()\n+    model.add(Dense(nb_hidden, input_dim=input_dim, activation='relu'))\n+    model.add(Dense(nb_class, activation='softmax'))\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+\n+    mode = 'max'\n+    monitor = 'val_acc'\n+    patience = 0\n+    cbks = [callbacks.EarlyStopping(patience=patience, monitor=monitor, mode=mode)]\n+    history = model.fit(X_train, y_train, batch_size=batch_size, show_accuracy=True,\n+                        validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=20)\n+\n+    mode = 'auto'\n+    monitor = 'val_acc'\n+    patience = 2\n+    cbks = [callbacks.EarlyStopping(patience=patience, monitor=monitor, mode=mode)]\n+    history = model.fit(X_train, y_train, batch_size=batch_size, show_accuracy=True,\n+                        validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=20)\n+\n+\n+def test_LearningRateScheduler():\n+    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=train_samples,\n+                                                         nb_test=test_samples,\n+                                                         input_shape=(input_dim,),\n+                                                         classification=True,\n+                                                         nb_class=nb_class)\n+    model = Sequential()\n+    model.add(Dense(nb_hidden, input_dim=input_dim, activation='relu'))\n+    model.add(Dense(nb_class, activation='softmax'))\n+    model.compile(loss='categorical_crossentropy', optimizer='sgd')\n+\n+    cbks = [callbacks.LearningRateScheduler(lambda x: 1. / (1. + x))]\n+    model.fit(X_train, y_train, batch_size=batch_size, show_accuracy=True,\n+              validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=5)\n+    assert (float(K.get_value(model.optimizer.lr)) - 0.2) < K.epsilon()\n+\n+\n+if __name__ == '__main__':\n+    pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6ec1f7a498097c8176598e8c60c50b103353b0ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 125 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,6 +8,7 @@ from keras.models import Graph, Sequential\n from keras.layers.core import Dense\n from keras.utils.test_utils import get_test_data\n from keras import backend as K\n+from keras.utils import np_utils\n \n input_dim = 2\n nb_hidden = 4\n@@ -24,6 +25,8 @@ def test_ModelCheckpoint():\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n                                                          nb_class=nb_class)\n+    y_test = np_utils.to_categorical(y_test)\n+    y_train = np_utils.to_categorical(y_train)\n     # case 1\n     monitor = 'val_loss'\n     save_best_only = False\n@@ -76,6 +79,8 @@ def test_EarlyStopping():\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n                                                          nb_class=nb_class)\n+    y_test = np_utils.to_categorical(y_test)\n+    y_train = np_utils.to_categorical(y_train)\n     model = Sequential()\n     model.add(Dense(nb_hidden, input_dim=input_dim, activation='relu'))\n     model.add(Dense(nb_class, activation='softmax'))\n@@ -102,6 +107,8 @@ def test_LearningRateScheduler():\n                                                          input_shape=(input_dim,),\n                                                          classification=True,\n                                                          nb_class=nb_class)\n+    y_test = np_utils.to_categorical(y_test)\n+    y_train = np_utils.to_categorical(y_train)\n     model = Sequential()\n     model.add(Dense(nb_hidden, input_dim=input_dim, activation='relu'))\n     model.add(Dense(nb_class, activation='softmax'))\n@@ -115,3 +122,4 @@ def test_LearningRateScheduler():\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#80eab1bc022d88d202d5aec87b4d6f446cf5d77a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 179 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 11 | Methods Changed: 7 | Complexity Δ (Sum/Max): 15/14 | Churn Δ: 186 | Churn Cumulative: 1321 | Contributors (this commit): 12 | Commits (past 90d): 10 | Contributors (cumulative): 14 | DMM Complexity: 0.6046511627906976\n\nDIFF:\n@@ -8,6 +8,7 @@ import warnings\n \n from collections import deque\n from .utils.generic_utils import Progbar\n+from .backend import _BACKEND\n from keras import backend as K\n \n \n@@ -44,9 +45,11 @@ class CallbackList(object):\n             callback.on_batch_begin(batch, logs)\n         self._delta_ts_batch_begin.append(time.time() - t_before_callbacks)\n         delta_t_median = np.median(self._delta_ts_batch_begin)\n-        if self._delta_t_batch > 0. and delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1:\n+        if self._delta_t_batch > 0. and delta_t_median > 0.95 * \\\n+           self._delta_t_batch and delta_t_median > 0.1:\n             warnings.warn('Method on_batch_begin() is slow compared '\n-                          'to the batch update (%f). Check your callbacks.' % delta_t_median)\n+                          'to the batch update (%f). Check your callbacks.'\n+                          % delta_t_median)\n         self._t_enter_batch = time.time()\n \n     def on_batch_end(self, batch, logs={}):\n@@ -58,9 +61,11 @@ class CallbackList(object):\n             callback.on_batch_end(batch, logs)\n         self._delta_ts_batch_end.append(time.time() - t_before_callbacks)\n         delta_t_median = np.median(self._delta_ts_batch_end)\n-        if self._delta_t_batch > 0. and delta_t_median > 0.95 * self._delta_t_batch and delta_t_median > 0.1:\n+        if self._delta_t_batch > 0. and delta_t_median > 0.95 * \\\n+           self._delta_t_batch and delta_t_median > 0.1:\n             warnings.warn('Method on_batch_end() is slow compared '\n-                          'to the batch update (%f). Check your callbacks.' % delta_t_median)\n+                          'to the batch update (%f). Check your callbacks.'\n+                          % delta_t_median)\n \n     def on_train_begin(self, logs={}):\n         for callback in self.callbacks:\n@@ -252,7 +257,8 @@ class ModelCheckpoint(Callback):\n \n         if mode not in ['auto', 'min', 'max']:\n             warnings.warn('ModelCheckpoint mode %s is unknown, '\n-                          'fallback to auto mode' % (self.mode), RuntimeWarning)\n+                          'fallback to auto mode' % (self.mode),\n+                          RuntimeWarning)\n             mode = 'auto'\n \n         if mode == 'min':\n@@ -279,7 +285,8 @@ class ModelCheckpoint(Callback):\n             else:\n                 if self.monitor_op(current, self.best):\n                     if self.verbose > 0:\n-                        print('Epoch %05d: %s improved from %0.5f to %0.5f, saving model to %s'\n+                        print('Epoch %05d: %s improved from %0.5f to %0.5f,'\n+                              ' saving model to %s'\n                               % (epoch, self.monitor, self.best,\n                                  current, filepath))\n                     self.best = current\n@@ -404,5 +411,88 @@ class LearningRateScheduler(Callback):\n         self.schedule = schedule\n \n     def on_epoch_begin(self, epoch, logs={}):\n-        assert hasattr(self.model.optimizer, 'lr'), 'Optimizer must have a \"lr\" attribute.'\n+        assert hasattr(self.model.optimizer, 'lr'), \\\n+            'Optimizer must have a \"lr\" attribute.'\n         K.set_value(self.model.optimizer.lr, self.schedule(epoch))\n+\n+\n+class TensorBoard(Callback):\n+    ''' Tensorboard basic visualizations.\n+\n+    This callback writes a log usable with TensorBoard.\n+    TensorBoard is a visualization tools provided with TensorFlow.\n+\n+    If you have installed TensorFlow with pip, you should be able\n+    to launch TensorBoard from the command line:\n+    ```\n+    tensorboard --logdir=/full_path_to_your_logs\n+    ```\n+    You could find more information at:\n+    https://www.tensorflow.org/versions/master/how_tos/summaries_and_tensorboard/index.html\n+\n+    # Arguments\n+        model: a keras model linked to a tensorflow session\n+        feed: a dictionnary mapping tensors (inputs, outputs, weigths)\n+            from the model._test keras function i.e. model._test.inputs\n+            to the corresponding arrays.\n+        freq: the frequency at which the callback will output\n+            parameters and metrics to the log\n+        log_dir: the path of the directory where to save the log\n+            files to be parsed by tensorboard\n+    '''\n+    def __init__(self, model, feed, freq=2, log_dir='./logs',\n+                 show_accuracy=False):\n+        super(Callback, self).__init__()\n+        assert _BACKEND == 'tensorflow', \\\n+            'TensorBoard callback only works with the tensorflow backend'\n+        import tensorflow as tf\n+        import keras.backend.tensorflow_backend as KTF\n+\n+        self.model = model\n+        self.freq = freq\n+        self.log_dir = log_dir\n+        self.sess = KTF._get_session()\n+        self.feed = feed\n+        mod_type = self.model.get_config()['name']\n+        if mod_type == 'Sequential':\n+            layers = {l.get_config()['name']: l for l in self.model.layers}\n+        elif mod_type == 'Graph':\n+            layers = self.model.nodes\n+        else:\n+            raise Exception('Unrecognized model:',\n+                            self.model.get_config()['name'])\n+        for l in layers:\n+            cur_layer = layers[l]\n+            if hasattr(cur_layer, 'W'):\n+                tf.histogram_summary('{}_W'.format(l), cur_layer.W)\n+            if hasattr(cur_layer, 'b'):\n+                tf.histogram_summary('{}_b'.format(l), cur_layer.b)\n+            if hasattr(cur_layer, 'get_output'):\n+                tf.histogram_summary('{}_out'.format(l),\n+                                     cur_layer.get_output())\n+        f_output = self.model._test\n+        if mod_type == 'Sequential':\n+            if show_accuracy is True:\n+                f_output = self.model._test_with_acc\n+                tf.scalar_summary('Accuracy',\n+                                  f_output.outputs[1])\n+            tf.scalar_summary('Loss',\n+                              f_output.outputs[0])\n+        else:\n+            losses = [self.model.loss[loss] for loss in self.model.loss]\n+            if len(losses) > 1:\n+                l_name = \" + \".join(losses)\n+            else:\n+                l_name = losses[0]\n+            tf.scalar_summary(l_name,\n+                              f_output.outputs[0])\n+        self.merged = tf.merge_all_summaries()\n+        self.writer = tf.train.SummaryWriter(self.log_dir,\n+                                             self.sess.graph_def)\n+\n+    def on_epoch_end(self, epoch, logs={}):\n+        if epoch % self.freq == 0:\n+            result = self.sess.run([self.merged],\n+                                   feed_dict=self.feed)\n+            summary_str = result[0]\n+            self.writer.add_summary(summary_str, epoch)\n\n@@ -9,6 +9,7 @@ from keras.layers.core import Dense\n from keras.utils.test_utils import get_test_data\n from keras import backend as K\n from keras.utils import np_utils\n+from keras.callbacks import _BACKEND\n \n input_dim = 2\n nb_hidden = 4\n@@ -120,6 +121,87 @@ def test_LearningRateScheduler():\n     assert (float(K.get_value(model.optimizer.lr)) - 0.2) < K.epsilon()\n \n \n+@pytest.mark.skipif(_BACKEND != 'tensorflow',\n+                    reason=\"Requires tensorflow backend\")\n+def test_TensorBoard():\n+    import shutil\n+    import tensorflow as tf\n+    import keras.backend.tensorflow_backend as KTF\n+    old_session = KTF._get_session()\n+    filepath = './logs'\n+    (X_train, y_train), (X_test, y_test) = get_test_data(nb_train=train_samples,\n+                                                         nb_test=test_samples,\n+                                                         input_shape=(input_dim,),\n+                                                         classification=True,\n+                                                         nb_class=nb_class)\n+    y_test = np_utils.to_categorical(y_test)\n+    y_train = np_utils.to_categorical(y_train)\n+    # case 1 Sequential wo accuracy\n+    with tf.Graph().as_default():\n+        session = tf.Session('')\n+        KTF._set_session(session)\n+        model = Sequential()\n+        model.add(Dense(nb_hidden, input_dim=input_dim, activation='relu'))\n+        model.add(Dense(nb_class, activation='softmax'))\n+        model.compile(loss='categorical_crossentropy', optimizer='sgd')\n+\n+        feed = {model._test.inputs[0]: X_train, model._test.inputs[1]: y_train,\n+                model._test.inputs[2]: np.ones(train_samples)}\n+        tsb = callbacks.TensorBoard(model=model, feed=feed, log_dir=filepath,\n+                                    show_accuracy=False)\n+        cbks = [tsb]\n+        model.fit(X_train, y_train, batch_size=batch_size, show_accuracy=True,\n+                  validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=5)\n+        assert os.path.exists(filepath)\n+        shutil.rmtree(filepath)\n+\n+    # case 2 Sequential w accuracy\n+    with tf.Graph().as_default():\n+        session = tf.Session('')\n+        KTF._set_session(session)\n+        model = Sequential()\n+        model.add(Dense(nb_hidden, input_dim=input_dim, activation='relu'))\n+        model.add(Dense(nb_class, activation='softmax'))\n+        model.compile(loss='categorical_crossentropy', optimizer='sgd')\n+\n+        feed = {model._test.inputs[0]: X_train, model._test.inputs[1]: y_train,\n+                model._test.inputs[2]: np.ones(train_samples)}\n+        tsb = callbacks.TensorBoard(model=model, feed=feed, log_dir=filepath,\n+                                    show_accuracy=False)\n+        cbks = [tsb]\n+        model.fit(X_train, y_train, batch_size=batch_size, show_accuracy=True,\n+                  validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=5)\n+        assert os.path.exists(filepath)\n+        shutil.rmtree(filepath)\n+\n+    # case 3 Graph\n+    with tf.Graph().as_default():\n+        session = tf.Session('')\n+        KTF._set_session(session)\n+        model = Graph()\n+        model.add_input(name='X_vars', input_shape=(input_dim, ))\n+\n+        model.add_node(Dense(nb_hidden, activation=\"sigmoid\"),\n+                       name='Dense1', input='X_vars')\n+        model.add_node(Dense(nb_class, activation=\"softmax\"),\n+                       name='last_dense',\n+                       input='Dense1')\n+        model.add_output(name='output', input='last_dense')\n+        model.compile(optimizer='sgd', loss={'output': 'mse'})\n+\n+        feed = {model._test.inputs[0]: X_train, model._test.inputs[1]: y_train,\n+                model._test.inputs[2]: np.ones(train_samples)}\n+        tsb = callbacks.TensorBoard(model=model, feed=feed, log_dir=filepath,\n+                                    show_accuracy=False)\n+        cbks = [tsb]\n+        model.fit({'X_vars': X_train, 'output': y_train}, batch_size=batch_size,\n+                  validation_data={'X_vars': X_test, 'output': y_test},\n+                  callbacks=cbks, nb_epoch=5)\n+        assert os.path.exists(filepath)\n+        shutil.rmtree(filepath)\n+\n+        KTF._set_session(old_session)\n+\n if __name__ == '__main__':\n     pytest.main([__file__])\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b602a93e17964cfcedda09eec90f140665246431", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 52 | Files Changed: 4 | Hunks: 23 | Methods Changed: 8 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 110 | Churn Cumulative: 6114 | Contributors (this commit): 37 | Commits (past 90d): 46 | Contributors (cumulative): 46 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -235,6 +235,7 @@ def permute_dimensions(x, pattern):\n     '''\n     return tf.transpose(x, perm=pattern)\n \n+\n def repeat_elements(x, rep, axis):\n     '''Repeats the elements of a tensor along an axis, like np.repeat\n \n\n@@ -8,7 +8,6 @@ import warnings\n \n from collections import deque\n from .utils.generic_utils import Progbar\n-from .backend import _BACKEND\n from keras import backend as K\n \n \n@@ -431,28 +430,27 @@ class TensorBoard(Callback):\n     https://www.tensorflow.org/versions/master/how_tos/summaries_and_tensorboard/index.html\n \n     # Arguments\n-        model: a keras model linked to a tensorflow session\n-        feed: a dictionnary mapping tensors (inputs, outputs, weigths)\n-            from the model._test keras function i.e. model._test.inputs\n-            to the corresponding arrays.\n-        freq: the frequency at which the callback will output\n-            parameters and metrics to the log\n         log_dir: the path of the directory where to save the log\n             files to be parsed by tensorboard\n+        histogram_freq: frequency (in epochs) at which to compute activation\n+            histograms for the layers of the model. If set to 0,\n+            histograms won't be computed.\n     '''\n-    def __init__(self, model, feed, freq=2, log_dir='./logs',\n-                 show_accuracy=False):\n+    def __init__(self, log_dir='./logs', histogram_freq=0):\n         super(Callback, self).__init__()\n-        assert _BACKEND == 'tensorflow', \\\n-            'TensorBoard callback only works with the tensorflow backend'\n+        if K._BACKEND != 'tensorflow':\n+            raise Exception('TensorBoard callback only works '\n+                            'with the TensorFlow backend')\n+        self.log_dir = log_dir\n+        self.histogram_freq = histogram_freq\n+\n+    def _set_model(self, model):\n         import tensorflow as tf\n         import keras.backend.tensorflow_backend as KTF\n \n         self.model = model\n-        self.freq = freq\n-        self.log_dir = log_dir\n         self.sess = KTF._get_session()\n-        self.feed = feed\n+        if self.histogram_freq:\n             mod_type = self.model.get_config()['name']\n             if mod_type == 'Sequential':\n                 layers = {l.get_config()['name']: l for l in self.model.layers}\n@@ -470,29 +468,44 @@ class TensorBoard(Callback):\n                 if hasattr(cur_layer, 'get_output'):\n                     tf.histogram_summary('{}_out'.format(l),\n                                          cur_layer.get_output())\n-        f_output = self.model._test\n-        if mod_type == 'Sequential':\n-            if show_accuracy is True:\n-                f_output = self.model._test_with_acc\n-                tf.scalar_summary('Accuracy',\n-                                  f_output.outputs[1])\n-            tf.scalar_summary('Loss',\n-                              f_output.outputs[0])\n-        else:\n-            losses = [self.model.loss[loss] for loss in self.model.loss]\n-            if len(losses) > 1:\n-                l_name = \" + \".join(losses)\n-            else:\n-                l_name = losses[0]\n-            tf.scalar_summary(l_name,\n-                              f_output.outputs[0])\n         self.merged = tf.merge_all_summaries()\n         self.writer = tf.train.SummaryWriter(self.log_dir,\n                                              self.sess.graph_def)\n \n+    def on_epoch_begin(self, epoch, logs={}):\n+        self.seen = 0\n+        self.totals = {}\n+\n+    def on_batch_end(self, batch, logs={}):\n+        batch_size = logs.get('size', 0)\n+        self.seen += batch_size\n+        for k, v in logs.items():\n+            if k in self.totals:\n+                self.totals[k] += v * batch_size\n+            else:\n+                self.totals[k] = v * batch_size\n+\n     def on_epoch_end(self, epoch, logs={}):\n-        if epoch % self.freq == 0:\n-            result = self.sess.run([self.merged],\n-                                   feed_dict=self.feed)\n+        import tensorflow as tf\n+\n+        if self.model.validation_data and self.histogram_freq:\n+            if epoch % self.histogram_freq == 0:\n+                if self.params.get('show_accuracy'):\n+                    test_function = self.model._test_with_acc\n+                else:\n+                    test_function = self.model._test\n+                names = [v.name for v in test_function.inputs]\n+                feed_dict = dict(zip(names, self.model.validation_data))\n+                result = self.sess.run([self.merged], feed_dict=feed_dict)\n                 summary_str = result[0]\n                 self.writer.add_summary(summary_str, epoch)\n+\n+        for name, value in self.totals.items() + logs.items():\n+            if name in ['batch', 'size']:\n+                continue\n+            summary = tf.Summary()\n+            summary_value = summary.value.add()\n+            summary_value.simple_value = value\n+            summary_value.tag = name\n+            self.writer.add_summary(summary, epoch)\n+        self.writer.flush()\n\n@@ -185,6 +185,8 @@ class Model(object):\n             Abstract fit function for f(ins).\n             Assume that f returns a list, labelled by out_labels.\n         '''\n+        self.training_data = ins\n+        self.validation_data = val_ins\n         do_validation = False\n         if val_f and val_ins:\n             do_validation = True\n\n@@ -9,7 +9,6 @@ from keras.layers.core import Dense\n from keras.utils.test_utils import get_test_data\n from keras import backend as K\n from keras.utils import np_utils\n-from keras.callbacks import _BACKEND\n \n input_dim = 2\n nb_hidden = 4\n@@ -121,7 +120,7 @@ def test_LearningRateScheduler():\n     assert (float(K.get_value(model.optimizer.lr)) - 0.2) < K.epsilon()\n \n \n-@pytest.mark.skipif(_BACKEND != 'tensorflow',\n+@pytest.mark.skipif(K._BACKEND != 'tensorflow',\n                     reason=\"Requires tensorflow backend\")\n def test_TensorBoard():\n     import shutil\n@@ -145,13 +144,10 @@ def test_TensorBoard():\n         model.add(Dense(nb_class, activation='softmax'))\n         model.compile(loss='categorical_crossentropy', optimizer='sgd')\n \n-        feed = {model._test.inputs[0]: X_train, model._test.inputs[1]: y_train,\n-                model._test.inputs[2]: np.ones(train_samples)}\n-        tsb = callbacks.TensorBoard(model=model, feed=feed, log_dir=filepath,\n-                                    show_accuracy=False)\n+        tsb = callbacks.TensorBoard(log_dir=filepath, histogram_freq=1)\n         cbks = [tsb]\n         model.fit(X_train, y_train, batch_size=batch_size, show_accuracy=True,\n-                  validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=5)\n+                  validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=2)\n         assert os.path.exists(filepath)\n         shutil.rmtree(filepath)\n \n@@ -164,13 +160,10 @@ def test_TensorBoard():\n         model.add(Dense(nb_class, activation='softmax'))\n         model.compile(loss='categorical_crossentropy', optimizer='sgd')\n \n-        feed = {model._test.inputs[0]: X_train, model._test.inputs[1]: y_train,\n-                model._test.inputs[2]: np.ones(train_samples)}\n-        tsb = callbacks.TensorBoard(model=model, feed=feed, log_dir=filepath,\n-                                    show_accuracy=False)\n+        tsb = callbacks.TensorBoard(log_dir=filepath, histogram_freq=1)\n         cbks = [tsb]\n         model.fit(X_train, y_train, batch_size=batch_size, show_accuracy=True,\n-                  validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=5)\n+                  validation_data=(X_test, y_test), callbacks=cbks, nb_epoch=2)\n         assert os.path.exists(filepath)\n         shutil.rmtree(filepath)\n \n@@ -189,14 +182,12 @@ def test_TensorBoard():\n         model.add_output(name='output', input='last_dense')\n         model.compile(optimizer='sgd', loss={'output': 'mse'})\n \n-        feed = {model._test.inputs[0]: X_train, model._test.inputs[1]: y_train,\n-                model._test.inputs[2]: np.ones(train_samples)}\n-        tsb = callbacks.TensorBoard(model=model, feed=feed, log_dir=filepath,\n-                                    show_accuracy=False)\n+        tsb = callbacks.TensorBoard(log_dir=filepath, histogram_freq=1)\n         cbks = [tsb]\n-        model.fit({'X_vars': X_train, 'output': y_train}, batch_size=batch_size,\n+        model.fit({'X_vars': X_train, 'output': y_train},\n+                  batch_size=batch_size,\n                   validation_data={'X_vars': X_test, 'output': y_test},\n-                  callbacks=cbks, nb_epoch=5)\n+                  callbacks=cbks, nb_epoch=2)\n         assert os.path.exists(filepath)\n         shutil.rmtree(filepath)\n \n@@ -204,4 +195,3 @@ def test_TensorBoard():\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
