{"custom_id": "scrapy#bf56517abfc3d2287c08e1c88f07de7d3b1c499c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 72 | Lines Deleted: 75 | Files Changed: 8 | Hunks: 34 | Methods Changed: 20 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 147 | Churn Cumulative: 2112 | Contributors (this commit): 29 | Commits (past 90d): 29 | Contributors (cumulative): 65 | DMM Complexity: 0.0\n\nDIFF:\n@@ -162,8 +162,7 @@ class ItemTest(unittest.TestCase):\n         item = D(save='X', load='Y')\n         self.assertEqual(item['save'], 'X')\n         self.assertEqual(item['load'], 'Y')\n-        self.assertEqual(D.fields, {'load': {'default': 'A'},\n-            'save': {'default': 'A'}})\n+        self.assertEqual(D.fields, {'load': {'default': 'A'}, 'save': {'default': 'A'}})\n \n         # D class inverted\n         class E(C, B):\n@@ -171,8 +170,7 @@ class ItemTest(unittest.TestCase):\n \n         self.assertEqual(E(save='X')['save'], 'X')\n         self.assertEqual(E(load='X')['load'], 'X')\n-        self.assertEqual(E.fields, {'load': {'default': 'C'},\n-            'save': {'default': 'C'}})\n+        self.assertEqual(E.fields, {'load': {'default': 'C'}, 'save': {'default': 'C'}})\n \n     def test_metaclass_multiple_inheritance_diamond(self):\n         class A(Item):\n@@ -193,8 +191,9 @@ class ItemTest(unittest.TestCase):\n \n         self.assertEqual(D(save='X')['save'], 'X')\n         self.assertEqual(D(load='X')['load'], 'X')\n-        self.assertEqual(D.fields, {'save': {'default': 'C'},\n-            'load': {'default': 'D'}, 'update': {'default': 'D'}})\n+        self.assertEqual(\n+            D.fields,\n+            {'save': {'default': 'C'}, 'load': {'default': 'D'}, 'update': {'default': 'D'}})\n \n         # D class inverted\n         class E(C, B):\n@@ -202,8 +201,9 @@ class ItemTest(unittest.TestCase):\n \n         self.assertEqual(E(save='X')['save'], 'X')\n         self.assertEqual(E(load='X')['load'], 'X')\n-        self.assertEqual(E.fields, {'save': {'default': 'C'},\n-            'load': {'default': 'E'}, 'update': {'default': 'C'}})\n+        self.assertEqual(\n+            E.fields,\n+            {'save': {'default': 'C'}, 'load': {'default': 'E'}, 'update': {'default': 'C'}})\n \n     def test_metaclass_multiple_inheritance_without_metaclass(self):\n         class A(Item):\n@@ -223,8 +223,7 @@ class ItemTest(unittest.TestCase):\n \n         self.assertRaises(KeyError, D, not_allowed='value')\n         self.assertEqual(D(save='X')['save'], 'X')\n-        self.assertEqual(D.fields, {'save': {'default': 'A'},\n-            'load': {'default': 'A'}})\n+        self.assertEqual(D.fields, {'save': {'default': 'A'}, 'load': {'default': 'A'}})\n \n         # D class inverted\n         class E(C, B):\n@@ -232,8 +231,7 @@ class ItemTest(unittest.TestCase):\n \n         self.assertRaises(KeyError, E, not_allowed='value')\n         self.assertEqual(E(save='X')['save'], 'X')\n-        self.assertEqual(E.fields, {'save': {'default': 'A'},\n-            'load': {'default': 'A'}})\n+        self.assertEqual(E.fields, {'save': {'default': 'A'}, 'load': {'default': 'A'}})\n \n     def test_to_dict(self):\n         class TestItem(Item):\n\n@@ -171,9 +171,9 @@ class Base:\n             self.assertEqual(lx.matches(url1), False)\n             self.assertEqual(lx.matches(url2), True)\n \n-            lx = self.extractor_cls(allow=('blah1',), deny=('blah2',),\n-                                   allow_domains=('blah1.com',),\n-                                   deny_domains=('blah2.com',))\n+            lx = self.extractor_cls(allow=['blah1'], deny=['blah2'],\n+                                    allow_domains=['blah1.com'],\n+                                    deny_domains=['blah2.com'])\n             self.assertEqual(lx.matches('http://blah1.com/blah1'), True)\n             self.assertEqual(lx.matches('http://blah1.com/blah2'), False)\n             self.assertEqual(lx.matches('http://blah2.com/blah1'), False)\n\n@@ -34,15 +34,15 @@ class LogFormatterTestCase(unittest.TestCase):\n         res = Response(\"http://www.example.com\")\n         logkws = self.formatter.crawled(req, res, self.spider)\n         logline = logkws['msg'] % logkws['args']\n-        self.assertEqual(logline,\n-            \"Crawled (200) <GET http://www.example.com> (referer: None)\")\n+        self.assertEqual(logline, \"Crawled (200) <GET http://www.example.com> (referer: None)\")\n \n     def test_crawled_without_referer(self):\n         req = Request(\"http://www.example.com\", headers={'referer': 'http://example.com'})\n         res = Response(\"http://www.example.com\", flags=['cached'])\n         logkws = self.formatter.crawled(req, res, self.spider)\n         logline = logkws['msg'] % logkws['args']\n-        self.assertEqual(logline,\n+        self.assertEqual(\n+            logline,\n             \"Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']\")\n \n     def test_flags_in_request(self):\n@@ -50,7 +50,8 @@ class LogFormatterTestCase(unittest.TestCase):\n         res = Response(\"http://www.example.com\")\n         logkws = self.formatter.crawled(req, res, self.spider)\n         logline = logkws['msg'] % logkws['args']\n-        self.assertEqual(logline,\n+        self.assertEqual(\n+            logline,\n             \"Crawled (200) <GET http://www.example.com> ['test', 'flag'] (referer: None)\")\n \n     def test_dropped(self):\n@@ -140,7 +141,8 @@ class LogformatterSubclassTest(LogFormatterTestCase):\n         res = Response(\"http://www.example.com\")\n         logkws = self.formatter.crawled(req, res, self.spider)\n         logline = logkws['msg'] % logkws['args']\n-        self.assertEqual(logline,\n+        self.assertEqual(\n+            logline,\n             \"Crawled (200) <GET http://www.example.com> (referer: None) []\")\n \n     def test_crawled_without_referer(self):\n@@ -148,7 +150,8 @@ class LogformatterSubclassTest(LogFormatterTestCase):\n         res = Response(\"http://www.example.com\")\n         logkws = self.formatter.crawled(req, res, self.spider)\n         logline = logkws['msg'] % logkws['args']\n-        self.assertEqual(logline,\n+        self.assertEqual(\n+            logline,\n             \"Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']\")\n \n     def test_flags_in_request(self):\n@@ -156,7 +159,9 @@ class LogformatterSubclassTest(LogFormatterTestCase):\n         res = Response(\"http://www.example.com\")\n         logkws = self.formatter.crawled(req, res, self.spider)\n         logline = logkws['msg'] % logkws['args']\n-        self.assertEqual(logline, \"Crawled (200) <GET http://www.example.com> (referer: None) ['test', 'flag']\")\n+        self.assertEqual(\n+            logline,\n+            \"Crawled (200) <GET http://www.example.com> (referer: None) ['test', 'flag']\")\n \n \n class SkipMessagesLogFormatter(LogFormatter):\n\n@@ -69,11 +69,14 @@ class MiddlewareManagerTest(unittest.TestCase):\n \n     def test_methods(self):\n         mwman = TestMiddlewareManager(M1(), M2(), M3())\n-        self.assertEqual([x.__self__.__class__ for x in mwman.methods['open_spider']],\n+        self.assertEqual(\n+            [x.__self__.__class__ for x in mwman.methods['open_spider']],\n             [M1, M2])\n-        self.assertEqual([x.__self__.__class__ for x in mwman.methods['close_spider']],\n+        self.assertEqual(\n+            [x.__self__.__class__ for x in mwman.methods['close_spider']],\n             [M2, M1])\n-        self.assertEqual([x.__self__.__class__ for x in mwman.methods['process']],\n+        self.assertEqual(\n+            [x.__self__.__class__ for x in mwman.methods['process']],\n             [M1, M3])\n \n     def test_enabled(self):\n\n@@ -132,7 +132,8 @@ class FileDownloadCrawlTestCase(TestCase):\n     def test_download_media(self):\n         crawler = self._create_crawler(MediaDownloadSpider)\n         with LogCapture() as log:\n-            yield crawler.crawl(self.mockserver.url(\"/files/images/\"),\n+            yield crawler.crawl(\n+                self.mockserver.url(\"/files/images/\"),\n                 media_key=self.media_key,\n                 media_urls_key=self.media_urls_key)\n         self._assert_files_downloaded(self.items, str(log))\n@@ -141,7 +142,8 @@ class FileDownloadCrawlTestCase(TestCase):\n     def test_download_media_wrong_urls(self):\n         crawler = self._create_crawler(BrokenLinksMediaDownloadSpider)\n         with LogCapture() as log:\n-            yield crawler.crawl(self.mockserver.url(\"/files/images/\"),\n+            yield crawler.crawl(\n+                self.mockserver.url(\"/files/images/\"),\n                 media_key=self.media_key,\n                 media_urls_key=self.media_urls_key)\n         self._assert_files_download_failure(crawler, self.items, 404, str(log))\n@@ -150,7 +152,8 @@ class FileDownloadCrawlTestCase(TestCase):\n     def test_download_media_redirected_default_failure(self):\n         crawler = self._create_crawler(RedirectedMediaDownloadSpider)\n         with LogCapture() as log:\n-            yield crawler.crawl(self.mockserver.url(\"/files/images/\"),\n+            yield crawler.crawl(\n+                self.mockserver.url(\"/files/images/\"),\n                 media_key=self.media_key,\n                 media_urls_key=self.media_urls_key,\n                 mockserver=self.mockserver)\n@@ -164,7 +167,8 @@ class FileDownloadCrawlTestCase(TestCase):\n \n         crawler = self._create_crawler(RedirectedMediaDownloadSpider)\n         with LogCapture() as log:\n-            yield crawler.crawl(self.mockserver.url(\"/files/images/\"),\n+            yield crawler.crawl(\n+                self.mockserver.url(\"/files/images/\"),\n                 media_key=self.media_key,\n                 media_urls_key=self.media_urls_key,\n                 mockserver=self.mockserver)\n\n@@ -214,9 +214,9 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n         item = dict(requests=req)\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item['results'], [(True, rsp)])\n-        self.assertEqual(self.pipe._mockcalled,\n-                ['get_media_requests', 'media_to_download',\n-                    'media_downloaded', 'request_callback', 'item_completed'])\n+        self.assertEqual(\n+            self.pipe._mockcalled,\n+            ['get_media_requests', 'media_to_download', 'media_downloaded', 'request_callback', 'item_completed'])\n \n     @inlineCallbacks\n     def test_result_failure(self):\n@@ -227,9 +227,9 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n         item = dict(requests=req)\n         new_item = yield self.pipe.process_item(item, self.spider)\n         self.assertEqual(new_item['results'], [(False, fail)])\n-        self.assertEqual(self.pipe._mockcalled,\n-                ['get_media_requests', 'media_to_download',\n-                    'media_failed', 'request_errback', 'item_completed'])\n+        self.assertEqual(\n+            self.pipe._mockcalled,\n+            ['get_media_requests', 'media_to_download', 'media_failed', 'request_errback', 'item_completed'])\n \n     @inlineCallbacks\n     def test_mix_of_success_and_failure(self):\n\n@@ -68,29 +68,23 @@ class TestHttpErrorMiddleware(TestCase):\n         self.res200, self.res404 = _responses(self.req, [200, 404])\n \n     def test_process_spider_input(self):\n-        self.assertEqual(None,\n-                self.mw.process_spider_input(self.res200, self.spider))\n-        self.assertRaises(HttpError,\n-                self.mw.process_spider_input, self.res404, self.spider)\n+        self.assertIsNone(self.mw.process_spider_input(self.res200, self.spider))\n+        self.assertRaises(HttpError, self.mw.process_spider_input, self.res404, self.spider)\n \n     def test_process_spider_exception(self):\n-        self.assertEqual([],\n-                self.mw.process_spider_exception(self.res404,\n-                        HttpError(self.res404), self.spider))\n-        self.assertEqual(None,\n-                self.mw.process_spider_exception(self.res404,\n-                        Exception(), self.spider))\n+        self.assertEqual(\n+            [],\n+            self.mw.process_spider_exception(self.res404, HttpError(self.res404), self.spider))\n+        self.assertIsNone(self.mw.process_spider_exception(self.res404, Exception(), self.spider))\n \n     def test_handle_httpstatus_list(self):\n         res = self.res404.copy()\n         res.request = Request('http://scrapytest.org',\n                               meta={'handle_httpstatus_list': [404]})\n-        self.assertEqual(None,\n-            self.mw.process_spider_input(res, self.spider))\n+        self.assertIsNone(self.mw.process_spider_input(res, self.spider))\n \n         self.spider.handle_httpstatus_list = [404]\n-        self.assertEqual(None,\n-            self.mw.process_spider_input(self.res404, self.spider))\n+        self.assertIsNone(self.mw.process_spider_input(self.res404, self.spider))\n \n \n class TestHttpErrorMiddlewareSettings(TestCase):\n@@ -103,12 +97,9 @@ class TestHttpErrorMiddlewareSettings(TestCase):\n         self.res200, self.res404, self.res402 = _responses(self.req, [200, 404, 402])\n \n     def test_process_spider_input(self):\n-        self.assertEqual(None,\n-                self.mw.process_spider_input(self.res200, self.spider))\n-        self.assertRaises(HttpError,\n-                self.mw.process_spider_input, self.res404, self.spider)\n-        self.assertEqual(None,\n-                self.mw.process_spider_input(self.res402, self.spider))\n+        self.assertIsNone(self.mw.process_spider_input(self.res200, self.spider))\n+        self.assertRaises(HttpError, self.mw.process_spider_input, self.res404, self.spider)\n+        self.assertIsNone(self.mw.process_spider_input(self.res402, self.spider))\n \n     def test_meta_overrides_settings(self):\n         request = Request('http://scrapytest.org', meta={'handle_httpstatus_list': [404]})\n@@ -117,17 +108,13 @@ class TestHttpErrorMiddlewareSettings(TestCase):\n         res402 = self.res402.copy()\n         res402.request = request\n \n-        self.assertEqual(None,\n-            self.mw.process_spider_input(res404, self.spider))\n-        self.assertRaises(HttpError,\n-                self.mw.process_spider_input, res402, self.spider)\n+        self.assertIsNone(self.mw.process_spider_input(res404, self.spider))\n+        self.assertRaises(HttpError, self.mw.process_spider_input, res402, self.spider)\n \n     def test_spider_override_settings(self):\n         self.spider.handle_httpstatus_list = [404]\n-        self.assertEqual(None,\n-            self.mw.process_spider_input(self.res404, self.spider))\n-        self.assertRaises(HttpError,\n-                self.mw.process_spider_input, self.res402, self.spider)\n+        self.assertIsNone(self.mw.process_spider_input(self.res404, self.spider))\n+        self.assertRaises(HttpError, self.mw.process_spider_input, self.res402, self.spider)\n \n \n class TestHttpErrorMiddlewareHandleAll(TestCase):\n@@ -139,10 +126,8 @@ class TestHttpErrorMiddlewareHandleAll(TestCase):\n         self.res200, self.res404, self.res402 = _responses(self.req, [200, 404, 402])\n \n     def test_process_spider_input(self):\n-        self.assertEqual(None,\n-                self.mw.process_spider_input(self.res200, self.spider))\n-        self.assertEqual(None,\n-                self.mw.process_spider_input(self.res404, self.spider))\n+        self.assertIsNone(self.mw.process_spider_input(self.res200, self.spider))\n+        self.assertIsNone(self.mw.process_spider_input(self.res404, self.spider))\n \n     def test_meta_overrides_settings(self):\n         request = Request('http://scrapytest.org', meta={'handle_httpstatus_list': [404]})\n@@ -151,10 +136,8 @@ class TestHttpErrorMiddlewareHandleAll(TestCase):\n         res402 = self.res402.copy()\n         res402.request = request\n \n-        self.assertEqual(None,\n-            self.mw.process_spider_input(res404, self.spider))\n-        self.assertRaises(HttpError,\n-                self.mw.process_spider_input, res402, self.spider)\n+        self.assertIsNone(self.mw.process_spider_input(res404, self.spider))\n+        self.assertRaises(HttpError, self.mw.process_spider_input, res402, self.spider)\n \n \n class TestHttpErrorMiddlewareIntegrational(TrialTestCase):\n\n@@ -22,20 +22,24 @@ class TestOffsiteMiddleware(TestCase):\n     def test_process_spider_output(self):\n         res = Response('http://scrapytest.org')\n \n-        onsite_reqs = [Request('http://scrapytest.org/1'),\n+        onsite_reqs = [\n+            Request('http://scrapytest.org/1'),\n             Request('http://scrapy.org/1'),\n             Request('http://sub.scrapy.org/1'),\n             Request('http://offsite.tld/letmepass', dont_filter=True),\n             Request('http://scrapy.test.org/'),\n-                       Request('http://scrapy.test.org:8000/')]\n-        offsite_reqs = [Request('http://scrapy2.org'),\n+            Request('http://scrapy.test.org:8000/'),\n+        ]\n+        offsite_reqs = [\n+            Request('http://scrapy2.org'),\n             Request('http://offsite.tld/'),\n             Request('http://offsite.tld/scrapytest.org'),\n             Request('http://offsite.tld/rogue.scrapytest.org'),\n             Request('http://rogue.scrapytest.org.haha.com'),\n             Request('http://roguescrapytest.org'),\n             Request('http://test.org/'),\n-                       Request('http://notscrapy.test.org/')]\n+            Request('http://notscrapy.test.org/'),\n+        ]\n         reqs = onsite_reqs + offsite_reqs\n \n         out = list(self.mw.process_spider_output(res, reqs, self.spider))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#881b4f417f645fa9719e6c54eb3788cc67d56053", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 71 | Lines Deleted: 37 | Files Changed: 8 | Hunks: 27 | Methods Changed: 14 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 108 | Churn Cumulative: 2226 | Contributors (this commit): 26 | Commits (past 90d): 27 | Contributors (cumulative): 65 | DMM Complexity: 1.0\n\nDIFF:\n@@ -314,13 +314,17 @@ class BaseSettingsTest(unittest.TestCase):\n                           'TEST_BASE': BaseSettings({1: 1, 2: 2}, 'project'),\n                           'TEST': BaseSettings({1: 10, 3: 30}, 'default'),\n                           'HASNOBASE': BaseSettings({3: 3000}, 'default')})\n-        self.assertDictEqual(s.copy_to_dict(),\n-                            {'HASNOBASE': {3: 3000},\n+        self.assertDictEqual(\n+            s.copy_to_dict(),\n+            {\n+                'HASNOBASE': {3: 3000},\n                 'TEST': {1: 10, 3: 30},\n                 'TEST_BASE': {1: 1, 2: 2},\n-                             'TEST_BOOLEAN': False,\n                 'TEST_LIST': [1, 2],\n-                             'TEST_STRING': 'a string'})\n+                'TEST_BOOLEAN': False,\n+                'TEST_STRING': 'a string',\n+            }\n+        )\n \n     def test_freeze(self):\n         self.settings.freeze()\n\n@@ -40,7 +40,8 @@ class SpiderLoaderTest(unittest.TestCase):\n         verifyObject(ISpiderLoader, self.spider_loader)\n \n     def test_list(self):\n-        self.assertEqual(set(self.spider_loader.list()),\n+        self.assertEqual(\n+            set(self.spider_loader.list()),\n             set(['spider1', 'spider2', 'spider3', 'spider4']))\n \n     def test_load(self):\n@@ -48,17 +49,23 @@ class SpiderLoaderTest(unittest.TestCase):\n         self.assertEqual(spider1.__name__, 'Spider1')\n \n     def test_find_by_request(self):\n-        self.assertEqual(self.spider_loader.find_by_request(Request('http://scrapy1.org/test')),\n+        self.assertEqual(\n+            self.spider_loader.find_by_request(Request('http://scrapy1.org/test')),\n             ['spider1'])\n-        self.assertEqual(self.spider_loader.find_by_request(Request('http://scrapy2.org/test')),\n+        self.assertEqual(\n+            self.spider_loader.find_by_request(Request('http://scrapy2.org/test')),\n             ['spider2'])\n-        self.assertEqual(set(self.spider_loader.find_by_request(Request('http://scrapy3.org/test'))),\n+        self.assertEqual(\n+            set(self.spider_loader.find_by_request(Request('http://scrapy3.org/test'))),\n             set(['spider1', 'spider2']))\n-        self.assertEqual(self.spider_loader.find_by_request(Request('http://scrapy999.org/test')),\n+        self.assertEqual(\n+            self.spider_loader.find_by_request(Request('http://scrapy999.org/test')),\n             [])\n-        self.assertEqual(self.spider_loader.find_by_request(Request('http://spider3.com')),\n+        self.assertEqual(\n+            self.spider_loader.find_by_request(Request('http://spider3.com')),\n             [])\n-        self.assertEqual(self.spider_loader.find_by_request(Request('http://spider3.com/onlythis')),\n+        self.assertEqual(\n+            self.spider_loader.find_by_request(Request('http://spider3.com/onlythis')),\n             ['spider3'])\n \n     def test_load_spider_module(self):\n\n@@ -93,7 +93,8 @@ class BuildComponentListTest(unittest.TestCase):\n class UtilsConfTestCase(unittest.TestCase):\n \n     def test_arglist_to_dict(self):\n-        self.assertEqual(arglist_to_dict(['arg1=val1', 'arg2=val2']),\n+        self.assertEqual(\n+            arglist_to_dict(['arg1=val1', 'arg2=val2']),\n             {'arg1': 'val1', 'arg2': 'val2'})\n \n \n\n@@ -47,8 +47,7 @@ class XmliterTestCase(unittest.TestCase):\n             </root>\n         \"\"\"\n         response = XmlResponse(url=\"http://example.com\", body=body)\n-        nodenames = [e.xpath('name()').getall()\n-                 for e in self.xmliter(response, 'matchme...')]\n+        nodenames = [e.xpath('name()').getall() for e in self.xmliter(response, 'matchme...')]\n         self.assertEqual(nodenames, [['matchme...']])\n \n     def test_xmliter_unicode(self):\n@@ -359,15 +358,23 @@ class UtilsCsvTestCase(unittest.TestCase):\n \n         response = TextResponse(url=\"http://example.com/\", body=body1, encoding='latin1')\n         csv = csviter(response)\n-        self.assertEqual([row for row in csv],\n-            [{u'id': u'1', u'name': u'latin1', u'value': u'test'},\n-             {u'id': u'2', u'name': u'something', u'value': u'\\xf1\\xe1\\xe9\\xf3'}])\n+        self.assertEqual(\n+            list(csv),\n+            [\n+                {u'id': u'1', u'name': u'latin1', u'value': u'test'},\n+                {u'id': u'2', u'name': u'something', u'value': u'\\xf1\\xe1\\xe9\\xf3'},\n+            ]\n+        )\n \n         response = TextResponse(url=\"http://example.com/\", body=body2, encoding='cp852')\n         csv = csviter(response)\n-        self.assertEqual([row for row in csv],\n-            [{u'id': u'1', u'name': u'cp852', u'value': u'test'},\n-             {u'id': u'2', u'name': u'something', u'value': u'\\u255a\\u2569\\u2569\\u2569\\u2550\\u2550\\u2557'}])\n+        self.assertEqual(\n+            list(csv),\n+            [\n+                {u'id': u'1', u'name': u'cp852', u'value': u'test'},\n+                {u'id': u'2', u'name': u'something', u'value': u'\\u255a\\u2569\\u2569\\u2569\\u2550\\u2550\\u2557'},\n+            ]\n+        )\n \n \n class TestHelper(unittest.TestCase):\n\n@@ -15,7 +15,8 @@ class RequestSerializationTest(unittest.TestCase):\n         self._assert_serializes_ok(r)\n \n     def test_all_attributes(self):\n-        r = Request(\"http://www.example.com\",\n+        r = Request(\n+            url=\"http://www.example.com\",\n             callback=self.spider.parse_item,\n             errback=self.spider.handle_error,\n             method=\"POST\",\n\n@@ -36,7 +36,8 @@ class UtilsRequestTest(unittest.TestCase):\n         self.assertEqual(request_fingerprint(r1),\n                          request_fingerprint(r1, include_headers=['Accept-Language']))\n \n-        self.assertNotEqual(request_fingerprint(r1),\n+        self.assertNotEqual(\n+            request_fingerprint(r1),\n             request_fingerprint(r2, include_headers=['Accept-Language']))\n \n         self.assertEqual(request_fingerprint(r3, include_headers=['accept-language', 'sessionid']),\n\n@@ -22,8 +22,13 @@ class SitemapTest(unittest.TestCase):\n   </url>\n </urlset>\"\"\")\n         assert s.type == 'urlset'\n-        self.assertEqual(list(s),\n-            [{'priority': '1', 'loc': 'http://www.example.com/', 'lastmod': '2009-08-16', 'changefreq': 'daily'}, {'priority': '0.8', 'loc': 'http://www.example.com/Special-Offers.html', 'lastmod': '2009-08-16', 'changefreq': 'weekly'}])\n+        self.assertEqual(\n+            list(s),\n+            [\n+                {'priority': '1', 'loc': 'http://www.example.com/', 'lastmod': '2009-08-16', 'changefreq': 'daily'},\n+                {'priority': '0.8', 'loc': 'http://www.example.com/Special-Offers.html', 'lastmod': '2009-08-16', 'changefreq': 'weekly'},\n+            ]\n+        )\n \n     def test_sitemap_index(self):\n         s = Sitemap(b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n@@ -47,8 +47,9 @@ def getPage(url, contextFactory=None, response_transform=None, *args, **kwargs):\n         return f\n \n     from twisted.web.client import _makeGetterFactory\n-    return _makeGetterFactory(to_bytes(url), _clientfactory,\n-        contextFactory=contextFactory, *args, **kwargs).deferred\n+    return _makeGetterFactory(\n+        to_bytes(url), _clientfactory, contextFactory=contextFactory, *args, **kwargs\n+    ).deferred\n \n \n class ParseUrlTestCase(unittest.TestCase):\n@@ -105,7 +106,8 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n                 'Content-Length': '12981',\n                 'Useful': 'value'}))\n \n-        self._test(factory,\n+        self._test(\n+            factory,\n             b\"GET /bar HTTP/1.0\\r\\n\"\n             b\"Content-Length: 9\\r\\n\"\n             b\"Useful: value\\r\\n\"\n@@ -118,7 +120,8 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n \n         # test minimal sent headers\n         factory = client.ScrapyHTTPClientFactory(Request('http://foo/bar'))\n-        self._test(factory,\n+        self._test(\n+            factory,\n             b\"GET /bar HTTP/1.0\\r\\n\"\n             b\"Host: foo\\r\\n\"\n             b\"\\r\\n\")\n@@ -130,7 +133,8 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n             body='name=value',\n             headers={'Content-Type': 'application/x-www-form-urlencoded'}))\n \n-        self._test(factory,\n+        self._test(\n+            factory,\n             b\"POST /bar HTTP/1.0\\r\\n\"\n             b\"Host: foo\\r\\n\"\n             b\"Connection: close\\r\\n\"\n@@ -145,7 +149,8 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n             url='http://foo/bar'\n         ))\n \n-        self._test(factory,\n+        self._test(\n+            factory,\n             b\"POST /bar HTTP/1.0\\r\\n\"\n             b\"Host: foo\\r\\n\"\n             b\"Content-Length: 0\\r\\n\"\n@@ -160,7 +165,8 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n             },\n         ))\n \n-        self._test(factory,\n+        self._test(\n+            factory,\n             b\"GET /bar HTTP/1.0\\r\\n\"\n             b\"Host: foo\\r\\n\"\n             b\"X-Meta-Multivalued: value1\\r\\n\"\n@@ -177,7 +183,8 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n             }),\n         ))\n \n-        self._test(factory,\n+        self._test(\n+            factory,\n             b\"GET /bar HTTP/1.0\\r\\n\"\n             b\"Host: foo\\r\\n\"\n             b\"X-Meta-Multivalued: value1\\r\\n\"\n@@ -206,8 +213,7 @@ class ScrapyHTTPPageGetterTests(unittest.TestCase):\n         protocol.dataReceived(b\"Hello: World\\n\")\n         protocol.dataReceived(b\"Foo: Bar\\n\")\n         protocol.dataReceived(b\"\\n\")\n-        self.assertEqual(protocol.headers,\n-            Headers({'Hello': ['World'], 'Foo': ['Bar']}))\n+        self.assertEqual(protocol.headers, Headers({'Hello': ['World'], 'Foo': ['Bar']}))\n \n \n class EncodingResource(resource.Resource):\n@@ -340,7 +346,8 @@ class WebClientTestCase(unittest.TestCase):\n         return getPage(self.getURL(\"redirect\")).addCallback(self._cbRedirect)\n \n     def _cbRedirect(self, pageData):\n-        self.assertEqual(pageData,\n+        self.assertEqual(\n+            pageData,\n             b'\\n<html>\\n    <head>\\n        <meta http-equiv=\"refresh\" content=\"0;URL=/file\">\\n'\n             b'    </head>\\n    <body bgcolor=\"#FFFFFF\" text=\"#000000\">\\n    '\n             b'<a href=\"/file\">click here</a>\\n    </body>\\n</html>\\n')\n@@ -403,8 +410,9 @@ class WebClientCustomCiphersSSLTestCase(WebClientSSLTestCase):\n         s = \"0123456789\" * 10\n         settings = Settings({'DOWNLOADER_CLIENT_TLS_CIPHERS': self.custom_ciphers})\n         client_context_factory = create_instance(ScrapyClientContextFactory, settings=settings, crawler=None)\n-        return getPage(self.getURL(\"payload\"), body=s,\n-                       contextFactory=client_context_factory).addCallback(self.assertEqual, to_bytes(s))\n+        return getPage(\n+            self.getURL(\"payload\"), body=s, contextFactory=client_context_factory\n+        ).addCallback(self.assertEqual, to_bytes(s))\n \n     def testPayloadDefaultCiphers(self):\n         s = \"0123456789\" * 10\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d472402a0232781753515d9552b7a1997b43543a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 153 | Contributors (this commit): 9 | Commits (past 90d): 7 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,6 @@\n+import pickle\n+import sys\n+\n from queuelib.tests import test_queue as t\n from scrapy.squeues import (\n     MarshalFifoDiskQueueNonRequest as MarshalFifoDiskQueue,\n@@ -108,8 +111,10 @@ class PickleFifoDiskQueueTest(t.FifoDiskQueueTest, FifoDiskQueueTestMixin):\n         try:\n             q.push(lambda x: x)\n         except ValueError as exc:\n+            if hasattr(sys, \"pypy_version_info\"):\n+                self.assertIsInstance(exc.__context__, pickle.PicklingError)\n+            else:\n                 self.assertIsInstance(exc.__context__, AttributeError)\n-\n         sel = Selector(text='<html><body><p>some text</p></body></html>')\n         try:\n             q.push(sel)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4605c66a80dabd64924e397580224a667cd73ec8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 349 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -114,7 +114,7 @@ def get_sources(use_closest=True):\n def feed_complete_default_values_from_settings(feed, settings):\n     out = feed.copy()\n     out.setdefault(\"encoding\", settings[\"FEED_EXPORT_ENCODING\"])\n-    out.setdefault(\"fields\", settings.settings.getdictorlist(\"FEED_EXPORT_FIELDS\") or None)\n+    out.setdefault(\"fields\", settings.getdictorlist(\"FEED_EXPORT_FIELDS\") or None)\n     out.setdefault(\"store_empty\", settings.getbool(\"FEED_STORE_EMPTY\"))\n     out.setdefault(\"uri_params\", settings[\"FEED_URI_PARAMS\"])\n     if settings[\"FEED_EXPORT_INDENT\"] is None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#422e6429b56e42b8344a0e46c45f4106d374d024", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 6 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 17 | Churn Cumulative: 169 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,7 @@\n # -*- coding: utf-8 -*-\n-from collections import defaultdict\n import traceback\n import warnings\n+from collections import defaultdict\n \n from zope.interface import implementer\n \n@@ -16,6 +16,7 @@ class SpiderLoader:\n     SpiderLoader is a class which locates and loads spiders\n     in a Scrapy project.\n     \"\"\"\n+\n     def __init__(self, settings):\n         self.spider_modules = settings.getlist('SPIDER_MODULES')\n         self.warn_only = settings.getbool('SPIDER_LOADER_WARN_ONLY')\n@@ -29,6 +30,7 @@ class SpiderLoader:\n             dupes.extend([\n                 \"  {cls} named {name!r} (in {module})\".format(module=mod, cls=cls, name=name)\n                 for mod, cls in locations\n+                if len(locations) > 1\n             ])\n \n         if dupes:\n@@ -49,10 +51,9 @@ class SpiderLoader:\n                     self._load_spiders(module)\n             except ImportError:\n                 if self.warn_only:\n-                    msg = (\n-                        \"\\n{tb}Could not load spiders from module '{modname}'. \"\n-                        \"See above traceback for details.\".format(modname=name, tb=traceback.format_exc())\n-                    )\n+                    msg = (\"\\n{tb}Could not load spiders from module '{modname}'. \"\n+                           \"See above traceback for details.\".format(\n+                                modname=name, tb=traceback.format_exc()))\n                     warnings.warn(msg, RuntimeWarning)\n                 else:\n                     raise\n@@ -76,8 +77,10 @@ class SpiderLoader:\n         \"\"\"\n         Return the list of spider names that can handle the given request.\n         \"\"\"\n-        return [name for name, cls in self._spiders.items()\n-                if cls.handles_request(request)]\n+        return [\n+            name for name, cls in self._spiders.items()\n+            if cls.handles_request(request)\n+        ]\n \n     def list(self):\n         \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e0127a31230d4be13b1bd29e62d75c2954b47d9e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 186 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -35,9 +35,11 @@ class SpiderLoader:\n \n         if dupes:\n             dupes_string = \"\\n\\n\".join(dupes)\n-            msg = (\"There are several spiders with the same name:\\n\\n\"\n-                   \"{}\\n\\n  This can cause unexpected behavior.\".format(dupes_string))\n-            warnings.warn(msg, UserWarning)\n+            warnings.warn(\n+                \"There are several spiders with the same name:\\n\\n\"\n+                \"{}\\n\\n  This can cause unexpected behavior.\".format(dupes_string),\n+                category=UserWarning,\n+            )\n \n     def _load_spiders(self, module):\n         for spcls in iter_spider_classes(module):\n@@ -51,10 +53,13 @@ class SpiderLoader:\n                     self._load_spiders(module)\n             except ImportError:\n                 if self.warn_only:\n-                    msg = (\"\\n{tb}Could not load spiders from module '{modname}'. \"\n+                    warnings.warn(\n+                        \"\\n{tb}Could not load spiders from module '{modname}'. \"\n                         \"See above traceback for details.\".format(\n-                                modname=name, tb=traceback.format_exc()))\n-                    warnings.warn(msg, RuntimeWarning)\n+                            modname=name, tb=traceback.format_exc()\n+                        ),\n+                        category=RuntimeWarning,\n+                    )\n                 else:\n                     raise\n         self._check_name_duplicates()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5256eae60d3685de51c1f3891abe157e15d14def", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 84 | Lines Deleted: 25 | Files Changed: 9 | Hunks: 28 | Methods Changed: 9 | Complexity Δ (Sum/Max): 9/6 | Churn Δ: 109 | Churn Cumulative: 4736 | Contributors (this commit): 58 | Commits (past 90d): 33 | Contributors (cumulative): 125 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,7 +5,7 @@ from w3lib.url import is_url\n \n from scrapy.commands import ScrapyCommand\n from scrapy.http import Request\n-from scrapy.item import BaseItem\n+from scrapy.item import _BaseItem\n from scrapy.utils import display\n from scrapy.utils.conf import arglist_to_dict\n from scrapy.utils.spider import iterate_spider_output, spidercls_for_request\n@@ -117,7 +117,7 @@ class Command(ScrapyCommand):\n         items, requests = [], []\n \n         for x in iterate_spider_output(callback(response, **cb_kwargs)):\n-            if isinstance(x, (BaseItem, dict)):\n+            if isinstance(x, (_BaseItem, dict)):\n                 items.append(x)\n             elif isinstance(x, Request):\n                 requests.append(x)\n\n@@ -1,6 +1,6 @@\n import json\n \n-from scrapy.item import BaseItem\n+from scrapy.item import _BaseItem\n from scrapy.http import Request\n from scrapy.exceptions import ContractFail\n \n@@ -51,8 +51,8 @@ class ReturnsContract(Contract):\n     objects = {\n         'request': Request,\n         'requests': Request,\n-        'item': (BaseItem, dict),\n-        'items': (BaseItem, dict),\n+        'item': (_BaseItem, dict),\n+        'items': (_BaseItem, dict),\n     }\n \n     def __init__(self, *args, **kwargs):\n@@ -103,7 +103,7 @@ class ScrapesContract(Contract):\n \n     def post_process(self, output):\n         for x in output:\n-            if isinstance(x, (BaseItem, dict)):\n+            if isinstance(x, (_BaseItem, dict)):\n                 missing = [arg for arg in self.args if arg not in x]\n                 if missing:\n                     raise ContractFail(\n\n@@ -14,7 +14,7 @@ from scrapy.utils.log import logformatter_adapter, failure_to_exc_info\n from scrapy.exceptions import CloseSpider, DropItem, IgnoreRequest\n from scrapy import signals\n from scrapy.http import Request, Response\n-from scrapy.item import BaseItem\n+from scrapy.item import _BaseItem\n from scrapy.core.spidermw import SpiderMiddlewareManager\n \n \n@@ -191,7 +191,7 @@ class Scraper:\n         \"\"\"\n         if isinstance(output, Request):\n             self.crawler.engine.crawl(request=output, spider=spider)\n-        elif isinstance(output, (BaseItem, dict)):\n+        elif isinstance(output, (_BaseItem, dict)):\n             self.slot.itemproc_size += 1\n             dfd = self.itemproc.process_item(output, spider)\n             dfd.addBoth(self._itemproc_finished, output, response, spider)\n\n@@ -12,7 +12,7 @@ from xml.sax.saxutils import XMLGenerator\n \n from scrapy.utils.serialize import ScrapyJSONEncoder\n from scrapy.utils.python import to_bytes, to_unicode, is_listlike\n-from scrapy.item import BaseItem\n+from scrapy.item import _BaseItem\n from scrapy.exceptions import ScrapyDeprecationWarning\n \n \n@@ -312,7 +312,7 @@ class PythonItemExporter(BaseItemExporter):\n         return serializer(value)\n \n     def _serialize_value(self, value):\n-        if isinstance(value, BaseItem):\n+        if isinstance(value, _BaseItem):\n             return self.export_item(value)\n         if isinstance(value, dict):\n             return dict(self._serialize_dict(value))\n\n@@ -14,7 +14,23 @@ from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.trackref import object_ref\n \n \n-class BaseItem(object_ref):\n+class _BaseItem(object_ref):\n+    \"\"\"\n+    Temporary class used internally to avoid the deprecation\n+    warning raised by isinstance checks using BaseItem.\n+    \"\"\"\n+    pass\n+\n+\n+class _BaseItemMeta(ABCMeta):\n+    def __instancecheck__(cls, instance):\n+        if cls is BaseItem:\n+            warn('scrapy.item.BaseItem is deprecated, please use scrapy.item.Item instead',\n+                 ScrapyDeprecationWarning, stacklevel=2)\n+        return super().__instancecheck__(instance)\n+\n+\n+class BaseItem(_BaseItem, metaclass=_BaseItemMeta):\n     \"\"\"\n     Deprecated, please use :class:`scrapy.item.Item` instead\n     \"\"\"\n@@ -30,7 +46,7 @@ class Field(dict):\n     \"\"\"Container of field metadata\"\"\"\n \n \n-class ItemMeta(ABCMeta):\n+class ItemMeta(_BaseItemMeta):\n     \"\"\"Metaclass_ of :class:`Item` that handles field definitions.\n \n     .. _metaclass: https://realpython.com/python-metaclasses\n\n@@ -13,7 +13,7 @@ from w3lib.url import any_to_uri\n from scrapy.crawler import Crawler\n from scrapy.exceptions import IgnoreRequest\n from scrapy.http import Request, Response\n-from scrapy.item import BaseItem\n+from scrapy.item import _BaseItem\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n from scrapy.utils.console import start_python_console\n@@ -26,8 +26,7 @@ from scrapy.utils.console import DEFAULT_PYTHON_SHELLS\n \n class Shell:\n \n-    relevant_classes = (Crawler, Spider, Request, Response, BaseItem,\n-                        Settings)\n+    relevant_classes = (Crawler, Spider, Request, Response, _BaseItem, Settings)\n \n     def __init__(self, crawler, update_vars=None, code=None):\n         self.crawler = crawler\n\n@@ -14,10 +14,10 @@ from w3lib.html import replace_entities\n \n from scrapy.utils.datatypes import LocalWeakReferencedCache\n from scrapy.utils.python import flatten, to_unicode\n-from scrapy.item import BaseItem\n+from scrapy.item import _BaseItem\n \n \n-_ITERABLE_SINGLE_VALUES = dict, BaseItem, str, bytes\n+_ITERABLE_SINGLE_VALUES = dict, _BaseItem, str, bytes\n \n \n def arg_to_iter(arg):\n\n@@ -5,7 +5,7 @@ import decimal\n from twisted.internet import defer\n \n from scrapy.http import Request, Response\n-from scrapy.item import BaseItem\n+from scrapy.item import _BaseItem\n \n \n class ScrapyJSONEncoder(json.JSONEncoder):\n@@ -26,7 +26,7 @@ class ScrapyJSONEncoder(json.JSONEncoder):\n             return str(o)\n         elif isinstance(o, defer.Deferred):\n             return str(o)\n-        elif isinstance(o, BaseItem):\n+        elif isinstance(o, _BaseItem):\n             return dict(o)\n         elif isinstance(o, Request):\n             return \"<%s %s %s>\" % (type(o).__name__, o.method, o.url)\n\n@@ -4,7 +4,7 @@ from unittest import mock\n from warnings import catch_warnings\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.item import ABCMeta, BaseItem, DictItem, Field, Item, ItemMeta\n+from scrapy.item import ABCMeta, _BaseItem, BaseItem, DictItem, Field, Item, ItemMeta\n \n \n PY36_PLUS = (sys.version_info.major >= 3) and (sys.version_info.minor >= 6)\n@@ -334,29 +334,73 @@ class DictItemTest(unittest.TestCase):\n \n class BaseItemTest(unittest.TestCase):\n \n+    def test_isinstance_check(self):\n+\n+        class SubclassedBaseItem(BaseItem):\n+            pass\n+\n+        class SubclassedItem(Item):\n+            pass\n+\n+        self.assertTrue(isinstance(BaseItem(), BaseItem))\n+        self.assertTrue(isinstance(SubclassedBaseItem(), BaseItem))\n+        self.assertTrue(isinstance(Item(), BaseItem))\n+        self.assertTrue(isinstance(SubclassedItem(), BaseItem))\n+\n+        # make sure internal checks using private _BaseItem class succeed\n+        self.assertTrue(isinstance(BaseItem(), _BaseItem))\n+        self.assertTrue(isinstance(SubclassedBaseItem(), _BaseItem))\n+        self.assertTrue(isinstance(Item(), _BaseItem))\n+        self.assertTrue(isinstance(SubclassedItem(), _BaseItem))\n+\n     def test_deprecation_warning(self):\n+        \"\"\"\n+        Make sure deprecation warnings are logged whenever BaseItem is used,\n+        either instantiated or in an isinstance check\n+        \"\"\"\n         with catch_warnings(record=True) as warnings:\n             BaseItem()\n             self.assertEqual(len(warnings), 1)\n             self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n+\n         with catch_warnings(record=True) as warnings:\n+\n             class SubclassedBaseItem(BaseItem):\n                 pass\n+\n             SubclassedBaseItem()\n             self.assertEqual(len(warnings), 1)\n             self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n \n+        with catch_warnings(record=True) as warnings:\n+            self.assertFalse(isinstance(\"foo\", BaseItem))\n+            self.assertEqual(len(warnings), 1)\n+            self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n+\n+        with catch_warnings(record=True) as warnings:\n+            self.assertTrue(isinstance(BaseItem(), BaseItem))\n+            self.assertEqual(len(warnings), 1)\n+            self.assertEqual(warnings[0].category, ScrapyDeprecationWarning)\n+\n \n class ItemNoDeprecationWarningTest(unittest.TestCase):\n-\n     def test_no_deprecation_warning(self):\n-        with catch_warnings(record=True) as warnings:\n-            Item()\n-            self.assertEqual(len(warnings), 0)\n-        with catch_warnings(record=True) as warnings:\n+        \"\"\"\n+        Make sure deprecation warnings are NOT logged whenever BaseItem subclasses are used.\n+        \"\"\"\n         class SubclassedItem(Item):\n             pass\n+\n+        with catch_warnings(record=True) as warnings:\n+            Item()\n             SubclassedItem()\n+            _BaseItem()\n+            self.assertFalse(isinstance(\"foo\", _BaseItem))\n+            self.assertFalse(isinstance(\"foo\", Item))\n+            self.assertFalse(isinstance(\"foo\", SubclassedItem))\n+            self.assertTrue(isinstance(_BaseItem(), _BaseItem))\n+            self.assertTrue(isinstance(Item(), Item))\n+            self.assertTrue(isinstance(SubclassedItem(), SubclassedItem))\n             self.assertEqual(len(warnings), 0)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#dcf7235f0e44c0199360cea9472b7a0da6bab1a7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 45 | Files Changed: 3 | Hunks: 10 | Methods Changed: 7 | Complexity Δ (Sum/Max): -4/4 | Churn Δ: 80 | Churn Cumulative: 538 | Contributors (this commit): 16 | Commits (past 90d): 4 | Contributors (cumulative): 28 | DMM Complexity: 0.0\n\nDIFF:\n@@ -0,0 +1,29 @@\n+from scrapy.commands import ScrapyCommand\n+from scrapy.utils.conf import arglist_to_dict, feed_process_params_from_cli\n+from scrapy.exceptions import UsageError\n+\n+\n+class CommonCommands(ScrapyCommand):\n+\n+    def add_options(self, parser):\n+        ScrapyCommand.add_options(self, parser)\n+        parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[],\n+                          metavar=\"NAME=VALUE\",\n+                          help=\"set spider argument (may be repeated)\")\n+        parser.add_option(\"-o\", \"--output\", metavar=\"FILE\", action=\"append\",\n+                          help=\"dump scraped items into FILE\"\n+                          + \"(use - for stdout)\")\n+        parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\",\n+                          help=\"format to use for dumping items with -o\")\n+\n+    def process_options(self, args, opts):\n+        ScrapyCommand.process_options(self, args, opts)\n+        try:\n+            opts.spargs = arglist_to_dict(opts.spargs)\n+        except ValueError:\n+            raise UsageError(\n+                \"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n+        if opts.output:\n+            feeds = feed_process_params_from_cli(\n+                self.settings, opts.output, opts.output_format)\n+            self.settings.set('FEEDS', feeds, priority='cmdline')\n\n@@ -1,9 +1,8 @@\n-from scrapy.commands import ScrapyCommand\n-from scrapy.utils.conf import arglist_to_dict, feed_process_params_from_cli\n from scrapy.exceptions import UsageError\n+from scrapy.commands.common_commands import CommonCommands\n \n \n-class Command(ScrapyCommand):\n+class Command(CommonCommands):\n \n     requires_project = True\n \n@@ -13,30 +12,12 @@ class Command(ScrapyCommand):\n     def short_desc(self):\n         return \"Run a spider\"\n \n-    def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n-                          help=\"set spider argument (may be repeated)\")\n-        parser.add_option(\"-o\", \"--output\", metavar=\"FILE\", action=\"append\",\n-                          help=\"dump scraped items into FILE (use - for stdout)\")\n-        parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\",\n-                          help=\"format to use for dumping items with -o\")\n-\n-    def process_options(self, args, opts):\n-        ScrapyCommand.process_options(self, args, opts)\n-        try:\n-            opts.spargs = arglist_to_dict(opts.spargs)\n-        except ValueError:\n-            raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n-        if opts.output:\n-            feeds = feed_process_params_from_cli(self.settings, opts.output, opts.output_format)\n-            self.settings.set('FEEDS', feeds, priority='cmdline')\n-\n     def run(self, args, opts):\n         if len(args) < 1:\n             raise UsageError()\n         elif len(args) > 1:\n-            raise UsageError(\"running 'scrapy crawl' with more than one spider is no longer supported\")\n+            raise UsageError(\n+                \"running 'scrapy crawl' with more than one spider is no longer supported\")\n         spname = args[0]\n \n         crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n\n@@ -3,9 +3,8 @@ import os\n from importlib import import_module\n \n from scrapy.utils.spider import iter_spider_classes\n-from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n-from scrapy.utils.conf import arglist_to_dict, feed_process_params_from_cli\n+from scrapy.commands.common_commands import CommonCommands\n \n \n def _import_file(filepath):\n@@ -24,7 +23,7 @@ def _import_file(filepath):\n     return module\n \n \n-class Command(ScrapyCommand):\n+class Command(CommonCommands):\n \n     requires_project = False\n     default_settings = {'SPIDER_LOADER_WARN_ONLY': True}\n@@ -38,25 +37,6 @@ class Command(ScrapyCommand):\n     def long_desc(self):\n         return \"Run the spider defined in the given file\"\n \n-    def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n-        parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n-                          help=\"set spider argument (may be repeated)\")\n-        parser.add_option(\"-o\", \"--output\", metavar=\"FILE\", action=\"append\",\n-                          help=\"dump scraped items into FILE (use - for stdout)\")\n-        parser.add_option(\"-t\", \"--output-format\", metavar=\"FORMAT\",\n-                          help=\"format to use for dumping items with -o\")\n-\n-    def process_options(self, args, opts):\n-        ScrapyCommand.process_options(self, args, opts)\n-        try:\n-            opts.spargs = arglist_to_dict(opts.spargs)\n-        except ValueError:\n-            raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n-        if opts.output:\n-            feeds = feed_process_params_from_cli(self.settings, opts.output, opts.output_format)\n-            self.settings.set('FEEDS', feeds, priority='cmdline')\n-\n     def run(self, args, opts):\n         if len(args) != 1:\n             raise UsageError()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
