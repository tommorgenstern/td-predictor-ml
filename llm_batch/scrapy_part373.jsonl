{"custom_id": "scrapy#aa1bf6907964f0281264052cabc28197c5d28107", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 13 | Files Changed: 4 | Hunks: 14 | Methods Changed: 11 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 49 | Churn Cumulative: 3812 | Contributors (this commit): 76 | Commits (past 90d): 16 | Contributors (cumulative): 118 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,7 +16,7 @@ from ftplib import FTP\n from io import BytesIO\n from os import PathLike\n from pathlib import Path\n-from typing import DefaultDict, Optional, Set, Union\n+from typing import IO, DefaultDict, Optional, Set, Union\n from urllib.parse import urlparse\n \n from itemadapter import ItemAdapter\n@@ -31,7 +31,6 @@ from scrapy.utils.boto import is_botocore_available\n from scrapy.utils.datatypes import CaseInsensitiveDict\n from scrapy.utils.ftp import ftp_store_file\n from scrapy.utils.log import failure_to_exc_info\n-from scrapy.utils.misc import md5sum\n from scrapy.utils.python import to_bytes\n from scrapy.utils.request import referer_str\n \n@@ -42,6 +41,23 @@ def _to_string(path: Union[str, PathLike]) -> str:\n     return str(path)  # convert a Path object to string\n \n \n+def _md5sum(file: IO) -> str:\n+    \"\"\"Calculate the md5 checksum of a file-like object without reading its\n+    whole content in memory.\n+\n+    >>> from io import BytesIO\n+    >>> _md5sum(BytesIO(b'file content to hash'))\n+    '784406af91dd5a54fbb9c84c2236595a'\n+    \"\"\"\n+    m = hashlib.md5()  # nosec\n+    while True:\n+        d = file.read(8096)\n+        if not d:\n+            break\n+        m.update(d)\n+    return m.hexdigest()\n+\n+\n class FileException(Exception):\n     \"\"\"General media error exception\"\"\"\n \n@@ -70,7 +86,7 @@ class FSFilesStore:\n             return {}\n \n         with absolute_path.open(\"rb\") as f:\n-            checksum = md5sum(f)\n+            checksum = _md5sum(f)\n \n         return {\"last_modified\": last_modified, \"checksum\": checksum}\n \n@@ -299,7 +315,7 @@ class FTPFilesStore:\n                     ftp.set_pasv(False)\n                 file_path = f\"{self.basedir}/{path}\"\n                 last_modified = float(ftp.voidcmd(f\"MDTM {file_path}\")[4:].strip())\n-                m = hashlib.md5()\n+                m = hashlib.md5()  # nosec\n                 ftp.retrbinary(f\"RETR {file_path}\", m.update)\n                 return {\"last_modified\": last_modified, \"checksum\": m.hexdigest()}\n             # The file doesn't exist\n@@ -531,7 +547,7 @@ class FilesPipeline(MediaPipeline):\n     def file_downloaded(self, response, request, info, *, item=None):\n         path = self.file_path(request, response=response, info=info, item=item)\n         buf = BytesIO(response.body)\n-        checksum = md5sum(buf)\n+        checksum = _md5sum(buf)\n         buf.seek(0)\n         self.store.persist_file(path, buf, info)\n         return checksum\n@@ -542,7 +558,7 @@ class FilesPipeline(MediaPipeline):\n         return item\n \n     def file_path(self, request, response=None, info=None, *, item=None):\n-        media_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n+        media_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()  # nosec\n         media_ext = Path(request.url).suffix\n         # Handles empty and wild extensions by trying to guess the\n         # mime type then extension or default to empty string otherwise\n\n@@ -17,11 +17,10 @@ from itemadapter import ItemAdapter\n from scrapy.exceptions import DropItem, NotConfigured, ScrapyDeprecationWarning\n from scrapy.http import Request\n from scrapy.http.request import NO_CALLBACK\n-from scrapy.pipelines.files import FileException, FilesPipeline\n+from scrapy.pipelines.files import FileException, FilesPipeline, _md5sum\n \n # TODO: from scrapy.pipelines.media import MediaPipeline\n from scrapy.settings import Settings\n-from scrapy.utils.misc import md5sum\n from scrapy.utils.python import get_func_args, to_bytes\n \n \n@@ -128,7 +127,7 @@ class ImagesPipeline(FilesPipeline):\n         for path, image, buf in self.get_images(response, request, info, item=item):\n             if checksum is None:\n                 buf.seek(0)\n-                checksum = md5sum(buf)\n+                checksum = _md5sum(buf)\n             width, height = image.size\n             self.store.persist_file(\n                 path,\n@@ -228,9 +227,9 @@ class ImagesPipeline(FilesPipeline):\n         return item\n \n     def file_path(self, request, response=None, info=None, *, item=None):\n-        image_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n+        image_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()  # nosec\n         return f\"full/{image_guid}.jpg\"\n \n     def thumb_path(self, request, thumb_id, response=None, info=None, *, item=None):\n-        thumb_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()\n+        thumb_guid = hashlib.sha1(to_bytes(request.url)).hexdigest()  # nosec\n         return f\"thumbs/{thumb_id}/{thumb_guid}.jpg\"\n\n@@ -113,7 +113,15 @@ def md5sum(file: IO) -> str:\n     >>> md5sum(BytesIO(b'file content to hash'))\n     '784406af91dd5a54fbb9c84c2236595a'\n     \"\"\"\n-    m = hashlib.md5()\n+    warnings.warn(\n+        (\n+            \"The scrapy.utils.misc.md5sum function is deprecated, and will be \"\n+            \"removed in a future version of Scrapy.\"\n+        ),\n+        ScrapyDeprecationWarning,\n+        stacklevel=2,\n+    )\n+    m = hashlib.md5()  # nosec\n     while True:\n         d = file.read(8096)\n         if not d:\n\n@@ -111,7 +111,7 @@ def fingerprint(\n             \"headers\": headers,\n         }\n         fingerprint_json = json.dumps(fingerprint_data, sort_keys=True)\n-        cache[cache_key] = hashlib.sha1(fingerprint_json.encode()).digest()\n+        cache[cache_key] = hashlib.sha1(fingerprint_json.encode()).digest()  # nosec\n     return cache[cache_key]\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bf149356fc6e519e92fb55150a60b40b14e45ae8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 19 | Files Changed: 7 | Hunks: 13 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 34 | Churn Cumulative: 3631 | Contributors (this commit): 59 | Commits (past 90d): 22 | Contributors (cumulative): 116 | DMM Complexity: 0.0\n\nDIFF:\n@@ -10,21 +10,16 @@ from __future__ import annotations\n from typing import TYPE_CHECKING, Any, Iterable, List, Optional, Tuple, Union, cast\n from urllib.parse import urlencode, urljoin, urlsplit, urlunsplit\n \n-from lxml.html import (\n-    FormElement,\n-    HTMLParser,\n-    InputElement,\n-    MultipleSelectOptions,\n-    SelectElement,\n-    TextareaElement,\n-)\n-from parsel.selector import create_root_node\n+from lxml.html import FormElement  # nosec\n+from lxml.html import InputElement  # nosec\n+from lxml.html import MultipleSelectOptions  # nosec\n+from lxml.html import SelectElement  # nosec\n+from lxml.html import TextareaElement  # nosec\n from w3lib.html import strip_html5_whitespace\n \n from scrapy.http.request import Request\n from scrapy.http.response.text import TextResponse\n from scrapy.utils.python import is_listlike, to_bytes\n-from scrapy.utils.response import get_base_url\n \n if TYPE_CHECKING:\n     # typing.Self requires Python 3.11\n@@ -120,7 +115,7 @@ def _get_form(\n     formxpath: Optional[str],\n ) -> FormElement:\n     \"\"\"Find the wanted form element within the given response.\"\"\"\n-    root = create_root_node(response.text, HTMLParser, base_url=get_base_url(response))\n+    root = response.selector.root\n     forms = root.xpath(\"//form\")\n     if not forms:\n         raise ValueError(f\"No <form> element found in {response}\")\n\n@@ -7,7 +7,7 @@ import operator\n from functools import partial\n from urllib.parse import urljoin, urlparse\n \n-from lxml import etree\n+from lxml import etree  # nosec\n from parsel.csstranslator import HTMLTranslator\n from w3lib.html import strip_html5_whitespace\n from w3lib.url import canonicalize_url, safe_url_string\n\n@@ -8,6 +8,7 @@ from parsel import Selector as _ParselSelector\n \n from scrapy.http import HtmlResponse, TextResponse, XmlResponse\n from scrapy.utils.python import to_bytes\n+from scrapy.utils.response import get_base_url\n from scrapy.utils.trackref import object_ref\n \n __all__ = [\"Selector\", \"SelectorList\"]\n@@ -88,7 +89,7 @@ class Selector(_ParselSelector, object_ref):\n \n         if response is not None:\n             text = response.text\n-            kwargs.setdefault(\"base_url\", response.url)\n+            kwargs.setdefault(\"base_url\", get_base_url(response))\n \n         self.response = response\n \n\n@@ -12,7 +12,6 @@ else:\n     try:\n         brotli.Decompressor.process\n     except AttributeError:\n-\n         warn(\n             (\n                 \"You have brotlipy installed, and Scrapy will use it, but \"\n\n@@ -18,7 +18,7 @@ from typing import (\n )\n from warnings import warn\n \n-from lxml import etree\n+from lxml import etree  # nosec\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Response, TextResponse\n@@ -26,7 +26,7 @@ from scrapy.selector import Selector\n from scrapy.utils.python import re_rsearch, to_unicode\n \n if TYPE_CHECKING:\n-    from lxml._types import SupportsReadClose\n+    from lxml._types import SupportsReadClose  # nosec\n \n logger = logging.getLogger(__name__)\n \n@@ -101,6 +101,7 @@ def xmliter_lxml(\n         cast(\"SupportsReadClose[bytes]\", reader),\n         encoding=reader.encoding,\n         events=(\"end\", \"start-ns\"),\n+        resolve_entities=False,\n         huge_tree=True,\n     )\n     selxpath = \"//\" + (f\"{prefix}:{nodename}\" if namespace else nodename)\n\n@@ -8,7 +8,7 @@ SitemapSpider, its API is subject to change without notice.\n from typing import Any, Dict, Generator, Iterator, Optional\n from urllib.parse import urljoin\n \n-import lxml.etree\n+import lxml.etree  # nosec\n \n \n class Sitemap:\n@@ -19,7 +19,7 @@ class Sitemap:\n         xmlp = lxml.etree.XMLParser(\n             recover=True, remove_comments=True, resolve_entities=False\n         )\n-        self._root = lxml.etree.fromstring(xmltext, parser=xmlp)\n+        self._root = lxml.etree.fromstring(xmltext, parser=xmlp)  # nosec\n         rt = self._root.tag\n         self.type = self._root.tag.split(\"}\", 1)[1] if \"}\" in rt else rt\n \n\n@@ -4,7 +4,7 @@ from typing import List, Tuple\n \n import cryptography\n import cssselect\n-import lxml.etree\n+import lxml.etree  # nosec\n import parsel\n import twisted\n import w3lib\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cab1016bb6f719b15043f65502c63fbaa191df36", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 15 | Methods Changed: 7 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 23 | Churn Cumulative: 1286 | Contributors (this commit): 20 | Commits (past 90d): 6 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,6 +6,8 @@ from scrapy.exceptions import ScrapyDeprecationWarning\n \n try:\n     import brotli\n+except ImportError:\n+    import brotlicffi as brotli\n except ImportError:\n     pass\n else:\n\n@@ -129,8 +129,11 @@ class HttpCompressionTest(TestCase):\n         self.assertStatsEqual(\"httpcompression/response_bytes\", 74837)\n \n     def test_process_response_br(self):\n+        try:\n             try:\n                 import brotli  # noqa: F401\n+            except ImportError:\n+                import brotlicffi  # noqa: F401\n         except ImportError:\n             raise SkipTest(\"no brotli\")\n         response = self._getresponse(\"br\")\n@@ -447,8 +450,11 @@ class HttpCompressionTest(TestCase):\n         )\n \n     def test_compression_bomb_setting_br(self):\n+        try:\n             try:\n                 import brotli  # noqa: F401\n+            except ImportError:\n+                import brotlicffi  # noqa: F401\n         except ImportError:\n             raise SkipTest(\"no brotli\")\n         self._test_compression_bomb_setting(\"br\")\n@@ -485,8 +491,11 @@ class HttpCompressionTest(TestCase):\n         )\n \n     def test_compression_bomb_spider_attr_br(self):\n+        try:\n             try:\n                 import brotli  # noqa: F401\n+            except ImportError:\n+                import brotlicffi  # noqa: F401\n         except ImportError:\n             raise SkipTest(\"no brotli\")\n         self._test_compression_bomb_spider_attr(\"br\")\n@@ -521,8 +530,11 @@ class HttpCompressionTest(TestCase):\n         )\n \n     def test_compression_bomb_request_meta_br(self):\n+        try:\n             try:\n                 import brotli  # noqa: F401\n+            except ImportError:\n+                import brotlicffi  # noqa: F401\n         except ImportError:\n             raise SkipTest(\"no brotli\")\n         self._test_compression_bomb_request_meta(\"br\")\n@@ -567,8 +579,11 @@ class HttpCompressionTest(TestCase):\n         )\n \n     def test_download_warnsize_setting_br(self):\n+        try:\n             try:\n                 import brotli  # noqa: F401\n+            except ImportError:\n+                import brotlicffi  # noqa: F401\n         except ImportError:\n             raise SkipTest(\"no brotli\")\n         self._test_download_warnsize_setting(\"br\")\n@@ -615,8 +630,11 @@ class HttpCompressionTest(TestCase):\n         )\n \n     def test_download_warnsize_spider_attr_br(self):\n+        try:\n             try:\n                 import brotli  # noqa: F401\n+            except ImportError:\n+                import brotlicffi  # noqa: F401\n         except ImportError:\n             raise SkipTest(\"no brotli\")\n         self._test_download_warnsize_spider_attr(\"br\")\n@@ -661,8 +679,11 @@ class HttpCompressionTest(TestCase):\n         )\n \n     def test_download_warnsize_request_meta_br(self):\n+        try:\n             try:\n                 import brotli  # noqa: F401\n+            except ImportError:\n+                import brotlicffi  # noqa: F401\n         except ImportError:\n             raise SkipTest(\"no brotli\")\n         self._test_download_warnsize_request_meta(\"br\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3421823dce94a693ee86915110d899d8da6f3e9f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 162 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -4,6 +4,7 @@ from warnings import warn\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n \n+try:\n     try:\n         import brotli\n     except ImportError:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a52429ae08ec15d70f7f3e2079d32933bb639d6b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 166 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -20,8 +20,8 @@ else:\n                 \"You have brotlipy installed, and Scrapy will use it, but \"\n                 \"Scrapy support for brotlipy is deprecated and will stop \"\n                 \"working in a future version of Scrapy. brotlipy itself is \"\n-                \"deprecated, it has been superseded by brotlicffi (not \"\n-                \"currently supported by Scrapy). Please, uninstall brotlipy \"\n+                \"deprecated, it has been superseded by brotlicffi \"\n+                \"Please, uninstall brotlipy \"\n                 \"and install brotli instead. brotlipy has the same import \"\n                 \"name as brotli, so keeping both installed is strongly \"\n                 \"discouraged.\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7f1fbdba3cc6f118cbf11285ed26e488f854aed1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 372 | Contributors (this commit): 16 | Commits (past 90d): 6 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -28,8 +28,11 @@ logger = getLogger(__name__)\n \n ACCEPTED_ENCODINGS: List[bytes] = [b\"gzip\", b\"deflate\"]\n \n+try:\n     try:\n         import brotli  # noqa: F401\n+    except ImportError:\n+        import brotlicffi  # noqa: F401\n except ImportError:\n     pass\n else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7be919138d84ec00feb80a78e13721dff998c10c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 168 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -20,7 +20,7 @@ else:\n                 \"You have brotlipy installed, and Scrapy will use it, but \"\n                 \"Scrapy support for brotlipy is deprecated and will stop \"\n                 \"working in a future version of Scrapy. brotlipy itself is \"\n-                \"deprecated, it has been superseded by brotlicffi \"\n+                \"deprecated, it has been superseded by brotlicffi. \"\n                 \"Please, uninstall brotlipy \"\n                 \"and install brotli instead. brotlipy has the same import \"\n                 \"name as brotli, so keeping both installed is strongly \"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#99f7165c63a8a2dba72090f65ba1093d476d669a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 170 | Contributors (this commit): 4 | Commits (past 90d): 7 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -22,7 +22,7 @@ else:\n                 \"working in a future version of Scrapy. brotlipy itself is \"\n                 \"deprecated, it has been superseded by brotlicffi. \"\n                 \"Please, uninstall brotlipy \"\n-                \"and install brotli instead. brotlipy has the same import \"\n+                \"and install brotli or brotlicffi instead. brotlipy has the same import \"\n                 \"name as brotli, so keeping both installed is strongly \"\n                 \"discouraged.\"\n             ),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6ecc9e0a34be6317d1b35a3ca1fc13cb98129732", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 355 | Lines Deleted: 185 | Files Changed: 18 | Hunks: 195 | Methods Changed: 209 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 540 | Churn Cumulative: 6718 | Contributors (this commit): 79 | Commits (past 90d): 29 | Contributors (cumulative): 293 | DMM Complexity: 0.7515527950310559\n\nDIFF:\n@@ -3,61 +3,62 @@ Base class for Scrapy commands\n \"\"\"\n \n import argparse\n+import builtins\n import os\n from pathlib import Path\n-from typing import Any, Dict, List, Optional\n+from typing import Any, Dict, Iterable, List, Optional\n \n from twisted.python import failure\n \n-from scrapy.crawler import CrawlerProcess\n+from scrapy.crawler import Crawler, CrawlerProcess\n from scrapy.exceptions import UsageError\n from scrapy.utils.conf import arglist_to_dict, feed_process_params_from_cli\n \n \n class ScrapyCommand:\n-    requires_project = False\n+    requires_project: bool = False\n     crawler_process: Optional[CrawlerProcess] = None\n \n     # default settings to be used for this command instead of global defaults\n     default_settings: Dict[str, Any] = {}\n \n-    exitcode = 0\n+    exitcode: int = 0\n \n     def __init__(self) -> None:\n         self.settings: Any = None  # set in scrapy.cmdline\n \n-    def set_crawler(self, crawler):\n+    def set_crawler(self, crawler: Crawler) -> None:\n         if hasattr(self, \"_crawler\"):\n             raise RuntimeError(\"crawler already set\")\n-        self._crawler = crawler\n+        self._crawler: Crawler = crawler\n \n-    def syntax(self):\n+    def syntax(self) -> str:\n         \"\"\"\n         Command syntax (preferably one-line). Do not include command name.\n         \"\"\"\n         return \"\"\n \n-    def short_desc(self):\n+    def short_desc(self) -> str:\n         \"\"\"\n         A short description of the command\n         \"\"\"\n         return \"\"\n \n-    def long_desc(self):\n+    def long_desc(self) -> str:\n         \"\"\"A long description of the command. Return short description when not\n         available. It cannot contain newlines since contents will be formatted\n         by optparser which removes newlines and wraps text.\n         \"\"\"\n         return self.short_desc()\n \n-    def help(self):\n+    def help(self) -> str:\n         \"\"\"An extensive help for the command. It will be shown when using the\n         \"help\" command. It can contain newlines since no post-formatting will\n         be applied to its contents.\n         \"\"\"\n         return self.long_desc()\n \n-    def add_options(self, parser):\n+    def add_options(self, parser: argparse.ArgumentParser) -> None:\n         \"\"\"\n         Populate option parse with options available for this command\n         \"\"\"\n@@ -92,7 +93,7 @@ class ScrapyCommand:\n         )\n         group.add_argument(\"--pdb\", action=\"store_true\", help=\"enable pdb on failure\")\n \n-    def process_options(self, args, opts):\n+    def process_options(self, args: List[str], opts: argparse.Namespace) -> None:\n         try:\n             self.settings.setdict(arglist_to_dict(opts.set), priority=\"cmdline\")\n         except ValueError:\n@@ -129,8 +130,8 @@ class BaseRunSpiderCommand(ScrapyCommand):\n     Common class used to share functionality between the crawl, parse and runspider commands\n     \"\"\"\n \n-    def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n+    def add_options(self, parser: argparse.ArgumentParser) -> None:\n+        super().add_options(parser)\n         parser.add_argument(\n             \"-a\",\n             dest=\"spargs\",\n@@ -162,8 +163,8 @@ class BaseRunSpiderCommand(ScrapyCommand):\n             help=\"format to use for dumping items\",\n         )\n \n-    def process_options(self, args, opts):\n-        ScrapyCommand.process_options(self, args, opts)\n+    def process_options(self, args: List[str], opts: argparse.Namespace) -> None:\n+        super().process_options(args, opts)\n         try:\n             opts.spargs = arglist_to_dict(opts.spargs)\n         except ValueError:\n@@ -183,7 +184,13 @@ class ScrapyHelpFormatter(argparse.HelpFormatter):\n     Help Formatter for scrapy command line help messages.\n     \"\"\"\n \n-    def __init__(self, prog, indent_increment=2, max_help_position=24, width=None):\n+    def __init__(\n+        self,\n+        prog: str,\n+        indent_increment: int = 2,\n+        max_help_position: int = 24,\n+        width: Optional[int] = None,\n+    ):\n         super().__init__(\n             prog,\n             indent_increment=indent_increment,\n@@ -191,11 +198,12 @@ class ScrapyHelpFormatter(argparse.HelpFormatter):\n             width=width,\n         )\n \n-    def _join_parts(self, part_strings):\n-        parts = self.format_part_strings(part_strings)\n+    def _join_parts(self, part_strings: Iterable[str]) -> str:\n+        # scrapy.commands.list shadows builtins.list\n+        parts = self.format_part_strings(builtins.list(part_strings))\n         return super()._join_parts(parts)\n \n-    def format_part_strings(self, part_strings):\n+    def format_part_strings(self, part_strings: List[str]) -> List[str]:\n         \"\"\"\n         Underline and title case command line help message headers.\n         \"\"\"\n\n@@ -1,10 +1,14 @@\n+import argparse\n import subprocess  # nosec\n import sys\n import time\n+from typing import Any, Iterable, List\n from urllib.parse import urlencode\n \n import scrapy\n+from scrapy import Request\n from scrapy.commands import ScrapyCommand\n+from scrapy.http import Response\n from scrapy.linkextractors import LinkExtractor\n \n \n@@ -15,26 +19,28 @@ class Command(ScrapyCommand):\n         \"CLOSESPIDER_TIMEOUT\": 10,\n     }\n \n-    def short_desc(self):\n+    def short_desc(self) -> str:\n         return \"Run quick benchmark test\"\n \n-    def run(self, args, opts):\n+    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n         with _BenchServer():\n+            assert self.crawler_process\n             self.crawler_process.crawl(_BenchSpider, total=100000)\n             self.crawler_process.start()\n \n \n class _BenchServer:\n-    def __enter__(self):\n+    def __enter__(self) -> None:\n         from scrapy.utils.test import get_testenv\n \n         pargs = [sys.executable, \"-u\", \"-m\", \"scrapy.utils.benchserver\"]\n         self.proc = subprocess.Popen(\n             pargs, stdout=subprocess.PIPE, env=get_testenv()\n         )  # nosec\n+        assert self.proc.stdout\n         self.proc.stdout.readline()\n \n-    def __exit__(self, exc_type, exc_value, traceback):\n+    def __exit__(self, exc_type, exc_value, traceback) -> None:\n         self.proc.kill()\n         self.proc.wait()\n         time.sleep(0.2)\n@@ -49,11 +55,11 @@ class _BenchSpider(scrapy.Spider):\n     baseurl = \"http://localhost:8998\"\n     link_extractor = LinkExtractor()\n \n-    def start_requests(self):\n+    def start_requests(self) -> Iterable[Request]:\n         qargs = {\"total\": self.total, \"show\": self.show}\n         url = f\"{self.baseurl}?{urlencode(qargs, doseq=True)}\"\n         return [scrapy.Request(url, dont_filter=True)]\n \n-    def parse(self, response):\n+    def parse(self, response: Response) -> Any:  # type: ignore[override]\n         for link in self.link_extractor.extract_links(response):\n             yield scrapy.Request(link.url, callback=self.parse)\n\n@@ -1,5 +1,7 @@\n+import argparse\n import time\n from collections import defaultdict\n+from typing import List\n from unittest import TextTestResult as _TextTestResult\n from unittest import TextTestRunner\n \n@@ -10,9 +12,10 @@ from scrapy.utils.misc import load_object, set_environ\n \n \n class TextTestResult(_TextTestResult):\n-    def printSummary(self, start, stop):\n+    def printSummary(self, start: float, stop: float) -> None:\n         write = self.stream.write\n-        writeln = self.stream.writeln\n+        # _WritelnDecorator isn't implemented in typeshed yet\n+        writeln = self.stream.writeln  # type: ignore[attr-defined]\n \n         run = self.testsRun\n         plural = \"s\" if run != 1 else \"\"\n@@ -42,14 +45,14 @@ class Command(ScrapyCommand):\n     requires_project = True\n     default_settings = {\"LOG_ENABLED\": False}\n \n-    def syntax(self):\n+    def syntax(self) -> str:\n         return \"[options] <spider>\"\n \n-    def short_desc(self):\n+    def short_desc(self) -> str:\n         return \"Check spider contracts\"\n \n-    def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n+    def add_options(self, parser: argparse.ArgumentParser) -> None:\n+        super().add_options(parser)\n         parser.add_argument(\n             \"-l\",\n             \"--list\",\n@@ -66,7 +69,7 @@ class Command(ScrapyCommand):\n             help=\"print contract tests for all spiders\",\n         )\n \n-    def run(self, args, opts):\n+    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n         # load contracts\n         contracts = build_component_list(self.settings.getwithbase(\"SPIDER_CONTRACTS\"))\n         conman = ContractsManager(load_object(c) for c in contracts)\n@@ -76,6 +79,7 @@ class Command(ScrapyCommand):\n         # contract requests\n         contract_reqs = defaultdict(list)\n \n+        assert self.crawler_process\n         spider_loader = self.crawler_process.spider_loader\n \n         with set_environ(SCRAPY_CHECK=\"true\"):\n\n@@ -1,3 +1,8 @@\n+import argparse\n+from typing import List, cast\n+\n+from twisted.python.failure import Failure\n+\n from scrapy.commands import BaseRunSpiderCommand\n from scrapy.exceptions import UsageError\n \n@@ -5,13 +10,13 @@ from scrapy.exceptions import UsageError\n class Command(BaseRunSpiderCommand):\n     requires_project = True\n \n-    def syntax(self):\n+    def syntax(self) -> str:\n         return \"[options] <spider>\"\n \n-    def short_desc(self):\n+    def short_desc(self) -> str:\n         return \"Run a spider\"\n \n-    def run(self, args, opts):\n+    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n         if len(args) < 1:\n             raise UsageError()\n         elif len(args) > 1:\n@@ -20,10 +25,11 @@ class Command(BaseRunSpiderCommand):\n             )\n         spname = args[0]\n \n+        assert self.crawler_process\n         crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n \n         if getattr(crawl_defer, \"result\", None) is not None and issubclass(\n-            crawl_defer.result.type, Exception\n+            cast(Failure, crawl_defer.result).type, Exception\n         ):\n             self.exitcode = 1\n         else:\n\n@@ -1,5 +1,7 @@\n+import argparse\n import os\n import sys\n+from typing import List\n \n from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n@@ -9,32 +11,34 @@ class Command(ScrapyCommand):\n     requires_project = True\n     default_settings = {\"LOG_ENABLED\": False}\n \n-    def syntax(self):\n+    def syntax(self) -> str:\n         return \"<spider>\"\n \n-    def short_desc(self):\n+    def short_desc(self) -> str:\n         return \"Edit spider\"\n \n-    def long_desc(self):\n+    def long_desc(self) -> str:\n         return (\n             \"Edit a spider using the editor defined in the EDITOR environment\"\n             \" variable or else the EDITOR setting\"\n         )\n \n-    def _err(self, msg):\n+    def _err(self, msg: str) -> None:\n         sys.stderr.write(msg + os.linesep)\n         self.exitcode = 1\n \n-    def run(self, args, opts):\n+    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n         if len(args) != 1:\n             raise UsageError()\n \n         editor = self.settings[\"EDITOR\"]\n+        assert self.crawler_process\n         try:\n             spidercls = self.crawler_process.spider_loader.load(args[0])\n         except KeyError:\n             return self._err(f\"Spider not found: {args[0]}\")\n \n         sfile = sys.modules[spidercls.__module__].__file__\n+        assert sfile\n         sfile = sfile.replace(\".pyc\", \".py\")\n         self.exitcode = os.system(f'{editor} \"{sfile}\"')  # nosec\n\n@@ -1,13 +1,13 @@\n import sys\n-from argparse import Namespace\n-from typing import List, Type\n+from argparse import ArgumentParser, Namespace\n+from typing import Dict, List, Type\n \n from w3lib.url import is_url\n \n from scrapy import Spider\n from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n-from scrapy.http import Request\n+from scrapy.http import Request, Response\n from scrapy.utils.datatypes import SequenceExclude\n from scrapy.utils.spider import DefaultSpider, spidercls_for_request\n \n@@ -15,20 +15,20 @@ from scrapy.utils.spider import DefaultSpider, spidercls_for_request\n class Command(ScrapyCommand):\n     requires_project = False\n \n-    def syntax(self):\n+    def syntax(self) -> str:\n         return \"[options] <url>\"\n \n-    def short_desc(self):\n+    def short_desc(self) -> str:\n         return \"Fetch a URL using the Scrapy downloader\"\n \n-    def long_desc(self):\n+    def long_desc(self) -> str:\n         return (\n             \"Fetch a URL using the Scrapy downloader and print its content\"\n             \" to stdout. You may want to use --nolog to disable logging\"\n         )\n \n-    def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n+    def add_options(self, parser: ArgumentParser) -> None:\n+        super().add_options(parser)\n         parser.add_argument(\"--spider\", dest=\"spider\", help=\"use this spider\")\n         parser.add_argument(\n             \"--headers\",\n@@ -44,20 +44,21 @@ class Command(ScrapyCommand):\n             help=\"do not handle HTTP 3xx status codes and print response as-is\",\n         )\n \n-    def _print_headers(self, headers, prefix):\n+    def _print_headers(self, headers: Dict[bytes, List[bytes]], prefix: bytes) -> None:\n         for key, values in headers.items():\n             for value in values:\n                 self._print_bytes(prefix + b\" \" + key + b\": \" + value)\n \n-    def _print_response(self, response, opts):\n+    def _print_response(self, response: Response, opts: Namespace) -> None:\n         if opts.headers:\n+            assert response.request\n             self._print_headers(response.request.headers, b\">\")\n             print(\">\")\n             self._print_headers(response.headers, b\"<\")\n         else:\n             self._print_bytes(response.body)\n \n-    def _print_bytes(self, bytes_):\n+    def _print_bytes(self, bytes_: bytes) -> None:\n         sys.stdout.buffer.write(bytes_ + b\"\\n\")\n \n     def run(self, args: List[str], opts: Namespace) -> None:\n\n@@ -1,9 +1,10 @@\n+import argparse\n import os\n import shutil\n import string\n from importlib import import_module\n from pathlib import Path\n-from typing import Optional, cast\n+from typing import List, Optional, Union, cast\n from urllib.parse import urlparse\n \n import scrapy\n@@ -12,7 +13,7 @@ from scrapy.exceptions import UsageError\n from scrapy.utils.template import render_templatefile, string_camelcase\n \n \n-def sanitize_module_name(module_name):\n+def sanitize_module_name(module_name: str) -> str:\n     \"\"\"Sanitize the given module name, by replacing dashes and points\n     with underscores and prefixing it with a letter if it doesn't start\n     with one\n@@ -23,7 +24,7 @@ def sanitize_module_name(module_name):\n     return module_name\n \n \n-def extract_domain(url):\n+def extract_domain(url: str) -> str:\n     \"\"\"Extract domain name from URL string\"\"\"\n     o = urlparse(url)\n     if o.scheme == \"\" and o.netloc == \"\":\n@@ -31,7 +32,7 @@ def extract_domain(url):\n     return o.netloc\n \n \n-def verify_url_scheme(url):\n+def verify_url_scheme(url: str) -> str:\n     \"\"\"Check url for scheme and insert https if none found.\"\"\"\n     parsed = urlparse(url)\n     if parsed.scheme == \"\" and parsed.netloc == \"\":\n@@ -43,14 +44,14 @@ class Command(ScrapyCommand):\n     requires_project = False\n     default_settings = {\"LOG_ENABLED\": False}\n \n-    def syntax(self):\n+    def syntax(self) -> str:\n         return \"[options] <name> <domain>\"\n \n-    def short_desc(self):\n+    def short_desc(self) -> str:\n         return \"Generate new spider using pre-defined templates\"\n \n-    def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n+    def add_options(self, parser: argparse.ArgumentParser) -> None:\n+        super().add_options(parser)\n         parser.add_argument(\n             \"-l\",\n             \"--list\",\n@@ -86,7 +87,7 @@ class Command(ScrapyCommand):\n             help=\"If the spider already exists, overwrite it with the template\",\n         )\n \n-    def run(self, args, opts):\n+    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n         if opts.list:\n             self._list_templates()\n             return\n@@ -115,7 +116,14 @@ class Command(ScrapyCommand):\n             if opts.edit:\n                 self.exitcode = os.system(f'scrapy edit \"{name}\"')  # nosec\n \n-    def _genspider(self, module, name, url, template_name, template_file):\n+    def _genspider(\n+        self,\n+        module: str,\n+        name: str,\n+        url: str,\n+        template_name: str,\n+        template_file: Union[str, os.PathLike],\n+    ) -> None:\n         \"\"\"Generate the spider module, based on the given template\"\"\"\n         capitalized_module = \"\".join(s.capitalize() for s in module.split(\"_\"))\n         domain = extract_domain(url)\n@@ -130,6 +138,7 @@ class Command(ScrapyCommand):\n         }\n         if self.settings.get(\"NEWSPIDER_MODULE\"):\n             spiders_module = import_module(self.settings[\"NEWSPIDER_MODULE\"])\n+            assert spiders_module.__file__\n             spiders_dir = Path(spiders_module.__file__).parent.resolve()\n         else:\n             spiders_module = None\n@@ -152,7 +161,7 @@ class Command(ScrapyCommand):\n         print('Use \"scrapy genspider --list\" to see all available templates.')\n         return None\n \n-    def _list_templates(self):\n+    def _list_templates(self) -> None:\n         print(\"Available templates:\")\n         for file in sorted(Path(self.templates_dir).iterdir()):\n             if file.suffix == \".tmpl\":\n\n@@ -1,3 +1,6 @@\n+import argparse\n+from typing import List\n+\n from scrapy.commands import ScrapyCommand\n \n \n@@ -5,9 +8,10 @@ class Command(ScrapyCommand):\n     requires_project = True\n     default_settings = {\"LOG_ENABLED\": False}\n \n-    def short_desc(self):\n+    def short_desc(self) -> str:\n         return \"List available spiders\"\n \n-    def run(self, args, opts):\n+    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n+        assert self.crawler_process\n         for s in sorted(self.crawler_process.spider_loader.list()):\n             print(s)\n\n@@ -1,16 +1,32 @@\n+import argparse\n import functools\n import inspect\n import json\n import logging\n-from typing import Dict\n+from types import CoroutineType\n+from typing import (\n+    Any,\n+    AsyncGenerator,\n+    Callable,\n+    Dict,\n+    Iterable,\n+    List,\n+    Optional,\n+    Tuple,\n+    TypeVar,\n+    Union,\n+    overload,\n+)\n \n from itemadapter import ItemAdapter, is_item\n-from twisted.internet.defer import maybeDeferred\n+from twisted.internet.defer import Deferred, maybeDeferred\n+from twisted.python.failure import Failure\n from w3lib.url import is_url\n \n from scrapy.commands import BaseRunSpiderCommand\n from scrapy.exceptions import UsageError\n-from scrapy.http import Request\n+from scrapy.http import Request, Response\n+from scrapy.spiders import Spider\n from scrapy.utils import display\n from scrapy.utils.asyncgen import collect_asyncgen\n from scrapy.utils.defer import aiter_errback, deferred_from_coro\n@@ -20,24 +36,26 @@ from scrapy.utils.spider import spidercls_for_request\n \n logger = logging.getLogger(__name__)\n \n+_T = TypeVar(\"_T\")\n+\n \n class Command(BaseRunSpiderCommand):\n     requires_project = True\n \n     spider = None\n-    items: Dict[int, list] = {}\n-    requests: Dict[int, list] = {}\n+    items: Dict[int, List[Any]] = {}\n+    requests: Dict[int, List[Request]] = {}\n \n     first_response = None\n \n-    def syntax(self):\n+    def syntax(self) -> str:\n         return \"[options] <url>\"\n \n-    def short_desc(self):\n+    def short_desc(self) -> str:\n         return \"Parse URL (using its spider) and print the results\"\n \n-    def add_options(self, parser):\n-        BaseRunSpiderCommand.add_options(self, parser)\n+    def add_options(self, parser: argparse.ArgumentParser) -> None:\n+        super().add_options(parser)\n         parser.add_argument(\n             \"--spider\",\n             dest=\"spider\",\n@@ -106,7 +124,7 @@ class Command(BaseRunSpiderCommand):\n         )\n \n     @property\n-    def max_level(self):\n+    def max_level(self) -> int:\n         max_items, max_requests = 0, 0\n         if self.items:\n             max_items = max(self.items)\n@@ -114,13 +132,21 @@ class Command(BaseRunSpiderCommand):\n             max_requests = max(self.requests)\n         return max(max_items, max_requests)\n \n-    def handle_exception(self, _failure):\n+    def handle_exception(self, _failure: Failure) -> None:\n         logger.error(\n             \"An error is caught while iterating the async iterable\",\n             exc_info=failure_to_exc_info(_failure),\n         )\n \n-    def iterate_spider_output(self, result):\n+    @overload\n+    def iterate_spider_output(\n+        self, result: Union[AsyncGenerator, CoroutineType]\n+    ) -> Deferred: ...\n+\n+    @overload\n+    def iterate_spider_output(self, result: _T) -> Iterable: ...\n+\n+    def iterate_spider_output(self, result: Any) -> Union[Iterable, Deferred]:\n         if inspect.isasyncgen(result):\n             d = deferred_from_coro(\n                 collect_asyncgen(aiter_errback(result, self.handle_exception))\n@@ -133,15 +159,15 @@ class Command(BaseRunSpiderCommand):\n             return d\n         return arg_to_iter(deferred_from_coro(result))\n \n-    def add_items(self, lvl, new_items):\n+    def add_items(self, lvl: int, new_items: List[Any]) -> None:\n         old_items = self.items.get(lvl, [])\n         self.items[lvl] = old_items + new_items\n \n-    def add_requests(self, lvl, new_reqs):\n+    def add_requests(self, lvl: int, new_reqs: List[Request]) -> None:\n         old_reqs = self.requests.get(lvl, [])\n         self.requests[lvl] = old_reqs + new_reqs\n \n-    def print_items(self, lvl=None, colour=True):\n+    def print_items(self, lvl: Optional[int] = None, colour: bool = True) -> None:\n         if lvl is None:\n             items = [item for lst in self.items.values() for item in lst]\n         else:\n@@ -150,7 +176,7 @@ class Command(BaseRunSpiderCommand):\n         print(\"# Scraped Items \", \"-\" * 60)\n         display.pprint([ItemAdapter(x).asdict() for x in items], colorize=colour)\n \n-    def print_requests(self, lvl=None, colour=True):\n+    def print_requests(self, lvl: Optional[int] = None, colour: bool = True) -> None:\n         if lvl is None:\n             if self.requests:\n                 requests = self.requests[max(self.requests)]\n@@ -162,7 +188,7 @@ class Command(BaseRunSpiderCommand):\n         print(\"# Requests \", \"-\" * 65)\n         display.pprint(requests, colorize=colour)\n \n-    def print_results(self, opts):\n+    def print_results(self, opts: argparse.Namespace) -> None:\n         colour = not opts.nocolour\n \n         if opts.verbose:\n@@ -179,7 +205,14 @@ class Command(BaseRunSpiderCommand):\n             if not opts.nolinks:\n                 self.print_requests(colour=colour)\n \n-    def _get_items_and_requests(self, spider_output, opts, depth, spider, callback):\n+    def _get_items_and_requests(\n+        self,\n+        spider_output: Iterable[Any],\n+        opts: argparse.Namespace,\n+        depth: int,\n+        spider: Spider,\n+        callback: Callable,\n+    ) -> Tuple[List[Any], List[Request], argparse.Namespace, int, Spider, Callable]:\n         items, requests = [], []\n         for x in spider_output:\n             if is_item(x):\n@@ -188,14 +221,21 @@ class Command(BaseRunSpiderCommand):\n                 requests.append(x)\n         return items, requests, opts, depth, spider, callback\n \n-    def run_callback(self, response, callback, cb_kwargs=None):\n+    def run_callback(\n+        self,\n+        response: Response,\n+        callback: Callable,\n+        cb_kwargs: Optional[Dict[str, Any]] = None,\n+    ) -> Deferred:\n         cb_kwargs = cb_kwargs or {}\n         d = maybeDeferred(self.iterate_spider_output, callback(response, **cb_kwargs))\n         return d\n \n-    def get_callback_from_rules(self, spider, response):\n+    def get_callback_from_rules(\n+        self, spider: Spider, response: Response\n+    ) -> Union[Callable, str, None]:\n         if getattr(spider, \"rules\", None):\n-            for rule in spider.rules:\n+            for rule in spider.rules:  # type: ignore[attr-defined]\n                 if rule.link_extractor.matches(response.url):\n                     return rule.callback or \"parse\"\n         else:\n@@ -204,8 +244,10 @@ class Command(BaseRunSpiderCommand):\n                 \"please specify a callback to use for parsing\",\n                 {\"spider\": spider.name},\n             )\n+        return None\n \n-    def set_spidercls(self, url, opts):\n+    def set_spidercls(self, url: str, opts: argparse.Namespace) -> None:\n+        assert self.crawler_process\n         spider_loader = self.crawler_process.spider_loader\n         if opts.spider:\n             try:\n@@ -219,13 +261,14 @@ class Command(BaseRunSpiderCommand):\n             if not self.spidercls:\n                 logger.error(\"Unable to find spider for: %(url)s\", {\"url\": url})\n \n-        def _start_requests(spider):\n+        def _start_requests(spider: Spider) -> Iterable[Request]:\n             yield self.prepare_request(spider, Request(url), opts)\n \n         if self.spidercls:\n             self.spidercls.start_requests = _start_requests\n \n-    def start_parsing(self, url, opts):\n+    def start_parsing(self, url: str, opts: argparse.Namespace) -> None:\n+        assert self.crawler_process\n         self.crawler_process.crawl(self.spidercls, **opts.spargs)\n         self.pcrawler = list(self.crawler_process.crawlers)[0]\n         self.crawler_process.start()\n@@ -233,7 +276,12 @@ class Command(BaseRunSpiderCommand):\n         if not self.first_response:\n             logger.error(\"No response downloaded for: %(url)s\", {\"url\": url})\n \n-    def scraped_data(self, args):\n+    def scraped_data(\n+        self,\n+        args: Tuple[\n+            List[Any], List[Request], argparse.Namespace, int, Spider, Callable\n+        ],\n+    ) -> List[Any]:\n         items, requests, opts, depth, spider, callback = args\n         if opts.pipelines:\n             itemproc = self.pcrawler.engine.scraper.itemproc\n@@ -252,8 +300,14 @@ class Command(BaseRunSpiderCommand):\n \n         return scraped_data\n \n-    def _get_callback(self, *, spider, opts, response=None):\n-        cb = None\n+    def _get_callback(\n+        self,\n+        *,\n+        spider: Spider,\n+        opts: argparse.Namespace,\n+        response: Optional[Response] = None,\n+    ) -> Callable:\n+        cb: Union[str, Callable, None] = None\n         if response:\n             cb = response.meta[\"_callback\"]\n         if not cb:\n@@ -270,6 +324,7 @@ class Command(BaseRunSpiderCommand):\n                 cb = \"parse\"\n \n         if not callable(cb):\n+            assert cb is not None\n             cb_method = getattr(spider, cb, None)\n             if callable(cb_method):\n                 cb = cb_method\n@@ -277,10 +332,13 @@ class Command(BaseRunSpiderCommand):\n                 raise ValueError(\n                     f\"Cannot find callback {cb!r} in spider: {spider.name}\"\n                 )\n+        assert callable(cb)\n         return cb\n \n-    def prepare_request(self, spider, request, opts):\n-        def callback(response, **cb_kwargs):\n+    def prepare_request(\n+        self, spider: Spider, request: Request, opts: argparse.Namespace\n+    ) -> Request:\n+        def callback(response: Response, **cb_kwargs: Any) -> Deferred:\n             # memorize first request\n             if not self.first_response:\n                 self.first_response = response\n@@ -288,7 +346,7 @@ class Command(BaseRunSpiderCommand):\n             cb = self._get_callback(spider=spider, opts=opts, response=response)\n \n             # parse items and requests\n-            depth = response.meta[\"_depth\"]\n+            depth: int = response.meta[\"_depth\"]\n \n             d = self.run_callback(response, cb, cb_kwargs)\n             d.addCallback(self._get_items_and_requests, opts, depth, spider, callback)\n@@ -311,13 +369,13 @@ class Command(BaseRunSpiderCommand):\n         request.callback = callback\n         return request\n \n-    def process_options(self, args, opts):\n-        BaseRunSpiderCommand.process_options(self, args, opts)\n+    def process_options(self, args: List[str], opts: argparse.Namespace) -> None:\n+        super().process_options(args, opts)\n \n         self.process_request_meta(opts)\n         self.process_request_cb_kwargs(opts)\n \n-    def process_request_meta(self, opts):\n+    def process_request_meta(self, opts: argparse.Namespace) -> None:\n         if opts.meta:\n             try:\n                 opts.meta = json.loads(opts.meta)\n@@ -328,7 +386,7 @@ class Command(BaseRunSpiderCommand):\n                     print_help=False,\n                 )\n \n-    def process_request_cb_kwargs(self, opts):\n+    def process_request_cb_kwargs(self, opts: argparse.Namespace) -> None:\n         if opts.cbkwargs:\n             try:\n                 opts.cbkwargs = json.loads(opts.cbkwargs)\n@@ -339,7 +397,7 @@ class Command(BaseRunSpiderCommand):\n                     print_help=False,\n                 )\n \n-    def run(self, args, opts):\n+    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n         # parse arguments\n         if not len(args) == 1 or not is_url(args[0]):\n             raise UsageError()\n\n@@ -1,9 +1,10 @@\n+import argparse\n import sys\n from importlib import import_module\n from os import PathLike\n from pathlib import Path\n from types import ModuleType\n-from typing import Union\n+from typing import List, Union\n \n from scrapy.commands import BaseRunSpiderCommand\n from scrapy.exceptions import UsageError\n@@ -27,16 +28,16 @@ class Command(BaseRunSpiderCommand):\n     requires_project = False\n     default_settings = {\"SPIDER_LOADER_WARN_ONLY\": True}\n \n-    def syntax(self):\n+    def syntax(self) -> str:\n         return \"[options] <spider_file>\"\n \n-    def short_desc(self):\n+    def short_desc(self) -> str:\n         return \"Run a self-contained spider (without creating a project)\"\n \n-    def long_desc(self):\n+    def long_desc(self) -> str:\n         return \"Run the spider defined in the given file\"\n \n-    def run(self, args, opts):\n+    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n         if len(args) != 1:\n             raise UsageError()\n         filename = Path(args[0])\n@@ -51,6 +52,7 @@ class Command(BaseRunSpiderCommand):\n             raise UsageError(f\"No spider found in file: {filename}\\n\")\n         spidercls = spclasses.pop()\n \n+        assert self.crawler_process\n         self.crawler_process.crawl(spidercls, **opts.spargs)\n         self.crawler_process.start()\n \n\n@@ -1,4 +1,6 @@\n+import argparse\n import json\n+from typing import List\n \n from scrapy.commands import ScrapyCommand\n from scrapy.settings import BaseSettings\n@@ -8,14 +10,14 @@ class Command(ScrapyCommand):\n     requires_project = False\n     default_settings = {\"LOG_ENABLED\": False, \"SPIDER_LOADER_WARN_ONLY\": True}\n \n-    def syntax(self):\n+    def syntax(self) -> str:\n         return \"[options]\"\n \n-    def short_desc(self):\n+    def short_desc(self) -> str:\n         return \"Get settings values\"\n \n-    def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n+    def add_options(self, parser: argparse.ArgumentParser) -> None:\n+        super().add_options(parser)\n         parser.add_argument(\n             \"--get\", dest=\"get\", metavar=\"SETTING\", help=\"print raw setting value\"\n         )\n@@ -44,7 +46,8 @@ class Command(ScrapyCommand):\n             help=\"print setting value, interpreted as a list\",\n         )\n \n-    def run(self, args, opts):\n+    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n+        assert self.crawler_process\n         settings = self.crawler_process.settings\n         if opts.get:\n             s = settings.get(opts.get)\n\n@@ -4,9 +4,9 @@ Scrapy Shell\n See documentation in docs/topics/shell.rst\n \"\"\"\n \n-from argparse import Namespace\n+from argparse import ArgumentParser, Namespace\n from threading import Thread\n-from typing import List, Type\n+from typing import Any, Dict, List, Type\n \n from scrapy import Spider\n from scrapy.commands import ScrapyCommand\n@@ -24,20 +24,20 @@ class Command(ScrapyCommand):\n         \"DUPEFILTER_CLASS\": \"scrapy.dupefilters.BaseDupeFilter\",\n     }\n \n-    def syntax(self):\n+    def syntax(self) -> str:\n         return \"[url|file]\"\n \n-    def short_desc(self):\n+    def short_desc(self) -> str:\n         return \"Interactive scraping console\"\n \n-    def long_desc(self):\n+    def long_desc(self) -> str:\n         return (\n             \"Interactive console for scraping the given url or file. \"\n             \"Use ./file.html syntax or full path for local file.\"\n         )\n \n-    def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n+    def add_options(self, parser: ArgumentParser) -> None:\n+        super().add_options(parser)\n         parser.add_argument(\n             \"-c\",\n             dest=\"code\",\n@@ -52,7 +52,7 @@ class Command(ScrapyCommand):\n             help=\"do not handle HTTP 3xx status codes and print response as-is\",\n         )\n \n-    def update_vars(self, vars):\n+    def update_vars(self, vars: Dict[str, Any]) -> None:\n         \"\"\"You can use this function to update the Scrapy objects that will be\n         available in the shell\n         \"\"\"\n@@ -88,7 +88,8 @@ class Command(ScrapyCommand):\n         shell = Shell(crawler, update_vars=self.update_vars, code=opts.code)\n         shell.start(url=url, redirect=not opts.no_redirect)\n \n-    def _start_crawler_thread(self):\n+    def _start_crawler_thread(self) -> None:\n+        assert self.crawler_process\n         t = Thread(\n             target=self.crawler_process.start,\n             kwargs={\"stop_after_crawl\": False, \"install_signal_handlers\": False},\n\n@@ -1,3 +1,4 @@\n+import argparse\n import os\n import re\n import string\n@@ -5,13 +6,14 @@ from importlib.util import find_spec\n from pathlib import Path\n from shutil import copy2, copystat, ignore_patterns, move\n from stat import S_IWUSR as OWNER_WRITE_PERMISSION\n+from typing import List, Tuple, Union\n \n import scrapy\n from scrapy.commands import ScrapyCommand\n from scrapy.exceptions import UsageError\n from scrapy.utils.template import render_templatefile, string_camelcase\n \n-TEMPLATES_TO_RENDER = (\n+TEMPLATES_TO_RENDER: Tuple[Tuple[str, ...], ...] = (\n     (\"scrapy.cfg\",),\n     (\"${project_name}\", \"settings.py.tmpl\"),\n     (\"${project_name}\", \"items.py.tmpl\"),\n@@ -22,7 +24,7 @@ TEMPLATES_TO_RENDER = (\n IGNORE = ignore_patterns(\"*.pyc\", \"__pycache__\", \".svn\")\n \n \n-def _make_writable(path):\n+def _make_writable(path: Union[str, os.PathLike]) -> None:\n     current_permissions = os.stat(path).st_mode\n     os.chmod(path, current_permissions | OWNER_WRITE_PERMISSION)\n \n@@ -31,14 +33,14 @@ class Command(ScrapyCommand):\n     requires_project = False\n     default_settings = {\"LOG_ENABLED\": False, \"SPIDER_LOADER_WARN_ONLY\": True}\n \n-    def syntax(self):\n+    def syntax(self) -> str:\n         return \"<project_name> [project_dir]\"\n \n-    def short_desc(self):\n+    def short_desc(self) -> str:\n         return \"Create new project\"\n \n-    def _is_valid_name(self, project_name):\n-        def _module_exists(module_name):\n+    def _is_valid_name(self, project_name: str) -> bool:\n+        def _module_exists(module_name: str) -> bool:\n             spec = find_spec(module_name)\n             return spec is not None and spec.loader is not None\n \n@@ -53,7 +55,7 @@ class Command(ScrapyCommand):\n             return True\n         return False\n \n-    def _copytree(self, src: Path, dst: Path):\n+    def _copytree(self, src: Path, dst: Path) -> None:\n         \"\"\"\n         Since the original function always creates the directory, to resolve\n         the issue a new function had to be created. It's a simple copy and\n@@ -84,7 +86,7 @@ class Command(ScrapyCommand):\n         copystat(src, dst)\n         _make_writable(dst)\n \n-    def run(self, args, opts):\n+    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n         if len(args) not in (1, 2):\n             raise UsageError()\n \n@@ -105,7 +107,9 @@ class Command(ScrapyCommand):\n             return\n \n         self._copytree(Path(self.templates_dir), project_dir.resolve())\n-        move(project_dir / \"module\", project_dir / project_name)\n+        # On 3.8 shutil.move doesn't fully support Path args, but it supports our use case\n+        # See https://bugs.python.org/issue32689\n+        move(project_dir / \"module\", project_dir / project_name)  # type: ignore[arg-type]\n         for paths in TEMPLATES_TO_RENDER:\n             tplfile = Path(\n                 project_dir,\n\n@@ -1,3 +1,6 @@\n+import argparse\n+from typing import List\n+\n import scrapy\n from scrapy.commands import ScrapyCommand\n from scrapy.utils.versions import scrapy_components_versions\n@@ -6,14 +9,14 @@ from scrapy.utils.versions import scrapy_components_versions\n class Command(ScrapyCommand):\n     default_settings = {\"LOG_ENABLED\": False, \"SPIDER_LOADER_WARN_ONLY\": True}\n \n-    def syntax(self):\n+    def syntax(self) -> str:\n         return \"[-v]\"\n \n-    def short_desc(self):\n+    def short_desc(self) -> str:\n         return \"Print Scrapy version\"\n \n-    def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n+    def add_options(self, parser: argparse.ArgumentParser) -> None:\n+        super().add_options(parser)\n         parser.add_argument(\n             \"--verbose\",\n             \"-v\",\n@@ -22,7 +25,7 @@ class Command(ScrapyCommand):\n             help=\"also display twisted/python/platform info (useful for bug reports)\",\n         )\n \n-    def run(self, args, opts):\n+    def run(self, args: List[str], opts: argparse.Namespace) -> None:\n         if opts.verbose:\n             versions = scrapy_components_versions()\n             width = max(len(n) for (n, _) in versions)\n\n@@ -1,21 +1,28 @@\n import argparse\n+import logging\n \n from scrapy.commands import fetch\n+from scrapy.http import Response, TextResponse\n from scrapy.utils.response import open_in_browser\n \n+logger = logging.getLogger(__name__)\n+\n \n class Command(fetch.Command):\n-    def short_desc(self):\n+    def short_desc(self) -> str:\n         return \"Open URL in browser, as seen by Scrapy\"\n \n-    def long_desc(self):\n+    def long_desc(self) -> str:\n         return (\n             \"Fetch a URL using the Scrapy downloader and show its contents in a browser\"\n         )\n \n-    def add_options(self, parser):\n+    def add_options(self, parser: argparse.ArgumentParser) -> None:\n         super().add_options(parser)\n         parser.add_argument(\"--headers\", help=argparse.SUPPRESS)\n \n-    def _print_response(self, response, opts):\n+    def _print_response(self, response: Response, opts: argparse.Namespace) -> None:\n+        if not isinstance(response, TextResponse):\n+            logger.error(\"Cannot view a non-text response.\")\n+            return\n         open_in_browser(response)\n\n@@ -6,6 +6,7 @@ See documentation in docs/topics/shell.rst\n \n import os\n import signal\n+from typing import Any, Callable, Dict, Optional, Tuple, Union\n \n from itemadapter import is_item\n from twisted.internet import defer, threads\n@@ -26,18 +27,32 @@ from scrapy.utils.response import open_in_browser\n \n \n class Shell:\n-    relevant_classes = (Crawler, Spider, Request, Response, Settings)\n+    relevant_classes: Tuple[type, ...] = (Crawler, Spider, Request, Response, Settings)\n \n-    def __init__(self, crawler, update_vars=None, code=None):\n-        self.crawler = crawler\n-        self.update_vars = update_vars or (lambda x: None)\n-        self.item_class = load_object(crawler.settings[\"DEFAULT_ITEM_CLASS\"])\n-        self.spider = None\n-        self.inthread = not threadable.isInIOThread()\n-        self.code = code\n-        self.vars = {}\n+    def __init__(\n+        self,\n+        crawler: Crawler,\n+        update_vars: Optional[Callable[[Dict[str, Any]], None]] = None,\n+        code: Optional[str] = None,\n+    ):\n+        self.crawler: Crawler = crawler\n+        self.update_vars: Callable[[Dict[str, Any]], None] = update_vars or (\n+            lambda x: None\n+        )\n+        self.item_class: type = load_object(crawler.settings[\"DEFAULT_ITEM_CLASS\"])\n+        self.spider: Optional[Spider] = None\n+        self.inthread: bool = not threadable.isInIOThread()\n+        self.code: Optional[str] = code\n+        self.vars: Dict[str, Any] = {}\n \n-    def start(self, url=None, request=None, response=None, spider=None, redirect=True):\n+    def start(\n+        self,\n+        url: Optional[str] = None,\n+        request: Optional[Request] = None,\n+        response: Optional[Response] = None,\n+        spider: Optional[Spider] = None,\n+        redirect: bool = True,\n+    ) -> None:\n         # disable accidental Ctrl-C key press from shutting down the engine\n         signal.signal(signal.SIGINT, signal.SIG_IGN)\n         if url:\n@@ -77,7 +92,7 @@ class Shell:\n                 self.vars, shells=shells, banner=self.vars.pop(\"banner\", \"\")\n             )\n \n-    def _schedule(self, request, spider):\n+    def _schedule(self, request: Request, spider: Optional[Spider]) -> defer.Deferred:\n         if is_asyncio_reactor_installed():\n             # set the asyncio event loop for the current thread\n             event_loop_path = self.crawler.settings[\"ASYNCIO_EVENT_LOOP\"]\n@@ -85,10 +100,11 @@ class Shell:\n         spider = self._open_spider(request, spider)\n         d = _request_deferred(request)\n         d.addCallback(lambda x: (x, spider))\n+        assert self.crawler.engine\n         self.crawler.engine.crawl(request)\n         return d\n \n-    def _open_spider(self, request, spider):\n+    def _open_spider(self, request: Request, spider: Optional[Spider]) -> Spider:\n         if self.spider:\n             return self.spider\n \n@@ -96,11 +112,18 @@ class Shell:\n             spider = self.crawler.spider or self.crawler._create_spider()\n \n         self.crawler.spider = spider\n+        assert self.crawler.engine\n         self.crawler.engine.open_spider(spider, close_if_idle=False)\n         self.spider = spider\n         return spider\n \n-    def fetch(self, request_or_url, spider=None, redirect=True, **kwargs):\n+    def fetch(\n+        self,\n+        request_or_url: Union[Request, str],\n+        spider: Optional[Spider] = None,\n+        redirect: bool = True,\n+        **kwargs: Any,\n+    ) -> None:\n         from twisted.internet import reactor\n \n         if isinstance(request_or_url, Request):\n@@ -123,7 +146,12 @@ class Shell:\n             pass\n         self.populate_vars(response, request, spider)\n \n-    def populate_vars(self, response=None, request=None, spider=None):\n+    def populate_vars(\n+        self,\n+        response: Optional[Response] = None,\n+        request: Optional[Request] = None,\n+        spider: Optional[Spider] = None,\n+    ) -> None:\n         import scrapy\n \n         self.vars[\"scrapy\"] = scrapy\n@@ -141,10 +169,10 @@ class Shell:\n         if not self.code:\n             self.vars[\"banner\"] = self.get_help()\n \n-    def print_help(self):\n+    def print_help(self) -> None:\n         print(self.get_help())\n \n-    def get_help(self):\n+    def get_help(self) -> str:\n         b = []\n         b.append(\"Available Scrapy objects:\")\n         b.append(\n@@ -168,11 +196,11 @@ class Shell:\n \n         return \"\\n\".join(f\"[s] {line}\" for line in b)\n \n-    def _is_relevant(self, value):\n+    def _is_relevant(self, value: Any) -> bool:\n         return isinstance(value, self.relevant_classes) or is_item(value)\n \n \n-def inspect_response(response, spider):\n+def inspect_response(response: Response, spider: Spider) -> None:\n     \"\"\"Open a shell to inspect the given response\"\"\"\n     # Shell.start removes the SIGINT handler, so save it and re-add it after\n     # the shell has closed\n@@ -181,7 +209,7 @@ def inspect_response(response, spider):\n     signal.signal(signal.SIGINT, sigint_handler)\n \n \n-def _request_deferred(request):\n+def _request_deferred(request: Request) -> defer.Deferred:\n     \"\"\"Wrap a request inside a Deferred.\n \n     This function is harmful, do not use it until you know what you are doing.\n@@ -195,12 +223,12 @@ def _request_deferred(request):\n     request_callback = request.callback\n     request_errback = request.errback\n \n-    def _restore_callbacks(result):\n+    def _restore_callbacks(result: Any) -> Any:\n         request.callback = request_callback\n         request.errback = request_errback\n         return result\n \n-    d = defer.Deferred()\n+    d: defer.Deferred = defer.Deferred()\n     d.addBoth(_restore_callbacks)\n     if request.callback:\n         d.addCallbacks(request.callback, request.errback)\n\n@@ -1,17 +1,27 @@\n from functools import wraps\n+from typing import Any, Callable, Dict, Iterable, Optional\n+\n+EmbedFuncT = Callable[..., None]\n+KnownShellsT = Dict[str, Callable[..., EmbedFuncT]]\n \n \n-def _embed_ipython_shell(namespace={}, banner=\"\"):\n+def _embed_ipython_shell(\n+    namespace: Dict[str, Any] = {}, banner: str = \"\"\n+) -> EmbedFuncT:\n     \"\"\"Start an IPython Shell\"\"\"\n     try:\n         from IPython.terminal.embed import InteractiveShellEmbed\n         from IPython.terminal.ipapp import load_default_config\n     except ImportError:\n-        from IPython.frontend.terminal.embed import InteractiveShellEmbed\n-        from IPython.frontend.terminal.ipapp import load_default_config\n+        from IPython.frontend.terminal.embed import (  # type: ignore[no-redef]\n+            InteractiveShellEmbed,\n+        )\n+        from IPython.frontend.terminal.ipapp import (  # type: ignore[no-redef]\n+            load_default_config,\n+        )\n \n     @wraps(_embed_ipython_shell)\n-    def wrapper(namespace=namespace, banner=\"\"):\n+    def wrapper(namespace: Dict[str, Any] = namespace, banner: str = \"\") -> None:\n         config = load_default_config()\n         # Always use .instance() to ensure _instance propagation to all parents\n         # this is needed for <TAB> completion works well for new imports\n@@ -26,30 +36,36 @@ def _embed_ipython_shell(namespace={}, banner=\"\"):\n     return wrapper\n \n \n-def _embed_bpython_shell(namespace={}, banner=\"\"):\n+def _embed_bpython_shell(\n+    namespace: Dict[str, Any] = {}, banner: str = \"\"\n+) -> EmbedFuncT:\n     \"\"\"Start a bpython shell\"\"\"\n     import bpython\n \n     @wraps(_embed_bpython_shell)\n-    def wrapper(namespace=namespace, banner=\"\"):\n+    def wrapper(namespace: Dict[str, Any] = namespace, banner: str = \"\") -> None:\n         bpython.embed(locals_=namespace, banner=banner)\n \n     return wrapper\n \n \n-def _embed_ptpython_shell(namespace={}, banner=\"\"):\n+def _embed_ptpython_shell(\n+    namespace: Dict[str, Any] = {}, banner: str = \"\"\n+) -> EmbedFuncT:\n     \"\"\"Start a ptpython shell\"\"\"\n     import ptpython.repl\n \n     @wraps(_embed_ptpython_shell)\n-    def wrapper(namespace=namespace, banner=\"\"):\n+    def wrapper(namespace: Dict[str, Any] = namespace, banner: str = \"\") -> None:\n         print(banner)\n         ptpython.repl.embed(locals=namespace)\n \n     return wrapper\n \n \n-def _embed_standard_shell(namespace={}, banner=\"\"):\n+def _embed_standard_shell(\n+    namespace: Dict[str, Any] = {}, banner: str = \"\"\n+) -> EmbedFuncT:\n     \"\"\"Start a standard python shell\"\"\"\n     import code\n \n@@ -63,13 +79,13 @@ def _embed_standard_shell(namespace={}, banner=\"\"):\n         readline.parse_and_bind(\"tab:complete\")\n \n     @wraps(_embed_standard_shell)\n-    def wrapper(namespace=namespace, banner=\"\"):\n+    def wrapper(namespace: Dict[str, Any] = namespace, banner: str = \"\") -> None:\n         code.interact(banner=banner, local=namespace)\n \n     return wrapper\n \n \n-DEFAULT_PYTHON_SHELLS = {\n+DEFAULT_PYTHON_SHELLS: KnownShellsT = {\n     \"ptpython\": _embed_ptpython_shell,\n     \"ipython\": _embed_ipython_shell,\n     \"bpython\": _embed_bpython_shell,\n@@ -77,7 +93,9 @@ DEFAULT_PYTHON_SHELLS = {\n }\n \n \n-def get_shell_embed_func(shells=None, known_shells=None):\n+def get_shell_embed_func(\n+    shells: Optional[Iterable[str]] = None, known_shells: Optional[KnownShellsT] = None\n+) -> Any:\n     \"\"\"Return the first acceptable shell-embed function\n     from a given list of shell names.\n     \"\"\"\n@@ -95,7 +113,11 @@ def get_shell_embed_func(shells=None, known_shells=None):\n                 continue\n \n \n-def start_python_console(namespace=None, banner=\"\", shells=None):\n+def start_python_console(\n+    namespace: Optional[Dict[str, Any]] = None,\n+    banner: str = \"\",\n+    shells: Optional[Iterable[str]] = None,\n+) -> None:\n     \"\"\"Start Python console bound to the given namespace.\n     Readline support and tab completion will be used on Unix, if available.\n     \"\"\"\n\n@@ -3,24 +3,27 @@ This module provides some useful functions for working with\n scrapy.http.Response objects\n \"\"\"\n \n+from __future__ import annotations\n+\n import os\n import re\n import tempfile\n import webbrowser\n-from typing import Any, Callable, Iterable, Tuple, Union\n+from typing import TYPE_CHECKING, Any, Callable, Iterable, Tuple, Union\n from weakref import WeakKeyDictionary\n \n from twisted.web import http\n from w3lib import html\n \n-import scrapy\n-from scrapy.http.response import Response\n from scrapy.utils.python import to_bytes, to_unicode\n \n-_baseurl_cache: \"WeakKeyDictionary[Response, str]\" = WeakKeyDictionary()\n+if TYPE_CHECKING:\n+    from scrapy.http import Response, TextResponse\n+\n+_baseurl_cache: WeakKeyDictionary[Response, str] = WeakKeyDictionary()\n \n \n-def get_base_url(response: \"scrapy.http.response.text.TextResponse\") -> str:\n+def get_base_url(response: TextResponse) -> str:\n     \"\"\"Return the base url of the given response, joined with the response url\"\"\"\n     if response not in _baseurl_cache:\n         text = response.text[0:4096]\n@@ -30,13 +33,13 @@ def get_base_url(response: \"scrapy.http.response.text.TextResponse\") -> str:\n     return _baseurl_cache[response]\n \n \n-_metaref_cache: (\n-    \"WeakKeyDictionary[Response, Union[Tuple[None, None], Tuple[float, str]]]\"\n-) = WeakKeyDictionary()\n+_metaref_cache: WeakKeyDictionary[\n+    Response, Union[Tuple[None, None], Tuple[float, str]]\n+] = WeakKeyDictionary()\n \n \n def get_meta_refresh(\n-    response: \"scrapy.http.response.text.TextResponse\",\n+    response: TextResponse,\n     ignore_tags: Iterable[str] = (\"script\", \"noscript\"),\n ) -> Union[Tuple[None, None], Tuple[float, str]]:\n     \"\"\"Parse the http-equiv refresh parameter from the given response\"\"\"\n@@ -68,10 +71,7 @@ def _remove_html_comments(body):\n \n \n def open_in_browser(\n-    response: Union[\n-        \"scrapy.http.response.html.HtmlResponse\",\n-        \"scrapy.http.response.text.TextResponse\",\n-    ],\n+    response: TextResponse,\n     _openfunc: Callable[[str], Any] = webbrowser.open,\n ) -> Any:\n     \"\"\"Open *response* in a local web browser, adjusting the `base tag`_ for\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
