{"custom_id": "keras#bda6bb1d8a11fce324e0e73d145ffd4d5f118a2c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 7117 | Contributors (this commit): 39 | Commits (past 90d): 4 | Contributors (cumulative): 39 | DMM Complexity: None\n\nDIFF:\n@@ -1443,7 +1443,7 @@ class TestBackend(object):\n                                          mean=mean,\n                                          stddev=std))\n         assert rand.shape == (200, 200)\n-        assert np.abs(np.mean(rand) - mean) < 0.015\n+        assert np.abs(np.mean(rand) - mean) < 0.016\n         assert np.max(rand) <= max_val\n         assert np.min(rand) >= min_val\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b10f5ca582cc5b97f9daad7c00345661e7d715a7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 449 | Contributors (this commit): 12 | Commits (past 90d): 4 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -94,6 +94,8 @@ def test_no_grad():\n                 batch_size=10, epochs=10)\n \n \n+@pytest.mark.skipif((K.backend() == 'cntk'),\n+                    reason='Flaky with CNTK')\n def test_sgd():\n     sgd = optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n     _test_optimizer(sgd)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4981b70b96cad1fa55186a98f254a6d1cb20fc8c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 1322 | Contributors (this commit): 11 | Commits (past 90d): 1 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -808,10 +808,9 @@ def test_cropping3d_legacy_interface():\n     assert json.dumps(old_layer.get_config()) == json.dumps(new_layer.get_config())\n \n \n-@pytest.mark.skipif(K.backend() in {'tensorflow', 'cntk'}\n-                    and 'TRAVIS_PYTHON_VERSION' in os.environ,\n-                    reason='Generators cannot use `spawn`.')\n-def test_generator_methods_interface():\n+def DISABLED_test_generator_methods_interface():\n+    \"\"\"This test may cause Travis to hang.\"\"\"\n+\n     def train_generator():\n         x = np.random.randn(2, 2)\n         y = np.random.randint(0, 2, size=[2, 1])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#daebe7ca6fcdc304a87ff01a3400ad201d826ee7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 451 | Contributors (this commit): 12 | Commits (past 90d): 5 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -135,6 +135,8 @@ def test_adam_amsgrad():\n     _test_optimizer(optimizers.Adam(amsgrad=True, decay=1e-3))\n \n \n+@pytest.mark.skipif((K.backend() == 'cntk'),\n+                    reason='Flaky with CNTK')\n def test_clipnorm():\n     sgd = optimizers.SGD(lr=0.01, momentum=0.9, clipnorm=0.5)\n     _test_optimizer(sgd)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a69eaa899abbd9f99a8a81abddb02503e84e137e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): -8/0 | Churn Δ: 2 | Churn Cumulative: 667 | Contributors (this commit): 13 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -415,7 +415,7 @@ def test_finite_generator_enqueuer_threads():\n \n \n @skip_generators\n-def test_finite_generator_enqueuer_processes():\n+def DISABLED_test_finite_generator_enqueuer_processes():\n     enqueuer = GeneratorEnqueuer(create_finite_generator_from_sequence_pcs(\n         DummySequence([3, 10, 10, 3])), use_multiprocessing=True)\n     enqueuer.start(3, 10)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#519fac7c4eed5c5115b46d2402adb7b01ade58be", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 671 | Contributors (this commit): 13 | Commits (past 90d): 3 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -223,7 +223,7 @@ def test_generator_enqueuer_threads():\n \n \n @skip_generators\n-def test_generator_enqueuer_processes():\n+def DISABLED_test_generator_enqueuer_processes():\n     enqueuer = GeneratorEnqueuer(create_generator_from_sequence_pcs(\n         DummySequence([3, 10, 10, 3])), use_multiprocessing=True)\n     enqueuer.start(3, 10)\n@@ -259,7 +259,7 @@ def test_generator_enqueuer_fail_threads():\n \n \n @skip_generators\n-def test_generator_enqueuer_fail_processes():\n+def DISABLED_test_generator_enqueuer_fail_processes():\n     enqueuer = GeneratorEnqueuer(create_generator_from_sequence_pcs(\n         FaultSequence()), use_multiprocessing=True)\n     enqueuer.start(3, 10)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3b853b939953040e8841911efa05407e7e1fa91f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 5/6 | Churn Δ: 33 | Churn Cumulative: 3986 | Contributors (this commit): 15 | Commits (past 90d): 8 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1164,6 +1164,25 @@ class Layer(object):\n                 'We found {} metrics with the name: \"{}\"'.format(len(match), name))\n         return match[0]\n \n+    def __setattr__(self, name, value):\n+        # Keep track of metric instance created in subclassed model/layer.\n+        # We do this so that we can maintain the correct order of metrics by adding\n+        # the instance to the `metrics` list as soon as it is created.\n+        from .. import metrics as metrics_module\n+        if isinstance(value, metrics_module.Metric):\n+            if not hasattr(self,  '_metrics'):\n+                self._metrics = []\n+            self._metrics.append(value)\n+        else:\n+            # Automatically track layers set as attributes.\n+            if isinstance(value, Layer):\n+                if not hasattr(self,  '_layers'):\n+                    self._layers = []\n+                if value not in self._layers:\n+                    self._layers.append(value)\n+\n+        super(Layer, self).__setattr__(name, value)\n+\n \n def _create_mean_metric(value, name=None):\n     from .. import metrics\n\n@@ -312,7 +312,7 @@ class Network(Layer):\n     def __setattr__(self, name, value):\n         # Automatically track layers set as Model\n         # attributes for subclassed Models.\n-        if isinstance(value, (Layer, Network)):\n+        if isinstance(value, Layer):\n             try:\n                 is_graph_network = self._is_graph_network\n             except AttributeError:\n@@ -320,25 +320,15 @@ class Network(Layer):\n                     'It looks like you are subclassing `Model` and you '\n                     'forgot to call `super(YourClass, self).__init__()`.'\n                     ' Always start with this line.')\n-            if not is_graph_network:\n-                if value not in self._layers:\n-                    self._layers.append(value)\n         super(Network, self).__setattr__(name, value)\n \n-        # Keep track of metric instance created in subclassed model/layer.\n-        # We do this so that we can maintain the correct order of metrics by adding\n-        # the instance to the `metrics` list as soon as it is created.\n-        from .. import metrics as metrics_module\n-        if isinstance(value, metrics_module.Metric):\n-            self._metrics.append(value)\n-\n     @property\n     def layers(self):\n         return self._layers\n \n     @property\n     def metrics(self):\n-        metrics = self._metrics\n+        metrics = self._metrics[:]\n         for l in self.layers:\n             metrics += l.metrics\n         return metrics\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#479fc3a978221bb835f7e2781dcef0db66c96e1c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 312 | Lines Deleted: 33 | Files Changed: 12 | Hunks: 43 | Methods Changed: 38 | Complexity Δ (Sum/Max): 48/24 | Churn Δ: 345 | Churn Cumulative: 46430 | Contributors (this commit): 234 | Commits (past 90d): 47 | Contributors (cumulative): 435 | DMM Complexity: 0.88\n\nDIFF:\n@@ -12,6 +12,7 @@ from .load_backend import set_learning_phase\n from .load_backend import is_sparse\r\n from .load_backend import to_dense\r\n from .load_backend import variable\r\n+from .load_backend import is_variable\r\n from .load_backend import constant\r\n from .load_backend import is_keras_tensor\r\n from .load_backend import is_tensor\r\n\n@@ -194,6 +194,10 @@ def variable(value, dtype=None, name=None, constraint=None):\n     return v\n \n \n+def is_variable(x):\n+    return isinstance(x, C.variables.Parameter)\n+\n+\n def bias_add(x, bias, data_format=None):\n     data_format = normalize_data_format(data_format)\n \n\n@@ -615,6 +615,10 @@ def variable(value, dtype=None, name=None, constraint=None):\n     return v\n \n \n+def is_variable(x):\n+    return isinstance(x, tf.Variable)\n+\n+\n def constant(value, dtype=None, shape=None, name=None):\n     \"\"\"Creates a constant tensor.\n \n\n@@ -154,6 +154,10 @@ def variable(value, dtype=None, name=None, constraint=None):\n     return variable\n \n \n+def is_variable(x):\n+    return isinstance(x, (T.TensorVariable, T.TensorSharedVariable))\n+\n+\n def constant(value, dtype=None, shape=None, name=None):\n     if dtype is None:\n         dtype = floatx()\n\n@@ -6,6 +6,7 @@ from __future__ import division\n \n import re\n from six.moves import zip\n+import threading\n \n from .. import backend as K\n from .. import initializers\n@@ -17,6 +18,20 @@ from ..utils.generic_utils import unpack_singleton\n from ..utils.generic_utils import is_all_none\n from ..legacy import interfaces\n \n+_DISABLE_TRACKING = threading.local()\n+_DISABLE_TRACKING.value = False\n+\n+\n+def disable_tracking(func):\n+    def wrapped_fn(*args, **kwargs):\n+        global _DISABLE_TRACKING\n+        prev_value = _DISABLE_TRACKING.value\n+        _DISABLE_TRACKING.value = True\n+        out = func(*args, **kwargs)\n+        _DISABLE_TRACKING.value = prev_value\n+        return out\n+    return wrapped_fn\n+\n \n class Layer(object):\n     \"\"\"Abstract base layer class.\n@@ -176,13 +191,19 @@ class Layer(object):\n \n     @property\n     def losses(self):\n-        return self._losses\n+        losses = self._losses[:]\n+        for l in getattr(self, '_layers', []):\n+            losses += l.losses\n+        return losses\n \n     @property\n     def updates(self):\n         if not self.trainable and not self.stateful:\n             return []\n-        return self._updates\n+        updates = self._updates[:]\n+        for l in getattr(self, '_layers', []):\n+            updates += l.updates\n+        return updates\n \n     @property\n     def built(self):\n@@ -196,7 +217,10 @@ class Layer(object):\n     def trainable_weights(self):\n         trainable = getattr(self, 'trainable', True)\n         if trainable:\n-            return self._trainable_weights\n+            trainable_weights = self._trainable_weights[:]\n+            for l in getattr(self, '_layers', []):\n+                trainable_weights += l.trainable_weights\n+            return trainable_weights\n         else:\n             return []\n \n@@ -207,19 +231,25 @@ class Layer(object):\n     @property\n     def non_trainable_weights(self):\n         trainable = getattr(self, 'trainable', True)\n-        if not trainable:\n-            return self._trainable_weights + self._non_trainable_weights\n+        if trainable:\n+            weights = self._non_trainable_weights[:]\n+            for l in getattr(self, '_layers', []):\n+                weights += l.non_trainable_weights\n+            return weights\n+\n         else:\n-            return self._non_trainable_weights\n+            weights = self._trainable_weights[:] + self._non_trainable_weights[:]\n+            for l in getattr(self, '_layers', []):\n+                weights += l.weights\n+            return weights\n \n     @non_trainable_weights.setter\n     def non_trainable_weights(self, weights):\n         self._non_trainable_weights = weights\n \n-    @interfaces.legacy_add_weight_support\n     def add_weight(self,\n-                   name,\n-                   shape,\n+                   name=None,\n+                   shape=None,\n                    dtype=None,\n                    initializer=None,\n                    regularizer=None,\n@@ -241,6 +271,8 @@ class Layer(object):\n         # Returns\n             The created weight variable.\n         \"\"\"\n+        if shape is None:\n+            shape = ()\n         initializer = initializers.get(initializer)\n         if dtype is None:\n             dtype = self.dtype\n@@ -255,6 +287,7 @@ class Layer(object):\n             self._trainable_weights.append(weight)\n         else:\n             self._non_trainable_weights.append(weight)\n+        weight._tracked = True\n         return weight\n \n     def assert_input_compatibility(self, inputs):\n@@ -932,7 +965,10 @@ class Layer(object):\n \n     @property\n     def metrics(self):\n-        return self._metrics\n+        metrics = self._metrics[:]\n+        for l in getattr(self, '_layers', []):\n+            metrics += l.metrics\n+        return metrics\n \n     def add_metric(self, value, name=None):\n         \"\"\"Adds metric tensor to the layer.\n@@ -1030,18 +1066,24 @@ class Layer(object):\n             inputs_hash = object_list_uid(inputs)\n         else:\n             inputs_hash = None\n+        updates = []\n         if inputs_hash in self._per_input_updates:\n-            return self._per_input_updates[inputs_hash]\n-        return []\n+            updates += self._per_input_updates[inputs_hash]\n+        for l in getattr(self, '_layers', []):\n+            updates += l.get_updates_for(inputs)\n+        return updates\n \n     def get_losses_for(self, inputs):\n         if inputs is not None:\n             inputs_hash = object_list_uid(inputs)\n         else:\n             inputs_hash = None\n+        losses = []\n         if inputs_hash in self._per_input_losses:\n-            return self._per_input_losses[inputs_hash]\n-        return []\n+            losses += self._per_input_losses[inputs_hash]\n+        for l in getattr(self, '_layers', []):\n+            losses += l.get_losses_for(inputs)\n+        return losses\n \n     @property\n     def weights(self):\n@@ -1168,6 +1210,7 @@ class Layer(object):\n         # Keep track of metric instance created in subclassed model/layer.\n         # We do this so that we can maintain the correct order of metrics by adding\n         # the instance to the `metrics` list as soon as it is created.\n+        if not _DISABLE_TRACKING.value:\n             from .. import metrics as metrics_module\n             if isinstance(value, metrics_module.Metric):\n                 if not hasattr(self,  '_metrics'):\n@@ -1180,6 +1223,20 @@ class Layer(object):\n                         self._layers = []\n                     if value not in self._layers:\n                         self._layers.append(value)\n+                if K.is_variable(value) and not getattr(value, '_tracked', False):\n+                    # Automatically track variables set as attributes.\n+                    trainable = getattr(value, 'trainable', False)\n+                    if trainable:\n+                        if not hasattr(self, '_trainable_weights'):\n+                            self._trainable_weights = []\n+                        if not any(v is value for v in self._trainable_weights):\n+                            print('tracking', value, name)\n+                            self._trainable_weights.append(value)\n+                    else:\n+                        if not hasattr(self, '_non_trainable_weights'):\n+                            self._non_trainable_weights = []\n+                        if not any(v is value for v in self._non_trainable_weights):\n+                            self._non_trainable_weights.append(value)\n \n         super(Layer, self).__setattr__(name, value)\n \n\n@@ -326,13 +326,6 @@ class Network(Layer):\n     def layers(self):\n         return self._layers\n \n-    @property\n-    def metrics(self):\n-        metrics = self._metrics[:]\n-        for l in self.layers:\n-            metrics += l.metrics\n-        return metrics\n-\n     def get_layer(self, name=None, index=None):\n         \"\"\"Retrieves a layer based on either its name (unique) or index.\n \n@@ -482,18 +475,18 @@ class Network(Layer):\n     def trainable_weights(self):\n         if not self.trainable:\n             return []\n-        weights = []\n+        weights = self._trainable_weights[:]\n         for layer in self.layers:\n             weights += layer.trainable_weights\n         return weights\n \n     @property\n     def non_trainable_weights(self):\n-        weights = []\n+        weights = self._non_trainable_weights[:]\n         for layer in self.layers:\n             weights += layer.non_trainable_weights\n         if not self.trainable:\n-            trainable_weights = []\n+            trainable_weights = self._trainable_weights[:]\n             for layer in self.layers:\n                 trainable_weights += layer.trainable_weights\n             return trainable_weights + weights\n@@ -505,7 +498,7 @@ class Network(Layer):\n         # Returns\n             A flat list of Numpy arrays.\n         \"\"\"\n-        weights = []\n+        weights = self._trainable_weights + self._non_trainable_weights\n         for layer in self.layers:\n             weights += layer.weights\n         return K.batch_get_value(weights)\n@@ -518,6 +511,13 @@ class Network(Layer):\n                 the output of `model.get_weights()`.\n         \"\"\"\n         tuples = []\n+        own_weight_vars = self._trainable_weights + self._non_trainable_weights\n+        num_param = len(own_weight_vars)\n+        own_weights = weights[:num_param]\n+        for sw, w in zip(own_weight_vars, own_weights):\n+            tuples.append((sw, w))\n+        weights = weights[num_param:]\n+\n         for layer in self.layers:\n             num_param = len(layer.weights)\n             layer_weights = weights[:num_param]\n\n@@ -63,7 +63,7 @@ def _uniquify(names):\n     for name in names:\n         if name in counts:\n             counts[name] += 1\n-            name = name + '_' + str(counts[name])\n+            name = str(name) + '_' + str(counts[name])\n         else:\n             counts[name] = 1\n         unique_names.append(name)\n\n@@ -5,7 +5,7 @@ from __future__ import absolute_import\n from __future__ import division\n from __future__ import print_function\n \n-from ..engine.base_layer import Layer, InputSpec\n+from ..engine.base_layer import Layer, InputSpec, disable_tracking\n from .. import initializers\n from .. import regularizers\n from .. import constraints\n\n@@ -14,6 +14,7 @@ from .. import initializers\n from .. import regularizers\n from .. import constraints\n from ..engine.base_layer import Layer\n+from ..engine.base_layer import disable_tracking\n from ..engine.base_layer import InputSpec\n from ..utils.generic_utils import has_arg\n from ..utils.generic_utils import to_list\n@@ -45,6 +46,7 @@ class StackedRNNCells(Layer):\n     ```\n     \"\"\"\n \n+    @disable_tracking\n     def __init__(self, cells, **kwargs):\n         for cell in cells:\n             if not hasattr(cell, 'call'):\n@@ -389,6 +391,7 @@ class RNN(Layer):\n     ```\n     \"\"\"\n \n+    @disable_tracking\n     def __init__(self, cell,\n                  return_sequences=False,\n                  return_state=False,\n\n@@ -7,6 +7,7 @@ from __future__ import print_function\n \n import copy\n from ..engine.base_layer import Layer\n+from ..engine.base_layer import disable_tracking\n from ..engine.base_layer import InputSpec\n from ..utils.generic_utils import has_arg\n from ..utils.generic_utils import object_list_uid\n@@ -26,6 +27,7 @@ class Wrapper(Layer):\n         layer: The layer to be wrapped.\n     \"\"\"\n \n+    @disable_tracking\n     def __init__(self, layer, **kwargs):\n         self.layer = layer\n         # Tracks mapping of Wrapper inputs to inner layer inputs. Useful when\n@@ -358,6 +360,7 @@ class Bidirectional(Wrapper):\n     ```\n     \"\"\"\n \n+    @disable_tracking\n     def __init__(self, layer, merge_mode='concat', weights=None, **kwargs):\n         if merge_mode not in ['sum', 'mul', 'ave', 'concat', None]:\n             raise ValueError('Invalid merge mode. '\n\n@@ -98,13 +98,13 @@ class MaxoutDense(Layer):\n         self.input_spec = InputSpec(dtype=K.floatx(),\n                                     shape=(None, input_dim))\n \n-        self.W = self.add_weight((self.nb_feature, input_dim, self.output_dim),\n+        self.W = self.add_weight(shape=(self.nb_feature, input_dim, self.output_dim),\n                                  initializer=self.init,\n                                  name='W',\n                                  regularizer=self.W_regularizer,\n                                  constraint=self.W_constraint)\n         if self.bias:\n-            self.b = self.add_weight((self.nb_feature, self.output_dim,),\n+            self.b = self.add_weight(shape=(self.nb_feature, self.output_dim,),\n                                      initializer='zero',\n                                      name='b',\n                                      regularizer=self.b_regularizer,\n@@ -227,21 +227,21 @@ class Highway(Layer):\n         self.input_spec = InputSpec(dtype=K.floatx(),\n                                     shape=(None, input_dim))\n \n-        self.W = self.add_weight((input_dim, input_dim),\n+        self.W = self.add_weight(shape=(input_dim, input_dim),\n                                  initializer=self.init,\n                                  name='W',\n                                  regularizer=self.W_regularizer,\n                                  constraint=self.W_constraint)\n-        self.W_carry = self.add_weight((input_dim, input_dim),\n+        self.W_carry = self.add_weight(shape=(input_dim, input_dim),\n                                        initializer=self.init,\n                                        name='W_carry')\n         if self.bias:\n-            self.b = self.add_weight((input_dim,),\n+            self.b = self.add_weight(shape=(input_dim,),\n                                      initializer='zero',\n                                      name='b',\n                                      regularizer=self.b_regularizer,\n                                      constraint=self.b_constraint)\n-            self.b_carry = self.add_weight((input_dim,),\n+            self.b_carry = self.add_weight(shape=(input_dim,),\n                                            initializer='one',\n                                            name='b_carry')\n         else:\n\n@@ -0,0 +1,203 @@\n+import pytest\n+import keras\n+import numpy as np\n+from keras import layers\n+from keras import backend as K\n+\n+\n+def test_sublayer_tracking():\n+    # basic case\n+    class MyLayer(layers.Layer):\n+\n+        def __init__(self):\n+            super(MyLayer, self).__init__()\n+            self._input_shape = (2, 4)\n+            self.dense = layers.Dense(3)\n+            self.bidir = layers.Bidirectional(keras.layers.LSTM(2))\n+\n+        def call(self, inputs):\n+            return self.dense(self.bidir(inputs))\n+\n+    layer = MyLayer()\n+    assert len(layer._layers) == 2\n+    layer(K.constant(np.random.random((2,) + layer._input_shape)))\n+    assert len(layer.weights) == 2 + 3 + 3\n+    assert len(layer._layers[0].weights) == 2\n+    assert len(layer._layers[1].weights) == 6\n+\n+    # recursive case\n+    class MyRecursiveLayer(layers.Layer):\n+\n+        def __init__(self):\n+            super(MyRecursiveLayer, self).__init__()\n+            self._input_shape = (2, 4)\n+            self.my_layer = MyLayer()\n+            self.dense = layers.Dense(3)\n+            self.bidir = layers.Bidirectional(\n+                keras.layers.LSTM(2, return_sequences=True))\n+\n+        def call(self, inputs):\n+            return self.my_layer(self.dense(self.bidir(inputs)))\n+\n+    layer = MyRecursiveLayer()\n+    assert len(layer._layers) == 3\n+    layer(K.constant(np.random.random((2,) + layer._input_shape)))\n+    assert len(layer.weights) == 16\n+\n+    # subnetwork case\n+    class MyLayerWithSubnetwork(keras.layers.Layer):\n+\n+        def __init__(self):\n+            super(MyLayerWithSubnetwork, self).__init__()\n+            self._input_shape = (2,)\n+            self.dense = layers.Dense(3)\n+            self.sequential = keras.Sequential(\n+                [layers.Dense(5), layers.Dense(1)], name='seq')\n+            inputs = keras.Input((1,))\n+            outputs = layers.Dense(1)(inputs)\n+            self.functional = keras.Model(inputs, outputs, name='func')\n+\n+        def call(self, inputs):\n+            x = self.dense(inputs)\n+            x = self.sequential(x)\n+            return self.functional(x)\n+\n+    layer = MyLayerWithSubnetwork()\n+    assert len(layer._layers) == 3\n+    layer(K.constant(np.random.random((2,) + layer._input_shape)))\n+    assert len(layer.weights) == 2 + (2 + 2) + 2\n+    assert len(layer._layers[0].weights) == 2\n+    assert len(layer._layers[1].weights) == 4\n+    assert len(layer._layers[2].weights) == 2\n+\n+\n+def test_weight_tracking():\n+\n+    class MyLayer(layers.Layer):\n+\n+        def __init__(self):\n+            super(MyLayer, self).__init__()\n+            self._input_shape = (2,)\n+            self.dense = layers.Dense(3)\n+            self.w1 = K.variable(0, name='w1')\n+\n+        def build(self, input_shape):\n+            self.w2 = K.variable(1, name='w2')\n+            self.w3 = self.add_weight(\n+                'w3', shape=(), trainable=False, initializer='zeros')\n+\n+        def call(self, inputs):\n+            return self.dense(inputs) + self.w1 + self.w2\n+\n+    layer = MyLayer()\n+    layer(K.constant(np.random.random((2,) + layer._input_shape)))\n+    assert len(layer.weights) == 5\n+    assert len(layer.trainable_weights) == 4\n+    assert len(layer.non_trainable_weights) == 1\n+    assert len(layer._trainable_weights) == 2\n+    assert layer._trainable_weights[0] is layer.w1\n+    assert layer._trainable_weights[1] is layer.w2\n+    assert len(layer._non_trainable_weights) == 1\n+    assert layer._non_trainable_weights[0] is layer.w3\n+\n+\n+def test_loss_tracking():\n+    # basic case\n+    class MyLayer(layers.Layer):\n+\n+        def __init__(self):\n+            super(MyLayer, self).__init__()\n+            self.dense = layers.Dense(\n+                3, kernel_regularizer='l2', activity_regularizer='l2')\n+\n+        def call(self, inputs):\n+            return self.dense(inputs)\n+\n+    inputs = keras.Input((2,))\n+    outputs = MyLayer()(inputs)\n+    model = keras.Model(inputs, outputs)\n+\n+    assert len(model.layers) == 2  # includes input layer\n+    assert len(model.weights) == 2\n+    assert len(model.losses) == 2\n+    assert len(model.get_losses_for(None)) == 1\n+    assert len(model.get_losses_for(inputs)) == 1\n+\n+\n+@pytest.mark.skipif(K.backend() != 'tensorflow',\n+                    reason='Requires TF symbols')\n+def test_tf_keras_guide():\n+    import tensorflow as tf\n+\n+    class Linear(layers.Layer):\n+\n+        def __init__(self, units=32, input_dim=32):\n+            super(Linear, self).__init__()\n+            w_init = tf.random_normal_initializer()\n+            self.w = tf.Variable(initial_value=w_init(shape=(input_dim, units),\n+                                                      dtype='float32'),\n+                                 trainable=True)\n+            b_init = tf.zeros_initializer()\n+            self.b = tf.Variable(initial_value=b_init(shape=(units,),\n+                                                      dtype='float32'),\n+                                 trainable=True)\n+\n+        def call(self, inputs):\n+            return tf.matmul(inputs, self.w) + self.b\n+\n+    x = tf.ones((2, 2))\n+    linear_layer = Linear(4, 2)\n+    y = linear_layer(x)\n+\n+    assert len(linear_layer.trainable_weights) == 2\n+\n+    class Linear(layers.Layer):\n+\n+        def __init__(self, units=32):\n+            super(Linear, self).__init__()\n+            self.units = units\n+\n+        def build(self, input_shape):\n+            self.w = self.add_weight(shape=(input_shape[-1], self.units),\n+                                     initializer='random_normal',\n+                                     trainable=True)\n+            self.b = self.add_weight(shape=(self.units,),\n+                                     initializer='random_normal',\n+                                     trainable=True)\n+\n+        def call(self, inputs):\n+            return tf.matmul(inputs, self.w) + self.b\n+\n+    class MLPBlock(layers.Layer):\n+\n+        def __init__(self):\n+            super(MLPBlock, self).__init__()\n+            self.linear_1 = Linear(32)\n+            self.linear_2 = Linear(32)\n+            self.linear_3 = Linear(1)\n+\n+        def call(self, inputs):\n+            x = self.linear_1(inputs)\n+            x = tf.nn.relu(x)\n+            x = self.linear_2(x)\n+            x = tf.nn.relu(x)\n+            return self.linear_3(x)\n+\n+    mlp = MLPBlock()\n+    y = mlp(tf.ones(shape=(3, 64)))\n+    assert len(mlp.weights) == 6\n+    assert len(mlp.trainable_weights) == 6\n+\n+    class OuterLayer(layers.Layer):\n+\n+        def __init__(self):\n+            super(OuterLayer, self).__init__()\n+            self.dense = layers.Dense(\n+                32, kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n+\n+        def call(self, inputs):\n+            return self.dense(inputs)\n+\n+    layer = OuterLayer()\n+    _ = layer(tf.zeros((1, 1)))\n+    assert len(layer.losses) == 1\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#655cfd33494ee45c80bcac4d57c41e7b516f5e4c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 6666 | Contributors (this commit): 93 | Commits (past 90d): 7 | Contributors (cumulative): 93 | DMM Complexity: None\n\nDIFF:\n@@ -155,7 +155,7 @@ def variable(value, dtype=None, name=None, constraint=None):\n \n \n def is_variable(x):\n-    return isinstance(x, (T.TensorVariable, T.TensorSharedVariable))\n+    return isinstance(x, theano.tensor.sharedvar.TensorSharedVariable)\n \n \n def constant(value, dtype=None, shape=None, name=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#969db5ad69b3efed0ad6bd293812c1c08d71feb0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 406 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 5 | Methods Changed: 19 | Complexity Δ (Sum/Max): 21/12 | Churn Δ: 409 | Churn Cumulative: 2416 | Contributors (this commit): 23 | Commits (past 90d): 24 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -261,7 +261,7 @@ class Mean(Reduce):\n     Usage:\n \n     ```python\n-    m = tf.keras.metrics.Mean()\n+    m = keras.metrics.Mean()\n     m.update_state([1, 3, 5, 7])\n     m.result()\n     ```\n@@ -321,7 +321,7 @@ class MeanMetricWrapper(Mean):\n     def get_config(self):\n         config = {}\n         for k, v in six.iteritems(self._fn_kwargs):\n-            config[k] = K.eval(v) if is_tensor_or_variable(v) else v\n+            config[k] = K.eval(v) if K.is_tensor(v) else v\n         base_config = super(MeanMetricWrapper, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n@@ -350,7 +350,199 @@ class MeanSquaredError(MeanMetricWrapper):\n             mean_squared_error, name, dtype=dtype)\n \n \n-def binary_accuracy(y_true, y_pred):\n+class Accuracy(MeanMetricWrapper):\n+    \"\"\"Calculates how often predictions matches labels.\n+\n+    For example, if `y_true` is [1, 2, 3, 4] and `y_pred` is [0, 2, 3, 4]\n+    then the accuracy is 3/4 or .75.  If the weights were specified as\n+    [1, 1, 0, 0] then the accuracy would be 1/2 or .5.\n+\n+    This metric creates two local variables, `total` and `count` that are used to\n+    compute the frequency with which `y_pred` matches `y_true`. This frequency is\n+    ultimately returned as `binary accuracy`: an idempotent operation that simply\n+    divides `total` by `count`.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    ```\n+\n+    Usage with the compile API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile('sgd', loss='mse', metrics=[keras.metrics.Accuracy()])\n+    ```\n+    \"\"\"\n+\n+    def __init__(self, name='accuracy', dtype=None):\n+        super(Accuracy, self).__init__(accuracy, name, dtype=dtype)\n+\n+\n+class BinaryAccuracy(MeanMetricWrapper):\n+    \"\"\"Calculates how often predictions matches labels.\n+\n+    For example, if `y_true` is [1, 1, 0, 0] and `y_pred` is [0.98, 1, 0, 0.6]\n+    then the binary accuracy is 3/4 or .75.  If the weights were specified as\n+    [1, 0, 0, 1] then the binary accuracy would be 1/2 or .5.\n+\n+    This metric creates two local variables, `total` and `count` that are used to\n+    compute the frequency with which `y_pred` matches `y_true`. This frequency is\n+    ultimately returned as `binary accuracy`: an idempotent operation that simply\n+    divides `total` by `count`.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Usage with the compile API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile('sgd', loss='mse', metrics=[keras.metrics.BinaryAccuracy()])\n+    ```\n+\n+    # Arguments\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+        threshold: (Optional) Float representing the threshold for deciding\n+            whether prediction values are 1 or 0.\n+    \"\"\"\n+\n+    def __init__(self, name='binary_accuracy', dtype=None, threshold=0.5):\n+        super(BinaryAccuracy, self).__init__(\n+            binary_accuracy, name, dtype=dtype, threshold=threshold)\n+\n+\n+class CategoricalAccuracy(MeanMetricWrapper):\n+    \"\"\"Calculates how often predictions matches labels.\n+\n+    For example, if `y_true` is [[0, 0, 1], [0, 1, 0]] and `y_pred` is\n+    [[0.1, 0.9, 0.8], [0.05, 0.95, 0]] then the categorical accuracy is 1/2 or .5.\n+    If the weights were specified as [0.7, 0.3] then the categorical accuracy\n+    would be .3. You can provide logits of classes as `y_pred`, since argmax of\n+    logits and probabilities are same.\n+\n+    This metric creates two local variables, `total` and `count` that are used to\n+    compute the frequency with which `y_pred` matches `y_true`. This frequency is\n+    ultimately returned as `categorical accuracy`: an idempotent operation that\n+    simply divides `total` by `count`.\n+\n+    `y_pred` and `y_true` should be passed in as vectors of probabilities, rather\n+    than as labels. If necessary, use `K.one_hot` to expand `y_true` as a vector.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Usage with the compile API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile(\n+        'sgd',\n+        loss='mse',\n+        metrics=[keras.metrics.CategoricalAccuracy()])\n+    ```\n+\n+    # Arguments\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+    \"\"\"\n+\n+    def __init__(self, name='categorical_accuracy', dtype=None):\n+        super(CategoricalAccuracy, self).__init__(\n+            categorical_accuracy, name, dtype=dtype)\n+\n+\n+class SparseCategoricalAccuracy(MeanMetricWrapper):\n+    \"\"\"Calculates how often predictions matches integer labels.\n+\n+    For example, if `y_true` is [[2], [1]] and `y_pred` is\n+    [[0.1, 0.9, 0.8], [0.05, 0.95, 0]] then the categorical accuracy is 1/2 or .5.\n+    If the weights were specified as [0.7, 0.3] then the categorical accuracy\n+    would be .3. You can provide logits of classes as `y_pred`, since argmax of\n+    logits and probabilities are same.\n+\n+    This metric creates two local variables, `total` and `count` that are used to\n+    compute the frequency with which `y_pred` matches `y_true`. This frequency is\n+    ultimately returned as `sparse categorical accuracy`: an idempotent operation\n+    that simply divides `total` by `count`.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Usage with the compile API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile(\n+        'sgd',\n+        loss='mse',\n+        metrics=[keras.metrics.SparseCategoricalAccuracy()])\n+    ```\n+    \"\"\"\n+\n+    def __init__(self, name='sparse_categorical_accuracy', dtype=None):\n+        super(SparseCategoricalAccuracy, self).__init__(\n+            sparse_categorical_accuracy, name, dtype=dtype)\n+\n+\n+class TopKCategoricalAccuracy(MeanMetricWrapper):\n+    \"\"\"Computes how often targets are in the top `K` predictions.\n+\n+    Usage with the compile API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile('sgd', metrics=[keras.metrics.TopKCategoricalAccuracy()])\n+    ```\n+\n+    # Arguments\n+        k: (Optional) Number of top elements to look at for computing accuracy.\n+            Defaults to 5.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+    \"\"\"\n+\n+    def __init__(self, k=5, name='top_k_categorical_accuracy', dtype=None):\n+        super(TopKCategoricalAccuracy, self).__init__(\n+            top_k_categorical_accuracy, name, dtype=dtype, k=k)\n+\n+\n+class SparseTopKCategoricalAccuracy(MeanMetricWrapper):\n+    \"\"\"Computes how often integer targets are in the top `K` predictions.\n+\n+    Usage with the compile API:\n+\n+    ```python\n+    model = keras.Model(inputs, outputs)\n+    model.compile(\n+        'sgd',\n+        metrics=[keras.metrics.SparseTopKCategoricalAccuracy()])\n+    ```\n+\n+    # Arguments\n+        k: (Optional) Number of top elements to look at for computing accuracy.\n+            Defaults to 5.\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+    \"\"\"\n+\n+    def __init__(self, k=5, name='sparse_top_k_categorical_accuracy', dtype=None):\n+        super(SparseTopKCategoricalAccuracy, self).__init__(\n+            sparse_top_k_categorical_accuracy, name, dtype=dtype, k=k)\n+\n+\n+def accuracy(y_true, y_pred):\n+    if not K.is_tensor(y_pred):\n+        y_pred = K.constant(y_pred)\n+    y_true = K.cast(y_true, y_pred.dtype)\n+    return K.cast(K.equal(y_true, y_pred), K.floatx())\n+\n+\n+def binary_accuracy(y_true, y_pred, threshold=0.5):\n+    if threshold != 0.5:\n+        threshold = K.cast(threshold, y_pred.dtype)\n+        y_pred = K.cast(y_pred > threshold, y_pred.dtype)\n     return K.mean(K.equal(y_true, K.round(y_pred)), axis=-1)\n \n \n\n@@ -177,6 +177,134 @@ class TestMean(object):\n         assert K.eval(m.count) == 1\n \n \n+class TestAccuracy(object):\n+\n+    def test_accuracy(self):\n+        acc_obj = metrics.Accuracy(name='my_acc')\n+\n+        # check config\n+        assert acc_obj.name == 'my_acc'\n+        assert acc_obj.stateful\n+        assert len(acc_obj.weights) == 2\n+        assert acc_obj.dtype == 'float32'\n+\n+        # verify that correct value is returned\n+        result = K.eval(acc_obj([[1], [2], [3], [4]], [[1], [2], [3], [4]]))\n+        assert result == 1  # 2/2\n+\n+        # Check save and restore config\n+        a2 = metrics.Accuracy.from_config(acc_obj.get_config())\n+        assert a2.name == 'my_acc'\n+        assert a2.stateful\n+        assert len(a2.weights) == 2\n+        assert a2.dtype, 'float32'\n+\n+        # check with sample_weight\n+        result_t = acc_obj([[2], [1]], [[2], [0]], sample_weight=[[0.5], [0.2]])\n+        result = K.eval(result_t)\n+        assert np.isclose(result, 4.5 / 4.7, atol=1e-3)\n+\n+    def test_binary_accuracy(self):\n+        acc_obj = metrics.BinaryAccuracy(name='my_acc')\n+\n+        # check config\n+        assert acc_obj.name == 'my_acc'\n+        assert acc_obj.stateful\n+        assert len(acc_obj.weights) == 2\n+        assert acc_obj.dtype == 'float32'\n+\n+        # verify that correct value is returned\n+        result_t = acc_obj([[1], [0]], [[1], [0]])\n+        result = K.eval(result_t)\n+        assert result == 1  # 2/2\n+\n+        # check y_pred squeeze\n+        result_t = acc_obj([[1], [1]], [[[1]], [[0]]])\n+        result = K.eval(result_t)\n+        assert np.isclose(result, 3. / 4., atol=1e-3)\n+\n+        # check y_true squeeze\n+        result_t = acc_obj([[[1]], [[1]]], [[1], [0]])\n+        result = K.eval(result_t)\n+        assert np.isclose(result, 4. / 6., atol=1e-3)\n+\n+        # check with sample_weight\n+        result_t = acc_obj([[1], [1]], [[1], [0]], [[0.5], [0.2]])\n+        result = K.eval(result_t)\n+        assert np.isclose(result, 4.5 / 6.7, atol=1e-3)\n+\n+    def test_binary_accuracy_threshold(self):\n+        acc_obj = metrics.BinaryAccuracy(threshold=0.7)\n+        result_t = acc_obj([[1], [1], [0], [0]], [[0.9], [0.6], [0.4], [0.8]])\n+        result = K.eval(result_t)\n+        assert np.isclose(result, 0.5, atol=1e-3)\n+\n+    def test_categorical_accuracy(self):\n+        acc_obj = metrics.CategoricalAccuracy(name='my_acc')\n+\n+        # check config\n+        assert acc_obj.name == 'my_acc'\n+        assert acc_obj.stateful\n+        assert len(acc_obj.weights) == 2\n+        assert acc_obj.dtype == 'float32'\n+\n+        # verify that correct value is returned\n+        result_t = acc_obj([[0, 0, 1], [0, 1, 0]],\n+                           [[0.1, 0.1, 0.8], [0.05, 0.95, 0]])\n+        result = K.eval(result_t)\n+        assert result == 1  # 2/2\n+\n+        # check with sample_weight\n+        result_t = acc_obj([[0, 0, 1], [0, 1, 0]],\n+                           [[0.1, 0.1, 0.8], [0.05, 0, 0.95]],\n+                           [[0.5], [0.2]])\n+        result = K.eval(result_t)\n+        assert np.isclose(result, 2.5 / 2.7, atol=1e-3)  # 2.5/2.7\n+\n+    def test_sparse_categorical_accuracy(self):\n+        acc_obj = metrics.SparseCategoricalAccuracy(name='my_acc')\n+\n+        # check config\n+        assert acc_obj.name == 'my_acc'\n+        assert acc_obj.stateful\n+        assert len(acc_obj.weights) == 2\n+        assert acc_obj.dtype == 'float32'\n+\n+        # verify that correct value is returned\n+        result_t = acc_obj([[2], [1]],\n+                           [[0.1, 0.1, 0.8],\n+                           [0.05, 0.95, 0]])\n+        result = K.eval(result_t)\n+        assert result == 1  # 2/2\n+\n+        # check with sample_weight\n+        result_t = acc_obj([[2], [1]],\n+                           [[0.1, 0.1, 0.8], [0.05, 0, 0.95]],\n+                           [[0.5], [0.2]])\n+        result = K.eval(result_t)\n+        assert np.isclose(result, 2.5 / 2.7, atol=1e-3)\n+\n+    def test_sparse_categorical_accuracy_mismatched_dims(self):\n+        acc_obj = metrics.SparseCategoricalAccuracy(name='my_acc')\n+\n+        # check config\n+        assert acc_obj.name == 'my_acc'\n+        assert acc_obj.stateful\n+        assert len(acc_obj.weights) == 2\n+        assert acc_obj.dtype == 'float32'\n+\n+        # verify that correct value is returned\n+        result_t = acc_obj([2, 1], [[0.1, 0.1, 0.8], [0.05, 0.95, 0]])\n+        result = K.eval(result_t)\n+        assert result == 1  # 2/2\n+\n+        # check with sample_weight\n+        result_t = acc_obj([2, 1], [[0.1, 0.1, 0.8], [0.05, 0, 0.95]],\n+                           [[0.5], [0.2]])\n+        result = K.eval(result_t)\n+        assert np.isclose(result, 2.5 / 2.7, atol=1e-3)\n+\n+\n class TestMeanSquaredErrorTest(object):\n \n     def test_config(self):\n@@ -204,3 +332,86 @@ class TestMeanSquaredErrorTest(object):\n         sample_weight = (1., 1.5, 2., 2.5)\n         result = mse_obj(y_true, y_pred, sample_weight=sample_weight)\n         np.isclose(0.54285, K.eval(result), atol=1e-5)\n+\n+\n+class TestTopKCategoricalAccuracy(object):\n+\n+    def test_config(self):\n+        a_obj = metrics.TopKCategoricalAccuracy(name='topkca', dtype='int32')\n+        assert a_obj.name == 'topkca'\n+        assert a_obj.dtype == 'int32'\n+\n+        a_obj2 = metrics.TopKCategoricalAccuracy.from_config(a_obj.get_config())\n+        assert a_obj2.name == 'topkca'\n+        assert a_obj2.dtype == 'int32'\n+\n+    def test_correctness(self):\n+        a_obj = metrics.TopKCategoricalAccuracy()\n+        y_true = [[0, 0, 1], [0, 1, 0]]\n+        y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]\n+\n+        result = a_obj(y_true, y_pred)\n+        assert 1 == K.eval(result)  # both the samples match\n+\n+        # With `k` < 5.\n+        a_obj = metrics.TopKCategoricalAccuracy(k=1)\n+        result = a_obj(y_true, y_pred)\n+        assert 0.5 == K.eval(result)  # only sample #2 matches\n+\n+        # With `k` > 5.\n+        y_true = ([[0, 0, 1, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0]])\n+        y_pred = [[0.5, 0.9, 0.1, 0.7, 0.6, 0.5, 0.4],\n+                  [0.05, 0.95, 0, 0, 0, 0, 0]]\n+        a_obj = metrics.TopKCategoricalAccuracy(k=6)\n+        result = a_obj(y_true, y_pred)\n+        assert 0.5 == K.eval(result)  # only 1 sample matches.\n+\n+    def test_weighted(self):\n+        a_obj = metrics.TopKCategoricalAccuracy(k=2)\n+        y_true = [[0, 1, 0], [1, 0, 0], [0, 0, 1]]\n+        y_pred = [[0, 0.9, 0.1], [0, 0.9, 0.1], [0, 0.9, 0.1]]\n+        sample_weight = (1.0, 0.0, 1.0)\n+        result = a_obj(y_true, y_pred, sample_weight=sample_weight)\n+        assert np.allclose(1.0, K.eval(result), atol=1e-5)\n+\n+\n+class TestSparseTopKCategoricalAccuracy(object):\n+\n+    def test_config(self):\n+        a_obj = metrics.SparseTopKCategoricalAccuracy(\n+            name='stopkca', dtype='int32')\n+        assert a_obj.name == 'stopkca'\n+        assert a_obj.dtype == 'int32'\n+\n+        a_obj2 = metrics.SparseTopKCategoricalAccuracy.from_config(\n+            a_obj.get_config())\n+        assert a_obj2.name == 'stopkca'\n+        assert a_obj2.dtype == 'int32'\n+\n+    def test_correctness(self):\n+        a_obj = metrics.SparseTopKCategoricalAccuracy()\n+        y_true = [2, 1]\n+        y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]\n+\n+        result = a_obj(y_true, y_pred)\n+        assert 1 == K.eval(result)  # both the samples match\n+\n+        # With `k` < 5.\n+        a_obj = metrics.SparseTopKCategoricalAccuracy(k=1)\n+        result = a_obj(y_true, y_pred)\n+        assert 0.5 == K.eval(result)  # only sample #2 matches\n+\n+        # With `k` > 5.\n+        y_pred = [[0.5, 0.9, 0.1, 0.7, 0.6, 0.5, 0.4],\n+                  [0.05, 0.95, 0, 0, 0, 0, 0]]\n+        a_obj = metrics.SparseTopKCategoricalAccuracy(k=6)\n+        result = a_obj(y_true, y_pred)\n+        assert 0.5 == K.eval(result)  # only 1 sample matches.\n+\n+    def test_weighted(self):\n+        a_obj = metrics.SparseTopKCategoricalAccuracy(k=2)\n+        y_true = [1, 0, 2]\n+        y_pred = [[0, 0.9, 0.1], [0, 0.9, 0.1], [0, 0.9, 0.1]]\n+        sample_weight = (1.0, 0.0, 1.0)\n+        result = a_obj(y_true, y_pred, sample_weight=sample_weight)\n+        assert np.allclose(1.0, K.eval(result), atol=1e-5)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
