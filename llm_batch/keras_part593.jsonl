{"custom_id": "keras#d360a45139e19a0436e5c4ce7f4ce5c1965593fb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 8792 | Contributors (this commit): 16 | Commits (past 90d): 3 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -1651,7 +1651,7 @@ class RandomWidth(base_layer.BaseRandomLayer):\n     Args:\n         factor: A positive float (fraction of original width),\n             or a tuple of size 2 representing lower and upper bound\n-            for resizing vertically. When represented as a single float,\n+            for resizing horizontally. When represented as a single float,\n             this value is used for both the upper and\n             lower bound. For instance, `factor=(0.2, 0.3)`\n             results in an output with\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5d81faef4de70fc9f9ee93e201a1e27cb444a129", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 34 | Churn Cumulative: 331 | Contributors (this commit): 4 | Commits (past 90d): 20 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -7,7 +7,7 @@ from keras_core.backend.config import floatx\n \n \n def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n-    \"\"\"Produce random number based on the normal distribution.\n+    \"\"\"Draw random samples from a normal (Gaussian) distribution.\n \n     Args:\n         shape: The shape of the random values to generate.\n@@ -34,7 +34,14 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n \n \n def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n-    \"\"\"Produce random number based on the uniform distribution.\n+    \"\"\"Draw samples from a uniform distribution.\n+\n+    The generated values follow a uniform distribution in the range\n+    `[minval, maxval)`. The lower bound `minval` is included in the range,\n+    while the upper bound `maxval` is excluded.\n+\n+    For floats, the default range is `[0, 1)`.  For ints, at least `maxval`\n+    must be specified explicitly.\n \n     Args:\n         shape: The shape of the random values to generate.\n@@ -63,7 +70,11 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n \n \n def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n-    \"\"\"Produce random number based on the truncated normal distribution.\n+    \"\"\"Draw samples from a truncated normal distribution.\n+\n+    The values are drawn from a normal distribution with specified mean and\n+    standard deviation, discarding and re-drawing any samples that are more\n+    than two standard deviations from the mean.\n \n     Args:\n         shape: The shape of the random values to generate.\n\n@@ -12,7 +12,7 @@ def tf_draw_seed(seed):\n \n \n def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n-    \"\"\"Produce random number based on the normal distribution.\n+    \"\"\"Draw random samples from a normal (Gaussian) distribution.\n \n     Args:\n         shape: The shape of the random values to generate.\n@@ -40,7 +40,14 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n \n \n def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n-    \"\"\"Produce random number based on the uniform distribution.\n+    \"\"\"Draw samples from a uniform distribution.\n+\n+    The generated values follow a uniform distribution in the range\n+    `[minval, maxval)`. The lower bound `minval` is included in the range,\n+    while the upper bound `maxval` is excluded.\n+\n+    For floats, the default range is `[0, 1)`.  For ints, at least `maxval`\n+    must be specified explicitly.\n \n     Args:\n         shape: The shape of the random values to generate.\n@@ -73,7 +80,11 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n \n \n def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n-    \"\"\"Produce random number based on the truncated normal distribution.\n+    \"\"\"Draw samples from a truncated normal distribution.\n+\n+    The values are drawn from a normal distribution with specified mean and\n+    standard deviation, discarding and re-drawing any samples that are more\n+    than two standard deviations from the mean.\n \n     Args:\n         shape: The shape of the random values to generate.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ca0b77d3456c10de4bb7e87e9fe76ebbfde3633f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 206 | Lines Deleted: 17 | Files Changed: 5 | Hunks: 16 | Methods Changed: 14 | Complexity Δ (Sum/Max): 10/6 | Churn Δ: 223 | Churn Cumulative: 720 | Contributors (this commit): 9 | Commits (past 90d): 61 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n from keras_core.layers.activations.activation import Activation\n+from keras_core.layers.attention.additive_attention import AdditiveAttention\n from keras_core.layers.attention.attention import Attention\n from keras_core.layers.convolutional.conv1d import Conv1D\n from keras_core.layers.convolutional.conv1d_transpose import Conv1DTranspose\n\n@@ -0,0 +1,103 @@\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.attention.attention import Attention\n+\n+\n+@keras_core_export(\"keras_core.layers.AdditiveAttention\")\n+class AdditiveAttention(Attention):\n+    \"\"\"Additive attention layer, a.k.a. Bahdanau-style attention.\n+\n+    Inputs are a list with 2 or 3 elements:\n+    1. A `query` tensor of shape `(batch_size, Tq, dim)`.\n+    2. A `value` tensor of shape `(batch_size, Tv, dim)`.\n+    3. A optional `key` tensor of shape `(batch_size, Tv, dim)`. If none\n+        supplied, `value` will be used as `key`.\n+\n+    The calculation follows the steps:\n+    1. Calculate attention scores using `query` and `key` with shape\n+        `(batch_size, Tq, Tv)` as a non-linear sum\n+        `scores = reduce_sum(tanh(query + key), axis=-1)`.\n+    2. Use scores to calculate a softmax distribution with shape\n+        `(batch_size, Tq, Tv)`.\n+    3. Use the softmax distribution to create a linear combination of `value`\n+        with shape `(batch_size, Tq, dim)`.\n+\n+    Args:\n+        use_scale: If `True`, will create a scalar variable to scale the\n+            attention scores.\n+        dropout: Float between 0 and 1. Fraction of the units to drop for the\n+            attention scores. Defaults to 0.0.\n+\n+    Call Args:\n+        inputs: List of the following tensors:\n+            - `query`: Query tensor of shape `(batch_size, Tq, dim)`.\n+            - `value`: Value tensor of shape `(batch_size, Tv, dim)`.\n+            - `key`: Optional key tensor of shape `(batch_size, Tv, dim)`. If\n+                not given, will use `value` for both `key` and `value`, which is\n+                the most common case.\n+        mask: List of the following tensors:\n+            - `query_mask`: A boolean mask tensor of shape `(batch_size, Tq)`.\n+                If given, the output will be zero at the positions where\n+                `mask==False`.\n+            - `value_mask`: A boolean mask tensor of shape `(batch_size, Tv)`.\n+                If given, will apply the mask such that values at positions\n+                 where `mask==False` do not contribute to the result.\n+        return_attention_scores: bool, it `True`, returns the attention scores\n+            (after masking and softmax) as an additional output argument.\n+        training: Python boolean indicating whether the layer should behave in\n+            training mode (adding dropout) or in inference mode (no dropout).\n+        use_causal_mask: Boolean. Set to `True` for decoder self-attention. Adds\n+            a mask such that position `i` cannot attend to positions `j > i`.\n+            This prevents the flow of information from the future towards the\n+            past. Defaults to `False`.\n+\n+    Output:\n+        Attention outputs of shape `(batch_size, Tq, dim)`.\n+        (Optional) Attention scores after masking and softmax with shape\n+            `(batch_size, Tq, Tv)`.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        use_scale=True,\n+        dropout=0.0,\n+        **kwargs,\n+    ):\n+        super().__init__(use_scale=use_scale, dropout=dropout, **kwargs)\n+\n+    def build(self, input_shape):\n+        self._validate_inputs(input_shape)\n+        dim = input_shape[0][-1]\n+        self.scale = None\n+        if self.use_scale:\n+            self.scale = self.add_weight(\n+                name=\"scale\",\n+                shape=[dim],\n+                initializer=\"glorot_uniform\",\n+                dtype=self.dtype,\n+                trainable=True,\n+            )\n+        self.built = True\n+\n+    def _calculate_scores(self, query, key):\n+        \"\"\"Calculates attention scores as a nonlinear sum of query and key.\n+\n+        Args:\n+            query: Query tensor of shape `(batch_size, Tq, dim)`.\n+            key: Key tensor of shape `(batch_size, Tv, dim)`.\n+\n+        Returns:\n+            Tensor of shape `(batch_size, Tq, Tv)`.\n+        \"\"\"\n+        # Reshape tensors to enable broadcasting.\n+        # Reshape into [batch_size, Tq, 1, dim].\n+        q_reshaped = ops.expand_dims(query, axis=-2)\n+        # Reshape into [batch_size, 1, Tv, dim].\n+        k_reshaped = ops.expand_dims(key, axis=-3)\n+        scale = self.scale if self.use_scale else 1.0\n+        return ops.sum(scale * ops.tanh(q_reshaped + k_reshaped), axis=-1)\n+\n+    def get_config(self):\n+        base_config = super().get_config()\n+        del base_config[\"score_mode\"]\n+        return base_config\n\n@@ -0,0 +1,84 @@\n+import numpy as np\n+\n+from keras_core import layers\n+from keras_core import testing\n+\n+\n+class AdditiveAttentionTest(testing.TestCase):\n+    def test_attention_basics(self):\n+        # No scale\n+        self.run_layer_test(\n+            layers.AdditiveAttention,\n+            init_kwargs={\n+                \"use_scale\": True,\n+                \"dropout\": 0.5,\n+            },\n+            input_shape=[(2, 3, 4), (2, 4, 4)],\n+            expected_output_shape=(2, 3, 4),\n+            expected_num_trainable_weights=1,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n+        # Sale.\n+        self.run_layer_test(\n+            layers.AdditiveAttention,\n+            init_kwargs={\n+                \"use_scale\": False,\n+                \"dropout\": 0.5,\n+            },\n+            input_shape=[(2, 3, 4), (2, 4, 4)],\n+            expected_output_shape=(2, 3, 4),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n+\n+    def test_attention_correctness(self):\n+        query = np.array([[[1.0, 0.0], [0.0, 1.0]]])\n+        key = np.array([[[0.0, 1.0], [1.0, 0.0]]])\n+        value = np.array([[[1.0, 2.0], [3.0, 4.0]]])\n+\n+        layer = layers.AdditiveAttention(use_scale=False)\n+        output, scores = layer(\n+            [query, value, key],\n+            return_attention_scores=True,\n+        )\n+        self.assertAllClose(\n+            output, [[[1.727, 2.727], [2.272, 3.272]]], atol=1e-3\n+        )\n+        self.assertAllClose(\n+            scores, [[[0.636, 0.363], [0.363, 0.636]]], atol=1e-3\n+        )\n+\n+    def test_attention_with_mask(self):\n+        layer = layers.AdditiveAttention(use_scale=False)\n+        query = np.array([[[1.0, 0.0], [0.0, 1.0]]])\n+        value = np.array([[[1.0, 1.0], [1.0, 1.0]]])\n+        query_mask = np.array([[True, False]])\n+        value_mask = np.array([[True, False]])\n+        output, scores = layer(\n+            [query, value],\n+            mask=[query_mask, value_mask],\n+            return_attention_scores=True,\n+        )\n+        self.assertAllClose(output, [[[1.0, 1.0], [0.0, 0.0]]])\n+        self.assertAllClose(scores, [[[1.0, 0.0], [1.0, 0.0]]])\n+\n+    def test_attention_errors(self):\n+        layer = layers.AdditiveAttention()\n+        tensor = np.array([[[1.0, 1.0], [1.0, 1.0]]])\n+        with self.assertRaisesRegex(ValueError, \"must be called on a list\"):\n+            layer(tensor)\n+\n+        with self.assertRaisesRegex(ValueError, \"length 2 or 3\"):\n+            layer([tensor, tensor, tensor, tensor])\n+\n+        with self.assertRaisesRegex(ValueError, \"layer mask must be a list\"):\n+            layer([tensor, tensor], mask=tensor)\n+\n+        with self.assertRaisesRegex(ValueError, \"length 2 or 3\"):\n+            layer([tensor, tensor], mask=[tensor])\n\n@@ -9,17 +9,17 @@ class Attention(Layer):\n     \"\"\"Dot-product attention layer, a.k.a. Luong-style attention.\n \n     Inputs are a list with 2 or 3 elements:\n-    1. A query tensor of shape `(batch_size, Tq, dim)`.\n-    2. A value tensor of shape `(batch_size, Tv, dim)`.\n-    3. A optional key tensor of shape `(batch_size, Tv, dim)`. If none supplied,\n-        the value tensor will be used as a key.\n+    1. A `query` tensor of shape `(batch_size, Tq, dim)`.\n+    2. A `value` tensor of shape `(batch_size, Tv, dim)`.\n+    3. A optional `key` tensor of shape `(batch_size, Tv, dim)`. If none\n+        supplied, `value` will be used as a `key`.\n \n     The calculation follows the steps:\n-    1. Calculate attention scores using query and key with shape\n+    1. Calculate attention scores using `query` and `key` with shape\n         `(batch_size, Tq, Tv)`.\n     2. Use scores to calculate a softmax distribution with shape\n         `(batch_size, Tq, Tv)`.\n-    3. Use the softmax distribution to create a linear combination of value\n+    3. Use the softmax distribution to create a linear combination of `value`\n         with shape `(batch_size, Tq, dim)`.\n \n     Args:\n@@ -30,20 +30,20 @@ class Attention(Layer):\n         score_mode: Function to use to compute attention scores, one of\n             `{\"dot\", \"concat\"}`. `\"dot\"` refers to the dot product between the\n             query and key vectors. `\"concat\"` refers to the hyperbolic tangent\n-            of the concatenation of the query and key vectors.\n+            of the concatenation of the `query` and `key` vectors.\n \n     Call Args:\n         inputs: List of the following tensors:\n-            - query: Query `Tensor` of shape `(batch_size, Tq, dim)`.\n-            - value: Value `Tensor` of shape `(batch_size, Tv, dim)`.\n-            - key: Optional key `Tensor` of shape `(batch_size, Tv, dim)`. If\n+            - `query`: Query tensor of shape `(batch_size, Tq, dim)`.\n+            - `value`: Value tensor of shape `(batch_size, Tv, dim)`.\n+            - `key`: Optional key tensor of shape `(batch_size, Tv, dim)`. If\n                 not given, will use `value` for both `key` and `value`, which is\n                 the most common case.\n         mask: List of the following tensors:\n-            - query_mask: A boolean mask `Tensor` of shape `(batch_size, Tq)`.\n+            - `query_mask`: A boolean mask tensor of shape `(batch_size, Tq)`.\n                 If given, the output will be zero at the positions where\n                 `mask==False`.\n-            - value_mask: A boolean mask `Tensor` of shape `(batch_size, Tv)`.\n+            - `value_mask`: A boolean mask tensor of shape `(batch_size, Tv)`.\n                 If given, will apply the mask such that values at positions\n                  where `mask==False` do not contribute to the result.\n         return_attention_scores: bool, it `True`, returns the attention scores\n@@ -80,6 +80,7 @@ class Attention(Layer):\n             )\n \n     def build(self, input_shape):\n+        self._validate_inputs(input_shape)\n         self.scale = None\n         self.concat_score_weight = None\n         if self.use_scale:\n@@ -147,7 +148,7 @@ class Attention(Layer):\n         Args:\n             scores: Scores float tensor of shape `(batch_size, Tq, Tv)`.\n             value: Value tensor of shape `(batch_size, Tv, dim)`.\n-            scores_mask: A boolean mask `Tensor` of shape `(batch_size, 1, Tv)`\n+            scores_mask: A boolean mask tensor of shape `(batch_size, 1, Tv)`\n                 or `(batch_size, Tq, Tv)`. If given, scores at positions where\n                 `scores_mask==False` do not contribute to the result. It must\n                 contain at least one `True` value in each line along the last\n@@ -208,7 +209,7 @@ class Attention(Layer):\n         return_attention_scores=False,\n         use_causal_mask=False,\n     ):\n-        self._validate_call_args(inputs=inputs, mask=mask)\n+        self._validate_inputs(inputs=inputs, mask=mask)\n         q = inputs[0]\n         v = inputs[1]\n         k = inputs[2] if len(inputs) > 2 else v\n@@ -230,7 +231,7 @@ class Attention(Layer):\n         return result\n \n     def compute_mask(self, inputs, mask=None):\n-        self._validate_call_args(inputs=inputs, mask=mask)\n+        self._validate_inputs(inputs=inputs, mask=mask)\n         if mask is None or mask[0] is None:\n             return None\n         return ops.convert_to_tensor(mask[0])\n@@ -238,7 +239,7 @@ class Attention(Layer):\n     def compute_output_shape(self, input_shape):\n         return input_shape[0]\n \n-    def _validate_call_args(self, inputs, mask):\n+    def _validate_inputs(self, inputs, mask=None):\n         \"\"\"Validates arguments of the call method.\"\"\"\n         class_name = self.__class__.__name__\n         if not isinstance(inputs, list):\n\n@@ -4,7 +4,7 @@ from keras_core import layers\n from keras_core import testing\n \n \n-class DenseTest(testing.TestCase):\n+class AttentionTest(testing.TestCase):\n     def test_attention_basics(self):\n         # No scale, no concat.\n         self.run_layer_test(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#765e400f6609f15b963d4bb3986872d5f70bea43", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 67 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 13 | Methods Changed: 11 | Complexity Δ (Sum/Max): 7/4 | Churn Δ: 76 | Churn Cumulative: 540 | Contributors (this commit): 4 | Commits (past 90d): 20 | Contributors (cumulative): 8 | DMM Complexity: 0.9\n\nDIFF:\n@@ -64,6 +64,7 @@ class EpochIterator:\n         self.steps_per_epoch = steps_per_epoch\n         if steps_per_epoch:\n             self._current_iterator = None\n+            self._insufficient_data = False\n         if isinstance(x, data_adapter_utils.ARRAY_TYPES):\n             self.data_adapter = array_data_adapter.ArrayDataAdapter(\n                 x,\n@@ -144,7 +145,10 @@ class EpochIterator:\n         if self.steps_per_epoch:\n             if not self._current_iterator:\n                 self._current_iterator = self._get_iterator(return_type)\n+                self._insufficient_data = False\n             for step in range(self.steps_per_epoch):\n+                if self._insufficient_data:\n+                    break\n                 try:\n                     data = next(self._current_iterator)\n                     yield step, data\n@@ -158,6 +162,7 @@ class EpochIterator:\n                         stacklevel=2,\n                     )\n                     self._current_iterator = None\n+                    self._insufficient_data = True\n         else:\n             for step, data in enumerate(self._get_iterator(return_type)):\n                 yield step, data\n\n@@ -35,3 +35,25 @@ class TestEpochIterator(testing.TestCase):\n \n     def test_basic_flow_tf(self):\n         self._test_basic_flow(\"tf\")\n+\n+    def test_insufficient_data(self):\n+        batch_size = 8\n+        steps_per_epoch = 6\n+        dataset_size = batch_size * (steps_per_epoch - 2)\n+\n+        x = np.arange(dataset_size).reshape((dataset_size, 1))\n+        y = x * 2\n+\n+        iterator = epoch_iterator.EpochIterator(\n+            x=x,\n+            y=y,\n+            batch_size=batch_size,\n+            steps_per_epoch=steps_per_epoch,\n+        )\n+\n+        steps_seen = []\n+        for step, _ in iterator.enumerate_epoch():\n+            steps_seen.append(step)\n+\n+        self.assertLen(steps_seen, steps_per_epoch - 2)\n+        self.assertTrue(iterator._insufficient_data)\n\n@@ -89,12 +89,14 @@ class TestTrainer(testing.TestCase):\n         # And those weights are tracked at the model level\n         self.assertEqual(len(model.metrics_variables), 6)\n \n-    def _test_fit_flow(self, run_eagerly, jit_compile):\n+    def _test_fit_flow(self, run_eagerly, jit_compile, use_steps_per_epoch):\n         model = ExampleModel(units=3)\n-        x = np.ones((100, 4))\n-        y = np.zeros((100, 3))\n-        batch_size = 16\n         epochs = 3\n+        batch_size = 20\n+        steps_per_epoch = 7\n+        dataset_size = batch_size * (steps_per_epoch - 2)\n+        x = np.ones((dataset_size, 4))\n+        y = np.zeros((dataset_size, 3))\n \n         model.compile(\n             optimizer=optimizers.SGD(),\n@@ -103,22 +105,51 @@ class TestTrainer(testing.TestCase):\n             run_eagerly=run_eagerly,\n             jit_compile=jit_compile,\n         )\n-        history = model.fit(x, y, batch_size=batch_size, epochs=epochs)\n+        history = model.fit(\n+            x,\n+            y,\n+            batch_size=batch_size,\n+            steps_per_epoch=steps_per_epoch if use_steps_per_epoch else None,\n+            epochs=epochs,\n+        )\n         history = history.history\n         self.assertIn(\"loss\", history)\n         self.assertIn(\"mean_squared_error\", history)\n         self.assertAllClose(\n-            history[\"mean_squared_error\"], [13.938, 9.547, 6.539], atol=1e-2\n+            history[\"mean_squared_error\"],\n+            [14.402393, 10.991339, 8.388159],\n+            atol=6.1051628e-1,\n         )\n \n     def test_fit_flow_eager(self):\n-        self._test_fit_flow(run_eagerly=True, jit_compile=False)\n+        self._test_fit_flow(\n+            run_eagerly=True, jit_compile=False, use_steps_per_epoch=False\n+        )\n \n     def test_fit_flow_graph_fn(self):\n-        self._test_fit_flow(run_eagerly=False, jit_compile=False)\n+        self._test_fit_flow(\n+            run_eagerly=False, jit_compile=False, use_steps_per_epoch=False\n+        )\n \n     def test_fit_flow_jit(self):\n-        self._test_fit_flow(run_eagerly=False, jit_compile=True)\n+        self._test_fit_flow(\n+            run_eagerly=False, jit_compile=True, use_steps_per_epoch=False\n+        )\n+\n+    def test_fit_steps_per_epoch_flow_eager(self):\n+        self._test_fit_flow(\n+            run_eagerly=True, jit_compile=False, use_steps_per_epoch=True\n+        )\n+\n+    def test_fit_steps_per_epoch_flow_graph_fn(self):\n+        self._test_fit_flow(\n+            run_eagerly=False, jit_compile=False, use_steps_per_epoch=True\n+        )\n+\n+    def test_fit_steps_per_epoch_flow_jit(self):\n+        self._test_fit_flow(\n+            run_eagerly=False, jit_compile=True, use_steps_per_epoch=True\n+        )\n \n     def _test_evaluate_flow(self, run_eagerly, jit_compile):\n         model = ExampleModel(units=3)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c036417b081b211dde99840d3aaef0cdf2001ce5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 245 | Contributors (this commit): 4 | Commits (past 90d): 8 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -94,7 +94,7 @@ class EpochIterator:\n             #     \"(via `.shuffle(tf.data.AUTOTUNE)`)\"\n             # )\n         elif isinstance(x, py_dataset_adapter.PyDataset):\n-            self.data_adapter = tf_dataset_adapter.PyDatasetAdapter(\n+            self.data_adapter = py_dataset_adapter.PyDatasetAdapter(\n                 x, class_weight=class_weight, shuffle=shuffle\n             )\n             if y is not None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d975c9c082bc1b6b09a8288bc72d1521095eb399", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 112 | Lines Deleted: 33 | Files Changed: 2 | Hunks: 14 | Methods Changed: 8 | Complexity Δ (Sum/Max): 1/2 | Churn Δ: 145 | Churn Cumulative: 2206 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -84,21 +84,16 @@ class RNN(Layer):\n             This is for use with cells that use dropout.\n \n     Input shape:\n-        N-D tensor with shape `[batch_size, timesteps, ...]` or\n-        `[timesteps, batch_size, ...]` when time_major is True.\n+        3-D tensor with shape `(batch_size, timesteps, features)`.\n \n     Output shape:\n \n     - If `return_state`: a list of tensors. The first tensor is\n     the output. The remaining tensors are the last states,\n-    each with shape `[batch_size, state_size]`, where `state_size` could\n+    each with shape `(batch_size, state_size)`, where `state_size` could\n     be a high dimension tensor shape.\n-    - If `return_sequences`: N-D tensor with shape\n-    `[batch_size, timesteps, output_size]`, where `output_size` could\n-    be a high dimension tensor shape, or\n-    `[timesteps, batch_size, output_size]` when `time_major` is True.\n-    - Else, N-D tensor with shape `[batch_size, output_size]`, where\n-    `output_size` could be a high dimension tensor shape.\n+    - If `return_sequences`: 3D tensor with shape\n+    `(batch_size, timesteps, output_size)`.\n \n     Masking:\n \n@@ -247,14 +242,7 @@ class RNN(Layer):\n             self.single_state = False\n \n     def compute_output_shape(self, sequence_shape, initial_state_shape=None):\n-        if self.single_state:\n-            # Single state case.\n-            state_shape = (sequence_shape[0], self.state_size[0])\n-        else:\n-            # Multiple states case.\n         state_shape = [(sequence_shape[0], d) for d in self.state_size]\n-\n-        # There can only be one output.\n         output_size = getattr(self.cell, \"output_size\", None)\n         if output_size is None:\n             output_size = self.state_size[0]\n@@ -265,7 +253,7 @@ class RNN(Layer):\n         else:\n             output_shape = (sequence_shape[0], output_size)\n         if self.return_state:\n-            return output_shape, state_shape\n+            return output_shape, *state_shape\n         return output_shape\n \n     def compute_mask(self, _, mask):\n@@ -303,25 +291,20 @@ class RNN(Layer):\n     @tracking.no_automatic_dependency_tracking\n     def _create_state_variables(self, batch_size):\n         self.states = [\n-            self.add_variable(\n-                initializer=\"zeros\", shape=(batch_size, d), trainable=False\n-            )\n-            for d in self.state_size\n+            backend.Variable(value, trainable=False, dtype=self.variable_dtype)\n+            for value in self.get_initial_state(batch_size)\n         ]\n \n-    def get_initial_state(self, sequence):\n-        input_shape = ops.shape(sequence)\n-        batch_size = input_shape[0]\n-        dtype = sequence.dtype\n-\n+    def get_initial_state(self, batch_size):\n         get_initial_state_fn = getattr(self.cell, \"get_initial_state\", None)\n         if get_initial_state_fn:\n             init_state = get_initial_state_fn(\n-                batch_size=batch_size, dtype=dtype\n+                batch_size=batch_size, dtype=self.compute_dtype\n             )\n         else:\n             return [\n-                ops.zeros((batch_size, d), dtype=dtype) for d in self.state_size\n+                ops.zeros((batch_size, d), dtype=self.compute_dtype)\n+                for d in self.state_size\n             ]\n \n         # RNN expect the states in a list, even if single state.\n@@ -372,7 +355,9 @@ class RNN(Layer):\n             if self.stateful:\n                 initial_state = self.states\n             else:\n-                initial_state = self.get_initial_state(sequence)\n+                initial_state = self.get_initial_state(\n+                    batch_size=ops.shape(sequence)[0]\n+                )\n         else:\n             # RNN expect the states in a list, even if single state.\n             if not nest.is_nested(initial_state):\n\n@@ -73,6 +73,48 @@ class TwoStatesRNNCell(layers.Layer):\n     reason=\"Only implemented for TF for now.\",\n )\n class RNNTest(testing.TestCase):\n+    def test_basics(self):\n+        self.run_layer_test(\n+            layers.RNN,\n+            init_kwargs={\"cell\": OneStateRNNCell(5)},\n+            input_shape=(3, 2, 4),\n+            expected_output_shape=(3, 5),\n+            expected_num_trainable_weights=2,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            supports_masking=True,\n+        )\n+        self.run_layer_test(\n+            layers.RNN,\n+            init_kwargs={\"cell\": OneStateRNNCell(5), \"return_sequences\": True},\n+            input_shape=(3, 2, 4),\n+            expected_output_shape=(3, 2, 5),\n+            expected_num_trainable_weights=2,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            supports_masking=True,\n+        )\n+        self.run_layer_test(\n+            layers.RNN,\n+            init_kwargs={\"cell\": TwoStatesRNNCell(5)},\n+            input_shape=(3, 2, 4),\n+            expected_output_shape=(3, 5),\n+            expected_num_trainable_weights=3,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            supports_masking=True,\n+        )\n+        self.run_layer_test(\n+            layers.RNN,\n+            init_kwargs={\"cell\": TwoStatesRNNCell(5), \"return_sequences\": True},\n+            input_shape=(3, 2, 4),\n+            expected_output_shape=(3, 2, 5),\n+            expected_num_trainable_weights=3,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            supports_masking=True,\n+        )\n+\n     def test_compute_output_shape_single_state(self):\n         sequence = np.ones((3, 4, 5))\n         layer = layers.RNN(OneStateRNNCell(8), return_sequences=False)\n@@ -112,16 +154,68 @@ class RNNTest(testing.TestCase):\n         )\n         output_shape = layer.compute_output_shape(sequence.shape)\n         self.assertEqual(output_shape[0], (3, 8))\n-        self.assertEqual(output_shape[1][0], (3, 8))\n-        self.assertEqual(output_shape[1][1], (3, 8))\n+        self.assertEqual(output_shape[1], (3, 8))\n+        self.assertEqual(output_shape[2], (3, 8))\n \n         layer = layers.RNN(\n             TwoStatesRNNCell(8), return_sequences=True, return_state=True\n         )\n         output_shape = layer.compute_output_shape(sequence.shape)\n         self.assertEqual(output_shape[0], (3, 4, 8))\n-        self.assertEqual(output_shape[1][0], (3, 8))\n-        self.assertEqual(output_shape[1][1], (3, 8))\n+        self.assertEqual(output_shape[1], (3, 8))\n+        self.assertEqual(output_shape[2], (3, 8))\n+\n+    @pytest.mark.skipif(\n+        not backend.DYNAMIC_SHAPES_OK,\n+        reason=\"Backend does not support dynamic shapes\",\n+    )\n+    def test_dynamic_shapes(self):\n+        sequence_shape = (None, None, 3)\n+        layer = layers.RNN(OneStateRNNCell(8), return_sequences=False)\n+        output_shape = layer.compute_output_shape(sequence_shape)\n+        self.assertEqual(output_shape, (None, 8))\n+\n+        layer = layers.RNN(OneStateRNNCell(8), return_sequences=True)\n+        output_shape = layer.compute_output_shape(sequence_shape)\n+        self.assertEqual(output_shape, (None, None, 8))\n+\n+        layer = layers.RNN(\n+            OneStateRNNCell(8), return_sequences=False, return_state=True\n+        )\n+        output_shape = layer.compute_output_shape(sequence_shape)\n+        self.assertEqual(output_shape[0], (None, 8))\n+        self.assertEqual(output_shape[1], (None, 8))\n+\n+        layer = layers.RNN(\n+            OneStateRNNCell(8), return_sequences=True, return_state=True\n+        )\n+        output_shape = layer.compute_output_shape(sequence_shape)\n+        self.assertEqual(output_shape[0], (None, None, 8))\n+        self.assertEqual(output_shape[1], (None, 8))\n+\n+        layer = layers.RNN(TwoStatesRNNCell(8), return_sequences=False)\n+        output_shape = layer.compute_output_shape(sequence_shape)\n+        self.assertEqual(output_shape, (None, 8))\n+\n+        layer = layers.RNN(TwoStatesRNNCell(8), return_sequences=True)\n+        output_shape = layer.compute_output_shape(sequence_shape)\n+        self.assertEqual(output_shape, (None, None, 8))\n+\n+        layer = layers.RNN(\n+            TwoStatesRNNCell(8), return_sequences=False, return_state=True\n+        )\n+        output_shape = layer.compute_output_shape(sequence_shape)\n+        self.assertEqual(output_shape[0], (None, 8))\n+        self.assertEqual(output_shape[1], (None, 8))\n+        self.assertEqual(output_shape[2], (None, 8))\n+\n+        layer = layers.RNN(\n+            TwoStatesRNNCell(8), return_sequences=True, return_state=True\n+        )\n+        output_shape = layer.compute_output_shape(sequence_shape)\n+        self.assertEqual(output_shape[0], (None, None, 8))\n+        self.assertEqual(output_shape[1], (None, 8))\n+        self.assertEqual(output_shape[2], (None, 8))\n \n     def test_forward_pass_single_state(self):\n         sequence = np.ones((1, 2, 3))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#488a464ffa78fefcb8e82517594cf4a53802acef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 31 | Churn Cumulative: 2237 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -32,7 +32,7 @@ class RNN(Layer):\n             state. This can also be a list of integers\n             (one size per state).\n             - A `output_size` attribute, a single integer.\n-            - A `get_initial_state(batch_size=None, dtype=None)`\n+            - A `get_initial_state(batch_size=None)`\n             method that creates a tensor meant to be fed to `call()` as the\n             initial state, if the user didn't specify any initial state\n             via other means. The returned initial state should have\n\n@@ -94,6 +94,20 @@ class RNNTest(testing.TestCase):\n             expected_num_seed_generators=0,\n             supports_masking=True,\n         )\n+        self.run_layer_test(\n+            layers.RNN,\n+            init_kwargs={\n+                \"cell\": OneStateRNNCell(5),\n+                \"go_backwards\": True,\n+                \"unroll\": True,\n+            },\n+            input_shape=(3, 2, 4),\n+            expected_output_shape=(3, 5),\n+            expected_num_trainable_weights=2,\n+            expected_num_non_trainable_weights=0,\n+            expected_num_seed_generators=0,\n+            supports_masking=True,\n+        )\n         self.run_layer_test(\n             layers.RNN,\n             init_kwargs={\"cell\": TwoStatesRNNCell(5)},\n@@ -325,6 +339,21 @@ class RNNTest(testing.TestCase):\n         self.assertAllClose(np.array([[45.0, 45.0]]), state_1)\n         self.assertAllClose(np.array([[45.0, 45.0]]), state_2)\n \n+    def test_go_backwards(self):\n+        sequence = np.arange(24).reshape((2, 3, 4)).astype(\"float32\")\n+        layer = layers.RNN(OneStateRNNCell(2), go_backwards=True)\n+        layer(sequence)\n+        output = layer(sequence)\n+        self.assertAllClose(np.array([[202.0, 202.0], [538.0, 538.0]]), output)\n+\n+        layer = layers.RNN(OneStateRNNCell(2), stateful=True, return_state=True)\n+        layer(sequence)\n+        output, state = layer(sequence)\n+        self.assertAllClose(\n+            np.array([[954.0, 954.0], [3978.0, 3978.0]]), output\n+        )\n+        self.assertAllClose(np.array([[954.0, 954.0], [3978.0, 3978.0]]), state)\n+\n     def test_serialization(self):\n         layer = layers.RNN(TwoStatesRNNCell(2), return_sequences=False)\n         self.run_class_serialization_test(layer)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0d9c77de3cd8115209e40ac58428e357a15a12d1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 122 | Lines Deleted: 0 | Files Changed: 3 | Hunks: 3 | Methods Changed: 7 | Complexity Δ (Sum/Max): 10/7 | Churn Δ: 122 | Churn Cumulative: 246 | Contributors (this commit): 9 | Commits (past 90d): 58 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -73,6 +73,7 @@ from keras_core.layers.preprocessing.normalization import Normalization\n from keras_core.layers.preprocessing.random_brightness import RandomBrightness\n from keras_core.layers.preprocessing.random_contrast import RandomContrast\n from keras_core.layers.preprocessing.random_crop import RandomCrop\n+from keras_core.layers.preprocessing.random_flip import RandomFlip\n from keras_core.layers.preprocessing.random_translation import RandomTranslation\n from keras_core.layers.preprocessing.rescaling import Rescaling\n from keras_core.layers.preprocessing.resizing import Resizing\n\n@@ -0,0 +1,68 @@\n+import numpy as np\n+import tensorflow as tf\n+\n+from keras_core import backend\n+from keras_core.api_export import keras_core_export\n+from keras_core.layers.layer import Layer\n+\n+HORIZONTAL = \"horizontal\"\n+VERTICAL = \"vertical\"\n+HORIZONTAL_AND_VERTICAL = \"horizontal_and_vertical\"\n+\n+\n+@keras_core_export(\"keras_core.layers.RandomFlip\")\n+class RandomFlip(Layer):\n+    \"\"\"A preprocessing layer which randomly flips images during training.\n+\n+    This layer will flip the images horizontally and or vertically based on the\n+    `mode` attribute. During inference time, the output will be identical to\n+    input. Call the layer with `training=True` to flip the input.\n+    Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n+    of integer or floating point dtype.\n+    By default, the layer will output floats.\n+\n+    Input shape:\n+        3D (unbatched) or 4D (batched) tensor with shape:\n+        `(..., height, width, channels)`, in `\"channels_last\"` format.\n+\n+    Output shape:\n+        3D (unbatched) or 4D (batched) tensor with shape:\n+        `(..., height, width, channels)`, in `\"channels_last\"` format.\n+\n+    Args:\n+        mode: String indicating which flip mode to use. Can be `\"horizontal\"`,\n+            `\"vertical\"`, or `\"horizontal_and_vertical\"`. `\"horizontal\"` is a\n+            left-right flip and `\"vertical\"` is a top-bottom flip. Defaults to\n+            `\"horizontal_and_vertical\"`\n+        seed: Integer. Used to create a random seed.\n+        **kwargs: Base layer keyword arguments, such as\n+            `name` and `dtype`.\n+    \"\"\"\n+\n+    def __init__(\n+        self, mode=HORIZONTAL_AND_VERTICAL, seed=None, name=None, **kwargs\n+    ):\n+        super().__init__(name=name)\n+        self.seed = seed or backend.random.make_default_seed()\n+        self.layer = tf.keras.layers.RandomFlip(\n+            mode=mode,\n+            name=name,\n+            seed=self.seed,\n+            **kwargs,\n+        )\n+\n+    def call(self, inputs, training=True):\n+        if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n+            inputs = tf.convert_to_tensor(np.array(inputs))\n+        outputs = self.layer.call(inputs)\n+        if backend.backend() != \"tensorflow\":\n+            outputs = backend.convert_to_tensor(outputs)\n+        return outputs\n+\n+    def compute_output_shape(self, input_shape):\n+        return tuple(self.layer.compute_output_shape(input_shape))\n+\n+    def get_config(self):\n+        config = self.layer.get_config()\n+        config.update({\"seed\": self.seed})\n+        return config\n\n@@ -0,0 +1,53 @@\n+import numpy as np\n+from absl.testing import parameterized\n+\n+from keras_core import backend\n+from keras_core import layers\n+from keras_core import testing\n+from keras_core import utils\n+\n+\n+class RandomFlipTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.named_parameters(\n+        (\"random_flip_horizontal\", \"horizontal\"),\n+        (\"random_flip_vertical\", \"vertical\"),\n+        (\"random_flip_both\", \"horizontal_and_vertical\"),\n+    )\n+    def test_random_flip(self, mode):\n+        self.run_layer_test(\n+            layers.RandomFlip,\n+            init_kwargs={\n+                \"mode\": mode,\n+            },\n+            input_shape=(2, 3, 4),\n+            expected_output_shape=(2, 3, 4),\n+            supports_masking=False,\n+        )\n+\n+    def test_random_flip_horizontal(self):\n+        utils.set_random_seed(0)\n+        self.run_layer_test(\n+            layers.RandomFlip,\n+            init_kwargs={\n+                \"mode\": \"horizontal\",\n+                \"seed\": 42,\n+            },\n+            input_data=np.asarray([[[2, 3, 4], [5, 6, 7]]]),\n+            expected_output=backend.convert_to_tensor([[[5, 6, 7], [2, 3, 4]]]),\n+            supports_masking=False,\n+        )\n+\n+    def test_random_flip_vertical(self):\n+        utils.set_random_seed(0)\n+        self.run_layer_test(\n+            layers.RandomFlip,\n+            init_kwargs={\n+                \"mode\": \"vertical\",\n+                \"seed\": 42,\n+            },\n+            input_data=np.asarray([[[2, 3, 4]], [[5, 6, 7]]]),\n+            expected_output=backend.convert_to_tensor(\n+                [[[5, 6, 7]], [[2, 3, 4]]]\n+            ),\n+            supports_masking=False,\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#27ffecc8e65eb2c09ef892384e3e78e8335d13df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 141 | Files Changed: 8 | Hunks: 23 | Methods Changed: 17 | Complexity Δ (Sum/Max): -9/1 | Churn Δ: 172 | Churn Cumulative: 5002 | Contributors (this commit): 11 | Commits (past 90d): 174 | Contributors (cumulative): 32 | DMM Complexity: 0.0\n\nDIFF:\n@@ -73,7 +73,6 @@ from keras_core.layers.preprocessing.normalization import Normalization\n from keras_core.layers.preprocessing.random_brightness import RandomBrightness\n from keras_core.layers.preprocessing.random_contrast import RandomContrast\n from keras_core.layers.preprocessing.random_crop import RandomCrop\n-from keras_core.layers.preprocessing.random_flip import RandomFlip\n from keras_core.layers.preprocessing.random_translation import RandomTranslation\n from keras_core.layers.preprocessing.rescaling import Rescaling\n from keras_core.layers.preprocessing.resizing import Resizing\n\n@@ -48,6 +48,7 @@ class Layer(Operation):\n         autocast=True,\n         name=None,\n     ):\n+        self._lock = False\n         super().__init__(name=name)\n         self.activity_regularizer = regularizers.get(activity_regularizer)\n \n@@ -835,17 +836,22 @@ class Layer(Operation):\n         )\n \n     def __setattr__(self, name, value):\n-        # Track Variables, Layers, Metrics\n+        # Prevent users from attaching state to the\n+        # layer before `super()` is called -- since that\n+        # state would silently not be tracked.\n+        if name != \"_lock\":\n+            self._check_super_called()\n+        # Track Variables, Layers, Metrics, SeedGenerators.\n         if hasattr(self, \"_tracker\"):\n             value = self._tracker.track(value)\n         return super().__setattr__(name, value)\n \n     def _check_super_called(self):\n-        if not hasattr(self, \"_tracker\"):\n+        if getattr(self, \"_lock\", True):\n             raise RuntimeError(\n                 f\"In layer '{self.__class__.__name__}', you forgot to call \"\n-                \"`super().__init__()` in the `__init__()` method. \"\n-                \"Go add it!\"\n+                \"`super().__init__()` as the first statement \"\n+                \"in the `__init__()` method. Go add it!\"\n             )\n \n     def _assert_input_compatibility(self, arg_0):\n\n@@ -1,68 +0,0 @@\n-import numpy as np\n-import tensorflow as tf\n-\n-from keras_core import backend\n-from keras_core.api_export import keras_core_export\n-from keras_core.layers.layer import Layer\n-\n-HORIZONTAL = \"horizontal\"\n-VERTICAL = \"vertical\"\n-HORIZONTAL_AND_VERTICAL = \"horizontal_and_vertical\"\n-\n-\n-@keras_core_export(\"keras_core.layers.RandomFlip\")\n-class RandomFlip(Layer):\n-    \"\"\"A preprocessing layer which randomly flips images during training.\n-\n-    This layer will flip the images horizontally and or vertically based on the\n-    `mode` attribute. During inference time, the output will be identical to\n-    input. Call the layer with `training=True` to flip the input.\n-    Input pixel values can be of any range (e.g. `[0., 1.)` or `[0, 255]`) and\n-    of integer or floating point dtype.\n-    By default, the layer will output floats.\n-\n-    Input shape:\n-        3D (unbatched) or 4D (batched) tensor with shape:\n-        `(..., height, width, channels)`, in `\"channels_last\"` format.\n-\n-    Output shape:\n-        3D (unbatched) or 4D (batched) tensor with shape:\n-        `(..., height, width, channels)`, in `\"channels_last\"` format.\n-\n-    Args:\n-        mode: String indicating which flip mode to use. Can be `\"horizontal\"`,\n-            `\"vertical\"`, or `\"horizontal_and_vertical\"`. `\"horizontal\"` is a\n-            left-right flip and `\"vertical\"` is a top-bottom flip. Defaults to\n-            `\"horizontal_and_vertical\"`\n-        seed: Integer. Used to create a random seed.\n-        **kwargs: Base layer keyword arguments, such as\n-            `name` and `dtype`.\n-    \"\"\"\n-\n-    def __init__(\n-        self, mode=HORIZONTAL_AND_VERTICAL, seed=None, name=None, **kwargs\n-    ):\n-        super().__init__(name=name)\n-        self.seed = seed or backend.random.make_default_seed()\n-        self.layer = tf.keras.layers.RandomFlip(\n-            mode=mode,\n-            name=name,\n-            seed=self.seed,\n-            **kwargs,\n-        )\n-\n-    def call(self, inputs, training=True):\n-        if not isinstance(inputs, (tf.Tensor, np.ndarray, list, tuple)):\n-            inputs = tf.convert_to_tensor(np.array(inputs))\n-        outputs = self.layer.call(inputs)\n-        if backend.backend() != \"tensorflow\":\n-            outputs = backend.convert_to_tensor(outputs)\n-        return outputs\n-\n-    def compute_output_shape(self, input_shape):\n-        return tuple(self.layer.compute_output_shape(input_shape))\n-\n-    def get_config(self):\n-        config = self.layer.get_config()\n-        config.update({\"seed\": self.seed})\n-        return config\n\n@@ -1,53 +0,0 @@\n-import numpy as np\n-from absl.testing import parameterized\n-\n-from keras_core import backend\n-from keras_core import layers\n-from keras_core import testing\n-from keras_core import utils\n-\n-\n-class RandomFlipTest(testing.TestCase, parameterized.TestCase):\n-    @parameterized.named_parameters(\n-        (\"random_flip_horizontal\", \"horizontal\"),\n-        (\"random_flip_vertical\", \"vertical\"),\n-        (\"random_flip_both\", \"horizontal_and_vertical\"),\n-    )\n-    def test_random_flip(self, mode):\n-        self.run_layer_test(\n-            layers.RandomFlip,\n-            init_kwargs={\n-                \"mode\": mode,\n-            },\n-            input_shape=(2, 3, 4),\n-            expected_output_shape=(2, 3, 4),\n-            supports_masking=False,\n-        )\n-\n-    def test_random_flip_horizontal(self):\n-        utils.set_random_seed(0)\n-        self.run_layer_test(\n-            layers.RandomFlip,\n-            init_kwargs={\n-                \"mode\": \"horizontal\",\n-                \"seed\": 42,\n-            },\n-            input_data=np.asarray([[[2, 3, 4], [5, 6, 7]]]),\n-            expected_output=backend.convert_to_tensor([[[5, 6, 7], [2, 3, 4]]]),\n-            supports_masking=False,\n-        )\n-\n-    def test_random_flip_vertical(self):\n-        utils.set_random_seed(0)\n-        self.run_layer_test(\n-            layers.RandomFlip,\n-            init_kwargs={\n-                \"mode\": \"vertical\",\n-                \"seed\": 42,\n-            },\n-            input_data=np.asarray([[[2, 3, 4]], [[5, 6, 7]]]),\n-            expected_output=backend.convert_to_tensor(\n-                [[[5, 6, 7]], [[2, 3, 4]]]\n-            ),\n-            supports_masking=False,\n-        )\n\n@@ -4,14 +4,11 @@ from keras_core import backend\n from keras_core import operations as ops\n from keras_core.api_export import keras_core_export\n from keras_core.layers.layer import Layer\n+from keras_core.layers.rnn.stacked_rnn_cells import StackedRNNCells\n from keras_core.saving import serialization_lib\n from keras_core.utils import tracking\n \n \n-class StackedRNNCells:\n-    pass\n-\n-\n class DropoutRNNCellMixin:\n     pass\n \n@@ -147,9 +144,9 @@ class RNN(Layer):\n     class MinimalRNNCell(keras_core.layers.Layer):\n \n         def __init__(self, units, **kwargs):\n+            super().__init__(**kwargs)\n             self.units = units\n             self.state_size = units\n-            super(MinimalRNNCell, self).__init__(**kwargs)\n \n         def build(self, input_shape):\n             self.kernel = self.add_weight(shape=(input_shape[-1], self.units),\n@@ -210,9 +207,10 @@ class RNN(Layer):\n         # If True, the output for masked timestep will be zeros, whereas in the\n         # False case, output from previous timestep is returned for masked\n         # timestep.\n-        self.zero_output_for_mask = kwargs.pop(\"zero_output_for_mask\", False)\n-\n+        zero_output_for_mask = kwargs.pop(\"zero_output_for_mask\", False)\n         super().__init__(**kwargs)\n+\n+        self.zero_output_for_mask = zero_output_for_mask\n         self.cell = cell\n         self.return_sequences = return_sequences\n         self.return_state = return_state\n@@ -298,9 +296,7 @@ class RNN(Layer):\n     def get_initial_state(self, batch_size):\n         get_initial_state_fn = getattr(self.cell, \"get_initial_state\", None)\n         if get_initial_state_fn:\n-            init_state = get_initial_state_fn(\n-                batch_size=batch_size, dtype=self.compute_dtype\n-            )\n+            init_state = get_initial_state_fn(batch_size=batch_size)\n         else:\n             return [\n                 ops.zeros((batch_size, d), dtype=self.compute_dtype)\n@@ -364,6 +360,13 @@ class RNN(Layer):\n                 initial_state = [initial_state]\n             initial_state = list(initial_state)\n \n+        # Cast states to compute dtype.\n+        # Note that states may be deeply nested\n+        # (e.g. in the stacked cells case).\n+        initial_state = nest.map_structure(\n+            lambda x: ops.cast(x, dtype=self.compute_dtype), initial_state\n+        )\n+\n         def step(inputs, states):\n             output, new_states = self.cell(inputs, states, **cell_kwargs)\n             if not nest.is_nested(new_states):\n@@ -405,9 +408,9 @@ class RNN(Layer):\n         if isinstance(cell, DropoutRNNCellMixin):\n             cell.reset_dropout_mask()\n             cell.reset_recurrent_dropout_mask()\n-        if isinstance(self.cell, StackedRNNCells):\n-            for cell in self.cell.cells:\n-                self._maybe_reset_cell_dropout_mask(cell)\n+        if isinstance(cell, StackedRNNCells):\n+            for c in cell.cells:\n+                self._maybe_reset_cell_dropout_mask(c)\n \n     def get_config(self):\n         config = {\n\n@@ -9,9 +9,9 @@ from keras_core import testing\n \n class OneStateRNNCell(layers.Layer):\n     def __init__(self, units, **kwargs):\n+        super().__init__(**kwargs)\n         self.units = units\n         self.state_size = units\n-        super().__init__(**kwargs)\n \n     def build(self, input_shape):\n         self.kernel = self.add_weight(\n@@ -35,10 +35,10 @@ class OneStateRNNCell(layers.Layer):\n \n class TwoStatesRNNCell(layers.Layer):\n     def __init__(self, units, **kwargs):\n+        super().__init__(**kwargs)\n         self.units = units\n         self.state_size = [units, units]\n         self.output_size = units\n-        super().__init__(**kwargs)\n \n     def build(self, input_shape):\n         self.kernel = self.add_weight(\n\n@@ -99,6 +99,7 @@ class Operation:\n         except TypeError:\n             auto_config = False\n         try:\n+            instance._lock = False\n             if auto_config:\n                 from keras_core.saving import serialization_lib\n \n@@ -107,6 +108,7 @@ class Operation:\n                 )\n             else:\n                 instance._auto_config = None\n+            instance._lock = True\n         except RecursionError:\n             # Setting an instance attribute in __new__ has the potential\n             # to trigger an infinite recursion if a subclass overrides\n\n@@ -12,6 +12,7 @@ from keras_core.utils import tracking\n \n class Trainer:\n     def __init__(self):\n+        self._lock = False\n         self._run_eagerly = False\n         self._jit_compile = True\n         self.compiled = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
