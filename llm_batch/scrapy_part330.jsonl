{"custom_id": "scrapy#002513438204eea5062b5a1d75fb4f261880da4f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 9 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 1463 | Contributors (this commit): 22 | Commits (past 90d): 2 | Contributors (cumulative): 22 | DMM Complexity: None\n\nDIFF:\n@@ -765,6 +765,7 @@ class MySpider(scrapy.Spider):\n         self.assertIn(\"error: Please use only one of -o/--output and -O/--overwrite-output\", log)\n \n \n+@skipIf(platform.system() != 'Windows', \"Windows required for .pyw files\")\n class WindowsRunSpiderCommandTest(RunSpiderCommandTest):\n \n     spider_filename = 'myspider.pyw'\n@@ -777,35 +778,27 @@ class WindowsRunSpiderCommandTest(RunSpiderCommandTest):\n         self.assertIn(\"start_requests\", log)\n         self.assertIn(\"badspider.pyw\", log)\n \n-    @skipIf(platform.system() != 'Windows', \"Windows required for .pyw files\")\n     def test_run_good_spider(self):\n         super().test_run_good_spider()\n \n-    @skipIf(platform.system() != 'Windows', \"Windows required for .pyw files\")\n     def test_runspider(self):\n         super().test_runspider()\n \n-    @skipIf(platform.system() != 'Windows', \"Windows required for .pyw files\")\n     def test_runspider_dnscache_disabled(self):\n         super().test_runspider_dnscache_disabled()\n \n-    @skipIf(platform.system() != 'Windows', \"Windows required for .pyw files\")\n     def test_runspider_log_level(self):\n         super().test_runspider_log_level()\n \n-    @skipIf(platform.system() != 'Windows', \"Windows required for .pyw files\")\n     def test_runspider_log_short_names(self):\n         super().test_runspider_log_short_names()\n \n-    @skipIf(platform.system() != 'Windows', \"Windows required for .pyw files\")\n     def test_runspider_no_spider_found(self):\n         super().test_runspider_no_spider_found()\n \n-    @skipIf(platform.system() != 'Windows', \"Windows required for .pyw files\")\n     def test_output(self):\n         super().test_output()\n \n-    @skipIf(platform.system() != 'Windows', \"Windows required for .pyw files\")\n     def test_overwrite_output(self):\n         super().test_overwrite_output()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9c4bfb48362f736fce81b71a6ca1fa0b3600231d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 921 | Contributors (this commit): 15 | Commits (past 90d): 6 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -7,7 +7,6 @@ import warnings\n from unittest import skipIf\n \n from pytest import raises, mark\n-from testfixtures import LogCapture\n from twisted import version as twisted_version\n from twisted.internet import defer\n from twisted.python.versions import Version\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d4565318c7061c2ccd17fa5d5eabcacef8c34826", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 955 | Contributors (this commit): 15 | Commits (past 90d): 9 | Contributors (cumulative): 16 | DMM Complexity: None\n\nDIFF:\n@@ -2,10 +2,10 @@ import scrapy\n from scrapy.crawler import CrawlerProcess\n \n \n-class PollReactorSpider(scrapy.Spider):\n-    name = 'poll_reactor'\n+class SelectReactorSpider(scrapy.Spider):\n+    name = 'select_reactor'\n     custom_settings = {\n-        \"TWISTED_REACTOR\": \"twisted.internet.pollreactor.PollReactor\",\n+        \"TWISTED_REACTOR\": \"twisted.internet.selectreactor.SelectReactor\",\n     }\n \n \n@@ -17,6 +17,6 @@ class AsyncioReactorSpider(scrapy.Spider):\n \n \n process = CrawlerProcess()\n-process.crawl(PollReactorSpider)\n+process.crawl(SelectReactorSpider)\n process.crawl(AsyncioReactorSpider)\n process.start()\n\n@@ -381,8 +381,8 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n             \"the asyncio reactor is broken on Windows when running Python ≥ 3.8\")\n     def test_reactor_asyncio_custom_settings_conflict(self):\n         log = self.run_script(\"twisted_reactor_custom_settings_conflict.py\")\n-        self.assertIn(\"Using reactor: twisted.internet.pollreactor.PollReactor\", log)\n-        self.assertIn(\"(twisted.internet.pollreactor.PollReactor) does not match the requested one\", log)\n+        self.assertIn(\"Using reactor: twisted.internet.selectreactor.SelectReactor\", log)\n+        self.assertIn(\"(twisted.internet.selectreactor.SelectReactor) does not match the requested one\", log)\n \n     @mark.skipif(sys.implementation.name == 'pypy', reason='uvloop does not support pypy properly')\n     @mark.skipif(platform.system() == 'Windows', reason='uvloop does not support Windows')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7380888cad2c255e6f4a7efb6fe4cfd1240fb42c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 444 | Contributors (this commit): 14 | Commits (past 90d): 1 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -164,7 +164,7 @@ def feed_process_params_from_cli(settings, output, output_format=None,\n             message = (\n                 'The -t command line option is deprecated in favor of '\n                 'specifying the output format within the output URI. See the '\n-                'documentation of the -o and -O options for more information.',\n+                'documentation of the -o and -O options for more information.'\n             )\n             warnings.warn(message, ScrapyDeprecationWarning, stacklevel=2)\n             return {output[0]: {'format': output_format}}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#64261d9e389737621caa85f320cf81ef2aef1faa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 1836 | Contributors (this commit): 28 | Commits (past 90d): 4 | Contributors (cumulative): 28 | DMM Complexity: 0.0\n\nDIFF:\n@@ -71,17 +71,18 @@ class Crawler:\n         lf_cls = load_object(self.settings['LOG_FORMATTER'])\n         self.logformatter = lf_cls.from_crawler(self)\n \n+        reactor_class = self.settings.get(\"TWISTED_REACTOR\")\n         if init_reactor:\n             # this needs to be done after the spider settings are merged,\n             # but before something imports twisted.internet.reactor\n-            if self.settings.get(\"TWISTED_REACTOR\"):\n-                install_reactor(self.settings[\"TWISTED_REACTOR\"], self.settings[\"ASYNCIO_EVENT_LOOP\"])\n+            if reactor_class:\n+                install_reactor(reactor_class, self.settings[\"ASYNCIO_EVENT_LOOP\"])\n             else:\n                 from twisted.internet import default\n                 default.install()\n             log_reactor_info()\n-        if self.settings.get(\"TWISTED_REACTOR\"):\n-            verify_installed_reactor(self.settings[\"TWISTED_REACTOR\"])\n+        if reactor_class:\n+            verify_installed_reactor(reactor_class)\n \n         self.extensions = ExtensionManager.from_crawler(self)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e4bdd1cb958b7d89b86ea66f0af1cec2d91a6d44", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 467 | Contributors (this commit): 14 | Commits (past 90d): 1 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,7 +3,7 @@ from time import time\n from urllib.parse import urlparse, urlunparse, urldefrag\n \n from twisted.web.http import HTTPClient\n-from twisted.internet import defer, reactor\n+from twisted.internet import defer\n from twisted.internet.protocol import ClientFactory\n \n from scrapy.http import Headers\n@@ -170,6 +170,7 @@ class ScrapyHTTPClientFactory(ClientFactory):\n         p.followRedirect = self.followRedirect\n         p.afterFoundGet = self.afterFoundGet\n         if self.timeout:\n+            from twisted.internet import reactor\n             timeoutCall = reactor.callLater(self.timeout, p.timeout)\n             self.deferred.addBoth(self._cancelTimeout, timeoutCall)\n         return p\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a2763c608d33a9254034d650457b7efa7b434ec0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 222 | Contributors (this commit): 11 | Commits (past 90d): 2 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -9,7 +9,7 @@ from scrapy import Spider\n from scrapy.exceptions import NotConfigured\n from scrapy.settings import Settings\n from scrapy.utils.misc import create_instance, load_object\n-from scrapy.utils.defer import process_parallel, process_chain, process_chain_both\n+from scrapy.utils.defer import process_parallel, process_chain\n \n logger = logging.getLogger(__name__)\n \n@@ -72,11 +72,6 @@ class MiddlewareManager:\n         methods = cast(Iterable[Callable], self.methods[methodname])\n         return process_chain(methods, obj, *args)\n \n-    def _process_chain_both(self, cb_methodname: str, eb_methodname: str, obj, *args) -> Deferred:\n-        cb_methods = cast(Iterable[Callable], self.methods[cb_methodname])\n-        eb_methods = cast(Iterable[Callable], self.methods[eb_methodname])\n-        return process_chain_both(cb_methods, eb_methods, obj, *args)\n-\n     def open_spider(self, spider: Spider) -> Deferred:\n         return self._process_parallel('open_spider', spider)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c5ab58056c29c2c35b183572b0780acfbb15dfe8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 28 | Lines Deleted: 28 | Files Changed: 6 | Hunks: 20 | Methods Changed: 5 | Complexity Δ (Sum/Max): 10/6 | Churn Δ: 56 | Churn Cumulative: 4685 | Contributors (this commit): 41 | Commits (past 90d): 15 | Contributors (cumulative): 79 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n import asyncio\n+import sys\n from contextlib import suppress\n \n from twisted.internet import asyncioreactor, error\n@@ -57,6 +58,10 @@ def install_reactor(reactor_path, event_loop_path=None):\n     reactor_class = load_object(reactor_path)\n     if reactor_class is asyncioreactor.AsyncioSelectorReactor:\n         with suppress(error.ReactorAlreadyInstalledError):\n+            if sys.version_info >= (3, 8) and sys.platform == \"win32\":\n+                policy = asyncio.get_event_loop_policy()\n+                if not isinstance(policy, asyncio.WindowsSelectorEventLoopPolicy):\n+                    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n             if event_loop_path is not None:\n                 event_loop_class = load_object(event_loop_path)\n                 event_loop = event_loop_class()\n\n@@ -1,6 +1,9 @@\n import asyncio\n+import sys\n \n from twisted.internet import asyncioreactor\n+if sys.version_info >= (3, 8) and sys.platform == \"win32\":\n+    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n asyncioreactor.install(asyncio.get_event_loop())\n \n import scrapy\n\n@@ -674,9 +674,6 @@ class MySpider(scrapy.Spider):\n         self.assertIn(\"start_requests\", log)\n         self.assertIn(\"badspider.py\", log)\n \n-    # https://twistedmatrix.com/trac/ticket/9766\n-    @skipIf(platform.system() == 'Windows' and sys.version_info >= (3, 8),\n-            \"the asyncio reactor is broken on Windows when running Python ≥ 3.8\")\n     def test_asyncio_enabled_true(self):\n         log = self.get_log(self.debug_log_spider, args=[\n             '-s', 'TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor'\n@@ -699,15 +696,15 @@ class MySpider(scrapy.Spider):\n         ])\n         self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n \n-    # https://twistedmatrix.com/trac/ticket/9766\n-    @skipIf(platform.system() == 'Windows' and sys.version_info >= (3, 8),\n-            \"the asyncio reactor is broken on Windows when running Python ≥ 3.8\")\n     def test_custom_asyncio_loop_enabled_false(self):\n         log = self.get_log(self.debug_log_spider, args=[\n             '-s', 'TWISTED_REACTOR=twisted.internet.asyncioreactor.AsyncioSelectorReactor'\n         ])\n         import asyncio\n+        if sys.platform != 'win32':\n             loop = asyncio.new_event_loop()\n+        else:\n+            loop = asyncio.SelectorEventLoop()\n         self.assertIn(f\"Using asyncio event loop: {loop.__module__}.{loop.__class__.__name__}\", log)\n \n     def test_output(self):\n\n@@ -4,7 +4,6 @@ import platform\n import subprocess\n import sys\n import warnings\n-from unittest import skipIf\n \n from pytest import raises, mark\n from testfixtures import LogCapture\n@@ -284,9 +283,6 @@ class CrawlerRunnerHasSpider(unittest.TestCase):\n                 })\n \n     @defer.inlineCallbacks\n-    # https://twistedmatrix.com/trac/ticket/9766\n-    @skipIf(platform.system() == 'Windows' and sys.version_info >= (3, 8),\n-            \"the asyncio reactor is broken on Windows when running Python ≥ 3.8\")\n     def test_crawler_process_asyncio_enabled_true(self):\n         with LogCapture(level=logging.DEBUG) as log:\n             if self.reactor_pytest == 'asyncio':\n@@ -328,17 +324,11 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         self.assertIn('Spider closed (finished)', log)\n         self.assertNotIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n-    # https://twistedmatrix.com/trac/ticket/9766\n-    @skipIf(platform.system() == 'Windows' and sys.version_info >= (3, 8),\n-            \"the asyncio reactor is broken on Windows when running Python ≥ 3.8\")\n     def test_asyncio_enabled_no_reactor(self):\n         log = self.run_script('asyncio_enabled_no_reactor.py')\n         self.assertIn('Spider closed (finished)', log)\n         self.assertIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n-    # https://twistedmatrix.com/trac/ticket/9766\n-    @skipIf(platform.system() == 'Windows' and sys.version_info >= (3, 8),\n-            \"the asyncio reactor is broken on Windows when running Python ≥ 3.8\")\n     def test_asyncio_enabled_reactor(self):\n         log = self.run_script('asyncio_enabled_reactor.py')\n         self.assertIn('Spider closed (finished)', log)\n@@ -377,9 +367,6 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         self.assertIn(\"Spider closed (finished)\", log)\n         self.assertIn(\"Using reactor: twisted.internet.pollreactor.PollReactor\", log)\n \n-    # https://twistedmatrix.com/trac/ticket/9766\n-    @skipIf(platform.system() == 'Windows' and sys.version_info >= (3, 8),\n-            \"the asyncio reactor is broken on Windows when running Python ≥ 3.8\")\n     def test_reactor_asyncio(self):\n         log = self.run_script(\"twisted_reactor_asyncio.py\")\n         self.assertIn(\"Spider closed (finished)\", log)\n@@ -404,9 +391,6 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         self.assertIn(\"Using asyncio event loop: uvloop.Loop\", log)\n         self.assertIn(\"async pipeline opened!\", log)\n \n-    # https://twistedmatrix.com/trac/ticket/9766\n-    @skipIf(platform.system() == 'Windows' and sys.version_info >= (3, 8),\n-            \"the asyncio reactor is broken on Windows when running Python ≥ 3.8\")\n     def test_default_loop_asyncio_deferred_signal(self):\n         log = self.run_script(\"asyncio_deferred_signal.py\")\n         self.assertIn(\"Spider closed (finished)\", log)\n\n@@ -1,6 +1,7 @@\n import contextlib\n import os\n import shutil\n+import sys\n import tempfile\n from typing import Optional, Type\n from unittest import mock\n@@ -287,6 +288,12 @@ class HttpTestCase(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_timeout_download_from_spider_nodata_rcvd(self):\n+        if self.reactor_pytest == \"asyncio\" and sys.platform == \"win32\":\n+            # https://twistedmatrix.com/trac/ticket/10279\n+            raise unittest.SkipTest(\n+                \"This test produces DirtyReactorAggregateError on Windows with asyncio\"\n+            )\n+\n         # client connects but no data is received\n         spider = Spider('foo')\n         meta = {'download_timeout': 0.5}\n@@ -296,6 +303,11 @@ class HttpTestCase(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_timeout_download_from_spider_server_hangs(self):\n+        if self.reactor_pytest == \"asyncio\" and sys.platform == \"win32\":\n+            # https://twistedmatrix.com/trac/ticket/10279\n+            raise unittest.SkipTest(\n+                \"This test produces DirtyReactorAggregateError on Windows with asyncio\"\n+            )\n         # client connects, server send headers and some body bytes but hangs\n         spider = Spider('foo')\n         meta = {'download_timeout': 0.5}\n@@ -1055,6 +1067,10 @@ class BaseFTPTestCase(unittest.TestCase):\n class FTPTestCase(BaseFTPTestCase):\n \n     def test_invalid_credentials(self):\n+        if self.reactor_pytest == \"asyncio\" and sys.platform == \"win32\":\n+            raise unittest.SkipTest(\n+                \"This test produces DirtyReactorAggregateError on Windows with asyncio\"\n+            )\n         from twisted.protocols.ftp import ConnectionLost\n \n         meta = dict(self.req_meta)\n\n@@ -1,6 +1,4 @@\n-import platform\n-import sys\n-from unittest import skipIf, TestCase\n+from unittest import TestCase\n \n from pytest import mark\n \n@@ -14,9 +12,6 @@ class AsyncioTest(TestCase):\n         # the result should depend only on the pytest --reactor argument\n         self.assertEqual(is_asyncio_reactor_installed(), self.reactor_pytest == 'asyncio')\n \n-    # https://twistedmatrix.com/trac/ticket/9766\n-    @skipIf(platform.system() == 'Windows' and sys.version_info >= (3, 8),\n-            \"the asyncio reactor is broken on Windows when running Python ≥ 3.8\")\n     def test_install_asyncio_reactor(self):\n         # this should do nothing\n         install_reactor(\"twisted.internet.asyncioreactor.AsyncioSelectorReactor\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f789547551ae0eb79f41c9de44525bf597a0ffa5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 451 | Lines Deleted: 212 | Files Changed: 12 | Hunks: 76 | Methods Changed: 90 | Complexity Δ (Sum/Max): 105/77 | Churn Δ: 663 | Churn Cumulative: 7010 | Contributors (this commit): 46 | Commits (past 90d): 17 | Contributors (cumulative): 107 | DMM Complexity: 0.7872340425531915\n\nDIFF:\n@@ -192,7 +192,8 @@ class Scraper:\n             spider=spider\n         )\n \n-    def handle_spider_output(self, result: Iterable, request: Request, response: Response, spider: Spider) -> Deferred:\n+    def handle_spider_output(self, result: Union[Iterable, AsyncIterable], request: Request,\n+                             response: Response, spider: Spider) -> Deferred:\n         if not result:\n             return defer_succeed(None)\n         it: Union[Generator, AsyncGenerator]\n\n@@ -3,22 +3,27 @@ Spider Middleware manager\n \n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n+import logging\n+from inspect import isasyncgenfunction\n from itertools import islice\n-from typing import Any, AsyncGenerator, AsyncIterable, Callable, Generator, Iterable, Union, cast\n+from typing import Any, AsyncGenerator, AsyncIterable, Callable, Generator, Iterable, Tuple, Union, cast\n \n-from twisted.internet.defer import Deferred\n+from twisted.internet.defer import Deferred, inlineCallbacks\n from twisted.python.failure import Failure\n \n from scrapy import Request, Spider\n from scrapy.exceptions import _InvalidOutput\n from scrapy.http import Response\n from scrapy.middleware import MiddlewareManager\n-from scrapy.utils.asyncgen import _process_iterable_universal\n+from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\n from scrapy.utils.conf import build_component_list\n-from scrapy.utils.defer import mustbe_deferred\n+from scrapy.utils.defer import mustbe_deferred, deferred_from_coro, deferred_f_from_coro_f, maybe_deferred_to_future\n from scrapy.utils.python import MutableAsyncChain, MutableChain\n \n \n+logger = logging.getLogger(__name__)\n+\n+\n ScrapeFunc = Callable[[Union[Response, Failure], Request, Spider], Any]\n \n \n@@ -30,6 +35,10 @@ class SpiderMiddlewareManager(MiddlewareManager):\n \n     component_name = 'spider middleware'\n \n+    def __init__(self, *middlewares):\n+        super().__init__(*middlewares)\n+        self.downgrade_warning_done = False\n+\n     @classmethod\n     def _get_mwlist_from_settings(cls, settings):\n         return build_component_list(settings.getwithbase('SPIDER_MIDDLEWARES'))\n@@ -40,7 +49,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             self.methods['process_spider_input'].append(mw.process_spider_input)\n         if hasattr(mw, 'process_start_requests'):\n             self.methods['process_start_requests'].appendleft(mw.process_start_requests)\n-        process_spider_output = getattr(mw, 'process_spider_output', None)\n+        process_spider_output = self._get_async_method_pair(mw, 'process_spider_output')\n         self.methods['process_spider_output'].appendleft(process_spider_output)\n         process_spider_exception = getattr(mw, 'process_spider_exception', None)\n         self.methods['process_spider_exception'].appendleft(process_spider_exception)\n@@ -64,8 +73,19 @@ class SpiderMiddlewareManager(MiddlewareManager):\n     def _evaluate_iterable(self, response: Response, spider: Spider, iterable: Union[Iterable, AsyncIterable],\n                            exception_processor_index: int, recover_to: Union[MutableChain, MutableAsyncChain]\n                            ) -> Union[Generator, AsyncGenerator]:\n-        @_process_iterable_universal\n-        async def _evaluate_async_iterable(iterable):\n+\n+        def process_sync(iterable: Iterable):\n+            try:\n+                for r in iterable:\n+                    yield r\n+            except Exception as ex:\n+                exception_result = self._process_spider_exception(response, spider, Failure(ex),\n+                                                                  exception_processor_index)\n+                if isinstance(exception_result, Failure):\n+                    raise\n+                recover_to.extend(exception_result)\n+\n+        async def process_async(iterable: AsyncIterable):\n             try:\n                 async for r in iterable:\n                     yield r\n@@ -75,7 +95,10 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 if isinstance(exception_result, Failure):\n                     raise\n                 recover_to.extend(exception_result)\n-        return _evaluate_async_iterable(iterable)\n+\n+        if isinstance(iterable, AsyncIterable):\n+            return process_async(iterable)\n+        return process_sync(iterable)\n \n     def _process_spider_exception(self, response: Response, spider: Spider, _failure: Failure,\n                                   start_index: int = 0) -> Union[Failure, MutableChain]:\n@@ -87,11 +110,22 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         for method_index, method in enumerate(method_list, start=start_index):\n             if method is None:\n                 continue\n+            method = cast(Callable, method)\n             result = method(response=response, exception=exception, spider=spider)\n             if _isiterable(result):\n                 # stop exception handling by handing control over to the\n                 # process_spider_output chain if an iterable has been returned\n-                return self._process_spider_output(response, spider, result, method_index + 1)\n+                dfd: Deferred = self._process_spider_output(response, spider, result, method_index + 1)\n+                # _process_spider_output() returns a Deferred only because of downgrading so this can be\n+                # simplified when downgrading is removed.\n+                if dfd.called:\n+                    # the result is available immediately if _process_spider_output didn't do downgrading\n+                    return dfd.result\n+                else:\n+                    # we forbid waiting here because otherwise we would need to return a deferred from\n+                    # _process_spider_exception too, which complicates the architecture\n+                    msg = f\"Async iterable returned from {method.__qualname__} cannot be downgraded\"\n+                    raise _InvalidOutput(msg)\n             elif result is None:\n                 continue\n             else:\n@@ -100,9 +134,13 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 raise _InvalidOutput(msg)\n         return _failure\n \n+    # This method cannot be made async def, as _process_spider_exception relies on the Deferred result\n+    # being available immediately which doesn't work when it's a wrapped coroutine.\n+    # It also needs @inlineCallbacks only because of downgrading so it can be removed when downgrading is removed.\n+    @inlineCallbacks\n     def _process_spider_output(self, response: Response, spider: Spider,\n                                result: Union[Iterable, AsyncIterable], start_index: int = 0\n-                               ) -> Union[MutableChain, MutableAsyncChain]:\n+                               ) -> Deferred:\n         # items in this iterable do not need to go through the process_spider_output\n         # chain, they went through it already from the process_spider_exception method\n         recovered: Union[MutableChain, MutableAsyncChain]\n@@ -112,11 +150,43 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         else:\n             recovered = MutableChain()\n \n+        # There are three cases for the middleware: def foo, async def foo, def foo + async def foo_async.\n+        # 1. def foo. Sync iterables are passed as is, async ones are downgraded.\n+        # 2. async def foo. Sync iterables are upgraded, async ones are passed as is.\n+        # 3. def foo + async def foo_async. Iterables are passed to the respective method.\n+        # Storing methods and method tuples in the same list is weird but we should be able to roll this back\n+        # when we drop this compatibility feature.\n+\n         method_list = islice(self.methods['process_spider_output'], start_index, None)\n-        for method_index, method in enumerate(method_list, start=start_index):\n-            if method is None:\n+        for method_index, method_pair in enumerate(method_list, start=start_index):\n+            if method_pair is None:\n                 continue\n+            need_upgrade = need_downgrade = False\n+            if isinstance(method_pair, tuple):\n+                # This tuple handling is only needed until _async compatibility methods are removed.\n+                method_sync, method_async = method_pair\n+                method = method_async if last_result_is_async else method_sync\n+            else:\n+                method = method_pair\n+                if not last_result_is_async and isasyncgenfunction(method):\n+                    need_upgrade = True\n+                elif last_result_is_async and not isasyncgenfunction(method):\n+                    need_downgrade = True\n             try:\n+                if need_upgrade:\n+                    # Iterable -> AsyncIterable\n+                    result = as_async_generator(result)\n+                elif need_downgrade:\n+                    if not self.downgrade_warning_done:\n+                        logger.warning(f\"Async iterable passed to {method.__qualname__} \"\n+                                       f\"was downgraded to a non-async one\")\n+                        self.downgrade_warning_done = True\n+                    assert isinstance(result, AsyncIterable)\n+                    # AsyncIterable -> Iterable\n+                    result = yield deferred_from_coro(collect_asyncgen(result))\n+                    if isinstance(recovered, AsyncIterable):\n+                        recovered_collected = yield deferred_from_coro(collect_asyncgen(recovered))\n+                        recovered = MutableChain(recovered_collected)\n                 # might fail directly if the output value is not a generator\n                 result = method(response=response, result=result, spider=spider)\n             except Exception as ex:\n@@ -130,8 +200,6 @@ class SpiderMiddlewareManager(MiddlewareManager):\n                 msg = (f\"Middleware {method.__qualname__} must return an \"\n                        f\"iterable, got {type(result)}\")\n                 raise _InvalidOutput(msg)\n-            if last_result_is_async and isinstance(result, Iterable):\n-                raise TypeError(f\"Synchronous {method.__qualname__} called with an async iterable\")\n             last_result_is_async = isinstance(result, AsyncIterable)\n \n         if last_result_is_async:\n@@ -139,7 +207,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         else:\n             return MutableChain(result, recovered)  # type: ignore[arg-type]\n \n-    def _process_callback_output(self, response: Response, spider: Spider, result: Union[Iterable, AsyncIterable]\n+    async def _process_callback_output(self, response: Response, spider: Spider, result: Union[Iterable, AsyncIterable]\n                                        ) -> Union[MutableChain, MutableAsyncChain]:\n         recovered: Union[MutableChain, MutableAsyncChain]\n         if isinstance(result, AsyncIterable):\n@@ -147,23 +215,50 @@ class SpiderMiddlewareManager(MiddlewareManager):\n         else:\n             recovered = MutableChain()\n         result = self._evaluate_iterable(response, spider, result, 0, recovered)\n-        result = self._process_spider_output(response, spider, result)\n+        result = await maybe_deferred_to_future(self._process_spider_output(response, spider, result))\n         if isinstance(result, AsyncIterable):\n             return MutableAsyncChain(result, recovered)\n         else:\n+            if isinstance(recovered, AsyncIterable):\n+                recovered_collected = await collect_asyncgen(recovered)\n+                recovered = MutableChain(recovered_collected)\n             return MutableChain(result, recovered)  # type: ignore[arg-type]\n \n     def scrape_response(self, scrape_func: ScrapeFunc, response: Response, request: Request,\n                         spider: Spider) -> Deferred:\n-        def process_callback_output(result: Union[Iterable, AsyncIterable]) -> Union[MutableChain, MutableAsyncChain]:\n-            return self._process_callback_output(response, spider, result)\n+        async def process_callback_output(result: Union[Iterable, AsyncIterable]\n+                                          ) -> Union[MutableChain, MutableAsyncChain]:\n+            return await self._process_callback_output(response, spider, result)\n \n         def process_spider_exception(_failure: Failure) -> Union[Failure, MutableChain]:\n             return self._process_spider_exception(response, spider, _failure)\n \n         dfd = mustbe_deferred(self._process_spider_input, scrape_func, response, request, spider)\n-        dfd.addCallbacks(callback=process_callback_output, errback=process_spider_exception)\n+        dfd.addCallbacks(callback=deferred_f_from_coro_f(process_callback_output), errback=process_spider_exception)\n         return dfd\n \n     def process_start_requests(self, start_requests, spider: Spider) -> Deferred:\n         return self._process_chain('process_start_requests', start_requests, spider)\n+\n+    # This method is only needed until _async compatibility methods are removed.\n+    @staticmethod\n+    def _get_async_method_pair(mw: Any, methodname: str) -> Union[None, Callable, Tuple[Callable, Callable]]:\n+        normal_method = getattr(mw, methodname, None)\n+        methodname_async = methodname + \"_async\"\n+        async_method = getattr(mw, methodname_async, None)\n+        if not async_method:\n+            return normal_method\n+        if not normal_method:\n+            logger.error(f\"Middleware {mw.__qualname__} has {methodname_async} \"\n+                         f\"without {methodname}, skipping this method.\")\n+            return None\n+        if not isasyncgenfunction(async_method):\n+            logger.error(f\"{async_method.__qualname__} is not \"\n+                         f\"an async generator function, skipping this method.\")\n+            return normal_method\n+        if isasyncgenfunction(normal_method):\n+            logger.error(f\"{normal_method.__qualname__} is an async \"\n+                         f\"generator function while {methodname_async} exists, \"\n+                         f\"skipping both methods.\")\n+            return None\n+        return normal_method, async_method\n\n@@ -1,7 +1,7 @@\n import logging\n import pprint\n from collections import defaultdict, deque\n-from typing import Callable, Deque, Dict, Optional, cast, Iterable\n+from typing import Callable, Deque, Dict, Iterable, Tuple, Union, cast\n \n from twisted.internet.defer import Deferred\n \n@@ -21,8 +21,9 @@ class MiddlewareManager:\n \n     def __init__(self, *middlewares):\n         self.middlewares = middlewares\n-        # Optional because process_spider_output and process_spider_exception can be None\n-        self.methods: Dict[str, Deque[Optional[Callable]]] = defaultdict(deque)\n+        # Only process_spider_output and process_spider_exception can be None.\n+        # Only process_spider_output can be a tuple, and only until _async compatibility methods are removed.\n+        self.methods: Dict[str, Deque[Union[None, Callable, Tuple[Callable, Callable]]]] = defaultdict(deque)\n         for mw in middlewares:\n             self._add_middleware(mw)\n \n\n@@ -7,7 +7,6 @@ See documentation in docs/topics/spider-middleware.rst\n import logging\n \n from scrapy.http import Request\n-from scrapy.utils.asyncgen import _process_iterable_universal\n \n logger = logging.getLogger(__name__)\n \n@@ -29,8 +28,28 @@ class DepthMiddleware:\n         return cls(maxdepth, crawler.stats, verbose, prio)\n \n     def process_spider_output(self, response, result, spider):\n-        def _filter(request):\n-            if isinstance(request, Request):\n+        # base case (depth=0)\n+        if 'depth' not in response.meta:\n+            response.meta['depth'] = 0\n+            if self.verbose_stats:\n+                self.stats.inc_value('request_depth_count/0', spider=spider)\n+\n+        return (r for r in result or () if self._filter(r, response, spider))\n+\n+    async def process_spider_output_async(self, response, result, spider):\n+        # base case (depth=0)\n+        if 'depth' not in response.meta:\n+            response.meta['depth'] = 0\n+            if self.verbose_stats:\n+                self.stats.inc_value('request_depth_count/0', spider=spider)\n+\n+        async for r in result or ():\n+            if self._filter(r, response, spider):\n+                yield r\n+\n+    def _filter(self, request, response, spider):\n+        if not isinstance(request, Request):\n+            return True\n         depth = response.meta['depth'] + 1\n         request.meta['depth'] = depth\n         if self.prio:\n@@ -42,23 +61,9 @@ class DepthMiddleware:\n                 extra={'spider': spider}\n             )\n             return False\n-                else:\n         if self.verbose_stats:\n             self.stats.inc_value(f'request_depth_count/{depth}',\n                                  spider=spider)\n         self.stats.max_value('request_depth_max', depth,\n                              spider=spider)\n         return True\n-\n-        @_process_iterable_universal\n-        async def process(result):\n-            # base case (depth=0)\n-            if 'depth' not in response.meta:\n-                response.meta['depth'] = 0\n-                if self.verbose_stats:\n-                    self.stats.inc_value('request_depth_count/0', spider=spider)\n-\n-            async for r in result or ():\n-                if _filter(r):\n-                    yield r\n-        return process(result)\n\n@@ -9,7 +9,6 @@ import warnings\n \n from scrapy import signals\n from scrapy.http import Request\n-from scrapy.utils.asyncgen import _process_iterable_universal\n from scrapy.utils.httpobj import urlparse_cached\n \n logger = logging.getLogger(__name__)\n@@ -27,24 +26,27 @@ class OffsiteMiddleware:\n         return o\n \n     def process_spider_output(self, response, result, spider):\n-        @_process_iterable_universal\n-        async def process(result):\n-            async for x in result:\n-                if isinstance(x, Request):\n-                    if x.dont_filter or self.should_follow(x, spider):\n-                        yield x\n-                    else:\n-                        domain = urlparse_cached(x).hostname\n+        return (r for r in result or () if self._filter(r, spider))\n+\n+    async def process_spider_output_async(self, response, result, spider):\n+        async for r in result or ():\n+            if self._filter(r, spider):\n+                yield r\n+\n+    def _filter(self, request, spider) -> bool:\n+        if not isinstance(request, Request):\n+            return True\n+        if request.dont_filter or self.should_follow(request, spider):\n+            return True\n+        domain = urlparse_cached(request).hostname\n         if domain and domain not in self.domains_seen:\n             self.domains_seen.add(domain)\n             logger.debug(\n                 \"Filtered offsite request to %(domain)r: %(request)s\",\n-                                {'domain': domain, 'request': x}, extra={'spider': spider})\n+                {'domain': domain, 'request': request}, extra={'spider': spider})\n             self.stats.inc_value('offsite/domains', spider=spider)\n         self.stats.inc_value('offsite/filtered', spider=spider)\n-                else:\n-                    yield x\n-        return process(result)\n+        return False\n \n     def should_follow(self, request, spider):\n         regex = self.host_regex\n\n@@ -11,7 +11,6 @@ from w3lib.url import safe_url_string\n from scrapy import signals\n from scrapy.exceptions import NotConfigured\n from scrapy.http import Request, Response\n-from scrapy.utils.asyncgen import _process_iterable_universal\n from scrapy.utils.misc import load_object\n from scrapy.utils.python import to_unicode\n from scrapy.utils.url import strip_url\n@@ -334,20 +333,19 @@ class RefererMiddleware:\n         return cls() if cls else self.default_policy()\n \n     def process_spider_output(self, response, result, spider):\n-        def _set_referer(r):\n+        return (self._set_referer(r, response) for r in result or ())\n+\n+    async def process_spider_output_async(self, response, result, spider):\n+        async for r in result or ():\n+            yield self._set_referer(r, response)\n+\n+    def _set_referer(self, r, response):\n         if isinstance(r, Request):\n             referrer = self.policy(response, r).referrer(response.url, r.url)\n             if referrer is not None:\n                 r.headers.setdefault('Referer', referrer)\n         return r\n \n-        @_process_iterable_universal\n-        async def process(result):\n-            async for r in result or ():\n-                yield _set_referer(r)\n-\n-        return process(result)\n-\n     def request_scheduled(self, request, spider):\n         # check redirected request to patch \"Referer\" header if necessary\n         redirected_urls = request.meta.get('redirect_urls', [])\n\n@@ -8,7 +8,6 @@ import logging\n \n from scrapy.http import Request\n from scrapy.exceptions import NotConfigured\n-from scrapy.utils.asyncgen import _process_iterable_universal\n \n logger = logging.getLogger(__name__)\n \n@@ -26,7 +25,14 @@ class UrlLengthMiddleware:\n         return cls(maxlength)\n \n     def process_spider_output(self, response, result, spider):\n-        def _filter(request):\n+        return (r for r in result or () if self._filter(r, spider))\n+\n+    async def process_spider_output_async(self, response, result, spider):\n+        async for r in result or ():\n+            if self._filter(r, spider):\n+                yield r\n+\n+    def _filter(self, request, spider):\n         if isinstance(request, Request) and len(request.url) > self.maxlength:\n             logger.info(\n                 \"Ignoring link (url length > %(maxlength)d): %(url)s \",\n@@ -35,13 +41,4 @@ class UrlLengthMiddleware:\n             )\n             spider.crawler.stats.inc_value('urllength/request_ignored_count', spider=spider)\n             return False\n-            else:\n         return True\n-\n-        @_process_iterable_universal\n-        async def process(result):\n-            async for r in result or ():\n-                if _filter(r):\n-                    yield r\n-\n-        return process(result)\n\n@@ -1,6 +1,4 @@\n-import functools\n-import inspect\n-from typing import AsyncGenerator, AsyncIterable, Callable, Generator, Iterable, Union\n+from typing import AsyncGenerator, AsyncIterable, Iterable, Union\n \n \n async def collect_asyncgen(result: AsyncIterable):\n@@ -18,46 +16,3 @@ async def as_async_generator(it: Union[Iterable, AsyncIterable]) -> AsyncGenerat\n     else:\n         for r in it:\n             yield r\n-\n-\n-# https://stackoverflow.com/a/66170760/113586\n-def _process_iterable_universal(process_async: Callable):\n-    \"\"\" Takes a function that takes an async iterable, args and kwargs. Returns\n-    a function that takes any iterable, args and kwargs.\n-\n-    Requires that process_async only awaits on the iterable and synchronous functions,\n-    so it's better to use this only in the Scrapy code itself.\n-    \"\"\"\n-\n-    # If this stops working, all internal uses can be just replaced with manually-written\n-    # process_sync functions.\n-\n-    def process_sync(iterable: Iterable, *args, **kwargs) -> Generator:\n-        agen = process_async(as_async_generator(iterable), *args, **kwargs)\n-        if not inspect.isasyncgen(agen):\n-            raise ValueError(f\"process_async returned wrong type {type(agen)}\")\n-        sent = None\n-        while True:\n-            try:\n-                gen = agen.asend(sent)\n-                gen.send(None)\n-            except StopIteration as e:\n-                sent = yield e.value\n-            except StopAsyncIteration:\n-                return\n-            else:\n-                gen.throw(RuntimeError,\n-                          f\"Synchronously-called function '{process_async.__name__}' has blocked, \"\n-                          f\"you can't use {_process_iterable_universal.__name__} with it.\")\n-\n-    @functools.wraps(process_async)\n-    def process(iterable: Union[Iterable, AsyncIterable], *args, **kwargs) -> Union[Generator, AsyncGenerator]:\n-        if isinstance(iterable, AsyncIterable):\n-            # call process_async directly\n-            return process_async(iterable, *args, **kwargs)\n-        if isinstance(iterable, Iterable):\n-            # convert process_async to process_sync\n-            return process_sync(iterable, *args, **kwargs)\n-        raise TypeError(f\"Wrong iterable type {type(iterable)}\")\n-\n-    return process\n\n@@ -10,7 +10,7 @@ import warnings\n import weakref\n from functools import partial, wraps\n from itertools import chain\n-from typing import AsyncIterable, Iterable, Union\n+from typing import AsyncGenerator, AsyncIterable, Iterable, Union\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.asyncgen import as_async_generator\n@@ -359,7 +359,7 @@ class MutableChain(Iterable):\n         return self.__next__()\n \n \n-async def _async_chain(*iterables: Union[Iterable, AsyncIterable]):\n+async def _async_chain(*iterables: Union[Iterable, AsyncIterable]) -> AsyncGenerator:\n     for it in iterables:\n         async for o in as_async_generator(it):\n             yield o\n\n@@ -1,6 +1,8 @@\n import collections.abc\n+from typing import Optional\n from unittest import mock\n \n+from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n from twisted.python.failure import Failure\n@@ -8,8 +10,8 @@ from twisted.python.failure import Failure\n from scrapy.spiders import Spider\n from scrapy.http import Request, Response\n from scrapy.exceptions import _InvalidOutput\n-from scrapy.utils.asyncgen import _process_iterable_universal, as_async_generator, collect_asyncgen\n-from scrapy.utils.defer import deferred_from_coro\n+from scrapy.utils.asyncgen import collect_asyncgen\n+from scrapy.utils.defer import deferred_from_coro, maybe_deferred_to_future\n from scrapy.utils.test import get_crawler\n from scrapy.core.spidermw import SpiderMiddlewareManager\n \n@@ -115,33 +117,40 @@ class BaseAsyncSpiderMiddlewareTestCase(SpiderMiddlewareTestCase):\n \n     RESULT_COUNT = 3  # to simplify checks, let everything return 3 objects\n \n+    @staticmethod\n+    def _construct_mw_setting(*mw_classes, start_index: Optional[int] = None):\n+        if start_index is None:\n+            start_index = 10\n+        return {i: c for c, i in enumerate(mw_classes, start=start_index)}\n+\n     @defer.inlineCallbacks\n-    def _get_middleware_result(self, *mw_classes):\n-        for mw_cls in mw_classes:\n-            self.mwman._add_middleware(mw_cls())\n+    def _get_middleware_result(self, *mw_classes, start_index: Optional[int] = None):\n+        setting = self._construct_mw_setting(*mw_classes, start_index=start_index)\n+        self.crawler = get_crawler(Spider, {'SPIDER_MIDDLEWARES_BASE': {}, 'SPIDER_MIDDLEWARES': setting})\n+        self.spider = self.crawler._create_spider('foo')\n+        self.mwman = SpiderMiddlewareManager.from_crawler(self.crawler)\n         result = yield self.mwman.scrape_response(self._scrape_func, self.response, self.request, self.spider)\n         return result\n \n     @defer.inlineCallbacks\n-    def _test_simple_base(self, *mw_classes):\n-        result = yield self._get_middleware_result(*mw_classes)\n+    def _test_simple_base(self, *mw_classes, downgrade: bool = False, start_index: Optional[int] = None):\n+        with LogCapture() as log:\n+            result = yield self._get_middleware_result(*mw_classes, start_index=start_index)\n         self.assertIsInstance(result, collections.abc.Iterable)\n         result_list = list(result)\n         self.assertEqual(len(result_list), self.RESULT_COUNT)\n         self.assertIsInstance(result_list[0], self.ITEM_TYPE)\n+        self.assertEqual(\"downgraded to a non-async\" in str(log), downgrade)\n \n     @defer.inlineCallbacks\n-    def _test_asyncgen_base(self, *mw_classes):\n-        result = yield self._get_middleware_result(*mw_classes)\n+    def _test_asyncgen_base(self, *mw_classes, downgrade: bool = False, start_index: Optional[int] = None):\n+        with LogCapture() as log:\n+            result = yield self._get_middleware_result(*mw_classes, start_index=start_index)\n         self.assertIsInstance(result, collections.abc.AsyncIterator)\n         result_list = yield deferred_from_coro(collect_asyncgen(result))\n         self.assertEqual(len(result_list), self.RESULT_COUNT)\n         self.assertIsInstance(result_list[0], self.ITEM_TYPE)\n-\n-    @defer.inlineCallbacks\n-    def _test_asyncgen_fail(self, *mw_classes):\n-        with self.assertRaisesRegex(TypeError, \"Synchronous .+ called with an async iterable\"):\n-            yield self._get_middleware_result(*mw_classes)\n+        self.assertEqual(\"downgraded to a non-async\" in str(log), downgrade)\n \n \n class ProcessSpiderOutputSimpleMiddleware:\n@@ -152,17 +161,36 @@ class ProcessSpiderOutputSimpleMiddleware:\n \n class ProcessSpiderOutputAsyncGenMiddleware:\n     async def process_spider_output(self, response, result, spider):\n-        async for r in as_async_generator(result):\n+        async for r in result:\n             yield r\n \n \n class ProcessSpiderOutputUniversalMiddleware:\n     def process_spider_output(self, response, result, spider):\n-        @_process_iterable_universal\n-        async def process(result):\n+        for r in result:\n+            yield r\n+\n+    async def process_spider_output_async(self, response, result, spider):\n         async for r in result:\n             yield r\n-        return process(result)\n+\n+\n+class ProcessSpiderExceptionSimpleIterableMiddleware:\n+    def process_spider_exception(self, response, exception, spider):\n+        yield {'foo': 1}\n+        yield {'foo': 2}\n+        yield {'foo': 3}\n+\n+\n+class ProcessSpiderExceptionAsyncIterableMiddleware:\n+    async def process_spider_exception(self, response, exception, spider):\n+        yield {'foo': 1}\n+        d = defer.Deferred()\n+        from twisted.internet import reactor\n+        reactor.callLater(0, d.callback, None)\n+        await maybe_deferred_to_future(d)\n+        yield {'foo': 2}\n+        yield {'foo': 3}\n \n \n class ProcessSpiderOutputSimple(BaseAsyncSpiderMiddlewareTestCase):\n@@ -183,18 +211,19 @@ class ProcessSpiderOutputSimple(BaseAsyncSpiderMiddlewareTestCase):\n         return self._test_simple_base(self.MW_SIMPLE)\n \n     def test_asyncgen(self):\n-        \"\"\" Asyncgen mw \"\"\"\n+        \"\"\" Asyncgen mw; upgrade \"\"\"\n         return self._test_asyncgen_base(self.MW_ASYNCGEN)\n \n     def test_simple_asyncgen(self):\n-        \"\"\" Simple mw -> asyncgen mw \"\"\"\n+        \"\"\" Simple mw -> asyncgen mw; upgrade \"\"\"\n         return self._test_asyncgen_base(self.MW_ASYNCGEN,\n                                         self.MW_SIMPLE)\n \n     def test_asyncgen_simple(self):\n-        \"\"\" Asyncgen mw -> simple mw; cannot work \"\"\"\n-        return self._test_asyncgen_fail(self.MW_SIMPLE,\n-                                        self.MW_ASYNCGEN)\n+        \"\"\" Asyncgen mw -> simple mw; upgrade then downgrade \"\"\"\n+        return self._test_simple_base(self.MW_SIMPLE,\n+                                      self.MW_ASYNCGEN,\n+                                      downgrade=True)\n \n     def test_universal(self):\n         \"\"\" Universal mw \"\"\"\n@@ -211,12 +240,12 @@ class ProcessSpiderOutputSimple(BaseAsyncSpiderMiddlewareTestCase):\n                                       self.MW_SIMPLE)\n \n     def test_universal_asyncgen(self):\n-        \"\"\" Universal mw -> asyncgen mw \"\"\"\n+        \"\"\" Universal mw -> asyncgen mw; upgrade \"\"\"\n         return self._test_asyncgen_base(self.MW_ASYNCGEN,\n                                         self.MW_UNIVERSAL)\n \n     def test_asyncgen_universal(self):\n-        \"\"\" Asyncgen mw -> universal mw \"\"\"\n+        \"\"\" Asyncgen mw -> universal mw; upgrade \"\"\"\n         return self._test_asyncgen_base(self.MW_UNIVERSAL,\n                                         self.MW_ASYNCGEN)\n \n@@ -229,27 +258,31 @@ class ProcessSpiderOutputAsyncGen(ProcessSpiderOutputSimple):\n             yield item\n \n     def test_simple(self):\n-        \"\"\" Simple mw; cannot work \"\"\"\n-        return self._test_asyncgen_fail(self.MW_SIMPLE)\n+        \"\"\" Simple mw; downgrade \"\"\"\n+        return self._test_simple_base(self.MW_SIMPLE,\n+                                      downgrade=True)\n \n     def test_simple_asyncgen(self):\n-        \"\"\" Simple mw -> asyncgen mw; cannot work \"\"\"\n-        return self._test_asyncgen_fail(self.MW_ASYNCGEN,\n-                                        self.MW_SIMPLE)\n+        \"\"\" Simple mw -> asyncgen mw; downgrade then upgrade \"\"\"\n+        return self._test_asyncgen_base(self.MW_ASYNCGEN,\n+                                        self.MW_SIMPLE,\n+                                        downgrade=True)\n \n     def test_universal(self):\n         \"\"\" Universal mw \"\"\"\n         return self._test_asyncgen_base(self.MW_UNIVERSAL)\n \n     def test_universal_simple(self):\n-        \"\"\" Universal mw -> simple mw; cannot work \"\"\"\n-        return self._test_asyncgen_fail(self.MW_SIMPLE,\n-                                        self.MW_UNIVERSAL)\n+        \"\"\" Universal mw -> simple mw; downgrade \"\"\"\n+        return self._test_simple_base(self.MW_SIMPLE,\n+                                      self.MW_UNIVERSAL,\n+                                      downgrade=True)\n \n     def test_simple_universal(self):\n-        \"\"\" Simple mw -> universal mw; cannot work \"\"\"\n-        return self._test_asyncgen_fail(self.MW_UNIVERSAL,\n-                                        self.MW_SIMPLE)\n+        \"\"\" Simple mw -> universal mw; downgrade \"\"\"\n+        return self._test_simple_base(self.MW_UNIVERSAL,\n+                                      self.MW_SIMPLE,\n+                                      downgrade=True)\n \n \n class ProcessStartRequestsSimpleMiddleware:\n@@ -269,13 +302,198 @@ class ProcessStartRequestsSimple(BaseAsyncSpiderMiddlewareTestCase):\n             yield Request(f'https://example.com/{i}', dont_filter=True)\n \n     @defer.inlineCallbacks\n-    def _get_middleware_result(self, *mw_classes):\n-        for mw_cls in mw_classes:\n-            self.mwman._add_middleware(mw_cls())\n+    def _get_middleware_result(self, *mw_classes, start_index: Optional[int] = None):\n+        setting = self._construct_mw_setting(*mw_classes, start_index=start_index)\n+        self.crawler = get_crawler(Spider, {'SPIDER_MIDDLEWARES_BASE': {}, 'SPIDER_MIDDLEWARES': setting})\n+        self.spider = self.crawler._create_spider('foo')\n+        self.mwman = SpiderMiddlewareManager.from_crawler(self.crawler)\n         start_requests = iter(self._start_requests())\n         results = yield self.mwman.process_start_requests(start_requests, self.spider)\n         return results\n \n     def test_simple(self):\n         \"\"\" Simple mw \"\"\"\n-        self._test_simple_base(self.MW_SIMPLE)\n+        return self._test_simple_base(self.MW_SIMPLE)\n+\n+\n+class UniversalMiddlewareNoSync:\n+    async def process_spider_output_async(self, response, result, spider):\n+        yield\n+\n+\n+class UniversalMiddlewareBothSync:\n+    def process_spider_output(self, response, result, spider):\n+        yield\n+\n+    def process_spider_output_async(self, response, result, spider):\n+        yield\n+\n+\n+class UniversalMiddlewareBothAsync:\n+    async def process_spider_output(self, response, result, spider):\n+        yield\n+\n+    async def process_spider_output_async(self, response, result, spider):\n+        yield\n+\n+\n+class UniversalMiddlewareManagerTest(TestCase):\n+    def setUp(self):\n+        self.mwman = SpiderMiddlewareManager()\n+\n+    def test_simple_mw(self):\n+        mw = ProcessSpiderOutputSimpleMiddleware\n+        self.mwman._add_middleware(mw)\n+        self.assertEqual(self.mwman.methods['process_spider_output'][0], mw.process_spider_output)\n+\n+    def test_async_mw(self):\n+        mw = ProcessSpiderOutputAsyncGenMiddleware\n+        self.mwman._add_middleware(mw)\n+        self.assertEqual(self.mwman.methods['process_spider_output'][0], mw.process_spider_output)\n+\n+    def test_universal_mw(self):\n+        mw = ProcessSpiderOutputUniversalMiddleware\n+        self.mwman._add_middleware(mw)\n+        self.assertEqual(self.mwman.methods['process_spider_output'][0],\n+                         (mw.process_spider_output, mw.process_spider_output_async))\n+\n+    def test_universal_mw_no_sync(self):\n+        with LogCapture() as log:\n+            self.mwman._add_middleware(UniversalMiddlewareNoSync)\n+        self.assertIn(\"UniversalMiddlewareNoSync has process_spider_output_async\"\n+                      \" without process_spider_output\", str(log))\n+        self.assertEqual(self.mwman.methods['process_spider_output'][0], None)\n+\n+    def test_universal_mw_both_sync(self):\n+        mw = UniversalMiddlewareBothSync\n+        with LogCapture() as log:\n+            self.mwman._add_middleware(mw)\n+        self.assertIn(\"UniversalMiddlewareBothSync.process_spider_output_async \"\n+                      \"is not an async generator function\", str(log))\n+        self.assertEqual(self.mwman.methods['process_spider_output'][0], mw.process_spider_output)\n+\n+    def test_universal_mw_both_async(self):\n+        with LogCapture() as log:\n+            self.mwman._add_middleware(UniversalMiddlewareBothAsync)\n+        self.assertIn(\"UniversalMiddlewareBothAsync.process_spider_output \"\n+                      \"is an async generator function while process_spider_output_async exists\",\n+                      str(log))\n+        self.assertEqual(self.mwman.methods['process_spider_output'][0], None)\n+\n+\n+class BuiltinMiddlewareSimpleTest(BaseAsyncSpiderMiddlewareTestCase):\n+    ITEM_TYPE = dict\n+    MW_SIMPLE = ProcessSpiderOutputSimpleMiddleware\n+    MW_ASYNCGEN = ProcessSpiderOutputAsyncGenMiddleware\n+    MW_UNIVERSAL = ProcessSpiderOutputUniversalMiddleware\n+\n+    def _scrape_func(self, *args, **kwargs):\n+        yield {'foo': 1}\n+        yield {'foo': 2}\n+        yield {'foo': 3}\n+\n+    @defer.inlineCallbacks\n+    def _get_middleware_result(self, *mw_classes, start_index: Optional[int] = None):\n+        setting = self._construct_mw_setting(*mw_classes, start_index=start_index)\n+        self.crawler = get_crawler(Spider, {'SPIDER_MIDDLEWARES': setting})\n+        self.spider = self.crawler._create_spider('foo')\n+        self.mwman = SpiderMiddlewareManager.from_crawler(self.crawler)\n+        result = yield self.mwman.scrape_response(self._scrape_func, self.response, self.request, self.spider)\n+        return result\n+\n+    def test_just_builtin(self):\n+        return self._test_simple_base()\n+\n+    def test_builtin_simple(self):\n+        return self._test_simple_base(self.MW_SIMPLE, start_index=1000)\n+\n+    def test_builtin_async(self):\n+        \"\"\" Upgrade \"\"\"\n+        return self._test_asyncgen_base(self.MW_ASYNCGEN, start_index=1000)\n+\n+    def test_builtin_universal(self):\n+        return self._test_simple_base(self.MW_UNIVERSAL, start_index=1000)\n+\n+    def test_simple_builtin(self):\n+        return self._test_simple_base(self.MW_SIMPLE)\n+\n+    def test_async_builtin(self):\n+        \"\"\" Upgrade \"\"\"\n+        return self._test_asyncgen_base(self.MW_ASYNCGEN)\n+\n+    def test_universal_builtin(self):\n+        return self._test_simple_base(self.MW_UNIVERSAL)\n+\n+\n+class BuiltinMiddlewareAsyncGenTest(BuiltinMiddlewareSimpleTest):\n+    async def _scrape_func(self, *args, **kwargs):\n+        for item in super()._scrape_func():\n+            yield item\n+\n+    def test_just_builtin(self):\n+        return self._test_asyncgen_base()\n+\n+    def test_builtin_simple(self):\n+        \"\"\" Downgrade \"\"\"\n+        return self._test_simple_base(self.MW_SIMPLE, downgrade=True, start_index=1000)\n+\n+    def test_builtin_async(self):\n+        return self._test_asyncgen_base(self.MW_ASYNCGEN, start_index=1000)\n+\n+    def test_builtin_universal(self):\n+        return self._test_asyncgen_base(self.MW_UNIVERSAL, start_index=1000)\n+\n+    def test_simple_builtin(self):\n+        \"\"\" Downgrade \"\"\"\n+        return self._test_simple_base(self.MW_SIMPLE, downgrade=True)\n+\n+    def test_async_builtin(self):\n+        return self._test_asyncgen_base(self.MW_ASYNCGEN)\n+\n+    def test_universal_builtin(self):\n+        return self._test_asyncgen_base(self.MW_UNIVERSAL)\n+\n+\n+class ProcessSpiderExceptionTest(BaseAsyncSpiderMiddlewareTestCase):\n+    ITEM_TYPE = dict\n+    MW_SIMPLE = ProcessSpiderOutputSimpleMiddleware\n+    MW_ASYNCGEN = ProcessSpiderOutputAsyncGenMiddleware\n+    MW_UNIVERSAL = ProcessSpiderOutputUniversalMiddleware\n+    MW_EXC_SIMPLE = ProcessSpiderExceptionSimpleIterableMiddleware\n+    MW_EXC_ASYNCGEN = ProcessSpiderExceptionAsyncIterableMiddleware\n+\n+    def _scrape_func(self, *args, **kwargs):\n+        1 / 0\n+\n+    @defer.inlineCallbacks\n+    def _test_asyncgen_nodowngrade(self, *mw_classes):\n+        with self.assertRaisesRegex(_InvalidOutput, \"Async iterable returned from .+ cannot be downgraded\"):\n+            yield self._get_middleware_result(*mw_classes)\n+\n+    def test_exc_simple(self):\n+        \"\"\" Simple exc mw \"\"\"\n+        return self._test_simple_base(self.MW_EXC_SIMPLE)\n+\n+    def test_exc_async(self):\n+        \"\"\" Async exc mw \"\"\"\n+        return self._test_asyncgen_base(self.MW_EXC_ASYNCGEN)\n+\n+    def test_exc_simple_simple(self):\n+        \"\"\" Simple exc mw -> simple output mw \"\"\"\n+        return self._test_simple_base(self.MW_SIMPLE,\n+                                      self.MW_EXC_SIMPLE)\n+\n+    def test_exc_async_async(self):\n+        \"\"\" Async exc mw -> async output mw \"\"\"\n+        return self._test_asyncgen_base(self.MW_ASYNCGEN,\n+                                        self.MW_EXC_ASYNCGEN)\n+\n+    def test_exc_simple_async(self):\n+        \"\"\" Simple exc mw -> async output mw; upgrade \"\"\"\n+        return self._test_asyncgen_base(self.MW_ASYNCGEN,\n+                                        self.MW_EXC_SIMPLE)\n+\n+    def test_exc_async_simple(self):\n+        \"\"\" Async exc mw -> simple output mw; cannot work as downgrading is not supported \"\"\"\n+        return self._test_asyncgen_nodowngrade(self.MW_SIMPLE,\n+                                               self.MW_EXC_ASYNCGEN)\n\n@@ -28,6 +28,7 @@ class RecoveryMiddleware:\n class RecoverySpider(Spider):\n     name = 'RecoverySpider'\n     custom_settings = {\n+        'SPIDER_MIDDLEWARES_BASE': {},\n         'SPIDER_MIDDLEWARES': {\n             RecoveryMiddleware: 10,\n         },\n@@ -107,6 +108,13 @@ class GeneratorCallbackSpider(Spider):\n         raise ImportError()\n \n \n+class AsyncGeneratorCallbackSpider(GeneratorCallbackSpider):\n+    async def parse(self, response):\n+        yield {'test': 1}\n+        yield {'test': 2}\n+        raise ImportError()\n+\n+\n # ================================================================================\n # (2.1) exceptions from a spider callback (generator, middleware right after callback)\n class GeneratorCallbackSpiderMiddlewareRightAfterSpider(GeneratorCallbackSpider):\n@@ -360,6 +368,15 @@ class TestSpiderMiddleware(TestCase):\n         self.assertIn(\"Middleware: ImportError exception caught\", str(log2))\n         self.assertIn(\"'item_scraped_count': 2\", str(log2))\n \n+    @defer.inlineCallbacks\n+    def test_async_generator_callback(self):\n+        \"\"\"\n+        Same as test_generator_callback but with an async callback.\n+        \"\"\"\n+        log2 = yield self.crawl_log(AsyncGeneratorCallbackSpider)\n+        self.assertIn(\"Middleware: ImportError exception caught\", str(log2))\n+        self.assertIn(\"'item_scraped_count': 2\", str(log2))\n+\n     @defer.inlineCallbacks\n     def test_generator_callback_right_after_callback(self):\n         \"\"\"\n\n@@ -1,7 +1,6 @@\n-from twisted.internet.defer import Deferred\n from twisted.trial import unittest\n \n-from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen, _process_iterable_universal\n+from scrapy.utils.asyncgen import as_async_generator, collect_asyncgen\n from scrapy.utils.defer import deferred_f_from_coro_f\n \n \n@@ -19,52 +18,3 @@ class AsyncgenUtilsTest(unittest.TestCase):\n         ag = as_async_generator(range(42))\n         results = await collect_asyncgen(ag)\n         self.assertEqual(results, list(range(42)))\n-\n-\n-@_process_iterable_universal\n-async def process_iterable(iterable):\n-    async for i in iterable:\n-        yield i * 2\n-\n-\n-@_process_iterable_universal\n-async def process_iterable_awaiting(iterable):\n-    async for i in iterable:\n-        yield i * 2\n-        d = Deferred()\n-        from twisted.internet import reactor\n-        reactor.callLater(0, d.callback, 42)\n-        await d\n-\n-\n-class ProcessIterableUniversalTest(unittest.TestCase):\n-\n-    def test_normal(self):\n-        iterable = iter([1, 2, 3])\n-        results = list(process_iterable(iterable))\n-        self.assertEqual(results, [2, 4, 6])\n-\n-    @deferred_f_from_coro_f\n-    async def test_async(self):\n-        iterable = as_async_generator([1, 2, 3])\n-        results = await collect_asyncgen(process_iterable(iterable))\n-        self.assertEqual(results, [2, 4, 6])\n-\n-    @deferred_f_from_coro_f\n-    async def test_blocking(self):\n-        iterable = [1, 2, 3]\n-        with self.assertRaisesRegex(RuntimeError, \"Synchronously-called function\"):\n-            list(process_iterable_awaiting(iterable))\n-\n-    def test_invalid_iterable(self):\n-        with self.assertRaisesRegex(TypeError, \"Wrong iterable type\"):\n-            process_iterable(None)\n-\n-    @deferred_f_from_coro_f\n-    async def test_invalid_process(self):\n-        @_process_iterable_universal\n-        def process_iterable_invalid(iterable):\n-            pass\n-\n-        with self.assertRaisesRegex(ValueError, \"process_async returned wrong type\"):\n-            list(process_iterable_invalid([]))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
