{"custom_id": "keras#54ac72621f9d799a3b587be9045aa49f4284c589", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 483 | Contributors (this commit): 5 | Commits (past 90d): 4 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -48,7 +48,7 @@ class Accuracy(reduction_metrics.MeanMetricWrapper):\n \n     ```python\n     model.compile(optimizer='sgd',\n-                  loss='mse',\n+                  loss='binary_crossentropy',\n                   metrics=[keras_core.metrics.Accuracy()])\n     ```\n     \"\"\"\n@@ -108,7 +108,7 @@ class BinaryAccuracy(reduction_metrics.MeanMetricWrapper):\n \n     ```python\n     model.compile(optimizer='sgd',\n-                  loss='mse',\n+                  loss='binary_crossentropy',\n                   metrics=[keras_core.metrics.BinaryAccuracy()])\n     ```\n     \"\"\"\n@@ -194,7 +194,7 @@ class CategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n \n     ```python\n     model.compile(optimizer='sgd',\n-                  loss='mse',\n+                  loss='categorical_crossentropy',\n                   metrics=[keras_core.metrics.CategoricalAccuracy()])\n     ```\n     \"\"\"\n@@ -278,7 +278,7 @@ class SparseCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n \n     ```python\n     model.compile(optimizer='sgd',\n-                  loss='mse',\n+                  loss='sparse_categorical_crossentropy',\n                   metrics=[keras_core.metrics.SparseCategoricalAccuracy()])\n     ```\n     \"\"\"\n@@ -349,7 +349,7 @@ class TopKCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n \n     ```python\n     model.compile(optimizer='sgd',\n-                  loss='mse',\n+                  loss='categorical_crossentropy',\n                   metrics=[keras_core.metrics.TopKCategoricalAccuracy()])\n     ```\n     \"\"\"\n@@ -423,7 +423,7 @@ class SparseTopKCategoricalAccuracy(reduction_metrics.MeanMetricWrapper):\n \n     ```python\n     model.compile(optimizer='sgd',\n-                  loss='mse',\n+                  loss='sparse_categorical_crossentropy',\n                   metrics=[keras_core.metrics.SparseTopKCategoricalAccuracy()])\n     ```\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#277b2086d834b4cbad40ddc4e9b41f02b19dbae0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 183 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 9 | Complexity Δ (Sum/Max): 13/8 | Churn Δ: 184 | Churn Cumulative: 1002 | Contributors (this commit): 3 | Commits (past 90d): 12 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -289,6 +289,102 @@ class DataParallel(Distribution):\n         return TensorLayout(variable_shard_spec, self.device_mesh)\n \n \n+class ModelParallel(Distribution):\n+    \"\"\"Distribution that shard model weights.\n+\n+    Compare to DataParallel which replicates the weights across all the devices,\n+    ModelParallel allows user to shard weights in addition to the input data.\n+\n+    To construct a ModelParallel distribution, user need to provide device mesh\n+    and layout mapping.\n+\n+    1. `DeviceMesh`contains physcial device information, and the axis names in\n+        the mesh will be used to map the weight and data layout.\n+    2. `LayoutMap` contains the mapping for the variable path to its\n+        corresponding `TensorLayout`.\n+\n+    Example:\n+    ```python\n+    devices = list_devices()    # Assume there are 8 devices.\n+\n+    # Create a mesh with 2 devices on data parallel and 4 devices on weight\n+    # parallel.\n+    device_mesh = DeviceMesh(shape=(2, 4), axis_names=('batch', 'model'),\n+                             devices=devices)\n+    # Create a layout map that shard the dense layer and conv2d layer weights\n+    # on the last dimension. Based on the device_mesh, this means the weights\n+    # will be split across 4 devices. Any other weights that doesn't match for\n+    # any key in layout map will get be fully replicated.\n+    layout_map = LayoutMap(device_mesh)\n+    layout_map['.*dense.*kernel'] = TensorLayout([None, 'model'])\n+    layout_map['.*dense.*bias'] = TensorLayout(['model'])\n+    layout_map['.*conv2d.*kernel'] = TensorLayout([None, None, None, 'model'])\n+    layout_map['.*conv2d.*bias'] = TensorLayout(['model'])\n+\n+    distribution = ModelParallel(device_mesh=device_mesh,\n+                                 layout_map=layout_map,\n+                                 batch_dim_name='batch')\n+    # Set the global distribution, or via `with distribution.scope():`\n+    set_distribution(distribution)\n+\n+    model = model_creation()\n+    model.compile()\n+    model.fit(data)\n+    ```\n+\n+    User can quickly update the device mesh shape to change the sharding factor\n+    of the weights. E.g.\n+    ```\n+    # With only the shape change for the device mesh, the weights will be\n+    # sharded across 8 devices instead of 4, which further reduce the memory\n+    # footprint of weights on each of the device.\n+    device_mesh = DeviceMesh(shape=(1, 8), axis_names=('batch', 'model'),\n+                             devices=devices)\n+    ```\n+\n+    To figure out a proper layout mapping rule for all the model weights, you\n+    can first list out all the model weights path, which will be used as the key\n+    to map the weights to `TensorLayout`.\n+\n+    e.g.\n+    ```\n+    model = create_model()\n+    for w in model.weights:\n+        print(w.path)\n+    ```\n+    \"\"\"\n+\n+    def __init__(self, device_mesh, layout_map, batch_dim_name=None):\n+        \"\"\"Initialize the model parallel distribution.\n+\n+        Args:\n+            device_mesh: `DeviceMesh` instance for physical device and its\n+                logical mapping.\n+            layout_map: `LayoutMap` instance which map the variable path to the\n+                corresponding `TensorLayout`. The axis names of the\n+                `TensorLayout`s should match to the axis names in the\n+                device_mesh, or exception will be raised.\n+            batch_dim_name: optional string, the axis name in the device_mesh\n+                that will be used to distribute data. The first axis from the\n+                device_mesh will be used if user didn't specify any.\n+        \"\"\"\n+        super().__init__(device_mesh)\n+        self._layout_map = layout_map\n+        self._batch_dim_name = batch_dim_name or self.device_mesh.axis_names[0]\n+\n+    def get_data_layout(self, data_shape):\n+        data_shard_spec = [None] * len(data_shape)\n+        data_shard_spec[0] = self._batch_dim_name  # Shard on the first dim\n+        return TensorLayout(data_shard_spec, self.device_mesh)\n+\n+    def get_variable_layout(self, variable):\n+        variable_layout = self._layout_map[variable.path]\n+        if variable_layout is not None:\n+            return variable_layout\n+        variable_shard_spec = [None] * len(variable.shape)\n+        return TensorLayout(variable_shard_spec, self.device_mesh)\n+\n+\n class LayoutMap(collections.abc.MutableMapping):\n     \"\"\"A dict-like object that maps string to `TensorLayout` instances.\n \n\n@@ -193,6 +193,53 @@ class DataParallelDistributionTest(testing.TestCase):\n         self.assertEqual(variable_layout.axes, [None])\n \n \n+class ModelParallelDistributionTest(testing.TestCase):\n+    def setUp(self):\n+        super().setUp()\n+        self.devices = [\"CPU:{i}\" for i in range(8)]\n+        shape = (2, 4)\n+        axis_names = [\"data\", \"model\"]\n+\n+        self.device_mesh = distribution_lib.DeviceMesh(\n+            shape, axis_names, self.devices\n+        )\n+\n+    def test_distribute_weights(self):\n+        layout_map = distribution_lib.LayoutMap(self.device_mesh)\n+        layout_map[\".*kernel\"] = distribution_lib.TensorLayout([None, \"model\"])\n+        layout_map[\".*bias\"] = distribution_lib.TensorLayout([\"model\"])\n+\n+        distribution = distribution_lib.ModelParallel(\n+            self.device_mesh, layout_map, batch_dim_name=\"data\"\n+        )\n+        kernel = backend.Variable(initializer=np.arange(8, 4), name=\"kernel\")\n+        bias = backend.Variable(initializer=np.arange(4), name=\"bias\")\n+        rng_seed = backend.Variable(initializer=[0, 1], name=\"seed\")\n+\n+        kernel_layout = distribution.get_variable_layout(kernel)\n+        self.assertIs(kernel_layout.device_mesh, self.device_mesh)\n+        self.assertEqual(kernel_layout.axes, [None, \"model\"])\n+\n+        bias_layout = distribution.get_variable_layout(bias)\n+        self.assertIs(bias_layout.device_mesh, self.device_mesh)\n+        self.assertEqual(bias_layout.axes, [\"model\"])\n+\n+        rng_seed_layout = distribution.get_variable_layout(rng_seed)\n+        self.assertIs(rng_seed_layout.device_mesh, self.device_mesh)\n+        self.assertEqual(rng_seed_layout.axes, [None])\n+\n+    def test_distribute_data(self):\n+        layout_map = distribution_lib.LayoutMap(self.device_mesh)\n+        distribution = distribution_lib.ModelParallel(\n+            self.device_mesh, layout_map, batch_dim_name=\"data\"\n+        )\n+\n+        data = np.arange(16).reshape((4, 2, 2))\n+        data_layout = distribution.get_data_layout(data.shape)\n+        self.assertIs(data_layout.device_mesh, self.device_mesh)\n+        self.assertEqual(data_layout.axes, [\"data\", None, None])\n+\n+\n class LayoutMapTest(testing.TestCase):\n     def setUp(self):\n         super().setUp()\n@@ -345,7 +392,7 @@ class JaxDistributionLibTest(testing.TestCase):\n         ):\n             backend_dlib.to_jax_layout(layout)\n \n-    def test_e2e_model(self):\n+    def test_e2e_data_parallel_model(self):\n         distribution = distribution_lib.DataParallel(\n             devices=backend_dlib.list_devices()\n         )\n@@ -368,3 +415,42 @@ class JaxDistributionLibTest(testing.TestCase):\n         with distribution.scope():\n             model.compile(loss=\"mse\")\n             model.fit(inputs, labels)\n+\n+    def test_e2e_model_parallel_model(self):\n+        shape = (4, 2)\n+        axis_names = [\"batch\", \"model\"]\n+        device_mesh = distribution_lib.DeviceMesh(\n+            shape, axis_names, backend_dlib.list_devices()\n+        )\n+\n+        layout_map = distribution_lib.LayoutMap(device_mesh)\n+        layout_map[\".*dense.*kernel\"] = distribution_lib.TensorLayout(\n+            [None, \"model\"]\n+        )\n+        layout_map[\".*dense.*bias\"] = distribution_lib.TensorLayout([\"model\"])\n+\n+        distribution = distribution_lib.ModelParallel(\n+            device_mesh, layout_map, batch_dim_name=\"batch\"\n+        )\n+        with distribution.scope():\n+            inputs = layers.Input(shape=[28, 28, 1])\n+            y = layers.Flatten()(inputs)\n+            y = layers.Dense(units=200, use_bias=False, activation=\"relu\")(y)\n+            y = layers.Dropout(0.4)(y)\n+            y = layers.Dense(units=10, activation=\"softmax\")(y)\n+            model = models.Model(inputs=inputs, outputs=y)\n+\n+        for weight in model.weights:\n+            if \"kernel\" in weight.name:\n+                self.assertEqual(weight._value.sharding.spec, (None, \"model\"))\n+            elif \"bias\" in weight.name:\n+                self.assertEqual(weight._value.sharding.spec, (\"model\",))\n+            else:\n+                self.assertTrue(weight._value.sharding.is_fully_replicated)\n+\n+        inputs = np.random.normal(size=(32, 28, 28, 1))\n+        labels = np.random.normal(size=(32, 10))\n+\n+        with distribution.scope():\n+            model.compile(loss=\"mse\")\n+            model.fit(inputs, labels)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#acca3b33b67fa34780941ddd943e5e40c22e82fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 83 | Lines Deleted: 81 | Files Changed: 1 | Hunks: 39 | Methods Changed: 7 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 164 | Churn Cumulative: 706 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -2,8 +2,8 @@\n \n !!!DO NOT USE!!! Currently under development and APIs are not final.\n \n-Currently only the JAX backend has been implemented, and the Tensorflow backend\n-will be implemented in future (via tf.dtensor API).\n+Currently only the JAX backend has been implemented. The TensorFlow backend\n+will be implemented in the future (via tf.dtensor API).\n \"\"\"\n \n import collections\n@@ -23,12 +23,13 @@ GLOBAL_ATTRIBUTE_NAME = \"distribution\"\n def list_devices(device_type=None):\n     \"\"\"Return all the available devices based on the device type.\n \n-    Note that this should return the global devices in a distributed setting.\n+    Note in a distributed setting, global devices are returned.\n \n     Args:\n-        device_type: string of `\"cpu\"`, `\"gpu\"` or `\"tpu\"`. Default to `gpu` or\n-            `tpu` if available when device_type is not provided. Otherwise\n-            will return the `cpu` devices.\n+        device_type: string, one of `\"cpu\"`, `\"gpu\"` or `\"tpu\"`.\n+            Default to `\"gpu\"` or `\"tpu\"` if available when\n+            `device_type` is not provided. Otherwise\n+            will return the `\"cpu\"` devices.\n \n     Return:\n         List of devices that are available for distribute computation.\n@@ -37,15 +38,27 @@ def list_devices(device_type=None):\n \n \n class DeviceMesh:\n-    \"\"\"The cluster of computation devices for distributed computation.\n+    \"\"\"A cluster of computation devices for distributed computation.\n \n-    This is aligned with `jax.sharding.Mesh` and `tf.dtensor.Mesh`, which\n+    This API is aligned with `jax.sharding.Mesh` and `tf.dtensor.Mesh`, which\n     represents the computation devices in the global context.\n \n     See more details in [jax.sharding.Mesh](\n         https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.Mesh)\n     and [tf.dtensor.Mesh](\n         https://www.tensorflow.org/api_docs/python/tf/experimental/dtensor/Mesh).\n+\n+    Args:\n+        shape: tuple of list of integers. The shape of the overall\n+            `DeviceMesh`, e.g. `(8,)` for a data parallel only distribution,\n+            or `(4, 2)` for a model+data parallel distribution.\n+        axis_names: List of string. The logical name of the each axis for\n+            the `DeviceMesh`. The length of the `axis_names` should match to\n+            the rank of the `shape`. The `axis_names` will be used to\n+            match/create the `TensorLayout` when distribute the data and\n+            variables.\n+        devices: Optional list of devices. Default to all the available\n+            devices locally from `list_devices()`.\n     \"\"\"\n \n     def __init__(\n@@ -54,30 +67,16 @@ class DeviceMesh:\n         axis_names,\n         devices=None,\n     ):\n-        \"\"\"Initialize the DeviceMesh for the given topology.\n-\n-        Args:\n-            shape: tuple of list of integers. The shape of the overall\n-                DeviceMesh, e.g. `(8,)` for a data parallel only distribution,\n-                or `(4, 2)` for a model+data parallel distribution.\n-            axis_names: List of string. The logical name of the each axis for\n-                the DeviceMesh. The length of the `axis_names` should match to\n-                the rank of the `shape`. The `axis_names` will be used to\n-                match/create the `TensorLayout` when distribute the data and\n-                weights.\n-            devices: Optional list of devices. Default to all the available\n-                devices locally from `list_devices()`.\n-        \"\"\"\n         if not shape or not axis_names:\n             raise ValueError(\n-                \"Shape and axis_names cannot be empty. Got \"\n-                f\"shape: {shape}, axis_names: {axis_names}\"\n+                \"Shape and axis_names cannot be empty. Received: \"\n+                f\"shape={shape}, axis_names={axis_names}\"\n             )\n \n         if len(shape) != len(axis_names):\n             raise ValueError(\n-                \"Shape and axis_names should have same size,\"\n-                f\"got shape: {shape} and axis_names: {axis_names}\"\n+                \"Shape and axis_names should have same size. \"\n+                f\"Received: shape={shape}, axis_names={axis_names}\"\n             )\n         if devices is None:\n             devices = list_devices()\n@@ -85,8 +84,8 @@ class DeviceMesh:\n         if np.prod(shape) != np.prod(devices.shape):\n             raise ValueError(\n                 \"Shape does not match the number of devices. \"\n-                f\"Got shape: {shape}, and shape of the \"\n-                f\"devices: {devices.shape}\"\n+                f\"Received: shape={shape}; devices.shape=\"\n+                f\"{devices.shape}\"\n             )\n \n         self._shape = shape\n@@ -107,21 +106,15 @@ class DeviceMesh:\n \n \n class TensorLayout:\n-    \"\"\"The layout of a Tensor.\n+    \"\"\"The layout of a tensor.\n \n-    This is aligned with `jax.sharding.NamedSharding` and `tf.dtensor.Layout`,\n-    which allocate the tensor to its logic axis based on the `DeviceMesh`. With\n-    `DeviceMesh` and `TensorLayout`, the actual mapping between a Tensor to the\n-    physical devices can be determined.\n+    This API is aligned with `jax.sharding.NamedSharding`\n+    and `tf.dtensor.Layout`.\n \n     See more details in [jax.sharding.NamedSharding](\n         https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.NamedSharding)\n     and [tf.dtensor.Layout](\n         https://www.tensorflow.org/api_docs/python/tf/experimental/dtensor/Layout).\n-    \"\"\"\n-\n-    def __init__(self, axes, device_mesh=None):\n-        \"\"\"Initialize the TensorLayout with axis names.\n \n     Args:\n         axes: list of strings that should map to the `axis_names` in\n@@ -131,6 +124,8 @@ class TensorLayout:\n             the layout. The actual mapping of tensor to physical device\n             is not known until the mesh is specified.\n     \"\"\"\n+\n+    def __init__(self, axes, device_mesh=None):\n         self._axes = axes\n         self._device_mesh = device_mesh\n         self._validate_axes()\n@@ -174,6 +169,9 @@ class Distribution:\n \n     It can create a context scope so that the framework to properly detect the\n     `Distribution` and distribute the variable/data accordingly.\n+\n+    Args:\n+        device_mesh: A `DeviceMesh` instance.\n     \"\"\"\n \n     def __init__(self, device_mesh):\n@@ -195,7 +193,7 @@ class Distribution:\n         \"\"\"Retrieve the `TensorLayout` for the variable.\n \n         Args:\n-            variable: A `KerasVariable` to retrieve the `TensorLayout`.\n+            variable: A `KerasVariable` instance.\n \n         return:\n             The `TensorLayout` for the variable, which can be used by\n@@ -219,15 +217,14 @@ class Distribution:\n \n \n class DataParallel(Distribution):\n-    def __init__(self, device_mesh=None, devices=None):\n-        \"\"\"Create the data parallel distribution.\n+    \"\"\"Distribution for data parallelism.\n \n     You can choose to create this instance by either specifing\n-        the `device_mesh` or `devices` parameters (but not both).\n+    the `device_mesh` or `devices` arguments (but not both).\n \n-        The device_mesh is expected to be a `DeviceMesh` instance, and is\n-        expected to be 1D only. In case that the mesh has multiple axes, then\n-        the first axis will be treated as the data parallel dimension\n+    The `device_mesh` argument is expected to be a `DeviceMesh` instance,\n+    and is expected to be 1D only. In case that the mesh has multiple axes,\n+    then the first axis will be treated as the data parallel dimension\n     (and a warning will be raised).\n \n     When a list of `devices` are provided, they will be used to construct a\n@@ -236,7 +233,13 @@ class DataParallel(Distribution):\n     When both `mesh` and `devices` are absent, then `list_devices()`\n     will be used to detect any available devices and create a 1D mesh from\n     them.\n+\n+    Args:\n+        device_mesh: Optional `DeviceMesh` instance.\n+        devices: Optional list of devices.\n     \"\"\"\n+\n+    def __init__(self, device_mesh=None, devices=None):\n         if device_mesh:\n             self._initialize_with_device_mesh(device_mesh)\n         elif devices:\n@@ -290,31 +293,33 @@ class DataParallel(Distribution):\n \n \n class ModelParallel(Distribution):\n-    \"\"\"Distribution that shard model weights.\n+    \"\"\"Distribution that shards model variables.\n \n-    Compare to DataParallel which replicates the weights across all the devices,\n-    ModelParallel allows user to shard weights in addition to the input data.\n+    Compare to `DataParallel` which replicates the variables across all devices,\n+    `ModelParallel` allows you to shard variables in addition to the input data.\n \n-    To construct a ModelParallel distribution, user need to provide device mesh\n-    and layout mapping.\n+    To construct a `ModelParallel` distribution, you need to provide a\n+    `DeviceMesh` and a `LayoutMap`.\n \n-    1. `DeviceMesh`contains physcial device information, and the axis names in\n-        the mesh will be used to map the weight and data layout.\n-    2. `LayoutMap` contains the mapping for the variable path to its\n+    1. `DeviceMesh` contains physcial device information. The axis names in\n+        the mesh will be used to map the variable and data layout.\n+    2. `LayoutMap` contains the mapping between variable paths to their\n         corresponding `TensorLayout`.\n \n     Example:\n+\n     ```python\n     devices = list_devices()    # Assume there are 8 devices.\n \n-    # Create a mesh with 2 devices on data parallel and 4 devices on weight\n-    # parallel.\n+    # Create a mesh with 2 devices for data parallelism and 4 devices for\n+    # model parallelism.\n     device_mesh = DeviceMesh(shape=(2, 4), axis_names=('batch', 'model'),\n                              devices=devices)\n-    # Create a layout map that shard the dense layer and conv2d layer weights\n-    # on the last dimension. Based on the device_mesh, this means the weights\n-    # will be split across 4 devices. Any other weights that doesn't match for\n-    # any key in layout map will get be fully replicated.\n+    # Create a layout map that shard the `Dense` layer and `Conv2D`\n+    # layer variables on the last dimension.\n+    # Based on the `device_mesh`, this means the variables\n+    # will be split across 4 devices. Any other variable that doesn't\n+    # match any key in the layout map will be fully replicated.\n     layout_map = LayoutMap(device_mesh)\n     layout_map['.*dense.*kernel'] = TensorLayout([None, 'model'])\n     layout_map['.*dense.*bias'] = TensorLayout(['model'])\n@@ -332,30 +337,26 @@ class ModelParallel(Distribution):\n     model.fit(data)\n     ```\n \n-    User can quickly update the device mesh shape to change the sharding factor\n-    of the weights. E.g.\n+    You can quickly update the device mesh shape to change the sharding factor\n+    of the variables. E.g.\n     ```\n-    # With only the shape change for the device mesh, the weights will be\n-    # sharded across 8 devices instead of 4, which further reduce the memory\n-    # footprint of weights on each of the device.\n+    # With only the shape change for the device mesh, the variables will be\n+    # sharded across 8 devices instead of 4, which further reduces the memory\n+    # footprint of variables on each of the device.\n     device_mesh = DeviceMesh(shape=(1, 8), axis_names=('batch', 'model'),\n                              devices=devices)\n     ```\n \n-    To figure out a proper layout mapping rule for all the model weights, you\n-    can first list out all the model weights path, which will be used as the key\n-    to map the weights to `TensorLayout`.\n+    To figure out a proper layout mapping rule for all the model variables, you\n+    can first list out all the model variable paths, which will be used as the\n+    key to map the variables to `TensorLayout`.\n \n     e.g.\n     ```\n     model = create_model()\n-    for w in model.weights:\n-        print(w.path)\n+    for v in model.variables:\n+        print(v.path)\n     ```\n-    \"\"\"\n-\n-    def __init__(self, device_mesh, layout_map, batch_dim_name=None):\n-        \"\"\"Initialize the model parallel distribution.\n \n     Args:\n         device_mesh: `DeviceMesh` instance for physical device and its\n@@ -364,10 +365,12 @@ class ModelParallel(Distribution):\n             corresponding `TensorLayout`. The axis names of the\n             `TensorLayout`s should match to the axis names in the\n             device_mesh, or exception will be raised.\n-            batch_dim_name: optional string, the axis name in the device_mesh\n-                that will be used to distribute data. The first axis from the\n-                device_mesh will be used if user didn't specify any.\n+        batch_dim_name: optional string, the axis name in the `device_mesh`\n+            that will be used to distribute data. If unspecified, the\n+            first axis from the `device_mesh` will be used.\n     \"\"\"\n+\n+    def __init__(self, device_mesh, layout_map, batch_dim_name=None):\n         super().__init__(device_mesh)\n         self._layout_map = layout_map\n         self._batch_dim_name = batch_dim_name or self.device_mesh.axis_names[0]\n@@ -419,8 +422,7 @@ class LayoutMap(collections.abc.MutableMapping):\n \n     Args:\n         device_mesh: An optional `DeviceMesh` that can be used to populate the\n-            `TensorLayout.device_mesh` if the `TensorLayout.device_mesh` is not\n-            set.\n+            `TensorLayout.device_mesh` if `TensorLayout.device_mesh` is not set.\n     \"\"\"\n \n     def __init__(self, device_mesh=None):\n@@ -428,15 +430,15 @@ class LayoutMap(collections.abc.MutableMapping):\n         self._device_mesh = device_mesh\n \n     def __getitem__(self, key):\n-        \"\"\"Retrieve the corresponding layout by the string key.\n+        \"\"\"Retrieves the corresponding layout by the string key.\n \n         When there isn't an exact match, all the existing keys in the layout map\n         will be treated as a regex and map against the input key again. The\n-        first match will be returned, based on the key insertion order. Return\n-        None if there isn't any match found.\n+        first match will be returned, based on the key insertion order. Returns\n+        `None` if there isn't any match found.\n \n         Args:\n-            key: the string key as the query for the layout.\n+            key: String key to query a layout.\n \n         Returns:\n             Corresponding layout based on the query.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fe06633f0df7d65dce1eed38c0f6466b8ea1f20b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 426 | Lines Deleted: 28 | Files Changed: 1 | Hunks: 23 | Methods Changed: 21 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 454 | Churn Cumulative: 750 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,6 +22,36 @@ def _ref_log_softmax(values):\n     return stabilized_values - log_sum_exp\n \n \n+def _ref_leaky_relu(x, alpha=0.2):\n+    return x if x > 0 else alpha * x\n+\n+\n+def _ref_relu6(x):\n+    return min(max(0, x), 6)\n+\n+\n+def _ref_silu(x):\n+    return x / (1 + np.exp(-x))\n+\n+\n+def _ref_hard_sigmoid(x):\n+    x = (x / 6.0) + 0.5\n+    z = 0.0 if x <= 0 else (1.0 if x >= 1 else x)\n+    return z\n+\n+\n+def _ref_sigmoid(x):\n+    if x >= 0:\n+        return 1 / (1 + np.exp(-x))\n+    else:\n+        z = np.exp(x)\n+        return z / (1 + z)\n+\n+\n+def _ref_softsign(x):\n+    return np.divide(x, np.ones_like(x) + np.absolute(x))\n+\n+\n class ActivationsTest(testing.TestCase):\n     def test_softmax(self):\n         x = np.random.random((2, 5))\n@@ -152,57 +182,279 @@ class ActivationsTest(testing.TestCase):\n         self.assertAllClose(result, true_result)\n \n     def test_softplus(self):\n-        x = np.random.random((2, 5))\n+        # Basic test for random values between 0 and 1\n+        x = np.random.uniform(0, 1, (2, 5))\n         result = activations.softplus(x[np.newaxis, :])[0]\n-        expected = _ref_softplus(x)\n+        expected = np.vectorize(_ref_softplus)(x)\n         self.assertAllClose(result, expected, rtol=1e-05)\n \n+        # Test with 1D array\n+        x_1d = np.random.uniform(-10, 10, 5)\n+        result_1d = activations.softplus(x_1d)\n+        expected_1d = np.vectorize(_ref_softplus)(x_1d)\n+        self.assertAllClose(result_1d, expected_1d, rtol=1e-05)\n+\n+        # Test with 3D array\n+        x_3d = np.random.uniform(-10, 10, (3, 3, 3))\n+        result_3d = activations.softplus(x_3d)\n+        expected_3d = np.vectorize(_ref_softplus)(x_3d)\n+        self.assertAllClose(result_3d, expected_3d, rtol=1e-05)\n+\n+        # Test near zero values\n+        x_zero = np.random.uniform(-1e-7, 1e-7, (2, 5))\n+        result_zero = activations.softplus(x_zero)\n+        expected_zero = np.vectorize(_ref_softplus)(x_zero)\n+        self.assertAllClose(result_zero, expected_zero, rtol=1e-05)\n+\n+        # Test large positive values\n+        x_large_positive = np.random.uniform(10, 100, (2, 5))\n+        result_large_positive = activations.softplus(x_large_positive)\n+        expected_large_positive = np.vectorize(_ref_softplus)(x_large_positive)\n+        self.assertAllClose(\n+            result_large_positive, expected_large_positive, rtol=1e-05\n+        )\n+\n+        # Test large negative values\n+        x_large_negative = np.random.uniform(-100, -10, (2, 5))\n+        result_large_negative = activations.softplus(x_large_negative)\n+        expected_large_negative = np.vectorize(_ref_softplus)(x_large_negative)\n+        self.assertAllClose(\n+            result_large_negative, expected_large_negative, rtol=1e-05\n+        )\n+\n     def test_softsign(self):\n-        def softsign(x):\n-            return np.divide(x, np.ones_like(x) + np.absolute(x))\n-\n-        x = np.random.random((2, 5))\n+        # Basic test for random values between 0 and 1\n+        x = np.random.uniform(0, 1, (2, 5))\n         result = activations.softsign(x[np.newaxis, :])[0]\n-        expected = softsign(x)\n+        expected = np.vectorize(_ref_softsign)(x)\n         self.assertAllClose(result, expected, rtol=1e-05)\n \n+        # Test with 1D array\n+        x_1d = np.random.uniform(-10, 10, 5)\n+        result_1d = activations.softsign(x_1d)\n+        expected_1d = np.vectorize(_ref_softsign)(x_1d)\n+        self.assertAllClose(result_1d, expected_1d, rtol=1e-05)\n+\n+        # Test with 3D array\n+        x_3d = np.random.uniform(-10, 10, (3, 3, 3))\n+        result_3d = activations.softsign(x_3d)\n+        expected_3d = np.vectorize(_ref_softsign)(x_3d)\n+        self.assertAllClose(result_3d, expected_3d, rtol=1e-05)\n+\n+        # Test near zero values\n+        x_zero = np.random.uniform(-1e-7, 1e-7, (2, 5))\n+        result_zero = activations.softsign(x_zero)\n+        expected_zero = np.vectorize(_ref_softsign)(x_zero)\n+        self.assertAllClose(result_zero, expected_zero, rtol=1e-05)\n+\n+        # Test large positive values\n+        x_large_positive = np.random.uniform(10, 100, (2, 5))\n+        result_large_positive = activations.softsign(x_large_positive)\n+        expected_large_positive = np.vectorize(_ref_softsign)(x_large_positive)\n+        self.assertAllClose(\n+            result_large_positive, expected_large_positive, rtol=1e-05\n+        )\n+\n+        # Test large negative values\n+        x_large_negative = np.random.uniform(-100, -10, (2, 5))\n+        result_large_negative = activations.softsign(x_large_negative)\n+        expected_large_negative = np.vectorize(_ref_softsign)(x_large_negative)\n+        self.assertAllClose(\n+            result_large_negative, expected_large_negative, rtol=1e-05\n+        )\n+\n     def test_sigmoid(self):\n-        def ref_sigmoid(x):\n-            if x >= 0:\n-                return 1 / (1 + np.exp(-x))\n-            else:\n-                z = np.exp(x)\n-                return z / (1 + z)\n-\n-        sigmoid = np.vectorize(ref_sigmoid)\n-\n-        x = np.random.random((2, 5))\n+        # Basic test for random values between 0 and 1\n+        x = np.random.uniform(0, 1, (2, 5))\n         result = activations.sigmoid(x[np.newaxis, :])[0]\n-        expected = sigmoid(x)\n+        expected = np.vectorize(_ref_sigmoid)(x)\n         self.assertAllClose(result, expected, rtol=1e-05)\n \n+        # Test with 1D array\n+        x_1d = np.random.uniform(-10, 10, 5)\n+        result_1d = activations.sigmoid(x_1d)\n+        expected_1d = np.vectorize(_ref_sigmoid)(x_1d)\n+        self.assertAllClose(result_1d, expected_1d, rtol=1e-05)\n+\n+        # Test with 3D array\n+        x_3d = np.random.uniform(-10, 10, (3, 3, 3))\n+        result_3d = activations.sigmoid(x_3d)\n+        expected_3d = np.vectorize(_ref_sigmoid)(x_3d)\n+        self.assertAllClose(result_3d, expected_3d, rtol=1e-05)\n+\n+        # Test near zero values\n+        x_zero = np.random.uniform(-1e-7, 1e-7, (2, 5))\n+        result_zero = activations.sigmoid(x_zero)\n+        expected_zero = np.vectorize(_ref_sigmoid)(x_zero)\n+        self.assertAllClose(result_zero, expected_zero, rtol=1e-05)\n+\n+        # Test large positive values\n+        x_large_positive = np.random.uniform(10, 100, (2, 5))\n+        result_large_positive = activations.sigmoid(x_large_positive)\n+        expected_large_positive = np.vectorize(_ref_sigmoid)(x_large_positive)\n+        self.assertAllClose(\n+            result_large_positive, expected_large_positive, rtol=1e-05\n+        )\n+\n+        # Test large negative values\n+        x_large_negative = np.random.uniform(-100, -10, (2, 5))\n+        result_large_negative = activations.sigmoid(x_large_negative)\n+        expected_large_negative = np.vectorize(_ref_sigmoid)(x_large_negative)\n+        self.assertAllClose(\n+            result_large_negative, expected_large_negative, rtol=1e-05\n+        )\n+\n     def test_hard_sigmoid(self):\n-        def ref_hard_sigmoid(x):\n-            x = (x / 6.0) + 0.5\n-            z = 0.0 if x <= 0 else (1.0 if x >= 1 else x)\n-            return z\n-\n-        hard_sigmoid = np.vectorize(ref_hard_sigmoid)\n-        x = np.random.random((2, 5))\n+        # Basic test for random values between 0 and 1\n+        x = np.random.uniform(0, 1, (2, 5))\n         result = activations.hard_sigmoid(x[np.newaxis, :])[0]\n-        expected = hard_sigmoid(x)\n+        expected = np.vectorize(_ref_hard_sigmoid)(x)\n         self.assertAllClose(result, expected, rtol=1e-05)\n \n+        # Test with 1D array\n+        x_1d = np.random.uniform(-10, 10, 5)\n+        result_1d = activations.hard_sigmoid(x_1d)\n+        expected_1d = np.vectorize(_ref_hard_sigmoid)(x_1d)\n+        self.assertAllClose(result_1d, expected_1d, rtol=1e-05)\n+\n+        # Test with 3D array\n+        x_3d = np.random.uniform(-10, 10, (3, 3, 3))\n+        result_3d = activations.hard_sigmoid(x_3d)\n+        expected_3d = np.vectorize(_ref_hard_sigmoid)(x_3d)\n+        self.assertAllClose(result_3d, expected_3d, rtol=1e-05)\n+\n+        # Test with strictly positive values much larger than 1\n+        x_positive_above_1 = np.random.uniform(\n+            5, 10, (2, 5)\n+        )  # Adjusted this range\n+        result_positive_above_1 = activations.hard_sigmoid(x_positive_above_1)\n+        expected_positive_above_1 = np.ones((2, 5))\n+        self.assertAllClose(\n+            result_positive_above_1, expected_positive_above_1, rtol=1e-05\n+        )\n+\n     def test_relu(self):\n-        positive_values = np.random.random((2, 5))\n+        # Basic test for positive values\n+        positive_values = np.random.uniform(0.1, 10, (2, 5))\n         result = activations.relu(positive_values[np.newaxis, :])[0]\n         self.assertAllClose(result, positive_values, rtol=1e-05)\n \n-        negative_values = np.random.uniform(-1, 0, (2, 5))\n+        # Basic test for negative values\n+        negative_values = np.random.uniform(-10, -0.1, (2, 5))\n         result = activations.relu(negative_values[np.newaxis, :])[0]\n         expected = np.zeros((2, 5))\n         self.assertAllClose(result, expected, rtol=1e-05)\n \n+        # Test with 1D array\n+        x_1d = np.random.uniform(-10, 10, 5)\n+        result_1d = activations.relu(x_1d)\n+        expected_1d = np.maximum(0, x_1d)\n+        self.assertAllClose(result_1d, expected_1d, rtol=1e-05)\n+\n+        # Test with 3D array\n+        x_3d = np.random.uniform(-10, 10, (3, 3, 3))\n+        result_3d = activations.relu(x_3d)\n+        expected_3d = np.maximum(0, x_3d)\n+        self.assertAllClose(result_3d, expected_3d, rtol=1e-05)\n+\n+        # Test near zero values\n+        x_zero = np.random.uniform(-1e-7, 1e-7, (2, 5))\n+        result_zero = activations.relu(x_zero)\n+        expected_zero = np.maximum(0, x_zero)\n+        self.assertAllClose(result_zero, expected_zero, rtol=1e-05)\n+\n+        # Test large positive values\n+        x_large_positive = np.random.uniform(1e4, 1e5, (2, 5))\n+        result_large_positive = activations.relu(x_large_positive)\n+        self.assertAllClose(result_large_positive, x_large_positive, rtol=1e-05)\n+\n+        # Test large negative values\n+        x_large_negative = np.random.uniform(-1e5, -1e4, (2, 5))\n+        result_large_negative = activations.relu(x_large_negative)\n+        expected_large_negative = np.zeros((2, 5))\n+        self.assertAllClose(\n+            result_large_negative, expected_large_negative, rtol=1e-05\n+        )\n+\n+    def test_leaky_relu(self):\n+        leaky_relu_vectorized = np.vectorize(_ref_leaky_relu)\n+\n+        # Test for negative_slope = 0.01\n+        # Test positive values\n+        positive_values = np.random.random((2, 5))\n+        result = activations.leaky_relu(\n+            positive_values[np.newaxis, :], negative_slope=0.01\n+        )[0]\n+        expected = leaky_relu_vectorized(positive_values, alpha=0.01)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+        # Test negative values\n+        negative_values = np.random.uniform(-1, 0, (2, 5))\n+        result = activations.leaky_relu(\n+            negative_values[np.newaxis, :], negative_slope=0.01\n+        )[0]\n+        expected = leaky_relu_vectorized(negative_values, alpha=0.01)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+        # Test for negative_slope = 0.3\n+        # Test positive values\n+        positive_values = np.random.random((2, 5))\n+        result = activations.leaky_relu(\n+            positive_values[np.newaxis, :], negative_slope=0.3\n+        )[0]\n+        expected = leaky_relu_vectorized(positive_values, alpha=0.3)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+        # Test negative values\n+        negative_values = np.random.uniform(-1, 0, (2, 5))\n+        result = activations.leaky_relu(\n+            negative_values[np.newaxis, :], negative_slope=0.3\n+        )[0]\n+        expected = leaky_relu_vectorized(negative_values, alpha=0.3)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+    def test_relu6(self):\n+        relu6_vectorized = np.vectorize(_ref_relu6)\n+\n+        # Test positive values less than 6\n+        positive_values = np.random.uniform(0, 5.9, (2, 5))\n+        result = activations.relu6(positive_values[np.newaxis, :])[0]\n+        expected = relu6_vectorized(positive_values)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+        # Test positive values greater than 6\n+        positive_values_above_6 = np.random.uniform(6.1, 10, (2, 5))\n+        result = activations.relu6(positive_values_above_6[np.newaxis, :])[0]\n+        expected = relu6_vectorized(positive_values_above_6)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+        # Test negative values\n+        negative_values = np.random.uniform(-1, 0, (2, 5))\n+        result = activations.relu6(negative_values[np.newaxis, :])[0]\n+        expected = relu6_vectorized(negative_values)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+    def test_silu(self):\n+        silu_vectorized = np.vectorize(_ref_silu)\n+\n+        # Test positive values\n+        positive_values = np.random.uniform(0, 5.9, (2, 5))\n+        result = activations.silu(positive_values[np.newaxis, :])[0]\n+        expected = silu_vectorized(positive_values)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+        # Test values around zero (to ensure sigmoid behaves correctly)\n+        around_zero_values = np.random.uniform(-1, 1, (2, 5))\n+        result = activations.silu(around_zero_values[np.newaxis, :])[0]\n+        expected = silu_vectorized(around_zero_values)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+        # Test negative values\n+        negative_values = np.random.uniform(-5.9, 0, (2, 5))\n+        result = activations.silu(negative_values[np.newaxis, :])[0]\n+        expected = silu_vectorized(negative_values)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n     def test_gelu(self):\n         def gelu(x, approximate=False):\n             if approximate:\n@@ -242,27 +494,173 @@ class ActivationsTest(testing.TestCase):\n         self.assertAllClose(result, true_result)\n \n     def test_tanh(self):\n+        # Basic test for the tanh activation function\n         x = np.random.random((2, 5))\n         result = activations.tanh(x[np.newaxis, :])[0]\n         expected = np.tanh(x)\n         self.assertAllClose(result, expected, rtol=1e-05)\n \n+        # Basic test for the tanh activation function\n+        x = np.random.uniform(-10, 10, (2, 5))\n+        result = activations.tanh(x[np.newaxis, :])[0]\n+        expected = np.tanh(x)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+        # Test with 1D array\n+        x_1d = np.random.uniform(-10, 10, 5)\n+        result_1d = activations.tanh(x_1d)\n+        expected_1d = np.tanh(x_1d)\n+        self.assertAllClose(result_1d, expected_1d, rtol=1e-05)\n+\n+        # Test with 3D array\n+        x_3d = np.random.uniform(-10, 10, (3, 3, 3))\n+        result_3d = activations.tanh(x_3d)\n+        expected_3d = np.tanh(x_3d)\n+        self.assertAllClose(result_3d, expected_3d, rtol=1e-05)\n+\n+        # Test with strictly positive values\n+        x_positive = np.random.uniform(0, 10, (2, 5))\n+        result_positive = activations.tanh(x_positive)\n+        expected_positive = np.tanh(x_positive)\n+        self.assertAllClose(result_positive, expected_positive, rtol=1e-05)\n+\n+        # Test with strictly negative values\n+        x_negative = np.random.uniform(-10, 0, (2, 5))\n+        result_negative = activations.tanh(x_negative)\n+        expected_negative = np.tanh(x_negative)\n+        self.assertAllClose(result_negative, expected_negative, rtol=1e-05)\n+\n+        # Test near zero values\n+        x_zero = np.random.uniform(-1e-7, 1e-7, (2, 5))\n+        result_zero = activations.tanh(x_zero)\n+        expected_zero = np.tanh(x_zero)\n+        self.assertAllClose(result_zero, expected_zero, rtol=1e-05)\n+\n+        # Test large values to check stability\n+        x_large = np.random.uniform(1e4, 1e5, (2, 5))\n+        result_large = activations.tanh(x_large)\n+        expected_large = np.tanh(x_large)\n+        self.assertAllClose(result_large, expected_large, rtol=1e-05)\n+\n     def test_exponential(self):\n+        # Basic test for the exponential activation function\n         x = np.random.random((2, 5))\n         result = activations.exponential(x[np.newaxis, :])[0]\n         expected = np.exp(x)\n         self.assertAllClose(result, expected, rtol=1e-05)\n \n+        x = np.random.uniform(-10, 10, (2, 5))\n+        result = activations.exponential(x[np.newaxis, :])[0]\n+        expected = np.exp(x)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+        # Test with 1D array\n+        x_1d = np.random.uniform(-10, 10, 5)\n+        result_1d = activations.exponential(x_1d)\n+        expected_1d = np.exp(x_1d)\n+        self.assertAllClose(result_1d, expected_1d, rtol=1e-05)\n+\n+        # Test with 3D array\n+        x_3d = np.random.uniform(-10, 10, (3, 3, 3))\n+        result_3d = activations.exponential(x_3d)\n+        expected_3d = np.exp(x_3d)\n+        self.assertAllClose(result_3d, expected_3d, rtol=1e-05)\n+\n+        # Test with strictly positive values\n+        x_positive = np.random.uniform(0, 10, (2, 5))\n+        result_positive = activations.exponential(x_positive)\n+        expected_positive = np.exp(x_positive)\n+        self.assertAllClose(result_positive, expected_positive, rtol=1e-05)\n+\n+        # Test with strictly negative values\n+        x_negative = np.random.uniform(-10, 0, (2, 5))\n+        result_negative = activations.exponential(x_negative)\n+        expected_negative = np.exp(x_negative)\n+        self.assertAllClose(result_negative, expected_negative, rtol=1e-05)\n+\n+        # Test near zero values\n+        x_zero = np.random.uniform(-1e-7, 1e-7, (2, 5))\n+        result_zero = activations.exponential(x_zero)\n+        expected_zero = np.exp(x_zero)\n+        self.assertAllClose(result_zero, expected_zero, rtol=1e-05)\n+\n+        # Test large values to check stability\n+        x_large = np.random.uniform(1e4, 1e5, (2, 5))\n+        result_large = activations.exponential(x_large)\n+        expected_large = np.exp(x_large)\n+        self.assertAllClose(result_large, expected_large, rtol=1e-05)\n+\n     def test_mish(self):\n+        # Basic test for the mish activation function\n         x = np.random.random((2, 5))\n         result = activations.mish(x[np.newaxis, :])[0]\n         expected = x * np.tanh(_ref_softplus(x))\n         self.assertAllClose(result, expected, rtol=1e-05)\n \n+        x = np.random.uniform(-10, 10, (2, 5))\n+        result = activations.mish(x[np.newaxis, :])[0]\n+        expected = x * np.tanh(_ref_softplus(x))\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+        # Test with 1D array\n+        x_1d = np.random.uniform(-10, 10, 5)\n+        result_1d = activations.mish(x_1d)\n+        expected_1d = x_1d * np.tanh(_ref_softplus(x_1d))\n+        self.assertAllClose(result_1d, expected_1d, rtol=1e-05)\n+\n+        # Test with 3D array\n+        x_3d = np.random.uniform(-10, 10, (3, 3, 3))\n+        result_3d = activations.mish(x_3d)\n+        expected_3d = x_3d * np.tanh(_ref_softplus(x_3d))\n+        self.assertAllClose(result_3d, expected_3d, rtol=1e-05)\n+\n+        # Test with strictly positive values\n+        x_positive = np.random.uniform(0, 10, (2, 5))\n+        result_positive = activations.mish(x_positive)\n+        expected_positive = x_positive * np.tanh(_ref_softplus(x_positive))\n+        self.assertAllClose(result_positive, expected_positive, rtol=1e-05)\n+\n+        # Test with strictly negative values\n+        x_negative = np.random.uniform(-10, 0, (2, 5))\n+        result_negative = activations.mish(x_negative)\n+        expected_negative = x_negative * np.tanh(_ref_softplus(x_negative))\n+        self.assertAllClose(result_negative, expected_negative, rtol=1e-05)\n+\n+        # Test near zero values\n+        x_zero = np.random.uniform(-1e-7, 1e-7, (2, 5))\n+        result_zero = activations.mish(x_zero)\n+        expected_zero = x_zero * np.tanh(_ref_softplus(x_zero))\n+        self.assertAllClose(result_zero, expected_zero, rtol=1e-05)\n+\n+        # Test large values to check stability\n+        x_large = np.random.uniform(1e4, 1e5, (2, 5))\n+        result_large = activations.mish(x_large)\n+        expected_large = x_large * np.tanh(_ref_softplus(x_large))\n+        self.assertAllClose(result_large, expected_large, rtol=1e-05)\n+\n     def test_linear(self):\n         x = np.random.random((10, 5))\n         self.assertAllClose(x, activations.linear(x))\n \n+        # Test with 1D array\n+        x_1d = np.random.uniform(-10, 10, 5)\n+        self.assertAllClose(x_1d, activations.linear(x_1d))\n+\n+        # Test with 2D array\n+        x = np.random.uniform(-10, 10, (10, 5))\n+        self.assertAllClose(x, activations.linear(x))\n+\n+        # Test with 3D array\n+        x_3d = np.random.uniform(-10, 10, (5, 5, 5))\n+        self.assertAllClose(x_3d, activations.linear(x_3d))\n+\n+        # Test with float32 data type\n+        x_float32 = np.random.uniform(-10, 10, (10, 5)).astype(np.float32)\n+        self.assertAllClose(x_float32, activations.linear(x_float32))\n+        # Test with int32 data type\n+        x_int32 = np.random.randint(-10, 10, (10, 5)).astype(np.int32)\n+        self.assertAllClose(x_int32, activations.linear(x_int32))\n+\n     def test_get_method(self):\n         obj = activations.get(\"relu\")\n         self.assertEqual(obj, activations.relu)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
