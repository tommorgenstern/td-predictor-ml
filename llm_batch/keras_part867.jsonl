{"custom_id": "keras#34c453662fd1727e9844d3f1684e3e5cf3b10dff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 101 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -2,7 +2,6 @@ import os\n \n import numpy as np\n import pytest\n-import tf_keras\n \n import keras\n from keras.src import layers\n@@ -17,6 +16,11 @@ from keras.src.saving import serialization_lib\n # on exact weight ordering for each layer, so we need\n # to test across all types of layers.\n \n+try:\n+    import tf_keras\n+except:\n+    tf_keras = None\n+\n \n def get_sequential_model(keras):\n     return keras.Sequential(\n@@ -73,6 +77,7 @@ def get_subclassed_model(keras):\n \n \n @pytest.mark.requires_trainable_backend\n+@pytest.mark.skipif(tf_keras is None, reason=\"Test requires tf_keras\")\n class LegacyH5WeightsTest(testing.TestCase):\n     def _check_reloading_weights(self, ref_input, model, tf_keras_model):\n         ref_output = tf_keras_model(ref_input)\n@@ -276,6 +281,7 @@ class LegacyH5WholeModelTest(testing.TestCase):\n \n \n @pytest.mark.requires_trainable_backend\n+@pytest.mark.skipif(tf_keras is None, reason=\"Test requires tf_keras\")\n class LegacyH5BackwardsCompatTest(testing.TestCase):\n     def _check_reloading_model(self, ref_input, model, tf_keras_model):\n         # Whole model file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f529d62eceb3fb8ce29e7a78577a84f2fa945eac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 131 | Lines Deleted: 0 | Files Changed: 15 | Hunks: 19 | Methods Changed: 14 | Complexity Δ (Sum/Max): 16/5 | Churn Δ: 131 | Churn Cumulative: 5739 | Contributors (this commit): 24 | Commits (past 90d): 217 | Contributors (cumulative): 121 | DMM Complexity: 1.0\n\nDIFF:\n@@ -37,3 +37,4 @@ from keras.src.activations.activations import sparsemax\n from keras.src.activations.activations import squareplus\n from keras.src.activations.activations import tanh\n from keras.src.activations.activations import tanh_shrink\n+from keras.src.activations.activations import threshold\n\n@@ -103,6 +103,7 @@ from keras.src.ops.nn import sparse_plus\n from keras.src.ops.nn import sparsemax\n from keras.src.ops.nn import squareplus\n from keras.src.ops.nn import tanh_shrink\n+from keras.src.ops.nn import threshold\n from keras.src.ops.numpy import abs\n from keras.src.ops.numpy import absolute\n from keras.src.ops.numpy import add\n\n@@ -48,3 +48,4 @@ from keras.src.ops.nn import sparse_plus\n from keras.src.ops.nn import sparsemax\n from keras.src.ops.nn import squareplus\n from keras.src.ops.nn import tanh_shrink\n+from keras.src.ops.nn import threshold\n\n@@ -37,3 +37,4 @@ from keras.src.activations.activations import sparsemax\n from keras.src.activations.activations import squareplus\n from keras.src.activations.activations import tanh\n from keras.src.activations.activations import tanh_shrink\n+from keras.src.activations.activations import threshold\n\n@@ -103,6 +103,7 @@ from keras.src.ops.nn import sparse_plus\n from keras.src.ops.nn import sparsemax\n from keras.src.ops.nn import squareplus\n from keras.src.ops.nn import tanh_shrink\n+from keras.src.ops.nn import threshold\n from keras.src.ops.numpy import abs\n from keras.src.ops.numpy import absolute\n from keras.src.ops.numpy import add\n\n@@ -48,3 +48,4 @@ from keras.src.ops.nn import sparse_plus\n from keras.src.ops.nn import sparsemax\n from keras.src.ops.nn import squareplus\n from keras.src.ops.nn import tanh_shrink\n+from keras.src.ops.nn import threshold\n\n@@ -28,6 +28,7 @@ from keras.src.activations.activations import sparsemax\n from keras.src.activations.activations import squareplus\n from keras.src.activations.activations import tanh\n from keras.src.activations.activations import tanh_shrink\n+from keras.src.activations.activations import threshold\n from keras.src.api_export import keras_export\n from keras.src.saving import object_registration\n from keras.src.saving import serialization_lib\n@@ -50,6 +51,7 @@ ALL_OBJECTS = {\n     glu,\n     tanh,\n     tanh_shrink,\n+    threshold,\n     sigmoid,\n     exponential,\n     hard_sigmoid,\n\n@@ -460,6 +460,24 @@ def hard_shrink(x, threshold=0.5):\n     return ops.hard_shrink(x, threshold=threshold)\n \n \n+@keras_export(\"keras.activations.threshold\")\n+def threshold(x, threshold, default_value):\n+    \"\"\"Threshold activation function.\n+\n+    It is defined as:\n+\n+    `threshold(x) = x` if `x > threshold`,\n+    `threshold(x) = default_value` otherwise.\n+\n+    Args:\n+        x: Input tensor.\n+        threshold: The value that decides when to retain or replace x.\n+        default_value: Value to assign when `x <= threshold`.\n+\n+    \"\"\"\n+    return ops.threshold(x, threshold, default_value)\n+\n+\n @keras_export(\"keras.activations.sigmoid\")\n def sigmoid(x):\n     \"\"\"Sigmoid activation function.\n\n@@ -683,6 +683,17 @@ class ActivationsTest(testing.TestCase):\n         expected = hard_shrink(x)\n         self.assertAllClose(result, expected, rtol=1e-05)\n \n+    def test_threshold(self):\n+        def threshold(x, threshold_value, value):\n+            return np.where(\n+                x > threshold_value, x, np.array(value, dtype=x.dtype)\n+            )\n+\n+        x = np.random.random((2, 5))\n+        result = activations.threshold(x[np.newaxis, :], 0, 0)[0]\n+        expected = threshold(x, 0, 0)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n     def test_squareplus(self):\n         def squareplus(x, b=4):\n             y = x + np.sqrt(x**2 + b)\n\n@@ -132,6 +132,11 @@ def hard_shrink(x, threshold=0.5):\n     return jnp.where(jnp.abs(x) > threshold, x, 0.0)\n \n \n+def threshold(x, threshold, default_value):\n+    x = convert_to_tensor(x)\n+    return jnp.where(x > threshold, x, default_value)\n+\n+\n def softmax(x, axis=-1):\n     x = convert_to_tensor(x)\n     return jnn.softmax(x, axis=axis)\n\n@@ -180,6 +180,11 @@ def hard_shrink(x, threshold=0.5):\n     )\n \n \n+def threshold(x, threshold, default_value):\n+    x = convert_to_tensor(x)\n+    return np.where(x > threshold, x, np.array(default_value, dtype=x.dtype))\n+\n+\n def softmax(x, axis=None):\n     exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n     return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n\n@@ -127,6 +127,10 @@ def hard_shrink(x, threshold=0.5):\n     return tf.where(tf.abs(x) > threshold, x, tf.zeros_like(x))\n \n \n+def threshold(x, threshold, default_value):\n+    return tf.where(x > threshold, x, default_value)\n+\n+\n def softmax(x, axis=-1):\n     logits = x\n     if axis is None:\n\n@@ -134,6 +134,11 @@ def hard_shrink(x, threshold=0.5):\n     return tnn.hardshrink(x, lambd=threshold)\n \n \n+def threshold(x, threshold, default_value):\n+    x = convert_to_tensor(x)\n+    return tnn.threshold(x, threshold=threshold, value=default_value)\n+\n+\n def softmax(x, axis=-1):\n     x = convert_to_tensor(x)\n     dtype = backend.standardize_dtype(x.dtype)\n\n@@ -818,6 +818,48 @@ def hard_shrink(x, threshold=0.5):\n     return backend.nn.hard_shrink(x, threshold)\n \n \n+class Threshold(Operation):\n+    def __init__(self, threshold_value, value):\n+        super().__init__()\n+        self.threshold_value = threshold_value\n+        self.value = value\n+\n+    def call(self, x):\n+        return backend.nn.threshold(x, self.threshold_value, self.value)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, dtype=x.dtype)\n+\n+\n+@keras_export([\"keras.ops.threshold\", \"keras.ops.nn.threshold\"])\n+def threshold(x, threshold, default_value):\n+    \"\"\"Threshold activation function.\n+\n+    The function thresholds the input `x` as follows:\n+    `f(x) = x` if `x > threshold`,\n+    `f(x) = default_value` otherwise.\n+\n+    Args:\n+        x: Input tensor.\n+        threshold: The value that decides when to retain or replace x.\n+        default_value: Value to assign when `x <= threshold`.\n+\n+    Returns:\n+        A tensor with the same shape as `x`.\n+\n+    Example:\n+\n+    >>> x = np.array([-1.0, 0.0, 1.0, 2.0])\n+    >>> x_threshold = keras.ops.threshold(x, 1, 0)\n+    >>> print(x_threshold)\n+    array([0., 0., 0., 2.], shape=(4,), dtype=float64)\n+\n+    \"\"\"\n+    if any_symbolic_tensors((x,)):\n+        return Threshold(threshold, default_value).symbolic_call(x)\n+    return backend.nn.threshold(x, threshold, default_value)\n+\n+\n class Softmax(Operation):\n     def __init__(self, axis=-1):\n         super().__init__()\n\n@@ -160,6 +160,10 @@ class NNOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor([None, 2, 3])\n         self.assertEqual(knn.hard_shrink(x).shape, (None, 2, 3))\n \n+    def test_threshld(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.threshold(x, 0, 0).shape, (None, 2, 3))\n+\n     def test_squareplus(self):\n         x = KerasTensor([None, 2, 3])\n         self.assertEqual(knn.squareplus(x).shape, (None, 2, 3))\n@@ -841,6 +845,10 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor([1, 2, 3])\n         self.assertEqual(knn.hard_shrink(x).shape, (1, 2, 3))\n \n+    def test_threshold(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.threshold(x, 0, 0).shape, (1, 2, 3))\n+\n     def test_squareplus(self):\n         x = KerasTensor([1, 2, 3])\n         self.assertEqual(knn.squareplus(x).shape, (1, 2, 3))\n@@ -1398,6 +1406,13 @@ class NNOpsCorrectnessTest(testing.TestCase):\n             [0.0, 0.0, 1.0, 2.0, 3.0],\n         )\n \n+    def test_threshold(self):\n+        x = np.array([-0.5, 0, 1, 2, 3], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.threshold(x, 0, 0),\n+            [0.0, 0.0, 1.0, 2.0, 3.0],\n+        )\n+\n     def test_squareplus(self):\n         x = np.array([-0.5, 0, 1, 2, 3], dtype=np.float32)\n         self.assertAllClose(\n@@ -2584,6 +2599,24 @@ class NNOpsDtypeTest(testing.TestCase):\n             expected_dtype,\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=FLOAT_DTYPES))\n+    def test_threshold(self, dtype):\n+        import torch\n+        import torch.nn.functional as tnn\n+\n+        x = knp.ones((1), dtype=dtype)\n+        x_torch = torch.ones(1, dtype=getattr(torch, dtype))\n+        expected_dtype = standardize_dtype(tnn.threshold(x_torch, 0, 0).dtype)\n+\n+        self.assertEqual(\n+            standardize_dtype(knn.threshold(x, 0, 0).dtype),\n+            expected_dtype,\n+        )\n+        self.assertEqual(\n+            standardize_dtype(knn.Threshold(0, 0).symbolic_call(x).dtype),\n+            expected_dtype,\n+        )\n+\n     @parameterized.named_parameters(named_product(dtype=FLOAT_DTYPES))\n     def test_soft_shrink(self, dtype):\n         import torch\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7943660b9d436d75bd4d816506c3262da63a22a7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 35 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -128,7 +128,7 @@ def make_class_weight_map_fn(class_weight):\n         if y.shape.rank >= 2:\n             y_classes = tf.__internal__.smart_cond.smart_cond(\n                 tf.shape(y)[-1] > 1,\n-                lambda: tf.argmax(y, axis=-1),\n+                lambda: tf.argmax(y, axis=-1, output_type=tf.int32),\n                 lambda: tf.cast(tf.round(tf.squeeze(y, axis=-1)), tf.int32),\n             )\n         else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b02085b9941d68f172e34645b02bdeb5bdd81bc0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 80 | Lines Deleted: 7 | Files Changed: 4 | Hunks: 13 | Methods Changed: 6 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 87 | Churn Cumulative: 966 | Contributors (this commit): 10 | Commits (past 90d): 31 | Contributors (cumulative): 23 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,8 +3,24 @@ from keras.src.backend.common.tensor_attributes import set_tensor_attr\n \n \n def set_keras_mask(x, mask):\n-    return set_tensor_attr(x, \"_keras_mask\", mask)\n+    \"\"\"Sets the Keras mask attribute for the given tensor in-place.\n+\n+    Args:\n+        x: Input tensor.\n+        mask: The mask tensor to be set. If `None`, the `_keras_mask` attribute\n+            will be cleared.\n+    \"\"\"\n+    set_tensor_attr(x, \"_keras_mask\", mask)\n \n \n def get_keras_mask(x):\n+    \"\"\"Gets the Keras mask attribute from the given tensor.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        The mask tensor associated with the input tensor, or `None` if no mask\n+        has been set.\n+    \"\"\"\n     return get_tensor_attr(x, \"_keras_mask\")\n\n@@ -528,6 +528,14 @@ class MultiHeadAttention(Layer):\n         self._return_attention_scores = return_attention_scores\n         if key is None:\n             key = value\n+\n+        # Delete the masks because the masks are handled at the level of the\n+        # layer\n+        query_mask = backend.get_keras_mask(query)\n+        backend.set_keras_mask(query, None)\n+        backend.set_keras_mask(value, None)\n+        backend.set_keras_mask(key, None)\n+\n         attention_mask = self._compute_attention_mask(\n             query,\n             value,\n@@ -541,13 +549,13 @@ class MultiHeadAttention(Layer):\n         #   H = `size_per_head`\n \n         # `query` = [B, T, N, H]\n-        query = self._query_dense.call(query)\n+        query = self._query_dense(query)\n \n         # `key` = [B, S, N, H]\n-        key = self._key_dense.call(key)\n+        key = self._key_dense(key)\n \n         # `value` = [B, S, N, H]\n-        value = self._value_dense.call(value)\n+        value = self._value_dense(value)\n         attention_output, attention_scores = self._compute_attention(\n             query,\n             key,\n@@ -555,7 +563,11 @@ class MultiHeadAttention(Layer):\n             attention_mask,\n             training,\n         )\n-        attention_output = self._output_dense.call(attention_output)\n+        attention_output = self._output_dense(attention_output)\n+\n+        # Set mask on output if needed\n+        if query_mask is not None:\n+            backend.set_keras_mask(attention_output, query_mask)\n \n         if return_attention_scores:\n             return attention_output, attention_scores\n\n@@ -1,4 +1,5 @@\n import os\n+import warnings\n \n import numpy as np\n import pytest\n@@ -315,6 +316,7 @@ class MultiHeadAttentionTest(testing.TestCase):\n                 [[1, 2, 3, 0, 0], [3, 3, 1, 1, 2], [1, 0, 0, 0, 0]]\n             )\n             masked_query = layers.Embedding(4, 8, mask_zero=True)(query)\n+            query_mask = backend.get_keras_mask(masked_query)\n             value = np.random.normal(size=(3, 3, 8))\n             output = layer(query=masked_query, value=value)\n         except RuntimeError as e:\n@@ -325,7 +327,7 @@ class MultiHeadAttentionTest(testing.TestCase):\n                     \"PyTorch errors out on GPU: issue to track bug is here \"\n                     \"https://github.com/keras-team/keras/issues/20459\"\n                 )\n-        self.assertAllClose(masked_query._keras_mask, output._keras_mask)\n+        self.assertAllClose(query_mask, output._keras_mask)\n \n     @parameterized.named_parameters((\"causal\", True), (\"not_causal\", 0))\n     @pytest.mark.skipif(\n@@ -373,6 +375,21 @@ class MultiHeadAttentionTest(testing.TestCase):\n         self.assertAllClose(output_1, output_2)\n         self.assertAllClose(output_1, output_3)\n \n+    @pytest.mark.skipif(\n+        backend.backend() == \"numpy\",\n+        reason=\"Numpy backend does not support masking.\",\n+    )\n+    def test_no_warning_with_keras_mask(self):\n+        layer = layers.MultiHeadAttention(num_heads=2, key_dim=2)\n+        query = np.array([[1, 2, 3, 0, 0], [3, 3, 1, 1, 2], [1, 0, 0, 0, 0]])\n+        masked_query = layers.Embedding(4, 8, mask_zero=True)(query)\n+        value = np.array([[5, 4, 0], [3, 0, 0], [2, 1, 1]])\n+        masked_value = layers.Embedding(6, 8, mask_zero=True)(value)\n+\n+        with warnings.catch_warnings(record=True) as warning_logs:\n+            _ = layer(query=masked_query, value=masked_value)\n+            self.assertLen(warning_logs, 0)\n+\n     @parameterized.named_parameters(\n         (\"disable_flash_attention\", False), (\"enable_flash_attention\", True)\n     )\n@@ -646,3 +663,31 @@ class MultiHeadAttentionTest(testing.TestCase):\n     def test_multi_head_attention_output_shape_error(self):\n         with self.assertRaisesRegex(ValueError, r\"Invalid `output_shape`\"):\n             layers.MultiHeadAttention(num_heads=2, key_dim=16, output_shape=8.0)\n+\n+    def test_quantize_int8(self):\n+        query = np.array([[[1.0, 0.0], [0.0, 1.0]]])\n+        key = np.array([[[0.0, 1.0], [1.0, 0.0]]])\n+        value = np.array([[[1.0, 2.0], [3.0, 4.0]]])\n+        layer = layers.MultiHeadAttention(\n+            num_heads=3,\n+            key_dim=8,\n+            use_bias=False,\n+        )\n+        layer.build(query.shape, value.shape, key.shape)\n+        output_float = layer(query, key, value)\n+        for sublayer in layer._flatten_layers():\n+            try:\n+                sublayer.quantize(\"int8\")\n+            except:\n+                pass\n+\n+        # Verify weights dtype\n+        self.assertDType(layer._query_dense._kernel, \"int8\")\n+        self.assertDType(layer._key_dense._kernel, \"int8\")\n+        self.assertDType(layer._value_dense._kernel, \"int8\")\n+        self.assertDType(layer._output_dense._kernel, \"int8\")\n+\n+        # Try eager call and verify output correctness\n+        output_quantized = layer(query, key, value)\n+        mse = ops.mean(ops.square(output_float - output_quantized))\n+        self.assertLess(mse, 1e-3)  # A weak correctness test\n\n@@ -32,7 +32,7 @@ class Masking(Layer):\n     inputs[:, 5, :] = 0.\n \n     model = keras.models.Sequential()\n-    model.add(keras.layers.Masking(mask_value=0.)\n+    model.add(keras.layers.Masking(mask_value=0.0))\n     model.add(keras.layers.LSTM(32))\n     output = model(inputs)\n     # The time step 3 and 5 will be skipped from LSTM calculation.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bcdb8e452a18c0c6038db20802cdb83e9046ca58", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 125 | Lines Deleted: 70 | Files Changed: 2 | Hunks: 20 | Methods Changed: 9 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 195 | Churn Cumulative: 226 | Contributors (this commit): 7 | Commits (past 90d): 4 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -15,14 +15,19 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n             supported. If not specified, `keras.config.floatx()` is used,\n             which defaults to `float32` unless you configured it otherwise (via\n             `keras.config.set_floatx(float_dtype)`).\n-        seed: A Python integer or instance of\n+        seed: Optional Python integer or instance of\n            `keras.random.SeedGenerator`.\n-            Used to make the behavior of the initializer\n-            deterministic. Note that an initializer seeded with an integer\n-            or None (unseeded) will produce the same random values\n-            across multiple calls. To get different random values\n-            across multiple calls, use as seed an instance\n-            of `keras.random.SeedGenerator`.\n+            By default, the `seed` argument is `None`, and an internal global\n+            `keras.random.SeedGenerator` is used. The `seed` argument can be\n+            used to ensure deterministic (repeatable) random number generation.\n+            Note that passing an integer as the `seed` value will produce the\n+            same random values for each call. To generate different random\n+            values for repeated calls, an instance of\n+            `keras.random.SeedGenerator` must be provided as the `seed` value.\n+            Remark concerning the JAX backend: When tracing functions with the\n+            JAX backend the global `keras.random.SeedGenerator` is not\n+            supported. Therefore, during tracing the default value seed=None\n+            will produce an error, and a `seed` argument must be provided.\n     \"\"\"\n     return backend.random.normal(\n         shape, mean=mean, stddev=stddev, dtype=dtype, seed=seed\n@@ -51,14 +56,19 @@ def categorical(logits, num_samples, dtype=\"int32\", seed=None):\n             row of the input. This will be the second dimension of the output\n             tensor's shape.\n         dtype: Optional dtype of the output tensor.\n-        seed: A Python integer or instance of\n+        seed: Optional Python integer or instance of\n            `keras.random.SeedGenerator`.\n-            Used to make the behavior of the initializer\n-            deterministic. Note that an initializer seeded with an integer\n-            or None (unseeded) will produce the same random values\n-            across multiple calls. To get different random values\n-            across multiple calls, use as seed an instance\n-            of `keras.random.SeedGenerator`.\n+            By default, the `seed` argument is `None`, and an internal global\n+            `keras.random.SeedGenerator` is used. The `seed` argument can be\n+            used to ensure deterministic (repeatable) random number generation.\n+            Note that passing an integer as the `seed` value will produce the\n+            same random values for each call. To generate different random\n+            values for repeated calls, an instance of\n+            `keras.random.SeedGenerator` must be provided as the `seed` value.\n+            Remark concerning the JAX backend: When tracing functions with the\n+            JAX backend the global `keras.random.SeedGenerator` is not\n+            supported. Therefore, during tracing the default value seed=None\n+            will produce an error, and a `seed` argument must be provided.\n \n     Returns:\n         A 2-D tensor with (batch_size, num_samples).\n@@ -94,14 +104,19 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n             supported. If not specified, `keras.config.floatx()` is used,\n             which defaults to `float32` unless you configured it otherwise (via\n             `keras.config.set_floatx(float_dtype)`)\n-        seed: A Python integer or instance of\n+        seed: Optional Python integer or instance of\n            `keras.random.SeedGenerator`.\n-            Used to make the behavior of the initializer\n-            deterministic. Note that an initializer seeded with an integer\n-            or None (unseeded) will produce the same random values\n-            across multiple calls. To get different random values\n-            across multiple calls, use as seed an instance\n-            of `keras.random.SeedGenerator`.\n+            By default, the `seed` argument is `None`, and an internal global\n+            `keras.random.SeedGenerator` is used. The `seed` argument can be\n+            used to ensure deterministic (repeatable) random number generation.\n+            Note that passing an integer as the `seed` value will produce the\n+            same random values for each call. To generate different random\n+            values for repeated calls, an instance of\n+            `keras.random.SeedGenerator` must be provided as the `seed` value.\n+            Remark concerning the JAX backend: When tracing functions with the\n+            JAX backend the global `keras.random.SeedGenerator` is not\n+            supported. Therefore, during tracing the default value seed=None\n+            will produce an error, and a `seed` argument must be provided.\n     \"\"\"\n     if dtype and not backend.is_float_dtype(dtype):\n         raise ValueError(\n@@ -133,14 +148,19 @@ def randint(shape, minval, maxval, dtype=\"int32\", seed=None):\n             supported. If not specified, `keras.config.floatx()` is used,\n             which defaults to `float32` unless you configured it otherwise (via\n             `keras.config.set_floatx(float_dtype)`)\n-        seed: A Python integer or instance of\n+        seed: Optional Python integer or instance of\n            `keras.random.SeedGenerator`.\n-            Used to make the behavior of the initializer\n-            deterministic. Note that an initializer seeded with an integer\n-            or None (unseeded) will produce the same random values\n-            across multiple calls. To get different random values\n-            across multiple calls, use as seed an instance\n-            of `keras.random.SeedGenerator`.\n+            By default, the `seed` argument is `None`, and an internal global\n+            `keras.random.SeedGenerator` is used. The `seed` argument can be\n+            used to ensure deterministic (repeatable) random number generation.\n+            Note that passing an integer as the `seed` value will produce the\n+            same random values for each call. To generate different random\n+            values for repeated calls, an instance of\n+            `keras.random.SeedGenerator` must be provided as the `seed` value.\n+            Remark concerning the JAX backend: When tracing functions with the\n+            JAX backend the global `keras.random.SeedGenerator` is not\n+            supported. Therefore, during tracing the default value seed=None\n+            will produce an error, and a `seed` argument must be provided.\n     \"\"\"\n     if dtype and not backend.is_int_dtype(dtype):\n         raise ValueError(\n@@ -169,14 +189,19 @@ def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n             supported. If not specified, `keras.config.floatx()` is used,\n             which defaults to `float32` unless you configured it otherwise (via\n             `keras.config.set_floatx(float_dtype)`)\n-        seed: A Python integer or instance of\n+        seed: Optional Python integer or instance of\n            `keras.random.SeedGenerator`.\n-            Used to make the behavior of the initializer\n-            deterministic. Note that an initializer seeded with an integer\n-            or None (unseeded) will produce the same random values\n-            across multiple calls. To get different random values\n-            across multiple calls, use as seed an instance\n-            of `keras.random.SeedGenerator`.\n+            By default, the `seed` argument is `None`, and an internal global\n+            `keras.random.SeedGenerator` is used. The `seed` argument can be\n+            used to ensure deterministic (repeatable) random number generation.\n+            Note that passing an integer as the `seed` value will produce the\n+            same random values for each call. To generate different random\n+            values for repeated calls, an instance of\n+            `keras.random.SeedGenerator` must be provided as the `seed` value.\n+            Remark concerning the JAX backend: When tracing functions with the\n+            JAX backend the global `keras.random.SeedGenerator` is not\n+            supported. Therefore, during tracing the default value seed=None\n+            will produce an error, and a `seed` argument must be provided.\n     \"\"\"\n     return backend.random.truncated_normal(\n         shape, mean=mean, stddev=stddev, dtype=dtype, seed=seed\n@@ -198,14 +223,19 @@ def shuffle(x, axis=0, seed=None):\n         x: The tensor to be shuffled.\n         axis: An integer specifying the axis along which to shuffle. Defaults to\n             `0`.\n-        seed: A Python integer or instance of\n+        seed: Optional Python integer or instance of\n            `keras.random.SeedGenerator`.\n-            Used to make the behavior of the initializer\n-            deterministic. Note that an initializer seeded with an integer\n-            or None (unseeded) will produce the same random values\n-            across multiple calls. To get different random values\n-            across multiple calls, use as seed an instance\n-            of `keras.random.SeedGenerator`.\n+            By default, the `seed` argument is `None`, and an internal global\n+            `keras.random.SeedGenerator` is used. The `seed` argument can be\n+            used to ensure deterministic (repeatable) random number generation.\n+            Note that passing an integer as the `seed` value will produce the\n+            same random values for each call. To generate different random\n+            values for repeated calls, an instance of\n+            `keras.random.SeedGenerator` must be provided as the `seed` value.\n+            Remark concerning the JAX backend: When tracing functions with the\n+            JAX backend the global `keras.random.SeedGenerator` is not\n+            supported. Therefore, during tracing the default value seed=None\n+            will produce an error, and a `seed` argument must be provided.\n     \"\"\"\n     return backend.random.shuffle(x, axis=axis, seed=seed)\n \n@@ -221,14 +251,19 @@ def gamma(shape, alpha, dtype=None, seed=None):\n             supported. If not specified, `keras.config.floatx()` is used,\n             which defaults to `float32` unless you configured it otherwise (via\n             `keras.config.set_floatx(float_dtype)`).\n-        seed: A Python integer or instance of\n+        seed: Optional Python integer or instance of\n            `keras.random.SeedGenerator`.\n-            Used to make the behavior of the initializer\n-            deterministic. Note that an initializer seeded with an integer\n-            or None (unseeded) will produce the same random values\n-            across multiple calls. To get different random values\n-            across multiple calls, use as seed an instance\n-            of `keras.random.SeedGenerator`.\n+            By default, the `seed` argument is `None`, and an internal global\n+            `keras.random.SeedGenerator` is used. The `seed` argument can be\n+            used to ensure deterministic (repeatable) random number generation.\n+            Note that passing an integer as the `seed` value will produce the\n+            same random values for each call. To generate different random\n+            values for repeated calls, an instance of\n+            `keras.random.SeedGenerator` must be provided as the `seed` value.\n+            Remark concerning the JAX backend: When tracing functions with the\n+            JAX backend the global `keras.random.SeedGenerator` is not\n+            supported. Therefore, during tracing the default value seed=None\n+            will produce an error, and a `seed` argument must be provided.\n     \"\"\"\n     return backend.random.gamma(shape, alpha=alpha, dtype=dtype, seed=seed)\n \n@@ -251,14 +286,19 @@ def binomial(shape, counts, probabilities, dtype=None, seed=None):\n             supported. If not specified, `keras.config.floatx()` is used,\n             which defaults to `float32` unless you configured it otherwise (via\n             `keras.config.set_floatx(float_dtype)`).\n-        seed: A Python integer or instance of\n+        seed: Optional Python integer or instance of\n            `keras.random.SeedGenerator`.\n-            Used to make the behavior of the initializer\n-            deterministic. Note that an initializer seeded with an integer\n-            or None (unseeded) will produce the same random values\n-            across multiple calls. To get different random values\n-            across multiple calls, use as seed an instance\n-            of `keras.random.SeedGenerator`.\n+            By default, the `seed` argument is `None`, and an internal global\n+            `keras.random.SeedGenerator` is used. The `seed` argument can be\n+            used to ensure deterministic (repeatable) random number generation.\n+            Note that passing an integer as the `seed` value will produce the\n+            same random values for each call. To generate different random\n+            values for repeated calls, an instance of\n+            `keras.random.SeedGenerator` must be provided as the `seed` value.\n+            Remark concerning the JAX backend: When tracing functions with the\n+            JAX backend the global `keras.random.SeedGenerator` is not\n+            supported. Therefore, during tracing the default value seed=None\n+            will produce an error, and a `seed` argument must be provided.\n     \"\"\"\n     return backend.random.binomial(\n         shape,\n@@ -286,14 +326,19 @@ def beta(shape, alpha, beta, dtype=None, seed=None):\n             supported. If not specified, `keras.config.floatx()` is used,\n             which defaults to `float32` unless you configured it otherwise (via\n             `keras.config.set_floatx(float_dtype)`).\n-        seed: A Python integer or instance of\n+        seed: Optional Python integer or instance of\n            `keras.random.SeedGenerator`.\n-            Used to make the behavior of the initializer\n-            deterministic. Note that an initializer seeded with an integer\n-            or None (unseeded) will produce the same random values\n-            across multiple calls. To get different random values\n-            across multiple calls, use as seed an instance\n-            of `keras.random.SeedGenerator`.\n+            By default, the `seed` argument is `None`, and an internal global\n+            `keras.random.SeedGenerator` is used. The `seed` argument can be\n+            used to ensure deterministic (repeatable) random number generation.\n+            Note that passing an integer as the `seed` value will produce the\n+            same random values for each call. To generate different random\n+            values for repeated calls, an instance of\n+            `keras.random.SeedGenerator` must be provided as the `seed` value.\n+            Remark concerning the JAX backend: When tracing functions with the\n+            JAX backend the global `keras.random.SeedGenerator` is not\n+            supported. Therefore, during tracing the default value seed=None\n+            will produce an error, and a `seed` argument must be provided.\n     \"\"\"\n     return backend.random.beta(\n         shape=shape, alpha=alpha, beta=beta, dtype=dtype, seed=seed\n\n@@ -11,14 +11,24 @@ from keras.src.utils.naming import auto_name\n \n @keras_export(\"keras.random.SeedGenerator\")\n class SeedGenerator:\n-    \"\"\"Generates variable seeds upon each call to a RNG-using function.\n+    \"\"\"Generates variable seeds upon each call to a function generating\n+    random numbers.\n \n-    In Keras, all RNG-using methods (such as `keras.random.normal()`)\n-    are stateless, meaning that if you pass an integer seed to them\n-    (such as `seed=42`), they will return the same values at each call.\n-    In order to get different values at each call, you must use a\n-    `SeedGenerator` instead as the seed argument. The `SeedGenerator`\n-    object is stateful.\n+    In Keras, all random number generators (such as\n+    `keras.random.normal()`) are stateless, meaning that if you pass an\n+    integer seed to them (such as `seed=42`), they will return the same\n+    values for repeated calls. To get different values for each\n+    call, a `SeedGenerator` providing the state of the random generator\n+    has to be used.\n+    Note that all the random number generators have a default seed of None,\n+    which implies that an internal global SeedGenerator is used.\n+    If you need to decouple the RNG from the global state you can provide\n+    a local `StateGenerator` with either a deterministic or random initial\n+    state.\n+    Remark concerning the JAX backen: Note that the use of a local\n+    `StateGenerator` as seed argument is required for JIT compilation of\n+    RNG with the JAX backend, because the use of global state is not\n+    supported.\n \n     Example:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0b8c6a4add21b4523b61ecf6f53dcffc01363bb5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 230 | Contributors (this commit): 8 | Commits (past 90d): 6 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -26,7 +26,7 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n             `keras.random.SeedGenerator` must be provided as the `seed` value.\n             Remark concerning the JAX backend: When tracing functions with the\n             JAX backend the global `keras.random.SeedGenerator` is not\n-            supported. Therefore, during tracing the default value seed=None\n+            supported. Therefore, during tracing the default value `seed=None`\n             will produce an error, and a `seed` argument must be provided.\n     \"\"\"\n     return backend.random.normal(\n\n@@ -20,11 +20,13 @@ class SeedGenerator:\n     values for repeated calls. To get different values for each\n     call, a `SeedGenerator` providing the state of the random generator\n     has to be used.\n+\n     Note that all the random number generators have a default seed of None,\n     which implies that an internal global SeedGenerator is used.\n     If you need to decouple the RNG from the global state you can provide\n     a local `StateGenerator` with either a deterministic or random initial\n     state.\n+\n     Remark concerning the JAX backen: Note that the use of a local\n     `StateGenerator` as seed argument is required for JIT compilation of\n     RNG with the JAX backend, because the use of global state is not\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ab53ed2013ff59cb67f8eb4bb1a34a13ebed4138", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 171 | Lines Deleted: 0 | Files Changed: 10 | Hunks: 13 | Methods Changed: 10 | Complexity Δ (Sum/Max): 29/15 | Churn Δ: 171 | Churn Cumulative: 4816 | Contributors (this commit): 22 | Commits (past 90d): 108 | Contributors (cumulative): 94 | DMM Complexity: 0.8508771929824561\n\nDIFF:\n@@ -256,6 +256,7 @@ from keras.src.ops.numpy import tril\n from keras.src.ops.numpy import triu\n from keras.src.ops.numpy import true_divide\n from keras.src.ops.numpy import trunc\n+from keras.src.ops.numpy import unravel_index\n from keras.src.ops.numpy import var\n from keras.src.ops.numpy import vdot\n from keras.src.ops.numpy import vectorize\n\n@@ -155,6 +155,7 @@ from keras.src.ops.numpy import tril\n from keras.src.ops.numpy import triu\n from keras.src.ops.numpy import true_divide\n from keras.src.ops.numpy import trunc\n+from keras.src.ops.numpy import unravel_index\n from keras.src.ops.numpy import var\n from keras.src.ops.numpy import vdot\n from keras.src.ops.numpy import vectorize\n\n@@ -256,6 +256,7 @@ from keras.src.ops.numpy import tril\n from keras.src.ops.numpy import triu\n from keras.src.ops.numpy import true_divide\n from keras.src.ops.numpy import trunc\n+from keras.src.ops.numpy import unravel_index\n from keras.src.ops.numpy import var\n from keras.src.ops.numpy import vdot\n from keras.src.ops.numpy import vectorize\n\n@@ -155,6 +155,7 @@ from keras.src.ops.numpy import tril\n from keras.src.ops.numpy import triu\n from keras.src.ops.numpy import true_divide\n from keras.src.ops.numpy import trunc\n+from keras.src.ops.numpy import unravel_index\n from keras.src.ops.numpy import var\n from keras.src.ops.numpy import vdot\n from keras.src.ops.numpy import vectorize\n\n@@ -904,6 +904,11 @@ def ravel(x):\n     return jnp.ravel(x)\n \n \n+def unravel_index(x, shape):\n+    x = convert_to_tensor(x)\n+    return jnp.unravel_index(x, shape)\n+\n+\n @sparse.elementwise_unary(linear=True)\n def real(x):\n     x = convert_to_tensor(x)\n\n@@ -828,6 +828,13 @@ def ravel(x):\n     return np.ravel(x)\n \n \n+def unravel_index(x, shape):\n+    dtype = dtypes.result_type(x.dtype)\n+    return tuple(\n+        indices.astype(dtype) for indices in np.unravel_index(x, shape)\n+    )\n+\n+\n def real(x):\n     return np.real(x)\n \n\n@@ -1855,6 +1855,31 @@ def ravel(x):\n     return tf.reshape(x, [-1])\n \n \n+def unravel_index(x, shape):\n+    x = tf.convert_to_tensor(x)\n+    input_dtype = x.dtype\n+\n+    if None in shape:\n+        raise ValueError(\n+            \"`shape` argument cannot contain `None`. Received: shape={shape}\"\n+        )\n+\n+    if x.ndim == 1:\n+        coords = []\n+        for dim in reversed(shape):\n+            coords.append(tf.cast(x % dim, input_dtype))\n+            x = x // dim\n+        return tuple(reversed(coords))\n+\n+    x_shape = x.shape\n+    coords = []\n+    for dim in shape:\n+        coords.append(tf.reshape(tf.cast(x % dim, input_dtype), x_shape))\n+        x = x // dim\n+\n+    return tuple(reversed(coords))\n+\n+\n @sparse.elementwise_unary\n def real(x):\n     x = convert_to_tensor(x)\n\n@@ -1214,6 +1214,12 @@ def ravel(x):\n     return torch.ravel(x)\n \n \n+def unravel_index(x, shape):\n+    x = convert_to_tensor(x)\n+    dtype = dtypes.result_type(x.dtype)\n+    return tuple(cast(idx, dtype) for idx in torch.unravel_index(x, shape))\n+\n+\n def real(x):\n     if not isinstance(x, torch.Tensor):\n         x = torch.from_numpy(x)  # needed for complex type conversion\n\n@@ -4659,6 +4659,57 @@ def ravel(x):\n     return backend.numpy.ravel(x)\n \n \n+class UnravelIndex(Operation):\n+    def __init__(self, shape):\n+        self.shape = shape\n+        self._inbound_nodes = []\n+\n+    def call(self, indices):\n+        return backend.numpy.unravel_index(indices, self.shape)\n+\n+    def compute_output_spec(self, indices):\n+        if None in self.shape:\n+            output_shapes = [[None] for _ in self.shape]\n+        else:\n+            if isinstance(indices, int):\n+                output_shapes = [[1] for _ in self.shape]\n+            elif hasattr(indices, \"shape\"):\n+                output_shapes = [list(indices.shape) for _ in self.shape]\n+            else:\n+                try:\n+                    indices_shape = np.shape(indices)\n+                    output_shapes = [list(indices_shape) for _ in self.shape]\n+                except Exception:\n+                    output_shapes = [[None] for _ in self.shape]\n+\n+        return [\n+            KerasTensor(shape, dtype=indices.dtype) for shape in output_shapes\n+        ]\n+\n+\n+@keras_export([\"keras.ops.unravel_index\", \"keras.ops.numpy.unravel_index\"])\n+def unravel_index(indices, shape):\n+    \"\"\"Convert flat indices to coordinate arrays in a given array shape.\n+\n+    Args:\n+        indices: An integer or array of integers representing flat indices.\n+        shape: The shape of the array to unravel into.\n+\n+    Returns:\n+        Tuple of arrays for each dimension with unraveled indices.\n+\n+    Example:\n+        >>> indices = 5\n+        >>> shape = (3, 3)\n+        >>> unravel_index(indices, shape)\n+        (1, 2)  # 5 is at row 1, column 2 in a 3x3 array\n+    \"\"\"\n+    if any_symbolic_tensors((indices,)):\n+        return UnravelIndex(shape).symbolic_call(indices)\n+\n+    return backend.numpy.unravel_index(indices, shape)\n+\n+\n class Real(Operation):\n     def call(self, x):\n         return backend.numpy.real(x)\n\n@@ -1455,6 +1455,26 @@ class NumpyOneInputOpsDynamicShapeTest(testing.TestCase):\n         x = KerasTensor((None, 3))\n         self.assertEqual(knp.ravel(x).shape, (None,))\n \n+    def test_unravel_index(self):\n+        x = KerasTensor((None,))\n+        indices = knp.unravel_index(x, (2, 3))\n+        self.assertEqual(len(indices), 2)\n+        self.assertEqual(indices[0].shape, (None,))\n+        self.assertEqual(indices[1].shape, (None,))\n+\n+        x = KerasTensor((None, 4))\n+        indices = knp.unravel_index(x, (3, 4))\n+        self.assertEqual(len(indices), 2)\n+        self.assertEqual(indices[0].shape, (None, 4))\n+        self.assertEqual(indices[1].shape, (None, 4))\n+\n+        x = KerasTensor((None, 3, 2))\n+        indices = knp.unravel_index(x, (5, 6, 4))\n+        self.assertEqual(len(indices), 3)\n+        self.assertEqual(indices[0].shape, (None, 3, 2))\n+        self.assertEqual(indices[1].shape, (None, 3, 2))\n+        self.assertEqual(indices[2].shape, (None, 3, 2))\n+\n     def test_real(self):\n         x = KerasTensor((None, 3))\n         self.assertEqual(knp.real(x).shape, (None, 3))\n@@ -1999,6 +2019,19 @@ class NumpyOneInputOpsStaticShapeTest(testing.TestCase):\n         x = KerasTensor((2, 3))\n         self.assertEqual(knp.ravel(x).shape, (6,))\n \n+    def test_unravel_index(self):\n+        x = KerasTensor((6,))\n+        indices = knp.unravel_index(x, (2, 3))\n+        self.assertEqual(len(indices), 2)\n+        self.assertEqual(indices[0].shape, (6,))\n+        self.assertEqual(indices[1].shape, (6,))\n+\n+        x = KerasTensor((2, 3))\n+        indices = knp.unravel_index(x, (3, 4))\n+        self.assertEqual(len(indices), 2)\n+        self.assertEqual(indices[0].shape, (2, 3))\n+        self.assertEqual(indices[1].shape, (2, 3))\n+\n     def test_real(self):\n         x = KerasTensor((2, 3))\n         self.assertEqual(knp.real(x).shape, (2, 3))\n@@ -4114,6 +4147,19 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(knp.ravel(x), np.ravel(x))\n         self.assertAllClose(knp.Ravel()(x), np.ravel(x))\n \n+    def test_unravel_index(self):\n+        x = np.array([0, 1, 2, 3])\n+        shape = (2, 2)\n+        self.assertAllClose(\n+            knp.unravel_index(x, shape), np.unravel_index(x, shape)\n+        )\n+\n+        x = np.array([[0, 1], [2, 3]])\n+        shape = (2, 2)\n+        self.assertAllClose(\n+            knp.unravel_index(x, shape), np.unravel_index(x, shape)\n+        )\n+\n     def test_real(self):\n         x = np.array([[1, 2, 3 - 3j], [3, 2, 1 + 5j]])\n         self.assertAllClose(knp.real(x), np.real(x))\n@@ -7789,6 +7835,33 @@ class NumpyDtypeTest(testing.TestCase):\n             expected_dtype,\n         )\n \n+    @parameterized.named_parameters(named_product(dtype=INT_DTYPES))\n+    def test_unravel_index(self, dtype):\n+        import jax.numpy as jnp\n+\n+        x = knp.ones((3,), dtype=dtype)\n+        x_jax = jnp.ones((3,), dtype=dtype)\n+\n+        indices = knp.array([2, 0], dtype=dtype)\n+        indices_jax = jnp.array([2, 0], dtype=dtype)\n+\n+        unravel_result_knp = knp.unravel_index(indices, x.shape)\n+        unravel_result_jax = jnp.unravel_index(indices_jax, x_jax.shape)\n+\n+        expected_dtype_knp = standardize_dtype(unravel_result_knp[0].dtype)\n+        expected_dtype_jax = standardize_dtype(unravel_result_jax[0].dtype)\n+\n+        self.assertEqual(expected_dtype_knp, expected_dtype_jax)\n+\n+        unravel_result_knp_symbolic = knp.UnravelIndex(x.shape).symbolic_call(\n+            indices\n+        )\n+        expected_dtype_symbolic = standardize_dtype(\n+            unravel_result_knp_symbolic[0].dtype\n+        )\n+\n+        self.assertEqual(expected_dtype_symbolic, expected_dtype_jax)\n+\n     @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n     def test_repeat(self, dtype):\n         import jax.numpy as jnp\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fa5b722b9184acc6301953089659cd1bb28e3ad0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 32 | Churn Cumulative: 148 | Contributors (this commit): 9 | Commits (past 90d): 9 | Contributors (cumulative): 14 | DMM Complexity: 0.2916666666666667\n\nDIFF:\n@@ -1,4 +1,3 @@\n-from keras.src.backend.common.name_scope import name_scope\n from keras.src.backend.jax import core\n from keras.src.backend.jax import distribution_lib\n from keras.src.backend.jax import image\n@@ -18,6 +17,7 @@ from keras.src.backend.jax.core import convert_to_numpy\n from keras.src.backend.jax.core import convert_to_tensor\n from keras.src.backend.jax.core import device_scope\n from keras.src.backend.jax.core import is_tensor\n+from keras.src.backend.jax.core import name_scope\n from keras.src.backend.jax.core import random_seed_dtype\n from keras.src.backend.jax.core import scatter\n from keras.src.backend.jax.core import shape\n\n@@ -9,6 +9,7 @@ from keras.src.backend.common import KerasVariable\n from keras.src.backend.common import global_state\n from keras.src.backend.common import standardize_dtype\n from keras.src.backend.common.keras_tensor import KerasTensor\n+from keras.src.backend.common.name_scope import name_scope as base_name_scope\n from keras.src.backend.common.stateless_scope import StatelessScope\n from keras.src.backend.common.symbolic_scope import SymbolicScope\n from keras.src.backend.jax import distribution_lib\n@@ -362,6 +363,35 @@ def custom_gradient(fun):\n     return jax.custom_gradient(fun=fun)\n \n \n+class name_scope(base_name_scope):\n+    def __init__(self, name, **kwargs):\n+        super().__init__(name, **kwargs)\n+        self._jax_name_scope = jax.named_scope(name)\n+\n+    def __enter__(self):\n+        name_scope_stack = global_state.get_global_attribute(\n+            \"name_scope_stack\", default=[], set_to_default=True\n+        )\n+        if self.deduplicate and name_scope_stack:\n+            parent_caller = name_scope_stack[-1].caller\n+            parent_name = name_scope_stack[-1].name\n+            if (\n+                self.caller is not None\n+                and self.caller is parent_caller\n+                and self.name == parent_name\n+            ):\n+                return self\n+        name_scope_stack.append(self)\n+        self._pop_on_exit = True\n+        self._jax_name_scope.__enter__()\n+        return self\n+\n+    def __exit__(self, *args, **kwargs):\n+        super().__exit__(*args, **kwargs)\n+        if self._pop_on_exit:\n+            self._jax_name_scope.__exit__(*args, **kwargs)\n+\n+\n def device_scope(device_name):\n     if isinstance(device_name, str):\n         # We support string value like \"cpu:0\", \"gpu:1\", etc.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
