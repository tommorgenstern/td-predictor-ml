{"custom_id": "keras#cbdb18e87ab5681e4247b6ee5d040793591df1dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 1 | Churn Cumulative: 1150 | Contributors (this commit): 8 | Commits (past 90d): 5 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -1112,6 +1112,7 @@ def test(states):\n         \"Expected `states` to be list or tuple\"\n     )\n \n+\n def _to_list(ls):\n     if isinstance(ls, list):\n         return ls\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#04e4017d8bb77e31184b54e24478dc926906d20f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 1433 | Contributors (this commit): 13 | Commits (past 90d): 2 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -479,7 +479,10 @@ class NormalizationAdaptTest(\n         self.assertAllClose(output_data, expected_output)\n \n         # Save the model to disk.\n-        output_path = os.path.join(self.get_temp_dir(), \"tf_keras_saved_model_invert\")\n+        output_path = os.path.join(\n+            self.get_temp_dir(),\n+            \"tf_keras_saved_model_invert\"\n+        )\n         model.save(output_path, save_format=save_format)\n         loaded_model = keras.models.load_model(\n             output_path, custom_objects={\"Normalization\": cls}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2db1bc03c784409f66e5a5873aa92d841b1aac36", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 4 | Files Changed: 4 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 66/46 | Churn Δ: 24 | Churn Cumulative: 21047 | Contributors (this commit): 140 | Commits (past 90d): 18 | Contributors (cumulative): 143 | DMM Complexity: None\n\nDIFF:\n@@ -3419,10 +3419,10 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n         ```\n \n         If you would like to customize your serving endpoints, you can\n-        use the lower-level `keras.saving.ExportArchive` class. The `export()`\n+        use the lower-level `keras.export.ExportArchive` class. The `export()`\n         method relies on `ExportArchive` internally.\n         \"\"\"\n-        from keras.saving import export_lib\n+        from keras.export import export_lib\n \n         export_lib.export_model(self, filepath)\n \n\n@@ -0,0 +1,16 @@\n+# Copyright 2023 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+from keras.export.export_lib import ExportArchive\n\n@@ -23,7 +23,7 @@ from keras.engine import sequential\n from keras.utils import io_utils\n \n \n-@keras_export(\"keras.saving.ExportArchive\")\n+@keras_export(\"keras.export.ExportArchive\")\n class ExportArchive(tf.__internal__.tracking.AutoTrackable):\n     \"\"\"ExportArchive is used to write SavedModel artifacts (e.g. for inference).\n \n\n@@ -20,7 +20,7 @@ import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n \n import keras\n-from keras.saving import export_lib\n+from keras.export import export_lib\n from keras.testing_infra import test_combinations\n from keras.testing_infra import test_utils\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b4d94fec82d083e31bdda92b0a0c20488bd9623e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 162 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -496,10 +496,9 @@ def deserialize_keras_object(\n \n     # Instantiate the class from its config inside a custom object scope\n     # so that we can catch any custom objects that the config refers to.\n-    with (\n-        object_registration.custom_object_scope(custom_objects),\n-        SafeModeScope(safe_mode),\n-    ):\n+    custom_obj_scope = object_registration.custom_object_scope(custom_objects)\n+    safe_mode_scope = SafeModeScope(safe_mode)\n+    with custom_obj_scope, safe_mode_scope:\n         instance = cls.from_config(inner_config)\n         build_config = config.get(\"build_config\", None)\n         if build_config:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a58282c603b233e189c791f610d81740e173ac8a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 693 | Contributors (this commit): 8 | Commits (past 90d): 4 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -26,6 +26,7 @@ from keras.testing_infra import test_combinations\n \n # isort: off\n from tensorflow.python.framework import type_spec\n+from tensorflow.python.framework import type_spec_registry\n \n \n class TwoTensors(tf.__internal__.CompositeTensor):\n@@ -67,7 +68,7 @@ def as_shape(shape):\n         return tf.TensorShape(shape)\n \n \n-@type_spec.register(\"tf.TwoTensorsSpec\")\n+@type_spec_registry.register(\"tf.TwoTensorsSpec\")\n class TwoTensorsSpecNoOneDtype(tf.TypeSpec):\n     \"\"\"A TypeSpec for the TwoTensors value type.\"\"\"\n \n\n@@ -33,7 +33,7 @@ import wrapt\n from keras.saving.legacy import serialization\n \n # isort: off\n-from tensorflow.python.framework import type_spec\n+from tensorflow.python.framework import type_spec_registry\n \n _EXTENSION_TYPE_SPEC = \"_EXTENSION_TYPE_SPEC\"\n \n@@ -108,7 +108,7 @@ def _decode_helper(\n         if obj[\"class_name\"] == \"TensorShape\":\n             return tf.TensorShape(obj[\"items\"])\n         elif obj[\"class_name\"] == \"TypeSpec\":\n-            return type_spec.lookup(obj[\"type_spec\"])._deserialize(\n+            return type_spec_registry.lookup(obj[\"type_spec\"])._deserialize(\n                 _decode_helper(obj[\"serialized\"])\n             )\n         elif obj[\"class_name\"] == \"CompositeTensor\":\n@@ -195,7 +195,7 @@ def get_json_type(obj):\n \n     if isinstance(obj, tf.TypeSpec):\n         try:\n-            type_spec_name = type_spec.get_name(type(obj))\n+            type_spec_name = type_spec_registry.get_name(type(obj))\n             return {\n                 \"class_name\": \"TypeSpec\",\n                 \"type_spec\": type_spec_name,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0eae2e38a9c4dcc714509467efd5394ce32959c8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 11 | Churn Cumulative: 73 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -15,6 +15,7 @@\n \"\"\"Base class of optimizer.\"\"\"\n \n import abc\n+import platform\n import re\n \n import tensorflow.compat.v2 as tf\n@@ -63,6 +64,16 @@ class _BaseOptimizer(tf.__internal__.tracking.AutoTrackable):\n             self.jit_compile = True\n         else:\n             self.jit_compile = False\n+\n+        if platform.system() == \"Darwin\" and platform.processor() == \"arm\":\n+            logging.warning(\n+                \"At this time, the v2.11+ optimizer \"\n+                f\"`tf.keras.optimizers.{self.__class__.__name__}` runs slowly \"\n+                \"on M1/M2 Macs, please use the legacy Keras optimizer \"\n+                \"instead, located at \"\n+                f\"`tf.keras.optimizers.legacy.{self.__class__.__name__}`.\"\n+            )\n+\n         if use_ema:\n             # Verify the arguments related to EMA.\n             if ema_momentum > 1 or ema_momentum < 0:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#06d7947a93050d8cfc908c0c304cc62f8c778eab", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6664 | Lines Deleted: 6388 | Files Changed: 15 | Hunks: 23 | Methods Changed: 275 | Complexity Δ (Sum/Max): 155/219 | Churn Δ: 13052 | Churn Cumulative: 18817 | Contributors (this commit): 13 | Commits (past 90d): 16 | Contributors (cumulative): 37 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,9 +14,10 @@\n # ==============================================================================\n \"\"\"All Keras metrics.\"\"\"\n \n+# isort: off\n+from tensorflow.python.util.tf_export import keras_export\n \n-# Utilities\n-# Base classes\n+# Base classes and utilities\n from keras.metrics.base_metric import Mean\n from keras.metrics.base_metric import MeanMetricWrapper\n from keras.metrics.base_metric import MeanTensor\n@@ -28,75 +29,86 @@ from keras.metrics.base_metric import SumOverBatchSizeMetricWrapper\n from keras.metrics.base_metric import clone_metric\n from keras.metrics.base_metric import clone_metrics\n \n-# Metric functions\n-# Individual metric classes\n-from keras.metrics.metrics import AUC\n-from keras.metrics.metrics import Accuracy\n-from keras.metrics.metrics import BinaryAccuracy\n-from keras.metrics.metrics import BinaryCrossentropy\n-from keras.metrics.metrics import BinaryIoU\n-from keras.metrics.metrics import CategoricalAccuracy\n-from keras.metrics.metrics import CategoricalCrossentropy\n-from keras.metrics.metrics import CategoricalHinge\n-from keras.metrics.metrics import CosineSimilarity\n-from keras.metrics.metrics import FalseNegatives\n-from keras.metrics.metrics import FalsePositives\n-from keras.metrics.metrics import Hinge\n-from keras.metrics.metrics import IoU\n-from keras.metrics.metrics import KLDivergence\n-from keras.metrics.metrics import LogCoshError\n-from keras.metrics.metrics import MeanAbsoluteError\n-from keras.metrics.metrics import MeanAbsolutePercentageError\n-from keras.metrics.metrics import MeanIoU\n-from keras.metrics.metrics import MeanRelativeError\n-from keras.metrics.metrics import MeanSquaredError\n-from keras.metrics.metrics import MeanSquaredLogarithmicError\n-from keras.metrics.metrics import OneHotIoU\n-from keras.metrics.metrics import OneHotMeanIoU\n-from keras.metrics.metrics import Poisson\n-from keras.metrics.metrics import Precision\n-from keras.metrics.metrics import PrecisionAtRecall\n-from keras.metrics.metrics import Recall\n-from keras.metrics.metrics import RecallAtPrecision\n-from keras.metrics.metrics import RootMeanSquaredError\n-from keras.metrics.metrics import SensitivityAtSpecificity\n-from keras.metrics.metrics import SensitivitySpecificityBase\n-from keras.metrics.metrics import SparseCategoricalAccuracy\n-from keras.metrics.metrics import SparseCategoricalCrossentropy\n-from keras.metrics.metrics import SparseTopKCategoricalAccuracy\n-from keras.metrics.metrics import SpecificityAtSensitivity\n-from keras.metrics.metrics import SquaredHinge\n-from keras.metrics.metrics import TopKCategoricalAccuracy\n-from keras.metrics.metrics import TrueNegatives\n-from keras.metrics.metrics import TruePositives\n-from keras.metrics.metrics import _ConfusionMatrixConditionCount\n-from keras.metrics.metrics import _IoUBase\n-from keras.metrics.metrics import accuracy\n-from keras.metrics.metrics import binary_accuracy\n-from keras.metrics.metrics import binary_crossentropy\n-from keras.metrics.metrics import categorical_accuracy\n-from keras.metrics.metrics import categorical_crossentropy\n-from keras.metrics.metrics import categorical_hinge\n-from keras.metrics.metrics import cosine_similarity\n-from keras.metrics.metrics import hinge\n-from keras.metrics.metrics import kullback_leibler_divergence\n-from keras.metrics.metrics import logcosh\n-from keras.metrics.metrics import mean_absolute_error\n-from keras.metrics.metrics import mean_absolute_percentage_error\n-from keras.metrics.metrics import mean_squared_error\n-from keras.metrics.metrics import mean_squared_logarithmic_error\n-from keras.metrics.metrics import poisson\n-from keras.metrics.metrics import sparse_categorical_accuracy\n-from keras.metrics.metrics import sparse_categorical_crossentropy\n-from keras.metrics.metrics import sparse_top_k_categorical_accuracy\n-from keras.metrics.metrics import squared_hinge\n-from keras.metrics.metrics import top_k_categorical_accuracy\n from keras.saving.legacy import serialization as legacy_serialization\n from keras.saving.legacy.serialization import deserialize_keras_object\n from keras.saving.legacy.serialization import serialize_keras_object\n \n-# isort: off\n-from tensorflow.python.util.tf_export import keras_export\n+# Individual metric classes\n+\n+# Accuracy metrics\n+from keras.metrics.accuracy_metrics import Accuracy\n+from keras.metrics.accuracy_metrics import BinaryAccuracy\n+from keras.metrics.accuracy_metrics import CategoricalAccuracy\n+from keras.metrics.accuracy_metrics import SparseCategoricalAccuracy\n+from keras.metrics.accuracy_metrics import SparseTopKCategoricalAccuracy\n+from keras.metrics.accuracy_metrics import TopKCategoricalAccuracy\n+\n+from keras.metrics.accuracy_metrics import accuracy\n+from keras.metrics.accuracy_metrics import binary_accuracy\n+from keras.metrics.accuracy_metrics import categorical_accuracy\n+from keras.metrics.accuracy_metrics import sparse_categorical_accuracy\n+from keras.metrics.accuracy_metrics import sparse_top_k_categorical_accuracy\n+from keras.metrics.accuracy_metrics import top_k_categorical_accuracy\n+\n+# Probabilistic metrics\n+from keras.metrics.probabilistic_metrics import BinaryCrossentropy\n+from keras.metrics.probabilistic_metrics import CategoricalCrossentropy\n+from keras.metrics.probabilistic_metrics import KLDivergence\n+from keras.metrics.probabilistic_metrics import Poisson\n+from keras.metrics.probabilistic_metrics import SparseCategoricalCrossentropy\n+\n+from keras.metrics.probabilistic_metrics import binary_crossentropy\n+from keras.metrics.probabilistic_metrics import categorical_crossentropy\n+from keras.metrics.probabilistic_metrics import poisson\n+from keras.metrics.probabilistic_metrics import kullback_leibler_divergence\n+from keras.metrics.probabilistic_metrics import sparse_categorical_crossentropy\n+\n+# Regression metrics\n+from keras.metrics.regression_metrics import CosineSimilarity\n+from keras.metrics.regression_metrics import LogCoshError\n+from keras.metrics.regression_metrics import MeanAbsoluteError\n+from keras.metrics.regression_metrics import MeanAbsolutePercentageError\n+from keras.metrics.regression_metrics import MeanRelativeError\n+from keras.metrics.regression_metrics import MeanSquaredError\n+from keras.metrics.regression_metrics import MeanSquaredLogarithmicError\n+from keras.metrics.regression_metrics import RootMeanSquaredError\n+\n+from keras.metrics.regression_metrics import cosine_similarity\n+from keras.metrics.regression_metrics import logcosh\n+from keras.metrics.regression_metrics import mean_absolute_error\n+from keras.metrics.regression_metrics import mean_absolute_percentage_error\n+from keras.metrics.regression_metrics import mean_squared_error\n+from keras.metrics.regression_metrics import mean_squared_logarithmic_error\n+\n+# Confusion metrics\n+from keras.metrics.confusion_metrics import AUC\n+from keras.metrics.confusion_metrics import FalseNegatives\n+from keras.metrics.confusion_metrics import FalsePositives\n+from keras.metrics.confusion_metrics import Precision\n+from keras.metrics.confusion_metrics import PrecisionAtRecall\n+from keras.metrics.confusion_metrics import Recall\n+from keras.metrics.confusion_metrics import RecallAtPrecision\n+from keras.metrics.confusion_metrics import SensitivityAtSpecificity\n+from keras.metrics.confusion_metrics import SensitivitySpecificityBase\n+from keras.metrics.confusion_metrics import SpecificityAtSensitivity\n+from keras.metrics.confusion_metrics import TrueNegatives\n+from keras.metrics.confusion_metrics import TruePositives\n+\n+# IoU metrics\n+from keras.metrics.iou_metrics import BinaryIoU\n+from keras.metrics.iou_metrics import IoU\n+from keras.metrics.iou_metrics import MeanIoU\n+from keras.metrics.iou_metrics import OneHotIoU\n+from keras.metrics.iou_metrics import OneHotMeanIoU\n+\n+# Hinge metrics\n+from keras.metrics.hinge_metrics import CategoricalHinge\n+from keras.metrics.hinge_metrics import Hinge\n+from keras.metrics.hinge_metrics import SquaredHinge\n+\n+from keras.metrics.hinge_metrics import categorical_hinge\n+from keras.metrics.hinge_metrics import squared_hinge\n+from keras.metrics.hinge_metrics import hinge\n \n # Aliases\n acc = ACC = accuracy\n\n@@ -0,0 +1,527 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Accuracy metrics.\"\"\"\n+\n+import tensorflow.compat.v2 as tf\n+\n+from keras import backend\n+from keras.dtensor import utils as dtensor_utils\n+from keras.metrics import base_metric\n+from keras.utils import metrics_utils\n+\n+# isort: off\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export(\"keras.metrics.Accuracy\")\n+class Accuracy(base_metric.MeanMetricWrapper):\n+    \"\"\"Calculates how often predictions equal labels.\n+\n+    This metric creates two local variables, `total` and `count` that are used\n+    to compute the frequency with which `y_pred` matches `y_true`. This\n+    frequency is ultimately returned as `binary accuracy`: an idempotent\n+    operation that simply divides `total` by `count`.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Args:\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.Accuracy()\n+    >>> m.update_state([[1], [2], [3], [4]], [[0], [2], [3], [4]])\n+    >>> m.result().numpy()\n+    0.75\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([[1], [2], [3], [4]], [[0], [2], [3], [4]],\n+    ...                sample_weight=[1, 1, 0, 0])\n+    >>> m.result().numpy()\n+    0.5\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(optimizer='sgd',\n+                  loss='mse',\n+                  metrics=[tf.keras.metrics.Accuracy()])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(self, name=\"accuracy\", dtype=None):\n+        super().__init__(accuracy, name, dtype=dtype)\n+\n+\n+@keras_export(\"keras.metrics.BinaryAccuracy\")\n+class BinaryAccuracy(base_metric.MeanMetricWrapper):\n+    \"\"\"Calculates how often predictions match binary labels.\n+\n+    This metric creates two local variables, `total` and `count` that are used\n+    to compute the frequency with which `y_pred` matches `y_true`. This\n+    frequency is ultimately returned as `binary accuracy`: an idempotent\n+    operation that simply divides `total` by `count`.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Args:\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+      threshold: (Optional) Float representing the threshold for deciding\n+      whether prediction values are 1 or 0.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.BinaryAccuracy()\n+    >>> m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]])\n+    >>> m.result().numpy()\n+    0.75\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]],\n+    ...                sample_weight=[1, 0, 0, 1])\n+    >>> m.result().numpy()\n+    0.5\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(optimizer='sgd',\n+                  loss='mse',\n+                  metrics=[tf.keras.metrics.BinaryAccuracy()])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(self, name=\"binary_accuracy\", dtype=None, threshold=0.5):\n+        super().__init__(\n+            metrics_utils.binary_matches, name, dtype=dtype, threshold=threshold\n+        )\n+\n+\n+@keras_export(\"keras.metrics.CategoricalAccuracy\")\n+class CategoricalAccuracy(base_metric.MeanMetricWrapper):\n+    \"\"\"Calculates how often predictions match one-hot labels.\n+\n+    You can provide logits of classes as `y_pred`, since argmax of\n+    logits and probabilities are same.\n+\n+    This metric creates two local variables, `total` and `count` that are used\n+    to compute the frequency with which `y_pred` matches `y_true`. This\n+    frequency is ultimately returned as `categorical accuracy`: an idempotent\n+    operation that simply divides `total` by `count`.\n+\n+    `y_pred` and `y_true` should be passed in as vectors of probabilities,\n+    rather than as labels. If necessary, use `tf.one_hot` to expand `y_true` as\n+    a vector.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Args:\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.CategoricalAccuracy()\n+    >>> m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8],\n+    ...                 [0.05, 0.95, 0]])\n+    >>> m.result().numpy()\n+    0.5\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8],\n+    ...                 [0.05, 0.95, 0]],\n+    ...                sample_weight=[0.7, 0.3])\n+    >>> m.result().numpy()\n+    0.3\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+      optimizer='sgd',\n+      loss='mse',\n+      metrics=[tf.keras.metrics.CategoricalAccuracy()])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(self, name=\"categorical_accuracy\", dtype=None):\n+        super().__init__(\n+            lambda y_true, y_pred: metrics_utils.sparse_categorical_matches(\n+                tf.math.argmax(y_true, axis=-1), y_pred\n+            ),\n+            name,\n+            dtype=dtype,\n+        )\n+\n+\n+@keras_export(\"keras.metrics.SparseCategoricalAccuracy\")\n+class SparseCategoricalAccuracy(base_metric.MeanMetricWrapper):\n+    \"\"\"Calculates how often predictions match integer labels.\n+\n+    ```python\n+    acc = np.dot(sample_weight, np.equal(y_true, np.argmax(y_pred, axis=1))\n+    ```\n+\n+    You can provide logits of classes as `y_pred`, since argmax of\n+    logits and probabilities are same.\n+\n+    This metric creates two local variables, `total` and `count` that are used\n+    to compute the frequency with which `y_pred` matches `y_true`. This\n+    frequency is ultimately returned as `sparse categorical accuracy`: an\n+    idempotent operation that simply divides `total` by `count`.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Args:\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.SparseCategoricalAccuracy()\n+    >>> m.update_state([[2], [1]], [[0.1, 0.6, 0.3], [0.05, 0.95, 0]])\n+    >>> m.result().numpy()\n+    0.5\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([[2], [1]], [[0.1, 0.6, 0.3], [0.05, 0.95, 0]],\n+    ...                sample_weight=[0.7, 0.3])\n+    >>> m.result().numpy()\n+    0.3\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(self, name=\"sparse_categorical_accuracy\", dtype=None):\n+        super().__init__(\n+            metrics_utils.sparse_categorical_matches, name, dtype=dtype\n+        )\n+\n+\n+_SPARSE_CATEGORICAL_UPDATE_STATE_DOCSTRING = \"\"\"Accumulates metric statistics.\n+\n+For sparse categorical metrics, the shapes of `y_true` and `y_pred` are\n+different.\n+\n+Args:\n+  y_true: Ground truth label values. shape = `[batch_size, d0, .. dN-1]` or\n+    shape = `[batch_size, d0, .. dN-1, 1]`.\n+  y_pred: The predicted probability values. shape = `[batch_size, d0, .. dN]`.\n+  sample_weight: Optional `sample_weight` acts as a\n+    coefficient for the metric. If a scalar is provided, then the metric is\n+    simply scaled by the given value. If `sample_weight` is a tensor of size\n+    `[batch_size]`, then the metric for each sample of the batch is rescaled\n+    by the corresponding element in the `sample_weight` vector. If the shape\n+    of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted\n+    to this shape), then each metric element of `y_pred` is scaled by the\n+    corresponding value of `sample_weight`. (Note on `dN-1`: all metric\n+    functions reduce by 1 dimension, usually the last axis (-1)).\n+\n+Returns:\n+  Update op.\n+\"\"\"\n+\n+SparseCategoricalAccuracy.update_state.__doc__ = (\n+    _SPARSE_CATEGORICAL_UPDATE_STATE_DOCSTRING\n+)\n+\n+\n+@keras_export(\"keras.metrics.TopKCategoricalAccuracy\")\n+class TopKCategoricalAccuracy(base_metric.MeanMetricWrapper):\n+    \"\"\"Computes how often targets are in the top `K` predictions.\n+\n+    Args:\n+      k: (Optional) Number of top elements to look at for computing accuracy.\n+        Defaults to 5.\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.TopKCategoricalAccuracy(k=1)\n+    >>> m.update_state([[0, 0, 1], [0, 1, 0]],\n+    ...                [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])\n+    >>> m.result().numpy()\n+    0.5\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([[0, 0, 1], [0, 1, 0]],\n+    ...                [[0.1, 0.9, 0.8], [0.05, 0.95, 0]],\n+    ...                sample_weight=[0.7, 0.3])\n+    >>> m.result().numpy()\n+    0.3\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(optimizer='sgd',\n+                  loss='mse',\n+                  metrics=[tf.keras.metrics.TopKCategoricalAccuracy()])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(self, k=5, name=\"top_k_categorical_accuracy\", dtype=None):\n+        super().__init__(\n+            lambda yt, yp, k: metrics_utils.sparse_top_k_categorical_matches(\n+                tf.math.argmax(yt, axis=-1), yp, k\n+            ),\n+            name,\n+            dtype=dtype,\n+            k=k,\n+        )\n+\n+\n+@keras_export(\"keras.metrics.SparseTopKCategoricalAccuracy\")\n+class SparseTopKCategoricalAccuracy(base_metric.MeanMetricWrapper):\n+    \"\"\"Computes how often integer targets are in the top `K` predictions.\n+\n+    Args:\n+      k: (Optional) Number of top elements to look at for computing accuracy.\n+        Defaults to 5.\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n+    >>> m.update_state([2, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])\n+    >>> m.result().numpy()\n+    0.5\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([2, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]],\n+    ...                sample_weight=[0.7, 0.3])\n+    >>> m.result().numpy()\n+    0.3\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+      optimizer='sgd',\n+      loss='mse',\n+      metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy()])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(\n+        self, k=5, name=\"sparse_top_k_categorical_accuracy\", dtype=None\n+    ):\n+        super().__init__(\n+            metrics_utils.sparse_top_k_categorical_matches,\n+            name,\n+            dtype=dtype,\n+            k=k,\n+        )\n+\n+\n+SparseTopKCategoricalAccuracy.update_state.__doc__ = (\n+    _SPARSE_CATEGORICAL_UPDATE_STATE_DOCSTRING\n+)\n+\n+\n+def accuracy(y_true, y_pred):\n+    [\n+        y_pred,\n+        y_true,\n+    ], _ = metrics_utils.ragged_assert_compatible_and_get_flat_values(\n+        [y_pred, y_true]\n+    )\n+    y_true.shape.assert_is_compatible_with(y_pred.shape)\n+    if y_true.dtype != y_pred.dtype:\n+        y_pred = tf.cast(y_pred, y_true.dtype)\n+    return tf.cast(tf.equal(y_true, y_pred), backend.floatx())\n+\n+\n+@keras_export(\"keras.metrics.binary_accuracy\")\n+@tf.__internal__.dispatch.add_dispatch_support\n+def binary_accuracy(y_true, y_pred, threshold=0.5):\n+    \"\"\"Calculates how often predictions match binary labels.\n+\n+    Standalone usage:\n+    >>> y_true = [[1], [1], [0], [0]]\n+    >>> y_pred = [[1], [1], [0], [0]]\n+    >>> m = tf.keras.metrics.binary_accuracy(y_true, y_pred)\n+    >>> assert m.shape == (4,)\n+    >>> m.numpy()\n+    array([1., 1., 1., 1.], dtype=float32)\n+\n+    Args:\n+      y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`.\n+      y_pred: The predicted values. shape = `[batch_size, d0, .. dN]`.\n+      threshold: (Optional) Float representing the threshold for deciding\n+        whether prediction values are 1 or 0.\n+\n+    Returns:\n+      Binary accuracy values. shape = `[batch_size, d0, .. dN-1]`\n+    \"\"\"\n+    # Note: calls metrics_utils.binary_matches with mean reduction. This\n+    # maintains public facing binary_accuracy behavior and seperates it from the\n+    # vital behavior of the binary_matches method needed in backend\n+    # dependencies.\n+\n+    return tf.reduce_mean(\n+        metrics_utils.binary_matches(y_true, y_pred, threshold), axis=-1\n+    )\n+\n+\n+@keras_export(\"keras.metrics.categorical_accuracy\")\n+@tf.__internal__.dispatch.add_dispatch_support\n+def categorical_accuracy(y_true, y_pred):\n+    \"\"\"Calculates how often predictions match one-hot labels.\n+\n+    Standalone usage:\n+    >>> y_true = [[0, 0, 1], [0, 1, 0]]\n+    >>> y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]\n+    >>> m = tf.keras.metrics.categorical_accuracy(y_true, y_pred)\n+    >>> assert m.shape == (2,)\n+    >>> m.numpy()\n+    array([0., 1.], dtype=float32)\n+\n+    You can provide logits of classes as `y_pred`, since argmax of\n+    logits and probabilities are same.\n+\n+    Args:\n+      y_true: One-hot ground truth values.\n+      y_pred: The prediction values.\n+\n+    Returns:\n+      Categorical accuracy values.\n+    \"\"\"\n+    # Note: wraps metrics_utils.categorical_matches. This seperates public\n+    # facing categorical_accuracy behavior from the vital behavior of the\n+    # categorical_matches method needed in backend dependencies.\n+\n+    return metrics_utils.sparse_categorical_matches(\n+        tf.math.argmax(y_true, axis=-1), y_pred\n+    )\n+\n+\n+@keras_export(\"keras.metrics.sparse_categorical_accuracy\")\n+@tf.__internal__.dispatch.add_dispatch_support\n+def sparse_categorical_accuracy(y_true, y_pred):\n+    \"\"\"Calculates how often predictions match integer labels.\n+\n+    Standalone usage:\n+    >>> y_true = [2, 1]\n+    >>> y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]\n+    >>> m = tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n+    >>> assert m.shape == (2,)\n+    >>> m.numpy()\n+    array([0., 1.], dtype=float32)\n+\n+    You can provide logits of classes as `y_pred`, since argmax of\n+    logits and probabilities are same.\n+\n+    Args:\n+      y_true: Integer ground truth values.\n+      y_pred: The prediction values.\n+\n+    Returns:\n+      Sparse categorical accuracy values.\n+    \"\"\"\n+    # Note: wraps metrics_utils.sparse_categorical_matches method and checks for\n+    # squeezing to align with expected public facing behavior. This seperates\n+    # public facing sparse_categorical_accuracy behavior from the vital behavior\n+    # of the sparse_categorical_matches method needed in backend dependencies.\n+\n+    matches = metrics_utils.sparse_categorical_matches(y_true, y_pred)\n+\n+    # if shape is (num_samples, 1) squeeze\n+    if matches.shape.ndims > 1 and matches.shape[-1] == 1:\n+        matches = tf.squeeze(matches, [-1])\n+\n+    return matches\n+\n+\n+@keras_export(\"keras.metrics.top_k_categorical_accuracy\")\n+@tf.__internal__.dispatch.add_dispatch_support\n+def top_k_categorical_accuracy(y_true, y_pred, k=5):\n+    \"\"\"Computes how often targets are in the top `K` predictions.\n+\n+    Standalone usage:\n+    >>> y_true = [[0, 0, 1], [0, 1, 0]]\n+    >>> y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]\n+    >>> m = tf.keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=3)\n+    >>> assert m.shape == (2,)\n+    >>> m.numpy()\n+    array([1., 1.], dtype=float32)\n+\n+    Args:\n+      y_true: The ground truth values.\n+      y_pred: The prediction values.\n+      k: (Optional) Number of top elements to look at for computing accuracy.\n+        Defaults to 5.\n+\n+    Returns:\n+      Top K categorical accuracy value.\n+    \"\"\"\n+    # Note: wraps metrics_utils.top_k_categorical_matches. This seperates\n+    # public facing top_k_categorical_accuracy behavior from the vital behavior\n+    # of the top_k_categorical_matches method needed in backend dependencies.\n+\n+    return metrics_utils.sparse_top_k_categorical_matches(\n+        tf.math.argmax(y_true, axis=-1), y_pred, k\n+    )\n+\n+\n+@keras_export(\"keras.metrics.sparse_top_k_categorical_accuracy\")\n+@tf.__internal__.dispatch.add_dispatch_support\n+def sparse_top_k_categorical_accuracy(y_true, y_pred, k=5):\n+    \"\"\"Computes how often integer targets are in the top `K` predictions.\n+\n+    Standalone usage:\n+    >>> y_true = [2, 1]\n+    >>> y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]\n+    >>> m = tf.keras.metrics.sparse_top_k_categorical_accuracy(\n+    ...     y_true, y_pred, k=3)\n+    >>> assert m.shape == (2,)\n+    >>> m.numpy()\n+    array([1., 1.], dtype=float32)\n+\n+    Args:\n+      y_true: tensor of true targets.\n+      y_pred: tensor of predicted targets.\n+      k: (Optional) Number of top elements to look at for computing accuracy.\n+        Defaults to 5.\n+\n+    Returns:\n+      Sparse top K categorical accuracy value.\n+    \"\"\"\n+    # Note: wraps metrics_utils.sparse_top_k_categorical_matches. This seperates\n+    # public facing sparse_top_k_categorical_accuracy behavior from the vital\n+    # behavior of the sparse_top_k_categorical_matches method needed in backend\n+    # dependencies.\n+\n+    return metrics_utils.sparse_top_k_categorical_matches(y_true, y_pred, k)\n\n@@ -0,0 +1,407 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for accuracy metrics.\"\"\"\n+\n+import tensorflow.compat.v2 as tf\n+\n+from keras import Model\n+from keras import layers\n+from keras import metrics\n+from keras.testing_infra import test_combinations\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class AccuracyTest(tf.test.TestCase):\n+    def test_accuracy(self):\n+        acc_obj = metrics.Accuracy(name=\"my_acc\")\n+\n+        # check config\n+        self.assertEqual(acc_obj.name, \"my_acc\")\n+        self.assertTrue(acc_obj.stateful)\n+        self.assertEqual(len(acc_obj.variables), 2)\n+        self.assertEqual(acc_obj.dtype, tf.float32)\n+        self.evaluate(tf.compat.v1.variables_initializer(acc_obj.variables))\n+\n+        # verify that correct value is returned\n+        update_op = acc_obj.update_state(\n+            [[1], [2], [3], [4]], [[1], [2], [3], [4]]\n+        )\n+        self.evaluate(update_op)\n+        result = self.evaluate(acc_obj.result())\n+        self.assertEqual(result, 1)  # 2/2\n+\n+        # Check save and restore config\n+        a2 = metrics.Accuracy.from_config(acc_obj.get_config())\n+        self.assertEqual(a2.name, \"my_acc\")\n+        self.assertTrue(a2.stateful)\n+        self.assertEqual(len(a2.variables), 2)\n+        self.assertEqual(a2.dtype, tf.float32)\n+\n+        # check with sample_weight\n+        result_t = acc_obj([[2], [1]], [[2], [0]], sample_weight=[[0.5], [0.2]])\n+        result = self.evaluate(result_t)\n+        self.assertAlmostEqual(result, 0.96, 2)  # 4.5/4.7\n+\n+    def test_accuracy_ragged(self):\n+        acc_obj = metrics.Accuracy(name=\"my_acc\")\n+        self.evaluate(tf.compat.v1.variables_initializer(acc_obj.variables))\n+\n+        # verify that correct value is returned\n+        rt1 = tf.ragged.constant([[1], [2], [3], [4]])\n+        rt2 = tf.ragged.constant([[1], [2], [3], [4]])\n+        update_op = acc_obj.update_state(rt1, rt2)\n+        self.evaluate(update_op)\n+        result = self.evaluate(acc_obj.result())\n+        self.assertEqual(result, 1)  # 2/2\n+\n+        # check with sample_weight\n+        rt1 = tf.ragged.constant([[2], [1]])\n+        rt2 = tf.ragged.constant([[2], [0]])\n+        sw_ragged = tf.ragged.constant([[0.5], [0.2]])\n+        result_t = acc_obj(rt1, rt2, sample_weight=sw_ragged)\n+        result = self.evaluate(result_t)\n+        self.assertAlmostEqual(result, 0.96, 2)  # 4.5/4.7\n+\n+    def test_binary_accuracy(self):\n+        acc_obj = metrics.BinaryAccuracy(name=\"my_acc\")\n+\n+        # check config\n+        self.assertEqual(acc_obj.name, \"my_acc\")\n+        self.assertTrue(acc_obj.stateful)\n+        self.assertEqual(len(acc_obj.variables), 2)\n+        self.assertEqual(acc_obj.dtype, tf.float32)\n+        self.evaluate(tf.compat.v1.variables_initializer(acc_obj.variables))\n+\n+        # verify that correct value is returned\n+        update_op = acc_obj.update_state([[1], [0]], [[1], [0]])\n+        self.evaluate(update_op)\n+        result = self.evaluate(acc_obj.result())\n+        self.assertEqual(result, 1)  # 2/2\n+\n+        # check y_pred squeeze\n+        update_op = acc_obj.update_state([[1], [1]], [[[1]], [[0]]])\n+        self.evaluate(update_op)\n+        result = self.evaluate(acc_obj.result())\n+        self.assertAlmostEqual(result, 0.75, 2)  # 3/4\n+\n+        # check y_true squeeze\n+        result_t = acc_obj([[[1]], [[1]]], [[1], [0]])\n+        result = self.evaluate(result_t)\n+        self.assertAlmostEqual(result, 0.67, 2)  # 4/6\n+\n+        # check with sample_weight\n+        result_t = acc_obj([[1], [1]], [[1], [0]], [[0.5], [0.2]])\n+        result = self.evaluate(result_t)\n+        self.assertAlmostEqual(result, 0.67, 2)  # 4.5/6.7\n+\n+    def test_binary_accuracy_ragged(self):\n+        acc_obj = metrics.BinaryAccuracy(name=\"my_acc\")\n+        self.evaluate(tf.compat.v1.variables_initializer(acc_obj.variables))\n+\n+        # verify that correct value is returned\n+        rt1 = tf.ragged.constant([[1], [0]])\n+        rt2 = tf.ragged.constant([[1], [0]])\n+        update_op = acc_obj.update_state(rt1, rt2)\n+        self.evaluate(update_op)\n+        result = self.evaluate(acc_obj.result())\n+        self.assertEqual(result, 1)  # 2/2\n+\n+        # check y_true squeeze only supported for dense tensors and is\n+        # not supported by ragged tensor (different ranks). --> error\n+        rt1 = tf.ragged.constant([[[1], [1]]])\n+        rt2 = tf.ragged.constant([[1], [0]])\n+        with self.assertRaises(ValueError):\n+            result_t = acc_obj(rt1, rt2)\n+            result = self.evaluate(result_t)\n+\n+    def test_binary_accuracy_threshold(self):\n+        acc_obj = metrics.BinaryAccuracy(threshold=0.7)\n+        self.evaluate(tf.compat.v1.variables_initializer(acc_obj.variables))\n+        result_t = acc_obj([[1], [1], [0], [0]], [[0.9], [0.6], [0.4], [0.8]])\n+        result = self.evaluate(result_t)\n+        self.assertAlmostEqual(result, 0.5, 2)\n+\n+    def test_binary_accuracy_threshold_ragged(self):\n+        acc_obj = metrics.BinaryAccuracy(threshold=0.7)\n+        self.evaluate(tf.compat.v1.variables_initializer(acc_obj.variables))\n+        rt1 = tf.ragged.constant([[1], [1], [0], [0]])\n+        rt2 = tf.ragged.constant([[0.9], [0.6], [0.4], [0.8]])\n+        result_t = acc_obj(rt1, rt2)\n+        result = self.evaluate(result_t)\n+        self.assertAlmostEqual(result, 0.5, 2)\n+\n+    def test_categorical_accuracy(self):\n+        acc_obj = metrics.CategoricalAccuracy(name=\"my_acc\")\n+\n+        # check config\n+        self.assertEqual(acc_obj.name, \"my_acc\")\n+        self.assertTrue(acc_obj.stateful)\n+        self.assertEqual(len(acc_obj.variables), 2)\n+        self.assertEqual(acc_obj.dtype, tf.float32)\n+        self.evaluate(tf.compat.v1.variables_initializer(acc_obj.variables))\n+\n+        # verify that correct value is returned\n+        update_op = acc_obj.update_state(\n+            [[0, 0, 1], [0, 1, 0]], [[0.1, 0.1, 0.8], [0.05, 0.95, 0]]\n+        )\n+        self.evaluate(update_op)\n+        result = self.evaluate(acc_obj.result())\n+        self.assertEqual(result, 1)  # 2/2\n+\n+        # check with sample_weight\n+        result_t = acc_obj(\n+            [[0, 0, 1], [0, 1, 0]],\n+            [[0.1, 0.1, 0.8], [0.05, 0, 0.95]],\n+            [[0.5], [0.2]],\n+        )\n+        result = self.evaluate(result_t)\n+        self.assertAlmostEqual(result, 0.93, 2)  # 2.5/2.7\n+\n+    def test_categorical_accuracy_ragged(self):\n+        acc_obj = metrics.CategoricalAccuracy(name=\"my_acc\")\n+        self.evaluate(tf.compat.v1.variables_initializer(acc_obj.variables))\n+\n+        # verify that correct value is returned\n+        rt1 = tf.ragged.constant([[0, 0, 1], [0, 1, 0]])\n+        rt2 = tf.ragged.constant([[0.1, 0.1, 0.8], [0.05, 0.95, 0]])\n+        update_op = acc_obj.update_state(rt1, rt2)\n+        self.evaluate(update_op)\n+        result = self.evaluate(acc_obj.result())\n+        self.assertEqual(result, 1)  # 2/2\n+\n+        # check with sample_weight\n+        rt1 = tf.ragged.constant([[0, 0, 1], [0, 1, 0]])\n+        rt2 = tf.ragged.constant([[0.1, 0.1, 0.8], [0.05, 0, 0.95]])\n+        sample_weight = tf.ragged.constant([[0.5], [0.2]])\n+        with self.assertRaises(tf.errors.InvalidArgumentError):\n+            result_t = acc_obj(rt1, rt2, sample_weight)\n+            result = self.evaluate(result_t)\n+\n+    def test_sparse_categorical_accuracy(self):\n+        acc_obj = metrics.SparseCategoricalAccuracy(name=\"my_acc\")\n+\n+        # check config\n+        self.assertEqual(acc_obj.name, \"my_acc\")\n+        self.assertTrue(acc_obj.stateful)\n+        self.assertEqual(len(acc_obj.variables), 2)\n+        self.assertEqual(acc_obj.dtype, tf.float32)\n+        self.evaluate(tf.compat.v1.variables_initializer(acc_obj.variables))\n+\n+        # verify that correct value is returned\n+        update_op = acc_obj.update_state(\n+            [[2], [1]], [[0.1, 0.1, 0.8], [0.05, 0.95, 0]]\n+        )\n+        self.evaluate(update_op)\n+        result = self.evaluate(acc_obj.result())\n+        self.assertEqual(result, 1)  # 2/2\n+\n+        # check with sample_weight\n+        result_t = acc_obj(\n+            [[2], [1]], [[0.1, 0.1, 0.8], [0.05, 0, 0.95]], [[0.5], [0.2]]\n+        )\n+        result = self.evaluate(result_t)\n+        self.assertAlmostEqual(result, 0.93, 2)  # 2.5/2.7\n+\n+    def test_sparse_categorical_accuracy_ragged(self):\n+        acc_obj = metrics.SparseCategoricalAccuracy(name=\"my_acc\")\n+\n+        # verify that correct value is returned\n+        rt1 = tf.ragged.constant([[2], [1]])\n+        rt2 = tf.ragged.constant([[0.1, 0.1, 0.8], [0.05, 0.95, 0]])\n+\n+        with self.assertRaises(tf.errors.InvalidArgumentError):\n+            # sparse_categorical_accuracy is not supported for composite/ragged\n+            # tensors.\n+            update_op = acc_obj.update_state(rt1, rt2)\n+            self.evaluate(update_op)\n+\n+    def test_sparse_categorical_accuracy_mismatched_dims(self):\n+        acc_obj = metrics.SparseCategoricalAccuracy(name=\"my_acc\")\n+\n+        # check config\n+        self.assertEqual(acc_obj.name, \"my_acc\")\n+        self.assertTrue(acc_obj.stateful)\n+        self.assertEqual(len(acc_obj.variables), 2)\n+        self.assertEqual(acc_obj.dtype, tf.float32)\n+        self.evaluate(tf.compat.v1.variables_initializer(acc_obj.variables))\n+\n+        # verify that correct value is returned\n+        update_op = acc_obj.update_state(\n+            [2, 1], [[0.1, 0.1, 0.8], [0.05, 0.95, 0]]\n+        )\n+        self.evaluate(update_op)\n+        result = self.evaluate(acc_obj.result())\n+        self.assertEqual(result, 1)  # 2/2\n+\n+        # check with sample_weight\n+        result_t = acc_obj(\n+            [2, 1], [[0.1, 0.1, 0.8], [0.05, 0, 0.95]], [[0.5], [0.2]]\n+        )\n+        result = self.evaluate(result_t)\n+        self.assertAlmostEqual(result, 0.93, 2)  # 2.5/2.7\n+\n+    def test_sparse_categorical_accuracy_mismatched_dims_dynamic(self):\n+        with tf.compat.v1.get_default_graph().as_default(), self.cached_session() as sess:  # noqa: E501\n+            acc_obj = metrics.SparseCategoricalAccuracy(name=\"my_acc\")\n+            self.evaluate(tf.compat.v1.variables_initializer(acc_obj.variables))\n+\n+            t = tf.compat.v1.placeholder(tf.float32)\n+            p = tf.compat.v1.placeholder(tf.float32)\n+            w = tf.compat.v1.placeholder(tf.float32)\n+\n+            result_t = acc_obj(t, p, w)\n+            result = sess.run(\n+                result_t,\n+                feed_dict=(\n+                    {\n+                        t: [2, 1],\n+                        p: [[0.1, 0.1, 0.8], [0.05, 0, 0.95]],\n+                        w: [[0.5], [0.2]],\n+                    }\n+                ),\n+            )\n+            self.assertAlmostEqual(result, 0.71, 2)  # 2.5/2.7\n+\n+    def test_get_acc(self):\n+        acc_fn = metrics.get(\"acc\")\n+        self.assertEqual(acc_fn, metrics.accuracy)\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class TopKCategoricalAccuracyTest(tf.test.TestCase):\n+    def test_config(self):\n+        a_obj = metrics.TopKCategoricalAccuracy(name=\"topkca\", dtype=tf.int32)\n+        self.assertEqual(a_obj.name, \"topkca\")\n+        self.assertEqual(a_obj._dtype, tf.int32)\n+\n+        a_obj2 = metrics.TopKCategoricalAccuracy.from_config(a_obj.get_config())\n+        self.assertEqual(a_obj2.name, \"topkca\")\n+        self.assertEqual(a_obj2._dtype, tf.int32)\n+\n+    def test_correctness(self):\n+        a_obj = metrics.TopKCategoricalAccuracy()\n+        self.evaluate(tf.compat.v1.variables_initializer(a_obj.variables))\n+        y_true = tf.constant([[0, 0, 1], [0, 1, 0]])\n+        y_pred = tf.constant([[0.1, 0.9, 0.8], [0.05, 0.95, 0]])\n+\n+        result = a_obj(y_true, y_pred)\n+        self.assertEqual(1, self.evaluate(result))  # both the samples match\n+\n+        # With `k` < 5.\n+        a_obj = metrics.TopKCategoricalAccuracy(k=1)\n+        self.evaluate(tf.compat.v1.variables_initializer(a_obj.variables))\n+        result = a_obj(y_true, y_pred)\n+        self.assertEqual(0.5, self.evaluate(result))  # only sample #2 matches\n+\n+        # With `k` > 5.\n+        y_true = tf.constant([[0, 0, 1, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0]])\n+        y_pred = tf.constant(\n+            [[0.5, 0.9, 0.1, 0.7, 0.6, 0.5, 0.4], [0.05, 0.95, 0, 0, 0, 0, 0]]\n+        )\n+        a_obj = metrics.TopKCategoricalAccuracy(k=6)\n+        self.evaluate(tf.compat.v1.variables_initializer(a_obj.variables))\n+        result = a_obj(y_true, y_pred)\n+        self.assertEqual(0.5, self.evaluate(result))  # only 1 sample matches.\n+\n+    def test_weighted(self):\n+        a_obj = metrics.TopKCategoricalAccuracy(k=2)\n+        self.evaluate(tf.compat.v1.variables_initializer(a_obj.variables))\n+        y_true = tf.constant([[0, 1, 0], [1, 0, 0], [0, 0, 1]])\n+        y_pred = tf.constant([[0, 0.9, 0.1], [0, 0.9, 0.1], [0, 0.9, 0.1]])\n+        sample_weight = tf.constant((1.0, 0.0, 1.0))\n+        result = a_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(1.0, self.evaluate(result), atol=1e-5)\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class SparseTopKCategoricalAccuracyTest(tf.test.TestCase):\n+    def test_config(self):\n+        a_obj = metrics.SparseTopKCategoricalAccuracy(\n+            name=\"stopkca\", dtype=tf.int32\n+        )\n+        self.assertEqual(a_obj.name, \"stopkca\")\n+        self.assertEqual(a_obj._dtype, tf.int32)\n+\n+        a_obj2 = metrics.SparseTopKCategoricalAccuracy.from_config(\n+            a_obj.get_config()\n+        )\n+        self.assertEqual(a_obj2.name, \"stopkca\")\n+        self.assertEqual(a_obj2._dtype, tf.int32)\n+\n+    def test_correctness(self):\n+        a_obj = metrics.SparseTopKCategoricalAccuracy()\n+        self.evaluate(tf.compat.v1.variables_initializer(a_obj.variables))\n+        y_true = tf.constant([2, 1])\n+        y_pred = tf.constant([[0.1, 0.9, 0.8], [0.05, 0.95, 0]])\n+\n+        result = a_obj(y_true, y_pred)\n+        self.assertEqual(1, self.evaluate(result))  # both the samples match\n+\n+        # With `k` < 5.\n+        a_obj = metrics.SparseTopKCategoricalAccuracy(k=1)\n+        self.evaluate(tf.compat.v1.variables_initializer(a_obj.variables))\n+        result = a_obj(y_true, y_pred)\n+        self.assertEqual(0.5, self.evaluate(result))  # only sample #2 matches\n+\n+        # With `k` > 5.\n+        y_pred = tf.constant(\n+            [[0.5, 0.9, 0.1, 0.7, 0.6, 0.5, 0.4], [0.05, 0.95, 0, 0, 0, 0, 0]]\n+        )\n+        a_obj = metrics.SparseTopKCategoricalAccuracy(k=6)\n+        self.evaluate(tf.compat.v1.variables_initializer(a_obj.variables))\n+        result = a_obj(y_true, y_pred)\n+        self.assertEqual(0.5, self.evaluate(result))  # only 1 sample matches.\n+\n+    def test_weighted(self):\n+        a_obj = metrics.SparseTopKCategoricalAccuracy(k=2)\n+        self.evaluate(tf.compat.v1.variables_initializer(a_obj.variables))\n+        y_true = tf.constant([1, 0, 2])\n+        y_pred = tf.constant([[0, 0.9, 0.1], [0, 0.9, 0.1], [0, 0.9, 0.1]])\n+        sample_weight = tf.constant((1.0, 0.0, 1.0))\n+        result = a_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(1.0, self.evaluate(result), atol=1e-5)\n+\n+    def test_sparse_top_k_categorical_accuracy_mismatched_dims_dynamic(self):\n+\n+        if not tf.compat.v1.executing_eagerly():\n+            # Test will fail in v1 graph mode since the metric is not a normal\n+            # layer.  It will aggregate the output by batch dim, which failed on\n+            # v1 code.\n+            self.skipTest(\"v2 eager mode only\")\n+\n+        class AccLayer(layers.Layer):\n+            def build(self, _):\n+                self.acc = metrics.SparseTopKCategoricalAccuracy(k=1)\n+\n+            def call(self, y_true, y_pred):\n+                return self.acc(y_true, y_pred)\n+\n+        label = layers.Input(shape=[1])\n+        predict = layers.Input(shape=[3])\n+        metric_result = AccLayer()(label, predict)\n+        model = Model([label, predict], metric_result)\n+\n+        result = model.predict(\n+            [\n+                tf.constant([[2], [1]]),\n+                tf.constant([[0.1, 0.1, 0.8], [0.05, 0, 0.95]]),\n+            ],\n+            steps=1,\n+        )\n+        self.assertAllClose(result, 0.5)\n+\n+\n+if __name__ == \"__main__\":\n+    tf.test.main()\n\n@@ -0,0 +1,1706 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Confusion metrics, i.e. metrics based on True/False positives/negatives.\"\"\"\n+\n+import abc\n+\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+from keras import activations\n+from keras import backend\n+from keras.dtensor import utils as dtensor_utils\n+from keras.metrics import base_metric\n+from keras.utils import metrics_utils\n+from keras.utils.generic_utils import to_list\n+from keras.utils.tf_utils import is_tensor_or_variable\n+\n+# isort: off\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+class _ConfusionMatrixConditionCount(base_metric.Metric):\n+    \"\"\"Calculates the number of the given confusion matrix condition.\n+\n+    Args:\n+      confusion_matrix_cond: One of `metrics_utils.ConfusionMatrix` conditions.\n+      thresholds: (Optional) Defaults to 0.5. A float value or a python\n+        list/tuple of float threshold values in [0, 1]. A threshold is compared\n+        with prediction values to determine the truth value of predictions\n+        (i.e., above the threshold is `true`, below is `false`). One metric\n+        value is generated for each threshold value.\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+    \"\"\"\n+\n+    def __init__(\n+        self, confusion_matrix_cond, thresholds=None, name=None, dtype=None\n+    ):\n+        super().__init__(name=name, dtype=dtype)\n+        self._confusion_matrix_cond = confusion_matrix_cond\n+        self.init_thresholds = thresholds\n+        self.thresholds = metrics_utils.parse_init_thresholds(\n+            thresholds, default_threshold=0.5\n+        )\n+        self._thresholds_distributed_evenly = (\n+            metrics_utils.is_evenly_distributed_thresholds(self.thresholds)\n+        )\n+        self.accumulator = self.add_weight(\n+            \"accumulator\", shape=(len(self.thresholds),), initializer=\"zeros\"\n+        )\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        \"\"\"Accumulates the metric statistics.\n+\n+        Args:\n+          y_true: The ground truth values.\n+          y_pred: The predicted values.\n+          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n+            and must be broadcastable to `y_true`.\n+\n+        Returns:\n+          Update op.\n+        \"\"\"\n+        return metrics_utils.update_confusion_matrix_variables(\n+            {self._confusion_matrix_cond: self.accumulator},\n+            y_true,\n+            y_pred,\n+            thresholds=self.thresholds,\n+            thresholds_distributed_evenly=self._thresholds_distributed_evenly,\n+            sample_weight=sample_weight,\n+        )\n+\n+    def result(self):\n+        if len(self.thresholds) == 1:\n+            result = self.accumulator[0]\n+        else:\n+            result = self.accumulator\n+        return tf.convert_to_tensor(result)\n+\n+    def reset_state(self):\n+        backend.batch_set_value(\n+            [(v, np.zeros(v.shape.as_list())) for v in self.variables]\n+        )\n+\n+    def get_config(self):\n+        config = {\"thresholds\": self.init_thresholds}\n+        base_config = super().get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+@keras_export(\"keras.metrics.FalsePositives\")\n+class FalsePositives(_ConfusionMatrixConditionCount):\n+    \"\"\"Calculates the number of false positives.\n+\n+    If `sample_weight` is given, calculates the sum of the weights of\n+    false positives. This metric creates one local variable, `accumulator`\n+    that is used to keep track of the number of false positives.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Args:\n+      thresholds: (Optional) Defaults to 0.5. A float value, or a Python\n+        list/tuple of float threshold values in [0, 1]. A threshold is compared\n+        with prediction values to determine the truth value of predictions\n+        (i.e., above the threshold is `true`, below is `false`). If used with a\n+        loss function that sets `from_logits=True` (i.e. no sigmoid applied to\n+        predictions), `thresholds` should be set to 0. One metric value is\n+        generated for each threshold value.\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.FalsePositives()\n+    >>> m.update_state([0, 1, 0, 0], [0, 0, 1, 1])\n+    >>> m.result().numpy()\n+    2.0\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 1, 0, 0], [0, 0, 1, 1], sample_weight=[0, 0, 1, 0])\n+    >>> m.result().numpy()\n+    1.0\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(optimizer='sgd',\n+                  loss='mse',\n+                  metrics=[tf.keras.metrics.FalsePositives()])\n+    ```\n+\n+    Usage with a loss with `from_logits=True`:\n+\n+    ```python\n+    model.compile(optimizer='adam',\n+                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n+                  metrics=[tf.keras.metrics.FalsePositives(thresholds=0)])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(self, thresholds=None, name=None, dtype=None):\n+        super().__init__(\n+            confusion_matrix_cond=metrics_utils.ConfusionMatrix.FALSE_POSITIVES,\n+            thresholds=thresholds,\n+            name=name,\n+            dtype=dtype,\n+        )\n+\n+\n+@keras_export(\"keras.metrics.FalseNegatives\")\n+class FalseNegatives(_ConfusionMatrixConditionCount):\n+    \"\"\"Calculates the number of false negatives.\n+\n+    If `sample_weight` is given, calculates the sum of the weights of\n+    false negatives. This metric creates one local variable, `accumulator`\n+    that is used to keep track of the number of false negatives.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Args:\n+      thresholds: (Optional) Defaults to 0.5. A float value, or a Python\n+        list/tuple of float threshold values in [0, 1]. A threshold is compared\n+        with prediction values to determine the truth value of predictions\n+        (i.e., above the threshold is `true`, below is `false`). If used with a\n+        loss function that sets `from_logits=True` (i.e. no sigmoid applied to\n+        predictions), `thresholds` should be set to 0. One metric value is\n+        generated for each threshold value.\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.FalseNegatives()\n+    >>> m.update_state([0, 1, 1, 1], [0, 1, 0, 0])\n+    >>> m.result().numpy()\n+    2.0\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 1, 1, 1], [0, 1, 0, 0], sample_weight=[0, 0, 1, 0])\n+    >>> m.result().numpy()\n+    1.0\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(optimizer='sgd',\n+                  loss='mse',\n+                  metrics=[tf.keras.metrics.FalseNegatives()])\n+    ```\n+\n+    Usage with a loss with `from_logits=True`:\n+\n+    ```python\n+    model.compile(optimizer='adam',\n+                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n+                  metrics=[tf.keras.metrics.FalseNegatives(thresholds=0)])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(self, thresholds=None, name=None, dtype=None):\n+        super().__init__(\n+            confusion_matrix_cond=metrics_utils.ConfusionMatrix.FALSE_NEGATIVES,\n+            thresholds=thresholds,\n+            name=name,\n+            dtype=dtype,\n+        )\n+\n+\n+@keras_export(\"keras.metrics.TrueNegatives\")\n+class TrueNegatives(_ConfusionMatrixConditionCount):\n+    \"\"\"Calculates the number of true negatives.\n+\n+    If `sample_weight` is given, calculates the sum of the weights of\n+    true negatives. This metric creates one local variable, `accumulator`\n+    that is used to keep track of the number of true negatives.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Args:\n+      thresholds: (Optional) Defaults to 0.5. A float value, or a Python\n+        list/tuple of float threshold values in [0, 1]. A threshold is compared\n+        with prediction values to determine the truth value of predictions\n+        (i.e., above the threshold is `true`, below is `false`). If used with a\n+        loss function that sets `from_logits=True` (i.e. no sigmoid applied to\n+        predictions), `thresholds` should be set to 0. One metric value is\n+        generated for each threshold value.\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.TrueNegatives()\n+    >>> m.update_state([0, 1, 0, 0], [1, 1, 0, 0])\n+    >>> m.result().numpy()\n+    2.0\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 1, 0, 0], [1, 1, 0, 0], sample_weight=[0, 0, 1, 0])\n+    >>> m.result().numpy()\n+    1.0\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(optimizer='sgd',\n+                  loss='mse',\n+                  metrics=[tf.keras.metrics.TrueNegatives()])\n+    ```\n+\n+    Usage with a loss with `from_logits=True`:\n+\n+    ```python\n+    model.compile(optimizer='adam',\n+                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n+                  metrics=[tf.keras.metrics.TrueNegatives(thresholds=0)])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(self, thresholds=None, name=None, dtype=None):\n+        super().__init__(\n+            confusion_matrix_cond=metrics_utils.ConfusionMatrix.TRUE_NEGATIVES,\n+            thresholds=thresholds,\n+            name=name,\n+            dtype=dtype,\n+        )\n+\n+\n+@keras_export(\"keras.metrics.TruePositives\")\n+class TruePositives(_ConfusionMatrixConditionCount):\n+    \"\"\"Calculates the number of true positives.\n+\n+    If `sample_weight` is given, calculates the sum of the weights of\n+    true positives. This metric creates one local variable, `true_positives`\n+    that is used to keep track of the number of true positives.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Args:\n+      thresholds: (Optional) Defaults to 0.5. A float value, or a Python\n+        list/tuple of float threshold values in [0, 1]. A threshold is compared\n+        with prediction values to determine the truth value of predictions\n+        (i.e., above the threshold is `true`, below is `false`). If used with a\n+        loss function that sets `from_logits=True` (i.e. no sigmoid applied to\n+        predictions), `thresholds` should be set to 0. One metric value is\n+        generated for each threshold value.\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.TruePositives()\n+    >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1])\n+    >>> m.result().numpy()\n+    2.0\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0])\n+    >>> m.result().numpy()\n+    1.0\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(optimizer='sgd',\n+                  loss='mse',\n+                  metrics=[tf.keras.metrics.TruePositives()])\n+    ```\n+\n+    Usage with a loss with `from_logits=True`:\n+\n+    ```python\n+    model.compile(optimizer='adam',\n+                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n+                  metrics=[tf.keras.metrics.TruePositives(thresholds=0)])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(self, thresholds=None, name=None, dtype=None):\n+        super().__init__(\n+            confusion_matrix_cond=metrics_utils.ConfusionMatrix.TRUE_POSITIVES,\n+            thresholds=thresholds,\n+            name=name,\n+            dtype=dtype,\n+        )\n+\n+\n+@keras_export(\"keras.metrics.Precision\")\n+class Precision(base_metric.Metric):\n+    \"\"\"Computes the precision of the predictions with respect to the labels.\n+\n+    The metric creates two local variables, `true_positives` and\n+    `false_positives` that are used to compute the precision. This value is\n+    ultimately returned as `precision`, an idempotent operation that simply\n+    divides `true_positives` by the sum of `true_positives` and\n+    `false_positives`.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    If `top_k` is set, we'll calculate precision as how often on average a class\n+    among the top-k classes with the highest predicted values of a batch entry\n+    is correct and can be found in the label for that entry.\n+\n+    If `class_id` is specified, we calculate precision by considering only the\n+    entries in the batch for which `class_id` is above the threshold and/or in\n+    the top-k highest predictions, and computing the fraction of them for which\n+    `class_id` is indeed a correct label.\n+\n+    Args:\n+      thresholds: (Optional) A float value, or a Python list/tuple of float\n+        threshold values in [0, 1]. A threshold is compared with prediction\n+        values to determine the truth value of predictions (i.e., above the\n+        threshold is `true`, below is `false`). If used with a loss function\n+        that sets `from_logits=True` (i.e. no sigmoid applied to predictions),\n+        `thresholds` should be set to 0. One metric value is generated for each\n+        threshold value. If neither thresholds nor top_k are set, the default is\n+        to calculate precision with `thresholds=0.5`.\n+      top_k: (Optional) Unset by default. An int value specifying the top-k\n+        predictions to consider when calculating precision.\n+      class_id: (Optional) Integer class ID for which we want binary metrics.\n+        This must be in the half-open interval `[0, num_classes)`, where\n+        `num_classes` is the last dimension of predictions.\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.Precision()\n+    >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1])\n+    >>> m.result().numpy()\n+    0.6666667\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0])\n+    >>> m.result().numpy()\n+    1.0\n+\n+    >>> # With top_k=2, it will calculate precision over y_true[:2]\n+    >>> # and y_pred[:2]\n+    >>> m = tf.keras.metrics.Precision(top_k=2)\n+    >>> m.update_state([0, 0, 1, 1], [1, 1, 1, 1])\n+    >>> m.result().numpy()\n+    0.0\n+\n+    >>> # With top_k=4, it will calculate precision over y_true[:4]\n+    >>> # and y_pred[:4]\n+    >>> m = tf.keras.metrics.Precision(top_k=4)\n+    >>> m.update_state([0, 0, 1, 1], [1, 1, 1, 1])\n+    >>> m.result().numpy()\n+    0.5\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(optimizer='sgd',\n+                  loss='mse',\n+                  metrics=[tf.keras.metrics.Precision()])\n+    ```\n+\n+    Usage with a loss with `from_logits=True`:\n+\n+    ```python\n+    model.compile(optimizer='adam',\n+                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n+                  metrics=[tf.keras.metrics.Precision(thresholds=0)])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(\n+        self, thresholds=None, top_k=None, class_id=None, name=None, dtype=None\n+    ):\n+        super().__init__(name=name, dtype=dtype)\n+        self.init_thresholds = thresholds\n+        self.top_k = top_k\n+        self.class_id = class_id\n+\n+        default_threshold = 0.5 if top_k is None else metrics_utils.NEG_INF\n+        self.thresholds = metrics_utils.parse_init_thresholds(\n+            thresholds, default_threshold=default_threshold\n+        )\n+        self._thresholds_distributed_evenly = (\n+            metrics_utils.is_evenly_distributed_thresholds(self.thresholds)\n+        )\n+        self.true_positives = self.add_weight(\n+            \"true_positives\", shape=(len(self.thresholds),), initializer=\"zeros\"\n+        )\n+        self.false_positives = self.add_weight(\n+            \"false_positives\",\n+            shape=(len(self.thresholds),),\n+            initializer=\"zeros\",\n+        )\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        \"\"\"Accumulates true positive and false positive statistics.\n+\n+        Args:\n+          y_true: The ground truth values, with the same dimensions as `y_pred`.\n+            Will be cast to `bool`.\n+          y_pred: The predicted values. Each element must be in the range\n+            `[0, 1]`.\n+          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n+            and must be broadcastable to `y_true`.\n+\n+        Returns:\n+          Update op.\n+        \"\"\"\n+        return metrics_utils.update_confusion_matrix_variables(\n+            {\n+                metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,  # noqa: E501\n+                metrics_utils.ConfusionMatrix.FALSE_POSITIVES: self.false_positives,  # noqa: E501\n+            },\n+            y_true,\n+            y_pred,\n+            thresholds=self.thresholds,\n+            thresholds_distributed_evenly=self._thresholds_distributed_evenly,\n+            top_k=self.top_k,\n+            class_id=self.class_id,\n+            sample_weight=sample_weight,\n+        )\n+\n+    def result(self):\n+        result = tf.math.divide_no_nan(\n+            self.true_positives,\n+            tf.math.add(self.true_positives, self.false_positives),\n+        )\n+        return result[0] if len(self.thresholds) == 1 else result\n+\n+    def reset_state(self):\n+        num_thresholds = len(to_list(self.thresholds))\n+        backend.batch_set_value(\n+            [\n+                (v, np.zeros((num_thresholds,)))\n+                for v in (self.true_positives, self.false_positives)\n+            ]\n+        )\n+\n+    def get_config(self):\n+        config = {\n+            \"thresholds\": self.init_thresholds,\n+            \"top_k\": self.top_k,\n+            \"class_id\": self.class_id,\n+        }\n+        base_config = super().get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+@keras_export(\"keras.metrics.Recall\")\n+class Recall(base_metric.Metric):\n+    \"\"\"Computes the recall of the predictions with respect to the labels.\n+\n+    This metric creates two local variables, `true_positives` and\n+    `false_negatives`, that are used to compute the recall. This value is\n+    ultimately returned as `recall`, an idempotent operation that simply divides\n+    `true_positives` by the sum of `true_positives` and `false_negatives`.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    If `top_k` is set, recall will be computed as how often on average a class\n+    among the labels of a batch entry is in the top-k predictions.\n+\n+    If `class_id` is specified, we calculate recall by considering only the\n+    entries in the batch for which `class_id` is in the label, and computing the\n+    fraction of them for which `class_id` is above the threshold and/or in the\n+    top-k predictions.\n+\n+    Args:\n+      thresholds: (Optional) A float value, or a Python list/tuple of float\n+        threshold values in [0, 1]. A threshold is compared with prediction\n+        values to determine the truth value of predictions (i.e., above the\n+        threshold is `true`, below is `false`). If used with a loss function\n+        that sets `from_logits=True` (i.e. no sigmoid applied to predictions),\n+        `thresholds` should be set to 0. One metric value is generated for each\n+        threshold value. If neither thresholds nor top_k are set, the default is\n+        to calculate recall with `thresholds=0.5`.\n+      top_k: (Optional) Unset by default. An int value specifying the top-k\n+        predictions to consider when calculating recall.\n+      class_id: (Optional) Integer class ID for which we want binary metrics.\n+        This must be in the half-open interval `[0, num_classes)`, where\n+        `num_classes` is the last dimension of predictions.\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.Recall()\n+    >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1])\n+    >>> m.result().numpy()\n+    0.6666667\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0])\n+    >>> m.result().numpy()\n+    1.0\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(optimizer='sgd',\n+                  loss='mse',\n+                  metrics=[tf.keras.metrics.Recall()])\n+    ```\n+\n+    Usage with a loss with `from_logits=True`:\n+\n+    ```python\n+    model.compile(optimizer='adam',\n+                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n+                  metrics=[tf.keras.metrics.Recall(thresholds=0)])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(\n+        self, thresholds=None, top_k=None, class_id=None, name=None, dtype=None\n+    ):\n+        super().__init__(name=name, dtype=dtype)\n+        self.init_thresholds = thresholds\n+        self.top_k = top_k\n+        self.class_id = class_id\n+\n+        default_threshold = 0.5 if top_k is None else metrics_utils.NEG_INF\n+        self.thresholds = metrics_utils.parse_init_thresholds(\n+            thresholds, default_threshold=default_threshold\n+        )\n+        self._thresholds_distributed_evenly = (\n+            metrics_utils.is_evenly_distributed_thresholds(self.thresholds)\n+        )\n+        self.true_positives = self.add_weight(\n+            \"true_positives\", shape=(len(self.thresholds),), initializer=\"zeros\"\n+        )\n+        self.false_negatives = self.add_weight(\n+            \"false_negatives\",\n+            shape=(len(self.thresholds),),\n+            initializer=\"zeros\",\n+        )\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        \"\"\"Accumulates true positive and false negative statistics.\n+\n+        Args:\n+          y_true: The ground truth values, with the same dimensions as `y_pred`.\n+            Will be cast to `bool`.\n+          y_pred: The predicted values. Each element must be in the range\n+            `[0, 1]`.\n+          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n+            and must be broadcastable to `y_true`.\n+\n+        Returns:\n+          Update op.\n+        \"\"\"\n+        return metrics_utils.update_confusion_matrix_variables(\n+            {\n+                metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,  # noqa: E501\n+                metrics_utils.ConfusionMatrix.FALSE_NEGATIVES: self.false_negatives,  # noqa: E501\n+            },\n+            y_true,\n+            y_pred,\n+            thresholds=self.thresholds,\n+            thresholds_distributed_evenly=self._thresholds_distributed_evenly,\n+            top_k=self.top_k,\n+            class_id=self.class_id,\n+            sample_weight=sample_weight,\n+        )\n+\n+    def result(self):\n+        result = tf.math.divide_no_nan(\n+            self.true_positives,\n+            tf.math.add(self.true_positives, self.false_negatives),\n+        )\n+        return result[0] if len(self.thresholds) == 1 else result\n+\n+    def reset_state(self):\n+        num_thresholds = len(to_list(self.thresholds))\n+        backend.batch_set_value(\n+            [\n+                (v, np.zeros((num_thresholds,)))\n+                for v in (self.true_positives, self.false_negatives)\n+            ]\n+        )\n+\n+    def get_config(self):\n+        config = {\n+            \"thresholds\": self.init_thresholds,\n+            \"top_k\": self.top_k,\n+            \"class_id\": self.class_id,\n+        }\n+        base_config = super().get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+class SensitivitySpecificityBase(base_metric.Metric, metaclass=abc.ABCMeta):\n+    \"\"\"Abstract base class for computing sensitivity and specificity.\n+\n+    For additional information about specificity and sensitivity, see\n+    [the following](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).\n+    \"\"\"\n+\n+    def __init__(\n+        self, value, num_thresholds=200, class_id=None, name=None, dtype=None\n+    ):\n+        super().__init__(name=name, dtype=dtype)\n+        if num_thresholds <= 0:\n+            raise ValueError(\n+                \"Argument `num_thresholds` must be an integer > 0. \"\n+                f\"Received: num_thresholds={num_thresholds}\"\n+            )\n+        self.value = value\n+        self.class_id = class_id\n+        self.true_positives = self.add_weight(\n+            \"true_positives\", shape=(num_thresholds,), initializer=\"zeros\"\n+        )\n+        self.true_negatives = self.add_weight(\n+            \"true_negatives\", shape=(num_thresholds,), initializer=\"zeros\"\n+        )\n+        self.false_positives = self.add_weight(\n+            \"false_positives\", shape=(num_thresholds,), initializer=\"zeros\"\n+        )\n+        self.false_negatives = self.add_weight(\n+            \"false_negatives\", shape=(num_thresholds,), initializer=\"zeros\"\n+        )\n+\n+        # Compute `num_thresholds` thresholds in [0, 1]\n+        if num_thresholds == 1:\n+            self.thresholds = [0.5]\n+            self._thresholds_distributed_evenly = False\n+        else:\n+            thresholds = [\n+                (i + 1) * 1.0 / (num_thresholds - 1)\n+                for i in range(num_thresholds - 2)\n+            ]\n+            self.thresholds = [0.0] + thresholds + [1.0]\n+            self._thresholds_distributed_evenly = True\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        \"\"\"Accumulates confusion matrix statistics.\n+\n+        Args:\n+          y_true: The ground truth values.\n+          y_pred: The predicted values.\n+          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n+            and must be broadcastable to `y_true`.\n+\n+        Returns:\n+          Update op.\n+        \"\"\"\n+        return metrics_utils.update_confusion_matrix_variables(\n+            {\n+                metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,  # noqa: E501\n+                metrics_utils.ConfusionMatrix.TRUE_NEGATIVES: self.true_negatives,  # noqa: E501\n+                metrics_utils.ConfusionMatrix.FALSE_POSITIVES: self.false_positives,  # noqa: E501\n+                metrics_utils.ConfusionMatrix.FALSE_NEGATIVES: self.false_negatives,  # noqa: E501\n+            },\n+            y_true,\n+            y_pred,\n+            thresholds=self.thresholds,\n+            thresholds_distributed_evenly=self._thresholds_distributed_evenly,\n+            class_id=self.class_id,\n+            sample_weight=sample_weight,\n+        )\n+\n+    def reset_state(self):\n+        num_thresholds = len(self.thresholds)\n+        confusion_matrix_variables = (\n+            self.true_positives,\n+            self.true_negatives,\n+            self.false_positives,\n+            self.false_negatives,\n+        )\n+        backend.batch_set_value(\n+            [\n+                (v, np.zeros((num_thresholds,)))\n+                for v in confusion_matrix_variables\n+            ]\n+        )\n+\n+    def get_config(self):\n+        config = {\"class_id\": self.class_id}\n+        base_config = super().get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+    def _find_max_under_constraint(self, constrained, dependent, predicate):\n+        \"\"\"Returns the maximum of dependent_statistic that satisfies the\n+        constraint.\n+\n+        Args:\n+          constrained: Over these values the constraint\n+            is specified. A rank-1 tensor.\n+          dependent: From these values the maximum that satiesfies the\n+            constraint is selected. Values in this tensor and in\n+            `constrained` are linked by having the same threshold at each\n+            position, hence this tensor must have the same shape.\n+          predicate: A binary boolean functor to be applied to arguments\n+          `constrained` and `self.value`, e.g. `tf.greater`.\n+\n+        Returns:\n+          maximal dependent value, if no value satiesfies the constraint 0.0.\n+        \"\"\"\n+        feasible = tf.where(predicate(constrained, self.value))\n+        feasible_exists = tf.greater(tf.size(feasible), 0)\n+        max_dependent = tf.reduce_max(tf.gather(dependent, feasible))\n+\n+        return tf.where(feasible_exists, max_dependent, 0.0)\n+\n+\n+@keras_export(\"keras.metrics.SensitivityAtSpecificity\")\n+class SensitivityAtSpecificity(SensitivitySpecificityBase):\n+    \"\"\"Computes best sensitivity where specificity is >= specified value.\n+\n+    the sensitivity at a given specificity.\n+\n+    `Sensitivity` measures the proportion of actual positives that are correctly\n+    identified as such (tp / (tp + fn)).\n+    `Specificity` measures the proportion of actual negatives that are correctly\n+    identified as such (tn / (tn + fp)).\n+\n+    This metric creates four local variables, `true_positives`,\n+    `true_negatives`, `false_positives` and `false_negatives` that are used to\n+    compute the sensitivity at the given specificity. The threshold for the\n+    given specificity value is computed and used to evaluate the corresponding\n+    sensitivity.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    If `class_id` is specified, we calculate precision by considering only the\n+    entries in the batch for which `class_id` is above the threshold\n+    predictions, and computing the fraction of them for which `class_id` is\n+    indeed a correct label.\n+\n+    For additional information about specificity and sensitivity, see\n+    [the following](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).\n+\n+    Args:\n+      specificity: A scalar value in range `[0, 1]`.\n+      num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n+        use for matching the given specificity.\n+      class_id: (Optional) Integer class ID for which we want binary metrics.\n+        This must be in the half-open interval `[0, num_classes)`, where\n+        `num_classes` is the last dimension of predictions.\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.SensitivityAtSpecificity(0.5)\n+    >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])\n+    >>> m.result().numpy()\n+    0.5\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],\n+    ...                sample_weight=[1, 1, 2, 2, 1])\n+    >>> m.result().numpy()\n+    0.333333\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[tf.keras.metrics.SensitivityAtSpecificity()])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(\n+        self,\n+        specificity,\n+        num_thresholds=200,\n+        class_id=None,\n+        name=None,\n+        dtype=None,\n+    ):\n+        if specificity < 0 or specificity > 1:\n+            raise ValueError(\n+                \"Argument `specificity` must be in the range [0, 1]. \"\n+                f\"Received: specificity={specificity}\"\n+            )\n+        self.specificity = specificity\n+        self.num_thresholds = num_thresholds\n+        super().__init__(\n+            specificity,\n+            num_thresholds=num_thresholds,\n+            class_id=class_id,\n+            name=name,\n+            dtype=dtype,\n+        )\n+\n+    def result(self):\n+        specificities = tf.math.divide_no_nan(\n+            self.true_negatives,\n+            tf.math.add(self.true_negatives, self.false_positives),\n+        )\n+        sensitivities = tf.math.divide_no_nan(\n+            self.true_positives,\n+            tf.math.add(self.true_positives, self.false_negatives),\n+        )\n+        return self._find_max_under_constraint(\n+            specificities, sensitivities, tf.greater_equal\n+        )\n+\n+    def get_config(self):\n+        config = {\n+            \"num_thresholds\": self.num_thresholds,\n+            \"specificity\": self.specificity,\n+        }\n+        base_config = super().get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+@keras_export(\"keras.metrics.SpecificityAtSensitivity\")\n+class SpecificityAtSensitivity(SensitivitySpecificityBase):\n+    \"\"\"Computes best specificity where sensitivity is >= specified value.\n+\n+    `Sensitivity` measures the proportion of actual positives that are correctly\n+    identified as such (tp / (tp + fn)).\n+    `Specificity` measures the proportion of actual negatives that are correctly\n+    identified as such (tn / (tn + fp)).\n+\n+    This metric creates four local variables, `true_positives`,\n+    `true_negatives`, `false_positives` and `false_negatives` that are used to\n+    compute the specificity at the given sensitivity. The threshold for the\n+    given sensitivity value is computed and used to evaluate the corresponding\n+    specificity.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    If `class_id` is specified, we calculate precision by considering only the\n+    entries in the batch for which `class_id` is above the threshold\n+    predictions, and computing the fraction of them for which `class_id` is\n+    indeed a correct label.\n+\n+    For additional information about specificity and sensitivity, see\n+    [the following](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).\n+\n+    Args:\n+      sensitivity: A scalar value in range `[0, 1]`.\n+      num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n+        use for matching the given sensitivity.\n+      class_id: (Optional) Integer class ID for which we want binary metrics.\n+        This must be in the half-open interval `[0, num_classes)`, where\n+        `num_classes` is the last dimension of predictions.\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.SpecificityAtSensitivity(0.5)\n+    >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])\n+    >>> m.result().numpy()\n+    0.66666667\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],\n+    ...                sample_weight=[1, 1, 2, 2, 2])\n+    >>> m.result().numpy()\n+    0.5\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[tf.keras.metrics.SpecificityAtSensitivity()])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(\n+        self,\n+        sensitivity,\n+        num_thresholds=200,\n+        class_id=None,\n+        name=None,\n+        dtype=None,\n+    ):\n+        if sensitivity < 0 or sensitivity > 1:\n+            raise ValueError(\n+                \"Argument `sensitivity` must be in the range [0, 1]. \"\n+                f\"Received: sensitivity={sensitivity}\"\n+            )\n+        self.sensitivity = sensitivity\n+        self.num_thresholds = num_thresholds\n+        super().__init__(\n+            sensitivity,\n+            num_thresholds=num_thresholds,\n+            class_id=class_id,\n+            name=name,\n+            dtype=dtype,\n+        )\n+\n+    def result(self):\n+        sensitivities = tf.math.divide_no_nan(\n+            self.true_positives,\n+            tf.math.add(self.true_positives, self.false_negatives),\n+        )\n+        specificities = tf.math.divide_no_nan(\n+            self.true_negatives,\n+            tf.math.add(self.true_negatives, self.false_positives),\n+        )\n+        return self._find_max_under_constraint(\n+            sensitivities, specificities, tf.greater_equal\n+        )\n+\n+    def get_config(self):\n+        config = {\n+            \"num_thresholds\": self.num_thresholds,\n+            \"sensitivity\": self.sensitivity,\n+        }\n+        base_config = super().get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+@keras_export(\"keras.metrics.PrecisionAtRecall\")\n+class PrecisionAtRecall(SensitivitySpecificityBase):\n+    \"\"\"Computes best precision where recall is >= specified value.\n+\n+    This metric creates four local variables, `true_positives`,\n+    `true_negatives`, `false_positives` and `false_negatives` that are used to\n+    compute the precision at the given recall. The threshold for the given\n+    recall value is computed and used to evaluate the corresponding precision.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    If `class_id` is specified, we calculate precision by considering only the\n+    entries in the batch for which `class_id` is above the threshold\n+    predictions, and computing the fraction of them for which `class_id` is\n+    indeed a correct label.\n+\n+    Args:\n+      recall: A scalar value in range `[0, 1]`.\n+      num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n+        use for matching the given recall.\n+      class_id: (Optional) Integer class ID for which we want binary metrics.\n+        This must be in the half-open interval `[0, num_classes)`, where\n+        `num_classes` is the last dimension of predictions.\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.PrecisionAtRecall(0.5)\n+    >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])\n+    >>> m.result().numpy()\n+    0.5\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],\n+    ...                sample_weight=[2, 2, 2, 1, 1])\n+    >>> m.result().numpy()\n+    0.33333333\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[tf.keras.metrics.PrecisionAtRecall(recall=0.8)])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(\n+        self, recall, num_thresholds=200, class_id=None, name=None, dtype=None\n+    ):\n+        if recall < 0 or recall > 1:\n+            raise ValueError(\n+                \"Argument `recall` must be in the range [0, 1]. \"\n+                f\"Received: recall={recall}\"\n+            )\n+        self.recall = recall\n+        self.num_thresholds = num_thresholds\n+        super().__init__(\n+            value=recall,\n+            num_thresholds=num_thresholds,\n+            class_id=class_id,\n+            name=name,\n+            dtype=dtype,\n+        )\n+\n+    def result(self):\n+        recalls = tf.math.divide_no_nan(\n+            self.true_positives,\n+            tf.math.add(self.true_positives, self.false_negatives),\n+        )\n+        precisions = tf.math.divide_no_nan(\n+            self.true_positives,\n+            tf.math.add(self.true_positives, self.false_positives),\n+        )\n+        return self._find_max_under_constraint(\n+            recalls, precisions, tf.greater_equal\n+        )\n+\n+    def get_config(self):\n+        config = {\"num_thresholds\": self.num_thresholds, \"recall\": self.recall}\n+        base_config = super().get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+@keras_export(\"keras.metrics.RecallAtPrecision\")\n+class RecallAtPrecision(SensitivitySpecificityBase):\n+    \"\"\"Computes best recall where precision is >= specified value.\n+\n+    For a given score-label-distribution the required precision might not\n+    be achievable, in this case 0.0 is returned as recall.\n+\n+    This metric creates four local variables, `true_positives`,\n+    `true_negatives`, `false_positives` and `false_negatives` that are used to\n+    compute the recall at the given precision. The threshold for the given\n+    precision value is computed and used to evaluate the corresponding recall.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    If `class_id` is specified, we calculate precision by considering only the\n+    entries in the batch for which `class_id` is above the threshold\n+    predictions, and computing the fraction of them for which `class_id` is\n+    indeed a correct label.\n+\n+    Args:\n+      precision: A scalar value in range `[0, 1]`.\n+      num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n+        use for matching the given precision.\n+      class_id: (Optional) Integer class ID for which we want binary metrics.\n+        This must be in the half-open interval `[0, num_classes)`, where\n+        `num_classes` is the last dimension of predictions.\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.RecallAtPrecision(0.8)\n+    >>> m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])\n+    >>> m.result().numpy()\n+    0.5\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9],\n+    ...                sample_weight=[1, 0, 0, 1])\n+    >>> m.result().numpy()\n+    1.0\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[tf.keras.metrics.RecallAtPrecision(precision=0.8)])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(\n+        self,\n+        precision,\n+        num_thresholds=200,\n+        class_id=None,\n+        name=None,\n+        dtype=None,\n+    ):\n+        if precision < 0 or precision > 1:\n+            raise ValueError(\n+                \"Argument `precision` must be in the range [0, 1]. \"\n+                f\"Received: precision={precision}\"\n+            )\n+        self.precision = precision\n+        self.num_thresholds = num_thresholds\n+        super().__init__(\n+            value=precision,\n+            num_thresholds=num_thresholds,\n+            class_id=class_id,\n+            name=name,\n+            dtype=dtype,\n+        )\n+\n+    def result(self):\n+        precisions = tf.math.divide_no_nan(\n+            self.true_positives,\n+            tf.math.add(self.true_positives, self.false_positives),\n+        )\n+        recalls = tf.math.divide_no_nan(\n+            self.true_positives,\n+            tf.math.add(self.true_positives, self.false_negatives),\n+        )\n+        return self._find_max_under_constraint(\n+            precisions, recalls, tf.greater_equal\n+        )\n+\n+    def get_config(self):\n+        config = {\n+            \"num_thresholds\": self.num_thresholds,\n+            \"precision\": self.precision,\n+        }\n+        base_config = super().get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+@keras_export(\"keras.metrics.AUC\")\n+class AUC(base_metric.Metric):\n+    \"\"\"Approximates the AUC (Area under the curve) of the ROC or PR curves.\n+\n+    The AUC (Area under the curve) of the ROC (Receiver operating\n+    characteristic; default) or PR (Precision Recall) curves are quality\n+    measures of binary classifiers. Unlike the accuracy, and like cross-entropy\n+    losses, ROC-AUC and PR-AUC evaluate all the operational points of a model.\n+\n+    This class approximates AUCs using a Riemann sum. During the metric\n+    accumulation phrase, predictions are accumulated within predefined buckets\n+    by value. The AUC is then computed by interpolating per-bucket averages.\n+    These buckets define the evaluated operational points.\n+\n+    This metric creates four local variables, `true_positives`,\n+    `true_negatives`, `false_positives` and `false_negatives` that are used to\n+    compute the AUC.  To discretize the AUC curve, a linearly spaced set of\n+    thresholds is used to compute pairs of recall and precision values. The area\n+    under the ROC-curve is therefore computed using the height of the recall\n+    values by the false positive rate, while the area under the PR-curve is the\n+    computed using the height of the precision values by the recall.\n+\n+    This value is ultimately returned as `auc`, an idempotent operation that\n+    computes the area under a discretized curve of precision versus recall\n+    values (computed using the aforementioned variables). The `num_thresholds`\n+    variable controls the degree of discretization with larger numbers of\n+    thresholds more closely approximating the true AUC. The quality of the\n+    approximation may vary dramatically depending on `num_thresholds`. The\n+    `thresholds` parameter can be used to manually specify thresholds which\n+    split the predictions more evenly.\n+\n+    For a best approximation of the real AUC, `predictions` should be\n+    distributed approximately uniformly in the range [0, 1] (if\n+    `from_logits=False`). The quality of the AUC approximation may be poor if\n+    this is not the case. Setting `summation_method` to 'minoring' or 'majoring'\n+    can help quantify the error in the approximation by providing lower or upper\n+    bound estimate of the AUC.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Args:\n+      num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n+        use when discretizing the roc curve. Values must be > 1.\n+      curve: (Optional) Specifies the name of the curve to be computed, 'ROC'\n+        [default] or 'PR' for the Precision-Recall-curve.\n+      summation_method: (Optional) Specifies the [Riemann summation method](\n+          https://en.wikipedia.org/wiki/Riemann_sum) used.\n+          'interpolation' (default) applies mid-point summation scheme for\n+          `ROC`.  For PR-AUC, interpolates (true/false) positives but not the\n+          ratio that is precision (see Davis & Goadrich 2006 for details);\n+          'minoring' applies left summation for increasing intervals and right\n+          summation for decreasing intervals; 'majoring' does the opposite.\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+      thresholds: (Optional) A list of floating point values to use as the\n+        thresholds for discretizing the curve. If set, the `num_thresholds`\n+        parameter is ignored. Values should be in [0, 1]. Endpoint thresholds\n+        equal to {-epsilon, 1+epsilon} for a small positive epsilon value will\n+        be automatically included with these to correctly handle predictions\n+        equal to exactly 0 or 1.\n+      multi_label: boolean indicating whether multilabel data should be\n+        treated as such, wherein AUC is computed separately for each label and\n+        then averaged across labels, or (when False) if the data should be\n+        flattened into a single label before AUC computation. In the latter\n+        case, when multilabel data is passed to AUC, each label-prediction pair\n+        is treated as an individual data point. Should be set to False for\n+        multi-class data.\n+      num_labels: (Optional) The number of labels, used when `multi_label` is\n+        True. If `num_labels` is not specified, then state variables get created\n+        on the first call to `update_state`.\n+      label_weights: (Optional) list, array, or tensor of non-negative weights\n+        used to compute AUCs for multilabel data. When `multi_label` is True,\n+        the weights are applied to the individual label AUCs when they are\n+        averaged to produce the multi-label AUC. When it's False, they are used\n+        to weight the individual label predictions in computing the confusion\n+        matrix on the flattened data. Note that this is unlike class_weights in\n+        that class_weights weights the example depending on the value of its\n+        label, whereas label_weights depends only on the index of that label\n+        before flattening; therefore `label_weights` should not be used for\n+        multi-class data.\n+      from_logits: boolean indicating whether the predictions (`y_pred` in\n+        `update_state`) are probabilities or sigmoid logits. As a rule of thumb,\n+        when using a keras loss, the `from_logits` constructor argument of the\n+        loss should match the AUC `from_logits` constructor argument.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.AUC(num_thresholds=3)\n+    >>> m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])\n+    >>> # threshold values are [0 - 1e-7, 0.5, 1 + 1e-7]\n+    >>> # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]\n+    >>> # tp_rate = recall = [1, 0.5, 0], fp_rate = [1, 0, 0]\n+    >>> # auc = ((((1+0.5)/2)*(1-0)) + (((0.5+0)/2)*(0-0))) = 0.75\n+    >>> m.result().numpy()\n+    0.75\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9],\n+    ...                sample_weight=[1, 0, 0, 1])\n+    >>> m.result().numpy()\n+    1.0\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    # Reports the AUC of a model outputting a probability.\n+    model.compile(optimizer='sgd',\n+                  loss=tf.keras.losses.BinaryCrossentropy(),\n+                  metrics=[tf.keras.metrics.AUC()])\n+\n+    # Reports the AUC of a model outputting a logit.\n+    model.compile(optimizer='sgd',\n+                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n+                  metrics=[tf.keras.metrics.AUC(from_logits=True)])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(\n+        self,\n+        num_thresholds=200,\n+        curve=\"ROC\",\n+        summation_method=\"interpolation\",\n+        name=None,\n+        dtype=None,\n+        thresholds=None,\n+        multi_label=False,\n+        num_labels=None,\n+        label_weights=None,\n+        from_logits=False,\n+    ):\n+        # Validate configurations.\n+        if isinstance(curve, metrics_utils.AUCCurve) and curve not in list(\n+            metrics_utils.AUCCurve\n+        ):\n+            raise ValueError(\n+                f'Invalid `curve` argument value \"{curve}\". '\n+                f\"Expected one of: {list(metrics_utils.AUCCurve)}\"\n+            )\n+        if isinstance(\n+            summation_method, metrics_utils.AUCSummationMethod\n+        ) and summation_method not in list(metrics_utils.AUCSummationMethod):\n+            raise ValueError(\n+                \"Invalid `summation_method` \"\n+                f'argument value \"{summation_method}\". '\n+                f\"Expected one of: {list(metrics_utils.AUCSummationMethod)}\"\n+            )\n+\n+        # Update properties.\n+        self._init_from_thresholds = thresholds is not None\n+        if thresholds is not None:\n+            # If specified, use the supplied thresholds.\n+            self.num_thresholds = len(thresholds) + 2\n+            thresholds = sorted(thresholds)\n+            self._thresholds_distributed_evenly = (\n+                metrics_utils.is_evenly_distributed_thresholds(\n+                    np.array([0.0] + thresholds + [1.0])\n+                )\n+            )\n+        else:\n+            if num_thresholds <= 1:\n+                raise ValueError(\n+                    \"Argument `num_thresholds` must be an integer > 1. \"\n+                    f\"Received: num_thresholds={num_thresholds}\"\n+                )\n+\n+            # Otherwise, linearly interpolate (num_thresholds - 2) thresholds in\n+            # (0, 1).\n+            self.num_thresholds = num_thresholds\n+            thresholds = [\n+                (i + 1) * 1.0 / (num_thresholds - 1)\n+                for i in range(num_thresholds - 2)\n+            ]\n+            self._thresholds_distributed_evenly = True\n+\n+        # Add an endpoint \"threshold\" below zero and above one for either\n+        # threshold method to account for floating point imprecisions.\n+        self._thresholds = np.array(\n+            [0.0 - backend.epsilon()] + thresholds + [1.0 + backend.epsilon()]\n+        )\n+\n+        if isinstance(curve, metrics_utils.AUCCurve):\n+            self.curve = curve\n+        else:\n+            self.curve = metrics_utils.AUCCurve.from_str(curve)\n+        if isinstance(summation_method, metrics_utils.AUCSummationMethod):\n+            self.summation_method = summation_method\n+        else:\n+            self.summation_method = metrics_utils.AUCSummationMethod.from_str(\n+                summation_method\n+            )\n+        super().__init__(name=name, dtype=dtype)\n+\n+        # Handle multilabel arguments.\n+        self.multi_label = multi_label\n+        self.num_labels = num_labels\n+        if label_weights is not None:\n+            label_weights = tf.constant(label_weights, dtype=self.dtype)\n+            tf.debugging.assert_non_negative(\n+                label_weights,\n+                message=\"All values of `label_weights` must be non-negative.\",\n+            )\n+            self.label_weights = label_weights\n+\n+        else:\n+            self.label_weights = None\n+\n+        self._from_logits = from_logits\n+\n+        self._built = False\n+        if self.multi_label:\n+            if num_labels:\n+                shape = tf.TensorShape([None, num_labels])\n+                self._build(shape)\n+        else:\n+            if num_labels:\n+                raise ValueError(\n+                    \"`num_labels` is needed only when `multi_label` is True.\"\n+                )\n+            self._build(None)\n+\n+    @property\n+    def thresholds(self):\n+        \"\"\"The thresholds used for evaluating AUC.\"\"\"\n+        return list(self._thresholds)\n+\n+    def _build(self, shape):\n+        \"\"\"Initialize TP, FP, TN, and FN tensors, given the shape of the\n+        data.\"\"\"\n+        if self.multi_label:\n+            if shape.ndims != 2:\n+                raise ValueError(\n+                    \"`y_true` must have rank 2 when `multi_label=True`. \"\n+                    f\"Found rank {shape.ndims}. \"\n+                    f\"Full shape received for `y_true`: {shape}\"\n+                )\n+            self._num_labels = shape[1]\n+            variable_shape = tf.TensorShape(\n+                [self.num_thresholds, self._num_labels]\n+            )\n+        else:\n+            variable_shape = tf.TensorShape([self.num_thresholds])\n+\n+        self._build_input_shape = shape\n+        # Create metric variables\n+        self.true_positives = self.add_weight(\n+            \"true_positives\", shape=variable_shape, initializer=\"zeros\"\n+        )\n+        self.true_negatives = self.add_weight(\n+            \"true_negatives\", shape=variable_shape, initializer=\"zeros\"\n+        )\n+        self.false_positives = self.add_weight(\n+            \"false_positives\", shape=variable_shape, initializer=\"zeros\"\n+        )\n+        self.false_negatives = self.add_weight(\n+            \"false_negatives\", shape=variable_shape, initializer=\"zeros\"\n+        )\n+\n+        if self.multi_label:\n+            with tf.init_scope():\n+                # This should only be necessary for handling v1 behavior. In v2,\n+                # AUC should be initialized outside of any tf.functions, and\n+                # therefore in eager mode.\n+                if not tf.executing_eagerly():\n+                    backend._initialize_variables(backend._get_session())\n+\n+        self._built = True\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        \"\"\"Accumulates confusion matrix statistics.\n+\n+        Args:\n+          y_true: The ground truth values.\n+          y_pred: The predicted values.\n+          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n+            and must be broadcastable to `y_true`.\n+\n+        Returns:\n+          Update op.\n+        \"\"\"\n+        if not self._built:\n+            self._build(tf.TensorShape(y_pred.shape))\n+\n+        if self.multi_label or (self.label_weights is not None):\n+            # y_true should have shape (number of examples, number of labels).\n+            shapes = [(y_true, (\"N\", \"L\"))]\n+            if self.multi_label:\n+                # TP, TN, FP, and FN should all have shape\n+                # (number of thresholds, number of labels).\n+                shapes.extend(\n+                    [\n+                        (self.true_positives, (\"T\", \"L\")),\n+                        (self.true_negatives, (\"T\", \"L\")),\n+                        (self.false_positives, (\"T\", \"L\")),\n+                        (self.false_negatives, (\"T\", \"L\")),\n+                    ]\n+                )\n+            if self.label_weights is not None:\n+                # label_weights should be of length equal to the number of\n+                # labels.\n+                shapes.append((self.label_weights, (\"L\",)))\n+                tf.debugging.assert_shapes(\n+                    shapes, message=\"Number of labels is not consistent.\"\n+                )\n+\n+        # Only forward label_weights to update_confusion_matrix_variables when\n+        # multi_label is False. Otherwise the averaging of individual label AUCs\n+        # is handled in AUC.result\n+        label_weights = None if self.multi_label else self.label_weights\n+\n+        if self._from_logits:\n+            y_pred = activations.sigmoid(y_pred)\n+\n+        return metrics_utils.update_confusion_matrix_variables(\n+            {\n+                metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,  # noqa: E501\n+                metrics_utils.ConfusionMatrix.TRUE_NEGATIVES: self.true_negatives,  # noqa: E501\n+                metrics_utils.ConfusionMatrix.FALSE_POSITIVES: self.false_positives,  # noqa: E501\n+                metrics_utils.ConfusionMatrix.FALSE_NEGATIVES: self.false_negatives,  # noqa: E501\n+            },\n+            y_true,\n+            y_pred,\n+            self._thresholds,\n+            thresholds_distributed_evenly=self._thresholds_distributed_evenly,\n+            sample_weight=sample_weight,\n+            multi_label=self.multi_label,\n+            label_weights=label_weights,\n+        )\n+\n+    def interpolate_pr_auc(self):\n+        \"\"\"Interpolation formula inspired by section 4 of Davis & Goadrich 2006.\n+\n+        https://www.biostat.wisc.edu/~page/rocpr.pdf\n+\n+        Note here we derive & use a closed formula not present in the paper\n+        as follows:\n+\n+          Precision = TP / (TP + FP) = TP / P\n+\n+        Modeling all of TP (true positive), FP (false positive) and their sum\n+        P = TP + FP (predicted positive) as varying linearly within each\n+        interval [A, B] between successive thresholds, we get\n+\n+          Precision slope = dTP / dP\n+                          = (TP_B - TP_A) / (P_B - P_A)\n+                          = (TP - TP_A) / (P - P_A)\n+          Precision = (TP_A + slope * (P - P_A)) / P\n+\n+        The area within the interval is (slope / total_pos_weight) times\n+\n+          int_A^B{Precision.dP} = int_A^B{(TP_A + slope * (P - P_A)) * dP / P}\n+          int_A^B{Precision.dP} = int_A^B{slope * dP + intercept * dP / P}\n+\n+        where intercept = TP_A - slope * P_A = TP_B - slope * P_B, resulting in\n+\n+          int_A^B{Precision.dP} = TP_B - TP_A + intercept * log(P_B / P_A)\n+\n+        Bringing back the factor (slope / total_pos_weight) we'd put aside, we\n+        get\n+\n+          slope * [dTP + intercept *  log(P_B / P_A)] / total_pos_weight\n+\n+        where dTP == TP_B - TP_A.\n+\n+        Note that when P_A == 0 the above calculation simplifies into\n+\n+          int_A^B{Precision.dTP} = int_A^B{slope * dTP} = slope * (TP_B - TP_A)\n+\n+        which is really equivalent to imputing constant precision throughout the\n+        first bucket having >0 true positives.\n+\n+        Returns:\n+          pr_auc: an approximation of the area under the P-R curve.\n+        \"\"\"\n+        dtp = (\n+            self.true_positives[: self.num_thresholds - 1]\n+            - self.true_positives[1:]\n+        )\n+        p = tf.math.add(self.true_positives, self.false_positives)\n+        dp = p[: self.num_thresholds - 1] - p[1:]\n+        prec_slope = tf.math.divide_no_nan(\n+            dtp, tf.maximum(dp, 0), name=\"prec_slope\"\n+        )\n+        intercept = self.true_positives[1:] - tf.multiply(prec_slope, p[1:])\n+\n+        safe_p_ratio = tf.where(\n+            tf.logical_and(p[: self.num_thresholds - 1] > 0, p[1:] > 0),\n+            tf.math.divide_no_nan(\n+                p[: self.num_thresholds - 1],\n+                tf.maximum(p[1:], 0),\n+                name=\"recall_relative_ratio\",\n+            ),\n+            tf.ones_like(p[1:]),\n+        )\n+\n+        pr_auc_increment = tf.math.divide_no_nan(\n+            prec_slope * (dtp + intercept * tf.math.log(safe_p_ratio)),\n+            tf.maximum(self.true_positives[1:] + self.false_negatives[1:], 0),\n+            name=\"pr_auc_increment\",\n+        )\n+\n+        if self.multi_label:\n+            by_label_auc = tf.reduce_sum(\n+                pr_auc_increment, name=self.name + \"_by_label\", axis=0\n+            )\n+            if self.label_weights is None:\n+                # Evenly weighted average of the label AUCs.\n+                return tf.reduce_mean(by_label_auc, name=self.name)\n+            else:\n+                # Weighted average of the label AUCs.\n+                return tf.math.divide_no_nan(\n+                    tf.reduce_sum(\n+                        tf.multiply(by_label_auc, self.label_weights)\n+                    ),\n+                    tf.reduce_sum(self.label_weights),\n+                    name=self.name,\n+                )\n+        else:\n+            return tf.reduce_sum(pr_auc_increment, name=\"interpolate_pr_auc\")\n+\n+    def result(self):\n+        if (\n+            self.curve == metrics_utils.AUCCurve.PR\n+            and self.summation_method\n+            == metrics_utils.AUCSummationMethod.INTERPOLATION\n+        ):\n+            # This use case is different and is handled separately.\n+            return self.interpolate_pr_auc()\n+\n+        # Set `x` and `y` values for the curves based on `curve` config.\n+        recall = tf.math.divide_no_nan(\n+            self.true_positives,\n+            tf.math.add(self.true_positives, self.false_negatives),\n+        )\n+        if self.curve == metrics_utils.AUCCurve.ROC:\n+            fp_rate = tf.math.divide_no_nan(\n+                self.false_positives,\n+                tf.math.add(self.false_positives, self.true_negatives),\n+            )\n+            x = fp_rate\n+            y = recall\n+        else:  # curve == 'PR'.\n+            precision = tf.math.divide_no_nan(\n+                self.true_positives,\n+                tf.math.add(self.true_positives, self.false_positives),\n+            )\n+            x = recall\n+            y = precision\n+\n+        # Find the rectangle heights based on `summation_method`.\n+        if (\n+            self.summation_method\n+            == metrics_utils.AUCSummationMethod.INTERPOLATION\n+        ):\n+            # Note: the case ('PR', 'interpolation') has been handled above.\n+            heights = (y[: self.num_thresholds - 1] + y[1:]) / 2.0\n+        elif self.summation_method == metrics_utils.AUCSummationMethod.MINORING:\n+            heights = tf.minimum(y[: self.num_thresholds - 1], y[1:])\n+        # self.summation_method = metrics_utils.AUCSummationMethod.MAJORING:\n+        else:\n+            heights = tf.maximum(y[: self.num_thresholds - 1], y[1:])\n+\n+        # Sum up the areas of all the rectangles.\n+        if self.multi_label:\n+            riemann_terms = tf.multiply(\n+                x[: self.num_thresholds - 1] - x[1:], heights\n+            )\n+            by_label_auc = tf.reduce_sum(\n+                riemann_terms, name=self.name + \"_by_label\", axis=0\n+            )\n+\n+            if self.label_weights is None:\n+                # Unweighted average of the label AUCs.\n+                return tf.reduce_mean(by_label_auc, name=self.name)\n+            else:\n+                # Weighted average of the label AUCs.\n+                return tf.math.divide_no_nan(\n+                    tf.reduce_sum(\n+                        tf.multiply(by_label_auc, self.label_weights)\n+                    ),\n+                    tf.reduce_sum(self.label_weights),\n+                    name=self.name,\n+                )\n+        else:\n+            return tf.reduce_sum(\n+                tf.multiply(x[: self.num_thresholds - 1] - x[1:], heights),\n+                name=self.name,\n+            )\n+\n+    def reset_state(self):\n+        if self._built:\n+            confusion_matrix_variables = (\n+                self.true_positives,\n+                self.true_negatives,\n+                self.false_positives,\n+                self.false_negatives,\n+            )\n+            if self.multi_label:\n+                backend.batch_set_value(\n+                    [\n+                        (v, np.zeros((self.num_thresholds, self._num_labels)))\n+                        for v in confusion_matrix_variables\n+                    ]\n+                )\n+            else:\n+                backend.batch_set_value(\n+                    [\n+                        (v, np.zeros((self.num_thresholds,)))\n+                        for v in confusion_matrix_variables\n+                    ]\n+                )\n+\n+    def get_config(self):\n+        if is_tensor_or_variable(self.label_weights):\n+            label_weights = backend.eval(self.label_weights)\n+        else:\n+            label_weights = self.label_weights\n+        config = {\n+            \"num_thresholds\": self.num_thresholds,\n+            \"curve\": self.curve.value,\n+            \"summation_method\": self.summation_method.value,\n+            \"multi_label\": self.multi_label,\n+            \"num_labels\": self.num_labels,\n+            \"label_weights\": label_weights,\n+            \"from_logits\": self._from_logits,\n+        }\n+        # optimization to avoid serializing a large number of generated\n+        # thresholds\n+        if self._init_from_thresholds:\n+            # We remove the endpoint thresholds as an inverse of how the\n+            # thresholds were initialized. This ensures that a metric\n+            # initialized from this config has the same thresholds.\n+            config[\"thresholds\"] = self.thresholds[1:-1]\n+        base_config = super().get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n\n@@ -12,23 +12,23 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Tests for Keras metrics functions.\"\"\"\n+\"\"\"Tests for confusion metrics.\"\"\"\n \n import json\n \n import numpy as np\n import tensorflow.compat.v2 as tf\n from absl.testing import parameterized\n+from tensorflow.python.platform import tf_logging\n \n+from keras import backend\n from keras import layers\n from keras import metrics\n from keras import models\n from keras.testing_infra import test_combinations\n+from keras.testing_infra import test_utils\n from keras.utils import metrics_utils\n \n-# isort: off\n-from tensorflow.python.platform import tf_logging\n-\n \n @test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n class FalsePositivesTest(tf.test.TestCase, parameterized.TestCase):\n@@ -2091,5 +2091,645 @@ class ThresholdsTest(tf.test.TestCase, parameterized.TestCase):\n                 self.assertAllClose(v1, v2)\n \n \n+class BinaryTruePositives(metrics.Metric):\n+    def __init__(self, name=\"binary_true_positives\", **kwargs):\n+        super().__init__(name=name, **kwargs)\n+        self.true_positives = self.add_weight(name=\"tp\", initializer=\"zeros\")\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        y_true = tf.cast(y_true, tf.bool)\n+        y_pred = tf.cast(y_pred, tf.bool)\n+\n+        values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))\n+        values = tf.cast(values, self.dtype)\n+        if sample_weight is not None:\n+            sample_weight = tf.cast(sample_weight, dtype=self.dtype)\n+            sample_weight = tf.__internal__.ops.broadcast_weights(\n+                sample_weight, values\n+            )\n+            values = tf.multiply(values, sample_weight)\n+        self.true_positives.assign_add(tf.reduce_sum(values))\n+\n+    def result(self):\n+        return self.true_positives\n+\n+\n+class BinaryTruePositivesViaControlFlow(metrics.Metric):\n+    def __init__(self, name=\"binary_true_positives\", **kwargs):\n+        super().__init__(name=name, **kwargs)\n+        self.true_positives = self.add_weight(name=\"tp\", initializer=\"zeros\")\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        y_true = tf.cast(y_true, tf.bool)\n+        y_pred = tf.cast(y_pred, tf.bool)\n+\n+        for i in range(len(y_true)):\n+            for j in range(len(y_true[i])):\n+                if y_true[i][j] and y_pred[i][j]:\n+                    if sample_weight is None:\n+                        self.true_positives.assign_add(1)\n+                    else:\n+                        self.true_positives.assign_add(sample_weight[i][0])\n+\n+    def result(self):\n+        if tf.constant(True):\n+            return self.true_positives\n+        return 0.0\n+\n+\n+def _get_model(compile_metrics):\n+    model_layers = [\n+        layers.Dense(3, activation=\"relu\", kernel_initializer=\"ones\"),\n+        layers.Dense(1, activation=\"sigmoid\", kernel_initializer=\"ones\"),\n+    ]\n+\n+    model = test_utils.get_model_from_layers(model_layers, input_shape=(4,))\n+    model.compile(\n+        loss=\"mae\",\n+        metrics=compile_metrics,\n+        optimizer=\"rmsprop\",\n+        run_eagerly=test_utils.should_run_eagerly(),\n+    )\n+    return model\n+\n+\n+@test_combinations.run_with_all_model_types\n+@test_combinations.run_all_keras_modes\n+class ResetStatesTest(test_combinations.TestCase):\n+    def test_reset_state_false_positives(self):\n+        fp_obj = metrics.FalsePositives()\n+        model = _get_model([fp_obj])\n+        x = np.ones((100, 4))\n+        y = np.zeros((100, 1))\n+        model.evaluate(x, y)\n+        self.assertEqual(self.evaluate(fp_obj.accumulator), 100.0)\n+        model.evaluate(x, y)\n+        self.assertEqual(self.evaluate(fp_obj.accumulator), 100.0)\n+\n+    def test_reset_state_false_negatives(self):\n+        fn_obj = metrics.FalseNegatives()\n+        model = _get_model([fn_obj])\n+        x = np.zeros((100, 4))\n+        y = np.ones((100, 1))\n+        model.evaluate(x, y)\n+        self.assertEqual(self.evaluate(fn_obj.accumulator), 100.0)\n+        model.evaluate(x, y)\n+        self.assertEqual(self.evaluate(fn_obj.accumulator), 100.0)\n+\n+    def test_reset_state_true_negatives(self):\n+        tn_obj = metrics.TrueNegatives()\n+        model = _get_model([tn_obj])\n+        x = np.zeros((100, 4))\n+        y = np.zeros((100, 1))\n+        model.evaluate(x, y)\n+        self.assertEqual(self.evaluate(tn_obj.accumulator), 100.0)\n+        model.evaluate(x, y)\n+        self.assertEqual(self.evaluate(tn_obj.accumulator), 100.0)\n+\n+    def test_reset_state_true_positives(self):\n+        tp_obj = metrics.TruePositives()\n+        model = _get_model([tp_obj])\n+        x = np.ones((100, 4))\n+        y = np.ones((100, 1))\n+        model.evaluate(x, y)\n+        self.assertEqual(self.evaluate(tp_obj.accumulator), 100.0)\n+        model.evaluate(x, y)\n+        self.assertEqual(self.evaluate(tp_obj.accumulator), 100.0)\n+\n+    def test_reset_state_precision(self):\n+        p_obj = metrics.Precision()\n+        model = _get_model([p_obj])\n+        x = np.concatenate((np.ones((50, 4)), np.ones((50, 4))))\n+        y = np.concatenate((np.ones((50, 1)), np.zeros((50, 1))))\n+        model.evaluate(x, y)\n+        self.assertEqual(self.evaluate(p_obj.true_positives), 50.0)\n+        self.assertEqual(self.evaluate(p_obj.false_positives), 50.0)\n+        model.evaluate(x, y)\n+        self.assertEqual(self.evaluate(p_obj.true_positives), 50.0)\n+        self.assertEqual(self.evaluate(p_obj.false_positives), 50.0)\n+\n+    def test_precision_update_state_with_logits(self):\n+        p_obj = metrics.Precision()\n+        # Update state with logits (not in range (0, 1)) should not an raise\n+        # error.\n+        p_obj.update_state([-0.5, 0.5], [-2.0, 2.0])\n+\n+    def test_reset_state_recall(self):\n+        r_obj = metrics.Recall()\n+        model = _get_model([r_obj])\n+        x = np.concatenate((np.ones((50, 4)), np.zeros((50, 4))))\n+        y = np.concatenate((np.ones((50, 1)), np.ones((50, 1))))\n+        model.evaluate(x, y)\n+        self.assertEqual(self.evaluate(r_obj.true_positives), 50.0)\n+        self.assertEqual(self.evaluate(r_obj.false_negatives), 50.0)\n+        model.evaluate(x, y)\n+        self.assertEqual(self.evaluate(r_obj.true_positives), 50.0)\n+        self.assertEqual(self.evaluate(r_obj.false_negatives), 50.0)\n+\n+    def test_reset_state_sensitivity_at_specificity(self):\n+        s_obj = metrics.SensitivityAtSpecificity(0.5, num_thresholds=1)\n+        model = _get_model([s_obj])\n+        x = np.concatenate(\n+            (\n+                np.ones((25, 4)),\n+                np.zeros((25, 4)),\n+                np.zeros((25, 4)),\n+                np.ones((25, 4)),\n+            )\n+        )\n+        y = np.concatenate(\n+            (\n+                np.ones((25, 1)),\n+                np.zeros((25, 1)),\n+                np.ones((25, 1)),\n+                np.zeros((25, 1)),\n+            )\n+        )\n+\n+        for _ in range(2):\n+            model.evaluate(x, y)\n+            self.assertEqual(self.evaluate(s_obj.true_positives), 25.0)\n+            self.assertEqual(self.evaluate(s_obj.false_positives), 25.0)\n+            self.assertEqual(self.evaluate(s_obj.false_negatives), 25.0)\n+            self.assertEqual(self.evaluate(s_obj.true_negatives), 25.0)\n+\n+    def test_reset_state_specificity_at_sensitivity(self):\n+        s_obj = metrics.SpecificityAtSensitivity(0.5, num_thresholds=1)\n+        model = _get_model([s_obj])\n+        x = np.concatenate(\n+            (\n+                np.ones((25, 4)),\n+                np.zeros((25, 4)),\n+                np.zeros((25, 4)),\n+                np.ones((25, 4)),\n+            )\n+        )\n+        y = np.concatenate(\n+            (\n+                np.ones((25, 1)),\n+                np.zeros((25, 1)),\n+                np.ones((25, 1)),\n+                np.zeros((25, 1)),\n+            )\n+        )\n+\n+        for _ in range(2):\n+            model.evaluate(x, y)\n+            self.assertEqual(self.evaluate(s_obj.true_positives), 25.0)\n+            self.assertEqual(self.evaluate(s_obj.false_positives), 25.0)\n+            self.assertEqual(self.evaluate(s_obj.false_negatives), 25.0)\n+            self.assertEqual(self.evaluate(s_obj.true_negatives), 25.0)\n+\n+    def test_reset_state_precision_at_recall(self):\n+        s_obj = metrics.PrecisionAtRecall(recall=0.5, num_thresholds=1)\n+        model = _get_model([s_obj])\n+        x = np.concatenate(\n+            (\n+                np.ones((25, 4)),\n+                np.zeros((25, 4)),\n+                np.zeros((25, 4)),\n+                np.ones((25, 4)),\n+            )\n+        )\n+        y = np.concatenate(\n+            (\n+                np.ones((25, 1)),\n+                np.zeros((25, 1)),\n+                np.ones((25, 1)),\n+                np.zeros((25, 1)),\n+            )\n+        )\n+\n+        for _ in range(2):\n+            model.evaluate(x, y)\n+            self.assertEqual(self.evaluate(s_obj.true_positives), 25.0)\n+            self.assertEqual(self.evaluate(s_obj.false_positives), 25.0)\n+            self.assertEqual(self.evaluate(s_obj.false_negatives), 25.0)\n+            self.assertEqual(self.evaluate(s_obj.true_negatives), 25.0)\n+\n+    def test_reset_state_recall_at_precision(self):\n+        s_obj = metrics.RecallAtPrecision(precision=0.5, num_thresholds=1)\n+        model = _get_model([s_obj])\n+        x = np.concatenate(\n+            (\n+                np.ones((25, 4)),\n+                np.zeros((25, 4)),\n+                np.zeros((25, 4)),\n+                np.ones((25, 4)),\n+            )\n+        )\n+        y = np.concatenate(\n+            (\n+                np.ones((25, 1)),\n+                np.zeros((25, 1)),\n+                np.ones((25, 1)),\n+                np.zeros((25, 1)),\n+            )\n+        )\n+\n+        for _ in range(2):\n+            model.evaluate(x, y)\n+            self.assertEqual(self.evaluate(s_obj.true_positives), 25.0)\n+            self.assertEqual(self.evaluate(s_obj.false_positives), 25.0)\n+            self.assertEqual(self.evaluate(s_obj.false_negatives), 25.0)\n+            self.assertEqual(self.evaluate(s_obj.true_negatives), 25.0)\n+\n+    def test_reset_state_auc(self):\n+        auc_obj = metrics.AUC(num_thresholds=3)\n+        model = _get_model([auc_obj])\n+        x = np.concatenate(\n+            (\n+                np.ones((25, 4)),\n+                np.zeros((25, 4)),\n+                np.zeros((25, 4)),\n+                np.ones((25, 4)),\n+            )\n+        )\n+        y = np.concatenate(\n+            (\n+                np.ones((25, 1)),\n+                np.zeros((25, 1)),\n+                np.ones((25, 1)),\n+                np.zeros((25, 1)),\n+            )\n+        )\n+\n+        for _ in range(2):\n+            model.evaluate(x, y)\n+            self.assertEqual(self.evaluate(auc_obj.true_positives[1]), 25.0)\n+            self.assertEqual(self.evaluate(auc_obj.false_positives[1]), 25.0)\n+            self.assertEqual(self.evaluate(auc_obj.false_negatives[1]), 25.0)\n+            self.assertEqual(self.evaluate(auc_obj.true_negatives[1]), 25.0)\n+\n+    def test_reset_state_auc_from_logits(self):\n+        auc_obj = metrics.AUC(num_thresholds=3, from_logits=True)\n+\n+        model_layers = [\n+            layers.Dense(1, kernel_initializer=\"ones\", use_bias=False)\n+        ]\n+        model = test_utils.get_model_from_layers(model_layers, input_shape=(4,))\n+        model.compile(\n+            loss=\"mae\",\n+            metrics=[auc_obj],\n+            optimizer=\"rmsprop\",\n+            run_eagerly=test_utils.should_run_eagerly(),\n+        )\n+\n+        x = np.concatenate(\n+            (\n+                np.ones((25, 4)),\n+                -np.ones((25, 4)),\n+                -np.ones((25, 4)),\n+                np.ones((25, 4)),\n+            )\n+        )\n+        y = np.concatenate(\n+            (\n+                np.ones((25, 1)),\n+                np.zeros((25, 1)),\n+                np.ones((25, 1)),\n+                np.zeros((25, 1)),\n+            )\n+        )\n+\n+        for _ in range(2):\n+            model.evaluate(x, y)\n+            self.assertEqual(self.evaluate(auc_obj.true_positives[1]), 25.0)\n+            self.assertEqual(self.evaluate(auc_obj.false_positives[1]), 25.0)\n+            self.assertEqual(self.evaluate(auc_obj.false_negatives[1]), 25.0)\n+            self.assertEqual(self.evaluate(auc_obj.true_negatives[1]), 25.0)\n+\n+    def test_reset_state_auc_manual_thresholds(self):\n+        auc_obj = metrics.AUC(thresholds=[0.5])\n+        model = _get_model([auc_obj])\n+        x = np.concatenate(\n+            (\n+                np.ones((25, 4)),\n+                np.zeros((25, 4)),\n+                np.zeros((25, 4)),\n+                np.ones((25, 4)),\n+            )\n+        )\n+        y = np.concatenate(\n+            (\n+                np.ones((25, 1)),\n+                np.zeros((25, 1)),\n+                np.ones((25, 1)),\n+                np.zeros((25, 1)),\n+            )\n+        )\n+\n+        for _ in range(2):\n+            model.evaluate(x, y)\n+            self.assertEqual(self.evaluate(auc_obj.true_positives[1]), 25.0)\n+            self.assertEqual(self.evaluate(auc_obj.false_positives[1]), 25.0)\n+            self.assertEqual(self.evaluate(auc_obj.false_negatives[1]), 25.0)\n+            self.assertEqual(self.evaluate(auc_obj.true_negatives[1]), 25.0)\n+\n+    def test_reset_state_mean_iou(self):\n+        m_obj = metrics.MeanIoU(num_classes=2)\n+        model = _get_model([m_obj])\n+        x = np.asarray(\n+            [[0, 0, 0, 0], [1, 1, 1, 1], [1, 0, 1, 0], [0, 1, 0, 1]],\n+            dtype=np.float32,\n+        )\n+        y = np.asarray([[0], [1], [1], [1]], dtype=np.float32)\n+        model.evaluate(x, y)\n+        self.assertArrayNear(self.evaluate(m_obj.total_cm)[0], [1, 0], 1e-1)\n+        self.assertArrayNear(self.evaluate(m_obj.total_cm)[1], [3, 0], 1e-1)\n+        model.evaluate(x, y)\n+        self.assertArrayNear(self.evaluate(m_obj.total_cm)[0], [1, 0], 1e-1)\n+        self.assertArrayNear(self.evaluate(m_obj.total_cm)[1], [3, 0], 1e-1)\n+\n+    def test_reset_state_recall_float64(self):\n+        # Test case for GitHub issue 36790.\n+        try:\n+            backend.set_floatx(\"float64\")\n+            r_obj = metrics.Recall()\n+            model = _get_model([r_obj])\n+            x = np.concatenate((np.ones((50, 4)), np.zeros((50, 4))))\n+            y = np.concatenate((np.ones((50, 1)), np.ones((50, 1))))\n+            model.evaluate(x, y)\n+            self.assertEqual(self.evaluate(r_obj.true_positives), 50.0)\n+            self.assertEqual(self.evaluate(r_obj.false_negatives), 50.0)\n+            model.evaluate(x, y)\n+            self.assertEqual(self.evaluate(r_obj.true_positives), 50.0)\n+            self.assertEqual(self.evaluate(r_obj.false_negatives), 50.0)\n+        finally:\n+            backend.set_floatx(\"float32\")\n+\n+    def test_function_wrapped_reset_state(self):\n+        m = metrics.Mean(name=\"my_mean\")\n+\n+        # check reset_state in function.\n+        @tf.function\n+        def reset_in_fn():\n+            m.reset_state()\n+            return m.update_state(100)\n+\n+        for _ in range(5):\n+            self.evaluate(reset_in_fn())\n+        self.assertEqual(self.evaluate(m.count), 1)\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class MergeStateTest(test_combinations.TestCase):\n+    def test_merge_state_incompatible_metrics(self):\n+        with self.assertRaisesRegex(\n+            ValueError, \"Metric .* is not compatible with .*\"\n+        ):\n+            obj1 = metrics.FalsePositives()\n+            self.evaluate(tf.compat.v1.variables_initializer(obj1.variables))\n+            obj2 = metrics.Accuracy()\n+            self.evaluate(tf.compat.v1.variables_initializer(obj2.variables))\n+            self.evaluate(obj1.merge_state([obj2]))\n+\n+    def test_merge_state_accuracy(self):\n+        a_objs = []\n+        for y_true, y_pred in zip(\n+            [[[1], [2]], [[3], [4]]], [[[0], [2]], [[3], [4]]]\n+        ):\n+            a_obj = metrics.Accuracy()\n+            a_objs.append(a_obj)\n+            self.evaluate(tf.compat.v1.variables_initializer(a_obj.variables))\n+            self.evaluate(a_obj.update_state(y_true, y_pred))\n+        self.evaluate(a_objs[0].merge_state(a_objs[1:]))\n+        self.assertEqual(self.evaluate(a_objs[0].total), 3.0)\n+        self.assertEqual(self.evaluate(a_objs[0].count), 4.0)\n+        self.assertEqual(self.evaluate(a_objs[0].result()), 0.75)\n+\n+    def test_merge_state_false_positives(self):\n+        fp_objs = []\n+        for _ in range(4):\n+            fp_obj = metrics.FalsePositives()\n+            fp_objs.append(fp_obj)\n+            self.evaluate(tf.compat.v1.variables_initializer(fp_obj.variables))\n+            y_true = np.zeros((25, 1))\n+            y_pred = np.ones((25, 1))\n+            self.evaluate(fp_obj.update_state(y_true, y_pred))\n+        self.evaluate(fp_objs[0].merge_state(fp_objs[1:]))\n+        self.assertEqual(self.evaluate(fp_objs[0].accumulator), 100.0)\n+\n+    def test_merge_state_false_negatives(self):\n+        fn_objs = []\n+        for _ in range(4):\n+            fn_obj = metrics.FalseNegatives()\n+            fn_objs.append(fn_obj)\n+            self.evaluate(tf.compat.v1.variables_initializer(fn_obj.variables))\n+            y_true = np.ones((25, 1))\n+            y_pred = np.zeros((25, 1))\n+            self.evaluate(fn_obj.update_state(y_true, y_pred))\n+        self.evaluate(fn_objs[0].merge_state(fn_objs[1:]))\n+        self.assertEqual(self.evaluate(fn_objs[0].accumulator), 100.0)\n+\n+    def test_merge_state_true_negatives(self):\n+        tn_objs = []\n+        for _ in range(4):\n+            tn_obj = metrics.TrueNegatives()\n+            tn_objs.append(tn_obj)\n+            self.evaluate(tf.compat.v1.variables_initializer(tn_obj.variables))\n+            y_true = np.zeros((25, 1))\n+            y_pred = np.zeros((25, 1))\n+            self.evaluate(tn_obj.update_state(y_true, y_pred))\n+        self.evaluate(tn_objs[0].merge_state(tn_objs[1:]))\n+        self.assertEqual(self.evaluate(tn_objs[0].accumulator), 100.0)\n+\n+    def test_merge_state_true_positives(self):\n+        tp_objs = []\n+        for _ in range(4):\n+            tp_obj = metrics.TruePositives()\n+            tp_objs.append(tp_obj)\n+            self.evaluate(tf.compat.v1.variables_initializer(tp_obj.variables))\n+            y_true = np.ones((25, 1))\n+            y_pred = np.ones((25, 1))\n+            self.evaluate(tp_obj.update_state(y_true, y_pred))\n+        self.evaluate(tp_objs[0].merge_state(tp_objs[1:]))\n+        self.assertEqual(self.evaluate(tp_objs[0].accumulator), 100.0)\n+\n+    def test_merge_state_precision(self):\n+        p_objs = []\n+        for _ in range(5):\n+            p_obj = metrics.Precision()\n+            p_objs.append(p_obj)\n+            self.evaluate(tf.compat.v1.variables_initializer(p_obj.variables))\n+            y_true = np.concatenate((np.ones((10, 1)), np.zeros((10, 1))))\n+            y_pred = np.concatenate((np.ones((10, 1)), np.ones((10, 1))))\n+            self.evaluate(p_obj.update_state(y_true, y_pred))\n+        self.evaluate(p_objs[0].merge_state(p_objs[1:]))\n+        self.assertEqual(self.evaluate(p_objs[0].true_positives), 50.0)\n+        self.assertEqual(self.evaluate(p_objs[0].false_positives), 50.0)\n+\n+    def test_merge_state_recall(self):\n+        r_objs = []\n+        for _ in range(5):\n+            r_obj = metrics.Recall()\n+            r_objs.append(r_obj)\n+            self.evaluate(tf.compat.v1.variables_initializer(r_obj.variables))\n+            y_true = np.concatenate((np.ones((10, 1)), np.ones((10, 1))))\n+            y_pred = np.concatenate((np.ones((10, 1)), np.zeros((10, 1))))\n+            self.evaluate(r_obj.update_state(y_true, y_pred))\n+        self.evaluate(r_objs[0].merge_state(r_objs[1:]))\n+        self.assertEqual(self.evaluate(r_objs[0].true_positives), 50.0)\n+        self.assertEqual(self.evaluate(r_objs[0].false_negatives), 50.0)\n+\n+    def test_merge_state_sensitivity_at_specificity(self):\n+        sas_objs = []\n+        for _ in range(5):\n+            sas_obj = metrics.SensitivityAtSpecificity(0.5, num_thresholds=1)\n+            sas_objs.append(sas_obj)\n+            self.evaluate(tf.compat.v1.variables_initializer(sas_obj.variables))\n+            y_true = np.concatenate(\n+                (\n+                    np.ones((5, 1)),\n+                    np.zeros((5, 1)),\n+                    np.ones((5, 1)),\n+                    np.zeros((5, 1)),\n+                )\n+            )\n+            y_pred = np.concatenate(\n+                (\n+                    np.ones((5, 1)),\n+                    np.zeros((5, 1)),\n+                    np.zeros((5, 1)),\n+                    np.ones((5, 1)),\n+                )\n+            )\n+            self.evaluate(sas_obj.update_state(y_true, y_pred))\n+        self.evaluate(sas_objs[0].merge_state(sas_objs[1:]))\n+        self.assertEqual(self.evaluate(sas_objs[0].true_positives), 25.0)\n+        self.assertEqual(self.evaluate(sas_objs[0].false_positives), 25.0)\n+        self.assertEqual(self.evaluate(sas_objs[0].false_negatives), 25.0)\n+        self.assertEqual(self.evaluate(sas_objs[0].true_negatives), 25.0)\n+\n+    def test_merge_state_specificity_at_sensitivity(self):\n+        sas_objs = []\n+        for _ in range(5):\n+            sas_obj = metrics.SpecificityAtSensitivity(0.5, num_thresholds=1)\n+            sas_objs.append(sas_obj)\n+            self.evaluate(tf.compat.v1.variables_initializer(sas_obj.variables))\n+            y_true = np.concatenate(\n+                (\n+                    np.ones((5, 1)),\n+                    np.zeros((5, 1)),\n+                    np.ones((5, 1)),\n+                    np.zeros((5, 1)),\n+                )\n+            )\n+            y_pred = np.concatenate(\n+                (\n+                    np.ones((5, 1)),\n+                    np.zeros((5, 1)),\n+                    np.zeros((5, 1)),\n+                    np.ones((5, 1)),\n+                )\n+            )\n+            self.evaluate(sas_obj.update_state(y_true, y_pred))\n+        self.evaluate(sas_objs[0].merge_state(sas_objs[1:]))\n+        self.assertEqual(self.evaluate(sas_objs[0].true_positives), 25.0)\n+        self.assertEqual(self.evaluate(sas_objs[0].false_positives), 25.0)\n+        self.assertEqual(self.evaluate(sas_objs[0].false_negatives), 25.0)\n+        self.assertEqual(self.evaluate(sas_objs[0].true_negatives), 25.0)\n+\n+    def test_merge_state_precision_at_recall(self):\n+        par_objs = []\n+        for _ in range(5):\n+            par_obj = metrics.PrecisionAtRecall(recall=0.5, num_thresholds=1)\n+            par_objs.append(par_obj)\n+            self.evaluate(tf.compat.v1.variables_initializer(par_obj.variables))\n+            y_true = np.concatenate(\n+                (\n+                    np.ones((5, 1)),\n+                    np.zeros((5, 1)),\n+                    np.ones((5, 1)),\n+                    np.zeros((5, 1)),\n+                )\n+            )\n+            y_pred = np.concatenate(\n+                (\n+                    np.ones((5, 1)),\n+                    np.zeros((5, 1)),\n+                    np.zeros((5, 1)),\n+                    np.ones((5, 1)),\n+                )\n+            )\n+            self.evaluate(par_obj.update_state(y_true, y_pred))\n+        self.evaluate(par_objs[0].merge_state(par_objs[1:]))\n+        self.assertEqual(self.evaluate(par_objs[0].true_positives), 25.0)\n+        self.assertEqual(self.evaluate(par_objs[0].false_positives), 25.0)\n+        self.assertEqual(self.evaluate(par_objs[0].false_negatives), 25.0)\n+        self.assertEqual(self.evaluate(par_objs[0].true_negatives), 25.0)\n+\n+    def test_merge_state_recall_at_precision(self):\n+        rap_objs = []\n+        for _ in range(5):\n+            rap_obj = metrics.PrecisionAtRecall(recall=0.5, num_thresholds=1)\n+            rap_objs.append(rap_obj)\n+            self.evaluate(tf.compat.v1.variables_initializer(rap_obj.variables))\n+            y_true = np.concatenate(\n+                (\n+                    np.ones((5, 1)),\n+                    np.zeros((5, 1)),\n+                    np.ones((5, 1)),\n+                    np.zeros((5, 1)),\n+                )\n+            )\n+            y_pred = np.concatenate(\n+                (\n+                    np.ones((5, 1)),\n+                    np.zeros((5, 1)),\n+                    np.zeros((5, 1)),\n+                    np.ones((5, 1)),\n+                )\n+            )\n+            self.evaluate(rap_obj.update_state(y_true, y_pred))\n+        self.evaluate(rap_objs[0].merge_state(rap_objs[1:]))\n+        self.assertEqual(self.evaluate(rap_objs[0].true_positives), 25.0)\n+        self.assertEqual(self.evaluate(rap_objs[0].false_positives), 25.0)\n+        self.assertEqual(self.evaluate(rap_objs[0].false_negatives), 25.0)\n+        self.assertEqual(self.evaluate(rap_objs[0].true_negatives), 25.0)\n+\n+    def test_merge_state_auc(self):\n+        auc_objs = []\n+        for _ in range(5):\n+            auc_obj = metrics.AUC(num_thresholds=3)\n+            auc_objs.append(auc_obj)\n+            self.evaluate(tf.compat.v1.variables_initializer(auc_obj.variables))\n+            y_true = np.concatenate(\n+                (\n+                    np.ones((5, 1)),\n+                    np.zeros((5, 1)),\n+                    np.ones((5, 1)),\n+                    np.zeros((5, 1)),\n+                )\n+            )\n+            y_pred = np.concatenate(\n+                (\n+                    np.ones((5, 1)),\n+                    np.zeros((5, 1)),\n+                    np.zeros((5, 1)),\n+                    np.ones((5, 1)),\n+                )\n+            )\n+            self.evaluate(auc_obj.update_state(y_true, y_pred))\n+        self.evaluate(auc_objs[0].merge_state(auc_objs[1:]))\n+        self.assertEqual(self.evaluate(auc_objs[0].true_positives[1]), 25.0)\n+        self.assertEqual(self.evaluate(auc_objs[0].false_positives[1]), 25.0)\n+        self.assertEqual(self.evaluate(auc_objs[0].false_negatives[1]), 25.0)\n+        self.assertEqual(self.evaluate(auc_objs[0].true_negatives[1]), 25.0)\n+\n+    def test_merge_state_mean_iou(self):\n+        m_objs = []\n+        for y_true, y_pred in zip(\n+            [[0], [1], [1], [1]], [[0.5], [1.0], [1.0], [1.0]]\n+        ):\n+            m_obj = metrics.MeanIoU(num_classes=2)\n+            m_objs.append(m_obj)\n+            self.evaluate(tf.compat.v1.variables_initializer(m_obj.variables))\n+            self.evaluate(m_obj.update_state(y_true, y_pred))\n+        self.evaluate(m_objs[0].merge_state(m_objs[1:]))\n+        self.assertArrayNear(self.evaluate(m_objs[0].total_cm)[0], [1, 0], 1e-1)\n+        self.assertArrayNear(self.evaluate(m_objs[0].total_cm)[1], [0, 3], 1e-1)\n+\n+\n if __name__ == \"__main__\":\n     tf.test.main()\n\n@@ -0,0 +1,136 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Hinge metrics.\"\"\"\n+\n+from keras.dtensor import utils as dtensor_utils\n+from keras.losses import categorical_hinge\n+from keras.losses import hinge\n+from keras.losses import squared_hinge\n+from keras.metrics import base_metric\n+\n+# isort: off\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export(\"keras.metrics.Hinge\")\n+class Hinge(base_metric.MeanMetricWrapper):\n+    \"\"\"Computes the hinge metric between `y_true` and `y_pred`.\n+\n+    `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are\n+    provided we will convert them to -1 or 1.\n+\n+    Args:\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.Hinge()\n+    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])\n+    >>> m.result().numpy()\n+    1.3\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],\n+    ...                sample_weight=[1, 0])\n+    >>> m.result().numpy()\n+    1.1\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.Hinge()])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(self, name=\"hinge\", dtype=None):\n+        super().__init__(hinge, name, dtype=dtype)\n+\n+\n+@keras_export(\"keras.metrics.SquaredHinge\")\n+class SquaredHinge(base_metric.MeanMetricWrapper):\n+    \"\"\"Computes the squared hinge metric between `y_true` and `y_pred`.\n+\n+    `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are\n+    provided we will convert them to -1 or 1.\n+\n+    Args:\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.SquaredHinge()\n+    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])\n+    >>> m.result().numpy()\n+    1.86\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],\n+    ...                sample_weight=[1, 0])\n+    >>> m.result().numpy()\n+    1.46\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[tf.keras.metrics.SquaredHinge()])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(self, name=\"squared_hinge\", dtype=None):\n+        super().__init__(squared_hinge, name, dtype=dtype)\n+\n+\n+@keras_export(\"keras.metrics.CategoricalHinge\")\n+class CategoricalHinge(base_metric.MeanMetricWrapper):\n+    \"\"\"Computes the categorical hinge metric between `y_true` and `y_pred`.\n+\n+    Args:\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.CategoricalHinge()\n+    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])\n+    >>> m.result().numpy()\n+    1.4000001\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],\n+    ...                sample_weight=[1, 0])\n+    >>> m.result().numpy()\n+    1.2\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[tf.keras.metrics.CategoricalHinge()])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(self, name=\"categorical_hinge\", dtype=None):\n+        super().__init__(categorical_hinge, name, dtype=dtype)\n\n@@ -0,0 +1,193 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for Keras metrics.\"\"\"\n+\n+import tensorflow.compat.v2 as tf\n+\n+from keras import metrics\n+from keras.testing_infra import test_combinations\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class HingeTest(tf.test.TestCase):\n+    def test_config(self):\n+        hinge_obj = metrics.Hinge(name=\"hinge\", dtype=tf.int32)\n+        self.assertEqual(hinge_obj.name, \"hinge\")\n+        self.assertEqual(hinge_obj._dtype, tf.int32)\n+\n+        # Check save and restore config\n+        hinge_obj2 = metrics.Hinge.from_config(hinge_obj.get_config())\n+        self.assertEqual(hinge_obj2.name, \"hinge\")\n+        self.assertEqual(hinge_obj2._dtype, tf.int32)\n+\n+    def test_unweighted(self):\n+        hinge_obj = metrics.Hinge()\n+        self.evaluate(tf.compat.v1.variables_initializer(hinge_obj.variables))\n+        y_true = tf.constant([[0, 1, 0, 1], [0, 0, 1, 1]])\n+        y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6], [-0.25, -1.0, 0.5, 0.6]])\n+\n+        # metric = max(0, 1-y_true * y_pred), where y_true is -1/1\n+\n+        # y_true = [[-1, 1, -1, 1], [-1, -1, 1, 1]]\n+        # y_true * y_pred = [[0.3, 0.2, 0.1, 1.6], [0.25, 1, 0.5, 0.6]]\n+        # 1 - y_true * y_pred = [[0.7, 0.8, 0.9, -0.6], [0.75, 0, 0.5, 0.4]]\n+        # metric = [(0.7 + 0.8 + 0.9 + 0) / 4, (0.75 + 0 + 0.5 + 0.4) / 4]\n+        #        = [0.6, 0.4125]\n+        # reduced metric = (0.6 + 0.4125) / 2\n+\n+        update_op = hinge_obj.update_state(y_true, y_pred)\n+        self.evaluate(update_op)\n+        result = hinge_obj.result()\n+        self.assertAllClose(0.506, result, atol=1e-3)\n+\n+    def test_weighted(self):\n+        hinge_obj = metrics.Hinge()\n+        self.evaluate(tf.compat.v1.variables_initializer(hinge_obj.variables))\n+        y_true = tf.constant([[-1, 1, -1, 1], [-1, -1, 1, 1]])\n+        y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6], [-0.25, -1.0, 0.5, 0.6]])\n+        sample_weight = tf.constant([1.5, 2.0])\n+\n+        # metric = max(0, 1-y_true * y_pred), where y_true is -1/1\n+\n+        # y_true * y_pred = [[0.3, 0.2, 0.1, 1.6], [0.25, 1, 0.5, 0.6]]\n+        # 1 - y_true * y_pred = [[0.7, 0.8, 0.9, -0.6], [0.75, 0, 0.5, 0.4]]\n+        # metric = [(0.7 + 0.8 + 0.9 + 0) / 4, (0.75 + 0 + 0.5 + 0.4) / 4]\n+        #        = [0.6, 0.4125]\n+        # weighted metric = [0.6 * 1.5, 0.4125 * 2]\n+        # reduced metric = (0.6 * 1.5 + 0.4125 * 2) / (1.5 + 2)\n+\n+        result = hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(0.493, self.evaluate(result), atol=1e-3)\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class SquaredHingeTest(tf.test.TestCase):\n+    def test_config(self):\n+        sq_hinge_obj = metrics.SquaredHinge(name=\"sq_hinge\", dtype=tf.int32)\n+        self.assertEqual(sq_hinge_obj.name, \"sq_hinge\")\n+        self.assertEqual(sq_hinge_obj._dtype, tf.int32)\n+\n+        # Check save and restore config\n+        sq_hinge_obj2 = metrics.SquaredHinge.from_config(\n+            sq_hinge_obj.get_config()\n+        )\n+        self.assertEqual(sq_hinge_obj2.name, \"sq_hinge\")\n+        self.assertEqual(sq_hinge_obj2._dtype, tf.int32)\n+\n+    def test_unweighted(self):\n+        sq_hinge_obj = metrics.SquaredHinge()\n+        self.evaluate(\n+            tf.compat.v1.variables_initializer(sq_hinge_obj.variables)\n+        )\n+        y_true = tf.constant([[0, 1, 0, 1], [0, 0, 1, 1]])\n+        y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6], [-0.25, -1.0, 0.5, 0.6]])\n+\n+        # metric = max(0, 1-y_true * y_pred), where y_true is -1/1\n+\n+        # y_true = [[-1, 1, -1, 1], [-1, -1, 1, 1]]\n+        # y_true * y_pred = [[0.3, 0.2, 0.1, 1.6], [0.25, 1, 0.5, 0.6]]\n+        # 1 - y_true * y_pred = [[0.7, 0.8, 0.9, -0.6], [0.75, 0, 0.5, 0.4]]\n+        # max(0, 1 - y_true * y_pred) = [[0.7, 0.8, 0.9, 0], [0.75, 0, 0.5,\n+        # 0.4]]\n+        # squared(max(0, 1 - y_true * y_pred)) = [[0.49, 0.64, 0.81, 0],\n+        #                                         [0.5625, 0, 0.25, 0.16]]\n+        # metric = [(0.49 + 0.64 + 0.81 + 0) / 4, (0.5625 + 0 + 0.25 + 0.16) /\n+        # 4]\n+        #        = [0.485, 0.2431]\n+        # reduced metric = (0.485 + 0.2431) / 2\n+\n+        update_op = sq_hinge_obj.update_state(y_true, y_pred)\n+        self.evaluate(update_op)\n+        result = sq_hinge_obj.result()\n+        self.assertAllClose(0.364, result, atol=1e-3)\n+\n+    def test_weighted(self):\n+        sq_hinge_obj = metrics.SquaredHinge()\n+        self.evaluate(\n+            tf.compat.v1.variables_initializer(sq_hinge_obj.variables)\n+        )\n+        y_true = tf.constant([[-1, 1, -1, 1], [-1, -1, 1, 1]])\n+        y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6], [-0.25, -1.0, 0.5, 0.6]])\n+        sample_weight = tf.constant([1.5, 2.0])\n+\n+        # metric = max(0, 1-y_true * y_pred), where y_true is -1/1\n+\n+        # y_true * y_pred = [[0.3, 0.2, 0.1, 1.6], [0.25, 1, 0.5, 0.6]]\n+        # 1 - y_true * y_pred = [[0.7, 0.8, 0.9, -0.6], [0.75, 0, 0.5, 0.4]]\n+        # max(0, 1 - y_true * y_pred) = [[0.7, 0.8, 0.9, 0], [0.75, 0, 0.5,\n+        # 0.4]]\n+        # squared(max(0, 1 - y_true * y_pred)) = [[0.49, 0.64, 0.81, 0],\n+        #                                         [0.5625, 0, 0.25, 0.16]]\n+        # metric = [(0.49 + 0.64 + 0.81 + 0) / 4, (0.5625 + 0 + 0.25 + 0.16) /\n+        # 4]\n+        #        = [0.485, 0.2431]\n+        # weighted metric = [0.485 * 1.5, 0.2431 * 2]\n+        # reduced metric = (0.485 * 1.5 + 0.2431 * 2) / (1.5 + 2)\n+\n+        result = sq_hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(0.347, self.evaluate(result), atol=1e-3)\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class CategoricalHingeTest(tf.test.TestCase):\n+    def test_config(self):\n+        cat_hinge_obj = metrics.CategoricalHinge(\n+            name=\"cat_hinge\", dtype=tf.int32\n+        )\n+        self.assertEqual(cat_hinge_obj.name, \"cat_hinge\")\n+        self.assertEqual(cat_hinge_obj._dtype, tf.int32)\n+\n+        # Check save and restore config\n+        cat_hinge_obj2 = metrics.CategoricalHinge.from_config(\n+            cat_hinge_obj.get_config()\n+        )\n+        self.assertEqual(cat_hinge_obj2.name, \"cat_hinge\")\n+        self.assertEqual(cat_hinge_obj2._dtype, tf.int32)\n+\n+    def test_unweighted(self):\n+        cat_hinge_obj = metrics.CategoricalHinge()\n+        self.evaluate(\n+            tf.compat.v1.variables_initializer(cat_hinge_obj.variables)\n+        )\n+        y_true = tf.constant(\n+            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        )\n+        y_pred = tf.constant(\n+            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+        )\n+\n+        update_op = cat_hinge_obj.update_state(y_true, y_pred)\n+        self.evaluate(update_op)\n+        result = cat_hinge_obj.result()\n+        self.assertAllClose(0.5, result, atol=1e-5)\n+\n+    def test_weighted(self):\n+        cat_hinge_obj = metrics.CategoricalHinge()\n+        self.evaluate(\n+            tf.compat.v1.variables_initializer(cat_hinge_obj.variables)\n+        )\n+        y_true = tf.constant(\n+            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        )\n+        y_pred = tf.constant(\n+            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+        )\n+        sample_weight = tf.constant((1.0, 1.5, 2.0, 2.5))\n+        result = cat_hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(0.5, self.evaluate(result), atol=1e-5)\n+\n+\n+if __name__ == \"__main__\":\n+    tf.test.main()\n\n@@ -0,0 +1,757 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"IoU metrics.\"\"\"\n+\n+from typing import List\n+from typing import Optional\n+from typing import Tuple\n+from typing import Union\n+\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+from keras import backend\n+from keras.dtensor import utils as dtensor_utils\n+from keras.metrics import base_metric\n+\n+# isort: off\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+class _IoUBase(base_metric.Metric):\n+    \"\"\"Computes the confusion matrix for Intersection-Over-Union metrics.\n+\n+    Intersection-Over-Union is a common evaluation metric for semantic image\n+    segmentation.\n+\n+    For an individual class, the IoU metric is defined as follows:\n+\n+    ```\n+    iou = true_positives / (true_positives + false_positives + false_negatives)\n+    ```\n+\n+    From IoUs of individual classes, the MeanIoU can be computed as the mean of\n+    the individual IoUs.\n+\n+    To compute IoUs, the predictions are accumulated in a confusion matrix,\n+    weighted by `sample_weight` and the metric is then calculated from it.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Args:\n+      num_classes: The possible number of labels the prediction task can have.\n+        This value must be provided, since a confusion matrix of size\n+        `(num_classes, num_classes)` will be allocated.\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+      ignore_class: Optional integer. The ID of a class to be ignored during\n+        metric computation. This is useful, for example, in segmentation\n+        problems featuring a \"void\" class (commonly -1 or 255) in segmentation\n+        maps. By default (`ignore_class=None`), all classes are considered.\n+      sparse_y_true: Whether labels are encoded using integers or\n+        dense floating point vectors. If `False`, the `tf.argmax` function\n+        will be used to determine each sample's most likely associated label.\n+      sparse_y_pred: Whether predictions are encoded using integers or\n+        dense floating point vectors. If `False`, the `tf.argmax` function\n+        will be used to determine each sample's most likely associated label.\n+      axis: (Optional) Defaults to -1. The dimension containing the logits.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        num_classes: int,\n+        name: Optional[str] = None,\n+        dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n+        ignore_class: Optional[int] = None,\n+        sparse_y_true: bool = True,\n+        sparse_y_pred: bool = True,\n+        axis: int = -1,\n+    ):\n+        super().__init__(name=name, dtype=dtype)\n+        self.num_classes = num_classes\n+        self.ignore_class = ignore_class\n+        self.sparse_y_true = sparse_y_true\n+        self.sparse_y_pred = sparse_y_pred\n+        self.axis = axis\n+\n+        # Variable to accumulate the predictions in the confusion matrix.\n+        self.total_cm = self.add_weight(\n+            \"total_confusion_matrix\",\n+            shape=(num_classes, num_classes),\n+            initializer=\"zeros\",\n+        )\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        \"\"\"Accumulates the confusion matrix statistics.\n+\n+        Args:\n+          y_true: The ground truth values.\n+          y_pred: The predicted values.\n+          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n+            and must be broadcastable to `y_true`.\n+\n+        Returns:\n+          Update op.\n+        \"\"\"\n+\n+        if not self.sparse_y_true:\n+            y_true = tf.argmax(y_true, axis=self.axis)\n+        if not self.sparse_y_pred:\n+            y_pred = tf.argmax(y_pred, axis=self.axis)\n+\n+        y_true = tf.cast(y_true, self._dtype)\n+        y_pred = tf.cast(y_pred, self._dtype)\n+\n+        # Flatten the input if its rank > 1.\n+        if y_pred.shape.ndims > 1:\n+            y_pred = tf.reshape(y_pred, [-1])\n+\n+        if y_true.shape.ndims > 1:\n+            y_true = tf.reshape(y_true, [-1])\n+\n+        if sample_weight is not None:\n+            sample_weight = tf.cast(sample_weight, self._dtype)\n+            if sample_weight.shape.ndims > 1:\n+                sample_weight = tf.reshape(sample_weight, [-1])\n+\n+        if self.ignore_class is not None:\n+            ignore_class = tf.cast(self.ignore_class, y_true.dtype)\n+            valid_mask = tf.not_equal(y_true, ignore_class)\n+            y_true = y_true[valid_mask]\n+            y_pred = y_pred[valid_mask]\n+            if sample_weight is not None:\n+                sample_weight = sample_weight[valid_mask]\n+\n+        # Accumulate the prediction to current confusion matrix.\n+        current_cm = tf.math.confusion_matrix(\n+            y_true,\n+            y_pred,\n+            self.num_classes,\n+            weights=sample_weight,\n+            dtype=self._dtype,\n+        )\n+        return self.total_cm.assign_add(current_cm)\n+\n+    def reset_state(self):\n+        backend.set_value(\n+            self.total_cm, np.zeros((self.num_classes, self.num_classes))\n+        )\n+\n+\n+@keras_export(\"keras.metrics.IoU\")\n+class IoU(_IoUBase):\n+    \"\"\"Computes the Intersection-Over-Union metric for specific target classes.\n+\n+    General definition and computation:\n+\n+    Intersection-Over-Union is a common evaluation metric for semantic image\n+    segmentation.\n+\n+    For an individual class, the IoU metric is defined as follows:\n+\n+    ```\n+    iou = true_positives / (true_positives + false_positives + false_negatives)\n+    ```\n+\n+    To compute IoUs, the predictions are accumulated in a confusion matrix,\n+    weighted by `sample_weight` and the metric is then calculated from it.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Note, this class first computes IoUs for all individual classes, then\n+    returns the mean of IoUs for the classes that are specified by\n+    `target_class_ids`. If `target_class_ids` has only one id value, the IoU of\n+    that specific class is returned.\n+\n+    Args:\n+      num_classes: The possible number of labels the prediction task can have.\n+        A confusion matrix of dimension = [num_classes, num_classes] will be\n+        allocated to accumulate predictions from which the metric is calculated.\n+      target_class_ids: A tuple or list of target class ids for which the metric\n+        is returned. To compute IoU for a specific class, a list (or tuple) of a\n+        single id value should be provided.\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+      ignore_class: Optional integer. The ID of a class to be ignored during\n+        metric computation. This is useful, for example, in segmentation\n+        problems featuring a \"void\" class (commonly -1 or 255) in segmentation\n+        maps. By default (`ignore_class=None`), all classes are considered.\n+      sparse_y_true: Whether labels are encoded using integers or\n+        dense floating point vectors. If `False`, the `tf.argmax` function\n+        will be used to determine each sample's most likely associated label.\n+      sparse_y_pred: Whether predictions are encoded using integers or\n+        dense floating point vectors. If `False`, the `tf.argmax` function\n+        will be used to determine each sample's most likely associated label.\n+      axis: (Optional) Defaults to -1. The dimension containing the logits.\n+\n+    Standalone usage:\n+\n+    >>> # cm = [[1, 1],\n+    >>> #        [1, 1]]\n+    >>> # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]\n+    >>> # iou = true_positives / (sum_row + sum_col - true_positives))\n+    >>> # iou = [0.33, 0.33]\n+    >>> m = tf.keras.metrics.IoU(num_classes=2, target_class_ids=[0])\n+    >>> m.update_state([0, 0, 1, 1], [0, 1, 0, 1])\n+    >>> m.result().numpy()\n+    0.33333334\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 0, 1, 1], [0, 1, 0, 1],\n+    ...                sample_weight=[0.3, 0.3, 0.3, 0.1])\n+    >>> # cm = [[0.3, 0.3],\n+    >>> #        [0.3, 0.1]]\n+    >>> # sum_row = [0.6, 0.4], sum_col = [0.6, 0.4],\n+    >>> # true_positives = [0.3, 0.1]\n+    >>> # iou = [0.33, 0.14]\n+    >>> m.result().numpy()\n+    0.33333334\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+      optimizer='sgd',\n+      loss='mse',\n+      metrics=[tf.keras.metrics.IoU(num_classes=2, target_class_ids=[0])])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(\n+        self,\n+        num_classes: int,\n+        target_class_ids: Union[List[int], Tuple[int, ...]],\n+        name: Optional[str] = None,\n+        dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n+        ignore_class: Optional[int] = None,\n+        sparse_y_true: bool = True,\n+        sparse_y_pred: bool = True,\n+        axis: int = -1,\n+    ):\n+        super().__init__(\n+            name=name,\n+            num_classes=num_classes,\n+            ignore_class=ignore_class,\n+            sparse_y_true=sparse_y_true,\n+            sparse_y_pred=sparse_y_pred,\n+            axis=axis,\n+            dtype=dtype,\n+        )\n+        if max(target_class_ids) >= num_classes:\n+            raise ValueError(\n+                f\"Target class id {max(target_class_ids)} \"\n+                \"is out of range, which is \"\n+                f\"[{0}, {num_classes}).\"\n+            )\n+        self.target_class_ids = list(target_class_ids)\n+\n+    def result(self):\n+        \"\"\"Compute the intersection-over-union via the confusion matrix.\"\"\"\n+        sum_over_row = tf.cast(\n+            tf.reduce_sum(self.total_cm, axis=0), dtype=self._dtype\n+        )\n+        sum_over_col = tf.cast(\n+            tf.reduce_sum(self.total_cm, axis=1), dtype=self._dtype\n+        )\n+        true_positives = tf.cast(\n+            tf.linalg.tensor_diag_part(self.total_cm), dtype=self._dtype\n+        )\n+\n+        # sum_over_row + sum_over_col =\n+        #     2 * true_positives + false_positives + false_negatives.\n+        denominator = sum_over_row + sum_over_col - true_positives\n+\n+        # Only keep the target classes\n+        true_positives = tf.gather(true_positives, self.target_class_ids)\n+        denominator = tf.gather(denominator, self.target_class_ids)\n+\n+        # If the denominator is 0, we need to ignore the class.\n+        num_valid_entries = tf.reduce_sum(\n+            tf.cast(tf.not_equal(denominator, 0), dtype=self._dtype)\n+        )\n+\n+        iou = tf.math.divide_no_nan(true_positives, denominator)\n+\n+        return tf.math.divide_no_nan(\n+            tf.reduce_sum(iou, name=\"mean_iou\"), num_valid_entries\n+        )\n+\n+    def get_config(self):\n+        config = {\n+            \"num_classes\": self.num_classes,\n+            \"target_class_ids\": self.target_class_ids,\n+            \"ignore_class\": self.ignore_class,\n+            \"sparse_y_true\": self.sparse_y_true,\n+            \"sparse_y_pred\": self.sparse_y_pred,\n+            \"axis\": self.axis,\n+        }\n+        base_config = super().get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+@keras_export(\"keras.metrics.BinaryIoU\")\n+class BinaryIoU(IoU):\n+    \"\"\"Computes the Intersection-Over-Union metric for class 0 and/or 1.\n+\n+    General definition and computation:\n+\n+    Intersection-Over-Union is a common evaluation metric for semantic image\n+    segmentation.\n+\n+    For an individual class, the IoU metric is defined as follows:\n+\n+    ```\n+    iou = true_positives / (true_positives + false_positives + false_negatives)\n+    ```\n+\n+    To compute IoUs, the predictions are accumulated in a confusion matrix,\n+    weighted by `sample_weight` and the metric is then calculated from it.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    This class can be used to compute IoUs for a binary classification task\n+    where the predictions are provided as logits. First a `threshold` is applied\n+    to the predicted values such that those that are below the `threshold` are\n+    converted to class 0 and those that are above the `threshold` are converted\n+    to class 1.\n+\n+    IoUs for classes 0 and 1 are then computed, the mean of IoUs for the classes\n+    that are specified by `target_class_ids` is returned.\n+\n+    Note: with `threshold=0`, this metric has the same behavior as `IoU`.\n+\n+    Args:\n+      target_class_ids: A tuple or list of target class ids for which the metric\n+        is returned. Options are `[0]`, `[1]`, or `[0, 1]`. With `[0]` (or\n+        `[1]`), the IoU metric for class 0 (or class 1, respectively) is\n+        returned. With `[0, 1]`, the mean of IoUs for the two classes is\n+        returned.\n+      threshold: A threshold that applies to the prediction logits to convert\n+        them to either predicted class 0 if the logit is below `threshold` or\n+        predicted class 1 if the logit is above `threshold`.\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.BinaryIoU(target_class_ids=[0, 1], threshold=0.3)\n+    >>> m.update_state([0, 1, 0, 1], [0.1, 0.2, 0.4, 0.7])\n+    >>> m.result().numpy()\n+    0.33333334\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 1, 0, 1], [0.1, 0.2, 0.4, 0.7],\n+    ...                sample_weight=[0.2, 0.3, 0.4, 0.1])\n+    >>> # cm = [[0.2, 0.4],\n+    >>> #        [0.3, 0.1]]\n+    >>> # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5],\n+    >>> # true_positives = [0.2, 0.1]\n+    >>> # iou = [0.222, 0.125]\n+    >>> m.result().numpy()\n+    0.17361112\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+      optimizer='sgd',\n+      loss='mse',\n+      metrics=[tf.keras.metrics.BinaryIoU(target_class_ids=[0], threshold=0.5)])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(\n+        self,\n+        target_class_ids: Union[List[int], Tuple[int, ...]] = (0, 1),\n+        threshold=0.5,\n+        name=None,\n+        dtype=None,\n+    ):\n+\n+        super().__init__(\n+            num_classes=2,\n+            target_class_ids=target_class_ids,\n+            name=name,\n+            dtype=dtype,\n+        )\n+        self.threshold = threshold\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        \"\"\"Accumulates the confusion matrix statistics.\n+\n+        Before the confusion matrix is updated, the predicted values are\n+        thresholded to be:\n+          0 for values that are smaller than the `threshold`\n+          1 for values that are larger or equal to the `threshold`\n+\n+        Args:\n+          y_true: The ground truth values.\n+          y_pred: The predicted values.\n+          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n+            and must be broadcastable to `y_true`.\n+\n+        Returns:\n+          Update op.\n+        \"\"\"\n+        y_pred = tf.cast(y_pred, self._dtype)\n+        y_pred = tf.cast(y_pred >= self.threshold, self._dtype)\n+        return super().update_state(y_true, y_pred, sample_weight)\n+\n+    def get_config(self):\n+        return {\n+            \"target_class_ids\": self.target_class_ids,\n+            \"threshold\": self.threshold,\n+            \"name\": self.name,\n+            \"dtype\": self._dtype,\n+        }\n+\n+\n+@keras_export(\"keras.metrics.MeanIoU\")\n+class MeanIoU(IoU):\n+    \"\"\"Computes the mean Intersection-Over-Union metric.\n+\n+    General definition and computation:\n+\n+    Intersection-Over-Union is a common evaluation metric for semantic image\n+    segmentation.\n+\n+    For an individual class, the IoU metric is defined as follows:\n+\n+    ```\n+    iou = true_positives / (true_positives + false_positives + false_negatives)\n+    ```\n+\n+    To compute IoUs, the predictions are accumulated in a confusion matrix,\n+    weighted by `sample_weight` and the metric is then calculated from it.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Note that this class first computes IoUs for all individual classes, then\n+    returns the mean of these values.\n+\n+    Args:\n+      num_classes: The possible number of labels the prediction task can have.\n+        This value must be provided, since a confusion matrix of dimension =\n+        [num_classes, num_classes] will be allocated.\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+      ignore_class: Optional integer. The ID of a class to be ignored during\n+        metric computation. This is useful, for example, in segmentation\n+        problems featuring a \"void\" class (commonly -1 or 255) in segmentation\n+        maps. By default (`ignore_class=None`), all classes are considered.\n+      sparse_y_true: Whether labels are encoded using integers or\n+        dense floating point vectors. If `False`, the `tf.argmax` function\n+        will be used to determine each sample's most likely associated label.\n+      sparse_y_pred: Whether predictions are encoded using integers or\n+        dense floating point vectors. If `False`, the `tf.argmax` function\n+        will be used to determine each sample's most likely associated label.\n+      axis: (Optional) Defaults to -1. The dimension containing the logits.\n+\n+    Standalone usage:\n+\n+    >>> # cm = [[1, 1],\n+    >>> #        [1, 1]]\n+    >>> # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]\n+    >>> # iou = true_positives / (sum_row + sum_col - true_positives))\n+    >>> # result = (1 / (2 + 2 - 1) + 1 / (2 + 2 - 1)) / 2 = 0.33\n+    >>> m = tf.keras.metrics.MeanIoU(num_classes=2)\n+    >>> m.update_state([0, 0, 1, 1], [0, 1, 0, 1])\n+    >>> m.result().numpy()\n+    0.33333334\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([0, 0, 1, 1], [0, 1, 0, 1],\n+    ...                sample_weight=[0.3, 0.3, 0.3, 0.1])\n+    >>> m.result().numpy()\n+    0.23809525\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+      optimizer='sgd',\n+      loss='mse',\n+      metrics=[tf.keras.metrics.MeanIoU(num_classes=2)])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(\n+        self,\n+        num_classes: int,\n+        name: Optional[str] = None,\n+        dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n+        ignore_class: Optional[int] = None,\n+        sparse_y_true: bool = True,\n+        sparse_y_pred: bool = True,\n+        axis: int = -1,\n+    ):\n+        target_class_ids = list(range(num_classes))\n+        super().__init__(\n+            name=name,\n+            num_classes=num_classes,\n+            target_class_ids=target_class_ids,\n+            axis=axis,\n+            dtype=dtype,\n+            ignore_class=ignore_class,\n+            sparse_y_true=sparse_y_true,\n+            sparse_y_pred=sparse_y_pred,\n+        )\n+\n+    def get_config(self):\n+        return {\n+            \"num_classes\": self.num_classes,\n+            \"name\": self.name,\n+            \"dtype\": self._dtype,\n+            \"ignore_class\": self.ignore_class,\n+            \"sparse_y_true\": self.sparse_y_true,\n+            \"sparse_y_pred\": self.sparse_y_pred,\n+            \"axis\": self.axis,\n+        }\n+\n+\n+@keras_export(\"keras.metrics.OneHotIoU\")\n+class OneHotIoU(IoU):\n+    \"\"\"Computes the Intersection-Over-Union metric for one-hot encoded labels.\n+\n+    General definition and computation:\n+\n+    Intersection-Over-Union is a common evaluation metric for semantic image\n+    segmentation.\n+\n+    For an individual class, the IoU metric is defined as follows:\n+\n+    ```\n+    iou = true_positives / (true_positives + false_positives + false_negatives)\n+    ```\n+\n+    To compute IoUs, the predictions are accumulated in a confusion matrix,\n+    weighted by `sample_weight` and the metric is then calculated from it.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    This class can be used to compute IoU for multi-class classification tasks\n+    where the labels are one-hot encoded (the last axis should have one\n+    dimension per class). Note that the predictions should also have the same\n+    shape. To compute the IoU, first the labels and predictions are converted\n+    back into integer format by taking the argmax over the class axis. Then the\n+    same computation steps as for the base `IoU` class apply.\n+\n+    Note, if there is only one channel in the labels and predictions, this class\n+    is the same as class `IoU`. In this case, use `IoU` instead.\n+\n+    Also, make sure that `num_classes` is equal to the number of classes in the\n+    data, to avoid a \"labels out of bound\" error when the confusion matrix is\n+    computed.\n+\n+    Args:\n+      num_classes: The possible number of labels the prediction task can have.\n+        A confusion matrix of shape `(num_classes, num_classes)` will be\n+        allocated to accumulate predictions from which the metric is calculated.\n+      target_class_ids: A tuple or list of target class ids for which the metric\n+        is returned. To compute IoU for a specific class, a list (or tuple) of a\n+        single id value should be provided.\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+      ignore_class: Optional integer. The ID of a class to be ignored during\n+        metric computation. This is useful, for example, in segmentation\n+        problems featuring a \"void\" class (commonly -1 or 255) in segmentation\n+        maps. By default (`ignore_class=None`), all classes are considered.\n+      sparse_y_pred: Whether predictions are encoded using natural numbers or\n+        probability distribution vectors. If `False`, the `tf.argmax` function\n+        will be used to determine each sample's most likely associated label.\n+      axis: (Optional) Defaults to -1. The dimension containing the logits.\n+\n+    Standalone usage:\n+\n+    >>> y_true = tf.constant([[0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0]])\n+    >>> y_pred = tf.constant([[0.2, 0.3, 0.5], [0.1, 0.2, 0.7], [0.5, 0.3, 0.1],\n+    ...                       [0.1, 0.4, 0.5]])\n+    >>> sample_weight = [0.1, 0.2, 0.3, 0.4]\n+    >>> m = tf.keras.metrics.OneHotIoU(num_classes=3, target_class_ids=[0, 2])\n+    >>> m.update_state(\n+    ...     y_true=y_true, y_pred=y_pred, sample_weight=sample_weight)\n+    >>> # cm = [[0, 0, 0.2+0.4],\n+    >>> #       [0.3, 0, 0],\n+    >>> #       [0, 0, 0.1]]\n+    >>> # sum_row = [0.3, 0, 0.7], sum_col = [0.6, 0.3, 0.1]\n+    >>> # true_positives = [0, 0, 0.1]\n+    >>> # single_iou = true_positives / (sum_row + sum_col - true_positives))\n+    >>> # mean_iou = (0 / (0.3 + 0.6 - 0) + 0.1 / (0.7 + 0.1 - 0.1)) / 2\n+    >>> m.result().numpy()\n+    0.071\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+      optimizer='sgd',\n+      loss='mse',\n+      metrics=[tf.keras.metrics.OneHotIoU(num_classes=3, target_class_id=[1])])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(\n+        self,\n+        num_classes: int,\n+        target_class_ids: Union[List[int], Tuple[int, ...]],\n+        name=None,\n+        dtype=None,\n+        ignore_class: Optional[int] = None,\n+        sparse_y_pred: bool = False,\n+        axis: int = -1,\n+    ):\n+        super().__init__(\n+            num_classes=num_classes,\n+            target_class_ids=target_class_ids,\n+            name=name,\n+            dtype=dtype,\n+            ignore_class=ignore_class,\n+            sparse_y_true=False,\n+            sparse_y_pred=sparse_y_pred,\n+            axis=axis,\n+        )\n+\n+    def get_config(self):\n+        return {\n+            \"num_classes\": self.num_classes,\n+            \"target_class_ids\": self.target_class_ids,\n+            \"name\": self.name,\n+            \"dtype\": self._dtype,\n+            \"ignore_class\": self.ignore_class,\n+            \"sparse_y_pred\": self.sparse_y_pred,\n+            \"axis\": self.axis,\n+        }\n+\n+\n+@keras_export(\"keras.metrics.OneHotMeanIoU\")\n+class OneHotMeanIoU(MeanIoU):\n+    \"\"\"Computes mean Intersection-Over-Union metric for one-hot encoded labels.\n+\n+    General definition and computation:\n+\n+    Intersection-Over-Union is a common evaluation metric for semantic image\n+    segmentation.\n+\n+    For an individual class, the IoU metric is defined as follows:\n+\n+    ```\n+    iou = true_positives / (true_positives + false_positives + false_negatives)\n+    ```\n+\n+    To compute IoUs, the predictions are accumulated in a confusion matrix,\n+    weighted by `sample_weight` and the metric is then calculated from it.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    This class can be used to compute the mean IoU for multi-class\n+    classification tasks where the labels are one-hot encoded (the last axis\n+    should have one dimension per class). Note that the predictions should also\n+    have the same shape. To compute the mean IoU, first the labels and\n+    predictions are converted back into integer format by taking the argmax over\n+    the class axis. Then the same computation steps as for the base `MeanIoU`\n+    class apply.\n+\n+    Note, if there is only one channel in the labels and predictions, this class\n+    is the same as class `MeanIoU`. In this case, use `MeanIoU` instead.\n+\n+    Also, make sure that `num_classes` is equal to the number of classes in the\n+    data, to avoid a \"labels out of bound\" error when the confusion matrix is\n+    computed.\n+\n+    Args:\n+      num_classes: The possible number of labels the prediction task can have.\n+        A confusion matrix of shape `(num_classes, num_classes)` will be\n+        allocated to accumulate predictions from which the metric is calculated.\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+      ignore_class: Optional integer. The ID of a class to be ignored during\n+        metric computation. This is useful, for example, in segmentation\n+        problems featuring a \"void\" class (commonly -1 or 255) in segmentation\n+        maps. By default (`ignore_class=None`), all classes are considered.\n+      sparse_y_pred: Whether predictions are encoded using natural numbers or\n+        probability distribution vectors. If `False`, the `tf.argmax` function\n+        will be used to determine each sample's most likely associated label.\n+      axis: (Optional) Defaults to -1. The dimension containing the logits.\n+\n+    Standalone usage:\n+\n+    >>> y_true = tf.constant([[0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0]])\n+    >>> y_pred = tf.constant([[0.2, 0.3, 0.5], [0.1, 0.2, 0.7], [0.5, 0.3, 0.1],\n+    ...                       [0.1, 0.4, 0.5]])\n+    >>> sample_weight = [0.1, 0.2, 0.3, 0.4]\n+    >>> m = tf.keras.metrics.OneHotMeanIoU(num_classes=3)\n+    >>> m.update_state(\n+    ...     y_true=y_true, y_pred=y_pred, sample_weight=sample_weight)\n+    >>> # cm = [[0, 0, 0.2+0.4],\n+    >>> #       [0.3, 0, 0],\n+    >>> #       [0, 0, 0.1]]\n+    >>> # sum_row = [0.3, 0, 0.7], sum_col = [0.6, 0.3, 0.1]\n+    >>> # true_positives = [0, 0, 0.1]\n+    >>> # single_iou = true_positives / (sum_row + sum_col - true_positives))\n+    >>> # mean_iou = (0 + 0 + 0.1 / (0.7 + 0.1 - 0.1)) / 3\n+    >>> m.result().numpy()\n+    0.048\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+      optimizer='sgd',\n+      loss='mse',\n+      metrics=[tf.keras.metrics.OneHotMeanIoU(num_classes=3)])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(\n+        self,\n+        num_classes: int,\n+        name: str = None,\n+        dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n+        ignore_class: Optional[int] = None,\n+        sparse_y_pred: bool = False,\n+        axis: int = -1,\n+    ):\n+        super().__init__(\n+            num_classes=num_classes,\n+            axis=axis,\n+            name=name,\n+            dtype=dtype,\n+            ignore_class=ignore_class,\n+            sparse_y_true=False,\n+            sparse_y_pred=sparse_y_pred,\n+        )\n+\n+    def get_config(self):\n+        return {\n+            \"num_classes\": self.num_classes,\n+            \"name\": self.name,\n+            \"dtype\": self._dtype,\n+            \"ignore_class\": self.ignore_class,\n+            \"sparse_y_pred\": self.sparse_y_pred,\n+            \"axis\": self.axis,\n+        }\n\n@@ -0,0 +1,475 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for Keras metrics.\"\"\"\n+\n+import tensorflow.compat.v2 as tf\n+\n+from keras import metrics\n+from keras.testing_infra import test_combinations\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class IoUTest(tf.test.TestCase):\n+    def test_config(self):\n+        obj = metrics.IoU(\n+            num_classes=2, target_class_ids=[1, 0], name=\"iou_class_1_0\"\n+        )\n+        self.assertEqual(obj.name, \"iou_class_1_0\")\n+        self.assertEqual(obj.num_classes, 2)\n+        self.assertEqual(obj.target_class_ids, [1, 0])\n+\n+        obj2 = metrics.IoU.from_config(obj.get_config())\n+        self.assertEqual(obj2.name, \"iou_class_1_0\")\n+        self.assertEqual(obj2.num_classes, 2)\n+        self.assertEqual(obj2.target_class_ids, [1, 0])\n+\n+    def test_unweighted(self):\n+        y_pred = [0, 1, 0, 1]\n+        y_true = [0, 0, 1, 1]\n+\n+        obj = metrics.IoU(num_classes=2, target_class_ids=[0, 1])\n+        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n+\n+        result = obj(y_true, y_pred)\n+\n+        # cm = [[1, 1],\n+        #       [1, 1]]\n+        # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (1 / (2 + 2 - 1) + 1 / (2 + 2 - 1)) / 2\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+    def test_weighted(self):\n+        y_pred = tf.constant([0, 1, 0, 1], dtype=tf.float32)\n+        y_true = tf.constant([0, 0, 1, 1])\n+        sample_weight = tf.constant([0.2, 0.3, 0.4, 0.1])\n+\n+        obj = metrics.IoU(num_classes=2, target_class_ids=[1, 0])\n+        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n+\n+        result = obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+        # cm = [[0.2, 0.3],\n+        #       [0.4, 0.1]]\n+        # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2,\n+        # 0.1]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (\n+            0.1 / (0.4 + 0.5 - 0.1) + 0.2 / (0.6 + 0.5 - 0.2)\n+        ) / 2\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+    def test_multi_dim_input(self):\n+        y_pred = tf.constant([[0, 1], [0, 1]], dtype=tf.float32)\n+        y_true = tf.constant([[0, 0], [1, 1]])\n+        sample_weight = tf.constant([[0.2, 0.3], [0.4, 0.1]])\n+\n+        obj = metrics.IoU(num_classes=2, target_class_ids=[0, 1])\n+        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n+\n+        result = obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+        # cm = [[0.2, 0.3],\n+        #       [0.4, 0.1]]\n+        # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2,\n+        # 0.1]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (\n+            0.2 / (0.6 + 0.5 - 0.2) + 0.1 / (0.4 + 0.5 - 0.1)\n+        ) / 2\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+    def test_zero_valid_entries(self):\n+        obj = metrics.IoU(num_classes=2, target_class_ids=[0, 1])\n+        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n+        self.assertAllClose(self.evaluate(obj.result()), 0, atol=1e-3)\n+\n+    def test_zero_and_non_zero_entries(self):\n+        y_pred = tf.constant([1], dtype=tf.float32)\n+        y_true = tf.constant([1])\n+\n+        obj = metrics.IoU(num_classes=2, target_class_ids=[0, 1])\n+        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n+        result = obj(y_true, y_pred)\n+\n+        # cm = [[0, 0],\n+        #       [0, 1]]\n+        # sum_row = [0, 1], sum_col = [0, 1], true_positives = [0, 1]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (1 / (1 + 1 - 1)) / 1\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class BinaryIoUTest(tf.test.TestCase):\n+    def test_config(self):\n+        obj = metrics.BinaryIoU(\n+            target_class_ids=[1, 0], threshold=0.1, name=\"iou_class_1_0\"\n+        )\n+        self.assertEqual(obj.name, \"iou_class_1_0\")\n+        self.assertAlmostEqual(obj.threshold, 0.1)\n+        self.assertEqual(obj.target_class_ids, [1, 0])\n+\n+        obj2 = metrics.BinaryIoU.from_config(obj.get_config())\n+        self.assertEqual(obj.name, \"iou_class_1_0\")\n+        self.assertAlmostEqual(obj2.threshold, 0.1)\n+        self.assertEqual(obj.target_class_ids, [1, 0])\n+\n+    def test_different_thresholds_weighted(self):\n+        y_true = [0, 1, 0, 1]\n+        y_pred = [0.1, 0.2, 0.4, 0.7]\n+\n+        sample_weight = tf.constant([0.2, 0.3, 0.4, 0.1])\n+        # with threshold = 0.3, y_pred will be converted to [0, 0, 1, 1]\n+        # cm = [[0.2, 0.4],\n+        #       [0.3, 0.1]]\n+        # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2,\n+        # 0.1]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (\n+            0.2 / (0.6 + 0.5 - 0.2) + 0.1 / (0.4 + 0.5 - 0.1)\n+        ) / 2\n+        obj = metrics.BinaryIoU(target_class_ids=[0, 1], threshold=0.3)\n+        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n+        result = obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+        sample_weight = tf.constant([0.1, 0.2, 0.4, 0.3])\n+        # with threshold = 0.5, y_pred will be converted to [0, 0, 0, 1]\n+        # cm = [[0.1+0.4, 0],\n+        #       [0.2, 0.3]]\n+        # sum_row = [0.5, 0.5], sum_col = [0.7, 0.3], true_positives = [0.5,\n+        # 0.3]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (\n+            0.5 / (0.5 + 0.7 - 0.5) + 0.3 / (0.5 + 0.3 - 0.3)\n+        ) / 2\n+        obj = metrics.BinaryIoU(target_class_ids=[0, 1], threshold=0.5)\n+        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n+        result = obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+    def test_different_thresholds_unweighted(self):\n+        y_true = [0, 1, 0, 1]\n+        y_pred = [0.1, 0.2, 0.4, 0.7]\n+\n+        # with threshold = 0.3, y_pred will be converted to [0, 0, 1, 1]\n+        # cm = [[1, 1],\n+        #       [1, 1]]\n+        # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (1 / (2 + 2 - 1) + 1 / (2 + 2 - 1)) / 2\n+        obj = metrics.BinaryIoU(target_class_ids=[0, 1], threshold=0.3)\n+        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n+        result = obj(y_true, y_pred)\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+        # with threshold = 0.5, y_pred will be converted to [0, 0, 0, 1]\n+        # cm = [[2, 0],\n+        #       [1, 1]]\n+        # sum_row = [2, 2], sum_col = [3, 1], true_positives = [2, 1]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (2 / (2 + 3 - 2) + 1 / (2 + 1 - 1)) / 2\n+        obj = metrics.BinaryIoU(target_class_ids=[0, 1], threshold=0.5)\n+        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n+        result = obj(y_true, y_pred)\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+    def test_multi_dim_input(self):\n+        y_true = tf.constant([[0, 1], [0, 1]], dtype=tf.float32)\n+        y_pred = tf.constant([[0.1, 0.7], [0.9, 0.3]])\n+        threshold = 0.4  # y_pred will become [[0, 1], [1, 0]]\n+        sample_weight = tf.constant([[0.2, 0.3], [0.4, 0.1]])\n+        # cm = [[0.2, 0.4],\n+        #       [0.1, 0.3]]\n+        # sum_row = [0.6, 0.4], sum_col = [0.3, 0.7], true_positives = [0.2,\n+        # 0.3]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (\n+            0.2 / (0.6 + 0.3 - 0.2) + 0.3 / (0.4 + 0.7 - 0.3)\n+        ) / 2\n+        obj = metrics.BinaryIoU(target_class_ids=[0, 1], threshold=threshold)\n+        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n+        result = obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+    def test_zero_valid_entries(self):\n+        obj = metrics.BinaryIoU(target_class_ids=[0, 1])\n+        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n+        self.assertAllClose(self.evaluate(obj.result()), 0, atol=1e-3)\n+\n+    def test_zero_and_non_zero_entries(self):\n+        y_pred = tf.constant([0.6], dtype=tf.float32)\n+        threshold = 0.5\n+        y_true = tf.constant([1])\n+\n+        obj = metrics.BinaryIoU(target_class_ids=[0, 1], threshold=threshold)\n+        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n+        result = obj(y_true, y_pred)\n+\n+        # cm = [[0, 0],\n+        #       [0, 1]]\n+        # sum_row = [0, 1], sum_col = [0, 1], true_positives = [0, 1]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = 1 / (1 + 1 - 1)\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class MeanIoUTest(tf.test.TestCase):\n+    def test_config(self):\n+        m_obj = metrics.MeanIoU(num_classes=2, name=\"mean_iou\")\n+        self.assertEqual(m_obj.name, \"mean_iou\")\n+        self.assertEqual(m_obj.num_classes, 2)\n+\n+        m_obj2 = metrics.MeanIoU.from_config(m_obj.get_config())\n+        self.assertEqual(m_obj2.name, \"mean_iou\")\n+        self.assertEqual(m_obj2.num_classes, 2)\n+\n+    def test_unweighted(self):\n+        y_pred = [0, 1, 0, 1]\n+        y_true = [0, 0, 1, 1]\n+\n+        m_obj = metrics.MeanIoU(num_classes=2)\n+        self.evaluate(tf.compat.v1.variables_initializer(m_obj.variables))\n+\n+        result = m_obj(y_true, y_pred)\n+\n+        # cm = [[1, 1],\n+        #       [1, 1]]\n+        # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (1 / (2 + 2 - 1) + 1 / (2 + 2 - 1)) / 2\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+    def test_unweighted_ignore_class_255(self):\n+        y_pred = [0, 1, 1, 1]\n+        y_true = [0, 1, 2, 255]\n+\n+        m_obj = metrics.MeanIoU(num_classes=3, ignore_class=255)\n+        self.evaluate(tf.compat.v1.variables_initializer(m_obj.variables))\n+\n+        result = m_obj(y_true, y_pred)\n+\n+        # cm = [[1, 0, 0],\n+        #       [0, 1, 0],\n+        #       [0, 1, 0]]\n+        # sum_row = [1, 1, 1], sum_col = [1, 2, 0], true_positives = [1, 1, 0]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (\n+            1 / (1 + 1 - 1) + 1 / (2 + 1 - 1) + 0 / (0 + 1 - 0)\n+        ) / 3\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+    def test_unweighted_ignore_class_1(self):\n+        y_pred = [0, 1, 1, 1]\n+        y_true = [0, 1, 2, -1]\n+\n+        m_obj = metrics.MeanIoU(num_classes=3, ignore_class=-1)\n+        self.evaluate(tf.compat.v1.variables_initializer(m_obj.variables))\n+\n+        result = m_obj(y_true, y_pred)\n+\n+        # cm = [[1, 0, 0],\n+        #       [0, 1, 0],\n+        #       [0, 1, 0]]\n+        # sum_row = [1, 1, 1], sum_col = [1, 2, 0], true_positives = [1, 1, 0]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (\n+            1 / (1 + 1 - 1) + 1 / (2 + 1 - 1) + 0 / (0 + 1 - 0)\n+        ) / 3\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+    def test_weighted(self):\n+        y_pred = tf.constant([0, 1, 0, 1], dtype=tf.float32)\n+        y_true = tf.constant([0, 0, 1, 1])\n+        sample_weight = tf.constant([0.2, 0.3, 0.4, 0.1])\n+\n+        m_obj = metrics.MeanIoU(num_classes=2)\n+        self.evaluate(tf.compat.v1.variables_initializer(m_obj.variables))\n+\n+        result = m_obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+        # cm = [[0.2, 0.3],\n+        #       [0.4, 0.1]]\n+        # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2,\n+        # 0.1]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (\n+            0.2 / (0.6 + 0.5 - 0.2) + 0.1 / (0.4 + 0.5 - 0.1)\n+        ) / 2\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+    def test_weighted_ignore_class_1(self):\n+        y_pred = tf.constant([0, 1, 0, 1], dtype=tf.float32)\n+        y_true = tf.constant([0, 0, 1, -1])\n+        sample_weight = tf.constant([0.2, 0.3, 0.4, 0.1])\n+\n+        m_obj = metrics.MeanIoU(num_classes=2, ignore_class=-1)\n+        self.evaluate(tf.compat.v1.variables_initializer(m_obj.variables))\n+\n+        result = m_obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+        # cm = [[0.2, 0.3],\n+        #       [0.4, 0.0]]\n+        # sum_row = [0.6, 0.3], sum_col = [0.5, 0.4], true_positives = [0.2,\n+        # 0.0]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (\n+            0.2 / (0.6 + 0.5 - 0.2) + 0.0 / (0.3 + 0.4 - 0.0)\n+        ) / 2\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+    def test_multi_dim_input(self):\n+        y_pred = tf.constant([[0, 1], [0, 1]], dtype=tf.float32)\n+        y_true = tf.constant([[0, 0], [1, 1]])\n+        sample_weight = tf.constant([[0.2, 0.3], [0.4, 0.1]])\n+\n+        m_obj = metrics.MeanIoU(num_classes=2)\n+        self.evaluate(tf.compat.v1.variables_initializer(m_obj.variables))\n+\n+        result = m_obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+        # cm = [[0.2, 0.3],\n+        #       [0.4, 0.1]]\n+        # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2,\n+        # 0.1]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (\n+            0.2 / (0.6 + 0.5 - 0.2) + 0.1 / (0.4 + 0.5 - 0.1)\n+        ) / 2\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+    def test_zero_valid_entries(self):\n+        m_obj = metrics.MeanIoU(num_classes=2)\n+        self.evaluate(tf.compat.v1.variables_initializer(m_obj.variables))\n+        self.assertAllClose(self.evaluate(m_obj.result()), 0, atol=1e-3)\n+\n+    def test_zero_and_non_zero_entries(self):\n+        y_pred = tf.constant([1], dtype=tf.float32)\n+        y_true = tf.constant([1])\n+\n+        m_obj = metrics.MeanIoU(num_classes=2)\n+        self.evaluate(tf.compat.v1.variables_initializer(m_obj.variables))\n+        result = m_obj(y_true, y_pred)\n+\n+        # cm = [[0, 0],\n+        #       [0, 1]]\n+        # sum_row = [0, 1], sum_col = [0, 1], true_positives = [0, 1]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (0 + 1 / (1 + 1 - 1)) / 1\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class OneHotIoUTest(tf.test.TestCase):\n+    def test_unweighted(self):\n+        y_true = tf.constant([[0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0]])\n+        # y_true will be converted to [2, 0, 1, 0]\n+        y_pred = tf.constant(\n+            [[0.2, 0.3, 0.5], [0.1, 0.2, 0.7], [0.5, 0.3, 0.1], [0.1, 0.4, 0.5]]\n+        )\n+        # y_pred will be converted to [2, 2, 0, 2]\n+        # cm = [[0, 0, 2],\n+        #       [1, 0, 0],\n+        #       [0, 0, 1]\n+        # sum_row = [1, 0, 3], sum_col = [2, 1, 1], true_positives = [0, 0, 1]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (0 / (1 + 2 - 0) + 1 / (3 + 1 - 1)) / 2\n+        obj = metrics.OneHotIoU(num_classes=3, target_class_ids=[0, 2])\n+        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n+        result = obj(y_true, y_pred)\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+    def test_weighted(self):\n+        y_true = tf.constant([[0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0]])\n+        # y_true will be converted to [2, 0, 1, 0]\n+        y_pred = tf.constant(\n+            [[0.2, 0.3, 0.5], [0.1, 0.2, 0.7], [0.5, 0.3, 0.1], [0.1, 0.4, 0.5]]\n+        )\n+        # y_pred will be converted to [2, 2, 0, 2]\n+        sample_weight = [0.1, 0.2, 0.3, 0.4]\n+        # cm = [[0, 0, 0.2+0.4],\n+        #       [0.3, 0, 0],\n+        #       [0, 0, 0.1]]\n+        # sum_row = [0.3, 0, 0.7], sum_col = [0.6, 0.3, 0.1]\n+        # true_positives = [0, 0, 0.1]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (0 / (0.3 + 0.6 - 0) + 0.1 / (0.7 + 0.1 - 0.1)) / 2\n+        obj = metrics.OneHotIoU(num_classes=3, target_class_ids=[0, 2])\n+        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n+        result = obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class OneHotMeanIoUTest(tf.test.TestCase):\n+    def test_unweighted(self):\n+        y_true = tf.constant([[0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0]])\n+        # y_true will be converted to [2, 0, 1, 0]\n+        y_pred = tf.constant(\n+            [[0.2, 0.3, 0.5], [0.1, 0.2, 0.7], [0.5, 0.3, 0.1], [0.1, 0.4, 0.5]]\n+        )\n+        # y_pred will be converted to [2, 2, 0, 2]\n+        # cm = [[0, 0, 2],\n+        #       [1, 0, 0],\n+        #       [0, 0, 1]\n+        # sum_row = [1, 0, 3], sum_col = [2, 1, 1], true_positives = [0, 0, 1]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (0 + 0 + 1 / (3 + 1 - 1)) / 3\n+        obj = metrics.OneHotMeanIoU(num_classes=3)\n+        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n+        result = obj(y_true, y_pred)\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+    def test_weighted(self):\n+        y_true = tf.constant(\n+            [\n+                [0, 0, 1],\n+                [1, 0, 0],\n+                [0, 1, 0],\n+                [1, 0, 0],\n+                [1, 0, 0],\n+            ]\n+        )\n+        # y_true will be converted to [2, 0, 1, 0, 0]\n+        y_pred = tf.constant(\n+            [\n+                [0.2, 0.3, 0.5],\n+                [0.1, 0.2, 0.7],\n+                [0.5, 0.3, 0.1],\n+                [0.1, 0.4, 0.5],\n+                [0.6, 0.2, 0.2],\n+            ]\n+        )\n+        # y_pred will be converted to [2, 2, 0, 2, 0]\n+        sample_weight = [0.1, 0.2, 0.3, 0.3, 0.1]\n+        # cm = [[0.1, 0, 0.2+0.3],\n+        #       [0.3, 0, 0],\n+        #       [0, 0, 0.1]]\n+        # sum_row = [0.4, 0, 0.6], sum_col = [0.6, 0.3, 0.1]\n+        # true_positives = [0.1, 0, 0.1]\n+        # iou = true_positives / (sum_row + sum_col - true_positives))\n+        expected_result = (\n+            0.1 / (0.4 + 0.6 - 0.1) + 0 + 0.1 / (0.6 + 0.1 - 0.1)\n+        ) / 3\n+        obj = metrics.OneHotMeanIoU(num_classes=3)\n+        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n+        result = obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+\n+if __name__ == \"__main__\":\n+    tf.test.main()\n\n@@ -1,3754 +0,0 @@\n-# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\n-\n-\"\"\"Built-in metrics.\"\"\"\n-\n-import abc\n-from typing import List\n-from typing import Optional\n-from typing import Tuple\n-from typing import Union\n-\n-import numpy as np\n-import tensorflow.compat.v2 as tf\n-\n-from keras import activations\n-from keras import backend\n-from keras.dtensor import utils as dtensor_utils\n-from keras.losses import binary_crossentropy\n-from keras.losses import categorical_crossentropy\n-from keras.losses import categorical_hinge\n-from keras.losses import hinge\n-from keras.losses import kullback_leibler_divergence\n-from keras.losses import logcosh\n-from keras.losses import mean_absolute_error\n-from keras.losses import mean_absolute_percentage_error\n-from keras.losses import mean_squared_error\n-from keras.losses import mean_squared_logarithmic_error\n-from keras.losses import poisson\n-from keras.losses import sparse_categorical_crossentropy\n-from keras.losses import squared_hinge\n-from keras.metrics import base_metric\n-from keras.utils import losses_utils\n-from keras.utils import metrics_utils\n-from keras.utils.generic_utils import to_list\n-from keras.utils.tf_utils import is_tensor_or_variable\n-\n-# isort: off\n-from tensorflow.python.util.tf_export import keras_export\n-\n-\n-@keras_export(\"keras.metrics.MeanRelativeError\")\n-class MeanRelativeError(base_metric.Mean):\n-    \"\"\"Computes the mean relative error by normalizing with the given values.\n-\n-    This metric creates two local variables, `total` and `count` that are used\n-    to compute the mean relative error. This is weighted by `sample_weight`, and\n-    it is ultimately returned as `mean_relative_error`: an idempotent operation\n-    that simply divides `total` by `count`.\n-\n-    If `sample_weight` is `None`, weights default to 1.\n-    Use `sample_weight` of 0 to mask values.\n-\n-    Args:\n-      normalizer: The normalizer values with same shape as predictions.\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.MeanRelativeError(normalizer=[1, 3, 2, 3])\n-    >>> m.update_state([1, 3, 2, 3], [2, 4, 6, 8])\n-\n-    >>> # metric = mean(|y_pred - y_true| / normalizer)\n-    >>> #        = mean([1, 1, 4, 5] / [1, 3, 2, 3]) = mean([1, 1/3, 2, 5/3])\n-    >>> #        = 5/4 = 1.25\n-    >>> m.result().numpy()\n-    1.25\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(\n-      optimizer='sgd',\n-      loss='mse',\n-      metrics=[tf.keras.metrics.MeanRelativeError(normalizer=[1, 3])])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(self, normalizer, name=None, dtype=None):\n-        super().__init__(name=name, dtype=dtype)\n-        normalizer = tf.cast(normalizer, self._dtype)\n-        self.normalizer = normalizer\n-\n-    def update_state(self, y_true, y_pred, sample_weight=None):\n-        \"\"\"Accumulates metric statistics.\n-\n-        Args:\n-          y_true: The ground truth values.\n-          y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can\n-            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n-            and must be broadcastable to `y_true`.\n-\n-        Returns:\n-          Update op.\n-        \"\"\"\n-        y_true = tf.cast(y_true, self._dtype)\n-        y_pred = tf.cast(y_pred, self._dtype)\n-        [\n-            y_pred,\n-            y_true,\n-        ], sample_weight = metrics_utils.ragged_assert_compatible_and_get_flat_values(  # noqa: E501\n-            [y_pred, y_true], sample_weight\n-        )\n-        y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(\n-            y_pred, y_true\n-        )\n-\n-        y_pred, self.normalizer = losses_utils.remove_squeezable_dimensions(\n-            y_pred, self.normalizer\n-        )\n-        y_pred.shape.assert_is_compatible_with(y_true.shape)\n-        relative_errors = tf.math.divide_no_nan(\n-            tf.abs(y_true - y_pred), self.normalizer\n-        )\n-\n-        return super().update_state(\n-            relative_errors, sample_weight=sample_weight\n-        )\n-\n-    def get_config(self):\n-        n = self.normalizer\n-        config = {\n-            \"normalizer\": backend.eval(n) if is_tensor_or_variable(n) else n\n-        }\n-        base_config = super().get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export(\"keras.metrics.Accuracy\")\n-class Accuracy(base_metric.MeanMetricWrapper):\n-    \"\"\"Calculates how often predictions equal labels.\n-\n-    This metric creates two local variables, `total` and `count` that are used\n-    to compute the frequency with which `y_pred` matches `y_true`. This\n-    frequency is ultimately returned as `binary accuracy`: an idempotent\n-    operation that simply divides `total` by `count`.\n-\n-    If `sample_weight` is `None`, weights default to 1.\n-    Use `sample_weight` of 0 to mask values.\n-\n-    Args:\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.Accuracy()\n-    >>> m.update_state([[1], [2], [3], [4]], [[0], [2], [3], [4]])\n-    >>> m.result().numpy()\n-    0.75\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([[1], [2], [3], [4]], [[0], [2], [3], [4]],\n-    ...                sample_weight=[1, 1, 0, 0])\n-    >>> m.result().numpy()\n-    0.5\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(optimizer='sgd',\n-                  loss='mse',\n-                  metrics=[tf.keras.metrics.Accuracy()])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(self, name=\"accuracy\", dtype=None):\n-        super().__init__(accuracy, name, dtype=dtype)\n-\n-\n-@keras_export(\"keras.metrics.BinaryAccuracy\")\n-class BinaryAccuracy(base_metric.MeanMetricWrapper):\n-    \"\"\"Calculates how often predictions match binary labels.\n-\n-    This metric creates two local variables, `total` and `count` that are used\n-    to compute the frequency with which `y_pred` matches `y_true`. This\n-    frequency is ultimately returned as `binary accuracy`: an idempotent\n-    operation that simply divides `total` by `count`.\n-\n-    If `sample_weight` is `None`, weights default to 1.\n-    Use `sample_weight` of 0 to mask values.\n-\n-    Args:\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-      threshold: (Optional) Float representing the threshold for deciding\n-      whether prediction values are 1 or 0.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.BinaryAccuracy()\n-    >>> m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]])\n-    >>> m.result().numpy()\n-    0.75\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]],\n-    ...                sample_weight=[1, 0, 0, 1])\n-    >>> m.result().numpy()\n-    0.5\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(optimizer='sgd',\n-                  loss='mse',\n-                  metrics=[tf.keras.metrics.BinaryAccuracy()])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(self, name=\"binary_accuracy\", dtype=None, threshold=0.5):\n-        super().__init__(\n-            metrics_utils.binary_matches, name, dtype=dtype, threshold=threshold\n-        )\n-\n-\n-@keras_export(\"keras.metrics.CategoricalAccuracy\")\n-class CategoricalAccuracy(base_metric.MeanMetricWrapper):\n-    \"\"\"Calculates how often predictions match one-hot labels.\n-\n-    You can provide logits of classes as `y_pred`, since argmax of\n-    logits and probabilities are same.\n-\n-    This metric creates two local variables, `total` and `count` that are used\n-    to compute the frequency with which `y_pred` matches `y_true`. This\n-    frequency is ultimately returned as `categorical accuracy`: an idempotent\n-    operation that simply divides `total` by `count`.\n-\n-    `y_pred` and `y_true` should be passed in as vectors of probabilities,\n-    rather than as labels. If necessary, use `tf.one_hot` to expand `y_true` as\n-    a vector.\n-\n-    If `sample_weight` is `None`, weights default to 1.\n-    Use `sample_weight` of 0 to mask values.\n-\n-    Args:\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.CategoricalAccuracy()\n-    >>> m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8],\n-    ...                 [0.05, 0.95, 0]])\n-    >>> m.result().numpy()\n-    0.5\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8],\n-    ...                 [0.05, 0.95, 0]],\n-    ...                sample_weight=[0.7, 0.3])\n-    >>> m.result().numpy()\n-    0.3\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(\n-      optimizer='sgd',\n-      loss='mse',\n-      metrics=[tf.keras.metrics.CategoricalAccuracy()])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(self, name=\"categorical_accuracy\", dtype=None):\n-        super().__init__(\n-            lambda y_true, y_pred: metrics_utils.sparse_categorical_matches(\n-                tf.math.argmax(y_true, axis=-1), y_pred\n-            ),\n-            name,\n-            dtype=dtype,\n-        )\n-\n-\n-@keras_export(\"keras.metrics.SparseCategoricalAccuracy\")\n-class SparseCategoricalAccuracy(base_metric.MeanMetricWrapper):\n-    \"\"\"Calculates how often predictions match integer labels.\n-\n-    ```python\n-    acc = np.dot(sample_weight, np.equal(y_true, np.argmax(y_pred, axis=1))\n-    ```\n-\n-    You can provide logits of classes as `y_pred`, since argmax of\n-    logits and probabilities are same.\n-\n-    This metric creates two local variables, `total` and `count` that are used\n-    to compute the frequency with which `y_pred` matches `y_true`. This\n-    frequency is ultimately returned as `sparse categorical accuracy`: an\n-    idempotent operation that simply divides `total` by `count`.\n-\n-    If `sample_weight` is `None`, weights default to 1.\n-    Use `sample_weight` of 0 to mask values.\n-\n-    Args:\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.SparseCategoricalAccuracy()\n-    >>> m.update_state([[2], [1]], [[0.1, 0.6, 0.3], [0.05, 0.95, 0]])\n-    >>> m.result().numpy()\n-    0.5\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([[2], [1]], [[0.1, 0.6, 0.3], [0.05, 0.95, 0]],\n-    ...                sample_weight=[0.7, 0.3])\n-    >>> m.result().numpy()\n-    0.3\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(\n-        optimizer='sgd',\n-        loss='mse',\n-        metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(self, name=\"sparse_categorical_accuracy\", dtype=None):\n-        super().__init__(\n-            metrics_utils.sparse_categorical_matches, name, dtype=dtype\n-        )\n-\n-\n-_SPARSE_CATEGORICAL_UPDATE_STATE_DOCSTRING = \"\"\"Accumulates metric statistics.\n-\n-For sparse categorical metrics, the shapes of `y_true` and `y_pred` are\n-different.\n-\n-Args:\n-  y_true: Ground truth label values. shape = `[batch_size, d0, .. dN-1]` or\n-    shape = `[batch_size, d0, .. dN-1, 1]`.\n-  y_pred: The predicted probability values. shape = `[batch_size, d0, .. dN]`.\n-  sample_weight: Optional `sample_weight` acts as a\n-    coefficient for the metric. If a scalar is provided, then the metric is\n-    simply scaled by the given value. If `sample_weight` is a tensor of size\n-    `[batch_size]`, then the metric for each sample of the batch is rescaled\n-    by the corresponding element in the `sample_weight` vector. If the shape\n-    of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted\n-    to this shape), then each metric element of `y_pred` is scaled by the\n-    corresponding value of `sample_weight`. (Note on `dN-1`: all metric\n-    functions reduce by 1 dimension, usually the last axis (-1)).\n-\n-Returns:\n-  Update op.\n-\"\"\"\n-\n-SparseCategoricalAccuracy.update_state.__doc__ = (\n-    _SPARSE_CATEGORICAL_UPDATE_STATE_DOCSTRING\n-)\n-\n-\n-@keras_export(\"keras.metrics.TopKCategoricalAccuracy\")\n-class TopKCategoricalAccuracy(base_metric.MeanMetricWrapper):\n-    \"\"\"Computes how often targets are in the top `K` predictions.\n-\n-    Args:\n-      k: (Optional) Number of top elements to look at for computing accuracy.\n-        Defaults to 5.\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.TopKCategoricalAccuracy(k=1)\n-    >>> m.update_state([[0, 0, 1], [0, 1, 0]],\n-    ...                [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])\n-    >>> m.result().numpy()\n-    0.5\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([[0, 0, 1], [0, 1, 0]],\n-    ...                [[0.1, 0.9, 0.8], [0.05, 0.95, 0]],\n-    ...                sample_weight=[0.7, 0.3])\n-    >>> m.result().numpy()\n-    0.3\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(optimizer='sgd',\n-                  loss='mse',\n-                  metrics=[tf.keras.metrics.TopKCategoricalAccuracy()])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(self, k=5, name=\"top_k_categorical_accuracy\", dtype=None):\n-        super().__init__(\n-            lambda yt, yp, k: metrics_utils.sparse_top_k_categorical_matches(\n-                tf.math.argmax(yt, axis=-1), yp, k\n-            ),\n-            name,\n-            dtype=dtype,\n-            k=k,\n-        )\n-\n-\n-@keras_export(\"keras.metrics.SparseTopKCategoricalAccuracy\")\n-class SparseTopKCategoricalAccuracy(base_metric.MeanMetricWrapper):\n-    \"\"\"Computes how often integer targets are in the top `K` predictions.\n-\n-    Args:\n-      k: (Optional) Number of top elements to look at for computing accuracy.\n-        Defaults to 5.\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)\n-    >>> m.update_state([2, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])\n-    >>> m.result().numpy()\n-    0.5\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([2, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]],\n-    ...                sample_weight=[0.7, 0.3])\n-    >>> m.result().numpy()\n-    0.3\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(\n-      optimizer='sgd',\n-      loss='mse',\n-      metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy()])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(\n-        self, k=5, name=\"sparse_top_k_categorical_accuracy\", dtype=None\n-    ):\n-        super().__init__(\n-            metrics_utils.sparse_top_k_categorical_matches,\n-            name,\n-            dtype=dtype,\n-            k=k,\n-        )\n-\n-\n-SparseTopKCategoricalAccuracy.update_state.__doc__ = (\n-    _SPARSE_CATEGORICAL_UPDATE_STATE_DOCSTRING\n-)\n-\n-\n-class _ConfusionMatrixConditionCount(base_metric.Metric):\n-    \"\"\"Calculates the number of the given confusion matrix condition.\n-\n-    Args:\n-      confusion_matrix_cond: One of `metrics_utils.ConfusionMatrix` conditions.\n-      thresholds: (Optional) Defaults to 0.5. A float value or a python\n-        list/tuple of float threshold values in [0, 1]. A threshold is compared\n-        with prediction values to determine the truth value of predictions\n-        (i.e., above the threshold is `true`, below is `false`). One metric\n-        value is generated for each threshold value.\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-    \"\"\"\n-\n-    def __init__(\n-        self, confusion_matrix_cond, thresholds=None, name=None, dtype=None\n-    ):\n-        super().__init__(name=name, dtype=dtype)\n-        self._confusion_matrix_cond = confusion_matrix_cond\n-        self.init_thresholds = thresholds\n-        self.thresholds = metrics_utils.parse_init_thresholds(\n-            thresholds, default_threshold=0.5\n-        )\n-        self._thresholds_distributed_evenly = (\n-            metrics_utils.is_evenly_distributed_thresholds(self.thresholds)\n-        )\n-        self.accumulator = self.add_weight(\n-            \"accumulator\", shape=(len(self.thresholds),), initializer=\"zeros\"\n-        )\n-\n-    def update_state(self, y_true, y_pred, sample_weight=None):\n-        \"\"\"Accumulates the metric statistics.\n-\n-        Args:\n-          y_true: The ground truth values.\n-          y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can\n-            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n-            and must be broadcastable to `y_true`.\n-\n-        Returns:\n-          Update op.\n-        \"\"\"\n-        return metrics_utils.update_confusion_matrix_variables(\n-            {self._confusion_matrix_cond: self.accumulator},\n-            y_true,\n-            y_pred,\n-            thresholds=self.thresholds,\n-            thresholds_distributed_evenly=self._thresholds_distributed_evenly,\n-            sample_weight=sample_weight,\n-        )\n-\n-    def result(self):\n-        if len(self.thresholds) == 1:\n-            result = self.accumulator[0]\n-        else:\n-            result = self.accumulator\n-        return tf.convert_to_tensor(result)\n-\n-    def reset_state(self):\n-        backend.batch_set_value(\n-            [(v, np.zeros(v.shape.as_list())) for v in self.variables]\n-        )\n-\n-    def get_config(self):\n-        config = {\"thresholds\": self.init_thresholds}\n-        base_config = super().get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export(\"keras.metrics.FalsePositives\")\n-class FalsePositives(_ConfusionMatrixConditionCount):\n-    \"\"\"Calculates the number of false positives.\n-\n-    If `sample_weight` is given, calculates the sum of the weights of\n-    false positives. This metric creates one local variable, `accumulator`\n-    that is used to keep track of the number of false positives.\n-\n-    If `sample_weight` is `None`, weights default to 1.\n-    Use `sample_weight` of 0 to mask values.\n-\n-    Args:\n-      thresholds: (Optional) Defaults to 0.5. A float value, or a Python\n-        list/tuple of float threshold values in [0, 1]. A threshold is compared\n-        with prediction values to determine the truth value of predictions\n-        (i.e., above the threshold is `true`, below is `false`). If used with a\n-        loss function that sets `from_logits=True` (i.e. no sigmoid applied to\n-        predictions), `thresholds` should be set to 0. One metric value is\n-        generated for each threshold value.\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.FalsePositives()\n-    >>> m.update_state([0, 1, 0, 0], [0, 0, 1, 1])\n-    >>> m.result().numpy()\n-    2.0\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([0, 1, 0, 0], [0, 0, 1, 1], sample_weight=[0, 0, 1, 0])\n-    >>> m.result().numpy()\n-    1.0\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(optimizer='sgd',\n-                  loss='mse',\n-                  metrics=[tf.keras.metrics.FalsePositives()])\n-    ```\n-\n-    Usage with a loss with `from_logits=True`:\n-\n-    ```python\n-    model.compile(optimizer='adam',\n-                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n-                  metrics=[tf.keras.metrics.FalsePositives(thresholds=0)])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(self, thresholds=None, name=None, dtype=None):\n-        super().__init__(\n-            confusion_matrix_cond=metrics_utils.ConfusionMatrix.FALSE_POSITIVES,\n-            thresholds=thresholds,\n-            name=name,\n-            dtype=dtype,\n-        )\n-\n-\n-@keras_export(\"keras.metrics.FalseNegatives\")\n-class FalseNegatives(_ConfusionMatrixConditionCount):\n-    \"\"\"Calculates the number of false negatives.\n-\n-    If `sample_weight` is given, calculates the sum of the weights of\n-    false negatives. This metric creates one local variable, `accumulator`\n-    that is used to keep track of the number of false negatives.\n-\n-    If `sample_weight` is `None`, weights default to 1.\n-    Use `sample_weight` of 0 to mask values.\n-\n-    Args:\n-      thresholds: (Optional) Defaults to 0.5. A float value, or a Python\n-        list/tuple of float threshold values in [0, 1]. A threshold is compared\n-        with prediction values to determine the truth value of predictions\n-        (i.e., above the threshold is `true`, below is `false`). If used with a\n-        loss function that sets `from_logits=True` (i.e. no sigmoid applied to\n-        predictions), `thresholds` should be set to 0. One metric value is\n-        generated for each threshold value.\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.FalseNegatives()\n-    >>> m.update_state([0, 1, 1, 1], [0, 1, 0, 0])\n-    >>> m.result().numpy()\n-    2.0\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([0, 1, 1, 1], [0, 1, 0, 0], sample_weight=[0, 0, 1, 0])\n-    >>> m.result().numpy()\n-    1.0\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(optimizer='sgd',\n-                  loss='mse',\n-                  metrics=[tf.keras.metrics.FalseNegatives()])\n-    ```\n-\n-    Usage with a loss with `from_logits=True`:\n-\n-    ```python\n-    model.compile(optimizer='adam',\n-                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n-                  metrics=[tf.keras.metrics.FalseNegatives(thresholds=0)])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(self, thresholds=None, name=None, dtype=None):\n-        super().__init__(\n-            confusion_matrix_cond=metrics_utils.ConfusionMatrix.FALSE_NEGATIVES,\n-            thresholds=thresholds,\n-            name=name,\n-            dtype=dtype,\n-        )\n-\n-\n-@keras_export(\"keras.metrics.TrueNegatives\")\n-class TrueNegatives(_ConfusionMatrixConditionCount):\n-    \"\"\"Calculates the number of true negatives.\n-\n-    If `sample_weight` is given, calculates the sum of the weights of\n-    true negatives. This metric creates one local variable, `accumulator`\n-    that is used to keep track of the number of true negatives.\n-\n-    If `sample_weight` is `None`, weights default to 1.\n-    Use `sample_weight` of 0 to mask values.\n-\n-    Args:\n-      thresholds: (Optional) Defaults to 0.5. A float value, or a Python\n-        list/tuple of float threshold values in [0, 1]. A threshold is compared\n-        with prediction values to determine the truth value of predictions\n-        (i.e., above the threshold is `true`, below is `false`). If used with a\n-        loss function that sets `from_logits=True` (i.e. no sigmoid applied to\n-        predictions), `thresholds` should be set to 0. One metric value is\n-        generated for each threshold value.\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.TrueNegatives()\n-    >>> m.update_state([0, 1, 0, 0], [1, 1, 0, 0])\n-    >>> m.result().numpy()\n-    2.0\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([0, 1, 0, 0], [1, 1, 0, 0], sample_weight=[0, 0, 1, 0])\n-    >>> m.result().numpy()\n-    1.0\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(optimizer='sgd',\n-                  loss='mse',\n-                  metrics=[tf.keras.metrics.TrueNegatives()])\n-    ```\n-\n-    Usage with a loss with `from_logits=True`:\n-\n-    ```python\n-    model.compile(optimizer='adam',\n-                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n-                  metrics=[tf.keras.metrics.TrueNegatives(thresholds=0)])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(self, thresholds=None, name=None, dtype=None):\n-        super().__init__(\n-            confusion_matrix_cond=metrics_utils.ConfusionMatrix.TRUE_NEGATIVES,\n-            thresholds=thresholds,\n-            name=name,\n-            dtype=dtype,\n-        )\n-\n-\n-@keras_export(\"keras.metrics.TruePositives\")\n-class TruePositives(_ConfusionMatrixConditionCount):\n-    \"\"\"Calculates the number of true positives.\n-\n-    If `sample_weight` is given, calculates the sum of the weights of\n-    true positives. This metric creates one local variable, `true_positives`\n-    that is used to keep track of the number of true positives.\n-\n-    If `sample_weight` is `None`, weights default to 1.\n-    Use `sample_weight` of 0 to mask values.\n-\n-    Args:\n-      thresholds: (Optional) Defaults to 0.5. A float value, or a Python\n-        list/tuple of float threshold values in [0, 1]. A threshold is compared\n-        with prediction values to determine the truth value of predictions\n-        (i.e., above the threshold is `true`, below is `false`). If used with a\n-        loss function that sets `from_logits=True` (i.e. no sigmoid applied to\n-        predictions), `thresholds` should be set to 0. One metric value is\n-        generated for each threshold value.\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.TruePositives()\n-    >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1])\n-    >>> m.result().numpy()\n-    2.0\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0])\n-    >>> m.result().numpy()\n-    1.0\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(optimizer='sgd',\n-                  loss='mse',\n-                  metrics=[tf.keras.metrics.TruePositives()])\n-    ```\n-\n-    Usage with a loss with `from_logits=True`:\n-\n-    ```python\n-    model.compile(optimizer='adam',\n-                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n-                  metrics=[tf.keras.metrics.TruePositives(thresholds=0)])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(self, thresholds=None, name=None, dtype=None):\n-        super().__init__(\n-            confusion_matrix_cond=metrics_utils.ConfusionMatrix.TRUE_POSITIVES,\n-            thresholds=thresholds,\n-            name=name,\n-            dtype=dtype,\n-        )\n-\n-\n-@keras_export(\"keras.metrics.Precision\")\n-class Precision(base_metric.Metric):\n-    \"\"\"Computes the precision of the predictions with respect to the labels.\n-\n-    The metric creates two local variables, `true_positives` and\n-    `false_positives` that are used to compute the precision. This value is\n-    ultimately returned as `precision`, an idempotent operation that simply\n-    divides `true_positives` by the sum of `true_positives` and\n-    `false_positives`.\n-\n-    If `sample_weight` is `None`, weights default to 1.\n-    Use `sample_weight` of 0 to mask values.\n-\n-    If `top_k` is set, we'll calculate precision as how often on average a class\n-    among the top-k classes with the highest predicted values of a batch entry\n-    is correct and can be found in the label for that entry.\n-\n-    If `class_id` is specified, we calculate precision by considering only the\n-    entries in the batch for which `class_id` is above the threshold and/or in\n-    the top-k highest predictions, and computing the fraction of them for which\n-    `class_id` is indeed a correct label.\n-\n-    Args:\n-      thresholds: (Optional) A float value, or a Python list/tuple of float\n-        threshold values in [0, 1]. A threshold is compared with prediction\n-        values to determine the truth value of predictions (i.e., above the\n-        threshold is `true`, below is `false`). If used with a loss function\n-        that sets `from_logits=True` (i.e. no sigmoid applied to predictions),\n-        `thresholds` should be set to 0. One metric value is generated for each\n-        threshold value. If neither thresholds nor top_k are set, the default is\n-        to calculate precision with `thresholds=0.5`.\n-      top_k: (Optional) Unset by default. An int value specifying the top-k\n-        predictions to consider when calculating precision.\n-      class_id: (Optional) Integer class ID for which we want binary metrics.\n-        This must be in the half-open interval `[0, num_classes)`, where\n-        `num_classes` is the last dimension of predictions.\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.Precision()\n-    >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1])\n-    >>> m.result().numpy()\n-    0.6666667\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0])\n-    >>> m.result().numpy()\n-    1.0\n-\n-    >>> # With top_k=2, it will calculate precision over y_true[:2]\n-    >>> # and y_pred[:2]\n-    >>> m = tf.keras.metrics.Precision(top_k=2)\n-    >>> m.update_state([0, 0, 1, 1], [1, 1, 1, 1])\n-    >>> m.result().numpy()\n-    0.0\n-\n-    >>> # With top_k=4, it will calculate precision over y_true[:4]\n-    >>> # and y_pred[:4]\n-    >>> m = tf.keras.metrics.Precision(top_k=4)\n-    >>> m.update_state([0, 0, 1, 1], [1, 1, 1, 1])\n-    >>> m.result().numpy()\n-    0.5\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(optimizer='sgd',\n-                  loss='mse',\n-                  metrics=[tf.keras.metrics.Precision()])\n-    ```\n-\n-    Usage with a loss with `from_logits=True`:\n-\n-    ```python\n-    model.compile(optimizer='adam',\n-                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n-                  metrics=[tf.keras.metrics.Precision(thresholds=0)])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(\n-        self, thresholds=None, top_k=None, class_id=None, name=None, dtype=None\n-    ):\n-        super().__init__(name=name, dtype=dtype)\n-        self.init_thresholds = thresholds\n-        self.top_k = top_k\n-        self.class_id = class_id\n-\n-        default_threshold = 0.5 if top_k is None else metrics_utils.NEG_INF\n-        self.thresholds = metrics_utils.parse_init_thresholds(\n-            thresholds, default_threshold=default_threshold\n-        )\n-        self._thresholds_distributed_evenly = (\n-            metrics_utils.is_evenly_distributed_thresholds(self.thresholds)\n-        )\n-        self.true_positives = self.add_weight(\n-            \"true_positives\", shape=(len(self.thresholds),), initializer=\"zeros\"\n-        )\n-        self.false_positives = self.add_weight(\n-            \"false_positives\",\n-            shape=(len(self.thresholds),),\n-            initializer=\"zeros\",\n-        )\n-\n-    def update_state(self, y_true, y_pred, sample_weight=None):\n-        \"\"\"Accumulates true positive and false positive statistics.\n-\n-        Args:\n-          y_true: The ground truth values, with the same dimensions as `y_pred`.\n-            Will be cast to `bool`.\n-          y_pred: The predicted values. Each element must be in the range\n-            `[0, 1]`.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can\n-            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n-            and must be broadcastable to `y_true`.\n-\n-        Returns:\n-          Update op.\n-        \"\"\"\n-        return metrics_utils.update_confusion_matrix_variables(\n-            {\n-                metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,  # noqa: E501\n-                metrics_utils.ConfusionMatrix.FALSE_POSITIVES: self.false_positives,  # noqa: E501\n-            },\n-            y_true,\n-            y_pred,\n-            thresholds=self.thresholds,\n-            thresholds_distributed_evenly=self._thresholds_distributed_evenly,\n-            top_k=self.top_k,\n-            class_id=self.class_id,\n-            sample_weight=sample_weight,\n-        )\n-\n-    def result(self):\n-        result = tf.math.divide_no_nan(\n-            self.true_positives,\n-            tf.math.add(self.true_positives, self.false_positives),\n-        )\n-        return result[0] if len(self.thresholds) == 1 else result\n-\n-    def reset_state(self):\n-        num_thresholds = len(to_list(self.thresholds))\n-        backend.batch_set_value(\n-            [\n-                (v, np.zeros((num_thresholds,)))\n-                for v in (self.true_positives, self.false_positives)\n-            ]\n-        )\n-\n-    def get_config(self):\n-        config = {\n-            \"thresholds\": self.init_thresholds,\n-            \"top_k\": self.top_k,\n-            \"class_id\": self.class_id,\n-        }\n-        base_config = super().get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export(\"keras.metrics.Recall\")\n-class Recall(base_metric.Metric):\n-    \"\"\"Computes the recall of the predictions with respect to the labels.\n-\n-    This metric creates two local variables, `true_positives` and\n-    `false_negatives`, that are used to compute the recall. This value is\n-    ultimately returned as `recall`, an idempotent operation that simply divides\n-    `true_positives` by the sum of `true_positives` and `false_negatives`.\n-\n-    If `sample_weight` is `None`, weights default to 1.\n-    Use `sample_weight` of 0 to mask values.\n-\n-    If `top_k` is set, recall will be computed as how often on average a class\n-    among the labels of a batch entry is in the top-k predictions.\n-\n-    If `class_id` is specified, we calculate recall by considering only the\n-    entries in the batch for which `class_id` is in the label, and computing the\n-    fraction of them for which `class_id` is above the threshold and/or in the\n-    top-k predictions.\n-\n-    Args:\n-      thresholds: (Optional) A float value, or a Python list/tuple of float\n-        threshold values in [0, 1]. A threshold is compared with prediction\n-        values to determine the truth value of predictions (i.e., above the\n-        threshold is `true`, below is `false`). If used with a loss function\n-        that sets `from_logits=True` (i.e. no sigmoid applied to predictions),\n-        `thresholds` should be set to 0. One metric value is generated for each\n-        threshold value. If neither thresholds nor top_k are set, the default is\n-        to calculate recall with `thresholds=0.5`.\n-      top_k: (Optional) Unset by default. An int value specifying the top-k\n-        predictions to consider when calculating recall.\n-      class_id: (Optional) Integer class ID for which we want binary metrics.\n-        This must be in the half-open interval `[0, num_classes)`, where\n-        `num_classes` is the last dimension of predictions.\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.Recall()\n-    >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1])\n-    >>> m.result().numpy()\n-    0.6666667\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([0, 1, 1, 1], [1, 0, 1, 1], sample_weight=[0, 0, 1, 0])\n-    >>> m.result().numpy()\n-    1.0\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(optimizer='sgd',\n-                  loss='mse',\n-                  metrics=[tf.keras.metrics.Recall()])\n-    ```\n-\n-    Usage with a loss with `from_logits=True`:\n-\n-    ```python\n-    model.compile(optimizer='adam',\n-                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n-                  metrics=[tf.keras.metrics.Recall(thresholds=0)])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(\n-        self, thresholds=None, top_k=None, class_id=None, name=None, dtype=None\n-    ):\n-        super().__init__(name=name, dtype=dtype)\n-        self.init_thresholds = thresholds\n-        self.top_k = top_k\n-        self.class_id = class_id\n-\n-        default_threshold = 0.5 if top_k is None else metrics_utils.NEG_INF\n-        self.thresholds = metrics_utils.parse_init_thresholds(\n-            thresholds, default_threshold=default_threshold\n-        )\n-        self._thresholds_distributed_evenly = (\n-            metrics_utils.is_evenly_distributed_thresholds(self.thresholds)\n-        )\n-        self.true_positives = self.add_weight(\n-            \"true_positives\", shape=(len(self.thresholds),), initializer=\"zeros\"\n-        )\n-        self.false_negatives = self.add_weight(\n-            \"false_negatives\",\n-            shape=(len(self.thresholds),),\n-            initializer=\"zeros\",\n-        )\n-\n-    def update_state(self, y_true, y_pred, sample_weight=None):\n-        \"\"\"Accumulates true positive and false negative statistics.\n-\n-        Args:\n-          y_true: The ground truth values, with the same dimensions as `y_pred`.\n-            Will be cast to `bool`.\n-          y_pred: The predicted values. Each element must be in the range\n-            `[0, 1]`.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can\n-            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n-            and must be broadcastable to `y_true`.\n-\n-        Returns:\n-          Update op.\n-        \"\"\"\n-        return metrics_utils.update_confusion_matrix_variables(\n-            {\n-                metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,  # noqa: E501\n-                metrics_utils.ConfusionMatrix.FALSE_NEGATIVES: self.false_negatives,  # noqa: E501\n-            },\n-            y_true,\n-            y_pred,\n-            thresholds=self.thresholds,\n-            thresholds_distributed_evenly=self._thresholds_distributed_evenly,\n-            top_k=self.top_k,\n-            class_id=self.class_id,\n-            sample_weight=sample_weight,\n-        )\n-\n-    def result(self):\n-        result = tf.math.divide_no_nan(\n-            self.true_positives,\n-            tf.math.add(self.true_positives, self.false_negatives),\n-        )\n-        return result[0] if len(self.thresholds) == 1 else result\n-\n-    def reset_state(self):\n-        num_thresholds = len(to_list(self.thresholds))\n-        backend.batch_set_value(\n-            [\n-                (v, np.zeros((num_thresholds,)))\n-                for v in (self.true_positives, self.false_negatives)\n-            ]\n-        )\n-\n-    def get_config(self):\n-        config = {\n-            \"thresholds\": self.init_thresholds,\n-            \"top_k\": self.top_k,\n-            \"class_id\": self.class_id,\n-        }\n-        base_config = super().get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-class SensitivitySpecificityBase(base_metric.Metric, metaclass=abc.ABCMeta):\n-    \"\"\"Abstract base class for computing sensitivity and specificity.\n-\n-    For additional information about specificity and sensitivity, see\n-    [the following](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).\n-    \"\"\"\n-\n-    def __init__(\n-        self, value, num_thresholds=200, class_id=None, name=None, dtype=None\n-    ):\n-        super().__init__(name=name, dtype=dtype)\n-        if num_thresholds <= 0:\n-            raise ValueError(\n-                \"Argument `num_thresholds` must be an integer > 0. \"\n-                f\"Received: num_thresholds={num_thresholds}\"\n-            )\n-        self.value = value\n-        self.class_id = class_id\n-        self.true_positives = self.add_weight(\n-            \"true_positives\", shape=(num_thresholds,), initializer=\"zeros\"\n-        )\n-        self.true_negatives = self.add_weight(\n-            \"true_negatives\", shape=(num_thresholds,), initializer=\"zeros\"\n-        )\n-        self.false_positives = self.add_weight(\n-            \"false_positives\", shape=(num_thresholds,), initializer=\"zeros\"\n-        )\n-        self.false_negatives = self.add_weight(\n-            \"false_negatives\", shape=(num_thresholds,), initializer=\"zeros\"\n-        )\n-\n-        # Compute `num_thresholds` thresholds in [0, 1]\n-        if num_thresholds == 1:\n-            self.thresholds = [0.5]\n-            self._thresholds_distributed_evenly = False\n-        else:\n-            thresholds = [\n-                (i + 1) * 1.0 / (num_thresholds - 1)\n-                for i in range(num_thresholds - 2)\n-            ]\n-            self.thresholds = [0.0] + thresholds + [1.0]\n-            self._thresholds_distributed_evenly = True\n-\n-    def update_state(self, y_true, y_pred, sample_weight=None):\n-        \"\"\"Accumulates confusion matrix statistics.\n-\n-        Args:\n-          y_true: The ground truth values.\n-          y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can\n-            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n-            and must be broadcastable to `y_true`.\n-\n-        Returns:\n-          Update op.\n-        \"\"\"\n-        return metrics_utils.update_confusion_matrix_variables(\n-            {\n-                metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,  # noqa: E501\n-                metrics_utils.ConfusionMatrix.TRUE_NEGATIVES: self.true_negatives,  # noqa: E501\n-                metrics_utils.ConfusionMatrix.FALSE_POSITIVES: self.false_positives,  # noqa: E501\n-                metrics_utils.ConfusionMatrix.FALSE_NEGATIVES: self.false_negatives,  # noqa: E501\n-            },\n-            y_true,\n-            y_pred,\n-            thresholds=self.thresholds,\n-            thresholds_distributed_evenly=self._thresholds_distributed_evenly,\n-            class_id=self.class_id,\n-            sample_weight=sample_weight,\n-        )\n-\n-    def reset_state(self):\n-        num_thresholds = len(self.thresholds)\n-        confusion_matrix_variables = (\n-            self.true_positives,\n-            self.true_negatives,\n-            self.false_positives,\n-            self.false_negatives,\n-        )\n-        backend.batch_set_value(\n-            [\n-                (v, np.zeros((num_thresholds,)))\n-                for v in confusion_matrix_variables\n-            ]\n-        )\n-\n-    def get_config(self):\n-        config = {\"class_id\": self.class_id}\n-        base_config = super().get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-    def _find_max_under_constraint(self, constrained, dependent, predicate):\n-        \"\"\"Returns the maximum of dependent_statistic that satisfies the\n-        constraint.\n-\n-        Args:\n-          constrained: Over these values the constraint\n-            is specified. A rank-1 tensor.\n-          dependent: From these values the maximum that satiesfies the\n-            constraint is selected. Values in this tensor and in\n-            `constrained` are linked by having the same threshold at each\n-            position, hence this tensor must have the same shape.\n-          predicate: A binary boolean functor to be applied to arguments\n-          `constrained` and `self.value`, e.g. `tf.greater`.\n-\n-        Returns:\n-          maximal dependent value, if no value satiesfies the constraint 0.0.\n-        \"\"\"\n-        feasible = tf.where(predicate(constrained, self.value))\n-        feasible_exists = tf.greater(tf.size(feasible), 0)\n-        max_dependent = tf.reduce_max(tf.gather(dependent, feasible))\n-\n-        return tf.where(feasible_exists, max_dependent, 0.0)\n-\n-\n-@keras_export(\"keras.metrics.SensitivityAtSpecificity\")\n-class SensitivityAtSpecificity(SensitivitySpecificityBase):\n-    \"\"\"Computes best sensitivity where specificity is >= specified value.\n-\n-    the sensitivity at a given specificity.\n-\n-    `Sensitivity` measures the proportion of actual positives that are correctly\n-    identified as such (tp / (tp + fn)).\n-    `Specificity` measures the proportion of actual negatives that are correctly\n-    identified as such (tn / (tn + fp)).\n-\n-    This metric creates four local variables, `true_positives`,\n-    `true_negatives`, `false_positives` and `false_negatives` that are used to\n-    compute the sensitivity at the given specificity. The threshold for the\n-    given specificity value is computed and used to evaluate the corresponding\n-    sensitivity.\n-\n-    If `sample_weight` is `None`, weights default to 1.\n-    Use `sample_weight` of 0 to mask values.\n-\n-    If `class_id` is specified, we calculate precision by considering only the\n-    entries in the batch for which `class_id` is above the threshold\n-    predictions, and computing the fraction of them for which `class_id` is\n-    indeed a correct label.\n-\n-    For additional information about specificity and sensitivity, see\n-    [the following](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).\n-\n-    Args:\n-      specificity: A scalar value in range `[0, 1]`.\n-      num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n-        use for matching the given specificity.\n-      class_id: (Optional) Integer class ID for which we want binary metrics.\n-        This must be in the half-open interval `[0, num_classes)`, where\n-        `num_classes` is the last dimension of predictions.\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.SensitivityAtSpecificity(0.5)\n-    >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])\n-    >>> m.result().numpy()\n-    0.5\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],\n-    ...                sample_weight=[1, 1, 2, 2, 1])\n-    >>> m.result().numpy()\n-    0.333333\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(\n-        optimizer='sgd',\n-        loss='mse',\n-        metrics=[tf.keras.metrics.SensitivityAtSpecificity()])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(\n-        self,\n-        specificity,\n-        num_thresholds=200,\n-        class_id=None,\n-        name=None,\n-        dtype=None,\n-    ):\n-        if specificity < 0 or specificity > 1:\n-            raise ValueError(\n-                \"Argument `specificity` must be in the range [0, 1]. \"\n-                f\"Received: specificity={specificity}\"\n-            )\n-        self.specificity = specificity\n-        self.num_thresholds = num_thresholds\n-        super().__init__(\n-            specificity,\n-            num_thresholds=num_thresholds,\n-            class_id=class_id,\n-            name=name,\n-            dtype=dtype,\n-        )\n-\n-    def result(self):\n-        specificities = tf.math.divide_no_nan(\n-            self.true_negatives,\n-            tf.math.add(self.true_negatives, self.false_positives),\n-        )\n-        sensitivities = tf.math.divide_no_nan(\n-            self.true_positives,\n-            tf.math.add(self.true_positives, self.false_negatives),\n-        )\n-        return self._find_max_under_constraint(\n-            specificities, sensitivities, tf.greater_equal\n-        )\n-\n-    def get_config(self):\n-        config = {\n-            \"num_thresholds\": self.num_thresholds,\n-            \"specificity\": self.specificity,\n-        }\n-        base_config = super().get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export(\"keras.metrics.SpecificityAtSensitivity\")\n-class SpecificityAtSensitivity(SensitivitySpecificityBase):\n-    \"\"\"Computes best specificity where sensitivity is >= specified value.\n-\n-    `Sensitivity` measures the proportion of actual positives that are correctly\n-    identified as such (tp / (tp + fn)).\n-    `Specificity` measures the proportion of actual negatives that are correctly\n-    identified as such (tn / (tn + fp)).\n-\n-    This metric creates four local variables, `true_positives`,\n-    `true_negatives`, `false_positives` and `false_negatives` that are used to\n-    compute the specificity at the given sensitivity. The threshold for the\n-    given sensitivity value is computed and used to evaluate the corresponding\n-    specificity.\n-\n-    If `sample_weight` is `None`, weights default to 1.\n-    Use `sample_weight` of 0 to mask values.\n-\n-    If `class_id` is specified, we calculate precision by considering only the\n-    entries in the batch for which `class_id` is above the threshold\n-    predictions, and computing the fraction of them for which `class_id` is\n-    indeed a correct label.\n-\n-    For additional information about specificity and sensitivity, see\n-    [the following](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).\n-\n-    Args:\n-      sensitivity: A scalar value in range `[0, 1]`.\n-      num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n-        use for matching the given sensitivity.\n-      class_id: (Optional) Integer class ID for which we want binary metrics.\n-        This must be in the half-open interval `[0, num_classes)`, where\n-        `num_classes` is the last dimension of predictions.\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.SpecificityAtSensitivity(0.5)\n-    >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])\n-    >>> m.result().numpy()\n-    0.66666667\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],\n-    ...                sample_weight=[1, 1, 2, 2, 2])\n-    >>> m.result().numpy()\n-    0.5\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(\n-        optimizer='sgd',\n-        loss='mse',\n-        metrics=[tf.keras.metrics.SpecificityAtSensitivity()])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(\n-        self,\n-        sensitivity,\n-        num_thresholds=200,\n-        class_id=None,\n-        name=None,\n-        dtype=None,\n-    ):\n-        if sensitivity < 0 or sensitivity > 1:\n-            raise ValueError(\n-                \"Argument `sensitivity` must be in the range [0, 1]. \"\n-                f\"Received: sensitivity={sensitivity}\"\n-            )\n-        self.sensitivity = sensitivity\n-        self.num_thresholds = num_thresholds\n-        super().__init__(\n-            sensitivity,\n-            num_thresholds=num_thresholds,\n-            class_id=class_id,\n-            name=name,\n-            dtype=dtype,\n-        )\n-\n-    def result(self):\n-        sensitivities = tf.math.divide_no_nan(\n-            self.true_positives,\n-            tf.math.add(self.true_positives, self.false_negatives),\n-        )\n-        specificities = tf.math.divide_no_nan(\n-            self.true_negatives,\n-            tf.math.add(self.true_negatives, self.false_positives),\n-        )\n-        return self._find_max_under_constraint(\n-            sensitivities, specificities, tf.greater_equal\n-        )\n-\n-    def get_config(self):\n-        config = {\n-            \"num_thresholds\": self.num_thresholds,\n-            \"sensitivity\": self.sensitivity,\n-        }\n-        base_config = super().get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export(\"keras.metrics.PrecisionAtRecall\")\n-class PrecisionAtRecall(SensitivitySpecificityBase):\n-    \"\"\"Computes best precision where recall is >= specified value.\n-\n-    This metric creates four local variables, `true_positives`,\n-    `true_negatives`, `false_positives` and `false_negatives` that are used to\n-    compute the precision at the given recall. The threshold for the given\n-    recall value is computed and used to evaluate the corresponding precision.\n-\n-    If `sample_weight` is `None`, weights default to 1.\n-    Use `sample_weight` of 0 to mask values.\n-\n-    If `class_id` is specified, we calculate precision by considering only the\n-    entries in the batch for which `class_id` is above the threshold\n-    predictions, and computing the fraction of them for which `class_id` is\n-    indeed a correct label.\n-\n-    Args:\n-      recall: A scalar value in range `[0, 1]`.\n-      num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n-        use for matching the given recall.\n-      class_id: (Optional) Integer class ID for which we want binary metrics.\n-        This must be in the half-open interval `[0, num_classes)`, where\n-        `num_classes` is the last dimension of predictions.\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.PrecisionAtRecall(0.5)\n-    >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])\n-    >>> m.result().numpy()\n-    0.5\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8],\n-    ...                sample_weight=[2, 2, 2, 1, 1])\n-    >>> m.result().numpy()\n-    0.33333333\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(\n-        optimizer='sgd',\n-        loss='mse',\n-        metrics=[tf.keras.metrics.PrecisionAtRecall(recall=0.8)])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(\n-        self, recall, num_thresholds=200, class_id=None, name=None, dtype=None\n-    ):\n-        if recall < 0 or recall > 1:\n-            raise ValueError(\n-                \"Argument `recall` must be in the range [0, 1]. \"\n-                f\"Received: recall={recall}\"\n-            )\n-        self.recall = recall\n-        self.num_thresholds = num_thresholds\n-        super().__init__(\n-            value=recall,\n-            num_thresholds=num_thresholds,\n-            class_id=class_id,\n-            name=name,\n-            dtype=dtype,\n-        )\n-\n-    def result(self):\n-        recalls = tf.math.divide_no_nan(\n-            self.true_positives,\n-            tf.math.add(self.true_positives, self.false_negatives),\n-        )\n-        precisions = tf.math.divide_no_nan(\n-            self.true_positives,\n-            tf.math.add(self.true_positives, self.false_positives),\n-        )\n-        return self._find_max_under_constraint(\n-            recalls, precisions, tf.greater_equal\n-        )\n-\n-    def get_config(self):\n-        config = {\"num_thresholds\": self.num_thresholds, \"recall\": self.recall}\n-        base_config = super().get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export(\"keras.metrics.RecallAtPrecision\")\n-class RecallAtPrecision(SensitivitySpecificityBase):\n-    \"\"\"Computes best recall where precision is >= specified value.\n-\n-    For a given score-label-distribution the required precision might not\n-    be achievable, in this case 0.0 is returned as recall.\n-\n-    This metric creates four local variables, `true_positives`,\n-    `true_negatives`, `false_positives` and `false_negatives` that are used to\n-    compute the recall at the given precision. The threshold for the given\n-    precision value is computed and used to evaluate the corresponding recall.\n-\n-    If `sample_weight` is `None`, weights default to 1.\n-    Use `sample_weight` of 0 to mask values.\n-\n-    If `class_id` is specified, we calculate precision by considering only the\n-    entries in the batch for which `class_id` is above the threshold\n-    predictions, and computing the fraction of them for which `class_id` is\n-    indeed a correct label.\n-\n-    Args:\n-      precision: A scalar value in range `[0, 1]`.\n-      num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n-        use for matching the given precision.\n-      class_id: (Optional) Integer class ID for which we want binary metrics.\n-        This must be in the half-open interval `[0, num_classes)`, where\n-        `num_classes` is the last dimension of predictions.\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.RecallAtPrecision(0.8)\n-    >>> m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])\n-    >>> m.result().numpy()\n-    0.5\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9],\n-    ...                sample_weight=[1, 0, 0, 1])\n-    >>> m.result().numpy()\n-    1.0\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(\n-        optimizer='sgd',\n-        loss='mse',\n-        metrics=[tf.keras.metrics.RecallAtPrecision(precision=0.8)])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(\n-        self,\n-        precision,\n-        num_thresholds=200,\n-        class_id=None,\n-        name=None,\n-        dtype=None,\n-    ):\n-        if precision < 0 or precision > 1:\n-            raise ValueError(\n-                \"Argument `precision` must be in the range [0, 1]. \"\n-                f\"Received: precision={precision}\"\n-            )\n-        self.precision = precision\n-        self.num_thresholds = num_thresholds\n-        super().__init__(\n-            value=precision,\n-            num_thresholds=num_thresholds,\n-            class_id=class_id,\n-            name=name,\n-            dtype=dtype,\n-        )\n-\n-    def result(self):\n-        precisions = tf.math.divide_no_nan(\n-            self.true_positives,\n-            tf.math.add(self.true_positives, self.false_positives),\n-        )\n-        recalls = tf.math.divide_no_nan(\n-            self.true_positives,\n-            tf.math.add(self.true_positives, self.false_negatives),\n-        )\n-        return self._find_max_under_constraint(\n-            precisions, recalls, tf.greater_equal\n-        )\n-\n-    def get_config(self):\n-        config = {\n-            \"num_thresholds\": self.num_thresholds,\n-            \"precision\": self.precision,\n-        }\n-        base_config = super().get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export(\"keras.metrics.AUC\")\n-class AUC(base_metric.Metric):\n-    \"\"\"Approximates the AUC (Area under the curve) of the ROC or PR curves.\n-\n-    The AUC (Area under the curve) of the ROC (Receiver operating\n-    characteristic; default) or PR (Precision Recall) curves are quality\n-    measures of binary classifiers. Unlike the accuracy, and like cross-entropy\n-    losses, ROC-AUC and PR-AUC evaluate all the operational points of a model.\n-\n-    This class approximates AUCs using a Riemann sum. During the metric\n-    accumulation phrase, predictions are accumulated within predefined buckets\n-    by value. The AUC is then computed by interpolating per-bucket averages.\n-    These buckets define the evaluated operational points.\n-\n-    This metric creates four local variables, `true_positives`,\n-    `true_negatives`, `false_positives` and `false_negatives` that are used to\n-    compute the AUC.  To discretize the AUC curve, a linearly spaced set of\n-    thresholds is used to compute pairs of recall and precision values. The area\n-    under the ROC-curve is therefore computed using the height of the recall\n-    values by the false positive rate, while the area under the PR-curve is the\n-    computed using the height of the precision values by the recall.\n-\n-    This value is ultimately returned as `auc`, an idempotent operation that\n-    computes the area under a discretized curve of precision versus recall\n-    values (computed using the aforementioned variables). The `num_thresholds`\n-    variable controls the degree of discretization with larger numbers of\n-    thresholds more closely approximating the true AUC. The quality of the\n-    approximation may vary dramatically depending on `num_thresholds`. The\n-    `thresholds` parameter can be used to manually specify thresholds which\n-    split the predictions more evenly.\n-\n-    For a best approximation of the real AUC, `predictions` should be\n-    distributed approximately uniformly in the range [0, 1] (if\n-    `from_logits=False`). The quality of the AUC approximation may be poor if\n-    this is not the case. Setting `summation_method` to 'minoring' or 'majoring'\n-    can help quantify the error in the approximation by providing lower or upper\n-    bound estimate of the AUC.\n-\n-    If `sample_weight` is `None`, weights default to 1.\n-    Use `sample_weight` of 0 to mask values.\n-\n-    Args:\n-      num_thresholds: (Optional) Defaults to 200. The number of thresholds to\n-        use when discretizing the roc curve. Values must be > 1.\n-      curve: (Optional) Specifies the name of the curve to be computed, 'ROC'\n-        [default] or 'PR' for the Precision-Recall-curve.\n-      summation_method: (Optional) Specifies the [Riemann summation method](\n-          https://en.wikipedia.org/wiki/Riemann_sum) used.\n-          'interpolation' (default) applies mid-point summation scheme for\n-          `ROC`.  For PR-AUC, interpolates (true/false) positives but not the\n-          ratio that is precision (see Davis & Goadrich 2006 for details);\n-          'minoring' applies left summation for increasing intervals and right\n-          summation for decreasing intervals; 'majoring' does the opposite.\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-      thresholds: (Optional) A list of floating point values to use as the\n-        thresholds for discretizing the curve. If set, the `num_thresholds`\n-        parameter is ignored. Values should be in [0, 1]. Endpoint thresholds\n-        equal to {-epsilon, 1+epsilon} for a small positive epsilon value will\n-        be automatically included with these to correctly handle predictions\n-        equal to exactly 0 or 1.\n-      multi_label: boolean indicating whether multilabel data should be\n-        treated as such, wherein AUC is computed separately for each label and\n-        then averaged across labels, or (when False) if the data should be\n-        flattened into a single label before AUC computation. In the latter\n-        case, when multilabel data is passed to AUC, each label-prediction pair\n-        is treated as an individual data point. Should be set to False for\n-        multi-class data.\n-      num_labels: (Optional) The number of labels, used when `multi_label` is\n-        True. If `num_labels` is not specified, then state variables get created\n-        on the first call to `update_state`.\n-      label_weights: (Optional) list, array, or tensor of non-negative weights\n-        used to compute AUCs for multilabel data. When `multi_label` is True,\n-        the weights are applied to the individual label AUCs when they are\n-        averaged to produce the multi-label AUC. When it's False, they are used\n-        to weight the individual label predictions in computing the confusion\n-        matrix on the flattened data. Note that this is unlike class_weights in\n-        that class_weights weights the example depending on the value of its\n-        label, whereas label_weights depends only on the index of that label\n-        before flattening; therefore `label_weights` should not be used for\n-        multi-class data.\n-      from_logits: boolean indicating whether the predictions (`y_pred` in\n-        `update_state`) are probabilities or sigmoid logits. As a rule of thumb,\n-        when using a keras loss, the `from_logits` constructor argument of the\n-        loss should match the AUC `from_logits` constructor argument.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.AUC(num_thresholds=3)\n-    >>> m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])\n-    >>> # threshold values are [0 - 1e-7, 0.5, 1 + 1e-7]\n-    >>> # tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]\n-    >>> # tp_rate = recall = [1, 0.5, 0], fp_rate = [1, 0, 0]\n-    >>> # auc = ((((1+0.5)/2)*(1-0)) + (((0.5+0)/2)*(0-0))) = 0.75\n-    >>> m.result().numpy()\n-    0.75\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9],\n-    ...                sample_weight=[1, 0, 0, 1])\n-    >>> m.result().numpy()\n-    1.0\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    # Reports the AUC of a model outputting a probability.\n-    model.compile(optimizer='sgd',\n-                  loss=tf.keras.losses.BinaryCrossentropy(),\n-                  metrics=[tf.keras.metrics.AUC()])\n-\n-    # Reports the AUC of a model outputting a logit.\n-    model.compile(optimizer='sgd',\n-                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n-                  metrics=[tf.keras.metrics.AUC(from_logits=True)])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(\n-        self,\n-        num_thresholds=200,\n-        curve=\"ROC\",\n-        summation_method=\"interpolation\",\n-        name=None,\n-        dtype=None,\n-        thresholds=None,\n-        multi_label=False,\n-        num_labels=None,\n-        label_weights=None,\n-        from_logits=False,\n-    ):\n-        # Validate configurations.\n-        if isinstance(curve, metrics_utils.AUCCurve) and curve not in list(\n-            metrics_utils.AUCCurve\n-        ):\n-            raise ValueError(\n-                f'Invalid `curve` argument value \"{curve}\". '\n-                f\"Expected one of: {list(metrics_utils.AUCCurve)}\"\n-            )\n-        if isinstance(\n-            summation_method, metrics_utils.AUCSummationMethod\n-        ) and summation_method not in list(metrics_utils.AUCSummationMethod):\n-            raise ValueError(\n-                \"Invalid `summation_method` \"\n-                f'argument value \"{summation_method}\". '\n-                f\"Expected one of: {list(metrics_utils.AUCSummationMethod)}\"\n-            )\n-\n-        # Update properties.\n-        self._init_from_thresholds = thresholds is not None\n-        if thresholds is not None:\n-            # If specified, use the supplied thresholds.\n-            self.num_thresholds = len(thresholds) + 2\n-            thresholds = sorted(thresholds)\n-            self._thresholds_distributed_evenly = (\n-                metrics_utils.is_evenly_distributed_thresholds(\n-                    np.array([0.0] + thresholds + [1.0])\n-                )\n-            )\n-        else:\n-            if num_thresholds <= 1:\n-                raise ValueError(\n-                    \"Argument `num_thresholds` must be an integer > 1. \"\n-                    f\"Received: num_thresholds={num_thresholds}\"\n-                )\n-\n-            # Otherwise, linearly interpolate (num_thresholds - 2) thresholds in\n-            # (0, 1).\n-            self.num_thresholds = num_thresholds\n-            thresholds = [\n-                (i + 1) * 1.0 / (num_thresholds - 1)\n-                for i in range(num_thresholds - 2)\n-            ]\n-            self._thresholds_distributed_evenly = True\n-\n-        # Add an endpoint \"threshold\" below zero and above one for either\n-        # threshold method to account for floating point imprecisions.\n-        self._thresholds = np.array(\n-            [0.0 - backend.epsilon()] + thresholds + [1.0 + backend.epsilon()]\n-        )\n-\n-        if isinstance(curve, metrics_utils.AUCCurve):\n-            self.curve = curve\n-        else:\n-            self.curve = metrics_utils.AUCCurve.from_str(curve)\n-        if isinstance(summation_method, metrics_utils.AUCSummationMethod):\n-            self.summation_method = summation_method\n-        else:\n-            self.summation_method = metrics_utils.AUCSummationMethod.from_str(\n-                summation_method\n-            )\n-        super().__init__(name=name, dtype=dtype)\n-\n-        # Handle multilabel arguments.\n-        self.multi_label = multi_label\n-        self.num_labels = num_labels\n-        if label_weights is not None:\n-            label_weights = tf.constant(label_weights, dtype=self.dtype)\n-            tf.debugging.assert_non_negative(\n-                label_weights,\n-                message=\"All values of `label_weights` must be non-negative.\",\n-            )\n-            self.label_weights = label_weights\n-\n-        else:\n-            self.label_weights = None\n-\n-        self._from_logits = from_logits\n-\n-        self._built = False\n-        if self.multi_label:\n-            if num_labels:\n-                shape = tf.TensorShape([None, num_labels])\n-                self._build(shape)\n-        else:\n-            if num_labels:\n-                raise ValueError(\n-                    \"`num_labels` is needed only when `multi_label` is True.\"\n-                )\n-            self._build(None)\n-\n-    @property\n-    def thresholds(self):\n-        \"\"\"The thresholds used for evaluating AUC.\"\"\"\n-        return list(self._thresholds)\n-\n-    def _build(self, shape):\n-        \"\"\"Initialize TP, FP, TN, and FN tensors, given the shape of the\n-        data.\"\"\"\n-        if self.multi_label:\n-            if shape.ndims != 2:\n-                raise ValueError(\n-                    \"`y_true` must have rank 2 when `multi_label=True`. \"\n-                    f\"Found rank {shape.ndims}. \"\n-                    f\"Full shape received for `y_true`: {shape}\"\n-                )\n-            self._num_labels = shape[1]\n-            variable_shape = tf.TensorShape(\n-                [self.num_thresholds, self._num_labels]\n-            )\n-        else:\n-            variable_shape = tf.TensorShape([self.num_thresholds])\n-\n-        self._build_input_shape = shape\n-        # Create metric variables\n-        self.true_positives = self.add_weight(\n-            \"true_positives\", shape=variable_shape, initializer=\"zeros\"\n-        )\n-        self.true_negatives = self.add_weight(\n-            \"true_negatives\", shape=variable_shape, initializer=\"zeros\"\n-        )\n-        self.false_positives = self.add_weight(\n-            \"false_positives\", shape=variable_shape, initializer=\"zeros\"\n-        )\n-        self.false_negatives = self.add_weight(\n-            \"false_negatives\", shape=variable_shape, initializer=\"zeros\"\n-        )\n-\n-        if self.multi_label:\n-            with tf.init_scope():\n-                # This should only be necessary for handling v1 behavior. In v2,\n-                # AUC should be initialized outside of any tf.functions, and\n-                # therefore in eager mode.\n-                if not tf.executing_eagerly():\n-                    backend._initialize_variables(backend._get_session())\n-\n-        self._built = True\n-\n-    def update_state(self, y_true, y_pred, sample_weight=None):\n-        \"\"\"Accumulates confusion matrix statistics.\n-\n-        Args:\n-          y_true: The ground truth values.\n-          y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can\n-            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n-            and must be broadcastable to `y_true`.\n-\n-        Returns:\n-          Update op.\n-        \"\"\"\n-        if not self._built:\n-            self._build(tf.TensorShape(y_pred.shape))\n-\n-        if self.multi_label or (self.label_weights is not None):\n-            # y_true should have shape (number of examples, number of labels).\n-            shapes = [(y_true, (\"N\", \"L\"))]\n-            if self.multi_label:\n-                # TP, TN, FP, and FN should all have shape\n-                # (number of thresholds, number of labels).\n-                shapes.extend(\n-                    [\n-                        (self.true_positives, (\"T\", \"L\")),\n-                        (self.true_negatives, (\"T\", \"L\")),\n-                        (self.false_positives, (\"T\", \"L\")),\n-                        (self.false_negatives, (\"T\", \"L\")),\n-                    ]\n-                )\n-            if self.label_weights is not None:\n-                # label_weights should be of length equal to the number of\n-                # labels.\n-                shapes.append((self.label_weights, (\"L\",)))\n-                tf.debugging.assert_shapes(\n-                    shapes, message=\"Number of labels is not consistent.\"\n-                )\n-\n-        # Only forward label_weights to update_confusion_matrix_variables when\n-        # multi_label is False. Otherwise the averaging of individual label AUCs\n-        # is handled in AUC.result\n-        label_weights = None if self.multi_label else self.label_weights\n-\n-        if self._from_logits:\n-            y_pred = activations.sigmoid(y_pred)\n-\n-        return metrics_utils.update_confusion_matrix_variables(\n-            {\n-                metrics_utils.ConfusionMatrix.TRUE_POSITIVES: self.true_positives,  # noqa: E501\n-                metrics_utils.ConfusionMatrix.TRUE_NEGATIVES: self.true_negatives,  # noqa: E501\n-                metrics_utils.ConfusionMatrix.FALSE_POSITIVES: self.false_positives,  # noqa: E501\n-                metrics_utils.ConfusionMatrix.FALSE_NEGATIVES: self.false_negatives,  # noqa: E501\n-            },\n-            y_true,\n-            y_pred,\n-            self._thresholds,\n-            thresholds_distributed_evenly=self._thresholds_distributed_evenly,\n-            sample_weight=sample_weight,\n-            multi_label=self.multi_label,\n-            label_weights=label_weights,\n-        )\n-\n-    def interpolate_pr_auc(self):\n-        \"\"\"Interpolation formula inspired by section 4 of Davis & Goadrich 2006.\n-\n-        https://www.biostat.wisc.edu/~page/rocpr.pdf\n-\n-        Note here we derive & use a closed formula not present in the paper\n-        as follows:\n-\n-          Precision = TP / (TP + FP) = TP / P\n-\n-        Modeling all of TP (true positive), FP (false positive) and their sum\n-        P = TP + FP (predicted positive) as varying linearly within each\n-        interval [A, B] between successive thresholds, we get\n-\n-          Precision slope = dTP / dP\n-                          = (TP_B - TP_A) / (P_B - P_A)\n-                          = (TP - TP_A) / (P - P_A)\n-          Precision = (TP_A + slope * (P - P_A)) / P\n-\n-        The area within the interval is (slope / total_pos_weight) times\n-\n-          int_A^B{Precision.dP} = int_A^B{(TP_A + slope * (P - P_A)) * dP / P}\n-          int_A^B{Precision.dP} = int_A^B{slope * dP + intercept * dP / P}\n-\n-        where intercept = TP_A - slope * P_A = TP_B - slope * P_B, resulting in\n-\n-          int_A^B{Precision.dP} = TP_B - TP_A + intercept * log(P_B / P_A)\n-\n-        Bringing back the factor (slope / total_pos_weight) we'd put aside, we\n-        get\n-\n-          slope * [dTP + intercept *  log(P_B / P_A)] / total_pos_weight\n-\n-        where dTP == TP_B - TP_A.\n-\n-        Note that when P_A == 0 the above calculation simplifies into\n-\n-          int_A^B{Precision.dTP} = int_A^B{slope * dTP} = slope * (TP_B - TP_A)\n-\n-        which is really equivalent to imputing constant precision throughout the\n-        first bucket having >0 true positives.\n-\n-        Returns:\n-          pr_auc: an approximation of the area under the P-R curve.\n-        \"\"\"\n-        dtp = (\n-            self.true_positives[: self.num_thresholds - 1]\n-            - self.true_positives[1:]\n-        )\n-        p = tf.math.add(self.true_positives, self.false_positives)\n-        dp = p[: self.num_thresholds - 1] - p[1:]\n-        prec_slope = tf.math.divide_no_nan(\n-            dtp, tf.maximum(dp, 0), name=\"prec_slope\"\n-        )\n-        intercept = self.true_positives[1:] - tf.multiply(prec_slope, p[1:])\n-\n-        safe_p_ratio = tf.where(\n-            tf.logical_and(p[: self.num_thresholds - 1] > 0, p[1:] > 0),\n-            tf.math.divide_no_nan(\n-                p[: self.num_thresholds - 1],\n-                tf.maximum(p[1:], 0),\n-                name=\"recall_relative_ratio\",\n-            ),\n-            tf.ones_like(p[1:]),\n-        )\n-\n-        pr_auc_increment = tf.math.divide_no_nan(\n-            prec_slope * (dtp + intercept * tf.math.log(safe_p_ratio)),\n-            tf.maximum(self.true_positives[1:] + self.false_negatives[1:], 0),\n-            name=\"pr_auc_increment\",\n-        )\n-\n-        if self.multi_label:\n-            by_label_auc = tf.reduce_sum(\n-                pr_auc_increment, name=self.name + \"_by_label\", axis=0\n-            )\n-            if self.label_weights is None:\n-                # Evenly weighted average of the label AUCs.\n-                return tf.reduce_mean(by_label_auc, name=self.name)\n-            else:\n-                # Weighted average of the label AUCs.\n-                return tf.math.divide_no_nan(\n-                    tf.reduce_sum(\n-                        tf.multiply(by_label_auc, self.label_weights)\n-                    ),\n-                    tf.reduce_sum(self.label_weights),\n-                    name=self.name,\n-                )\n-        else:\n-            return tf.reduce_sum(pr_auc_increment, name=\"interpolate_pr_auc\")\n-\n-    def result(self):\n-        if (\n-            self.curve == metrics_utils.AUCCurve.PR\n-            and self.summation_method\n-            == metrics_utils.AUCSummationMethod.INTERPOLATION\n-        ):\n-            # This use case is different and is handled separately.\n-            return self.interpolate_pr_auc()\n-\n-        # Set `x` and `y` values for the curves based on `curve` config.\n-        recall = tf.math.divide_no_nan(\n-            self.true_positives,\n-            tf.math.add(self.true_positives, self.false_negatives),\n-        )\n-        if self.curve == metrics_utils.AUCCurve.ROC:\n-            fp_rate = tf.math.divide_no_nan(\n-                self.false_positives,\n-                tf.math.add(self.false_positives, self.true_negatives),\n-            )\n-            x = fp_rate\n-            y = recall\n-        else:  # curve == 'PR'.\n-            precision = tf.math.divide_no_nan(\n-                self.true_positives,\n-                tf.math.add(self.true_positives, self.false_positives),\n-            )\n-            x = recall\n-            y = precision\n-\n-        # Find the rectangle heights based on `summation_method`.\n-        if (\n-            self.summation_method\n-            == metrics_utils.AUCSummationMethod.INTERPOLATION\n-        ):\n-            # Note: the case ('PR', 'interpolation') has been handled above.\n-            heights = (y[: self.num_thresholds - 1] + y[1:]) / 2.0\n-        elif self.summation_method == metrics_utils.AUCSummationMethod.MINORING:\n-            heights = tf.minimum(y[: self.num_thresholds - 1], y[1:])\n-        # self.summation_method = metrics_utils.AUCSummationMethod.MAJORING:\n-        else:\n-            heights = tf.maximum(y[: self.num_thresholds - 1], y[1:])\n-\n-        # Sum up the areas of all the rectangles.\n-        if self.multi_label:\n-            riemann_terms = tf.multiply(\n-                x[: self.num_thresholds - 1] - x[1:], heights\n-            )\n-            by_label_auc = tf.reduce_sum(\n-                riemann_terms, name=self.name + \"_by_label\", axis=0\n-            )\n-\n-            if self.label_weights is None:\n-                # Unweighted average of the label AUCs.\n-                return tf.reduce_mean(by_label_auc, name=self.name)\n-            else:\n-                # Weighted average of the label AUCs.\n-                return tf.math.divide_no_nan(\n-                    tf.reduce_sum(\n-                        tf.multiply(by_label_auc, self.label_weights)\n-                    ),\n-                    tf.reduce_sum(self.label_weights),\n-                    name=self.name,\n-                )\n-        else:\n-            return tf.reduce_sum(\n-                tf.multiply(x[: self.num_thresholds - 1] - x[1:], heights),\n-                name=self.name,\n-            )\n-\n-    def reset_state(self):\n-        if self._built:\n-            confusion_matrix_variables = (\n-                self.true_positives,\n-                self.true_negatives,\n-                self.false_positives,\n-                self.false_negatives,\n-            )\n-            if self.multi_label:\n-                backend.batch_set_value(\n-                    [\n-                        (v, np.zeros((self.num_thresholds, self._num_labels)))\n-                        for v in confusion_matrix_variables\n-                    ]\n-                )\n-            else:\n-                backend.batch_set_value(\n-                    [\n-                        (v, np.zeros((self.num_thresholds,)))\n-                        for v in confusion_matrix_variables\n-                    ]\n-                )\n-\n-    def get_config(self):\n-        if is_tensor_or_variable(self.label_weights):\n-            label_weights = backend.eval(self.label_weights)\n-        else:\n-            label_weights = self.label_weights\n-        config = {\n-            \"num_thresholds\": self.num_thresholds,\n-            \"curve\": self.curve.value,\n-            \"summation_method\": self.summation_method.value,\n-            \"multi_label\": self.multi_label,\n-            \"num_labels\": self.num_labels,\n-            \"label_weights\": label_weights,\n-            \"from_logits\": self._from_logits,\n-        }\n-        # optimization to avoid serializing a large number of generated\n-        # thresholds\n-        if self._init_from_thresholds:\n-            # We remove the endpoint thresholds as an inverse of how the\n-            # thresholds were initialized. This ensures that a metric\n-            # initialized from this config has the same thresholds.\n-            config[\"thresholds\"] = self.thresholds[1:-1]\n-        base_config = super().get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export(\"keras.metrics.CosineSimilarity\")\n-class CosineSimilarity(base_metric.MeanMetricWrapper):\n-    \"\"\"Computes the cosine similarity between the labels and predictions.\n-\n-    `cosine similarity = (a . b) / ||a|| ||b||`\n-\n-    See: [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity).\n-\n-    This metric keeps the average cosine similarity between `predictions` and\n-    `labels` over a stream of data.\n-\n-    Args:\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-      axis: (Optional) Defaults to -1. The dimension along which the cosine\n-        similarity is computed.\n-\n-    Standalone usage:\n-\n-    >>> # l2_norm(y_true) = [[0., 1.], [1./1.414, 1./1.414]]\n-    >>> # l2_norm(y_pred) = [[1., 0.], [1./1.414, 1./1.414]]\n-    >>> # l2_norm(y_true) . l2_norm(y_pred) = [[0., 0.], [0.5, 0.5]]\n-    >>> # result = mean(sum(l2_norm(y_true) . l2_norm(y_pred), axis=1))\n-    >>> #        = ((0. + 0.) +  (0.5 + 0.5)) / 2\n-    >>> m = tf.keras.metrics.CosineSimilarity(axis=1)\n-    >>> m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]])\n-    >>> m.result().numpy()\n-    0.49999997\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]],\n-    ...                sample_weight=[0.3, 0.7])\n-    >>> m.result().numpy()\n-    0.6999999\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(\n-        optimizer='sgd',\n-        loss='mse',\n-        metrics=[tf.keras.metrics.CosineSimilarity(axis=1)])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(self, name=\"cosine_similarity\", dtype=None, axis=-1):\n-        super().__init__(cosine_similarity, name, dtype=dtype, axis=axis)\n-\n-\n-@keras_export(\"keras.metrics.MeanAbsoluteError\")\n-class MeanAbsoluteError(base_metric.MeanMetricWrapper):\n-    \"\"\"Computes the mean absolute error between the labels and predictions.\n-\n-    Args:\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.MeanAbsoluteError()\n-    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n-    >>> m.result().numpy()\n-    0.25\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n-    ...                sample_weight=[1, 0])\n-    >>> m.result().numpy()\n-    0.5\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(\n-        optimizer='sgd',\n-        loss='mse',\n-        metrics=[tf.keras.metrics.MeanAbsoluteError()])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(self, name=\"mean_absolute_error\", dtype=None):\n-        super().__init__(mean_absolute_error, name, dtype=dtype)\n-\n-\n-@keras_export(\"keras.metrics.MeanAbsolutePercentageError\")\n-class MeanAbsolutePercentageError(base_metric.MeanMetricWrapper):\n-    \"\"\"Computes the mean absolute percentage error between `y_true` and\n-    `y_pred`.\n-\n-    Args:\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.MeanAbsolutePercentageError()\n-    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n-    >>> m.result().numpy()\n-    250000000.0\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n-    ...                sample_weight=[1, 0])\n-    >>> m.result().numpy()\n-    500000000.0\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(\n-        optimizer='sgd',\n-        loss='mse',\n-        metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(self, name=\"mean_absolute_percentage_error\", dtype=None):\n-        super().__init__(mean_absolute_percentage_error, name, dtype=dtype)\n-\n-\n-@keras_export(\"keras.metrics.MeanSquaredError\")\n-class MeanSquaredError(base_metric.MeanMetricWrapper):\n-    \"\"\"Computes the mean squared error between `y_true` and `y_pred`.\n-\n-    Args:\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.MeanSquaredError()\n-    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n-    >>> m.result().numpy()\n-    0.25\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n-    ...                sample_weight=[1, 0])\n-    >>> m.result().numpy()\n-    0.5\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(\n-        optimizer='sgd',\n-        loss='mse',\n-        metrics=[tf.keras.metrics.MeanSquaredError()])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(self, name=\"mean_squared_error\", dtype=None):\n-        super().__init__(mean_squared_error, name, dtype=dtype)\n-\n-\n-@keras_export(\"keras.metrics.MeanSquaredLogarithmicError\")\n-class MeanSquaredLogarithmicError(base_metric.MeanMetricWrapper):\n-    \"\"\"Computes the mean squared logarithmic error between `y_true` and\n-    `y_pred`.\n-\n-    Args:\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.MeanSquaredLogarithmicError()\n-    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n-    >>> m.result().numpy()\n-    0.12011322\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n-    ...                sample_weight=[1, 0])\n-    >>> m.result().numpy()\n-    0.24022643\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(\n-        optimizer='sgd',\n-        loss='mse',\n-        metrics=[tf.keras.metrics.MeanSquaredLogarithmicError()])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(self, name=\"mean_squared_logarithmic_error\", dtype=None):\n-        super().__init__(mean_squared_logarithmic_error, name, dtype=dtype)\n-\n-\n-@keras_export(\"keras.metrics.Hinge\")\n-class Hinge(base_metric.MeanMetricWrapper):\n-    \"\"\"Computes the hinge metric between `y_true` and `y_pred`.\n-\n-    `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are\n-    provided we will convert them to -1 or 1.\n-\n-    Args:\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.Hinge()\n-    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])\n-    >>> m.result().numpy()\n-    1.3\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],\n-    ...                sample_weight=[1, 0])\n-    >>> m.result().numpy()\n-    1.1\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(\n-        optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.Hinge()])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(self, name=\"hinge\", dtype=None):\n-        super().__init__(hinge, name, dtype=dtype)\n-\n-\n-@keras_export(\"keras.metrics.SquaredHinge\")\n-class SquaredHinge(base_metric.MeanMetricWrapper):\n-    \"\"\"Computes the squared hinge metric between `y_true` and `y_pred`.\n-\n-    `y_true` values are expected to be -1 or 1. If binary (0 or 1) labels are\n-    provided we will convert them to -1 or 1.\n-\n-    Args:\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.SquaredHinge()\n-    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])\n-    >>> m.result().numpy()\n-    1.86\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],\n-    ...                sample_weight=[1, 0])\n-    >>> m.result().numpy()\n-    1.46\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(\n-        optimizer='sgd',\n-        loss='mse',\n-        metrics=[tf.keras.metrics.SquaredHinge()])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(self, name=\"squared_hinge\", dtype=None):\n-        super().__init__(squared_hinge, name, dtype=dtype)\n-\n-\n-@keras_export(\"keras.metrics.CategoricalHinge\")\n-class CategoricalHinge(base_metric.MeanMetricWrapper):\n-    \"\"\"Computes the categorical hinge metric between `y_true` and `y_pred`.\n-\n-    Args:\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.CategoricalHinge()\n-    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])\n-    >>> m.result().numpy()\n-    1.4000001\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],\n-    ...                sample_weight=[1, 0])\n-    >>> m.result().numpy()\n-    1.2\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(\n-        optimizer='sgd',\n-        loss='mse',\n-        metrics=[tf.keras.metrics.CategoricalHinge()])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(self, name=\"categorical_hinge\", dtype=None):\n-        super().__init__(categorical_hinge, name, dtype=dtype)\n-\n-\n-@keras_export(\"keras.metrics.RootMeanSquaredError\")\n-class RootMeanSquaredError(base_metric.Mean):\n-    \"\"\"Computes root mean squared error metric between `y_true` and `y_pred`.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.RootMeanSquaredError()\n-    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n-    >>> m.result().numpy()\n-    0.5\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n-    ...                sample_weight=[1, 0])\n-    >>> m.result().numpy()\n-    0.70710677\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(\n-        optimizer='sgd',\n-        loss='mse',\n-        metrics=[tf.keras.metrics.RootMeanSquaredError()])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(self, name=\"root_mean_squared_error\", dtype=None):\n-        super().__init__(name, dtype=dtype)\n-\n-    def update_state(self, y_true, y_pred, sample_weight=None):\n-        \"\"\"Accumulates root mean squared error statistics.\n-\n-        Args:\n-          y_true: The ground truth values.\n-          y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can\n-            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n-            and must be broadcastable to `y_true`.\n-\n-        Returns:\n-          Update op.\n-        \"\"\"\n-        y_true = tf.cast(y_true, self._dtype)\n-        y_pred = tf.cast(y_pred, self._dtype)\n-        y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(\n-            y_pred, y_true\n-        )\n-        error_sq = tf.math.squared_difference(y_pred, y_true)\n-        return super().update_state(error_sq, sample_weight=sample_weight)\n-\n-    def result(self):\n-        return tf.sqrt(tf.math.divide_no_nan(self.total, self.count))\n-\n-\n-@keras_export(\"keras.metrics.LogCoshError\")\n-class LogCoshError(base_metric.MeanMetricWrapper):\n-    \"\"\"Computes the logarithm of the hyperbolic cosine of the prediction error.\n-\n-    `logcosh = log((exp(x) + exp(-x))/2)`, where x is the error (y_pred -\n-    y_true)\n-\n-    Args:\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.LogCoshError()\n-    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n-    >>> m.result().numpy()\n-    0.10844523\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n-    ...                sample_weight=[1, 0])\n-    >>> m.result().numpy()\n-    0.21689045\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(optimizer='sgd',\n-                  loss='mse',\n-                  metrics=[tf.keras.metrics.LogCoshError()])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(self, name=\"logcosh\", dtype=None):\n-        super().__init__(logcosh, name, dtype=dtype)\n-\n-\n-@keras_export(\"keras.metrics.Poisson\")\n-class Poisson(base_metric.MeanMetricWrapper):\n-    \"\"\"Computes the Poisson metric between `y_true` and `y_pred`.\n-\n-    `metric = y_pred - y_true * log(y_pred)`\n-\n-    Args:\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.Poisson()\n-    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n-    >>> m.result().numpy()\n-    0.49999997\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n-    ...                sample_weight=[1, 0])\n-    >>> m.result().numpy()\n-    0.99999994\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(optimizer='sgd',\n-                  loss='mse',\n-                  metrics=[tf.keras.metrics.Poisson()])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(self, name=\"poisson\", dtype=None):\n-        super().__init__(poisson, name, dtype=dtype)\n-\n-\n-@keras_export(\"keras.metrics.KLDivergence\")\n-class KLDivergence(base_metric.MeanMetricWrapper):\n-    \"\"\"Computes Kullback-Leibler divergence metric between `y_true` and\n-    `y_pred`.\n-\n-    `metric = y_true * log(y_true / y_pred)`\n-\n-    Args:\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.KLDivergence()\n-    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])\n-    >>> m.result().numpy()\n-    0.45814306\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],\n-    ...                sample_weight=[1, 0])\n-    >>> m.result().numpy()\n-    0.9162892\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(optimizer='sgd',\n-                  loss='mse',\n-                  metrics=[tf.keras.metrics.KLDivergence()])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(self, name=\"kullback_leibler_divergence\", dtype=None):\n-        super().__init__(kullback_leibler_divergence, name, dtype=dtype)\n-\n-\n-class _IoUBase(base_metric.Metric):\n-    \"\"\"Computes the confusion matrix for Intersection-Over-Union metrics.\n-\n-    Intersection-Over-Union is a common evaluation metric for semantic image\n-    segmentation.\n-\n-    For an individual class, the IoU metric is defined as follows:\n-\n-    ```\n-    iou = true_positives / (true_positives + false_positives + false_negatives)\n-    ```\n-\n-    From IoUs of individual classes, the MeanIoU can be computed as the mean of\n-    the individual IoUs.\n-\n-    To compute IoUs, the predictions are accumulated in a confusion matrix,\n-    weighted by `sample_weight` and the metric is then calculated from it.\n-\n-    If `sample_weight` is `None`, weights default to 1.\n-    Use `sample_weight` of 0 to mask values.\n-\n-    Args:\n-      num_classes: The possible number of labels the prediction task can have.\n-        This value must be provided, since a confusion matrix of size\n-        `(num_classes, num_classes)` will be allocated.\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-      ignore_class: Optional integer. The ID of a class to be ignored during\n-        metric computation. This is useful, for example, in segmentation\n-        problems featuring a \"void\" class (commonly -1 or 255) in segmentation\n-        maps. By default (`ignore_class=None`), all classes are considered.\n-      sparse_y_true: Whether labels are encoded using integers or\n-        dense floating point vectors. If `False`, the `tf.argmax` function\n-        will be used to determine each sample's most likely associated label.\n-      sparse_y_pred: Whether predictions are encoded using integers or\n-        dense floating point vectors. If `False`, the `tf.argmax` function\n-        will be used to determine each sample's most likely associated label.\n-      axis: (Optional) Defaults to -1. The dimension containing the logits.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        num_classes: int,\n-        name: Optional[str] = None,\n-        dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n-        ignore_class: Optional[int] = None,\n-        sparse_y_true: bool = True,\n-        sparse_y_pred: bool = True,\n-        axis: int = -1,\n-    ):\n-        super().__init__(name=name, dtype=dtype)\n-        self.num_classes = num_classes\n-        self.ignore_class = ignore_class\n-        self.sparse_y_true = sparse_y_true\n-        self.sparse_y_pred = sparse_y_pred\n-        self.axis = axis\n-\n-        # Variable to accumulate the predictions in the confusion matrix.\n-        self.total_cm = self.add_weight(\n-            \"total_confusion_matrix\",\n-            shape=(num_classes, num_classes),\n-            initializer=\"zeros\",\n-        )\n-\n-    def update_state(self, y_true, y_pred, sample_weight=None):\n-        \"\"\"Accumulates the confusion matrix statistics.\n-\n-        Args:\n-          y_true: The ground truth values.\n-          y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can\n-            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n-            and must be broadcastable to `y_true`.\n-\n-        Returns:\n-          Update op.\n-        \"\"\"\n-\n-        if not self.sparse_y_true:\n-            y_true = tf.argmax(y_true, axis=self.axis)\n-        if not self.sparse_y_pred:\n-            y_pred = tf.argmax(y_pred, axis=self.axis)\n-\n-        y_true = tf.cast(y_true, self._dtype)\n-        y_pred = tf.cast(y_pred, self._dtype)\n-\n-        # Flatten the input if its rank > 1.\n-        if y_pred.shape.ndims > 1:\n-            y_pred = tf.reshape(y_pred, [-1])\n-\n-        if y_true.shape.ndims > 1:\n-            y_true = tf.reshape(y_true, [-1])\n-\n-        if sample_weight is not None:\n-            sample_weight = tf.cast(sample_weight, self._dtype)\n-            if sample_weight.shape.ndims > 1:\n-                sample_weight = tf.reshape(sample_weight, [-1])\n-\n-        if self.ignore_class is not None:\n-            ignore_class = tf.cast(self.ignore_class, y_true.dtype)\n-            valid_mask = tf.not_equal(y_true, ignore_class)\n-            y_true = y_true[valid_mask]\n-            y_pred = y_pred[valid_mask]\n-            if sample_weight is not None:\n-                sample_weight = sample_weight[valid_mask]\n-\n-        # Accumulate the prediction to current confusion matrix.\n-        current_cm = tf.math.confusion_matrix(\n-            y_true,\n-            y_pred,\n-            self.num_classes,\n-            weights=sample_weight,\n-            dtype=self._dtype,\n-        )\n-        return self.total_cm.assign_add(current_cm)\n-\n-    def reset_state(self):\n-        backend.set_value(\n-            self.total_cm, np.zeros((self.num_classes, self.num_classes))\n-        )\n-\n-\n-@keras_export(\"keras.metrics.IoU\")\n-class IoU(_IoUBase):\n-    \"\"\"Computes the Intersection-Over-Union metric for specific target classes.\n-\n-    General definition and computation:\n-\n-    Intersection-Over-Union is a common evaluation metric for semantic image\n-    segmentation.\n-\n-    For an individual class, the IoU metric is defined as follows:\n-\n-    ```\n-    iou = true_positives / (true_positives + false_positives + false_negatives)\n-    ```\n-\n-    To compute IoUs, the predictions are accumulated in a confusion matrix,\n-    weighted by `sample_weight` and the metric is then calculated from it.\n-\n-    If `sample_weight` is `None`, weights default to 1.\n-    Use `sample_weight` of 0 to mask values.\n-\n-    Note, this class first computes IoUs for all individual classes, then\n-    returns the mean of IoUs for the classes that are specified by\n-    `target_class_ids`. If `target_class_ids` has only one id value, the IoU of\n-    that specific class is returned.\n-\n-    Args:\n-      num_classes: The possible number of labels the prediction task can have.\n-        A confusion matrix of dimension = [num_classes, num_classes] will be\n-        allocated to accumulate predictions from which the metric is calculated.\n-      target_class_ids: A tuple or list of target class ids for which the metric\n-        is returned. To compute IoU for a specific class, a list (or tuple) of a\n-        single id value should be provided.\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-      ignore_class: Optional integer. The ID of a class to be ignored during\n-        metric computation. This is useful, for example, in segmentation\n-        problems featuring a \"void\" class (commonly -1 or 255) in segmentation\n-        maps. By default (`ignore_class=None`), all classes are considered.\n-      sparse_y_true: Whether labels are encoded using integers or\n-        dense floating point vectors. If `False`, the `tf.argmax` function\n-        will be used to determine each sample's most likely associated label.\n-      sparse_y_pred: Whether predictions are encoded using integers or\n-        dense floating point vectors. If `False`, the `tf.argmax` function\n-        will be used to determine each sample's most likely associated label.\n-      axis: (Optional) Defaults to -1. The dimension containing the logits.\n-\n-    Standalone usage:\n-\n-    >>> # cm = [[1, 1],\n-    >>> #        [1, 1]]\n-    >>> # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]\n-    >>> # iou = true_positives / (sum_row + sum_col - true_positives))\n-    >>> # iou = [0.33, 0.33]\n-    >>> m = tf.keras.metrics.IoU(num_classes=2, target_class_ids=[0])\n-    >>> m.update_state([0, 0, 1, 1], [0, 1, 0, 1])\n-    >>> m.result().numpy()\n-    0.33333334\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([0, 0, 1, 1], [0, 1, 0, 1],\n-    ...                sample_weight=[0.3, 0.3, 0.3, 0.1])\n-    >>> # cm = [[0.3, 0.3],\n-    >>> #        [0.3, 0.1]]\n-    >>> # sum_row = [0.6, 0.4], sum_col = [0.6, 0.4],\n-    >>> # true_positives = [0.3, 0.1]\n-    >>> # iou = [0.33, 0.14]\n-    >>> m.result().numpy()\n-    0.33333334\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(\n-      optimizer='sgd',\n-      loss='mse',\n-      metrics=[tf.keras.metrics.IoU(num_classes=2, target_class_ids=[0])])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(\n-        self,\n-        num_classes: int,\n-        target_class_ids: Union[List[int], Tuple[int, ...]],\n-        name: Optional[str] = None,\n-        dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n-        ignore_class: Optional[int] = None,\n-        sparse_y_true: bool = True,\n-        sparse_y_pred: bool = True,\n-        axis: int = -1,\n-    ):\n-        super().__init__(\n-            name=name,\n-            num_classes=num_classes,\n-            ignore_class=ignore_class,\n-            sparse_y_true=sparse_y_true,\n-            sparse_y_pred=sparse_y_pred,\n-            axis=axis,\n-            dtype=dtype,\n-        )\n-        if max(target_class_ids) >= num_classes:\n-            raise ValueError(\n-                f\"Target class id {max(target_class_ids)} \"\n-                \"is out of range, which is \"\n-                f\"[{0}, {num_classes}).\"\n-            )\n-        self.target_class_ids = list(target_class_ids)\n-\n-    def result(self):\n-        \"\"\"Compute the intersection-over-union via the confusion matrix.\"\"\"\n-        sum_over_row = tf.cast(\n-            tf.reduce_sum(self.total_cm, axis=0), dtype=self._dtype\n-        )\n-        sum_over_col = tf.cast(\n-            tf.reduce_sum(self.total_cm, axis=1), dtype=self._dtype\n-        )\n-        true_positives = tf.cast(\n-            tf.linalg.tensor_diag_part(self.total_cm), dtype=self._dtype\n-        )\n-\n-        # sum_over_row + sum_over_col =\n-        #     2 * true_positives + false_positives + false_negatives.\n-        denominator = sum_over_row + sum_over_col - true_positives\n-\n-        # Only keep the target classes\n-        true_positives = tf.gather(true_positives, self.target_class_ids)\n-        denominator = tf.gather(denominator, self.target_class_ids)\n-\n-        # If the denominator is 0, we need to ignore the class.\n-        num_valid_entries = tf.reduce_sum(\n-            tf.cast(tf.not_equal(denominator, 0), dtype=self._dtype)\n-        )\n-\n-        iou = tf.math.divide_no_nan(true_positives, denominator)\n-\n-        return tf.math.divide_no_nan(\n-            tf.reduce_sum(iou, name=\"mean_iou\"), num_valid_entries\n-        )\n-\n-    def get_config(self):\n-        config = {\n-            \"num_classes\": self.num_classes,\n-            \"target_class_ids\": self.target_class_ids,\n-            \"ignore_class\": self.ignore_class,\n-            \"sparse_y_true\": self.sparse_y_true,\n-            \"sparse_y_pred\": self.sparse_y_pred,\n-            \"axis\": self.axis,\n-        }\n-        base_config = super().get_config()\n-        return dict(list(base_config.items()) + list(config.items()))\n-\n-\n-@keras_export(\"keras.metrics.BinaryIoU\")\n-class BinaryIoU(IoU):\n-    \"\"\"Computes the Intersection-Over-Union metric for class 0 and/or 1.\n-\n-    General definition and computation:\n-\n-    Intersection-Over-Union is a common evaluation metric for semantic image\n-    segmentation.\n-\n-    For an individual class, the IoU metric is defined as follows:\n-\n-    ```\n-    iou = true_positives / (true_positives + false_positives + false_negatives)\n-    ```\n-\n-    To compute IoUs, the predictions are accumulated in a confusion matrix,\n-    weighted by `sample_weight` and the metric is then calculated from it.\n-\n-    If `sample_weight` is `None`, weights default to 1.\n-    Use `sample_weight` of 0 to mask values.\n-\n-    This class can be used to compute IoUs for a binary classification task\n-    where the predictions are provided as logits. First a `threshold` is applied\n-    to the predicted values such that those that are below the `threshold` are\n-    converted to class 0 and those that are above the `threshold` are converted\n-    to class 1.\n-\n-    IoUs for classes 0 and 1 are then computed, the mean of IoUs for the classes\n-    that are specified by `target_class_ids` is returned.\n-\n-    Note: with `threshold=0`, this metric has the same behavior as `IoU`.\n-\n-    Args:\n-      target_class_ids: A tuple or list of target class ids for which the metric\n-        is returned. Options are `[0]`, `[1]`, or `[0, 1]`. With `[0]` (or\n-        `[1]`), the IoU metric for class 0 (or class 1, respectively) is\n-        returned. With `[0, 1]`, the mean of IoUs for the two classes is\n-        returned.\n-      threshold: A threshold that applies to the prediction logits to convert\n-        them to either predicted class 0 if the logit is below `threshold` or\n-        predicted class 1 if the logit is above `threshold`.\n-      name: (Optional) string name of the metric instance.\n-      dtype: (Optional) data type of the metric result.\n-\n-    Standalone usage:\n-\n-    >>> m = tf.keras.metrics.BinaryIoU(target_class_ids=[0, 1], threshold=0.3)\n-    >>> m.update_state([0, 1, 0, 1], [0.1, 0.2, 0.4, 0.7])\n-    >>> m.result().numpy()\n-    0.33333334\n-\n-    >>> m.reset_state()\n-    >>> m.update_state([0, 1, 0, 1], [0.1, 0.2, 0.4, 0.7],\n-    ...                sample_weight=[0.2, 0.3, 0.4, 0.1])\n-    >>> # cm = [[0.2, 0.4],\n-    >>> #        [0.3, 0.1]]\n-    >>> # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5],\n-    >>> # true_positives = [0.2, 0.1]\n-    >>> # iou = [0.222, 0.125]\n-    >>> m.result().numpy()\n-    0.17361112\n-\n-    Usage with `compile()` API:\n-\n-    ```python\n-    model.compile(\n-      optimizer='sgd',\n-      loss='mse',\n-      metrics=[tf.keras.metrics.BinaryIoU(target_class_ids=[0], threshold=0.5)])\n-    ```\n-    \"\"\"\n-\n-    @dtensor_utils.inject_mesh\n-    def __init__(\n-        self,\n-        target_class_ids: Union[List[int], Tuple[int, ...]] = (0, 1),\n-        threshold=0.5,\n-        name=None,\n-        dtype=None,\n-    ):\n-\n-        super().__init__(\n-            num_classes=2,\n-            target_class_ids=target_class_ids,\n-            name=name,\n-            dtype=dtype,\n-        )\n-        self.threshold = threshold\n-\n-    def update_state(self, y_true, y_pred, sample_weight=None):\n-        \"\"\"Accumulates the confusion matrix statistics.\n-\n-        Before the confusion matrix is updated, the predicted values are\n-        thresholded to be:\n-          0 for values that are smaller than the `threshold`\n-          1 for values that are larger or equal to the `threshold`\n-\n-        Args:\n-          y_true: The ground truth values.\n-          y_pred: The predicted values.\n-          sample_weight: Optional weighting of each example. Defaults to 1. Can\n-            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n-            and must be broadcastable to `y_true`.\n@@ -1,2563 +0,0 @@\n-# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"Tests for Keras metrics.\"\"\"\n-\n-import json\n-import math\n-\n-import numpy as np\n-import tensorflow.compat.v2 as tf\n-\n-from keras import Model\n-from keras import backend\n-from keras import layers\n-from keras import metrics\n-from keras.testing_infra import test_combinations\n-from keras.testing_infra import test_utils\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class KerasAccuracyTest(tf.test.TestCase):\n-    def test_accuracy(self):\n-        acc_obj = metrics.Accuracy(name=\"my_acc\")\n-\n-        # check config\n-        self.assertEqual(acc_obj.name, \"my_acc\")\n-        self.assertTrue(acc_obj.stateful)\n-        self.assertEqual(len(acc_obj.variables), 2)\n-        self.assertEqual(acc_obj.dtype, tf.float32)\n-        self.evaluate(tf.compat.v1.variables_initializer(acc_obj.variables))\n-\n-        # verify that correct value is returned\n-        update_op = acc_obj.update_state(\n-            [[1], [2], [3], [4]], [[1], [2], [3], [4]]\n-        )\n-        self.evaluate(update_op)\n-        result = self.evaluate(acc_obj.result())\n-        self.assertEqual(result, 1)  # 2/2\n-\n-        # Check save and restore config\n-        a2 = metrics.Accuracy.from_config(acc_obj.get_config())\n-        self.assertEqual(a2.name, \"my_acc\")\n-        self.assertTrue(a2.stateful)\n-        self.assertEqual(len(a2.variables), 2)\n-        self.assertEqual(a2.dtype, tf.float32)\n-\n-        # check with sample_weight\n-        result_t = acc_obj([[2], [1]], [[2], [0]], sample_weight=[[0.5], [0.2]])\n-        result = self.evaluate(result_t)\n-        self.assertAlmostEqual(result, 0.96, 2)  # 4.5/4.7\n-\n-    def test_accuracy_ragged(self):\n-        acc_obj = metrics.Accuracy(name=\"my_acc\")\n-        self.evaluate(tf.compat.v1.variables_initializer(acc_obj.variables))\n-\n-        # verify that correct value is returned\n-        rt1 = tf.ragged.constant([[1], [2], [3], [4]])\n-        rt2 = tf.ragged.constant([[1], [2], [3], [4]])\n-        update_op = acc_obj.update_state(rt1, rt2)\n-        self.evaluate(update_op)\n-        result = self.evaluate(acc_obj.result())\n-        self.assertEqual(result, 1)  # 2/2\n-\n-        # check with sample_weight\n-        rt1 = tf.ragged.constant([[2], [1]])\n-        rt2 = tf.ragged.constant([[2], [0]])\n-        sw_ragged = tf.ragged.constant([[0.5], [0.2]])\n-        result_t = acc_obj(rt1, rt2, sample_weight=sw_ragged)\n-        result = self.evaluate(result_t)\n-        self.assertAlmostEqual(result, 0.96, 2)  # 4.5/4.7\n-\n-    def test_binary_accuracy(self):\n-        acc_obj = metrics.BinaryAccuracy(name=\"my_acc\")\n-\n-        # check config\n-        self.assertEqual(acc_obj.name, \"my_acc\")\n-        self.assertTrue(acc_obj.stateful)\n-        self.assertEqual(len(acc_obj.variables), 2)\n-        self.assertEqual(acc_obj.dtype, tf.float32)\n-        self.evaluate(tf.compat.v1.variables_initializer(acc_obj.variables))\n-\n-        # verify that correct value is returned\n-        update_op = acc_obj.update_state([[1], [0]], [[1], [0]])\n-        self.evaluate(update_op)\n-        result = self.evaluate(acc_obj.result())\n-        self.assertEqual(result, 1)  # 2/2\n-\n-        # check y_pred squeeze\n-        update_op = acc_obj.update_state([[1], [1]], [[[1]], [[0]]])\n-        self.evaluate(update_op)\n-        result = self.evaluate(acc_obj.result())\n-        self.assertAlmostEqual(result, 0.75, 2)  # 3/4\n-\n-        # check y_true squeeze\n-        result_t = acc_obj([[[1]], [[1]]], [[1], [0]])\n-        result = self.evaluate(result_t)\n-        self.assertAlmostEqual(result, 0.67, 2)  # 4/6\n-\n-        # check with sample_weight\n-        result_t = acc_obj([[1], [1]], [[1], [0]], [[0.5], [0.2]])\n-        result = self.evaluate(result_t)\n-        self.assertAlmostEqual(result, 0.67, 2)  # 4.5/6.7\n-\n-    def test_binary_accuracy_ragged(self):\n-        acc_obj = metrics.BinaryAccuracy(name=\"my_acc\")\n-        self.evaluate(tf.compat.v1.variables_initializer(acc_obj.variables))\n-\n-        # verify that correct value is returned\n-        rt1 = tf.ragged.constant([[1], [0]])\n-        rt2 = tf.ragged.constant([[1], [0]])\n-        update_op = acc_obj.update_state(rt1, rt2)\n-        self.evaluate(update_op)\n-        result = self.evaluate(acc_obj.result())\n-        self.assertEqual(result, 1)  # 2/2\n-\n-        # check y_true squeeze only supported for dense tensors and is\n-        # not supported by ragged tensor (different ranks). --> error\n-        rt1 = tf.ragged.constant([[[1], [1]]])\n-        rt2 = tf.ragged.constant([[1], [0]])\n-        with self.assertRaises(ValueError):\n-            result_t = acc_obj(rt1, rt2)\n-            result = self.evaluate(result_t)\n-\n-    def test_binary_accuracy_threshold(self):\n-        acc_obj = metrics.BinaryAccuracy(threshold=0.7)\n-        self.evaluate(tf.compat.v1.variables_initializer(acc_obj.variables))\n-        result_t = acc_obj([[1], [1], [0], [0]], [[0.9], [0.6], [0.4], [0.8]])\n-        result = self.evaluate(result_t)\n-        self.assertAlmostEqual(result, 0.5, 2)\n-\n-    def test_binary_accuracy_threshold_ragged(self):\n-        acc_obj = metrics.BinaryAccuracy(threshold=0.7)\n-        self.evaluate(tf.compat.v1.variables_initializer(acc_obj.variables))\n-        rt1 = tf.ragged.constant([[1], [1], [0], [0]])\n-        rt2 = tf.ragged.constant([[0.9], [0.6], [0.4], [0.8]])\n-        result_t = acc_obj(rt1, rt2)\n-        result = self.evaluate(result_t)\n-        self.assertAlmostEqual(result, 0.5, 2)\n-\n-    def test_categorical_accuracy(self):\n-        acc_obj = metrics.CategoricalAccuracy(name=\"my_acc\")\n-\n-        # check config\n-        self.assertEqual(acc_obj.name, \"my_acc\")\n-        self.assertTrue(acc_obj.stateful)\n-        self.assertEqual(len(acc_obj.variables), 2)\n-        self.assertEqual(acc_obj.dtype, tf.float32)\n-        self.evaluate(tf.compat.v1.variables_initializer(acc_obj.variables))\n-\n-        # verify that correct value is returned\n-        update_op = acc_obj.update_state(\n-            [[0, 0, 1], [0, 1, 0]], [[0.1, 0.1, 0.8], [0.05, 0.95, 0]]\n-        )\n-        self.evaluate(update_op)\n-        result = self.evaluate(acc_obj.result())\n-        self.assertEqual(result, 1)  # 2/2\n-\n-        # check with sample_weight\n-        result_t = acc_obj(\n-            [[0, 0, 1], [0, 1, 0]],\n-            [[0.1, 0.1, 0.8], [0.05, 0, 0.95]],\n-            [[0.5], [0.2]],\n-        )\n-        result = self.evaluate(result_t)\n-        self.assertAlmostEqual(result, 0.93, 2)  # 2.5/2.7\n-\n-    def test_categorical_accuracy_ragged(self):\n-        acc_obj = metrics.CategoricalAccuracy(name=\"my_acc\")\n-        self.evaluate(tf.compat.v1.variables_initializer(acc_obj.variables))\n-\n-        # verify that correct value is returned\n-        rt1 = tf.ragged.constant([[0, 0, 1], [0, 1, 0]])\n-        rt2 = tf.ragged.constant([[0.1, 0.1, 0.8], [0.05, 0.95, 0]])\n-        update_op = acc_obj.update_state(rt1, rt2)\n-        self.evaluate(update_op)\n-        result = self.evaluate(acc_obj.result())\n-        self.assertEqual(result, 1)  # 2/2\n-\n-        # check with sample_weight\n-        rt1 = tf.ragged.constant([[0, 0, 1], [0, 1, 0]])\n-        rt2 = tf.ragged.constant([[0.1, 0.1, 0.8], [0.05, 0, 0.95]])\n-        sample_weight = tf.ragged.constant([[0.5], [0.2]])\n-        with self.assertRaises(tf.errors.InvalidArgumentError):\n-            result_t = acc_obj(rt1, rt2, sample_weight)\n-            result = self.evaluate(result_t)\n-\n-    def test_sparse_categorical_accuracy(self):\n-        acc_obj = metrics.SparseCategoricalAccuracy(name=\"my_acc\")\n-\n-        # check config\n-        self.assertEqual(acc_obj.name, \"my_acc\")\n-        self.assertTrue(acc_obj.stateful)\n-        self.assertEqual(len(acc_obj.variables), 2)\n-        self.assertEqual(acc_obj.dtype, tf.float32)\n-        self.evaluate(tf.compat.v1.variables_initializer(acc_obj.variables))\n-\n-        # verify that correct value is returned\n-        update_op = acc_obj.update_state(\n-            [[2], [1]], [[0.1, 0.1, 0.8], [0.05, 0.95, 0]]\n-        )\n-        self.evaluate(update_op)\n-        result = self.evaluate(acc_obj.result())\n-        self.assertEqual(result, 1)  # 2/2\n-\n-        # check with sample_weight\n-        result_t = acc_obj(\n-            [[2], [1]], [[0.1, 0.1, 0.8], [0.05, 0, 0.95]], [[0.5], [0.2]]\n-        )\n-        result = self.evaluate(result_t)\n-        self.assertAlmostEqual(result, 0.93, 2)  # 2.5/2.7\n-\n-    def test_sparse_categorical_accuracy_ragged(self):\n-        acc_obj = metrics.SparseCategoricalAccuracy(name=\"my_acc\")\n-\n-        # verify that correct value is returned\n-        rt1 = tf.ragged.constant([[2], [1]])\n-        rt2 = tf.ragged.constant([[0.1, 0.1, 0.8], [0.05, 0.95, 0]])\n-\n-        with self.assertRaises(tf.errors.InvalidArgumentError):\n-            # sparse_categorical_accuracy is not supported for composite/ragged\n-            # tensors.\n-            update_op = acc_obj.update_state(rt1, rt2)\n-            self.evaluate(update_op)\n-\n-    def test_sparse_categorical_accuracy_mismatched_dims(self):\n-        acc_obj = metrics.SparseCategoricalAccuracy(name=\"my_acc\")\n-\n-        # check config\n-        self.assertEqual(acc_obj.name, \"my_acc\")\n-        self.assertTrue(acc_obj.stateful)\n-        self.assertEqual(len(acc_obj.variables), 2)\n-        self.assertEqual(acc_obj.dtype, tf.float32)\n-        self.evaluate(tf.compat.v1.variables_initializer(acc_obj.variables))\n-\n-        # verify that correct value is returned\n-        update_op = acc_obj.update_state(\n-            [2, 1], [[0.1, 0.1, 0.8], [0.05, 0.95, 0]]\n-        )\n-        self.evaluate(update_op)\n-        result = self.evaluate(acc_obj.result())\n-        self.assertEqual(result, 1)  # 2/2\n-\n-        # check with sample_weight\n-        result_t = acc_obj(\n-            [2, 1], [[0.1, 0.1, 0.8], [0.05, 0, 0.95]], [[0.5], [0.2]]\n-        )\n-        result = self.evaluate(result_t)\n-        self.assertAlmostEqual(result, 0.93, 2)  # 2.5/2.7\n-\n-    def test_sparse_categorical_accuracy_mismatched_dims_dynamic(self):\n-        with tf.compat.v1.get_default_graph().as_default(), self.cached_session() as sess:  # noqa: E501\n-            acc_obj = metrics.SparseCategoricalAccuracy(name=\"my_acc\")\n-            self.evaluate(tf.compat.v1.variables_initializer(acc_obj.variables))\n-\n-            t = tf.compat.v1.placeholder(tf.float32)\n-            p = tf.compat.v1.placeholder(tf.float32)\n-            w = tf.compat.v1.placeholder(tf.float32)\n-\n-            result_t = acc_obj(t, p, w)\n-            result = sess.run(\n-                result_t,\n-                feed_dict=(\n-                    {\n-                        t: [2, 1],\n-                        p: [[0.1, 0.1, 0.8], [0.05, 0, 0.95]],\n-                        w: [[0.5], [0.2]],\n-                    }\n-                ),\n-            )\n-            self.assertAlmostEqual(result, 0.71, 2)  # 2.5/2.7\n-\n-    def test_get_acc(self):\n-        acc_fn = metrics.get(\"acc\")\n-        self.assertEqual(acc_fn, metrics.accuracy)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class CosineSimilarityTest(tf.test.TestCase):\n-    def l2_norm(self, x, axis):\n-        epsilon = 1e-12\n-        square_sum = np.sum(np.square(x), axis=axis, keepdims=True)\n-        x_inv_norm = 1 / np.sqrt(np.maximum(square_sum, epsilon))\n-        return np.multiply(x, x_inv_norm)\n-\n-    def setup(self, axis=1):\n-        self.np_y_true = np.asarray([[1, 9, 2], [-5, -2, 6]], dtype=np.float32)\n-        self.np_y_pred = np.asarray([[4, 8, 12], [8, 1, 3]], dtype=np.float32)\n-\n-        y_true = self.l2_norm(self.np_y_true, axis)\n-        y_pred = self.l2_norm(self.np_y_pred, axis)\n-        self.expected_loss = np.sum(np.multiply(y_true, y_pred), axis=(axis,))\n-\n-        self.y_true = tf.constant(self.np_y_true)\n-        self.y_pred = tf.constant(self.np_y_pred)\n-\n-    def test_config(self):\n-        cosine_obj = metrics.CosineSimilarity(\n-            axis=2, name=\"my_cos\", dtype=tf.int32\n-        )\n-        self.assertEqual(cosine_obj.name, \"my_cos\")\n-        self.assertEqual(cosine_obj._dtype, tf.int32)\n-\n-        # Check save and restore config\n-        cosine_obj2 = metrics.CosineSimilarity.from_config(\n-            cosine_obj.get_config()\n-        )\n-        self.assertEqual(cosine_obj2.name, \"my_cos\")\n-        self.assertEqual(cosine_obj2._dtype, tf.int32)\n-\n-    def test_unweighted(self):\n-        self.setup()\n-        cosine_obj = metrics.CosineSimilarity()\n-        self.evaluate(tf.compat.v1.variables_initializer(cosine_obj.variables))\n-        loss = cosine_obj(self.y_true, self.y_pred)\n-        expected_loss = np.mean(self.expected_loss)\n-        self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)\n-\n-    def test_weighted(self):\n-        self.setup()\n-        cosine_obj = metrics.CosineSimilarity()\n-        self.evaluate(tf.compat.v1.variables_initializer(cosine_obj.variables))\n-        sample_weight = np.asarray([1.2, 3.4])\n-        loss = cosine_obj(\n-            self.y_true, self.y_pred, sample_weight=tf.constant(sample_weight)\n-        )\n-        expected_loss = np.sum(self.expected_loss * sample_weight) / np.sum(\n-            sample_weight\n-        )\n-        self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)\n-\n-    def test_axis(self):\n-        self.setup(axis=1)\n-        cosine_obj = metrics.CosineSimilarity(axis=1)\n-        self.evaluate(tf.compat.v1.variables_initializer(cosine_obj.variables))\n-        loss = cosine_obj(self.y_true, self.y_pred)\n-        expected_loss = np.mean(self.expected_loss)\n-        self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class MeanAbsoluteErrorTest(tf.test.TestCase):\n-    def test_config(self):\n-        mae_obj = metrics.MeanAbsoluteError(name=\"my_mae\", dtype=tf.int32)\n-        self.assertEqual(mae_obj.name, \"my_mae\")\n-        self.assertEqual(mae_obj._dtype, tf.int32)\n-\n-        # Check save and restore config\n-        mae_obj2 = metrics.MeanAbsoluteError.from_config(mae_obj.get_config())\n-        self.assertEqual(mae_obj2.name, \"my_mae\")\n-        self.assertEqual(mae_obj2._dtype, tf.int32)\n-\n-    def test_unweighted(self):\n-        mae_obj = metrics.MeanAbsoluteError()\n-        self.evaluate(tf.compat.v1.variables_initializer(mae_obj.variables))\n-        y_true = tf.constant(\n-            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n-        )\n-        y_pred = tf.constant(\n-            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n-        )\n-\n-        update_op = mae_obj.update_state(y_true, y_pred)\n-        self.evaluate(update_op)\n-        result = mae_obj.result()\n-        self.assertAllClose(0.5, result, atol=1e-5)\n-\n-    def test_weighted(self):\n-        mae_obj = metrics.MeanAbsoluteError()\n-        self.evaluate(tf.compat.v1.variables_initializer(mae_obj.variables))\n-        y_true = tf.constant(\n-            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n-        )\n-        y_pred = tf.constant(\n-            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n-        )\n-        sample_weight = tf.constant((1.0, 1.5, 2.0, 2.5))\n-        result = mae_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAllClose(0.54285, self.evaluate(result), atol=1e-5)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class MeanAbsolutePercentageErrorTest(tf.test.TestCase):\n-    def test_config(self):\n-        mape_obj = metrics.MeanAbsolutePercentageError(\n-            name=\"my_mape\", dtype=tf.int32\n-        )\n-        self.assertEqual(mape_obj.name, \"my_mape\")\n-        self.assertEqual(mape_obj._dtype, tf.int32)\n-\n-        # Check save and restore config\n-        mape_obj2 = metrics.MeanAbsolutePercentageError.from_config(\n-            mape_obj.get_config()\n-        )\n-        self.assertEqual(mape_obj2.name, \"my_mape\")\n-        self.assertEqual(mape_obj2._dtype, tf.int32)\n-\n-    def test_unweighted(self):\n-        mape_obj = metrics.MeanAbsolutePercentageError()\n-        self.evaluate(tf.compat.v1.variables_initializer(mape_obj.variables))\n-        y_true = tf.constant(\n-            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n-        )\n-        y_pred = tf.constant(\n-            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n-        )\n-\n-        update_op = mape_obj.update_state(y_true, y_pred)\n-        self.evaluate(update_op)\n-        result = mape_obj.result()\n-        self.assertAllClose(35e7, result, atol=1e-5)\n-\n-    def test_weighted(self):\n-        mape_obj = metrics.MeanAbsolutePercentageError()\n-        self.evaluate(tf.compat.v1.variables_initializer(mape_obj.variables))\n-        y_true = tf.constant(\n-            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n-        )\n-        y_pred = tf.constant(\n-            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n-        )\n-        sample_weight = tf.constant((1.0, 1.5, 2.0, 2.5))\n-        result = mape_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAllClose(40e7, self.evaluate(result), atol=1e-5)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class MeanSquaredErrorTest(tf.test.TestCase):\n-    def test_config(self):\n-        mse_obj = metrics.MeanSquaredError(name=\"my_mse\", dtype=tf.int32)\n-        self.assertEqual(mse_obj.name, \"my_mse\")\n-        self.assertEqual(mse_obj._dtype, tf.int32)\n-\n-        # Check save and restore config\n-        mse_obj2 = metrics.MeanSquaredError.from_config(mse_obj.get_config())\n-        self.assertEqual(mse_obj2.name, \"my_mse\")\n-        self.assertEqual(mse_obj2._dtype, tf.int32)\n-\n-    def test_unweighted(self):\n-        mse_obj = metrics.MeanSquaredError()\n-        self.evaluate(tf.compat.v1.variables_initializer(mse_obj.variables))\n-        y_true = tf.constant(\n-            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n-        )\n-        y_pred = tf.constant(\n-            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n-        )\n-\n-        update_op = mse_obj.update_state(y_true, y_pred)\n-        self.evaluate(update_op)\n-        result = mse_obj.result()\n-        self.assertAllClose(0.5, result, atol=1e-5)\n-\n-    def test_weighted(self):\n-        mse_obj = metrics.MeanSquaredError()\n-        self.evaluate(tf.compat.v1.variables_initializer(mse_obj.variables))\n-        y_true = tf.constant(\n-            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n-        )\n-        y_pred = tf.constant(\n-            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n-        )\n-        sample_weight = tf.constant((1.0, 1.5, 2.0, 2.5))\n-        result = mse_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAllClose(0.54285, self.evaluate(result), atol=1e-5)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class MeanSquaredLogarithmicErrorTest(tf.test.TestCase):\n-    def test_config(self):\n-        msle_obj = metrics.MeanSquaredLogarithmicError(\n-            name=\"my_msle\", dtype=tf.int32\n-        )\n-        self.assertEqual(msle_obj.name, \"my_msle\")\n-        self.assertEqual(msle_obj._dtype, tf.int32)\n-\n-        # Check save and restore config\n-        msle_obj2 = metrics.MeanSquaredLogarithmicError.from_config(\n-            msle_obj.get_config()\n-        )\n-        self.assertEqual(msle_obj2.name, \"my_msle\")\n-        self.assertEqual(msle_obj2._dtype, tf.int32)\n-\n-    def test_unweighted(self):\n-        msle_obj = metrics.MeanSquaredLogarithmicError()\n-        self.evaluate(tf.compat.v1.variables_initializer(msle_obj.variables))\n-        y_true = tf.constant(\n-            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n-        )\n-        y_pred = tf.constant(\n-            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n-        )\n-\n-        update_op = msle_obj.update_state(y_true, y_pred)\n-        self.evaluate(update_op)\n-        result = msle_obj.result()\n-        self.assertAllClose(0.24022, result, atol=1e-5)\n-\n-    def test_weighted(self):\n-        msle_obj = metrics.MeanSquaredLogarithmicError()\n-        self.evaluate(tf.compat.v1.variables_initializer(msle_obj.variables))\n-        y_true = tf.constant(\n-            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n-        )\n-        y_pred = tf.constant(\n-            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n-        )\n-        sample_weight = tf.constant((1.0, 1.5, 2.0, 2.5))\n-        result = msle_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAllClose(0.26082, self.evaluate(result), atol=1e-5)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class HingeTest(tf.test.TestCase):\n-    def test_config(self):\n-        hinge_obj = metrics.Hinge(name=\"hinge\", dtype=tf.int32)\n-        self.assertEqual(hinge_obj.name, \"hinge\")\n-        self.assertEqual(hinge_obj._dtype, tf.int32)\n-\n-        # Check save and restore config\n-        hinge_obj2 = metrics.Hinge.from_config(hinge_obj.get_config())\n-        self.assertEqual(hinge_obj2.name, \"hinge\")\n-        self.assertEqual(hinge_obj2._dtype, tf.int32)\n-\n-    def test_unweighted(self):\n-        hinge_obj = metrics.Hinge()\n-        self.evaluate(tf.compat.v1.variables_initializer(hinge_obj.variables))\n-        y_true = tf.constant([[0, 1, 0, 1], [0, 0, 1, 1]])\n-        y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6], [-0.25, -1.0, 0.5, 0.6]])\n-\n-        # metric = max(0, 1-y_true * y_pred), where y_true is -1/1\n-\n-        # y_true = [[-1, 1, -1, 1], [-1, -1, 1, 1]]\n-        # y_true * y_pred = [[0.3, 0.2, 0.1, 1.6], [0.25, 1, 0.5, 0.6]]\n-        # 1 - y_true * y_pred = [[0.7, 0.8, 0.9, -0.6], [0.75, 0, 0.5, 0.4]]\n-        # metric = [(0.7 + 0.8 + 0.9 + 0) / 4, (0.75 + 0 + 0.5 + 0.4) / 4]\n-        #        = [0.6, 0.4125]\n-        # reduced metric = (0.6 + 0.4125) / 2\n-\n-        update_op = hinge_obj.update_state(y_true, y_pred)\n-        self.evaluate(update_op)\n-        result = hinge_obj.result()\n-        self.assertAllClose(0.506, result, atol=1e-3)\n-\n-    def test_weighted(self):\n-        hinge_obj = metrics.Hinge()\n-        self.evaluate(tf.compat.v1.variables_initializer(hinge_obj.variables))\n-        y_true = tf.constant([[-1, 1, -1, 1], [-1, -1, 1, 1]])\n-        y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6], [-0.25, -1.0, 0.5, 0.6]])\n-        sample_weight = tf.constant([1.5, 2.0])\n-\n-        # metric = max(0, 1-y_true * y_pred), where y_true is -1/1\n-\n-        # y_true * y_pred = [[0.3, 0.2, 0.1, 1.6], [0.25, 1, 0.5, 0.6]]\n-        # 1 - y_true * y_pred = [[0.7, 0.8, 0.9, -0.6], [0.75, 0, 0.5, 0.4]]\n-        # metric = [(0.7 + 0.8 + 0.9 + 0) / 4, (0.75 + 0 + 0.5 + 0.4) / 4]\n-        #        = [0.6, 0.4125]\n-        # weighted metric = [0.6 * 1.5, 0.4125 * 2]\n-        # reduced metric = (0.6 * 1.5 + 0.4125 * 2) / (1.5 + 2)\n-\n-        result = hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAllClose(0.493, self.evaluate(result), atol=1e-3)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class SquaredHingeTest(tf.test.TestCase):\n-    def test_config(self):\n-        sq_hinge_obj = metrics.SquaredHinge(name=\"sq_hinge\", dtype=tf.int32)\n-        self.assertEqual(sq_hinge_obj.name, \"sq_hinge\")\n-        self.assertEqual(sq_hinge_obj._dtype, tf.int32)\n-\n-        # Check save and restore config\n-        sq_hinge_obj2 = metrics.SquaredHinge.from_config(\n-            sq_hinge_obj.get_config()\n-        )\n-        self.assertEqual(sq_hinge_obj2.name, \"sq_hinge\")\n-        self.assertEqual(sq_hinge_obj2._dtype, tf.int32)\n-\n-    def test_unweighted(self):\n-        sq_hinge_obj = metrics.SquaredHinge()\n-        self.evaluate(\n-            tf.compat.v1.variables_initializer(sq_hinge_obj.variables)\n-        )\n-        y_true = tf.constant([[0, 1, 0, 1], [0, 0, 1, 1]])\n-        y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6], [-0.25, -1.0, 0.5, 0.6]])\n-\n-        # metric = max(0, 1-y_true * y_pred), where y_true is -1/1\n-\n-        # y_true = [[-1, 1, -1, 1], [-1, -1, 1, 1]]\n-        # y_true * y_pred = [[0.3, 0.2, 0.1, 1.6], [0.25, 1, 0.5, 0.6]]\n-        # 1 - y_true * y_pred = [[0.7, 0.8, 0.9, -0.6], [0.75, 0, 0.5, 0.4]]\n-        # max(0, 1 - y_true * y_pred) = [[0.7, 0.8, 0.9, 0], [0.75, 0, 0.5,\n-        # 0.4]]\n-        # squared(max(0, 1 - y_true * y_pred)) = [[0.49, 0.64, 0.81, 0],\n-        #                                         [0.5625, 0, 0.25, 0.16]]\n-        # metric = [(0.49 + 0.64 + 0.81 + 0) / 4, (0.5625 + 0 + 0.25 + 0.16) /\n-        # 4]\n-        #        = [0.485, 0.2431]\n-        # reduced metric = (0.485 + 0.2431) / 2\n-\n-        update_op = sq_hinge_obj.update_state(y_true, y_pred)\n-        self.evaluate(update_op)\n-        result = sq_hinge_obj.result()\n-        self.assertAllClose(0.364, result, atol=1e-3)\n-\n-    def test_weighted(self):\n-        sq_hinge_obj = metrics.SquaredHinge()\n-        self.evaluate(\n-            tf.compat.v1.variables_initializer(sq_hinge_obj.variables)\n-        )\n-        y_true = tf.constant([[-1, 1, -1, 1], [-1, -1, 1, 1]])\n-        y_pred = tf.constant([[-0.3, 0.2, -0.1, 1.6], [-0.25, -1.0, 0.5, 0.6]])\n-        sample_weight = tf.constant([1.5, 2.0])\n-\n-        # metric = max(0, 1-y_true * y_pred), where y_true is -1/1\n-\n-        # y_true * y_pred = [[0.3, 0.2, 0.1, 1.6], [0.25, 1, 0.5, 0.6]]\n-        # 1 - y_true * y_pred = [[0.7, 0.8, 0.9, -0.6], [0.75, 0, 0.5, 0.4]]\n-        # max(0, 1 - y_true * y_pred) = [[0.7, 0.8, 0.9, 0], [0.75, 0, 0.5,\n-        # 0.4]]\n-        # squared(max(0, 1 - y_true * y_pred)) = [[0.49, 0.64, 0.81, 0],\n-        #                                         [0.5625, 0, 0.25, 0.16]]\n-        # metric = [(0.49 + 0.64 + 0.81 + 0) / 4, (0.5625 + 0 + 0.25 + 0.16) /\n-        # 4]\n-        #        = [0.485, 0.2431]\n-        # weighted metric = [0.485 * 1.5, 0.2431 * 2]\n-        # reduced metric = (0.485 * 1.5 + 0.2431 * 2) / (1.5 + 2)\n-\n-        result = sq_hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAllClose(0.347, self.evaluate(result), atol=1e-3)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class CategoricalHingeTest(tf.test.TestCase):\n-    def test_config(self):\n-        cat_hinge_obj = metrics.CategoricalHinge(\n-            name=\"cat_hinge\", dtype=tf.int32\n-        )\n-        self.assertEqual(cat_hinge_obj.name, \"cat_hinge\")\n-        self.assertEqual(cat_hinge_obj._dtype, tf.int32)\n-\n-        # Check save and restore config\n-        cat_hinge_obj2 = metrics.CategoricalHinge.from_config(\n-            cat_hinge_obj.get_config()\n-        )\n-        self.assertEqual(cat_hinge_obj2.name, \"cat_hinge\")\n-        self.assertEqual(cat_hinge_obj2._dtype, tf.int32)\n-\n-    def test_unweighted(self):\n-        cat_hinge_obj = metrics.CategoricalHinge()\n-        self.evaluate(\n-            tf.compat.v1.variables_initializer(cat_hinge_obj.variables)\n-        )\n-        y_true = tf.constant(\n-            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n-        )\n-        y_pred = tf.constant(\n-            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n-        )\n-\n-        update_op = cat_hinge_obj.update_state(y_true, y_pred)\n-        self.evaluate(update_op)\n-        result = cat_hinge_obj.result()\n-        self.assertAllClose(0.5, result, atol=1e-5)\n-\n-    def test_weighted(self):\n-        cat_hinge_obj = metrics.CategoricalHinge()\n-        self.evaluate(\n-            tf.compat.v1.variables_initializer(cat_hinge_obj.variables)\n-        )\n-        y_true = tf.constant(\n-            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n-        )\n-        y_pred = tf.constant(\n-            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n-        )\n-        sample_weight = tf.constant((1.0, 1.5, 2.0, 2.5))\n-        result = cat_hinge_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAllClose(0.5, self.evaluate(result), atol=1e-5)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class RootMeanSquaredErrorTest(tf.test.TestCase):\n-    def test_config(self):\n-        rmse_obj = metrics.RootMeanSquaredError(name=\"rmse\", dtype=tf.int32)\n-        self.assertEqual(rmse_obj.name, \"rmse\")\n-        self.assertEqual(rmse_obj._dtype, tf.int32)\n-\n-        rmse_obj2 = metrics.RootMeanSquaredError.from_config(\n-            rmse_obj.get_config()\n-        )\n-        self.assertEqual(rmse_obj2.name, \"rmse\")\n-        self.assertEqual(rmse_obj2._dtype, tf.int32)\n-\n-    def test_unweighted(self):\n-        rmse_obj = metrics.RootMeanSquaredError()\n-        self.evaluate(tf.compat.v1.variables_initializer(rmse_obj.variables))\n-        y_true = tf.constant((2, 4, 6))\n-        y_pred = tf.constant((1, 3, 2))\n-\n-        update_op = rmse_obj.update_state(y_true, y_pred)\n-        self.evaluate(update_op)\n-        result = rmse_obj.result()\n-        # error = [-1, -1, -4], square(error) = [1, 1, 16], mean = 18/3 = 6\n-        self.assertAllClose(math.sqrt(6), result, atol=1e-3)\n-\n-    def test_weighted(self):\n-        rmse_obj = metrics.RootMeanSquaredError()\n-        self.evaluate(tf.compat.v1.variables_initializer(rmse_obj.variables))\n-        y_true = tf.constant((2, 4, 6, 8))\n-        y_pred = tf.constant((1, 3, 2, 3))\n-        sample_weight = tf.constant((0, 1, 0, 1))\n-        result = rmse_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAllClose(math.sqrt(13), self.evaluate(result), atol=1e-3)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class TopKCategoricalAccuracyTest(tf.test.TestCase):\n-    def test_config(self):\n-        a_obj = metrics.TopKCategoricalAccuracy(name=\"topkca\", dtype=tf.int32)\n-        self.assertEqual(a_obj.name, \"topkca\")\n-        self.assertEqual(a_obj._dtype, tf.int32)\n-\n-        a_obj2 = metrics.TopKCategoricalAccuracy.from_config(a_obj.get_config())\n-        self.assertEqual(a_obj2.name, \"topkca\")\n-        self.assertEqual(a_obj2._dtype, tf.int32)\n-\n-    def test_correctness(self):\n-        a_obj = metrics.TopKCategoricalAccuracy()\n-        self.evaluate(tf.compat.v1.variables_initializer(a_obj.variables))\n-        y_true = tf.constant([[0, 0, 1], [0, 1, 0]])\n-        y_pred = tf.constant([[0.1, 0.9, 0.8], [0.05, 0.95, 0]])\n-\n-        result = a_obj(y_true, y_pred)\n-        self.assertEqual(1, self.evaluate(result))  # both the samples match\n-\n-        # With `k` < 5.\n-        a_obj = metrics.TopKCategoricalAccuracy(k=1)\n-        self.evaluate(tf.compat.v1.variables_initializer(a_obj.variables))\n-        result = a_obj(y_true, y_pred)\n-        self.assertEqual(0.5, self.evaluate(result))  # only sample #2 matches\n-\n-        # With `k` > 5.\n-        y_true = tf.constant([[0, 0, 1, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0]])\n-        y_pred = tf.constant(\n-            [[0.5, 0.9, 0.1, 0.7, 0.6, 0.5, 0.4], [0.05, 0.95, 0, 0, 0, 0, 0]]\n-        )\n-        a_obj = metrics.TopKCategoricalAccuracy(k=6)\n-        self.evaluate(tf.compat.v1.variables_initializer(a_obj.variables))\n-        result = a_obj(y_true, y_pred)\n-        self.assertEqual(0.5, self.evaluate(result))  # only 1 sample matches.\n-\n-    def test_weighted(self):\n-        a_obj = metrics.TopKCategoricalAccuracy(k=2)\n-        self.evaluate(tf.compat.v1.variables_initializer(a_obj.variables))\n-        y_true = tf.constant([[0, 1, 0], [1, 0, 0], [0, 0, 1]])\n-        y_pred = tf.constant([[0, 0.9, 0.1], [0, 0.9, 0.1], [0, 0.9, 0.1]])\n-        sample_weight = tf.constant((1.0, 0.0, 1.0))\n-        result = a_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAllClose(1.0, self.evaluate(result), atol=1e-5)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class SparseTopKCategoricalAccuracyTest(tf.test.TestCase):\n-    def test_config(self):\n-        a_obj = metrics.SparseTopKCategoricalAccuracy(\n-            name=\"stopkca\", dtype=tf.int32\n-        )\n-        self.assertEqual(a_obj.name, \"stopkca\")\n-        self.assertEqual(a_obj._dtype, tf.int32)\n-\n-        a_obj2 = metrics.SparseTopKCategoricalAccuracy.from_config(\n-            a_obj.get_config()\n-        )\n-        self.assertEqual(a_obj2.name, \"stopkca\")\n-        self.assertEqual(a_obj2._dtype, tf.int32)\n-\n-    def test_correctness(self):\n-        a_obj = metrics.SparseTopKCategoricalAccuracy()\n-        self.evaluate(tf.compat.v1.variables_initializer(a_obj.variables))\n-        y_true = tf.constant([2, 1])\n-        y_pred = tf.constant([[0.1, 0.9, 0.8], [0.05, 0.95, 0]])\n-\n-        result = a_obj(y_true, y_pred)\n-        self.assertEqual(1, self.evaluate(result))  # both the samples match\n-\n-        # With `k` < 5.\n-        a_obj = metrics.SparseTopKCategoricalAccuracy(k=1)\n-        self.evaluate(tf.compat.v1.variables_initializer(a_obj.variables))\n-        result = a_obj(y_true, y_pred)\n-        self.assertEqual(0.5, self.evaluate(result))  # only sample #2 matches\n-\n-        # With `k` > 5.\n-        y_pred = tf.constant(\n-            [[0.5, 0.9, 0.1, 0.7, 0.6, 0.5, 0.4], [0.05, 0.95, 0, 0, 0, 0, 0]]\n-        )\n-        a_obj = metrics.SparseTopKCategoricalAccuracy(k=6)\n-        self.evaluate(tf.compat.v1.variables_initializer(a_obj.variables))\n-        result = a_obj(y_true, y_pred)\n-        self.assertEqual(0.5, self.evaluate(result))  # only 1 sample matches.\n-\n-    def test_weighted(self):\n-        a_obj = metrics.SparseTopKCategoricalAccuracy(k=2)\n-        self.evaluate(tf.compat.v1.variables_initializer(a_obj.variables))\n-        y_true = tf.constant([1, 0, 2])\n-        y_pred = tf.constant([[0, 0.9, 0.1], [0, 0.9, 0.1], [0, 0.9, 0.1]])\n-        sample_weight = tf.constant((1.0, 0.0, 1.0))\n-        result = a_obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAllClose(1.0, self.evaluate(result), atol=1e-5)\n-\n-    def test_sparse_top_k_categorical_accuracy_mismatched_dims_dynamic(self):\n-\n-        if not tf.compat.v1.executing_eagerly():\n-            # Test will fail in v1 graph mode since the metric is not a normal\n-            # layer.  It will aggregate the output by batch dim, which failed on\n-            # v1 code.\n-            self.skipTest(\"v2 eager mode only\")\n-\n-        class AccLayer(layers.Layer):\n-            def build(self, _):\n-                self.acc = metrics.SparseTopKCategoricalAccuracy(k=1)\n-\n-            def call(self, y_true, y_pred):\n-                return self.acc(y_true, y_pred)\n-\n-        label = layers.Input(shape=[1])\n-        predict = layers.Input(shape=[3])\n-        metric_result = AccLayer()(label, predict)\n-        model = Model([label, predict], metric_result)\n-\n-        result = model.predict(\n-            [\n-                tf.constant([[2], [1]]),\n-                tf.constant([[0.1, 0.1, 0.8], [0.05, 0, 0.95]]),\n-            ],\n-            steps=1,\n-        )\n-        self.assertAllClose(result, 0.5)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class LogCoshErrorTest(tf.test.TestCase):\n-    def setup(self):\n-        y_pred = np.asarray([1, 9, 2, -5, -2, 6]).reshape((2, 3))\n-        y_true = np.asarray([4, 8, 12, 8, 1, 3]).reshape((2, 3))\n-\n-        self.batch_size = 6\n-        error = y_pred - y_true\n-        self.expected_results = np.log((np.exp(error) + np.exp(-error)) / 2)\n-\n-        self.y_pred = tf.constant(y_pred, dtype=tf.float32)\n-        self.y_true = tf.constant(y_true)\n-\n-    def test_config(self):\n-        logcosh_obj = metrics.LogCoshError(name=\"logcosh\", dtype=tf.int32)\n-        self.assertEqual(logcosh_obj.name, \"logcosh\")\n-        self.assertEqual(logcosh_obj._dtype, tf.int32)\n-\n-    def test_unweighted(self):\n-        self.setup()\n-        logcosh_obj = metrics.LogCoshError()\n-        self.evaluate(tf.compat.v1.variables_initializer(logcosh_obj.variables))\n-\n-        update_op = logcosh_obj.update_state(self.y_true, self.y_pred)\n-        self.evaluate(update_op)\n-        result = logcosh_obj.result()\n-        expected_result = np.sum(self.expected_results) / self.batch_size\n-        self.assertAllClose(result, expected_result, atol=1e-3)\n-\n-    def test_weighted(self):\n-        self.setup()\n-        logcosh_obj = metrics.LogCoshError()\n-        self.evaluate(tf.compat.v1.variables_initializer(logcosh_obj.variables))\n-        sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))\n-        result = logcosh_obj(\n-            self.y_true, self.y_pred, sample_weight=sample_weight\n-        )\n-\n-        sample_weight = np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape(\n-            (2, 3)\n-        )\n-        expected_result = np.multiply(self.expected_results, sample_weight)\n-        expected_result = np.sum(expected_result) / np.sum(sample_weight)\n-        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class PoissonTest(tf.test.TestCase):\n-    def setup(self):\n-        y_pred = np.asarray([1, 9, 2, 5, 2, 6]).reshape((2, 3))\n-        y_true = np.asarray([4, 8, 12, 8, 1, 3]).reshape((2, 3))\n-\n-        self.batch_size = 6\n-        self.expected_results = y_pred - np.multiply(y_true, np.log(y_pred))\n-\n-        self.y_pred = tf.constant(y_pred, dtype=tf.float32)\n-        self.y_true = tf.constant(y_true)\n-\n-    def test_config(self):\n-        poisson_obj = metrics.Poisson(name=\"poisson\", dtype=tf.int32)\n-        self.assertEqual(poisson_obj.name, \"poisson\")\n-        self.assertEqual(poisson_obj._dtype, tf.int32)\n-\n-        poisson_obj2 = metrics.Poisson.from_config(poisson_obj.get_config())\n-        self.assertEqual(poisson_obj2.name, \"poisson\")\n-        self.assertEqual(poisson_obj2._dtype, tf.int32)\n-\n-    def test_unweighted(self):\n-        self.setup()\n-        poisson_obj = metrics.Poisson()\n-        self.evaluate(tf.compat.v1.variables_initializer(poisson_obj.variables))\n-\n-        update_op = poisson_obj.update_state(self.y_true, self.y_pred)\n-        self.evaluate(update_op)\n-        result = poisson_obj.result()\n-        expected_result = np.sum(self.expected_results) / self.batch_size\n-        self.assertAllClose(result, expected_result, atol=1e-3)\n-\n-    def test_weighted(self):\n-        self.setup()\n-        poisson_obj = metrics.Poisson()\n-        self.evaluate(tf.compat.v1.variables_initializer(poisson_obj.variables))\n-        sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))\n-\n-        result = poisson_obj(\n-            self.y_true, self.y_pred, sample_weight=sample_weight\n-        )\n-        sample_weight = np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape(\n-            (2, 3)\n-        )\n-        expected_result = np.multiply(self.expected_results, sample_weight)\n-        expected_result = np.sum(expected_result) / np.sum(sample_weight)\n-        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class KLDivergenceTest(tf.test.TestCase):\n-    def setup(self):\n-        y_pred = np.asarray([0.4, 0.9, 0.12, 0.36, 0.3, 0.4]).reshape((2, 3))\n-        y_true = np.asarray([0.5, 0.8, 0.12, 0.7, 0.43, 0.8]).reshape((2, 3))\n-\n-        self.batch_size = 2\n-        self.expected_results = np.multiply(y_true, np.log(y_true / y_pred))\n-\n-        self.y_pred = tf.constant(y_pred, dtype=tf.float32)\n-        self.y_true = tf.constant(y_true)\n-\n-    def test_config(self):\n-        k_obj = metrics.KLDivergence(name=\"kld\", dtype=tf.int32)\n-        self.assertEqual(k_obj.name, \"kld\")\n-        self.assertEqual(k_obj._dtype, tf.int32)\n-\n-        k_obj2 = metrics.KLDivergence.from_config(k_obj.get_config())\n-        self.assertEqual(k_obj2.name, \"kld\")\n-        self.assertEqual(k_obj2._dtype, tf.int32)\n-\n-    def test_unweighted(self):\n-        self.setup()\n-        k_obj = metrics.KLDivergence()\n-        self.evaluate(tf.compat.v1.variables_initializer(k_obj.variables))\n-\n-        update_op = k_obj.update_state(self.y_true, self.y_pred)\n-        self.evaluate(update_op)\n-        result = k_obj.result()\n-        expected_result = np.sum(self.expected_results) / self.batch_size\n-        self.assertAllClose(result, expected_result, atol=1e-3)\n-\n-    def test_weighted(self):\n-        self.setup()\n-        k_obj = metrics.KLDivergence()\n-        self.evaluate(tf.compat.v1.variables_initializer(k_obj.variables))\n-\n-        sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))\n-        result = k_obj(self.y_true, self.y_pred, sample_weight=sample_weight)\n-\n-        sample_weight = np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape(\n-            (2, 3)\n-        )\n-        expected_result = np.multiply(self.expected_results, sample_weight)\n-        expected_result = np.sum(expected_result) / (1.2 + 3.4)\n-        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class MeanRelativeErrorTest(tf.test.TestCase):\n-    def test_config(self):\n-        normalizer = tf.constant([1, 3], dtype=tf.float32)\n-        mre_obj = metrics.MeanRelativeError(normalizer=normalizer, name=\"mre\")\n-        self.assertEqual(mre_obj.name, \"mre\")\n-        self.assertArrayNear(self.evaluate(mre_obj.normalizer), [1, 3], 1e-1)\n-\n-        mre_obj2 = metrics.MeanRelativeError.from_config(mre_obj.get_config())\n-        self.assertEqual(mre_obj2.name, \"mre\")\n-        self.assertArrayNear(self.evaluate(mre_obj2.normalizer), [1, 3], 1e-1)\n-\n-    def test_unweighted(self):\n-        np_y_pred = np.asarray([2, 4, 6, 8], dtype=np.float32)\n-        np_y_true = np.asarray([1, 3, 2, 3], dtype=np.float32)\n-        expected_error = np.mean(\n-            np.divide(np.absolute(np_y_pred - np_y_true), np_y_true)\n-        )\n-\n-        y_pred = tf.constant(np_y_pred, shape=(1, 4), dtype=tf.float32)\n-        y_true = tf.constant(np_y_true, shape=(1, 4))\n-\n-        mre_obj = metrics.MeanRelativeError(normalizer=y_true)\n-        self.evaluate(tf.compat.v1.variables_initializer(mre_obj.variables))\n-\n-        result = mre_obj(y_true, y_pred)\n-        self.assertAllClose(self.evaluate(result), expected_error, atol=1e-3)\n-\n-    def test_weighted(self):\n-        np_y_pred = np.asarray([2, 4, 6, 8], dtype=np.float32)\n-        np_y_true = np.asarray([1, 3, 2, 3], dtype=np.float32)\n-        sample_weight = np.asarray([0.2, 0.3, 0.5, 0], dtype=np.float32)\n-        rel_errors = np.divide(np.absolute(np_y_pred - np_y_true), np_y_true)\n-        expected_error = np.sum(rel_errors * sample_weight)\n-\n-        y_pred = tf.constant(np_y_pred, dtype=tf.float32)\n-        y_true = tf.constant(np_y_true)\n-\n-        mre_obj = metrics.MeanRelativeError(normalizer=y_true)\n-        self.evaluate(tf.compat.v1.variables_initializer(mre_obj.variables))\n-\n-        result = mre_obj(\n-            y_true, y_pred, sample_weight=tf.constant(sample_weight)\n-        )\n-        self.assertAllClose(self.evaluate(result), expected_error, atol=1e-3)\n-\n-    def test_zero_normalizer(self):\n-        y_pred = tf.constant([2, 4], dtype=tf.float32)\n-        y_true = tf.constant([1, 3])\n-\n-        mre_obj = metrics.MeanRelativeError(normalizer=tf.zeros_like(y_true))\n-        self.evaluate(tf.compat.v1.variables_initializer(mre_obj.variables))\n-\n-        result = mre_obj(y_true, y_pred)\n-        self.assertEqual(self.evaluate(result), 0)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class IoUTest(tf.test.TestCase):\n-    def test_config(self):\n-        obj = metrics.IoU(\n-            num_classes=2, target_class_ids=[1, 0], name=\"iou_class_1_0\"\n-        )\n-        self.assertEqual(obj.name, \"iou_class_1_0\")\n-        self.assertEqual(obj.num_classes, 2)\n-        self.assertEqual(obj.target_class_ids, [1, 0])\n-\n-        obj2 = metrics.IoU.from_config(obj.get_config())\n-        self.assertEqual(obj2.name, \"iou_class_1_0\")\n-        self.assertEqual(obj2.num_classes, 2)\n-        self.assertEqual(obj2.target_class_ids, [1, 0])\n-\n-    def test_unweighted(self):\n-        y_pred = [0, 1, 0, 1]\n-        y_true = [0, 0, 1, 1]\n-\n-        obj = metrics.IoU(num_classes=2, target_class_ids=[0, 1])\n-        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n-\n-        result = obj(y_true, y_pred)\n-\n-        # cm = [[1, 1],\n-        #       [1, 1]]\n-        # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = (1 / (2 + 2 - 1) + 1 / (2 + 2 - 1)) / 2\n-        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n-\n-    def test_weighted(self):\n-        y_pred = tf.constant([0, 1, 0, 1], dtype=tf.float32)\n-        y_true = tf.constant([0, 0, 1, 1])\n-        sample_weight = tf.constant([0.2, 0.3, 0.4, 0.1])\n-\n-        obj = metrics.IoU(num_classes=2, target_class_ids=[1, 0])\n-        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n-\n-        result = obj(y_true, y_pred, sample_weight=sample_weight)\n-\n-        # cm = [[0.2, 0.3],\n-        #       [0.4, 0.1]]\n-        # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2,\n-        # 0.1]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = (\n-            0.1 / (0.4 + 0.5 - 0.1) + 0.2 / (0.6 + 0.5 - 0.2)\n-        ) / 2\n-        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n-\n-    def test_multi_dim_input(self):\n-        y_pred = tf.constant([[0, 1], [0, 1]], dtype=tf.float32)\n-        y_true = tf.constant([[0, 0], [1, 1]])\n-        sample_weight = tf.constant([[0.2, 0.3], [0.4, 0.1]])\n-\n-        obj = metrics.IoU(num_classes=2, target_class_ids=[0, 1])\n-        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n-\n-        result = obj(y_true, y_pred, sample_weight=sample_weight)\n-\n-        # cm = [[0.2, 0.3],\n-        #       [0.4, 0.1]]\n-        # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2,\n-        # 0.1]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = (\n-            0.2 / (0.6 + 0.5 - 0.2) + 0.1 / (0.4 + 0.5 - 0.1)\n-        ) / 2\n-        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n-\n-    def test_zero_valid_entries(self):\n-        obj = metrics.IoU(num_classes=2, target_class_ids=[0, 1])\n-        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n-        self.assertAllClose(self.evaluate(obj.result()), 0, atol=1e-3)\n-\n-    def test_zero_and_non_zero_entries(self):\n-        y_pred = tf.constant([1], dtype=tf.float32)\n-        y_true = tf.constant([1])\n-\n-        obj = metrics.IoU(num_classes=2, target_class_ids=[0, 1])\n-        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n-        result = obj(y_true, y_pred)\n-\n-        # cm = [[0, 0],\n-        #       [0, 1]]\n-        # sum_row = [0, 1], sum_col = [0, 1], true_positives = [0, 1]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = (1 / (1 + 1 - 1)) / 1\n-        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class BinaryIoUTest(tf.test.TestCase):\n-    def test_config(self):\n-        obj = metrics.BinaryIoU(\n-            target_class_ids=[1, 0], threshold=0.1, name=\"iou_class_1_0\"\n-        )\n-        self.assertEqual(obj.name, \"iou_class_1_0\")\n-        self.assertAlmostEqual(obj.threshold, 0.1)\n-        self.assertEqual(obj.target_class_ids, [1, 0])\n-\n-        obj2 = metrics.BinaryIoU.from_config(obj.get_config())\n-        self.assertEqual(obj.name, \"iou_class_1_0\")\n-        self.assertAlmostEqual(obj2.threshold, 0.1)\n-        self.assertEqual(obj.target_class_ids, [1, 0])\n-\n-    def test_different_thresholds_weighted(self):\n-        y_true = [0, 1, 0, 1]\n-        y_pred = [0.1, 0.2, 0.4, 0.7]\n-\n-        sample_weight = tf.constant([0.2, 0.3, 0.4, 0.1])\n-        # with threshold = 0.3, y_pred will be converted to [0, 0, 1, 1]\n-        # cm = [[0.2, 0.4],\n-        #       [0.3, 0.1]]\n-        # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2,\n-        # 0.1]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = (\n-            0.2 / (0.6 + 0.5 - 0.2) + 0.1 / (0.4 + 0.5 - 0.1)\n-        ) / 2\n-        obj = metrics.BinaryIoU(target_class_ids=[0, 1], threshold=0.3)\n-        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n-        result = obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n-\n-        sample_weight = tf.constant([0.1, 0.2, 0.4, 0.3])\n-        # with threshold = 0.5, y_pred will be converted to [0, 0, 0, 1]\n-        # cm = [[0.1+0.4, 0],\n-        #       [0.2, 0.3]]\n-        # sum_row = [0.5, 0.5], sum_col = [0.7, 0.3], true_positives = [0.5,\n-        # 0.3]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = (\n-            0.5 / (0.5 + 0.7 - 0.5) + 0.3 / (0.5 + 0.3 - 0.3)\n-        ) / 2\n-        obj = metrics.BinaryIoU(target_class_ids=[0, 1], threshold=0.5)\n-        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n-        result = obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n-\n-    def test_different_thresholds_unweighted(self):\n-        y_true = [0, 1, 0, 1]\n-        y_pred = [0.1, 0.2, 0.4, 0.7]\n-\n-        # with threshold = 0.3, y_pred will be converted to [0, 0, 1, 1]\n-        # cm = [[1, 1],\n-        #       [1, 1]]\n-        # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = (1 / (2 + 2 - 1) + 1 / (2 + 2 - 1)) / 2\n-        obj = metrics.BinaryIoU(target_class_ids=[0, 1], threshold=0.3)\n-        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n-        result = obj(y_true, y_pred)\n-        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n-\n-        # with threshold = 0.5, y_pred will be converted to [0, 0, 0, 1]\n-        # cm = [[2, 0],\n-        #       [1, 1]]\n-        # sum_row = [2, 2], sum_col = [3, 1], true_positives = [2, 1]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = (2 / (2 + 3 - 2) + 1 / (2 + 1 - 1)) / 2\n-        obj = metrics.BinaryIoU(target_class_ids=[0, 1], threshold=0.5)\n-        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n-        result = obj(y_true, y_pred)\n-        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n-\n-    def test_multi_dim_input(self):\n-        y_true = tf.constant([[0, 1], [0, 1]], dtype=tf.float32)\n-        y_pred = tf.constant([[0.1, 0.7], [0.9, 0.3]])\n-        threshold = 0.4  # y_pred will become [[0, 1], [1, 0]]\n-        sample_weight = tf.constant([[0.2, 0.3], [0.4, 0.1]])\n-        # cm = [[0.2, 0.4],\n-        #       [0.1, 0.3]]\n-        # sum_row = [0.6, 0.4], sum_col = [0.3, 0.7], true_positives = [0.2,\n-        # 0.3]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = (\n-            0.2 / (0.6 + 0.3 - 0.2) + 0.3 / (0.4 + 0.7 - 0.3)\n-        ) / 2\n-        obj = metrics.BinaryIoU(target_class_ids=[0, 1], threshold=threshold)\n-        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n-        result = obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n-\n-    def test_zero_valid_entries(self):\n-        obj = metrics.BinaryIoU(target_class_ids=[0, 1])\n-        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n-        self.assertAllClose(self.evaluate(obj.result()), 0, atol=1e-3)\n-\n-    def test_zero_and_non_zero_entries(self):\n-        y_pred = tf.constant([0.6], dtype=tf.float32)\n-        threshold = 0.5\n-        y_true = tf.constant([1])\n-\n-        obj = metrics.BinaryIoU(target_class_ids=[0, 1], threshold=threshold)\n-        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n-        result = obj(y_true, y_pred)\n-\n-        # cm = [[0, 0],\n-        #       [0, 1]]\n-        # sum_row = [0, 1], sum_col = [0, 1], true_positives = [0, 1]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = 1 / (1 + 1 - 1)\n-        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class MeanIoUTest(tf.test.TestCase):\n-    def test_config(self):\n-        m_obj = metrics.MeanIoU(num_classes=2, name=\"mean_iou\")\n-        self.assertEqual(m_obj.name, \"mean_iou\")\n-        self.assertEqual(m_obj.num_classes, 2)\n-\n-        m_obj2 = metrics.MeanIoU.from_config(m_obj.get_config())\n-        self.assertEqual(m_obj2.name, \"mean_iou\")\n-        self.assertEqual(m_obj2.num_classes, 2)\n-\n-    def test_unweighted(self):\n-        y_pred = [0, 1, 0, 1]\n-        y_true = [0, 0, 1, 1]\n-\n-        m_obj = metrics.MeanIoU(num_classes=2)\n-        self.evaluate(tf.compat.v1.variables_initializer(m_obj.variables))\n-\n-        result = m_obj(y_true, y_pred)\n-\n-        # cm = [[1, 1],\n-        #       [1, 1]]\n-        # sum_row = [2, 2], sum_col = [2, 2], true_positives = [1, 1]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = (1 / (2 + 2 - 1) + 1 / (2 + 2 - 1)) / 2\n-        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n-\n-    def test_unweighted_ignore_class_255(self):\n-        y_pred = [0, 1, 1, 1]\n-        y_true = [0, 1, 2, 255]\n-\n-        m_obj = metrics.MeanIoU(num_classes=3, ignore_class=255)\n-        self.evaluate(tf.compat.v1.variables_initializer(m_obj.variables))\n-\n-        result = m_obj(y_true, y_pred)\n-\n-        # cm = [[1, 0, 0],\n-        #       [0, 1, 0],\n-        #       [0, 1, 0]]\n-        # sum_row = [1, 1, 1], sum_col = [1, 2, 0], true_positives = [1, 1, 0]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = (\n-            1 / (1 + 1 - 1) + 1 / (2 + 1 - 1) + 0 / (0 + 1 - 0)\n-        ) / 3\n-        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n-\n-    def test_unweighted_ignore_class_1(self):\n-        y_pred = [0, 1, 1, 1]\n-        y_true = [0, 1, 2, -1]\n-\n-        m_obj = metrics.MeanIoU(num_classes=3, ignore_class=-1)\n-        self.evaluate(tf.compat.v1.variables_initializer(m_obj.variables))\n-\n-        result = m_obj(y_true, y_pred)\n-\n-        # cm = [[1, 0, 0],\n-        #       [0, 1, 0],\n-        #       [0, 1, 0]]\n-        # sum_row = [1, 1, 1], sum_col = [1, 2, 0], true_positives = [1, 1, 0]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = (\n-            1 / (1 + 1 - 1) + 1 / (2 + 1 - 1) + 0 / (0 + 1 - 0)\n-        ) / 3\n-        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n-\n-    def test_weighted(self):\n-        y_pred = tf.constant([0, 1, 0, 1], dtype=tf.float32)\n-        y_true = tf.constant([0, 0, 1, 1])\n-        sample_weight = tf.constant([0.2, 0.3, 0.4, 0.1])\n-\n-        m_obj = metrics.MeanIoU(num_classes=2)\n-        self.evaluate(tf.compat.v1.variables_initializer(m_obj.variables))\n-\n-        result = m_obj(y_true, y_pred, sample_weight=sample_weight)\n-\n-        # cm = [[0.2, 0.3],\n-        #       [0.4, 0.1]]\n-        # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2,\n-        # 0.1]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = (\n-            0.2 / (0.6 + 0.5 - 0.2) + 0.1 / (0.4 + 0.5 - 0.1)\n-        ) / 2\n-        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n-\n-    def test_weighted_ignore_class_1(self):\n-        y_pred = tf.constant([0, 1, 0, 1], dtype=tf.float32)\n-        y_true = tf.constant([0, 0, 1, -1])\n-        sample_weight = tf.constant([0.2, 0.3, 0.4, 0.1])\n-\n-        m_obj = metrics.MeanIoU(num_classes=2, ignore_class=-1)\n-        self.evaluate(tf.compat.v1.variables_initializer(m_obj.variables))\n-\n-        result = m_obj(y_true, y_pred, sample_weight=sample_weight)\n-\n-        # cm = [[0.2, 0.3],\n-        #       [0.4, 0.0]]\n-        # sum_row = [0.6, 0.3], sum_col = [0.5, 0.4], true_positives = [0.2,\n-        # 0.0]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = (\n-            0.2 / (0.6 + 0.5 - 0.2) + 0.0 / (0.3 + 0.4 - 0.0)\n-        ) / 2\n-        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n-\n-    def test_multi_dim_input(self):\n-        y_pred = tf.constant([[0, 1], [0, 1]], dtype=tf.float32)\n-        y_true = tf.constant([[0, 0], [1, 1]])\n-        sample_weight = tf.constant([[0.2, 0.3], [0.4, 0.1]])\n-\n-        m_obj = metrics.MeanIoU(num_classes=2)\n-        self.evaluate(tf.compat.v1.variables_initializer(m_obj.variables))\n-\n-        result = m_obj(y_true, y_pred, sample_weight=sample_weight)\n-\n-        # cm = [[0.2, 0.3],\n-        #       [0.4, 0.1]]\n-        # sum_row = [0.6, 0.4], sum_col = [0.5, 0.5], true_positives = [0.2,\n-        # 0.1]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = (\n-            0.2 / (0.6 + 0.5 - 0.2) + 0.1 / (0.4 + 0.5 - 0.1)\n-        ) / 2\n-        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n-\n-    def test_zero_valid_entries(self):\n-        m_obj = metrics.MeanIoU(num_classes=2)\n-        self.evaluate(tf.compat.v1.variables_initializer(m_obj.variables))\n-        self.assertAllClose(self.evaluate(m_obj.result()), 0, atol=1e-3)\n-\n-    def test_zero_and_non_zero_entries(self):\n-        y_pred = tf.constant([1], dtype=tf.float32)\n-        y_true = tf.constant([1])\n-\n-        m_obj = metrics.MeanIoU(num_classes=2)\n-        self.evaluate(tf.compat.v1.variables_initializer(m_obj.variables))\n-        result = m_obj(y_true, y_pred)\n-\n-        # cm = [[0, 0],\n-        #       [0, 1]]\n-        # sum_row = [0, 1], sum_col = [0, 1], true_positives = [0, 1]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = (0 + 1 / (1 + 1 - 1)) / 1\n-        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class OneHotIoUTest(tf.test.TestCase):\n-    def test_unweighted(self):\n-        y_true = tf.constant([[0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0]])\n-        # y_true will be converted to [2, 0, 1, 0]\n-        y_pred = tf.constant(\n-            [[0.2, 0.3, 0.5], [0.1, 0.2, 0.7], [0.5, 0.3, 0.1], [0.1, 0.4, 0.5]]\n-        )\n-        # y_pred will be converted to [2, 2, 0, 2]\n-        # cm = [[0, 0, 2],\n-        #       [1, 0, 0],\n-        #       [0, 0, 1]\n-        # sum_row = [1, 0, 3], sum_col = [2, 1, 1], true_positives = [0, 0, 1]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = (0 / (1 + 2 - 0) + 1 / (3 + 1 - 1)) / 2\n-        obj = metrics.OneHotIoU(num_classes=3, target_class_ids=[0, 2])\n-        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n-        result = obj(y_true, y_pred)\n-        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n-\n-    def test_weighted(self):\n-        y_true = tf.constant([[0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0]])\n-        # y_true will be converted to [2, 0, 1, 0]\n-        y_pred = tf.constant(\n-            [[0.2, 0.3, 0.5], [0.1, 0.2, 0.7], [0.5, 0.3, 0.1], [0.1, 0.4, 0.5]]\n-        )\n-        # y_pred will be converted to [2, 2, 0, 2]\n-        sample_weight = [0.1, 0.2, 0.3, 0.4]\n-        # cm = [[0, 0, 0.2+0.4],\n-        #       [0.3, 0, 0],\n-        #       [0, 0, 0.1]]\n-        # sum_row = [0.3, 0, 0.7], sum_col = [0.6, 0.3, 0.1]\n-        # true_positives = [0, 0, 0.1]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = (0 / (0.3 + 0.6 - 0) + 0.1 / (0.7 + 0.1 - 0.1)) / 2\n-        obj = metrics.OneHotIoU(num_classes=3, target_class_ids=[0, 2])\n-        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n-        result = obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class OneHotMeanIoUTest(tf.test.TestCase):\n-    def test_unweighted(self):\n-        y_true = tf.constant([[0, 0, 1], [1, 0, 0], [0, 1, 0], [1, 0, 0]])\n-        # y_true will be converted to [2, 0, 1, 0]\n-        y_pred = tf.constant(\n-            [[0.2, 0.3, 0.5], [0.1, 0.2, 0.7], [0.5, 0.3, 0.1], [0.1, 0.4, 0.5]]\n-        )\n-        # y_pred will be converted to [2, 2, 0, 2]\n-        # cm = [[0, 0, 2],\n-        #       [1, 0, 0],\n-        #       [0, 0, 1]\n-        # sum_row = [1, 0, 3], sum_col = [2, 1, 1], true_positives = [0, 0, 1]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = (0 + 0 + 1 / (3 + 1 - 1)) / 3\n-        obj = metrics.OneHotMeanIoU(num_classes=3)\n-        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n-        result = obj(y_true, y_pred)\n-        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n-\n-    def test_weighted(self):\n-        y_true = tf.constant(\n-            [\n-                [0, 0, 1],\n-                [1, 0, 0],\n-                [0, 1, 0],\n-                [1, 0, 0],\n-                [1, 0, 0],\n-            ]\n-        )\n-        # y_true will be converted to [2, 0, 1, 0, 0]\n-        y_pred = tf.constant(\n-            [\n-                [0.2, 0.3, 0.5],\n-                [0.1, 0.2, 0.7],\n-                [0.5, 0.3, 0.1],\n-                [0.1, 0.4, 0.5],\n-                [0.6, 0.2, 0.2],\n-            ]\n-        )\n-        # y_pred will be converted to [2, 2, 0, 2, 0]\n-        sample_weight = [0.1, 0.2, 0.3, 0.3, 0.1]\n-        # cm = [[0.1, 0, 0.2+0.3],\n-        #       [0.3, 0, 0],\n-        #       [0, 0, 0.1]]\n-        # sum_row = [0.4, 0, 0.6], sum_col = [0.6, 0.3, 0.1]\n-        # true_positives = [0.1, 0, 0.1]\n-        # iou = true_positives / (sum_row + sum_col - true_positives))\n-        expected_result = (\n-            0.1 / (0.4 + 0.6 - 0.1) + 0 + 0.1 / (0.6 + 0.1 - 0.1)\n-        ) / 3\n-        obj = metrics.OneHotMeanIoU(num_classes=3)\n-        self.evaluate(tf.compat.v1.variables_initializer(obj.variables))\n-        result = obj(y_true, y_pred, sample_weight=sample_weight)\n-        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class BinaryCrossentropyTest(tf.test.TestCase):\n-    def test_config(self):\n-        bce_obj = metrics.BinaryCrossentropy(\n-            name=\"bce\", dtype=tf.int32, label_smoothing=0.2\n-        )\n-        self.assertEqual(bce_obj.name, \"bce\")\n-        self.assertEqual(bce_obj._dtype, tf.int32)\n-\n-        old_config = bce_obj.get_config()\n-        self.assertAllClose(old_config[\"label_smoothing\"], 0.2, 1e-3)\n-\n-        # Check save and restore config\n-        bce_obj2 = metrics.BinaryCrossentropy.from_config(old_config)\n-        self.assertEqual(bce_obj2.name, \"bce\")\n-        self.assertEqual(bce_obj2._dtype, tf.int32)\n-        new_config = bce_obj2.get_config()\n-        self.assertDictEqual(old_config, new_config)\n-\n-    def test_unweighted(self):\n-        bce_obj = metrics.BinaryCrossentropy()\n-        self.evaluate(tf.compat.v1.variables_initializer(bce_obj.variables))\n-        y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n-        y_pred = np.asarray([1, 1, 1, 0], dtype=np.float32).reshape([2, 2])\n-        result = bce_obj(y_true, y_pred)\n-\n-        # EPSILON = 1e-7, y = y_true, y` = y_pred, Y_MAX = 0.9999999\n-        # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)\n-        # y` = [Y_MAX, Y_MAX, Y_MAX, EPSILON]\n-\n-        # Metric = -(y log(y` + EPSILON) + (1 - y) log(1 - y` + EPSILON))\n-        #        = [-log(Y_MAX + EPSILON), -log(1 - Y_MAX + EPSILON),\n-        #           -log(Y_MAX + EPSILON), -log(1)]\n-        #        = [(0 + 15.33) / 2, (0 + 0) / 2]\n-        # Reduced metric = 7.665 / 2\n-\n-        self.assertAllClose(self.evaluate(result), 3.833, atol=1e-3)\n-\n-    def test_unweighted_with_logits(self):\n-        bce_obj = metrics.BinaryCrossentropy(from_logits=True)\n-        self.evaluate(tf.compat.v1.variables_initializer(bce_obj.variables))\n-\n-        y_true = tf.constant([[1, 0, 1], [0, 1, 1]])\n-        y_pred = tf.constant([[100.0, -100.0, 100.0], [100.0, 100.0, -100.0]])\n-        result = bce_obj(y_true, y_pred)\n-\n-        # Metric = max(x, 0) - x * z + log(1 + exp(-abs(x)))\n-        #              (where x = logits and z = y_true)\n-        #        = [((100 - 100 * 1 + log(1 + exp(-100))) +\n-        #            (0 + 100 * 0 + log(1 + exp(-100))) +\n-        #            (100 - 100 * 1 + log(1 + exp(-100))),\n-        #           ((100 - 100 * 0 + log(1 + exp(-100))) +\n-        #            (100 - 100 * 1 + log(1 + exp(-100))) +\n-        #            (0 + 100 * 1 + log(1 + exp(-100))))]\n-        #        = [(0 + 0 + 0) / 3, 200 / 3]\n-        # Reduced metric = (0 + 66.666) / 2\n-\n-        self.assertAllClose(self.evaluate(result), 33.333, atol=1e-3)\n-\n-    def test_weighted(self):\n-        bce_obj = metrics.BinaryCrossentropy()\n-        self.evaluate(tf.compat.v1.variables_initializer(bce_obj.variables))\n-        y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n-        y_pred = np.asarray([1, 1, 1, 0], dtype=np.float32).reshape([2, 2])\n-        sample_weight = tf.constant([1.5, 2.0])\n-        result = bce_obj(y_true, y_pred, sample_weight=sample_weight)\n-\n-        # EPSILON = 1e-7, y = y_true, y` = y_pred, Y_MAX = 0.9999999\n-        # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)\n-        # y` = [Y_MAX, Y_MAX, Y_MAX, EPSILON]\n-\n-        # Metric = -(y log(y` + EPSILON) + (1 - y) log(1 - y` + EPSILON))\n-        #        = [-log(Y_MAX + EPSILON), -log(1 - Y_MAX + EPSILON),\n-        #           -log(Y_MAX + EPSILON), -log(1)]\n-        #        = [(0 + 15.33) / 2, (0 + 0) / 2]\n-        # Weighted metric = [7.665 * 1.5, 0]\n-        # Reduced metric = 7.665 * 1.5 / (1.5 + 2)\n-\n-        self.assertAllClose(self.evaluate(result), 3.285, atol=1e-3)\n-\n-    def test_weighted_from_logits(self):\n-        bce_obj = metrics.BinaryCrossentropy(from_logits=True)\n-        self.evaluate(tf.compat.v1.variables_initializer(bce_obj.variables))\n-        y_true = tf.constant([[1, 0, 1], [0, 1, 1]])\n-        y_pred = tf.constant([[100.0, -100.0, 100.0], [100.0, 100.0, -100.0]])\n-        sample_weight = tf.constant([2.0, 2.5])\n-        result = bce_obj(y_true, y_pred, sample_weight=sample_weight)\n-\n-        # Metric = max(x, 0) - x * z + log(1 + exp(-abs(x)))\n-        #              (where x = logits and z = y_true)\n-        #        = [(0 + 0 + 0) / 3, 200 / 3]\n-        # Weighted metric = [0, 66.666 * 2.5]\n-        # Reduced metric = 66.666 * 2.5 / (2 + 2.5)\n-\n-        self.assertAllClose(self.evaluate(result), 37.037, atol=1e-3)\n-\n-    def test_label_smoothing(self):\n-        logits = tf.constant(((100.0, -100.0, -100.0)))\n-        y_true = tf.constant(((1, 0, 1)))\n-        label_smoothing = 0.1\n-        # Metric: max(x, 0) - x * z + log(1 + exp(-abs(x)))\n-        #             (where x = logits and z = y_true)\n-        # Label smoothing: z' = z * (1 - L) + 0.5L\n-        # After label smoothing, label 1 becomes 1 - 0.5L\n-        #                        label 0 becomes 0.5L\n-        # Applying the above two fns to the given input:\n-        # (100 - 100 * (1 - 0.5 L)  + 0 +\n-        #  0   + 100 * (0.5 L)      + 0 +\n-        #  0   + 100 * (1 - 0.5 L)  + 0) * (1/3)\n-        #  = (100 + 50L) * 1/3\n-        bce_obj = metrics.BinaryCrossentropy(\n-            from_logits=True, label_smoothing=label_smoothing\n-        )\n-        self.evaluate(tf.compat.v1.variables_initializer(bce_obj.variables))\n-        result = bce_obj(y_true, logits)\n-        expected_value = (100.0 + 50.0 * label_smoothing) / 3.0\n-        self.assertAllClose(expected_value, self.evaluate(result), atol=1e-3)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class CategoricalCrossentropyTest(tf.test.TestCase):\n-    def test_config(self):\n-        cce_obj = metrics.CategoricalCrossentropy(\n-            name=\"cce\", dtype=tf.int32, label_smoothing=0.2\n-        )\n-        self.assertEqual(cce_obj.name, \"cce\")\n-        self.assertEqual(cce_obj._dtype, tf.int32)\n-\n-        old_config = cce_obj.get_config()\n-        self.assertAllClose(old_config[\"label_smoothing\"], 0.2, 1e-3)\n-\n-        # Check save and restore config\n-        cce_obj2 = metrics.CategoricalCrossentropy.from_config(old_config)\n-        self.assertEqual(cce_obj2.name, \"cce\")\n-        self.assertEqual(cce_obj2._dtype, tf.int32)\n-        new_config = cce_obj2.get_config()\n-        self.assertDictEqual(old_config, new_config)\n-\n-    def test_unweighted(self):\n-        cce_obj = metrics.CategoricalCrossentropy()\n-        self.evaluate(tf.compat.v1.variables_initializer(cce_obj.variables))\n-\n-        y_true = np.asarray([[0, 1, 0], [0, 0, 1]])\n-        y_pred = np.asarray([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n-        result = cce_obj(y_true, y_pred)\n-\n-        # EPSILON = 1e-7, y = y_true, y` = y_pred\n-        # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)\n-        # y` = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]\n-\n-        # Metric = -sum(y * log(y'), axis = -1)\n-        #        = -((log 0.95), (log 0.1))\n-        #        = [0.051, 2.302]\n-        # Reduced metric = (0.051 + 2.302) / 2\n-\n-        self.assertAllClose(self.evaluate(result), 1.176, atol=1e-3)\n-\n-    def test_unweighted_from_logits(self):\n-        cce_obj = metrics.CategoricalCrossentropy(from_logits=True)\n-        self.evaluate(tf.compat.v1.variables_initializer(cce_obj.variables))\n-\n-        y_true = np.asarray([[0, 1, 0], [0, 0, 1]])\n-        logits = np.asarray([[1, 9, 0], [1, 8, 1]], dtype=np.float32)\n-        result = cce_obj(y_true, logits)\n-\n-        # softmax = exp(logits) / sum(exp(logits), axis=-1)\n-        # xent = -sum(labels * log(softmax), 1)\n-\n-        # exp(logits) = [[2.718, 8103.084, 1], [2.718, 2980.958, 2.718]]\n-        # sum(exp(logits), axis=-1) = [8106.802, 2986.394]\n-        # softmax = [[0.00033, 0.99954, 0.00012], [0.00091, 0.99817, 0.00091]]\n-        # log(softmax) = [[-8.00045, -0.00045, -9.00045],\n-        #                 [-7.00182, -0.00182, -7.00182]]\n-        # labels * log(softmax) = [[0, -0.00045, 0], [0, 0, -7.00182]]\n-        # xent = [0.00045, 7.00182]\n-        # Reduced xent = (0.00045 + 7.00182) / 2\n-\n-        self.assertAllClose(self.evaluate(result), 3.5011, atol=1e-3)\n-\n-    def test_weighted(self):\n-        cce_obj = metrics.CategoricalCrossentropy()\n-        self.evaluate(tf.compat.v1.variables_initializer(cce_obj.variables))\n-\n-        y_true = np.asarray([[0, 1, 0], [0, 0, 1]])\n-        y_pred = np.asarray([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n-        sample_weight = tf.constant([1.5, 2.0])\n-        result = cce_obj(y_true, y_pred, sample_weight=sample_weight)\n-\n-        # EPSILON = 1e-7, y = y_true, y` = y_pred\n-        # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)\n-        # y` = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]\n-\n-        # Metric = -sum(y * log(y'), axis = -1)\n-        #        = -((log 0.95), (log 0.1))\n-        #        = [0.051, 2.302]\n-        # Weighted metric = [0.051 * 1.5, 2.302 * 2.]\n-        # Reduced metric = (0.051 * 1.5 + 2.302 * 2.) / 3.5\n-\n-        self.assertAllClose(self.evaluate(result), 1.338, atol=1e-3)\n-\n-    def test_weighted_from_logits(self):\n-        cce_obj = metrics.CategoricalCrossentropy(from_logits=True)\n-        self.evaluate(tf.compat.v1.variables_initializer(cce_obj.variables))\n-\n-        y_true = np.asarray([[0, 1, 0], [0, 0, 1]])\n-        logits = np.asarray([[1, 9, 0], [1, 8, 1]], dtype=np.float32)\n-        sample_weight = tf.constant([1.5, 2.0])\n-        result = cce_obj(y_true, logits, sample_weight=sample_weight)\n-\n-        # softmax = exp(logits) / sum(exp(logits), axis=-1)\n-        # xent = -sum(labels * log(softmax), 1)\n-        # xent = [0.00045, 7.00182]\n-        # weighted xent = [0.000675, 14.00364]\n-        # Reduced xent = (0.000675 + 14.00364) / (1.5 + 2)\n-\n-        self.assertAllClose(self.evaluate(result), 4.0012, atol=1e-3)\n-\n-    def test_label_smoothing(self):\n-        y_true = np.asarray([[0, 1, 0], [0, 0, 1]])\n-        logits = np.asarray([[1, 9, 0], [1, 8, 1]], dtype=np.float32)\n-        label_smoothing = 0.1\n-\n-        # Label smoothing: z' = z * (1 - L) + L/n,\n-        #     where L = label smoothing value and n = num classes\n-        # Label value 1 becomes: 1 - L + L/n\n-        # Label value 0 becomes: L/n\n-        # y_true with label_smoothing = [[0.0333, 0.9333, 0.0333],\n-        #                               [0.0333, 0.0333, 0.9333]]\n-\n-        # softmax = exp(logits) / sum(exp(logits), axis=-1)\n-        # xent = -sum(labels * log(softmax), 1)\n-        # log(softmax) = [[-8.00045, -0.00045, -9.00045],\n-        #                 [-7.00182, -0.00182, -7.00182]]\n-        # labels * log(softmax) = [[-0.26641, -0.00042, -0.29971],\n-        #                          [-0.23316, -0.00006, -6.53479]]\n-        # xent = [0.56654, 6.76801]\n-        # Reduced xent = (0.56654 + 6.76801) / 2\n-\n-        cce_obj = metrics.CategoricalCrossentropy(\n-            from_logits=True, label_smoothing=label_smoothing\n-        )\n-        self.evaluate(tf.compat.v1.variables_initializer(cce_obj.variables))\n-        loss = cce_obj(y_true, logits)\n-        self.assertAllClose(self.evaluate(loss), 3.667, atol=1e-3)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class SparseCategoricalCrossentropyTest(tf.test.TestCase):\n-    def test_config(self):\n-        scce_obj = metrics.SparseCategoricalCrossentropy(\n-            name=\"scce\", dtype=tf.int32\n-        )\n-        self.assertEqual(scce_obj.name, \"scce\")\n-        self.assertEqual(scce_obj.dtype, tf.int32)\n-        old_config = scce_obj.get_config()\n-        self.assertDictEqual(old_config, json.loads(json.dumps(old_config)))\n-\n-        # Check save and restore config\n-        scce_obj2 = metrics.SparseCategoricalCrossentropy.from_config(\n-            old_config\n-        )\n-        self.assertEqual(scce_obj2.name, \"scce\")\n-        self.assertEqual(scce_obj2.dtype, tf.int32)\n-        new_config = scce_obj2.get_config()\n-        self.assertDictEqual(old_config, new_config)\n-\n-    def test_unweighted(self):\n-        scce_obj = metrics.SparseCategoricalCrossentropy()\n-        self.evaluate(tf.compat.v1.variables_initializer(scce_obj.variables))\n-\n-        y_true = np.asarray([1, 2])\n-        y_pred = np.asarray([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n-        result = scce_obj(y_true, y_pred)\n-\n-        # EPSILON = 1e-7, y = y_true, y` = y_pred\n-        # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)\n-        # y` = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]\n-        # logits = log(y`) =  [[-2.9957, -0.0513, -16.1181],\n-        #                      [-2.3026, -0.2231, -2.3026]]\n-\n-        # softmax = exp(logits) / sum(exp(logits), axis=-1)\n-        # y = one_hot(y) = [[0, 1, 0], [0, 0, 1]]\n-        # xent = -sum(y * log(softmax), 1)\n-\n-        # exp(logits) = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]\n-        # sum(exp(logits), axis=-1) = [1, 1]\n-        # softmax = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]\n-        # log(softmax) = [[-2.9957, -0.0513, -16.1181],\n-        #                 [-2.3026, -0.2231, -2.3026]]\n-        # y * log(softmax) = [[0, -0.0513, 0], [0, 0, -2.3026]]\n-        # xent = [0.0513, 2.3026]\n-        # Reduced xent = (0.0513 + 2.3026) / 2\n-\n-        self.assertAllClose(self.evaluate(result), 1.176, atol=1e-3)\n-\n-    def test_unweighted_ignore_class(self):\n-        scce_obj = metrics.SparseCategoricalCrossentropy(ignore_class=-1)\n-        self.evaluate(tf.compat.v1.variables_initializer(scce_obj.variables))\n-\n-        y_true = np.asarray([-1, 2])\n-        y_pred = np.asarray([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n-        result = scce_obj(y_true, y_pred)\n-\n-        self.assertAllClose(self.evaluate(result), 2.3026, atol=1e-3)\n-\n-    def test_unweighted_from_logits(self):\n-        scce_obj = metrics.SparseCategoricalCrossentropy(from_logits=True)\n-        self.evaluate(tf.compat.v1.variables_initializer(scce_obj.variables))\n-\n-        y_true = np.asarray([1, 2])\n-        logits = np.asarray([[1, 9, 0], [1, 8, 1]], dtype=np.float32)\n-        result = scce_obj(y_true, logits)\n-\n-        # softmax = exp(logits) / sum(exp(logits), axis=-1)\n-        # y_true = one_hot(y_true) = [[0, 1, 0], [0, 0, 1]]\n-        # xent = -sum(y_true * log(softmax), 1)\n-\n-        # exp(logits) = [[2.718, 8103.084, 1], [2.718, 2980.958, 2.718]]\n-        # sum(exp(logits), axis=-1) = [8106.802, 2986.394]\n-        # softmax = [[0.00033, 0.99954, 0.00012], [0.00091, 0.99817, 0.00091]]\n-        # log(softmax) = [[-8.00045, -0.00045, -9.00045],\n-        #                 [-7.00182, -0.00182, -7.00182]]\n-        # y_true * log(softmax) = [[0, -0.00045, 0], [0, 0, -7.00182]]\n-        # xent = [0.00045, 7.00182]\n-        # Reduced xent = (0.00045 + 7.00182) / 2\n-\n-        self.assertAllClose(self.evaluate(result), 3.5011, atol=1e-3)\n-\n-    def test_weighted(self):\n-        scce_obj = metrics.SparseCategoricalCrossentropy()\n-        self.evaluate(tf.compat.v1.variables_initializer(scce_obj.variables))\n-\n-        y_true = np.asarray([1, 2])\n-        y_pred = np.asarray([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n-        sample_weight = tf.constant([1.5, 2.0])\n-        result = scce_obj(y_true, y_pred, sample_weight=sample_weight)\n-\n-        # EPSILON = 1e-7, y = y_true, y` = y_pred\n-        # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)\n-        # y` = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]\n-        # logits = log(y`) =  [[-2.9957, -0.0513, -16.1181],\n-        #                      [-2.3026, -0.2231, -2.3026]]\n-\n-        # softmax = exp(logits) / sum(exp(logits), axis=-1)\n-        # y = one_hot(y) = [[0, 1, 0], [0, 0, 1]]\n-        # xent = -sum(y * log(softmax), 1)\n-\n-        # exp(logits) = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]\n-        # sum(exp(logits), axis=-1) = [1, 1]\n-        # softmax = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]\n-        # log(softmax) = [[-2.9957, -0.0513, -16.1181],\n-        #                 [-2.3026, -0.2231, -2.3026]]\n-        # y * log(softmax) = [[0, -0.0513, 0], [0, 0, -2.3026]]\n-        # xent = [0.0513, 2.3026]\n-        # Weighted xent = [0.051 * 1.5, 2.302 * 2.]\n-        # Reduced xent = (0.051 * 1.5 + 2.302 * 2.) / 3.5\n-\n-        self.assertAllClose(self.evaluate(result), 1.338, atol=1e-3)\n-\n-    def test_weighted_ignore_class(self):\n-        scce_obj = metrics.SparseCategoricalCrossentropy(ignore_class=-1)\n-        self.evaluate(tf.compat.v1.variables_initializer(scce_obj.variables))\n-\n-        y_true = np.asarray([1, 2, -1])\n-        y_pred = np.asarray([[0.05, 0.95, 0], [0.1, 0.8, 0.1], [0.1, 0.8, 0.1]])\n-        sample_weight = tf.constant([1.5, 2.0, 1.5])\n-        result = scce_obj(y_true, y_pred, sample_weight=sample_weight)\n-\n-        self.assertAllClose(self.evaluate(result), 1.338, atol=1e-3)\n-\n-    def test_weighted_from_logits(self):\n-        scce_obj = metrics.SparseCategoricalCrossentropy(from_logits=True)\n-        self.evaluate(tf.compat.v1.variables_initializer(scce_obj.variables))\n-\n-        y_true = np.asarray([1, 2])\n-        logits = np.asarray([[1, 9, 0], [1, 8, 1]], dtype=np.float32)\n-        sample_weight = tf.constant([1.5, 2.0])\n-        result = scce_obj(y_true, logits, sample_weight=sample_weight)\n-\n-        # softmax = exp(logits) / sum(exp(logits), axis=-1)\n-        # y_true = one_hot(y_true) = [[0, 1, 0], [0, 0, 1]]\n-        # xent = -sum(y_true * log(softmax), 1)\n-        # xent = [0.00045, 7.00182]\n-        # weighted xent = [0.000675, 14.00364]\n-        # Reduced xent = (0.000675 + 14.00364) / (1.5 + 2)\n-\n-        self.assertAllClose(self.evaluate(result), 4.0012, atol=1e-3)\n-\n-    def test_axis(self):\n-        scce_obj = metrics.SparseCategoricalCrossentropy(axis=0)\n-        self.evaluate(tf.compat.v1.variables_initializer(scce_obj.variables))\n-\n-        y_true = np.asarray([1, 2])\n-        y_pred = np.asarray([[0.05, 0.1], [0.95, 0.8], [0, 0.1]])\n-        result = scce_obj(y_true, y_pred)\n-\n-        # EPSILON = 1e-7, y = y_true, y` = y_pred\n-        # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)\n-        # y` = [[0.05, 0.1], [0.95, 0.8], [EPSILON, 0.1]]\n-        # logits = log(y`) =  [[-2.9957, -2.3026],\n-        #                      [-0.0513, -0.2231],\n-        #                      [-16.1181, -2.3026]]\n-\n-        # softmax = exp(logits) / sum(exp(logits), axis=-1)\n-        # y = one_hot(y) = [[0, 0], [1, 0], [0, 1]]\n-        # xent = -sum(y * log(softmax), 1)\n-\n-        # exp(logits) = [[0.05, 0.1], [0.95, 0.8], [EPSILON, 0.1]]\n-        # sum(exp(logits)) = [1, 1]\n-        # softmax = [[0.05, 0.1], [0.95, 0.8], [EPSILON, 0.1]]\n-        # log(softmax) = [[-2.9957, -2.3026],\n-        #                 [-0.0513, -0.2231],\n-        #                 [-16.1181, -2.3026]]\n-        # y * log(softmax) = [[0, 0], [-0.0513, 0], [0, -2.3026]]\n-        # xent = [0.0513, 2.3026]\n-        # Reduced xent = (0.0513 + 2.3026) / 2\n-\n-        self.assertAllClose(self.evaluate(result), 1.176, atol=1e-3)\n-\n-\n-class BinaryTruePositives(metrics.Metric):\n-    def __init__(self, name=\"binary_true_positives\", **kwargs):\n-        super().__init__(name=name, **kwargs)\n-        self.true_positives = self.add_weight(name=\"tp\", initializer=\"zeros\")\n-\n-    def update_state(self, y_true, y_pred, sample_weight=None):\n-        y_true = tf.cast(y_true, tf.bool)\n-        y_pred = tf.cast(y_pred, tf.bool)\n-\n-        values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))\n-        values = tf.cast(values, self.dtype)\n-        if sample_weight is not None:\n-            sample_weight = tf.cast(sample_weight, dtype=self.dtype)\n-            sample_weight = tf.__internal__.ops.broadcast_weights(\n-                sample_weight, values\n-            )\n-            values = tf.multiply(values, sample_weight)\n-        self.true_positives.assign_add(tf.reduce_sum(values))\n-\n-    def result(self):\n-        return self.true_positives\n-\n-\n-class BinaryTruePositivesViaControlFlow(metrics.Metric):\n-    def __init__(self, name=\"binary_true_positives\", **kwargs):\n-        super().__init__(name=name, **kwargs)\n-        self.true_positives = self.add_weight(name=\"tp\", initializer=\"zeros\")\n-\n-    def update_state(self, y_true, y_pred, sample_weight=None):\n-        y_true = tf.cast(y_true, tf.bool)\n-        y_pred = tf.cast(y_pred, tf.bool)\n-\n-        for i in range(len(y_true)):\n-            for j in range(len(y_true[i])):\n-                if y_true[i][j] and y_pred[i][j]:\n-                    if sample_weight is None:\n-                        self.true_positives.assign_add(1)\n-                    else:\n-                        self.true_positives.assign_add(sample_weight[i][0])\n-\n-    def result(self):\n-        if tf.constant(True):\n-            return self.true_positives\n-        return 0.0\n-\n-\n-def _get_model(compile_metrics):\n-    model_layers = [\n-        layers.Dense(3, activation=\"relu\", kernel_initializer=\"ones\"),\n-        layers.Dense(1, activation=\"sigmoid\", kernel_initializer=\"ones\"),\n-    ]\n-\n-    model = test_utils.get_model_from_layers(model_layers, input_shape=(4,))\n-    model.compile(\n-        loss=\"mae\",\n-        metrics=compile_metrics,\n-        optimizer=\"rmsprop\",\n-        run_eagerly=test_utils.should_run_eagerly(),\n-    )\n-    return model\n-\n-\n-@test_combinations.run_with_all_model_types\n-@test_combinations.run_all_keras_modes\n-class ResetStatesTest(test_combinations.TestCase):\n-    def test_reset_state_false_positives(self):\n-        fp_obj = metrics.FalsePositives()\n-        model = _get_model([fp_obj])\n-        x = np.ones((100, 4))\n-        y = np.zeros((100, 1))\n-        model.evaluate(x, y)\n-        self.assertEqual(self.evaluate(fp_obj.accumulator), 100.0)\n-        model.evaluate(x, y)\n-        self.assertEqual(self.evaluate(fp_obj.accumulator), 100.0)\n-\n-    def test_reset_state_false_negatives(self):\n-        fn_obj = metrics.FalseNegatives()\n-        model = _get_model([fn_obj])\n-        x = np.zeros((100, 4))\n-        y = np.ones((100, 1))\n-        model.evaluate(x, y)\n-        self.assertEqual(self.evaluate(fn_obj.accumulator), 100.0)\n-        model.evaluate(x, y)\n-        self.assertEqual(self.evaluate(fn_obj.accumulator), 100.0)\n-\n-    def test_reset_state_true_negatives(self):\n-        tn_obj = metrics.TrueNegatives()\n-        model = _get_model([tn_obj])\n-        x = np.zeros((100, 4))\n-        y = np.zeros((100, 1))\n-        model.evaluate(x, y)\n-        self.assertEqual(self.evaluate(tn_obj.accumulator), 100.0)\n-        model.evaluate(x, y)\n-        self.assertEqual(self.evaluate(tn_obj.accumulator), 100.0)\n-\n-    def test_reset_state_true_positives(self):\n-        tp_obj = metrics.TruePositives()\n-        model = _get_model([tp_obj])\n-        x = np.ones((100, 4))\n-        y = np.ones((100, 1))\n-        model.evaluate(x, y)\n-        self.assertEqual(self.evaluate(tp_obj.accumulator), 100.0)\n-        model.evaluate(x, y)\n-        self.assertEqual(self.evaluate(tp_obj.accumulator), 100.0)\n-\n-    def test_reset_state_precision(self):\n-        p_obj = metrics.Precision()\n-        model = _get_model([p_obj])\n-        x = np.concatenate((np.ones((50, 4)), np.ones((50, 4))))\n-        y = np.concatenate((np.ones((50, 1)), np.zeros((50, 1))))\n-        model.evaluate(x, y)\n-        self.assertEqual(self.evaluate(p_obj.true_positives), 50.0)\n-        self.assertEqual(self.evaluate(p_obj.false_positives), 50.0)\n-        model.evaluate(x, y)\n-        self.assertEqual(self.evaluate(p_obj.true_positives), 50.0)\n-        self.assertEqual(self.evaluate(p_obj.false_positives), 50.0)\n-\n-    def test_precision_update_state_with_logits(self):\n-        p_obj = metrics.Precision()\n-        # Update state with logits (not in range (0, 1)) should not an raise\n-        # error.\n-        p_obj.update_state([-0.5, 0.5], [-2.0, 2.0])\n-\n-    def test_reset_state_recall(self):\n-        r_obj = metrics.Recall()\n-        model = _get_model([r_obj])\n-        x = np.concatenate((np.ones((50, 4)), np.zeros((50, 4))))\n-        y = np.concatenate((np.ones((50, 1)), np.ones((50, 1))))\n-        model.evaluate(x, y)\n-        self.assertEqual(self.evaluate(r_obj.true_positives), 50.0)\n-        self.assertEqual(self.evaluate(r_obj.false_negatives), 50.0)\n-        model.evaluate(x, y)\n-        self.assertEqual(self.evaluate(r_obj.true_positives), 50.0)\n-        self.assertEqual(self.evaluate(r_obj.false_negatives), 50.0)\n-\n-    def test_reset_state_sensitivity_at_specificity(self):\n-        s_obj = metrics.SensitivityAtSpecificity(0.5, num_thresholds=1)\n-        model = _get_model([s_obj])\n-        x = np.concatenate(\n-            (\n-                np.ones((25, 4)),\n-                np.zeros((25, 4)),\n-                np.zeros((25, 4)),\n-                np.ones((25, 4)),\n-            )\n-        )\n-        y = np.concatenate(\n-            (\n-                np.ones((25, 1)),\n-                np.zeros((25, 1)),\n-                np.ones((25, 1)),\n-                np.zeros((25, 1)),\n-            )\n-        )\n-\n-        for _ in range(2):\n-            model.evaluate(x, y)\n-            self.assertEqual(self.evaluate(s_obj.true_positives), 25.0)\n-            self.assertEqual(self.evaluate(s_obj.false_positives), 25.0)\n-            self.assertEqual(self.evaluate(s_obj.false_negatives), 25.0)\n-            self.assertEqual(self.evaluate(s_obj.true_negatives), 25.0)\n-\n-    def test_reset_state_specificity_at_sensitivity(self):\n-        s_obj = metrics.SpecificityAtSensitivity(0.5, num_thresholds=1)\n-        model = _get_model([s_obj])\n-        x = np.concatenate(\n-            (\n-                np.ones((25, 4)),\n-                np.zeros((25, 4)),\n-                np.zeros((25, 4)),\n-                np.ones((25, 4)),\n-            )\n-        )\n-        y = np.concatenate(\n-            (\n-                np.ones((25, 1)),\n-                np.zeros((25, 1)),\n-                np.ones((25, 1)),\n-                np.zeros((25, 1)),\n-            )\n-        )\n-\n-        for _ in range(2):\n-            model.evaluate(x, y)\n-            self.assertEqual(self.evaluate(s_obj.true_positives), 25.0)\n-            self.assertEqual(self.evaluate(s_obj.false_positives), 25.0)\n-            self.assertEqual(self.evaluate(s_obj.false_negatives), 25.0)\n-            self.assertEqual(self.evaluate(s_obj.true_negatives), 25.0)\n-\n-    def test_reset_state_precision_at_recall(self):\n-        s_obj = metrics.PrecisionAtRecall(recall=0.5, num_thresholds=1)\n-        model = _get_model([s_obj])\n-        x = np.concatenate(\n-            (\n-                np.ones((25, 4)),\n-                np.zeros((25, 4)),\n-                np.zeros((25, 4)),\n-                np.ones((25, 4)),\n-            )\n-        )\n-        y = np.concatenate(\n-            (\n-                np.ones((25, 1)),\n-                np.zeros((25, 1)),\n-                np.ones((25, 1)),\n-                np.zeros((25, 1)),\n-            )\n-        )\n-\n-        for _ in range(2):\n-            model.evaluate(x, y)\n-            self.assertEqual(self.evaluate(s_obj.true_positives), 25.0)\n-            self.assertEqual(self.evaluate(s_obj.false_positives), 25.0)\n-            self.assertEqual(self.evaluate(s_obj.false_negatives), 25.0)\n-            self.assertEqual(self.evaluate(s_obj.true_negatives), 25.0)\n-\n-    def test_reset_state_recall_at_precision(self):\n-        s_obj = metrics.RecallAtPrecision(precision=0.5, num_thresholds=1)\n-        model = _get_model([s_obj])\n-        x = np.concatenate(\n-            (\n-                np.ones((25, 4)),\n-                np.zeros((25, 4)),\n-                np.zeros((25, 4)),\n-                np.ones((25, 4)),\n-            )\n-        )\n-        y = np.concatenate(\n-            (\n-                np.ones((25, 1)),\n-                np.zeros((25, 1)),\n-                np.ones((25, 1)),\n-                np.zeros((25, 1)),\n-            )\n-        )\n-\n-        for _ in range(2):\n-            model.evaluate(x, y)\n-            self.assertEqual(self.evaluate(s_obj.true_positives), 25.0)\n-            self.assertEqual(self.evaluate(s_obj.false_positives), 25.0)\n-            self.assertEqual(self.evaluate(s_obj.false_negatives), 25.0)\n-            self.assertEqual(self.evaluate(s_obj.true_negatives), 25.0)\n-\n-    def test_reset_state_auc(self):\n-        auc_obj = metrics.AUC(num_thresholds=3)\n-        model = _get_model([auc_obj])\n-        x = np.concatenate(\n-            (\n-                np.ones((25, 4)),\n-                np.zeros((25, 4)),\n-                np.zeros((25, 4)),\n-                np.ones((25, 4)),\n-            )\n-        )\n-        y = np.concatenate(\n-            (\n-                np.ones((25, 1)),\n-                np.zeros((25, 1)),\n-                np.ones((25, 1)),\n-                np.zeros((25, 1)),\n-            )\n-        )\n-\n-        for _ in range(2):\n-            model.evaluate(x, y)\n-            self.assertEqual(self.evaluate(auc_obj.true_positives[1]), 25.0)\n-            self.assertEqual(self.evaluate(auc_obj.false_positives[1]), 25.0)\n-            self.assertEqual(self.evaluate(auc_obj.false_negatives[1]), 25.0)\n-            self.assertEqual(self.evaluate(auc_obj.true_negatives[1]), 25.0)\n-\n-    def test_reset_state_auc_from_logits(self):\n-        auc_obj = metrics.AUC(num_thresholds=3, from_logits=True)\n-\n-        model_layers = [\n-            layers.Dense(1, kernel_initializer=\"ones\", use_bias=False)\n-        ]\n-        model = test_utils.get_model_from_layers(model_layers, input_shape=(4,))\n-        model.compile(\n-            loss=\"mae\",\n-            metrics=[auc_obj],\n-            optimizer=\"rmsprop\",\n-            run_eagerly=test_utils.should_run_eagerly(),\n-        )\n-\n-        x = np.concatenate(\n-            (\n-                np.ones((25, 4)),\n-                -np.ones((25, 4)),\n-                -np.ones((25, 4)),\n-                np.ones((25, 4)),\n-            )\n-        )\n-        y = np.concatenate(\n-            (\n-                np.ones((25, 1)),\n-                np.zeros((25, 1)),\n-                np.ones((25, 1)),\n-                np.zeros((25, 1)),\n-            )\n-        )\n-\n-        for _ in range(2):\n-            model.evaluate(x, y)\n-            self.assertEqual(self.evaluate(auc_obj.true_positives[1]), 25.0)\n-            self.assertEqual(self.evaluate(auc_obj.false_positives[1]), 25.0)\n-            self.assertEqual(self.evaluate(auc_obj.false_negatives[1]), 25.0)\n-            self.assertEqual(self.evaluate(auc_obj.true_negatives[1]), 25.0)\n-\n-    def test_reset_state_auc_manual_thresholds(self):\n-        auc_obj = metrics.AUC(thresholds=[0.5])\n-        model = _get_model([auc_obj])\n-        x = np.concatenate(\n-            (\n-                np.ones((25, 4)),\n-                np.zeros((25, 4)),\n-                np.zeros((25, 4)),\n-                np.ones((25, 4)),\n-            )\n-        )\n-        y = np.concatenate(\n-            (\n-                np.ones((25, 1)),\n-                np.zeros((25, 1)),\n-                np.ones((25, 1)),\n-                np.zeros((25, 1)),\n-            )\n-        )\n-\n-        for _ in range(2):\n-            model.evaluate(x, y)\n-            self.assertEqual(self.evaluate(auc_obj.true_positives[1]), 25.0)\n-            self.assertEqual(self.evaluate(auc_obj.false_positives[1]), 25.0)\n-            self.assertEqual(self.evaluate(auc_obj.false_negatives[1]), 25.0)\n-            self.assertEqual(self.evaluate(auc_obj.true_negatives[1]), 25.0)\n-\n-    def test_reset_state_mean_iou(self):\n-        m_obj = metrics.MeanIoU(num_classes=2)\n-        model = _get_model([m_obj])\n-        x = np.asarray(\n-            [[0, 0, 0, 0], [1, 1, 1, 1], [1, 0, 1, 0], [0, 1, 0, 1]],\n-            dtype=np.float32,\n-        )\n-        y = np.asarray([[0], [1], [1], [1]], dtype=np.float32)\n-        model.evaluate(x, y)\n-        self.assertArrayNear(self.evaluate(m_obj.total_cm)[0], [1, 0], 1e-1)\n-        self.assertArrayNear(self.evaluate(m_obj.total_cm)[1], [3, 0], 1e-1)\n-        model.evaluate(x, y)\n-        self.assertArrayNear(self.evaluate(m_obj.total_cm)[0], [1, 0], 1e-1)\n-        self.assertArrayNear(self.evaluate(m_obj.total_cm)[1], [3, 0], 1e-1)\n-\n-    def test_reset_state_recall_float64(self):\n-        # Test case for GitHub issue 36790.\n-        try:\n-            backend.set_floatx(\"float64\")\n-            r_obj = metrics.Recall()\n-            model = _get_model([r_obj])\n-            x = np.concatenate((np.ones((50, 4)), np.zeros((50, 4))))\n-            y = np.concatenate((np.ones((50, 1)), np.ones((50, 1))))\n-            model.evaluate(x, y)\n-            self.assertEqual(self.evaluate(r_obj.true_positives), 50.0)\n-            self.assertEqual(self.evaluate(r_obj.false_negatives), 50.0)\n-            model.evaluate(x, y)\n-            self.assertEqual(self.evaluate(r_obj.true_positives), 50.0)\n-            self.assertEqual(self.evaluate(r_obj.false_negatives), 50.0)\n-        finally:\n-            backend.set_floatx(\"float32\")\n-\n-    def test_function_wrapped_reset_state(self):\n-        m = metrics.Mean(name=\"my_mean\")\n-\n-        # check reset_state in function.\n-        @tf.function\n-        def reset_in_fn():\n-            m.reset_state()\n-            return m.update_state(100)\n-\n-        for _ in range(5):\n-            self.evaluate(reset_in_fn())\n-        self.assertEqual(self.evaluate(m.count), 1)\n-\n-\n-@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n-class MergeStateTest(test_combinations.TestCase):\n-    def test_merge_state_incompatible_metrics(self):\n-        with self.assertRaisesRegex(\n-            ValueError, \"Metric .* is not compatible with .*\"\n-        ):\n-            obj1 = metrics.FalsePositives()\n-            self.evaluate(tf.compat.v1.variables_initializer(obj1.variables))\n-            obj2 = metrics.Accuracy()\n-            self.evaluate(tf.compat.v1.variables_initializer(obj2.variables))\n-            self.evaluate(obj1.merge_state([obj2]))\n-\n-    def test_merge_state_accuracy(self):\n-        a_objs = []\n-        for y_true, y_pred in zip(\n-            [[[1], [2]], [[3], [4]]], [[[0], [2]], [[3], [4]]]\n-        ):\n-            a_obj = metrics.Accuracy()\n-            a_objs.append(a_obj)\n-            self.evaluate(tf.compat.v1.variables_initializer(a_obj.variables))\n-            self.evaluate(a_obj.update_state(y_true, y_pred))\n-        self.evaluate(a_objs[0].merge_state(a_objs[1:]))\n-        self.assertEqual(self.evaluate(a_objs[0].total), 3.0)\n-        self.assertEqual(self.evaluate(a_objs[0].count), 4.0)\n-        self.assertEqual(self.evaluate(a_objs[0].result()), 0.75)\n-\n-    def test_merge_state_false_positives(self):\n-        fp_objs = []\n-        for _ in range(4):\n-            fp_obj = metrics.FalsePositives()\n-            fp_objs.append(fp_obj)\n-            self.evaluate(tf.compat.v1.variables_initializer(fp_obj.variables))\n-            y_true = np.zeros((25, 1))\n-            y_pred = np.ones((25, 1))\n-            self.evaluate(fp_obj.update_state(y_true, y_pred))\n-        self.evaluate(fp_objs[0].merge_state(fp_objs[1:]))\n-        self.assertEqual(self.evaluate(fp_objs[0].accumulator), 100.0)\n-\n-    def test_merge_state_false_negatives(self):\n-        fn_objs = []\n-        for _ in range(4):\n-            fn_obj = metrics.FalseNegatives()\n-            fn_objs.append(fn_obj)\n-            self.evaluate(tf.compat.v1.variables_initializer(fn_obj.variables))\n-            y_true = np.ones((25, 1))\n-            y_pred = np.zeros((25, 1))\n-            self.evaluate(fn_obj.update_state(y_true, y_pred))\n-        self.evaluate(fn_objs[0].merge_state(fn_objs[1:]))\n-        self.assertEqual(self.evaluate(fn_objs[0].accumulator), 100.0)\n-\n-    def test_merge_state_true_negatives(self):\n-        tn_objs = []\n-        for _ in range(4):\n-            tn_obj = metrics.TrueNegatives()\n-            tn_objs.append(tn_obj)\n-            self.evaluate(tf.compat.v1.variables_initializer(tn_obj.variables))\n-            y_true = np.zeros((25, 1))\n-            y_pred = np.zeros((25, 1))\n-            self.evaluate(tn_obj.update_state(y_true, y_pred))\n-        self.evaluate(tn_objs[0].merge_state(tn_objs[1:]))\n-        self.assertEqual(self.evaluate(tn_objs[0].accumulator), 100.0)\n-\n-    def test_merge_state_true_positives(self):\n-        tp_objs = []\n-        for _ in range(4):\n-            tp_obj = metrics.TruePositives()\n-            tp_objs.append(tp_obj)\n-            self.evaluate(tf.compat.v1.variables_initializer(tp_obj.variables))\n-            y_true = np.ones((25, 1))\n-            y_pred = np.ones((25, 1))\n-            self.evaluate(tp_obj.update_state(y_true, y_pred))\n-        self.evaluate(tp_objs[0].merge_state(tp_objs[1:]))\n-        self.assertEqual(self.evaluate(tp_objs[0].accumulator), 100.0)\n-\n-    def test_merge_state_precision(self):\n-        p_objs = []\n-        for _ in range(5):\n-            p_obj = metrics.Precision()\n-            p_objs.append(p_obj)\n-            self.evaluate(tf.compat.v1.variables_initializer(p_obj.variables))\n-            y_true = np.concatenate((np.ones((10, 1)), np.zeros((10, 1))))\n-            y_pred = np.concatenate((np.ones((10, 1)), np.ones((10, 1))))\n-            self.evaluate(p_obj.update_state(y_true, y_pred))\n-        self.evaluate(p_objs[0].merge_state(p_objs[1:]))\n-        self.assertEqual(self.evaluate(p_objs[0].true_positives), 50.0)\n-        self.assertEqual(self.evaluate(p_objs[0].false_positives), 50.0)\n-\n-    def test_merge_state_recall(self):\n-        r_objs = []\n-        for _ in range(5):\n-            r_obj = metrics.Recall()\n-            r_objs.append(r_obj)\n-            self.evaluate(tf.compat.v1.variables_initializer(r_obj.variables))\n-            y_true = np.concatenate((np.ones((10, 1)), np.ones((10, 1))))\n-            y_pred = np.concatenate((np.ones((10, 1)), np.zeros((10, 1))))\n-            self.evaluate(r_obj.update_state(y_true, y_pred))\n-        self.evaluate(r_objs[0].merge_state(r_objs[1:]))\n-        self.assertEqual(self.evaluate(r_objs[0].true_positives), 50.0)\n-        self.assertEqual(self.evaluate(r_objs[0].false_negatives), 50.0)\n-\n-    def test_merge_state_sensitivity_at_specificity(self):\n-        sas_objs = []\n-        for _ in range(5):\n-            sas_obj = metrics.SensitivityAtSpecificity(0.5, num_thresholds=1)\n-            sas_objs.append(sas_obj)\n-            self.evaluate(tf.compat.v1.variables_initializer(sas_obj.variables))\n-            y_true = np.concatenate(\n-                (\n-                    np.ones((5, 1)),\n-                    np.zeros((5, 1)),\n-                    np.ones((5, 1)),\n-                    np.zeros((5, 1)),\n-                )\n-            )\n-            y_pred = np.concatenate(\n-                (\n-                    np.ones((5, 1)),\n-                    np.zeros((5, 1)),\n-                    np.zeros((5, 1)),\n-                    np.ones((5, 1)),\n-                )\n-            )\n-            self.evaluate(sas_obj.update_state(y_true, y_pred))\n-        self.evaluate(sas_objs[0].merge_state(sas_objs[1:]))\n-        self.assertEqual(self.evaluate(sas_objs[0].true_positives), 25.0)\n-        self.assertEqual(self.evaluate(sas_objs[0].false_positives), 25.0)\n-        self.assertEqual(self.evaluate(sas_objs[0].false_negatives), 25.0)\n-        self.assertEqual(self.evaluate(sas_objs[0].true_negatives), 25.0)\n-\n-    def test_merge_state_specificity_at_sensitivity(self):\n-        sas_objs = []\n-        for _ in range(5):\n-            sas_obj = metrics.SpecificityAtSensitivity(0.5, num_thresholds=1)\n-            sas_objs.append(sas_obj)\n-            self.evaluate(tf.compat.v1.variables_initializer(sas_obj.variables))\n-            y_true = np.concatenate(\n-                (\n-                    np.ones((5, 1)),\n-                    np.zeros((5, 1)),\n-                    np.ones((5, 1)),\n-                    np.zeros((5, 1)),\n-                )\n-            )\n-            y_pred = np.concatenate(\n-                (\n-                    np.ones((5, 1)),\n-                    np.zeros((5, 1)),\n-                    np.zeros((5, 1)),\n-                    np.ones((5, 1)),\n-                )\n-            )\n-            self.evaluate(sas_obj.update_state(y_true, y_pred))\n-        self.evaluate(sas_objs[0].merge_state(sas_objs[1:]))\n-        self.assertEqual(self.evaluate(sas_objs[0].true_positives), 25.0)\n-        self.assertEqual(self.evaluate(sas_objs[0].false_positives), 25.0)\n-        self.assertEqual(self.evaluate(sas_objs[0].false_negatives), 25.0)\n-        self.assertEqual(self.evaluate(sas_objs[0].true_negatives), 25.0)\n-\n-    def test_merge_state_precision_at_recall(self):\n-        par_objs = []\n-        for _ in range(5):\n-            par_obj = metrics.PrecisionAtRecall(recall=0.5, num_thresholds=1)\n-            par_objs.append(par_obj)\n-            self.evaluate(tf.compat.v1.variables_initializer(par_obj.variables))\n-            y_true = np.concatenate(\n-                (\n-                    np.ones((5, 1)),\n-                    np.zeros((5, 1)),\n-                    np.ones((5, 1)),\n-                    np.zeros((5, 1)),\n-                )\n-            )\n-            y_pred = np.concatenate(\n-                (\n-                    np.ones((5, 1)),\n-                    np.zeros((5, 1)),\n-                    np.zeros((5, 1)),\n-                    np.ones((5, 1)),\n-                )\n-            )\n-            self.evaluate(par_obj.update_state(y_true, y_pred))\n-        self.evaluate(par_objs[0].merge_state(par_objs[1:]))\n-        self.assertEqual(self.evaluate(par_objs[0].true_positives), 25.0)\n-        self.assertEqual(self.evaluate(par_objs[0].false_positives), 25.0)\n-        self.assertEqual(self.evaluate(par_objs[0].false_negatives), 25.0)\n-        self.assertEqual(self.evaluate(par_objs[0].true_negatives), 25.0)\n-\n-    def test_merge_state_recall_at_precision(self):\n-        rap_objs = []\n-        for _ in range(5):\n-            rap_obj = metrics.PrecisionAtRecall(recall=0.5, num_thresholds=1)\n-            rap_objs.append(rap_obj)\n-            self.evaluate(tf.compat.v1.variables_initializer(rap_obj.variables))\n-            y_true = np.concatenate(\n-                (\n-                    np.ones((5, 1)),\n-                    np.zeros((5, 1)),\n-                    np.ones((5, 1)),\n-                    np.zeros((5, 1)),\n-                )\n-            )\n-            y_pred = np.concatenate(\n-                (\n-                    np.ones((5, 1)),\n-                    np.zeros((5, 1)),\n-                    np.zeros((5, 1)),\n-                    np.ones((5, 1)),\n-                )\n-            )\n-            self.evaluate(rap_obj.update_state(y_true, y_pred))\n-        self.evaluate(rap_objs[0].merge_state(rap_objs[1:]))\n-        self.assertEqual(self.evaluate(rap_objs[0].true_positives), 25.0)\n-        self.assertEqual(self.evaluate(rap_objs[0].false_positives), 25.0)\n-        self.assertEqual(self.evaluate(rap_objs[0].false_negatives), 25.0)\n-        self.assertEqual(self.evaluate(rap_objs[0].true_negatives), 25.0)\n-\n-    def test_merge_state_auc(self):\n-        auc_objs = []\n-        for _ in range(5):\n-            auc_obj = metrics.AUC(num_thresholds=3)\n-            auc_objs.append(auc_obj)\n-            self.evaluate(tf.compat.v1.variables_initializer(auc_obj.variables))\n-            y_true = np.concatenate(\n-                (\n-                    np.ones((5, 1)),\n-                    np.zeros((5, 1)),\n-                    np.ones((5, 1)),\n-                    np.zeros((5, 1)),\n-                )\n-            )\n-            y_pred = np.concatenate(\n-                (\n-                    np.ones((5, 1)),\n-                    np.zeros((5, 1)),\n-                    np.zeros((5, 1)),\n-                    np.ones((5, 1)),\n-                )\n-            )\n-            self.evaluate(auc_obj.update_state(y_true, y_pred))\n-        self.evaluate(auc_objs[0].merge_state(auc_objs[1:]))\n-        self.assertEqual(self.evaluate(auc_objs[0].true_positives[1]), 25.0)\n-        self.assertEqual(self.evaluate(auc_objs[0].false_positives[1]), 25.0)\n-        self.assertEqual(self.evaluate(auc_objs[0].false_negatives[1]), 25.0)\n-        self.assertEqual(self.evaluate(auc_objs[0].true_negatives[1]), 25.0)\n-\n-    def test_merge_state_mean_iou(self):\n-        m_objs = []\n-        for y_true, y_pred in zip(\n-            [[0], [1], [1], [1]], [[0.5], [1.0], [1.0], [1.0]]\n-        ):\n-            m_obj = metrics.MeanIoU(num_classes=2)\n-            m_objs.append(m_obj)\n-            self.evaluate(tf.compat.v1.variables_initializer(m_obj.variables))\n-            self.evaluate(m_obj.update_state(y_true, y_pred))\n-        self.evaluate(m_objs[0].merge_state(m_objs[1:]))\n-        self.assertArrayNear(self.evaluate(m_objs[0].total_cm)[0], [1, 0], 1e-1)\n-        self.assertArrayNear(self.evaluate(m_objs[0].total_cm)[1], [0, 3], 1e-1)\n-\n-\n-if __name__ == \"__main__\":\n-    tf.test.main()\n\n@@ -0,0 +1,344 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Probabilistic metrics (based on Entropy).\"\"\"\n+\n+from typing import Optional\n+from typing import Union\n+\n+import tensorflow.compat.v2 as tf\n+\n+from keras.dtensor import utils as dtensor_utils\n+from keras.losses import binary_crossentropy\n+from keras.losses import categorical_crossentropy\n+from keras.losses import kullback_leibler_divergence\n+from keras.losses import poisson\n+from keras.losses import sparse_categorical_crossentropy\n+from keras.metrics import base_metric\n+\n+# isort: off\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export(\"keras.metrics.Poisson\")\n+class Poisson(base_metric.MeanMetricWrapper):\n+    \"\"\"Computes the Poisson metric between `y_true` and `y_pred`.\n+\n+    `metric = y_pred - y_true * log(y_pred)`\n+\n+    Args:\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.Poisson()\n+    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n+    >>> m.result().numpy()\n+    0.49999997\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n+    ...                sample_weight=[1, 0])\n+    >>> m.result().numpy()\n+    0.99999994\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(optimizer='sgd',\n+                  loss='mse',\n+                  metrics=[tf.keras.metrics.Poisson()])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(self, name=\"poisson\", dtype=None):\n+        super().__init__(poisson, name, dtype=dtype)\n+\n+\n+@keras_export(\"keras.metrics.KLDivergence\")\n+class KLDivergence(base_metric.MeanMetricWrapper):\n+    \"\"\"Computes Kullback-Leibler divergence metric between `y_true` and\n+    `y_pred`.\n+\n+    `metric = y_true * log(y_true / y_pred)`\n+\n+    Args:\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.KLDivergence()\n+    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])\n+    >>> m.result().numpy()\n+    0.45814306\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],\n+    ...                sample_weight=[1, 0])\n+    >>> m.result().numpy()\n+    0.9162892\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(optimizer='sgd',\n+                  loss='mse',\n+                  metrics=[tf.keras.metrics.KLDivergence()])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(self, name=\"kullback_leibler_divergence\", dtype=None):\n+        super().__init__(kullback_leibler_divergence, name, dtype=dtype)\n+\n+\n+@keras_export(\"keras.metrics.BinaryCrossentropy\")\n+class BinaryCrossentropy(base_metric.MeanMetricWrapper):\n+    \"\"\"Computes the crossentropy metric between the labels and predictions.\n+\n+    This is the crossentropy metric class to be used when there are only two\n+    label classes (0 and 1).\n+\n+    Args:\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+      from_logits: (Optional )Whether output is expected to be a logits tensor.\n+        By default, we consider that output encodes a probability distribution.\n+      label_smoothing: (Optional) Float in [0, 1]. When > 0, label values are\n+        smoothed, meaning the confidence on label values are relaxed.\n+        e.g. `label_smoothing=0.2` means that we will use a value of `0.1` for\n+        label `0` and `0.9` for label `1`\".\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.BinaryCrossentropy()\n+    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])\n+    >>> m.result().numpy()\n+    0.81492424\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]],\n+    ...                sample_weight=[1, 0])\n+    >>> m.result().numpy()\n+    0.9162905\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[tf.keras.metrics.BinaryCrossentropy()])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(\n+        self,\n+        name=\"binary_crossentropy\",\n+        dtype=None,\n+        from_logits=False,\n+        label_smoothing=0,\n+    ):\n+        super().__init__(\n+            binary_crossentropy,\n+            name,\n+            dtype=dtype,\n+            from_logits=from_logits,\n+            label_smoothing=label_smoothing,\n+        )\n+\n+\n+@keras_export(\"keras.metrics.CategoricalCrossentropy\")\n+class CategoricalCrossentropy(base_metric.MeanMetricWrapper):\n+    \"\"\"Computes the crossentropy metric between the labels and predictions.\n+\n+    This is the crossentropy metric class to be used when there are multiple\n+    label classes (2 or more). Here we assume that labels are given as a\n+    `one_hot` representation. eg., When labels values are [2, 0, 1],\n+     `y_true` = [[0, 0, 1], [1, 0, 0], [0, 1, 0]].\n+\n+    Args:\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+      from_logits: (Optional) Whether output is expected to be a logits tensor.\n+        By default, we consider that output encodes a probability distribution.\n+      label_smoothing: (Optional) Float in [0, 1]. When > 0, label values are\n+        smoothed, meaning the confidence on label values are relaxed. e.g.\n+        `label_smoothing=0.2` means that we will use a value of `0.1` for label\n+        `0` and `0.9` for label `1`\"\n+      axis: (Optional) Defaults to -1. The dimension along which entropy is\n+        computed.\n+\n+    Standalone usage:\n+\n+    >>> # EPSILON = 1e-7, y = y_true, y` = y_pred\n+    >>> # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)\n+    >>> # y` = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]\n+    >>> # xent = -sum(y * log(y'), axis = -1)\n+    >>> #      = -((log 0.95), (log 0.1))\n+    >>> #      = [0.051, 2.302]\n+    >>> # Reduced xent = (0.051 + 2.302) / 2\n+    >>> m = tf.keras.metrics.CategoricalCrossentropy()\n+    >>> m.update_state([[0, 1, 0], [0, 0, 1]],\n+    ...                [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n+    >>> m.result().numpy()\n+    1.1769392\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([[0, 1, 0], [0, 0, 1]],\n+    ...                [[0.05, 0.95, 0], [0.1, 0.8, 0.1]],\n+    ...                sample_weight=tf.constant([0.3, 0.7]))\n+    >>> m.result().numpy()\n+    1.6271976\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+      optimizer='sgd',\n+      loss='mse',\n+      metrics=[tf.keras.metrics.CategoricalCrossentropy()])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(\n+        self,\n+        name=\"categorical_crossentropy\",\n+        dtype=None,\n+        from_logits=False,\n+        label_smoothing=0,\n+        axis=-1,\n+    ):\n+        super().__init__(\n+            categorical_crossentropy,\n+            name,\n+            dtype=dtype,\n+            from_logits=from_logits,\n+            label_smoothing=label_smoothing,\n+            axis=axis,\n+        )\n+\n+\n+@keras_export(\"keras.metrics.SparseCategoricalCrossentropy\")\n+class SparseCategoricalCrossentropy(base_metric.MeanMetricWrapper):\n+    \"\"\"Computes the crossentropy metric between the labels and predictions.\n+\n+    Use this crossentropy metric when there are two or more label classes.\n+    We expect labels to be provided as integers. If you want to provide labels\n+    using `one-hot` representation, please use `CategoricalCrossentropy` metric.\n+    There should be `# classes` floating point values per feature for `y_pred`\n+    and a single floating point value per feature for `y_true`.\n+\n+    In the snippet below, there is a single floating point value per example for\n+    `y_true` and `# classes` floating pointing values per example for `y_pred`.\n+    The shape of `y_true` is `[batch_size]` and the shape of `y_pred` is\n+    `[batch_size, num_classes]`.\n+\n+    Args:\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+      from_logits: (Optional) Whether output is expected to be a logits tensor.\n+        By default, we consider that output encodes a probability distribution.\n+      ignore_class: Optional integer. The ID of a class to be ignored during\n+        metric computation. This is useful, for example, in segmentation\n+        problems featuring a \"void\" class (commonly -1 or 255) in segmentation\n+        maps. By default (`ignore_class=None`), all classes are considered.\n+      axis: (Optional) Defaults to -1. The dimension along which entropy is\n+        computed.\n+\n+    Standalone usage:\n+\n+    >>> # y_true = one_hot(y_true) = [[0, 1, 0], [0, 0, 1]]\n+    >>> # logits = log(y_pred)\n+    >>> # softmax = exp(logits) / sum(exp(logits), axis=-1)\n+    >>> # softmax = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]\n+    >>> # xent = -sum(y * log(softmax), 1)\n+    >>> # log(softmax) = [[-2.9957, -0.0513, -16.1181],\n+    >>> #                [-2.3026, -0.2231, -2.3026]]\n+    >>> # y_true * log(softmax) = [[0, -0.0513, 0], [0, 0, -2.3026]]\n+    >>> # xent = [0.0513, 2.3026]\n+    >>> # Reduced xent = (0.0513 + 2.3026) / 2\n+    >>> m = tf.keras.metrics.SparseCategoricalCrossentropy()\n+    >>> m.update_state([1, 2],\n+    ...                [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n+    >>> m.result().numpy()\n+    1.1769392\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([1, 2],\n+    ...                [[0.05, 0.95, 0], [0.1, 0.8, 0.1]],\n+    ...                sample_weight=tf.constant([0.3, 0.7]))\n+    >>> m.result().numpy()\n+    1.6271976\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+      optimizer='sgd',\n+      loss='mse',\n+      metrics=[tf.keras.metrics.SparseCategoricalCrossentropy()])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(\n+        self,\n+        name: str = \"sparse_categorical_crossentropy\",\n+        dtype: Optional[Union[str, tf.dtypes.DType]] = None,\n+        from_logits: bool = False,\n+        ignore_class: Optional[int] = None,\n+        axis: int = -1,\n+    ):\n+        super().__init__(\n+            sparse_categorical_crossentropy,\n+            name,\n+            dtype=dtype,\n+            from_logits=from_logits,\n+            ignore_class=ignore_class,\n+            axis=axis,\n+        )\n+\n+\n+_SPARSE_CATEGORICAL_UPDATE_STATE_DOCSTRING = \"\"\"Accumulates metric statistics.\n+\n+For sparse categorical metrics, the shapes of `y_true` and `y_pred` are\n+different.\n+\n+Args:\n+  y_true: Ground truth label values. shape = `[batch_size, d0, .. dN-1]` or\n+    shape = `[batch_size, d0, .. dN-1, 1]`.\n+  y_pred: The predicted probability values. shape = `[batch_size, d0, .. dN]`.\n+  sample_weight: Optional `sample_weight` acts as a\n+    coefficient for the metric. If a scalar is provided, then the metric is\n+    simply scaled by the given value. If `sample_weight` is a tensor of size\n+    `[batch_size]`, then the metric for each sample of the batch is rescaled\n+    by the corresponding element in the `sample_weight` vector. If the shape\n+    of `sample_weight` is `[batch_size, d0, .. dN-1]` (or can be broadcasted\n+    to this shape), then each metric element of `y_pred` is scaled by the\n+    corresponding value of `sample_weight`. (Note on `dN-1`: all metric\n+    functions reduce by 1 dimension, usually the last axis (-1)).\n+\n+Returns:\n+  Update op.\n+\"\"\"\n+\n+SparseCategoricalCrossentropy.update_state.__doc__ = (\n+    _SPARSE_CATEGORICAL_UPDATE_STATE_DOCSTRING\n+)\n\n@@ -0,0 +1,567 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for Keras metrics.\"\"\"\n+\n+import json\n+\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+from keras import metrics\n+from keras.testing_infra import test_combinations\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class PoissonTest(tf.test.TestCase):\n+    def setup(self):\n+        y_pred = np.asarray([1, 9, 2, 5, 2, 6]).reshape((2, 3))\n+        y_true = np.asarray([4, 8, 12, 8, 1, 3]).reshape((2, 3))\n+\n+        self.batch_size = 6\n+        self.expected_results = y_pred - np.multiply(y_true, np.log(y_pred))\n+\n+        self.y_pred = tf.constant(y_pred, dtype=tf.float32)\n+        self.y_true = tf.constant(y_true)\n+\n+    def test_config(self):\n+        poisson_obj = metrics.Poisson(name=\"poisson\", dtype=tf.int32)\n+        self.assertEqual(poisson_obj.name, \"poisson\")\n+        self.assertEqual(poisson_obj._dtype, tf.int32)\n+\n+        poisson_obj2 = metrics.Poisson.from_config(poisson_obj.get_config())\n+        self.assertEqual(poisson_obj2.name, \"poisson\")\n+        self.assertEqual(poisson_obj2._dtype, tf.int32)\n+\n+    def test_unweighted(self):\n+        self.setup()\n+        poisson_obj = metrics.Poisson()\n+        self.evaluate(tf.compat.v1.variables_initializer(poisson_obj.variables))\n+\n+        update_op = poisson_obj.update_state(self.y_true, self.y_pred)\n+        self.evaluate(update_op)\n+        result = poisson_obj.result()\n+        expected_result = np.sum(self.expected_results) / self.batch_size\n+        self.assertAllClose(result, expected_result, atol=1e-3)\n+\n+    def test_weighted(self):\n+        self.setup()\n+        poisson_obj = metrics.Poisson()\n+        self.evaluate(tf.compat.v1.variables_initializer(poisson_obj.variables))\n+        sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))\n+\n+        result = poisson_obj(\n+            self.y_true, self.y_pred, sample_weight=sample_weight\n+        )\n+        sample_weight = np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape(\n+            (2, 3)\n+        )\n+        expected_result = np.multiply(self.expected_results, sample_weight)\n+        expected_result = np.sum(expected_result) / np.sum(sample_weight)\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class KLDivergenceTest(tf.test.TestCase):\n+    def setup(self):\n+        y_pred = np.asarray([0.4, 0.9, 0.12, 0.36, 0.3, 0.4]).reshape((2, 3))\n+        y_true = np.asarray([0.5, 0.8, 0.12, 0.7, 0.43, 0.8]).reshape((2, 3))\n+\n+        self.batch_size = 2\n+        self.expected_results = np.multiply(y_true, np.log(y_true / y_pred))\n+\n+        self.y_pred = tf.constant(y_pred, dtype=tf.float32)\n+        self.y_true = tf.constant(y_true)\n+\n+    def test_config(self):\n+        k_obj = metrics.KLDivergence(name=\"kld\", dtype=tf.int32)\n+        self.assertEqual(k_obj.name, \"kld\")\n+        self.assertEqual(k_obj._dtype, tf.int32)\n+\n+        k_obj2 = metrics.KLDivergence.from_config(k_obj.get_config())\n+        self.assertEqual(k_obj2.name, \"kld\")\n+        self.assertEqual(k_obj2._dtype, tf.int32)\n+\n+    def test_unweighted(self):\n+        self.setup()\n+        k_obj = metrics.KLDivergence()\n+        self.evaluate(tf.compat.v1.variables_initializer(k_obj.variables))\n+\n+        update_op = k_obj.update_state(self.y_true, self.y_pred)\n+        self.evaluate(update_op)\n+        result = k_obj.result()\n+        expected_result = np.sum(self.expected_results) / self.batch_size\n+        self.assertAllClose(result, expected_result, atol=1e-3)\n+\n+    def test_weighted(self):\n+        self.setup()\n+        k_obj = metrics.KLDivergence()\n+        self.evaluate(tf.compat.v1.variables_initializer(k_obj.variables))\n+\n+        sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))\n+        result = k_obj(self.y_true, self.y_pred, sample_weight=sample_weight)\n+\n+        sample_weight = np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape(\n+            (2, 3)\n+        )\n+        expected_result = np.multiply(self.expected_results, sample_weight)\n+        expected_result = np.sum(expected_result) / (1.2 + 3.4)\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class BinaryCrossentropyTest(tf.test.TestCase):\n+    def test_config(self):\n+        bce_obj = metrics.BinaryCrossentropy(\n+            name=\"bce\", dtype=tf.int32, label_smoothing=0.2\n+        )\n+        self.assertEqual(bce_obj.name, \"bce\")\n+        self.assertEqual(bce_obj._dtype, tf.int32)\n+\n+        old_config = bce_obj.get_config()\n+        self.assertAllClose(old_config[\"label_smoothing\"], 0.2, 1e-3)\n+\n+        # Check save and restore config\n+        bce_obj2 = metrics.BinaryCrossentropy.from_config(old_config)\n+        self.assertEqual(bce_obj2.name, \"bce\")\n+        self.assertEqual(bce_obj2._dtype, tf.int32)\n+        new_config = bce_obj2.get_config()\n+        self.assertDictEqual(old_config, new_config)\n+\n+    def test_unweighted(self):\n+        bce_obj = metrics.BinaryCrossentropy()\n+        self.evaluate(tf.compat.v1.variables_initializer(bce_obj.variables))\n+        y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n+        y_pred = np.asarray([1, 1, 1, 0], dtype=np.float32).reshape([2, 2])\n+        result = bce_obj(y_true, y_pred)\n+\n+        # EPSILON = 1e-7, y = y_true, y` = y_pred, Y_MAX = 0.9999999\n+        # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)\n+        # y` = [Y_MAX, Y_MAX, Y_MAX, EPSILON]\n+\n+        # Metric = -(y log(y` + EPSILON) + (1 - y) log(1 - y` + EPSILON))\n+        #        = [-log(Y_MAX + EPSILON), -log(1 - Y_MAX + EPSILON),\n+        #           -log(Y_MAX + EPSILON), -log(1)]\n+        #        = [(0 + 15.33) / 2, (0 + 0) / 2]\n+        # Reduced metric = 7.665 / 2\n+\n+        self.assertAllClose(self.evaluate(result), 3.833, atol=1e-3)\n+\n+    def test_unweighted_with_logits(self):\n+        bce_obj = metrics.BinaryCrossentropy(from_logits=True)\n+        self.evaluate(tf.compat.v1.variables_initializer(bce_obj.variables))\n+\n+        y_true = tf.constant([[1, 0, 1], [0, 1, 1]])\n+        y_pred = tf.constant([[100.0, -100.0, 100.0], [100.0, 100.0, -100.0]])\n+        result = bce_obj(y_true, y_pred)\n+\n+        # Metric = max(x, 0) - x * z + log(1 + exp(-abs(x)))\n+        #              (where x = logits and z = y_true)\n+        #        = [((100 - 100 * 1 + log(1 + exp(-100))) +\n+        #            (0 + 100 * 0 + log(1 + exp(-100))) +\n+        #            (100 - 100 * 1 + log(1 + exp(-100))),\n+        #           ((100 - 100 * 0 + log(1 + exp(-100))) +\n+        #            (100 - 100 * 1 + log(1 + exp(-100))) +\n+        #            (0 + 100 * 1 + log(1 + exp(-100))))]\n+        #        = [(0 + 0 + 0) / 3, 200 / 3]\n+        # Reduced metric = (0 + 66.666) / 2\n+\n+        self.assertAllClose(self.evaluate(result), 33.333, atol=1e-3)\n+\n+    def test_weighted(self):\n+        bce_obj = metrics.BinaryCrossentropy()\n+        self.evaluate(tf.compat.v1.variables_initializer(bce_obj.variables))\n+        y_true = np.asarray([1, 0, 1, 0]).reshape([2, 2])\n+        y_pred = np.asarray([1, 1, 1, 0], dtype=np.float32).reshape([2, 2])\n+        sample_weight = tf.constant([1.5, 2.0])\n+        result = bce_obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+        # EPSILON = 1e-7, y = y_true, y` = y_pred, Y_MAX = 0.9999999\n+        # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)\n+        # y` = [Y_MAX, Y_MAX, Y_MAX, EPSILON]\n+\n+        # Metric = -(y log(y` + EPSILON) + (1 - y) log(1 - y` + EPSILON))\n+        #        = [-log(Y_MAX + EPSILON), -log(1 - Y_MAX + EPSILON),\n+        #           -log(Y_MAX + EPSILON), -log(1)]\n+        #        = [(0 + 15.33) / 2, (0 + 0) / 2]\n+        # Weighted metric = [7.665 * 1.5, 0]\n+        # Reduced metric = 7.665 * 1.5 / (1.5 + 2)\n+\n+        self.assertAllClose(self.evaluate(result), 3.285, atol=1e-3)\n+\n+    def test_weighted_from_logits(self):\n+        bce_obj = metrics.BinaryCrossentropy(from_logits=True)\n+        self.evaluate(tf.compat.v1.variables_initializer(bce_obj.variables))\n+        y_true = tf.constant([[1, 0, 1], [0, 1, 1]])\n+        y_pred = tf.constant([[100.0, -100.0, 100.0], [100.0, 100.0, -100.0]])\n+        sample_weight = tf.constant([2.0, 2.5])\n+        result = bce_obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+        # Metric = max(x, 0) - x * z + log(1 + exp(-abs(x)))\n+        #              (where x = logits and z = y_true)\n+        #        = [(0 + 0 + 0) / 3, 200 / 3]\n+        # Weighted metric = [0, 66.666 * 2.5]\n+        # Reduced metric = 66.666 * 2.5 / (2 + 2.5)\n+\n+        self.assertAllClose(self.evaluate(result), 37.037, atol=1e-3)\n+\n+    def test_label_smoothing(self):\n+        logits = tf.constant(((100.0, -100.0, -100.0)))\n+        y_true = tf.constant(((1, 0, 1)))\n+        label_smoothing = 0.1\n+        # Metric: max(x, 0) - x * z + log(1 + exp(-abs(x)))\n+        #             (where x = logits and z = y_true)\n+        # Label smoothing: z' = z * (1 - L) + 0.5L\n+        # After label smoothing, label 1 becomes 1 - 0.5L\n+        #                        label 0 becomes 0.5L\n+        # Applying the above two fns to the given input:\n+        # (100 - 100 * (1 - 0.5 L)  + 0 +\n+        #  0   + 100 * (0.5 L)      + 0 +\n+        #  0   + 100 * (1 - 0.5 L)  + 0) * (1/3)\n+        #  = (100 + 50L) * 1/3\n+        bce_obj = metrics.BinaryCrossentropy(\n+            from_logits=True, label_smoothing=label_smoothing\n+        )\n+        self.evaluate(tf.compat.v1.variables_initializer(bce_obj.variables))\n+        result = bce_obj(y_true, logits)\n+        expected_value = (100.0 + 50.0 * label_smoothing) / 3.0\n+        self.assertAllClose(expected_value, self.evaluate(result), atol=1e-3)\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class CategoricalCrossentropyTest(tf.test.TestCase):\n+    def test_config(self):\n+        cce_obj = metrics.CategoricalCrossentropy(\n+            name=\"cce\", dtype=tf.int32, label_smoothing=0.2\n+        )\n+        self.assertEqual(cce_obj.name, \"cce\")\n+        self.assertEqual(cce_obj._dtype, tf.int32)\n+\n+        old_config = cce_obj.get_config()\n+        self.assertAllClose(old_config[\"label_smoothing\"], 0.2, 1e-3)\n+\n+        # Check save and restore config\n+        cce_obj2 = metrics.CategoricalCrossentropy.from_config(old_config)\n+        self.assertEqual(cce_obj2.name, \"cce\")\n+        self.assertEqual(cce_obj2._dtype, tf.int32)\n+        new_config = cce_obj2.get_config()\n+        self.assertDictEqual(old_config, new_config)\n+\n+    def test_unweighted(self):\n+        cce_obj = metrics.CategoricalCrossentropy()\n+        self.evaluate(tf.compat.v1.variables_initializer(cce_obj.variables))\n+\n+        y_true = np.asarray([[0, 1, 0], [0, 0, 1]])\n+        y_pred = np.asarray([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n+        result = cce_obj(y_true, y_pred)\n+\n+        # EPSILON = 1e-7, y = y_true, y` = y_pred\n+        # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)\n+        # y` = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]\n+\n+        # Metric = -sum(y * log(y'), axis = -1)\n+        #        = -((log 0.95), (log 0.1))\n+        #        = [0.051, 2.302]\n+        # Reduced metric = (0.051 + 2.302) / 2\n+\n+        self.assertAllClose(self.evaluate(result), 1.176, atol=1e-3)\n+\n+    def test_unweighted_from_logits(self):\n+        cce_obj = metrics.CategoricalCrossentropy(from_logits=True)\n+        self.evaluate(tf.compat.v1.variables_initializer(cce_obj.variables))\n+\n+        y_true = np.asarray([[0, 1, 0], [0, 0, 1]])\n+        logits = np.asarray([[1, 9, 0], [1, 8, 1]], dtype=np.float32)\n+        result = cce_obj(y_true, logits)\n+\n+        # softmax = exp(logits) / sum(exp(logits), axis=-1)\n+        # xent = -sum(labels * log(softmax), 1)\n+\n+        # exp(logits) = [[2.718, 8103.084, 1], [2.718, 2980.958, 2.718]]\n+        # sum(exp(logits), axis=-1) = [8106.802, 2986.394]\n+        # softmax = [[0.00033, 0.99954, 0.00012], [0.00091, 0.99817, 0.00091]]\n+        # log(softmax) = [[-8.00045, -0.00045, -9.00045],\n+        #                 [-7.00182, -0.00182, -7.00182]]\n+        # labels * log(softmax) = [[0, -0.00045, 0], [0, 0, -7.00182]]\n+        # xent = [0.00045, 7.00182]\n+        # Reduced xent = (0.00045 + 7.00182) / 2\n+\n+        self.assertAllClose(self.evaluate(result), 3.5011, atol=1e-3)\n+\n+    def test_weighted(self):\n+        cce_obj = metrics.CategoricalCrossentropy()\n+        self.evaluate(tf.compat.v1.variables_initializer(cce_obj.variables))\n+\n+        y_true = np.asarray([[0, 1, 0], [0, 0, 1]])\n+        y_pred = np.asarray([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n+        sample_weight = tf.constant([1.5, 2.0])\n+        result = cce_obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+        # EPSILON = 1e-7, y = y_true, y` = y_pred\n+        # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)\n+        # y` = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]\n+\n+        # Metric = -sum(y * log(y'), axis = -1)\n+        #        = -((log 0.95), (log 0.1))\n+        #        = [0.051, 2.302]\n+        # Weighted metric = [0.051 * 1.5, 2.302 * 2.]\n+        # Reduced metric = (0.051 * 1.5 + 2.302 * 2.) / 3.5\n+\n+        self.assertAllClose(self.evaluate(result), 1.338, atol=1e-3)\n+\n+    def test_weighted_from_logits(self):\n+        cce_obj = metrics.CategoricalCrossentropy(from_logits=True)\n+        self.evaluate(tf.compat.v1.variables_initializer(cce_obj.variables))\n+\n+        y_true = np.asarray([[0, 1, 0], [0, 0, 1]])\n+        logits = np.asarray([[1, 9, 0], [1, 8, 1]], dtype=np.float32)\n+        sample_weight = tf.constant([1.5, 2.0])\n+        result = cce_obj(y_true, logits, sample_weight=sample_weight)\n+\n+        # softmax = exp(logits) / sum(exp(logits), axis=-1)\n+        # xent = -sum(labels * log(softmax), 1)\n+        # xent = [0.00045, 7.00182]\n+        # weighted xent = [0.000675, 14.00364]\n+        # Reduced xent = (0.000675 + 14.00364) / (1.5 + 2)\n+\n+        self.assertAllClose(self.evaluate(result), 4.0012, atol=1e-3)\n+\n+    def test_label_smoothing(self):\n+        y_true = np.asarray([[0, 1, 0], [0, 0, 1]])\n+        logits = np.asarray([[1, 9, 0], [1, 8, 1]], dtype=np.float32)\n+        label_smoothing = 0.1\n+\n+        # Label smoothing: z' = z * (1 - L) + L/n,\n+        #     where L = label smoothing value and n = num classes\n+        # Label value 1 becomes: 1 - L + L/n\n+        # Label value 0 becomes: L/n\n+        # y_true with label_smoothing = [[0.0333, 0.9333, 0.0333],\n+        #                               [0.0333, 0.0333, 0.9333]]\n+\n+        # softmax = exp(logits) / sum(exp(logits), axis=-1)\n+        # xent = -sum(labels * log(softmax), 1)\n+        # log(softmax) = [[-8.00045, -0.00045, -9.00045],\n+        #                 [-7.00182, -0.00182, -7.00182]]\n+        # labels * log(softmax) = [[-0.26641, -0.00042, -0.29971],\n+        #                          [-0.23316, -0.00006, -6.53479]]\n+        # xent = [0.56654, 6.76801]\n+        # Reduced xent = (0.56654 + 6.76801) / 2\n+\n+        cce_obj = metrics.CategoricalCrossentropy(\n+            from_logits=True, label_smoothing=label_smoothing\n+        )\n+        self.evaluate(tf.compat.v1.variables_initializer(cce_obj.variables))\n+        loss = cce_obj(y_true, logits)\n+        self.assertAllClose(self.evaluate(loss), 3.667, atol=1e-3)\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class SparseCategoricalCrossentropyTest(tf.test.TestCase):\n+    def test_config(self):\n+        scce_obj = metrics.SparseCategoricalCrossentropy(\n+            name=\"scce\", dtype=tf.int32\n+        )\n+        self.assertEqual(scce_obj.name, \"scce\")\n+        self.assertEqual(scce_obj.dtype, tf.int32)\n+        old_config = scce_obj.get_config()\n+        self.assertDictEqual(old_config, json.loads(json.dumps(old_config)))\n+\n+        # Check save and restore config\n+        scce_obj2 = metrics.SparseCategoricalCrossentropy.from_config(\n+            old_config\n+        )\n+        self.assertEqual(scce_obj2.name, \"scce\")\n+        self.assertEqual(scce_obj2.dtype, tf.int32)\n+        new_config = scce_obj2.get_config()\n+        self.assertDictEqual(old_config, new_config)\n+\n+    def test_unweighted(self):\n+        scce_obj = metrics.SparseCategoricalCrossentropy()\n+        self.evaluate(tf.compat.v1.variables_initializer(scce_obj.variables))\n+\n+        y_true = np.asarray([1, 2])\n+        y_pred = np.asarray([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n+        result = scce_obj(y_true, y_pred)\n+\n+        # EPSILON = 1e-7, y = y_true, y` = y_pred\n+        # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)\n+        # y` = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]\n+        # logits = log(y`) =  [[-2.9957, -0.0513, -16.1181],\n+        #                      [-2.3026, -0.2231, -2.3026]]\n+\n+        # softmax = exp(logits) / sum(exp(logits), axis=-1)\n+        # y = one_hot(y) = [[0, 1, 0], [0, 0, 1]]\n+        # xent = -sum(y * log(softmax), 1)\n+\n+        # exp(logits) = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]\n+        # sum(exp(logits), axis=-1) = [1, 1]\n+        # softmax = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]\n+        # log(softmax) = [[-2.9957, -0.0513, -16.1181],\n+        #                 [-2.3026, -0.2231, -2.3026]]\n+        # y * log(softmax) = [[0, -0.0513, 0], [0, 0, -2.3026]]\n+        # xent = [0.0513, 2.3026]\n+        # Reduced xent = (0.0513 + 2.3026) / 2\n+\n+        self.assertAllClose(self.evaluate(result), 1.176, atol=1e-3)\n+\n+    def test_unweighted_ignore_class(self):\n+        scce_obj = metrics.SparseCategoricalCrossentropy(ignore_class=-1)\n+        self.evaluate(tf.compat.v1.variables_initializer(scce_obj.variables))\n+\n+        y_true = np.asarray([-1, 2])\n+        y_pred = np.asarray([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n+        result = scce_obj(y_true, y_pred)\n+\n+        self.assertAllClose(self.evaluate(result), 2.3026, atol=1e-3)\n+\n+    def test_unweighted_from_logits(self):\n+        scce_obj = metrics.SparseCategoricalCrossentropy(from_logits=True)\n+        self.evaluate(tf.compat.v1.variables_initializer(scce_obj.variables))\n+\n+        y_true = np.asarray([1, 2])\n+        logits = np.asarray([[1, 9, 0], [1, 8, 1]], dtype=np.float32)\n+        result = scce_obj(y_true, logits)\n+\n+        # softmax = exp(logits) / sum(exp(logits), axis=-1)\n+        # y_true = one_hot(y_true) = [[0, 1, 0], [0, 0, 1]]\n+        # xent = -sum(y_true * log(softmax), 1)\n+\n+        # exp(logits) = [[2.718, 8103.084, 1], [2.718, 2980.958, 2.718]]\n+        # sum(exp(logits), axis=-1) = [8106.802, 2986.394]\n+        # softmax = [[0.00033, 0.99954, 0.00012], [0.00091, 0.99817, 0.00091]]\n+        # log(softmax) = [[-8.00045, -0.00045, -9.00045],\n+        #                 [-7.00182, -0.00182, -7.00182]]\n+        # y_true * log(softmax) = [[0, -0.00045, 0], [0, 0, -7.00182]]\n+        # xent = [0.00045, 7.00182]\n+        # Reduced xent = (0.00045 + 7.00182) / 2\n+\n+        self.assertAllClose(self.evaluate(result), 3.5011, atol=1e-3)\n+\n+    def test_weighted(self):\n+        scce_obj = metrics.SparseCategoricalCrossentropy()\n+        self.evaluate(tf.compat.v1.variables_initializer(scce_obj.variables))\n+\n+        y_true = np.asarray([1, 2])\n+        y_pred = np.asarray([[0.05, 0.95, 0], [0.1, 0.8, 0.1]])\n+        sample_weight = tf.constant([1.5, 2.0])\n+        result = scce_obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+        # EPSILON = 1e-7, y = y_true, y` = y_pred\n+        # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)\n+        # y` = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]\n+        # logits = log(y`) =  [[-2.9957, -0.0513, -16.1181],\n+        #                      [-2.3026, -0.2231, -2.3026]]\n+\n+        # softmax = exp(logits) / sum(exp(logits), axis=-1)\n+        # y = one_hot(y) = [[0, 1, 0], [0, 0, 1]]\n+        # xent = -sum(y * log(softmax), 1)\n+\n+        # exp(logits) = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]\n+        # sum(exp(logits), axis=-1) = [1, 1]\n+        # softmax = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]\n+        # log(softmax) = [[-2.9957, -0.0513, -16.1181],\n+        #                 [-2.3026, -0.2231, -2.3026]]\n+        # y * log(softmax) = [[0, -0.0513, 0], [0, 0, -2.3026]]\n+        # xent = [0.0513, 2.3026]\n+        # Weighted xent = [0.051 * 1.5, 2.302 * 2.]\n+        # Reduced xent = (0.051 * 1.5 + 2.302 * 2.) / 3.5\n+\n+        self.assertAllClose(self.evaluate(result), 1.338, atol=1e-3)\n+\n+    def test_weighted_ignore_class(self):\n+        scce_obj = metrics.SparseCategoricalCrossentropy(ignore_class=-1)\n+        self.evaluate(tf.compat.v1.variables_initializer(scce_obj.variables))\n+\n+        y_true = np.asarray([1, 2, -1])\n+        y_pred = np.asarray([[0.05, 0.95, 0], [0.1, 0.8, 0.1], [0.1, 0.8, 0.1]])\n+        sample_weight = tf.constant([1.5, 2.0, 1.5])\n+        result = scce_obj(y_true, y_pred, sample_weight=sample_weight)\n+\n+        self.assertAllClose(self.evaluate(result), 1.338, atol=1e-3)\n+\n+    def test_weighted_from_logits(self):\n+        scce_obj = metrics.SparseCategoricalCrossentropy(from_logits=True)\n+        self.evaluate(tf.compat.v1.variables_initializer(scce_obj.variables))\n+\n+        y_true = np.asarray([1, 2])\n+        logits = np.asarray([[1, 9, 0], [1, 8, 1]], dtype=np.float32)\n+        sample_weight = tf.constant([1.5, 2.0])\n+        result = scce_obj(y_true, logits, sample_weight=sample_weight)\n+\n+        # softmax = exp(logits) / sum(exp(logits), axis=-1)\n+        # y_true = one_hot(y_true) = [[0, 1, 0], [0, 0, 1]]\n+        # xent = -sum(y_true * log(softmax), 1)\n+        # xent = [0.00045, 7.00182]\n+        # weighted xent = [0.000675, 14.00364]\n+        # Reduced xent = (0.000675 + 14.00364) / (1.5 + 2)\n+\n+        self.assertAllClose(self.evaluate(result), 4.0012, atol=1e-3)\n+\n+    def test_axis(self):\n+        scce_obj = metrics.SparseCategoricalCrossentropy(axis=0)\n+        self.evaluate(tf.compat.v1.variables_initializer(scce_obj.variables))\n+\n+        y_true = np.asarray([1, 2])\n+        y_pred = np.asarray([[0.05, 0.1], [0.95, 0.8], [0, 0.1]])\n+        result = scce_obj(y_true, y_pred)\n+\n+        # EPSILON = 1e-7, y = y_true, y` = y_pred\n+        # y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)\n+        # y` = [[0.05, 0.1], [0.95, 0.8], [EPSILON, 0.1]]\n+        # logits = log(y`) =  [[-2.9957, -2.3026],\n+        #                      [-0.0513, -0.2231],\n+        #                      [-16.1181, -2.3026]]\n+\n+        # softmax = exp(logits) / sum(exp(logits), axis=-1)\n+        # y = one_hot(y) = [[0, 0], [1, 0], [0, 1]]\n+        # xent = -sum(y * log(softmax), 1)\n+\n+        # exp(logits) = [[0.05, 0.1], [0.95, 0.8], [EPSILON, 0.1]]\n+        # sum(exp(logits)) = [1, 1]\n+        # softmax = [[0.05, 0.1], [0.95, 0.8], [EPSILON, 0.1]]\n+        # log(softmax) = [[-2.9957, -2.3026],\n+        #                 [-0.0513, -0.2231],\n+        #                 [-16.1181, -2.3026]]\n+        # y * log(softmax) = [[0, 0], [-0.0513, 0], [0, -2.3026]]\n+        # xent = [0.0513, 2.3026]\n+        # Reduced xent = (0.0513 + 2.3026) / 2\n+\n+        self.assertAllClose(self.evaluate(result), 1.176, atol=1e-3)\n+\n+\n+class BinaryTruePositives(metrics.Metric):\n+    def __init__(self, name=\"binary_true_positives\", **kwargs):\n+        super().__init__(name=name, **kwargs)\n+        self.true_positives = self.add_weight(name=\"tp\", initializer=\"zeros\")\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        y_true = tf.cast(y_true, tf.bool)\n+        y_pred = tf.cast(y_pred, tf.bool)\n+\n+        values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))\n+        values = tf.cast(values, self.dtype)\n+        if sample_weight is not None:\n+            sample_weight = tf.cast(sample_weight, dtype=self.dtype)\n+            sample_weight = tf.__internal__.ops.broadcast_weights(\n+                sample_weight, values\n+            )\n+            values = tf.multiply(values, sample_weight)\n+        self.true_positives.assign_add(tf.reduce_sum(values))\n+\n+    def result(self):\n+        return self.true_positives\n+\n+\n+if __name__ == \"__main__\":\n+    tf.test.main()\n\n@@ -0,0 +1,429 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Regression metrics, e.g. MAE/MSE/etc.\"\"\"\n+\n+import tensorflow.compat.v2 as tf\n+\n+from keras import backend\n+from keras.dtensor import utils as dtensor_utils\n+from keras.losses import logcosh\n+from keras.losses import mean_absolute_error\n+from keras.losses import mean_absolute_percentage_error\n+from keras.losses import mean_squared_error\n+from keras.losses import mean_squared_logarithmic_error\n+from keras.metrics import base_metric\n+from keras.utils import losses_utils\n+from keras.utils import metrics_utils\n+from keras.utils.tf_utils import is_tensor_or_variable\n+\n+# isort: off\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export(\"keras.metrics.MeanRelativeError\")\n+class MeanRelativeError(base_metric.Mean):\n+    \"\"\"Computes the mean relative error by normalizing with the given values.\n+\n+    This metric creates two local variables, `total` and `count` that are used\n+    to compute the mean relative error. This is weighted by `sample_weight`, and\n+    it is ultimately returned as `mean_relative_error`: an idempotent operation\n+    that simply divides `total` by `count`.\n+\n+    If `sample_weight` is `None`, weights default to 1.\n+    Use `sample_weight` of 0 to mask values.\n+\n+    Args:\n+      normalizer: The normalizer values with same shape as predictions.\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.MeanRelativeError(normalizer=[1, 3, 2, 3])\n+    >>> m.update_state([1, 3, 2, 3], [2, 4, 6, 8])\n+\n+    >>> # metric = mean(|y_pred - y_true| / normalizer)\n+    >>> #        = mean([1, 1, 4, 5] / [1, 3, 2, 3]) = mean([1, 1/3, 2, 5/3])\n+    >>> #        = 5/4 = 1.25\n+    >>> m.result().numpy()\n+    1.25\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+      optimizer='sgd',\n+      loss='mse',\n+      metrics=[tf.keras.metrics.MeanRelativeError(normalizer=[1, 3])])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(self, normalizer, name=None, dtype=None):\n+        super().__init__(name=name, dtype=dtype)\n+        normalizer = tf.cast(normalizer, self._dtype)\n+        self.normalizer = normalizer\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        \"\"\"Accumulates metric statistics.\n+\n+        Args:\n+          y_true: The ground truth values.\n+          y_pred: The predicted values.\n+          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n+            and must be broadcastable to `y_true`.\n+\n+        Returns:\n+          Update op.\n+        \"\"\"\n+        y_true = tf.cast(y_true, self._dtype)\n+        y_pred = tf.cast(y_pred, self._dtype)\n+        [\n+            y_pred,\n+            y_true,\n+        ], sample_weight = metrics_utils.ragged_assert_compatible_and_get_flat_values(  # noqa: E501\n+            [y_pred, y_true], sample_weight\n+        )\n+        y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(\n+            y_pred, y_true\n+        )\n+\n+        y_pred, self.normalizer = losses_utils.remove_squeezable_dimensions(\n+            y_pred, self.normalizer\n+        )\n+        y_pred.shape.assert_is_compatible_with(y_true.shape)\n+        relative_errors = tf.math.divide_no_nan(\n+            tf.abs(y_true - y_pred), self.normalizer\n+        )\n+\n+        return super().update_state(\n+            relative_errors, sample_weight=sample_weight\n+        )\n+\n+    def get_config(self):\n+        n = self.normalizer\n+        config = {\n+            \"normalizer\": backend.eval(n) if is_tensor_or_variable(n) else n\n+        }\n+        base_config = super().get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n+@keras_export(\"keras.metrics.CosineSimilarity\")\n+class CosineSimilarity(base_metric.MeanMetricWrapper):\n+    \"\"\"Computes the cosine similarity between the labels and predictions.\n+\n+    `cosine similarity = (a . b) / ||a|| ||b||`\n+\n+    See: [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity).\n+\n+    This metric keeps the average cosine similarity between `predictions` and\n+    `labels` over a stream of data.\n+\n+    Args:\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+      axis: (Optional) Defaults to -1. The dimension along which the cosine\n+        similarity is computed.\n+\n+    Standalone usage:\n+\n+    >>> # l2_norm(y_true) = [[0., 1.], [1./1.414, 1./1.414]]\n+    >>> # l2_norm(y_pred) = [[1., 0.], [1./1.414, 1./1.414]]\n+    >>> # l2_norm(y_true) . l2_norm(y_pred) = [[0., 0.], [0.5, 0.5]]\n+    >>> # result = mean(sum(l2_norm(y_true) . l2_norm(y_pred), axis=1))\n+    >>> #        = ((0. + 0.) +  (0.5 + 0.5)) / 2\n+    >>> m = tf.keras.metrics.CosineSimilarity(axis=1)\n+    >>> m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]])\n+    >>> m.result().numpy()\n+    0.49999997\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]],\n+    ...                sample_weight=[0.3, 0.7])\n+    >>> m.result().numpy()\n+    0.6999999\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[tf.keras.metrics.CosineSimilarity(axis=1)])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(self, name=\"cosine_similarity\", dtype=None, axis=-1):\n+        super().__init__(cosine_similarity, name, dtype=dtype, axis=axis)\n+\n+\n+@keras_export(\"keras.metrics.MeanAbsoluteError\")\n+class MeanAbsoluteError(base_metric.MeanMetricWrapper):\n+    \"\"\"Computes the mean absolute error between the labels and predictions.\n+\n+    Args:\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.MeanAbsoluteError()\n+    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n+    >>> m.result().numpy()\n+    0.25\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n+    ...                sample_weight=[1, 0])\n+    >>> m.result().numpy()\n+    0.5\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[tf.keras.metrics.MeanAbsoluteError()])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(self, name=\"mean_absolute_error\", dtype=None):\n+        super().__init__(mean_absolute_error, name, dtype=dtype)\n+\n+\n+@keras_export(\"keras.metrics.MeanAbsolutePercentageError\")\n+class MeanAbsolutePercentageError(base_metric.MeanMetricWrapper):\n+    \"\"\"Computes the mean absolute percentage error between `y_true` and\n+    `y_pred`.\n+\n+    Args:\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.MeanAbsolutePercentageError()\n+    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n+    >>> m.result().numpy()\n+    250000000.0\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n+    ...                sample_weight=[1, 0])\n+    >>> m.result().numpy()\n+    500000000.0\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(self, name=\"mean_absolute_percentage_error\", dtype=None):\n+        super().__init__(mean_absolute_percentage_error, name, dtype=dtype)\n+\n+\n+@keras_export(\"keras.metrics.MeanSquaredError\")\n+class MeanSquaredError(base_metric.MeanMetricWrapper):\n+    \"\"\"Computes the mean squared error between `y_true` and `y_pred`.\n+\n+    Args:\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.MeanSquaredError()\n+    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n+    >>> m.result().numpy()\n+    0.25\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n+    ...                sample_weight=[1, 0])\n+    >>> m.result().numpy()\n+    0.5\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[tf.keras.metrics.MeanSquaredError()])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(self, name=\"mean_squared_error\", dtype=None):\n+        super().__init__(mean_squared_error, name, dtype=dtype)\n+\n+\n+@keras_export(\"keras.metrics.MeanSquaredLogarithmicError\")\n+class MeanSquaredLogarithmicError(base_metric.MeanMetricWrapper):\n+    \"\"\"Computes the mean squared logarithmic error between `y_true` and\n+    `y_pred`.\n+\n+    Args:\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.MeanSquaredLogarithmicError()\n+    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n+    >>> m.result().numpy()\n+    0.12011322\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n+    ...                sample_weight=[1, 0])\n+    >>> m.result().numpy()\n+    0.24022643\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[tf.keras.metrics.MeanSquaredLogarithmicError()])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(self, name=\"mean_squared_logarithmic_error\", dtype=None):\n+        super().__init__(mean_squared_logarithmic_error, name, dtype=dtype)\n+\n+\n+@keras_export(\"keras.metrics.RootMeanSquaredError\")\n+class RootMeanSquaredError(base_metric.Mean):\n+    \"\"\"Computes root mean squared error metric between `y_true` and `y_pred`.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.RootMeanSquaredError()\n+    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n+    >>> m.result().numpy()\n+    0.5\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n+    ...                sample_weight=[1, 0])\n+    >>> m.result().numpy()\n+    0.70710677\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(\n+        optimizer='sgd',\n+        loss='mse',\n+        metrics=[tf.keras.metrics.RootMeanSquaredError()])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(self, name=\"root_mean_squared_error\", dtype=None):\n+        super().__init__(name, dtype=dtype)\n+\n+    def update_state(self, y_true, y_pred, sample_weight=None):\n+        \"\"\"Accumulates root mean squared error statistics.\n+\n+        Args:\n+          y_true: The ground truth values.\n+          y_pred: The predicted values.\n+          sample_weight: Optional weighting of each example. Defaults to 1. Can\n+            be a `Tensor` whose rank is either 0, or the same rank as `y_true`,\n+            and must be broadcastable to `y_true`.\n+\n+        Returns:\n+          Update op.\n+        \"\"\"\n+        y_true = tf.cast(y_true, self._dtype)\n+        y_pred = tf.cast(y_pred, self._dtype)\n+        y_pred, y_true = losses_utils.squeeze_or_expand_dimensions(\n+            y_pred, y_true\n+        )\n+        error_sq = tf.math.squared_difference(y_pred, y_true)\n+        return super().update_state(error_sq, sample_weight=sample_weight)\n+\n+    def result(self):\n+        return tf.sqrt(tf.math.divide_no_nan(self.total, self.count))\n+\n+\n+@keras_export(\"keras.metrics.LogCoshError\")\n+class LogCoshError(base_metric.MeanMetricWrapper):\n+    \"\"\"Computes the logarithm of the hyperbolic cosine of the prediction error.\n+\n+    `logcosh = log((exp(x) + exp(-x))/2)`, where x is the error (y_pred -\n+    y_true)\n+\n+    Args:\n+      name: (Optional) string name of the metric instance.\n+      dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    >>> m = tf.keras.metrics.LogCoshError()\n+    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])\n+    >>> m.result().numpy()\n+    0.10844523\n+\n+    >>> m.reset_state()\n+    >>> m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]],\n+    ...                sample_weight=[1, 0])\n+    >>> m.result().numpy()\n+    0.21689045\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model.compile(optimizer='sgd',\n+                  loss='mse',\n+                  metrics=[tf.keras.metrics.LogCoshError()])\n+    ```\n+    \"\"\"\n+\n+    @dtensor_utils.inject_mesh\n+    def __init__(self, name=\"logcosh\", dtype=None):\n+        super().__init__(logcosh, name, dtype=dtype)\n+\n+\n+def cosine_similarity(y_true, y_pred, axis=-1):\n+    \"\"\"Computes the cosine similarity between labels and predictions.\n+\n+    Args:\n+      y_true: The ground truth values.\n+      y_pred: The prediction values.\n+      axis: (Optional) Defaults to -1. The dimension along which the cosine\n+        similarity is computed.\n+\n+    Returns:\n+      Cosine similarity value.\n+    \"\"\"\n+    y_true = tf.linalg.l2_normalize(y_true, axis=axis)\n+    y_pred = tf.linalg.l2_normalize(y_pred, axis=axis)\n+    return tf.reduce_sum(y_true * y_pred, axis=axis)\n\n@@ -0,0 +1,400 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for Keras metrics.\"\"\"\n+\n+import math\n+\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+\n+from keras import metrics\n+from keras.testing_infra import test_combinations\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class CosineSimilarityTest(tf.test.TestCase):\n+    def l2_norm(self, x, axis):\n+        epsilon = 1e-12\n+        square_sum = np.sum(np.square(x), axis=axis, keepdims=True)\n+        x_inv_norm = 1 / np.sqrt(np.maximum(square_sum, epsilon))\n+        return np.multiply(x, x_inv_norm)\n+\n+    def setup(self, axis=1):\n+        self.np_y_true = np.asarray([[1, 9, 2], [-5, -2, 6]], dtype=np.float32)\n+        self.np_y_pred = np.asarray([[4, 8, 12], [8, 1, 3]], dtype=np.float32)\n+\n+        y_true = self.l2_norm(self.np_y_true, axis)\n+        y_pred = self.l2_norm(self.np_y_pred, axis)\n+        self.expected_loss = np.sum(np.multiply(y_true, y_pred), axis=(axis,))\n+\n+        self.y_true = tf.constant(self.np_y_true)\n+        self.y_pred = tf.constant(self.np_y_pred)\n+\n+    def test_config(self):\n+        cosine_obj = metrics.CosineSimilarity(\n+            axis=2, name=\"my_cos\", dtype=tf.int32\n+        )\n+        self.assertEqual(cosine_obj.name, \"my_cos\")\n+        self.assertEqual(cosine_obj._dtype, tf.int32)\n+\n+        # Check save and restore config\n+        cosine_obj2 = metrics.CosineSimilarity.from_config(\n+            cosine_obj.get_config()\n+        )\n+        self.assertEqual(cosine_obj2.name, \"my_cos\")\n+        self.assertEqual(cosine_obj2._dtype, tf.int32)\n+\n+    def test_unweighted(self):\n+        self.setup()\n+        cosine_obj = metrics.CosineSimilarity()\n+        self.evaluate(tf.compat.v1.variables_initializer(cosine_obj.variables))\n+        loss = cosine_obj(self.y_true, self.y_pred)\n+        expected_loss = np.mean(self.expected_loss)\n+        self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)\n+\n+    def test_weighted(self):\n+        self.setup()\n+        cosine_obj = metrics.CosineSimilarity()\n+        self.evaluate(tf.compat.v1.variables_initializer(cosine_obj.variables))\n+        sample_weight = np.asarray([1.2, 3.4])\n+        loss = cosine_obj(\n+            self.y_true, self.y_pred, sample_weight=tf.constant(sample_weight)\n+        )\n+        expected_loss = np.sum(self.expected_loss * sample_weight) / np.sum(\n+            sample_weight\n+        )\n+        self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)\n+\n+    def test_axis(self):\n+        self.setup(axis=1)\n+        cosine_obj = metrics.CosineSimilarity(axis=1)\n+        self.evaluate(tf.compat.v1.variables_initializer(cosine_obj.variables))\n+        loss = cosine_obj(self.y_true, self.y_pred)\n+        expected_loss = np.mean(self.expected_loss)\n+        self.assertAlmostEqual(self.evaluate(loss), expected_loss, 3)\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class MeanAbsoluteErrorTest(tf.test.TestCase):\n+    def test_config(self):\n+        mae_obj = metrics.MeanAbsoluteError(name=\"my_mae\", dtype=tf.int32)\n+        self.assertEqual(mae_obj.name, \"my_mae\")\n+        self.assertEqual(mae_obj._dtype, tf.int32)\n+\n+        # Check save and restore config\n+        mae_obj2 = metrics.MeanAbsoluteError.from_config(mae_obj.get_config())\n+        self.assertEqual(mae_obj2.name, \"my_mae\")\n+        self.assertEqual(mae_obj2._dtype, tf.int32)\n+\n+    def test_unweighted(self):\n+        mae_obj = metrics.MeanAbsoluteError()\n+        self.evaluate(tf.compat.v1.variables_initializer(mae_obj.variables))\n+        y_true = tf.constant(\n+            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        )\n+        y_pred = tf.constant(\n+            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+        )\n+\n+        update_op = mae_obj.update_state(y_true, y_pred)\n+        self.evaluate(update_op)\n+        result = mae_obj.result()\n+        self.assertAllClose(0.5, result, atol=1e-5)\n+\n+    def test_weighted(self):\n+        mae_obj = metrics.MeanAbsoluteError()\n+        self.evaluate(tf.compat.v1.variables_initializer(mae_obj.variables))\n+        y_true = tf.constant(\n+            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        )\n+        y_pred = tf.constant(\n+            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+        )\n+        sample_weight = tf.constant((1.0, 1.5, 2.0, 2.5))\n+        result = mae_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(0.54285, self.evaluate(result), atol=1e-5)\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class MeanAbsolutePercentageErrorTest(tf.test.TestCase):\n+    def test_config(self):\n+        mape_obj = metrics.MeanAbsolutePercentageError(\n+            name=\"my_mape\", dtype=tf.int32\n+        )\n+        self.assertEqual(mape_obj.name, \"my_mape\")\n+        self.assertEqual(mape_obj._dtype, tf.int32)\n+\n+        # Check save and restore config\n+        mape_obj2 = metrics.MeanAbsolutePercentageError.from_config(\n+            mape_obj.get_config()\n+        )\n+        self.assertEqual(mape_obj2.name, \"my_mape\")\n+        self.assertEqual(mape_obj2._dtype, tf.int32)\n+\n+    def test_unweighted(self):\n+        mape_obj = metrics.MeanAbsolutePercentageError()\n+        self.evaluate(tf.compat.v1.variables_initializer(mape_obj.variables))\n+        y_true = tf.constant(\n+            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        )\n+        y_pred = tf.constant(\n+            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+        )\n+\n+        update_op = mape_obj.update_state(y_true, y_pred)\n+        self.evaluate(update_op)\n+        result = mape_obj.result()\n+        self.assertAllClose(35e7, result, atol=1e-5)\n+\n+    def test_weighted(self):\n+        mape_obj = metrics.MeanAbsolutePercentageError()\n+        self.evaluate(tf.compat.v1.variables_initializer(mape_obj.variables))\n+        y_true = tf.constant(\n+            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        )\n+        y_pred = tf.constant(\n+            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+        )\n+        sample_weight = tf.constant((1.0, 1.5, 2.0, 2.5))\n+        result = mape_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(40e7, self.evaluate(result), atol=1e-5)\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class MeanSquaredErrorTest(tf.test.TestCase):\n+    def test_config(self):\n+        mse_obj = metrics.MeanSquaredError(name=\"my_mse\", dtype=tf.int32)\n+        self.assertEqual(mse_obj.name, \"my_mse\")\n+        self.assertEqual(mse_obj._dtype, tf.int32)\n+\n+        # Check save and restore config\n+        mse_obj2 = metrics.MeanSquaredError.from_config(mse_obj.get_config())\n+        self.assertEqual(mse_obj2.name, \"my_mse\")\n+        self.assertEqual(mse_obj2._dtype, tf.int32)\n+\n+    def test_unweighted(self):\n+        mse_obj = metrics.MeanSquaredError()\n+        self.evaluate(tf.compat.v1.variables_initializer(mse_obj.variables))\n+        y_true = tf.constant(\n+            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        )\n+        y_pred = tf.constant(\n+            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+        )\n+\n+        update_op = mse_obj.update_state(y_true, y_pred)\n+        self.evaluate(update_op)\n+        result = mse_obj.result()\n+        self.assertAllClose(0.5, result, atol=1e-5)\n+\n+    def test_weighted(self):\n+        mse_obj = metrics.MeanSquaredError()\n+        self.evaluate(tf.compat.v1.variables_initializer(mse_obj.variables))\n+        y_true = tf.constant(\n+            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        )\n+        y_pred = tf.constant(\n+            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+        )\n+        sample_weight = tf.constant((1.0, 1.5, 2.0, 2.5))\n+        result = mse_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(0.54285, self.evaluate(result), atol=1e-5)\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class MeanSquaredLogarithmicErrorTest(tf.test.TestCase):\n+    def test_config(self):\n+        msle_obj = metrics.MeanSquaredLogarithmicError(\n+            name=\"my_msle\", dtype=tf.int32\n+        )\n+        self.assertEqual(msle_obj.name, \"my_msle\")\n+        self.assertEqual(msle_obj._dtype, tf.int32)\n+\n+        # Check save and restore config\n+        msle_obj2 = metrics.MeanSquaredLogarithmicError.from_config(\n+            msle_obj.get_config()\n+        )\n+        self.assertEqual(msle_obj2.name, \"my_msle\")\n+        self.assertEqual(msle_obj2._dtype, tf.int32)\n+\n+    def test_unweighted(self):\n+        msle_obj = metrics.MeanSquaredLogarithmicError()\n+        self.evaluate(tf.compat.v1.variables_initializer(msle_obj.variables))\n+        y_true = tf.constant(\n+            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        )\n+        y_pred = tf.constant(\n+            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+        )\n+\n+        update_op = msle_obj.update_state(y_true, y_pred)\n+        self.evaluate(update_op)\n+        result = msle_obj.result()\n+        self.assertAllClose(0.24022, result, atol=1e-5)\n+\n+    def test_weighted(self):\n+        msle_obj = metrics.MeanSquaredLogarithmicError()\n+        self.evaluate(tf.compat.v1.variables_initializer(msle_obj.variables))\n+        y_true = tf.constant(\n+            ((0, 1, 0, 1, 0), (0, 0, 1, 1, 1), (1, 1, 1, 1, 0), (0, 0, 0, 0, 1))\n+        )\n+        y_pred = tf.constant(\n+            ((0, 0, 1, 1, 0), (1, 1, 1, 1, 1), (0, 1, 0, 1, 0), (1, 1, 1, 1, 1))\n+        )\n+        sample_weight = tf.constant((1.0, 1.5, 2.0, 2.5))\n+        result = msle_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(0.26082, self.evaluate(result), atol=1e-5)\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class RootMeanSquaredErrorTest(tf.test.TestCase):\n+    def test_config(self):\n+        rmse_obj = metrics.RootMeanSquaredError(name=\"rmse\", dtype=tf.int32)\n+        self.assertEqual(rmse_obj.name, \"rmse\")\n+        self.assertEqual(rmse_obj._dtype, tf.int32)\n+\n+        rmse_obj2 = metrics.RootMeanSquaredError.from_config(\n+            rmse_obj.get_config()\n+        )\n+        self.assertEqual(rmse_obj2.name, \"rmse\")\n+        self.assertEqual(rmse_obj2._dtype, tf.int32)\n+\n+    def test_unweighted(self):\n+        rmse_obj = metrics.RootMeanSquaredError()\n+        self.evaluate(tf.compat.v1.variables_initializer(rmse_obj.variables))\n+        y_true = tf.constant((2, 4, 6))\n+        y_pred = tf.constant((1, 3, 2))\n+\n+        update_op = rmse_obj.update_state(y_true, y_pred)\n+        self.evaluate(update_op)\n+        result = rmse_obj.result()\n+        # error = [-1, -1, -4], square(error) = [1, 1, 16], mean = 18/3 = 6\n+        self.assertAllClose(math.sqrt(6), result, atol=1e-3)\n+\n+    def test_weighted(self):\n+        rmse_obj = metrics.RootMeanSquaredError()\n+        self.evaluate(tf.compat.v1.variables_initializer(rmse_obj.variables))\n+        y_true = tf.constant((2, 4, 6, 8))\n+        y_pred = tf.constant((1, 3, 2, 3))\n+        sample_weight = tf.constant((0, 1, 0, 1))\n+        result = rmse_obj(y_true, y_pred, sample_weight=sample_weight)\n+        self.assertAllClose(math.sqrt(13), self.evaluate(result), atol=1e-3)\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class LogCoshErrorTest(tf.test.TestCase):\n+    def setup(self):\n+        y_pred = np.asarray([1, 9, 2, -5, -2, 6]).reshape((2, 3))\n+        y_true = np.asarray([4, 8, 12, 8, 1, 3]).reshape((2, 3))\n+\n+        self.batch_size = 6\n+        error = y_pred - y_true\n+        self.expected_results = np.log((np.exp(error) + np.exp(-error)) / 2)\n+\n+        self.y_pred = tf.constant(y_pred, dtype=tf.float32)\n+        self.y_true = tf.constant(y_true)\n+\n+    def test_config(self):\n+        logcosh_obj = metrics.LogCoshError(name=\"logcosh\", dtype=tf.int32)\n+        self.assertEqual(logcosh_obj.name, \"logcosh\")\n+        self.assertEqual(logcosh_obj._dtype, tf.int32)\n+\n+    def test_unweighted(self):\n+        self.setup()\n+        logcosh_obj = metrics.LogCoshError()\n+        self.evaluate(tf.compat.v1.variables_initializer(logcosh_obj.variables))\n+\n+        update_op = logcosh_obj.update_state(self.y_true, self.y_pred)\n+        self.evaluate(update_op)\n+        result = logcosh_obj.result()\n+        expected_result = np.sum(self.expected_results) / self.batch_size\n+        self.assertAllClose(result, expected_result, atol=1e-3)\n+\n+    def test_weighted(self):\n+        self.setup()\n+        logcosh_obj = metrics.LogCoshError()\n+        self.evaluate(tf.compat.v1.variables_initializer(logcosh_obj.variables))\n+        sample_weight = tf.constant([1.2, 3.4], shape=(2, 1))\n+        result = logcosh_obj(\n+            self.y_true, self.y_pred, sample_weight=sample_weight\n+        )\n+\n+        sample_weight = np.asarray([1.2, 1.2, 1.2, 3.4, 3.4, 3.4]).reshape(\n+            (2, 3)\n+        )\n+        expected_result = np.multiply(self.expected_results, sample_weight)\n+        expected_result = np.sum(expected_result) / np.sum(sample_weight)\n+        self.assertAllClose(self.evaluate(result), expected_result, atol=1e-3)\n+\n+\n+@test_combinations.generate(test_combinations.combine(mode=[\"graph\", \"eager\"]))\n+class MeanRelativeErrorTest(tf.test.TestCase):\n+    def test_config(self):\n+        normalizer = tf.constant([1, 3], dtype=tf.float32)\n+        mre_obj = metrics.MeanRelativeError(normalizer=normalizer, name=\"mre\")\n+        self.assertEqual(mre_obj.name, \"mre\")\n+        self.assertArrayNear(self.evaluate(mre_obj.normalizer), [1, 3], 1e-1)\n+\n+        mre_obj2 = metrics.MeanRelativeError.from_config(mre_obj.get_config())\n+        self.assertEqual(mre_obj2.name, \"mre\")\n+        self.assertArrayNear(self.evaluate(mre_obj2.normalizer), [1, 3], 1e-1)\n+\n+    def test_unweighted(self):\n+        np_y_pred = np.asarray([2, 4, 6, 8], dtype=np.float32)\n+        np_y_true = np.asarray([1, 3, 2, 3], dtype=np.float32)\n+        expected_error = np.mean(\n+            np.divide(np.absolute(np_y_pred - np_y_true), np_y_true)\n+        )\n+\n+        y_pred = tf.constant(np_y_pred, shape=(1, 4), dtype=tf.float32)\n+        y_true = tf.constant(np_y_true, shape=(1, 4))\n+\n+        mre_obj = metrics.MeanRelativeError(normalizer=y_true)\n+        self.evaluate(tf.compat.v1.variables_initializer(mre_obj.variables))\n+\n+        result = mre_obj(y_true, y_pred)\n+        self.assertAllClose(self.evaluate(result), expected_error, atol=1e-3)\n+\n+    def test_weighted(self):\n+        np_y_pred = np.asarray([2, 4, 6, 8], dtype=np.float32)\n+        np_y_true = np.asarray([1, 3, 2, 3], dtype=np.float32)\n+        sample_weight = np.asarray([0.2, 0.3, 0.5, 0], dtype=np.float32)\n+        rel_errors = np.divide(np.absolute(np_y_pred - np_y_true), np_y_true)\n+        expected_error = np.sum(rel_errors * sample_weight)\n+\n+        y_pred = tf.constant(np_y_pred, dtype=tf.float32)\n+        y_true = tf.constant(np_y_true)\n+\n+        mre_obj = metrics.MeanRelativeError(normalizer=y_true)\n+        self.evaluate(tf.compat.v1.variables_initializer(mre_obj.variables))\n+\n+        result = mre_obj(\n+            y_true, y_pred, sample_weight=tf.constant(sample_weight)\n+        )\n+        self.assertAllClose(self.evaluate(result), expected_error, atol=1e-3)\n+\n+    def test_zero_normalizer(self):\n+        y_pred = tf.constant([2, 4], dtype=tf.float32)\n+        y_true = tf.constant([1, 3])\n+\n+        mre_obj = metrics.MeanRelativeError(normalizer=tf.zeros_like(y_true))\n+        self.evaluate(tf.compat.v1.variables_initializer(mre_obj.variables))\n+\n+        result = mre_obj(y_true, y_pred)\n+        self.assertEqual(self.evaluate(result), 0)\n+\n+\n+if __name__ == \"__main__\":\n+    tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
