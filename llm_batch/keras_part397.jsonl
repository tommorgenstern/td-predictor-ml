{"custom_id": "keras#8f66e1aee5d08bb66e92efbe5204db66fe2564af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1554 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1151,7 +1151,8 @@ class TestLayerCallTracing(tf.test.TestCase, parameterized.TestCase):\n     fn = call_collection.add_function(layer.call, 'call', True)\n     fn(np.ones((2, 3)))\n \n-    self.assertAllEqual(previous_losses, layer.losses)\n+    self.assertAllEqual(self.evaluate(previous_losses),\n+                        self.evaluate(layer.losses))\n \n \n @generic_utils.register_keras_serializable('Testing')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9e77f605c2a6cae3a7fe4e5cf59415766dff1089", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 773 | Contributors (this commit): 4 | Commits (past 90d): 1 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -363,8 +363,8 @@ class ModelSubclassingTest(keras_parameterized.TestCase):\n \n     m = Foo()\n     self.assertEqual([m.isdep, m.notdep], m.layers)\n-    self.assertEqual(1, len(m._checkpoint_dependencies))\n-    self.assertIs(m.isdep, m._checkpoint_dependencies[0].ref)\n+    self.assertEqual(1, len(m._trackable_children()))\n+    self.assertIs(m.isdep, m._trackable_children()['isdep'])\n     self.assertEqual('notdep_var:0', m.notdep_var.name)\n \n   def test_extra_variable(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2c8864dd5363d464a2b9c6ad0a37c5eed13b7534", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 7431 | Contributors (this commit): 101 | Commits (past 90d): 13 | Contributors (cumulative): 101 | DMM Complexity: None\n\nDIFF:\n@@ -1762,7 +1762,8 @@ class EarlyStopping(Callback):\n         improvement.\n     patience: Number of epochs with no improvement\n         after which training will be stopped.\n-    verbose: verbosity mode.\n+    verbose: int. 0: quiet, 1: message when the training is stopped\n+        and if weights got restored.\n     mode: One of `{\"auto\", \"min\", \"max\"}`. In `min` mode,\n         training will stop when the quantity\n         monitored has stopped decreasing; in `\"max\"`\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c66d6387926617ecf4665160bd7e13fb453f9d8b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1756 | Contributors (this commit): 11 | Commits (past 90d): 6 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -410,7 +410,8 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n \n   @property\n   def global_clipnorm(self):\n-    \"\"\"`float` or `None`. If set, clips gradients to a maximum norm.\"\"\"\n+    \"\"\"`float` or `None`. If set, clips gradients to a maximum norm.\n+    Check `tf.clip_by_global_norm` for more details.\"\"\"\n     return self._global_clipnorm\n \n   @clipnorm.setter\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6e0f2666072fb09a67afa71510cce02b9f3c1aef", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 511 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -75,15 +75,6 @@ class Dropout(base_layer.BaseRandomLayer):\n   \"\"\"\n \n   def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n-    # Note that the constructor is annotated with\n-    # @no_automatic_dependency_tracking. This is to skip the auto\n-    # tracking of self._random_generator instance, which is an AutoTrackable.\n-    # The backend.RandomGenerator could contain a tf.random.Generator instance\n-    # which will have tf.Variable as the internal state. We want to avoid saving\n-    # that state into model.weights and checkpoints for backward compatibility\n-    # reason. In the meantime, we still need to make them visible to SavedModel\n-    # when it is tracing the tf.function for the `call()`.\n-    # See _list_extra_dependencies_for_serialization below for more details.\n     super(Dropout, self).__init__(seed=seed, **kwargs)\n     if isinstance(rate, (int, float)) and not 0 <= rate <= 1:\n       raise ValueError(f'Invalid value {rate} received for '\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5790a9f922286df90b8e6fd6910bb1e996ad4a73", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 251 | Lines Deleted: 363 | Files Changed: 10 | Hunks: 72 | Methods Changed: 36 | Complexity Δ (Sum/Max): -20/2 | Churn Δ: 614 | Churn Cumulative: 5122 | Contributors (this commit): 7 | Commits (past 90d): 73 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,7 +23,6 @@ from keras import optimizers\n from keras.mixed_precision import loss_scale_optimizer\n from keras.mixed_precision import test_util as mp_test_util\n from keras.optimizer_experimental import optimizer as optimizer_experimental\n-from keras.optimizer_experimental import optimizer_lib\n from keras.optimizer_experimental import sgd as sgd_experimental\n from keras.optimizer_v2 import adam\n from keras.optimizer_v2 import gradient_descent\n@@ -301,15 +300,7 @@ class LossScaleOptimizerTest(tf.test.TestCase, parameterized.TestCase):\n     for clip_type in ('clipnorm', 'global_clipnorm', 'clipvalue'):\n       with strategy.scope(), self.subTest(clip_type=clip_type):\n         var = tf.Variable([5.0])\n-        if opt_cls == optimizer_v2.OptimizerV2:\n         opt = create_sgd(opt_cls, learning_rate, **{clip_type: 2.0})\n-        else:\n-          # This branch is for experimental optimizer.\n-          opt = create_sgd(\n-              opt_cls, learning_rate, **{\n-                  'gradients_clip_option':\n-                      optimizer_lib.GradientsClipOption(**{clip_type: 2.0})\n-              })\n         opt = create_lso(opt, initial_scale=2, dynamic_growth_steps=1)\n         if isinstance(opt, loss_scale_optimizer.LossScaleOptimizer):\n           # Only OptimizerV2 exposes the clipping attributes\n\n@@ -47,22 +47,21 @@ class Adadelta(optimizer.Optimizer):\n     rho: A `Tensor` or a floating point value. The decay rate. Defaults to 0.95.\n     epsilon: Small floating point value used to maintain numerical stability.\n       Defaults to 1e-7.\n-    gradients_clip_option: an instance of\n-      `optimizer_experimental.GradientsClipOption`, for attributes related to\n-      gradients clipping, such as clipnorm and clipvalue. Default to None\n-      (not applying gradients clipping).\n-    ema_option: an instance of `optimizer_experimental.EMAOption`, for\n-      attributes related to exponenatial moving average, such as `use_ema` (a\n-      boolean field indicates if EMA is used) and EMA momentum. Default to None\n-      (not applying EMA).\n-    jit_compile: Bool, default to False. If True, the optimizer will use XLA\n-        acceleration. `jit_compile` can only be False when using Parameter\n-        Server Strategy.\n+    clipnorm: see the `clipnorm` argument of `optimizer_experimental.Optimizer`.\n+    clipvalue: see the `clipvalue` argument of\n+      `optimizer_experimental.Optimizer`.\n+    global_clipnorm: see the `global_clipnorm` argument of\n+      `optimizer_experimental.Optimizer`.\n+    use_ema: see the `use_ema` argument of `optimizer_experimental.Optimizer`.\n+    ema_momentum: see the `ema_momentum` argument of\n+      `optimizer_experimental.Optimizer`.\n+    ema_overwrite_frequency: see the `ema_overwrite_frequency` argument of\n+      `optimizer_experimental.Optimizer`.\n+    jit_compile: see the `jit_compile` argument of\n+      `optimizer_experimental.Optimizer`.\n     name: Optional name prefix for the operations created when applying\n       gradients. Defaults to `\"Adadelta\"`.\n-    **kwargs: keyword arguments only used for backward compatibility with\n-      `optimizer_v2.OptimizerV2`. Any new code using\n-      `optimizer_experimental.Optimizer` should leave this parameter empty.\n+    **kwargs: see the `**kwargs` argument of `optimizer_experimental.Optimizer`.\n \n   Reference:\n     - [Zeiler, 2012](http://arxiv.org/abs/1212.5701)\n@@ -72,14 +71,22 @@ class Adadelta(optimizer.Optimizer):\n                learning_rate=0.001,\n                rho=0.95,\n                epsilon=1e-7,\n-               gradients_clip_option=None,\n-               ema_option=None,\n+               clipnorm=None,\n+               clipvalue=None,\n+               global_clipnorm=None,\n+               use_ema=False,\n+               ema_momentum=0.99,\n+               ema_overwrite_frequency=100,\n                jit_compile=False,\n                name='Adadelta',\n                **kwargs):\n     super(Adadelta, self).__init__(\n-        gradients_clip_option=gradients_clip_option,\n-        ema_option=ema_option,\n+        clipnorm=clipnorm,\n+        clipvalue=clipvalue,\n+        global_clipnorm=global_clipnorm,\n+        use_ema=use_ema,\n+        ema_momentum=ema_momentum,\n+        ema_overwrite_frequency=ema_overwrite_frequency,\n         jit_compile=jit_compile,\n         name=name,\n         **kwargs)\n\n@@ -41,22 +41,21 @@ class Adagrad(optimizer.Optimizer):\n       Starting value for the accumulators (per-parameter momentum values).\n       Must be non-negative.\n     epsilon: Small floating point value used to maintain numerical stability.\n-    gradients_clip_option: an instance of\n-      `optimizer_experimental.GradientsClipOption`, for attributes related to\n-      gradients clipping, such as clipnorm and clipvalue. Default to None\n-      (not applying gradients clipping).\n-    ema_option: an instance of `optimizer_experimental.EMAOption`, for\n-      attributes related to exponenatial moving average, such as `use_ema` (a\n-      boolean field indicates if EMA is used) and EMA momentum. Default to None\n-      (not applying EMA).\n-    jit_compile: Bool, default to False. If True, the optimizer will use XLA\n-        acceleration. `jit_compile` can only be False when using Parameter\n-        Server Strategy.\n+    clipnorm: see the `clipnorm` argument of `optimizer_experimental.Optimizer`.\n+    clipvalue: see the `clipvalue` argument of\n+      `optimizer_experimental.Optimizer`.\n+    global_clipnorm: see the `global_clipnorm` argument of\n+      `optimizer_experimental.Optimizer`.\n+    use_ema: see the `use_ema` argument of `optimizer_experimental.Optimizer`.\n+    ema_momentum: see the `ema_momentum` argument of\n+      `optimizer_experimental.Optimizer`.\n+    ema_overwrite_frequency: see the `ema_overwrite_frequency` argument of\n+      `optimizer_experimental.Optimizer`.\n+    jit_compile: see the `jit_compile` argument of\n+      `optimizer_experimental.Optimizer`.\n     name: Optional name prefix for the operations created when applying\n       gradients. Defaults to `\"Adagrad\"`.\n-    **kwargs: keyword arguments only used for backward compatibility with\n-      `optimizer_v2.OptimizerV2`. Any new code using\n-      `optimizer_experimental.Optimizer` should leave this parameter empty.\n+    **kwargs: see the `**kwargs` argument of `optimizer_experimental.Optimizer`.\n \n   Reference:\n     - [Duchi et al., 2011](\n@@ -67,14 +66,22 @@ class Adagrad(optimizer.Optimizer):\n                learning_rate=0.001,\n                initial_accumulator_value=0.1,\n                epsilon=1e-7,\n-               gradients_clip_option=None,\n-               ema_option=None,\n+               clipnorm=None,\n+               clipvalue=None,\n+               global_clipnorm=None,\n+               use_ema=False,\n+               ema_momentum=0.99,\n+               ema_overwrite_frequency=100,\n                jit_compile=False,\n                name='Adagrad',\n                **kwargs):\n     super(Adagrad, self).__init__(\n-        gradients_clip_option=gradients_clip_option,\n-        ema_option=ema_option,\n+        clipnorm=clipnorm,\n+        clipvalue=clipvalue,\n+        global_clipnorm=global_clipnorm,\n+        use_ema=use_ema,\n+        ema_momentum=ema_momentum,\n+        ema_overwrite_frequency=ema_overwrite_frequency,\n         jit_compile=jit_compile,\n         name=name,\n         **kwargs)\n\n@@ -50,22 +50,21 @@ class Adam(optimizer.Optimizer):\n       1e-7.\n     amsgrad: Boolean. Whether to apply AMSGrad variant of this algorithm from\n       the paper \"On the Convergence of Adam and beyond\". Defaults to `False`.\n-    gradients_clip_option: an instance of\n-      `optimizer_experimental.GradientsClipOption`, for attributes related to\n-      gradients clipping, such as clipnorm and clipvalue. Default to None\n-      (not applying gradients clipping).\n-    ema_option: an instance of `optimizer_experimental.EMAOption`, for\n-      attributes related to exponenatial moving average, such as `use_ema` (a\n-      boolean field indicates if EMA is used) and EMA momentum. Default to None\n-      (not applying EMA).\n-    jit_compile: Bool, default to False. If True, the optimizer will use XLA\n-      acceleration. `jit_compile` can only be False when using Parameter\n-      Server Strategy.\n-    name: Optional name for the operations created when applying gradients.\n-      Defaults to `\"Adam\"`.\n-    **kwargs: keyword arguments only used for backward compatibility with\n-      `optimizer_v2.OptimizerV2`. Any new code using\n-      `optimizer_experimental.Optimizer` should leave this parameter empty.\n+    clipnorm: see the `clipnorm` argument of `optimizer_experimental.Optimizer`.\n+    clipvalue: see the `clipvalue` argument of\n+      `optimizer_experimental.Optimizer`.\n+    global_clipnorm: see the `global_clipnorm` argument of\n+      `optimizer_experimental.Optimizer`.\n+    use_ema: see the `use_ema` argument of `optimizer_experimental.Optimizer`.\n+    ema_momentum: see the `ema_momentum` argument of\n+      `optimizer_experimental.Optimizer`.\n+    ema_overwrite_frequency: see the `ema_overwrite_frequency` argument of\n+      `optimizer_experimental.Optimizer`.\n+    jit_compile: see the `jit_compile` argument of\n+      `optimizer_experimental.Optimizer`.\n+    name: Optional name prefix for the operations created when applying\n+      gradients. Defaults to `\"Adam\"`.\n+    **kwargs: see the `**kwargs` argument of `optimizer_experimental.Optimizer`.\n \n   Reference:\n     - [Kingma et al., 2014](http://arxiv.org/abs/1412.6980)\n@@ -97,15 +96,23 @@ class Adam(optimizer.Optimizer):\n                beta_2=0.999,\n                epsilon=1e-7,\n                amsgrad=False,\n-               gradients_clip_option=None,\n-               ema_option=None,\n+               clipnorm=None,\n+               clipvalue=None,\n+               global_clipnorm=None,\n+               use_ema=False,\n+               ema_momentum=0.99,\n+               ema_overwrite_frequency=100,\n                jit_compile=False,\n                name='Adam',\n                **kwargs):\n     super(Adam, self).__init__(\n         name=name,\n-        gradients_clip_option=gradients_clip_option,\n-        ema_option=ema_option,\n+        clipnorm=clipnorm,\n+        clipvalue=clipvalue,\n+        global_clipnorm=global_clipnorm,\n+        use_ema=use_ema,\n+        ema_momentum=ema_momentum,\n+        ema_overwrite_frequency=ema_overwrite_frequency,\n         jit_compile=jit_compile,\n         **kwargs)\n     self._learning_rate = self._build_learning_rate(learning_rate)\n\n@@ -22,7 +22,6 @@ from absl import logging\n \n from keras import backend\n from keras import initializers\n-from keras.optimizer_experimental import optimizer_lib\n from keras.optimizer_v2 import learning_rate_schedule\n from keras.optimizer_v2 import utils as optimizer_utils\n import tensorflow.compat.v2 as tf\n@@ -33,34 +32,37 @@ class _BaseOptimizer(tf.Module):\n \n   def __init__(self,\n                name,\n-               gradients_clip_option=None,\n-               ema_option=None,\n+               clipnorm=None,\n+               clipvalue=None,\n+               global_clipnorm=None,\n+               use_ema=False,\n+               ema_momentum=0.99,\n+               ema_overwrite_frequency=100,\n                jit_compile=False,\n                **kwargs):\n-    \"\"\"Create a new Optimizer.\n-\n-    Args:\n-      name: String. The name to use for momentum accumulator weights created by\n-        the optimizer.\n-      gradients_clip_option: an instance of\n-        `optimizer_experimental.GradientsClipOption`, for attributes related to\n-        gradients clipping, such as clipnorm and clipvalue. Default to None\n-        (not applying gradients clipping).\n-      ema_option: an instance of `optimizer_experimental.EMAOption`, for\n-        attributes related to exponenatial moving average, such as use_ema (a\n-        boolean field indicates if EMA is used) and EMA momentum. Default to\n-        None (not applying EMA).\n-      jit_compile: Bool, default to False. If True, the optimizer will use XLA\n-        acceleration. `jit_compile` can only be False when using Parameter\n-        Server Strategy.\n-      **kwargs: keyword arguments only used for backward compatibility with\n-        `optimizer_v2.OptimizerV2`. Any new code using\n-        `optimizer_experimental.Optimizer` should leave this parameter empty.\n-    \"\"\"\n     self._name = name\n-    self._gradients_clip_option = gradients_clip_option\n-    self._ema_option = ema_option\n+    self._clipnorm = clipnorm\n+    self._global_clipnorm = global_clipnorm\n+    self._clipvalue = clipvalue\n+    self._use_ema = use_ema\n     self._jit_compile = jit_compile\n+    if use_ema:\n+      # Verify the arguments related to EMA.\n+      if ema_momentum > 1 or ema_momentum < 0:\n+        raise ValueError(\"`ema_momentum` must be in the range [0, 1]. \"\n+                         f\"Received: ema_momentum={ema_momentum}\")\n+      if ema_overwrite_frequency and (not isinstance(\n+          ema_overwrite_frequency, int) or ema_overwrite_frequency < 1):\n+        raise ValueError(\n+            \"`ema_overwrite_frequency` must be an integer > 1 or None. \"\n+            f\"Received: ema_overwrite_frequency={ema_overwrite_frequency}\")\n+    self._ema_momentum = ema_momentum\n+    self._ema_overwrite_frequency = ema_overwrite_frequency\n+\n+    if self._clipnorm is not None and self._global_clipnorm is not None:\n+      raise ValueError(f\"At most one of `clipnorm` and `global_clipnorm` can \"\n+                       f\"be set. Received: clipnorm={self._clipnorm}, \"\n+                       f\"global_clipnorm={self._global_clipnorm}.\")\n \n     self._create_iteration_variable()\n     self._process_kwargs(kwargs)\n@@ -72,17 +74,11 @@ class _BaseOptimizer(tf.Module):\n       self._iterations = tf.Variable(0, name=\"iteration\", dtype=tf.int64)\n \n   def _process_kwargs(self, kwargs):\n-    legacy_gradients_clip_kwargs = {\"clipnorm\", \"clipvalue\", \"global_clipnorm\"}\n-    other_legacy_kwargs = {\n+    legacy_kwargs = {\n         \"lr\", \"decay\", \"gradient_transformers\", \"gradient_aggregator\"\n     }\n     for k in kwargs:\n-      if k in legacy_gradients_clip_kwargs:\n-        logging.warning(\n-            \"%s is deprecated in `optimizer_experimental.Optimizer`\"\n-            \", please use `GradientsClipOption` instead to specify \"\n-            \" your gradients clipping logic.\", k)\n-      elif k in other_legacy_kwargs:\n+      if k in legacy_kwargs:\n         logging.warning(\n             \"%s is deprecated in `optimizer_experimental.Optimizer`\"\n             \", please check the docstring for valid arguments.\", k)\n@@ -90,16 +86,6 @@ class _BaseOptimizer(tf.Module):\n         raise TypeError(f\"{k} is not a valid argument, kwargs should be empty \"\n                         \" for `optimizer_experimental.Optimizer`.\")\n \n-    # TODO(b/208301504): gradients clipping options are populated for\n-    # backward compatibility, we should delete it when the migration is done.\n-    clipnorm = kwargs.pop(\"clipnorm\", None)\n-    clipvalue = kwargs.pop(\"clipvalue\", None)\n-    global_clipnorm = kwargs.pop(\"global_clipnorm\", None)\n-    if self._gradients_clip_option is None and (clipnorm or clipvalue or\n-                                                global_clipnorm):\n-      self._gradients_clip_option = optimizer_lib.GradientsClipOption(\n-          clipnorm, clipvalue, global_clipnorm)\n-\n   def _var_key(self, variable):\n     \"\"\"Get a unique identifier of the given variable.\"\"\"\n     # Get the distributed variable if it exists.\n@@ -147,24 +133,19 @@ class _BaseOptimizer(tf.Module):\n     return list(zip(grads, var_list))\n \n   def _clip_gradients(self, grads):\n-    if not self._gradients_clip_option:\n-      return grads\n-\n     clipped_grads = []\n-    if self._gradients_clip_option.clipnorm:\n+    if self._clipnorm and self._clipnorm > 0:\n       for g in grads:\n         if g is None:\n           clipped_grads.append(g)\n         else:\n-          clipped_grads.append(\n-              tf.clip_by_norm(g, self._gradients_clip_option.clipnorm))\n+          clipped_grads.append(tf.clip_by_norm(g, self._clipnorm))\n       return clipped_grads\n \n-    if self._gradients_clip_option.global_clipnorm:\n-      return tf.clip_by_global_norm(\n-          grads, self._gradients_clip_option.global_clipnorm)[0]\n+    if self._global_clipnorm and self._global_clipnorm > 0:\n+      return tf.clip_by_global_norm(grads, self._global_clipnorm)[0]\n \n-    if self._gradients_clip_option.clipvalue:\n+    if self._clipvalue and self._clipvalue > 0:\n       for g in grads:\n         if g is None:\n           clipped_grads.append(g)\n@@ -172,8 +153,8 @@ class _BaseOptimizer(tf.Module):\n           clipped_grads.append(\n               tf.clip_by_value(\n                   g,\n-                  clip_value_min=(-self._gradients_clip_option.clipvalue),\n-                  clip_value_max=self._gradients_clip_option.clipvalue))\n+                  clip_value_min=-self._clipvalue,  # pylint: disable=invalid-unary-operand-type\n+                  clip_value_max=self._clipvalue))\n       return clipped_grads\n \n     return grads\n@@ -237,7 +218,7 @@ class _BaseOptimizer(tf.Module):\n     if getattr(self, \"_built\", False):\n       return\n     self._build_index_dict(var_list)\n-    if self._ema_option and self._ema_option.use_ema:\n+    if self._use_ema:\n       self._model_variables_moving_average = []\n       for var in var_list:\n         # Make a copy of the model variables, we will use the copy to store the\n@@ -382,10 +363,10 @@ class _BaseOptimizer(tf.Module):\n \n   def _update_model_variables_moving_average(self, var_list):\n     \"\"\"Update the stored moving average using the latest value.\"\"\"\n-    if self._ema_option and self._ema_option.use_ema:\n-      ema_momentum = self._ema_option.ema_momentum\n+    if self._use_ema:\n       for (var, average) in zip(var_list, self._model_variables_moving_average):\n-        average.assign(ema_momentum * average + (1 - ema_momentum) * var)\n+        average.assign(self._ema_momentum * average +\n+                       (1 - self._ema_momentum) * var)\n \n   def _overwrite_model_variables_with_average_value(self, var_list):\n     \"\"\"Overwrite model variables with its moving average.\"\"\"\n@@ -437,11 +418,15 @@ class _BaseOptimizer(tf.Module):\n     Returns:\n         Python dictionary.\n     \"\"\"\n-    config = {}\n-    if self._gradients_clip_option:\n-      config[\"gradients_clip_option\"] = self._gradients_clip_option.get_config()\n-    if self._ema_option:\n-      config[\"ema_option\"] = self._ema_option.get_config()\n+    config = {\n+        \"clipnorm\": self._clipnorm,\n+        \"global_clipnorm\": self._global_clipnorm,\n+        \"clipvalue\": self._clipvalue,\n+        \"use_ema\": self._use_ema,\n+        \"ema_momentum\": self._ema_momentum,\n+        \"ema_overwrite_frequency\": self._ema_overwrite_frequency,\n+        \"jit_compile\": self._jit_compile,\n+    }\n     return config\n \n   @classmethod\n@@ -461,11 +446,6 @@ class _BaseOptimizer(tf.Module):\n       if isinstance(config[\"learning_rate\"], dict):\n         config[\"learning_rate\"] = learning_rate_schedule.deserialize(\n             config[\"learning_rate\"])\n-    if \"gradients_clip_option\" in config:\n-      config[\"gradients_clip_option\"] = optimizer_lib.GradientsClipOption(\n-          **config[\"gradients_clip_option\"])\n-    if \"ema_option\" in config:\n-      config[\"ema_option\"] = optimizer_lib.EMAOption(**config[\"ema_option\"])\n     return cls(**config)\n \n \n@@ -474,35 +454,53 @@ class Optimizer(_BaseOptimizer):\n \n   This class supports distributed training. If you want to implement your own\n   optimizer, please subclass this class instead of _BaseOptimizer.\n-  \"\"\"\n \n-  def __init__(self,\n-               name,\n-               gradients_clip_option=None,\n-               ema_option=None,\n-               jit_compile=False,\n-               **kwargs):\n-    \"\"\"Create a new Optimizer.\n-\n-    Args:\n-      name: String. The name to use for momentum accumulator weights created by\n+  Attributes:\n+    name: string. The name to use for momentum accumulator weights created by\n       the optimizer.\n-      gradients_clip_option: an instance of\n-        `optimizer_experimental.GradientsClipOption`, for attributes related to\n-        gradients clipping, such as clipnorm and clipvalue. Default to None\n-        (not applying gradients clipping).\n-      ema_option: an instance of `optimizer_experimental.EMAOption`, for\n-        attributes related to exponenatial moving average, such as `use_ema` (a\n-        boolean field indicates if EMA is used) and EMA momentum. Default to\n-        None (not applying EMA).\n-      jit_compile: Bool, default to False. If True, the optimizer will use XLA\n+    clipnorm: float. If set, the gradient of each weight is individually\n+      clipped so that its norm is no higher than this value.\n+    clipvalue: float. If set, the gradient of each weight is clipped to be no\n+      higher than this value.\n+    global_clipnorm: float. If set, the gradient of all weights is clipped so\n+      that their global norm is no higher than this value.\n+    use_ema: boolean, default to False. If True, exponential moving average\n+      (EMA) is applied. EMA consists of computing an exponential moving\n+      average of the weights of the model (as the weight values change after\n+      each training batch), and periodically overwriting the weights with\n+      their moving average.\n+    ema_momentum: float, default to 0.99. Only used if `use_ema=True`. This is\n+      the momentum to use when computing the EMA of the model's weights:\n+        `new_average = ema_momentum * old_average + (1 - ema_momentum) *\n+        current_variable_value`.\n+    ema_overwrite_frequency: int or None, default to 100. Only used if\n+      `use_ema=True`. Every ema_overwrite_frequency steps of iterations, we\n+      overwrite the model variable by its stored moving average. If None, we\n+      do not overwrite model variables in the middle of training, and users\n+      need to explicitly overwrite the model variable by calling\n+      `finalize_variable_update()`.\n+    jit_compile: bool, default to False. If True, the optimizer will use XLA\n       acceleration. `jit_compile` can only be False when using Parameter\n       Server Strategy.\n     **kwargs: keyword arguments only used for backward compatibility with\n       `optimizer_v2.OptimizerV2`. Any new code using\n       `optimizer_experimental.Optimizer` should leave this parameter empty.\n   \"\"\"\n-    super().__init__(name, gradients_clip_option, ema_option, jit_compile,\n+\n+  def __init__(self,\n+               name,\n+               clipnorm=None,\n+               clipvalue=None,\n+               global_clipnorm=None,\n+               use_ema=False,\n+               ema_momentum=0.99,\n+               ema_overwrite_frequency=100,\n+               jit_compile=False,\n+               **kwargs):\n+    \"\"\"Create a new Optimizer.\"\"\"\n+\n+    super().__init__(name, clipnorm, clipvalue, global_clipnorm, use_ema,\n+                     ema_momentum, ema_overwrite_frequency, jit_compile,\n                      **kwargs)\n     self._distribution_strategy = tf.distribute.get_strategy()\n \n@@ -594,6 +592,17 @@ class Optimizer(_BaseOptimizer):\n       strategy.extended.update(\n           var, lambda a, b: a.assign(b), args=(average_var,))\n \n+  def _update_model_variables_moving_average(self, var_list):\n+    \"\"\"Update the stored moving average using the latest value.\"\"\"\n+    if self._use_ema:\n+      def update_average(average, var):\n+        average.assign(self._ema_momentum * average +\n+                       (1 - self._ema_momentum) * var)\n+\n+      for (var, average) in zip(var_list, self._model_variables_moving_average):\n+        self._distribution_strategy.extended.update(\n+            average, update_average, args=(var,), group=False)\n+\n   def _distributed_apply_gradients_fn(self, distribution, grads_and_vars,\n                                       **kwargs):\n     \"\"\"`apply_gradients` using a `DistributionStrategy`.\"\"\"\n@@ -609,15 +618,14 @@ class Optimizer(_BaseOptimizer):\n           var, apply_grad_to_update_var, args=(grad,), group=False)\n     self.iterations.assign_add(1)\n \n-    if self._ema_option and self._ema_option.use_ema:\n+    if self._use_ema:\n       _, var_list = zip(*grads_and_vars)\n       self._update_model_variables_moving_average(var_list)\n-\n-      if self._ema_option.ema_overwrite_frequency:\n-        # Only when self._ema_option.ema_overwrite_frequency is not None, we\n-        # overwrite the model variables.\n+      if self._ema_overwrite_frequency:\n+        # Only when self._ema_overwrite_frequency is not None, we overwrite the\n+        # model variables.\n         should_overwrite_model_vars = (\n-            self.iterations % self._ema_option.ema_overwrite_frequency == 0)\n+            self.iterations % self._ema_overwrite_frequency == 0)\n         tf.cond(\n             should_overwrite_model_vars,\n             true_fn=lambda: self._overwrite_model_variables_with_average_value(  # pylint: disable=g-long-lambda\n\n@@ -1,88 +0,0 @@\n-\"\"\"Library of helper classes of optimizer.\"\"\"\n-\n-\n-class GradientsClipOption:\n-  \"\"\"Gradients clip option for optimizer class.\n-\n-  Attributes:\n-    clipnorm: float. If set, the gradient of each weight is individually clipped\n-      so that its norm is no higher than this value.\n-    clipvalue: float. If set, the gradient of each weight is clipped to be no\n-      higher than this value.\n-    global_clipnorm: float. If set, the gradient of all weights is clipped so\n-      that their global norm is no higher than this value.\n-  \"\"\"\n-\n-  def __init__(self, clipnorm=None, clipvalue=None, global_clipnorm=None):\n-    if clipnorm is not None and global_clipnorm is not None:\n-      raise ValueError(f\"At most one of `clipnorm` and `global_clipnorm` can \"\n-                       f\"be set. Received: clipnorm={clipnorm}, \"\n-                       f\"global_clipnorm={global_clipnorm}.\")\n-\n-    if clipnorm and clipnorm <= 0:\n-      raise ValueError(\"Clipnorm should be a positive number, but received \"\n-                       f\"clipnorm={clipnorm}.\")\n-    if global_clipnorm and global_clipnorm <= 0:\n-      raise ValueError(\"global_clipnorm should be a positive number, but \"\n-                       f\"received global_clipnorm={global_clipnorm}.\")\n-    if clipvalue and clipvalue <= 0:\n-      raise ValueError(\"clipvalue should be a positive number, but received \"\n-                       f\"clipvalue={clipvalue}.\")\n-    self.clipnorm = clipnorm\n-    self.global_clipnorm = global_clipnorm\n-    self.clipvalue = clipvalue\n-\n-  def get_config(self):\n-    return {\n-        \"clipnorm\": self.clipnorm,\n-        \"global_clipnorm\": self.global_clipnorm,\n-        \"clipvalue\": self.clipvalue,\n-    }\n-\n-\n-class EMAOption:\n-  # TODO(b/207532340): Add examples on how to use this EMAOption.\n-  \"\"\"EMA option for optimizer class.\n-\n-  Attributes:\n-    use_ema: boolean, default to False. If True, exponential moving average\n-      (EMA) is applied. EMA consists of computing an exponential moving average\n-      of the weights of the model (as the weight values change after each\n-      training batch), and periodically overwriting the weights with their\n-      moving average.\n-    ema_momentum: float, default to 0.99. Only used if `use_ema=True`. This is\n-      the momentum to use when computing the EMA of the model's weights:\n-        `new_average = ema_momentum * old_average + (1 - ema_momentum) *\n-        current_variable_value`.\n-    ema_overwrite_frequency: int or None, default to 100. Only used if\n-      `use_ema=True`. Every `ema_overwrite_frequency` steps of iterations, we\n-      overwrite the model variable by its stored moving average. If None, we do\n-      not overwrite model variables in the middle of training, and users need to\n-      explicitly overwrite the model variable by calling\n-      `finalize_variable_update()`.\n-  \"\"\"\n-\n-  def __init__(self,\n-               use_ema=False,\n-               ema_momentum=0.99,\n-               ema_overwrite_frequency=100):\n-    self.use_ema = use_ema\n-    if use_ema:\n-      # Verify the arguments related to EMA.\n-      if ema_momentum > 1 or ema_momentum < 0:\n-        raise ValueError(\"`ema_momentum` must be in the range [0, 1]. \"\n-                         f\"Received: ema_momentum={ema_momentum}\")\n-      if ema_overwrite_frequency and (not isinstance(\n-          ema_overwrite_frequency, int) or ema_overwrite_frequency < 1):\n-        raise ValueError(\n-            \"`ema_overwrite_frequency` must be an integer > 1 or None. \"\n-            f\"Received: ema_overwrite_frequency={ema_overwrite_frequency}\")\n-    self.ema_momentum = ema_momentum\n-    self.ema_overwrite_frequency = ema_overwrite_frequency\n-\n-  def get_config(self):\n-    return {\n-        \"use_ema\": self.use_ema,\n-        \"ema_momentum\": self.ema_momentum,\n-        \"ema_overwrite_frequency\": self.ema_overwrite_frequency,\n-    }\n\n@@ -1,37 +0,0 @@\n-\"\"\"Tests for optimizer_lib.\"\"\"\n-\n-from keras.optimizer_experimental import optimizer_lib\n-import tensorflow.compat.v2 as tf\n-\n-\n-class OptimizerLibTest(tf.test.TestCase):\n-\n-  def test_gradients_clip_option(self):\n-    gradients_clip_option = optimizer_lib.GradientsClipOption(\n-        clipnorm=1, clipvalue=1)\n-    self.assertEqual(gradients_clip_option.clipnorm, 1)\n-    self.assertEqual(gradients_clip_option.clipvalue, 1)\n-    with self.assertRaisesRegex(ValueError, \"At most one of.*\"):\n-      _ = optimizer_lib.GradientsClipOption(clipnorm=1, global_clipnorm=1)\n-\n-    with self.assertRaisesRegex(ValueError, \".*should be a positive number.*\"):\n-      _ = optimizer_lib.GradientsClipOption(clipnorm=-1)\n-\n-  def test_get_and_from_config(self):\n-    gradients_clip_option = optimizer_lib.GradientsClipOption(\n-        clipnorm=1, clipvalue=1)\n-    config = gradients_clip_option.get_config()\n-    restored = optimizer_lib.GradientsClipOption(**config)\n-    self.assertDictEqual(restored.get_config(), config)\n-\n-  def test_invalid_ema_option(self):\n-    ema_option = optimizer_lib.EMAOption(\n-        use_ema=True, ema_momentum=0.5, ema_overwrite_frequency=50)\n-    self.assertEqual(ema_option.ema_momentum, 0.5)\n-    self.assertEqual(ema_option.ema_overwrite_frequency, 50)\n-    with self.assertRaisesRegex(ValueError, \"`ema_momentum` must be in the*\"):\n-      _ = optimizer_lib.EMAOption(use_ema=True, ema_momentum=-1)\n-\n-\n-if __name__ == \"__main__\":\n-  tf.test.main()\n\n@@ -12,7 +12,6 @@ import keras\n from keras.optimizer_experimental import adadelta as adadelta_new\n from keras.optimizer_experimental import adagrad as adagrad_new\n from keras.optimizer_experimental import adam as adam_new\n-from keras.optimizer_experimental import optimizer_lib\n from keras.optimizer_experimental import rmsprop as rmsprop_new\n from keras.optimizer_experimental import sgd as sgd_new\n from keras.optimizer_v2 import adadelta as adadelta_old\n@@ -42,9 +41,7 @@ STRATEGIES = [\n adadelta_new_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentaladadelta\",\n     lambda: adadelta_new.Adadelta(  # pylint: disable=g-long-lambda\n-        0.002,\n-        ema_option=optimizer_lib.EMAOption(\n-            use_ema=True, ema_overwrite_frequency=None)))\n+        0.002, use_ema=True, ema_overwrite_frequency=None))\n adagrad_new_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentaladagrad\", lambda: adagrad_new.Adagrad(0.002))\n adam_new_fn = tf.__internal__.test.combinations.NamedObject(\n@@ -54,9 +51,7 @@ rmsprop_new_fn = tf.__internal__.test.combinations.NamedObject(\n sgd_new_fn = tf.__internal__.test.combinations.NamedObject(\n     \"experimentalsgdaverage\",\n     lambda: sgd_new.SGD(  # pylint: disable=g-long-lambda\n-        0.002,\n-        ema_option=optimizer_lib.EMAOption(\n-            use_ema=True, ema_overwrite_frequency=1)))\n+        0.002, use_ema=True, ema_overwrite_frequency=1))\n \n OPTIMIZER_FN = [\n     adadelta_new_fn,\n@@ -84,22 +79,19 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertEqual(optimizer._index_dict[optimizer._var_key(var_list[7])], 7)\n \n   def testClipNorm(self):\n-    gradients_clip_option = optimizer_lib.GradientsClipOption(clipnorm=1)\n-    optimizer = adam_new.Adam(gradients_clip_option=gradients_clip_option)\n+    optimizer = adam_new.Adam(clipnorm=1)\n     grad = [tf.convert_to_tensor([100.0, 100.0])]\n     clipped_grad = optimizer._clip_gradients(grad)\n     self.assertAllClose(clipped_grad[0], [2**0.5 / 2, 2**0.5 / 2])\n \n   def testClipValue(self):\n-    gradients_clip_option = optimizer_lib.GradientsClipOption(clipvalue=1)\n-    optimizer = adam_new.Adam(gradients_clip_option=gradients_clip_option)\n+    optimizer = adam_new.Adam(clipvalue=1)\n     grad = [tf.convert_to_tensor([100.0, 100.0])]\n     clipped_grad = optimizer._clip_gradients(grad)\n     self.assertAllEqual(clipped_grad[0], [1.0, 1.0])\n \n   def testClipGlobalNorm(self):\n-    gradients_clip_option = optimizer_lib.GradientsClipOption(global_clipnorm=1)\n-    optimizer = adam_new.Adam(gradients_clip_option=gradients_clip_option)\n+    optimizer = adam_new.Adam(global_clipnorm=1)\n     grad = [\n         tf.cast([100.0, 100.0], dtype=tf.float32),\n         tf.cast([100.0, 100.0], dtype=tf.float32)\n@@ -111,17 +103,14 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n     with self.assertLogs(level=\"WARNING\") as log_output:\n       logging.set_verbosity(logging.WARNING)\n       _ = adam_new.Adam(clipnorm=1, decay=0.5)\n-      expected_log_0 = \"clipnorm is deprecated in\"\n-      expected_log_1 = \"decay is deprecated in\"\n-      output0 = log_output[0][0].message\n-      output1 = log_output[0][1].message\n+      expected_log = \"decay is deprecated in\"\n+      output = log_output[0][0].message\n \n-      self.assertTrue(re.search(expected_log_0, output0))\n-      self.assertTrue(re.search(expected_log_1, output1))\n+      self.assertTrue(re.search(expected_log, output))\n \n   def testPassingLegacyClipnorm(self):\n     optimizer = adam_new.Adam(clipnorm=1)\n-    self.assertEqual(optimizer._gradients_clip_option.clipnorm, 1)\n+    self.assertEqual(optimizer._clipnorm, 1)\n \n   def testReturnAllOptimizerVariables(self):\n     x = tf.Variable([[1.0, 2.0], [3.0, 4.0]], dtype=tf.float32)\n@@ -161,9 +150,8 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n   def testMovingAverageOptimizer(self):\n     # We set polyak averaging with ema_momentum = 1 so that the\n     #  moving average is always the original value of the variables.\n-    ema_option = optimizer_lib.EMAOption(\n+    optimizer = adam_new.Adam(\n         use_ema=True, ema_momentum=1, ema_overwrite_frequency=2)\n-    optimizer = adam_new.Adam(ema_option=ema_option)\n     x = tf.Variable([1.0, 2.0], dtype=tf.float32)\n     x_origin = tf.Variable(x)\n     grads = tf.convert_to_tensor([1.0, 2.0])\n@@ -177,17 +165,16 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n     self.assertAllEqual(x, x_origin)\n \n   def testGetAndFromConfig(self):\n-    gradients_clip_option = optimizer_lib.GradientsClipOption(clipnorm=0.5)\n-    ema_option = optimizer_lib.EMAOption(\n-        use_ema=True, ema_momentum=0.5, ema_overwrite_frequency=50)\n     optimizer = adam_new.Adam(\n         learning_rate=np.float64(0.05),\n         beta_1=0.7,\n         beta_2=0.77,\n         amsgrad=True,\n         epsilon=0.001,\n-        gradients_clip_option=gradients_clip_option,\n-        ema_option=ema_option)\n+        clipnorm=0.5,\n+        use_ema=True,\n+        ema_momentum=0.5,\n+        ema_overwrite_frequency=50)\n     config = optimizer.get_config()\n     self.assertDictEqual(\n         config, {\n@@ -196,16 +183,13 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n             \"beta_2\": 0.77,\n             \"epsilon\": 0.001,\n             \"amsgrad\": True,\n-            \"gradients_clip_option\": {\n             \"clipnorm\": 0.5,\n             \"global_clipnorm\": None,\n             \"clipvalue\": None,\n-            },\n-            \"ema_option\": {\n             \"use_ema\": True,\n             \"ema_momentum\": 0.5,\n             \"ema_overwrite_frequency\": 50,\n-            }\n+            \"jit_compile\": False,\n         })\n     restored_optimizer = adam_new.Adam.from_config(config)\n     self.assertDictEqual(restored_optimizer.get_config(),\n@@ -246,8 +230,7 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n         [keras.layers.Input(shape=(1,)),\n          keras.layers.Dense(1)])\n     optimizer = optimizer_fn()\n-    gradients_clip_option = optimizer_lib.GradientsClipOption(clipnorm=0.1)\n-    optimizer._gradients_clip_option = gradients_clip_option\n+    optimizer._clipnorm = 0.1\n     x = tf.expand_dims(tf.convert_to_tensor([1, 1, 1, 0, 0, 0]), axis=1)\n     y = tf.expand_dims(tf.convert_to_tensor([1, 1, 1, 0, 0, 0]), axis=1)\n     model.compile(loss=\"mse\", optimizer=optimizer)\n@@ -261,7 +244,7 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n     loaded_optimizer = loaded_model.optimizer\n     self.assertEqual(type(optimizer), type(loaded_optimizer))\n     self.assertEqual(loaded_optimizer.learning_rate, 0.002)\n-    self.assertEqual(loaded_optimizer._gradients_clip_option.clipnorm, 0.1)\n+    self.assertEqual(loaded_optimizer._clipnorm, 0.1)\n \n     # Save in Keras SavedModel format.\n     model.fit(x, y)\n@@ -272,7 +255,7 @@ class OptimizerFuntionalityTest(tf.test.TestCase, parameterized.TestCase):\n     loaded_optimizer = loaded_model.optimizer\n     self.assertEqual(type(optimizer), type(loaded_optimizer))\n     self.assertEqual(loaded_optimizer.learning_rate, 0.002)\n-    self.assertEqual(loaded_optimizer._gradients_clip_option.clipnorm, 0.1)\n+    self.assertEqual(loaded_optimizer._clipnorm, 0.1)\n \n \n class OptimizerRegressionTest(tf.test.TestCase, parameterized.TestCase):\n@@ -394,13 +377,9 @@ class DistributedTrainingTest(tf.test.TestCase, parameterized.TestCase):\n     # Test the optimizer yields same numerical results when jit_compile is\n     # on and off.\n     with strategy.scope():\n-      optimizer_1 = adam_new.Adam(\n-          ema_option=optimizer_lib.EMAOption(\n-              use_ema=True, ema_overwrite_frequency=1))\n+      optimizer_1 = adam_new.Adam(use_ema=True, ema_overwrite_frequency=1)\n       optimizer_2 = adam_new.Adam(\n-          jit_compile=True,\n-          ema_option=optimizer_lib.EMAOption(\n-              use_ema=True, ema_overwrite_frequency=1))\n+          jit_compile=True, use_ema=True, ema_overwrite_frequency=1)\n       model_1 = keras.Sequential([\n           keras.layers.Input(shape=(2,)),\n           keras.layers.Dense(5),\n\n@@ -49,22 +49,21 @@ class RMSprop(optimizer.Optimizer):\n       variance of the gradient; if False, by the uncentered second moment.\n       Setting this to `True` may help with training, but is slightly more\n       expensive in terms of computation and memory. Defaults to `False`.\n-    gradients_clip_option: an instance of\n-      `optimizer_experimental.GradientsClipOption`, for attributes related to\n-      gradients clipping, such as clipnorm and clipvalue. Default to None\n-      (not applying gradients clipping).\n-    ema_option: an instance of `optimizer_experimental.EMAOption`, for\n-      attributes related to exponenatial moving average, such as `use_ema` (a\n-      boolean field indicates if EMA is used) and EMA momentum. Default to None\n-      (not applying EMA).\n-    jit_compile: Bool, default to False. If True, the optimizer will use XLA\n-      acceleration. `jit_compile` can only be False when using Parameter\n-      Server Strategy.\n+    clipnorm: see the `clipnorm` argument of `optimizer_experimental.Optimizer`.\n+    clipvalue: see the `clipvalue` argument of\n+      `optimizer_experimental.Optimizer`.\n+    global_clipnorm: see the `global_clipnorm` argument of\n+      `optimizer_experimental.Optimizer`.\n+    use_ema: see the `use_ema` argument of `optimizer_experimental.Optimizer`.\n+    ema_momentum: see the `ema_momentum` argument of\n+      `optimizer_experimental.Optimizer`.\n+    ema_overwrite_frequency: see the `ema_overwrite_frequency` argument of\n+      `optimizer_experimental.Optimizer`.\n+    jit_compile: see the `jit_compile` argument of\n+      `optimizer_experimental.Optimizer`.\n     name: Optional name prefix for the operations created when applying\n       gradients. Defaults to `\"RMSprop\"`.\n-    **kwargs: keyword arguments only used for backward compatibility with\n-      `optimizer_v2.OptimizerV2`. Any new code using\n-      `optimizer_experimental.Optimizer` should leave this parameter empty.\n+    **kwargs: see the `**kwargs` argument of `optimizer_experimental.Optimizer`.\n \n   Usage:\n \n@@ -87,14 +86,22 @@ class RMSprop(optimizer.Optimizer):\n                momentum=0.0,\n                epsilon=1e-7,\n                centered=False,\n-               gradients_clip_option=None,\n-               ema_option=None,\n+               clipnorm=None,\n+               clipvalue=None,\n+               global_clipnorm=None,\n+               use_ema=False,\n+               ema_momentum=0.99,\n+               ema_overwrite_frequency=100,\n                jit_compile=False,\n                name='RMSprop',\n                **kwargs):\n     super(RMSprop, self).__init__(\n-        gradients_clip_option=gradients_clip_option,\n-        ema_option=ema_option,\n+        clipnorm=clipnorm,\n+        clipvalue=clipvalue,\n+        global_clipnorm=global_clipnorm,\n+        use_ema=use_ema,\n+        ema_momentum=ema_momentum,\n+        ema_overwrite_frequency=ema_overwrite_frequency,\n         jit_compile=jit_compile,\n         name=name,\n         **kwargs)\n\n@@ -54,22 +54,21 @@ class SGD(optimizer.Optimizer):\n       descent.\n     nesterov: boolean. Whether to apply Nesterov momentum.\n       Defaults to `False`.\n-    gradients_clip_option: an instance of\n-      `optimizer_experimental.GradientsClipOption`, for attributes related to\n-      gradients clipping, such as clipnorm and clipvalue. Default to None\n-      (not apply gradients clipping).\n-    ema_option: an instance of `optimizer_experimental.EMAOption`, for\n-      attributes related to exponenatial moving average, such as use_ema (a\n-      boolean field indicates if EMA is used) and EMA momentum. Default to None\n-      (not apply EMA).\n-    jit_compile: Bool, default to False. If True, the optimizer will use XLA\n-        acceleration. `jit_compile` can only be False when using Parameter\n-        Server Strategy.\n+    clipnorm: see the `clipnorm` argument of `optimizer_experimental.Optimizer`.\n+    clipvalue: see the `clipvalue` argument of\n+      `optimizer_experimental.Optimizer`.\n+    global_clipnorm: see the `global_clipnorm` argument of\n+      `optimizer_experimental.Optimizer`.\n+    use_ema: see the `use_ema` argument of `optimizer_experimental.Optimizer`.\n+    ema_momentum: see the `ema_momentum` argument of\n+      `optimizer_experimental.Optimizer`.\n+    ema_overwrite_frequency: see the `ema_overwrite_frequency` argument of\n+      `optimizer_experimental.Optimizer`.\n+    jit_compile: see the `jit_compile` argument of\n+      `optimizer_experimental.Optimizer`.\n     name: Optional name prefix for the operations created when applying\n       gradients. Defaults to `\"SGD\"`.\n-    **kwargs: keyword arguments only used for backward compatibility with\n-      `optimizer_v2.OptimizerV2`. Any new code using\n-      `optimizer_experimental.Optimizer` should leave this parameter empty.\n+    **kwargs: see the `**kwargs` argument of `optimizer_experimental.Optimizer`.\n \n   Usage:\n \n@@ -106,15 +105,23 @@ class SGD(optimizer.Optimizer):\n                momentum=0.0,\n                nesterov=False,\n                amsgrad=False,\n-               gradients_clip_option=None,\n-               ema_option=None,\n+               clipnorm=None,\n+               clipvalue=None,\n+               global_clipnorm=None,\n+               use_ema=False,\n+               ema_momentum=0.99,\n+               ema_overwrite_frequency=100,\n                jit_compile=False,\n                name='SGD',\n                **kwargs):\n     super(SGD, self).__init__(\n         name=name,\n-        gradients_clip_option=gradients_clip_option,\n-        ema_option=ema_option,\n+        clipnorm=clipnorm,\n+        clipvalue=clipvalue,\n+        global_clipnorm=global_clipnorm,\n+        use_ema=use_ema,\n+        ema_momentum=ema_momentum,\n+        ema_overwrite_frequency=ema_overwrite_frequency,\n         jit_compile=jit_compile,\n         **kwargs)\n     self._learning_rate = self._build_learning_rate(learning_rate)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
