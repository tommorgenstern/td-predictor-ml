{"custom_id": "keras#817b16e0caeb2b779ea9fdf9843d2c6a78f95c39", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 5 | Files Changed: 4 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 5862 | Contributors (this commit): 19 | Commits (past 90d): 19 | Contributors (cumulative): 30 | DMM Complexity: 1.0\n\nDIFF:\n@@ -196,7 +196,7 @@ class Dense(Layer):\n         outputs = tf.nn.embedding_lookup_sparse(\n             self.kernel, ids, weights, combiner='sum')\n       else:\n-        outputs = tf.matmul(a=inputs, b=self.kernel)\n+        outputs = tf.raw_ops.MatMul(a=inputs, b=self.kernel)\n     # Broadcast kernel to inputs.\n     else:\n       outputs = tf.tensordot(inputs, self.kernel, [[rank - 1], [0]])\n\n@@ -210,7 +210,7 @@ class RandomFourierFeatures(base_layer.Layer):\n     inputs = tf.convert_to_tensor(inputs, dtype=self.dtype)\n     inputs = tf.cast(inputs, tf.float32)\n     kernel = (1.0 / self.kernel_scale) * self.unscaled_kernel\n-    outputs = tf.matmul(a=inputs, b=kernel)\n+    outputs = tf.raw_ops.MatMul(a=inputs, b=kernel)\n     outputs = tf.nn.bias_add(outputs, self.bias)\n     return tf.cos(outputs)\n \n\n@@ -775,8 +775,10 @@ def local_conv_sparse_matmul(inputs, kernel, kernel_idxs, kernel_shape,\n       Output (N+2)-D dense tensor with shape `output_shape`.\n   \"\"\"\n   inputs_flat = backend.reshape(inputs, (backend.shape(inputs)[0], -1))\n-  output_flat = tf.sparse.sparse_dense_matmul(\n-      sp_a=tf.SparseTensor(kernel_idxs, kernel, kernel_shape),\n+  output_flat = tf.raw_ops.SparseTensorDenseMatMul(\n+      a_indices=kernel_idxs,\n+      a_values=kernel,\n+      a_shape=kernel_shape,\n       b=inputs_flat,\n       adjoint_b=True)\n   output_flat_transpose = backend.transpose(output_flat)\n\n@@ -642,7 +642,7 @@ def update_confusion_matrix_variables(variables_to_update,\n   if y_pred.shape.ndims == 1:\n     num_labels = 1\n   else:\n-    num_labels = tf.math.reduce_prod(pred_shape[1:], axis=0)\n+    num_labels = tf.raw_ops.Prod(input=pred_shape[1:], axis=0)\n   thresh_label_tile = tf.where(one_thresh, num_labels,\n                                          tf.ones([], dtype=tf.int32))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6cf8b3493bd834a2c2b273abb03a1c9751a5580e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 101 | Lines Deleted: 153 | Files Changed: 2 | Hunks: 42 | Methods Changed: 28 | Complexity Δ (Sum/Max): -4/0 | Churn Δ: 254 | Churn Cumulative: 4834 | Contributors (this commit): 6 | Commits (past 90d): 18 | Contributors (cumulative): 11 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,15 +14,16 @@\n # ==============================================================================\n \"\"\"Keras image preprocessing layers.\"\"\"\n \n-import tensorflow.compat.v2 as tf\n # pylint: disable=g-classes-have-attributes\n+# pylint: disable=g-direct-tensorflow-import\n \n-import numpy as np\n from keras import backend\n from keras.engine import base_layer\n from keras.engine import base_preprocessing_layer\n-from keras.preprocessing import image as image_preprocessing\n+from keras.preprocessing.image import smart_resize\n from keras.utils import control_flow_util\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n from tensorflow.python.util.tf_export import keras_export\n \n ResizeMethod = tf.image.ResizeMethod\n@@ -83,8 +84,8 @@ class Resizing(base_layer.Layer):\n                interpolation='bilinear',\n                crop_to_aspect_ratio=False,\n                **kwargs):\n-    self.target_height = height\n-    self.target_width = width\n+    self.height = height\n+    self.width = width\n     self.interpolation = interpolation\n     self.crop_to_aspect_ratio = crop_to_aspect_ratio\n     self._interpolation_method = get_interpolation(interpolation)\n@@ -93,27 +94,27 @@ class Resizing(base_layer.Layer):\n \n   def call(self, inputs):\n     if self.crop_to_aspect_ratio:\n-      outputs = image_preprocessing.smart_resize(\n+      outputs = smart_resize(\n           inputs,\n-          size=[self.target_height, self.target_width],\n+          size=[self.height, self.width],\n           interpolation=self._interpolation_method)\n     else:\n       outputs = tf.image.resize(\n           inputs,\n-          size=[self.target_height, self.target_width],\n+          size=[self.height, self.width],\n           method=self._interpolation_method)\n     return outputs\n \n   def compute_output_shape(self, input_shape):\n     input_shape = tf.TensorShape(input_shape).as_list()\n-    input_shape[H_AXIS] = self.target_height\n-    input_shape[W_AXIS] = self.target_width\n+    input_shape[H_AXIS] = self.height\n+    input_shape[W_AXIS] = self.width\n     return tf.TensorShape(input_shape)\n \n   def get_config(self):\n     config = {\n-        'height': self.target_height,\n-        'width': self.target_width,\n+        'height': self.height,\n+        'width': self.width,\n         'interpolation': self.interpolation,\n         'crop_to_aspect_ratio': self.crop_to_aspect_ratio,\n     }\n@@ -126,8 +127,10 @@ class Resizing(base_layer.Layer):\n class CenterCrop(base_layer.Layer):\n   \"\"\"A preprocessing layer which crops images.\n \n-  This layers crops the central portion of the images to a target height and\n-  width.\n+  This layers crops the central portion of the images to a target size. If an\n+  image is smaller than the target size, it will be resized and cropped so as to\n+  return the largest possible window in the image that matches the target aspect\n+  ratio.\n \n   For an overview and full list of preprocessing layers, see the preprocessing\n   [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n@@ -149,52 +152,38 @@ class CenterCrop(base_layer.Layer):\n   \"\"\"\n \n   def __init__(self, height, width, **kwargs):\n-    self.target_height = height\n-    self.target_width = width\n-    super(CenterCrop, self).__init__(**kwargs)\n+    self.height = height\n+    self.width = width\n+    super(CenterCrop, self).__init__(**kwargs, autocast=False)\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('CenterCrop').set(True)\n \n   def call(self, inputs):\n     inputs = tf.convert_to_tensor(inputs)\n-    inputs_shape = tf.shape(inputs)\n-    unbatched = inputs.shape.rank == 3\n-    img_hd = inputs_shape[H_AXIS]\n-    img_wd = inputs_shape[W_AXIS]\n-    img_hd_diff = img_hd - self.target_height\n-    img_wd_diff = img_wd - self.target_width\n-    checks = []\n-    checks.append(\n-        tf.debugging.assert_non_negative(\n-            img_hd_diff,\n-            message='The crop height {} should not be greater than input '\n-            'height.'.format(self.target_height)))\n-    checks.append(\n-        tf.debugging.assert_non_negative(\n-            img_wd_diff,\n-            message='The crop width {} should not be greater than input '\n-            'width.'.format(self.target_width)))\n-    with tf.control_dependencies(checks):\n-      bbox_h_start = tf.cast(img_hd_diff / 2, tf.int32)\n-      bbox_w_start = tf.cast(img_wd_diff / 2, tf.int32)\n-      if unbatched:\n-        bbox_begin = tf.stack([bbox_h_start, bbox_w_start, 0])\n-        bbox_size = tf.stack([self.target_height, self.target_width, -1])\n-      else:\n-        bbox_begin = tf.stack([0, bbox_h_start, bbox_w_start, 0])\n-        bbox_size = tf.stack([-1, self.target_height, self.target_width, -1])\n-      outputs = tf.slice(inputs, bbox_begin, bbox_size)\n-      return outputs\n+    input_shape = tf.shape(inputs)\n+    h_diff = input_shape[H_AXIS] - self.height\n+    w_diff = input_shape[W_AXIS] - self.width\n+\n+    def center_crop():\n+      h_start = tf.cast(h_diff / 2, tf.int32)\n+      w_start = tf.cast(w_diff / 2, tf.int32)\n+      return tf.image.crop_to_bounding_box(inputs, h_start, w_start,\n+                                           self.height, self.width)\n+\n+    outputs = tf.cond(\n+        tf.reduce_all((h_diff >= 0, w_diff >= 0)), center_crop,\n+        lambda: smart_resize(inputs, [self.height, self.width]))\n+    return tf.cast(outputs, inputs.dtype)\n \n   def compute_output_shape(self, input_shape):\n     input_shape = tf.TensorShape(input_shape).as_list()\n-    input_shape[H_AXIS] = self.target_height\n-    input_shape[W_AXIS] = self.target_width\n+    input_shape[H_AXIS] = self.height\n+    input_shape[W_AXIS] = self.width\n     return tf.TensorShape(input_shape)\n \n   def get_config(self):\n     config = {\n-        'height': self.target_height,\n-        'width': self.target_width,\n+        'height': self.height,\n+        'width': self.width,\n     }\n     base_config = super(CenterCrop, self).get_config()\n     return dict(list(base_config.items()) + list(config.items()))\n@@ -205,12 +194,15 @@ class CenterCrop(base_layer.Layer):\n class RandomCrop(base_layer.Layer):\n   \"\"\"A preprocessing layer which randomly crops images during training.\n \n-  This layer will crop all the images in the same batch to the same cropping\n-  location.\n-  By default, random cropping is only applied during training. At inference\n-  time, the images will be first rescaled to preserve the shorter side, and\n-  center cropped. If you need to apply random cropping at inference time,\n-  set `training` to True when calling the layer.\n+  During training, this layer will randomly choose a location to crop images\n+  down to a target size. The layer will crop all the images in the same batch to\n+  the same cropping location.\n+\n+  At inference time, and during training if an input image is smaller than the\n+  target size, the input will be resized and cropped so as to return the largest\n+  possible window in the image that matches the target aspect ratio. If you need\n+  to apply random cropping at inference time, set `training` to True when\n+  calling the layer.\n \n   For an overview and full list of preprocessing layers, see the preprocessing\n   [guide](https://www.tensorflow.org/guide/keras/preprocessing_layers).\n@@ -231,7 +223,7 @@ class RandomCrop(base_layer.Layer):\n \n   def __init__(self, height, width, seed=None, **kwargs):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell('RandomCrop').set(True)\n-    super(RandomCrop, self).__init__(**kwargs)\n+    super(RandomCrop, self).__init__(**kwargs, autocast=False)\n     self.height = height\n     self.width = width\n     self.seed = seed\n@@ -240,71 +232,23 @@ class RandomCrop(base_layer.Layer):\n   def call(self, inputs, training=True):\n     if training is None:\n       training = backend.learning_phase()\n-\n     inputs = tf.convert_to_tensor(inputs)\n-    unbatched = inputs.shape.rank == 3\n-\n-    def random_cropped_inputs():\n-      \"\"\"Cropped inputs with stateless random ops.\"\"\"\n-      shape = tf.shape(inputs)\n-      if unbatched:\n-        crop_size = tf.stack([self.height, self.width, shape[-1]])\n-      else:\n-        crop_size = tf.stack([shape[0], self.height, self.width, shape[-1]])\n-      check = tf.Assert(\n-          tf.reduce_all(shape >= crop_size),\n-          [self.height, self.width])\n-      with tf.control_dependencies([check]):\n-        limit = shape - crop_size + 1\n-        offset = self._random_generator.random_uniform(\n-            tf.shape(shape),\n-            dtype=crop_size.dtype,\n-            minval=0,\n-            maxval=crop_size.dtype.max) % limit\n-        return tf.slice(inputs, offset, crop_size)\n-\n-    # TODO(b/143885775): Share logic with Resize and CenterCrop.\n-    def resize_and_center_cropped_inputs():\n-      \"\"\"Deterministically resize to shorter side and center crop.\"\"\"\n     input_shape = tf.shape(inputs)\n-      input_height_t = input_shape[H_AXIS]\n-      input_width_t = input_shape[W_AXIS]\n-      ratio_cond = (input_height_t / input_width_t > (self.height / self.width))\n-      # pylint: disable=g-long-lambda\n-      resized_height = control_flow_util.smart_cond(\n-          ratio_cond,\n-          lambda: tf.cast(self.width * input_height_t / input_width_t,\n-                          input_height_t.dtype), lambda: self.height)\n-      resized_width = control_flow_util.smart_cond(\n-          ratio_cond, lambda: self.width,\n-          lambda: tf.cast(self.height * input_width_t / input_height_t,\n-                          input_width_t.dtype))\n-      # pylint: enable=g-long-lambda\n-      resized_inputs = tf.image.resize(\n-          images=inputs, size=tf.stack([resized_height, resized_width]))\n+    h_diff = input_shape[H_AXIS] - self.height\n+    w_diff = input_shape[W_AXIS] - self.width\n \n-      img_hd_diff = resized_height - self.height\n-      img_wd_diff = resized_width - self.width\n-      bbox_h_start = tf.cast(img_hd_diff / 2, tf.int32)\n-      bbox_w_start = tf.cast(img_wd_diff / 2, tf.int32)\n-      if unbatched:\n-        bbox_begin = tf.stack([bbox_h_start, bbox_w_start, 0])\n-        bbox_size = tf.stack([self.height, self.width, -1])\n-      else:\n-        bbox_begin = tf.stack([0, bbox_h_start, bbox_w_start, 0])\n-        bbox_size = tf.stack([-1, self.height, self.width, -1])\n-      outputs = tf.slice(resized_inputs, bbox_begin, bbox_size)\n-      return outputs\n+    def random_crop():\n+      dtype = input_shape.dtype\n+      rands = self._random_generator.random_uniform([2], 0, dtype.max, dtype)\n+      h_start = rands[0] % (h_diff + 1)\n+      w_start = rands[1] % (w_diff + 1)\n+      return tf.image.crop_to_bounding_box(inputs, h_start, w_start,\n+                                           self.height, self.width)\n \n-    output = control_flow_util.smart_cond(training, random_cropped_inputs,\n-                                          resize_and_center_cropped_inputs)\n-    input_shape = inputs.shape.as_list()\n-    if unbatched:\n-      output_shape = [self.height, self.width, input_shape[-1]]\n-    else:\n-      output_shape = [input_shape[0], self.height, self.width, input_shape[-1]]\n-    output.set_shape(output_shape)\n-    return output\n+    outputs = tf.cond(\n+        tf.reduce_all((training, h_diff >= 0, w_diff >= 0)), random_crop,\n+        lambda: smart_resize(inputs, [self.height, self.width]))\n+    return tf.cast(outputs, inputs.dtype)\n \n   def compute_output_shape(self, input_shape):\n     input_shape = tf.TensorShape(input_shape).as_list()\n@@ -1427,4 +1371,3 @@ def get_interpolation(interpolation):\n         'Value not recognized for `interpolation`: {}. Supported values '\n         'are: {}'.format(interpolation, _RESIZE_METHODS.keys()))\n   return _RESIZE_METHODS[interpolation]\n-\n\n@@ -242,16 +242,18 @@ class CenterCropTest(keras_parameterized.TestCase):\n   def test_center_crop_half_mis_aligned(self, expected_height, expected_width):\n     self._run_test(expected_height, expected_width)\n \n-  @parameterized.named_parameters(('center_crop_5_by_12', 5, 12),\n-                                  ('center_crop_10_by_8', 10, 8),\n-                                  ('center_crop_10_by_12', 10, 12))\n-  def test_invalid_center_crop(self, expected_height, expected_width):\n-    # InternelError is raised by tf.function MLIR lowering pass when TFRT\n-    # is enabled.\n-    with self.assertRaisesRegex(\n-        (tf.errors.InvalidArgumentError, tf.errors.InternalError),\n-        r'assertion failed|error: \\'tf.Slice\\' op'):\n-      self._run_test(expected_height, expected_width)\n+  def test_input_smaller_than_crop_box(self):\n+    np.random.seed(1337)\n+    height, width = 10, 8\n+    inp = np.random.random((12, 3, 3, 3))\n+    with testing_utils.use_gpu():\n+      layer = image_preprocessing.CenterCrop(height, width)\n+      actual_output = layer(inp)\n+      # In this case, output should equal resizing with crop_to_aspect ratio.\n+      resize_layer = image_preprocessing.Resizing(\n+          height, width, crop_to_aspect_ratio=True)\n+      expected_output = resize_layer(inp)\n+      self.assertAllEqual(expected_output, actual_output)\n \n   def test_config_with_custom_name(self):\n     layer = image_preprocessing.CenterCrop(5, 5, name='image_preproc')\n@@ -290,28 +292,31 @@ class RandomCropTest(keras_parameterized.TestCase):\n           expected_output_shape=(None, expected_height, expected_width,\n                                  channels))\n \n-  @parameterized.named_parameters(('random_crop_5_by_12', 5, 12),\n-                                  ('random_crop_10_by_8', 10, 8),\n-                                  ('random_crop_10_by_12', 10, 12))\n-  def test_invalid_random_crop(self, expected_height, expected_width):\n-    # InternelError is raised by tf.function MLIR lowering pass when TFRT\n-    # is enabled.\n-    with self.assertRaises(\n-        (tf.errors.InvalidArgumentError, tf.errors.InternalError)):\n-      self._run_test(expected_height, expected_width)\n+  def test_input_smaller_than_crop_box(self):\n+    np.random.seed(1337)\n+    height, width = 10, 8\n+    inp = np.random.random((12, 3, 3, 3))\n+    with testing_utils.use_gpu():\n+      layer = image_preprocessing.RandomCrop(height, width)\n+      actual_output = layer(inp)\n+      # In this case, output should equal resizing with crop_to_aspect ratio.\n+      resize_layer = image_preprocessing.Resizing(\n+          height, width, crop_to_aspect_ratio=True)\n+      expected_output = resize_layer(inp)\n+      self.assertAllEqual(expected_output, actual_output)\n \n   def test_training_with_mock(self):\n     np.random.seed(1337)\n     height, width = 3, 4\n     height_offset = np.random.randint(low=0, high=3)\n     width_offset = np.random.randint(low=0, high=5)\n-    mock_offset = [0, height_offset, width_offset, 0]\n+    mock_offset = [height_offset, width_offset]\n     with testing_utils.use_gpu():\n       layer = image_preprocessing.RandomCrop(height, width)\n       with tf.compat.v1.test.mock.patch.object(\n           layer._random_generator, 'random_uniform', return_value=mock_offset):\n         inp = np.random.random((12, 5, 8, 3))\n-        actual_output = layer(inp, training=1)\n+        actual_output = layer(inp, training=True)\n         expected_output = inp[:, height_offset:(height_offset + height),\n                               width_offset:(width_offset + width), :]\n         self.assertAllClose(expected_output, actual_output)\n@@ -333,7 +338,7 @@ class RandomCropTest(keras_parameterized.TestCase):\n     inp = np.random.random((12, 8, 16, 3))\n     with testing_utils.use_gpu():\n       layer = image_preprocessing.RandomCrop(height, width)\n-      actual_output = layer(inp, training=0)\n+      actual_output = layer(inp, training=False)\n       self.assertAllClose(inp, actual_output)\n \n   def test_predicting_with_mock_longer_height(self):\n@@ -342,7 +347,7 @@ class RandomCropTest(keras_parameterized.TestCase):\n     inp = np.random.random((12, 10, 6, 3))\n     with testing_utils.use_gpu():\n       layer = image_preprocessing.RandomCrop(height, width)\n-      actual_output = layer(inp, training=0)\n+      actual_output = layer(inp, training=False)\n       resized_inp = tf.image.resize(inp, size=[5, 3])\n       expected_output = resized_inp[:, 1:4, :, :]\n       self.assertAllClose(expected_output, actual_output)\n@@ -353,7 +358,7 @@ class RandomCropTest(keras_parameterized.TestCase):\n     inp = np.random.random((12, 8, 16, 3))\n     with testing_utils.use_gpu():\n       layer = image_preprocessing.RandomCrop(height, width)\n-      actual_output = layer(inp, training=0)\n+      actual_output = layer(inp, training=False)\n       resized_inp = tf.image.resize(inp, size=[4, 8])\n       expected_output = resized_inp[:, :, 1:7, :]\n       self.assertAllClose(expected_output, actual_output)\n@@ -367,14 +372,14 @@ class RandomCropTest(keras_parameterized.TestCase):\n   def test_unbatched_image(self):\n     np.random.seed(1337)\n     inp = np.random.random((16, 16, 3))\n-    mock_offset = [2, 2, 0]\n+    mock_offset = [2, 2]\n     with testing_utils.use_gpu():\n       layer = image_preprocessing.RandomCrop(8, 8)\n       with tf.compat.v1.test.mock.patch.object(\n           layer._random_generator,\n           'random_uniform',\n           return_value=mock_offset):\n-        actual_output = layer(inp, training=1)\n+        actual_output = layer(inp, training=True)\n         self.assertAllClose(inp[2:10, 2:10, :], actual_output)\n \n \n@@ -444,7 +449,7 @@ class RandomFlipTest(keras_parameterized.TestCase):\n     ):\n       with testing_utils.use_gpu():\n         layer = image_preprocessing.RandomFlip(mode)\n-        actual_output = layer(inp, training=1)\n+        actual_output = layer(inp, training=True)\n         self.assertAllClose(expected_output, actual_output)\n \n   @parameterized.named_parameters(\n@@ -477,7 +482,7 @@ class RandomFlipTest(keras_parameterized.TestCase):\n     expected_output = input_images\n     with testing_utils.use_gpu():\n       layer = image_preprocessing.RandomFlip()\n-      actual_output = layer(input_images, training=0)\n+      actual_output = layer(input_images, training=False)\n       self.assertAllClose(expected_output, actual_output)\n \n   def test_random_flip_default(self):\n@@ -492,7 +497,7 @@ class RandomFlipTest(keras_parameterized.TestCase):\n     ):\n       with self.cached_session():\n         layer = image_preprocessing.RandomFlip()\n-        actual_output = layer(input_images, training=1)\n+        actual_output = layer(input_images, training=True)\n         self.assertAllClose(expected_output, actual_output)\n \n   @testing_utils.run_v2_only\n@@ -513,7 +518,7 @@ class RandomFlipTest(keras_parameterized.TestCase):\n     ):\n       with self.cached_session():\n         layer = image_preprocessing.RandomFlip('vertical')\n-        actual_output = layer(input_image, training=1)\n+        actual_output = layer(input_image, training=True)\n         self.assertAllClose(expected_output, actual_output)\n \n \n@@ -768,7 +773,7 @@ class RandomTranslationTest(keras_parameterized.TestCase):\n     expected_output = input_images\n     with testing_utils.use_gpu():\n       layer = image_preprocessing.RandomTranslation(.5, .5)\n-      actual_output = layer(input_images, training=0)\n+      actual_output = layer(input_images, training=False)\n       self.assertAllClose(expected_output, actual_output)\n \n   @testing_utils.run_v2_only\n@@ -1188,7 +1193,7 @@ class RandomRotationTest(keras_parameterized.TestCase):\n     expected_output = input_images\n     with testing_utils.use_gpu():\n       layer = image_preprocessing.RandomRotation(.5)\n-      actual_output = layer(input_images, training=0)\n+      actual_output = layer(input_images, training=False)\n       self.assertAllClose(expected_output, actual_output)\n \n   def test_distribution_strategy(self):\n@@ -1313,7 +1318,7 @@ class RandomZoomTest(keras_parameterized.TestCase):\n     expected_output = input_images\n     with testing_utils.use_gpu():\n       layer = image_preprocessing.RandomZoom(.5, .5)\n-      actual_output = layer(input_images, training=0)\n+      actual_output = layer(input_images, training=False)\n       self.assertAllClose(expected_output, actual_output)\n \n   @testing_utils.run_v2_only\n@@ -1419,7 +1424,7 @@ class RandomHeightTest(keras_parameterized.TestCase):\n     expected_output = input_images\n     with testing_utils.use_gpu():\n       layer = image_preprocessing.RandomHeight(.5)\n-      actual_output = layer(input_images, training=0)\n+      actual_output = layer(input_images, training=False)\n       self.assertAllClose(expected_output, actual_output)\n \n   @testing_utils.run_v2_only\n@@ -1519,7 +1524,7 @@ class RandomWidthTest(keras_parameterized.TestCase):\n     expected_output = input_images\n     with testing_utils.use_gpu():\n       layer = image_preprocessing.RandomWidth(.5)\n-      actual_output = layer(input_images, training=0)\n+      actual_output = layer(input_images, training=False)\n       self.assertAllClose(expected_output, actual_output)\n \n   @testing_utils.run_v2_only\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d044881de3770d2fcdbbd87f87a6e2392875ccdb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 19189 | Contributors (this commit): 80 | Commits (past 90d): 22 | Contributors (cumulative): 80 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3522,9 +3522,8 @@ class Cropping2D(Layer):\n   def call(self, inputs):\n     # pylint: disable=invalid-unary-operand-type\n     if self.data_format == 'channels_first':\n-      if (inputs.shape[2] is not None and inputs.shape[3] is not None and\n-        (sum(self.cropping[0]) >= inputs.shape[2] or\n-          sum(self.cropping[1]) >= inputs.shape[3])):\n+      if ((inputs.shape[2] is not None and sum(self.cropping[0]) >= inputs.shape[2]) or\n+          (inputs.shape[3] is not None and sum(self.cropping[1]) >= inputs.shape[3])):\n           raise ValueError('cropping parameter of Cropping layer must be '\n                         'greater than the input shape. Received: inputs.shape='\n                         f'{inputs.shape}, and cropping={self.cropping}')  \n@@ -3539,9 +3538,8 @@ class Cropping2D(Layer):\n       return inputs[:, :, self.cropping[0][0]:-self.cropping[0][1],\n                     self.cropping[1][0]:-self.cropping[1][1]]\n     else:\n-      if (inputs.shape[1] is not None and inputs.shape[2] is not None and\n-        (sum(self.cropping[0]) >= inputs.shape[1] or\n-          sum(self.cropping[1]) >= inputs.shape[2])):\n+      if ((inputs.shape[1] is not None and sum(self.cropping[0]) >= inputs.shape[1]) or\n+          (inputs.shape[2] is not None and sum(self.cropping[1]) >= inputs.shape[2])):\n           raise ValueError('cropping parameter of Cropping layer must be '\n                         'greater than the input shape. Received: inputs.shape='\n                         f'{inputs.shape}, and cropping={self.cropping}')   \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6595fd81b17e83000f2ff2abce0e3a93036266f8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 25 | Churn Cumulative: 4152 | Contributors (this commit): 8 | Commits (past 90d): 15 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,6 +19,7 @@ import tensorflow.compat.v2 as tf\n import collections\n import io\n import tempfile\n+import unittest\n import sys\n \n from absl.testing import parameterized\n@@ -1427,6 +1428,30 @@ class TrainingTest(keras_parameterized.TestCase):\n     self.assertIsInstance(loss, float)\n     self.assertIsInstance(accuracy, float)\n \n+  @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n+  @unittest.expectedFailure\n+  def test_loss_acc_is_corrupted(self):\n+    batch_size = 32\n+    n_samples, n_features = batch_size * 10, 5\n+    rng = np.random.RandomState(0)\n+    x = rng.normal(size=(n_samples, n_features))\n+    y = rng.randint(low=0, high=2, size=x.shape[0])\n+    model = sequential.Sequential([layers_module.Dense(1,)])\n+    model.compile('adam', 'binary_crossentropy', metrics=['accuracy'], run_eagerly=testing_utils.should_run_eagerly())\n+    loss = {}\n+    accurancy = {}\n+    loss_1 = {}\n+    accurancy_1 = {}\n+    for i in range(3):\n+      loss[i], accurancy[i] = model.test_on_batch(x[:batch_size], y[:batch_size])\n+    model.evaluate(x[:batch_size],y[:batch_size], batch_size=batch_size, verbose=0)\n+    for i in range(3):\n+      loss_1[i], accurancy_1[i] = model.test_on_batch(x[:batch_size], y[:batch_size])\n+    self.assertAllEqual(loss, loss_1, \n+                        \"https://github.com/keras-team/keras/issues/14086\")\n+    self.assertAllEqual(accurancy, accurancy_1, \n+                        \"https://github.com/keras-team/keras/issues/14086\")\n+\n   @keras_parameterized.run_all_keras_modes(always_skip_v1=True)\n   def test_int_output(self):\n     x, y = np.ones((10, 1)), np.ones((10, 1))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6859a01cf1df31b8f31e2e8b13aa5db793ec2b4e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 57 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 8/5 | Churn Δ: 60 | Churn Cumulative: 2132 | Contributors (this commit): 8 | Commits (past 90d): 12 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -671,14 +671,35 @@ class Functional(training_lib.Model):\n \n     Raises:\n         ValueError: In case of improperly formatted config dict.\n+        TypeError: In case the config does match the cls constructor.\n     \"\"\"\n+\n     with generic_utils.SharedObjectLoadingScope():\n+      if all(key in config for key in [\n+          'name', 'layers', 'input_layers', 'output_layers']):\n         input_tensors, output_tensors, created_layers = reconstruct_from_config(\n             config, custom_objects)\n-      model = cls(inputs=input_tensors, outputs=output_tensors,\n+        model = cls(\n+            inputs=input_tensors,\n+            outputs=output_tensors,\n             name=config.get('name'))\n         connect_ancillary_layers(model, created_layers)\n         return model\n+      # The config does not contain all the information necessary to revive a\n+      # Functional model. This happens when the user creates subclassed models\n+      # with a Functional constructor and has overriden the `get_config` method\n+      # to return a completely new dictionary.\n+      try:\n+        return cls(**config)\n+      except TypeError as e:\n+        raise TypeError('Unable to revive model from config. When overriding '\n+                        'the `get_config`, make sure that the returned config '\n+                        'contains all items used as arguments in the '\n+                        f'constructor to {cls}, which is the default behavior. '\n+                        'You can override this default behavior by defining a '\n+                        '`from_config` method to specify how to create an '\n+                        f'instance of {cls.__name__} from the config. \\n\\n'\n+                        f'Error encountered during deserialization:\\n{e}')\n \n   def _validate_graph_inputs_and_outputs(self):\n     \"\"\"Validates the inputs and outputs of a Graph Network.\"\"\"\n\n@@ -21,7 +21,6 @@ SavedModel have the expected structure.\n \n import tensorflow.compat.v2 as tf\n # TODO(kathywu): Move relevant tests from saved_model_test to\n-\n import shutil\n \n from absl.testing import parameterized\n@@ -174,6 +173,26 @@ class UnregisteredCustomSequentialModel(keras.Sequential):\n     self.add(keras.layers.InputLayer(input_shape=(2, 3)))\n \n \n+class FunctionalSubclassModel(keras.Model):\n+\n+  def __init__(self, units):\n+    self.units = units\n+    my_input = keras.Input(shape=(2, 3), name='inputs')\n+    dense = keras.layers.Dense(self.units, activation='relu', name='dense')\n+    output = dense(my_input)\n+    outputs = {'output': output}\n+    super().__init__(inputs=[my_input], outputs=outputs)\n+\n+  def get_config(self):\n+    return {'units': self.units}\n+\n+\n+class FunctionalSubclassModelWrongConfig(FunctionalSubclassModel):\n+\n+  def get_config(self):\n+    return {}\n+\n+\n class ReviveTestBase(keras_parameterized.TestCase):\n \n   def setUp(self):\n@@ -346,6 +365,18 @@ class TestModelRevive(ReviveTestBase):\n     revived = keras_load.load(self.path, compile=False)\n     self._assert_revived_correctness(model, revived)\n \n+  def test_functional_subclass(self):\n+    model = FunctionalSubclassModel(32)\n+    model.save(self.path, save_format='tf')\n+    revived = keras_load.load(self.path, compile=False)\n+    self._assert_revived_correctness(model, revived)\n+\n+  def test_functional_subclass_wrong_config(self):\n+    model = FunctionalSubclassModelWrongConfig(32)\n+    model.save(self.path, save_format='tf')\n+    with self.assertRaisesRegex(TypeError, 'Unable to revive model'):\n+      keras_load.load(self.path, compile=False)\n+\n   def test_load_compiled_metrics(self):\n     model = testing_utils.get_small_sequential_mlp(1, 3)\n \n@@ -385,6 +416,8 @@ if __name__ == '__main__':\n       'CustomLayerWithConfig': CustomLayerWithConfig,\n       'CustomNetworkWithConfig': CustomNetworkWithConfig,\n       'CustomNetworkWithConfigName': CustomNetworkWithConfigName,\n-      'SubclassedModelWithConfig': SubclassedModelWithConfig\n+      'SubclassedModelWithConfig': SubclassedModelWithConfig,\n+      'FunctionalSubclassModel': FunctionalSubclassModel,\n+      'FunctionalSubclassModelWrongConfig': FunctionalSubclassModelWrongConfig\n   }):\n     tf.test.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a392a0859ab1aec57aa0593cf40c65837d829d10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 2203 | Contributors (this commit): 32 | Commits (past 90d): 8 | Contributors (cumulative): 32 | DMM Complexity: 0.0\n\nDIFF:\n@@ -207,13 +207,15 @@ def print_summary(model,\n           cutoff = min(candidate_cutoffs)\n           fit_into_line = fit_into_line[:cutoff]\n \n+        if col == 0:\n+            line += '|' * (nested_level) + ' '\n         line += fit_into_line\n         line += ' ' * space if space else ''\n         left_to_print[col] = left_to_print[col][cutoff:]\n \n         # Pad out to the next position\n         if nested_level:\n-          line += ' ' * (positions[col] - len(line) - (2 * nested_level) - 1)\n+          line += ' ' * (positions[col] - len(line) - nested_level)\n         else:\n           line += ' ' * (positions[col] - len(line))\n       line += '|' * nested_level\n@@ -294,14 +296,13 @@ def print_summary(model,\n       for i in range(len(nested_layer)):\n         if i == len(nested_layer) - 1:\n           is_nested_last = True\n-        print_fn('|' * (nested_level + 1), end=' ')\n         print_layer(nested_layer[i], nested_level + 1, is_nested_last)\n \n       print_fn('|' * nested_level + '¯' * (line_length - 2 * nested_level) +\n                '|' * nested_level)\n \n     if not is_nested_last:\n-      print_fn('|' * nested_level + '_' * (line_length - 2 * nested_level) +\n+      print_fn('|' * nested_level + ' ' * (line_length - 2 * nested_level) +\n                '|' * nested_level)\n \n   layers = model.layers\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#279340e9110e051b105f1d29c99f3fb6f4213513", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 7 | Files Changed: 6 | Hunks: 9 | Methods Changed: 7 | Complexity Δ (Sum/Max): -4/3 | Churn Δ: 51 | Churn Cumulative: 33750 | Contributors (this commit): 112 | Commits (past 90d): 36 | Contributors (cumulative): 162 | DMM Complexity: 1.0\n\nDIFF:\n@@ -136,7 +136,7 @@ class Conv(Layer):\n                        f'Was expecting a positive value. Received {filters}.')\n     self.filters = filters\n     self.groups = groups or 1\n-    self.kernel_size = conv_utils.normalize_tuple(\n+    self.kernel_size = conv_utils.normalize_positive_tuple(\n         kernel_size, rank, 'kernel_size')\n     self.strides = conv_utils.normalize_tuple(strides, rank, 'strides')\n     self.padding = conv_utils.normalize_padding(padding)\n\n@@ -487,7 +487,7 @@ class ConvLSTMCell(DropoutRNNCellMixin, Layer):\n       raise ValueError(f'Rank {rank} convolutions are not currently '\n                        f'implemented. Received: rank={rank}')\n     self.filters = filters\n-    self.kernel_size = conv_utils.normalize_tuple(kernel_size, self.rank,\n+    self.kernel_size = conv_utils.normalize_positive_tuple(kernel_size, self.rank,\n                                                   'kernel_size')\n     self.strides = conv_utils.normalize_tuple(strides, self.rank, 'strides')\n     self.padding = conv_utils.normalize_padding(padding)\n\n@@ -132,7 +132,7 @@ class LocallyConnected1D(Layer):\n                **kwargs):\n     super(LocallyConnected1D, self).__init__(**kwargs)\n     self.filters = filters\n-    self.kernel_size = conv_utils.normalize_tuple(kernel_size, 1, 'kernel_size')\n+    self.kernel_size = conv_utils.normalize_positive_tuple(kernel_size, 1, 'kernel_size')\n     self.strides = conv_utils.normalize_tuple(strides, 1, 'strides')\n     self.padding = conv_utils.normalize_padding(padding)\n     if self.padding != 'valid' and implementation == 1:\n@@ -439,7 +439,7 @@ class LocallyConnected2D(Layer):\n                **kwargs):\n     super(LocallyConnected2D, self).__init__(**kwargs)\n     self.filters = filters\n-    self.kernel_size = conv_utils.normalize_tuple(kernel_size, 2, 'kernel_size')\n+    self.kernel_size = conv_utils.normalize_positive_tuple(kernel_size, 2, 'kernel_size')\n     self.strides = conv_utils.normalize_tuple(strides, 2, 'strides')\n     self.padding = conv_utils.normalize_padding(padding)\n     if self.padding != 'valid' and implementation == 1:\n\n@@ -56,7 +56,7 @@ class Pooling1D(Layer):\n     if strides is None:\n       strides = pool_size\n     self.pool_function = pool_function\n-    self.pool_size = conv_utils.normalize_tuple(pool_size, 1, 'pool_size')\n+    self.pool_size = conv_utils.normalize_positive_tuple(pool_size, 1, 'pool_size')\n     self.strides = conv_utils.normalize_tuple(strides, 1, 'strides')\n     self.padding = conv_utils.normalize_padding(padding)\n     self.data_format = conv_utils.normalize_data_format(data_format)\n@@ -339,7 +339,7 @@ class Pooling2D(Layer):\n     if strides is None:\n       strides = pool_size\n     self.pool_function = pool_function\n-    self.pool_size = conv_utils.normalize_tuple(pool_size, 2, 'pool_size')\n+    self.pool_size = conv_utils.normalize_positive_tuple(pool_size, 2, 'pool_size')\n     self.strides = conv_utils.normalize_tuple(strides, 2, 'strides')\n     self.padding = conv_utils.normalize_padding(padding)\n     self.data_format = conv_utils.normalize_data_format(data_format)\n@@ -677,7 +677,7 @@ class Pooling3D(Layer):\n     if strides is None:\n       strides = pool_size\n     self.pool_function = pool_function\n-    self.pool_size = conv_utils.normalize_tuple(pool_size, 3, 'pool_size')\n+    self.pool_size = conv_utils.normalize_positive_tuple(pool_size, 3, 'pool_size')\n     self.strides = conv_utils.normalize_tuple(strides, 3, 'strides')\n     self.padding = conv_utils.normalize_padding(padding)\n     self.data_format = conv_utils.normalize_data_format(data_format)\n\n@@ -49,6 +49,31 @@ def convert_data_format(data_format, ndim):\n         'Expected values are [\"channels_first\", \"channels_last\"]')\n \n \n+def normalize_positive_tuple(value, n, name):\n+  \"\"\"Transforms a single positive integer or iterable of positive integers into an\n+    integer tuple.\n+\n+  Args:\n+    value: The value to validate and convert. Could a positive int, or any iterable\n+      of positive ints.\n+    n: The size of the tuple to be returned.\n+    name: The name of the argument being validated, e.g. \"pool_size\" or \"kernel_size\".\n+    This is only used for format error messages.\n+\n+  Returns:\n+    A tuple of n positive integers.\n+\n+  Raises:\n+    ValueError: If something else than an postive int/long or iterable thereof was passed.\n+  \"\"\"\n+  value_tuple = normalize_tuple(value, n, name)\n+  for value in value_tuple:\n+    if value <= 0:\n+      raise ValueError(f'The `{name}` argument must be a positive int or a tuple of positive integers. '\n+                       f'Received: {value}.')\n+  return value_tuple\n+\n+\n def normalize_tuple(value, n, name):\n   \"\"\"Transforms a single integer or iterable of integers into an integer tuple.\n \n\n@@ -77,6 +77,18 @@ class TestBasicConvUtilsTest(tf.test.TestCase):\n     with self.assertRaises(ValueError):\n       conv_utils.normalize_tuple(None, n=3, name='strides')\n \n+  def test_normalize_positive_tuple(self):\n+    self.assertEqual((1, 2, 3,),\n+                     conv_utils.normalize_positive_tuple((1,2,3), n=3, name='pool_size'))\n+    self.assertEqual((3,3,3),\n+                     conv_utils.normalize_positive_tuple(3, n=3, name='pool_size'))\n+\n+    with self.assertRaises(ValueError):\n+      conv_utils.normalize_positive_tuple(-4, n=3, name='pool_size')\n+\n+    with self.assertRaises(ValueError):\n+      conv_utils.normalize_positive_tuple((0, 1, 2), n=3, name='pool_size')\n+\n   def test_normalize_data_format(self):\n     self.assertEqual('channels_last',\n                      conv_utils.normalize_data_format('Channels_Last'))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9bc0bc308b86bcd3ad8da1154f1c4807d9288ae6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 4 | Churn Cumulative: 19195 | Contributors (this commit): 81 | Commits (past 90d): 24 | Contributors (cumulative): 81 | DMM Complexity: None\n\nDIFF:\n@@ -3516,7 +3516,7 @@ class Cropping2D(Layer):\n     if self.data_format == 'channels_first':\n       if ((inputs.shape[2] is not None and sum(self.cropping[0]) >= inputs.shape[2]) or\n           (inputs.shape[3] is not None and sum(self.cropping[1]) >= inputs.shape[3])):\n-          raise ValueError('cropping parameter of Cropping layer must be '\n+          raise ValueError('Argument `cropping` must be '\n                            'greater than the input shape. Received: inputs.shape='\n                            f'{inputs.shape}, and cropping={self.cropping}')  \n       if self.cropping[0][1] == self.cropping[1][1] == 0:\n@@ -3532,7 +3532,7 @@ class Cropping2D(Layer):\n     else:\n       if ((inputs.shape[1] is not None and sum(self.cropping[0]) >= inputs.shape[1]) or\n           (inputs.shape[2] is not None and sum(self.cropping[1]) >= inputs.shape[2])):\n-          raise ValueError('cropping parameter of Cropping layer must be '\n+          raise ValueError('Argument `cropping` must be '\n                            'greater than the input shape. Received: inputs.shape='\n                            f'{inputs.shape}, and cropping={self.cropping}')   \n       if self.cropping[0][1] == self.cropping[1][1] == 0:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#652ae13acd97cb971f974d248000de0901e75edc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 7230 | Contributors (this commit): 95 | Commits (past 90d): 15 | Contributors (cumulative): 95 | DMM Complexity: None\n\nDIFF:\n@@ -2073,8 +2073,7 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n       profile_batch: Profile the batch(es) to sample compute characteristics.\n         profile_batch must be a non-negative integer or a tuple of integers.\n         A pair of positive integers signify a range of batches to profile.\n-        By default, it will profile the second batch. Set profile_batch=0\n-        to disable profiling.\n+        By default, profiling is disabled.\n       embeddings_freq: frequency (in epochs) at which embedding layers will be\n         visualized. If set to 0, embeddings won't be visualized.\n       embeddings_metadata: Dictionary which maps embedding layer names to the\n@@ -2159,7 +2158,7 @@ class TensorBoard(Callback, version_utils.TensorBoardVersionSelector):\n                write_images=False,\n                write_steps_per_second=False,\n                update_freq='epoch',\n-               profile_batch=2,\n+               profile_batch=0,\n                embeddings_freq=0,\n                embeddings_metadata=None,\n                **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4c5f556c30da2a2ab3f72060ed1ff8b0208f94d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 7 | Files Changed: 4 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 5878 | Contributors (this commit): 20 | Commits (past 90d): 24 | Contributors (cumulative): 31 | DMM Complexity: 0.0\n\nDIFF:\n@@ -196,7 +196,7 @@ class Dense(Layer):\n         outputs = tf.nn.embedding_lookup_sparse(\n             self.kernel, ids, weights, combiner='sum')\n       else:\n-        outputs = tf.raw_ops.MatMul(a=inputs, b=self.kernel)\n+        outputs = tf.matmul(a=inputs, b=self.kernel)\n     # Broadcast kernel to inputs.\n     else:\n       outputs = tf.tensordot(inputs, self.kernel, [[rank - 1], [0]])\n\n@@ -210,7 +210,7 @@ class RandomFourierFeatures(base_layer.Layer):\n     inputs = tf.convert_to_tensor(inputs, dtype=self.dtype)\n     inputs = tf.cast(inputs, tf.float32)\n     kernel = (1.0 / self.kernel_scale) * self.unscaled_kernel\n-    outputs = tf.raw_ops.MatMul(a=inputs, b=kernel)\n+    outputs = tf.matmul(a=inputs, b=kernel)\n     outputs = tf.nn.bias_add(outputs, self.bias)\n     return tf.cos(outputs)\n \n\n@@ -775,10 +775,8 @@ def local_conv_sparse_matmul(inputs, kernel, kernel_idxs, kernel_shape,\n       Output (N+2)-D dense tensor with shape `output_shape`.\n   \"\"\"\n   inputs_flat = backend.reshape(inputs, (backend.shape(inputs)[0], -1))\n-  output_flat = tf.raw_ops.SparseTensorDenseMatMul(\n-      a_indices=kernel_idxs,\n-      a_values=kernel,\n-      a_shape=kernel_shape,\n+  output_flat = tf.sparse.sparse_dense_matmul(\n+      sp_a=tf.SparseTensor(kernel_idxs, kernel, kernel_shape),\n       b=inputs_flat,\n       adjoint_b=True)\n   output_flat_transpose = backend.transpose(output_flat)\n\n@@ -642,7 +642,7 @@ def update_confusion_matrix_variables(variables_to_update,\n   if y_pred.shape.ndims == 1:\n     num_labels = 1\n   else:\n-    num_labels = tf.raw_ops.Prod(input=pred_shape[1:], axis=0)\n+    num_labels = tf.math.reduce_prod(pred_shape[1:], axis=0)\n   thresh_label_tile = tf.where(one_thresh, num_labels,\n                                          tf.ones([], dtype=tf.int32))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#001976c534c85b5e0ab964bb691ee42762de3aa4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 46 | Churn Cumulative: 6418 | Contributors (this commit): 17 | Commits (past 90d): 11 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1653,7 +1653,7 @@ def categorical_crossentropy(y_true,\n   y_pred = tf.convert_to_tensor(y_pred)\n   y_true = tf.cast(y_true, y_pred.dtype)\n   label_smoothing = tf.convert_to_tensor(\n-      label_smoothing, dtype=backend.floatx())\n+      label_smoothing, dtype=y_pred.dtype)\n \n   def _smooth_labels():\n     num_classes = tf.cast(tf.shape(y_true)[-1], y_pred.dtype)\n@@ -1797,7 +1797,7 @@ def binary_crossentropy(y_true,\n   y_pred = tf.convert_to_tensor(y_pred)\n   y_true = tf.cast(y_true, y_pred.dtype)\n   label_smoothing = tf.convert_to_tensor(\n-      label_smoothing, dtype=backend.floatx())\n+      label_smoothing, dtype=y_pred.dtype)\n \n   def _smooth_labels():\n     return y_true * (1.0 - label_smoothing) + 0.5 * label_smoothing\n\n@@ -948,6 +948,26 @@ class BinaryCrossentropyTest(tf.test.TestCase):\n     expected_value = (100.0 + 50.0 * label_smoothing) / 3.0\n     self.assertAlmostEqual(self.evaluate(loss), expected_value, 3)\n \n+  def test_label_smoothing_ndarray(self):\n+    logits = np.asarray([[100.0, -100.0, -100.0]])\n+    y_true = np.asarray([[1, 0, 1]])\n+    label_smoothing = 0.1\n+    # Loss: max(x, 0) - x * z + log(1 + exp(-abs(x)))\n+    #            (where x = logits and z = y_true)\n+    # Label smoothing: z' = z * (1 - L) + 0.5L\n+    #                  1  = 1 - 0.5L\n+    #                  0  = 0.5L\n+    # Applying the above two fns to the given input:\n+    # (100 - 100 * (1 - 0.5 L)  + 0 +\n+    #  0   + 100 * (0.5 L)      + 0 +\n+    #  0   + 100 * (1 - 0.5 L)  + 0) * (1/3)\n+    #  = (100 + 50L) * 1/3\n+    bce_obj = losses.BinaryCrossentropy(\n+        from_logits=True, label_smoothing=label_smoothing)\n+    loss = bce_obj(y_true, logits)\n+    expected_value = (100.0 + 50.0 * label_smoothing) / 3.0\n+    self.assertAlmostEqual(self.evaluate(loss), expected_value, 3)\n+\n   def test_ragged_tensors(self):\n     bce_obj = losses.BinaryCrossentropy()\n     y_true = tf.ragged.constant([[1, 0, 1], [0]])\n@@ -1075,6 +1095,28 @@ class CategoricalCrossentropyTest(tf.test.TestCase):\n     expected_value = 400.0 * label_smoothing / 3.0\n     self.assertAlmostEqual(self.evaluate(loss), expected_value, 3)\n \n+  def test_label_smoothing_ndarray(self):\n+    logits = np.asarray([[100.0, -100.0, -100.0]])\n+    y_true = np.asarray([[1, 0, 0]])\n+    label_smoothing = 0.1\n+    # Softmax Cross Entropy Loss: -\\sum_i p_i \\log q_i\n+    # where for a softmax activation\n+    # \\log q_i = x_i - \\log \\sum_j \\exp x_j\n+    #          = x_i - x_max - \\log \\sum_j \\exp (x_j - x_max)\n+    # For our activations, [100, -100, -100]\n+    # \\log ( exp(0) + exp(-200) + exp(-200) ) = 0\n+    # so our log softmaxes become: [0, -200, -200]\n+    # Label smoothing: z' = z * (1 - L) + L/n\n+    #                  1  = 1 - L + L/n\n+    #                  0  = L/n\n+    # Applying the above two fns to the given input:\n+    # -0 * (1 - L + L/n) + 200 * L/n + 200 * L/n = 400 L/n\n+    cce_obj = losses.CategoricalCrossentropy(\n+        from_logits=True, label_smoothing=label_smoothing)\n+    loss = cce_obj(y_true, logits)\n+    expected_value = 400.0 * label_smoothing / 3.0\n+    self.assertAlmostEqual(self.evaluate(loss), expected_value, 3)\n+\n   def test_shape_mismatch(self):\n     y_true = tf.constant([[0], [1], [2]])\n     y_pred = tf.constant([[.9, .05, .05], [.5, .89, .6],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
