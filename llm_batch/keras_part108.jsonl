{"custom_id": "keras#b008bb35cc462c626101769dcad9edcf969e9d3d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 72 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -15,11 +15,10 @@ np.random.seed(1337)  # for reproducibility\n \n from keras.preprocessing import sequence\n from keras.models import Sequential\n-from keras.layers import Dense, Activation, Flatten\n+from keras.layers import Dense, Flatten\n from keras.layers import Embedding\n from keras.layers import AveragePooling1D\n from keras.datasets import imdb\n-from keras import backend as K\n \n \n # set parameters:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#dc98019d4965c728a7b2326cacd1b921b61dba70", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 16 | Churn Cumulative: 7870 | Contributors (this commit): 47 | Commits (past 90d): 13 | Contributors (cumulative): 47 | DMM Complexity: 1.0\n\nDIFF:\n@@ -339,6 +339,22 @@ class Sequential(Model):\n         self.built = False\n         self._flattened_layers = None\n \n+    def get_layer(self, name=None, index=None):\n+        '''Returns a layer based on either its name (unique)\n+        or its index in the graph. Indices are based on\n+        order of horizontal graph traversal (bottom-up).\n+\n+        # Arguments\n+            name: string, name of layer.\n+            index: integer, index of layer.\n+\n+        # Returns\n+            A layer instance.\n+        '''\n+        if not self.built:\n+            self.build()\n+        return self.model.get_layer(name, index)\n+\n     def call(self, x, mask=None):\n         if not self.built:\n             self.build()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#91310971b9b8a52e650ccbab79717d261418f104", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 93 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 93 | Churn Cumulative: 93 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -0,0 +1,93 @@\n+\"\"\"\n+This is an example of using Hierarchical RNN (HRNN) to predict MNIST digits.\n+\n+HRNNs can learn across multiple levels of boundaries from a complex sequence.\n+In text processing, a normal RNN can only learn the relationship of words in\n+one sentence. HRNNs can also learn the relationships between sentences.\n+Usually, the first recurrent layer of an HRNN encodes a sentence(word vectors)\n+into a  sentence vector. The second recurrent layer then encodes a sequence of\n+sentence vectors(encoded by the first layer) into a document vector. This\n+document vector is considered to preserve both the word-level and\n+sentence-level structure of the context.\n+\n+Paper references:\n+\"A Hierarchical Neural Autoencoder for Paragraphs and Documents\"\n+(https://web.stanford.edu/~jurafsky/pubs/P15-1107.pdf) encodes paragraphs\n+and documents with HRNN. Results have shown that HRNN outperforms standard\n+RNNs and may play some role in more sophisticated generation tasks like\n+summarization or question answering.\n+\n+\"Hierarchical recurrent neural network for skeleton based action recognition\"\n+(http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7298714) achieved\n+state-of-the-art results on skeleton based action recogntion with 3 levels\n+of bidirectional HRNN combind with fully connected layers.\n+\n+In the below MNIST example the first LSTM layer first encodes every\n+column of pixels(28,1) to a column vector(128,). The second LSTM\n+layer encodes then these 28 column vectors(28,128) to a image vector\n+representing the whole image. A final Dense layer is added for prediction.\n+\n+After 5 epochs: train acc: 0.9858, val acc: 0.9864\n+\n+PS: Modified from mnist_irnn.py\n+\"\"\"\n+from __future__ import print_function\n+\n+from keras.datasets import mnist\n+from keras.models import Sequential, Model\n+from keras.layers import Input, Dense, TimeDistributed\n+from keras.layers import LSTM\n+from keras.utils import np_utils\n+\n+batch_size = 32\n+nb_classes = 10\n+nb_epochs = 5\n+\n+# the data, shuffled and split between train and test sets\n+(X_train, y_train), (X_test, y_test) = mnist.load_data()\n+\n+# Reshape to 4D for Hierarchical RNN\n+X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n+X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n+X_train = X_train.astype('float32')\n+X_test = X_test.astype('float32')\n+X_train /= 255\n+X_test /= 255\n+print('X_train shape:', X_train.shape)\n+print(X_train.shape[0], 'train samples')\n+print(X_test.shape[0], 'test samples')\n+\n+# convert class vectors to binary class matrices\n+Y_train = np_utils.to_categorical(y_train, nb_classes)\n+Y_test = np_utils.to_categorical(y_test, nb_classes)\n+\n+# Define model\n+pixel_hidden = 128\n+col_hidden = 128\n+\n+row, col, pixel = X_train.shape[1:]\n+\n+# 4D input\n+x = Input(shape=(row, col, pixel))\n+\n+# Encodes a col of pixels, uses TimeDistributed Wrapper\n+encoded_pixels = TimeDistributed(LSTM(output_dim=pixel_hidden))(x)\n+\n+# Encodes columns of encoded pixels\n+encoded_columns = LSTM(col_hidden)(encoded_pixels)\n+\n+prediction = Dense(nb_classes, activation='softmax')(encoded_columns)\n+\n+model = Model(input=[x], output=[prediction])\n+\n+model.compile(loss='categorical_crossentropy',\n+              optimizer='rmsprop',\n+              metrics=['accuracy'])\n+\n+model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epochs,\n+          verbose=1, validation_data=(X_test, Y_test))\n+\n+scores = model.evaluate(X_test, Y_test, verbose=0)\n+\n+print('Hierarchical RNN test loss:', scores[0])\n+print('Hierarchical RNN test accuracy:', scores[1])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#28d9c0c5116c0a61dadda65139af15c2ccc1562a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 38 | Files Changed: 1 | Hunks: 20 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 70 | Churn Cumulative: 163 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,35 +1,28 @@\n-\"\"\"\n-This is an example of using Hierarchical RNN (HRNN) to predict MNIST digits.\n+\"\"\"This is an example of using Hierarchical RNN (HRNN) to classify MNIST digits.\n \n-HRNNs can learn across multiple levels of boundaries from a complex sequence.\n-In text processing, a normal RNN can only learn the relationship of words in\n-one sentence. HRNNs can also learn the relationships between sentences.\n-Usually, the first recurrent layer of an HRNN encodes a sentence(word vectors)\n+HRNNs can learn across multiple levels of temporal hiearchy over a complex sequence.\n+Usually, the first recurrent layer of an HRNN encodes a sentence (e.g. of word vectors)\n into a  sentence vector. The second recurrent layer then encodes a sequence of\n-sentence vectors(encoded by the first layer) into a document vector. This\n+such vectors (encoded by the first layer) into a document vector. This\n document vector is considered to preserve both the word-level and\n sentence-level structure of the context.\n \n-Paper references:\n-\"A Hierarchical Neural Autoencoder for Paragraphs and Documents\"\n-(https://web.stanford.edu/~jurafsky/pubs/P15-1107.pdf) encodes paragraphs\n-and documents with HRNN. Results have shown that HRNN outperforms standard\n+# References\n+    - [A Hierarchical Neural Autoencoder for Paragraphs and Documents](https://web.stanford.edu/~jurafsky/pubs/P15-1107.pdf)\n+        Encodes paragraphs and documents with HRNN.\n+        Results have shown that HRNN outperforms standard\n         RNNs and may play some role in more sophisticated generation tasks like\n         summarization or question answering.\n-\n-\"Hierarchical recurrent neural network for skeleton based action recognition\"\n-(http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7298714) achieved\n-state-of-the-art results on skeleton based action recogntion with 3 levels\n-of bidirectional HRNN combind with fully connected layers.\n+    - [Hierarchical recurrent neural network for skeleton based action recognition](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7298714)\n+        Achieved state-of-the-art results on skeleton based action recognition with 3 levels\n+        of bidirectional HRNN combined with fully connected layers.\n \n In the below MNIST example the first LSTM layer first encodes every\n-column of pixels(28,1) to a column vector(128,). The second LSTM\n-layer encodes then these 28 column vectors(28,128) to a image vector\n+column of pixels of shape (28, 1) to a column vector of shape (128,). The second LSTM\n+layer encodes then these 28 column vectors of shape (28, 128) to a image vector\n representing the whole image. A final Dense layer is added for prediction.\n \n After 5 epochs: train acc: 0.9858, val acc: 0.9864\n-\n-PS: Modified from mnist_irnn.py\n \"\"\"\n from __future__ import print_function\n \n@@ -39,14 +32,19 @@ from keras.layers import Input, Dense, TimeDistributed\n from keras.layers import LSTM\n from keras.utils import np_utils\n \n+# Training parameters.\n batch_size = 32\n nb_classes = 10\n nb_epochs = 5\n \n-# the data, shuffled and split between train and test sets\n+# Embedding dimensions.\n+row_hidden = 128\n+col_hidden = 128\n+\n+# The data, shuffled and split between train and test sets.\n (X_train, y_train), (X_test, y_test) = mnist.load_data()\n \n-# Reshape to 4D for Hierarchical RNN\n+# Reshapes data to 4D for Hierarchical RNN.\n X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n X_train = X_train.astype('float32')\n@@ -57,37 +55,33 @@ print('X_train shape:', X_train.shape)\n print(X_train.shape[0], 'train samples')\n print(X_test.shape[0], 'test samples')\n \n-# convert class vectors to binary class matrices\n+# Converts class vectors to binary class matrices.\n Y_train = np_utils.to_categorical(y_train, nb_classes)\n Y_test = np_utils.to_categorical(y_test, nb_classes)\n \n-# Define model\n-pixel_hidden = 128\n-col_hidden = 128\n-\n row, col, pixel = X_train.shape[1:]\n \n-# 4D input\n+# 4D input.\n x = Input(shape=(row, col, pixel))\n \n-# Encodes a col of pixels, uses TimeDistributed Wrapper\n-encoded_pixels = TimeDistributed(LSTM(output_dim=pixel_hidden))(x)\n+# Encodes a row of pixels using TimeDistributed Wrapper.\n+encoded_rows = TimeDistributed(LSTM(output_dim=row_hidden))(x)\n \n-# Encodes columns of encoded pixels\n-encoded_columns = LSTM(col_hidden)(encoded_pixels)\n+# Encodes columns of encoded rows.\n+encoded_columns = LSTM(col_hidden)(encoded_rows)\n \n+# Final predictions and model.\n prediction = Dense(nb_classes, activation='softmax')(encoded_columns)\n-\n-model = Model(input=[x], output=[prediction])\n-\n+model = Model(input=x, output=prediction)\n model.compile(loss='categorical_crossentropy',\n               optimizer='rmsprop',\n               metrics=['accuracy'])\n \n+# Training.\n model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epochs,\n           verbose=1, validation_data=(X_test, Y_test))\n \n+# Evaluation.\n scores = model.evaluate(X_test, Y_test, verbose=0)\n-\n-print('Hierarchical RNN test loss:', scores[0])\n-print('Hierarchical RNN test accuracy:', scores[1])\n+print('Test loss:', scores[0])\n+print('Test accuracy:', scores[1])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#04d785f4bf0e726bf75dbc5357a158449813e3b2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 2070 | Contributors (this commit): 18 | Commits (past 90d): 19 | Contributors (cumulative): 18 | DMM Complexity: 0.0\n\nDIFF:\n@@ -421,7 +421,7 @@ def generator_queue(generator, max_q_size=10,\n         def data_generator_task():\n             while not _stop.is_set():\n                 try:\n-                    if q.qsize() < max_q_size:\n+                    if pickle_safe or q.qsize() < max_q_size:\n                         try:\n                             generator_output = next(generator)\n                         except ValueError:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#458edeed9a9010f1ba6184ae40b474181a606502", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 136 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 13/11 | Churn Δ: 142 | Churn Cumulative: 16328 | Contributors (this commit): 77 | Commits (past 90d): 77 | Contributors (cumulative): 119 | DMM Complexity: 0.6590909090909091\n\nDIFF:\n@@ -1270,14 +1270,16 @@ def tanh(x):\n     return tf.nn.tanh(x)\n \n \n-def dropout(x, level, seed=None):\n+def dropout(x, level, noise_shape=None, seed=None):\n     '''Sets entries in `x` to zero at random,\n     while scaling the entire tensor.\n \n     # Arguments\n         x: tensor\n         level: fraction of the entries in the tensor\n-            that will be set to 0\n+            that will be set to 0.\n+        noise_shape: shape for randomly generated keep/drop flags,\n+            must be broadcastable to the shape of `x`\n         seed: random seed to ensure determinism.\n     '''\n     retain_prob = 1. - level\n@@ -1285,7 +1287,7 @@ def dropout(x, level, seed=None):\n         seed = np.random.randint(10e6)\n     # the dummy 1. works around a TF bug\n     # (float32_ref vs. float32 incomptability)\n-    return tf.nn.dropout(x * 1., retain_prob, seed=seed)\n+    return tf.nn.dropout(x * 1., retain_prob, noise_shape, seed=seed)\n \n \n def l2_normalize(x, axis):\n\n@@ -930,14 +930,33 @@ def tanh(x):\n     return T.tanh(x)\n \n \n-def dropout(x, level, seed=None):\n+def dropout(x, level, noise_shape=None, seed=None):\n+    '''Sets entries in `x` to zero at random,\n+    while scaling the entire tensor.\n+\n+    # Arguments\n+        x: tensor\n+        level: fraction of the entries in the tensor\n+            that will be set to 0.\n+        noise_shape: shape for randomly generated keep/drop flags,\n+            must be broadcastable to the shape of `x`\n+        seed: random seed to ensure determinism.\n+    '''\n     if level < 0. or level >= 1:\n         raise Exception('Dropout level must be in interval [0, 1[.')\n     if seed is None:\n         seed = np.random.randint(1, 10e6)\n+\n     rng = RandomStreams(seed=seed)\n     retain_prob = 1. - level\n-    x *= rng.binomial(x.shape, p=retain_prob, dtype=x.dtype)\n+\n+    if noise_shape is None:\n+        random_tensor = rng.binomial(x.shape, p=retain_prob, dtype=x.dtype)\n+    else:\n+        random_tensor = rng.binomial(noise_shape, p=retain_prob, dtype=x.dtype)\n+        random_tensor = T.patternbroadcast(random_tensor, [dim == 1 for dim in noise_shape])\n+\n+    x *= random_tensor\n     x /= retain_prob\n     return x\n \n\n@@ -82,9 +82,13 @@ class Dropout(Layer):\n         self.supports_masking = True\n         super(Dropout, self).__init__(**kwargs)\n \n+    def _get_noise_shape(self, x):\n+        return None\n+\n     def call(self, x, mask=None):\n         if 0. < self.p < 1.:\n-            x = K.in_train_phase(K.dropout(x, level=self.p), x)\n+            noise_shape = self._get_noise_shape(x)\n+            x = K.in_train_phase(K.dropout(x, self.p, noise_shape), x)\n         return x\n \n     def get_config(self):\n@@ -93,6 +97,102 @@ class Dropout(Layer):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n+class SpatialDropout2D(Dropout):\n+    '''This version performs the same function as Dropout, however it drops\n+    entire 2D feature maps instead of individual elements. If adjacent pixels\n+    within feature maps are strongly correlated (as is normally the case in\n+    early convolution layers) then regular dropout will not regularize the\n+    activations and will otherwise just result in an effective learning rate\n+    decrease. In this case, SpatialDropout2D will help promote independence\n+    between feature maps and should be used instead.\n+\n+    # Arguments\n+        p: float between 0 and 1. Fraction of the input units to drop.\n+        dim_ordering: 'th' or 'tf'. In 'th' mode, the channels dimension\n+            (the depth) is at index 1, in 'tf' mode is it at index 3.\n+            It defaults to the `image_dim_ordering` value found in your\n+            Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be \"th\".\n+\n+    # Input shape\n+        4D tensor with shape:\n+        `(samples, channels, rows, cols)` if dim_ordering='th'\n+        or 4D tensor with shape:\n+        `(samples, rows, cols, channels)` if dim_ordering='tf'.\n+\n+    # Output shape\n+        Same as input\n+\n+    # References\n+        - [Efficient Object Localization Using Convolutional Networks](https://arxiv.org/pdf/1411.4280.pdf)\n+    '''\n+    def __init__(self, p, dim_ordering='default', **kwargs):\n+        if dim_ordering == 'default':\n+            dim_ordering = K.image_dim_ordering()\n+        assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n+        self.dim_ordering = dim_ordering\n+        super(SpatialDropout2D, self).__init__(p, **kwargs)\n+\n+    def _get_noise_shape(self, x):\n+        input_shape = K.shape(x)\n+        if self.dim_ordering == 'th':\n+            noise_shape = (input_shape[0], input_shape[1], 1, 1)\n+        elif self.dim_ordering == 'tf':\n+            noise_shape = (input_shape[0], 1, 1, input_shape[3])\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+\n+        return noise_shape\n+\n+class SpatialDropout3D(Dropout):\n+    '''This version performs the same function as Dropout, however it drops\n+    entire 3D feature maps instead of individual elements. If adjacent voxels\n+    within feature maps are strongly correlated (as is normally the case in\n+    early convolution layers) then regular dropout will not regularize the\n+    activations and will otherwise just result in an effective learning rate\n+    decrease. In this case, SpatialDropout3D will help promote independence\n+    between feature maps and should be used instead.\n+\n+    # Arguments\n+        p: float between 0 and 1. Fraction of the input units to drop.\n+        dim_ordering: 'th' or 'tf'.\n+            In 'th' mode, the channels dimension (the depth)\n+            is at index 1, in 'tf' mode is it at index 4.\n+            It defaults to the `image_dim_ordering` value found in your\n+            Keras config file at `~/.keras/keras.json`.\n+            If you never set it, then it will be \"th\".\n+\n+    # Input shape\n+        5D tensor with shape:\n+        `(samples, channels, dim1, dim2, dim3)` if dim_ordering='th'\n+        or 5D tensor with shape:\n+        `(samples, dim1, dim2, dim3, channels)` if dim_ordering='tf'.\n+\n+    # Output shape\n+        Same as input\n+\n+    # References\n+        - [Efficient Object Localization Using Convolutional Networks](https://arxiv.org/pdf/1411.4280.pdf)\n+    '''\n+    def __init__(self, p, dim_ordering='default', **kwargs):\n+        if dim_ordering == 'default':\n+            dim_ordering = K.image_dim_ordering()\n+        assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n+        self.dim_ordering = dim_ordering\n+        super(SpatialDropout3D, self).__init__(p, **kwargs)\n+\n+    def _get_noise_shape(self, x):\n+        input_shape = K.shape(x)\n+        if self.dim_ordering == 'th':\n+            noise_shape = (input_shape[0], input_shape[1], 1, 1, 1)\n+        elif self.dim_ordering == 'tf':\n+            noise_shape = (input_shape[0], 1, 1, 1, input_shape[4])\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+\n+        return noise_shape\n+\n+\n class Activation(Layer):\n     '''Applies an activation function to an output.\n \n\n@@ -120,6 +120,7 @@ def test_merge_mask_2d():\n     model_concat.compile(loss='mse', optimizer='sgd')\n     model_concat.fit([rand(2, 3), rand(2, 3)], [rand(2, 6)], nb_epoch=1)\n \n+\n @keras_test\n def test_merge_mask_3d():\n     from keras.layers import Input, merge, Embedding, SimpleRNN\n@@ -152,6 +153,14 @@ def test_dropout():\n                kwargs={'p': 0.5},\n                input_shape=(3, 2))\n \n+    layer_test(core.SpatialDropout2D,\n+               kwargs={'p': 0.5},\n+               input_shape=(2, 3, 4, 5))\n+\n+    layer_test(core.SpatialDropout3D,\n+               kwargs={'p': 0.5},\n+               input_shape=(2, 3, 4, 5, 6))\n+\n \n @keras_test\n def test_activation():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4e155139ca939d54736ff3dd65591107cd9aa91d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 10549 | Contributors (this commit): 50 | Commits (past 90d): 10 | Contributors (cumulative): 50 | DMM Complexity: None\n\nDIFF:\n@@ -141,9 +141,9 @@ class SpatialDropout2D(Dropout):\n             noise_shape = (input_shape[0], 1, 1, input_shape[3])\n         else:\n             raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n-\n         return noise_shape\n \n+\n class SpatialDropout3D(Dropout):\n     '''This version performs the same function as Dropout, however it drops\n     entire 3D feature maps instead of individual elements. If adjacent voxels\n@@ -189,7 +189,6 @@ class SpatialDropout3D(Dropout):\n             noise_shape = (input_shape[0], 1, 1, 1, input_shape[4])\n         else:\n             raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n-\n         return noise_shape\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e8190a8d8d4a59359f93bc9b366d04b9c72cc2ed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 695 | Lines Deleted: 1 | Files Changed: 4 | Hunks: 5 | Methods Changed: 31 | Complexity Δ (Sum/Max): 78/62 | Churn Δ: 696 | Churn Cumulative: 6093 | Contributors (this commit): 40 | Commits (past 90d): 76 | Contributors (cumulative): 76 | DMM Complexity: 0.8147138964577657\n\nDIFF:\n@@ -0,0 +1,442 @@\n+'''This example uses a convolutional stack followed by a recurrent stack\n+and a CTC logloss function to perform optical character recognition\n+of generated text images. I have no evidence of whether it actually\n+learns general shapes of text, or just is able to recognize all\n+the different fonts thrown at it...the purpose is more to demonstrate CTC\n+inside of Keras.  Note that the font list may need to be updated\n+for the particular OS in use.\n+\n+This starts off with 4 letter words. After 10 or so epochs, CTC\n+learns translational invariance, so longer words and groups of words\n+with spaces are gradually fed in.  This gradual increase in difficulty\n+is handled using the TextImageGenerator class which is both a generator\n+class for test/train data and a Keras callback class. Every 10 epochs\n+the wordlist that the generator draws from increases in difficulty.\n+\n+The table below shows normalized edit distance values. Theano uses\n+a slightly different CTC implementation, so some Theano-specific\n+hyperparameter tuning would be needed to get it to match Tensorflow.\n+\n+            Norm. ED\n+Epoch |   TF   |   TH\n+------------------------\n+    10   0.072    0.272\n+    20   0.032    0.115\n+    30   0.024    0.098\n+    40   0.023    0.108\n+\n+This requires cairo and editdistance packages:\n+pip install cairocffi\n+pip install editdistance\n+\n+Due to the use of a dummy loss function, Theano requires the following flags:\n+on_unused_input='ignore'\n+\n+Created by Mike Henry\n+https://github.com/mbhenry/\n+'''\n+\n+import os\n+import itertools\n+import re\n+import datetime\n+import cairocffi as cairo\n+import editdistance\n+import numpy as np\n+from scipy import ndimage\n+import pylab\n+from keras import backend as K\n+from keras.layers.convolutional import Convolution2D, MaxPooling2D\n+from keras.layers import Input, Layer, Dense, Activation, Flatten\n+from keras.layers import Reshape, Lambda, merge, Permute, TimeDistributed\n+from keras.models import Model\n+from keras.layers.recurrent import GRU\n+from keras.optimizers import SGD\n+from keras.utils import np_utils\n+from keras.utils.data_utils import get_file\n+from keras.preprocessing import image\n+import keras.callbacks\n+\n+OUTPUT_DIR = \"image_ocr\"\n+\n+np.random.seed(55)\n+\n+# this creates larger \"blotches\" of noise which look\n+# more realistic than just adding gaussian noise\n+# assumes greyscale with pixels ranging from 0 to 1\n+\n+def speckle(img):\n+    severity = np.random.uniform(0, 0.6)\n+    blur = ndimage.gaussian_filter(np.random.randn(*img.shape) * severity, 1)\n+    img_speck = (img + blur)\n+    img_speck[img_speck > 1] = 1\n+    img_speck[img_speck <= 0] = 0\n+    return img_speck\n+\n+# paints the string in a random location the bounding box\n+# also uses a random font, a slight random rotation,\n+# and a random amount of speckle noise\n+\n+def paint_text(text, w, h):\n+    surface = cairo.ImageSurface(cairo.FORMAT_RGB24, w, h)\n+    with cairo.Context(surface) as context:\n+        context.set_source_rgb(1, 1, 1)  # White\n+        context.paint()\n+        # this font list works in Centos 7\n+        fonts = ['Century Schoolbook', 'Courier', 'STIX', 'URW Chancery L', 'FreeMono']\n+        context.select_font_face(np.random.choice(fonts), cairo.FONT_SLANT_NORMAL,\n+                                 np.random.choice([cairo.FONT_WEIGHT_BOLD, cairo.FONT_WEIGHT_NORMAL]))\n+        context.set_font_size(40)\n+        box = context.text_extents(text)\n+        if box[2] > w or box[3] > h:\n+            raise IOError('Could not fit string into image. Max char count is too large for given image width.')\n+\n+        # teach the RNN translational invariance by\n+        # fitting text box randomly on canvas, with some room to rotate\n+        border_w_h = (10, 16)\n+        max_shift_x = w - box[2] - border_w_h[0]\n+        max_shift_y = h - box[3] - border_w_h[1]\n+        top_left_x = np.random.randint(0, int(max_shift_x))\n+        top_left_y = np.random.randint(0, int(max_shift_y))\n+\n+        context.move_to(top_left_x - int(box[0]), top_left_y - int(box[1]))\n+        context.set_source_rgb(0, 0, 0)\n+        context.show_text(text)\n+\n+    buf = surface.get_data()\n+    a = np.frombuffer(buf, np.uint8)\n+    a.shape = (h, w, 4)\n+    a = a[:, :, 0]  # grab single channel\n+    a /= 255\n+    a = np.expand_dims(a, 0)\n+    a = speckle(a)\n+    a = image.random_rotation(a, 3 * (w - top_left_x) / w + 1)\n+\n+    return a\n+\n+def shuffle_mats_or_lists(matrix_list, stop_ind=None):\n+    ret = []\n+    assert all([len(i) == len(matrix_list[0]) for i in matrix_list])\n+    len_val = len(matrix_list[0])\n+    if stop_ind is None:\n+        stop_ind = len_val\n+    assert stop_ind <= len_val\n+\n+    a = range(stop_ind)\n+    np.random.shuffle(a)\n+    a += range(stop_ind, len_val)\n+    for mat in matrix_list:\n+        if isinstance(mat, np.ndarray):\n+            ret.append(mat[a])\n+        elif isinstance(mat, list):\n+            ret.append([mat[i] for i in a])\n+        else:\n+            raise TypeError('shuffle_mats_or_lists only supports numpy.array and list objects')\n+    return ret\n+\n+def text_to_labels(text, num_classes):\n+    ret = []\n+    for char in text:\n+        if char >= 'a' and char <= 'z':\n+            ret.append(ord(char) - ord('a'))\n+        elif char == ' ':\n+            ret.append(26)\n+    return ret\n+\n+# only a-z and space..probably not to difficult\n+# to expand to uppercase and symbols\n+\n+def is_valid_str(in_str):\n+    search = re.compile(r'[^a-z\\ ]').search\n+    return not bool(search(in_str))\n+\n+# Uses generator functions to supply train/test with\n+# data. Image renderings are text are created on the fly\n+# each time with random perturbations\n+\n+class TextImageGenerator(keras.callbacks.Callback):\n+\n+    def __init__(self, monogram_file, bigram_file, minibatch_size, img_w,\n+                 img_h, downsample_width, val_split,\n+                 absolute_max_string_len=16):\n+\n+        self.minibatch_size = minibatch_size\n+        self.img_w = img_w\n+        self.img_h = img_h\n+        self.monogram_file = monogram_file\n+        self.bigram_file = bigram_file\n+        self.downsample_width = downsample_width\n+        self.val_split = val_split\n+        self.blank_label = self.get_output_size() - 1\n+        self.absolute_max_string_len = absolute_max_string_len\n+\n+    def get_output_size(self):\n+        return 28\n+\n+    # num_words can be independent of the epoch size due to the use of generators\n+    # as max_string_len grows, num_words can grow\n+    def build_word_list(self, num_words, max_string_len=None, mono_fraction=0.5):\n+        assert max_string_len <= self.absolute_max_string_len\n+        assert num_words % self.minibatch_size == 0\n+        assert (self.val_split * num_words) % self.minibatch_size == 0\n+        self.num_words = num_words\n+        self.string_list = []\n+        self.max_string_len = max_string_len\n+        self.Y_data = np.ones([self.num_words, self.absolute_max_string_len]) * -1\n+        self.X_text = []\n+        self.Y_len = [0] * self.num_words\n+\n+        # monogram file is sorted by frequency in english speech\n+        with open(self.monogram_file, 'rt') as f:\n+            for line in f:\n+                if len(self.string_list) == int(self.num_words * mono_fraction):\n+                    break\n+                word = line.rstrip()\n+                if max_string_len == -1 or max_string_len is None or len(word) <= max_string_len:\n+                    self.string_list.append(word)\n+\n+        # bigram file contains common word pairings in english speech\n+        with open(self.bigram_file, 'rt') as f:\n+            lines = f.readlines()\n+            for line in lines:\n+                if len(self.string_list) == self.num_words:\n+                    break\n+                columns = line.lower().split()\n+                word = columns[0] + ' ' + columns[1]\n+                if is_valid_str(word) and \\\n+                        (max_string_len == -1 or max_string_len is None or len(word) <= max_string_len):\n+                    self.string_list.append(word)\n+        if len(self.string_list) != self.num_words:\n+            raise IOError('Could not pull enough words from supplied monogram and bigram files. ')\n+\n+        for i, word in enumerate(self.string_list):\n+            self.Y_len[i] = len(word)\n+            self.Y_data[i, 0:len(word)] = text_to_labels(word, self.get_output_size())\n+            self.X_text.append(word)\n+        self.Y_len = np.expand_dims(np.array(self.Y_len), 1)\n+\n+        self.cur_val_index = self.val_split\n+        self.cur_train_index = 0\n+\n+    # each time an image is requested from train/val/test, a new random\n+    # painting of the text is performed\n+    def get_batch(self, index, size, train):\n+        X_data = np.ones([size, 1, self.img_h, self.img_w])\n+        labels = np.ones([size, self.absolute_max_string_len])\n+        input_length = np.zeros([size, 1])\n+        label_length = np.zeros([size, 1])\n+        source_str = []\n+\n+        for i in range(0, size):\n+            # Mix in some blank inputs.  This seems to be important for\n+            # achieving translational invariance\n+            if train and i > size - 4:\n+                X_data[i, 0, :, :] = paint_text('', self.img_w, self.img_h)\n+                labels[i, 0] = self.blank_label\n+                input_length[i] = self.downsample_width\n+                label_length[i] = 1\n+                source_str.append('')\n+            else:\n+                X_data[i, 0, :, :] = paint_text(self.X_text[index + i], self.img_w, self.img_h)\n+                labels[i, :] = self.Y_data[index + i]\n+                input_length[i] = self.downsample_width\n+                label_length[i] = self.Y_len[index + i]\n+                source_str.append(self.X_text[index + i])\n+\n+        inputs = {'the_input': X_data,\n+                  'the_labels': labels,\n+                  'input_length': input_length,\n+                  'label_length': label_length,\n+                  'source_str': source_str  # used for visualization only\n+                  }\n+        outputs = {'ctc': np.zeros([size])}  # dummy data for dummy loss function\n+        return (inputs, outputs)\n+\n+    def next_train(self):\n+        while 1:\n+            ret = self.get_batch(self.cur_train_index, self.minibatch_size, train=True)\n+            self.cur_train_index += self.minibatch_size\n+            if self.cur_train_index >= self.val_split:\n+                self.cur_train_index = self.cur_train_index % 32\n+                (self.X_text, self.Y_data, self.Y_len) = shuffle_mats_or_lists(\n+                    [self.X_text, self.Y_data, self.Y_len], self.val_split)\n+            yield ret\n+\n+    def next_val(self):\n+        while 1:\n+            ret = self.get_batch(self.cur_val_index, self.minibatch_size, train=False)\n+            self.cur_val_index += self.minibatch_size\n+            if self.cur_val_index >= self.num_words:\n+                self.cur_val_index = self.val_split + self.cur_val_index % 32\n+            yield ret\n+\n+    def on_train_begin(self, logs={}):\n+        # translational invariance seems to be the hardest thing\n+        # for the RNN to learn, so start with <= 4 letter words.\n+        self.build_word_list(16000, 4, 1)\n+\n+    def on_epoch_begin(self, epoch, logs={}):\n+        # After 10 epochs, translational invariance should be learned\n+        # so start feeding longer words and eventually multiple words with spaces\n+        if epoch == 10:\n+            self.build_word_list(32000, 8, 1)\n+        if epoch == 20:\n+            self.build_word_list(32000, 8, 0.6)\n+        if epoch == 30:\n+            self.build_word_list(64000, 12, 0.5)\n+\n+# the actual loss calc occurs here despite it not being\n+# an internal Keras loss function\n+\n+def ctc_lambda_func(args):\n+    y_pred, labels, input_length, label_length = args\n+    # the 2 is critical here since the first couple outputs of the RNN\n+    # tend to be garbage:\n+    y_pred = y_pred[:, 2:, :]\n+    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n+\n+# For a real OCR application, this should be beam search with a dictionary\n+# and language model.  For this example, best path is sufficient.\n+\n+def decode_batch(test_func, word_batch):\n+    out = test_func([word_batch])[0]\n+    ret = []\n+    for j in range(out.shape[0]):\n+        out_best = list(np.argmax(out[j, 2:], 1))\n+        out_best = [k for k, g in itertools.groupby(out_best)]\n+        # 26 is space, 27 is CTC blank char\n+        outstr = ''\n+        for c in out_best:\n+            if c >= 0 and c < 26:\n+                outstr += chr(c + ord('a'))\n+            elif c == 26:\n+                outstr += ' '\n+        ret.append(outstr)\n+    return ret\n+\n+class VizCallback(keras.callbacks.Callback):\n+\n+    def __init__(self, test_func, text_img_gen, num_display_words = 6):\n+        self.test_func = test_func\n+        self.output_dir = os.path.join(\n+            OUTPUT_DIR, datetime.datetime.now().strftime('%A, %d. %B %Y %I.%M%p'))\n+        self.text_img_gen = text_img_gen\n+        self.num_display_words = num_display_words\n+        os.makedirs(self.output_dir)\n+\n+    def show_edit_distance(self, num):\n+        num_left = num\n+        mean_norm_ed = 0.0\n+        mean_ed = 0.0\n+        while num_left > 0:\n+            word_batch = next(self.text_img_gen)[0]\n+            num_proc = min(word_batch['the_input'].shape[0], num_left)\n+            decoded_res = decode_batch(self.test_func, word_batch['the_input'][0:num_proc])\n+            for j in range(0, num_proc):\n+                edit_dist = editdistance.eval(decoded_res[j], word_batch['source_str'][j])\n+                mean_ed += float(edit_dist)\n+                mean_norm_ed += float(edit_dist) / len(word_batch['source_str'][j])\n+            num_left -= num_proc\n+        mean_norm_ed = mean_norm_ed / num\n+        mean_ed = mean_ed / num\n+        print('\\nOut of %d samples:  Mean edit distance: %.3f Mean normalized edit distance: %0.3f'\n+              % (num, mean_ed, mean_norm_ed))\n+\n+    def on_epoch_end(self, epoch, logs={}):\n+        self.model.save_weights(os.path.join(self.output_dir, 'weights%02d.h5' % epoch))\n+        self.show_edit_distance(256)\n+        word_batch = next(self.text_img_gen)[0]\n+        res = decode_batch(self.test_func, word_batch['the_input'][0:self.num_display_words])\n+\n+        for i in range(self.num_display_words):\n+            pylab.subplot(self.num_display_words, 1, i + 1)\n+            pylab.imshow(word_batch['the_input'][i, 0, :, :], cmap='Greys_r')\n+            pylab.xlabel('Truth = \\'%s\\' Decoded = \\'%s\\'' % (word_batch['source_str'][i], res[i]))\n+        fig = pylab.gcf()\n+        fig.set_size_inches(10, 12)\n+        pylab.savefig(os.path.join(self.output_dir, 'e%02d.png' % epoch))\n+        pylab.close()\n+\n+# Input Parameters\n+img_h = 64\n+img_w = 512\n+nb_epoch = 50\n+minibatch_size = 32\n+words_per_epoch = 16000\n+val_split = 0.2\n+val_words = int(words_per_epoch * (val_split))\n+\n+# Network parameters\n+conv_num_filters = 16\n+filter_size = 3\n+pool_size_1 = 4\n+pool_size_2 = 2\n+time_dense_size = 32\n+rnn_size = 512\n+time_steps = img_w / (pool_size_1 * pool_size_2)\n+\n+fdir = os.path.dirname(get_file('wordlists.tgz',\n+                                origin='http://www.isosemi.com/datasets/wordlists.tgz', untar=True))\n+\n+img_gen = TextImageGenerator(monogram_file=os.path.join(fdir, 'wordlist_mono_clean.txt'),\n+                             bigram_file=os.path.join(fdir, 'wordlist_bi_clean.txt'),\n+                             minibatch_size=32,\n+                             img_w=img_w,\n+                             img_h=img_h,\n+                             downsample_width=img_w / (pool_size_1 * pool_size_2) - 2,\n+                             val_split=words_per_epoch - val_words)\n+\n+act = 'relu'\n+input_data = Input(name='the_input', shape=(1, img_h, img_w), dtype='float32')\n+inner = Convolution2D(conv_num_filters, filter_size, filter_size, border_mode='same',\n+                      activation=act, input_shape=(1, img_h, img_w), name='conv1')(input_data)\n+inner = MaxPooling2D(pool_size=(pool_size_1, pool_size_1), name='max1')(inner)\n+inner = Convolution2D(conv_num_filters, filter_size, filter_size, border_mode='same',\n+                      activation=act, name='conv2')(inner)\n+inner = MaxPooling2D(pool_size=(pool_size_2, pool_size_2), name='max2')(inner)\n+\n+conv_to_rnn_dims = ((img_h / (pool_size_1 * pool_size_2)) * conv_num_filters, img_w / (pool_size_1 * pool_size_2))\n+inner = Reshape(target_shape=conv_to_rnn_dims, name='reshape')(inner)\n+inner = Permute(dims=(2, 1), name='permute')(inner)\n+\n+# cuts down input size going into RNN:\n+inner = TimeDistributed(Dense(time_dense_size, activation=act, name='dense1'))(inner)\n+\n+# Two layers of bidirecitonal GRUs\n+# GRU seems to work as well, if not better than LSTM:\n+gru_1 = GRU(rnn_size, return_sequences=True, name='gru1')(inner)\n+gru_1b = GRU(rnn_size, return_sequences=True, go_backwards=True, name='gru1_b')(inner)\n+gru1_merged = merge([gru_1, gru_1b], mode='sum')\n+gru_2 = GRU(rnn_size, return_sequences=True, name='gru2')(gru1_merged)\n+gru_2b = GRU(rnn_size, return_sequences=True, go_backwards=True)(gru1_merged)\n+\n+# transforms RNN output to character activations:\n+inner = TimeDistributed(Dense(img_gen.get_output_size(), name='dense2'))(merge([gru_2, gru_2b], mode='concat'))\n+y_pred = Activation('softmax', name='softmax')(inner)\n+Model(input=[input_data], output=y_pred).summary()\n+\n+labels = Input(name='the_labels', shape=[img_gen.absolute_max_string_len], dtype='float32')\n+input_length = Input(name='input_length', shape=[1], dtype='int64')\n+label_length = Input(name='label_length', shape=[1], dtype='int64')\n+# Keras doesn't currently support loss funcs with extra parameters\n+# so CTC loss is implemented in a lambda layer\n+loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name=\"ctc\")([y_pred, labels, input_length, label_length])\n+\n+lr = 0.03\n+# clipnorm seems to speeds up convergence\n+clipnorm = 5\n+sgd = SGD(lr=lr, decay=3e-7, momentum=0.9, nesterov=True, clipnorm=clipnorm)\n+\n+model = Model(input=[input_data, labels, input_length, label_length], output=[loss_out])\n+\n+# the loss calc occurs elsewhere, so use a dummy lambda func for the loss\n+model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer=sgd)\n+\n+# captures output of softmax so we can decode the output during visualization\n+test_func = K.function([input_data], [y_pred])\n+\n+viz_cb = VizCallback(test_func, img_gen.next_val())\n+\n+model.fit_generator(generator=img_gen.next_train(), samples_per_epoch=(words_per_epoch - val_words),\n+                    nb_epoch=nb_epoch, validation_data=img_gen.next_val(), nb_val_samples=val_words,\n+                    callbacks=[viz_cb, img_gen])\n\n@@ -1586,3 +1586,112 @@ def random_binomial(shape, p=0.0, dtype=_FLOATX, seed=None):\n     return tf.select(tf.random_uniform(shape, dtype=dtype, seed=seed) <= p,\n                      tf.ones(shape, dtype=dtype),\n                      tf.zeros(shape, dtype=dtype))\n+\n+# CTC\n+# tensorflow has a native implemenation, but it uses sparse tensors\n+# and therefore requires a wrapper for Keras. The functions below convert\n+# dense to sparse tensors and also wraps up the beam search code that is\n+# in tensorflow's CTC implementation\n+\n+def ctc_label_dense_to_sparse(labels, label_lengths):\n+    # undocumented feature soon to be made public\n+    from tensorflow.python.ops import functional_ops\n+    label_shape = tf.shape(labels)\n+    num_batches_tns = tf.pack([label_shape[0]])\n+    max_num_labels_tns = tf.pack([label_shape[1]])\n+\n+    def range_less_than(previous_state, current_input):\n+        return tf.expand_dims(tf.range(label_shape[1]), 0) < current_input\n+\n+    init = tf.cast(tf.fill(max_num_labels_tns, 0), tf.bool)\n+    dense_mask = functional_ops.scan(range_less_than, label_lengths,\n+                                     initializer=init, parallel_iterations=1)\n+    dense_mask = dense_mask[:, 0, :]\n+\n+    label_array = tf.reshape(tf.tile(tf.range(0, label_shape[1]), num_batches_tns),\n+                             label_shape)\n+    label_ind = tf.boolean_mask(label_array, dense_mask)\n+\n+    batch_array = tf.transpose(tf.reshape(tf.tile(tf.range(0, label_shape[0]), \n+                                                  max_num_labels_tns), tf.reverse(label_shape, [True])))\n+    batch_ind = tf.boolean_mask(batch_array, dense_mask)\n+    indices = tf.transpose(tf.reshape(tf.concat(0, [batch_ind, label_ind]), [2,-1]))\n+\n+    vals_sparse = tf.gather_nd(labels, indices)\n+\n+    return tf.SparseTensor(tf.to_int64(indices), vals_sparse, tf.to_int64(label_shape))\n+\n+\n+def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n+\n+    '''Runs CTC loss algorithm on each batch element.\n+\n+    # Arguments\n+        y_true: tensor (samples, max_string_length) containing the truth labels\n+        y_pred: tensor (samples, time_steps, num_categories) containing the prediction,\n+                or output of the softmax\n+        input_length: tensor (samples,1) containing the sequence length for\n+                each batch item in y_pred\n+        label_length: tensor (samples,1) containing the sequence length for\n+                each batch item in y_true\n+\n+    # Returns\n+        Tensor with shape (samples,1) containing the\n+            CTC loss of each element\n+    '''\n+    label_length = tf.to_int32(tf.squeeze(label_length))\n+    input_length = tf.to_int32(tf.squeeze(input_length))\n+    sparse_labels = tf.to_int32(ctc_label_dense_to_sparse(y_true, label_length))\n+\n+    y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + 1e-8)\n+\n+    return tf.expand_dims(tf.contrib.ctc.ctc_loss(inputs = y_pred,\n+                                                  labels = sparse_labels,\n+                                                  sequence_length = input_length), 1)\n+\n+def ctc_decode(y_pred, input_length, greedy = True, beam_width = None,\n+               dict_seq_lens = None, dict_values = None):\n+    '''Decodes the output of a softmax using either\n+       greedy (also known as best path) or a constrained dictionary\n+       search.  \n+\n+    # Arguments\n+        y_pred: tensor (samples, time_steps, num_categories) containing the prediction,\n+                or output of the softmax\n+        input_length: tensor (samples,1) containing the sequence length for\n+                each batch item in y_pred\n+        greedy:  perform much faster best-path search if true.  This does\n+                not use a dictionary\n+        beam_width:  if greedy is false and this value is not none, then\n+                the constrained dictionary search uses a beam of this width\n+        dict_seq_lens: the length of each element in the dict_values list\n+        dict_values:  list of lists representing the dictionary.\n+\n+    # Returns\n+        Tensor with shape (samples,time_steps,num_categories) containing the\n+            path probabilities (in softmax output format).  Note that a function that\n+            pulls out the argmax and collapses blank labels is still needed.\n+    '''\n+    y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + 1e-8)\n+    input_length = tf.to_int32(tf.squeeze(input_length))\n+\n+    if greedy:\n+        (decoded, log_prob) = tf.contrib.ctc.ctc_greedy_decoder(\n+            inputs = y_pred,\n+            sequence_length = input_length)\n+    else:\n+        if beam_width is not None:\n+            (decoded, log_prob) = tf.contrib.ctc.ctc_beam_search_decoder(\n+                inputs = y_pred,\n+                sequence_length = input_length,\n+                dict_seq_lens = dict_seq_lens, dict_values = dict_values)\n+        else:\n+            (decoded, log_prob) = tf.contrib.ctc.ctc_beam_search_decoder(\n+                inputs = y_pred,\n+                sequence_length = input_length, beam_width = beam_width,\n+                dict_seq_lens = dict_seq_lens, dict_values = dict_values)\n+\n+    decoded_dense = [tf.sparse_to_dense(st.indices, st.shape, st.values, default_value = -1)\n+                     for st in decoded]\n+\n+    return (decoded_dense, log_prob)\n\n@@ -1319,3 +1319,105 @@ def random_binomial(shape, p=0.0, dtype=_FLOATX, seed=None):\n         seed = np.random.randint(1, 10e6)\n     rng = RandomStreams(seed=seed)\n     return rng.binomial(shape, p=p, dtype=dtype)\n+\n+# Theano implementation of CTC\n+# Used with permission from Shawn Tan\n+# https://github.com/shawntan/\n+# Note that tensorflow's native CTC code is significantly\n+# faster than this\n+\n+def ctc_interleave_blanks(Y):\n+    Y_ = T.alloc(-1, Y.shape[0] * 2 + 1)\n+    Y_ = T.set_subtensor(Y_[T.arange(Y.shape[0]) * 2 + 1], Y)\n+    return Y_\n+\n+def ctc_create_skip_idxs(Y):\n+    skip_idxs = T.arange((Y.shape[0] - 3) // 2) * 2 + 1\n+    non_repeats = T.neq(Y[skip_idxs], Y[skip_idxs + 2])\n+    return skip_idxs[non_repeats.nonzero()]\n+\n+def ctc_update_log_p(skip_idxs, zeros, active, log_p_curr, log_p_prev):\n+    active_skip_idxs = skip_idxs[(skip_idxs < active).nonzero()]\n+    active_next = T.cast(T.minimum(\n+        T.maximum(\n+            active + 1,\n+            T.max(T.concatenate([active_skip_idxs, [-1]])) + 2 + 1\n+        ), log_p_curr.shape[0]), 'int32')\n+\n+    common_factor = T.max(log_p_prev[:active])\n+    p_prev = T.exp(log_p_prev[:active] - common_factor)\n+    _p_prev = zeros[:active_next]\n+    # copy over\n+    _p_prev = T.set_subtensor(_p_prev[:active], p_prev)\n+    # previous transitions\n+    _p_prev = T.inc_subtensor(_p_prev[1:], _p_prev[:-1])\n+    # skip transitions\n+    _p_prev = T.inc_subtensor(_p_prev[active_skip_idxs + 2], p_prev[active_skip_idxs])\n+    updated_log_p_prev = T.log(_p_prev) + common_factor\n+\n+    log_p_next = T.set_subtensor(\n+        zeros[:active_next],\n+        log_p_curr[:active_next] + updated_log_p_prev\n+    )\n+    return active_next, log_p_next\n+\n+def ctc_path_probs(predict, Y, alpha=1e-4):\n+    smoothed_predict = (1 - alpha) * predict[:, Y] + alpha * np.float32(1.) / Y.shape[0]\n+    L = T.log(smoothed_predict)\n+    zeros = T.zeros_like(L[0])\n+    base = T.set_subtensor(zeros[:1], np.float32(1))\n+    log_first = zeros\n+\n+    f_skip_idxs = ctc_create_skip_idxs(Y)\n+    b_skip_idxs = ctc_create_skip_idxs(Y[::-1])  # there should be a shortcut to calculating this\n+\n+    def step(log_f_curr, log_b_curr, f_active, log_f_prev, b_active, log_b_prev):\n+        f_active_next, log_f_next = ctc_update_log_p(f_skip_idxs, zeros, f_active, log_f_curr, log_f_prev)\n+        b_active_next, log_b_next = ctc_update_log_p(b_skip_idxs, zeros, b_active, log_b_curr, log_b_prev)\n+        return f_active_next, log_f_next, b_active_next, log_b_next\n+\n+    [f_active, log_f_probs, b_active, log_b_probs], _ = theano.scan(\n+        step, sequences=[L, L[::-1, ::-1]], outputs_info=[np.int32(1), log_first, np.int32(1), log_first])\n+\n+    idxs = T.arange(L.shape[1]).dimshuffle('x', 0)\n+    mask = (idxs < f_active.dimshuffle(0, 'x')) & (idxs < b_active.dimshuffle(0, 'x'))[::-1, ::-1]\n+    log_probs = log_f_probs + log_b_probs[::-1, ::-1] - L\n+    return log_probs, mask\n+\n+def ctc_cost(predict, Y):\n+    log_probs, mask = ctc_path_probs(predict, ctc_interleave_blanks(Y))\n+    common_factor = T.max(log_probs)\n+    total_log_prob = T.log(T.sum(T.exp(log_probs - common_factor)[mask.nonzero()])) + common_factor\n+    return -total_log_prob\n+\n+# batchifies original CTC code\n+def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n+    '''Runs CTC loss algorithm on each batch element.\n+\n+    # Arguments\n+        y_true: tensor (samples, max_string_length) containing the truth labels\n+        y_pred: tensor (samples, time_steps, num_categories) containing the prediction,\n+                or output of the softmax\n+        input_length: tensor (samples,1) containing the sequence length for\n+                each batch item in y_pred\n+        label_length: tensor (samples,1) containing the sequence length for\n+                each batch item in y_true\n+\n+    # Returns\n+        Tensor with shape (samples,1) containing the\n+            CTC loss of each element\n+    '''\n+\n+    def ctc_step(y_true_step, y_pred_step, input_length_step, label_length_step):\n+        y_pred_step = y_pred_step[0: input_length_step[0]]\n+        y_true_step = y_true_step[0:label_length_step[0]]\n+        return ctc_cost(y_pred_step, y_true_step)\n+\n+    ret, _ = theano.scan(\n+        fn = ctc_step,\n+        outputs_info=None,\n+        sequences=[y_true, y_pred, input_length, label_length]\n+    )\n+\n+    ret = ret.dimshuffle('x', 0)\n+    return ret\n\n@@ -581,6 +581,48 @@ class TestBackend(object):\n         assert(np.max(rand) == 1)\n         assert(np.min(rand) == 0)\n \n+    def test_ctc(self):\n+        # simplified version of TensorFlow's test\n+\n+        label_lens = np.expand_dims(np.asarray([5, 4]), 1)\n+        input_lens = np.expand_dims(np.asarray([5, 5]), 1)  # number of timesteps\n+\n+        # the Theano and Tensorflow CTC code use different methods to ensure\n+        # numerical stability.  The Theano code subtracts out the max\n+        # before the final log, so the results are different but scale\n+        # identically and still train properly\n+        loss_log_probs_tf = [3.34211, 5.42262]\n+        loss_log_probs_th = [1.73308, 3.81351]\n+\n+        # dimensions are batch x time x categories\n+        labels = np.asarray([[0, 1, 2, 1, 0], [0, 1, 1, 0, -1]])\n+        inputs = np.asarray(\n+            [[[0.633766, 0.221185, 0.0917319, 0.0129757, 0.0142857, 0.0260553],\n+              [0.111121, 0.588392, 0.278779, 0.0055756, 0.00569609, 0.010436],\n+              [0.0357786, 0.633813, 0.321418, 0.00249248, 0.00272882, 0.0037688],\n+              [0.0663296, 0.643849, 0.280111, 0.00283995, 0.0035545, 0.00331533],\n+              [0.458235, 0.396634, 0.123377, 0.00648837, 0.00903441, 0.00623107]],\n+             [[0.30176, 0.28562, 0.0831517, 0.0862751, 0.0816851, 0.161508],\n+              [0.24082, 0.397533, 0.0557226, 0.0546814, 0.0557528, 0.19549],\n+              [0.230246, 0.450868, 0.0389607, 0.038309, 0.0391602, 0.202456],\n+              [0.280884, 0.429522, 0.0326593, 0.0339046, 0.0326856, 0.190345],\n+              [0.423286, 0.315517, 0.0338439, 0.0393744, 0.0339315, 0.154046]]],\n+            dtype=np.float32)\n+\n+        labels_tf = KTF.variable(labels, dtype=\"int32\")\n+        inputs_tf = KTF.variable(inputs, dtype=\"float32\")\n+        input_lens_tf = KTF.variable(input_lens, dtype=\"int32\")\n+        label_lens_tf = KTF.variable(label_lens, dtype=\"int32\")\n+        res = KTF.eval(KTF.ctc_batch_cost(labels_tf, inputs_tf, input_lens_tf, label_lens_tf))\n+        assert_allclose(res[:, 0], loss_log_probs_tf, atol=1e-05)\n+\n+        labels_th = KTH.variable(labels, dtype=\"int32\")\n+        inputs_th = KTH.variable(inputs, dtype=\"float32\")\n+        input_lens_th = KTH.variable(input_lens, dtype=\"int32\")\n+        label_lens_th = KTH.variable(label_lens, dtype=\"int32\")\n+        res = KTH.eval(KTH.ctc_batch_cost(labels_th, inputs_th, input_lens_th, label_lens_th))\n+        assert_allclose(res[0, :], loss_log_probs_th, atol=1e-05)\n+\n     def test_one_hot(self):\n         input_length = 10\n         nb_classes = 20\n@@ -591,6 +633,5 @@ class TestBackend(object):\n             koh = K.eval(K.one_hot(K.variable(indices, dtype='int32'), nb_classes))\n             assert np.all(koh == oh)\n \n-\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
