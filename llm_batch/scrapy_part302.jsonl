{"custom_id": "scrapy#9aea1f096171d38348b1403302c6c40eeef7f0a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 75 | Lines Deleted: 61 | Files Changed: 14 | Hunks: 20 | Methods Changed: 15 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 136 | Churn Cumulative: 6343 | Contributors (this commit): 39 | Commits (past 90d): 38 | Contributors (cumulative): 124 | DMM Complexity: 0.0\n\nDIFF:\n@@ -5,8 +5,7 @@ from gzip import GzipFile\n \n from scrapy.spiders import Spider\n from scrapy.http import Response, Request, HtmlResponse\n-from scrapy.downloadermiddlewares.httpcompression import HttpCompressionMiddleware, \\\n-    ACCEPTED_ENCODINGS\n+from scrapy.downloadermiddlewares.httpcompression import HttpCompressionMiddleware, ACCEPTED_ENCODINGS\n from scrapy.responsetypes import responsetypes\n from scrapy.utils.gz import gunzip\n from tests import tests_datadir\n\n@@ -77,12 +77,9 @@ class RedirectMiddlewareTest(unittest.TestCase):\n         assert isinstance(req2, Request)\n         self.assertEqual(req2.url, url2)\n         self.assertEqual(req2.method, 'GET')\n-        assert 'Content-Type' not in req2.headers, \\\n-            \"Content-Type header must not be present in redirected request\"\n-        assert 'Content-Length' not in req2.headers, \\\n-            \"Content-Length header must not be present in redirected request\"\n-        assert not req2.body, \\\n-            \"Redirected body must be empty, not '%s'\" % req2.body\n+        assert 'Content-Type' not in req2.headers, \"Content-Type header must not be present in redirected request\"\n+        assert 'Content-Length' not in req2.headers, \"Content-Length header must not be present in redirected request\"\n+        assert not req2.body, \"Redirected body must be empty, not '%s'\" % req2.body\n \n         # response without Location header but with status code is 3XX should be ignored\n         del rsp.headers['Location']\n@@ -244,12 +241,9 @@ class MetaRefreshMiddlewareTest(unittest.TestCase):\n         assert isinstance(req2, Request)\n         self.assertEqual(req2.url, 'http://example.org/newpage')\n         self.assertEqual(req2.method, 'GET')\n-        assert 'Content-Type' not in req2.headers, \\\n-            \"Content-Type header must not be present in redirected request\"\n-        assert 'Content-Length' not in req2.headers, \\\n-            \"Content-Length header must not be present in redirected request\"\n-        assert not req2.body, \\\n-            \"Redirected body must be empty, not '%s'\" % req2.body\n+        assert 'Content-Type' not in req2.headers, \"Content-Type header must not be present in redirected request\"\n+        assert 'Content-Length' not in req2.headers, \"Content-Length header must not be present in redirected request\"\n+        assert not req2.body, \"Redirected body must be empty, not '%s'\" % req2.body\n \n     def test_max_redirect_times(self):\n         self.mw.max_redirect_times = 1\n\n@@ -88,8 +88,7 @@ class SelectorTestCase(unittest.TestCase):\n         \"\"\"Check that classes are using slots and are weak-referenceable\"\"\"\n         x = Selector(text='')\n         weakref.ref(x)\n-        assert not hasattr(x, '__dict__'), \"%s does not use __slots__\" % \\\n-            x.__class__.__name__\n+        assert not hasattr(x, '__dict__'), \"%s does not use __slots__\" % x.__class__.__name__\n \n     def test_selector_bad_args(self):\n         with self.assertRaisesRegex(ValueError, 'received both response and text'):\n\n@@ -86,8 +86,7 @@ class BaseSettingsTest(unittest.TestCase):\n \n     def test_set_calls_settings_attributes_methods_on_update(self):\n         attr = SettingsAttribute('value', 10)\n-        with mock.patch.object(attr, '__setattr__') as mock_setattr, \\\n-                mock.patch.object(attr, 'set') as mock_set:\n+        with mock.patch.object(attr, '__setattr__') as mock_setattr, mock.patch.object(attr, 'set') as mock_set:\n \n             self.settings.attributes = {'TEST_OPTION': attr}\n \n\n@@ -11,8 +11,14 @@ from scrapy import signals\n from scrapy.settings import Settings\n from scrapy.http import Request, Response, TextResponse, XmlResponse, HtmlResponse\n from scrapy.spiders.init import InitSpider\n-from scrapy.spiders import Spider, CrawlSpider, Rule, XMLFeedSpider, \\\n-    CSVFeedSpider, SitemapSpider\n+from scrapy.spiders import (\n+    CSVFeedSpider,\n+    CrawlSpider,\n+    Rule,\n+    SitemapSpider,\n+    Spider,\n+    XMLFeedSpider,\n+)\n from scrapy.linkextractors import LinkExtractor\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.test import get_crawler\n\n@@ -6,16 +6,28 @@ from scrapy.http import Response, Request\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n from scrapy.downloadermiddlewares.redirect import RedirectMiddleware\n-from scrapy.spidermiddlewares.referer import RefererMiddleware, \\\n-    POLICY_NO_REFERRER, POLICY_NO_REFERRER_WHEN_DOWNGRADE, \\\n-    POLICY_SAME_ORIGIN, POLICY_ORIGIN, POLICY_ORIGIN_WHEN_CROSS_ORIGIN, \\\n-    POLICY_SCRAPY_DEFAULT, POLICY_UNSAFE_URL, \\\n-    POLICY_STRICT_ORIGIN, POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN, \\\n-    DefaultReferrerPolicy, \\\n-    NoReferrerPolicy, NoReferrerWhenDowngradePolicy, \\\n-    OriginWhenCrossOriginPolicy, OriginPolicy, \\\n-    StrictOriginWhenCrossOriginPolicy, StrictOriginPolicy, \\\n-    SameOriginPolicy, UnsafeUrlPolicy, ReferrerPolicy\n+from scrapy.spidermiddlewares.referer import (\n+    DefaultReferrerPolicy,\n+    NoReferrerPolicy,\n+    NoReferrerWhenDowngradePolicy,\n+    OriginPolicy,\n+    OriginWhenCrossOriginPolicy,\n+    POLICY_NO_REFERRER,\n+    POLICY_NO_REFERRER_WHEN_DOWNGRADE,\n+    POLICY_ORIGIN,\n+    POLICY_ORIGIN_WHEN_CROSS_ORIGIN,\n+    POLICY_SAME_ORIGIN,\n+    POLICY_SCRAPY_DEFAULT,\n+    POLICY_STRICT_ORIGIN,\n+    POLICY_STRICT_ORIGIN_WHEN_CROSS_ORIGIN,\n+    POLICY_UNSAFE_URL,\n+    RefererMiddleware,\n+    ReferrerPolicy,\n+    SameOriginPolicy,\n+    StrictOriginPolicy,\n+    StrictOriginWhenCrossOriginPolicy,\n+    UnsafeUrlPolicy,\n+)\n \n \n class TestRefererMiddleware(TestCase):\n\n@@ -29,8 +29,7 @@ class CurlToRequestKwargsTest(unittest.TestCase):\n         self._test_command(curl_command, expected_result)\n \n     def test_get_basic_auth(self):\n-        curl_command = 'curl \"https://api.test.com/\" -u ' \\\n-                       '\"some_username:some_password\"'\n+        curl_command = 'curl \"https://api.test.com/\" -u \"some_username:some_password\"'\n         expected_result = {\n             \"method\": \"GET\",\n             \"url\": \"https://api.test.com/\",\n@@ -212,8 +211,7 @@ class CurlToRequestKwargsTest(unittest.TestCase):\n         with warnings.catch_warnings():  # avoid warning when executing tests\n             warnings.simplefilter('ignore')\n             curl_command = 'curl --bar --baz http://www.example.com'\n-            expected_result = \\\n-                {\"method\": \"GET\", \"url\": \"http://www.example.com\"}\n+            expected_result = {\"method\": \"GET\", \"url\": \"http://www.example.com\"}\n             self.assertEqual(curl_to_request_kwargs(curl_command), expected_result)\n \n         # case 2: ignore_unknown_options=False (raise exception):\n\n@@ -2,8 +2,13 @@ from twisted.trial import unittest\n from twisted.internet import reactor, defer\n from twisted.python.failure import Failure\n \n-from scrapy.utils.defer import mustbe_deferred, process_chain, \\\n-    process_chain_both, process_parallel, iter_errback\n+from scrapy.utils.defer import (\n+    iter_errback,\n+    mustbe_deferred,\n+    process_chain,\n+    process_chain_both,\n+    process_parallel,\n+)\n \n \n class MustbeDeferredTest(unittest.TestCase):\n\n@@ -15,18 +15,20 @@ class XmliterTestCase(unittest.TestCase):\n     xmliter = staticmethod(xmliter)\n \n     def test_xmliter(self):\n-        body = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\\n+        body = b\"\"\"\n+            <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n             <products xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n-                      xsi:noNamespaceSchemaLocation=\"someschmea.xsd\">\\\n-              <product id=\"001\">\\\n-                <type>Type 1</type>\\\n-                <name>Name 1</name>\\\n-              </product>\\\n-              <product id=\"002\">\\\n-                <type>Type 2</type>\\\n-                <name>Name 2</name>\\\n-              </product>\\\n-            </products>\"\"\"\n+                      xsi:noNamespaceSchemaLocation=\"someschmea.xsd\">\n+              <product id=\"001\">\n+                <type>Type 1</type>\n+                <name>Name 1</name>\n+              </product>\n+              <product id=\"002\">\n+                <type>Type 2</type>\n+                <name>Name 2</name>\n+              </product>\n+            </products>\n+        \"\"\"\n \n         response = XmlResponse(url=\"http://example.com\", body=body)\n         attrs = []\n@@ -115,7 +117,7 @@ class XmliterTestCase(unittest.TestCase):\n                          [[u'one'], [u'two']])\n \n     def test_xmliter_namespaces(self):\n-        body = b\"\"\"\\\n+        body = b\"\"\"\n             <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n             <rss version=\"2.0\" xmlns:g=\"http://base.google.com/ns/1.0\">\n                 <channel>\n@@ -185,7 +187,7 @@ class LxmlXmliterTestCase(XmliterTestCase):\n     xmliter = staticmethod(xmliter_lxml)\n \n     def test_xmliter_iterate_namespace(self):\n-        body = b\"\"\"\\\n+        body = b\"\"\"\n             <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n             <rss version=\"2.0\" xmlns=\"http://base.google.com/ns/1.0\">\n                 <channel>\n@@ -214,7 +216,7 @@ class LxmlXmliterTestCase(XmliterTestCase):\n         self.assertEqual(node.xpath('text()').getall(), ['http://www.mydummycompany.com/images/item2.jpg'])\n \n     def test_xmliter_namespaces_prefix(self):\n-        body = b\"\"\"\\\n+        body = b\"\"\"\n         <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n         <root>\n             <h:table xmlns:h=\"http://www.w3.org/TR/html4/\">\n\n@@ -1,7 +1,11 @@\n import unittest\n from scrapy.http import Request\n-from scrapy.utils.request import request_fingerprint, _fingerprint_cache, \\\n-    request_authenticate, request_httprepr\n+from scrapy.utils.request import (\n+    _fingerprint_cache,\n+    request_authenticate,\n+    request_fingerprint,\n+    request_httprepr,\n+)\n \n \n class UtilsRequestTest(unittest.TestCase):\n\n@@ -37,8 +37,7 @@ class ResponseUtilsTest(unittest.TestCase):\n             self.assertIn(b'<base href=\"' + to_bytes(url) + b'\">', bbody)\n             return True\n         response = HtmlResponse(url, body=body)\n-        assert open_in_browser(response, _openfunc=browser_open), \\\n-            \"Browser not called\"\n+        assert open_in_browser(response, _openfunc=browser_open), \"Browser not called\"\n \n         resp = Response(url, body=body)\n         self.assertRaises(TypeError, open_in_browser, resp, debug=True)\n\n@@ -156,8 +156,7 @@ Disallow: /forum/active/\n \n     def test_sitemap_blanklines(self):\n         \"\"\"Assert we can deal with starting blank lines before <xml> tag\"\"\"\n-        s = Sitemap(b\"\"\"\\\n-\n+        s = Sitemap(b\"\"\"\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <sitemapindex xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n \n\n@@ -207,8 +207,7 @@ def create_guess_scheme_t(args):\n     def do_expected(self):\n         url = guess_scheme(args[0])\n         assert url.startswith(args[1]), \\\n-            'Wrong scheme guessed: for `%s` got `%s`, expected `%s...`' % (\n-                args[0], url, args[1])\n+            'Wrong scheme guessed: for `%s` got `%s`, expected `%s...`' % (args[0], url, args[1])\n     return do_expected\n \n \n\n@@ -356,9 +356,8 @@ class WebClientTestCase(unittest.TestCase):\n         \"\"\" Test that non-standart body encoding matches\n         Content-Encoding header \"\"\"\n         body = b'\\xd0\\x81\\xd1\\x8e\\xd0\\xaf'\n-        return getPage(\n-            self.getURL('encoding'), body=body, response_transform=lambda r: r)\\\n-            .addCallback(self._check_Encoding, body)\n+        dfd = getPage(self.getURL('encoding'), body=body, response_transform=lambda r: r)\n+        return dfd.addCallback(self._check_Encoding, body)\n \n     def _check_Encoding(self, response, original_body):\n         content_encoding = to_unicode(response.headers[b'Content-Encoding'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a6a5fa91da8944943e2c9d8f34f09662be17b781", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 91 | Files Changed: 1 | Hunks: 1 | Methods Changed: 8 | Complexity Δ (Sum/Max): -23/0 | Churn Δ: 91 | Churn Cumulative: 133 | Contributors (this commit): 6 | Commits (past 90d): 1 | Contributors (cumulative): 6 | DMM Complexity: 0.5172413793103449\n\nDIFF:\n@@ -1,91 +0,0 @@\n-\"\"\"\n-HTMLParser-based link extractor\n-\"\"\"\n-import warnings\n-from html.parser import HTMLParser\n-from urllib.parse import urljoin\n-\n-from w3lib.url import safe_url_string\n-from w3lib.html import strip_html5_whitespace\n-\n-from scrapy.link import Link\n-from scrapy.utils.python import unique as unique_list\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-\n-\n-class HtmlParserLinkExtractor(HTMLParser):\n-\n-    def __init__(self, tag=\"a\", attr=\"href\", process=None, unique=False,\n-                 strip=True):\n-        HTMLParser.__init__(self)\n-\n-        warnings.warn(\n-            \"HtmlParserLinkExtractor is deprecated and will be removed in \"\n-            \"future releases. Please use scrapy.linkextractors.LinkExtractor\",\n-            ScrapyDeprecationWarning, stacklevel=2,\n-        )\n-\n-        self.scan_tag = tag if callable(tag) else lambda t: t == tag\n-        self.scan_attr = attr if callable(attr) else lambda a: a == attr\n-        self.process_attr = process if callable(process) else lambda v: v\n-        self.unique = unique\n-        self.strip = strip\n-\n-    def _extract_links(self, response_text, response_url, response_encoding):\n-        self.reset()\n-        self.feed(response_text)\n-        self.close()\n-\n-        links = unique_list(self.links, key=lambda link: link.url) if self.unique else self.links\n-\n-        ret = []\n-        base_url = urljoin(response_url, self.base_url) if self.base_url else response_url\n-        for link in links:\n-            if isinstance(link.url, str):\n-                link.url = link.url.encode(response_encoding)\n-            try:\n-                link.url = urljoin(base_url, link.url)\n-            except ValueError:\n-                continue\n-            link.url = safe_url_string(link.url, response_encoding)\n-            link.text = link.text.decode(response_encoding)\n-            ret.append(link)\n-\n-        return ret\n-\n-    def extract_links(self, response):\n-        # wrapper needed to allow to work directly with text\n-        return self._extract_links(response.body, response.url, response.encoding)\n-\n-    def reset(self):\n-        HTMLParser.reset(self)\n-\n-        self.base_url = None\n-        self.current_link = None\n-        self.links = []\n-\n-    def handle_starttag(self, tag, attrs):\n-        if tag == 'base':\n-            self.base_url = dict(attrs).get('href')\n-        if self.scan_tag(tag):\n-            for attr, value in attrs:\n-                if self.scan_attr(attr):\n-                    if self.strip:\n-                        value = strip_html5_whitespace(value)\n-                    url = self.process_attr(value)\n-                    link = Link(url=url)\n-                    self.links.append(link)\n-                    self.current_link = link\n-\n-    def handle_endtag(self, tag):\n-        if self.scan_tag(tag):\n-            self.current_link = None\n-\n-    def handle_data(self, data):\n-        if self.current_link:\n-            self.current_link.text = self.current_link.text + data\n-\n-    def matches(self, url):\n-        \"\"\"This extractor matches with any url, since\n-        it doesn't contain any patterns\"\"\"\n-        return True\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3f7e8635f479f4de2ab1c3d518010730a5f6f0a6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 18 | Files Changed: 3 | Hunks: 13 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/1 | Churn Δ: 49 | Churn Cumulative: 1120 | Contributors (this commit): 21 | Commits (past 90d): 13 | Contributors (cumulative): 33 | DMM Complexity: 0.75\n\nDIFF:\n@@ -108,7 +108,7 @@ class ScrapyCommand:\n \n class BaseRunSpiderCommand(ScrapyCommand):\n     \"\"\"\n-    Common class used to share functionality between the crawl and runspider commands\n+    Common class used to share functionality between the crawl, parse and runspider commands\n     \"\"\"\n     def add_options(self, parser):\n         ScrapyCommand.add_options(self, parser)\n\n@@ -4,18 +4,16 @@ import logging\n from itemadapter import is_item, ItemAdapter\n from w3lib.url import is_url\n \n-from scrapy.commands import ScrapyCommand\n+from scrapy.commands import BaseRunSpiderCommand\n from scrapy.http import Request\n from scrapy.utils import display\n-from scrapy.utils.conf import arglist_to_dict\n from scrapy.utils.spider import iterate_spider_output, spidercls_for_request\n from scrapy.exceptions import UsageError\n \n logger = logging.getLogger(__name__)\n \n \n-class Command(ScrapyCommand):\n-\n+class Command(BaseRunSpiderCommand):\n     requires_project = True\n \n     spider = None\n@@ -31,11 +29,9 @@ class Command(ScrapyCommand):\n         return \"Parse URL (using its spider) and print the results\"\n \n     def add_options(self, parser):\n-        ScrapyCommand.add_options(self, parser)\n+        BaseRunSpiderCommand.add_options(self, parser)\n         parser.add_option(\"--spider\", dest=\"spider\", default=None,\n                           help=\"use this spider without looking for one\")\n-        parser.add_option(\"-a\", dest=\"spargs\", action=\"append\", default=[], metavar=\"NAME=VALUE\",\n-                          help=\"set spider argument (may be repeated)\")\n         parser.add_option(\"--pipelines\", action=\"store_true\",\n                           help=\"process items through pipelines\")\n         parser.add_option(\"--nolinks\", dest=\"nolinks\", action=\"store_true\",\n@@ -200,12 +196,15 @@ class Command(ScrapyCommand):\n             self.add_items(depth, items)\n             self.add_requests(depth, requests)\n \n+            scraped_data = items if opts.output else []\n             if depth < opts.depth:\n                 for req in requests:\n                     req.meta['_depth'] = depth + 1\n                     req.meta['_callback'] = req.callback\n                     req.callback = callback\n-                return requests\n+                scraped_data += requests\n+\n+            return scraped_data\n \n         # update request meta if any extra meta was passed through the --meta/-m opts.\n         if opts.meta:\n@@ -221,18 +220,11 @@ class Command(ScrapyCommand):\n         return request\n \n     def process_options(self, args, opts):\n-        ScrapyCommand.process_options(self, args, opts)\n+        BaseRunSpiderCommand.process_options(self, args, opts)\n \n-        self.process_spider_arguments(opts)\n         self.process_request_meta(opts)\n         self.process_request_cb_kwargs(opts)\n \n-    def process_spider_arguments(self, opts):\n-        try:\n-            opts.spargs = arglist_to_dict(opts.spargs)\n-        except ValueError:\n-            raise UsageError(\"Invalid -a value, use -a NAME=VALUE\", print_help=False)\n-\n     def process_request_meta(self, opts):\n         if opts.meta:\n             try:\n\n@@ -1,5 +1,5 @@\n import os\n-from os.path import join, abspath\n+from os.path import join, abspath, isfile, exists\n from twisted.internet import defer\n from scrapy.utils.testsite import SiteTest\n from scrapy.utils.testproc import ProcessTest\n@@ -218,3 +218,24 @@ ITEM_PIPELINES = {'%s.pipelines.MyPipeline': 1}\n         )\n         self.assertRegex(_textmode(out), r\"\"\"# Scraped Items  -+\\n\\[\\]\"\"\")\n         self.assertIn(\"\"\"Cannot find a rule that matches\"\"\", _textmode(stderr))\n+\n+    @defer.inlineCallbacks\n+    def test_output_flag(self):\n+        \"\"\"Checks if a file was created successfully having\n+        correct format containing correct data in it.\n+        \"\"\"\n+        file_name = 'data.json'\n+        file_path = join(self.proj_path, file_name)\n+        yield self.execute([\n+            '--spider', self.spider_name,\n+            '-c', 'parse',\n+            '-o', file_name,\n+            self.url('/html')\n+        ])\n+\n+        self.assertTrue(exists(file_path))\n+        self.assertTrue(isfile(file_path))\n+\n+        content = '[\\n{},\\n{\"foo\": \"bar\"}\\n]'\n+        with open(file_path, 'r') as f:\n+            self.assertEqual(f.read(), content)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#64c6af10e1b276db68ea722845621912d2023a75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 15 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 36 | Churn Cumulative: 2165 | Contributors (this commit): 2 | Commits (past 90d): 36 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,7 +16,6 @@ from scrapy.core.http2.types import H2ResponseDict\n from scrapy.http import Request\n from scrapy.http.headers import Headers\n from scrapy.responsetypes import responsetypes\n-from scrapy.utils.python import to_unicode\n \n if TYPE_CHECKING:\n     from scrapy.core.http2.protocol import H2ClientProtocol\n@@ -177,8 +176,8 @@ class Stream:\n         # Make sure that we are sending the request to the correct URL\n         url = urlparse(self._request.url)\n         return (\n-            url.netloc == to_unicode(self._protocol.metadata['uri'].host)\n-            or url.netloc == to_unicode(self._protocol.metadata['uri'].netloc)\n+            url.netloc == str(self._protocol.metadata['uri'].host, 'utf-8')\n+            or url.netloc == str(self._protocol.metadata['uri'].netloc, 'utf-8')\n             or url.netloc == f'{self._protocol.metadata[\"ip_address\"]}:{self._protocol.metadata[\"uri\"].port}'\n         )\n \n@@ -394,7 +393,7 @@ class Stream:\n         elif reason is StreamCloseReason.INVALID_HOSTNAME:\n             self._deferred_response.errback(InvalidHostname(\n                 self._request,\n-                to_unicode(self._protocol.metadata['uri'].host),\n+                str(self._protocol.metadata['uri'].host, 'utf-8'),\n                 f'{self._protocol.metadata[\"ip_address\"]}:{self._protocol.metadata[\"uri\"].port}'\n             ))\n \n\n@@ -23,7 +23,6 @@ from scrapy.core.http2.protocol import H2ClientFactory\n from scrapy.core.http2.stream import InactiveStreamClosed, InvalidHostname\n from scrapy.http import Request, Response, JsonRequest\n from scrapy.settings import Settings\n-from scrapy.utils.python import to_bytes, to_unicode\n from tests.mockserver import ssl_context_factory, LeafResource, Status\n \n \n@@ -39,7 +38,7 @@ def make_html_body(val):\n <h1>Hello from HTTP2<h1>\n <p>{val}</p>\n </html>'''\n-    return to_bytes(response)\n+    return bytes(response, 'utf-8')\n \n \n class Data:\n@@ -83,10 +82,11 @@ class PostDataJsonMixin:\n             'extra-data': extra_data\n         }\n         for k, v in request.requestHeaders.getAllRawHeaders():\n-            response['request-headers'][to_unicode(k)] = to_unicode(v[0])\n+            response['request-headers'][str(k, 'utf-8')] = str(v[0], 'utf-8')\n \n-        response_bytes = to_bytes(json.dumps(response))\n-        request.setHeader('Content-Type', 'application/json')\n+        response_bytes = bytes(json.dumps(response), 'utf-8')\n+        request.setHeader('Content-Type', 'application/json; charset=UTF-8')\n+        request.setHeader('Content-Encoding', 'UTF-8')\n         return response_bytes\n \n \n@@ -127,13 +127,14 @@ class NoContentLengthHeader(LeafResource):\n \n class QueryParams(LeafResource):\n     def render_GET(self, request: TxRequest):\n-        request.setHeader('Content-Type', 'application/json')\n+        request.setHeader('Content-Type', 'application/json; charset=UTF-8')\n+        request.setHeader('Content-Encoding', 'UTF-8')\n \n         query_params = {}\n         for k, v in request.args.items():\n-            query_params[to_unicode(k)] = to_unicode(v[0])\n+            query_params[str(k, 'utf-8')] = str(v[0], 'utf-8')\n \n-        return to_bytes(json.dumps(query_params))\n+        return bytes(json.dumps(query_params), 'utf-8')\n \n \n def get_client_certificate(key_file, certificate_file) -> PrivateCertificate:\n@@ -184,7 +185,7 @@ class Https2ClientProtocolTestCase(TestCase):\n             trustRoot=self.client_certificate,\n             acceptableProtocols=[b'h2']\n         )\n-        uri = URI.fromBytes(to_bytes(self.get_url('/')))\n+        uri = URI.fromBytes(bytes(self.get_url('/'), 'utf-8'))\n         h2_client_factory = H2ClientFactory(uri, Settings())\n         client_endpoint = SSL4ClientEndpoint(reactor, self.hostname, self.port_number, client_options)\n         self.client = yield client_endpoint.connect(h2_client_factory)\n@@ -278,7 +279,8 @@ class Https2ClientProtocolTestCase(TestCase):\n             self.assertEqual(len(response.body), content_length)\n \n             # Parse the body\n-            body = json.loads(to_unicode(response.body))\n+            content_encoding = str(response.headers[b'Content-Encoding'], 'utf-8')\n+            body = json.loads(str(response.body, content_encoding))\n             self.assertIn('request-body', body)\n             self.assertIn('extra-data', body)\n             self.assertIn('request-headers', body)\n@@ -292,9 +294,9 @@ class Https2ClientProtocolTestCase(TestCase):\n             # Check if headers were sent successfully\n             request_headers = body['request-headers']\n             for k, v in request.headers.items():\n-                k_str = to_unicode(k)\n+                k_str = str(k, 'utf-8')\n                 self.assertIn(k_str, request_headers)\n-                self.assertEqual(request_headers[k_str], to_unicode(v[0]))\n+                self.assertEqual(request_headers[k_str], str(v[0], 'utf-8'))\n \n         d.addCallback(assert_response)\n         d.addErrback(self.fail)\n@@ -494,7 +496,8 @@ class Https2ClientProtocolTestCase(TestCase):\n         request = Request(self.get_url(f'/query-params?{urlencode(params)}'))\n \n         def assert_query_params(response: Response):\n-            data = json.loads(to_unicode(response.body))\n+            content_encoding = str(response.headers[b'Content-Encoding'], 'utf-8')\n+            data = json.loads(str(response.body, content_encoding))\n             self.assertEqual(data, params)\n \n         d = self.client.request(request)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d54c4496ee57785f3d6f882e2d128bb64b6b262c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 69 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): 5/3 | Churn Δ: 81 | Churn Cumulative: 2226 | Contributors (this commit): 22 | Commits (past 90d): 9 | Contributors (cumulative): 29 | DMM Complexity: 1.0\n\nDIFF:\n@@ -83,25 +83,53 @@ def add_http_if_no_scheme(url):\n     return url\n \n \n-def guess_scheme(url):\n-    \"\"\"Add an URL scheme if missing: file:// for filepath-like input or\n-    http:// otherwise.\"\"\"\n-    # POSIX path\n-    if re.match(r'''^                   # start with...\n+def _is_posix_path(string):\n+    return bool(\n+        re.match(\n+            r'''\n+            ^                   # start with...\n             (\n                 \\.              # ...a single dot,\n                 (\n                     \\. | [^/\\.]+  # optionally followed by\n                 )?                # either a second dot or some characters\n+                |\n+                ~   # $HOME\n             )?      # optional match of \".\", \"..\" or \".blabla\"\n             /       # at least one \"/\" for a file path,\n             .       # and something after the \"/\"\n-                    ''', url, flags=re.VERBOSE):\n+            ''',\n+            string,\n+            flags=re.VERBOSE,\n+        )\n+    )\n+\n+\n+def _is_windows_path(string):\n+    return bool(\n+        re.match(\n+            r'''\n+            ^\n+            (\n+                [a-z]:\\\\\n+                | \\\\\\\\\n+            )\n+            ''',\n+            string,\n+            flags=re.IGNORECASE | re.VERBOSE,\n+        )\n+    )\n+\n+\n+def _is_path(string):\n+    return _is_posix_path(string) or _is_windows_path(string)\n+\n+\n+def guess_scheme(url):\n+    \"\"\"Add an URL scheme if missing: file:// for filepath-like input or\n+    http:// otherwise.\"\"\"\n+    if _is_path(url):\n         return any_to_uri(url)\n-    # Windows drive-letter path\n-    elif re.match(r'''^[a-z]:\\\\''', url, flags=re.IGNORECASE):\n-        return any_to_uri(url)\n-    else:\n     return add_http_if_no_scheme(url)\n \n \n\n@@ -1,8 +1,14 @@\n import unittest\n \n from scrapy.spiders import Spider\n-from scrapy.utils.url import (url_is_from_any_domain, url_is_from_spider,\n-                              add_http_if_no_scheme, guess_scheme, strip_url)\n+from scrapy.utils.url import (\n+    add_http_if_no_scheme,\n+    guess_scheme,\n+    _is_path,\n+    strip_url,\n+    url_is_from_any_domain,\n+    url_is_from_spider,\n+)\n \n \n __doctests__ = ['scrapy.utils.url']\n@@ -434,5 +440,28 @@ class StripUrl(unittest.TestCase):\n             self.assertEqual(strip_url(i, origin_only=True), o)\n \n \n+class IsPathTestCase(unittest.TestCase):\n+\n+    def test_path(self):\n+        for input_value, output_value in (\n+            # https://en.wikipedia.org/wiki/Path_(computing)#Representations_of_paths_by_operating_system_and_shell\n+            # Unix-like OS, Microsoft Windows / cmd.exe\n+            (\"/home/user/docs/Letter.txt\", True),\n+            (\"./inthisdir\", True),\n+            (\"../../greatgrandparent\", True),\n+            (\"~/.rcinfo\", True),\n+            (r\"C:\\user\\docs\\Letter.txt\", True),\n+            (\"/user/docs/Letter.txt\", True),\n+            (r\"C:\\Letter.txt\", True),\n+            (r\"\\\\Server01\\user\\docs\\Letter.txt\", True),\n+            (r\"\\\\?\\UNC\\Server01\\user\\docs\\Letter.txt\", True),\n+            (r\"\\\\?\\C:\\user\\docs\\Letter.txt\", True),\n+            (r\"C:\\user\\docs\\somefile.ext:alternate_stream_name\", True),\n+\n+            (r\"https://example.com\", False),\n+        ):\n+            self.assertEqual(_is_path(input_value), output_value, input_value)\n+\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#53c323b19d81784e6c376ce8b9602de24d8e3037", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 2234 | Contributors (this commit): 22 | Commits (past 90d): 11 | Contributors (cumulative): 29 | DMM Complexity: None\n\nDIFF:\n@@ -121,14 +121,14 @@ def _is_windows_path(string):\n     )\n \n \n-def _is_path(string):\n+def _is_filesystem_path(string):\n     return _is_posix_path(string) or _is_windows_path(string)\n \n \n def guess_scheme(url):\n     \"\"\"Add an URL scheme if missing: file:// for filepath-like input or\n     http:// otherwise.\"\"\"\n-    if _is_path(url):\n+    if _is_filesystem_path(url):\n         return any_to_uri(url)\n     return add_http_if_no_scheme(url)\n \n\n@@ -4,7 +4,7 @@ from scrapy.spiders import Spider\n from scrapy.utils.url import (\n     add_http_if_no_scheme,\n     guess_scheme,\n-    _is_path,\n+    _is_filesystem_path,\n     strip_url,\n     url_is_from_any_domain,\n     url_is_from_spider,\n@@ -460,7 +460,7 @@ class IsPathTestCase(unittest.TestCase):\n \n             (r\"https://example.com\", False),\n         ):\n-            self.assertEqual(_is_path(input_value), output_value, input_value)\n+            self.assertEqual(_is_filesystem_path(input_value), output_value, input_value)\n \n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0770961054f24d56d219a81d9e0c467de98312c7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 34 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 37 | Churn Cumulative: 970 | Contributors (this commit): 15 | Commits (past 90d): 7 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -66,9 +66,14 @@ class ProjectTest(unittest.TestCase):\n \n     def proc(self, *new_args, **popen_kwargs):\n         args = (sys.executable, '-m', 'scrapy.cmdline') + new_args\n-        p = subprocess.Popen(args, cwd=self.cwd, env=self.env,\n-                             stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n-                             **popen_kwargs)\n+        p = subprocess.Popen(\n+            args,\n+            cwd=popen_kwargs.pop('cwd', self.cwd),\n+            env=self.env,\n+            stdout=subprocess.PIPE,\n+            stderr=subprocess.PIPE,\n+            **popen_kwargs,\n+        )\n \n         def kill_proc():\n             p.kill()\n@@ -122,6 +127,32 @@ class StartprojectTest(ProjectTest):\n         self.assertEqual(2, self.call('startproject'))\n         self.assertEqual(2, self.call('startproject', self.project_name, project_dir, 'another_params'))\n \n+    def test_existing_project_dir(self):\n+        project_dir = mkdtemp()\n+        os.mkdir(os.path.join(project_dir, self.project_name))\n+\n+        p, out, err = self.proc('startproject', self.project_name, cwd=project_dir)\n+        print(out)\n+        print(err, file=sys.stderr)\n+        self.assertEqual(p.returncode, 0)\n+\n+        assert exists(join(abspath(project_dir), 'scrapy.cfg'))\n+        assert exists(join(abspath(project_dir), 'testproject'))\n+        assert exists(join(join(abspath(project_dir), self.project_name), '__init__.py'))\n+        assert exists(join(join(abspath(project_dir), self.project_name), 'items.py'))\n+        assert exists(join(join(abspath(project_dir), self.project_name), 'pipelines.py'))\n+        assert exists(join(join(abspath(project_dir), self.project_name), 'settings.py'))\n+        assert exists(join(join(abspath(project_dir), self.project_name), 'spiders', '__init__.py'))\n+\n+        self.assertEqual(0, self.call('startproject', self.project_name, project_dir + '2'))\n+\n+        self.assertEqual(1, self.call('startproject', self.project_name, project_dir))\n+        self.assertEqual(1, self.call('startproject', self.project_name + '2', project_dir))\n+        self.assertEqual(1, self.call('startproject', 'wrong---project---name'))\n+        self.assertEqual(1, self.call('startproject', 'sys'))\n+        self.assertEqual(2, self.call('startproject'))\n+        self.assertEqual(2, self.call('startproject', self.project_name, project_dir, 'another_params'))\n+\n \n def get_permissions_dict(path, renamings=None, ignore=None):\n     renamings = renamings or tuple()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#544c1f6e390c72053f768d3992ea2d0801363b83", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 23 | Files Changed: 2 | Hunks: 7 | Methods Changed: 4 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 42 | Churn Cumulative: 1331 | Contributors (this commit): 21 | Commits (past 90d): 16 | Contributors (cumulative): 29 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,7 +1,7 @@\n import re\n import os\n import string\n-from importlib import import_module\n+from importlib.util import find_spec\n from os.path import join, exists, abspath\n from shutil import ignore_patterns, move, copy2, copystat\n from stat import S_IWUSR as OWNER_WRITE_PERMISSION\n@@ -43,10 +43,10 @@ class Command(ScrapyCommand):\n     def _is_valid_name(self, project_name):\n         def _module_exists(module_name):\n             try:\n-                import_module(module_name)\n-                return True\n-            except ImportError:\n+                spec = find_spec(module_name)\n+            except ModuleNotFoundError:\n                 return False\n+            return spec is not None and spec.loader is not None\n \n         if not re.search(r'^[_a-zA-Z]\\w*$', project_name):\n             print('Error: Project names must begin with a letter and contain'\n\n@@ -92,7 +92,10 @@ class ProjectTest(unittest.TestCase):\n class StartprojectTest(ProjectTest):\n \n     def test_startproject(self):\n-        self.assertEqual(0, self.call('startproject', self.project_name))\n+        p, out, err = self.proc('startproject', self.project_name)\n+        print(out)\n+        print(err, file=sys.stderr)\n+        self.assertEqual(p.returncode, 0)\n \n         assert exists(join(self.proj_path, 'scrapy.cfg'))\n         assert exists(join(self.proj_path, 'testproject'))\n@@ -129,29 +132,22 @@ class StartprojectTest(ProjectTest):\n \n     def test_existing_project_dir(self):\n         project_dir = mkdtemp()\n-        os.mkdir(os.path.join(project_dir, self.project_name))\n+        project_name = self.project_name + '_existing'\n+        project_path = os.path.join(project_dir, project_name)\n+        os.mkdir(project_path)\n \n-        p, out, err = self.proc('startproject', self.project_name, cwd=project_dir)\n+        p, out, err = self.proc('startproject', project_name, cwd=project_dir)\n         print(out)\n         print(err, file=sys.stderr)\n         self.assertEqual(p.returncode, 0)\n \n-        assert exists(join(abspath(project_dir), 'scrapy.cfg'))\n-        assert exists(join(abspath(project_dir), 'testproject'))\n-        assert exists(join(join(abspath(project_dir), self.project_name), '__init__.py'))\n-        assert exists(join(join(abspath(project_dir), self.project_name), 'items.py'))\n-        assert exists(join(join(abspath(project_dir), self.project_name), 'pipelines.py'))\n-        assert exists(join(join(abspath(project_dir), self.project_name), 'settings.py'))\n-        assert exists(join(join(abspath(project_dir), self.project_name), 'spiders', '__init__.py'))\n-\n-        self.assertEqual(0, self.call('startproject', self.project_name, project_dir + '2'))\n-\n-        self.assertEqual(1, self.call('startproject', self.project_name, project_dir))\n-        self.assertEqual(1, self.call('startproject', self.project_name + '2', project_dir))\n-        self.assertEqual(1, self.call('startproject', 'wrong---project---name'))\n-        self.assertEqual(1, self.call('startproject', 'sys'))\n-        self.assertEqual(2, self.call('startproject'))\n-        self.assertEqual(2, self.call('startproject', self.project_name, project_dir, 'another_params'))\n+        assert exists(join(abspath(project_path), 'scrapy.cfg'))\n+        assert exists(join(abspath(project_path), project_name))\n+        assert exists(join(join(abspath(project_path), project_name), '__init__.py'))\n+        assert exists(join(join(abspath(project_path), project_name), 'items.py'))\n+        assert exists(join(join(abspath(project_path), project_name), 'pipelines.py'))\n+        assert exists(join(join(abspath(project_path), project_name), 'settings.py'))\n+        assert exists(join(join(abspath(project_path), project_name), 'spiders', '__init__.py'))\n \n \n def get_permissions_dict(path, renamings=None, ignore=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#aeaeb7385b7d1e6570bad38da4db492de8fb4206", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 56 | Lines Deleted: 19 | Files Changed: 3 | Hunks: 21 | Methods Changed: 13 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 75 | Churn Cumulative: 3269 | Contributors (this commit): 2 | Commits (past 90d): 60 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,10 +12,12 @@ from h2.events import (\n )\n from h2.exceptions import ProtocolError\n from twisted.internet.defer import Deferred\n+from twisted.internet.interfaces import IHandshakeListener, IProtocolNegotiationFactory\n from twisted.internet.protocol import connectionDone, Factory, Protocol\n from twisted.internet.ssl import Certificate\n from twisted.python.failure import Failure\n from twisted.web.client import URI\n+from zope.interface import implementer\n \n from scrapy.core.http2.stream import Stream, StreamCloseReason\n from scrapy.core.http2.types import H2ConnectionMetadataDict\n@@ -25,15 +27,29 @@ from scrapy.settings import Settings\n logger = logging.getLogger(__name__)\n \n \n+class InvalidNegotiatedProtocol(ProtocolError):\n+\n+    def __init__(self, negotiated_protocol: str) -> None:\n+        self.negotiated_protocol = negotiated_protocol\n+\n+    def __str__(self) -> str:\n+        return f'InvalidHostname: Expected h2 as negotiated protocol, received {self.negotiated_protocol}'\n+\n+\n+@implementer(IHandshakeListener)\n class H2ClientProtocol(Protocol):\n-    def __init__(self, uri: URI, settings: Settings) -> None:\n+    def __init__(self, uri: URI, settings: Settings, conn_lost_deferred: Optional[Deferred] = None) -> None:\n         \"\"\"\n         Arguments:\n             uri -- URI of the base url to which HTTP/2 Connection will be made.\n                 uri is used to verify that incoming client requests have correct\n                 base URL.\n             settings -- Scrapy project settings\n+            conn_lost_deferred -- Deferred fires with the reason: Failure to notify\n+                that connection was lost\n         \"\"\"\n+        self._conn_lost_deferred = conn_lost_deferred\n+\n         config = H2Configuration(client_side=True, header_encoding='utf-8')\n         self.conn = H2Connection(config=config)\n \n@@ -55,9 +71,9 @@ class H2ClientProtocol(Protocol):\n         # We use simple FIFO policy to handle pending requests\n         self._active_streams = 0\n \n-        # Save an instance of ProtocolError raised by hyper-h2\n-        # We pass this instance to the streams ResponseFailed() failure\n-        self._protocol_error: Optional[ProtocolError] = None\n+        # Save an instance of errors raised which lead to losing the connection\n+        # We pass these instances to the streams ResponseFailed() failure\n+        self._conn_lost_errors: List[BaseException] = []\n \n         self.metadata: H2ConnectionMetadataDict = {\n             'certificate': None,\n@@ -136,6 +152,12 @@ class H2ClientProtocol(Protocol):\n \n         # Add the stream to the request pool\n         self._pending_request_stream_pool.append(stream)\n+\n+        # If we are connection and receive a request\n+        # There is a good chance that the connection was IDLE\n+        # Hence, we need to initiate pending requests\n+        if self.is_connected:\n+            self._send_pending_requests()\n         return d\n \n     def connectionMade(self) -> None:\n@@ -149,6 +171,19 @@ class H2ClientProtocol(Protocol):\n         self.conn.initiate_connection()\n         self._write_to_transport()\n \n+    def _lose_connection_with_error(self, errors: List[BaseException]):\n+        \"\"\"Helper function to lose the connection with the error sent as a\n+        reason\"\"\"\n+        self._conn_lost_errors += errors\n+        self.transport.loseConnection()\n+\n+    def handshakeCompleted(self):\n+        \"\"\"We close the connection with InvalidNegotiatedProtocol exception\n+        when the connection was not made via h2 protocol\"\"\"\n+        negotiated_protocol = str(self.transport.negotiatedProtocol, 'utf-8')\n+        if negotiated_protocol != 'h2':\n+            self._lose_connection_with_error([InvalidNegotiatedProtocol(negotiated_protocol)])\n+\n     def dataReceived(self, data: bytes) -> None:\n         try:\n             events = self.conn.receive_data(data)\n@@ -157,10 +192,7 @@ class H2ClientProtocol(Protocol):\n             # Save this error as ultimately the connection will be dropped\n             # internally by hyper-h2. Saved error will be passed to all the streams\n             # closed with the connection.\n-            self._protocol_error = e\n-\n-            # We lose the transport connection here\n-            self.transport.loseConnection()\n+            self._lose_connection_with_error([e])\n         finally:\n             self._write_to_transport()\n \n@@ -168,17 +200,17 @@ class H2ClientProtocol(Protocol):\n         \"\"\"Called by Twisted when the transport connection is lost.\n         No need to write anything to transport here.\n         \"\"\"\n-        errors = []\n+        # Notify the connection pool instance such that no new requests are\n+        # sent over current connection\n         if not reason.check(connectionDone):\n-            logger.warning(\"Connection lost with reason \" + str(reason))\n-            errors.append(reason)\n+            self._conn_lost_errors.append(reason)\n \n-        if self._protocol_error:\n-            errors.append(self._protocol_error)\n+        if self._conn_lost_deferred:\n+            self._conn_lost_deferred.callback(self._conn_lost_errors)\n \n         for stream in self.streams.values():\n             if stream.request_sent:\n-                stream.close(StreamCloseReason.CONNECTION_LOST, errors, from_protocol=True)\n+                stream.close(StreamCloseReason.CONNECTION_LOST, self._conn_lost_errors, from_protocol=True)\n             else:\n                 stream.close(StreamCloseReason.INACTIVE, from_protocol=True)\n \n@@ -242,10 +274,15 @@ class H2ClientProtocol(Protocol):\n                 stream.receive_window_update()\n \n \n+@implementer(IProtocolNegotiationFactory)\n class H2ClientFactory(Factory):\n-    def __init__(self, uri: URI, settings: Settings) -> None:\n+    def __init__(self, uri: URI, settings: Settings, conn_lost_deferred: Optional[Deferred] = None) -> None:\n         self.uri = uri\n         self.settings = settings\n+        self.conn_lost_deferred = conn_lost_deferred\n \n     def buildProtocol(self, addr) -> H2ClientProtocol:\n-        return H2ClientProtocol(self.uri, self.settings)\n+        return H2ClientProtocol(self.uri, self.settings, self.conn_lost_deferred)\n+\n+    def acceptableProtocols(self) -> List[bytes]:\n+        return [b'h2']\n\n@@ -44,7 +44,7 @@ class InvalidHostname(Exception):\n \n \n class StreamCloseReason(Enum):\n-    # Received a StreamEnded event\n+    # Received a StreamEnded event from the remote\n     ENDED = 1\n \n     # Received a StreamReset event -- ended abruptly\n@@ -329,7 +329,7 @@ class Stream:\n     def close(\n         self,\n         reason: StreamCloseReason,\n-        errors: Optional[List[Exception]] = None,\n+        errors: Optional[List[BaseException]] = None,\n         from_protocol: bool = False\n     ) -> None:\n         \"\"\"Based on the reason sent we will handle each case.\n\n@@ -9,7 +9,7 @@ from urllib.parse import urlencode\n \n from h2.exceptions import InvalidBodyLengthError\n from twisted.internet import reactor\n-from twisted.internet.defer import inlineCallbacks, DeferredList, CancelledError\n+from twisted.internet.defer import CancelledError, DeferredList, inlineCallbacks\n from twisted.internet.endpoints import SSL4ClientEndpoint, SSL4ServerEndpoint\n from twisted.internet.ssl import optionsForClientTLS, PrivateCertificate, Certificate\n from twisted.python.failure import Failure\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1dd27a92fa53be87c71df43bd6f3043a225a2c10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 59 | Lines Deleted: 12 | Files Changed: 3 | Hunks: 22 | Methods Changed: 15 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 71 | Churn Cumulative: 3340 | Contributors (this commit): 2 | Commits (past 90d): 63 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,15 +6,18 @@ from typing import Dict, List, Optional\n \n from h2.config import H2Configuration\n from h2.connection import H2Connection\n+from h2.errors import ErrorCodes\n from h2.events import (\n     Event, DataReceived, ResponseReceived, SettingsAcknowledged,\n     StreamEnded, StreamReset, WindowUpdated\n )\n from h2.exceptions import ProtocolError\n from twisted.internet.defer import Deferred\n+from twisted.internet.error import TimeoutError\n from twisted.internet.interfaces import IHandshakeListener, IProtocolNegotiationFactory\n from twisted.internet.protocol import connectionDone, Factory, Protocol\n from twisted.internet.ssl import Certificate\n+from twisted.protocols.policies import TimeoutMixin\n from twisted.python.failure import Failure\n from twisted.web.client import URI\n from zope.interface import implementer\n@@ -37,7 +40,9 @@ class InvalidNegotiatedProtocol(ProtocolError):\n \n \n @implementer(IHandshakeListener)\n-class H2ClientProtocol(Protocol):\n+class H2ClientProtocol(Protocol, TimeoutMixin):\n+    IDLE_TIMEOUT = 240\n+\n     def __init__(self, uri: URI, settings: Settings, conn_lost_deferred: Optional[Deferred] = None) -> None:\n         \"\"\"\n         Arguments:\n@@ -71,6 +76,10 @@ class H2ClientProtocol(Protocol):\n         # We use simple FIFO policy to handle pending requests\n         self._active_streams = 0\n \n+        # Flag to keep track if settings were acknowledged by the remote\n+        # This ensures that we have established a HTTP/2 connection\n+        self._settings_acknowledged = False\n+\n         # Save an instance of errors raised which lead to losing the connection\n         # We pass these instances to the streams ResponseFailed() failure\n         self._conn_lost_errors: List[BaseException] = []\n@@ -84,12 +93,12 @@ class H2ClientProtocol(Protocol):\n         }\n \n     @property\n-    def is_connected(self) -> bool:\n+    def h2_connected(self) -> bool:\n         \"\"\"Boolean to keep track of the connection status.\n         This is used while initiating pending streams to make sure\n         that we initiate stream only during active HTTP/2 Connection\n         \"\"\"\n-        return bool(self.transport.connected)\n+        return bool(self.transport.connected) and self._settings_acknowledged\n \n     @property\n     def allowed_max_concurrent_streams(self) -> int:\n@@ -111,7 +120,7 @@ class H2ClientProtocol(Protocol):\n         while (\n             self._pending_request_stream_pool\n             and self._active_streams < self.allowed_max_concurrent_streams\n-            and self.is_connected\n+            and self.h2_connected\n         ):\n             self._active_streams += 1\n             stream = self._pending_request_stream_pool.popleft()\n@@ -140,6 +149,9 @@ class H2ClientProtocol(Protocol):\n         \"\"\" Write data to the underlying transport connection\n         from the HTTP2 connection instance if any\n         \"\"\"\n+        # Reset the idle timeout as connection is still actively sending data\n+        self.resetTimeout()\n+\n         data = self.conn.data_to_send()\n         self.transport.write(data)\n \n@@ -153,10 +165,8 @@ class H2ClientProtocol(Protocol):\n         # Add the stream to the request pool\n         self._pending_request_stream_pool.append(stream)\n \n-        # If we are connection and receive a request\n-        # There is a good chance that the connection was IDLE\n-        # Hence, we need to initiate pending requests\n-        if self.is_connected:\n+        # If we receive a request when connection is idle\n+        # We need to initiate pending requests\n         self._send_pending_requests()\n         return d\n \n@@ -164,10 +174,14 @@ class H2ClientProtocol(Protocol):\n         \"\"\"Called by Twisted when the connection is established. We can start\n         sending some data now: we should open with the connection preamble.\n         \"\"\"\n+        # Initialize the timeout\n+        self.setTimeout(self.IDLE_TIMEOUT)\n+\n         destination = self.transport.getPeer()\n         logger.debug('Connection made to {}'.format(destination))\n         self.metadata['ip_address'] = ipaddress.ip_address(destination.host)\n \n+        # Initiate H2 Connection\n         self.conn.initiate_connection()\n         self._write_to_transport()\n \n@@ -185,6 +199,9 @@ class H2ClientProtocol(Protocol):\n             self._lose_connection_with_error([InvalidNegotiatedProtocol(negotiated_protocol)])\n \n     def dataReceived(self, data: bytes) -> None:\n+        # Reset the idle timeout as connection is still actively receiving data\n+        self.resetTimeout()\n+\n         try:\n             events = self.conn.receive_data(data)\n             self._handle_events(events)\n@@ -196,10 +213,33 @@ class H2ClientProtocol(Protocol):\n         finally:\n             self._write_to_transport()\n \n+    def timeoutConnection(self):\n+        \"\"\"Called when the connection times out.\n+        We lose the connection with TimeoutError\"\"\"\n+\n+        # Check whether there are open streams. If there are, we're going to\n+        # want to use the error code PROTOCOL_ERROR. If there aren't, use\n+        # NO_ERROR.\n+        if (\n+            self.conn.open_outbound_streams > 0\n+            or self.conn.open_inbound_streams > 0\n+            or self._active_streams > 0\n+        ):\n+            error_code = ErrorCodes.PROTOCOL_ERROR\n+        else:\n+            error_code = ErrorCodes.NO_ERROR\n+        self.conn.close_connection(error_code=error_code)\n+        self._write_to_transport()\n+\n+        self._lose_connection_with_error([TimeoutError(\"Hello\")])\n+\n     def connectionLost(self, reason: Failure = connectionDone) -> None:\n         \"\"\"Called by Twisted when the transport connection is lost.\n         No need to write anything to transport here.\n         \"\"\"\n+        # Cancel the timeout if not done yet\n+        self.setTimeout(None)\n+\n         # Notify the connection pool instance such that no new requests are\n         # sent over current connection\n         if not reason.check(connectionDone):\n@@ -250,6 +290,8 @@ class H2ClientProtocol(Protocol):\n         self.streams[event.stream_id].receive_headers(event.headers)\n \n     def settings_acknowledged(self, event: SettingsAcknowledged) -> None:\n+        self._settings_acknowledged = True\n+\n         # Send off all the pending requests as now we have\n         # established a proper HTTP/2 connection\n         self._send_pending_requests()\n\n@@ -134,7 +134,8 @@ class Stream:\n \n         def _cancel(_):\n             # Close this stream as gracefully as possible\n-            # Check if the stream has started\n+            # If the associated request is initiated we reset this stream\n+            # else we directly call close() method\n             if self.request_sent:\n                 self.reset_stream(StreamCloseReason.CANCELLED)\n             else:\n\n@@ -9,7 +9,7 @@ from urllib.parse import urlencode\n \n from h2.exceptions import InvalidBodyLengthError\n from twisted.internet import reactor\n-from twisted.internet.defer import CancelledError, DeferredList, inlineCallbacks\n+from twisted.internet.defer import CancelledError, Deferred, DeferredList, inlineCallbacks\n from twisted.internet.endpoints import SSL4ClientEndpoint, SSL4ServerEndpoint\n from twisted.internet.ssl import optionsForClientTLS, PrivateCertificate, Certificate\n from twisted.python.failure import Failure\n@@ -174,6 +174,7 @@ class Https2ClientProtocolTestCase(TestCase):\n         # Start server for testing\n         self.hostname = u'localhost'\n         context_factory = ssl_context_factory(self.key_file, self.certificate_file)\n+\n         server_endpoint = SSL4ServerEndpoint(reactor, 0, context_factory, interface=self.hostname)\n         self.server = yield server_endpoint.listen(self.site)\n         self.port_number = self.server.getHost().port\n@@ -186,17 +187,20 @@ class Https2ClientProtocolTestCase(TestCase):\n             acceptableProtocols=[b'h2']\n         )\n         uri = URI.fromBytes(bytes(self.get_url('/'), 'utf-8'))\n-        h2_client_factory = H2ClientFactory(uri, Settings())\n+\n+        self.conn_closed_deferred = Deferred()\n+        h2_client_factory = H2ClientFactory(uri, Settings(), self.conn_closed_deferred)\n         client_endpoint = SSL4ClientEndpoint(reactor, self.hostname, self.port_number, client_options)\n         self.client = yield client_endpoint.connect(h2_client_factory)\n \n     @inlineCallbacks\n     def tearDown(self):\n-        if self.client.is_connected:\n+        if self.client.connected:\n             yield self.client.transport.loseConnection()\n             yield self.client.transport.abortConnection()\n         yield self.server.stopListening()\n         shutil.rmtree(self.temp_directory)\n+        self.conn_closed_deferred = None\n \n     def get_url(self, path):\n         \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
