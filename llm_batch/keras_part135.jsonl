{"custom_id": "keras#d811048887134013d3a0bcd9898f02ac3b4688b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 729 | Contributors (this commit): 4 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -23,7 +23,10 @@ example output\n from __future__ import print_function\n \n from collections import defaultdict\n+try:\n     import cPickle as pickle\n+except ImportError:\n+    import pickle\n from PIL import Image\n \n from six.moves import range\n\n@@ -55,6 +55,7 @@ Results\n '''\n \n from __future__ import print_function\n+from six.moves import xrange\n import numpy as np\n np.random.seed(1337)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b40b8a00e40db8765a06ad4cd5f1274bab93543e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 1395 | Contributors (this commit): 21 | Commits (past 90d): 8 | Contributors (cumulative): 21 | DMM Complexity: 0.0\n\nDIFF:\n@@ -549,7 +549,10 @@ class NumpyArrayIterator(Iterator):\n                              'either 1, 3 or 4 channels on axis ' + str(channels_axis) + '. '\n                              'However, it was passed an array with shape ' + str(self.X.shape) +\n                              ' (' + str(self.X.shape[channels_axis]) + ' channels).')\n+        if y is not None:\n             self.y = np.asarray(y)\n+        else:\n+            self.y = None\n         self.image_data_generator = image_data_generator\n         self.dim_ordering = dim_ordering\n         self.save_to_dir = save_to_dir\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5e75b8506c25f384311722f35b5dbbc3dd85f3fe", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 5 | Churn Cumulative: 3080 | Contributors (this commit): 46 | Commits (past 90d): 21 | Contributors (cumulative): 46 | DMM Complexity: 1.0\n\nDIFF:\n@@ -864,6 +864,11 @@ def print_tensor(x, message=''):\n class Function(object):\n \n     def __init__(self, inputs, outputs, updates=[], **kwargs):\n+        unique_variables_to_update = {}\n+        for v, nv in updates:\n+            if v not in unique_variables_to_update:\n+                unique_variables_to_update[v] = nv\n+        updates = unique_variables_to_update.items()\n         self.function = theano.function(inputs, outputs, updates=updates,\n                                         allow_input_downcast=True,\n                                         on_unused_input='ignore',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4b1b706aa4effbf7c2886755ab99f5b75377762b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 5576 | Contributors (this commit): 30 | Commits (past 90d): 15 | Contributors (cumulative): 30 | DMM Complexity: None\n\nDIFF:\n@@ -1750,7 +1750,7 @@ class Cropping2D(Layer):\n \n \n class Cropping3D(Layer):\n-    '''Cropping layer for 3D data (e.g. spatial or saptio-temporal).\n+    '''Cropping layer for 3D data (e.g. spatial or spatio-temporal).\n \n     # Arguments\n         cropping: tuple of tuple of int (length 3)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fa9f863dbf92b037e18de1e57984bf5f0510589d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 503 | Lines Deleted: 43 | Files Changed: 1 | Hunks: 40 | Methods Changed: 22 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 546 | Churn Cumulative: 4078 | Contributors (this commit): 47 | Commits (past 90d): 27 | Contributors (cumulative): 47 | DMM Complexity: None\n\nDIFF:\n@@ -93,6 +93,10 @@ def get_session():\n \n     Note that you can manually set the global session\n     via `K.set_session(sess)`.\n+\n+    # Returns\n+        A TensorFlow session.\n+\n     '''\n     global _SESSION\n     if tf.get_default_session() is not None:\n@@ -150,10 +154,48 @@ def _to_tensor(x, dtype):\n \n \n def is_sparse(tensor):\n+    '''Returns whether a tensor is a sparse tensor.\n+\n+    # Arguments\n+        tensor: a Keras tensor.\n+\n+    # Returns\n+        Boolean.\n+\n+    # Example\n+    ```python\n+        >>> a = keras.backend.placeholder((2, 2), sparse=False)\n+        >>> print(keras.backend.is_sparse(a))\n+        False\n+        >>> b = keras.backend.placeholder((2, 2), sparse=True)\n+        >>> print(keras.backend.is_sparse(b))\n+        True\n+    ```\n+\n+    '''\n     return isinstance(tensor, tf.SparseTensor)\n \n \n def to_dense(tensor):\n+    '''Converts a Keras tensor into a Keras dense tensor\n+    and returns it.\n+\n+    # Arguments\n+        tensor: Keras tensor.\n+\n+    # Returns\n+        A Keras dense tensor.\n+\n+    # Examples\n+    ```python\n+        >>> b = keras.backend.placeholder((2, 2), sparse=True)\n+        >>> print(keras.backend.is_sparse(b))\n+        True\n+        >>> c = keras.backend.to_dense(b)\n+        >>> print(keras.backend.is_sparse(c))\n+        False\n+    ```\n+    '''\n     if is_sparse(tensor):\n         return tf.sparse_tensor_to_dense(tensor)\n     else:\n@@ -161,15 +203,31 @@ def to_dense(tensor):\n \n \n def variable(value, dtype=_FLOATX, name=None):\n-    '''Instantiates a tensor.\n+    '''Instantiates a tensor and returns it.\n \n     # Arguments\n         value: numpy array, initial value of the tensor.\n+\n         dtype: tensor type.\n+\n         name: optional name string for the tensor.\n \n     # Returns\n-        Tensor variable instance.\n+        A Keras variable instance.\n+\n+    # Examples\n+    ```python\n+        >>> from keras import backend as K\n+        >>> val = np.array([[1, 2], [3, 4]])\n+        >>> kvar = K.variable(value=val, dtype='float64', name='example_kvar')\n+        >>> K.dtype(kvar)\n+        'float64'\n+        >>> print(kvar)\n+        example_kvar\n+        >>> kvar.eval()\n+        array([[ 1.,  2.],\n+               [ 3.,  4.]])\n+    ```\n     '''\n     if hasattr(value, 'tocoo'):\n         sparse_coo = value.tocoo()\n@@ -205,19 +263,31 @@ def _initialize_variables():\n \n \n def placeholder(shape=None, ndim=None, dtype=_FLOATX, sparse=False, name=None):\n-    '''Instantiates a placeholder.\n+    '''Instantiates a placeholder (Keras tensor) and returns it.\n \n     # Arguments\n         shape: shape of the placeholder\n-            (integer tuple, may include None entries).\n+            (integer tuple, may include `None` entries).\n+\n         ndim: number of axes of the tensor.\n             At least one of {`shape`, `ndim`} must be specified.\n             If both are specified, `shape` is used.\n+\n         dtype: placeholder type.\n+\n         name: optional name string for the placeholder.\n \n     # Returns\n-        Placeholder tensor instance.\n+        Keras tensor instance.\n+\n+    # Examples\n+    ```python\n+        >>> input = keras.backend.placeholder(shape=(2, 4, 5))\n+        >>> input._keras_shape\n+        (2, 4, 5)\n+        >>> input\n+        <tf.Tensor 'Placeholder_4:0' shape=(2, 4, 5) dtype=float32>\n+    ```\n     '''\n     if not shape:\n         if ndim:\n@@ -233,15 +303,58 @@ def placeholder(shape=None, ndim=None, dtype=_FLOATX, sparse=False, name=None):\n \n \n def shape(x):\n-    '''Returns the symbolic shape of a tensor.\n+    '''Returns the symbolic shape of a Keras tensor or variable.\n+\n+    # Arguments\n+        x: A Keras tensor or variable.\n+\n+    # Returns\n+        A symbolic shape.\n+\n+    # Examples\n+    ```\n+        # TensorFlow example\n+        >>> from keras import backend as K\n+        >>> tf_session = K.get_session()\n+        >>> val = np.array([[1, 2], [3, 4]])\n+        >>> kvar = K.variable(value=val)\n+        >>> input = keras.backend.placeholder(shape=(2, 4, 5))\n+        >>> K.shape(kvar)\n+        <tf.Tensor 'Shape_8:0' shape=(2,) dtype=int32>\n+        >>> K.shape(input)\n+        <tf.Tensor 'Shape_9:0' shape=(3,) dtype=int32>\n+        # To get integer shape (Instead, you can use K.int_shape(x))\n+        >>> K.shape(kvar).eval(session=tf_session)\n+        array([2, 2], dtype=int32)\n+        >>> K.shape(input).eval(session=tf_session)\n+        array([2, 4, 5], dtype=int32)\n+    ```\n     '''\n     return tf.shape(x)\n \n \n def int_shape(x):\n-    '''Returns the shape of a tensor as a tuple of\n+    '''Returns the shape of a Keras tensor or a Keras variable as a tuple of\n     integers or None entries.\n     Note that this function only works with TensorFlow.\n+\n+    # Arguments\n+        x: A Keras tensor or variable.\n+\n+    # Returns\n+        A tuple of integers.\n+\n+    # Examples\n+    ```python\n+        >>> from keras import backend as K\n+        >>> input = K.placeholder(shape=(2, 4, 5))\n+        >>> K.int_shape(input)\n+        (2, 4, 5)\n+        >>> val = np.array([[1, 2], [3, 4]])\n+        >>> kvar = K.variable(value=val)\n+        >>> K.int_shape(kvar)\n+        (2, 2)\n+    ```\n     '''\n     shape = x.get_shape()\n     return tuple([i.__int__() for i in shape])\n@@ -249,6 +362,24 @@ def int_shape(x):\n \n def ndim(x):\n     '''Returns the number of axes in a tensor, as an integer.\n+\n+    # Arguments\n+        x: Keras tensor or variable.\n+\n+    # Returns\n+        integer (scalar), number of dimensions.\n+\n+    # Examples\n+    ```python\n+        >>> from keras import backend as K\n+        >>> input = K.placeholder(shape=(2, 4, 5))\n+        >>> val = np.array([[1, 2], [3, 4]])\n+        >>> kvar = K.variable(value=val)\n+        >>> K.ndim(input)\n+        3\n+        >>> K.ndim(kvar)\n+        2\n+    ```\n     '''\n     if is_sparse(x):\n         return x._dims\n@@ -260,20 +391,80 @@ def ndim(x):\n \n \n def dtype(x):\n-    '''Returns the dtype of a tensor, as a string.\n+    '''Returns the dtype of a Keras tensor or variable, as a string.\n+\n+    # Arguments\n+        x: Keras tensor or variable.\n+\n+    # Returns\n+        String, dtype of `x`.\n+\n+    # Examples\n+    ```python\n+        >>> from keras import backend as K\n+        # Keras tensor\n+        >>> K.dtype(K.placeholder(shape=(2,4,5)))\n+        'float32'\n+        >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float32'))\n+        'float32'\n+        >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float64'))\n+        'float64'\n+        # Keras variable\n+        >>> kvar = K.variable(np.array([[1, 2], [3, 4]]))\n+        >>> K.dtype(kvar)\n+        'float32_ref'\n+        >>> kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')\n+        >>> K.dtype(kvar)\n+        'float32_ref'\n+    ```\n     '''\n     return x.dtype.name\n \n \n def eval(x):\n-    '''Evaluates the value of a tensor.\n+    '''Evaluates the value of a Keras variable.\n     Returns a Numpy array.\n+\n+    # Arguments\n+        x: A Keras variable.\n+\n+    # Returns\n+        A Numpy array.\n+\n+    # Examples\n+    ```python\n+        >>> from keras import backend as K\n+        >>> kvar = K.variable(np.array([[1, 2], [3, 4]]), dtype='float32')\n+        >>> K.eval(kvar)\n+        array([[ 1.,  2.],\n+               [ 3.,  4.]], dtype=float32)\n+    ```\n     '''\n     return to_dense(x).eval(session=get_session())\n \n \n def zeros(shape, dtype=_FLOATX, name=None):\n-    '''Instantiates an all-zeros tensor variable.\n+    '''Instantiates an all-zeros Keras variable and returns it.\n+\n+    # Arguments\n+        shape: Tuple of integers, shape of returned Keras variable\n+\n+        dtype: String, data type of returned Keras variable\n+\n+        name: String, name of returned Keras variable\n+\n+    # Returns\n+        A Keras variable, filled with `0.0`\n+\n+    # Example\n+    ```python\n+        >>> from keras import backend as K\n+        >>> kvar = K.zeros((3,4))\n+        >>> K.eval(kvar)\n+        array([[ 0.,  0.,  0.,  0.],\n+               [ 0.,  0.,  0.,  0.],\n+               [ 0.,  0.,  0.,  0.]], dtype=float32)\n+    ```\n     '''\n     shape = tuple(map(int, shape))\n     tf_dtype = _convert_string_dtype(dtype)\n@@ -282,7 +473,27 @@ def zeros(shape, dtype=_FLOATX, name=None):\n \n \n def ones(shape, dtype=_FLOATX, name=None):\n-    '''Instantiates an all-ones tensor variable.\n+    '''Instantiates an all-ones tensor variable and returns it.\n+\n+    # Arguments\n+        shape: Tuple of integers, shape of returned Keras variable.\n+\n+        dtype: String, data type of returned Keras variable.\n+\n+        name: String, name of returned Keras variable.\n+\n+    # Returns\n+        A Keras variable, filled with `1.0`.\n+\n+    # Example\n+    ```python\n+        >>> from keras import backend as K\n+        >>> kvar = K.ones((3,4))\n+        >>> K.eval(kvar)\n+        array([[ 1.,  1.,  1.,  1.],\n+               [ 1.,  1.,  1.,  1.],\n+               [ 1.,  1.,  1.,  1.]], dtype=float32)\n+    ```\n     '''\n     shape = tuple(map(int, shape))\n     tf_dtype = _convert_string_dtype(dtype)\n@@ -291,27 +502,110 @@ def ones(shape, dtype=_FLOATX, name=None):\n \n \n def eye(size, dtype=_FLOATX, name=None):\n-    '''Instantiate an identity matrix.\n+    '''Instantiate an identity matrix and returns it.\n+\n+    # Arguments\n+        size: Integer, number of rows/columns.\n+\n+        dtype: String, data type of returned Keras variable.\n+\n+        name: String, name of returned Keras variable.\n+\n+    # Returns\n+        A Keras variable, an identity matrix.\n+\n+    # Example\n+    ```python\n+        >>> from keras import backend as K\n+        >>> kvar = K.eye(3)\n+        >>> K.eval(kvar)\n+        array([[ 1.,  0.,  0.],\n+               [ 0.,  1.,  0.],\n+               [ 0.,  0.,  1.]], dtype=float32)\n+    ```\n+\n     '''\n     return variable(np.eye(size), dtype, name)\n \n \n def zeros_like(x, name=None):\n-    '''Instantiates an all-zeros tensor\n-    of the same shape as another tensor.\n+    '''Instantiates an all-zeros Keras variable\n+    of the same shape as another Keras variable or tensor and returns it.\n+\n+    # Arguments\n+        x: Keras variable or Keras tensor.\n+\n+    # Returns\n+        A Keras variable, filled with `0.0`.\n+\n+    # Example\n+    ```python\n+        >>> from keras import backend as K\n+        >>> kvar = K.variable(np.random.random((2,3)))\n+        >>> kvar_zeros = K.zeros_like(kvar)\n+        >>> K.eval(kvar_zeros)\n+        array([[ 0.,  0.,  0.],\n+               [ 0.,  0.,  0.]], dtype=float32)\n+    ```\n     '''\n     return tf.zeros_like(x, name=name)\n \n \n def ones_like(x, name=None):\n-    '''Instantiates an all-ones tensor\n-    of the same shape as another tensor.\n+    '''Instantiates an all-ones Keras variable\n+    of the same shape as another Keras variable or tensor and returns it.\n+\n+    # Arguments\n+        x: Keras variable or tensor.\n+\n+    # Returns\n+        A Keras variable, filled with `1.0`.\n+\n+    # Example\n+    ```python\n+        >>> from keras import backend as K\n+        >>> kvar = K.variable(np.random.random((2,3)))\n+        >>> kvar_ones = K.ones_like(kvar)\n+        >>> K.eval(kvar_ones)\n+        array([[ 1.,  1.,  1.],\n+               [ 1.,  1.,  1.]], dtype=float32)\n+    ```\n     '''\n     return tf.ones_like(x, name=name)\n \n \n def random_uniform_variable(shape, low, high, dtype=_FLOATX,\n                             name=None, seed=None):\n+    '''Instantiates an Keras variable filled with\n+    samples drawn from a uniform distribution and returns it.\n+\n+    # Arguments\n+        shape: Tuple of integers, shape of returned Keras variable.\n+\n+        low: Float, lower boundary of the output inteval.\n+\n+        high: Float, upper boundary of the output interval.\n+\n+        dtype: String, dtype of returned Keras variable.\n+\n+        name: String, name of returned Keras variable.\n+\n+        seed: Integer, random seed.\n+\n+    # Returns\n+        A Keras variable, filled with drawn samples.\n+\n+    # Example\n+    ```python\n+        # TensorFlow example\n+        >>> kvar = K.random_uniform_variable((2,3), 0, 1)\n+        >>> kvar\n+        <tensorflow.python.ops.variables.Variable object at 0x10ab40b10>\n+        >>> K.eval(kvar)\n+        array([[ 0.10940075,  0.10047495,  0.476143  ],\n+               [ 0.66137183,  0.00869417,  0.89220798]], dtype=float32)\n+    ```\n+    '''\n     shape = tuple(map(int, shape))\n     tf_dtype = _convert_string_dtype(dtype)\n     if seed is None:\n@@ -324,6 +618,36 @@ def random_uniform_variable(shape, low, high, dtype=_FLOATX,\n \n def random_normal_variable(shape, mean, scale, dtype=_FLOATX,\n                            name=None, seed=None):\n+    '''Instantiates an Keras variable filled with\n+    samples drawn from a normal distribution and returns it.\n+\n+    # Arguments\n+        shape: Tuple of integers, shape of returned Keras variable.\n+\n+        mean: Float, mean of the normal distribution.\n+\n+        scale: Float, standard deviation of the normal distribution.\n+\n+        dtype: String, dtype of returned Keras variable.\n+\n+        name: String, name of returned Keras variable.\n+\n+        seed: Integer, random seed.\n+\n+    # Returns\n+        A Keras variable, filled with drawn samples.\n+\n+    # Example\n+    ```python\n+        # TensorFlow example\n+        >>> kvar = K.random_normal_variable((2,3), 0, 1)\n+        >>> kvar\n+        <tensorflow.python.ops.variables.Variable object at 0x10ab12dd0>\n+        >>> K.eval(kvar)\n+        array([[ 1.19591331,  0.68685907, -0.63814116],\n+               [ 0.92629528,  0.28055015,  1.70484698]], dtype=float32)\n+    ```\n+    '''\n     shape = tuple(map(int, shape))\n     tf_dtype = _convert_string_dtype(dtype)\n     if seed is None:\n@@ -335,14 +659,55 @@ def random_normal_variable(shape, mean, scale, dtype=_FLOATX,\n \n \n def count_params(x):\n-    '''Returns the number of scalars in a tensor.\n+    '''Returns the number of scalars in a Keras variable.\n+\n+    # Arguments\n+        x: Keras variable.\n+\n+    # Returns\n+        Integer, the number of scalars in `x`.\n+\n+    # Example\n+    ```python\n+        >>> kvar = K.zeros((2,3))\n+        >>> K.count_params(kvar)\n+        6\n+        >>> K.eval(kvar)\n+        array([[ 0.,  0.,  0.],\n+               [ 0.,  0.,  0.]], dtype=float32)\n+    ```\n     '''\n     shape = x.get_shape()\n     return np.prod([shape[i]._value for i in range(len(shape))])\n \n \n def cast(x, dtype):\n-    '''Casts a tensor to a different dtype.\n+    '''Casts a Keras tensor to a tensor of a different dtype and returns it.\n+\n+    You can cast a Keras variable but it still returns a Keras tensor.\n+\n+    # Arguments\n+        x: Keras tensor (or variable).\n+\n+        dtype: String, either (`'float16'`, `'float32'`, or `'float64'`).\n+\n+    # Returns\n+        Keras tensor with dtype of `dtype`.\n+\n+    # Example\n+    ```python\n+        >>> input = K.placeholder((2, 3), dtype='float32')\n+        >>> input\n+        <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32>\n+        # It doesn't work in-place as below.\n+        >>> K.cast(input, dtype='float16')\n+        <tf.Tensor 'Cast_1:0' shape=(2, 3) dtype=float16>\n+        >>> input\n+        <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32>\n+        # you need to assign it.\n+        >>> input = K.cast(input, dtype='float16')\n+        >>> input\n+        <tf.Tensor 'Cast_2:0' shape=(2, 3) dtype=float16>    ```\n     '''\n     return tf.cast(x, dtype)\n \n@@ -374,10 +739,59 @@ def moving_average_update(variable, value, momentum):\n # LINEAR ALGEBRA\n \n def dot(x, y):\n-    '''Multiplies 2 tensors.\n+    '''Multiplies 2 tensors (and/or variables) and returns a *tensor*.\n     When attempting to multiply a ND tensor\n-    with a ND tensor, reproduces the Theano behavior\n+    with a ND tensor, it reproduces the Theano behavior.\n     (e.g. (2, 3).(4, 3, 5) = (2, 4, 5))\n+\n+    Like `K.cast()`, it can take Keras variables as inputs but the\n+        returned object is a Keras tensor.\n+\n+    # Arguments\n+        x: Keras tensor (or variable).\n+\n+        y: Keras tensor (or variable).\n+\n+    # Returns\n+        A Keras tensor, dot product of `x` and `y`.\n+\n+    # Examples\n+    ```python\n+        # dot product between Keras tensors\n+        >>> x = K.placeholder(shape=(2, 3))\n+        >>> y = K.placeholder(shape=(3, 4))\n+        >>> xy = K.dot(x, y)\n+        >>> xy\n+        <tf.Tensor 'MatMul_9:0' shape=(2, 4) dtype=float32>\n+    ```\n+\n+    ```python\n+        # dot product between Keras tensors\n+        >>> x = K.placeholder(shape=(32, 28, 3))\n+        >>> y = K.placeholder(shape=(3, 4))\n+        >>> xy = K.dot(x, y)\n+        >>> xy\n+        <tf.Tensor 'MatMul_9:0' shape=(2, 4) dtype=float32>\n+    ```\n+\n+    ```python\n+        # dot product between Keras tensor and Keras variable\n+        >>> input = K.placeholder((2, 3))\n+        >>> y = K.random_uniform_variable(shape=(3, 4), low=0, high=1)\n+        >>> xy = K.dot(x, y)\n+        # It returns a Keras tensor\n+        >>> xy\n+        <tf.Tensor 'MatMul_1:0' shape=(2, 4) dtype=float32>\n+    ```\n+\n+    ```python\n+        # The Theano behavior example\n+        >>> x = K.random_uniform_variable(shape=(2, 3), low=0, high=1)\n+        >>> y = K.ones((4, 3, 5))\n+        >>> xy = K.dot(x, y)\n+        >>> K.int_shape(xy)\n+        (2, 4, 5)\n+    ```\n     '''\n     if ndim(x) is not None and (ndim(x) > 2 or ndim(y) > 2):\n         x_shape = (-1,) + int_shape(x)[1:]\n@@ -398,40 +812,57 @@ def dot(x, y):\n def batch_dot(x, y, axes=None):\n     '''Batchwise dot product.\n \n-    batch_dot results in a tensor with less dimensions than the input.\n-    If the number of dimensions is reduced to 1, we use `expand_dims` to\n-    make sure that ndim is at least 2.\n+    `batch_dot` is used to compute dot product of `x` and `y` when\n+    `x` and `y` are data in batch, i.e. in a shape of\n+    `(batch_size, :)`.\n+    `batch_dot` results in a tensor or variable with less dimensions\n+    than the input. If the number of dimensions is reduced to 1,\n+    we use `expand_dims` to make sure that ndim is at least 2.\n+\n+    With TensorFlow backend, `batch_dot()` only supports `ndim >= 3` inputs.\n \n     # Arguments\n-        x, y: tensors with ndim >= 2\n-        axes: list (or single) int with target dimensions\n+        x, y: Keras tensors or variables with `ndim >= 2`\n+            (With TensorFlow backend, `batch_dot()` only supports `ndim >= 3`)\n+\n+        axes: list of (or single) int with target dimensions.\n+            The lengths of `axes[0]` and `axes[1]` should be the same.\n \n     # Returns\n-        A tensor with shape equal to the concatenation of x's shape\n-        (less the dimension that was summed over) and y's shape\n+        A tensor with shape equal to the concatenation of `x`'s shape\n+        (less the dimension that was summed over) and `y`'s shape\n         (less the batch dimension and the dimension that was summed over).\n-        If the final rank is 1, we reshape it to (batch_size, 1).\n+\n+        If the final rank is 1, we reshape it to `(batch_size, 1)`.\n \n     # Examples\n-        Assume x = [[1, 2], [3, 4]]   and y = [[5, 6], [7, 8]]\n-        batch_dot(x, y, axes=1) = [[17, 53]] which is the main diagonal\n-        of x.dot(y.T), although we never have to calculate the off-diagonal\n+        Assume `x = [[1, 2], [3, 4]]` and `y = [[5, 6], [7, 8]]`\n+        `batch_dot(x, y, axes=1) = [[17, 53]]` which is the main diagonal\n+        of `x.dot(y.T)`, although we never have to calculate the off-diagonal\n         elements.\n \n         Shape inference:\n-        Let x's shape be (100, 20) and y's shape be (100, 30, 20).\n-        If dot_axes is (1, 2), to find the output shape of resultant tensor,\n+        Let `x`'s shape be `(100, 20)` and `y`'s shape be `(100, 30, 20)`.\n+        If `axes` is (1, 2), to find the output shape of resultant tensor,\n             loop through each dimension in x's shape and y's shape:\n-        x.shape[0] : 100 : append to output shape\n-        x.shape[1] : 20 : do not append to output shape,\n-            dimension 1 of x has been summed over. (dot_axes[0] = 1)\n-        y.shape[0] : 100 : do not append to output shape,\n-            always ignore first dimension of y\n-        y.shape[1] : 30 : append to output shape\n-        y.shape[2] : 20 : do not append to output shape,\n-            dimension 2 of y has been summed over. (dot_axes[1] = 2)\n \n-        output_shape = (100, 30)\n+        * `x.shape[0]` : 100 : append to output shape\n+        * `x.shape[1]` : 20 : do not append to output shape,\n+            dimension 1 of x has been summed over. (`dot_axes[0]` = 1)\n+        * `y.shape[0]` : 100 : do not append to output shape,\n+            always ignore first dimension of y\n+        * `y.shape[1]` : 30 : append to output shape\n+        * `y.shape[2]` : 20 : do not append to output shape,\n+            dimension 2 of y has been summed over. (`dot_axes[1]` = 2)\n+        output_shape = `(100, 30)`\n+\n+    ```python\n+        >>> x_batch = K.ones(shape=(32, 20, 1))\n+        >>> y_batch = K.ones(shape=(32, 30, 20))\n+        >>> xy_batch_dot = K.batch_dot(x_batch, y_batch, axes=[1, 2])\n+        >>> K.int_shape(xy_batch_dot)\n+        (32, 1, 30)\n+    ```\n     '''\n     if isinstance(axes, int):\n         axes = (axes, axes)\n@@ -448,7 +879,36 @@ def batch_dot(x, y, axes=None):\n \n \n def transpose(x):\n-    '''Transposes a matrix.\n+    '''Transposes a matrix and returns it.\n+\n+    # Arguments\n+        x: Keras tensor or variable.\n+\n+    # Returns\n+        Keras tensor or variable, transposed.\n+\n+    # Examples\n+    ```python\n+        >>> var = K.variable([[1, 2, 3], [4, 5, 6]])\n+        >>> K.eval(var)\n+        array([[ 1.,  2.,  3.],\n+               [ 4.,  5.,  6.]], dtype=float32)\n+        >>> var_transposed = K.transpose(var)\n+        >>> K.eval(var_transposed)\n+        array([[ 1.,  4.],\n+               [ 2.,  5.],\n+               [ 3.,  6.]], dtype=float32)\n+    ```\n+\n+    ```python\n+        >>> input = K.placeholder((2, 3))\n+        >>> input\n+        <tf.Tensor 'Placeholder_11:0' shape=(2, 3) dtype=float32>\n+        >>> input_transposed = K.transpose(input)\n+        >>> input_transposed\n+        <tf.Tensor 'transpose_4:0' shape=(3, 2) dtype=float32>\n+\n+    ```\n     '''\n     return tf.transpose(x)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9090704f1d4f36958ee9080f8a3ba89866512cca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 79 | Lines Deleted: 94 | Files Changed: 1 | Hunks: 66 | Methods Changed: 21 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 173 | Churn Cumulative: 4251 | Contributors (this commit): 47 | Commits (past 90d): 28 | Contributors (cumulative): 47 | DMM Complexity: 1.0\n\nDIFF:\n@@ -45,11 +45,16 @@ def clear_session():\n \n \n def manual_variable_initialization(value):\n-    '''Returns a boolean:\n-    whether variables should be initialized\n+    '''Sets the manual variable initialization flag.\n+\n+    This boolean flag determines whether\n+    variables should be initialized\n     as they are instantiated (default), or if\n     the user should handle the initialization\n-    (e.g. via tf.initialize_all_variables()).\n+    (e.g. via `tf.initialize_all_variables()`).\n+\n+    # Arguments\n+        value: Python boolean.\n     '''\n     global _MANUAL_VAR_INIT\n     _MANUAL_VAR_INIT = value\n@@ -96,7 +101,6 @@ def get_session():\n \n     # Returns\n         A TensorFlow session.\n-\n     '''\n     global _SESSION\n     if tf.get_default_session() is not None:\n@@ -157,42 +161,43 @@ def is_sparse(tensor):\n     '''Returns whether a tensor is a sparse tensor.\n \n     # Arguments\n-        tensor: a Keras tensor.\n+        tensor: A tensor instance.\n \n     # Returns\n-        Boolean.\n+        A boolean.\n \n     # Example\n     ```python\n-        >>> a = keras.backend.placeholder((2, 2), sparse=False)\n-        >>> print(keras.backend.is_sparse(a))\n+        >>> from keras import backend as K\n+        >>> a = K.placeholder((2, 2), sparse=False)\n+        >>> print(K.is_sparse(a))\n         False\n-        >>> b = keras.backend.placeholder((2, 2), sparse=True)\n-        >>> print(keras.backend.is_sparse(b))\n+        >>> b = K.placeholder((2, 2), sparse=True)\n+        >>> print(K.is_sparse(b))\n         True\n     ```\n-\n     '''\n     return isinstance(tensor, tf.SparseTensor)\n \n \n def to_dense(tensor):\n-    '''Converts a Keras tensor into a Keras dense tensor\n+    '''Converts a sparse tensor into a dense tensor\n     and returns it.\n \n     # Arguments\n-        tensor: Keras tensor.\n+        tensor: A tensor instance (potentially sparse).\n \n     # Returns\n-        A Keras dense tensor.\n+        A dense tensor.\n \n     # Examples\n     ```python\n-        >>> b = keras.backend.placeholder((2, 2), sparse=True)\n-        >>> print(keras.backend.is_sparse(b))\n+        >>> from keras import backend as K\n+        >>> b = K.placeholder((2, 2), sparse=True)\n+        >>> print(K.is_sparse(b))\n         True\n-        >>> c = keras.backend.to_dense(b)\n-        >>> print(keras.backend.is_sparse(c))\n+        >>> c = K.to_dense(b)\n+        >>> print(K.is_sparse(c))\n         False\n     ```\n     '''\n@@ -203,27 +208,25 @@ def to_dense(tensor):\n \n \n def variable(value, dtype=_FLOATX, name=None):\n-    '''Instantiates a tensor and returns it.\n+    '''Instantiates a variable and returns it.\n \n     # Arguments\n-        value: numpy array, initial value of the tensor.\n-\n-        dtype: tensor type.\n-\n-        name: optional name string for the tensor.\n+        value: Numpy array, initial value of the tensor.\n+        dtype: Tensor type.\n+        name: Optional name string for the tensor.\n \n     # Returns\n-        A Keras variable instance.\n+        A variable instance (with Keras metadata included).\n \n     # Examples\n     ```python\n         >>> from keras import backend as K\n         >>> val = np.array([[1, 2], [3, 4]])\n-        >>> kvar = K.variable(value=val, dtype='float64', name='example_kvar')\n+        >>> kvar = K.variable(value=val, dtype='float64', name='example_var')\n         >>> K.dtype(kvar)\n         'float64'\n         >>> print(kvar)\n-        example_kvar\n+        example_var\n         >>> kvar.eval()\n         array([[ 1.,  2.],\n                [ 3.,  4.]])\n@@ -233,13 +236,19 @@ def variable(value, dtype=_FLOATX, name=None):\n         sparse_coo = value.tocoo()\n         indices = np.concatenate((np.expand_dims(sparse_coo.row, 1),\n                                   np.expand_dims(sparse_coo.col, 1)), 1)\n-        # SparseTensor doesn't need initialization\n         v = tf.SparseTensor(indices=indices,\n                             values=sparse_coo.data,\n                             shape=sparse_coo.shape)\n         v._dims = len(sparse_coo.shape)\n+        v._keras_shape = sparse_coo.shape\n+        v._uses_learning_phase = False\n         return v\n     v = tf.Variable(value, dtype=_convert_string_dtype(dtype), name=name)\n+    if isinstance(value, np.array):\n+        v._keras_shape = value.shape\n+    elif hasattr(value, 'get_shape'):\n+        v._keras_shape = tuple(map(int, value.get_shape()))\n+    v._uses_learning_phase = False\n     return v\n \n \n@@ -263,29 +272,27 @@ def _initialize_variables():\n \n \n def placeholder(shape=None, ndim=None, dtype=_FLOATX, sparse=False, name=None):\n-    '''Instantiates a placeholder (Keras tensor) and returns it.\n+    '''Instantiates a placeholder tensor and returns it.\n \n     # Arguments\n-        shape: shape of the placeholder\n+        shape: Shape of the placeholder\n             (integer tuple, may include `None` entries).\n-\n-        ndim: number of axes of the tensor.\n+        ndim: Number of axes of the tensor.\n             At least one of {`shape`, `ndim`} must be specified.\n             If both are specified, `shape` is used.\n-\n-        dtype: placeholder type.\n-\n-        name: optional name string for the placeholder.\n+        dtype: Placeholder type.\n+        name: Optional name string for the placeholder.\n \n     # Returns\n-        Keras tensor instance.\n+        Tensor instance (with Keras metadata included).\n \n     # Examples\n     ```python\n-        >>> input = keras.backend.placeholder(shape=(2, 4, 5))\n-        >>> input._keras_shape\n+        >>> from keras import backend as K\n+        >>> input_ph = K.placeholder(shape=(2, 4, 5))\n+        >>> input_ph._keras_shape\n         (2, 4, 5)\n-        >>> input\n+        >>> input_ph\n         <tf.Tensor 'Placeholder_4:0' shape=(2, 4, 5) dtype=float32>\n     ```\n     '''\n@@ -303,13 +310,13 @@ def placeholder(shape=None, ndim=None, dtype=_FLOATX, sparse=False, name=None):\n \n \n def shape(x):\n-    '''Returns the symbolic shape of a Keras tensor or variable.\n+    '''Returns the symbolic shape of a tensor or variable.\n \n     # Arguments\n-        x: A Keras tensor or variable.\n+        x: A tensor or variable.\n \n     # Returns\n-        A symbolic shape.\n+        A symbolic shape (which is itself a tensor).\n \n     # Examples\n     ```\n@@ -336,10 +343,9 @@ def shape(x):\n def int_shape(x):\n     '''Returns the shape of a Keras tensor or a Keras variable as a tuple of\n     integers or None entries.\n-    Note that this function only works with TensorFlow.\n \n     # Arguments\n-        x: A Keras tensor or variable.\n+        x: Tensor or variable.\n \n     # Returns\n         A tuple of integers.\n@@ -364,10 +370,10 @@ def ndim(x):\n     '''Returns the number of axes in a tensor, as an integer.\n \n     # Arguments\n-        x: Keras tensor or variable.\n+        x: Tensor or variable.\n \n     # Returns\n-        integer (scalar), number of dimensions.\n+        Integer (scalar), number of axes.\n \n     # Examples\n     ```python\n@@ -394,7 +400,7 @@ def dtype(x):\n     '''Returns the dtype of a Keras tensor or variable, as a string.\n \n     # Arguments\n-        x: Keras tensor or variable.\n+        x: Tensor or variable.\n \n     # Returns\n         String, dtype of `x`.\n@@ -402,7 +408,6 @@ def dtype(x):\n     # Examples\n     ```python\n         >>> from keras import backend as K\n-        # Keras tensor\n         >>> K.dtype(K.placeholder(shape=(2,4,5)))\n         'float32'\n         >>> K.dtype(K.placeholder(shape=(2,4,5), dtype='float32'))\n@@ -422,11 +427,11 @@ def dtype(x):\n \n \n def eval(x):\n-    '''Evaluates the value of a Keras variable.\n+    '''Evaluates the value of a variable.\n     Returns a Numpy array.\n \n     # Arguments\n-        x: A Keras variable.\n+        x: A variable.\n \n     # Returns\n         A Numpy array.\n@@ -444,17 +449,15 @@ def eval(x):\n \n \n def zeros(shape, dtype=_FLOATX, name=None):\n-    '''Instantiates an all-zeros Keras variable and returns it.\n+    '''Instantiates an all-zeros variable and returns it.\n \n     # Arguments\n         shape: Tuple of integers, shape of returned Keras variable\n-\n         dtype: String, data type of returned Keras variable\n-\n         name: String, name of returned Keras variable\n \n     # Returns\n-        A Keras variable, filled with `0.0`\n+        A variable (including Keras metadata), filled with `0.0`.\n \n     # Example\n     ```python\n@@ -682,20 +685,20 @@ def count_params(x):\n \n \n def cast(x, dtype):\n-    '''Casts a Keras tensor to a tensor of a different dtype and returns it.\n+    '''Casts a tensor to a different dtype and returns it.\n \n     You can cast a Keras variable but it still returns a Keras tensor.\n \n     # Arguments\n         x: Keras tensor (or variable).\n-\n         dtype: String, either (`'float16'`, `'float32'`, or `'float64'`).\n \n     # Returns\n-        Keras tensor with dtype of `dtype`.\n+        Keras tensor with dtype `dtype`.\n \n     # Example\n     ```python\n+        >>> from keras import backend as K\n         >>> input = K.placeholder((2, 3), dtype='float32')\n         >>> input\n         <tf.Tensor 'Placeholder_2:0' shape=(2, 3) dtype=float32>\n@@ -744,20 +747,16 @@ def dot(x, y):\n     with a ND tensor, it reproduces the Theano behavior.\n     (e.g. (2, 3).(4, 3, 5) = (2, 4, 5))\n \n-    Like `K.cast()`, it can take Keras variables as inputs but the\n-        returned object is a Keras tensor.\n-\n     # Arguments\n-        x: Keras tensor (or variable).\n-\n-        y: Keras tensor (or variable).\n+        x: Tensor or variable.\n+        y: Tensor or variable.\n \n     # Returns\n-        A Keras tensor, dot product of `x` and `y`.\n+        A tensor, dot product of `x` and `y`.\n \n     # Examples\n     ```python\n-        # dot product between Keras tensors\n+        # dot product between tensors\n         >>> x = K.placeholder(shape=(2, 3))\n         >>> y = K.placeholder(shape=(3, 4))\n         >>> xy = K.dot(x, y)\n@@ -766,7 +765,7 @@ def dot(x, y):\n     ```\n \n     ```python\n-        # dot product between Keras tensors\n+        # dot product between tensors\n         >>> x = K.placeholder(shape=(32, 28, 3))\n         >>> y = K.placeholder(shape=(3, 4))\n         >>> xy = K.dot(x, y)\n@@ -775,17 +774,7 @@ def dot(x, y):\n     ```\n \n     ```python\n-        # dot product between Keras tensor and Keras variable\n-        >>> input = K.placeholder((2, 3))\n-        >>> y = K.random_uniform_variable(shape=(3, 4), low=0, high=1)\n-        >>> xy = K.dot(x, y)\n-        # It returns a Keras tensor\n-        >>> xy\n-        <tf.Tensor 'MatMul_1:0' shape=(2, 4) dtype=float32>\n-    ```\n-\n-    ```python\n-        # The Theano behavior example\n+        # Theano-like behavior example\n         >>> x = K.random_uniform_variable(shape=(2, 3), low=0, high=1)\n         >>> y = K.ones((4, 3, 5))\n         >>> xy = K.dot(x, y)\n@@ -819,12 +808,9 @@ def batch_dot(x, y, axes=None):\n     than the input. If the number of dimensions is reduced to 1,\n     we use `expand_dims` to make sure that ndim is at least 2.\n \n-    With TensorFlow backend, `batch_dot()` only supports `ndim >= 3` inputs.\n-\n     # Arguments\n         x, y: Keras tensors or variables with `ndim >= 2`\n             (With TensorFlow backend, `batch_dot()` only supports `ndim >= 3`)\n-\n         axes: list of (or single) int with target dimensions.\n             The lengths of `axes[0]` and `axes[1]` should be the same.\n \n@@ -832,7 +818,6 @@ def batch_dot(x, y, axes=None):\n         A tensor with shape equal to the concatenation of `x`'s shape\n         (less the dimension that was summed over) and `y`'s shape\n         (less the batch dimension and the dimension that was summed over).\n-\n         If the final rank is 1, we reshape it to `(batch_size, 1)`.\n \n     # Examples\n@@ -879,13 +864,13 @@ def batch_dot(x, y, axes=None):\n \n \n def transpose(x):\n-    '''Transposes a matrix and returns it.\n+    '''Transposes a tensor and returns it.\n \n     # Arguments\n-        x: Keras tensor or variable.\n+        x: Tensor or variable.\n \n     # Returns\n-        Keras tensor or variable, transposed.\n+        A tensor.\n \n     # Examples\n     ```python\n@@ -914,15 +899,15 @@ def transpose(x):\n \n \n def gather(reference, indices):\n-    '''Retrieves the vectors of indices `indices`\n-    in the 2D tensor `reference`.\n+    '''Retrieves the elements of indices `indices`\n+    in the tensor `reference`.\n \n     # Arguments\n-        reference: a 2D tensor.\n-        indices: an int tensor of indices.\n+        reference: A tensor.\n+        indices: An integer tensor of indices.\n \n     # Returns\n-        A 3D tensor of same type as `reference`.\n+        A tensor of same type as `reference`.\n     '''\n     return tf.gather(reference, indices)\n \n@@ -1167,7 +1152,7 @@ def cos(x):\n \n def normalize_batch_in_training(x, gamma, beta,\n                                 reduction_axes, epsilon=1e-3):\n-    '''Compute mean and std for batch then apply batch_normalization on batch.\n+    '''Computes mean and std for batch then apply batch_normalization on batch.\n     '''\n     mean, var = tf.nn.moments(x, reduction_axes,\n                               shift=None, name=None, keep_dims=False)\n@@ -1196,7 +1181,7 @@ def normalize_batch_in_training(x, gamma, beta,\n \n \n def batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n-    '''Apply batch normalization on x given mean, var, beta and gamma:\n+    '''Applies batch normalization on x given mean, var, beta and gamma:\n \n     output = (x - mean) / (sqrt(var) + epsilon) * gamma + beta\n     '''\n@@ -1206,7 +1191,7 @@ def batch_normalization(x, mean, var, beta, gamma, epsilon=1e-3):\n # SHAPE OPERATIONS\n \n def concatenate(tensors, axis=-1):\n-    '''Concantes a list of tensors alongside the specified axis.\n+    '''Concatenates a list of tensors alongside the specified axis.\n     '''\n     if axis < 0:\n         dims = ndim(tensors[0])\n@@ -1267,7 +1252,7 @@ def resize_images(X, height_factor, width_factor, dim_ordering):\n \n \n def resize_volumes(X, depth_factor, height_factor, width_factor, dim_ordering):\n-    '''Resize the volume contained in a 5D tensor of shape\n+    '''Resizes the volume contained in a 5D tensor of shape\n     - [batch, channels, depth, height, width] (for 'th' dim_ordering)\n     - [batch, depth, height, width, channels] (for 'tf' dim_ordering)\n     by a factor of (depth_factor, height_factor, width_factor).\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2752a587305f7a7c2e05a31bbb82c62fc0a9cf77", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 10 | Churn Cumulative: 4261 | Contributors (this commit): 47 | Commits (past 90d): 29 | Contributors (cumulative): 47 | DMM Complexity: 0.0\n\nDIFF:\n@@ -244,11 +244,11 @@ def variable(value, dtype=_FLOATX, name=None):\n         v._uses_learning_phase = False\n         return v\n     v = tf.Variable(value, dtype=_convert_string_dtype(dtype), name=name)\n-    if isinstance(value, np.array):\n-        v._keras_shape = value.shape\n-    elif hasattr(value, 'get_shape'):\n-        v._keras_shape = tuple(map(int, value.get_shape()))\n-    v._uses_learning_phase = False\n+    # if isinstance(value, np.array):\n+    #     v._keras_shape = value.shape\n+    # elif hasattr(value, 'get_shape'):\n+    #     v._keras_shape = tuple(map(int, value.get_shape()))\n+    # v._uses_learning_phase = False\n     return v\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e3dd5d7ca573eeb3b01b81c783d06f0a81779e06", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 10 | Churn Cumulative: 4271 | Contributors (this commit): 47 | Commits (past 90d): 30 | Contributors (cumulative): 47 | DMM Complexity: 1.0\n\nDIFF:\n@@ -244,11 +244,11 @@ def variable(value, dtype=_FLOATX, name=None):\n         v._uses_learning_phase = False\n         return v\n     v = tf.Variable(value, dtype=_convert_string_dtype(dtype), name=name)\n-    # if isinstance(value, np.array):\n-    #     v._keras_shape = value.shape\n-    # elif hasattr(value, 'get_shape'):\n-    #     v._keras_shape = tuple(map(int, value.get_shape()))\n-    # v._uses_learning_phase = False\n+    if isinstance(value, np.ndarray):\n+        v._keras_shape = value.shape\n+    elif hasattr(value, 'get_shape'):\n+        v._keras_shape = tuple(map(int, value.get_shape()))\n+    v._uses_learning_phase = False\n     return v\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e9b8424839ecceb106deb77df0b4230b97b06261", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 3084 | Contributors (this commit): 47 | Commits (past 90d): 22 | Contributors (cumulative): 47 | DMM Complexity: None\n\nDIFF:\n@@ -129,11 +129,11 @@ def eye(size, dtype=_FLOATX, name=None):\n     return variable(np.eye(size), dtype, name)\n \n \n-def ones_like(x):\n+def ones_like(x, name=None):\n     return T.ones_like(x)\n \n \n-def zeros_like(x):\n+def zeros_like(x, name=None):\n     return T.zeros_like(x)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5e73db6c00d62ac61cbae9056628a7683122b392", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 76 | Lines Deleted: 63 | Files Changed: 4 | Hunks: 38 | Methods Changed: 21 | Complexity Δ (Sum/Max): 5/5 | Churn Δ: 139 | Churn Cumulative: 13286 | Contributors (this commit): 80 | Commits (past 90d): 70 | Contributors (cumulative): 143 | DMM Complexity: 0.5\n\nDIFF:\n@@ -857,6 +857,9 @@ def batch_dot(x, y, axes=None):\n     else:\n         adj_x = None\n         adj_y = None\n+    try:\n+        out = tf.batch_matmul(x, y, adj_a=adj_x, adj_b=adj_y)\n+    except TypeError:\n         out = tf.batch_matmul(x, y, adj_x=adj_x, adj_y=adj_y)\n     if ndim(out) == 1:\n         out = expand_dims(out, 1)\n@@ -1168,7 +1171,7 @@ def normalize_batch_in_training(x, gamma, beta,\n                 target_shape.append(1)\n             else:\n                 target_shape.append(tf.shape(x)[axis])\n-        target_shape = tf.pack(target_shape)\n+        target_shape = tf.stack(target_shape)\n \n         broadcast_mean = tf.reshape(mean, target_shape)\n         broadcast_var = tf.reshape(var, target_shape)\n@@ -1203,6 +1206,9 @@ def concatenate(tensors, axis=-1):\n     if py_all([is_sparse(x) for x in tensors]):\n         return tf.sparse_concat(axis, tensors)\n     else:\n+        try:\n+            return tf.concat_v2([to_dense(x) for x in tensors], axis)\n+        except AttributeError:\n             return tf.concat(axis, [to_dense(x) for x in tensors])\n \n \n@@ -1280,10 +1286,13 @@ def repeat_elements(x, rep, axis):\n     '''\n     x_shape = x.get_shape().as_list()\n     # slices along the repeat axis\n-    splits = tf.split(axis, x_shape[axis], x)\n+    try:\n+        splits = tf.split(value=x, num_or_size_splits=x_shape[axis], axis=axis)\n+    except TypeError:\n+        splits = tf.split(value=x, num_split=x_shape[axis], split_dim=axis)\n     # repeat each slice the given number of reps\n     x_rep = [s for s in splits for i in range(rep)]\n-    return tf.concat(axis, x_rep)\n+    return concatenate(x_rep, axis)\n \n \n def repeat(x, n):\n@@ -1294,7 +1303,7 @@ def repeat(x, n):\n     '''\n     assert ndim(x) == 2\n     x = tf.expand_dims(x, 1)\n-    pattern = tf.pack([1, n, 1])\n+    pattern = tf.stack([1, n, 1])\n     return tf.tile(x, pattern)\n \n \n@@ -1331,7 +1340,7 @@ def batch_flatten(x):\n     '''Turn a n-D tensor into a 2D tensor where\n     the first dimension is conserved.\n     '''\n-    x = tf.reshape(x, tf.pack([-1, prod(shape(x)[1:])]))\n+    x = tf.reshape(x, tf.stack([-1, prod(shape(x)[1:])]))\n     return x\n \n \n@@ -1438,8 +1447,8 @@ def spatial_3d_padding(x, padding=(1, 1, 1), dim_ordering='default'):\n     return tf.pad(x, pattern)\n \n \n-def pack(x):\n-    return tf.pack(x)\n+def stack(x):\n+    return tf.stack(x)\n \n \n def one_hot(indices, nb_classes):\n@@ -1455,6 +1464,10 @@ def reverse(x, axes):\n     '''\n     if isinstance(axes, int):\n         axes = [axes]\n+    try:\n+        return tf.reverse_v2(x, axes)\n+    except AttributeError:\n+        # Older TF versions.\n         dims = [True if i in axes else False for i in range(len(x.get_shape()._dims))]\n         return tf.reverse(x, dims)\n \n@@ -1679,19 +1692,19 @@ def rnn(step_function, inputs, initial_states,\n         successive_states = []\n         successive_outputs = []\n \n-        input_list = tf.unpack(inputs)\n+        input_list = tf.unstack(inputs)\n         if go_backwards:\n             input_list.reverse()\n \n         if mask is not None:\n-            mask_list = tf.unpack(mask)\n+            mask_list = tf.unstack(mask)\n             if go_backwards:\n                 mask_list.reverse()\n \n             for input, mask_t in zip(input_list, mask_list):\n                 output, new_states = step_function(input, states + constants)\n \n-                # tf.select needs its condition tensor\n+                # tf.where needs its condition tensor\n                 # to be the same shape as its two\n                 # result tensors, but in our case\n                 # the condition (mask) tensor is\n@@ -1702,21 +1715,21 @@ def rnn(step_function, inputs, initial_states,\n                 # it just repeats the mask along its second dimension\n                 # n times.\n                 tiled_mask_t = tf.tile(mask_t,\n-                                       tf.pack([1, tf.shape(output)[1]]))\n+                                       tf.stack([1, tf.shape(output)[1]]))\n \n                 if len(successive_outputs) == 0:\n                     prev_output = zeros_like(output)\n                 else:\n                     prev_output = successive_outputs[-1]\n \n-                output = tf.select(tiled_mask_t, output, prev_output)\n+                output = tf.where(tiled_mask_t, output, prev_output)\n \n                 return_states = []\n                 for state, new_state in zip(states, new_states):\n                     # (see earlier comment for tile explanation)\n                     tiled_mask_t = tf.tile(mask_t,\n-                                           tf.pack([1, tf.shape(new_state)[1]]))\n-                    return_states.append(tf.select(tiled_mask_t,\n+                                           tf.stack([1, tf.shape(new_state)[1]]))\n+                    return_states.append(tf.where(tiled_mask_t,\n                                                   new_state,\n                                                   state))\n                 states = return_states\n@@ -1724,7 +1737,7 @@ def rnn(step_function, inputs, initial_states,\n                 successive_states.append(states)\n                 last_output = successive_outputs[-1]\n                 new_states = successive_states[-1]\n-                outputs = tf.pack(successive_outputs)\n+                outputs = tf.stack(successive_outputs)\n         else:\n             for input in input_list:\n                 output, states = step_function(input, states + constants)\n@@ -1732,11 +1745,11 @@ def rnn(step_function, inputs, initial_states,\n                 successive_states.append(states)\n             last_output = successive_outputs[-1]\n             new_states = successive_states[-1]\n-            outputs = tf.pack(successive_outputs)\n+            outputs = tf.stack(successive_outputs)\n \n     else:\n         if go_backwards:\n-            inputs = tf.reverse(inputs, [True] + [False] * (ndim - 1))\n+            inputs = reverse(inputs, 0)\n \n         states = tuple(initial_states)\n \n@@ -1761,7 +1774,7 @@ def rnn(step_function, inputs, initial_states,\n                                  'as its first state at time `t` '\n                                  'the output at time `t-1`).')\n             if go_backwards:\n-                mask = tf.reverse(mask, [True] + [False] * (ndim - 1))\n+                mask = reverse(mask, 0)\n \n             mask_ta = tensor_array_ops.TensorArray(\n                 dtype=tf.bool,\n@@ -1778,9 +1791,9 @@ def rnn(step_function, inputs, initial_states,\n                 for state, new_state in zip(states, new_states):\n                     new_state.set_shape(state.get_shape())\n                 tiled_mask_t = tf.tile(mask_t,\n-                                       tf.pack([1, tf.shape(output)[1]]))\n-                output = tf.select(tiled_mask_t, output, states[0])\n-                new_states = [tf.select(tiled_mask_t, new_states[i], states[i]) for i in range(len(states))]\n+                                       tf.stack([1, tf.shape(output)[1]]))\n+                output = tf.where(tiled_mask_t, output, states[0])\n+                new_states = [tf.where(tiled_mask_t, new_states[i], states[i]) for i in range(len(states))]\n                 output_ta_t = output_ta_t.write(time, output)\n                 return (time + 1, output_ta_t) + tuple(new_states)\n         else:\n@@ -1905,7 +1918,7 @@ def elu(x, alpha=1.):\n     if alpha == 1:\n         return res\n     else:\n-        return tf.select(x > 0, res, alpha * res)\n+        return tf.where(x > 0, res, alpha * res)\n \n \n def softmax(x):\n@@ -2368,7 +2381,7 @@ def random_uniform(shape, low=0.0, high=1.0, dtype=_FLOATX, seed=None):\n def random_binomial(shape, p=0.0, dtype=_FLOATX, seed=None):\n     if seed is None:\n         seed = np.random.randint(10e6)\n-    return tf.select(tf.random_uniform(shape, dtype=dtype, seed=seed) <= p,\n+    return tf.where(tf.random_uniform(shape, dtype=dtype, seed=seed) <= p,\n                      tf.ones(shape, dtype=dtype),\n                      tf.zeros(shape, dtype=dtype))\n \n@@ -2384,8 +2397,8 @@ def ctc_label_dense_to_sparse(labels, label_lengths):\n     # undocumented feature soon to be made public\n     from tensorflow.python.ops import functional_ops\n     label_shape = tf.shape(labels)\n-    num_batches_tns = tf.pack([label_shape[0]])\n-    max_num_labels_tns = tf.pack([label_shape[1]])\n+    num_batches_tns = tf.stack([label_shape[0]])\n+    max_num_labels_tns = tf.stack([label_shape[1]])\n \n     def range_less_than(previous_state, current_input):\n         return tf.expand_dims(tf.range(label_shape[1]), 0) < tf.fill(max_num_labels_tns, current_input)\n@@ -2400,9 +2413,9 @@ def ctc_label_dense_to_sparse(labels, label_lengths):\n     label_ind = tf.boolean_mask(label_array, dense_mask)\n \n     batch_array = tf.transpose(tf.reshape(tf.tile(tf.range(0, label_shape[0]),\n-                                                  max_num_labels_tns), tf.reverse(label_shape, [True])))\n+                                                  max_num_labels_tns), reverse(label_shape, 0)))\n     batch_ind = tf.boolean_mask(batch_array, dense_mask)\n-    indices = tf.transpose(tf.reshape(tf.concat(0, [batch_ind, label_ind]), [2, -1]))\n+    indices = tf.transpose(tf.reshape(concatenate([batch_ind, label_ind], axis=0), [2, -1]))\n \n     vals_sparse = tf.gather_nd(labels, indices)\n \n\n@@ -796,7 +796,7 @@ def spatial_3d_padding(x, padding=(1, 1, 1), dim_ordering='default'):\n     return T.set_subtensor(output[indices], x)\n \n \n-def pack(x):\n+def stack(x):\n     return T.stack(*x)\n \n \n\n@@ -32,7 +32,7 @@ def time_distributed_dense(x, w, b=None, dropout=None,\n         x = x + b\n     # reshape to 3D tensor\n     if K.backend() == 'tensorflow':\n-        x = K.reshape(x, K.pack([-1, timesteps, output_dim]))\n+        x = K.reshape(x, K.stack([-1, timesteps, output_dim]))\n         x.set_shape([None, None, output_dim])\n     else:\n         x = K.reshape(x, (-1, timesteps, output_dim))\n\n@@ -625,43 +625,43 @@ class TestBackend(object):\n         mean = 0.\n         std = 1.\n         rand = KTF.eval(KTF.random_normal((1000, 1000), mean=mean, std=std))\n-        assert(rand.shape == (1000, 1000))\n-        assert(np.abs(np.mean(rand) - mean) < 0.01)\n-        assert(np.abs(np.std(rand) - std) < 0.01)\n+        assert rand.shape == (1000, 1000)\n+        assert np.abs(np.mean(rand) - mean) < 0.01\n+        assert np.abs(np.std(rand) - std) < 0.01\n \n         rand = KTH.eval(KTH.random_normal((1000, 1000), mean=mean, std=std))\n-        assert(rand.shape == (1000, 1000))\n-        assert(np.abs(np.mean(rand) - mean) < 0.01)\n-        assert(np.abs(np.std(rand) - std) < 0.01)\n+        assert rand.shape == (1000, 1000)\n+        assert np.abs(np.mean(rand) - mean) < 0.01\n+        assert np.abs(np.std(rand) - std) < 0.01\n \n     def test_random_uniform(self):\n-        min = -1.\n-        max = 1.\n-        rand = KTF.eval(KTF.random_uniform((1000, 1000), min, max))\n-        assert(rand.shape == (1000, 1000))\n-        assert(np.abs(np.mean(rand)) < 0.01)\n-        assert(np.max(rand) <= max)\n-        assert(np.min(rand) >= min)\n+        min_val = -1.\n+        max_val = 1.\n+        rand = KTF.eval(KTF.random_uniform((1000, 1000), min_val, max_val))\n+        assert rand.shape == (1000, 1000)\n+        assert np.abs(np.mean(rand)) < 0.01\n+        assert np.max(rand) <= max_val\n+        assert np.min(rand) >= min_val\n \n-        rand = KTH.eval(KTH.random_uniform((1000, 1000), min, max))\n-        assert(rand.shape == (1000, 1000))\n-        assert(np.abs(np.mean(rand)) < 0.01)\n-        assert(np.max(rand) <= max)\n-        assert(np.min(rand) >= min)\n+        rand = KTH.eval(KTH.random_uniform((1000, 1000), min_val, max_val))\n+        assert rand.shape == (1000, 1000)\n+        assert np.abs(np.mean(rand)) < 0.01\n+        assert np.max(rand) <= max_val\n+        assert np.min(rand) >= min_val\n \n     def test_random_binomial(self):\n         p = 0.5\n         rand = KTF.eval(KTF.random_binomial((1000, 1000), p))\n-        assert(rand.shape == (1000, 1000))\n-        assert(np.abs(np.mean(rand) - p) < 0.01)\n-        assert(np.max(rand) == 1)\n-        assert(np.min(rand) == 0)\n+        assert rand.shape == (1000, 1000)\n+        assert np.abs(np.mean(rand) - p) < 0.01\n+        assert np.max(rand) == 1\n+        assert np.min(rand) == 0\n \n         rand = KTH.eval(KTH.random_binomial((1000, 1000), p))\n-        assert(rand.shape == (1000, 1000))\n-        assert(np.abs(np.mean(rand) - p) < 0.01)\n-        assert(np.max(rand) == 1)\n-        assert(np.min(rand) == 0)\n+        assert rand.shape == (1000, 1000)\n+        assert np.abs(np.mean(rand) - p) < 0.01\n+        assert np.max(rand) == 1\n+        assert np.min(rand) == 0\n \n     def test_ctc(self):\n         # simplified version of TensorFlow's test\n@@ -913,22 +913,22 @@ class TestBackend(object):\n         for test_value in (-20, 0, 1, 10):\n             t_a = KTF.arange(test_value)\n             a = KTF.eval(t_a)\n-            assert(np.array_equal(a, np.arange(test_value)))\n+            assert np.array_equal(a, np.arange(test_value))\n             t_b = KTH.arange(test_value)\n             b = KTH.eval(t_b)\n-            assert(np.array_equal(b, np.arange(test_value)))\n-            assert(np.array_equal(a, b))\n-            assert(KTF.dtype(t_a) == KTH.dtype(t_b), 'default dtypes are equal')\n+            assert np.array_equal(b, np.arange(test_value))\n+            assert np.array_equal(a, b)\n+            assert KTF.dtype(t_a) == KTH.dtype(t_b)\n         for start, stop, step in ((0, 5, 1), (-5, 5, 2), (0, 1, 2)):\n             a = KTF.eval(KTF.arange(start, stop, step))\n-            assert(np.array_equal(a, np.arange(start, stop, step)))\n+            assert np.array_equal(a, np.arange(start, stop, step))\n             b = KTH.eval(KTH.arange(start, stop, step))\n-            assert(np.array_equal(b, np.arange(start, stop, step)))\n-            assert(np.array_equal(a, b))\n+            assert np.array_equal(b, np.arange(start, stop, step))\n+            assert np.array_equal(a, b)\n         for dtype in ('int32', 'int64', 'float32', 'float64'):\n             for backend in (KTF, KTH):\n                 t = backend.arange(10, dtype=dtype)\n-                assert(backend.dtype(t) == dtype)\n+                assert backend.dtype(t) == dtype\n \n \n if __name__ == '__main__':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
