{"custom_id": "keras#c19e2a5c2630b79316d22fc9f1faf38310c21c00", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 112 | Contributors (this commit): 5 | Commits (past 90d): 6 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -39,7 +39,7 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n \n     You start from `Input`,\n     you chain layer calls to specify the model's forward pass,\n-    and finally you create your model from inputs and outputs:\n+    and finally, you create your model from inputs and outputs:\n \n     ```python\n     inputs = keras.Input(shape=(37,))\n@@ -245,10 +245,10 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n                 which is the starting layer name and ending layer name\n                 (both inclusive) indicating the range of layers to be printed\n                 in summary. It also accepts regex patterns instead of exact\n-                name. In such case, start predicate will be the first element\n+                names. In such case, the start predicate will be the first element\n                 it matches to `layer_range[0]` and the end predicate will be\n                 the last element it matches to `layer_range[1]`.\n-                By default `None` which considers all layers of model.\n+                By default `None` considers all layers of the model.\n \n         Raises:\n             ValueError: if `summary()` is called before the model is built.\n@@ -268,7 +268,7 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n         \"\"\"Saves a model as a `.keras` file.\n \n         Args:\n-            filepath: `str` or `pathlib.Path` object. Path where to save\n+            filepath: `str` or `pathlib.Path` object.The path where to save\n                 the model. Must end in `.keras`.\n             overwrite: Whether we should overwrite any existing model at\n                 the target location, or instead ask the user via\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b6570de6734e83c973452a787e0277b8bee31b7e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 11 | Churn Cumulative: 123 | Contributors (this commit): 5 | Commits (past 90d): 7 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -245,9 +245,10 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n                 which is the starting layer name and ending layer name\n                 (both inclusive) indicating the range of layers to be printed\n                 in summary. It also accepts regex patterns instead of exact\n-                names. In such case, the start predicate will be the first element\n-                it matches to `layer_range[0]` and the end predicate will be\n-                the last element it matches to `layer_range[1]`.\n+                names. In this case, the start predicate will be\n+                the first element that matches `layer_range[0]`\n+                and the end predicate will be the last element\n+                that matches `layer_range[1]`.\n                 By default `None` considers all layers of the model.\n \n         Raises:\n@@ -268,8 +269,8 @@ class Model(Trainer, base_trainer.Trainer, Layer):\n         \"\"\"Saves a model as a `.keras` file.\n \n         Args:\n-            filepath: `str` or `pathlib.Path` object.The path where to save\n-                the model. Must end in `.keras`.\n+            filepath: `str` or `pathlib.Path` object.\n+                The path where to save the model. Must end in `.keras`.\n             overwrite: Whether we should overwrite any existing model at\n                 the target location, or instead ask the user via\n                 an interactive prompt.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3756ef96ee6534509e260b89efaca8b5ba1bb474", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 168 | Contributors (this commit): 5 | Commits (past 90d): 16 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1007,7 +1007,7 @@ def ndim(x):\n \n def nonzero(x):\n     x = convert_to_tensor(x)\n-    return tuple(cast(indices, \"int32\") for indices in torch.nonzero(x).T)\n+    return cast(torch.nonzero(x).T, \"int32\")\n \n \n def not_equal(x1, x2):\n\n@@ -664,10 +664,7 @@ class SensitivitySpecificityBase(Metric):\n         Returns:\n             maximal dependent value, if no value satisfies the constraint 0.0.\n         \"\"\"\n-        feasible = backend.convert_to_numpy(\n-            ops.nonzero(predicate(constrained, self.value))\n-        )\n-\n+        feasible = ops.nonzero(predicate(constrained, self.value))\n         feasible_exists = ops.greater(ops.size(feasible), 0)\n         max_dependent = ops.max(ops.take(dependent, feasible), initial=0)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5d8c541c5a29482e0d3a924221703500c096bc05", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 17 | Files Changed: 5 | Hunks: 16 | Methods Changed: 8 | Complexity Δ (Sum/Max): 6/6 | Churn Δ: 41 | Churn Cumulative: 301 | Contributors (this commit): 4 | Commits (past 90d): 17 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -4,6 +4,7 @@ from keras.src.layers.layer import Layer\n from keras.src.utils import argument_validation\n from keras.src.utils import backend_utils\n from keras.src.utils import numerical_utils\n+from keras.src.utils import tf_utils\n from keras.src.utils.module_utils import tensorflow as tf\n \n \n@@ -138,7 +139,7 @@ class HashedCrossing(Layer):\n         from keras.src.backend import tensorflow as tf_backend\n \n         self._check_at_least_two_inputs(inputs)\n-        inputs = [tf_backend.convert_to_tensor(x) for x in inputs]\n+        inputs = [tf_utils.ensure_tensor(x) for x in inputs]\n         self._check_input_shape_and_type(inputs)\n \n         # Uprank to rank 2 for the cross_hashed op.\n\n@@ -3,6 +3,7 @@ from keras.src.api_export import keras_export\n from keras.src.layers.layer import Layer\n from keras.src.utils import backend_utils\n from keras.src.utils import numerical_utils\n+from keras.src.utils import tf_utils\n from keras.src.utils.module_utils import tensorflow as tf\n \n \n@@ -211,7 +212,7 @@ class Hashing(Layer):\n     def call(self, inputs):\n         from keras.src.backend import tensorflow as tf_backend\n \n-        inputs = tf_backend.convert_to_tensor(inputs)\n+        inputs = tf_utils.ensure_tensor(inputs)\n         if self.output_mode == \"one_hot\" and inputs.shape[-1] == 1:\n             # One hot only unpranks if the final dimension is not 1.\n             inputs = tf_backend.numpy.squeeze(inputs, axis=-1)\n\n@@ -6,6 +6,7 @@ from keras.src import backend\n from keras.src.layers.layer import Layer\n from keras.src.utils import argument_validation\n from keras.src.utils import numerical_utils\n+from keras.src.utils import tf_utils\n from keras.src.utils.module_utils import tensorflow as tf\n \n \n@@ -565,8 +566,6 @@ class IndexLookup(Layer):\n         return backend.KerasTensor(output_shape, dtype=output_dtype)\n \n     def adapt(self, data, steps=None):\n-        from keras.src.backend import tensorflow as tf_backend\n-\n         self.reset_state()\n         if isinstance(data, tf.data.Dataset):\n             if steps is not None:\n@@ -574,9 +573,7 @@ class IndexLookup(Layer):\n             for batch in data:\n                 self.update_state(batch)\n         else:\n-            data = tf_backend.convert_to_tensor(\n-                data, dtype=self.vocabulary_dtype\n-            )\n+            data = tf_utils.ensure_tensor(data, dtype=self.vocabulary_dtype)\n             if data.shape.rank == 1:\n                 # A plain list of strings\n                 # is treated as as many documents\n@@ -585,8 +582,6 @@ class IndexLookup(Layer):\n         self.finalize_state()\n \n     def update_state(self, data):\n-        from keras.src.backend import tensorflow as tf_backend\n-\n         if self._has_input_vocabulary:\n             raise ValueError(\n                 f\"Cannot adapt layer '{self.name}' after setting a static \"\n@@ -594,7 +589,7 @@ class IndexLookup(Layer):\n                 \"`set_vocabulary()` method.\"\n             )\n \n-        data = tf_backend.convert_to_tensor(data, dtype=self.vocabulary_dtype)\n+        data = tf_utils.ensure_tensor(data, dtype=self.vocabulary_dtype)\n         if data.shape.rank == 0:\n             data = tf.expand_dims(data, 0)\n         if data.shape.rank == 1:\n@@ -708,7 +703,7 @@ class IndexLookup(Layer):\n \n         self._ensure_known_vocab_size()\n \n-        inputs = tf_backend.convert_to_tensor(inputs, dtype=self._key_dtype)\n+        inputs = tf_utils.ensure_tensor(inputs, dtype=self._key_dtype)\n         original_shape = inputs.shape\n         # Some ops will not handle scalar input, so uprank to rank 1.\n         if inputs.shape.rank == 0:\n\n@@ -8,6 +8,7 @@ from keras.src.layers.preprocessing.string_lookup import StringLookup\n from keras.src.saving import serialization_lib\n from keras.src.utils import argument_validation\n from keras.src.utils import backend_utils\n+from keras.src.utils import tf_utils\n from keras.src.utils.module_utils import tensorflow as tf\n \n \n@@ -412,8 +413,6 @@ class TextVectorization(Layer):\n                 repeating dataset, you must specify the `steps` argument. This\n                 argument is not supported with array inputs or list inputs.\n         \"\"\"\n-        from keras.src.backend import tensorflow as tf_backend\n-\n         self.reset_state()\n         if isinstance(data, tf.data.Dataset):\n             if steps is not None:\n@@ -421,7 +420,7 @@ class TextVectorization(Layer):\n             for batch in data:\n                 self.update_state(batch)\n         else:\n-            data = tf_backend.convert_to_tensor(data, dtype=\"string\")\n+            data = tf_utils.ensure_tensor(data, dtype=\"string\")\n             if data.shape.rank == 1:\n                 # A plain list of strings\n                 # is treated as as many documents\n@@ -520,9 +519,7 @@ class TextVectorization(Layer):\n         self._lookup_layer.set_vocabulary(vocabulary, idf_weights=idf_weights)\n \n     def _preprocess(self, inputs):\n-        from keras.src.backend import tensorflow as tf_backend\n-\n-        inputs = tf_backend.convert_to_tensor(inputs, dtype=tf.string)\n+        inputs = tf_utils.ensure_tensor(inputs, dtype=tf.string)\n         if self._standardize in (\"lower\", \"lower_and_strip_punctuation\"):\n             inputs = tf.strings.lower(inputs)\n         if self._standardize in (\n\n@@ -1,3 +1,4 @@\n+from keras.src import backend\n from keras.src.utils.module_utils import tensorflow as tf\n \n \n@@ -25,3 +26,15 @@ def get_tensor_spec(t, dynamic_batch=False, name=None):\n     shape = tf.TensorShape(shape_list)\n     spec._shape = shape\n     return spec\n+\n+\n+def ensure_tensor(inputs, dtype=None):\n+    \"\"\"Ensures the input is a Tensor, SparseTensor or RaggedTensor.\"\"\"\n+    if not isinstance(inputs, (tf.Tensor, tf.SparseTensor, tf.RaggedTensor)):\n+        if backend.backend() == \"torch\" and backend.is_tensor(inputs):\n+            # Plain `np.asarray()` conversion fails with PyTorch.\n+            inputs = backend.convert_to_numpy(inputs)\n+        inputs = tf.convert_to_tensor(inputs, dtype)\n+    if dtype is not None and inputs.dtype != dtype:\n+        inputs = tf.cast(inputs, dtype)\n+    return inputs\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#479a0e04c3201c4a4e4002614d1a2a8e0c2e89b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 23 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -466,7 +466,9 @@ class TensorBoard(Callback):\n     def on_epoch_begin(self, epoch, logs=None):\n         # Keeps track of epoch for profiling.\n         if self.write_steps_per_second:\n-            self._previous_epoch_iterations = self.model.optimizer.iterations\n+            self._previous_epoch_iterations = ops.convert_to_tensor(\n+                self.model.optimizer.iterations, \"float32\"\n+            )\n             self._epoch_start_time = time.time()\n \n     def on_epoch_end(self, epoch, logs=None):\n@@ -505,9 +507,6 @@ class TensorBoard(Callback):\n         current_iteration = self.model.optimizer.iterations\n         time_since_epoch_begin = time.time() - self._epoch_start_time\n         current_iteration = ops.convert_to_tensor(current_iteration, \"float32\")\n-        self._previous_epoch_iterations = ops.convert_to_tensor(\n-            self._previous_epoch_iterations, \"float32\"\n-        )\n         time_since_epoch_begin = ops.convert_to_tensor(\n             time_since_epoch_begin, \"float32\"\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b47630436f80d4a71416cd1ecd0bec6d9ac570bf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 3 | Files Changed: 4 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 17 | Churn Cumulative: 852 | Contributors (this commit): 5 | Commits (past 90d): 33 | Contributors (cumulative): 13 | DMM Complexity: 0.5\n\nDIFF:\n@@ -79,7 +79,7 @@ def hashing_trick(\n \n \n @keras_export(\"keras._legacy.preprocessing.text.Tokenizer\")\n-class Tokenizer(object):\n+class Tokenizer:\n     \"\"\"DEPRECATED.\"\"\"\n \n     def __init__(\n\n@@ -6125,12 +6125,16 @@ def select(condlist, choicelist, default=0):\n     # Returns: tensor([0,  1,  2, 42, 16, 25])\n     ```\n     \"\"\"\n-    if not isinstance(condlist, list) or not isinstance(choicelist, list):\n+    if not isinstance(condlist, (list, tuple)) or not isinstance(\n+        choicelist, (list, tuple)\n+    ):\n         raise ValueError(\n             \"condlist and choicelist must be lists. Received: \"\n             f\"type(condlist) = {type(condlist)}, \"\n             f\"type(choicelist) = {type(choicelist)}\"\n         )\n+    condlist = list(condlist)\n+    choicelist = list(choicelist)\n     if not condlist or not choicelist:\n         raise ValueError(\n             \"condlist and choicelist must not be empty. Received: \"\n\n@@ -4265,6 +4265,13 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         y = knp.select(condlist, choicelist, 42)\n         self.assertAllClose(y, [0, 1, 2, 42, 16, 25])\n \n+        # Test with tuples\n+        condlist = (x < 3, x > 3)\n+        choicelist = (x, x**2)\n+        y = knp.select(condlist, choicelist, 42)\n+        self.assertAllClose(y, [0, 1, 2, 42, 16, 25])\n+\n+        # Test with symbolic tensors\n         x = backend.KerasTensor((6,))\n         condlist = [x < 3, x > 3]\n         choicelist = [x, x**2]\n\n@@ -1,4 +1,4 @@\n-class DataAdapter(object):\n+class DataAdapter:\n     \"\"\"Base class for input data adapters.\n \n     The purpose of a DataAdapter is to provide a unfied interface to\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#490b1f013dcf3bb94f9f185cb2205430477b87e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 39 | Churn Cumulative: 93 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -41,16 +41,28 @@ class GeneratorDataAdapter(DataAdapter):\n     def get_tf_dataset(self):\n         from keras.src.utils.module_utils import tensorflow as tf\n \n-        def convert_to_tf(x):\n+        def convert_to_tf(x, spec):\n             if data_adapter_utils.is_scipy_sparse(x):\n                 x = data_adapter_utils.scipy_sparse_to_tf_sparse(x)\n             elif data_adapter_utils.is_jax_sparse(x):\n                 x = data_adapter_utils.jax_sparse_to_tf_sparse(x)\n+            if not spec.shape.is_compatible_with(x.shape):\n+                raise TypeError(\n+                    f\"Generator yielded an element of shape {x.shape} where \"\n+                    f\"an element of shape {spec.shape} was expected. Your \"\n+                    \"generator provides tensors with variable input \"\n+                    \"dimensions other than the batch size. Make sure that the \"\n+                    \"generator's first two batches do not have the same \"\n+                    \"dimension value wherever there is a variable input \"\n+                    \"dimension.\"\n+                )\n             return x\n \n         def get_tf_iterator():\n             for batch in self.generator:\n-                batch = tree.map_structure(convert_to_tf, batch)\n+                batch = tree.map_structure(\n+                    convert_to_tf, batch, self._output_signature\n+                )\n                 yield batch\n \n         if self._output_signature is None:\n\n@@ -134,6 +134,29 @@ class GeneratorDataAdapterTest(testing.TestCase, parameterized.TestCase):\n                 self.assertEqual(bx.shape, (2, 6))\n                 self.assertEqual(by.shape, (2, 2))\n \n+    @pytest.mark.skipif(\n+        backend.backend() != \"tensorflow\",\n+        reason=\"tf.data.Dataset specific behavior\",\n+    )\n+    def test_with_unexpected_shapes(self):\n+        def generator():\n+            yield np.ones([16, 4], \"float32\"), np.ones([16, 2], \"float32\")\n+            yield np.ones([16, 5], \"float32\"), np.ones([16, 2], \"float32\")\n+            yield np.ones([16, 6], \"float32\"), np.ones([16, 3], \"float32\")\n+\n+        adapter = generator_data_adapter.GeneratorDataAdapter(generator())\n+\n+        it = iter(adapter.get_tf_dataset())\n+        next(it)\n+        next(it)\n+        # note that Tensorflow wraps the TypeError in an InvalidArgumentError.\n+        with self.assertRaisesRegex(\n+            tf.errors.InvalidArgumentError,\n+            \"TypeError:.* shape \\\\(16, 3\\\\).* shape \\\\(None, 2\\\\) was expected\"\n+            \".*first two batches\",\n+        ):\n+            next(it)\n+\n     @parameterized.named_parameters(\n         named_product(generator_type=[\"tf\", \"jax\", \"scipy\"])\n     )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#510d406be59048e411b6fd3b94a15463d8d9f5db", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 7 | Files Changed: 3 | Hunks: 7 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 57 | Churn Cumulative: 237 | Contributors (this commit): 5 | Commits (past 90d): 13 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -87,9 +87,7 @@ def get(identifier):\n \n     if identifier is None:\n         return dtype_policy.dtype_policy()\n-    if isinstance(\n-        identifier, (DTypePolicy, FloatDTypePolicy, QuantizedDTypePolicy)\n-    ):\n+    if isinstance(identifier, DTypePolicy):\n         return identifier\n     if isinstance(identifier, dict):\n         return deserialize(identifier)\n\n@@ -103,10 +103,17 @@ class Operation:\n         # Generate a config to be returned by default by `get_config()`.\n         arg_names = inspect.getfullargspec(cls.__init__).args\n         kwargs.update(dict(zip(arg_names[1 : len(args) + 1], args)))\n-        if \"dtype\" in kwargs and isinstance(\n-            kwargs[\"dtype\"], dtype_policies.DTypePolicy\n-        ):\n-            kwargs[\"dtype\"] = kwargs[\"dtype\"].get_config()\n+\n+        # Explicitly serialize `dtype` to support auto_config\n+        dtype = kwargs.get(\"dtype\", None)\n+        if dtype is not None and isinstance(dtype, dtype_policies.DTypePolicy):\n+            # For backward compatibility, we use a str (`name`) for\n+            # `FloatDTypePolicy`\n+            if not dtype.is_quantized:\n+                kwargs[\"dtype\"] = dtype.name\n+            # Otherwise, use `dtype_policies.serialize`\n+            else:\n+                kwargs[\"dtype\"] = dtype_policies.serialize(dtype)\n \n         # For safety, we only rely on auto-configs for a small set of\n         # serializable types.\n@@ -198,12 +205,22 @@ class Operation:\n         This method is the reverse of `get_config`, capable of instantiating the\n         same operation from the config dictionary.\n \n+        Note: If you override this method, you might receive a serialized dtype\n+        config, which is a `dict`. You can deserialize it as follows:\n+\n+        ```python\n+        if \"dtype\" in config and isinstance(config[\"dtype\"], dict):\n+            policy = dtype_policies.deserialize(config[\"dtype\"])\n+        ```\n+\n         Args:\n             config: A Python dictionary, typically the output of `get_config`.\n \n         Returns:\n             An operation instance.\n         \"\"\"\n+        # Explicitly deserialize dtype config if needed. This enables users to\n+        # directly interact with the instance of `DTypePolicy`.\n         if \"dtype\" in config and isinstance(config[\"dtype\"], dict):\n             config = config.copy()\n             config[\"dtype\"] = dtype_policies.deserialize(config[\"dtype\"])\n\n@@ -46,6 +46,11 @@ class OpWithCustomConstructor(operation.Operation):\n \n class OpWithCustomDtype(operation.Operation):\n     def __init__(self, dtype):\n+        if not isinstance(dtype, (str, dtype_policies.DTypePolicy)):\n+            raise AssertionError(\n+                \"`dtype` must be a instance of `DTypePolicy` or str. \"\n+                f\"Received: dtype={dtype} of type {type(dtype)}\"\n+            )\n         super().__init__(dtype=dtype)\n \n     def call(self, x):\n@@ -174,9 +179,32 @@ class OperationTest(testing.TestCase):\n             OpWithMultipleOutputs(name=\"test/op\")\n \n     def test_dtype(self):\n+        # Test dtype argument\n         op = OpWithCustomDtype(dtype=\"bfloat16\")\n         self.assertEqual(op._dtype_policy.name, \"bfloat16\")\n \n         policy = dtype_policies.DTypePolicy(\"mixed_bfloat16\")\n         op = OpWithCustomDtype(dtype=policy)\n         self.assertEqual(op._dtype_policy.name, \"mixed_bfloat16\")\n+\n+        # Test dtype config to ensure it remains unchanged\n+        config = op.get_config()\n+        copied_config = config.copy()\n+        OpWithCustomDtype.from_config(config)\n+        self.assertEqual(config, copied_config)\n+\n+        # Test floating dtype serialization\n+        op = OpWithCustomDtype(dtype=\"mixed_bfloat16\")\n+        config = op.get_config()\n+        self.assertEqual(config[\"dtype\"], \"mixed_bfloat16\")  # A plain string\n+        revived_op = OpWithCustomDtype.from_config(config)\n+        self.assertEqual(op._dtype_policy.name, revived_op._dtype_policy.name)\n+\n+        # Test quantized dtype serialization\n+        policy = dtype_policies.QuantizedDTypePolicy(\"int8\", \"bfloat16\")\n+        op = OpWithCustomDtype(policy)\n+        self.assertEqual(op._dtype_policy.name, \"int8_from_bfloat16\")\n+        config = op.get_config()  # A serialized config\n+        self.assertEqual(config[\"dtype\"], dtype_policies.serialize(policy))\n+        revived_op = OpWithCustomDtype.from_config(config)\n+        self.assertEqual(op._dtype_policy.name, revived_op._dtype_policy.name)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a243d91e43b4c43fe8d184b541b608b6ddd80f71", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 23 | Churn Cumulative: 45 | Contributors (this commit): 3 | Commits (past 90d): 5 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -382,7 +382,7 @@ class StringLookup(IndexLookup):\n         return {**base_config, **config}\n \n     def call(self, inputs):\n-        if isinstance(inputs, (tf.Tensor, tf.RaggedTensor)):\n+        if isinstance(inputs, (tf.Tensor, tf.RaggedTensor, tf.SparseTensor)):\n             tf_inputs = True\n         else:\n             tf_inputs = False\n\n@@ -1,4 +1,5 @@\n import numpy as np\n+import pytest\n from tensorflow import data as tf_data\n \n from keras.src import backend\n@@ -38,6 +39,26 @@ class StringLookupTest(testing.TestCase):\n         self.assertTrue(backend.is_tensor(output))\n         self.assertAllClose(output, np.array([2, 3, 0]))\n \n+    @pytest.mark.skipif(\n+        not backend.backend() == \"tensorflow\", reason=\"Requires tf.SparseTensor\"\n+    )\n+    def test_sparse_inputs(self):\n+        import tensorflow as tf\n+\n+        layer = layers.StringLookup(\n+            output_mode=\"int\",\n+            vocabulary=[\"a\", \"b\", \"c\"],\n+        )\n+        input_data = tf.SparseTensor(\n+            indices=[[0, 0], [1, 1], [2, 2]],\n+            values=[\"b\", \"c\", \"d\"],\n+            dense_shape=(3, 3),\n+        )\n+        output = layer(input_data)\n+        self.assertIsInstance(output, tf.SparseTensor)\n+        self.assertAllClose(output, np.array([[2, 0, 0], [0, 3, 0], [0, 0, 0]]))\n+        self.assertAllClose(output.values, np.array([2, 3, 0]))\n+\n     def test_set_vocabulary(self):\n         layer = layers.StringLookup(\n             output_mode=\"int\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d59cccaae62308799d636480078bc5e02dffe9f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 19 | Files Changed: 2 | Hunks: 11 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 41 | Churn Cumulative: 1088 | Contributors (this commit): 5 | Commits (past 90d): 30 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -1310,6 +1310,10 @@ def less_equal(x1, x2):\n def linspace(\n     start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0\n ):\n+    if num < 0:\n+        raise ValueError(\n+            f\"`num` must be a non-negative integer. Received: num={num}\"\n+        )\n     if dtype is None:\n         dtypes_to_resolve = [\n             getattr(start, \"dtype\", type(start)),\n@@ -1321,19 +1325,15 @@ def linspace(\n         dtype = standardize_dtype(dtype)\n     start = convert_to_tensor(start, dtype=dtype)\n     stop = convert_to_tensor(stop, dtype=dtype)\n-    if num < 0:\n-        raise ValueError(\n-            f\"`num` must be a non-negative integer. Received: num={num}\"\n-        )\n-    step = tf.convert_to_tensor(np.nan)\n+    step = convert_to_tensor(np.nan)\n     if endpoint:\n         result = tf.linspace(start, stop, num, axis=axis)\n         if num > 1:\n-            step = (stop - start) / (num - 1)\n+            step = (stop - start) / (tf.cast(num, dtype) - 1)\n     else:\n         # tf.linspace doesn't support endpoint=False, so we manually handle it\n         if num > 0:\n-            step = (stop - start) / num\n+            step = (stop - start) / tf.cast(num, dtype)\n         if num > 1:\n             new_stop = tf.cast(stop, step.dtype) - step\n             start = tf.cast(start, new_stop.dtype)\n\n@@ -2488,17 +2488,13 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase, parameterized.TestCase):\n             np.linspace(start, stop, 5, retstep=True)[0],\n         )\n         self.assertAllClose(\n-            backend.convert_to_numpy(\n-                knp.linspace(start, stop, 5, endpoint=False, retstep=True)[0]\n-            ),\n+            knp.linspace(start, stop, 5, endpoint=False, retstep=True)[0],\n             np.linspace(start, stop, 5, endpoint=False, retstep=True)[0],\n         )\n         self.assertAllClose(\n-            backend.convert_to_numpy(\n             knp.linspace(\n                 start, stop, 5, endpoint=False, retstep=True, dtype=\"int32\"\n-                )[0]\n-            ),\n+            )[0],\n             np.linspace(\n                 start, stop, 5, endpoint=False, retstep=True, dtype=\"int32\"\n             )[0],\n@@ -2509,22 +2505,29 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase, parameterized.TestCase):\n             np.linspace(start, stop, 5, retstep=True)[0],\n         )\n         self.assertAllClose(\n-            backend.convert_to_numpy(\n-                knp.Linspace(5, endpoint=False, retstep=True)(start, stop)[0]\n-            ),\n+            knp.Linspace(5, endpoint=False, retstep=True)(start, stop)[0],\n             np.linspace(start, stop, 5, endpoint=False, retstep=True)[0],\n         )\n         self.assertAllClose(\n-            backend.convert_to_numpy(\n             knp.Linspace(5, endpoint=False, retstep=True, dtype=\"int32\")(\n                 start, stop\n-                )[0]\n-            ),\n+            )[0],\n             np.linspace(\n                 start, stop, 5, endpoint=False, retstep=True, dtype=\"int32\"\n             )[0],\n         )\n \n+        # Test `num` as a tensor\n+        # https://github.com/keras-team/keras/issues/19772\n+        self.assertAllClose(\n+            knp.linspace(0, 10, backend.convert_to_tensor(5)),\n+            np.linspace(0, 10, 5),\n+        )\n+        self.assertAllClose(\n+            knp.linspace(0, 10, backend.convert_to_tensor(5), endpoint=False),\n+            np.linspace(0, 10, 5, endpoint=False),\n+        )\n+\n     def test_logical_and(self):\n         x = np.array([[True, False], [True, True]])\n         y = np.array([[False, False], [True, False]])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0eeba147fa099409cdb232cca546b71d70b6d64d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 405 | Lines Deleted: 215 | Files Changed: 6 | Hunks: 112 | Methods Changed: 64 | Complexity Δ (Sum/Max): 13/15 | Churn Δ: 620 | Churn Cumulative: 1221 | Contributors (this commit): 4 | Commits (past 90d): 21 | Contributors (cumulative): 19 | DMM Complexity: 0.48360655737704916\n\nDIFF:\n@@ -3,6 +3,7 @@ import functools\n import jax\n import jax.numpy as jnp\n \n+from keras.src import backend\n from keras.src.backend.jax.core import convert_to_tensor\n \n RESIZE_INTERPOLATIONS = (\n@@ -14,7 +15,8 @@ RESIZE_INTERPOLATIONS = (\n )\n \n \n-def rgb_to_grayscale(image, data_format=\"channels_last\"):\n+def rgb_to_grayscale(image, data_format=None):\n+    data_format = backend.standardize_data_format(data_format)\n     if data_format == \"channels_first\":\n         if len(image.shape) == 4:\n             image = jnp.transpose(image, (0, 2, 3, 1))\n@@ -46,8 +48,9 @@ def resize(\n     pad_to_aspect_ratio=False,\n     fill_mode=\"constant\",\n     fill_value=0.0,\n-    data_format=\"channels_last\",\n+    data_format=None,\n ):\n+    data_format = backend.standardize_data_format(data_format)\n     if interpolation not in RESIZE_INTERPOLATIONS:\n         raise ValueError(\n             \"Invalid value for argument `interpolation`. Expected of one \"\n@@ -297,8 +300,9 @@ def affine_transform(\n     interpolation=\"bilinear\",\n     fill_mode=\"constant\",\n     fill_value=0,\n-    data_format=\"channels_last\",\n+    data_format=None,\n ):\n+    data_format = backend.standardize_data_format(data_format)\n     if interpolation not in AFFINE_TRANSFORM_INTERPOLATIONS.keys():\n         raise ValueError(\n             \"Invalid value for argument `interpolation`. Expected of one \"\n\n@@ -1,6 +1,7 @@\n import jax\n import numpy as np\n \n+from keras.src import backend\n from keras.src.backend.numpy.core import convert_to_tensor\n from keras.src.utils.module_utils import scipy\n \n@@ -13,7 +14,8 @@ RESIZE_INTERPOLATIONS = (\n )\n \n \n-def rgb_to_grayscale(image, data_format=\"channels_last\"):\n+def rgb_to_grayscale(image, data_format=None):\n+    data_format = backend.standardize_data_format(data_format)\n     if data_format == \"channels_first\":\n         if len(image.shape) == 4:\n             image = np.transpose(image, (0, 2, 3, 1))\n@@ -45,8 +47,9 @@ def resize(\n     pad_to_aspect_ratio=False,\n     fill_mode=\"constant\",\n     fill_value=0.0,\n-    data_format=\"channels_last\",\n+    data_format=None,\n ):\n+    data_format = backend.standardize_data_format(data_format)\n     if interpolation not in RESIZE_INTERPOLATIONS:\n         raise ValueError(\n             \"Invalid value for argument `interpolation`. Expected of one \"\n@@ -231,8 +234,9 @@ def affine_transform(\n     interpolation=\"bilinear\",\n     fill_mode=\"constant\",\n     fill_value=0,\n-    data_format=\"channels_last\",\n+    data_format=None,\n ):\n+    data_format = backend.standardize_data_format(data_format)\n     if interpolation not in AFFINE_TRANSFORM_INTERPOLATIONS.keys():\n         raise ValueError(\n             \"Invalid value for argument `interpolation`. Expected of one \"\n\n@@ -4,6 +4,7 @@ import operator\n \n import tensorflow as tf\n \n+from keras.src import backend\n from keras.src.backend.tensorflow.core import convert_to_tensor\n \n RESIZE_INTERPOLATIONS = (\n@@ -15,7 +16,8 @@ RESIZE_INTERPOLATIONS = (\n )\n \n \n-def rgb_to_grayscale(image, data_format=\"channels_last\"):\n+def rgb_to_grayscale(image, data_format=None):\n+    data_format = backend.standardize_data_format(data_format)\n     if data_format == \"channels_first\":\n         if len(image.shape) == 4:\n             image = tf.transpose(image, (0, 2, 3, 1))\n@@ -45,8 +47,9 @@ def resize(\n     pad_to_aspect_ratio=False,\n     fill_mode=\"constant\",\n     fill_value=0.0,\n-    data_format=\"channels_last\",\n+    data_format=None,\n ):\n+    data_format = backend.standardize_data_format(data_format)\n     if interpolation not in RESIZE_INTERPOLATIONS:\n         raise ValueError(\n             \"Invalid value for argument `interpolation`. Expected of one \"\n@@ -255,8 +258,9 @@ def affine_transform(\n     interpolation=\"bilinear\",\n     fill_mode=\"constant\",\n     fill_value=0,\n-    data_format=\"channels_last\",\n+    data_format=None,\n ):\n+    data_format = backend.standardize_data_format(data_format)\n     if interpolation not in AFFINE_TRANSFORM_INTERPOLATIONS:\n         raise ValueError(\n             \"Invalid value for argument `interpolation`. Expected of one \"\n\n@@ -4,6 +4,7 @@ import operator\n \n import torch\n \n+from keras.src import backend\n from keras.src.backend.torch.core import convert_to_tensor\n \n RESIZE_INTERPOLATIONS = {}  # populated after torchvision import\n@@ -14,7 +15,8 @@ UNSUPPORTED_INTERPOLATIONS = (\n )\n \n \n-def rgb_to_grayscale(image, data_format=\"channel_last\"):\n+def rgb_to_grayscale(image, data_format=None):\n+    data_format = backend.standardize_data_format(data_format)\n     try:\n         import torchvision\n     except:\n@@ -55,8 +57,9 @@ def resize(\n     pad_to_aspect_ratio=False,\n     fill_mode=\"constant\",\n     fill_value=0.0,\n-    data_format=\"channels_last\",\n+    data_format=None,\n ):\n+    data_format = backend.standardize_data_format(data_format)\n     try:\n         import torchvision\n         from torchvision.transforms import InterpolationMode as im\n@@ -216,8 +219,9 @@ def affine_transform(\n     interpolation=\"bilinear\",\n     fill_mode=\"constant\",\n     fill_value=0,\n-    data_format=\"channels_last\",\n+    data_format=None,\n ):\n+    data_format = backend.standardize_data_format(data_format)\n     if interpolation not in AFFINE_TRANSFORM_INTERPOLATIONS.keys():\n         raise ValueError(\n             \"Invalid value for argument `interpolation`. Expected of one \"\n\n@@ -8,17 +8,13 @@ from keras.src.ops.operation_utils import compute_conv_output_shape\n \n \n class RGBToGrayscale(Operation):\n-    def __init__(\n-        self,\n-        data_format=\"channels_last\",\n-    ):\n+    def __init__(self, data_format=None):\n         super().__init__()\n-        self.data_format = data_format\n+        self.data_format = backend.standardize_data_format(data_format)\n \n     def call(self, image):\n         return backend.image.rgb_to_grayscale(\n-            image,\n-            data_format=self.data_format,\n+            image, data_format=self.data_format\n         )\n \n     def compute_output_spec(self, image):\n@@ -52,10 +48,7 @@ class RGBToGrayscale(Operation):\n \n \n @keras_export(\"keras.ops.image.rgb_to_grayscale\")\n-def rgb_to_grayscale(\n-    image,\n-    data_format=\"channels_last\",\n-):\n+def rgb_to_grayscale(image, data_format=None):\n     \"\"\"Convert RGB images to grayscale.\n \n     This function converts RGB images to grayscale images. It supports both\n@@ -70,7 +63,8 @@ def rgb_to_grayscale(\n             `\"channels_last\"` corresponds to inputs with shape\n             `(batch, height, width, channels)`, while `\"channels_first\"`\n             corresponds to inputs with shape `(batch, channels, height, width)`.\n-            Defaults to `\"channels_last\"`.\n+            If not specified, the value will be interpreted by\n+            `keras.config.image_data_format`. Defaults to `None`.\n \n     Returns:\n         Grayscale image or batch of grayscale images.\n@@ -114,17 +108,17 @@ class Resize(Operation):\n         pad_to_aspect_ratio=False,\n         fill_mode=\"constant\",\n         fill_value=0.0,\n-        data_format=\"channels_last\",\n+        data_format=None,\n     ):\n         super().__init__()\n         self.size = tuple(size)\n         self.interpolation = interpolation\n         self.antialias = antialias\n-        self.data_format = data_format\n         self.crop_to_aspect_ratio = crop_to_aspect_ratio\n         self.pad_to_aspect_ratio = pad_to_aspect_ratio\n         self.fill_mode = fill_mode\n         self.fill_value = fill_value\n+        self.data_format = backend.standardize_data_format(data_format)\n \n     def call(self, image):\n         return backend.image.resize(\n@@ -172,7 +166,7 @@ def resize(\n     pad_to_aspect_ratio=False,\n     fill_mode=\"constant\",\n     fill_value=0.0,\n-    data_format=\"channels_last\",\n+    data_format=None,\n ):\n     \"\"\"Resize images to size using the specified interpolation method.\n \n@@ -199,14 +193,13 @@ def resize(\n             supported at this time\n             (fill with constant value, equal to `fill_value`).\n         fill_value: Float. Padding value to use when `pad_to_aspect_ratio=True`.\n-        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n-            The ordering of the dimensions in the inputs. `\"channels_last\"`\n-            corresponds to inputs with shape `(batch, height, width, channels)`\n-            while `\"channels_first\"` corresponds to inputs with shape\n-            `(batch, channels, height, weight)`. It defaults to the\n-            `image_data_format` value found in your Keras config file at\n-            `~/.keras/keras.json`. If you never set it, then it will be\n-            `\"channels_last\"`.\n+        data_format: A string specifying the data format of the input tensor.\n+            It can be either `\"channels_last\"` or `\"channels_first\"`.\n+            `\"channels_last\"` corresponds to inputs with shape\n+            `(batch, height, width, channels)`, while `\"channels_first\"`\n+            corresponds to inputs with shape `(batch, channels, height, width)`.\n+            If not specified, the value will be interpreted by\n+            `keras.config.image_data_format`. Defaults to `None`.\n \n     Returns:\n         Resized image or batch of images.\n@@ -275,13 +268,13 @@ class AffineTransform(Operation):\n         interpolation=\"bilinear\",\n         fill_mode=\"constant\",\n         fill_value=0,\n-        data_format=\"channels_last\",\n+        data_format=None,\n     ):\n         super().__init__()\n         self.interpolation = interpolation\n         self.fill_mode = fill_mode\n         self.fill_value = fill_value\n-        self.data_format = data_format\n+        self.data_format = backend.standardize_data_format(data_format)\n \n     def call(self, image, transform):\n         return backend.image.affine_transform(\n@@ -316,7 +309,7 @@ def affine_transform(\n     interpolation=\"bilinear\",\n     fill_mode=\"constant\",\n     fill_value=0,\n-    data_format=\"channels_last\",\n+    data_format=None,\n ):\n     \"\"\"Applies the given transform(s) to the image(s).\n \n@@ -350,14 +343,13 @@ def affine_transform(\n                 The input is extended by the nearest pixel.\n         fill_value: Value used for points outside the boundaries of the input if\n             `fill_mode=\"constant\"`. Defaults to `0`.\n-        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n-            The ordering of the dimensions in the inputs. `\"channels_last\"`\n-            corresponds to inputs with shape `(batch, height, width, channels)`\n-            while `\"channels_first\"` corresponds to inputs with shape\n-            `(batch, channels, height, weight)`. It defaults to the\n-            `image_data_format` value found in your Keras config file at\n-            `~/.keras/keras.json`. If you never set it, then it will be\n-            `\"channels_last\"`.\n+        data_format: A string specifying the data format of the input tensor.\n+            It can be either `\"channels_last\"` or `\"channels_first\"`.\n+            `\"channels_last\"` corresponds to inputs with shape\n+            `(batch, height, width, channels)`, while `\"channels_first\"`\n+            corresponds to inputs with shape `(batch, channels, height, width)`.\n+            If not specified, the value will be interpreted by\n+            `keras.config.image_data_format`. Defaults to `None`.\n \n     Returns:\n         Applied affine transform image or batch of images.\n@@ -417,7 +409,7 @@ class ExtractPatches(Operation):\n         strides=None,\n         dilation_rate=1,\n         padding=\"valid\",\n-        data_format=\"channels_last\",\n+        data_format=None,\n     ):\n         super().__init__()\n         if isinstance(size, int):\n@@ -426,7 +418,7 @@ class ExtractPatches(Operation):\n         self.strides = strides\n         self.dilation_rate = dilation_rate\n         self.padding = padding\n-        self.data_format = data_format\n+        self.data_format = backend.standardize_data_format(data_format)\n \n     def call(self, image):\n         return _extract_patches(\n@@ -471,7 +463,7 @@ def extract_patches(\n     strides=None,\n     dilation_rate=1,\n     padding=\"valid\",\n-    data_format=\"channels_last\",\n+    data_format=None,\n ):\n     \"\"\"Extracts patches from the image(s).\n \n@@ -485,14 +477,13 @@ def extract_patches(\n             strides must be 1. NOTE: `strides > 1` is not supported in\n             conjunction with `dilation_rate > 1`\n         padding: The type of padding algorithm to use: `\"same\"` or `\"valid\"`.\n-        data_format: string, either `\"channels_last\"` or `\"channels_first\"`.\n-            The ordering of the dimensions in the inputs. `\"channels_last\"`\n-            corresponds to inputs with shape `(batch, height, width, channels)`\n-            while `\"channels_first\"` corresponds to inputs with shape\n-            `(batch, channels, height, weight)`. It defaults to the\n-            `image_data_format` value found in your Keras config file at\n-            `~/.keras/keras.json`. If you never set it, then it will be\n-            `\"channels_last\"`.\n+        data_format: A string specifying the data format of the input tensor.\n+            It can be either `\"channels_last\"` or `\"channels_first\"`.\n+            `\"channels_last\"` corresponds to inputs with shape\n+            `(batch, height, width, channels)`, while `\"channels_first\"`\n+            corresponds to inputs with shape `(batch, channels, height, width)`.\n+            If not specified, the value will be interpreted by\n+            `keras.config.image_data_format`. Defaults to `None`.\n \n     Returns:\n         Extracted patches 3D (if not batched) or 4D (if batched)\n@@ -530,7 +521,7 @@ def _extract_patches(\n     strides=None,\n     dilation_rate=1,\n     padding=\"valid\",\n-    data_format=\"channels_last\",\n+    data_format=None,\n ):\n     if isinstance(size, int):\n         patch_h = patch_w = size\n@@ -541,6 +532,7 @@ def _extract_patches(\n             \"Invalid `size` argument. Expected an \"\n             f\"int or a tuple of length 2. Received: size={size}\"\n         )\n+    data_format = backend.standardize_data_format(data_format)\n     if data_format == \"channels_last\":\n         channels_in = image.shape[-1]\n     elif data_format == \"channels_first\":\n@@ -665,6 +657,7 @@ class PadImages(Operation):\n         right_padding=None,\n         target_height=None,\n         target_width=None,\n+        data_format=None,\n     ):\n         super().__init__()\n         self.top_padding = top_padding\n@@ -673,6 +666,7 @@ class PadImages(Operation):\n         self.right_padding = right_padding\n         self.target_height = target_height\n         self.target_width = target_width\n+        self.data_format = backend.standardize_data_format(data_format)\n \n     def call(self, images):\n         return _pad_images(\n@@ -683,36 +677,27 @@ class PadImages(Operation):\n             self.right_padding,\n             self.target_height,\n             self.target_width,\n+            self.data_format,\n         )\n \n     def compute_output_spec(self, images):\n-        images_shape = ops.shape(images)\n-        if self.target_height is None:\n-            height_axis = 0 if len(images_shape) == 3 else 1\n-            self.target_height = (\n-                self.top_padding\n-                + images_shape[height_axis]\n-                + self.bottom_padding\n-            )\n-        if self.target_width is None:\n-            width_axis = 0 if len(images_shape) == 3 else 2\n-            self.target_width = (\n-                self.left_padding\n-                + images_shape[width_axis]\n-                + self.right_padding\n-            )\n-        out_shape = (\n-            images_shape[0],\n-            self.target_height,\n-            self.target_width,\n-            images_shape[-1],\n-        )\n-        if len(images_shape) == 3:\n-            out_shape = out_shape[1:]\n-        return KerasTensor(\n-            shape=out_shape,\n-            dtype=images.dtype,\n-        )\n+        images_shape = list(images.shape)\n+\n+        if self.data_format == \"channels_last\":\n+            height_axis, width_axis = -3, -2\n+            height, width = images_shape[height_axis], images_shape[width_axis]\n+        else:\n+            height_axis, width_axis = -2, -1\n+            height, width = images_shape[height_axis], images_shape[width_axis]\n+\n+        if self.target_height is None and height is not None:\n+            self.target_height = self.top_padding + height + self.bottom_padding\n+        if self.target_width is None and width is not None:\n+            self.target_width = self.left_padding + width + self.right_padding\n+\n+        images_shape[height_axis] = self.target_height\n+        images_shape[width_axis] = self.target_width\n+        return KerasTensor(shape=images_shape, dtype=images.dtype)\n \n \n @keras_export(\"keras.ops.image.pad_images\")\n@@ -720,10 +705,11 @@ def pad_images(\n     images,\n     top_padding=None,\n     left_padding=None,\n-    target_height=None,\n-    target_width=None,\n     bottom_padding=None,\n     right_padding=None,\n+    target_height=None,\n+    target_width=None,\n+    data_format=None,\n ):\n     \"\"\"Pad `images` with zeros to the specified `height` and `width`.\n \n@@ -736,12 +722,16 @@ def pad_images(\n         right_padding: Number of columns of zeros to add on the right.\n         target_height: Height of output images.\n         target_width: Width of output images.\n+        data_format: A string specifying the data format of the input tensor.\n+            It can be either `\"channels_last\"` or `\"channels_first\"`.\n+            `\"channels_last\"` corresponds to inputs with shape\n+            `(batch, height, width, channels)`, while `\"channels_first\"`\n+            corresponds to inputs with shape `(batch, channels, height, width)`.\n+            If not specified, the value will be interpreted by\n+            `keras.config.image_data_format`. Defaults to `None`.\n \n     Returns:\n-        If `images` were 4D, a 4D float Tensor of shape\n-            `(batch, target_height, target_width, channels)`\n-        If `images` were 3D, a 3D float Tensor of shape\n-            `(target_height, target_width, channels)`\n+        Padded image or batch of images.\n \n     Example:\n \n@@ -767,6 +757,7 @@ def pad_images(\n             right_padding,\n             target_height,\n             target_width,\n+            data_format,\n         ).symbolic_call(images)\n \n     return _pad_images(\n@@ -777,6 +768,7 @@ def pad_images(\n         right_padding,\n         target_height,\n         target_width,\n+        data_format,\n     )\n \n \n@@ -788,22 +780,19 @@ def _pad_images(\n     right_padding,\n     target_height,\n     target_width,\n+    data_format=None,\n ):\n+    data_format = backend.standardize_data_format(data_format)\n     images = backend.convert_to_tensor(images)\n-    is_batch = True\n     images_shape = ops.shape(images)\n-    if len(images_shape) == 3:\n-        is_batch = False\n-        images = backend.numpy.expand_dims(images, 0)\n-    elif len(images_shape) != 4:\n+\n+    # Check\n+    if len(images_shape) not in (3, 4):\n         raise ValueError(\n             f\"Invalid shape for argument `images`: \"\n             \"it must have rank 3 or 4. \"\n             f\"Received: images.shape={images_shape}\"\n         )\n-\n-    batch, height, width, depth = ops.shape(images)\n-\n     if [top_padding, bottom_padding, target_height].count(None) != 1:\n         raise ValueError(\n             \"Must specify exactly two of \"\n@@ -821,6 +810,13 @@ def _pad_images(\n             f\"target_width={target_width}\"\n         )\n \n+    is_batch = False if len(images_shape) == 3 else True\n+    if data_format == \"channels_last\":\n+        height, width = images_shape[-3], images_shape[-2]\n+    else:\n+        height, width = images_shape[-2], images_shape[-1]\n+\n+    # Infer padding\n     if top_padding is None:\n         top_padding = target_height - bottom_padding - height\n     if bottom_padding is None:\n@@ -832,7 +828,7 @@ def _pad_images(\n \n     if top_padding < 0:\n         raise ValueError(\n-            \"top_padding must be >= 0. \" f\"Received: top_padding={top_padding}\"\n+            f\"top_padding must be >= 0. Received: top_padding={top_padding}\"\n         )\n     if left_padding < 0:\n         raise ValueError(\n@@ -850,33 +846,17 @@ def _pad_images(\n             f\"Received: bottom_padding={bottom_padding}\"\n         )\n \n-    paddings = backend.numpy.reshape(\n-        backend.numpy.stack(\n-            [\n-                0,\n-                0,\n-                top_padding,\n-                bottom_padding,\n-                left_padding,\n-                right_padding,\n-                0,\n-                0,\n-            ]\n-        ),\n-        [4, 2],\n-    )\n-    padded = backend.numpy.pad(images, paddings)\n+    # Compute pad_width\n+    pad_width = [[top_padding, bottom_padding], [left_padding, right_padding]]\n+    if data_format == \"channels_last\":\n+        pad_width = pad_width + [[0, 0]]\n+    else:\n+        pad_width = [[0, 0]] + pad_width\n+    if is_batch:\n+        pad_width = [[0, 0]] + pad_width\n \n-    if target_height is None:\n-        target_height = top_padding + height + bottom_padding\n-    if target_width is None:\n-        target_width = left_padding + width + right_padding\n-    padded_shape = [batch, target_height, target_width, depth]\n-    padded = backend.numpy.reshape(padded, padded_shape)\n-\n-    if not is_batch:\n-        padded = backend.numpy.squeeze(padded, axis=[0])\n-    return padded\n+    padded_images = backend.numpy.pad(images, pad_width)\n+    return padded_images\n \n \n class CropImages(Operation):\n@@ -888,6 +868,7 @@ class CropImages(Operation):\n         right_cropping,\n         target_height,\n         target_width,\n+        data_format=None,\n     ):\n         super().__init__()\n         self.top_cropping = top_cropping\n@@ -896,6 +877,7 @@ class CropImages(Operation):\n         self.right_cropping = right_cropping\n         self.target_height = target_height\n         self.target_width = target_width\n+        self.data_format = backend.standardize_data_format(data_format)\n \n     def call(self, images):\n         return _crop_images(\n@@ -906,42 +888,44 @@ class CropImages(Operation):\n             self.right_cropping,\n             self.target_height,\n             self.target_width,\n+            self.data_format,\n         )\n \n     def compute_output_spec(self, images):\n-        images_shape = ops.shape(images)\n-        out_shape = (\n-            images_shape[0],\n-            self.target_height,\n-            self.target_width,\n-            images_shape[-1],\n+        images_shape = list(images.shape)\n+\n+        if self.data_format == \"channels_last\":\n+            height_axis, width_axis = -3, -2\n+            height, width = images_shape[height_axis], images_shape[width_axis]\n+        else:\n+            height_axis, width_axis = -2, -1\n+            height, width = images_shape[height_axis], images_shape[width_axis]\n+\n+        if height is None and self.target_height is None:\n+            raise ValueError(\n+                \"When the height of the images is unknown, `target_height` \"\n+                \"must be specified.\"\n+                f\"Received images.shape={images_shape} and \"\n+                f\"target_height={self.target_height}\"\n             )\n+        if width is None and self.target_width is None:\n+            raise ValueError(\n+                \"When the width of the images is unknown, `target_width` \"\n+                \"must be specified.\"\n+                f\"Received images.shape={images_shape} and \"\n+                f\"target_width={self.target_width}\"\n+            )\n+\n         if self.target_height is None:\n-            height_axis = 0 if len(images_shape) == 3 else 1\n             self.target_height = (\n-                self.top_cropping\n-                - images_shape[height_axis]\n-                - self.bottom_cropping\n+                height - self.top_cropping - self.bottom_cropping\n             )\n         if self.target_width is None:\n-            width_axis = 0 if len(images_shape) == 3 else 2\n-            self.target_width = (\n-                self.left_cropping\n-                - images_shape[width_axis]\n-                - self.right_cropping\n-            )\n-        out_shape = (\n-            images_shape[0],\n-            self.target_height,\n-            self.target_width,\n-            images_shape[-1],\n-        )\n-        if len(images_shape) == 3:\n-            out_shape = out_shape[1:]\n-        return KerasTensor(\n-            shape=out_shape,\n-            dtype=images.dtype,\n-        )\n+            self.target_width = width - self.left_cropping - self.right_cropping\n+\n+        images_shape[height_axis] = self.target_height\n+        images_shape[width_axis] = self.target_width\n+        return KerasTensor(shape=images_shape, dtype=images.dtype)\n \n \n @keras_export(\"keras.ops.image.crop_images\")\n@@ -949,10 +933,11 @@ def crop_images(\n     images,\n     top_cropping=None,\n     left_cropping=None,\n-    target_height=None,\n-    target_width=None,\n     bottom_cropping=None,\n     right_cropping=None,\n+    target_height=None,\n+    target_width=None,\n+    data_format=None,\n ):\n     \"\"\"Crop `images` to a specified `height` and `width`.\n \n@@ -965,12 +950,16 @@ def crop_images(\n         right_cropping: Number of columns to crop from the right.\n         target_height: Height of the output images.\n         target_width: Width of the output images.\n+        data_format: A string specifying the data format of the input tensor.\n+            It can be either `\"channels_last\"` or `\"channels_first\"`.\n+            `\"channels_last\"` corresponds to inputs with shape\n+            `(batch, height, width, channels)`, while `\"channels_first\"`\n+            corresponds to inputs with shape `(batch, channels, height, width)`.\n+            If not specified, the value will be interpreted by\n+            `keras.config.image_data_format`. Defaults to `None`.\n \n     Returns:\n-        If `images` were 4D, a 4D float Tensor of shape\n-            `(batch, target_height, target_width, channels)`\n-        If `images` were 3D, a 3D float Tensor of shape\n-            `(target_height, target_width, channels)`\n+        Cropped image or batch of images.\n \n     Example:\n \n@@ -992,6 +981,7 @@ def crop_images(\n             right_cropping,\n             target_height,\n             target_width,\n+            data_format,\n         ).symbolic_call(images)\n \n     return _crop_images(\n@@ -1002,6 +992,7 @@ def crop_images(\n         right_cropping,\n         target_height,\n         target_width,\n+        data_format,\n     )\n \n \n@@ -1013,22 +1004,19 @@ def _crop_images(\n     right_cropping,\n     target_height,\n     target_width,\n+    data_format=None,\n ):\n+    data_format = backend.standardize_data_format(data_format)\n     images = backend.convert_to_tensor(images)\n-    is_batch = True\n     images_shape = ops.shape(images)\n-    if len(images_shape) == 3:\n-        is_batch = False\n-        images = backend.numpy.expand_dims(images, 0)\n-    elif len(images_shape) != 4:\n+\n+    # Check\n+    if len(images_shape) not in (3, 4):\n         raise ValueError(\n             f\"Invalid shape for argument `images`: \"\n             \"it must have rank 3 or 4. \"\n             f\"Received: images.shape={images_shape}\"\n         )\n-\n-    batch, height, width, depth = ops.shape(images)\n-\n     if [top_cropping, bottom_cropping, target_height].count(None) != 1:\n         raise ValueError(\n             \"Must specify exactly two of \"\n@@ -1046,6 +1034,15 @@ def _crop_images(\n             f\"target_width={target_width}\"\n         )\n \n+    is_batch = False if len(images_shape) == 3 else True\n+    if data_format == \"channels_last\":\n+        height, width = images_shape[-3], images_shape[-2]\n+        channels = images_shape[-1]\n+    else:\n+        height, width = images_shape[-2], images_shape[-1]\n+        channels = images_shape[-3]\n+\n+    # Infer padding\n     if top_cropping is None:\n         top_cropping = height - target_height - bottom_cropping\n     if target_height is None:\n@@ -1076,26 +1073,19 @@ def _crop_images(\n             f\"Received: target_width={target_width}\"\n         )\n \n-    if isinstance(top_cropping, int) and isinstance(left_cropping, int):\n-        start_indices = [0, top_cropping, left_cropping, 0]\n+    # Compute start_indices and shape\n+    start_indices = [top_cropping, left_cropping]\n+    shape = [target_height, target_width]\n+    if data_format == \"channels_last\":\n+        start_indices = start_indices + [0]\n+        shape = shape + [channels]\n     else:\n-        start_indices = backend.numpy.stack([0, top_cropping, left_cropping, 0])\n-    if (\n-        isinstance(batch, int)\n-        and isinstance(target_height, int)\n-        and isinstance(target_width, int)\n-        and isinstance(depth, int)\n-    ):\n-        shape = [batch, target_height, target_width, depth]\n-    else:\n-        shape = backend.numpy.stack([batch, target_height, target_width, depth])\n-    cropped = ops.slice(\n-        images,\n-        start_indices,\n-        shape,\n-    )\n+        start_indices = [0] + start_indices\n+        shape = [channels] + shape\n+    if is_batch:\n+        batch_size = images_shape[0]\n+        start_indices = [0] + start_indices\n+        shape = [batch_size] + shape\n \n-    cropped = backend.numpy.reshape(cropped, shape)\n-    if not is_batch:\n-        cropped = backend.numpy.squeeze(cropped, axis=[0])\n-    return cropped\n+    cropped_images = ops.slice(images, start_indices, shape)\n+    return cropped_images\n\n@@ -13,6 +13,16 @@ from keras.src.ops import image as kimage\n \n \n class ImageOpsDynamicShapeTest(testing.TestCase):\n+    def setUp(self):\n+        # Defaults to channels_last\n+        self.data_format = backend.image_data_format()\n+        backend.set_image_data_format(\"channels_last\")\n+        return super().setUp()\n+\n+    def tearDown(self) -> None:\n+        backend.set_image_data_format(self.data_format)\n+        return super().tearDown()\n+\n     def test_rgb_to_grayscale(self):\n         x = KerasTensor([None, 20, 20, 3])\n         out = kimage.rgb_to_grayscale(x)\n@@ -56,6 +66,35 @@ class ImageOpsDynamicShapeTest(testing.TestCase):\n         out = kimage.pad_images(x, 2, 3, target_height=20, target_width=30)\n         self.assertEqual(out.shape, (20, 30, 3))\n \n+        # Test unknown shape\n+        x = KerasTensor([None, None, 3])\n+        out = kimage.pad_images(x, 2, 3, 2, 3)\n+        self.assertEqual(out.shape, (None, None, 3))\n+\n+        # Test channels_first and batched\n+        x = KerasTensor([None, 3, 15, 25])\n+        out = kimage.pad_images(\n+            x,\n+            2,\n+            3,\n+            target_height=20,\n+            target_width=30,\n+            data_format=\"channels_first\",\n+        )\n+        self.assertEqual(out.shape, (None, 3, 20, 30))\n+\n+        # Test channels_first and unbatched\n+        x = KerasTensor([3, None, None])\n+        out = kimage.pad_images(\n+            x,\n+            2,\n+            3,\n+            target_height=20,\n+            target_width=30,\n+            data_format=\"channels_first\",\n+        )\n+        self.assertEqual(out.shape, (3, 20, 30))\n+\n     def test_crop_images(self):\n         x = KerasTensor([None, 15, 25, 3])\n         out = kimage.crop_images(x, 2, 3, target_height=10, target_width=20)\n@@ -65,8 +104,42 @@ class ImageOpsDynamicShapeTest(testing.TestCase):\n         out = kimage.crop_images(x, 2, 3, target_height=10, target_width=20)\n         self.assertEqual(out.shape, (10, 20, 3))\n \n+        # Test channels_first and batched\n+        x = KerasTensor([None, 3, 15, 25])\n+        out = kimage.crop_images(\n+            x,\n+            2,\n+            3,\n+            target_height=10,\n+            target_width=20,\n+            data_format=\"channels_first\",\n+        )\n+        self.assertEqual(out.shape, (None, 3, 10, 20))\n+\n+        # Test channels_first and unbatched\n+        x = KerasTensor([3, None, None])\n+        out = kimage.crop_images(\n+            x,\n+            2,\n+            3,\n+            target_height=10,\n+            target_width=20,\n+            data_format=\"channels_first\",\n+        )\n+        self.assertEqual(out.shape, (3, 10, 20))\n+\n \n class ImageOpsStaticShapeTest(testing.TestCase):\n+    def setUp(self):\n+        # Defaults to channels_last\n+        self.data_format = backend.image_data_format()\n+        backend.set_image_data_format(\"channels_last\")\n+        return super().setUp()\n+\n+    def tearDown(self) -> None:\n+        backend.set_image_data_format(self.data_format)\n+        return super().tearDown()\n+\n     def test_rgb_to_grayscale(self):\n         x = KerasTensor([20, 20, 3])\n         out = kimage.rgb_to_grayscale(x)\n@@ -108,6 +181,30 @@ class ImageOpsStaticShapeTest(testing.TestCase):\n         )\n         self.assertEqual(out_batch.shape, (2, 20, 30, 3))\n \n+        # Test channels_first and unbatched\n+        x = KerasTensor([3, 15, 25])\n+        out = kimage.pad_images(\n+            x,\n+            2,\n+            3,\n+            target_height=20,\n+            target_width=30,\n+            data_format=\"channels_first\",\n+        )\n+        self.assertEqual(out.shape, (3, 20, 30))\n+\n+        # Test channels_first and batched\n+        x_batch = KerasTensor([2, 3, 15, 25])\n+        out_batch = kimage.pad_images(\n+            x_batch,\n+            2,\n+            3,\n+            target_height=20,\n+            target_width=30,\n+            data_format=\"channels_first\",\n+        )\n+        self.assertEqual(out_batch.shape, (2, 3, 20, 30))\n+\n     def test_crop_images(self):\n         x = KerasTensor([15, 25, 3])\n         out = kimage.crop_images(x, 2, 3, target_height=10, target_width=20)\n@@ -119,6 +216,30 @@ class ImageOpsStaticShapeTest(testing.TestCase):\n         )\n         self.assertEqual(out_batch.shape, (2, 10, 20, 3))\n \n+        # Test channels_first and unbatched\n+        x = KerasTensor([3, 15, 25])\n+        out = kimage.crop_images(\n+            x,\n+            2,\n+            3,\n+            target_height=10,\n+            target_width=20,\n+            data_format=\"channels_first\",\n+        )\n+        self.assertEqual(out.shape, (3, 10, 20))\n+\n+        # Test channels_first and batched\n+        x_batch = KerasTensor([2, 3, 15, 25])\n+        out_batch = kimage.crop_images(\n+            x_batch,\n+            2,\n+            3,\n+            target_height=10,\n+            target_width=20,\n+            data_format=\"channels_first\",\n+        )\n+        self.assertEqual(out_batch.shape, (2, 3, 10, 20))\n+\n \n AFFINE_TRANSFORM_INTERPOLATIONS = {  # map to order\n     \"nearest\": 0,\n@@ -197,6 +318,16 @@ def _fixed_map_coordinates(\n \n \n class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n+    def setUp(self):\n+        # Defaults to channels_last\n+        self.data_format = backend.image_data_format()\n+        backend.set_image_data_format(\"channels_last\")\n+        return super().setUp()\n+\n+    def tearDown(self) -> None:\n+        backend.set_image_data_format(self.data_format)\n+        return super().tearDown()\n+\n     @parameterized.parameters(\n         [\n             (\"channels_last\"),\n@@ -589,14 +720,16 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n \n     @parameterized.parameters(\n         [\n-            (0, 0, 3, 3, None, None),\n-            (1, 0, 4, 3, None, None),\n-            (0, 1, 3, 4, None, None),\n-            (0, 0, 4, 3, None, None),\n-            (0, 0, 3, 4, None, None),\n-            (0, 0, None, None, 0, 1),\n-            (0, 0, None, None, 1, 0),\n-            (1, 2, None, None, 3, 4),\n+            (0, 0, 3, 3, None, None, \"channels_last\"),\n+            (1, 0, 4, 3, None, None, \"channels_last\"),\n+            (0, 1, 3, 4, None, None, \"channels_last\"),\n+            (0, 0, 4, 3, None, None, \"channels_last\"),\n+            (0, 0, 3, 4, None, None, \"channels_last\"),\n+            (0, 0, None, None, 0, 1, \"channels_last\"),\n+            (0, 0, None, None, 1, 0, \"channels_last\"),\n+            (1, 2, None, None, 3, 4, \"channels_last\"),\n+            (0, 0, 3, 3, None, None, \"channels_first\"),\n+            (1, 0, 4, 3, None, None, \"channels_first\"),\n         ]\n     )\n     def test_pad_images(\n@@ -607,43 +740,56 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         target_width,\n         bottom_padding,\n         right_padding,\n+        data_format,\n     ):\n+        if data_format == \"channels_last\":\n             image = np.random.uniform(size=(3, 3, 1))\n+        else:\n+            image = np.random.uniform(size=(1, 3, 3))\n         padded_image = kimage.pad_images(\n             image,\n             top_padding,\n             left_padding,\n-            target_height,\n-            target_width,\n             bottom_padding,\n             right_padding,\n+            target_height,\n+            target_width,\n+            data_format=data_format,\n         )\n         if target_height is None:\n             target_height = image.shape[0] + top_padding + bottom_padding\n         if target_width is None:\n             target_width = image.shape[1] + left_padding + right_padding\n+\n+        # `tf.image.pad_to_bounding_box` only accepts channels_last format\n+        if data_format == \"channels_first\":\n+            image = np.transpose(image, [1, 2, 0])\n         ref_padded_image = tf.image.pad_to_bounding_box(\n             image, top_padding, left_padding, target_height, target_width\n         )\n+        ref_padded_image = ref_padded_image.numpy()\n+        if data_format == \"channels_first\":\n+            ref_padded_image = np.transpose(ref_padded_image, [2, 0, 1])\n+\n         self.assertEqual(\n             tuple(padded_image.shape), tuple(ref_padded_image.shape)\n         )\n         self.assertAllClose(\n-            ref_padded_image.numpy(),\n-            backend.convert_to_numpy(padded_image),\n-            atol=1e-5,\n+            ref_padded_image, backend.convert_to_numpy(padded_image)\n         )\n \n     @parameterized.parameters(\n         [\n-            (0, 0, 3, 3, None, None),\n-            (1, 0, 4, 3, None, None),\n-            (0, 1, 3, 4, None, None),\n-            (0, 0, 4, 3, None, None),\n-            (0, 0, 3, 4, None, None),\n-            (0, 0, None, None, 0, 1),\n-            (0, 0, None, None, 1, 0),\n-            (1, 2, None, None, 3, 4),\n+            (0, 0, 3, 3, None, None, \"channels_last\"),\n+            (1, 0, 4, 3, None, None, \"channels_last\"),\n+            (0, 1, 3, 4, None, None, \"channels_last\"),\n+            (0, 0, 4, 3, None, None, \"channels_last\"),\n+            (0, 0, 3, 4, None, None, \"channels_last\"),\n+            (0, 0, None, None, 0, 1, \"channels_last\"),\n+            (0, 0, None, None, 1, 0, \"channels_last\"),\n+            (1, 2, None, None, 3, 4, \"channels_last\"),\n+            (0, 0, 3, 3, None, None, \"channels_first\"),\n+            (1, 0, 4, 3, None, None, \"channels_first\"),\n         ]\n     )\n     def test_crop_images(\n@@ -654,31 +800,42 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         target_width,\n         bottom_cropping,\n         right_cropping,\n+        data_format,\n     ):\n+        if data_format == \"channels_last\":\n             image = np.random.uniform(size=(10, 10, 1))\n+        else:\n+            image = np.random.uniform(size=(1, 10, 10))\n         cropped_image = kimage.crop_images(\n             image,\n             top_cropping,\n             left_cropping,\n-            target_height,\n-            target_width,\n             bottom_cropping,\n             right_cropping,\n+            target_height,\n+            target_width,\n+            data_format,\n         )\n         if target_height is None:\n             target_height = image.shape[0] - top_cropping - bottom_cropping\n         if target_width is None:\n             target_width = image.shape[1] - left_cropping - right_cropping\n+\n+        # `tf.image.crop_to_bounding_box` only accepts channels_last format\n+        if data_format == \"channels_first\":\n+            image = np.transpose(image, [1, 2, 0])\n         ref_cropped_image = tf.image.crop_to_bounding_box(\n             image, top_cropping, left_cropping, target_height, target_width\n         )\n+        ref_cropped_image = ref_cropped_image.numpy()\n+        if data_format == \"channels_first\":\n+            ref_cropped_image = np.transpose(ref_cropped_image, [2, 0, 1])\n+\n         self.assertEqual(\n             tuple(cropped_image.shape), tuple(ref_cropped_image.shape)\n         )\n         self.assertAllClose(\n-            ref_cropped_image.numpy(),\n-            backend.convert_to_numpy(cropped_image),\n-            atol=1e-5,\n+            ref_cropped_image, backend.convert_to_numpy(cropped_image)\n         )\n \n     def test_rgb_to_grayscale_invalid_rank_two_tensor(self):\n@@ -834,3 +991,30 @@ class ImageOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             expected_shape,\n             \"Output shape should be correct for valid inputs\",\n         )\n+\n+\n+class ImageOpsBehaviorTests(testing.TestCase):\n+    def setUp(self):\n+        # Defaults to channels_last\n+        self.data_format = backend.image_data_format()\n+        backend.set_image_data_format(\"channels_last\")\n+        return super().setUp()\n+\n+    def tearDown(self) -> None:\n+        backend.set_image_data_format(self.data_format)\n+        return super().tearDown()\n+\n+    def test_crop_images_unknown_shape(self):\n+        # Test unknown height and target_height\n+        x = KerasTensor([None, 10, 3])\n+        with self.assertRaisesRegex(\n+            ValueError, \"When the height of the images is unknown\"\n+        ):\n+            kimage.crop_images(x, 2, 3, 4, 5)\n+\n+        # Test unknown width and target_width\n+        x = KerasTensor([10, None, 3])\n+        with self.assertRaisesRegex(\n+            ValueError, \"When the width of the images is unknown\"\n+        ):\n+            kimage.crop_images(x, 2, 3, 4, 5)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
