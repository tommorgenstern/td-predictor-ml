{"custom_id": "scrapy#72fbb687d7a35ddf52f82b165df0a842d0b653e0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 69 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -57,8 +57,6 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n     Handle redirection of requests based on response status\n     and meta-refresh html tag.\n     \"\"\"\n-    allowed_status = (301, 302, 303, 307)\n-\n     def process_response(self, request, response, spider):\n         if (request.meta.get('dont_redirect', False) or\n                 response.status in getattr(spider, 'handle_httpstatus_list', []) or\n@@ -66,7 +64,8 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n                 request.meta.get('handle_httpstatus_all', False)):\n             return response\n \n-        if 'Location' not in response.headers or response.status not in self.allowed_status:\n+        allowed_status = (301, 302, 303, 307)\n+        if 'Location' not in response.headers or response.status not in allowed_status:\n             return response\n \n         location = safe_url_string(response.headers['location'])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ecde166ee18935456bcf4c6fce89ab909bbffbaf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 26 | Files Changed: 2 | Hunks: 11 | Methods Changed: 5 | Complexity Δ (Sum/Max): -4/1 | Churn Δ: 66 | Churn Cumulative: 264 | Contributors (this commit): 6 | Commits (past 90d): 3 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -6,8 +6,8 @@ from collections import defaultdict\n from twisted.internet.defer import Deferred, DeferredList\n from twisted.python.failure import Failure\n \n-from scrapy.downloadermiddlewares.redirect import RedirectMiddleware\n from scrapy.settings import Settings\n+from scrapy.utils.datatypes import SequenceExclude\n from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.misc import arg_to_iter\n@@ -29,6 +29,7 @@ class MediaPipeline(object):\n \n     def __init__(self, download_func=None, settings=None):\n         self.download_func = download_func\n+\n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n         resolve = functools.partial(self._key_for_pipe,\n@@ -36,20 +37,12 @@ class MediaPipeline(object):\n         self.allow_redirects = settings.getbool(\n             resolve('MEDIA_ALLOW_REDIRECTS'), False\n         )\n-        self.handle_httpstatus_list = settings.getlist(\n-            resolve('MEDIA_HTTPSTATUS_LIST'), []\n-        )\n+        self._handle_statuses(self.allow_redirects)\n \n-        self.httpstatus_list = []\n-        if self.handle_httpstatus_list:\n-            self.httpstatus_list = self.handle_httpstatus_list\n-        if self.allow_redirects:\n-            if not self.httpstatus_list:\n-                self.httpstatus_list = [i for i in range(1000)\n-                                   if i not in RedirectMiddleware.allowed_status]\n-            else:\n-                self.httpstatus_list = [i for i in self.httpstatus_list\n-                                   if i not in RedirectMiddleware.allowed_status]\n+    def _handle_statuses(self, allow_redirects):\n+        self.handle_httpstatus_list = None\n+        if allow_redirects:\n+            self.handle_httpstatus_list = SequenceExclude(range(300, 400))\n \n     def _key_for_pipe(self, key, base_class_name=None,\n                       settings=None):\n@@ -117,8 +110,8 @@ class MediaPipeline(object):\n         return dfd.addBoth(lambda _: wad)  # it must return wad at last\n \n     def _modify_media_request(self, request):\n-        if self.httpstatus_list:\n-            request.meta['handle_httpstatus_list'] = self.httpstatus_list\n+        if self.handle_httpstatus_list:\n+            request.meta['handle_httpstatus_list'] = self.handle_httpstatus_list\n         else:\n             request.meta['handle_httpstatus_all'] = True\n         return request\n\n@@ -24,11 +24,12 @@ def _mocked_download_func(request, info):\n class BaseMediaPipelineTestCase(unittest.TestCase):\n \n     pipeline_class = MediaPipeline\n+    settings = None\n \n     def setUp(self):\n         self.spider = Spider('media.com')\n         self.pipe = self.pipeline_class(download_func=_mocked_download_func,\n-                                        settings=Settings())\n+                                        settings=Settings(self.settings))\n         self.pipe.open_spider(self.spider)\n         self.info = self.pipe.spiderinfo\n \n@@ -89,17 +90,37 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n         request = Request('http://url')\n         assert self.pipe._modify_media_request(request).meta == {'handle_httpstatus_all': True}\n \n-        request = Request('http://url')\n-        self.pipe.handle_httpstatus_list = list(range(100))\n-        assert self.pipe._modify_media_request(request).meta == {'handle_httpstatus_list': list(range(100))}\n-        self.pipe.handle_httpstatus_list = None\n \n+class MediaPipelineAllowRedirectsTestCase(BaseMediaPipelineTestCase):\n+\n+    pipeline_class = MediaPipeline\n+    settings = {\n+        'MEDIA_ALLOW_REDIRECTS': True\n+    }\n+\n+    def test_modify_media_request(self):\n         request = Request('http://url')\n-        self.pipe.allow_redirects = True\n-        correct = {'handle_httpstatus_list': [i for i in range(1000)\n-                                              if i not in RedirectMiddleware.allowed_status]}\n-        assert self.pipe._modify_media_request(request).meta == correct\n-        self.pipe.allow_redirects = False\n+        meta = self.pipe._modify_media_request(request).meta\n+        self.assertIn('handle_httpstatus_list', meta)\n+        for status, check in [\n+                (200, True),\n+\n+                # These are the status codes we want\n+                # the downloader to handle itself\n+                (301, False),\n+                (302, False),\n+                (302, False),\n+                (307, False),\n+                (308, False),\n+\n+                # we still want to get 4xx and 5xx\n+                (400, True),\n+                (404, True),\n+                (500, True)]:\n+            if check:\n+                self.assertIn(status, meta['handle_httpstatus_list'])\n+            else:\n+                self.assertNotIn(status, meta['handle_httpstatus_list'])\n \n \n class MockedMediaPipeline(MediaPipeline):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f7e11b198efd0213bb51205b6829123476ccf2ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 279 | Contributors (this commit): 6 | Commits (past 90d): 5 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -114,7 +114,6 @@ class MediaPipeline(object):\n             request.meta['handle_httpstatus_list'] = self.handle_httpstatus_list\n         else:\n             request.meta['handle_httpstatus_all'] = True\n-        return request\n \n     def _check_media_to_download(self, result, request, info):\n         if result is not None:\n@@ -126,7 +125,7 @@ class MediaPipeline(object):\n                 callback=self.media_downloaded, callbackArgs=(request, info),\n                 errback=self.media_failed, errbackArgs=(request, info))\n         else:\n-            request = self._modify_media_request(request)\n+            self._modify_media_request(request)\n             dfd = self.crawler.engine.download(request, info.spider)\n             dfd.addCallbacks(\n                 callback=self.media_downloaded, callbackArgs=(request, info),\n\n@@ -5,7 +5,6 @@ from twisted.python.failure import Failure\n from twisted.internet import reactor\n from twisted.internet.defer import Deferred, inlineCallbacks\n \n-from scrapy.downloadermiddlewares.redirect import RedirectMiddleware\n from scrapy.http import Request, Response\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n@@ -88,7 +87,8 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n \n     def test_modify_media_request(self):\n         request = Request('http://url')\n-        assert self.pipe._modify_media_request(request).meta == {'handle_httpstatus_all': True}\n+        self.pipe._modify_media_request(request)\n+        assert request.meta == {'handle_httpstatus_all': True}\n \n \n class MediaPipelineAllowRedirectsTestCase(BaseMediaPipelineTestCase):\n@@ -100,8 +100,8 @@ class MediaPipelineAllowRedirectsTestCase(BaseMediaPipelineTestCase):\n \n     def test_modify_media_request(self):\n         request = Request('http://url')\n-        meta = self.pipe._modify_media_request(request).meta\n-        self.assertIn('handle_httpstatus_list', meta)\n+        self.pipe._modify_media_request(request)\n+        self.assertIn('handle_httpstatus_list', request.meta)\n         for status, check in [\n                 (200, True),\n \n@@ -118,9 +118,9 @@ class MediaPipelineAllowRedirectsTestCase(BaseMediaPipelineTestCase):\n                 (404, True),\n                 (500, True)]:\n             if check:\n-                self.assertIn(status, meta['handle_httpstatus_list'])\n+                self.assertIn(status, request.meta['handle_httpstatus_list'])\n             else:\n-                self.assertNotIn(status, meta['handle_httpstatus_list'])\n+                self.assertNotIn(status, request.meta['handle_httpstatus_list'])\n \n \n class MockedMediaPipeline(MediaPipeline):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ef04cfd237ec3d072a487f92e217bad68195f2d8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 73 | Lines Deleted: 6 | Files Changed: 4 | Hunks: 12 | Methods Changed: 8 | Complexity Δ (Sum/Max): 7/4 | Churn Δ: 79 | Churn Cumulative: 2482 | Contributors (this commit): 15 | Commits (past 90d): 7 | Contributors (cumulative): 30 | DMM Complexity: 1.0\n\nDIFF:\n@@ -16,7 +16,9 @@ from scrapy.signalmanager import SignalManager\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.utils.ossignal import install_shutdown_handlers, signal_names\n from scrapy.utils.misc import load_object\n-from scrapy.utils.log import LogCounterHandler, configure_logging, log_scrapy_info\n+from scrapy.utils.log import (\n+    LogCounterHandler, configure_logging, log_scrapy_info,\n+    get_scrapy_root_handler, install_scrapy_root_handler)\n from scrapy import signals\n \n logger = logging.getLogger(__name__)\n@@ -35,8 +37,11 @@ class Crawler(object):\n         self.signals = SignalManager(self)\n         self.stats = load_object(self.settings['STATS_CLASS'])(self)\n \n-        handler = LogCounterHandler(self, level=settings.get('LOG_LEVEL'))\n+        handler = LogCounterHandler(self, level=self.settings.get('LOG_LEVEL'))\n         logging.root.addHandler(handler)\n+        if get_scrapy_root_handler() is not None:\n+            # scrapy root handler alread installed: update it with new settings\n+            install_scrapy_root_handler(self.settings)\n         # lambda is assigned to Crawler attribute because this way it is not\n         # garbage collected after leaving __init__ scope\n         self.__remove_handler = lambda: logging.root.removeHandler(handler)\n\n@@ -96,9 +96,25 @@ def configure_logging(settings=None, install_root_handler=True):\n         sys.stdout = StreamLogger(logging.getLogger('stdout'))\n \n     if install_root_handler:\n+        install_scrapy_root_handler(settings)\n+\n+\n+def install_scrapy_root_handler(settings):\n+    global _scrapy_root_handler\n+\n+    if (_scrapy_root_handler is not None\n+            and _scrapy_root_handler in logging.root.handlers):\n+        logging.root.removeHandler(_scrapy_root_handler)\n     logging.root.setLevel(logging.NOTSET)\n-        handler = _get_handler(settings)\n-        logging.root.addHandler(handler)\n+    _scrapy_root_handler = _get_handler(settings)\n+    logging.root.addHandler(_scrapy_root_handler)\n+\n+\n+def get_scrapy_root_handler():\n+    return _scrapy_root_handler\n+\n+\n+_scrapy_root_handler = None\n \n \n def _get_handler(settings):\n\n@@ -97,8 +97,8 @@ class CrawlTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_start_requests_bug_before_yield(self):\n-        with LogCapture('scrapy', level=logging.ERROR) as l:\n         crawler = self.runner.create_crawler(BrokenStartRequestsSpider)\n+        with LogCapture('scrapy', level=logging.ERROR) as l:\n             yield crawler.crawl(fail_before_yield=1)\n \n         self.assertEqual(len(l.records), 1)\n@@ -108,8 +108,8 @@ class CrawlTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_start_requests_bug_yielding(self):\n-        with LogCapture('scrapy', level=logging.ERROR) as l:\n         crawler = self.runner.create_crawler(BrokenStartRequestsSpider)\n+        with LogCapture('scrapy', level=logging.ERROR) as l:\n             yield crawler.crawl(fail_yielding=1)\n \n         self.assertEqual(len(l.records), 1)\n\n@@ -1,3 +1,6 @@\n+import logging\n+import os\n+import tempfile\n import warnings\n import unittest\n \n@@ -5,6 +8,7 @@ import scrapy\n from scrapy.crawler import Crawler, CrawlerRunner, CrawlerProcess\n from scrapy.settings import Settings, default_settings\n from scrapy.spiderloader import SpiderLoader\n+from scrapy.utils.log import configure_logging, get_scrapy_root_handler\n from scrapy.utils.spider import DefaultSpider\n from scrapy.utils.misc import load_object\n from scrapy.extensions.throttle import AutoThrottle\n@@ -74,6 +78,48 @@ class SpiderSettingsTestCase(unittest.TestCase):\n         self.assertIn(AutoThrottle, enabled_exts)\n \n \n+class CrawlerLoggingTestCase(unittest.TestCase):\n+    def test_no_root_handler_installed(self):\n+        handler = get_scrapy_root_handler()\n+        if handler is not None:\n+            logging.root.removeHandler(handler)\n+\n+        class MySpider(scrapy.Spider):\n+            name = 'spider'\n+\n+        crawler = Crawler(MySpider, {})\n+        assert get_scrapy_root_handler() is None\n+\n+    def test_spider_custom_settings_log_level(self):\n+        with tempfile.NamedTemporaryFile() as log_file:\n+            class MySpider(scrapy.Spider):\n+                name = 'spider'\n+                custom_settings = {\n+                    'LOG_LEVEL': 'INFO',\n+                    'LOG_FILE': log_file.name,\n+                }\n+\n+            configure_logging()\n+            self.assertEqual(get_scrapy_root_handler().level, logging.DEBUG)\n+            crawler = Crawler(MySpider, {})\n+            self.assertEqual(get_scrapy_root_handler().level, logging.INFO)\n+            info_count = crawler.stats.get_value('log_count/INFO')\n+            logging.debug('debug message')\n+            logging.info('info message')\n+            logging.warning('warning message')\n+            logging.error('error message')\n+            logged = log_file.read().decode('utf8')\n+        self.assertNotIn('debug message', logged)\n+        self.assertIn('info message', logged)\n+        self.assertIn('warning message', logged)\n+        self.assertIn('error message', logged)\n+        self.assertEqual(crawler.stats.get_value('log_count/ERROR'), 1)\n+        self.assertEqual(crawler.stats.get_value('log_count/WARNING'), 1)\n+        self.assertEqual(\n+            crawler.stats.get_value('log_count/INFO') - info_count, 1)\n+        self.assertEqual(crawler.stats.get_value('log_count/DEBUG', 0), 0)\n+\n+\n class SpiderLoaderWithWrongInterface(object):\n \n     def unneeded_method(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c3b6feca0e15309386c080d71b0b03a07d4c8635", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 171 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -33,7 +33,8 @@ class MediaPipeline(object):\n         if isinstance(settings, dict) or settings is None:\n             settings = Settings(settings)\n         resolve = functools.partial(self._key_for_pipe,\n-                                    base_class_name=\"MediaPipeline\")\n+                                    base_class_name=\"MediaPipeline\",\n+                                    settings=settings)\n         self.allow_redirects = settings.getbool(\n             resolve('MEDIA_ALLOW_REDIRECTS'), False\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c68f99eed843bd35224d8bf0e22b0c66bdc2122b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 32 | Files Changed: 1 | Hunks: 2 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 90 | Churn Cumulative: 201 | Contributors (this commit): 5 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -91,38 +91,6 @@ class BaseMediaPipelineTestCase(unittest.TestCase):\n         assert request.meta == {'handle_httpstatus_all': True}\n \n \n-class MediaPipelineAllowRedirectsTestCase(BaseMediaPipelineTestCase):\n-\n-    pipeline_class = MediaPipeline\n-    settings = {\n-        'MEDIA_ALLOW_REDIRECTS': True\n-    }\n-\n-    def test_modify_media_request(self):\n-        request = Request('http://url')\n-        self.pipe._modify_media_request(request)\n-        self.assertIn('handle_httpstatus_list', request.meta)\n-        for status, check in [\n-                (200, True),\n-\n-                # These are the status codes we want\n-                # the downloader to handle itself\n-                (301, False),\n-                (302, False),\n-                (302, False),\n-                (307, False),\n-                (308, False),\n-\n-                # we still want to get 4xx and 5xx\n-                (400, True),\n-                (404, True),\n-                (500, True)]:\n-            if check:\n-                self.assertIn(status, request.meta['handle_httpstatus_list'])\n-            else:\n-                self.assertNotIn(status, request.meta['handle_httpstatus_list'])\n-\n-\n class MockedMediaPipeline(MediaPipeline):\n \n     def __init__(self, *args, **kwargs):\n@@ -289,3 +257,61 @@ class MediaPipelineTestCase(BaseMediaPipelineTestCase):\n         self.assertEqual(new_item['results'], [(True, 'ITSME')])\n         self.assertEqual(self.pipe._mockcalled, \\\n                 ['get_media_requests', 'media_to_download', 'item_completed'])\n+\n+\n+class MediaPipelineAllowRedirectSettingsTestCase(unittest.TestCase):\n+\n+    def _assert_request_no3xx(self, pipeline_class, settings):\n+        pipe = pipeline_class(settings=Settings(settings))\n+        request = Request('http://url')\n+        pipe._modify_media_request(request)\n+\n+        self.assertIn('handle_httpstatus_list', request.meta)\n+        for status, check in [\n+                (200, True),\n+\n+                # These are the status codes we want\n+                # the downloader to handle itself\n+                (301, False),\n+                (302, False),\n+                (302, False),\n+                (307, False),\n+                (308, False),\n+\n+                # we still want to get 4xx and 5xx\n+                (400, True),\n+                (404, True),\n+                (500, True)]:\n+            if check:\n+                self.assertIn(status, request.meta['handle_httpstatus_list'])\n+            else:\n+                self.assertNotIn(status, request.meta['handle_httpstatus_list'])\n+\n+    def test_standard_setting(self):\n+        self._assert_request_no3xx(\n+            MediaPipeline,\n+            {\n+                'MEDIA_ALLOW_REDIRECTS': True\n+            })\n+\n+    def test_subclass_standard_setting(self):\n+\n+        class UserDefinedPipeline(MediaPipeline):\n+            pass\n+\n+        self._assert_request_no3xx(\n+            UserDefinedPipeline,\n+            {\n+                'MEDIA_ALLOW_REDIRECTS': True\n+            })\n+\n+    def test_subclass_specific_setting(self):\n+\n+        class UserDefinedPipeline(MediaPipeline):\n+            pass\n+\n+        self._assert_request_no3xx(\n+            UserDefinedPipeline,\n+            {\n+                'USERDEFINEDPIPELINE_MEDIA_ALLOW_REDIRECTS': True\n+            })\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#30d812eea233914b3aead55b8053a9c804e594d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 2209 | Contributors (this commit): 18 | Commits (past 90d): 1 | Contributors (cumulative): 18 | DMM Complexity: 0.0\n\nDIFF:\n@@ -218,10 +218,8 @@ class ExecutionEngine(object):\n                                         request=request, spider=spider)\n \n     def download(self, request, spider):\n-        slot = self.slot\n-        slot.add_request(request)\n         d = self._download(request, spider)\n-        d.addBoth(self._downloaded, slot, request, spider)\n+        d.addBoth(self._downloaded, self.slot, request, spider)\n         return d\n \n     def _downloaded(self, response, slot, request, spider):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#768f3155e57ed54419f0c137fc066a95197d1b46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 15 | Churn Cumulative: 2138 | Contributors (this commit): 4 | Commits (past 90d): 21 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -261,6 +261,9 @@ _policy_classes = {p.name: p for p in (\n     DefaultReferrerPolicy,\n )}\n \n+# Reference: https://www.w3.org/TR/referrer-policy/#referrer-policy-empty-string\n+_policy_classes[''] = NoReferrerWhenDowngradePolicy\n+\n \n def _load_policy_class(policy, warning_only=False):\n     \"\"\"\n@@ -317,8 +320,9 @@ class RefererMiddleware(object):\n         policy_name = request.meta.get('referrer_policy')\n         if policy_name is None:\n             if isinstance(resp_or_url, Response):\n-                policy_name = to_native_str(\n-                    resp_or_url.headers.get('Referrer-Policy', '').decode('latin1'))\n+                policy_header = resp_or_url.headers.get('Referrer-Policy')\n+                if policy_header is not None:\n+                    policy_name = to_native_str(policy_header.decode('latin1'))\n         if policy_name is None:\n             return self.default_policy()\n \n\n@@ -526,6 +526,13 @@ class TestPolicyHeaderPredecence003(MixinNoReferrerWhenDowngrade, TestRefererMid\n     settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy'}\n     resp_headers = {'Referrer-Policy': POLICY_NO_REFERRER_WHEN_DOWNGRADE.title()}\n \n+class TestPolicyHeaderPredecence004(MixinNoReferrerWhenDowngrade, TestRefererMiddleware):\n+    \"\"\"\n+    The empty string means \"no-referrer-when-downgrade\"\n+    \"\"\"\n+    settings = {'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy'}\n+    resp_headers = {'Referrer-Policy': ''}\n+\n \n class TestReferrerOnRedirect(TestRefererMiddleware):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e42b846a9fc68a624e94052aac2ba44224b05cad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 17 | Churn Cumulative: 139 | Contributors (this commit): 6 | Commits (past 90d): 9 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -38,7 +38,7 @@ class HttpCompressionMiddleware(object):\n                 encoding = content_encoding.pop()\n                 decoded_body = self._decode(response.body, encoding.lower())\n                 respcls = responsetypes.from_args(headers=response.headers, \\\n-                    url=response.url)\n+                    url=response.url, body=decoded_body)\n                 kwargs = dict(cls=respcls, body=decoded_body)\n                 if issubclass(respcls, TextResponse):\n                     # force recalculating the encoding until we make sure the\n\n@@ -7,6 +7,7 @@ from scrapy.spiders import Spider\n from scrapy.http import Response, Request, HtmlResponse\n from scrapy.downloadermiddlewares.httpcompression import HttpCompressionMiddleware, \\\n     ACCEPTED_ENCODINGS\n+from scrapy.responsetypes import responsetypes\n from tests import tests_datadir\n from w3lib.encoding import resolve_encoding\n \n@@ -152,6 +153,20 @@ class HttpCompressionTest(TestCase):\n         self.assertEqual(newresponse.body, plainbody)\n         self.assertEqual(newresponse.encoding, resolve_encoding('gb2312'))\n \n+    def test_process_response_no_content_type_header(self):\n+        headers = {\n+            'Content-Encoding': 'identity',\n+        }\n+        plainbody = b\"\"\"<html><head><title>Some page</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\">\"\"\"\n+        respcls = responsetypes.from_args(url=\"http://www.example.com/index\", headers=headers, body=plainbody)\n+        response = respcls(\"http://www.example.com/index\", headers=headers, body=plainbody)\n+        request = Request(\"http://www.example.com/index\")\n+\n+        newresponse = self.mw.process_response(request, response, self.spider)\n+        assert isinstance(newresponse, respcls)\n+        self.assertEqual(newresponse.body, plainbody)\n+        self.assertEqual(newresponse.encoding, resolve_encoding('gb2312'))\n+\n     def test_process_response_gzipped_contenttype(self):\n         response = self._getresponse('gzip')\n         response.headers['Content-Type'] = 'application/gzip'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#11cdf58abe26a9f65d8c1aefa717b6735e5734c9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 77 | Lines Deleted: 15 | Files Changed: 5 | Hunks: 11 | Methods Changed: 8 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 92 | Churn Cumulative: 597 | Contributors (this commit): 16 | Commits (past 90d): 17 | Contributors (cumulative): 31 | DMM Complexity: 0.7297297297297297\n\nDIFF:\n@@ -34,7 +34,7 @@ class HttpCompressionMiddleware(object):\n             return response\n         if isinstance(response, Response):\n             content_encoding = response.headers.getlist('Content-Encoding')\n-            if content_encoding and not is_gzipped(response):\n+            if content_encoding:\n                 encoding = content_encoding.pop()\n                 decoded_body = self._decode(response.body, encoding.lower())\n                 respcls = responsetypes.from_args(headers=response.headers, \\\n\n@@ -5,7 +5,8 @@ import six\n from scrapy.spiders import Spider\n from scrapy.http import Request, XmlResponse\n from scrapy.utils.sitemap import Sitemap, sitemap_urls_from_robots\n-from scrapy.utils.gz import gunzip, is_gzipped\n+from scrapy.utils.gz import gunzip, gzip_magic_number\n+\n \n logger = logging.getLogger(__name__)\n \n@@ -59,12 +60,22 @@ class SitemapSpider(Spider):\n         \"\"\"\n         if isinstance(response, XmlResponse):\n             return response.body\n-        elif is_gzipped(response):\n+        elif gzip_magic_number(response):\n+            try:\n                 return gunzip(response.body)\n-        elif response.url.endswith('.xml'):\n+            except:\n+                pass\n+        # actual gzipped sitemap files are decompressed above ;\n+        # if we are here (response body is not gzipped)\n+        # and have a response for .xml.gz,\n+        # it usually means that it was already gunzipped\n+        # by HttpCompression middleware,\n+        # the HTTP response being sent with \"Content-Encoding: gzip\"\n+        # without actually being a .xml.gz file in the first place,\n+        # merely XML gzip-compressed on the fly,\n+        # in other word, here, we have plain XML\n+        elif response.url.endswith('.xml') or response.url.endswith('.xml.gz'):\n             return response.body\n-        elif response.url.endswith('.xml.gz'):\n-            return gunzip(response.body)\n \n \n def regex(x):\n\n@@ -59,3 +59,7 @@ def is_gzipped(response):\n     cenc = response.headers.get('Content-Encoding', b'').lower()\n     return (_is_gzipped(ctype) or\n             (_is_octetstream(ctype) and cenc in (b'gzip', b'x-gzip')))\n+\n+\n+def gzip_magic_number(response):\n+    return response.body[:2] == b'\\x1f\\x8b'\n\n@@ -8,6 +8,7 @@ from scrapy.http import Response, Request, HtmlResponse\n from scrapy.downloadermiddlewares.httpcompression import HttpCompressionMiddleware, \\\n     ACCEPTED_ENCODINGS\n from scrapy.responsetypes import responsetypes\n+from scrapy.utils.gz import gunzip\n from tests import tests_datadir\n from w3lib.encoding import resolve_encoding\n \n@@ -173,9 +174,9 @@ class HttpCompressionTest(TestCase):\n         request = response.request\n \n         newresponse = self.mw.process_response(request, response, self.spider)\n-        self.assertIs(newresponse, response)\n-        self.assertEqual(response.headers['Content-Encoding'], b'gzip')\n-        self.assertEqual(response.headers['Content-Type'], b'application/gzip')\n+        assert newresponse is not response\n+        assert newresponse.body.startswith(b'<!DOCTYPE')\n+        assert 'Content-Encoding' not in newresponse.headers\n \n     def test_process_response_gzip_app_octetstream_contenttype(self):\n         response = self._getresponse('gzip')\n@@ -183,9 +184,9 @@ class HttpCompressionTest(TestCase):\n         request = response.request\n \n         newresponse = self.mw.process_response(request, response, self.spider)\n-        self.assertIs(newresponse, response)\n-        self.assertEqual(response.headers['Content-Encoding'], b'gzip')\n-        self.assertEqual(response.headers['Content-Type'], b'application/octet-stream')\n+        assert newresponse is not response\n+        assert newresponse.body.startswith(b'<!DOCTYPE')\n+        assert 'Content-Encoding' not in newresponse.headers\n \n     def test_process_response_gzip_binary_octetstream_contenttype(self):\n         response = self._getresponse('x-gzip')\n@@ -193,9 +194,51 @@ class HttpCompressionTest(TestCase):\n         request = response.request\n \n         newresponse = self.mw.process_response(request, response, self.spider)\n-        self.assertIs(newresponse, response)\n-        self.assertEqual(response.headers['Content-Encoding'], b'gzip')\n-        self.assertEqual(response.headers['Content-Type'], b'binary/octet-stream')\n+        assert newresponse is not response\n+        assert newresponse.body.startswith(b'<!DOCTYPE')\n+        assert 'Content-Encoding' not in newresponse.headers\n+\n+    def test_process_response_gzipped_gzip_file(self):\n+        \"\"\"Test that a gzip Content-Encoded .gz file is gunzipped\n+        only once by the middleware, leaving gunzipping of the file\n+        to upper layers.\n+        \"\"\"\n+        headers = {\n+            'Content-Type': 'application/gzip',\n+            'Content-Encoding': 'gzip',\n+        }\n+        # build a gzipped file (here, a sitemap)\n+        f = BytesIO()\n+        plainbody = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<urlset xmlns=\"http://www.google.com/schemas/sitemap/0.84\">\n+  <url>\n+    <loc>http://www.example.com/</loc>\n+    <lastmod>2009-08-16</lastmod>\n+    <changefreq>daily</changefreq>\n+    <priority>1</priority>\n+  </url>\n+  <url>\n+    <loc>http://www.example.com/Special-Offers.html</loc>\n+    <lastmod>2009-08-16</lastmod>\n+    <changefreq>weekly</changefreq>\n+    <priority>0.8</priority>\n+  </url>\n+</urlset>\"\"\"\n+        gz_file = GzipFile(fileobj=f, mode='wb')\n+        gz_file.write(plainbody)\n+        gz_file.close()\n+\n+        # build a gzipped response body containing this gzipped file\n+        r = BytesIO()\n+        gz_resp = GzipFile(fileobj=r, mode='wb')\n+        gz_resp.write(f.getvalue())\n+        gz_resp.close()\n+\n+        response = Response(\"http;//www.example.com/\", headers=headers, body=r.getvalue())\n+        request = Request(\"http://www.example.com/\")\n+\n+        newresponse = self.mw.process_response(request, response, self.spider)\n+        self.assertEqual(gunzip(newresponse.body), plainbody)\n \n     def test_process_response_head_request_no_decode_required(self):\n         response = self._getresponse('gzip')\n\n@@ -328,6 +328,10 @@ class SitemapSpiderTest(SpiderTest):\n         r = Response(url=\"http://www.example.com/sitemap.xml.gz\", body=self.GZBODY)\n         self.assertSitemapBody(r, self.BODY)\n \n+        # .xml.gz but body decoded by HttpCompression middleware already\n+        r = Response(url=\"http://www.example.com/sitemap.xml.gz\", body=self.BODY)\n+        self.assertSitemapBody(r, self.BODY)\n+\n     def test_get_sitemap_urls_from_robotstxt(self):\n         robots = b\"\"\"# Sitemap files\n Sitemap: http://example.com/sitemap.xml\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b174744b80fd12efc6e4008ea5eb38256f21038a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 5 | Churn Cumulative: 83 | Contributors (this commit): 8 | Commits (past 90d): 10 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,6 @@\n import zlib\n \n-from scrapy.utils.gz import gunzip, is_gzipped\n+from scrapy.utils.gz import gunzip\n from scrapy.http import Response, TextResponse\n from scrapy.responsetypes import responsetypes\n from scrapy.exceptions import NotConfigured\n\n@@ -61,10 +61,7 @@ class SitemapSpider(Spider):\n         if isinstance(response, XmlResponse):\n             return response.body\n         elif gzip_magic_number(response):\n-            try:\n             return gunzip(response.body)\n-            except:\n-                pass\n         # actual gzipped sitemap files are decompressed above ;\n         # if we are here (response body is not gzipped)\n         # and have a response for .xml.gz,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4caceccd594f2563a3d32ea708579ce8e70132f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 148 | Contributors (this commit): 7 | Commits (past 90d): 1 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -62,4 +62,4 @@ def is_gzipped(response):\n \n \n def gzip_magic_number(response):\n-    return response.body[:2] == b'\\x1f\\x8b'\n+    return response.body[:3] == b'\\x1f\\x8b\\x08'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6f55ca4643dfff7a163714d696eef66d06cb81a8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 368 | Contributors (this commit): 8 | Commits (past 90d): 5 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -97,8 +97,8 @@ class CrawlTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_start_requests_bug_before_yield(self):\n-        crawler = self.runner.create_crawler(BrokenStartRequestsSpider)\n         with LogCapture('scrapy', level=logging.ERROR) as l:\n+            crawler = self.runner.create_crawler(BrokenStartRequestsSpider)\n             yield crawler.crawl(fail_before_yield=1)\n \n         self.assertEqual(len(l.records), 1)\n@@ -108,8 +108,8 @@ class CrawlTestCase(TestCase):\n \n     @defer.inlineCallbacks\n     def test_start_requests_bug_yielding(self):\n-        crawler = self.runner.create_crawler(BrokenStartRequestsSpider)\n         with LogCapture('scrapy', level=logging.ERROR) as l:\n+            crawler = self.runner.create_crawler(BrokenStartRequestsSpider)\n             yield crawler.crawl(fail_yielding=1)\n \n         self.assertEqual(len(l.records), 1)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b6378c7ef6393412165239fd6bf489d45a1c5196", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 183 | Contributors (this commit): 6 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -174,9 +174,9 @@ class HttpCompressionTest(TestCase):\n         request = response.request\n \n         newresponse = self.mw.process_response(request, response, self.spider)\n-        assert newresponse is not response\n-        assert newresponse.body.startswith(b'<!DOCTYPE')\n-        assert 'Content-Encoding' not in newresponse.headers\n+        self.assertIsNot(newresponse, response)\n+        self.assertTrue(newresponse.body.startswith(b'<!DOCTYPE'))\n+        self.assertNotIn('Content-Encoding', newresponse.headers)\n \n     def test_process_response_gzip_app_octetstream_contenttype(self):\n         response = self._getresponse('gzip')\n@@ -184,9 +184,9 @@ class HttpCompressionTest(TestCase):\n         request = response.request\n \n         newresponse = self.mw.process_response(request, response, self.spider)\n-        assert newresponse is not response\n-        assert newresponse.body.startswith(b'<!DOCTYPE')\n-        assert 'Content-Encoding' not in newresponse.headers\n+        self.assertIsNot(newresponse, response)\n+        self.assertTrue(newresponse.body.startswith(b'<!DOCTYPE'))\n+        self.assertNotIn('Content-Encoding', newresponse.headers)\n \n     def test_process_response_gzip_binary_octetstream_contenttype(self):\n         response = self._getresponse('x-gzip')\n@@ -194,9 +194,9 @@ class HttpCompressionTest(TestCase):\n         request = response.request\n \n         newresponse = self.mw.process_response(request, response, self.spider)\n-        assert newresponse is not response\n-        assert newresponse.body.startswith(b'<!DOCTYPE')\n-        assert 'Content-Encoding' not in newresponse.headers\n+        self.assertIsNot(newresponse, response)\n+        self.assertTrue(newresponse.body.startswith(b'<!DOCTYPE'))\n+        self.assertNotIn('Content-Encoding', newresponse.headers)\n \n     def test_process_response_gzipped_gzip_file(self):\n         \"\"\"Test that a gzip Content-Encoded .gz file is gunzipped\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a017f7b93298f8fcf5252afd8674466de95f04f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 12 | Churn Cumulative: 96 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n # -*- coding: utf-8 -*-\n from __future__ import absolute_import\n+from collections import defaultdict\n import traceback\n import warnings\n \n@@ -19,14 +20,21 @@ class SpiderLoader(object):\n     def __init__(self, settings):\n         self.spider_modules = settings.getlist('SPIDER_MODULES')\n         self._spiders = {}\n+        self._found = defaultdict(list)\n         self._load_all_spiders()\n \n     def _load_spiders(self, module):\n         for spcls in iter_spider_classes(module):\n+            self._found[spcls.name].append((module.__name__, spcls.__name__))\n             if spcls.name in self._spiders.keys():\n                 import warnings\n-                warnings.warn(\"There are several spiders with the same name (\" + spcls.name +\n-                              \"), this can cause unexpected behavior\", UserWarning)\n+                msg = (\"There are several spiders with the same name {!r}:\\n\"\n+                       \"{}\\n    This can cause unexpected behavior.\".format(\n+                            spcls.name,\n+                            \"\\n\".join(\n+                                \"        {1} (in {0})\".format(mod, cls)\n+                                for (mod, cls) in self._found[spcls.name])))\n+                warnings.warn(msg, UserWarning)\n             self._spiders[spcls.name] = spcls\n \n     def _load_all_spiders(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#978306a2236403f6275bad11e85d1e42ca37d6f1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 175 | Contributors (this commit): 4 | Commits (past 90d): 3 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -127,7 +127,7 @@ class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n             spider_loader = SpiderLoader.from_settings(self.settings)\n \n             self.assertEqual(len(w), 1)\n-            self.assertIn(\"several spiders with the same name (spider3)\", str(w[0].message))\n+            self.assertIn(\"several spiders with the same name 'spider3'\", str(w[0].message))\n \n             spiders = set(spider_loader.list())\n             self.assertEqual(spiders, set(['spider1', 'spider2', 'spider3', 'spider4']))\n@@ -145,8 +145,8 @@ class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n \n             self.assertEqual(len(w), 2)\n             msgs = sorted(str(wrn.message) for wrn in w)\n-            self.assertIn(\"several spiders with the same name (spider1)\", msgs[0])\n-            self.assertIn(\"several spiders with the same name (spider2)\", msgs[1])\n+            self.assertIn(\"several spiders with the same name 'spider1'\", msgs[0])\n+            self.assertIn(\"several spiders with the same name 'spider2'\", msgs[1])\n \n             spiders = set(spider_loader.list())\n             self.assertEqual(spiders, set(['spider1', 'spider2', 'spider3', 'spider4']))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0b9a18e1a120751b8a2b2f1cc04c62b97967d612", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 5 | Methods Changed: 5 | Complexity Δ (Sum/Max): 2/3 | Churn Δ: 35 | Churn Cumulative: 306 | Contributors (this commit): 6 | Commits (past 90d): 8 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -23,18 +23,21 @@ class SpiderLoader(object):\n         self._found = defaultdict(list)\n         self._load_all_spiders()\n \n+    def _check_name_duplicates(self):\n+        dupes = [\"\\n\".join(\"  {cls} named {name!r} (in {module})\".format(\n+                                module=mod, cls=cls, name=name)\n+                           for (mod, cls) in locations)\n+                 for name, locations in self._found.items()\n+                 if len(locations)>1]\n+        if dupes:\n+            msg = (\"There are several spiders with the same name:\\n\\n\"\n+                   \"{}\\n\\n  This can cause unexpected behavior.\".format(\n+                        \"\\n\\n\".join(dupes)))\n+            warnings.warn(msg, UserWarning)\n+\n     def _load_spiders(self, module):\n         for spcls in iter_spider_classes(module):\n             self._found[spcls.name].append((module.__name__, spcls.__name__))\n-            if spcls.name in self._spiders.keys():\n-                import warnings\n-                msg = (\"There are several spiders with the same name {!r}:\\n\"\n-                       \"{}\\n    This can cause unexpected behavior.\".format(\n-                            spcls.name,\n-                            \"\\n\".join(\n-                                \"        {1} (in {0})\".format(mod, cls)\n-                                for (mod, cls) in self._found[spcls.name])))\n-                warnings.warn(msg, UserWarning)\n             self._spiders[spcls.name] = spcls\n \n     def _load_all_spiders(self):\n@@ -47,6 +50,7 @@ class SpiderLoader(object):\n                        \"Check SPIDER_MODULES setting\".format(\n                             modname=name, tb=traceback.format_exc()))\n                 warnings.warn(msg, RuntimeWarning)\n+        self._check_name_duplicates()\n \n     @classmethod\n     def from_settings(cls, settings):\n\n@@ -127,7 +127,9 @@ class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n             spider_loader = SpiderLoader.from_settings(self.settings)\n \n             self.assertEqual(len(w), 1)\n-            self.assertIn(\"several spiders with the same name 'spider3'\", str(w[0].message))\n+            msg = str(w[0].message)\n+            self.assertIn(\"several spiders with the same name\", msg)\n+            self.assertIn(\"'spider3'\", msg)\n \n             spiders = set(spider_loader.list())\n             self.assertEqual(spiders, set(['spider1', 'spider2', 'spider3', 'spider4']))\n@@ -143,10 +145,11 @@ class DuplicateSpiderNameLoaderTest(unittest.TestCase):\n         with warnings.catch_warnings(record=True) as w:\n             spider_loader = SpiderLoader.from_settings(self.settings)\n \n-            self.assertEqual(len(w), 2)\n-            msgs = sorted(str(wrn.message) for wrn in w)\n-            self.assertIn(\"several spiders with the same name 'spider1'\", msgs[0])\n-            self.assertIn(\"several spiders with the same name 'spider2'\", msgs[1])\n+            self.assertEqual(len(w), 1)\n+            msg = str(w[0].message)\n+            self.assertIn(\"several spiders with the same name\", msg)\n+            self.assertIn(\"'spider1'\", msg)\n+            self.assertIn(\"'spider2'\", msg)\n \n             spiders = set(spider_loader.list())\n             self.assertEqual(spiders, set(['spider1', 'spider2', 'spider3', 'spider4']))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7e9153b38d7ef70f1f19a82506b669433d134b01", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 135 | Lines Deleted: 12 | Files Changed: 4 | Hunks: 21 | Methods Changed: 12 | Complexity Δ (Sum/Max): 7/4 | Churn Δ: 147 | Churn Cumulative: 1174 | Contributors (this commit): 39 | Commits (past 90d): 15 | Contributors (cumulative): 59 | DMM Complexity: 0.9380530973451328\n\nDIFF:\n@@ -36,6 +36,7 @@ class BaseItemExporter(object):\n         self.encoding = options.pop('encoding', None)\n         self.fields_to_export = options.pop('fields_to_export', None)\n         self.export_empty_fields = options.pop('export_empty_fields', False)\n+        self.indent_width = options.pop('indent_width', None)\n         if not dont_fail and options:\n             raise TypeError(\"Unexpected options: %s\" % ', '.join(options.keys()))\n \n@@ -99,7 +100,7 @@ class JsonItemExporter(BaseItemExporter):\n         self._configure(kwargs, dont_fail=True)\n         self.file = file\n         kwargs.setdefault('ensure_ascii', not self.encoding)\n-        self.encoder = ScrapyJSONEncoder(**kwargs)\n+        self.encoder = ScrapyJSONEncoder(indent=self.indent_width, **kwargs)\n         self.first_item = True\n \n     def start_exporting(self):\n@@ -128,33 +129,52 @@ class XmlItemExporter(BaseItemExporter):\n             self.encoding = 'utf-8'\n         self.xg = XMLGenerator(file, encoding=self.encoding)\n \n+    def _beautify_newline(self):\n+        if self.indent_width:\n+            self._xg_characters('\\n')\n+\n+    def _beautify_indent(self, depth=1):\n+        if self.indent_width:\n+            self._xg_characters(' ' * self.indent_width * depth)\n+\n     def start_exporting(self):\n         self.xg.startDocument()\n         self.xg.startElement(self.root_element, {})\n+        self._beautify_newline()\n \n     def export_item(self, item):\n+        self._beautify_indent(depth=1)\n         self.xg.startElement(self.item_element, {})\n+        self._beautify_newline()\n         for name, value in self._get_serialized_fields(item, default_value=''):\n-            self._export_xml_field(name, value)\n+            self._export_xml_field(name, value, depth=2)\n+        self._beautify_indent(depth=1)\n         self.xg.endElement(self.item_element)\n+        self._beautify_newline()\n \n     def finish_exporting(self):\n         self.xg.endElement(self.root_element)\n         self.xg.endDocument()\n \n-    def _export_xml_field(self, name, serialized_value):\n+    def _export_xml_field(self, name, serialized_value, depth):\n+        self._beautify_indent(depth=depth)\n         self.xg.startElement(name, {})\n         if hasattr(serialized_value, 'items'):\n+            self._beautify_newline()\n             for subname, value in serialized_value.items():\n-                self._export_xml_field(subname, value)\n+                self._export_xml_field(subname, value, depth=depth+1)\n+            self._beautify_indent(depth=depth)\n         elif is_listlike(serialized_value):\n+            self._beautify_newline()\n             for value in serialized_value:\n-                self._export_xml_field('value', value)\n+                self._export_xml_field('value', value, depth=depth+1)\n+            self._beautify_indent(depth=depth)\n         elif isinstance(serialized_value, six.text_type):\n             self._xg_characters(serialized_value)\n         else:\n             self._xg_characters(str(serialized_value))\n         self.xg.endElement(name)\n+        self._beautify_newline()\n \n     # Workaround for http://bugs.python.org/issue17606\n     # Before Python 2.7.4 xml.sax.saxutils required bytes;\n\n@@ -172,6 +172,7 @@ class FeedExporter(object):\n         self.store_empty = settings.getbool('FEED_STORE_EMPTY')\n         self._exporting = False\n         self.export_fields = settings.getlist('FEED_EXPORT_FIELDS') or None\n+        self.indent_width = settings.getint('FEED_EXPORT_INDENT_WIDTH') or None\n         uripar = settings['FEED_URI_PARAMS']\n         self._uripar = load_object(uripar) if uripar else lambda x, y: None\n \n@@ -188,7 +189,7 @@ class FeedExporter(object):\n         storage = self._get_storage(uri)\n         file = storage.open(spider)\n         exporter = self._get_exporter(file, fields_to_export=self.export_fields,\n-            encoding=self.export_encoding)\n+            encoding=self.export_encoding, indent_width=self.indent_width)\n         if self.store_empty:\n             exporter.start_exporting()\n             self._exporting = True\n\n@@ -161,6 +161,7 @@ FEED_EXPORTERS_BASE = {\n     'marshal': 'scrapy.exporters.MarshalItemExporter',\n     'pickle': 'scrapy.exporters.PickleItemExporter',\n }\n+FEED_EXPORT_INDENT_WIDTH = None\n \n FILES_STORE_S3_ACL = 'private'\n \n\n@@ -431,10 +431,10 @@ class FeedExportTest(unittest.TestCase):\n             'csv': u'foo\\r\\nTest\\xd6\\r\\n'.encode('utf-8'),\n         }\n \n-        for format in formats:\n-            settings = {'FEED_FORMAT': format}\n+        for format, expected in formats.items():\n+            settings = {'FEED_FORMAT': format, 'FEED_EXPORT_INDENT_WIDTH': None}\n             data = yield self.exported_data(items, settings)\n-            self.assertEqual(formats[format], data)\n+            self.assertEqual(expected, data)\n \n         formats = {\n             'json': u'[\\n{\"foo\": \"Test\\xd6\"}\\n]'.encode('latin-1'),\n@@ -443,7 +443,108 @@ class FeedExportTest(unittest.TestCase):\n             'csv': u'foo\\r\\nTest\\xd6\\r\\n'.encode('latin-1'),\n         }\n \n-        for format in formats:\n-            settings = {'FEED_FORMAT': format, 'FEED_EXPORT_ENCODING': 'latin-1'}\n+        settings = {'FEED_EXPORT_INDENT_WIDTH': None, 'FEED_EXPORT_ENCODING': 'latin-1'}\n+        for format, expected in formats.items():\n+            settings['FEED_FORMAT'] = format\n             data = yield self.exported_data(items, settings)\n-            self.assertEqual(formats[format], data)\n+            self.assertEqual(expected, data)\n+\n+    @defer.inlineCallbacks\n+    def test_export_indentation(self):\n+        items = [dict({'foo': ['bar']})]\n+\n+        output = [\n+            # JSON\n+            {\n+                'format': 'json',\n+                'indent_width': None,\n+                'expected': b'[\\n{\"foo\": [\"bar\"]}\\n]',\n+            },\n+            {\n+                'format': 'json',\n+                'indent_width': 2,\n+                'expected': b\"\"\"\n+[\n+{\n+  \"foo\": [\n+    \"bar\"\n+  ]\n+}\n+]\"\"\",\n+            },\n+            {\n+                'format': 'json',\n+                'indent_width': 4,\n+                'expected': b\"\"\"\n+[\n+{\n+    \"foo\": [\n+        \"bar\"\n+    ]\n+}\n+]\"\"\",\n+            },\n+            {\n+                'format': 'json',\n+                'indent_width': 5,\n+                'expected': b\"\"\"\n+[\n+{\n+     \"foo\": [\n+          \"bar\"\n+     ]\n+}\n+]\"\"\",\n+            },\n+\n+            # XML\n+            {\n+                'format': 'xml',\n+                'indent_width': None,\n+                'expected': b'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items><item><foo><value>bar</value></foo></item></items>',\n+            },\n+            {\n+                'format': 'xml',\n+                'indent_width': 2,\n+                'expected': b\"\"\"\n+<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+<items>\n+  <item>\n+    <foo>\n+      <value>bar</value>\n+    </foo>\n+  </item>\n+</items>\"\"\",\n+            },\n+            {\n+                'format': 'xml',\n+                'indent_width': 4,\n+                'expected': b\"\"\"\n+<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+<items>\n+    <item>\n+        <foo>\n+            <value>bar</value>\n+        </foo>\n+    </item>\n+</items>\"\"\",\n+            },\n+            {\n+                'format': 'xml',\n+                'indent_width': 5,\n+                'expected': b\"\"\"\n+<?xml version=\"1.0\" encoding=\"utf-8\"?>\n+<items>\n+     <item>\n+          <foo>\n+               <value>bar</value>\n+          </foo>\n+     </item>\n+</items>\"\"\",\n+            },\n+        ]\n+\n+        for row in output:\n+            settings = {'FEED_FORMAT': row['format'], 'FEED_EXPORT_INDENT_WIDTH': row['indent_width']}\n+            data = yield self.exported_data(items, settings)\n+            self.assertEqual(row['expected'].strip(), data)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
