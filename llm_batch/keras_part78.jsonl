{"custom_id": "keras#763a2a953638ac1a1374c11d65716938060553c0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 53 | Files Changed: 1 | Hunks: 35 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 103 | Churn Cumulative: 894 | Contributors (this commit): 4 | Commits (past 90d): 2 | Contributors (cumulative): 4 | DMM Complexity: 0.0\n\nDIFF:\n@@ -9,46 +9,44 @@ from ..models import Sequential\n \n \n class BaseWrapper(object):\n-\n     '''Base class for the Keras scikit-learn wrapper.\n \n     Warning: This class should not be used directly.\n-    Use derived classes instead.\n+    Use descendant classes instead.\n \n     # Arguments\n-        build_fn: callable function or class instance,  optional\n-            Implementing the logic of the model.\n+        build_fn: callable function or class instance\n         sk_params: model parameters & fitting parameters\n \n     The build_fn should construct, compile and return a Keras model, which\n-    will then be used to fit data or predict unknow data. One of the following\n+    will then be used to fit/predict. One of the following\n     three values could be passed to build_fn:\n-    1. A function instance\n-    2. An instance of a class that implements the __call__ function\n-    3. None. This means you implement a class that inherits either\n-    KerasClassifier or KerasRegressor. The __call__ function of the class will\n-    then be treated as the default build_fn\n+    1. A function\n+    2. An instance of a class that implements the __call__ method\n+    3. None. This means you implement a class that inherits from either\n+    `KerasClassifier` or `KerasRegressor`. The __call__ method of the\n+    present class will then be treated as the default build_fn.\n \n-    'sk_params' takes both model parameters and fitting parameters. Legal model\n-    parameters are the arguments of 'build_fn'. Note that like all other\n-    estimators in scikit_learn, 'build_fn' should provide defalult velues for\n+    `sk_params` takes both model parameters and fitting parameters. Legal model\n+    parameters are the arguments of `build_fn`. Note that like all other\n+    estimators in scikit-learn, 'build_fn' should provide defalult values for\n     its arguments, so that you could create the estimator without passing any\n-    values to 'sk_params'.\n+    values to `sk_params`.\n \n-    'sk_params' could also accept parameters for calling 'fit', 'predict',\n-    'predict_proba', and 'score' function (e.g., nb_epoch, batch_size).\n-    fitting (predicting) parameters are adopts in the following order:\n+    `sk_params` could also accept parameters for calling `fit`, `predict`,\n+    `predict_proba`, and `score` methods (e.g., `nb_epoch`, `batch_size`).\n+    fitting (predicting) parameters are selected in the following order:\n \n     1. Values passed to the dictionary arguments of\n-    'fit', 'predict', 'predict_proba', and 'score' functions\n-    2. Values passed to 'sk_params'\n-    3. The default values of the keras.models.Sequential's\n-    'fit', 'predict', 'predict_proba' and 'score' functions\n+    `fit`, `predict`, `predict_proba`, and `score` methods\n+    2. Values passed to `sk_params`\n+    3. The default values of the `keras.models.Sequential`\n+    `fit`, `predict`, `predict_proba` and `score` methods\n \n-    When using scikit_learn's grid_search api, legal tunable parameters are\n-    those you could pass to 'sk_params', including fitting parameters.\n-    In other words, you could use grid_search to search for the best\n-    batch_size or nb_epoch as well as the model parameters.\n+    When using scikit-learn's `grid_search` API, legal tunable parameters are\n+    those you could pass to `sk_params`, including fitting parameters.\n+    In other words, you could use `grid_search` to search for the best\n+    `batch_size` or `nb_epoch` as well as the model parameters.\n     '''\n \n     def __init__(self, build_fn=None, **sk_params):\n@@ -61,7 +59,7 @@ class BaseWrapper(object):\n         # Arguments\n             deep: boolean, optional\n                 If True, will return the parameters for this estimator and\n-                contained subobjects that are estimators.\n+                contained sub-objects that are estimators.\n \n         # Returns\n             params : dict\n@@ -89,13 +87,13 @@ class BaseWrapper(object):\n         to the given training data.\n \n         # Arguments\n-            X : array-like, shape = (n_samples, n_features)\n+            X : array-like, shape `(n_samples, n_features)`\n                 Training samples where n_samples in the number of samples\n                 and n_features is the number of features.\n-            y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n+            y : array-like, shape `(n_samples,)` or `(n_samples, n_outputs)`\n                 True labels for X.\n             kwargs: dictionary arguments\n-                Legal arguments are the arguments of Sequential.fit\n+                Legal arguments are the arguments of `Sequential.fit`\n \n         # Returns\n             history : object\n@@ -110,8 +108,7 @@ class BaseWrapper(object):\n         else:\n             self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n \n-        if self.model.loss.__name__ == 'categorical_crossentropy'\\\n-                and len(y.shape) != 2:\n+        if self.model.loss.__name__ == 'categorical_crossentropy' and len(y.shape) != 2:\n             y = to_categorical(y)\n \n         fit_args = copy.deepcopy(self.filter_sk_params(Sequential.fit))\n@@ -142,54 +139,54 @@ class BaseWrapper(object):\n \n \n class KerasClassifier(BaseWrapper):\n-\n-    '''Implementation of the scikit-learn classifier API for Keras.'''\n+    '''Implementation of the scikit-learn classifier API for Keras.\n+    '''\n \n     def predict(self, X, **kwargs):\n-        '''# Returns the class predictions for the given test data.\n+        '''Returns the class predictions for the given test data.\n \n         # Arguments\n-            X : array-like, shape = (n_samples, n_features)\n+            X: array-like, shape `(n_samples, n_features)`\n                 Test samples where n_samples in the number of samples\n                 and n_features is the number of features.\n             kwargs: dictionary arguments\n-                Legal arguments are the arguments of Sequential.predict_classes\n+                Legal arguments are the arguments of `Sequential.predict_classes`.\n \n         # Returns\n-            preds : array-like, shape = (n_samples)\n+            preds: array-like, shape `(n_samples,)`\n                 Class predictions.\n         '''\n         kwargs = self.filter_sk_params(Sequential.predict_classes, kwargs)\n         return self.model.predict_classes(X, **kwargs)\n \n     def predict_proba(self, X, **kwargs):\n-        '''# Returns class probability estimates for the given test data.\n+        '''Returns class probability estimates for the given test data.\n \n         # Arguments\n-            X : array-like, shape = (n_samples, n_features)\n+            X: array-like, shape `(n_samples, n_features)`\n                 Test samples where n_samples in the number of samples\n                 and n_features is the number of features.\n             kwargs: dictionary arguments\n-                Legal arguments are the arguments of Sequential.predict_classes\n+                Legal arguments are the arguments of `Sequential.predict_classes`.\n \n         # Returns\n-            proba : array-like, shape = (n_samples, n_outputs)\n+            proba: array-like, shape `(n_samples, n_outputs)`\n                 Class probability estimates.\n         '''\n         kwargs = self.filter_sk_params(Sequential.predict_proba, kwargs)\n         return self.model.predict_proba(X, **kwargs)\n \n     def score(self, X, y, **kwargs):\n-        '''# Returns the mean accuracy on the given test data and labels.\n+        '''Returns the mean accuracy on the given test data and labels.\n \n         # Arguments\n-            X : array-like, shape = (n_samples, n_features)\n+            X: array-like, shape `(n_samples, n_features)`\n                 Test samples where n_samples in the number of samples\n                 and n_features is the number of features.\n-            y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n+            y: array-like, shape `(n_samples,)` or `(n_samples, n_outputs)`\n                 True labels for X.\n             kwargs: dictionary arguments\n-                Legal arguments are the arguments of Sequential.evaluate\n+                Legal arguments are the arguments of `Sequential.evaluate`.\n \n         # Returns\n             score: float\n@@ -202,20 +199,20 @@ class KerasClassifier(BaseWrapper):\n \n \n class KerasRegressor(BaseWrapper):\n-\n-    '''Implementation of the scikit-learn regressor API for Keras.'''\n+    '''Implementation of the scikit-learn regressor API for Keras.\n+    '''\n \n     def predict(self, X, **kwargs):\n         '''Returns predictions for the given test data.\n \n         # Arguments\n-            X : array-like, shape = (n_samples, n_features)\n+            X: array-like, shape `(n_samples, n_features)`\n                 Test samples where n_samples in the number of samples\n                 and n_features is the number of features.\n             kwargs: dictionary arguments\n-                Legal arguments are the arguments of Sequential.predict\n+                Legal arguments are the arguments of `Sequential.predict`.\n         # Returns\n-            preds : array-like, shape = (n_samples)\n+            preds: array-like, shape `(n_samples,)`\n                 Predictions.\n         '''\n         kwargs = self.filter_sk_params(Sequential.predict, kwargs)\n@@ -225,13 +222,13 @@ class KerasRegressor(BaseWrapper):\n         '''Returns the mean accuracy on the given test data and labels.\n \n         # Arguments\n-            X : array-like, shape = (n_samples, n_features)\n+            X: array-like, shape `(n_samples, n_features)`\n                 Test samples where n_samples in the number of samples\n                 and n_features is the number of features.\n-            y : array-like, shape = (n_samples)\n+            y: array-like, shape `(n_samples,)`\n                 True labels for X.\n             kwargs: dictionary arguments\n-                Legal arguments are the arguments of Sequential.evaluate\n+                Legal arguments are the arguments of `Sequential.evaluate`.\n \n         # Returns\n             score: float\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9efe17aeeafc6d8c1406a48f82fc63731d4b2b6c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 3593 | Contributors (this commit): 20 | Commits (past 90d): 39 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -149,9 +149,10 @@ class Recurrent(MaskedLayer):\n \n     def get_initial_states(self, x):\n         # build an all-zero tensor of shape (samples, output_dim)\n-        initial_state = x[:, 0, 0] * 0  # (samples, )\n-        initial_state = K.pack([initial_state] * self.output_dim)  # (output_dim, samples)\n-        initial_state = K.permute_dimensions(initial_state, (1, 0))  # (samples, output_dim)\n+        initial_state = K.zeros_like(x)  # (samples, timesteps, input_dim)\n+        initial_state = K.sum(initial_state, axis=1)  # (samples, input_dim)\n+        reducer = K.zeros((self.input_dim, self.output_dim))\n+        initial_state = K.dot(initial_state, reducer)  # (samples, output_dim)\n         initial_states = [initial_state for _ in range(len(self.states))]\n         return initial_states\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#48ce23086b18c07aba6daec86ffd91e919eadf0f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 8 | Churn Cumulative: 3601 | Contributors (this commit): 20 | Commits (past 90d): 39 | Contributors (cumulative): 20 | DMM Complexity: None\n\nDIFF:\n@@ -20,13 +20,17 @@ def time_distributed_dense(x, w, b=None, dropout=None,\n     if not output_dim:\n         # won't work with TensorFlow\n         output_dim = K.shape(w)[1]\n+\n     if dropout:\n+        # apply the same dropout pattern at every timestep\n         ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n         dropout_matrix = K.dropout(ones, dropout)\n+        expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n+        x *= expanded_dropout_matrix\n+\n     # collapse time dimension and batch dimension together\n     x = K.reshape(x, (-1, input_dim))\n-    if dropout:\n-        x *= K.concatenate([dropout_matrix] * timesteps, 0)\n+\n     x = K.dot(x, w)\n     if b:\n         x = x + b\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#becc5f3a2cd0599be17ac9f62e9ac78bde42702f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 37 | Lines Deleted: 19 | Files Changed: 3 | Hunks: 14 | Methods Changed: 16 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 56 | Churn Cumulative: 3098 | Contributors (this commit): 21 | Commits (past 90d): 31 | Contributors (cumulative): 25 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3,9 +3,26 @@ import numpy as np\n from . import backend as K\n \n \n-def get_fans(shape):\n-    fan_in = shape[0] if len(shape) == 2 else np.prod(shape[1:])\n-    fan_out = shape[1] if len(shape) == 2 else shape[0]\n+def get_fans(shape, dim_ordering='th'):\n+    if len(shape) == 2:\n+        fan_in = shape[0]\n+        fan_out = shape[1]\n+    elif len(shape) == 4 or len(shape) == 5:\n+        # assuming convolution kernels (2D or 3D).\n+        # TH kernel shape: (depth, input_depth, ...)\n+        # TF kernel shape: (..., input_depth, depth)\n+        if dim_ordering == 'th':\n+            fan_in = np.prod(shape[1:])\n+            fan_out = shape[0]\n+        elif dim_ordering == 'tf':\n+            fan_in = np.prod(shape[:-1])\n+            fan_out = shape[-1]\n+        else:\n+            raise Exception('Invalid dim_ordering: ' + dim_ordering)\n+    else:\n+        # no specific assumptions\n+        fan_in = np.sqrt(np.prod(shape))\n+        fan_out = np.sqrt(np.prod(shape))\n     return fan_in, fan_out\n \n \n@@ -19,39 +36,39 @@ def normal(shape, scale=0.05, name=None):\n                       name=name)\n \n \n-def lecun_uniform(shape, name=None):\n+def lecun_uniform(shape, name=None, dim_ordering='th'):\n     ''' Reference: LeCun 98, Efficient Backprop\n         http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf\n     '''\n-    fan_in, fan_out = get_fans(shape)\n+    fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n     scale = np.sqrt(3. / fan_in)\n     return uniform(shape, scale, name=name)\n \n \n-def glorot_normal(shape, name=None):\n+def glorot_normal(shape, name=None, dim_ordering='th'):\n     ''' Reference: Glorot & Bengio, AISTATS 2010\n     '''\n-    fan_in, fan_out = get_fans(shape)\n+    fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n     s = np.sqrt(2. / (fan_in + fan_out))\n     return normal(shape, s, name=name)\n \n \n-def glorot_uniform(shape, name=None):\n-    fan_in, fan_out = get_fans(shape)\n+def glorot_uniform(shape, name=None, dim_ordering='th'):\n+    fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n     s = np.sqrt(6. / (fan_in + fan_out))\n     return uniform(shape, s, name=name)\n \n \n-def he_normal(shape, name=None):\n+def he_normal(shape, name=None, dim_ordering='th'):\n     ''' Reference:  He et al., http://arxiv.org/abs/1502.01852\n     '''\n-    fan_in, fan_out = get_fans(shape)\n+    fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n     s = np.sqrt(2. / fan_in)\n     return normal(shape, s, name=name)\n \n \n-def he_uniform(shape, name=None):\n-    fan_in, fan_out = get_fans(shape)\n+def he_uniform(shape, name=None, dim_ordering='th'):\n+    fan_in, fan_out = get_fans(shape, dim_ordering=dim_ordering)\n     s = np.sqrt(6. / fan_in)\n     return uniform(shape, s, name=name)\n \n@@ -85,5 +102,6 @@ def one(shape, name=None):\n \n \n from .utils.generic_utils import get_from_module\n-def get(identifier):\n-    return get_from_module(identifier, globals(), 'initialization')\n+def get(identifier, **kwargs):\n+    return get_from_module(identifier, globals(),\n+                           'initialization', kwargs=kwargs)\n\n@@ -79,7 +79,7 @@ class Convolution1D(Layer):\n             raise Exception('Invalid border mode for Convolution1D:', border_mode)\n         self.nb_filter = nb_filter\n         self.filter_length = filter_length\n-        self.init = initializations.get(init)\n+        self.init = initializations.get(init, dim_ordering='th')\n         self.activation = activations.get(activation)\n         assert border_mode in {'valid', 'same'}, 'border_mode must be in {valid, same}'\n         self.border_mode = border_mode\n@@ -233,7 +233,7 @@ class Convolution2D(Layer):\n         self.nb_filter = nb_filter\n         self.nb_row = nb_row\n         self.nb_col = nb_col\n-        self.init = initializations.get(init)\n+        self.init = initializations.get(init, dim_ordering=dim_ordering)\n         self.activation = activations.get(activation)\n         assert border_mode in {'valid', 'same'}, 'border_mode must be in {valid, same}'\n         self.border_mode = border_mode\n@@ -412,7 +412,7 @@ class Convolution3D(Layer):\n         self.kernel_dim1 = kernel_dim1\n         self.kernel_dim2 = kernel_dim2\n         self.kernel_dim3 = kernel_dim3\n-        self.init = initializations.get(init)\n+        self.init = initializations.get(init, dim_ordering=dim_ordering)\n         self.activation = activations.get(activation)\n         assert border_mode in {'valid', 'same'}, 'border_mode must be in {valid, same}'\n         self.border_mode = border_mode\n\n@@ -35,7 +35,7 @@ def test_image_classification():\n         Activation('relu'),\n         Dense(y_test.shape[-1], activation='softmax')\n     ])\n-    model.compile(loss='categorical_crossentropy', optimizer='sgd')\n+    model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n     history = model.fit(X_train, y_train, nb_epoch=10, batch_size=16,\n                         validation_data=(X_test, y_test),\n                         show_accuracy=True, verbose=0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d9ca798c60641a0ed5eea22e45be95054157af0d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 498 | Contributors (this commit): 8 | Commits (past 90d): 15 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -37,13 +37,21 @@ class BatchNormalization(Layer):\n         weights: Initialization weights.\n             List of 2 numpy arrays, with shapes:\n             `[(input_shape,), (input_shape,)]`\n-\n+        beta_init: name of initialization function for shift parameter\n+            (see [initializations](../initializations.md)), or alternatively,\n+            Theano function to use for weights initialization. This parameter\n+            is only relevant if you don't pass a `weights` argument.\n+        gamma_init: name of initialization function for scale parameter (see\n+            [initializations](../initializations.md)), or alternatively, Theano\n+            function to use for weights initialization. This parameter is only\n+            relevant if you don't pass a `weights` argument.\n     # References\n         - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://arxiv.org/pdf/1502.03167v3.pdf)\n     '''\n     def __init__(self, epsilon=1e-6, mode=0, axis=-1, momentum=0.9,\n-                 weights=None, **kwargs):\n-        self.init = initializations.get(\"uniform\")\n+                 weights=None, beta_init=\"zero\", gamma_init=\"one\", **kwargs):\n+        self.beta_init = initializations.get(beta_init)\n+        self.gamma_init = initializations.get(gamma_init)\n         self.epsilon = epsilon\n         self.mode = mode\n         self.axis = axis\n@@ -55,9 +63,8 @@ class BatchNormalization(Layer):\n         input_shape = self.input_shape  # starts with samples axis\n         shape = (input_shape[self.axis],)\n \n-        self.gamma = self.init(shape,\n-                               name='{}_gamma'.format(self.name))\n-        self.beta = K.zeros(shape, name='{}_beta'.format(self.name))\n+        self.gamma = self.gamma_init(shape, name='{}_gamma'.format(self.name))\n+        self.beta = self.beta_init(shape, name='{}_beta'.format(self.name))\n         self.trainable_weights = [self.gamma, self.beta]\n \n         self.running_mean = K.zeros(shape,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0daec53acbf4c3df6c054b36ece5c1ae2db55d86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 510 | Contributors (this commit): 8 | Commits (past 90d): 16 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -39,17 +39,17 @@ class BatchNormalization(Layer):\n             `[(input_shape,), (input_shape,)]`\n         beta_init: name of initialization function for shift parameter\n             (see [initializations](../initializations.md)), or alternatively,\n-            Theano function to use for weights initialization. This parameter\n-            is only relevant if you don't pass a `weights` argument.\n+            Theano/TensorFlow function to use for weights initialization.\n+            This parameter is only relevant if you don't pass a `weights` argument.\n         gamma_init: name of initialization function for scale parameter (see\n-            [initializations](../initializations.md)), or alternatively, Theano\n-            function to use for weights initialization. This parameter is only\n-            relevant if you don't pass a `weights` argument.\n+            [initializations](../initializations.md)), or alternatively,\n+            Theano/TensorFlow function to use for weights initialization.\n+            This parameter is only relevant if you don't pass a `weights` argument.\n     # References\n         - [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://arxiv.org/pdf/1502.03167v3.pdf)\n     '''\n     def __init__(self, epsilon=1e-6, mode=0, axis=-1, momentum=0.9,\n-                 weights=None, beta_init=\"zero\", gamma_init=\"one\", **kwargs):\n+                 weights=None, beta_init='zero', gamma_init='one', **kwargs):\n         self.beta_init = initializations.get(beta_init)\n         self.gamma_init = initializations.get(gamma_init)\n         self.epsilon = epsilon\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7768ae04a2388e8608de9ed72e41a9d57f67f2c1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 16/16 | Churn Δ: 11 | Churn Cumulative: 4931 | Contributors (this commit): 39 | Commits (past 90d): 42 | Contributors (cumulative): 39 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1091,7 +1091,7 @@ class Sequential(Model, containers.Sequential):\n \n         # start generator thread storing batches into a queue\n         data_gen_queue, _data_stop = generator_queue(generator, max_q_size=max_data_q_size,\n-                                                     wait_time=wait_time)\n+                                                     wait_time=wait_time, nb_worker=nb_worker)\n         if do_validation and not val_gen:\n             X_val, y_val, sample_weight_val = self._check_generator_output(validation_data,\n                                                                            _data_stop)\n@@ -1142,7 +1142,8 @@ class Sequential(Model, containers.Sequential):\n                         val_outs = self.evaluate_generator(validation_data,\n                                                            nb_val_samples,\n                                                            show_accuracy=show_accuracy,\n-                                                           verbose=0)\n+                                                           verbose=0, nb_worker=nb_val_worker,\n+                                                           wait_time=wait_time)\n                     else:\n                         val_outs = self.evaluate(X_val, y_val,\n                                                  show_accuracy=show_accuracy,\n@@ -1600,7 +1601,7 @@ class Graph(Model, containers.Graph):\n \n         # start generator thread storing batches into a queue\n         data_gen_queue, _data_stop = generator_queue(generator, max_q_size=max_data_q_size,\n-                                                     wait_time=wait_time)\n+                                                     wait_time=wait_time, nb_worker=nb_worker)\n         if do_validation and not val_gen:\n             # TODO: _data_stop not really sensical here\n             data_val, sample_weight_val = self._check_generator_output(validation_data, _data_stop)\n@@ -1649,7 +1650,9 @@ class Graph(Model, containers.Graph):\n                     if val_gen:\n                         val_outs = self.evaluate_generator(validation_data,\n                                                            nb_val_samples,\n-                                                           verbose=0)\n+                                                           verbose=0,\n+                                                           nb_worker=nb_val_worker,\n+                                                           wait_time=wait_time)\n                     else:\n                         val_outs = self.evaluate(data_val,\n                                                  sample_weight=sample_weight_val,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4ec84541e3a33f906d49a3a58f5d249296a62c76", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 285 | Contributors (this commit): 8 | Commits (past 90d): 11 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -217,7 +217,7 @@ class ImageDataGenerator(object):\n         if self.featurewise_center:\n             x -= self.mean\n         if self.featurewise_std_normalization:\n-            x /= self.std\n+            x /= (self.std + 1e-7)\n \n         if self.zca_whitening:\n             flatx = np.reshape(x, (x.shape[0]*x.shape[1]*x.shape[2]))\n@@ -225,9 +225,9 @@ class ImageDataGenerator(object):\n             x = np.reshape(whitex, (x.shape[0], x.shape[1], x.shape[2]))\n \n         if self.samplewise_center:\n-            x -= np.mean(x)\n+            x -= np.mean(x, axis=1, keepdims=True)\n         if self.samplewise_std_normalization:\n-            x /= np.std(x)\n+            x /= (np.std(x, axis=1, keepdims=True) + 1e-7)\n \n         return x\n \n@@ -247,7 +247,6 @@ class ImageDataGenerator(object):\n         # TODO:\n         # zoom\n         # barrel/fisheye\n-        # shearing\n         # channel shifting\n         return x\n \n@@ -272,14 +271,13 @@ class ImageDataGenerator(object):\n             X -= self.mean\n         if self.featurewise_std_normalization:\n             self.std = np.std(X, axis=0)\n-            X /= self.std\n+            X /= (self.std + 1e-7)\n \n         if self.zca_whitening:\n             flatX = np.reshape(X, (X.shape[0], X.shape[1]*X.shape[2]*X.shape[3]))\n-            fudge = 10e-6\n             sigma = np.dot(flatX.T, flatX) / flatX.shape[1]\n             U, S, V = linalg.svd(sigma)\n-            self.principal_components = np.dot(np.dot(U, np.diag(1. / np.sqrt(S + fudge))), U.T)\n+            self.principal_components = np.dot(np.dot(U, np.diag(1. / np.sqrt(S + 10e-7))), U.T)\n \n \n class GraphImageDataGenerator(ImageDataGenerator):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d748db43aef07fda7e9222f35ec089559fc8fb23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 7937 | Contributors (this commit): 43 | Commits (past 90d): 70 | Contributors (cumulative): 43 | DMM Complexity: None\n\nDIFF:\n@@ -968,7 +968,8 @@ class Dense(Layer):\n             If you don't specify anything, no activation is applied\n             (ie. \"linear\" activation: a(x) = x).\n         weights: list of numpy arrays to set as initial weights.\n-            The list should have 1 element, of shape `(input_dim, output_dim)`.\n+            The list should have 2 elements, of shape `(input_dim, output_dim)`,\n+            and (input_dim,) for weights and biases respectively.\n         W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n             (eg. L1 or L2 regularization), applied to the main weights matrix.\n         b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n@@ -1081,7 +1082,8 @@ class TimeDistributedDense(MaskedLayer):\n             If you don't specify anything, no activation is applied\n             (ie. \"linear\" activation: a(x) = x).\n         weights: list of numpy arrays to set as initial weights.\n-            The list should have 1 element, of shape `(input_dim, output_dim)`.\n+            The list should have 2 elements, of shape `(input_dim, output_dim)`,\n+            and (input_dim,) for weights and biases respectively.\n         W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n             (eg. L1 or L2 regularization), applied to the main weights matrix.\n         b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n@@ -1962,7 +1964,8 @@ class Highway(Layer):\n             If you don't specify anything, no activation is applied\n             (ie. \"linear\" activation: a(x) = x).\n         weights: list of numpy arrays to set as initial weights.\n-            The list should have 1 element, of shape `(input_dim, output_dim)`.\n+            The list should have 2 elements, of shape `(input_dim, output_dim)`,\n+            and (input_dim,) for weights and biases respectively.\n         W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n             (eg. L1 or L2 regularization), applied to the main weights matrix.\n         b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#537fb1cc01dd05e894c85c2cd7ec4c6408a26441", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 7949 | Contributors (this commit): 43 | Commits (past 90d): 71 | Contributors (cumulative): 43 | DMM Complexity: None\n\nDIFF:\n@@ -968,8 +968,8 @@ class Dense(Layer):\n             If you don't specify anything, no activation is applied\n             (ie. \"linear\" activation: a(x) = x).\n         weights: list of numpy arrays to set as initial weights.\n-            The list should have 2 elements, of shape `(input_dim, output_dim)`,\n-            and (input_dim,) for weights and biases respectively.\n+            The list should have 2 elements, of shape `(input_dim, output_dim)`\n+            and (output_dim,) for weights and biases respectively.\n         W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n             (eg. L1 or L2 regularization), applied to the main weights matrix.\n         b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n@@ -1082,8 +1082,8 @@ class TimeDistributedDense(MaskedLayer):\n             If you don't specify anything, no activation is applied\n             (ie. \"linear\" activation: a(x) = x).\n         weights: list of numpy arrays to set as initial weights.\n-            The list should have 2 elements, of shape `(input_dim, output_dim)`,\n-            and (input_dim,) for weights and biases respectively.\n+            The list should have 2 elements, of shape `(input_dim, output_dim)`\n+            and (output_dim,) for weights and biases respectively.\n         W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n             (eg. L1 or L2 regularization), applied to the main weights matrix.\n         b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n@@ -1964,8 +1964,8 @@ class Highway(Layer):\n             If you don't specify anything, no activation is applied\n             (ie. \"linear\" activation: a(x) = x).\n         weights: list of numpy arrays to set as initial weights.\n-            The list should have 2 elements, of shape `(input_dim, output_dim)`,\n-            and (input_dim,) for weights and biases respectively.\n+            The list should have 2 elements, of shape `(input_dim, output_dim)`\n+            and (output_dim,) for weights and biases respectively.\n         W_regularizer: instance of [WeightRegularizer](../regularizers.md)\n             (eg. L1 or L2 regularization), applied to the main weights matrix.\n         b_regularizer: instance of [WeightRegularizer](../regularizers.md),\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9e2628e811b05885ee7cd298a86a80ee1b63ba14", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 372 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -7,11 +7,11 @@ and make sure the variable `weights_path` in this script matches the location of\n \n Run the script with:\n ```\n-python neural_style.py path_to_your_base_image.jpg path_to_your_reference.jpg prefix_for_results\n+python neural_style_transfer.py path_to_your_base_image.jpg path_to_your_reference.jpg prefix_for_results\n ```\n e.g.:\n ```\n-python neural_style.py img/tuebingen.jpg img/starry_night.jpg results/my_result\n+python neural_style_transfer.py img/tuebingen.jpg img/starry_night.jpg results/my_result\n ```\n \n It is preferrable to run this script on GPU, for speed.\n@@ -89,17 +89,17 @@ assert img_height == img_width, 'Due to the use of the Gram matrix, width and he\n def preprocess_image(image_path):\n     img = imresize(imread(image_path), (img_width, img_height))\n     img = img.transpose((2, 0, 1)).astype('float64')\n-    img[:, :, 0] -= 103.939\n-    img[:, :, 1] -= 116.779\n-    img[:, :, 2] -= 123.68\n+    img[0, :, :] -= 103.939\n+    img[1, :, :] -= 116.779\n+    img[2, :, :] -= 123.68\n     img = np.expand_dims(img, axis=0)\n     return img\n \n # util function to convert a tensor into a valid image\n def deprocess_image(x):\n-    x[:, :, 0] += 103.939\n-    x[:, :, 1] += 116.779\n-    x[:, :, 2] += 123.68\n+    x[0, :, :] += 103.939\n+    x[1, :, :] += 116.779\n+    x[2, :, :] += 123.68\n     x = x.transpose((1, 2, 0))\n     x = np.clip(x, 0, 255).astype('uint8')\n     return x\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b883761820e86903ce6132a458ee95e23427f7fa", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 378 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -89,17 +89,11 @@ assert img_height == img_width, 'Due to the use of the Gram matrix, width and he\n def preprocess_image(image_path):\n     img = imresize(imread(image_path), (img_width, img_height))\n     img = img.transpose((2, 0, 1)).astype('float64')\n-    img[0, :, :] -= 103.939\n-    img[1, :, :] -= 116.779\n-    img[2, :, :] -= 123.68\n     img = np.expand_dims(img, axis=0)\n     return img\n \n # util function to convert a tensor into a valid image\n def deprocess_image(x):\n-    x[0, :, :] += 103.939\n-    x[1, :, :] += 116.779\n-    x[2, :, :] += 123.68\n     x = x.transpose((1, 2, 0))\n     x = np.clip(x, 0, 255).astype('uint8')\n     return x\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6596cc79d60cfa08bdac4e9807da0501cdfe8534", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 11 | Files Changed: 1 | Hunks: 10 | Methods Changed: 1 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 33 | Churn Cumulative: 2922 | Contributors (this commit): 17 | Commits (past 90d): 17 | Contributors (cumulative): 17 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1105,8 +1105,9 @@ class ZeroPadding1D(Layer):\n     @property\n     def output_shape(self):\n         input_shape = self.input_shape\n+        length = input_shape[1] + self.padding * 2 if input_shape[1] is not None else None\n         return (input_shape[0],\n-                input_shape[1] + self.padding * 2,\n+                length,\n                 input_shape[2])\n \n     def get_output(self, train=False):\n@@ -1148,14 +1149,18 @@ class ZeroPadding2D(Layer):\n     def output_shape(self):\n         input_shape = self.input_shape\n         if self.dim_ordering == 'th':\n+            width = input_shape[2] + 2 * self.padding[0] if input_shape[2] is not None else None\n+            height = input_shape[3] + 2 * self.padding[1] if input_shape[3] is not None else None\n             return (input_shape[0],\n                     input_shape[1],\n-                    input_shape[2] + 2 * self.padding[0],\n-                    input_shape[3] + 2 * self.padding[1])\n+                    width,\n+                    height)\n         elif self.dim_ordering == 'tf':\n+            width = input_shape[1] + 2 * self.padding[0] if input_shape[1] is not None else None\n+            height = input_shape[2] + 2 * self.padding[1] if input_shape[2] is not None else None\n             return (input_shape[0],\n-                    input_shape[1] + 2 * self.padding[0],\n-                    input_shape[2] + 2 * self.padding[1],\n+                    width,\n+                    height,\n                     input_shape[3])\n         else:\n             raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n@@ -1205,16 +1210,22 @@ class ZeroPadding3D(Layer):\n     def output_shape(self):\n         input_shape = self.input_shape\n         if self.dim_ordering == 'th':\n+            dim1 = input_shape[2] + 2 * self.padding[0] if input_shape[2] is not None else None\n+            dim2 = input_shape[3] + 2 * self.padding[1] if input_shape[3] is not None else None\n+            dim3 = input_shape[4] + 2 * self.padding[2] if input_shape[4] is not None else None\n             return (input_shape[0],\n                     input_shape[1],\n-                    input_shape[2] + 2 * self.padding[0],\n-                    input_shape[3] + 2 * self.padding[1],\n-                    input_shape[4] + 2 * self.padding[2])\n+                    dim1,\n+                    dim2,\n+                    dim3)\n         elif self.dim_ordering == 'tf':\n+            dim1 = input_shape[1] + 2 * self.padding[0] if input_shape[1] is not None else None\n+            dim2 = input_shape[2] + 2 * self.padding[1] if input_shape[2] is not None else None\n+            dim3 = input_shape[3] + 2 * self.padding[2] if input_shape[3] is not None else None\n             return (input_shape[0],\n-                    input_shape[1] + 2 * self.padding[0],\n-                    input_shape[2] + 2 * self.padding[1],\n-                    input_shape[3] + 2 * self.padding[2],\n+                    dim1,\n+                    dim2,\n+                    dim3,\n                     input_shape[4])\n         else:\n             raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3cc242615dd1420e5dc97424ff5952ba73f99563", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 61 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 62 | Churn Cumulative: 116 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,38 @@ from .. import backend as K\n \n \n class TimeDistributed(MaskedLayer):\n+    \"\"\"This wrapper allows to apply a layer to every\n+    temporal slice of an input.\n+\n+    The input should be at least 3D,\n+    and the dimension of index one will be considered to be\n+    the temporal dimension.\n+\n+    Consider a batch of 32 samples, where each sample is a sequence of 10\n+    vectors of 16 dimensions. The batch input shape of the layer is then `(32, 10, 16)`\n+    (and the `input_shape`, not including the samples dimension, is `(10, 16)`).\n+\n+    You can then use `TimeDistributed` to apply a `Dense` layer to each of the 10 timesteps, independently:\n+    ```python\n+        model = Sequential()\n+        model.add(TimeDistributed(Dense(8), input_shape=(10, 16)))\n+    ```\n+\n+    The output will then have shape `(32, 10, 8)`.\n+\n+    Note this is strictly equivalent to using `layers.core.TimeDistributedDense`.\n+    However what is different about `TimeDistributed`\n+    is that it can be used with arbitrary layers, not just `Dense`,\n+    for instance with a `Convolution2D` layer:\n+\n+    ```python\n+        model = Sequential()\n+        model.add(TimeDistributed(Convolution2D(64, 3, 3), input_shape=(10, 3, 299, 299)))\n+    ```\n+\n+    # Arguments\n+        layer: a layer instance.\n+    \"\"\"\n \n     def __init__(self, layer, **kwargs):\n         self.layer = layer\n@@ -17,6 +49,7 @@ class TimeDistributed(MaskedLayer):\n \n         trainable_weights, regularizers, constraints, updates = self.layer.get_params()\n         self.trainable_weights = trainable_weights\n+        self.non_trainable_weights = self.layer.non_traible_weights\n         self.regularizers = regularizers\n         self.constraints = constraints\n         self.updates = updates\n@@ -31,6 +64,20 @@ class TimeDistributed(MaskedLayer):\n         X = self.get_input(train)\n         mask = self.get_input_mask(train)\n \n+        if K._BACKEND == 'tensorflow':\n+            if not self.input_shape[1]:\n+                raise Exception('When using TensorFlow, you should define ' +\n+                                'explicitly the number of timesteps of ' +\n+                                'your sequences.\\n' +\n+                                'If your first layer is an Embedding, ' +\n+                                'make sure to pass it an \"input_length\" ' +\n+                                'argument. Otherwise, make sure ' +\n+                                'the first layer has ' +\n+                                'an \"input_shape\" or \"batch_input_shape\" ' +\n+                                'argument, including the time axis.')\n+\n+        if self.input_shape[0]:\n+            # batch size matters, use rnn-based implementation\n             def step(x, states):\n                 output = self.layer(x, train=train)\n                 return output, []\n@@ -38,7 +85,20 @@ class TimeDistributed(MaskedLayer):\n             last_output, outputs, states = K.rnn(step, X,\n                                                  initial_states=[],\n                                                  mask=mask)\n-        return outputs\n+            y = outputs\n+        else:\n+            # no batch size specified, therefore the layer will be able\n+            # to process batches of any size\n+            # we can go with reshape-based implementation for performance\n+            input_shape = self.input_shape\n+            x = K.reshape(X, (-1, ) + input_shape[2:])  # (nb_samples * timesteps, ...)\n+            y = self.layer(x, train=False)  # (nb_samples * timesteps, ...)\n+            input_length = input_shape[1]\n+            if not input_length:\n+                input_length = K.shape(X)[1]\n+            # (nb_samples, timesteps, ...)\n+            y = K.reshape(y, (-1, input_length) + self.layer.output_shape[1:])\n+        return y\n \n     def get_weights(self):\n         weights = self.layer.get_weights()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b8c59acd779aede1c07782b2b174a344f242891e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 69 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,10 +12,10 @@ def test_TimeDistributed():\n     model = Sequential()\n     model.add(wrappers.TimeDistributed(core.Dense(2), input_shape=(3, 4)))\n     model.add(core.Activation('relu'))\n-\n     model.compile(optimizer='rmsprop', loss='mse')\n     model.fit(np.random.random((10, 3, 4)), np.random.random((10, 3, 2)), nb_epoch=1, batch_size=10)\n \n+    # test config\n     model.get_config()\n \n     # compare to TimeDistributedDense\n@@ -26,7 +26,15 @@ def test_TimeDistributed():\n     reference = Sequential()\n     reference.add(core.TimeDistributedDense(2, input_shape=(3, 4), weights=weights))\n     reference.add(core.Activation('relu'))\n+    reference.compile(optimizer='rmsprop', loss='mse')\n \n+    reference_output = reference.predict(test_input)\n+    assert_allclose(test_output, reference_output, atol=1e-05)\n+\n+    # test when specifying a batch_input_shape\n+    reference = Sequential()\n+    reference.add(core.TimeDistributedDense(2, batch_input_shape=(1, 3, 4), weights=weights))\n+    reference.add(core.Activation('relu'))\n     reference.compile(optimizer='rmsprop', loss='mse')\n \n     reference_output = reference.predict(test_input)\n@@ -36,13 +44,22 @@ def test_TimeDistributed():\n     model = Sequential()\n     model.add(wrappers.TimeDistributed(convolutional.Convolution2D(5, 2, 2, border_mode='same'), input_shape=(2, 3, 4, 4)))\n     model.add(core.Activation('relu'))\n-\n     model.compile(optimizer='rmsprop', loss='mse')\n     model.train_on_batch(np.random.random((1, 2, 3, 4, 4)), np.random.random((1, 2, 5, 4, 4)))\n \n     model = model_from_json(model.to_json())\n     model.summary()\n \n+    # test stacked layers\n+    model = Sequential()\n+    model.add(wrappers.TimeDistributed(core.Dense(2), input_shape=(3, 4)))\n+    model.add(wrappers.TimeDistributed(core.Dense(3)))\n+    model.add(core.Activation('relu'))\n+    model.compile(optimizer='rmsprop', loss='mse')\n+\n+    model.fit(np.random.random((10, 3, 4)), np.random.random((10, 3, 3)), nb_epoch=1, batch_size=10)\n+\n+\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
