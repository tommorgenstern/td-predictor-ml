{"custom_id": "scrapy#47a992615a046d20c31bfca6f8b65c3194e9fd30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 5 | Methods Changed: 7 | Complexity Δ (Sum/Max): 8/6 | Churn Δ: 46 | Churn Cumulative: 385 | Contributors (this commit): 13 | Commits (past 90d): 3 | Contributors (cumulative): 19 | DMM Complexity: 0.4642857142857143\n\nDIFF:\n@@ -1,6 +1,8 @@\n \"\"\"\n Helper functions for serializing (and deserializing) requests.\n \"\"\"\n+import inspect\n+\n from scrapy.http import Request\n from scrapy.utils.python import to_unicode\n from scrapy.utils.misc import load_object\n@@ -90,7 +92,9 @@ def _find_method(obj, func):\n             pass\n         else:\n             if func_self is obj:\n-                name = func.__func__.__name__\n+                members = inspect.getmembers(obj, predicate=inspect.ismethod)\n+                for name, obj_func in members:\n+                    if obj_func.__func__ is func.__func__:\n                         if _is_private_method(name):\n                             return _mangle_private_name(obj, func, name)\n                         return name\n\n@@ -69,6 +69,26 @@ class RequestSerializationTest(unittest.TestCase):\n                     errback=self.spider.handle_error)\n         self._assert_serializes_ok(r, spider=self.spider)\n \n+    def test_reference_callback_serialization(self):\n+        r = Request(\"http://www.example.com\",\n+                    callback=self.spider.parse_item_reference,\n+                    errback=self.spider.handle_error_reference)\n+        self._assert_serializes_ok(r, spider=self.spider)\n+        request_dict = request_to_dict(r, self.spider)\n+        self.assertEqual(request_dict['callback'], 'parse_item_reference')\n+        self.assertEqual(request_dict['errback'], 'handle_error_reference')\n+\n+    def test_private_reference_callback_serialization(self):\n+        r = Request(\"http://www.example.com\",\n+                    callback=self.spider._TestSpider__parse_item_reference,\n+                    errback=self.spider._TestSpider__handle_error_reference)\n+        self._assert_serializes_ok(r, spider=self.spider)\n+        request_dict = request_to_dict(r, self.spider)\n+        self.assertEqual(request_dict['callback'],\n+                         '_TestSpider__parse_item_reference')\n+        self.assertEqual(request_dict['errback'],\n+                         '_TestSpider__handle_error_reference')\n+\n     def test_private_callback_serialization(self):\n         r = Request(\"http://www.example.com\",\n                     callback=self.spider._TestSpider__parse_item_private,\n@@ -131,8 +151,28 @@ class TestSpiderMixin:\n         pass\n \n \n+def parse_item(response):\n+    pass\n+\n+\n+def handle_error(failure):\n+    pass\n+\n+\n+def private_parse_item(response):\n+    pass\n+\n+\n+def private_handle_error(failure):\n+    pass\n+\n+\n class TestSpider(Spider, TestSpiderMixin):\n     name = 'test'\n+    parse_item_reference = parse_item\n+    handle_error_reference = handle_error\n+    __parse_item_reference = private_parse_item\n+    __handle_error_reference = private_handle_error\n \n     def parse_item(self, response):\n         pass\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5980ae72c6cb177f47fbb41d17837e8d98d50025", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 6 | Methods Changed: 5 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 15 | Churn Cumulative: 2071 | Contributors (this commit): 21 | Commits (past 90d): 8 | Contributors (cumulative): 21 | DMM Complexity: 0.0\n\nDIFF:\n@@ -428,7 +428,7 @@ class FeedExportTestBase(ABC, unittest.TestCase):\n                     yield item\n \n         data = yield self.run_and_export(TestSpider, settings)\n-        defer.returnValue(data)\n+        return data\n \n     @defer.inlineCallbacks\n     def exported_no_data(self, settings):\n@@ -443,7 +443,7 @@ class FeedExportTestBase(ABC, unittest.TestCase):\n                 pass\n \n         data = yield self.run_and_export(TestSpider, settings)\n-        defer.returnValue(data)\n+        return data\n \n     @defer.inlineCallbacks\n     def assertExported(self, items, header, rows, settings=None, ordered=True):\n@@ -735,8 +735,7 @@ class FeedExportTest(FeedExportTestBase):\n         formats = {\n             'json': u'[{\"foo\": \"Test\\\\u00d6\"}]'.encode('utf-8'),\n             'jsonlines': u'{\"foo\": \"Test\\\\u00d6\"}\\n'.encode('utf-8'),\n-            'xml': u'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items><item><foo>Test\\xd6</foo></item></items>'.encode(\n-                'utf-8'),\n+            'xml': u'<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n<items><item><foo>Test\\xd6</foo></item></items>'.encode('utf-8'),\n             'csv': u'foo\\r\\nTest\\xd6\\r\\n'.encode('utf-8'),\n         }\n \n@@ -753,8 +752,7 @@ class FeedExportTest(FeedExportTestBase):\n         formats = {\n             'json': u'[{\"foo\": \"Test\\xd6\"}]'.encode('latin-1'),\n             'jsonlines': u'{\"foo\": \"Test\\xd6\"}\\n'.encode('latin-1'),\n-            'xml': u'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n<items><item><foo>Test\\xd6</foo></item></items>'.encode(\n-                'latin-1'),\n+            'xml': u'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n<items><item><foo>Test\\xd6</foo></item></items>'.encode('latin-1'),\n             'csv': u'foo\\r\\nTest\\xd6\\r\\n'.encode('latin-1'),\n         }\n \n@@ -775,8 +773,7 @@ class FeedExportTest(FeedExportTestBase):\n \n         formats = {\n             'json': u'[\\n{\"bar\": \"BAR\"}\\n]'.encode('utf-8'),\n-            'xml': u'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n<items>\\n  <item>\\n    <foo>FOO</foo>\\n  </item>\\n</items>'.encode(\n-                'latin-1'),\n+            'xml': u'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n<items>\\n  <item>\\n    <foo>FOO</foo>\\n  </item>\\n</items>'.encode('latin-1'),\n             'csv': u'bar,foo\\r\\nBAR,FOO\\r\\n'.encode('utf-8'),\n         }\n \n@@ -1148,7 +1145,7 @@ class PartialDeliveriesTest(FeedExportTestBase):\n         yield self.assertExported(items, header, rows, settings=settings)\n \n     def test_wrong_path(self):\n-        \"\"\"If path without %(time)s or %(batch_id)s an exception must be raised\"\"\"\n+        \"\"\"If path is without %(time)s or %(batch_id)s an exception must be raised\"\"\"\n         settings = {\n             'FEEDS': {\n                 self._random_temp_filename(): {'format': 'xml'},\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#901892dab380d54186ae855bf65a20eb2467de04", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 238 | Contributors (this commit): 15 | Commits (past 90d): 6 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -295,8 +295,6 @@ intersphinx_mapping = {\n # ------------------------------------\n \n hoverxref_auto_ref = True\n-hoverxref_project = \"scrapy\"\n-hoverxref_version = release\n hoverxref_role_types = {\n     \"class\": \"tooltip\",\n     \"confval\": \"tooltip\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#94ee68695a42ec8d3ccdff71bfc2fa33d5ea7049", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): -7/0 | Churn Δ: 5 | Churn Cumulative: 365 | Contributors (this commit): 12 | Commits (past 90d): 4 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -218,9 +218,8 @@ class MockServer():\n         self.proc.communicate()\n \n     def url(self, path, is_secure=False):\n-        host = self.http_address.replace('0.0.0.0', '127.0.0.1')\n-        if is_secure:\n-            host = self.https_address\n+        host = self.https_address if is_secure else self.http_address\n+        host = host.replace('0.0.0.0', '127.0.0.1')\n         return host + path\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e0921cab667a1fefe9730363ed45091edf1250e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 52 | Files Changed: 2 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): -8/0 | Churn Δ: 53 | Churn Cumulative: 438 | Contributors (this commit): 13 | Commits (past 90d): 5 | Contributors (cumulative): 19 | DMM Complexity: 0.046511627906976744\n\nDIFF:\n@@ -70,20 +70,6 @@ def request_from_dict(d, spider=None):\n     )\n \n \n-def _is_private_method(name):\n-    return name.startswith('__') and not name.endswith('__')\n-\n-\n-def _mangle_private_name(obj, func, name):\n-    qualname = getattr(func, '__qualname__', None)\n-    if qualname is None:\n-        classname = obj.__class__.__name__.lstrip('_')\n-        return '_%s%s' % (classname, name)\n-    else:\n-        splits = qualname.split('.')\n-        return '_%s%s' % (splits[-2], splits[-1])\n-\n-\n def _find_method(obj, func):\n     if obj:\n         try:\n@@ -95,8 +81,6 @@ def _find_method(obj, func):\n                 members = inspect.getmembers(obj, predicate=inspect.ismethod)\n                 for name, obj_func in members:\n                     if obj_func.__func__ is func.__func__:\n-                        if _is_private_method(name):\n-                            return _mangle_private_name(obj, func, name)\n                         return name\n     raise ValueError(\"Function %s is not a method of: %s\" % (func, obj))\n \n\n@@ -2,7 +2,7 @@ import unittest\n \n from scrapy.http import Request, FormRequest\n from scrapy.spiders import Spider\n-from scrapy.utils.reqser import request_to_dict, request_from_dict, _is_private_method, _mangle_private_name\n+from scrapy.utils.reqser import request_to_dict, request_from_dict\n \n \n class RequestSerializationTest(unittest.TestCase):\n@@ -101,41 +101,6 @@ class RequestSerializationTest(unittest.TestCase):\n                     errback=self.spider.handle_error)\n         self._assert_serializes_ok(r, spider=self.spider)\n \n-    def test_private_callback_name_matching(self):\n-        self.assertTrue(_is_private_method('__a'))\n-        self.assertTrue(_is_private_method('__a_'))\n-        self.assertTrue(_is_private_method('__a_a'))\n-        self.assertTrue(_is_private_method('__a_a_'))\n-        self.assertTrue(_is_private_method('__a__a'))\n-        self.assertTrue(_is_private_method('__a__a_'))\n-        self.assertTrue(_is_private_method('__a___a'))\n-        self.assertTrue(_is_private_method('__a___a_'))\n-        self.assertTrue(_is_private_method('___a'))\n-        self.assertTrue(_is_private_method('___a_'))\n-        self.assertTrue(_is_private_method('___a_a'))\n-        self.assertTrue(_is_private_method('___a_a_'))\n-        self.assertTrue(_is_private_method('____a_a_'))\n-\n-        self.assertFalse(_is_private_method('_a'))\n-        self.assertFalse(_is_private_method('_a_'))\n-        self.assertFalse(_is_private_method('__a__'))\n-        self.assertFalse(_is_private_method('__'))\n-        self.assertFalse(_is_private_method('___'))\n-        self.assertFalse(_is_private_method('____'))\n-\n-    def _assert_mangles_to(self, obj, name):\n-        func = getattr(obj, name)\n-        self.assertEqual(\n-            _mangle_private_name(obj, func, func.__name__),\n-            name\n-        )\n-\n-    def test_private_name_mangling(self):\n-        self._assert_mangles_to(\n-            self.spider, '_TestSpider__parse_item_private')\n-        self._assert_mangles_to(\n-            self.spider, '_TestSpiderMixin__mixin_callback')\n-\n     def test_unserializable_callback1(self):\n         r = Request(\"http://www.example.com\", callback=lambda x: x)\n         self.assertRaises(ValueError, request_to_dict, r)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#94c95020b391c3298f4a7fd7608d48d74117bf43", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 200 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -80,6 +80,13 @@ def _find_method(obj, func):\n             if func_self is obj:\n                 members = inspect.getmembers(obj, predicate=inspect.ismethod)\n                 for name, obj_func in members:\n+                    # We need to use __func__ to access the original\n+                    # function object because instance method objects\n+                    # are generated each time attribute is retrieved from\n+                    # instance.\n+                    #\n+                    # Reference: The standard type hierarchy\n+                    # https://docs.python.org/3/reference/datamodel.html\n                     if obj_func.__func__ is func.__func__:\n                         return name\n     raise ValueError(\"Function %s is not a method of: %s\" % (func, obj))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c9229922772a4d7f92a26786d6ea441609043a09", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 38 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -23,6 +23,7 @@ class LocalhostSpider(Spider):\n         self.logger.info(\"IP address: %s\" % response.ip_address)\n \n \n+if __name__ == \"__main__\":\n     with MockServer() as mock_http_server, MockDNSServer() as mock_dns_server:\n         port = urlparse(mock_http_server.http_address).port\n         url = \"http://not.a.real.domain:{port}/echo\".format(port=port)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7cc9601029274124804e63ebebfa8783ac175205", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 744 | Contributors (this commit): 12 | Commits (past 90d): 8 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -309,14 +309,8 @@ class CrawlerProcessSubprocess(unittest.TestCase):\n     def test_ipv6_alternative_name_resolver(self):\n         log = self.run_script('alternative_name_resolver.py')\n         self.assertIn('Spider closed (finished)', log)\n-        self.assertTrue(any([\n-            \"twisted.internet.error.ConnectionRefusedError\" in log,\n-            \"twisted.internet.error.ConnectError\" in log,\n-        ]))\n-        self.assertTrue(any([\n-            \"'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 1,\" in log,\n-            \"'downloader/exception_type_count/twisted.internet.error.ConnectError': 1,\" in log,\n-        ]))\n+        self.assertRegex(log, r\"twisted\\.internet\\.error\\.(?:ConnectionRefusedError|ConnectError)\")\n+        self.assertRegex(log, r\"'downloader/exception_type_count/twisted\\.internet\\.error\\.(?:ConnectionRefusedError|ConnectError)': 1,\")\n \n     def test_reactor_select(self):\n         log = self.run_script(\"twisted_reactor_select.py\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cf4180308982d9cd017167238a3df8de7900646d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 746 | Contributors (this commit): 12 | Commits (past 90d): 9 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,6 @@\n import logging\n import os\n+import platform\n import subprocess\n import sys\n import warnings\n@@ -317,6 +318,7 @@ class CrawlerProcessSubprocess(unittest.TestCase):\n         self.assertIn(\"Spider closed (finished)\", log)\n         self.assertIn(\"Using reactor: twisted.internet.selectreactor.SelectReactor\", log)\n \n+    @mark.skipif(platform.system() == 'Windows', reason=\"PollReactor is not supported on Windows\")\n     def test_reactor_poll(self):\n         log = self.run_script(\"twisted_reactor_poll.py\")\n         self.assertIn(\"Spider closed (finished)\", log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ea3e675801fe41c7517c58e21b46831940fbd064", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 313 | Contributors (this commit): 13 | Commits (past 90d): 3 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -8,7 +8,7 @@ from scrapy.http import XmlResponse, TextResponse, Response\n from tests import get_testdata\n \n \n-FOOBAR_NL = u\"foo\\nbar\"\n+FOOBAR_NL = \"foo{}bar\".format(os.linesep)\n \n \n class XmliterTestCase(unittest.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1ade3fc723d1e5d7b6a3300b454d32656bdb8d28", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 15 | Churn Cumulative: 260 | Contributors (this commit): 12 | Commits (past 90d): 4 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -110,6 +110,21 @@ class RequestSerializationTest(unittest.TestCase):\n         r = Request(\"http://www.example.com\", callback=self.spider.parse_item)\n         self.assertRaises(ValueError, request_to_dict, r)\n \n+    def test_unserializable_callback3(self):\n+        \"\"\"Parser method is removed or replaced dynamically.\"\"\"\n+\n+        class MySpider(Spider):\n+\n+            name = 'my_spider'\n+\n+            def parse(self, response):\n+                pass\n+\n+        spider = MySpider()\n+        r = Request(\"http://www.example.com\", callback=spider.parse)\n+        setattr(spider, 'parse', None)\n+        self.assertRaises(ValueError, request_to_dict, r, spider=spider)\n+\n \n class TestSpiderMixin:\n     def __mixin_callback(self, response):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ec76445dd6753074c1531571f66467eecf22b498", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 64 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 64 | Churn Cumulative: 2135 | Contributors (this commit): 21 | Commits (past 90d): 9 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1128,6 +1128,7 @@ class PartialDeliveriesTest(FeedExportTestBase):\n \n     @defer.inlineCallbacks\n     def test_export_items(self):\n+        \"\"\" Test partial deliveries in all supported formats \"\"\"\n         items = [\n             self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n             self.MyItem({'foo': 'bar2', 'egg': 'spam2', 'baz': 'quux2'}),\n@@ -1189,3 +1190,66 @@ class PartialDeliveriesTest(FeedExportTestBase):\n             data = yield self.exported_no_data(settings)\n             data = dict(data)\n             self.assertEqual(expctd, data[fmt][0])\n+\n+    @defer.inlineCallbacks\n+    def test_export_multiple_configs(self):\n+        items = [dict({'foo': u'FOO', 'bar': u'BAR'}), dict({'foo': u'FOO1', 'bar': u'BAR1'})]\n+\n+        formats = {\n+            'json': [u'[\\n{\"bar\": \"BAR\"}\\n]'.encode('utf-8'),\n+                     u'[\\n{\"bar\": \"BAR1\"}\\n]'.encode('utf-8')],\n+            'xml': [u'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n<items>\\n  <item>\\n    <foo>FOO</foo>\\n  </item>\\n</items>'.encode('latin-1'),\n+                    u'<?xml version=\"1.0\" encoding=\"latin-1\"?>\\n<items>\\n  <item>\\n    <foo>FOO1</foo>\\n  </item>\\n</items>'.encode('latin-1')],\n+            'csv': [u'bar,foo\\r\\nBAR,FOO\\r\\n'.encode('utf-8'),\n+                    u'bar,foo\\r\\nBAR1,FOO1\\r\\n'.encode('utf-8')],\n+        }\n+\n+        settings = {\n+            'FEEDS': {\n+                os.path.join(self._random_temp_filename(), 'json', self._file_mark): {\n+                    'format': 'json',\n+                    'indent': 0,\n+                    'fields': ['bar'],\n+                    'encoding': 'utf-8',\n+                },\n+                os.path.join(self._random_temp_filename(), 'xml', self._file_mark): {\n+                    'format': 'xml',\n+                    'indent': 2,\n+                    'fields': ['foo'],\n+                    'encoding': 'latin-1',\n+                },\n+                os.path.join(self._random_temp_filename(), 'csv', self._file_mark): {\n+                    'format': 'csv',\n+                    'indent': None,\n+                    'fields': ['bar', 'foo'],\n+                    'encoding': 'utf-8',\n+                },\n+            },\n+            'FEED_STORAGE_BATCH_SIZE': 1,\n+        }\n+        data = yield self.exported_data(items, settings)\n+        for fmt, expected in formats.items():\n+            for expected_batch, got_batch in zip(expected, data[fmt]):\n+                self.assertEqual(expected_batch, got_batch)\n+\n+    @defer.inlineCallbacks\n+    def test_batch_path_differ(self):\n+        \"\"\"\n+        Test that the name of all batch files differ from each other.\n+        So %(time)s replaced with the current date.\n+        \"\"\"\n+        items = [\n+            self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n+            self.MyItem({'foo': 'bar2', 'egg': 'spam2', 'baz': 'quux2'}),\n+            self.MyItem({'foo': 'bar3', 'baz': 'quux3'}),\n+        ]\n+        settings = {\n+            'FEEDS': {\n+                os.path.join(self._random_temp_filename(), '%(time)s'): {\n+                    'format': 'json',\n+                },\n+            },\n+            'FEED_STORAGE_BATCH_SIZE': 1,\n+        }\n+        data = yield self.exported_data(items, settings)\n+        self.assertEqual(len(items) + 1, len(data['json']))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#773ddf77ba36613a90d35e583b29e9020cc2aace", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 28 | Churn Cumulative: 491 | Contributors (this commit): 15 | Commits (past 90d): 4 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -84,6 +84,7 @@ class FilesPipelineTestCase(unittest.TestCase):\n \n         result = yield self.pipeline.process_item(item, None)\n         self.assertEqual(result['files'][0]['checksum'], 'abc')\n+        self.assertEqual(result['files'][0]['status'], 'uptodate')\n \n         for p in patchers:\n             p.stop()\n@@ -105,6 +106,29 @@ class FilesPipelineTestCase(unittest.TestCase):\n \n         result = yield self.pipeline.process_item(item, None)\n         self.assertNotEqual(result['files'][0]['checksum'], 'abc')\n+        self.assertEqual(result['files'][0]['status'], 'downloaded')\n+\n+        for p in patchers:\n+            p.stop()\n+\n+    @defer.inlineCallbacks\n+    def test_file_cached(self):\n+        item_url = \"http://example.com/file3.pdf\"\n+        item = _create_item_with_files(item_url)\n+        patchers = [\n+            mock.patch.object(FilesPipeline, 'inc_stats', return_value=True),\n+            mock.patch.object(FSFilesStore, 'stat_file', return_value={\n+                'checksum': 'abc',\n+                'last_modified': time.time() - (self.pipeline.expires * 60 * 60 * 24 * 2)}),\n+            mock.patch.object(FilesPipeline, 'get_media_requests',\n+                              return_value=[_prepare_request_object(item_url, flags=['cached'])])\n+        ]\n+        for p in patchers:\n+            p.start()\n+\n+        result = yield self.pipeline.process_item(item, None)\n+        self.assertNotEqual(result['files'][0]['checksum'], 'abc')\n+        self.assertEqual(result['files'][0]['status'], 'cached')\n \n         for p in patchers:\n             p.stop()\n@@ -403,10 +427,10 @@ def _create_item_with_files(*files):\n     return item\n \n \n-def _prepare_request_object(item_url):\n+def _prepare_request_object(item_url, flags=None):\n     return Request(\n         item_url,\n-        meta={'response': Response(item_url, status=200, body=b'data')})\n+        meta={'response': Response(item_url, status=200, body=b'data', flags=flags)})\n \n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1fecacbb1a3c2c4c61c202561f9c51f7f6b191ba", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 749 | Contributors (this commit): 12 | Commits (past 90d): 10 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -310,8 +310,7 @@ class CrawlerProcessSubprocess(unittest.TestCase):\n     def test_ipv6_alternative_name_resolver(self):\n         log = self.run_script('alternative_name_resolver.py')\n         self.assertIn('Spider closed (finished)', log)\n-        self.assertRegex(log, r\"twisted\\.internet\\.error\\.(?:ConnectionRefusedError|ConnectError)\")\n-        self.assertRegex(log, r\"'downloader/exception_type_count/twisted\\.internet\\.error\\.(?:ConnectionRefusedError|ConnectError)': 1,\")\n+        self.assertNotIn(\"twisted.internet.error.DNSLookupError\", log)\n \n     def test_reactor_select(self):\n         log = self.run_script(\"twisted_reactor_select.py\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#efb6f13debf9406a214a9cee3d94d47875d542f5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 50 | Lines Deleted: 23 | Files Changed: 11 | Hunks: 17 | Methods Changed: 18 | Complexity Δ (Sum/Max): 16/6 | Churn Δ: 73 | Churn Cumulative: 7284 | Contributors (this commit): 65 | Commits (past 90d): 37 | Contributors (cumulative): 166 | DMM Complexity: 0.2962962962962963\n\nDIFF:\n@@ -23,7 +23,8 @@ class ScrapyCommand:\n         self.settings = None  # set in scrapy.cmdline\n \n     def set_crawler(self, crawler):\n-        assert not hasattr(self, '_crawler'), \"crawler already set\"\n+        if hasattr(self, '_crawler'):\n+            raise RuntimeError(\"crawler already set\")\n         self._crawler = crawler\n \n     def syntax(self):\n\n@@ -58,7 +58,11 @@ class ReturnsContract(Contract):\n     def __init__(self, *args, **kwargs):\n         super(ReturnsContract, self).__init__(*args, **kwargs)\n \n-        assert len(self.args) in [1, 2, 3]\n+        if len(self.args) not in [1, 2, 3]:\n+            raise ValueError(\n+                \"Incorrect argument quantity: expected 1, 2 or 3, got %i\"\n+                % len(self.args)\n+            )\n         self.obj_name = self.args[0] or None\n         self.obj_type = self.objects[self.obj_name]\n \n\n@@ -45,8 +45,9 @@ class DownloaderMiddlewareManager(MiddlewareManager):\n \n         @defer.inlineCallbacks\n         def process_response(response):\n-            assert response is not None, 'Received None in process_response'\n-            if isinstance(response, Request):\n+            if response is None:\n+                raise TypeError(\"Received None in process_response\")\n+            elif isinstance(response, Request):\n                 return response\n \n             for method in self.methods['process_response']:\n\n@@ -73,7 +73,8 @@ class ExecutionEngine:\n     @defer.inlineCallbacks\n     def start(self):\n         \"\"\"Start the execution engine\"\"\"\n-        assert not self.running, \"Engine already running\"\n+        if self.running:\n+            raise RuntimeError(\"Engine already running\")\n         self.start_time = time()\n         yield self.signals.send_catch_log_deferred(signal=signals.engine_started)\n         self.running = True\n@@ -82,7 +83,8 @@ class ExecutionEngine:\n \n     def stop(self):\n         \"\"\"Stop the execution engine gracefully\"\"\"\n-        assert self.running, \"Engine not running\"\n+        if not self.running:\n+            raise RuntimeError(\"Engine not running\")\n         self.running = False\n         dfd = self._close_all_spiders()\n         return dfd.addBoth(lambda _: self._finish_stopping_engine())\n@@ -165,7 +167,11 @@ class ExecutionEngine:\n         return d\n \n     def _handle_downloader_output(self, response, request, spider):\n-        assert isinstance(response, (Request, Response, Failure)), response\n+        if not isinstance(response, (Request, Response, Failure)):\n+            raise TypeError(\n+                \"Incorrect type: expected Request, Response or Failure, got %s: %r\"\n+                % (type(response), response)\n+            )\n         # downloader middleware can return requests (for example, redirects)\n         if isinstance(response, Request):\n             self.crawl(response, spider)\n@@ -205,8 +211,8 @@ class ExecutionEngine:\n         return not bool(self.slot)\n \n     def crawl(self, request, spider):\n-        assert spider in self.open_spiders, \\\n-            \"Spider %r not opened when crawling: %s\" % (spider.name, request)\n+        if spider not in self.open_spiders:\n+            raise RuntimeError(\"Spider %r not opened when crawling: %s\" % (spider.name, request))\n         self.schedule(request, spider)\n         self.slot.nextcall.schedule()\n \n@@ -232,7 +238,11 @@ class ExecutionEngine:\n         slot.add_request(request)\n \n         def _on_success(response):\n-            assert isinstance(response, (Response, Request))\n+            if not isinstance(response, (Response, Request)):\n+                raise TypeError(\n+                    \"Incorrect type: expected Response or Request, got %s: %r\"\n+                    % (type(response), response)\n+                )\n             if isinstance(response, Response):\n                 response.request = request  # tie request to response received\n                 logkws = self.logformatter.crawled(request, response, spider)\n@@ -253,8 +263,8 @@ class ExecutionEngine:\n \n     @defer.inlineCallbacks\n     def open_spider(self, spider, start_requests=(), close_if_idle=True):\n-        assert self.has_capacity(), \"No free spider slot when opening %r\" % \\\n-            spider.name\n+        if not self.has_capacity():\n+            raise RuntimeError(\"No free spider slot when opening %r\" % spider.name)\n         logger.info(\"Spider opened\", extra={'spider': spider})\n         nextcall = CallLaterOnce(self._next_request, spider)\n         scheduler = self.scheduler_cls.from_crawler(self.crawler)\n\n@@ -123,7 +123,11 @@ class Scraper:\n     def _scrape(self, response, request, spider):\n         \"\"\"Handle the downloaded response or failure through the spider\n         callback/errback\"\"\"\n-        assert isinstance(response, (Response, Failure))\n+        if not isinstance(response, (Response, Failure)):\n+            raise TypeError(\n+                \"Incorrect type: expected Response or Failure, got %s: %r\"\n+                % (type(response), response)\n+            )\n \n         dfd = self._scrape2(response, request, spider)  # returns spider's processed output\n         dfd.addErrback(self.handle_spider_error, request, response, spider)\n\n@@ -78,7 +78,8 @@ class Crawler:\n \n     @defer.inlineCallbacks\n     def crawl(self, *args, **kwargs):\n-        assert not self.crawling, \"Crawling already taking place\"\n+        if self.crawling:\n+            raise RuntimeError(\"Crawling already taking place\")\n         self.crawling = True\n \n         try:\n\n@@ -24,7 +24,8 @@ class Request(object_ref):\n         self.method = str(method).upper()\n         self._set_url(url)\n         self._set_body(body)\n-        assert isinstance(priority, int), \"Request priority not an integer: %r\" % priority\n+        if not isinstance(priority, int):\n+            raise TypeError(\"Request priority not an integer: %r\" % priority)\n         self.priority = priority\n \n         if callback is not None and not callable(callback):\n\n@@ -106,7 +106,8 @@ class S3FilesStore:\n         else:\n             from boto.s3.connection import S3Connection\n             self.S3Connection = S3Connection\n-        assert uri.startswith('s3://')\n+        if not uri.startswith(\"s3://\"):\n+            raise ValueError(\"Incorrect URI scheme in %s, expected 's3'\" % uri)\n         self.bucket, self.prefix = uri[5:].split('/', 1)\n \n     def stat_file(self, path, info):\n@@ -266,7 +267,8 @@ class FTPFilesStore:\n     USE_ACTIVE_MODE = None\n \n     def __init__(self, uri):\n-        assert uri.startswith('ftp://')\n+        if not uri.startswith(\"ftp://\"):\n+            raise ValueError(\"Incorrect URI scheme in %s, expected 'ftp'\" % uri)\n         u = urlparse(uri)\n         self.port = u.port\n         self.host = u.hostname\n\n@@ -128,10 +128,12 @@ def csviter(obj, delimiter=None, headers=None, encoding=None, quotechar=None):\n \n def _body_or_str(obj, unicode=True):\n     expected_types = (Response, str, bytes)\n-    assert isinstance(obj, expected_types), \\\n-        \"obj must be %s, not %s\" % (\n-            \" or \".join(t.__name__ for t in expected_types),\n-            type(obj).__name__)\n+    if not isinstance(obj, expected_types):\n+        expected_types_str = \" or \".join(t.__name__ for t in expected_types)\n+        raise TypeError(\n+            \"Object %r must be %s, not %s\"\n+            % (obj, expected_types_str, type(obj).__name__)\n+        )\n     if isinstance(obj, Response):\n         if not unicode:\n             return obj.body\n\n@@ -9,7 +9,8 @@ from scrapy.utils.misc import load_object\n def listen_tcp(portrange, host, factory):\n     \"\"\"Like reactor.listenTCP but tries different ports in a range.\"\"\"\n     from twisted.internet import reactor\n-    assert len(portrange) <= 2, \"invalid portrange: %s\" % portrange\n+    if len(portrange) > 2:\n+        raise ValueError(\"invalid portrange: %s\" % portrange)\n     if not portrange:\n         return reactor.listenTCP(0, factory, interface=host)\n     if not hasattr(portrange, '__iter__'):\n\n@@ -157,7 +157,7 @@ class XmliterTestCase(unittest.TestCase):\n \n     def test_xmliter_objtype_exception(self):\n         i = self.xmliter(42, 'product')\n-        self.assertRaises(AssertionError, next, i)\n+        self.assertRaises(TypeError, next, i)\n \n     def test_xmliter_encoding(self):\n         body = b'<?xml version=\"1.0\" encoding=\"ISO-8859-9\"?>\\n<xml>\\n    <item>Some Turkish Characters \\xd6\\xc7\\xde\\xdd\\xd0\\xdc \\xfc\\xf0\\xfd\\xfe\\xe7\\xf6</item>\\n</xml>\\n\\n'\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f0f1be76d1e6cef65ac9a01d13c5d5060a03f648", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 9 | Files Changed: 2 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 2828 | Contributors (this commit): 26 | Commits (past 90d): 17 | Contributors (cumulative): 43 | DMM Complexity: 1.0\n\nDIFF:\n@@ -292,7 +292,7 @@ class FeedExporter:\n         :param uri: uri of the new batch to start\n         :param feed: dict with parameters of feed\n         :param spider: user spider\n-        :param template_uri: template uri which contains %(time)s or %(batch_id)s to create new uri\n+        :param template_uri: template uri which contains %(time_id)s or %(batch_id)s to create new uri\n         \"\"\"\n         if previous_batch_slot is not None:\n             previous_batch_id = previous_batch_slot.batch_id\n@@ -360,12 +360,12 @@ class FeedExporter:\n \n     def _batch_deliveries_supported(self, uri):\n         \"\"\"\n-        If FEED_STORAGE_BATCH_SIZE setting is specified uri has to contain %(time)s or %(batch_id)s\n+        If FEED_STORAGE_BATCH_SIZE setting is specified uri has to contain %(time_id)s or %(batch_id)s\n         to distinguish different files of partial output\n         \"\"\"\n-        if self.storage_batch_size is None or '%(time)s' in uri or '%(batch_id)s' in uri:\n+        if self.storage_batch_size is None or '%(time_id)s' in uri or '%(batch_id)s' in uri:\n             return True\n-        logger.warning('%(time)s or %(batch_id)s must be in uri if FEED_STORAGE_BATCH_SIZE setting is specified')\n+        logger.warning('%(time_id)s or %(batch_id)s must be in uri if FEED_STORAGE_BATCH_SIZE setting is specified')\n         return False\n \n     def _storage_supported(self, uri):\n@@ -397,8 +397,9 @@ class FeedExporter:\n         params = {}\n         for k in dir(spider):\n             params[k] = getattr(spider, k)\n+        params['time'] = datetime.utcnow().replace(microsecond=0).isoformat().replace(':', '-')\n+        params['time_id'] = datetime.utcnow().isoformat().replace(':', '-')\n         params['batch_id'] = slot.batch_id + 1 if slot is not None else 1\n-        params['time'] = datetime.utcnow().isoformat().replace(':', '-')\n         uripar_function = load_object(uri_params) if uri_params else lambda x, y: None\n         uripar_function(params, spider)\n         return params\n\n@@ -989,7 +989,7 @@ class FeedExportTest(FeedExportTestBase):\n \n class PartialDeliveriesTest(FeedExportTestBase):\n     __test__ = True\n-    _file_mark = '_%(time)s_#%(batch_id)s_'\n+    _file_mark = '_%(time_id)s_#%(batch_id)s_'\n \n     @defer.inlineCallbacks\n     def run_and_export(self, spider_cls, settings):\n@@ -1146,7 +1146,7 @@ class PartialDeliveriesTest(FeedExportTestBase):\n         yield self.assertExported(items, header, rows, settings=settings)\n \n     def test_wrong_path(self):\n-        \"\"\" If path is without %(time)s or %(batch_id)s an exception must be raised \"\"\"\n+        \"\"\" If path is without %(time_id)s or %(batch_id)s an exception must be raised \"\"\"\n         settings = {\n             'FEEDS': {\n                 self._random_temp_filename(): {'format': 'xml'},\n@@ -1236,7 +1236,7 @@ class PartialDeliveriesTest(FeedExportTestBase):\n     def test_batch_path_differ(self):\n         \"\"\"\n         Test that the name of all batch files differ from each other.\n-        So %(time)s replaced with the current date.\n+        So %(time_id)s replaced with the current date.\n         \"\"\"\n         items = [\n             self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n@@ -1245,7 +1245,7 @@ class PartialDeliveriesTest(FeedExportTestBase):\n         ]\n         settings = {\n             'FEEDS': {\n-                os.path.join(self._random_temp_filename(), '%(time)s'): {\n+                os.path.join(self._random_temp_filename(), '%(time_id)s'): {\n                     'format': 'json',\n                 },\n             },\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#204737042ac6672eee73c975d0bd6735893d684c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 30 | Churn Cumulative: 715 | Contributors (this commit): 22 | Commits (past 90d): 8 | Contributors (cumulative): 22 | DMM Complexity: 0.0\n\nDIFF:\n@@ -255,7 +255,7 @@ class FeedExporter:\n         for uri, feed in self.feeds.items():\n             uri_params = self._get_uri_params(spider, feed['uri_params'], None)\n             self.slots.append(self._start_new_batch(\n-                previous_batch_slot=None,\n+                batch_id=1,\n                 uri=uri % uri_params,\n                 feed=feed,\n                 spider=spider,\n@@ -265,6 +265,11 @@ class FeedExporter:\n     def close_spider(self, spider):\n         deferred_list = []\n         for slot in self.slots:\n+            d = self._close_slot(slot, spider)\n+            deferred_list.append(d)\n+        return defer.DeferredList(deferred_list) if deferred_list else None\n+\n+    def _close_slot(self, slot, spider):\n         if not slot.itemcount and not slot.store_empty:\n             # We need to call slot.storage.store nonetheless to get the file\n             # properly closed.\n@@ -280,27 +285,18 @@ class FeedExporter:\n         d.addErrback(lambda f: logger.error(logfmt % \"Error storing\", log_args,\n                                             exc_info=failure_to_exc_info(f),\n                                             extra={'spider': spider}))\n-            deferred_list.append(d)\n-        return defer.DeferredList(deferred_list) if deferred_list else None\n+        return d\n \n-    def _start_new_batch(self, previous_batch_slot, uri, feed, spider, template_uri):\n+    def _start_new_batch(self, batch_id, uri, feed, spider, template_uri):\n         \"\"\"\n         Redirect the output data stream to a new file.\n         Execute multiple times if 'FEED_STORAGE_BATCH' setting is specified.\n-        :param previous_batch_slot: slot of previous batch. We need to call slot.storage.store\n-        to get the file properly closed.\n+        :param batch_id: sequence number of current batch\n         :param uri: uri of the new batch to start\n         :param feed: dict with parameters of feed\n         :param spider: user spider\n         :param template_uri: template uri which contains %(time_id)s or %(batch_id)s to create new uri\n         \"\"\"\n-        if previous_batch_slot is not None:\n-            previous_batch_id = previous_batch_slot.batch_id\n-            previous_batch_slot.exporter.finish_exporting()\n-            previous_batch_slot.storage.store(previous_batch_slot.file)\n-        else:\n-            previous_batch_id = 0\n-\n         storage = self._get_storage(uri)\n         file = storage.open(spider)\n         exporter = self._get_exporter(\n@@ -317,7 +313,7 @@ class FeedExporter:\n             uri=uri,\n             format=feed['format'],\n             store_empty=feed['store_empty'],\n-            batch_id=previous_batch_id + 1,\n+            batch_id=batch_id,\n             template_uri=template_uri,\n         )\n         if slot.store_empty:\n@@ -330,10 +326,12 @@ class FeedExporter:\n             slot.start_exporting()\n             slot.exporter.export_item(item)\n             slot.itemcount += 1\n-            if self.storage_batch_size and slot.itemcount % self.storage_batch_size == 0:\n+            # create new slot for each slot with itemcount == FEED_STORAGE_BATCH_SIZE and close the old one\n+            if self.storage_batch_size and slot.itemcount == self.storage_batch_size:\n                 uri_params = self._get_uri_params(spider, self.feeds[slot.template_uri]['uri_params'], slot)\n+                self._close_slot(slot, spider)\n                 slots.append(self._start_new_batch(\n-                    previous_batch_slot=slot,\n+                    batch_id=slot.batch_id + 1,\n                     uri=slot.template_uri % uri_params,\n                     feed=self.feeds[slot.template_uri],\n                     spider=spider,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f787b8483ceb37ad8c9764d5a28be07028d85f70", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 9 | Churn Cumulative: 758 | Contributors (this commit): 12 | Commits (past 90d): 11 | Contributors (cumulative): 12 | DMM Complexity: 0.0\n\nDIFF:\n@@ -311,14 +311,7 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n     def test_ipv6_alternative_name_resolver(self):\n         log = self.run_script('alternative_name_resolver.py')\n         self.assertIn('Spider closed (finished)', log)\n-        self.assertTrue(any([\n-            \"twisted.internet.error.ConnectionRefusedError\" in log,\n-            \"twisted.internet.error.ConnectError\" in log,\n-        ]))\n-        self.assertTrue(any([\n-            \"'downloader/exception_type_count/twisted.internet.error.ConnectionRefusedError': 1,\" in log,\n-            \"'downloader/exception_type_count/twisted.internet.error.ConnectError': 1,\" in log,\n-        ]))\n+        self.assertNotIn(\"twisted.internet.error.DNSLookupError\", log)\n \n     def test_reactor_select(self):\n         log = self.run_script(\"twisted_reactor_select.py\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3f9874fac9f93c0956afa5975d7b2bbb21816894", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 70 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 70 | Churn Cumulative: 2213 | Contributors (this commit): 21 | Commits (past 90d): 11 | Contributors (cumulative): 21 | DMM Complexity: 0.057692307692307696\n\nDIFF:\n@@ -1253,3 +1253,73 @@ class PartialDeliveriesTest(FeedExportTestBase):\n         }\n         data = yield self.exported_data(items, settings)\n         self.assertEqual(len(items) + 1, len(data['json']))\n+\n+    @defer.inlineCallbacks\n+    def test_s3_export(self):\n+        \"\"\"\n+        Test export of items into s3 bucket.\n+        S3_TEST_BUCKET_NAME, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY must be specified in tox.ini\n+        to perform this test:\n+        [testenv]\n+        setenv =\n+            AWS_SECRET_ACCESS_KEY = ABCD\n+            AWS_ACCESS_KEY_ID = ABCD\n+            S3_TEST_BUCKET_NAME = ABCD\n+        \"\"\"\n+        try:\n+            import boto3\n+        except ImportError:\n+            raise unittest.SkipTest(\"S3FeedStorage requires boto3\")\n+\n+        assert_aws_environ()\n+        s3_test_bucket_name = os.environ.get('S3_TEST_BUCKET_NAME')\n+        access_key = os.environ.get('AWS_ACCESS_KEY_ID')\n+        secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY')\n+        if not s3_test_bucket_name:\n+            raise unittest.SkipTest(\"No S3 BUCKET available for testing\")\n+\n+        chars = [random.choice(ascii_letters + digits) for _ in range(15)]\n+        filename = ''.join(chars)\n+        prefix = 'tmp/{filename}'.format(filename=filename)\n+        s3_test_file_uri = 's3://{bucket_name}/{prefix}/%(time_id)s.json'.format(\n+            bucket_name=s3_test_bucket_name, prefix=prefix\n+        )\n+        storage = S3FeedStorage(s3_test_bucket_name, access_key, secret_key)\n+        settings = {\n+            'FEEDS': {\n+                s3_test_file_uri: {\n+                    'format': 'json',\n+                },\n+            },\n+            'FEED_STORAGE_BATCH_SIZE': 1,\n+        }\n+        items = [\n+            self.MyItem({'foo': 'bar1', 'egg': 'spam1'}),\n+            self.MyItem({'foo': 'bar2', 'egg': 'spam2', 'baz': 'quux2'}),\n+            self.MyItem({'foo': 'bar3', 'baz': 'quux3'}),\n+        ]\n+        verifyObject(IFeedStorage, storage)\n+\n+        class TestSpider(scrapy.Spider):\n+            name = 'testspider'\n+\n+            def parse(self, response):\n+                for item in items:\n+                    yield item\n+\n+        s3 = boto3.resource('s3')\n+        my_bucket = s3.Bucket(s3_test_bucket_name)\n+        batch_size = settings['FEED_STORAGE_BATCH_SIZE']\n+\n+        with MockServer() as s:\n+            runner = CrawlerRunner(Settings(settings))\n+            TestSpider.start_urls = [s.url('/')]\n+            yield runner.crawl(TestSpider)\n+\n+        for file_uri in my_bucket.objects.filter(Prefix=prefix):\n+            content = get_s3_content_and_delete(s3_test_bucket_name, file_uri.key)\n+            if not content and not items:\n+                break\n+            content = json.loads(content.decode('utf-8'))\n+            expected_batch, items = items[:batch_size], items[batch_size:]\n+            self.assertEqual(expected_batch, content)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
