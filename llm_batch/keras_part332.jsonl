{"custom_id": "keras#bf6859b0dadc34c39ac26e4e25ee434e756e8914", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1424 | Lines Deleted: 1278 | Files Changed: 32 | Hunks: 238 | Methods Changed: 148 | Complexity Δ (Sum/Max): 271/170 | Churn Δ: 2702 | Churn Cumulative: 53492 | Contributors (this commit): 119 | Commits (past 90d): 211 | Contributors (cumulative): 190 | DMM Complexity: 0.9655172413793104\n\nDIFF:\n@@ -30,7 +30,7 @@ from keras import backend\n from keras import combinations\n from keras.engine import input_layer\n from keras.layers import advanced_activations\n-from keras.layers import normalization\n+from keras.layers.normalization import batch_normalization_v1\n from keras.utils import tf_inspect\n \n \n@@ -154,7 +154,7 @@ class BackendUtilsTest(tf.test.TestCase):\n       # Test running with a learning-phase-consuming layer\n       with backend.learning_phase_scope(0):\n         x = input_layer.Input((3,))\n-        y = normalization.BatchNormalization()(x)\n+        y = batch_normalization_v1.BatchNormalization()(x)\n         if not tf.executing_eagerly():\n           self.evaluate(tf.compat.v1.global_variables_initializer())\n           sess.run(y, feed_dict={x: np.random.random((2, 3))})\n\n@@ -85,10 +85,13 @@ class DatasetCreatorModelFitTestBase(tf.test.TestCase, parameterized.TestCase):\n         norm = keras.layers.BatchNormalization(\n             axis=-1, input_shape=(4, 4, 3), momentum=0.8)\n         model.add(norm)\n+      model.add(core_layers.Dense(1, activation=\"sigmoid\"))\n+      self._metric = keras.metrics.Accuracy()\n \n     model.compile(\n         gradient_descent.SGD(),\n-        loss=\"mse\",\n+        loss=\"binary_crossentropy\",\n+        metrics=[self._metric],\n         steps_per_execution=steps_per_execution,\n         run_eagerly=run_eagerly)\n     return model, [ResultAssertingCallback()]\n@@ -119,13 +122,45 @@ class DatasetCreatorModelFitTestBase(tf.test.TestCase, parameterized.TestCase):\n           (x, y)).shuffle(10).repeat().batch(2)\n \n     x = x or dataset_creator.DatasetCreator(dataset_fn)\n+    validation_data = (\n+        validation_data or dataset_creator.DatasetCreator(dataset_fn))\n \n     model.fit(\n         x,\n         epochs=10,\n         steps_per_epoch=steps_per_epoch,\n         callbacks=callbacks,\n-        validation_data=validation_data)\n+        validation_data=validation_data,\n+        validation_steps=steps_per_epoch)\n+    return model\n+\n+  def _model_evaluate(self,\n+                      strategy,\n+                      steps_per_execution=1,\n+                      validation_data=None,\n+                      steps=10,\n+                      run_eagerly=False,\n+                      with_normalization_layer=False,\n+                      callbacks=None):\n+    if callbacks is None:\n+      callbacks = []\n+\n+    model, default_callbacks = self._model_compile(strategy,\n+                                                   steps_per_execution,\n+                                                   run_eagerly,\n+                                                   with_normalization_layer)\n+    callbacks += default_callbacks\n+\n+    def dataset_fn(input_context):\n+      del input_context\n+      x = tf.random.uniform((10, 10))\n+      y = tf.random.uniform((10, 1))\n+      return tf.data.Dataset.from_tensor_slices(\n+          (x, y)).shuffle(10).repeat().batch(8)\n+\n+    validation_data = (\n+        validation_data or dataset_creator.DatasetCreator(dataset_fn))\n+    model.evaluate(x=validation_data, steps=steps, callbacks=callbacks)\n     return model\n \n \n@@ -140,7 +175,6 @@ class DatasetCreatorModelFitTest(DatasetCreatorModelFitTestBase):\n   def testModelFit(self, strategy):\n     model = self._model_fit(strategy)\n     self.assertEqual(model.optimizer.iterations, 100)\n-    return model\n \n   def testModelFitWithNormalizationLayer(self, strategy):\n     model = self._model_fit(strategy, with_normalization_layer=True)\n@@ -153,12 +187,62 @@ class DatasetCreatorModelFitTest(DatasetCreatorModelFitTestBase):\n   def testModelFitWithNoStepsPerEpoch(self, strategy):\n     with self.assertRaisesRegex(\n         ValueError, \"When using a \"\n-        \"`tf.keras.utils.experimental.DatasetCreator`, \"\n-        \"`steps_per_epoch` argument must be provided in \"\n-        \"`Model.fit`.\"):\n+        \"`tf.keras.utils.experimental.DatasetCreator`, `steps_per_epoch`, \"\n+        \"`validation_steps` or `steps` argument must be provided in \"\n+        \"`Model.fit` or `Model.evaluate`.\"):\n       self._model_fit(strategy, steps_per_epoch=None)\n \n \n+@tf.__internal__.distribute.combinations.generate(\n+    tf.__internal__.test.combinations.combine(strategy=[\"ParameterServerStrategy\"], mode=\"eager\"))\n+class DatasetCreatorModelEvaluateParameterServerStrategyOnlyTest(\n+    DatasetCreatorModelFitTestBase):\n+\n+  def testModelEvaluate(self, strategy):\n+    self._model_evaluate(strategy)\n+    self.assertGreaterEqual(self._metric.result(), 0.0)\n+\n+  def testModelEvaluateWithNormalizationLayer(self, strategy):\n+    self._model_evaluate(strategy, with_normalization_layer=True)\n+    self.assertGreaterEqual(self._metric.result(), 0.0)\n+\n+  def testModelEvaluateWithStepsPerExecution(self, strategy):\n+    self._model_evaluate(strategy, steps_per_execution=10)\n+    self.assertGreaterEqual(self._metric.result(), 0.0)\n+\n+  def testModelEvaluateWithNoStepsPerEpoch(self, strategy):\n+    with self.assertRaisesRegex(\n+        ValueError, \"When using a \"\n+        \"`tf.keras.utils.experimental.DatasetCreator`, `steps_per_epoch`, \"\n+        \"`validation_steps` or `steps` argument must be provided in \"\n+        \"`Model.fit` or `Model.evaluate`.\"):\n+      self._model_evaluate(strategy, steps=None)\n+\n+  def testModelEvaluateWithDatasetInstance(self, strategy):\n+    with self.assertRaisesRegex(\n+        NotImplementedError,\n+        \"Only `tf.keras.utils.experimental.DatasetCreator` input is supported \"\n+        \"with `ParameterServerStrategy` at this time. Please see \"\n+        \"`tf.keras.utils.experimental.DatasetCreator` class docstring for more \"\n+        \"information.\"\n+    ):\n+      self._model_evaluate(\n+          strategy,\n+          validation_data=tf.data.Dataset.from_tensor_slices([1, 1]))\n+\n+  def testModelFitErrorOnBatchLevelCallbacks(self, strategy):\n+\n+    class BatchLevelCallback(callbacks_lib.Callback):\n+\n+      def on_train_batch_end(self, batch, logs=None):\n+        pass\n+\n+    with self.assertRaisesRegex(ValueError,\n+                                \"Batch-level `Callback`s are not supported\"):\n+      callbacks = [BatchLevelCallback()]\n+      self._model_evaluate(strategy, callbacks=callbacks)\n+\n+\n @tf.__internal__.distribute.combinations.generate(\n     tf.__internal__.test.combinations.combine(strategy=[\"ParameterServerStrategy\"], mode=\"eager\"))\n class DatasetCreatorModelFitParameterServerStrategyOnlyTest(\n@@ -170,14 +254,6 @@ class DatasetCreatorModelFitParameterServerStrategyOnlyTest(\n         \"`run_eagerly` is not supported.\"):\n       self._model_fit(strategy, run_eagerly=True)\n \n-  def testModelFitWithValidationData(self, strategy):\n-    with self.assertRaisesRegex(\n-        NotImplementedError, \"Evaluation in `model.fit` with \"\n-        \"`ParameterServerStrategy` is not yet supported.\"):\n-      self._model_fit(\n-          strategy,\n-          validation_data=tf.data.Dataset.from_tensor_slices([1, 1]))\n-\n   def testModelFitWithDatasetInstance(self, strategy):\n     with self.assertRaisesRegex(\n         NotImplementedError,\n@@ -188,13 +264,6 @@ class DatasetCreatorModelFitParameterServerStrategyOnlyTest(\n       self._model_fit(\n           strategy, x=tf.data.Dataset.from_tensor_slices([1, 1]))\n \n-  def testModelEvaluate(self, strategy):\n-    model, _ = self._model_compile(strategy)\n-    with self.assertRaisesRegex(\n-        NotImplementedError, \"`model.evaluate` is not yet supported with \"\n-        \"`ParameterServerStrategy`.\"):\n-      model.evaluate(x=tf.data.Dataset.from_tensor_slices([1, 1]))\n-\n   def testModelPredict(self, strategy):\n     model, _ = self._model_compile(strategy)\n     with self.assertRaisesRegex(\n\n@@ -494,8 +494,9 @@ class DatasetCreatorAdapter(DataAdapter):\n     if steps is None:\n       raise ValueError(\"When using a \"\n                        \"`tf.keras.utils.experimental.DatasetCreator`, \"\n-                       \"`steps_per_epoch` argument must be provided in \"\n-                       \"`Model.fit`.\")\n+                       \"`steps_per_epoch`, `validation_steps` or `steps` \"\n+                       \"argument must be provided in `Model.fit` or \"\n+                       \"`Model.evaluate`.\")\n     self.dataset_creator = x\n     self.steps = steps\n     self.strategy = distribution_strategy\n\n@@ -1309,6 +1309,11 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n           test_function, experimental_relax_shapes=True)\n \n     self.test_function = test_function\n+\n+    if self._cluster_coordinator:\n+      self.test_function = lambda iterator: self._cluster_coordinator.schedule(  # pylint: disable=g-long-lambda\n+          test_function, args=(iterator,))\n+\n     return self.test_function\n \n   def evaluate(self,\n@@ -1415,8 +1420,8 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n       raise TypeError('Invalid keyword arguments: %s' % (kwargs,))\n \n     if self.distribute_strategy._should_use_with_coordinator:  # pylint: disable=protected-access\n-      raise NotImplementedError('`model.evaluate` is not yet supported with '\n-                                '`ParameterServerStrategy`.')\n+      self._cluster_coordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(\n+          self.distribute_strategy)\n \n     with self.distribute_strategy.scope():\n       # Use cached evaluation data only when it's called in `Model.fit`\n@@ -2692,10 +2697,6 @@ class Model(base_layer.Layer, version_utils.ModelVersionSelector):\n     return functions\n \n   def _should_eval(self, epoch, validation_freq):\n-    if self._cluster_coordinator:\n-      raise NotImplementedError(\n-          'Evaluation in `model.fit` with '\n-          '`ParameterServerStrategy` is not yet supported.')\n     epoch = epoch + 1  # one-index the user-facing epoch.\n     if isinstance(validation_freq, int):\n       return epoch % validation_freq == 0\n\n@@ -145,16 +145,16 @@ from keras.layers.noise import GaussianNoise\n from keras.layers.noise import GaussianDropout\n \n # Normalization layers.\n-from keras.layers.normalization import LayerNormalization\n-from keras.layers.normalization_v2 import SyncBatchNormalization\n+from keras.layers.normalization.layer_normalization import LayerNormalization\n+from keras.layers.normalization.batch_normalization import SyncBatchNormalization\n \n if tf.__internal__.tf2.enabled():\n-  from keras.layers.normalization_v2 import BatchNormalization\n-  from keras.layers.normalization import BatchNormalization as BatchNormalizationV1\n+  from keras.layers.normalization.batch_normalization import BatchNormalization\n+  from keras.layers.normalization.batch_normalization_v1 import BatchNormalization as BatchNormalizationV1\n   BatchNormalizationV2 = BatchNormalization\n else:\n-  from keras.layers.normalization import BatchNormalization\n-  from keras.layers.normalization_v2 import BatchNormalization as BatchNormalizationV2\n+  from keras.layers.normalization.batch_normalization_v1 import BatchNormalization\n+  from keras.layers.normalization.batch_normalization import BatchNormalization as BatchNormalizationV2\n   BatchNormalizationV1 = BatchNormalization\n \n # Kernelized layers.\n\n@@ -0,0 +1 @@\n+\n\n@@ -1,4 +1,4 @@\n-# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -12,10 +12,9 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n # ==============================================================================\n-\"\"\"Normalization layers.\"\"\"\n+\"\"\"The V2 implementation of Normalization layers.\"\"\"\n \n import tensorflow.compat.v2 as tf\n-from tensorflow.python.framework import tensor_shape\n from keras import backend\n from keras import constraints\n from keras import initializers\n@@ -103,8 +102,8 @@ class BatchNormalizationBase(Layer):\n       if the fused implementation cannot be used. If `None`, use the faster\n       implementation if possible. If False, do not used the fused\n       implementation.\n-      Note that in TensorFlow 1.x, the meaning of `fused=True` is different:\n-      if `False`, the layer uses the system-recommended implementation.\n+      Note that in TensorFlow 1.x, the meaning of `fused=True` is different: if\n+        `False`, the layer uses the system-recommended implementation.\n     trainable: Boolean, if `True` the variables will be marked as trainable.\n     virtual_batch_size: An `int`. By default, `virtual_batch_size` is `None`,\n       which means batch normalization is performed across the whole batch. When\n@@ -134,13 +133,11 @@ class BatchNormalizationBase(Layer):\n       - `training=False`: The layer will normalize its inputs using the mean and\n         variance of its moving statistics, learned during training.\n \n-  Input shape:\n-    Arbitrary. Use the keyword argument `input_shape` (tuple of\n+  Input shape: Arbitrary. Use the keyword argument `input_shape` (tuple of\n     integers, does not include the samples axis) when using this layer as the\n     first layer in a model.\n \n-  Output shape:\n-    Same shape as input.\n+  Output shape: Same shape as input.\n \n   Reference:\n     - [Ioffe and Szegedy, 2015](https://arxiv.org/abs/1502.03167).\n@@ -551,8 +548,8 @@ class BatchNormalizationBase(Layer):\n     # take exponential_avg_factor as a tensor input.\n     use_fused_avg_updates = (\n         tf.compat.v1.executing_eagerly_outside_functions() and\n-        isinstance(self.momentum, (float, int)) and\n-        get_enclosing_xla_context() is None)\n+        isinstance(self.momentum,\n+                   (float, int)) and get_enclosing_xla_context() is None)\n     if use_fused_avg_updates:\n       exponential_avg_factor = 1.0 - self.momentum\n     else:\n@@ -717,8 +714,8 @@ class BatchNormalizationBase(Layer):\n     # code as well.\n     if self._support_zero_size_input():\n       input_batch_size = tf.compat.v1.shape(inputs)[0]\n-      mean = tf.compat.v1.where(\n-          input_batch_size > 0, mean, backend.zeros_like(mean))\n+      mean = tf.compat.v1.where(input_batch_size > 0, mean,\n+                             backend.zeros_like(mean))\n       variance = tf.compat.v1.where(input_batch_size > 0, variance,\n                                  backend.zeros_like(variance))\n     return mean, variance\n@@ -964,122 +961,236 @@ class BatchNormalizationBase(Layer):\n     return dict(list(base_config.items()) + list(config.items()))\n \n \n-# pylint: disable=missing-docstring\n-@keras_export(v1=['keras.layers.BatchNormalization'])\n-class BatchNormalization(BatchNormalizationBase):\n-  _USE_V2_BEHAVIOR = False\n+# pylint: disable=g-classes-have-attributes\n+@keras_export('keras.layers.experimental.SyncBatchNormalization', v1=[])\n+class SyncBatchNormalization(BatchNormalizationBase):\n+  r\"\"\"Normalize and scale inputs or activations synchronously across replicas.\n \n+  Applies batch normalization to activations of the previous layer at each batch\n+  by synchronizing the global batch statistics across all devices that are\n+  training the model. For specific details about batch normalization please\n+  refer to the `tf.keras.layers.BatchNormalization` layer docs.\n \n-@keras_export('keras.layers.LayerNormalization')\n-class LayerNormalization(Layer):\n-  \"\"\"Layer normalization layer (Ba et al., 2016).\n+  If this layer is used when using tf.distribute strategy to train models\n+  across devices/workers, there will be an allreduce call to aggregate batch\n+  statistics across all replicas at every training step. Without tf.distribute\n+  strategy, this layer behaves as a regular `tf.keras.layers.BatchNormalization`\n+  layer.\n \n-  Normalize the activations of the previous layer for each given example in a\n-  batch independently, rather than across a batch like Batch Normalization.\n-  i.e. applies a transformation that maintains the mean activation within each\n-  example close to 0 and the activation standard deviation close to 1.\n-\n-  Given a tensor `inputs`, moments are calculated and normalization\n-  is performed across the axes specified in `axis`.\n-\n-  Example:\n-\n-  >>> data = tf.constant(np.arange(10).reshape(5, 2) * 10, dtype=tf.float32)\n-  >>> print(data)\n-  tf.Tensor(\n-  [[ 0. 10.]\n-   [20. 30.]\n-   [40. 50.]\n-   [60. 70.]\n-   [80. 90.]], shape=(5, 2), dtype=float32)\n-\n-  >>> layer = tf.keras.layers.LayerNormalization(axis=1)\n-  >>> output = layer(data)\n-  >>> print(output)\n-  tf.Tensor(\n-  [[-1. 1.]\n-   [-1. 1.]\n-   [-1. 1.]\n-   [-1. 1.]\n-   [-1. 1.]], shape=(5, 2), dtype=float32)\n-\n-  Notice that with Layer Normalization the normalization happens across the\n-  axes *within* each example, rather than across different examples in the\n-  batch.\n-\n-  If `scale` or `center` are enabled, the layer will scale the normalized\n-  outputs by broadcasting them with a trainable variable `gamma`, and center\n-  the outputs by broadcasting with a trainable variable `beta`. `gamma` will\n-  default to a ones tensor and `beta` will default to a zeros tensor, so that\n-  centering and scaling are no-ops before training has begun.\n-\n-  So, with scaling and centering enabled the normalization equations\n-  are as follows:\n-\n-  Let the intermediate activations for a mini-batch to be the `inputs`.\n-\n-  For each sample `x_i` in `inputs` with `k` features, we compute the mean and\n-  variance of the sample:\n+  Example usage:\n \n   ```python\n-  mean_i = sum(x_i[j] for j in range(k)) / k\n-  var_i = sum((x_i[j] - mean_i) ** 2 for j in range(k)) / k\n+  strategy = tf.distribute.MirroredStrategy()\n+\n+  with strategy.scope():\n+    model = tf.keras.Sequential()\n+    model.add(tf.keras.layers.Dense(16))\n+    model.add(tf.keras.layers.experimental.SyncBatchNormalization())\n   ```\n \n-  and then compute a normalized `x_i_normalized`, including a small factor\n-  `epsilon` for numerical stability.\n-\n-  ```python\n-  x_i_normalized = (x_i - mean_i) / sqrt(var_i + epsilon)\n-  ```\n-\n-  And finally `x_i_normalized ` is linearly transformed by `gamma` and `beta`,\n-  which are learned parameters:\n-\n-  ```python\n-  output_i = x_i_normalized * gamma + beta\n-  ```\n-\n-  `gamma` and `beta` will span the axes of `inputs` specified in `axis`, and\n-  this part of the inputs' shape must be fully defined.\n-\n-  For example:\n-\n-  >>> layer = tf.keras.layers.LayerNormalization(axis=[1, 2, 3])\n-  >>> layer.build([5, 20, 30, 40])\n-  >>> print(layer.beta.shape)\n-  (20, 30, 40)\n-  >>> print(layer.gamma.shape)\n-  (20, 30, 40)\n-\n-  Note that other implementations of layer normalization may choose to define\n-  `gamma` and `beta` over a separate set of axes from the axes being\n-  normalized across. For example, Group Normalization\n-  ([Wu et al. 2018](https://arxiv.org/abs/1803.08494)) with group size of 1\n-  corresponds to a Layer Normalization that normalizes across height, width,\n-  and channel and has `gamma` and `beta` span only the channel dimension.\n-  So, this Layer Normalization implementation will not match a Group\n-  Normalization layer with group size set to 1.\n-\n   Args:\n-    axis: Integer or List/Tuple. The axis or axes to normalize across. Typically\n-      this is the features axis/axes. The left-out axes are typically the batch\n-      axis/axes. This argument defaults to `-1`, the last dimension in the\n-      input.\n-    epsilon: Small float added to variance to avoid dividing by zero. Defaults\n-      to 1e-3\n+    axis: Integer, the axis that should be normalized\n+      (typically the features axis).\n+      For instance, after a `Conv2D` layer with\n+      `data_format=\"channels_first\"`,\n+      set `axis=1` in `BatchNormalization`.\n+    momentum: Momentum for the moving average.\n+    epsilon: Small float added to variance to avoid dividing by zero.\n+    center: If True, add offset of `beta` to normalized tensor.\n+      If False, `beta` is ignored.\n+    scale: If True, multiply by `gamma`.\n+      If False, `gamma` is not used.\n+      When the next layer is linear (also e.g. `nn.relu`),\n+      this can be disabled since the scaling\n+      will be done by the next layer.\n+    beta_initializer: Initializer for the beta weight.\n+    gamma_initializer: Initializer for the gamma weight.\n+    moving_mean_initializer: Initializer for the moving mean.\n+    moving_variance_initializer: Initializer for the moving variance.\n+    beta_regularizer: Optional regularizer for the beta weight.\n+    gamma_regularizer: Optional regularizer for the gamma weight.\n+    beta_constraint: Optional constraint for the beta weight.\n+    gamma_constraint: Optional constraint for the gamma weight.\n+\n+  Call arguments:\n+    inputs: Input tensor (of any rank).\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode or in inference mode.\n+      - `training=True`: The layer will normalize its inputs using the\n+        mean and variance of the current batch of inputs.\n+      - `training=False`: The layer will normalize its inputs using the\n+        mean and variance of its moving statistics, learned during training.\n+\n+  Input shape:\n+    Arbitrary. Use the keyword argument `input_shape`\n+    (tuple of integers, does not include the samples axis)\n+    when using this layer as the first layer in a model.\n+\n+  Output shape:\n+    Same shape as input.\n+\n+  \"\"\"\n+\n+  def __init__(self,\n+               axis=-1,\n+               momentum=0.99,\n+               epsilon=1e-3,\n+               center=True,\n+               scale=True,\n+               beta_initializer='zeros',\n+               gamma_initializer='ones',\n+               moving_mean_initializer='zeros',\n+               moving_variance_initializer='ones',\n+               beta_regularizer=None,\n+               gamma_regularizer=None,\n+               beta_constraint=None,\n+               gamma_constraint=None,\n+               **kwargs):\n+    if kwargs.pop('fused', None):\n+      raise ValueError(\n+          '`fused` argument cannot be True for SyncBatchNormalization.')\n+\n+    # Currently we only support aggregating over the global batch size.\n+    super(SyncBatchNormalization, self).__init__(\n+        axis=axis,\n+        momentum=momentum,\n+        epsilon=epsilon,\n+        center=center,\n+        scale=scale,\n+        beta_initializer=beta_initializer,\n+        gamma_initializer=gamma_initializer,\n+        moving_mean_initializer=moving_mean_initializer,\n+        moving_variance_initializer=moving_variance_initializer,\n+        beta_regularizer=beta_regularizer,\n+        gamma_regularizer=gamma_regularizer,\n+        beta_constraint=beta_constraint,\n+        gamma_constraint=gamma_constraint,\n+        fused=False,\n+        **kwargs)\n+\n+  def _calculate_mean_and_var(self, x, axes, keep_dims):\n+\n+    with backend.name_scope('moments'):\n+      # The dynamic range of fp16 is too limited to support the collection of\n+      # sufficient statistics. As a workaround we simply perform the operations\n+      # on 32-bit floats before converting the mean and variance back to fp16\n+      y = tf.cast(x, tf.float32) if x.dtype == tf.float16 else x\n+      replica_ctx = tf.distribute.get_replica_context()\n+      if replica_ctx:\n+        local_sum = tf.reduce_sum(y, axis=axes, keepdims=True)\n+        local_squared_sum = tf.reduce_sum(tf.square(y), axis=axes,\n+                                                keepdims=True)\n+        batch_size = tf.cast(tf.shape(y)[axes[0]],\n+                                   tf.float32)\n+        # TODO(b/163099951): batch the all-reduces once we sort out the ordering\n+        # issue for NCCL. We don't have a mechanism to launch NCCL in the same\n+        # order in each replica nowadays, so we limit NCCL to batch all-reduces.\n+        y_sum = replica_ctx.all_reduce(tf.distribute.ReduceOp.SUM, local_sum)\n+        y_squared_sum = replica_ctx.all_reduce(tf.distribute.ReduceOp.SUM,\n+                                               local_squared_sum)\n+        global_batch_size = replica_ctx.all_reduce(tf.distribute.ReduceOp.SUM,\n+                                                   batch_size)\n+\n+        axes_vals = [(tf.shape(y))[axes[i]]\n+                     for i in range(1, len(axes))]\n+        multiplier = tf.cast(tf.reduce_prod(axes_vals),\n+                                   tf.float32)\n+        multiplier = multiplier * global_batch_size\n+\n+        mean = y_sum / multiplier\n+        y_squared_mean = y_squared_sum / multiplier\n+        # var = E(x^2) - E(x)^2\n+        variance = y_squared_mean - tf.square(mean)\n+      else:\n+        # Compute true mean while keeping the dims for proper broadcasting.\n+        mean = tf.reduce_mean(y, axes, keepdims=True, name='mean')\n+        # sample variance, not unbiased variance\n+        # Note: stop_gradient does not change the gradient that gets\n+        #       backpropagated to the mean from the variance calculation,\n+        #       because that gradient is zero\n+        variance = tf.reduce_mean(\n+            tf.math.squared_difference(y, tf.stop_gradient(mean)),\n+            axes,\n+            keepdims=True,\n+            name='variance')\n+      if not keep_dims:\n+        mean = tf.compat.v1.squeeze(mean, axes)\n+        variance = tf.compat.v1.squeeze(variance, axes)\n+      if x.dtype == tf.float16:\n+        return (tf.cast(mean, tf.float16),\n+                tf.cast(variance, tf.float16))\n+      else:\n+        return (mean, variance)\n+\n+\n+@keras_export('keras.layers.BatchNormalization', v1=[])\n+class BatchNormalization(BatchNormalizationBase):\n+  \"\"\"Layer that normalizes its inputs.\n+\n+  Batch normalization applies a transformation that maintains the mean output\n+  close to 0 and the output standard deviation close to 1.\n+\n+  Importantly, batch normalization works differently during training and\n+  during inference.\n+\n+  **During training** (i.e. when using `fit()` or when calling the layer/model\n+  with the argument `training=True`), the layer normalizes its output using\n+  the mean and standard deviation of the current batch of inputs. That is to\n+  say, for each channel being normalized, the layer returns\n+  `gamma * (batch - mean(batch)) / sqrt(var(batch) + epsilon) + beta`, where:\n+\n+  - `epsilon` is small constant (configurable as part of the constructor\n+  arguments)\n+  - `gamma` is a learned scaling factor (initialized as 1), which\n+  can be disabled by passing `scale=False` to the constructor.\n+  - `beta` is a learned offset factor (initialized as 0), which\n+  can be disabled by passing `center=False` to the constructor.\n+\n+  **During inference** (i.e. when using `evaluate()` or `predict()` or when\n+  calling the layer/model with the argument `training=False` (which is the\n+  default), the layer normalizes its output using a moving average of the\n+  mean and standard deviation of the batches it has seen during training. That\n+  is to say, it returns\n+  `gamma * (batch - self.moving_mean) / sqrt(self.moving_var + epsilon) + beta`.\n+\n+  `self.moving_mean` and `self.moving_var` are non-trainable variables that\n+  are updated each time the layer in called in training mode, as such:\n+\n+  - `moving_mean = moving_mean * momentum + mean(batch) * (1 - momentum)`\n+  - `moving_var = moving_var * momentum + var(batch) * (1 - momentum)`\n+\n+  As such, the layer will only normalize its inputs during inference\n+  *after having been trained on data that has similar statistics as the\n+  inference data*.\n+\n+  Args:\n+    axis: Integer, the axis that should be normalized (typically the features\n+      axis). For instance, after a `Conv2D` layer with\n+      `data_format=\"channels_first\"`, set `axis=1` in `BatchNormalization`.\n+    momentum: Momentum for the moving average.\n+    epsilon: Small float added to variance to avoid dividing by zero.\n     center: If True, add offset of `beta` to normalized tensor. If False, `beta`\n-      is ignored. Defaults to True.\n-    scale: If True, multiply by `gamma`. If False, `gamma` is not used. Defaults\n-      to True. When the next layer is linear (also e.g. `nn.relu`), this can be\n-      disabled since the scaling will be done by the next layer.\n-    beta_initializer: Initializer for the beta weight. Defaults to zeros.\n-    gamma_initializer: Initializer for the gamma weight. Defaults to ones.\n-    beta_regularizer: Optional regularizer for the beta weight. None by default.\n-    gamma_regularizer: Optional regularizer for the gamma weight. None by\n-      default.\n-    beta_constraint: Optional constraint for the beta weight. None by default.\n-    gamma_constraint: Optional constraint for the gamma weight. None by default.\n+      is ignored.\n+    scale: If True, multiply by `gamma`. If False, `gamma` is not used. When the\n+      next layer is linear (also e.g. `nn.relu`), this can be disabled since the\n+      scaling will be done by the next layer.\n+    beta_initializer: Initializer for the beta weight.\n+    gamma_initializer: Initializer for the gamma weight.\n+    moving_mean_initializer: Initializer for the moving mean.\n+    moving_variance_initializer: Initializer for the moving variance.\n+    beta_regularizer: Optional regularizer for the beta weight.\n+    gamma_regularizer: Optional regularizer for the gamma weight.\n+    beta_constraint: Optional constraint for the beta weight.\n+    gamma_constraint: Optional constraint for the gamma weight.\n+\n+  Call arguments:\n+    inputs: Input tensor (of any rank).\n+    training: Python boolean indicating whether the layer should behave in\n+      training mode or in inference mode.\n+      - `training=True`: The layer will normalize its inputs using the mean and\n+        variance of the current batch of inputs.\n+      - `training=False`: The layer will normalize its inputs using the mean and\n+        variance of its moving statistics, learned during training.\n \n   Input shape:\n     Arbitrary. Use the keyword argument `input_shape` (tuple of\n@@ -1090,217 +1201,67 @@ class LayerNormalization(Layer):\n     Same shape as input.\n \n   Reference:\n-    - [Lei Ba et al., 2016](https://arxiv.org/abs/1607.06450).\n+    - [Ioffe and Szegedy, 2015](https://arxiv.org/abs/1502.03167).\n+\n+  **About setting `layer.trainable = False` on a `BatchNormalization` layer:**\n+\n+  The meaning of setting `layer.trainable = False` is to freeze the layer,\n+  i.e. its internal state will not change during training:\n+  its trainable weights will not be updated\n+  during `fit()` or `train_on_batch()`, and its state updates will not be run.\n+\n+  Usually, this does not necessarily mean that the layer is run in inference\n+  mode (which is normally controlled by the `training` argument that can\n+  be passed when calling a layer). \"Frozen state\" and \"inference mode\"\n+  are two separate concepts.\n+\n+  However, in the case of the `BatchNormalization` layer, **setting\n+  `trainable = False` on the layer means that the layer will be\n+  subsequently run in inference mode** (meaning that it will use\n+  the moving mean and the moving variance to normalize the current batch,\n+  rather than using the mean and variance of the current batch).\n+\n+  This behavior has been introduced in TensorFlow 2.0, in order\n+  to enable `layer.trainable = False` to produce the most commonly\n+  expected behavior in the convnet fine-tuning use case.\n+\n+  Note that:\n+    - Setting `trainable` on an model containing other layers will\n+      recursively set the `trainable` value of all inner layers.\n+    - If the value of the `trainable`\n+      attribute is changed after calling `compile()` on a model,\n+      the new value doesn't take effect for this model\n+      until `compile()` is called again.\n   \"\"\"\n+  _USE_V2_BEHAVIOR = True\n \n   def __init__(self,\n                axis=-1,\n+               momentum=0.99,\n                epsilon=1e-3,\n                center=True,\n                scale=True,\n                beta_initializer='zeros',\n                gamma_initializer='ones',\n+               moving_mean_initializer='zeros',\n+               moving_variance_initializer='ones',\n                beta_regularizer=None,\n                gamma_regularizer=None,\n                beta_constraint=None,\n                gamma_constraint=None,\n                **kwargs):\n-    super(LayerNormalization, self).__init__(**kwargs)\n-    if isinstance(axis, (list, tuple)):\n-      self.axis = axis[:]\n-    elif isinstance(axis, int):\n-      self.axis = axis\n-    else:\n-      raise TypeError('Expected an int or a list/tuple of ints for the '\n-                      'argument \\'axis\\', but received: %r' % axis)\n-\n-    self.epsilon = epsilon\n-    self.center = center\n-    self.scale = scale\n-    self.beta_initializer = initializers.get(beta_initializer)\n-    self.gamma_initializer = initializers.get(gamma_initializer)\n-    self.beta_regularizer = regularizers.get(beta_regularizer)\n-    self.gamma_regularizer = regularizers.get(gamma_regularizer)\n-    self.beta_constraint = constraints.get(beta_constraint)\n-    self.gamma_constraint = constraints.get(gamma_constraint)\n-\n-    self.supports_masking = True\n-\n-    # Indicates whether a faster fused implementation can be used. This will be\n-    # set to True or False in build()\"\n-    self._fused = None\n-\n-  def _fused_can_be_used(self, ndims):\n-    \"\"\"Return false if fused implementation cannot be used.\n-\n-    Check if the axis is contiguous and can be collapsed into the last axis.\n-    The self.axis is assumed to have no duplicates.\n-    \"\"\"\n-    axis = sorted(self.axis)\n-    can_use_fused = False\n-\n-    if axis[-1] == ndims - 1 and axis[-1] - axis[0] == len(axis) - 1:\n-      can_use_fused = True\n-\n-    # fused_batch_norm will silently raise epsilon to be at least 1.001e-5, so\n-    # we cannot used the fused version if epsilon is below that value. Also, the\n-    # variable dtype must be float32, as fused_batch_norm only supports float32\n-    # variables.\n-    if self.epsilon < 1.001e-5 or self.dtype != 'float32':\n-      can_use_fused = False\n-\n-    return can_use_fused\n-\n-  def build(self, input_shape):\n-    ndims = len(input_shape)\n-    if ndims is None:\n-      raise ValueError('Input shape %s has undefined rank.' % input_shape)\n-\n-    # Convert axis to list and resolve negatives\n-    if isinstance(self.axis, int):\n-      self.axis = [self.axis]\n-    elif isinstance(self.axis, tuple):\n-      self.axis = list(self.axis)\n-    for idx, x in enumerate(self.axis):\n-      if x < 0:\n-        self.axis[idx] = ndims + x\n-\n-    # Validate axes\n-    for x in self.axis:\n-      if x < 0 or x >= ndims:\n-        raise ValueError('Invalid axis: %d' % x)\n-    if len(self.axis) != len(set(self.axis)):\n-      raise ValueError('Duplicate axis: {}'.format(tuple(self.axis)))\n-\n-    param_shape = [input_shape[dim] for dim in self.axis]\n-    if self.scale:\n-      self.gamma = self.add_weight(\n-          name='gamma',\n-          shape=param_shape,\n-          initializer=self.gamma_initializer,\n-          regularizer=self.gamma_regularizer,\n-          constraint=self.gamma_constraint,\n-          trainable=True,\n-          experimental_autocast=False)\n-    else:\n-      self.gamma = None\n-\n-    if self.center:\n-      self.beta = self.add_weight(\n-          name='beta',\n-          shape=param_shape,\n-          initializer=self.beta_initializer,\n-          regularizer=self.beta_regularizer,\n-          constraint=self.beta_constraint,\n-          trainable=True,\n-          experimental_autocast=False)\n-    else:\n-      self.beta = None\n-\n-    self._fused = self._fused_can_be_used(ndims)\n-\n-    self.built = True\n-\n-  def call(self, inputs):\n-    # Compute the axes along which to reduce the mean / variance\n-    input_shape = inputs.shape\n-    ndims = len(input_shape)\n-\n-    # Broadcasting only necessary for norm when the axis is not just\n-    # the last dimension\n-    broadcast_shape = [1] * ndims\n-    for dim in self.axis:\n-      broadcast_shape[dim] = input_shape.dims[dim].value\n-\n-    def _broadcast(v):\n-      if (v is not None and len(v.shape) != ndims and self.axis != [ndims - 1]):\n-        return tf.reshape(v, broadcast_shape)\n-      return v\n-\n-    if not self._fused:\n-      input_dtype = inputs.dtype\n-      if input_dtype in ('float16', 'bfloat16') and self.dtype == 'float32':\n-        # If mixed precision is used, cast inputs to float32 so that this is at\n-        # least as numerically stable as the fused version.\n-        inputs = tf.cast(inputs, 'float32')\n-\n-      # Calculate the moments on the last axis (layer activations).\n-      mean, variance = tf.compat.v1.nn.moments(inputs, self.axis, keep_dims=True)\n-\n-      scale, offset = _broadcast(self.gamma), _broadcast(self.beta)\n-\n-      # Compute layer normalization using the batch_normalization function.\n-      outputs = tf.nn.batch_normalization(\n-          inputs,\n-          mean,\n-          variance,\n-          offset=offset,\n+    super(BatchNormalization, self).__init__(\n+        axis=axis,\n+        momentum=momentum,\n+        epsilon=epsilon,\n+        center=center,\n         scale=scale,\n-          variance_epsilon=self.epsilon)\n-      outputs = tf.cast(outputs, input_dtype)\n-    else:\n-      # Collapse dims before self.axis, and dims in self.axis\n-      pre_dim, in_dim = (1, 1)\n-      axis = sorted(self.axis)\n-      tensor_shape = tf.compat.v1.shape(inputs)\n-      for dim in range(0, ndims):\n-        dim_tensor = tensor_shape[dim]\n-        if dim < axis[0]:\n-          pre_dim = pre_dim * dim_tensor\n-        else:\n-          assert dim in axis\n-          in_dim = in_dim * dim_tensor\n-\n-      squeezed_shape = [1, pre_dim, in_dim, 1]\n-      # This fused operation requires reshaped inputs to be NCHW.\n-      data_format = 'NCHW'\n-\n-      inputs = tf.reshape(inputs, squeezed_shape)\n-\n-      # self.gamma and self.beta have the wrong shape for fused_batch_norm, so\n-      # we cannot pass them as the scale and offset parameters. Therefore, we\n-      # create two constant tensors in correct shapes for fused_batch_norm and\n-      # later construct a separate calculation on the scale and offset.\n-      scale = tf.ones([pre_dim], dtype=self.dtype)\n-      offset = tf.zeros([pre_dim], dtype=self.dtype)\n-\n-      # Compute layer normalization using the fused_batch_norm function.\n-      outputs, _, _ = tf.compat.v1.nn.fused_batch_norm(\n-          inputs,\n-          scale=scale,\n-          offset=offset,\n-          epsilon=self.epsilon,\n-          data_format=data_format)\n-\n-      outputs = tf.reshape(outputs, tensor_shape)\n-\n-      scale, offset = _broadcast(self.gamma), _broadcast(self.beta)\n-\n-      if scale is not None:\n-        outputs = outputs * tf.cast(scale, outputs.dtype)\n-      if offset is not None:\n-        outputs = outputs + tf.cast(offset, outputs.dtype)\n-\n-    # If some components of the shape got lost due to adjustments, fix that.\n-    outputs.set_shape(input_shape)\n-\n-    return outputs\n-\n-  def compute_output_shape(self, input_shape):\n-    return input_shape\n-\n-  def get_config(self):\n-    config = {\n-        'axis': self.axis,\n-        'epsilon': self.epsilon,\n-        'center': self.center,\n-        'scale': self.scale,\n-        'beta_initializer': initializers.serialize(self.beta_initializer),\n-        'gamma_initializer': initializers.serialize(self.gamma_initializer),\n-        'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n-        'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n-        'beta_constraint': constraints.serialize(self.beta_constraint),\n-        'gamma_constraint': constraints.serialize(self.gamma_constraint)\n-    }\n-    base_config = super(LayerNormalization, self).get_config()\n-    return dict(list(base_config.items()) + list(config.items()))\n+        beta_initializer=beta_initializer,\n+        gamma_initializer=gamma_initializer,\n+        moving_mean_initializer=moving_mean_initializer,\n+        moving_variance_initializer=moving_variance_initializer,\n+        beta_regularizer=beta_regularizer,\n+        gamma_regularizer=gamma_regularizer,\n+        beta_constraint=beta_constraint,\n+        gamma_constraint=gamma_constraint,\n+        **kwargs)\n\n@@ -23,8 +23,8 @@ import keras\n from keras import combinations\n from keras import keras_parameterized\n from keras import testing_utils\n-from keras.layers import normalization\n-from keras.layers import normalization_v2\n+from keras.layers.normalization import batch_normalization\n+from keras.layers.normalization import batch_normalization_v1\n \n \n class BatchNormalizationTest(keras_parameterized.TestCase):\n@@ -136,16 +136,16 @@ class BatchNormalizationTest(keras_parameterized.TestCase):\n   @keras_parameterized.run_all_keras_modes\n   def test_batchnorm_correctness(self):\n     _run_batchnorm_correctness_test(\n-        normalization.BatchNormalization, dtype='float32')\n+        batch_normalization_v1.BatchNormalization, dtype='float32')\n     _run_batchnorm_correctness_test(\n-        normalization_v2.BatchNormalization, dtype='float32')\n+        batch_normalization.BatchNormalization, dtype='float32')\n \n   @keras_parameterized.run_all_keras_modes\n   def test_batchnorm_float16(self):\n     _run_batchnorm_correctness_test(\n-        normalization.BatchNormalization, dtype='float16')\n+        batch_normalization_v1.BatchNormalization, dtype='float16')\n     _run_batchnorm_correctness_test(\n-        normalization_v2.BatchNormalization, dtype='float16')\n+        batch_normalization.BatchNormalization, dtype='float16')\n \n   @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n   @testing_utils.enable_v2_dtype_behavior\n@@ -185,7 +185,7 @@ class BatchNormalizationTest(keras_parameterized.TestCase):\n     data_shape = (100, 3)\n \n     inputs = keras.Input((3,))\n-    bn = normalization_v2.BatchNormalization()\n+    bn = batch_normalization.BatchNormalization()\n     outputs = bn(inputs)\n     model = keras.Model(inputs, outputs)\n     model.compile(\n@@ -235,7 +235,7 @@ class BatchNormalizationTest(keras_parameterized.TestCase):\n     # future, it may be used in the nonfused case as well.\n \n     x = tf.constant([0., 2.], shape=[2, 1, 1, 1])\n-    layer = normalization_v2.BatchNormalization(\n+    layer = batch_normalization.BatchNormalization(\n         momentum=0.5, moving_variance_initializer='zeros')\n     layer(x, training=True)\n     self.assertTrue(layer.fused)\n@@ -245,7 +245,7 @@ class BatchNormalizationTest(keras_parameterized.TestCase):\n     self.assertAllEqual(self.evaluate(layer.moving_variance), [1.])\n \n     x = tf.constant([0., 2.], shape=[2, 1, 1, 1, 1])\n-    layer = normalization_v2.BatchNormalization(\n+    layer = batch_normalization.BatchNormalization(\n         momentum=0.5, moving_variance_initializer='zeros')\n     layer(x, training=True)\n     self.assertTrue(layer.fused)\n@@ -259,18 +259,18 @@ class BatchNormalizationV1Test(keras_parameterized.TestCase):\n \n   @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n   def test_v1_fused_attribute(self):\n-    norm = normalization.BatchNormalization()\n+    norm = batch_normalization_v1.BatchNormalization()\n     inp = keras.layers.Input((4, 4, 4))\n     norm(inp)\n     self.assertEqual(norm.fused, True)\n \n-    norm = normalization.BatchNormalization(fused=False)\n+    norm = batch_normalization_v1.BatchNormalization(fused=False)\n     self.assertEqual(norm.fused, False)\n     inp = keras.layers.Input(shape=(4, 4, 4))\n     norm(inp)\n     self.assertEqual(norm.fused, False)\n \n-    norm = normalization.BatchNormalization(virtual_batch_size=2)\n+    norm = batch_normalization_v1.BatchNormalization(virtual_batch_size=2)\n     self.assertEqual(norm.fused, True)\n     inp = keras.layers.Input(shape=(2, 2, 2))\n     norm(inp)\n@@ -282,69 +282,69 @@ class BatchNormalizationV2Test(keras_parameterized.TestCase):\n   @keras_parameterized.run_all_keras_modes\n   def test_basic_batchnorm_v2(self):\n     testing_utils.layer_test(\n-        normalization_v2.BatchNormalization,\n+        batch_normalization.BatchNormalization,\n         kwargs={'fused': True},\n         input_shape=(3, 3, 3, 3))\n     testing_utils.layer_test(\n-        normalization_v2.BatchNormalization,\n+        batch_normalization.BatchNormalization,\n         kwargs={'fused': None},\n         input_shape=(3, 3, 3))\n \n   @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n   def test_v2_fused_attribute(self):\n-    norm = normalization_v2.BatchNormalization()\n-    self.assertEqual(norm.fused, None)\n+    norm = batch_normalization.BatchNormalization()\n+    self.assertIsNone(norm.fused)\n     inp = keras.layers.Input(shape=(4, 4, 4))\n     norm(inp)\n     self.assertEqual(norm.fused, True)\n \n-    norm = normalization_v2.BatchNormalization()\n-    self.assertEqual(norm.fused, None)\n+    norm = batch_normalization.BatchNormalization()\n+    self.assertIsNone(norm.fused)\n     inp = keras.layers.Input(shape=(4, 4))\n     norm(inp)\n     self.assertEqual(norm.fused, False)\n \n-    norm = normalization_v2.BatchNormalization()\n+    norm = batch_normalization.BatchNormalization()\n     self.assertIsNone(norm.fused)\n     inp = keras.layers.Input(shape=(4, 4, 4, 4))\n     norm(inp)\n     self.assertEqual(norm.fused, True)\n \n-    norm = normalization_v2.BatchNormalization(virtual_batch_size=2)\n+    norm = batch_normalization.BatchNormalization(virtual_batch_size=2)\n     self.assertEqual(norm.fused, False)\n     inp = keras.layers.Input(shape=(4, 4, 4))\n     norm(inp)\n     self.assertEqual(norm.fused, False)\n \n-    norm = normalization_v2.BatchNormalization(fused=False)\n+    norm = batch_normalization.BatchNormalization(fused=False)\n     self.assertEqual(norm.fused, False)\n     inp = keras.layers.Input(shape=(4, 4, 4))\n     norm(inp)\n     self.assertEqual(norm.fused, False)\n \n-    norm = normalization_v2.BatchNormalization(fused=True, axis=[3])\n+    norm = batch_normalization.BatchNormalization(fused=True, axis=[3])\n     self.assertEqual(norm.fused, True)\n     inp = keras.layers.Input(shape=(4, 4, 4))\n     norm(inp)\n     self.assertEqual(norm.fused, True)\n \n     with self.assertRaisesRegex(ValueError, 'fused.*renorm'):\n-      normalization_v2.BatchNormalization(fused=True, renorm=True)\n+      batch_normalization.BatchNormalization(fused=True, renorm=True)\n \n     with self.assertRaisesRegex(ValueError, 'fused.*when axis is 1 or 3'):\n-      normalization_v2.BatchNormalization(fused=True, axis=2)\n+      batch_normalization.BatchNormalization(fused=True, axis=2)\n \n     with self.assertRaisesRegex(ValueError, 'fused.*when axis is 1 or 3'):\n-      normalization_v2.BatchNormalization(fused=True, axis=[1, 3])\n+      batch_normalization.BatchNormalization(fused=True, axis=[1, 3])\n \n     with self.assertRaisesRegex(ValueError, 'fused.*virtual_batch_size'):\n-      normalization_v2.BatchNormalization(fused=True, virtual_batch_size=2)\n+      batch_normalization.BatchNormalization(fused=True, virtual_batch_size=2)\n \n     with self.assertRaisesRegex(ValueError, 'fused.*adjustment'):\n-      normalization_v2.BatchNormalization(fused=True,\n-                                          adjustment=lambda _: (1, 0))\n+      batch_normalization.BatchNormalization(\n+          fused=True, adjustment=lambda _: (1, 0))\n \n-    norm = normalization_v2.BatchNormalization(fused=True)\n+    norm = batch_normalization.BatchNormalization(fused=True)\n     self.assertEqual(norm.fused, True)\n     inp = keras.layers.Input(shape=(4, 4))\n     with self.assertRaisesRegex(ValueError, '4D or 5D input tensors'):\n@@ -353,7 +353,7 @@ class BatchNormalizationV2Test(keras_parameterized.TestCase):\n   def test_updates_in_wrap_function(self):\n \n     def my_func():\n-      layer = normalization.BatchNormalization()\n+      layer = batch_normalization_v1.BatchNormalization()\n       x = tf.ones((10, 1))\n       y = layer(x, training=True)\n       # Updates should be tracked in a `wrap_function`.\n@@ -366,7 +366,7 @@ class BatchNormalizationV2Test(keras_parameterized.TestCase):\n   @keras_parameterized.run_all_keras_modes\n   def test_basic_batchnorm_v2_none_shape_and_virtual_batch_size(self):\n     # Test case for GitHub issue for 32380\n-    norm = normalization_v2.BatchNormalization(virtual_batch_size=8)\n+    norm = batch_normalization.BatchNormalization(virtual_batch_size=8)\n     inp = keras.layers.Input(shape=(None, None, 3))\n     _ = norm(inp)\n \n@@ -396,8 +396,10 @@ def _run_batchnorm_correctness_test(layer, dtype='float32', fused=False):\n   np.testing.assert_allclose(out.std(), 1.0, atol=2e-1)\n \n \n-@parameterized.parameters(\n-    [normalization.BatchNormalization, normalization_v2.BatchNormalization])\n+@parameterized.parameters([\n+    batch_normalization_v1.BatchNormalization,\n+    batch_normalization.BatchNormalization\n+])\n class NormalizationLayersGraphModeOnlyTest(\n     tf.test.TestCase, parameterized.TestCase):\n \n@@ -497,303 +499,5 @@ class NormalizationLayersGraphModeOnlyTest(\n             (val_a - np.mean(val_a)) / np.std(val_a), out, atol=1e-3)\n \n \n-def _run_layernorm_correctness_test(layer, dtype='float32'):\n-  model = keras.models.Sequential()\n-  model.add(keras.layers.Lambda(lambda x: tf.cast(x, dtype='float16')))\n-  norm = layer(input_shape=(2, 2, 2), dtype=dtype)\n-  model.add(norm)\n-  model.compile(\n-      loss='mse',\n-      optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),\n-      run_eagerly=testing_utils.should_run_eagerly())\n-\n-  # centered on 5.0, variance 10.0\n-  x = (np.random.normal(loc=5.0, scale=10.0, size=(1000, 2, 2, 2))\n-       .astype(dtype))\n-  model.fit(x, x, epochs=4, verbose=0)\n-  out = model.predict(x)\n-  out -= keras.backend.eval(norm.beta)\n-  out /= keras.backend.eval(norm.gamma)\n-\n-  np.testing.assert_allclose(out.mean(), 0.0, atol=1e-1)\n-  np.testing.assert_allclose(out.std(), 1.0, atol=1e-1)\n-\n-\n-class LayerNormalizationTest(keras_parameterized.TestCase):\n-\n-  @keras_parameterized.run_all_keras_modes\n-  def test_basic_layernorm(self):\n-    testing_utils.layer_test(\n-        keras.layers.LayerNormalization,\n-        kwargs={\n-            'gamma_regularizer': keras.regularizers.l2(0.01),\n-            'beta_regularizer': keras.regularizers.l2(0.01)\n-        },\n-        input_shape=(3, 4, 2))\n-    testing_utils.layer_test(\n-        keras.layers.LayerNormalization,\n-        kwargs={\n-            'gamma_initializer': 'ones',\n-            'beta_initializer': 'ones',\n-        },\n-        input_shape=(3, 4, 2))\n-    testing_utils.layer_test(\n-        keras.layers.LayerNormalization,\n-        kwargs={'scale': False,\n-                'center': False},\n-        input_shape=(3, 3))\n-    testing_utils.layer_test(\n-        keras.layers.LayerNormalization,\n-        kwargs={'axis': (-3, -2, -1)},\n-        input_shape=(2, 8, 8, 3))\n-\n-  @keras_parameterized.run_all_keras_modes\n-  def test_non_fused_layernorm(self):\n-    testing_utils.layer_test(\n-        keras.layers.LayerNormalization,\n-        kwargs={'axis': -2},\n-        input_shape=(3, 4, 2))\n-    testing_utils.layer_test(\n-        keras.layers.LayerNormalization,\n-        kwargs={'axis': (-3, -2)},\n-        input_shape=(2, 8, 8, 3))\n-    testing_utils.layer_test(\n-        keras.layers.LayerNormalization,\n-        kwargs={'axis': (-3, -1)},\n-        input_shape=(2, 8, 8, 3))\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def test_layernorm_weights(self):\n-    layer = keras.layers.LayerNormalization(scale=False, center=False)\n-    layer.build((None, 3, 4))\n-    self.assertEqual(len(layer.trainable_weights), 0)\n-    self.assertEqual(len(layer.weights), 0)\n-\n-    layer = keras.layers.LayerNormalization()\n-    layer.build((None, 3, 4))\n-    self.assertEqual(len(layer.trainable_weights), 2)\n-    self.assertEqual(len(layer.weights), 2)\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def test_layernorm_regularization(self):\n-    layer = keras.layers.LayerNormalization(\n-        gamma_regularizer='l1', beta_regularizer='l1')\n-    layer.build((None, 3, 4))\n-    self.assertEqual(len(layer.losses), 2)\n-    max_norm = keras.constraints.max_norm\n-    layer = keras.layers.LayerNormalization(\n-        gamma_constraint=max_norm, beta_constraint=max_norm)\n-    layer.build((None, 3, 4))\n-    self.assertEqual(layer.gamma.constraint, max_norm)\n-    self.assertEqual(layer.beta.constraint, max_norm)\n-\n-  @keras_parameterized.run_all_keras_modes\n-  def test_layernorm_convnet_channel_last(self):\n-    model = keras.models.Sequential()\n-    norm = keras.layers.LayerNormalization(input_shape=(4, 4, 3))\n-    model.add(norm)\n-    model.compile(\n-        loss='mse',\n-        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),\n-        run_eagerly=testing_utils.should_run_eagerly())\n-\n-    # centered on 5.0, variance 10.0\n-    x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 4, 4, 3))\n-    model.fit(x, x, epochs=4, verbose=0)\n-    out = model.predict(x)\n-    out -= np.reshape(keras.backend.eval(norm.beta), (1, 1, 1, 3))\n-    out /= np.reshape(keras.backend.eval(norm.gamma), (1, 1, 1, 3))\n-\n-    np.testing.assert_allclose(np.mean(out, axis=(0, 1, 2)), 0.0, atol=1e-1)\n-    np.testing.assert_allclose(np.std(out, axis=(0, 1, 2)), 1.0, atol=1e-1)\n-\n-  @keras_parameterized.run_all_keras_modes\n-  def test_layernorm_correctness(self):\n-    _run_layernorm_correctness_test(\n-        normalization.LayerNormalization, dtype='float32')\n-\n-  @keras_parameterized.run_all_keras_modes\n-  def test_layernorm_mixed_precision(self):\n-    _run_layernorm_correctness_test(\n-        normalization.LayerNormalization, dtype='float16')\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def testIncorrectAxisType(self):\n-    with self.assertRaisesRegex(TypeError,\n-                                r'Expected an int or a list/tuple of ints'):\n-      _ = normalization.LayerNormalization(axis={'axis': -1})\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def testInvalidAxis(self):\n-    with self.assertRaisesRegex(ValueError, r'Invalid axis: 3'):\n-      layer_norm = normalization.LayerNormalization(axis=3)\n-      layer_norm.build(input_shape=(2, 2, 2))\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def testDuplicateAxis(self):\n-    with self.assertRaisesRegex(ValueError, r'Duplicate axis:'):\n-      layer_norm = normalization.LayerNormalization(axis=[-1, -1])\n-      layer_norm.build(input_shape=(2, 2, 2))\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def testFusedAttr(self):\n-    layer_norm = normalization.LayerNormalization(axis=[-2, -1])\n-    layer_norm.build(input_shape=(2, 2, 2))\n-    self.assertEqual(layer_norm._fused, True)\n-\n-\n-class LayerNormalizationNumericsTest(keras_parameterized.TestCase):\n-  \"\"\"Tests LayerNormalization has correct and numerically stable outputs.\"\"\"\n-\n-  def _expected_layer_norm(self, x, beta, gamma, batch_input_shape, axis,\n-                           epsilon):\n-    \"\"\"Returns the layer norm, which is computed using NumPy.\"\"\"\n-    broadcast_shape = [batch_input_shape[i] if i in axis else 1\n-                       for i in range(len(batch_input_shape))]\n-    mean = np.mean(x, axis=axis, keepdims=True)\n-    var = np.var(x, axis=axis, keepdims=True)\n-    expected = (x - mean) / np.sqrt(var + epsilon)\n-    expected *= np.reshape(gamma, broadcast_shape)\n-    expected += np.reshape(beta, broadcast_shape)\n-    return expected\n-\n-  def _test_forward_pass(self, batch_input_shape, axis, fp64_tol=1e-14,\n-                         fp32_tol=1e-6, fp16_tol=1e-2):\n-    \"\"\"Tests the forward pass of layer normalization.\n-\n-    Args:\n-      batch_input_shape: The input shape that will be used to test, including\n-        the batch dimension.\n-      axis: A list of axises to normalize. Will be passed to the `axis` argument\n-        of LayerNormalization.\n-      fp64_tol: The relative and absolute tolerance for float64.\n-      fp32_tol: The relative and absolute tolerance for float32.\n-      fp16_tol: The relative and absolute tolerance for float16.\n-    \"\"\"\n-    param_shape = [batch_input_shape[i] for i in axis]\n-    param_elems = 1\n-    for dim in param_shape:\n-      param_elems *= dim\n-    beta = np.arange(param_elems, dtype='float64').reshape(param_shape)\n-    gamma = np.arange(1, param_elems + 1, dtype='float64').reshape(param_shape)\n-    x = np.random.normal(size=batch_input_shape)\n-\n-    for epsilon in 1e-12, 1e-3:\n-      expected = self._expected_layer_norm(x, beta, gamma, batch_input_shape,\n-                                           axis, epsilon)\n-      for dtype in 'float64', 'float32', 'float16':\n-        norm = normalization.LayerNormalization(\n-            axis=axis, dtype=dtype, batch_input_shape=batch_input_shape,\n-            epsilon=epsilon, beta_initializer=keras.initializers.constant(beta),\n-            gamma_initializer=keras.initializers.constant(gamma))\n-        y = norm(keras.backend.cast(x, dtype))\n-        actual = keras.backend.eval(y)\n-\n-        if dtype == 'float64':\n-          tol = fp64_tol\n-        elif dtype == 'float32':\n-          tol = fp32_tol\n-        else:\n-          assert dtype == 'float16'\n-          tol = fp16_tol\n-\n-        # We use absolute tolerances in addition to relative tolerances, because\n-        # some of the values are very close to zero.\n-        self.assertAllClose(expected, actual, rtol=tol, atol=tol)\n-\n-  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n-  def test_forward(self):\n-    # For numeric stability, we ensure the axis's dimension(s) have at least 4\n-    # elements.\n-    self._test_forward_pass((4, 3), (0,))\n-    self._test_forward_pass((3, 4), (1,))\n-    self._test_forward_pass((4, 3, 2), (0,))\n-    self._test_forward_pass((2, 4, 2), (1,))\n-    self._test_forward_pass((2, 3, 4), (2,), fp16_tol=5e-2)\n-    self._test_forward_pass((2, 3, 2), (0, 2))\n-    self._test_forward_pass((2, 2, 2, 2), (1, 3))\n-    self._test_forward_pass((2, 2, 2, 2), (2, 3))\n-    self._test_forward_pass((2, 3, 4, 5), (3,))\n-\n-  def _test_backward_pass(self, batch_input_shape, axis, fp64_tol=1e-5,\n-                          fp32_tol=1e-5, fp16_tol=2e-2):\n-    \"\"\"Tests the backwards pass of layer normalization.\n-\n-    Args:\n-      batch_input_shape: The input shape that will be used to test, including\n-        the batch dimension.\n-      axis: A list of axises to normalize. Will be passed to the `axis` argument\n-        of LayerNormalization.\n-      fp64_tol: The relative and absolute tolerance for float64.\n-      fp32_tol: The relative and absolute tolerance for float32.\n-      fp16_tol: The relative and absolute tolerance for float16.\n-    \"\"\"\n-    param_shape = [batch_input_shape[i] for i in axis]\n-    param_elems = 1\n-    for dim in param_shape:\n-      param_elems *= dim\n-    beta = np.arange(param_elems, dtype='float64').reshape(param_shape)\n-    gamma = np.arange(1, param_elems + 1, dtype='float64').reshape(param_shape)\n-    x = np.random.normal(size=batch_input_shape)\n-\n-    for epsilon in 1e-12, 1e-3:\n-      # Float64 must come first in this list, as we use the float64 numerical\n-      # gradients to compare to the float32 and float16 symbolic gradients as\n-      # well. Computing float32/float16 numerical gradients is too numerically\n-      # unstable.\n-      for dtype in 'float64', 'float32', 'float16':\n-        norm = normalization.LayerNormalization(\n-            axis=axis, dtype=dtype, batch_input_shape=batch_input_shape,\n-            epsilon=epsilon, beta_initializer=keras.initializers.constant(beta),\n-            gamma_initializer=keras.initializers.constant(gamma))\n-        norm.build(x.shape)\n-\n-        # pylint: disable=cell-var-from-loop\n-        def forward_fn(x, beta, gamma):\n-          # We must monkey-patch the attributes of `norm` with the function\n-          # arguments, so that the gradient checker will properly compute their\n-          # gradients. The gradient checker computes gradients with respect to\n-          # the input arguments of `f`.\n-          with tf.compat.v1.test.mock.patch.object(norm, 'beta', beta):\n-            with tf.compat.v1.test.mock.patch.object(norm, 'gamma', gamma):\n-              return norm(x)\n-        # pylint: enable=cell-var-from-loop\n-        results = tf.test.compute_gradient(\n-            forward_fn, [keras.backend.cast(x, dtype), norm.beta, norm.gamma])\n-        ([x_grad_t, beta_grad_t, gamma_grad_t],\n-         [x_grad_n, beta_grad_n, gamma_grad_n]) = results\n-\n-        if dtype == 'float64':\n-          # We use the float64 numeric gradients as the reference, to compare\n-          # against the symbolic gradients for all dtypes.\n-          x_grad_ref = x_grad_n\n-          beta_grad_ref = beta_grad_n\n-          gamma_grad_ref = gamma_grad_n\n-          tol = fp64_tol\n-        elif dtype == 'float32':\n-          tol = fp32_tol\n-        else:\n-          assert dtype == 'float16'\n-          tol = fp16_tol\n-\n-        # We use absolute tolerances in addition to relative tolerances, because\n-        # some of the values are very close to zero.\n-        self.assertAllClose(x_grad_t, x_grad_ref, rtol=tol, atol=tol)\n-        self.assertAllClose(beta_grad_t, beta_grad_ref, rtol=tol, atol=tol)\n-        self.assertAllClose(gamma_grad_t, gamma_grad_ref, rtol=tol, atol=tol)\n-\n-  # The gradient_checker_v2 does not work properly with LayerNorm in graph mode.\n-  @testing_utils.run_v2_only\n-  def test_backward(self):\n-    # For numeric stability, we ensure the axis's dimension(s) have at least 4\n-    # elements.\n-    self._test_backward_pass((4, 3), (0,))\n-    self._test_backward_pass((2, 4, 2), (1,))\n-    self._test_backward_pass((2, 3, 4), (2,))\n-    self._test_backward_pass((2, 3, 2), (0, 2), fp64_tol=5e-4, fp32_tol=5e-4)\n-    self._test_backward_pass((2, 2, 2, 2), (1, 3))\n-    self._test_backward_pass((2, 2, 2, 2), (2, 3))\n-\n-\n if __name__ == '__main__':\n   tf.test.main()\n\n@@ -0,0 +1,25 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Batch Normalization V1 layer.\"\"\"\n+# pylint: disable=g-classes-have-attributes\n+\n+from keras.layers.normalization import batch_normalization\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+# pylint: disable=missing-docstring\n+@keras_export(v1=['keras.layers.BatchNormalization'])\n+class BatchNormalization(batch_normalization.BatchNormalizationBase):\n+  _USE_V2_BEHAVIOR = False\n\n@@ -0,0 +1,361 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Layer Normalization layer.\"\"\"\n+\n+import tensorflow.compat.v2 as tf\n+# pylint: disable=g-classes-have-attributes\n+\n+from keras import constraints\n+from keras import initializers\n+from keras import regularizers\n+from keras.engine.base_layer import Layer\n+\n+from tensorflow.python.util.tf_export import keras_export\n+\n+\n+@keras_export('keras.layers.LayerNormalization')\n+class LayerNormalization(Layer):\n+  \"\"\"Layer normalization layer (Ba et al., 2016).\n+\n+  Normalize the activations of the previous layer for each given example in a\n+  batch independently, rather than across a batch like Batch Normalization.\n+  i.e. applies a transformation that maintains the mean activation within each\n+  example close to 0 and the activation standard deviation close to 1.\n+\n+  Given a tensor `inputs`, moments are calculated and normalization\n+  is performed across the axes specified in `axis`.\n+\n+  Example:\n+\n+  >>> data = tf.constant(np.arange(10).reshape(5, 2) * 10, dtype=tf.float32)\n+  >>> print(data)\n+  tf.Tensor(\n+  [[ 0. 10.]\n+   [20. 30.]\n+   [40. 50.]\n+   [60. 70.]\n+   [80. 90.]], shape=(5, 2), dtype=float32)\n+\n+  >>> layer = tf.keras.layers.LayerNormalization(axis=1)\n+  >>> output = layer(data)\n+  >>> print(output)\n+  tf.Tensor(\n+  [[-1. 1.]\n+   [-1. 1.]\n+   [-1. 1.]\n+   [-1. 1.]\n+   [-1. 1.]], shape=(5, 2), dtype=float32)\n+\n+  Notice that with Layer Normalization the normalization happens across the\n+  axes *within* each example, rather than across different examples in the\n+  batch.\n+\n+  If `scale` or `center` are enabled, the layer will scale the normalized\n+  outputs by broadcasting them with a trainable variable `gamma`, and center\n+  the outputs by broadcasting with a trainable variable `beta`. `gamma` will\n+  default to a ones tensor and `beta` will default to a zeros tensor, so that\n+  centering and scaling are no-ops before training has begun.\n+\n+  So, with scaling and centering enabled the normalization equations\n+  are as follows:\n+\n+  Let the intermediate activations for a mini-batch to be the `inputs`.\n+\n+  For each sample `x_i` in `inputs` with `k` features, we compute the mean and\n+  variance of the sample:\n+\n+  ```python\n+  mean_i = sum(x_i[j] for j in range(k)) / k\n+  var_i = sum((x_i[j] - mean_i) ** 2 for j in range(k)) / k\n+  ```\n+\n+  and then compute a normalized `x_i_normalized`, including a small factor\n+  `epsilon` for numerical stability.\n+\n+  ```python\n+  x_i_normalized = (x_i - mean_i) / sqrt(var_i + epsilon)\n+  ```\n+\n+  And finally `x_i_normalized ` is linearly transformed by `gamma` and `beta`,\n+  which are learned parameters:\n+\n+  ```python\n+  output_i = x_i_normalized * gamma + beta\n+  ```\n+\n+  `gamma` and `beta` will span the axes of `inputs` specified in `axis`, and\n+  this part of the inputs' shape must be fully defined.\n+\n+  For example:\n+\n+  >>> layer = tf.keras.layers.LayerNormalization(axis=[1, 2, 3])\n+  >>> layer.build([5, 20, 30, 40])\n+  >>> print(layer.beta.shape)\n+  (20, 30, 40)\n+  >>> print(layer.gamma.shape)\n+  (20, 30, 40)\n+\n+  Note that other implementations of layer normalization may choose to define\n+  `gamma` and `beta` over a separate set of axes from the axes being\n+  normalized across. For example, Group Normalization\n+  ([Wu et al. 2018](https://arxiv.org/abs/1803.08494)) with group size of 1\n+  corresponds to a Layer Normalization that normalizes across height, width,\n+  and channel and has `gamma` and `beta` span only the channel dimension.\n+  So, this Layer Normalization implementation will not match a Group\n+  Normalization layer with group size set to 1.\n+\n+  Args:\n+    axis: Integer or List/Tuple. The axis or axes to normalize across. Typically\n+      this is the features axis/axes. The left-out axes are typically the batch\n+      axis/axes. This argument defaults to `-1`, the last dimension in the\n+      input.\n+    epsilon: Small float added to variance to avoid dividing by zero. Defaults\n+      to 1e-3\n+    center: If True, add offset of `beta` to normalized tensor. If False, `beta`\n+      is ignored. Defaults to True.\n+    scale: If True, multiply by `gamma`. If False, `gamma` is not used. Defaults\n+      to True. When the next layer is linear (also e.g. `nn.relu`), this can be\n+      disabled since the scaling will be done by the next layer.\n+    beta_initializer: Initializer for the beta weight. Defaults to zeros.\n+    gamma_initializer: Initializer for the gamma weight. Defaults to ones.\n+    beta_regularizer: Optional regularizer for the beta weight. None by default.\n+    gamma_regularizer: Optional regularizer for the gamma weight. None by\n+      default.\n+    beta_constraint: Optional constraint for the beta weight. None by default.\n+    gamma_constraint: Optional constraint for the gamma weight. None by default.\n+\n+  Input shape:\n+    Arbitrary. Use the keyword argument `input_shape` (tuple of\n+    integers, does not include the samples axis) when using this layer as the\n+    first layer in a model.\n+\n+  Output shape:\n+    Same shape as input.\n+\n+  Reference:\n+    - [Lei Ba et al., 2016](https://arxiv.org/abs/1607.06450).\n+  \"\"\"\n+\n+  def __init__(self,\n+               axis=-1,\n+               epsilon=1e-3,\n+               center=True,\n+               scale=True,\n+               beta_initializer='zeros',\n+               gamma_initializer='ones',\n+               beta_regularizer=None,\n+               gamma_regularizer=None,\n+               beta_constraint=None,\n+               gamma_constraint=None,\n+               **kwargs):\n+    super(LayerNormalization, self).__init__(**kwargs)\n+    if isinstance(axis, (list, tuple)):\n+      self.axis = axis[:]\n+    elif isinstance(axis, int):\n+      self.axis = axis\n+    else:\n+      raise TypeError('Expected an int or a list/tuple of ints for the '\n+                      'argument \\'axis\\', but received: %r' % axis)\n+\n+    self.epsilon = epsilon\n+    self.center = center\n+    self.scale = scale\n+    self.beta_initializer = initializers.get(beta_initializer)\n+    self.gamma_initializer = initializers.get(gamma_initializer)\n+    self.beta_regularizer = regularizers.get(beta_regularizer)\n+    self.gamma_regularizer = regularizers.get(gamma_regularizer)\n+    self.beta_constraint = constraints.get(beta_constraint)\n+    self.gamma_constraint = constraints.get(gamma_constraint)\n+\n+    self.supports_masking = True\n+\n+    # Indicates whether a faster fused implementation can be used. This will be\n+    # set to True or False in build()\"\n+    self._fused = None\n+\n+  def _fused_can_be_used(self, ndims):\n+    \"\"\"Returns false if fused implementation cannot be used.\n+\n+    Check if the axis is contiguous and can be collapsed into the last axis.\n+    The self.axis is assumed to have no duplicates.\n+    \"\"\"\n+    axis = sorted(self.axis)\n+    can_use_fused = False\n+\n+    if axis[-1] == ndims - 1 and axis[-1] - axis[0] == len(axis) - 1:\n+      can_use_fused = True\n+\n+    # fused_batch_norm will silently raise epsilon to be at least 1.001e-5, so\n+    # we cannot used the fused version if epsilon is below that value. Also, the\n+    # variable dtype must be float32, as fused_batch_norm only supports float32\n+    # variables.\n+    if self.epsilon < 1.001e-5 or self.dtype != 'float32':\n+      can_use_fused = False\n+\n+    return can_use_fused\n+\n+  def build(self, input_shape):\n+    ndims = len(input_shape)\n+    if ndims is None:\n+      raise ValueError('Input shape %s has undefined rank.' % input_shape)\n+\n+    # Convert axis to list and resolve negatives\n+    if isinstance(self.axis, int):\n+      self.axis = [self.axis]\n+    elif isinstance(self.axis, tuple):\n+      self.axis = list(self.axis)\n+    for idx, x in enumerate(self.axis):\n+      if x < 0:\n+        self.axis[idx] = ndims + x\n+\n+    # Validate axes\n+    for x in self.axis:\n+      if x < 0 or x >= ndims:\n+        raise ValueError('Invalid axis: %d' % x)\n+    if len(self.axis) != len(set(self.axis)):\n+      raise ValueError('Duplicate axis: {}'.format(tuple(self.axis)))\n+\n+    param_shape = [input_shape[dim] for dim in self.axis]\n+    if self.scale:\n+      self.gamma = self.add_weight(\n+          name='gamma',\n+          shape=param_shape,\n+          initializer=self.gamma_initializer,\n+          regularizer=self.gamma_regularizer,\n+          constraint=self.gamma_constraint,\n+          trainable=True,\n+          experimental_autocast=False)\n+    else:\n+      self.gamma = None\n+\n+    if self.center:\n+      self.beta = self.add_weight(\n+          name='beta',\n+          shape=param_shape,\n+          initializer=self.beta_initializer,\n+          regularizer=self.beta_regularizer,\n+          constraint=self.beta_constraint,\n+          trainable=True,\n+          experimental_autocast=False)\n+    else:\n+      self.beta = None\n+\n+    self._fused = self._fused_can_be_used(ndims)\n+\n+    self.built = True\n+\n+  def call(self, inputs):\n+    # Compute the axes along which to reduce the mean / variance\n+    input_shape = inputs.shape\n+    ndims = len(input_shape)\n+\n+    # Broadcasting only necessary for norm when the axis is not just\n+    # the last dimension\n+    broadcast_shape = [1] * ndims\n+    for dim in self.axis:\n+      broadcast_shape[dim] = input_shape.dims[dim].value\n+\n+    def _broadcast(v):\n+      if (v is not None and len(v.shape) != ndims and self.axis != [ndims - 1]):\n+        return tf.reshape(v, broadcast_shape)\n+      return v\n+\n+    if not self._fused:\n+      input_dtype = inputs.dtype\n+      if input_dtype in ('float16', 'bfloat16') and self.dtype == 'float32':\n+        # If mixed precision is used, cast inputs to float32 so that this is at\n+        # least as numerically stable as the fused version.\n+        inputs = tf.cast(inputs, 'float32')\n+\n+      # Calculate the moments on the last axis (layer activations).\n+      mean, variance = tf.compat.v1.nn.moments(inputs, self.axis, keep_dims=True)\n+\n+      scale, offset = _broadcast(self.gamma), _broadcast(self.beta)\n+\n+      # Compute layer normalization using the batch_normalization function.\n+      outputs = tf.nn.batch_normalization(\n+          inputs,\n+          mean,\n+          variance,\n+          offset=offset,\n+          scale=scale,\n+          variance_epsilon=self.epsilon)\n+      outputs = tf.cast(outputs, input_dtype)\n+    else:\n+      # Collapse dims before self.axis, and dims in self.axis\n+      pre_dim, in_dim = (1, 1)\n+      axis = sorted(self.axis)\n+      tensor_shape = tf.compat.v1.shape(inputs)\n+      for dim in range(0, ndims):\n+        dim_tensor = tensor_shape[dim]\n+        if dim < axis[0]:\n+          pre_dim = pre_dim * dim_tensor\n+        else:\n+          assert dim in axis\n+          in_dim = in_dim * dim_tensor\n+\n+      squeezed_shape = [1, pre_dim, in_dim, 1]\n+      # This fused operation requires reshaped inputs to be NCHW.\n+      data_format = 'NCHW'\n+\n+      inputs = tf.reshape(inputs, squeezed_shape)\n+\n+      # self.gamma and self.beta have the wrong shape for fused_batch_norm, so\n+      # we cannot pass them as the scale and offset parameters. Therefore, we\n+      # create two constant tensors in correct shapes for fused_batch_norm and\n+      # later construct a separate calculation on the scale and offset.\n+      scale = tf.ones([pre_dim], dtype=self.dtype)\n+      offset = tf.zeros([pre_dim], dtype=self.dtype)\n+\n+      # Compute layer normalization using the fused_batch_norm function.\n+      outputs, _, _ = tf.compat.v1.nn.fused_batch_norm(\n+          inputs,\n+          scale=scale,\n+          offset=offset,\n+          epsilon=self.epsilon,\n+          data_format=data_format)\n+\n+      outputs = tf.reshape(outputs, tensor_shape)\n+\n+      scale, offset = _broadcast(self.gamma), _broadcast(self.beta)\n+\n+      if scale is not None:\n+        outputs = outputs * tf.cast(scale, outputs.dtype)\n+      if offset is not None:\n+        outputs = outputs + tf.cast(offset, outputs.dtype)\n+\n+    # If some components of the shape got lost due to adjustments, fix that.\n+    outputs.set_shape(input_shape)\n+\n+    return outputs\n+\n+  def compute_output_shape(self, input_shape):\n+    return input_shape\n+\n+  def get_config(self):\n+    config = {\n+        'axis': self.axis,\n+        'epsilon': self.epsilon,\n+        'center': self.center,\n+        'scale': self.scale,\n+        'beta_initializer': initializers.serialize(self.beta_initializer),\n+        'gamma_initializer': initializers.serialize(self.gamma_initializer),\n+        'beta_regularizer': regularizers.serialize(self.beta_regularizer),\n+        'gamma_regularizer': regularizers.serialize(self.gamma_regularizer),\n+        'beta_constraint': constraints.serialize(self.beta_constraint),\n+        'gamma_constraint': constraints.serialize(self.gamma_constraint)\n+    }\n+    base_config = super(LayerNormalization, self).get_config()\n+    return dict(list(base_config.items()) + list(config.items()))\n\n@@ -0,0 +1,327 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for normalization layers.\"\"\"\n+\n+import tensorflow.compat.v2 as tf\n+\n+import numpy as np\n+\n+import keras\n+from keras import combinations\n+from keras import keras_parameterized\n+from keras import testing_utils\n+from keras.layers.normalization import layer_normalization\n+\n+\n+def _run_layernorm_correctness_test(layer, dtype='float32'):\n+  model = keras.models.Sequential()\n+  model.add(keras.layers.Lambda(lambda x: tf.cast(x, dtype='float16')))\n+  norm = layer(input_shape=(2, 2, 2), dtype=dtype)\n+  model.add(norm)\n+  model.compile(\n+      loss='mse',\n+      optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),\n+      run_eagerly=testing_utils.should_run_eagerly())\n+\n+  # centered on 5.0, variance 10.0\n+  x = (np.random.normal(loc=5.0, scale=10.0, size=(1000, 2, 2, 2))\n+       .astype(dtype))\n+  model.fit(x, x, epochs=4, verbose=0)\n+  out = model.predict(x)\n+  out -= keras.backend.eval(norm.beta)\n+  out /= keras.backend.eval(norm.gamma)\n+\n+  np.testing.assert_allclose(out.mean(), 0.0, atol=1e-1)\n+  np.testing.assert_allclose(out.std(), 1.0, atol=1e-1)\n+\n+\n+class LayerNormalizationTest(keras_parameterized.TestCase):\n+\n+  @keras_parameterized.run_all_keras_modes\n+  def test_basic_layernorm(self):\n+    testing_utils.layer_test(\n+        keras.layers.LayerNormalization,\n+        kwargs={\n+            'gamma_regularizer': keras.regularizers.l2(0.01),\n+            'beta_regularizer': keras.regularizers.l2(0.01)\n+        },\n+        input_shape=(3, 4, 2))\n+    testing_utils.layer_test(\n+        keras.layers.LayerNormalization,\n+        kwargs={\n+            'gamma_initializer': 'ones',\n+            'beta_initializer': 'ones',\n+        },\n+        input_shape=(3, 4, 2))\n+    testing_utils.layer_test(\n+        keras.layers.LayerNormalization,\n+        kwargs={'scale': False,\n+                'center': False},\n+        input_shape=(3, 3))\n+    testing_utils.layer_test(\n+        keras.layers.LayerNormalization,\n+        kwargs={'axis': (-3, -2, -1)},\n+        input_shape=(2, 8, 8, 3))\n+\n+  @keras_parameterized.run_all_keras_modes\n+  def test_non_fused_layernorm(self):\n+    testing_utils.layer_test(\n+        keras.layers.LayerNormalization,\n+        kwargs={'axis': -2},\n+        input_shape=(3, 4, 2))\n+    testing_utils.layer_test(\n+        keras.layers.LayerNormalization,\n+        kwargs={'axis': (-3, -2)},\n+        input_shape=(2, 8, 8, 3))\n+    testing_utils.layer_test(\n+        keras.layers.LayerNormalization,\n+        kwargs={'axis': (-3, -1)},\n+        input_shape=(2, 8, 8, 3))\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_layernorm_weights(self):\n+    layer = keras.layers.LayerNormalization(scale=False, center=False)\n+    layer.build((None, 3, 4))\n+    self.assertEqual(len(layer.trainable_weights), 0)\n+    self.assertEqual(len(layer.weights), 0)\n+\n+    layer = keras.layers.LayerNormalization()\n+    layer.build((None, 3, 4))\n+    self.assertEqual(len(layer.trainable_weights), 2)\n+    self.assertEqual(len(layer.weights), 2)\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_layernorm_regularization(self):\n+    layer = keras.layers.LayerNormalization(\n+        gamma_regularizer='l1', beta_regularizer='l1')\n+    layer.build((None, 3, 4))\n+    self.assertEqual(len(layer.losses), 2)\n+    max_norm = keras.constraints.max_norm\n+    layer = keras.layers.LayerNormalization(\n+        gamma_constraint=max_norm, beta_constraint=max_norm)\n+    layer.build((None, 3, 4))\n+    self.assertEqual(layer.gamma.constraint, max_norm)\n+    self.assertEqual(layer.beta.constraint, max_norm)\n+\n+  @keras_parameterized.run_all_keras_modes\n+  def test_layernorm_convnet_channel_last(self):\n+    model = keras.models.Sequential()\n+    norm = keras.layers.LayerNormalization(input_shape=(4, 4, 3))\n+    model.add(norm)\n+    model.compile(\n+        loss='mse',\n+        optimizer=tf.compat.v1.train.GradientDescentOptimizer(0.01),\n+        run_eagerly=testing_utils.should_run_eagerly())\n+\n+    # centered on 5.0, variance 10.0\n+    x = np.random.normal(loc=5.0, scale=10.0, size=(1000, 4, 4, 3))\n+    model.fit(x, x, epochs=4, verbose=0)\n+    out = model.predict(x)\n+    out -= np.reshape(keras.backend.eval(norm.beta), (1, 1, 1, 3))\n+    out /= np.reshape(keras.backend.eval(norm.gamma), (1, 1, 1, 3))\n+\n+    np.testing.assert_allclose(np.mean(out, axis=(0, 1, 2)), 0.0, atol=1e-1)\n+    np.testing.assert_allclose(np.std(out, axis=(0, 1, 2)), 1.0, atol=1e-1)\n+\n+  @keras_parameterized.run_all_keras_modes\n+  def test_layernorm_correctness(self):\n+    _run_layernorm_correctness_test(\n+        layer_normalization.LayerNormalization, dtype='float32')\n+\n+  @keras_parameterized.run_all_keras_modes\n+  def test_layernorm_mixed_precision(self):\n+    _run_layernorm_correctness_test(\n+        layer_normalization.LayerNormalization, dtype='float16')\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def testIncorrectAxisType(self):\n+    with self.assertRaisesRegex(TypeError,\n+                                r'Expected an int or a list/tuple of ints'):\n+      _ = layer_normalization.LayerNormalization(axis={'axis': -1})\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def testInvalidAxis(self):\n+    with self.assertRaisesRegex(ValueError, r'Invalid axis: 3'):\n+      layer_norm = layer_normalization.LayerNormalization(axis=3)\n+      layer_norm.build(input_shape=(2, 2, 2))\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def testDuplicateAxis(self):\n+    with self.assertRaisesRegex(ValueError, r'Duplicate axis:'):\n+      layer_norm = layer_normalization.LayerNormalization(axis=[-1, -1])\n+      layer_norm.build(input_shape=(2, 2, 2))\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def testFusedAttr(self):\n+    layer_norm = layer_normalization.LayerNormalization(axis=[-2, -1])\n+    layer_norm.build(input_shape=(2, 2, 2))\n+    self.assertEqual(layer_norm._fused, True)\n+\n+\n+class LayerNormalizationNumericsTest(keras_parameterized.TestCase):\n+  \"\"\"Tests LayerNormalization has correct and numerically stable outputs.\"\"\"\n+\n+  def _expected_layer_norm(self, x, beta, gamma, batch_input_shape, axis,\n+                           epsilon):\n+    \"\"\"Returns the layer norm, which is computed using NumPy.\"\"\"\n+    broadcast_shape = [batch_input_shape[i] if i in axis else 1\n+                       for i in range(len(batch_input_shape))]\n+    mean = np.mean(x, axis=axis, keepdims=True)\n+    var = np.var(x, axis=axis, keepdims=True)\n+    expected = (x - mean) / np.sqrt(var + epsilon)\n+    expected *= np.reshape(gamma, broadcast_shape)\n+    expected += np.reshape(beta, broadcast_shape)\n+    return expected\n+\n+  def _test_forward_pass(self, batch_input_shape, axis, fp64_tol=1e-14,\n+                         fp32_tol=1e-6, fp16_tol=1e-2):\n+    \"\"\"Tests the forward pass of layer layer_normalization.\n+\n+    Args:\n+      batch_input_shape: The input shape that will be used to test, including\n+        the batch dimension.\n+      axis: A list of axises to normalize. Will be passed to the `axis` argument\n+        of Layerlayer_normalization.\n+      fp64_tol: The relative and absolute tolerance for float64.\n+      fp32_tol: The relative and absolute tolerance for float32.\n+      fp16_tol: The relative and absolute tolerance for float16.\n+    \"\"\"\n+    param_shape = [batch_input_shape[i] for i in axis]\n+    param_elems = 1\n+    for dim in param_shape:\n+      param_elems *= dim\n+    beta = np.arange(param_elems, dtype='float64').reshape(param_shape)\n+    gamma = np.arange(1, param_elems + 1, dtype='float64').reshape(param_shape)\n+    x = np.random.normal(size=batch_input_shape)\n+\n+    for epsilon in 1e-12, 1e-3:\n+      expected = self._expected_layer_norm(x, beta, gamma, batch_input_shape,\n+                                           axis, epsilon)\n+      for dtype in 'float64', 'float32', 'float16':\n+        norm = layer_normalization.LayerNormalization(\n+            axis=axis, dtype=dtype, batch_input_shape=batch_input_shape,\n+            epsilon=epsilon, beta_initializer=keras.initializers.constant(beta),\n+            gamma_initializer=keras.initializers.constant(gamma))\n+        y = norm(keras.backend.cast(x, dtype))\n+        actual = keras.backend.eval(y)\n+\n+        if dtype == 'float64':\n+          tol = fp64_tol\n+        elif dtype == 'float32':\n+          tol = fp32_tol\n+        else:\n+          assert dtype == 'float16'\n+          tol = fp16_tol\n+\n+        # We use absolute tolerances in addition to relative tolerances, because\n+        # some of the values are very close to zero.\n+        self.assertAllClose(expected, actual, rtol=tol, atol=tol)\n+\n+  @combinations.generate(combinations.combine(mode=['graph', 'eager']))\n+  def test_forward(self):\n+    # For numeric stability, we ensure the axis's dimension(s) have at least 4\n+    # elements.\n+    self._test_forward_pass((4, 3), (0,))\n+    self._test_forward_pass((3, 4), (1,))\n+    self._test_forward_pass((4, 3, 2), (0,))\n+    self._test_forward_pass((2, 4, 2), (1,))\n+    self._test_forward_pass((2, 3, 4), (2,), fp16_tol=5e-2)\n+    self._test_forward_pass((2, 3, 2), (0, 2))\n+    self._test_forward_pass((2, 2, 2, 2), (1, 3))\n+    self._test_forward_pass((2, 2, 2, 2), (2, 3))\n+    self._test_forward_pass((2, 3, 4, 5), (3,))\n+\n+  def _test_backward_pass(self, batch_input_shape, axis, fp64_tol=1e-5,\n+                          fp32_tol=1e-5, fp16_tol=2e-2):\n+    \"\"\"Tests the backwards pass of layer layer_normalization.\n+\n+    Args:\n+      batch_input_shape: The input shape that will be used to test, including\n+        the batch dimension.\n+      axis: A list of axises to normalize. Will be passed to the `axis` argument\n+        of Layerlayer_normalization.\n+      fp64_tol: The relative and absolute tolerance for float64.\n+      fp32_tol: The relative and absolute tolerance for float32.\n+      fp16_tol: The relative and absolute tolerance for float16.\n+    \"\"\"\n+    param_shape = [batch_input_shape[i] for i in axis]\n+    param_elems = 1\n+    for dim in param_shape:\n+      param_elems *= dim\n+    beta = np.arange(param_elems, dtype='float64').reshape(param_shape)\n+    gamma = np.arange(1, param_elems + 1, dtype='float64').reshape(param_shape)\n+    x = np.random.normal(size=batch_input_shape)\n+\n+    for epsilon in 1e-12, 1e-3:\n+      # Float64 must come first in this list, as we use the float64 numerical\n+      # gradients to compare to the float32 and float16 symbolic gradients as\n+      # well. Computing float32/float16 numerical gradients is too numerically\n+      # unstable.\n+      for dtype in 'float64', 'float32', 'float16':\n+        norm = layer_normalization.LayerNormalization(\n+            axis=axis, dtype=dtype, batch_input_shape=batch_input_shape,\n+            epsilon=epsilon, beta_initializer=keras.initializers.constant(beta),\n+            gamma_initializer=keras.initializers.constant(gamma))\n+        norm.build(x.shape)\n+\n+        # pylint: disable=cell-var-from-loop\n+        def forward_fn(x, beta, gamma):\n+          # We must monkey-patch the attributes of `norm` with the function\n+          # arguments, so that the gradient checker will properly compute their\n+          # gradients. The gradient checker computes gradients with respect to\n+          # the input arguments of `f`.\n+          with tf.compat.v1.test.mock.patch.object(norm, 'beta', beta):\n+            with tf.compat.v1.test.mock.patch.object(norm, 'gamma', gamma):\n+              return norm(x)\n+        # pylint: enable=cell-var-from-loop\n+        results = tf.test.compute_gradient(\n+            forward_fn, [keras.backend.cast(x, dtype), norm.beta, norm.gamma])\n+        ([x_grad_t, beta_grad_t, gamma_grad_t],\n+         [x_grad_n, beta_grad_n, gamma_grad_n]) = results\n+\n+        if dtype == 'float64':\n+          # We use the float64 numeric gradients as the reference, to compare\n+          # against the symbolic gradients for all dtypes.\n+          x_grad_ref = x_grad_n\n+          beta_grad_ref = beta_grad_n\n+          gamma_grad_ref = gamma_grad_n\n+          tol = fp64_tol\n+        elif dtype == 'float32':\n+          tol = fp32_tol\n+        else:\n+          assert dtype == 'float16'\n+          tol = fp16_tol\n+\n+        # We use absolute tolerances in addition to relative tolerances, because\n+        # some of the values are very close to zero.\n+        self.assertAllClose(x_grad_t, x_grad_ref, rtol=tol, atol=tol)\n+        self.assertAllClose(beta_grad_t, beta_grad_ref, rtol=tol, atol=tol)\n+        self.assertAllClose(gamma_grad_t, gamma_grad_ref, rtol=tol, atol=tol)\n+\n+  # The gradient_checker_v2 does not work properly with LayerNorm in graph mode.\n+  @testing_utils.run_v2_only\n+  def test_backward(self):\n+    # For numeric stability, we ensure the axis's dimension(s) have at least 4\n+    # elements.\n+    self._test_backward_pass((4, 3), (0,))\n+    self._test_backward_pass((2, 4, 2), (1,))\n+    self._test_backward_pass((2, 3, 4), (2,))\n+    self._test_backward_pass((2, 3, 2), (0, 2), fp64_tol=5e-4, fp32_tol=5e-4)\n+    self._test_backward_pass((2, 2, 2, 2), (1, 3))\n+    self._test_backward_pass((2, 2, 2, 2), (2, 3))\n+\n+\n+if __name__ == '__main__':\n+  tf.test.main()\n\n@@ -1,326 +0,0 @@\n-# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-# ==============================================================================\n-\"\"\"The V2 implementation of Normalization layers.\"\"\"\n-\n-import tensorflow.compat.v2 as tf\n-from keras import backend\n-from keras.layers import normalization\n-from tensorflow.python.util.tf_export import keras_export\n-\n-\n-# pylint: disable=g-classes-have-attributes\n-@keras_export('keras.layers.experimental.SyncBatchNormalization', v1=[])\n-class SyncBatchNormalization(normalization.BatchNormalizationBase):\n-  r\"\"\"Normalize and scale inputs or activations synchronously across replicas.\n-\n-  Applies batch normalization to activations of the previous layer at each batch\n-  by synchronizing the global batch statistics across all devices that are\n-  training the model. For specific details about batch normalization please\n-  refer to the `tf.keras.layers.BatchNormalization` layer docs.\n-\n-  If this layer is used when using tf.distribute strategy to train models\n-  across devices/workers, there will be an allreduce call to aggregate batch\n-  statistics across all replicas at every training step. Without tf.distribute\n-  strategy, this layer behaves as a regular `tf.keras.layers.BatchNormalization`\n-  layer.\n-\n-  Example usage:\n-\n-  ```python\n-  strategy = tf.distribute.MirroredStrategy()\n-\n-  with strategy.scope():\n-    model = tf.keras.Sequential()\n-    model.add(tf.keras.layers.Dense(16))\n-    model.add(tf.keras.layers.experimental.SyncBatchNormalization())\n-  ```\n-\n-  Args:\n-    axis: Integer, the axis that should be normalized\n-      (typically the features axis).\n-      For instance, after a `Conv2D` layer with\n-      `data_format=\"channels_first\"`,\n-      set `axis=1` in `BatchNormalization`.\n-    momentum: Momentum for the moving average.\n-    epsilon: Small float added to variance to avoid dividing by zero.\n-    center: If True, add offset of `beta` to normalized tensor.\n-      If False, `beta` is ignored.\n-    scale: If True, multiply by `gamma`.\n-      If False, `gamma` is not used.\n-      When the next layer is linear (also e.g. `nn.relu`),\n-      this can be disabled since the scaling\n-      will be done by the next layer.\n-    beta_initializer: Initializer for the beta weight.\n-    gamma_initializer: Initializer for the gamma weight.\n-    moving_mean_initializer: Initializer for the moving mean.\n-    moving_variance_initializer: Initializer for the moving variance.\n-    beta_regularizer: Optional regularizer for the beta weight.\n-    gamma_regularizer: Optional regularizer for the gamma weight.\n-    beta_constraint: Optional constraint for the beta weight.\n-    gamma_constraint: Optional constraint for the gamma weight.\n-\n-  Call arguments:\n-    inputs: Input tensor (of any rank).\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode or in inference mode.\n-      - `training=True`: The layer will normalize its inputs using the\n-        mean and variance of the current batch of inputs.\n-      - `training=False`: The layer will normalize its inputs using the\n-        mean and variance of its moving statistics, learned during training.\n-\n-  Input shape:\n-    Arbitrary. Use the keyword argument `input_shape`\n-    (tuple of integers, does not include the samples axis)\n-    when using this layer as the first layer in a model.\n-\n-  Output shape:\n-    Same shape as input.\n-\n-  \"\"\"\n-\n-  def __init__(self,\n-               axis=-1,\n-               momentum=0.99,\n-               epsilon=1e-3,\n-               center=True,\n-               scale=True,\n-               beta_initializer='zeros',\n-               gamma_initializer='ones',\n-               moving_mean_initializer='zeros',\n-               moving_variance_initializer='ones',\n-               beta_regularizer=None,\n-               gamma_regularizer=None,\n-               beta_constraint=None,\n-               gamma_constraint=None,\n-               **kwargs):\n-    if kwargs.pop('fused', None):\n-      raise ValueError(\n-          '`fused` argument cannot be True for SyncBatchNormalization.')\n-\n-    # Currently we only support aggregating over the global batch size.\n-    super(SyncBatchNormalization, self).__init__(\n-        axis=axis,\n-        momentum=momentum,\n-        epsilon=epsilon,\n-        center=center,\n-        scale=scale,\n-        beta_initializer=beta_initializer,\n-        gamma_initializer=gamma_initializer,\n-        moving_mean_initializer=moving_mean_initializer,\n-        moving_variance_initializer=moving_variance_initializer,\n-        beta_regularizer=beta_regularizer,\n-        gamma_regularizer=gamma_regularizer,\n-        beta_constraint=beta_constraint,\n-        gamma_constraint=gamma_constraint,\n-        fused=False,\n-        **kwargs)\n-\n-  def _calculate_mean_and_var(self, x, axes, keep_dims):\n-\n-    with backend.name_scope('moments'):\n-      # The dynamic range of fp16 is too limited to support the collection of\n-      # sufficient statistics. As a workaround we simply perform the operations\n-      # on 32-bit floats before converting the mean and variance back to fp16\n-      y = tf.cast(x, tf.float32) if x.dtype == tf.float16 else x\n-      replica_ctx = tf.distribute.get_replica_context()\n-      if replica_ctx:\n-        local_sum = tf.reduce_sum(y, axis=axes, keepdims=True)\n-        local_squared_sum = tf.reduce_sum(tf.square(y), axis=axes,\n-                                                keepdims=True)\n-        batch_size = tf.cast(tf.shape(y)[axes[0]],\n-                                   tf.float32)\n-        # TODO(b/163099951): batch the all-reduces once we sort out the ordering\n-        # issue for NCCL. We don't have a mechanism to launch NCCL in the same\n-        # order in each replica nowadays, so we limit NCCL to batch all-reduces.\n-        y_sum = replica_ctx.all_reduce(tf.distribute.ReduceOp.SUM, local_sum)\n-        y_squared_sum = replica_ctx.all_reduce(tf.distribute.ReduceOp.SUM,\n-                                               local_squared_sum)\n-        global_batch_size = replica_ctx.all_reduce(tf.distribute.ReduceOp.SUM,\n-                                                   batch_size)\n-\n-        axes_vals = [(tf.shape(y))[axes[i]]\n-                     for i in range(1, len(axes))]\n-        multiplier = tf.cast(tf.reduce_prod(axes_vals),\n-                                   tf.float32)\n-        multiplier = multiplier * global_batch_size\n-\n-        mean = y_sum / multiplier\n-        y_squared_mean = y_squared_sum / multiplier\n-        # var = E(x^2) - E(x)^2\n-        variance = y_squared_mean - tf.square(mean)\n-      else:\n-        # Compute true mean while keeping the dims for proper broadcasting.\n-        mean = tf.reduce_mean(y, axes, keepdims=True, name='mean')\n-        # sample variance, not unbiased variance\n-        # Note: stop_gradient does not change the gradient that gets\n-        #       backpropagated to the mean from the variance calculation,\n-        #       because that gradient is zero\n-        variance = tf.reduce_mean(\n-            tf.math.squared_difference(y, tf.stop_gradient(mean)),\n-            axes,\n-            keepdims=True,\n-            name='variance')\n-      if not keep_dims:\n-        mean = tf.compat.v1.squeeze(mean, axes)\n-        variance = tf.compat.v1.squeeze(variance, axes)\n-      if x.dtype == tf.float16:\n-        return (tf.cast(mean, tf.float16),\n-                tf.cast(variance, tf.float16))\n-      else:\n-        return (mean, variance)\n-\n-\n-@keras_export('keras.layers.BatchNormalization', v1=[])\n-class BatchNormalization(normalization.BatchNormalizationBase):\n-  \"\"\"Layer that normalizes its inputs.\n-\n-  Batch normalization applies a transformation that maintains the mean output\n-  close to 0 and the output standard deviation close to 1.\n-\n-  Importantly, batch normalization works differently during training and\n-  during inference.\n-\n-  **During training** (i.e. when using `fit()` or when calling the layer/model\n-  with the argument `training=True`), the layer normalizes its output using\n-  the mean and standard deviation of the current batch of inputs. That is to\n-  say, for each channel being normalized, the layer returns\n-  `gamma * (batch - mean(batch)) / sqrt(var(batch) + epsilon) + beta`, where:\n-\n-  - `epsilon` is small constant (configurable as part of the constructor\n-  arguments)\n-  - `gamma` is a learned scaling factor (initialized as 1), which\n-  can be disabled by passing `scale=False` to the constructor.\n-  - `beta` is a learned offset factor (initialized as 0), which\n-  can be disabled by passing `center=False` to the constructor.\n-\n-  **During inference** (i.e. when using `evaluate()` or `predict()` or when\n-  calling the layer/model with the argument `training=False` (which is the\n-  default), the layer normalizes its output using a moving average of the\n-  mean and standard deviation of the batches it has seen during training. That\n-  is to say, it returns\n-  `gamma * (batch - self.moving_mean) / sqrt(self.moving_var + epsilon) + beta`.\n-\n-  `self.moving_mean` and `self.moving_var` are non-trainable variables that\n-  are updated each time the layer in called in training mode, as such:\n-\n-  - `moving_mean = moving_mean * momentum + mean(batch) * (1 - momentum)`\n-  - `moving_var = moving_var * momentum + var(batch) * (1 - momentum)`\n-\n-  As such, the layer will only normalize its inputs during inference\n-  *after having been trained on data that has similar statistics as the\n-  inference data*.\n-\n-  Args:\n-    axis: Integer, the axis that should be normalized (typically the features\n-      axis). For instance, after a `Conv2D` layer with\n-      `data_format=\"channels_first\"`, set `axis=1` in `BatchNormalization`.\n-    momentum: Momentum for the moving average.\n-    epsilon: Small float added to variance to avoid dividing by zero.\n-    center: If True, add offset of `beta` to normalized tensor. If False, `beta`\n-      is ignored.\n-    scale: If True, multiply by `gamma`. If False, `gamma` is not used. When the\n-      next layer is linear (also e.g. `nn.relu`), this can be disabled since the\n-      scaling will be done by the next layer.\n-    beta_initializer: Initializer for the beta weight.\n-    gamma_initializer: Initializer for the gamma weight.\n-    moving_mean_initializer: Initializer for the moving mean.\n-    moving_variance_initializer: Initializer for the moving variance.\n-    beta_regularizer: Optional regularizer for the beta weight.\n-    gamma_regularizer: Optional regularizer for the gamma weight.\n-    beta_constraint: Optional constraint for the beta weight.\n-    gamma_constraint: Optional constraint for the gamma weight.\n-\n-  Call arguments:\n-    inputs: Input tensor (of any rank).\n-    training: Python boolean indicating whether the layer should behave in\n-      training mode or in inference mode.\n-      - `training=True`: The layer will normalize its inputs using the mean and\n-        variance of the current batch of inputs.\n-      - `training=False`: The layer will normalize its inputs using the mean and\n-        variance of its moving statistics, learned during training.\n-\n-  Input shape:\n-    Arbitrary. Use the keyword argument `input_shape` (tuple of\n-    integers, does not include the samples axis) when using this layer as the\n-    first layer in a model.\n-\n-  Output shape:\n-    Same shape as input.\n-\n-  Reference:\n-    - [Ioffe and Szegedy, 2015](https://arxiv.org/abs/1502.03167).\n-\n-  **About setting `layer.trainable = False` on a `BatchNormalization` layer:**\n-\n-  The meaning of setting `layer.trainable = False` is to freeze the layer,\n-  i.e. its internal state will not change during training:\n-  its trainable weights will not be updated\n-  during `fit()` or `train_on_batch()`, and its state updates will not be run.\n-\n-  Usually, this does not necessarily mean that the layer is run in inference\n-  mode (which is normally controlled by the `training` argument that can\n-  be passed when calling a layer). \"Frozen state\" and \"inference mode\"\n-  are two separate concepts.\n-\n-  However, in the case of the `BatchNormalization` layer, **setting\n-  `trainable = False` on the layer means that the layer will be\n-  subsequently run in inference mode** (meaning that it will use\n-  the moving mean and the moving variance to normalize the current batch,\n-  rather than using the mean and variance of the current batch).\n-\n-  This behavior has been introduced in TensorFlow 2.0, in order\n-  to enable `layer.trainable = False` to produce the most commonly\n-  expected behavior in the convnet fine-tuning use case.\n-\n-  Note that:\n-    - Setting `trainable` on an model containing other layers will\n-      recursively set the `trainable` value of all inner layers.\n-    - If the value of the `trainable`\n-      attribute is changed after calling `compile()` on a model,\n-      the new value doesn't take effect for this model\n-      until `compile()` is called again.\n-  \"\"\"\n-  _USE_V2_BEHAVIOR = True\n-\n-  def __init__(self,\n-               axis=-1,\n-               momentum=0.99,\n-               epsilon=1e-3,\n-               center=True,\n-               scale=True,\n-               beta_initializer='zeros',\n-               gamma_initializer='ones',\n-               moving_mean_initializer='zeros',\n-               moving_variance_initializer='ones',\n-               beta_regularizer=None,\n-               gamma_regularizer=None,\n-               beta_constraint=None,\n-               gamma_constraint=None,\n-               **kwargs):\n-    super(BatchNormalization, self).__init__(\n-        axis=axis,\n-        momentum=momentum,\n-        epsilon=epsilon,\n-        center=center,\n-        scale=scale,\n-        beta_initializer=beta_initializer,\n-        gamma_initializer=gamma_initializer,\n-        moving_mean_initializer=moving_mean_initializer,\n-        moving_variance_initializer=moving_variance_initializer,\n-        beta_regularizer=beta_regularizer,\n-        gamma_regularizer=gamma_regularizer,\n-        beta_constraint=beta_constraint,\n-        gamma_constraint=gamma_constraint,\n-        **kwargs)\n\n@@ -26,7 +26,7 @@ from tensorflow.python.platform import tf_logging as logging\n from tensorflow.python.util.tf_export import keras_export\n \n INT = \"int\"\n-BINARY = \"binary\"\n+MULTI_HOT = \"multi_hot\"\n COUNT = \"count\"\n \n \n@@ -46,7 +46,7 @@ class CategoryEncoding(base_layer.Layer):\n   **Multi-hot encoding data**\n \n   >>> layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(\n-  ...           num_tokens=4, output_mode=\"binary\")\n+  ...           num_tokens=4, output_mode=\"multi_hot\")\n   >>> layer([[0, 1], [0, 0], [1, 2], [3, 1]])\n   <tf.Tensor: shape=(4, 4), dtype=float32, numpy=\n     array([[1., 1., 0., 0.],\n@@ -54,7 +54,7 @@ class CategoryEncoding(base_layer.Layer):\n            [0., 1., 1., 0.],\n            [0., 1., 0., 1.]], dtype=float32)>\n \n-  **Using weighted inputs in `count` mode**\n+  **Using weighted inputs in `\"count\"` mode**\n \n   >>> layer = tf.keras.layers.experimental.preprocessing.CategoryEncoding(\n   ...           num_tokens=4, output_mode=\"count\")\n@@ -71,13 +71,13 @@ class CategoryEncoding(base_layer.Layer):\n       to the layer must integers in the range 0 <= value < num_tokens or an\n       error will be thrown.\n     output_mode: Specification for the output of the layer.\n-      Defaults to \"binary\". Values can\n-      be \"binary\" or \"count\", configuring the layer as follows:\n-        \"binary\": Outputs a single int array per batch, of num_tokens size,\n-          containing 1s in all elements where the token mapped to that index\n-          exists at least once in the batch item.\n-        \"count\": As \"binary\", but the int array contains a count of the number\n-          of times the token at that index appeared in the batch item.\n+      Defaults to `\"multi_hot\"`. Values can be `\"multi_hot\"` or `\"count\"`,\n+      configuring the layer as follows:\n+        - `\"multi_hot\"`: Outputs a single int array per batch, of num_tokens\n+          size, containing 1s in all elements where the token mapped to that\n+          index exists at least once in the batch item.\n+        - `\"count\"`: As `\"multi_hot\"`, but the int array contains a count of the\n+          number of times the token at that index appeared in the batch item.\n     sparse: Boolean. If true, returns a `SparseTensor` instead of a dense\n       `Tensor`. Defaults to `False`.\n \n@@ -85,12 +85,12 @@ class CategoryEncoding(base_layer.Layer):\n     inputs: A 2D tensor `(samples, timesteps)`.\n     count_weights: A 2D tensor in the same shape as `inputs` indicating the\n       weight for each sample value when summing up in `count` mode. Not used in\n-      `binary` mode.\n+      `\"multi_hot\"` mode.\n   \"\"\"\n \n   def __init__(self,\n                num_tokens=None,\n-               output_mode=BINARY,\n+               output_mode=MULTI_HOT,\n                sparse=False,\n                **kwargs):\n     # max_tokens is an old name for the num_tokens arg we continue to support\n@@ -105,10 +105,13 @@ class CategoryEncoding(base_layer.Layer):\n     base_preprocessing_layer.keras_kpl_gauge.get_cell(\"CategoryEncoding\").set(\n         True)\n \n-    # 'output_mode' must be one of (COUNT, BINARY)\n+    # Support deprecated names for output_modes.\n+    if output_mode == \"binary\":\n+      output_mode = MULTI_HOT\n+    # 'output_mode' must be one of (COUNT, MULTI_HOT)\n     layer_utils.validate_string_arg(\n         output_mode,\n-        allowable_strings=(COUNT, BINARY),\n+        allowable_strings=(COUNT, MULTI_HOT),\n         layer_name=\"CategoryEncoding\",\n         arg_name=\"output_mode\")\n \n@@ -150,11 +153,11 @@ class CategoryEncoding(base_layer.Layer):\n       inputs = tf.compat.v1.expand_dims(inputs, 1)\n \n     if count_weights is not None and self.output_mode != COUNT:\n-      raise ValueError(\"count_weights is not used in `output_mode='binary'`. \"\n-                       \"Please pass a single input.\")\n+      raise ValueError(\"count_weights is not used in \"\n+                       \"`output_mode='multi_hot'`. Please pass a single input.\")\n \n     out_depth = self.num_tokens\n-    binary_output = (self.output_mode == BINARY)\n+    multi_hot_output = (self.output_mode == MULTI_HOT)\n     if isinstance(inputs, tf.SparseTensor):\n       max_value = tf.reduce_max(inputs.values)\n       min_value = tf.reduce_min(inputs.values)\n@@ -171,12 +174,12 @@ class CategoryEncoding(base_layer.Layer):\n         \" with num_tokens={}\".format(out_depth)\n     ])\n     if self.sparse:\n-      return sparse_bincount(inputs, out_depth, binary_output, count_weights)\n+      return sparse_bincount(inputs, out_depth, multi_hot_output, count_weights)\n     else:\n-      return dense_bincount(inputs, out_depth, binary_output, count_weights)\n+      return dense_bincount(inputs, out_depth, multi_hot_output, count_weights)\n \n \n-def sparse_bincount(inputs, out_depth, binary_output, count_weights=None):\n+def sparse_bincount(inputs, out_depth, multi_hot_output, count_weights=None):\n   \"\"\"Apply binary or count encoding to an input and return a sparse tensor.\"\"\"\n   result = tf.sparse.bincount(\n       inputs,\n@@ -184,7 +187,7 @@ def sparse_bincount(inputs, out_depth, binary_output, count_weights=None):\n       minlength=out_depth,\n       maxlength=out_depth,\n       axis=-1,\n-      binary_output=binary_output)\n+      binary_output=multi_hot_output)\n   result = tf.cast(result, backend.floatx())\n   batch_size = tf.compat.v1.shape(result)[0]\n   result = tf.SparseTensor(\n@@ -194,7 +197,7 @@ def sparse_bincount(inputs, out_depth, binary_output, count_weights=None):\n   return result\n \n \n-def dense_bincount(inputs, out_depth, binary_output, count_weights=None):\n+def dense_bincount(inputs, out_depth, multi_hot_output, count_weights=None):\n   \"\"\"Apply binary or count encoding to an input.\"\"\"\n   result = tf.math.bincount(\n       inputs,\n@@ -203,7 +206,7 @@ def dense_bincount(inputs, out_depth, binary_output, count_weights=None):\n       maxlength=out_depth,\n       dtype=backend.floatx(),\n       axis=-1,\n-      binary_output=binary_output)\n+      binary_output=multi_hot_output)\n   batch_size = inputs.shape.as_list()[0]\n   result.set_shape(tf.TensorShape((batch_size, out_depth)))\n   return result\n\n@@ -61,7 +61,7 @@ class CategoryEncodingDistributionTest(\n     with distribution.scope():\n       input_data = keras.Input(shape=(4,), dtype=tf.int32)\n       layer = category_encoding.CategoryEncoding(\n-          num_tokens=num_tokens, output_mode=category_encoding.BINARY)\n+          num_tokens=num_tokens, output_mode=category_encoding.MULTI_HOT)\n       int_data = layer(input_data)\n       model = keras.Model(inputs=input_data, outputs=int_data)\n     output_dataset = model.predict(inp_dataset)\n\n@@ -78,7 +78,7 @@ class CategoryEncodingInputTest(keras_parameterized.TestCase,\n     input_data = keras.Input(shape=(None,), dtype=tf.int64, sparse=True)\n \n     layer = category_encoding.CategoryEncoding(\n-        num_tokens=num_tokens, output_mode=category_encoding.BINARY)\n+        num_tokens=num_tokens, output_mode=category_encoding.MULTI_HOT)\n     int_data = layer(input_data)\n     self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n \n@@ -189,7 +189,7 @@ class CategoryEncodingInputTest(keras_parameterized.TestCase,\n     input_data = keras.Input(shape=(None,), dtype=tf.int32, ragged=True)\n \n     layer = category_encoding.CategoryEncoding(\n-        num_tokens=num_tokens, output_mode=category_encoding.BINARY)\n+        num_tokens=num_tokens, output_mode=category_encoding.MULTI_HOT)\n     int_data = layer(input_data)\n \n     self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n@@ -281,7 +281,7 @@ class CategoryEncodingInputTest(keras_parameterized.TestCase,\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int32)\n     layer = category_encoding.CategoryEncoding(\n-        max_tokens=num_tokens, output_mode=category_encoding.BINARY)\n+        max_tokens=num_tokens, output_mode=category_encoding.MULTI_HOT)\n     int_data = layer(input_data)\n     self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n \n@@ -307,7 +307,7 @@ class CategoryEncodingOutputTest(keras_parameterized.TestCase,\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int32)\n     layer = category_encoding.CategoryEncoding(\n-        num_tokens=num_tokens, output_mode=category_encoding.BINARY)\n+        num_tokens=num_tokens, output_mode=category_encoding.MULTI_HOT)\n     int_data = layer(input_data)\n     self.assertAllEqual(expected_output_shape, int_data.shape.as_list())\n \n@@ -348,7 +348,7 @@ class CategoryEncodingModelBuildingTest(\n       }, {\n           \"testcase_name\": \"binary_output\",\n           \"num_tokens\": 5,\n-          \"output_mode\": category_encoding.BINARY\n+          \"output_mode\": category_encoding.MULTI_HOT\n       })\n   def test_end_to_end_bagged_modeling(self, output_mode, num_tokens):\n     input_array = np.array([[1, 2, 3, 1], [0, 3, 1, 0]])\n\n@@ -662,7 +662,6 @@ def transform(images,\n     fill_value = tf.convert_to_tensor(\n         fill_value, tf.float32, name='fill_value')\n \n-    if tf.compat.forward_compatible(2020, 8, 5):\n     return tf.raw_ops.ImageProjectiveTransformV3(\n         images=images,\n         output_shape=output_shape,\n@@ -671,13 +670,6 @@ def transform(images,\n         fill_mode=fill_mode.upper(),\n         interpolation=interpolation.upper())\n \n-    return tf.raw_ops.ImageProjectiveTransformV2(\n-        images=images,\n-        output_shape=output_shape,\n-        transforms=transforms,\n-        fill_mode=fill_mode.upper(),\n-        interpolation=interpolation.upper())\n-\n \n def get_rotation_matrix(angles, image_height, image_width, name=None):\n   \"\"\"Returns projective transform(s) for the given angle(s).\n\n@@ -31,9 +31,9 @@ from keras.utils import layer_utils\n from tensorflow.python.platform import tf_logging as logging\n \n INT = \"int\"\n-BINARY = \"binary\"\n+MULTI_HOT = \"multi_hot\"\n COUNT = \"count\"\n-TFIDF = \"tf-idf\"\n+TF_IDF = \"tf_idf\"\n \n _VOCAB_NAME = \"vocab\"\n _IDF_WEIGHTS_NAME = \"idf_weights\"\n@@ -91,38 +91,38 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n       includes the OOV and mask tokens.\n     num_oov_indices: The number of out-of-vocabulary tokens to use. If this\n       value is more than 1, OOV inputs are hashed to determine their OOV value.\n-      If this value is 0, OOV inputs will map to -1 when `output_mode` is \"int\"\n-      and are dropped otherwise.\n+      If this value is 0, OOV inputs will map to -1 when `output_mode` is\n+      `\"int\"` and are dropped otherwise.\n     mask_token: A token that represents masked inputs. When `output_mode` is\n-      \"int\", the token is included in vocabulary and mapped to index 0. In other\n-      output modes, the token will not appear in the vocabulary and instances\n-      of the mask token in the input will be dropped. If set to None, no mask\n-      term will be added.\n+      `\"int\"`, the token is included in vocabulary and mapped to index 0. In\n+      other output modes, the token will not appear in the vocabulary and\n+      instances of the mask token in the input will be dropped. If set to None,\n+      no mask term will be added.\n     oov_token: Only used when `invert` is True. The token to return for OOV\n       indices.\n     vocabulary: An optional list of vocabulary terms. If the list contains the\n       same token multiple times, an error will be thrown.\n-    invert: Only valid when `output_mode` is \"int\". If True, this layer will map\n-      indices to vocabulary items instead of mapping vocabulary items to\n+    invert: Only valid when `output_mode` is `\"int\"`. If True, this layer will\n+      map indices to vocabulary items instead of mapping vocabulary items to\n       indices. Default to False.\n-    output_mode: Specification for the output of the layer. Defaults to \"int\".\n-      Values can be \"int\", \"binary\", \"count\", or \"tf-idf\" configuring the layer\n-      as follows:\n-        \"int\": Return the raw integer indices of the input tokens.\n-        \"binary\": Outputs a single int array per sample, of either vocab_size or\n-          max_tokens size, containing 1s in all elements where the token mapped\n-          to that index exists at least once in the sample.\n-        \"count\": Like \"binary\", but the int array contains a count of the number\n-          of times the token at that index appeared in the sample.\n-        \"tf-idf\": As \"binary\", but the TF-IDF algorithm is applied to find the\n-          value in each token slot.\n-    pad_to_max_tokens: Only valid when `output_mode` is \"binary\", \"count\", or\n-      \"tf-idf\". If True, the output will have its feature axis padded to\n-      `max_tokens` even if the number of unique tokens in the vocabulary is less\n-      than max_tokens, resulting in a tensor of shape [batch_size, max_tokens]\n-      regardless of vocabulary size. Defaults to False.\n-    sparse: Boolean. Only applicable to \"binary\" and \"count\" output modes.\n-      If True, returns a `SparseTensor` instead of a dense `Tensor`.\n+    output_mode: Specification for the output of the layer. Defaults to `\"int\"`.\n+      Values can be `\"int\"`, `\"multi_hot\"`, `\"count\"`, or `\"tf_idf\"` configuring\n+      the layer as follows:\n+        - `\"int\"`: Return the raw integer indices of the input tokens.\n+        - `\"multi_hot\"`: Outputs a single int array per sample, of either\n+          vocab_size or max_tokens size, containing 1s in all elements where the\n+          token mapped to that index exists at least once in the sample.\n+        - `\"count\"`: As `\"multi_hot\"`, but the int array contains a count of the\n+          number of times the token at that index appeared in the sample.\n+        - `\"tf_idf\"`: As `\"multi_hot\"`, but the TF-IDF algorithm is applied to\n+          find the value in each token slot.\n+    pad_to_max_tokens: Only valid when `output_mode` is `\"multi_hot\"`,\n+      `\"count\"`, or `\"tf_idf\"`. If True, the output will have its feature axis\n+      padded to `max_tokens` even if the number of unique tokens in the\n+      vocabulary is less than max_tokens, resulting in a tensor of shape\n+      [batch_size, max_tokens] regardless of vocabulary size. Defaults to False.\n+    sparse: Boolean. Only applicable to `\"multi_hot\"` and `\"count\"` output\n+      modes. If True, returns a `SparseTensor` instead of a dense `Tensor`.\n       Defaults to False.\n   \"\"\"\n \n@@ -147,10 +147,15 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n       raise ValueError(\"`num_oov_indices` must be greater than or equal to 0. \"\n                        \"You passed {}\".format(num_oov_indices))\n \n-    # 'output_mode' must be one of (INT, BINARY, COUNT, TFIDF)\n+    # Support deprecated names for output_modes.\n+    if output_mode == \"binary\":\n+      output_mode = MULTI_HOT\n+    if output_mode == \"tf-idf\":\n+      output_mode = TF_IDF\n+    # 'output_mode' must be one of (INT, MULTI_HOT, COUNT, TF_IDF)\n     layer_utils.validate_string_arg(\n         output_mode,\n-        allowable_strings=(INT, BINARY, COUNT, TFIDF),\n+        allowable_strings=(INT, MULTI_HOT, COUNT, TF_IDF),\n         layer_name=self.__class__.__name__,\n         arg_name=\"output_mode\")\n \n@@ -203,7 +208,7 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n             vocab_size=available_vocab_size,\n             mask_value=mask_token,\n             oov_value=oov_token,\n-            compute_idf=(output_mode == TFIDF)),\n+            compute_idf=(output_mode == TF_IDF)),\n         **kwargs)\n \n     # We need to save the key dtype so that we know if we're expecting int64\n@@ -296,7 +301,7 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n         self.set_vocabulary(vocabulary)\n       tracked_table = self._add_trackable(self._table, trainable=False)\n \n-    if self.output_mode == TFIDF:\n+    if self.output_mode == TF_IDF:\n       # The TF-IDF weight may have a (None,) tensorshape. This creates\n       # a 1D variable with arbitrary shape, which we can assign any weight to\n       # so long as it has 1 dimension. In order to properly initialize this\n@@ -419,7 +424,7 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     \"\"\"Sets vocabulary (and optionally document frequency) data for this layer.\n \n     This method sets the vocabulary and idf weights for this layer directly,\n-    instead of analyzing a dataset through 'adapt'. It should be used whenever\n+    instead of analyzing a dataset through `adapt`. It should be used whenever\n     the vocab (and optionally document frequency) information is already known.\n     If vocabulary data is already present in the layer, this method will replace\n     it.\n@@ -427,14 +432,14 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     Args:\n       vocabulary: An array of hashable tokens.\n       idf_weights: An array of inverse document frequency weights with equal\n-        length to vocab. Only necessary if the layer output_mode is TFIDF.\n+        length to vocab. Only necessary if the layer output_mode is TF_IDF.\n \n     Raises:\n       ValueError: If there are too many inputs, the inputs do not match, or\n         input data is missing.\n       RuntimeError: If the vocabulary cannot be set when this function is\n-        called. This happens when \"binary\", \"count\", and \"tfidf\" modes,\n-        if \"pad_to_max_tokens\" is False and the layer itself has already been\n+        called. This happens when `\"multi_hot\"`, `\"count\"`, and `\"tfidf\"` modes,\n+        if `pad_to_max_tokens` is False and the layer itself has already been\n         called.\n     \"\"\"\n     if self._has_static_table:\n@@ -444,11 +449,11 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n                          \"do not support changing the vocabulary after \"\n                          \"creation.\".format(self.name))\n \n-    if self.output_mode != TFIDF and idf_weights is not None:\n+    if self.output_mode != TF_IDF and idf_weights is not None:\n       raise ValueError(\"`idf_weights` should only be set if output_mode is \"\n-                       \"TFIDF. output_mode is {}.\".format(self.output_mode))\n+                       \"TF_IDF. output_mode is {}.\".format(self.output_mode))\n \n-    if (self.output_mode in [BINARY, COUNT, TFIDF] and self._called and\n+    if (self.output_mode in [MULTI_HOT, COUNT, TF_IDF] and self._called and\n         not self.pad_to_max_tokens):\n       raise RuntimeError(\"When using {} mode and `pad_to_max_tokens` is \"\n                          \"False, the vocabulary cannot be changed after the \"\n@@ -530,9 +535,9 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n           \"Passed vocab size is {}, max vocab size is {}.\".format(\n               self._vocab_size, self.max_tokens))\n \n-    if self.output_mode == TFIDF:\n+    if self.output_mode == TF_IDF:\n       if idf_weights is None:\n-        raise ValueError(\"`idf_weights` must be set if output_mode is TFIDF\")\n+        raise ValueError(\"`idf_weights` must be set if output_mode is TF_IDF\")\n       if len(vocabulary) != len(idf_weights):\n         raise ValueError(\"`idf_weights` must be the same length as vocabulary. \"\n                          \"len(idf_weights) is {}, len(vocabulary) is {}\".format(\n@@ -555,7 +560,7 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     if self.mask_token is not None:\n       self._table_handler.insert([self._mask_key], [self._mask_value])\n \n-    if self.output_mode == TFIDF:\n+    if self.output_mode == TF_IDF:\n       # If the passed vocabulary has no special tokens, we need to pad the front\n       # of idf_weights. We don't have real document frequencies for these tokens\n       # so we will use an average of all idf_weights passed in as a reasonable\n@@ -598,19 +603,19 @@ class IndexLookup(base_preprocessing_layer.CombinerPreprocessingLayer):\n     if self.output_mode == INT:\n       return lookup_result\n \n-    binary_output = (self.output_mode == BINARY)\n+    multi_hot_output = (self.output_mode == MULTI_HOT)\n     if self._vocab_size and not self.pad_to_max_tokens:\n       out_depth = self._vocab_size\n     else:\n       out_depth = self.max_tokens\n     if self.sparse:\n       bincounts = category_encoding.sparse_bincount(lookup_result, out_depth,\n-                                                    binary_output)\n+                                                    multi_hot_output)\n     else:\n       bincounts = category_encoding.dense_bincount(lookup_result, out_depth,\n-                                                   binary_output)\n+                                                   multi_hot_output)\n \n-    if self.output_mode == TFIDF:\n+    if self.output_mode == TF_IDF:\n       return tf.multiply(bincounts, self.tf_idf_weights)\n \n     return bincounts\n@@ -807,7 +812,7 @@ class _IndexLookupCombiner(base_preprocessing_layer.Combiner):\n     return _IndexLookupAccumulator(data, count_dict, per_doc_count_dict)\n \n   def _inverse_document_frequency(self, document_counts, num_documents):\n-    \"\"\"Computes the inverse-document-frequency (IDF) component of TFIDF.\n+    \"\"\"Computes the inverse-document-frequency (IDF) component of TF-IDF.\n \n     Uses the default weighting scheme described in\n     https://en.wikipedia.org/wiki/Tf%E2%80%93idf.\n\n@@ -243,7 +243,7 @@ def _get_end_to_end_test_cases():\n               \"num_oov_indices\": 1,\n               \"mask_token\": 0,\n               \"oov_token\": -1,\n-              \"output_mode\": index_lookup.TFIDF,\n+              \"output_mode\": index_lookup.TF_IDF,\n               \"dtype\": tf.int64,\n           },\n           \"expected_output\": [[0, 1.098612, 0, 0, 0], [0, 0, 1.252763, 0, 0],\n@@ -267,7 +267,7 @@ def _get_end_to_end_test_cases():\n               \"num_oov_indices\": 1,\n               \"mask_token\": \"\",\n               \"oov_token\": \"[OOV]\",\n-              \"output_mode\": index_lookup.TFIDF,\n+              \"output_mode\": index_lookup.TF_IDF,\n               \"dtype\": tf.string,\n           },\n           \"expected_output\": [[0, 1.098612, 0, 0, 0], [0, 0, 1.252763, 0, 0],\n@@ -791,7 +791,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        output_mode=index_lookup.BINARY,\n+        output_mode=index_lookup.MULTI_HOT,\n         pad_to_max_tokens=True,\n         dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n@@ -816,7 +816,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         num_oov_indices=0,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        output_mode=index_lookup.BINARY,\n+        output_mode=index_lookup.MULTI_HOT,\n         pad_to_max_tokens=True,\n         dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n@@ -848,7 +848,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        output_mode=index_lookup.BINARY,\n+        output_mode=index_lookup.MULTI_HOT,\n         pad_to_max_tokens=True,\n         dtype=tf.string)\n     int_data = layer(input_data)\n@@ -879,7 +879,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        output_mode=index_lookup.BINARY,\n+        output_mode=index_lookup.MULTI_HOT,\n         dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     binary_data = layer(input_data)\n@@ -894,7 +894,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        output_mode=index_lookup.BINARY,\n+        output_mode=index_lookup.MULTI_HOT,\n         dtype=tf.string)\n     binary_data = layer(input_data)\n     self.assertAllEqual(binary_data.shape.as_list(), [16, 2])\n@@ -978,7 +978,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        output_mode=index_lookup.TFIDF,\n+        output_mode=index_lookup.TF_IDF,\n         pad_to_max_tokens=True,\n         dtype=tf.string)\n     layer.set_vocabulary(vocab_data, idf_weights=idf_weights)\n@@ -1005,7 +1005,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        output_mode=index_lookup.TFIDF,\n+        output_mode=index_lookup.TF_IDF,\n         dtype=tf.string)\n     layer.set_vocabulary(vocab_data, idf_weights=idf_weights)\n     layer_output = layer(input_data)\n@@ -1067,7 +1067,7 @@ class IndexLookupOutputTest(keras_parameterized.TestCase,\n           num_oov_indices=1,\n           mask_token=\"\",\n           oov_token=\"[OOV]\",\n-          output_mode=index_lookup.BINARY,\n+          output_mode=index_lookup.MULTI_HOT,\n           dtype=tf.string)\n       return layer(data)\n \n@@ -1465,7 +1465,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n         pad_to_max_tokens=False,\n-        output_mode=index_lookup.BINARY,\n+        output_mode=index_lookup.MULTI_HOT,\n         dtype=tf.string)\n     layer.set_vocabulary(vocab_data)\n     # Calling the layer should lock the vocabulary.\n@@ -1481,7 +1481,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        output_mode=index_lookup.BINARY,\n+        output_mode=index_lookup.MULTI_HOT,\n         dtype=tf.string)\n     with self.assertRaisesRegex(ValueError,\n                                 \"`idf_weights` should only be set if\"):\n@@ -1495,7 +1495,7 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        output_mode=index_lookup.TFIDF,\n+        output_mode=index_lookup.TF_IDF,\n         dtype=tf.string)\n     with self.assertRaisesRegex(\n         ValueError, \"`idf_weights` must be the same length as vocab\"):\n@@ -1508,10 +1508,10 @@ class IndexLookupVocabularyTest(keras_parameterized.TestCase,\n         num_oov_indices=1,\n         mask_token=\"\",\n         oov_token=\"[OOV]\",\n-        output_mode=index_lookup.TFIDF,\n+        output_mode=index_lookup.TF_IDF,\n         dtype=tf.string)\n     with self.assertRaisesRegex(\n-        ValueError, \"`idf_weights` must be set if output_mode is TFIDF\"):\n+        ValueError, \"`idf_weights` must be set if output_mode is TF_IDF\"):\n       layer.set_vocabulary(vocab_data)\n \n   def test_non_unique_int_vocab_fails(self):\n\n@@ -40,9 +40,9 @@ class IntegerLookup(index_lookup.IndexLookup):\n   create the vocabulary and all others will be treated as OOV.\n \n   There are two possible output modes for the layer.\n-  When `output_mode` is \"int\",\n+  When `output_mode` is `\"int\"`,\n   input integers are converted to their index in the vocabulary (an integer).\n-  When `output_mode` is \"binary\", \"count\", or \"tf-idf\", input integers\n+  When `output_mode` is `\"multi_hot\"`, `\"count\"`, or `\"tf_idf\"`, input integers\n   are encoded into an array where each dimension corresponds to an element in\n   the vocabulary.\n \n@@ -50,10 +50,10 @@ class IntegerLookup(index_lookup.IndexLookup):\n   (which can optionally occupy multiple indices in the vocabulary, as set\n   by `num_oov_indices`).\n   The position of these tokens in the vocabulary is fixed. When `output_mode` is\n-  \"int\", the vocabulary will begin with the mask token at index 0, followed by\n+  `\"int\"`, the vocabulary will begin with the mask token at index 0, followed by\n   OOV indices, followed by the rest of the vocabulary. When `output_mode` is\n-  \"binary\", \"count\", or \"tf-idf\" the vocabulary will begin with OOV indices and\n-  instances of the mask token will be dropped.\n+  `\"multi_hot\"`, `\"count\"`, or `\"tf_idf\"` the vocabulary will begin with OOV\n+  indices and instances of the mask token will be dropped.\n \n   Args:\n     max_tokens: The maximum size of the vocabulary for this layer. If None,\n@@ -62,10 +62,10 @@ class IntegerLookup(index_lookup.IndexLookup):\n     num_oov_indices: The number of out-of-vocabulary tokens to use. If this\n       value is more than 1, OOV inputs are modulated to determine their OOV\n       value. If this value is 0, OOV inputs will map to -1 when `output_mode` is\n-      \"int\" and are dropped otherwise. Defaults to 1.\n+      `\"int\"` and are dropped otherwise. Defaults to 1.\n     mask_token: An integer token that represents masked inputs. When\n-      `output_mode` is \"int\", the token is included in vocabulary and mapped to\n-      index 0. In other output modes, the token will not appear in the\n+      `output_mode` is `\"int\"`, the token is included in vocabulary and mapped\n+      to index 0. In other output modes, the token will not appear in the\n       vocabulary and instances of the mask token in the input will be dropped.\n       If set to None, no mask term will be added. Defaults to 0.\n     oov_token: Only used when `invert` is True. The token to return for OOV\n@@ -74,28 +74,29 @@ class IntegerLookup(index_lookup.IndexLookup):\n       containing a vocabulary to load into this layer. The file should contain\n       one integer token per line. If the list or file contains the same token\n       multiple times, an error will be thrown.\n-    invert: Only valid when `output_mode` is \"int\". If True, this layer will map\n-      indices to vocabulary items instead of mapping vocabulary items to\n+    invert: Only valid when `output_mode` is `\"int\"`. If True, this layer will\n+      map indices to vocabulary items instead of mapping vocabulary items to\n       indices. Default to False.\n-    output_mode: Specification for the output of the layer. Defaults to \"int\".\n-      Values can be \"int\", \"binary\", \"count\", or \"tf-idf\" configuring the layer\n-      as follows:\n-        \"int\": Return the vocabulary indices of the input tokens.\n-        \"binary\": Outputs a single int array per sample, of either vocabulary\n-          size or `max_tokens` size, containing 1s in all elements where the\n-          token mapped to that index exists at least once in the sample.\n-        \"count\": Like \"binary\", but the int array contains a count of the number\n-          of times the token at that index appeared in the sample.\n-        \"tf-idf\": As \"binary\", but the TF-IDF algorithm is applied to find the\n-          value in each token slot.\n-    pad_to_max_tokens: Only applicable when `output_mode` is \"binary\", \"count\",\n-      or \"tf-idf\". If True, the output will have its feature axis padded to\n-      `max_tokens` even if the number of unique tokens in the vocabulary is less\n-      than max_tokens, resulting in a tensor of shape [batch_size, max_tokens]\n-      regardless of vocabulary size. Defaults to False.\n-    sparse: Boolean. Only applicable when `output_mode` is \"binary\", \"count\",\n-      or \"tf-idf\". If True, returns a `SparseTensor` instead of a dense\n-      `Tensor`. Defaults to False.\n+    output_mode: Specification for the output of the layer. Defaults to `\"int\"`.\n+      Values can be `\"int\"`, `\"multi_hot\"`, `\"count\"`, or `\"tf_idf\"` configuring\n+      the layer as follows:\n+        - `\"int\"`: Return the vocabulary indices of the input tokens.\n+        - `\"multi_hot\"`: Outputs a single int array per sample, of either\n+          vocabulary size or `max_tokens` size, containing 1s in all elements\n+          where the token mapped to that index exists at least once in the\n+          sample.\n+        - `\"count\"`: As `\"multi_hot\"`, but the int array contains a count of the\n+          number of times the token at that index appeared in the sample.\n+        - `\"tf_idf\"`: As `\"multi_hot\"`, but the TF-IDF algorithm is applied to\n+          find the value in each token slot.\n+    pad_to_max_tokens: Only applicable when `output_mode` is `\"multi_hot\"`,\n+      `\"count\"`, or `\"tf_idf\"`. If True, the output will have its feature axis\n+      padded to `max_tokens` even if the number of unique tokens in the\n+      vocabulary is less than max_tokens, resulting in a tensor of shape\n+      [batch_size, max_tokens] regardless of vocabulary size. Defaults to False.\n+    sparse: Boolean. Only applicable when `output_mode` is `\"multi_hot\"`,\n+      `\"count\"`, or `\"tf_idf\"`. If True, returns a `SparseTensor` instead of a\n+      dense `Tensor`. Defaults to False.\n \n   Examples:\n \n@@ -157,12 +158,12 @@ class IntegerLookup(index_lookup.IndexLookup):\n \n   **Multi-hot output**\n \n-  Configure the layer with `output_mode='binary'`. Note that the first\n-  `num_oov_indices` dimensions in the binary encoding represent OOV tokens\n+  Configure the layer with `output_mode='multi_hot'`. Note that the first\n+  `num_oov_indices` dimensions in the multi_hot encoding represent OOV tokens\n \n   >>> vocab = [12, 36, 1138, 42]\n   >>> data = tf.constant([[12, 1138, 42, 42], [42, 7, 36, 7]]) # Note OOV tokens\n-  >>> layer = IntegerLookup(vocabulary=vocab, output_mode='binary')\n+  >>> layer = IntegerLookup(vocabulary=vocab, output_mode='multi_hot')\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n     array([[0., 1., 0., 1., 1.],\n@@ -170,7 +171,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n \n   **Token count output**\n \n-  Configure the layer with `output_mode='count'`. As with binary output, the\n+  Configure the layer with `output_mode='count'`. As with multi_hot output, the\n   first `num_oov_indices` dimensions in the output represent OOV tokens.\n \n   >>> vocab = [12, 36, 1138, 42]\n@@ -183,7 +184,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n \n   **TF-IDF output**\n \n-  Configure the layer with `output_mode='tf-idf'`. As with binary output, the\n+  Configure the layer with `output_mode='tf_idf'`. As with multi_hot output, the\n   first `num_oov_indices` dimensions in the output represent OOV tokens.\n \n   Each token bin will output `token_count * idf_weight`, where the idf weights\n@@ -194,7 +195,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n   >>> vocab = [12, 36, 1138, 42]\n   >>> idf_weights = [0.25, 0.75, 0.6, 0.4]\n   >>> data = tf.constant([[12, 1138, 42, 42], [42, 7, 36, 7]]) # Note OOV tokens\n-  >>> layer = IntegerLookup(output_mode='tf-idf')\n+  >>> layer = IntegerLookup(output_mode='tf_idf')\n   >>> layer.set_vocabulary(vocab, idf_weights=idf_weights)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n@@ -207,14 +208,14 @@ class IntegerLookup(index_lookup.IndexLookup):\n   >>> vocab = [-1, 12, 36, 1138, 42]\n   >>> idf_weights = [0.9, 0.25, 0.75, 0.6, 0.4]\n   >>> data = tf.constant([[12, 1138, 42, 42], [42, 7, 36, 7]]) # Note OOV tokens\n-  >>> layer = IntegerLookup(output_mode='tf-idf')\n+  >>> layer = IntegerLookup(output_mode='tf_idf')\n   >>> layer.set_vocabulary(vocab, idf_weights=idf_weights)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n     array([[0.  , 0.25, 0.  , 0.6 , 0.8 ],\n            [1.8 , 0.  , 0.75, 0.  , 0.4 ]], dtype=float32)>\n \n-  When adapting the layer in tf-idf mode, each input sample will be considered a\n+  When adapting the layer in tf_idf mode, each input sample will be considered a\n   document, and idf weight per token will be calculated as\n   `log(1 + num_documents / (1 + token_document_count))`.\n \n@@ -319,7 +320,7 @@ class IntegerLookup(index_lookup.IndexLookup):\n \n   def set_vocabulary(self, vocabulary, idf_weights=None):\n     if isinstance(vocabulary, str):\n-      if self.output_mode == index_lookup.TFIDF:\n+      if self.output_mode == index_lookup.TF_IDF:\n         raise RuntimeError(\n             \"Setting vocabulary directly from a file is not \"\n             \"supported in TF-IDF mode, since this layer cannot \"\n\n@@ -391,7 +391,7 @@ class IntegerLookupVocabularyTest(\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n     layer = integer_lookup.IntegerLookup(\n-        vocabulary=vocab_data, output_mode=\"binary\")\n+        vocabulary=vocab_data, output_mode=\"multi_hot\")\n     res = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=res)\n     output_data = model.predict(input_array)\n@@ -415,7 +415,7 @@ class IntegerLookupVocabularyTest(\n \n     input_data = keras.Input(shape=(None,), dtype=tf.int64)\n     layer = integer_lookup.IntegerLookup(\n-        vocabulary=vocab_data, output_mode=\"binary\", sparse=True)\n+        vocabulary=vocab_data, output_mode=\"multi_hot\", sparse=True)\n     res = layer(input_data)\n     self.assertTrue(res.__class__.__name__, \"SparseKerasTensor\")\n \n\n@@ -36,9 +36,9 @@ class StringLookup(index_lookup.IndexLookup):\n   (OOV).\n \n   There are two possible output modes for the layer.\n-  When `output_mode` is \"int\",\n+  When `output_mode` is `\"int\"`,\n   input strings are converted to their index in the vocabulary (an integer).\n-  When `output_mode` is \"binary\", \"count\", or \"tf-idf\", input strings\n+  When `output_mode` is `\"multi_hot\"`, `\"count\"`, or `\"tf_idf\"`, input strings\n   are encoded into an array where each dimension corresponds to an element in\n   the vocabulary.\n \n@@ -46,10 +46,10 @@ class StringLookup(index_lookup.IndexLookup):\n   (which can optionally occupy multiple indices in the vocabulary, as set\n   by `num_oov_indices`).\n   The position of these tokens in the vocabulary is fixed. When `output_mode` is\n-  \"int\", the vocabulary will begin with the mask token at index 0, followed by\n+  `\"int\"`, the vocabulary will begin with the mask token at index 0, followed by\n   OOV indices, followed by the rest of the vocabulary. When `output_mode` is\n-  \"binary\", \"count\", or \"tf-idf\" the vocabulary will begin with OOV indices and\n-  instances of the mask token will be dropped.\n+  `\"multi_hot\"`, `\"count\"`, or `\"tf_idf\"` the vocabulary will begin with OOV\n+  indices and instances of the mask token will be dropped.\n \n   Args:\n     max_tokens: The maximum size of the vocabulary for this layer. If None,\n@@ -57,41 +57,41 @@ class StringLookup(index_lookup.IndexLookup):\n       includes the OOV and mask tokens. Default to None.\n     num_oov_indices: The number of out-of-vocabulary tokens to use. If this\n       value is more than 1, OOV inputs are hashed to determine their OOV value.\n-      If this value is 0, OOV inputs will map to -1 when `output_mode` is \"int\"\n-      and are dropped otherwise. Defaults to 1.\n+      If this value is 0, OOV inputs will map to -1 when `output_mode` is\n+      `\"int\"` and are dropped otherwise. Defaults to 1.\n     mask_token: A token that represents masked inputs. When `output_mode` is\n-      \"int\", the token is included in vocabulary and mapped to index 0. In other\n-      output modes, the token will not appear in the vocabulary and instances\n-      of the mask token in the input will be dropped. If set to None, no mask\n-      term will be added. Defaults to `\"\"`.\n+      `\"int\"`, the token is included in vocabulary and mapped to index 0. In\n+      other output modes, the token will not appear in the vocabulary and\n+      instances of the mask token in the input will be dropped. If set to None,\n+      no mask term will be added. Defaults to `\"\"`.\n     oov_token: Only used when `invert` is True. The token to return for OOV\n       indices. Defaults to `\"[UNK]\"`.\n     vocabulary: An optional list of tokens, or a path to a text file containing\n       a vocabulary to load into this layer. The file should contain one token\n       per line. If the list or file contains the same token multiple times, an\n       error will be thrown.\n-    invert: Only valid when `output_mode` is \"int\". If True, this layer will map\n-      indices to vocabulary items instead of mapping vocabulary items to\n+    invert: Only valid when `output_mode` is `\"int\"`. If True, this layer will\n+      map indices to vocabulary items instead of mapping vocabulary items to\n       indices. Default to False.\n-    output_mode: Specification for the output of the layer. Defaults to \"int\".\n-      Values can be \"int\", \"binary\", \"count\", or \"tf-idf\" configuring the layer\n-      as follows:\n-        \"int\": Return the raw integer indices of the input tokens.\n-        \"binary\": Outputs a single int array per sample, of either vocab_size or\n-          max_tokens size, containing 1s in all elements where the token mapped\n-          to that index exists at least once in the sample.\n-        \"count\": Like \"binary\", but the int array contains a count of the number\n-          of times the token at that index appeared in the sample.\n-        \"tf-idf\": As \"binary\", but the TF-IDF algorithm is applied to find the\n-          value in each token slot.\n-    pad_to_max_tokens: Only applicable when `output_mode` is \"binary\", \"count\",\n-      or \"tf-idf\". If True, the output will have its feature axis padded to\n-      `max_tokens` even if the number of unique tokens in the vocabulary is less\n-      than max_tokens, resulting in a tensor of shape [batch_size, max_tokens]\n-      regardless of vocabulary size. Defaults to False.\n-    sparse: Boolean. Only applicable when `output_mode` is \"binary\", \"count\",\n-      or \"tf-idf\". If True, returns a `SparseTensor` instead of a dense\n-      `Tensor`. Defaults to False.\n+    output_mode: Specification for the output of the layer. Defaults to `\"int\"`.\n+      Values can be `\"int\"`, `\"multi_hot\"`, `\"count\"`, or `\"tf_idf\"` configuring\n+      the layer as follows:\n+        - `\"int\"`: Return the raw integer indices of the input tokens.\n+        - `\"multi_hot\"`: Outputs a single int array per sample, of either\n+          vocab_size or max_tokens size, containing 1s in all elements where the\n+          token mapped to that index exists at least once in the sample.\n+        - `\"count\"`: As `\"multi_hot\"`, but the int array contains a count of the\n+          number of times the token at that index appeared in the sample.\n+        - `\"tf_idf\"`: As `\"multi_hot\"`, but the TF-IDF algorithm is applied to\n+          find the value in each token slot.\n+    pad_to_max_tokens: Only applicable when `output_mode` is `\"multi_hot\"`,\n+      `\"count\"`, or `\"tf_idf\"`. If True, the output will have its feature axis\n+      padded to `max_tokens` even if the number of unique tokens in the\n+      vocabulary is less than max_tokens, resulting in a tensor of shape\n+      [batch_size, max_tokens] regardless of vocabulary size. Defaults to False.\n+    sparse: Boolean. Only applicable when `output_mode` is `\"multi_hot\"`,\n+      `\"count\"`, or `\"tf_idf\"`. If True, returns a `SparseTensor` instead of a\n+      dense `Tensor`. Defaults to False.\n \n   Examples:\n \n@@ -152,12 +152,12 @@ class StringLookup(index_lookup.IndexLookup):\n \n   **Multi-hot output**\n \n-  Configure the layer with `output_mode='binary'`. Note that the first\n-  `num_oov_indices` dimensions in the binary encoding represent OOV values.\n+  Configure the layer with `output_mode='multi_hot'`. Note that the first\n+  `num_oov_indices` dimensions in the multi_hot encoding represent OOV values.\n \n   >>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n   >>> data = tf.constant([[\"a\", \"c\", \"d\", \"d\"], [\"d\", \"z\", \"b\", \"z\"]])\n-  >>> layer = StringLookup(vocabulary=vocab, output_mode='binary')\n+  >>> layer = StringLookup(vocabulary=vocab, output_mode='multi_hot')\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n     array([[0., 1., 0., 1., 1.],\n@@ -165,7 +165,7 @@ class StringLookup(index_lookup.IndexLookup):\n \n   **Token count output**\n \n-  Configure the layer with `output_mode='count'`. As with binary output, the\n+  Configure the layer with `output_mode='count'`. As with multi_hot output, the\n   first `num_oov_indices` dimensions in the output represent OOV values.\n \n   >>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n@@ -178,7 +178,7 @@ class StringLookup(index_lookup.IndexLookup):\n \n   **TF-IDF output**\n \n-  Configure the layer with `output_mode='tf-idf'`. As with binary output, the\n+  Configure the layer with `output_mode='tf_idf'`. As with multi_hot output, the\n   first `num_oov_indices` dimensions in the output represent OOV values.\n \n   Each token bin will output `token_count * idf_weight`, where the idf weights\n@@ -189,7 +189,7 @@ class StringLookup(index_lookup.IndexLookup):\n   >>> vocab = [\"a\", \"b\", \"c\", \"d\"]\n   >>> idf_weights = [0.25, 0.75, 0.6, 0.4]\n   >>> data = tf.constant([[\"a\", \"c\", \"d\", \"d\"], [\"d\", \"z\", \"b\", \"z\"]])\n-  >>> layer = StringLookup(output_mode='tf-idf')\n+  >>> layer = StringLookup(output_mode='tf_idf')\n   >>> layer.set_vocabulary(vocab, idf_weights=idf_weights)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n@@ -202,14 +202,14 @@ class StringLookup(index_lookup.IndexLookup):\n   >>> vocab = [\"[UNK]\", \"a\", \"b\", \"c\", \"d\"]\n   >>> idf_weights = [0.9, 0.25, 0.75, 0.6, 0.4]\n   >>> data = tf.constant([[\"a\", \"c\", \"d\", \"d\"], [\"d\", \"z\", \"b\", \"z\"]])\n-  >>> layer = StringLookup(output_mode='tf-idf')\n+  >>> layer = StringLookup(output_mode='tf_idf')\n   >>> layer.set_vocabulary(vocab, idf_weights=idf_weights)\n   >>> layer(data)\n   <tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n     array([[0.  , 0.25, 0.  , 0.6 , 0.8 ],\n            [1.8 , 0.  , 0.75, 0.  , 0.4 ]], dtype=float32)>\n \n-  When adapting the layer in tf-idf mode, each input sample will be considered a\n+  When adapting the layer in tf_idf mode, each input sample will be considered a\n   document, and idf weight per token will be calculated as\n   `log(1 + num_documents / (1 + token_document_count))`.\n \n@@ -304,7 +304,7 @@ class StringLookup(index_lookup.IndexLookup):\n \n   def set_vocabulary(self, vocabulary, idf_weights=None):\n     if isinstance(vocabulary, str):\n-      if self.output_mode == index_lookup.TFIDF:\n+      if self.output_mode == index_lookup.TF_IDF:\n         raise RuntimeError(\"Setting vocabulary directly from a file is not \"\n                            \"supported in TF-IDF mode, since this layer cannot \"\n                            \"read files containing TF-IDF weight data. Please \"\n\n@@ -160,7 +160,7 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n     layer = string_lookup.StringLookup(\n-        vocabulary=vocab_data, output_mode=\"binary\")\n+        vocabulary=vocab_data, output_mode=\"multi_hot\")\n     res = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=res)\n     output_data = model.predict(input_array)\n@@ -185,7 +185,7 @@ class StringLookupVocabularyTest(keras_parameterized.TestCase,\n \n     input_data = keras.Input(shape=(None,), dtype=tf.string)\n     layer = string_lookup.StringLookup(\n-        vocabulary=vocab_data, output_mode=\"binary\", sparse=True)\n+        vocabulary=vocab_data, output_mode=\"multi_hot\", sparse=True)\n     res = layer(input_data)\n     self.assertTrue(res.__class__.__name__, \"SparseKerasTensor\")\n \n\n@@ -30,9 +30,9 @@ LOWER_AND_STRIP_PUNCTUATION = \"lower_and_strip_punctuation\"\n \n SPLIT_ON_WHITESPACE = \"whitespace\"\n \n-TFIDF = index_lookup.TFIDF\n+TF_IDF = index_lookup.TF_IDF\n INT = index_lookup.INT\n-BINARY = index_lookup.BINARY\n+MULTI_HOT = index_lookup.MULTI_HOT\n COUNT = index_lookup.COUNT\n \n # This is an explicit regex of all the tokens that will be stripped if\n@@ -108,41 +108,42 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n     max_tokens: The maximum size of the vocabulary for this layer. If None,\n       there is no cap on the size of the vocabulary. Note that this vocabulary\n       contains 1 OOV token, so the effective number of tokens is `(max_tokens -\n-      1 - (1 if output == \"int\" else 0))`.\n+      1 - (1 if output == `\"int\"` else 0))`.\n     standardize: Optional specification for standardization to apply to the\n       input text. Values can be None (no standardization),\n-      'lower_and_strip_punctuation' (lowercase and remove punctuation) or a\n-      Callable. Default is 'lower_and_strip_punctuation'.\n+      `\"lower_and_strip_punctuation\"` (lowercase and remove punctuation) or a\n+      Callable. Default is `\"lower_and_strip_punctuation\"`.\n     split: Optional specification for splitting the input text. Values can be\n-      None (no splitting), 'whitespace' (split on ASCII whitespace), or a\n-      Callable. The default is 'whitespace'.\n+      None (no splitting), `\"whitespace\"` (split on ASCII whitespace), or a\n+      Callable. The default is `\"whitespace\"`.\n     ngrams: Optional specification for ngrams to create from the possibly-split\n       input text. Values can be None, an integer or tuple of integers; passing\n       an integer will create ngrams up to that integer, and passing a tuple of\n       integers will create ngrams for the specified values in the tuple. Passing\n       None means that no ngrams will be created.\n     output_mode: Optional specification for the output of the layer. Values can\n-      be \"int\", \"binary\", \"count\" or \"tf-idf\", configuring the layer as follows:\n-        \"int\": Outputs integer indices, one integer index per split string\n-          token. When output == \"int\", 0 is reserved for masked locations;\n+      be `\"int\"`, `\"multi_hot\"`, `\"count\"` or `\"tf_idf\"`, configuring the layer\n+      as follows:\n+        - `\"int\"`: Outputs integer indices, one integer index per split string\n+          token. When output == `\"int\"`, 0 is reserved for masked locations;\n           this reduces the vocab size to max_tokens-2 instead of max_tokens-1\n-        \"binary\": Outputs a single int array per batch, of either vocab_size or\n-          max_tokens size, containing 1s in all elements where the token mapped\n-          to that index exists at least once in the batch item.\n-        \"count\": As \"binary\", but the int array contains a count of the number\n-          of times the token at that index appeared in the batch item.\n-        \"tf-idf\": As \"binary\", but the TF-IDF algorithm is applied to find the\n-          value in each token slot.\n+        - `\"multi_hot\"`: Outputs a single int array per batch, of either\n+          vocab_size or max_tokens size, containing 1s in all elements where the\n+          token mapped to that index exists at least once in the batch item.\n+        - `\"count\"`: As `\"multi_hot\"`, but the int array contains a count of the\n+          number of times the token at that index appeared in the batch item.\n+        - `\"tf_idf\"`: As `\"multi_hot\"`, but the TF-IDF algorithm is applied to\n+          find the value in each token slot.\n     output_sequence_length: Only valid in INT mode. If set, the output will have\n       its time dimension padded or truncated to exactly `output_sequence_length`\n       values, resulting in a tensor of shape [batch_size,\n       output_sequence_length] regardless of how many tokens resulted from the\n       splitting step. Defaults to None.\n-    pad_to_max_tokens: Only valid in  \"binary\", \"count\", and \"tf-idf\" modes. If\n-      True, the output will have its feature axis padded to `max_tokens` even if\n-      the number of unique tokens in the vocabulary is less than max_tokens,\n-      resulting in a tensor of shape [batch_size, max_tokens] regardless of\n-      vocabulary size. Defaults to False.\n+    pad_to_max_tokens: Only valid in  `\"multi_hot\"`, `\"count\"`, and `\"tf_idf\"`\n+      modes. If True, the output will have its feature axis padded to\n+      `max_tokens` even if the number of unique tokens in the vocabulary is less\n+      than max_tokens, resulting in a tensor of shape [batch_size, max_tokens]\n+      regardless of vocabulary size. Defaults to False.\n     vocabulary: An optional list of vocabulary terms, or a path to a text file\n       containing a vocabulary to load into this layer. The file should contain\n       one token per line. If the list or file contains the same token multiple\n@@ -250,10 +251,15 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n         allow_none=True,\n         allow_callables=True)\n \n-    # 'output_mode' must be one of (None, INT, COUNT, BINARY, TFIDF)\n+    # Support deprecated names for output_modes.\n+    if output_mode == \"binary\":\n+      output_mode = MULTI_HOT\n+    if output_mode == \"tf-idf\":\n+      output_mode = TF_IDF\n+    # 'output_mode' must be one of (None, INT, COUNT, MULTI_HOT, TF_IDF)\n     layer_utils.validate_string_arg(\n         output_mode,\n-        allowable_strings=(INT, COUNT, BINARY, TFIDF),\n+        allowable_strings=(INT, COUNT, MULTI_HOT, TF_IDF),\n         layer_name=\"TextVectorization\",\n         arg_name=\"output_mode\",\n         allow_none=True)\n@@ -433,14 +439,14 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n       vocabulary: An array of string tokens, or a path to a file containing one\n         token per line.\n       idf_weights: An array of document frequency data with equal length to\n-        vocab. Only necessary if the layer output_mode is TFIDF.\n+        vocab. Only necessary if the layer output_mode is TF_IDF.\n \n     Raises:\n       ValueError: If there are too many inputs, the inputs do not match, or\n         input data is missing.\n       RuntimeError: If the vocabulary cannot be set when this function is\n-        called. This happens when \"binary\", \"count\", and \"tfidf\" modes,\n-        if \"pad_to_max_tokens\" is False and the layer itself has already been\n+        called. This happens when `\"multi_hot\"`, `\"count\"`, and \"tfidf\" modes,\n+        if `pad_to_max_tokens` is False and the layer itself has already been\n         called.\n     \"\"\"\n     self._index_lookup_layer.set_vocabulary(vocabulary, idf_weights=idf_weights)\n@@ -462,7 +468,7 @@ class TextVectorization(base_preprocessing_layer.CombinerPreprocessingLayer):\n   def _set_state_variables(self, updates):\n     if not self.built:\n       raise RuntimeError(\"_set_state_variables() must be called after build().\")\n-    if self._output_mode == TFIDF:\n+    if self._output_mode == TF_IDF:\n       self.set_vocabulary(updates[_VOCAB_NAME], idf_weights=updates[_IDF_NAME])\n     else:\n       self.set_vocabulary(updates[_VOCAB_NAME])\n\n@@ -144,7 +144,7 @@ def _get_end_to_end_test_cases():\n               \"max_tokens\": 5,\n               \"standardize\": None,\n               \"split\": None,\n-              \"output_mode\": text_vectorization.BINARY\n+              \"output_mode\": text_vectorization.MULTI_HOT\n           },\n           \"expected_output\": [[0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0],\n                               [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 1, 0],\n@@ -163,7 +163,7 @@ def _get_end_to_end_test_cases():\n               \"max_tokens\": 5,\n               \"standardize\": None,\n               \"split\": text_vectorization.SPLIT_ON_WHITESPACE,\n-              \"output_mode\": text_vectorization.BINARY\n+              \"output_mode\": text_vectorization.MULTI_HOT\n           },\n           \"expected_output\": [[0, 1, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1],\n                               [1, 1, 0, 0, 0]],\n@@ -218,7 +218,7 @@ def _get_end_to_end_test_cases():\n               \"max_tokens\": 5,\n               \"standardize\": None,\n               \"split\": None,\n-              \"output_mode\": text_vectorization.TFIDF\n+              \"output_mode\": text_vectorization.TF_IDF\n           },\n           \"expected_output\": [[0, 1.098612, 0, 0, 0], [0, 0, 1.252763, 0, 0],\n                               [0, 0, 0, 1.466337, 0], [0, 0, 0, 0, 1.7917595],\n@@ -238,7 +238,7 @@ def _get_end_to_end_test_cases():\n               \"max_tokens\": 5,\n               \"standardize\": None,\n               \"split\": text_vectorization.SPLIT_ON_WHITESPACE,\n-              \"output_mode\": text_vectorization.TFIDF\n+              \"output_mode\": text_vectorization.TF_IDF\n           },\n           \"expected_output\": [[0., 0.847298, 0.847298, 0., 0.],\n                               [0., 0., 0., 1.098612, 0.],\n@@ -430,7 +430,7 @@ class TextVectorizationPreprocessingTest(\n         standardize=text_vectorization.LOWER_AND_STRIP_PUNCTUATION,\n         split=None,\n         ngrams=None,\n-        output_mode=text_vectorization.TFIDF)\n+        output_mode=text_vectorization.TF_IDF)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n     # We are testing that model.summary() can be called without erroring out.\n@@ -937,7 +937,7 @@ class TextVectorizationOutputTest(\n         max_tokens=max_tokens,\n         standardize=None,\n         split=None,\n-        output_mode=text_vectorization.BINARY,\n+        output_mode=text_vectorization.MULTI_HOT,\n         pad_to_max_tokens=True)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n@@ -964,7 +964,7 @@ class TextVectorizationOutputTest(\n         max_tokens=10,\n         standardize=None,\n         split=None,\n-        output_mode=text_vectorization.BINARY,\n+        output_mode=text_vectorization.MULTI_HOT,\n         pad_to_max_tokens=False)\n     layer.set_vocabulary(vocab_data)\n     int_data = layer(input_data)\n@@ -991,7 +991,7 @@ class TextVectorizationOutputTest(\n         max_tokens=max_tokens,\n         standardize=None,\n         split=None,\n-        output_mode=text_vectorization.BINARY,\n+        output_mode=text_vectorization.MULTI_HOT,\n         pad_to_max_tokens=True)\n     int_data = layer(input_data)\n     layer.set_vocabulary(vocab_data)\n@@ -1021,7 +1021,7 @@ class TextVectorizationOutputTest(\n         max_tokens=max_tokens,\n         standardize=None,\n         split=None,\n-        output_mode=text_vectorization.BINARY,\n+        output_mode=text_vectorization.MULTI_HOT,\n         pad_to_max_tokens=True)\n     int_data = layer(input_data)\n     layer.adapt(vocab_data)\n@@ -1050,7 +1050,7 @@ class TextVectorizationOutputTest(\n         max_tokens=max_tokens,\n         standardize=None,\n         split=None,\n-        output_mode=text_vectorization.BINARY,\n+        output_mode=text_vectorization.MULTI_HOT,\n         pad_to_max_tokens=True)\n     int_data = layer(input_data)\n     layer._set_state_variables(state_variables)\n@@ -1082,7 +1082,7 @@ class TextVectorizationOutputTest(\n         max_tokens=5,\n         standardize=None,\n         split=None,\n-        output_mode=text_vectorization.BINARY,\n+        output_mode=text_vectorization.MULTI_HOT,\n         pad_to_max_tokens=True)\n     int_data = layer(input_data)\n     model = keras.Model(inputs=input_data, outputs=int_data)\n@@ -1113,7 +1113,7 @@ class TextVectorizationOutputTest(\n         max_tokens=10,\n         standardize=None,\n         split=None,\n-        output_mode=text_vectorization.BINARY,\n+        output_mode=text_vectorization.MULTI_HOT,\n         pad_to_max_tokens=False)\n     layer.build(input_data.shape)\n     layer.set_vocabulary(vocab_data)\n@@ -1132,7 +1132,7 @@ class TextVectorizationOutputTest(\n         max_tokens=None,\n         standardize=None,\n         split=None,\n-        output_mode=text_vectorization.BINARY,\n+        output_mode=text_vectorization.MULTI_HOT,\n         pad_to_max_tokens=False)\n     layer.adapt(vocab_data)\n     _ = layer(input_data)\n@@ -1149,7 +1149,7 @@ class TextVectorizationOutputTest(\n         max_tokens=None,\n         standardize=None,\n         split=None,\n-        output_mode=text_vectorization.BINARY,\n+        output_mode=text_vectorization.MULTI_HOT,\n         pad_to_max_tokens=False)\n     layer.adapt([\"earth\", \"wind\"])\n     _ = layer(input_data)\n@@ -1231,7 +1231,7 @@ class TextVectorizationOutputTest(\n         max_tokens=6,\n         standardize=None,\n         split=None,\n-        output_mode=text_vectorization.TFIDF,\n+        output_mode=text_vectorization.TF_IDF,\n         pad_to_max_tokens=True)\n     layer.set_vocabulary(vocab_data, idf_weights=idf_weights)\n     int_data = layer(input_data)\n@@ -1262,7 +1262,7 @@ class TextVectorizationOutputTest(\n         max_tokens=10,\n         standardize=None,\n         split=None,\n-        output_mode=text_vectorization.TFIDF,\n+        output_mode=text_vectorization.TF_IDF,\n         pad_to_max_tokens=False)\n     layer.set_vocabulary(vocab_data, idf_weights=idf_weights)\n     int_data = layer(input_data)\n@@ -1292,7 +1292,7 @@ class TextVectorizationOutputTest(\n         max_tokens=10,\n         standardize=None,\n         split=None,\n-        output_mode=text_vectorization.TFIDF,\n+        output_mode=text_vectorization.TF_IDF,\n         pad_to_max_tokens=False)\n     layer.set_vocabulary(vocab_data, idf_weights=idf_weights)\n     int_data = layer(input_data)\n@@ -1328,19 +1328,19 @@ class TextVectorizationModelBuildingTest(\n       }, {\n           \"testcase_name\": \"binary_hard_max\",\n           \"pad_to_max_tokens\": True,\n-          \"output_mode\": text_vectorization.BINARY\n+          \"output_mode\": text_vectorization.MULTI_HOT\n       }, {\n           \"testcase_name\": \"binary_soft_max\",\n           \"pad_to_max_tokens\": False,\n-          \"output_mode\": text_vectorization.BINARY\n+          \"output_mode\": text_vectorization.MULTI_HOT\n       }, {\n           \"testcase_name\": \"tfidf_hard_max\",\n           \"pad_to_max_tokens\": True,\n-          \"output_mode\": text_vectorization.TFIDF\n+          \"output_mode\": text_vectorization.TF_IDF\n       }, {\n           \"testcase_name\": \"tfidf_soft_max\",\n           \"pad_to_max_tokens\": False,\n-          \"output_mode\": text_vectorization.TFIDF\n+          \"output_mode\": text_vectorization.TF_IDF\n       })\n   def test_end_to_end_bagged_modeling(self, output_mode, pad_to_max_tokens):\n     vocab_data = [\"earth\", \"wind\", \"and\", \"fire\"]\n@@ -1355,7 +1355,7 @@ class TextVectorizationModelBuildingTest(\n         split=None,\n         output_mode=output_mode,\n         pad_to_max_tokens=pad_to_max_tokens)\n-    if output_mode == text_vectorization.TFIDF:\n+    if output_mode == text_vectorization.TF_IDF:\n       layer.set_vocabulary(vocab_data, idf_weights=idf_weights)\n     else:\n       layer.set_vocabulary(vocab_data)\n@@ -1418,9 +1418,9 @@ class TextVectorizationErrorTest(keras_parameterized.TestCase,\n         max_tokens=5,\n         standardize=None,\n         split=None,\n-        output_mode=text_vectorization.TFIDF)\n+        output_mode=text_vectorization.TF_IDF)\n     with self.assertRaisesRegex(\n-        ValueError, \"`idf_weights` must be set if output_mode is TFIDF\"):\n+        ValueError, \"`idf_weights` must be set if output_mode is TF_IDF\"):\n       layer.set_vocabulary(vocab_data)\n \n   def test_idf_weights_length_mismatch_fails(self):\n@@ -1430,7 +1430,7 @@ class TextVectorizationErrorTest(keras_parameterized.TestCase,\n         max_tokens=5,\n         standardize=None,\n         split=None,\n-        output_mode=text_vectorization.TFIDF)\n+        output_mode=text_vectorization.TF_IDF)\n     with self.assertRaisesRegex(\n         ValueError, \"`idf_weights` must be the same length as vocab\"):\n       layer.set_vocabulary(vocab_data, idf_weights)\n@@ -1442,7 +1442,7 @@ class TextVectorizationErrorTest(keras_parameterized.TestCase,\n         max_tokens=5,\n         standardize=None,\n         split=None,\n-        output_mode=text_vectorization.BINARY)\n+        output_mode=text_vectorization.MULTI_HOT)\n     with self.assertRaisesRegex(ValueError,\n                                 \"`idf_weights` should only be set if\"):\n       layer.set_vocabulary(vocab_data, idf_weights)\n@@ -1601,7 +1601,7 @@ class TextVectorizationSavingTest(\n         max_tokens=5,\n         standardize=None,\n         split=None,\n-        output_mode=text_vectorization.TFIDF)\n+        output_mode=text_vectorization.TF_IDF)\n     layer.set_vocabulary(vocab_data, idf_weights=idf_weights)\n \n     int_data = layer(input_data)\n\n@@ -35,13 +35,14 @@ from keras.layers import local\n from keras.layers import merge\n from keras.layers import multi_head_attention\n from keras.layers import noise\n-from keras.layers import normalization\n-from keras.layers import normalization_v2\n from keras.layers import pooling\n from keras.layers import recurrent\n from keras.layers import recurrent_v2\n from keras.layers import rnn_cell_wrapper_v2\n from keras.layers import wrappers\n+from keras.layers.normalization import batch_normalization\n+from keras.layers.normalization import batch_normalization_v1\n+from keras.layers.normalization import layer_normalization\n from keras.layers.preprocessing import category_crossing\n from keras.layers.preprocessing import category_encoding\n from keras.layers.preprocessing import discretization\n@@ -57,12 +58,14 @@ from tensorflow.python.util.tf_export import keras_export\n \n ALL_MODULES = (base_layer, input_layer, advanced_activations, convolutional,\n                convolutional_recurrent, core, cudnn_recurrent, dense_attention,\n-               embeddings, einsum_dense, local, merge, noise, normalization,\n+               embeddings, einsum_dense, local, merge, noise,\n+               batch_normalization_v1, layer_normalization,\n                pooling, image_preprocessing, recurrent, wrappers, hashing,\n                category_crossing, category_encoding, discretization,\n                multi_head_attention, integer_lookup,\n                preprocessing_normalization, string_lookup, text_vectorization)\n-ALL_V2_MODULES = (rnn_cell_wrapper_v2, normalization_v2, recurrent_v2)\n+ALL_V2_MODULES = (rnn_cell_wrapper_v2, batch_normalization, layer_normalization,\n+                  recurrent_v2)\n # ALL_OBJECTS is meant to be a global mutable. Hence we need to make it\n # thread-local to avoid concurrent mutations.\n LOCAL = threading.local()\n@@ -101,9 +104,10 @@ def populate_deserializable_objects():\n   # as in TF 1.13, \"BatchNormalizationV1\" and \"BatchNormalizationV2\"\n   # were used as class name for v1 and v2 version of BatchNormalization,\n   # respectively. Here we explicitly convert them to their canonical names.\n-  LOCAL.ALL_OBJECTS['BatchNormalizationV1'] = normalization.BatchNormalization\n   LOCAL.ALL_OBJECTS[\n-      'BatchNormalizationV2'] = normalization_v2.BatchNormalization\n+      'BatchNormalizationV1'] = batch_normalization_v1.BatchNormalization\n+  LOCAL.ALL_OBJECTS[\n+      'BatchNormalizationV2'] = batch_normalization.BatchNormalization\n \n   # Prevent circular dependencies.\n   from keras import models  # pylint: disable=g-import-not-at-top\n\n@@ -20,10 +20,10 @@ from absl.testing import parameterized\n \n import keras\n from keras import combinations\n-from keras.layers import normalization as batchnorm_v1\n-from keras.layers import normalization_v2 as batchnorm_v2\n from keras.layers import recurrent as rnn_v1\n from keras.layers import recurrent_v2 as rnn_v2\n+from keras.layers.normalization import batch_normalization as batchnorm_v2\n+from keras.layers.normalization import batch_normalization_v1 as batchnorm_v1\n \n \n class SerializableInt(int):\n\n@@ -22,7 +22,7 @@ import tensorflow.compat.v2 as tf\n \n import warnings\n \n-from keras.layers import normalization as keras_normalization\n+from keras.layers.normalization import batch_normalization_v1\n from keras.legacy_tf_layers import base\n from tensorflow.python.util.tf_export import keras_export\n from tensorflow.python.util.tf_export import tf_export\n@@ -30,7 +30,7 @@ from tensorflow.python.util.tf_export import tf_export\n \n @keras_export(v1=['keras.__internal__.legacy.layers.BatchNormalization'])\n @tf_export(v1=['layers.BatchNormalization'])\n-class BatchNormalization(keras_normalization.BatchNormalization, base.Layer):\n+class BatchNormalization(batch_normalization_v1.BatchNormalization, base.Layer):\n   \"\"\"Batch Normalization layer from (Ioffe et al., 2015).\n \n   Keras APIs handle BatchNormalization updates to the moving_mean and\n\n@@ -2080,7 +2080,7 @@ def get(identifier):\n   <class 'function'>\n   >>> loss = tf.keras.losses.get(\"CategoricalCrossentropy\")\n   >>> type(loss)\n-  <class '...tensorflow.python.keras.losses.CategoricalCrossentropy'>\n+  <class '...keras.losses.CategoricalCrossentropy'>\n \n   You can also specify `config` of the loss to this function by passing dict\n   containing `class_name` and `config` as an identifier. Also note that the\n@@ -2090,7 +2090,7 @@ def get(identifier):\n   ...               \"config\": {\"from_logits\": True}}\n   >>> loss = tf.keras.losses.get(identifier)\n   >>> type(loss)\n-  <class '...tensorflow.python.keras.losses.CategoricalCrossentropy'>\n+  <class '...keras.losses.CategoricalCrossentropy'>\n \n   Args:\n     identifier: A loss identifier. One of None or string name of a loss\n\n@@ -3613,7 +3613,7 @@ def get(identifier):\n   <class 'function'>\n   >>> metric = tf.keras.metrics.get(\"CategoricalCrossentropy\")\n   >>> type(metric)\n-  <class '...tensorflow.python.keras.metrics.CategoricalCrossentropy'>\n+  <class '...keras.metrics.CategoricalCrossentropy'>\n \n   You can also specify `config` of the metric to this function by passing dict\n   containing `class_name` and `config` as an identifier. Also note that the\n@@ -3623,7 +3623,7 @@ def get(identifier):\n   ...               \"config\": {\"from_logits\": True}}\n   >>> metric = tf.keras.metrics.get(identifier)\n   >>> type(metric)\n-  <class '...tensorflow.python.keras.metrics.CategoricalCrossentropy'>\n+  <class '...keras.metrics.CategoricalCrossentropy'>\n \n   Args:\n     identifier: A metric identifier. One of None or string name of a metric\n\n@@ -30,12 +30,12 @@ from keras.layers import embeddings\n from keras.layers import local\n from keras.layers import merge\n from keras.layers import noise\n-from keras.layers import normalization\n-from keras.layers import normalization_v2\n from keras.layers import pooling\n from keras.layers import recurrent\n from keras.layers import recurrent_v2\n from keras.layers import wrappers\n+from keras.layers.normalization import batch_normalization\n+from keras.layers.normalization import layer_normalization\n from keras.mixed_precision import policy\n \n \n@@ -114,11 +114,11 @@ class LayerCorrectnessTest(keras_parameterized.TestCase):\n       ('GaussianNoise', lambda: noise.GaussianNoise(0.5), (2, 2)),\n       ('GaussianDropout', lambda: noise.GaussianDropout(0.5), (2, 2)),\n       ('AlphaDropout', lambda: noise.AlphaDropout(0.5), (2, 2)),\n-      ('BatchNormalization', normalization_v2.BatchNormalization,\n+      ('BatchNormalization', batch_normalization.BatchNormalization,\n        (2, 2), 1e-2, 1e-2),\n-      ('LayerNormalization', normalization.LayerNormalization, (2, 2)),\n+      ('LayerNormalization', layer_normalization.LayerNormalization, (2, 2)),\n       ('LayerNormalizationUnfused',\n-       lambda: normalization.LayerNormalization(axis=1), (2, 2, 2)),\n+       lambda: layer_normalization.LayerNormalization(axis=1), (2, 2, 2)),\n       ('MaxPooling2D', pooling.MaxPooling2D, (2, 2, 2, 1)),\n       ('AveragePooling2D', pooling.AveragePooling2D, (2, 2, 2, 1)),\n       ('GlobalMaxPooling2D', pooling.GlobalMaxPooling2D, (2, 2, 2, 1)),\n\n@@ -24,7 +24,7 @@ from keras import keras_parameterized\n from keras.engine import sequential\n from keras.engine import training\n from keras.layers import core\n-from keras.layers import normalization\n+from keras.layers.normalization import batch_normalization_v1\n from tensorflow.python.training.tracking import data_structures\n from tensorflow.python.training.tracking import util\n \n@@ -49,7 +49,7 @@ class HasList(training.Model):\n         tf.__internal__.tracking.wrap(\n             list([core.Dense(11)]) + [core.Dense(12)]))\n     self.layers_with_updates = tf.__internal__.tracking.wrap(\n-        [normalization.BatchNormalization()])\n+        [batch_normalization_v1.BatchNormalization()])\n \n   def call(self, x):\n     aggregation = 0.\n@@ -236,9 +236,9 @@ class HasMapping(training.Model):\n         [core.Dense(5),\n          core.Dense(6, kernel_regularizer=tf.reduce_sum)])\n     self.layer_dict[\"norm\"].append(\n-        normalization.BatchNormalization())\n+        batch_normalization_v1.BatchNormalization())\n     self.layer_dict[\"norm\"].append(\n-        normalization.BatchNormalization())\n+        batch_normalization_v1.BatchNormalization())\n \n   def call(self, x):\n     aggregation = 0.\n@@ -387,7 +387,7 @@ class HasTuple(training.Model):\n     self.layer_list = (\n         core.Dense(3), core.Dense(4),\n         core.Dense(5, kernel_regularizer=tf.reduce_sum))\n-    self.layers_with_updates = (normalization.BatchNormalization(),)\n+    self.layers_with_updates = (batch_normalization_v1.BatchNormalization(),)\n \n   def call(self, x):\n     aggregation = 0.\n\n@@ -32,10 +32,21 @@ class ReductionV2(object):\n      used with `tf.distribute.Strategy`, outside of built-in training loops such\n      as `tf.keras` `compile` and `fit`, we expect reduction value to be\n      `SUM` or `NONE`. Using `AUTO` in that case will raise an error.\n-  * `NONE`: Weighted losses with one dimension reduced (axis=-1, or axis\n-     specified by loss function). When this reduction type used with built-in\n-     Keras training loops like `fit`/`evaluate`, the unreduced vector loss is\n-     passed to the optimizer but the reported loss will be a scalar value.\n+  * `NONE`: No **additional** reduction is applied to the output of the wrapped\n+     loss function. When non-scalar losses are returned to Keras functions like\n+     `fit`/`evaluate`, the unreduced vector loss is passed to the optimizer\n+     but the reported loss will be a scalar value.\n+\n+     Caution: **Verify the shape of the outputs when using** `Reduction.NONE`.\n+     The builtin loss functions wrapped by the loss classes reduce\n+     one dimension (`axis=-1`, or `axis` if specified by loss function).\n+     `Reduction.NONE` just means that no **additional** reduction is applied by\n+     the class wrapper. For categorical losses with an example input shape of\n+     `[batch, W, H, n_classes]` the `n_classes` dimension is reduced. For\n+     pointwise losses your must include a dummy axis so that `[batch, W, H, 1]`\n+     is reduced to `[batch, W, H]`. Without the dummy axis `[batch, W, H]`\n+     will be incorrectly reduced to `[batch, W]`.\n+\n   * `SUM`: Scalar sum of weighted losses.\n   * `SUM_OVER_BATCH_SIZE`: Scalar `SUM` divided by number of elements in losses.\n      This reduction type is not supported when used with\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
