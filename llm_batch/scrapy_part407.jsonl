{"custom_id": "scrapy#7bbe775040d5d695bc3b48a73de5d6fa99312b4c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 436 | Lines Deleted: 511 | Files Changed: 19 | Hunks: 214 | Methods Changed: 113 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 947 | Churn Cumulative: 7425 | Contributors (this commit): 53 | Commits (past 90d): 51 | Contributors (cumulative): 169 | DMM Complexity: 0.21052631578947367\n\nDIFF:\n@@ -1,11 +1,10 @@\n import logging\n-import unittest\n \n import pytest\n from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.python.failure import Failure\n-from twisted.trial.unittest import TestCase as TwistedTestCase\n+from twisted.trial.unittest import TestCase\n \n from scrapy.exceptions import DropItem\n from scrapy.http import Request, Response\n@@ -24,8 +23,8 @@ class CustomItem(Item):\n         return f\"name: {self['name']}\"\n \n \n-class LogFormatterTestCase(unittest.TestCase):\n-    def setUp(self):\n+class TestLogFormatter:\n+    def setup_method(self):\n         self.formatter = LogFormatter()\n         self.spider = Spider(\"default\")\n         self.spider.crawler = get_crawler()\n@@ -35,9 +34,7 @@ class LogFormatterTestCase(unittest.TestCase):\n         res = Response(\"http://www.example.com\")\n         logkws = self.formatter.crawled(req, res, self.spider)\n         logline = logkws[\"msg\"] % logkws[\"args\"]\n-        self.assertEqual(\n-            logline, \"Crawled (200) <GET http://www.example.com> (referer: None)\"\n-        )\n+        assert logline == \"Crawled (200) <GET http://www.example.com> (referer: None)\"\n \n     def test_crawled_without_referer(self):\n         req = Request(\n@@ -46,9 +43,9 @@ class LogFormatterTestCase(unittest.TestCase):\n         res = Response(\"http://www.example.com\", flags=[\"cached\"])\n         logkws = self.formatter.crawled(req, res, self.spider)\n         logline = logkws[\"msg\"] % logkws[\"args\"]\n-        self.assertEqual(\n-            logline,\n-            \"Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']\",\n+        assert (\n+            logline\n+            == \"Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']\"\n         )\n \n     def test_flags_in_request(self):\n@@ -56,9 +53,9 @@ class LogFormatterTestCase(unittest.TestCase):\n         res = Response(\"http://www.example.com\")\n         logkws = self.formatter.crawled(req, res, self.spider)\n         logline = logkws[\"msg\"] % logkws[\"args\"]\n-        self.assertEqual(\n-            logline,\n-            \"Crawled (200) <GET http://www.example.com> ['test', 'flag'] (referer: None)\",\n+        assert (\n+            logline\n+            == \"Crawled (200) <GET http://www.example.com> ['test', 'flag'] (referer: None)\"\n         )\n \n     def test_dropped(self):\n@@ -69,7 +66,7 @@ class LogFormatterTestCase(unittest.TestCase):\n         logline = logkws[\"msg\"] % logkws[\"args\"]\n         lines = logline.splitlines()\n         assert all(isinstance(x, str) for x in lines)\n-        self.assertEqual(lines, [\"Dropped: \\u2018\", \"{}\"])\n+        assert lines == [\"Dropped: \\u2018\", \"{}\"]\n \n     def test_dropitem_default_log_level(self):\n         item = {}\n@@ -79,38 +76,38 @@ class LogFormatterTestCase(unittest.TestCase):\n         spider.crawler = get_crawler(Spider)\n \n         logkws = self.formatter.dropped(item, exception, response, spider)\n-        self.assertEqual(logkws[\"level\"], logging.WARNING)\n+        assert logkws[\"level\"] == logging.WARNING\n \n         spider.crawler.settings.frozen = False\n         spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = logging.INFO\n         spider.crawler.settings.frozen = True\n         logkws = self.formatter.dropped(item, exception, response, spider)\n-        self.assertEqual(logkws[\"level\"], logging.INFO)\n+        assert logkws[\"level\"] == logging.INFO\n \n         spider.crawler.settings.frozen = False\n         spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = \"INFO\"\n         spider.crawler.settings.frozen = True\n         logkws = self.formatter.dropped(item, exception, response, spider)\n-        self.assertEqual(logkws[\"level\"], logging.INFO)\n+        assert logkws[\"level\"] == logging.INFO\n \n         spider.crawler.settings.frozen = False\n         spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = 10\n         spider.crawler.settings.frozen = True\n         logkws = self.formatter.dropped(item, exception, response, spider)\n-        self.assertEqual(logkws[\"level\"], logging.DEBUG)\n+        assert logkws[\"level\"] == logging.DEBUG\n \n         spider.crawler.settings.frozen = False\n         spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = 0\n         spider.crawler.settings.frozen = True\n         logkws = self.formatter.dropped(item, exception, response, spider)\n-        self.assertEqual(logkws[\"level\"], logging.NOTSET)\n+        assert logkws[\"level\"] == logging.NOTSET\n \n         unsupported_value = object()\n         spider.crawler.settings.frozen = False\n         spider.crawler.settings[\"DEFAULT_DROPITEM_LOG_LEVEL\"] = unsupported_value\n         spider.crawler.settings.frozen = True\n         logkws = self.formatter.dropped(item, exception, response, spider)\n-        self.assertEqual(logkws[\"level\"], unsupported_value)\n+        assert logkws[\"level\"] == unsupported_value\n \n         with pytest.raises(TypeError):\n             logging.log(logkws[\"level\"], \"message\")\n@@ -121,11 +118,11 @@ class LogFormatterTestCase(unittest.TestCase):\n \n         exception = DropItem(\"Test drop\", log_level=\"INFO\")\n         logkws = self.formatter.dropped(item, exception, response, self.spider)\n-        self.assertEqual(logkws[\"level\"], logging.INFO)\n+        assert logkws[\"level\"] == logging.INFO\n \n         exception = DropItem(\"Test drop\", log_level=\"ERROR\")\n         logkws = self.formatter.dropped(item, exception, response, self.spider)\n-        self.assertEqual(logkws[\"level\"], logging.ERROR)\n+        assert logkws[\"level\"] == logging.ERROR\n \n     def test_item_error(self):\n         # In practice, the complete traceback is shown by passing the\n@@ -135,7 +132,7 @@ class LogFormatterTestCase(unittest.TestCase):\n         response = Response(\"http://www.example.com\")\n         logkws = self.formatter.item_error(item, exception, response, self.spider)\n         logline = logkws[\"msg\"] % logkws[\"args\"]\n-        self.assertEqual(logline, \"Error processing {'key': 'value'}\")\n+        assert logline == \"Error processing {'key': 'value'}\"\n \n     def test_spider_error(self):\n         # In practice, the complete traceback is shown by passing the\n@@ -147,9 +144,9 @@ class LogFormatterTestCase(unittest.TestCase):\n         response = Response(\"http://www.example.com\", request=request)\n         logkws = self.formatter.spider_error(failure, request, response, self.spider)\n         logline = logkws[\"msg\"] % logkws[\"args\"]\n-        self.assertEqual(\n-            logline,\n-            \"Spider error processing <GET http://www.example.com> (referer: http://example.org)\",\n+        assert (\n+            logline\n+            == \"Spider error processing <GET http://www.example.com> (referer: http://example.org)\"\n         )\n \n     def test_download_error_short(self):\n@@ -159,7 +156,7 @@ class LogFormatterTestCase(unittest.TestCase):\n         request = Request(\"http://www.example.com\")\n         logkws = self.formatter.download_error(failure, request, self.spider)\n         logline = logkws[\"msg\"] % logkws[\"args\"]\n-        self.assertEqual(logline, \"Error downloading <GET http://www.example.com>\")\n+        assert logline == \"Error downloading <GET http://www.example.com>\"\n \n     def test_download_error_long(self):\n         # In practice, the complete traceback is shown by passing the\n@@ -170,9 +167,7 @@ class LogFormatterTestCase(unittest.TestCase):\n             failure, request, self.spider, \"Some message\"\n         )\n         logline = logkws[\"msg\"] % logkws[\"args\"]\n-        self.assertEqual(\n-            logline, \"Error downloading <GET http://www.example.com>: Some message\"\n-        )\n+        assert logline == \"Error downloading <GET http://www.example.com>: Some message\"\n \n     def test_scraped(self):\n         item = CustomItem()\n@@ -182,9 +177,7 @@ class LogFormatterTestCase(unittest.TestCase):\n         logline = logkws[\"msg\"] % logkws[\"args\"]\n         lines = logline.splitlines()\n         assert all(isinstance(x, str) for x in lines)\n-        self.assertEqual(\n-            lines, [\"Scraped from <200 http://www.example.com>\", \"name: \\xa3\"]\n-        )\n+        assert lines == [\"Scraped from <200 http://www.example.com>\", \"name: \\xa3\"]\n \n \n class LogFormatterSubclass(LogFormatter):\n@@ -200,8 +193,8 @@ class LogFormatterSubclass(LogFormatter):\n         }\n \n \n-class LogformatterSubclassTest(LogFormatterTestCase):\n-    def setUp(self):\n+class TestLogformatterSubclass(TestLogFormatter):\n+    def setup_method(self):\n         self.formatter = LogFormatterSubclass()\n         self.spider = Spider(\"default\")\n         self.spider.crawler = get_crawler(Spider)\n@@ -211,8 +204,8 @@ class LogformatterSubclassTest(LogFormatterTestCase):\n         res = Response(\"http://www.example.com\")\n         logkws = self.formatter.crawled(req, res, self.spider)\n         logline = logkws[\"msg\"] % logkws[\"args\"]\n-        self.assertEqual(\n-            logline, \"Crawled (200) <GET http://www.example.com> (referer: None) []\"\n+        assert (\n+            logline == \"Crawled (200) <GET http://www.example.com> (referer: None) []\"\n         )\n \n     def test_crawled_without_referer(self):\n@@ -224,9 +217,9 @@ class LogformatterSubclassTest(LogFormatterTestCase):\n         res = Response(\"http://www.example.com\")\n         logkws = self.formatter.crawled(req, res, self.spider)\n         logline = logkws[\"msg\"] % logkws[\"args\"]\n-        self.assertEqual(\n-            logline,\n-            \"Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']\",\n+        assert (\n+            logline\n+            == \"Crawled (200) <GET http://www.example.com> (referer: http://example.com) ['cached']\"\n         )\n \n     def test_flags_in_request(self):\n@@ -234,9 +227,9 @@ class LogformatterSubclassTest(LogFormatterTestCase):\n         res = Response(\"http://www.example.com\")\n         logkws = self.formatter.crawled(req, res, self.spider)\n         logline = logkws[\"msg\"] % logkws[\"args\"]\n-        self.assertEqual(\n-            logline,\n-            \"Crawled (200) <GET http://www.example.com> (referer: None) ['test', 'flag']\",\n+        assert (\n+            logline\n+            == \"Crawled (200) <GET http://www.example.com> (referer: None) ['test', 'flag']\"\n         )\n \n \n@@ -261,7 +254,7 @@ class DropSomeItemsPipeline:\n         self.drop = True\n \n \n-class ShowOrSkipMessagesTestCase(TwistedTestCase):\n+class TestShowOrSkipMessages(TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls.mockserver = MockServer()\n@@ -284,9 +277,9 @@ class ShowOrSkipMessagesTestCase(TwistedTestCase):\n         crawler = get_crawler(ItemSpider, self.base_settings)\n         with LogCapture() as lc:\n             yield crawler.crawl(mockserver=self.mockserver)\n-        self.assertIn(\"Scraped from <200 http://127.0.0.1:\", str(lc))\n-        self.assertIn(\"Crawled (200) <GET http://127.0.0.1:\", str(lc))\n-        self.assertIn(\"Dropped: Ignoring item\", str(lc))\n+        assert \"Scraped from <200 http://127.0.0.1:\" in str(lc)\n+        assert \"Crawled (200) <GET http://127.0.0.1:\" in str(lc)\n+        assert \"Dropped: Ignoring item\" in str(lc)\n \n     @defer.inlineCallbacks\n     def test_skip_messages(self):\n@@ -295,6 +288,6 @@ class ShowOrSkipMessagesTestCase(TwistedTestCase):\n         crawler = get_crawler(ItemSpider, settings)\n         with LogCapture() as lc:\n             yield crawler.crawl(mockserver=self.mockserver)\n-        self.assertNotIn(\"Scraped from <200 http://127.0.0.1:\", str(lc))\n-        self.assertNotIn(\"Crawled (200) <GET http://127.0.0.1:\", str(lc))\n-        self.assertNotIn(\"Dropped: Ignoring item\", str(lc))\n+        assert \"Scraped from <200 http://127.0.0.1:\" not in str(lc)\n+        assert \"Crawled (200) <GET http://127.0.0.1:\" not in str(lc)\n+        assert \"Dropped: Ignoring item\" not in str(lc)\n\n@@ -1,4 +1,3 @@\n-import unittest\n from datetime import datetime\n \n import pytest\n@@ -8,8 +7,8 @@ from scrapy.utils.test import get_crawler\n from tests.spiders import SimpleSpider\n \n \n-class TestLogStats(unittest.TestCase):\n-    def setUp(self):\n+class TestLogStats:\n+    def setup_method(self):\n         self.crawler = get_crawler(SimpleSpider)\n         self.spider = self.crawler._create_spider(\"spidey\")\n         self.stats = self.crawler.stats\n@@ -26,34 +25,34 @@ class TestLogStats(unittest.TestCase):\n             logstats.itemsprev\n \n         logstats.spider_opened(self.spider)\n-        self.assertEqual(logstats.pagesprev, 4802)\n-        self.assertEqual(logstats.itemsprev, 3201)\n+        assert logstats.pagesprev == 4802\n+        assert logstats.itemsprev == 3201\n \n         logstats.calculate_stats()\n-        self.assertEqual(logstats.items, 3201)\n-        self.assertEqual(logstats.pages, 4802)\n-        self.assertEqual(logstats.irate, 0.0)\n-        self.assertEqual(logstats.prate, 0.0)\n-        self.assertEqual(logstats.pagesprev, 4802)\n-        self.assertEqual(logstats.itemsprev, 3201)\n+        assert logstats.items == 3201\n+        assert logstats.pages == 4802\n+        assert logstats.irate == 0.0\n+        assert logstats.prate == 0.0\n+        assert logstats.pagesprev == 4802\n+        assert logstats.itemsprev == 3201\n \n         # Simulate what happens after a minute\n         self.stats.set_value(\"response_received_count\", 5187)\n         self.stats.set_value(\"item_scraped_count\", 3492)\n         logstats.calculate_stats()\n-        self.assertEqual(logstats.items, 3492)\n-        self.assertEqual(logstats.pages, 5187)\n-        self.assertEqual(logstats.irate, 291.0)\n-        self.assertEqual(logstats.prate, 385.0)\n-        self.assertEqual(logstats.pagesprev, 5187)\n-        self.assertEqual(logstats.itemsprev, 3492)\n+        assert logstats.items == 3492\n+        assert logstats.pages == 5187\n+        assert logstats.irate == 291.0\n+        assert logstats.prate == 385.0\n+        assert logstats.pagesprev == 5187\n+        assert logstats.itemsprev == 3492\n \n         # Simulate when spider closes after running for 30 mins\n         self.stats.set_value(\"start_time\", datetime.fromtimestamp(1655100172))\n         self.stats.set_value(\"finish_time\", datetime.fromtimestamp(1655101972))\n         logstats.spider_closed(self.spider, \"test reason\")\n-        self.assertEqual(self.stats.get_value(\"responses_per_minute\"), 172.9)\n-        self.assertEqual(self.stats.get_value(\"items_per_minute\"), 116.4)\n+        assert self.stats.get_value(\"responses_per_minute\") == 172.9\n+        assert self.stats.get_value(\"items_per_minute\") == 116.4\n \n     def test_stats_calculations_no_time(self):\n         \"\"\"The stat values should be None since the start and finish time are\n@@ -61,8 +60,8 @@ class TestLogStats(unittest.TestCase):\n         \"\"\"\n         logstats = LogStats.from_crawler(self.crawler)\n         logstats.spider_closed(self.spider, \"test reason\")\n-        self.assertIsNone(self.stats.get_value(\"responses_per_minute\"))\n-        self.assertIsNone(self.stats.get_value(\"items_per_minute\"))\n+        assert self.stats.get_value(\"responses_per_minute\") is None\n+        assert self.stats.get_value(\"items_per_minute\") is None\n \n     def test_stats_calculation_no_elapsed_time(self):\n         \"\"\"The stat values should be None since the elapsed time is 0.\"\"\"\n@@ -70,5 +69,5 @@ class TestLogStats(unittest.TestCase):\n         self.stats.set_value(\"start_time\", datetime.fromtimestamp(1655100172))\n         self.stats.set_value(\"finish_time\", datetime.fromtimestamp(1655100172))\n         logstats.spider_closed(self.spider, \"test reason\")\n-        self.assertIsNone(self.stats.get_value(\"responses_per_minute\"))\n-        self.assertIsNone(self.stats.get_value(\"items_per_minute\"))\n+        assert self.stats.get_value(\"responses_per_minute\") is None\n+        assert self.stats.get_value(\"items_per_minute\") is None\n\n@@ -1,4 +1,3 @@\n-import unittest\n from email.charset import Charset\n from io import BytesIO\n \n@@ -8,7 +7,7 @@ from twisted.internet._sslverify import ClientTLSOptions\n from scrapy.mail import MailSender\n \n \n-class MailSenderTest(unittest.TestCase):\n+class TestMailSender:\n     def test_send(self):\n         mailsender = MailSender(debug=True)\n         mailsender.send(\n@@ -20,15 +19,15 @@ class MailSenderTest(unittest.TestCase):\n \n         assert self.catched_msg\n \n-        self.assertEqual(self.catched_msg[\"to\"], [\"test@scrapy.org\"])\n-        self.assertEqual(self.catched_msg[\"subject\"], \"subject\")\n-        self.assertEqual(self.catched_msg[\"body\"], \"body\")\n+        assert self.catched_msg[\"to\"] == [\"test@scrapy.org\"]\n+        assert self.catched_msg[\"subject\"] == \"subject\"\n+        assert self.catched_msg[\"body\"] == \"body\"\n \n         msg = self.catched_msg[\"msg\"]\n-        self.assertEqual(msg[\"to\"], \"test@scrapy.org\")\n-        self.assertEqual(msg[\"subject\"], \"subject\")\n-        self.assertEqual(msg.get_payload(), \"body\")\n-        self.assertEqual(msg.get(\"Content-Type\"), \"text/plain\")\n+        assert msg[\"to\"] == \"test@scrapy.org\"\n+        assert msg[\"subject\"] == \"subject\"\n+        assert msg.get_payload() == \"body\"\n+        assert msg.get(\"Content-Type\") == \"text/plain\"\n \n     def test_send_single_values_to_and_cc(self):\n         mailsender = MailSender(debug=True)\n@@ -51,8 +50,8 @@ class MailSenderTest(unittest.TestCase):\n         )\n \n         msg = self.catched_msg[\"msg\"]\n-        self.assertEqual(msg.get_payload(), \"<p>body</p>\")\n-        self.assertEqual(msg.get(\"Content-Type\"), \"text/html\")\n+        assert msg.get_payload() == \"<p>body</p>\"\n+        assert msg.get(\"Content-Type\") == \"text/html\"\n \n     def test_send_attach(self):\n         attach = BytesIO()\n@@ -70,22 +69,22 @@ class MailSenderTest(unittest.TestCase):\n         )\n \n         assert self.catched_msg\n-        self.assertEqual(self.catched_msg[\"to\"], [\"test@scrapy.org\"])\n-        self.assertEqual(self.catched_msg[\"subject\"], \"subject\")\n-        self.assertEqual(self.catched_msg[\"body\"], \"body\")\n+        assert self.catched_msg[\"to\"] == [\"test@scrapy.org\"]\n+        assert self.catched_msg[\"subject\"] == \"subject\"\n+        assert self.catched_msg[\"body\"] == \"body\"\n \n         msg = self.catched_msg[\"msg\"]\n-        self.assertEqual(msg[\"to\"], \"test@scrapy.org\")\n-        self.assertEqual(msg[\"subject\"], \"subject\")\n+        assert msg[\"to\"] == \"test@scrapy.org\"\n+        assert msg[\"subject\"] == \"subject\"\n \n         payload = msg.get_payload()\n         assert isinstance(payload, list)\n-        self.assertEqual(len(payload), 2)\n+        assert len(payload) == 2\n \n         text, attach = payload\n-        self.assertEqual(text.get_payload(decode=True), b\"body\")\n-        self.assertEqual(text.get_charset(), Charset(\"us-ascii\"))\n-        self.assertEqual(attach.get_payload(decode=True), b\"content\")\n+        assert text.get_payload(decode=True) == b\"body\"\n+        assert text.get_charset() == Charset(\"us-ascii\")\n+        assert attach.get_payload(decode=True) == b\"content\"\n \n     def _catch_mail_sent(self, **kwargs):\n         self.catched_msg = {**kwargs}\n@@ -103,14 +102,14 @@ class MailSenderTest(unittest.TestCase):\n         )\n \n         assert self.catched_msg\n-        self.assertEqual(self.catched_msg[\"subject\"], subject)\n-        self.assertEqual(self.catched_msg[\"body\"], body)\n+        assert self.catched_msg[\"subject\"] == subject\n+        assert self.catched_msg[\"body\"] == body\n \n         msg = self.catched_msg[\"msg\"]\n-        self.assertEqual(msg[\"subject\"], subject)\n-        self.assertEqual(msg.get_payload(decode=True).decode(\"utf-8\"), body)\n-        self.assertEqual(msg.get_charset(), Charset(\"utf-8\"))\n-        self.assertEqual(msg.get(\"Content-Type\"), 'text/plain; charset=\"utf-8\"')\n+        assert msg[\"subject\"] == subject\n+        assert msg.get_payload(decode=True).decode(\"utf-8\") == body\n+        assert msg.get_charset() == Charset(\"utf-8\")\n+        assert msg.get(\"Content-Type\") == 'text/plain; charset=\"utf-8\"'\n \n     def test_send_attach_utf8(self):\n         subject = \"sübjèçt\"\n@@ -131,22 +130,22 @@ class MailSenderTest(unittest.TestCase):\n         )\n \n         assert self.catched_msg\n-        self.assertEqual(self.catched_msg[\"subject\"], subject)\n-        self.assertEqual(self.catched_msg[\"body\"], body)\n+        assert self.catched_msg[\"subject\"] == subject\n+        assert self.catched_msg[\"body\"] == body\n \n         msg = self.catched_msg[\"msg\"]\n-        self.assertEqual(msg[\"subject\"], subject)\n-        self.assertEqual(msg.get_charset(), Charset(\"utf-8\"))\n-        self.assertEqual(msg.get(\"Content-Type\"), 'multipart/mixed; charset=\"utf-8\"')\n+        assert msg[\"subject\"] == subject\n+        assert msg.get_charset() == Charset(\"utf-8\")\n+        assert msg.get(\"Content-Type\") == 'multipart/mixed; charset=\"utf-8\"'\n \n         payload = msg.get_payload()\n         assert isinstance(payload, list)\n-        self.assertEqual(len(payload), 2)\n+        assert len(payload) == 2\n \n         text, attach = payload\n-        self.assertEqual(text.get_payload(decode=True).decode(\"utf-8\"), body)\n-        self.assertEqual(text.get_charset(), Charset(\"utf-8\"))\n-        self.assertEqual(attach.get_payload(decode=True).decode(\"utf-8\"), body)\n+        assert text.get_payload(decode=True).decode(\"utf-8\") == body\n+        assert text.get_charset() == Charset(\"utf-8\")\n+        assert attach.get_payload(decode=True).decode(\"utf-8\") == body\n \n     def test_create_sender_factory_with_host(self):\n         mailsender = MailSender(debug=False, smtphost=\"smtp.testhost.com\")\n@@ -156,4 +155,4 @@ class MailSenderTest(unittest.TestCase):\n         )\n \n         context = factory.buildProtocol(\"test@scrapy.org\").context\n-        self.assertIsInstance(context, ClientTLSOptions)\n+        assert isinstance(context, ClientTLSOptions)\n\n@@ -1,5 +1,3 @@\n-from twisted.trial import unittest\n-\n from scrapy.exceptions import NotConfigured\n from scrapy.middleware import MiddlewareManager\n from scrapy.utils.test import get_crawler\n@@ -51,37 +49,27 @@ class MyMiddlewareManager(MiddlewareManager):\n             self.methods[\"process\"].append(mw.process)\n \n \n-class MiddlewareManagerTest(unittest.TestCase):\n+class TestMiddlewareManager:\n     def test_init(self):\n         m1, m2, m3 = M1(), M2(), M3()\n         mwman = MyMiddlewareManager(m1, m2, m3)\n-        self.assertEqual(\n-            list(mwman.methods[\"open_spider\"]), [m1.open_spider, m2.open_spider]\n-        )\n-        self.assertEqual(\n-            list(mwman.methods[\"close_spider\"]), [m2.close_spider, m1.close_spider]\n-        )\n-        self.assertEqual(list(mwman.methods[\"process\"]), [m1.process, m3.process])\n+        assert list(mwman.methods[\"open_spider\"]) == [m1.open_spider, m2.open_spider]\n+        assert list(mwman.methods[\"close_spider\"]) == [m2.close_spider, m1.close_spider]\n+        assert list(mwman.methods[\"process\"]) == [m1.process, m3.process]\n \n     def test_methods(self):\n         mwman = MyMiddlewareManager(M1(), M2(), M3())\n-        self.assertEqual(\n-            [x.__self__.__class__ for x in mwman.methods[\"open_spider\"]], [M1, M2]\n-        )\n-        self.assertEqual(\n-            [x.__self__.__class__ for x in mwman.methods[\"close_spider\"]], [M2, M1]\n-        )\n-        self.assertEqual(\n-            [x.__self__.__class__ for x in mwman.methods[\"process\"]], [M1, M3]\n-        )\n+        assert [x.__self__.__class__ for x in mwman.methods[\"open_spider\"]] == [M1, M2]\n+        assert [x.__self__.__class__ for x in mwman.methods[\"close_spider\"]] == [M2, M1]\n+        assert [x.__self__.__class__ for x in mwman.methods[\"process\"]] == [M1, M3]\n \n     def test_enabled(self):\n         m1, m2, m3 = M1(), M2(), M3()\n         mwman = MiddlewareManager(m1, m2, m3)\n-        self.assertEqual(mwman.middlewares, (m1, m2, m3))\n+        assert mwman.middlewares == (m1, m2, m3)\n \n     def test_enabled_from_settings(self):\n         crawler = get_crawler()\n         mwman = MyMiddlewareManager.from_crawler(crawler)\n         classes = [x.__class__ for x in mwman.middlewares]\n-        self.assertEqual(classes, [M1, M3])\n+        assert classes == [M1, M3]\n\n@@ -76,7 +76,7 @@ class ItemSpider(Spider):\n         return {\"field\": 42}\n \n \n-class PipelineTestCase(unittest.TestCase):\n+class TestPipeline(unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls.mockserver = MockServer()\n@@ -87,8 +87,8 @@ class PipelineTestCase(unittest.TestCase):\n         cls.mockserver.__exit__(None, None, None)\n \n     def _on_item_scraped(self, item):\n-        self.assertIsInstance(item, dict)\n-        self.assertTrue(item.get(\"pipeline_passed\"))\n+        assert isinstance(item, dict)\n+        assert item.get(\"pipeline_passed\")\n         self.items.append(item)\n \n     def _create_crawler(self, pipeline_class):\n@@ -104,30 +104,30 @@ class PipelineTestCase(unittest.TestCase):\n     def test_simple_pipeline(self):\n         crawler = self._create_crawler(SimplePipeline)\n         yield crawler.crawl(mockserver=self.mockserver)\n-        self.assertEqual(len(self.items), 1)\n+        assert len(self.items) == 1\n \n     @defer.inlineCallbacks\n     def test_deferred_pipeline(self):\n         crawler = self._create_crawler(DeferredPipeline)\n         yield crawler.crawl(mockserver=self.mockserver)\n-        self.assertEqual(len(self.items), 1)\n+        assert len(self.items) == 1\n \n     @defer.inlineCallbacks\n     def test_asyncdef_pipeline(self):\n         crawler = self._create_crawler(AsyncDefPipeline)\n         yield crawler.crawl(mockserver=self.mockserver)\n-        self.assertEqual(len(self.items), 1)\n+        assert len(self.items) == 1\n \n     @pytest.mark.only_asyncio\n     @defer.inlineCallbacks\n     def test_asyncdef_asyncio_pipeline(self):\n         crawler = self._create_crawler(AsyncDefAsyncioPipeline)\n         yield crawler.crawl(mockserver=self.mockserver)\n-        self.assertEqual(len(self.items), 1)\n+        assert len(self.items) == 1\n \n     @pytest.mark.only_not_asyncio\n     @defer.inlineCallbacks\n     def test_asyncdef_not_asyncio_pipeline(self):\n         crawler = self._create_crawler(AsyncDefNotAsyncioPipeline)\n         yield crawler.crawl(mockserver=self.mockserver)\n-        self.assertEqual(len(self.items), 1)\n+        assert len(self.items) == 1\n\n@@ -1,5 +1,4 @@\n import tempfile\n-import unittest\n \n import pytest\n import queuelib\n@@ -12,8 +11,8 @@ from scrapy.utils.test import get_crawler\n from tests.test_scheduler import MockDownloader, MockEngine\n \n \n-class PriorityQueueTest(unittest.TestCase):\n-    def setUp(self):\n+class TestPriorityQueue:\n+    def setup_method(self):\n         self.crawler = get_crawler(Spider)\n         self.spider = self.crawler._create_spider(\"foo\")\n \n@@ -22,20 +21,20 @@ class PriorityQueueTest(unittest.TestCase):\n         queue = ScrapyPriorityQueue.from_crawler(\n             self.crawler, FifoMemoryQueue, temp_dir\n         )\n-        self.assertIsNone(queue.pop())\n-        self.assertEqual(len(queue), 0)\n+        assert queue.pop() is None\n+        assert len(queue) == 0\n         req1 = Request(\"https://example.org/1\", priority=1)\n         queue.push(req1)\n-        self.assertEqual(len(queue), 1)\n+        assert len(queue) == 1\n         dequeued = queue.pop()\n-        self.assertEqual(len(queue), 0)\n-        self.assertEqual(dequeued.url, req1.url)\n-        self.assertEqual(dequeued.priority, req1.priority)\n-        self.assertEqual(queue.close(), [])\n+        assert len(queue) == 0\n+        assert dequeued.url == req1.url\n+        assert dequeued.priority == req1.priority\n+        assert not queue.close()\n \n     def test_no_peek_raises(self):\n         if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n-            raise unittest.SkipTest(\"queuelib.queue.FifoMemoryQueue.peek is defined\")\n+            pytest.skip(\"queuelib.queue.FifoMemoryQueue.peek is defined\")\n         temp_dir = tempfile.mkdtemp()\n         queue = ScrapyPriorityQueue.from_crawler(\n             self.crawler, FifoMemoryQueue, temp_dir\n@@ -50,53 +49,53 @@ class PriorityQueueTest(unittest.TestCase):\n \n     def test_peek(self):\n         if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n-            raise unittest.SkipTest(\"queuelib.queue.FifoMemoryQueue.peek is undefined\")\n+            pytest.skip(\"queuelib.queue.FifoMemoryQueue.peek is undefined\")\n         temp_dir = tempfile.mkdtemp()\n         queue = ScrapyPriorityQueue.from_crawler(\n             self.crawler, FifoMemoryQueue, temp_dir\n         )\n-        self.assertEqual(len(queue), 0)\n-        self.assertIsNone(queue.peek())\n+        assert len(queue) == 0\n+        assert queue.peek() is None\n         req1 = Request(\"https://example.org/1\")\n         req2 = Request(\"https://example.org/2\")\n         req3 = Request(\"https://example.org/3\")\n         queue.push(req1)\n         queue.push(req2)\n         queue.push(req3)\n-        self.assertEqual(len(queue), 3)\n-        self.assertEqual(queue.peek().url, req1.url)\n-        self.assertEqual(queue.pop().url, req1.url)\n-        self.assertEqual(len(queue), 2)\n-        self.assertEqual(queue.peek().url, req2.url)\n-        self.assertEqual(queue.pop().url, req2.url)\n-        self.assertEqual(len(queue), 1)\n-        self.assertEqual(queue.peek().url, req3.url)\n-        self.assertEqual(queue.pop().url, req3.url)\n-        self.assertEqual(queue.close(), [])\n+        assert len(queue) == 3\n+        assert queue.peek().url == req1.url\n+        assert queue.pop().url == req1.url\n+        assert len(queue) == 2\n+        assert queue.peek().url == req2.url\n+        assert queue.pop().url == req2.url\n+        assert len(queue) == 1\n+        assert queue.peek().url == req3.url\n+        assert queue.pop().url == req3.url\n+        assert not queue.close()\n \n     def test_queue_push_pop_priorities(self):\n         temp_dir = tempfile.mkdtemp()\n         queue = ScrapyPriorityQueue.from_crawler(\n             self.crawler, FifoMemoryQueue, temp_dir, [-1, -2, -3]\n         )\n-        self.assertIsNone(queue.pop())\n-        self.assertEqual(len(queue), 0)\n+        assert queue.pop() is None\n+        assert len(queue) == 0\n         req1 = Request(\"https://example.org/1\", priority=1)\n         req2 = Request(\"https://example.org/2\", priority=2)\n         req3 = Request(\"https://example.org/3\", priority=3)\n         queue.push(req1)\n         queue.push(req2)\n         queue.push(req3)\n-        self.assertEqual(len(queue), 3)\n+        assert len(queue) == 3\n         dequeued = queue.pop()\n-        self.assertEqual(len(queue), 2)\n-        self.assertEqual(dequeued.url, req3.url)\n-        self.assertEqual(dequeued.priority, req3.priority)\n-        self.assertEqual(queue.close(), [-1, -2])\n+        assert len(queue) == 2\n+        assert dequeued.url == req3.url\n+        assert dequeued.priority == req3.priority\n+        assert queue.close() == [-1, -2]\n \n \n-class DownloaderAwarePriorityQueueTest(unittest.TestCase):\n-    def setUp(self):\n+class TestDownloaderAwarePriorityQueue:\n+    def setup_method(self):\n         crawler = get_crawler(Spider)\n         crawler.engine = MockEngine(downloader=MockDownloader())\n         self.queue = DownloaderAwarePriorityQueue.from_crawler(\n@@ -105,30 +104,30 @@ class DownloaderAwarePriorityQueueTest(unittest.TestCase):\n             key=\"foo/bar\",\n         )\n \n-    def tearDown(self):\n+    def teardown_method(self):\n         self.queue.close()\n \n     def test_push_pop(self):\n-        self.assertEqual(len(self.queue), 0)\n-        self.assertIsNone(self.queue.pop())\n+        assert len(self.queue) == 0\n+        assert self.queue.pop() is None\n         req1 = Request(\"http://www.example.com/1\")\n         req2 = Request(\"http://www.example.com/2\")\n         req3 = Request(\"http://www.example.com/3\")\n         self.queue.push(req1)\n         self.queue.push(req2)\n         self.queue.push(req3)\n-        self.assertEqual(len(self.queue), 3)\n-        self.assertEqual(self.queue.pop().url, req1.url)\n-        self.assertEqual(len(self.queue), 2)\n-        self.assertEqual(self.queue.pop().url, req2.url)\n-        self.assertEqual(len(self.queue), 1)\n-        self.assertEqual(self.queue.pop().url, req3.url)\n-        self.assertEqual(len(self.queue), 0)\n-        self.assertIsNone(self.queue.pop())\n+        assert len(self.queue) == 3\n+        assert self.queue.pop().url == req1.url\n+        assert len(self.queue) == 2\n+        assert self.queue.pop().url == req2.url\n+        assert len(self.queue) == 1\n+        assert self.queue.pop().url == req3.url\n+        assert len(self.queue) == 0\n+        assert self.queue.pop() is None\n \n     def test_no_peek_raises(self):\n         if hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n-            raise unittest.SkipTest(\"queuelib.queue.FifoMemoryQueue.peek is defined\")\n+            pytest.skip(\"queuelib.queue.FifoMemoryQueue.peek is defined\")\n         self.queue.push(Request(\"https://example.org\"))\n         with pytest.raises(\n             NotImplementedError,\n@@ -138,21 +137,21 @@ class DownloaderAwarePriorityQueueTest(unittest.TestCase):\n \n     def test_peek(self):\n         if not hasattr(queuelib.queue.FifoMemoryQueue, \"peek\"):\n-            raise unittest.SkipTest(\"queuelib.queue.FifoMemoryQueue.peek is undefined\")\n-        self.assertEqual(len(self.queue), 0)\n+            pytest.skip(\"queuelib.queue.FifoMemoryQueue.peek is undefined\")\n+        assert len(self.queue) == 0\n         req1 = Request(\"https://example.org/1\")\n         req2 = Request(\"https://example.org/2\")\n         req3 = Request(\"https://example.org/3\")\n         self.queue.push(req1)\n         self.queue.push(req2)\n         self.queue.push(req3)\n-        self.assertEqual(len(self.queue), 3)\n-        self.assertEqual(self.queue.peek().url, req1.url)\n-        self.assertEqual(self.queue.pop().url, req1.url)\n-        self.assertEqual(len(self.queue), 2)\n-        self.assertEqual(self.queue.peek().url, req2.url)\n-        self.assertEqual(self.queue.pop().url, req2.url)\n-        self.assertEqual(len(self.queue), 1)\n-        self.assertEqual(self.queue.peek().url, req3.url)\n-        self.assertEqual(self.queue.pop().url, req3.url)\n-        self.assertIsNone(self.queue.peek())\n+        assert len(self.queue) == 3\n+        assert self.queue.peek().url == req1.url\n+        assert self.queue.pop().url == req1.url\n+        assert len(self.queue) == 2\n+        assert self.queue.peek().url == req2.url\n+        assert self.queue.pop().url == req2.url\n+        assert len(self.queue) == 1\n+        assert self.queue.peek().url == req3.url\n+        assert self.queue.pop().url == req3.url\n+        assert self.queue.peek() is None\n\n@@ -6,6 +6,7 @@ from pathlib import Path\n from subprocess import PIPE, Popen\n from urllib.parse import urlsplit, urlunsplit\n \n+import pytest\n from testfixtures import LogCapture\n from twisted.internet import defer\n from twisted.trial.unittest import TestCase\n@@ -61,7 +62,7 @@ def _wrong_credentials(proxy_url):\n     return urlunsplit(bad_auth_proxy)\n \n \n-class ProxyConnectTestCase(TestCase):\n+class TestProxyConnect(TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls.mockserver = MockServer()\n@@ -75,7 +76,7 @@ class ProxyConnectTestCase(TestCase):\n         try:\n             import mitmproxy  # noqa: F401\n         except ImportError:\n-            self.skipTest(\"mitmproxy is not installed\")\n+            pytest.skip(\"mitmproxy is not installed\")\n \n         self._oldenv = os.environ.copy()\n \n@@ -113,12 +114,12 @@ class ProxyConnectTestCase(TestCase):\n             yield crawler.crawl(seed=request)\n         self._assert_got_response_code(200, log)\n         echo = json.loads(crawler.spider.meta[\"responses\"][0].text)\n-        self.assertTrue(\"Proxy-Authorization\" not in echo[\"headers\"])\n+        assert \"Proxy-Authorization\" not in echo[\"headers\"]\n \n     def _assert_got_response_code(self, code, log):\n         print(log)\n-        self.assertEqual(str(log).count(f\"Crawled ({code})\"), 1)\n+        assert str(log).count(f\"Crawled ({code})\") == 1\n \n     def _assert_got_tunnel_error(self, log):\n         print(log)\n-        self.assertIn(\"TunnelError\", str(log))\n+        assert \"TunnelError\" in str(log)\n\n@@ -56,7 +56,7 @@ class AlternativeCallbacksMiddleware:\n         return response.replace(request=new_request)\n \n \n-class CrawlTestCase(TestCase):\n+class TestCrawl(TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls.mockserver = MockServer()\n@@ -72,7 +72,7 @@ class CrawlTestCase(TestCase):\n         crawler = get_crawler(SingleRequestSpider)\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n         response = crawler.spider.meta[\"responses\"][0]\n-        self.assertEqual(response.request.url, url)\n+        assert response.request.url == url\n \n     @defer.inlineCallbacks\n     def test_response_error(self):\n@@ -82,8 +82,8 @@ class CrawlTestCase(TestCase):\n             yield crawler.crawl(seed=url, mockserver=self.mockserver)\n             failure = crawler.spider.meta[\"failure\"]\n             response = failure.value.response\n-            self.assertEqual(failure.request.url, url)\n-            self.assertEqual(response.request.url, url)\n+            assert failure.request.url == url\n+            assert response.request.url == url\n \n     @defer.inlineCallbacks\n     def test_downloader_middleware_raise_exception(self):\n@@ -98,8 +98,8 @@ class CrawlTestCase(TestCase):\n         )\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n         failure = crawler.spider.meta[\"failure\"]\n-        self.assertEqual(failure.request.url, url)\n-        self.assertIsInstance(failure.value, ZeroDivisionError)\n+        assert failure.request.url == url\n+        assert isinstance(failure.value, ZeroDivisionError)\n \n     @defer.inlineCallbacks\n     def test_downloader_middleware_override_request_in_process_response(self):\n@@ -131,10 +131,10 @@ class CrawlTestCase(TestCase):\n             yield crawler.crawl(seed=url, mockserver=self.mockserver)\n \n         response = crawler.spider.meta[\"responses\"][0]\n-        self.assertEqual(response.request.url, OVERRIDDEN_URL)\n+        assert response.request.url == OVERRIDDEN_URL\n \n-        self.assertEqual(signal_params[\"response\"].url, url)\n-        self.assertEqual(signal_params[\"request\"].url, OVERRIDDEN_URL)\n+        assert signal_params[\"response\"].url == url\n+        assert signal_params[\"request\"].url == OVERRIDDEN_URL\n \n         log.check_present(\n             (\n@@ -164,8 +164,8 @@ class CrawlTestCase(TestCase):\n         )\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n         response = crawler.spider.meta[\"responses\"][0]\n-        self.assertEqual(response.body, b\"Caught ZeroDivisionError\")\n-        self.assertEqual(response.request.url, OVERRIDDEN_URL)\n+        assert response.body == b\"Caught ZeroDivisionError\"\n+        assert response.request.url == OVERRIDDEN_URL\n \n     @defer.inlineCallbacks\n     def test_downloader_middleware_do_not_override_in_process_exception(self):\n@@ -187,8 +187,8 @@ class CrawlTestCase(TestCase):\n         )\n         yield crawler.crawl(seed=url, mockserver=self.mockserver)\n         response = crawler.spider.meta[\"responses\"][0]\n-        self.assertEqual(response.body, b\"Caught ZeroDivisionError\")\n-        self.assertEqual(response.request.url, url)\n+        assert response.body == b\"Caught ZeroDivisionError\"\n+        assert response.request.url == url\n \n     @defer.inlineCallbacks\n     def test_downloader_middleware_alternative_callback(self):\n\n@@ -151,7 +151,7 @@ class KeywordArgumentsSpider(MockServerSpider):\n         self.crawler.stats.inc_value(\"boolean_checks\", 1)\n \n \n-class CallbackKeywordArgumentsTestCase(TestCase):\n+class TestCallbackKeywordArguments(TestCase):\n     maxDiff = None\n \n     @classmethod\n@@ -168,27 +168,19 @@ class CallbackKeywordArgumentsTestCase(TestCase):\n         crawler = get_crawler(KeywordArgumentsSpider)\n         with LogCapture() as log:\n             yield crawler.crawl(mockserver=self.mockserver)\n-        self.assertTrue(all(crawler.spider.checks))\n-        self.assertEqual(\n-            len(crawler.spider.checks), crawler.stats.get_value(\"boolean_checks\")\n-        )\n+        assert all(crawler.spider.checks)\n+        assert len(crawler.spider.checks) == crawler.stats.get_value(\"boolean_checks\")\n         # check exceptions for argument mismatch\n         exceptions = {}\n         for line in log.records:\n             for key in (\"takes_less\", \"takes_more\"):\n                 if key in line.getMessage():\n                     exceptions[key] = line\n-        self.assertEqual(exceptions[\"takes_less\"].exc_info[0], TypeError)\n-        self.assertTrue(\n-            str(exceptions[\"takes_less\"].exc_info[1]).endswith(\n+        assert exceptions[\"takes_less\"].exc_info[0] is TypeError\n+        assert str(exceptions[\"takes_less\"].exc_info[1]).endswith(\n             \"parse_takes_less() got an unexpected keyword argument 'number'\"\n-            ),\n-            msg=\"Exception message: \" + str(exceptions[\"takes_less\"].exc_info[1]),\n-        )\n-        self.assertEqual(exceptions[\"takes_more\"].exc_info[0], TypeError)\n-        self.assertTrue(\n-            str(exceptions[\"takes_more\"].exc_info[1]).endswith(\n+        ), \"Exception message: \" + str(exceptions[\"takes_less\"].exc_info[1])\n+        assert exceptions[\"takes_more\"].exc_info[0] is TypeError\n+        assert str(exceptions[\"takes_more\"].exc_info[1]).endswith(\n             \"parse_takes_more() missing 1 required positional argument: 'other'\"\n-            ),\n-            msg=\"Exception message: \" + str(exceptions[\"takes_more\"].exc_info[1]),\n-        )\n+        ), \"Exception message: \" + str(exceptions[\"takes_more\"].exc_info[1])\n\n@@ -1,5 +1,3 @@\n-import unittest\n-\n import pytest\n \n from scrapy import Request, Spider\n@@ -11,8 +9,8 @@ class CustomRequest(Request):\n     pass\n \n \n-class RequestSerializationTest(unittest.TestCase):\n-    def setUp(self):\n+class TestRequestSerialization:\n+    def setup_method(self):\n         self.spider = MethodsSpider()\n \n     def test_basic(self):\n@@ -50,23 +48,23 @@ class RequestSerializationTest(unittest.TestCase):\n         self._assert_same_request(request, request2)\n \n     def _assert_same_request(self, r1, r2):\n-        self.assertEqual(r1.__class__, r2.__class__)\n-        self.assertEqual(r1.url, r2.url)\n-        self.assertEqual(r1.callback, r2.callback)\n-        self.assertEqual(r1.errback, r2.errback)\n-        self.assertEqual(r1.method, r2.method)\n-        self.assertEqual(r1.body, r2.body)\n-        self.assertEqual(r1.headers, r2.headers)\n-        self.assertEqual(r1.cookies, r2.cookies)\n-        self.assertEqual(r1.meta, r2.meta)\n-        self.assertEqual(r1.cb_kwargs, r2.cb_kwargs)\n-        self.assertEqual(r1.encoding, r2.encoding)\n-        self.assertEqual(r1._encoding, r2._encoding)\n-        self.assertEqual(r1.priority, r2.priority)\n-        self.assertEqual(r1.dont_filter, r2.dont_filter)\n-        self.assertEqual(r1.flags, r2.flags)\n+        assert r1.__class__ == r2.__class__\n+        assert r1.url == r2.url\n+        assert r1.callback == r2.callback\n+        assert r1.errback == r2.errback\n+        assert r1.method == r2.method\n+        assert r1.body == r2.body\n+        assert r1.headers == r2.headers\n+        assert r1.cookies == r2.cookies\n+        assert r1.meta == r2.meta\n+        assert r1.cb_kwargs == r2.cb_kwargs\n+        assert r1.encoding == r2.encoding\n+        assert r1._encoding == r2._encoding\n+        assert r1.priority == r2.priority\n+        assert r1.dont_filter == r2.dont_filter\n+        assert r1.flags == r2.flags\n         if isinstance(r1, JsonRequest):\n-            self.assertEqual(r1.dumps_kwargs, r2.dumps_kwargs)\n+            assert r1.dumps_kwargs == r2.dumps_kwargs\n \n     def test_request_class(self):\n         r1 = FormRequest(\"http://www.example.com\")\n@@ -92,8 +90,8 @@ class RequestSerializationTest(unittest.TestCase):\n         )\n         self._assert_serializes_ok(r, spider=self.spider)\n         request_dict = r.to_dict(spider=self.spider)\n-        self.assertEqual(request_dict[\"callback\"], \"parse_item_reference\")\n-        self.assertEqual(request_dict[\"errback\"], \"handle_error_reference\")\n+        assert request_dict[\"callback\"] == \"parse_item_reference\"\n+        assert request_dict[\"errback\"] == \"handle_error_reference\"\n \n     def test_private_reference_callback_serialization(self):\n         r = Request(\n@@ -103,12 +101,8 @@ class RequestSerializationTest(unittest.TestCase):\n         )\n         self._assert_serializes_ok(r, spider=self.spider)\n         request_dict = r.to_dict(spider=self.spider)\n-        self.assertEqual(\n-            request_dict[\"callback\"], \"_MethodsSpider__parse_item_reference\"\n-        )\n-        self.assertEqual(\n-            request_dict[\"errback\"], \"_MethodsSpider__handle_error_reference\"\n-        )\n+        assert request_dict[\"callback\"] == \"_MethodsSpider__parse_item_reference\"\n+        assert request_dict[\"errback\"] == \"_MethodsSpider__handle_error_reference\"\n \n     def test_private_callback_serialization(self):\n         r = Request(\n\n@@ -38,22 +38,22 @@ class TestCatching(TestCase):\n     def test_success(self):\n         crawler = get_crawler(SignalCatcherSpider)\n         yield crawler.crawl(self.mockserver.url(\"/status?n=200\"))\n-        self.assertEqual(crawler.spider.caught_times, 1)\n+        assert crawler.spider.caught_times == 1\n \n     @defer.inlineCallbacks\n     def test_timeout(self):\n         crawler = get_crawler(SignalCatcherSpider, {\"DOWNLOAD_TIMEOUT\": 0.1})\n         yield crawler.crawl(self.mockserver.url(\"/delay?n=0.2\"))\n-        self.assertEqual(crawler.spider.caught_times, 1)\n+        assert crawler.spider.caught_times == 1\n \n     @defer.inlineCallbacks\n     def test_disconnect(self):\n         crawler = get_crawler(SignalCatcherSpider)\n         yield crawler.crawl(self.mockserver.url(\"/drop\"))\n-        self.assertEqual(crawler.spider.caught_times, 1)\n+        assert crawler.spider.caught_times == 1\n \n     @defer.inlineCallbacks\n     def test_noconnect(self):\n         crawler = get_crawler(SignalCatcherSpider)\n         yield crawler.crawl(\"http://thereisdefinetelynosuchdomain.com\")\n-        self.assertEqual(crawler.spider.caught_times, 1)\n+        assert crawler.spider.caught_times == 1\n\n@@ -1,5 +1,3 @@\n-import unittest\n-\n from scrapy.http import (\n     Headers,\n     HtmlResponse,\n@@ -11,7 +9,7 @@ from scrapy.http import (\n from scrapy.responsetypes import responsetypes\n \n \n-class ResponseTypesTest(unittest.TestCase):\n+class TestResponseTypes:\n     def test_from_filename(self):\n         mappings = [\n             (\"data.bin\", Response),\n@@ -123,6 +121,4 @@ class ResponseTypesTest(unittest.TestCase):\n \n     def test_custom_mime_types_loaded(self):\n         # check that mime.types files shipped with scrapy are loaded\n-        self.assertEqual(\n-            responsetypes.mimetypes.guess_type(\"x.scrapytest\")[0], \"x-scrapy/test\"\n-        )\n+        assert responsetypes.mimetypes.guess_type(\"x.scrapytest\")[0] == \"x-scrapy/test\"\n\n@@ -1,4 +1,4 @@\n-from twisted.trial import unittest\n+import pytest\n \n from scrapy.robotstxt import decode_robotstxt\n \n@@ -32,8 +32,8 @@ class BaseRobotParserTest:\n         rp = self.parser_cls.from_crawler(\n             crawler=None, robotstxt_body=robotstxt_robotstxt_body\n         )\n-        self.assertTrue(rp.allowed(\"https://www.site.local/allowed\", \"*\"))\n-        self.assertFalse(rp.allowed(\"https://www.site.local/disallowed\", \"*\"))\n+        assert rp.allowed(\"https://www.site.local/allowed\", \"*\")\n+        assert not rp.allowed(\"https://www.site.local/disallowed\", \"*\")\n \n     def test_allowed_wildcards(self):\n         robotstxt_robotstxt_body = b\"\"\"User-agent: first\n@@ -47,42 +47,36 @@ class BaseRobotParserTest:\n             crawler=None, robotstxt_body=robotstxt_robotstxt_body\n         )\n \n-        self.assertTrue(rp.allowed(\"https://www.site.local/disallowed\", \"first\"))\n-        self.assertFalse(\n-            rp.allowed(\"https://www.site.local/disallowed/xyz/end\", \"first\")\n-        )\n-        self.assertFalse(\n-            rp.allowed(\"https://www.site.local/disallowed/abc/end\", \"first\")\n-        )\n-        self.assertTrue(\n-            rp.allowed(\"https://www.site.local/disallowed/xyz/endinglater\", \"first\")\n-        )\n+        assert rp.allowed(\"https://www.site.local/disallowed\", \"first\")\n+        assert not rp.allowed(\"https://www.site.local/disallowed/xyz/end\", \"first\")\n+        assert not rp.allowed(\"https://www.site.local/disallowed/abc/end\", \"first\")\n+        assert rp.allowed(\"https://www.site.local/disallowed/xyz/endinglater\", \"first\")\n \n-        self.assertTrue(rp.allowed(\"https://www.site.local/allowed\", \"second\"))\n-        self.assertTrue(rp.allowed(\"https://www.site.local/is_still_allowed\", \"second\"))\n-        self.assertTrue(rp.allowed(\"https://www.site.local/is_allowed_too\", \"second\"))\n+        assert rp.allowed(\"https://www.site.local/allowed\", \"second\")\n+        assert rp.allowed(\"https://www.site.local/is_still_allowed\", \"second\")\n+        assert rp.allowed(\"https://www.site.local/is_allowed_too\", \"second\")\n \n     def test_length_based_precedence(self):\n         robotstxt_robotstxt_body = b\"User-agent: * \\nDisallow: / \\nAllow: /page\"\n         rp = self.parser_cls.from_crawler(\n             crawler=None, robotstxt_body=robotstxt_robotstxt_body\n         )\n-        self.assertTrue(rp.allowed(\"https://www.site.local/page\", \"*\"))\n+        assert rp.allowed(\"https://www.site.local/page\", \"*\")\n \n     def test_order_based_precedence(self):\n         robotstxt_robotstxt_body = b\"User-agent: * \\nDisallow: / \\nAllow: /page\"\n         rp = self.parser_cls.from_crawler(\n             crawler=None, robotstxt_body=robotstxt_robotstxt_body\n         )\n-        self.assertFalse(rp.allowed(\"https://www.site.local/page\", \"*\"))\n+        assert not rp.allowed(\"https://www.site.local/page\", \"*\")\n \n     def test_empty_response(self):\n         \"\"\"empty response should equal 'allow all'\"\"\"\n         rp = self.parser_cls.from_crawler(crawler=None, robotstxt_body=b\"\")\n-        self.assertTrue(rp.allowed(\"https://site.local/\", \"*\"))\n-        self.assertTrue(rp.allowed(\"https://site.local/\", \"chrome\"))\n-        self.assertTrue(rp.allowed(\"https://site.local/index.html\", \"*\"))\n-        self.assertTrue(rp.allowed(\"https://site.local/disallowed\", \"*\"))\n+        assert rp.allowed(\"https://site.local/\", \"*\")\n+        assert rp.allowed(\"https://site.local/\", \"chrome\")\n+        assert rp.allowed(\"https://site.local/index.html\", \"*\")\n+        assert rp.allowed(\"https://site.local/disallowed\", \"*\")\n \n     def test_garbage_response(self):\n         \"\"\"garbage response should be discarded, equal 'allow all'\"\"\"\n@@ -90,10 +84,10 @@ class BaseRobotParserTest:\n         rp = self.parser_cls.from_crawler(\n             crawler=None, robotstxt_body=robotstxt_robotstxt_body\n         )\n-        self.assertTrue(rp.allowed(\"https://site.local/\", \"*\"))\n-        self.assertTrue(rp.allowed(\"https://site.local/\", \"chrome\"))\n-        self.assertTrue(rp.allowed(\"https://site.local/index.html\", \"*\"))\n-        self.assertTrue(rp.allowed(\"https://site.local/disallowed\", \"*\"))\n+        assert rp.allowed(\"https://site.local/\", \"*\")\n+        assert rp.allowed(\"https://site.local/\", \"chrome\")\n+        assert rp.allowed(\"https://site.local/index.html\", \"*\")\n+        assert rp.allowed(\"https://site.local/disallowed\", \"*\")\n \n     def test_unicode_url_and_useragent(self):\n         robotstxt_robotstxt_body = \"\"\"\n@@ -109,79 +103,67 @@ class BaseRobotParserTest:\n         rp = self.parser_cls.from_crawler(\n             crawler=None, robotstxt_body=robotstxt_robotstxt_body\n         )\n-        self.assertTrue(rp.allowed(\"https://site.local/\", \"*\"))\n-        self.assertFalse(rp.allowed(\"https://site.local/admin/\", \"*\"))\n-        self.assertFalse(rp.allowed(\"https://site.local/static/\", \"*\"))\n-        self.assertTrue(rp.allowed(\"https://site.local/admin/\", \"UnicödeBöt\"))\n-        self.assertFalse(\n-            rp.allowed(\"https://site.local/wiki/K%C3%A4ytt%C3%A4j%C3%A4:\", \"*\")\n-        )\n-        self.assertFalse(rp.allowed(\"https://site.local/wiki/Käyttäjä:\", \"*\"))\n-        self.assertTrue(rp.allowed(\"https://site.local/some/randome/page.html\", \"*\"))\n-        self.assertFalse(\n-            rp.allowed(\"https://site.local/some/randome/page.html\", \"UnicödeBöt\")\n-        )\n+        assert rp.allowed(\"https://site.local/\", \"*\")\n+        assert not rp.allowed(\"https://site.local/admin/\", \"*\")\n+        assert not rp.allowed(\"https://site.local/static/\", \"*\")\n+        assert rp.allowed(\"https://site.local/admin/\", \"UnicödeBöt\")\n+        assert not rp.allowed(\"https://site.local/wiki/K%C3%A4ytt%C3%A4j%C3%A4:\", \"*\")\n+        assert not rp.allowed(\"https://site.local/wiki/Käyttäjä:\", \"*\")\n+        assert rp.allowed(\"https://site.local/some/randome/page.html\", \"*\")\n+        assert not rp.allowed(\"https://site.local/some/randome/page.html\", \"UnicödeBöt\")\n \n \n-class DecodeRobotsTxtTest(unittest.TestCase):\n+class TestDecodeRobotsTxt:\n     def test_native_string_conversion(self):\n         robotstxt_body = b\"User-agent: *\\nDisallow: /\\n\"\n         decoded_content = decode_robotstxt(\n             robotstxt_body, spider=None, to_native_str_type=True\n         )\n-        self.assertEqual(decoded_content, \"User-agent: *\\nDisallow: /\\n\")\n+        assert decoded_content == \"User-agent: *\\nDisallow: /\\n\"\n \n     def test_decode_utf8(self):\n         robotstxt_body = b\"User-agent: *\\nDisallow: /\\n\"\n         decoded_content = decode_robotstxt(robotstxt_body, spider=None)\n-        self.assertEqual(decoded_content, \"User-agent: *\\nDisallow: /\\n\")\n+        assert decoded_content == \"User-agent: *\\nDisallow: /\\n\"\n \n     def test_decode_non_utf8(self):\n         robotstxt_body = b\"User-agent: *\\n\\xffDisallow: /\\n\"\n         decoded_content = decode_robotstxt(robotstxt_body, spider=None)\n-        self.assertEqual(decoded_content, \"User-agent: *\\nDisallow: /\\n\")\n+        assert decoded_content == \"User-agent: *\\nDisallow: /\\n\"\n \n \n-class PythonRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n-    def setUp(self):\n+class TestPythonRobotParser(BaseRobotParserTest):\n+    def setup_method(self):\n         from scrapy.robotstxt import PythonRobotParser\n \n         super()._setUp(PythonRobotParser)\n \n     def test_length_based_precedence(self):\n-        raise unittest.SkipTest(\n+        pytest.skip(\n             \"RobotFileParser does not support length based directives precedence.\"\n         )\n \n     def test_allowed_wildcards(self):\n-        raise unittest.SkipTest(\"RobotFileParser does not support wildcards.\")\n+        pytest.skip(\"RobotFileParser does not support wildcards.\")\n \n \n-class RerpRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n-    if not rerp_available():\n-        skip = \"Rerp parser is not installed\"\n-\n-    def setUp(self):\n+@pytest.mark.skipif(not rerp_available(), reason=\"Rerp parser is not installed\")\n+class TestRerpRobotParser(BaseRobotParserTest):\n+    def setup_method(self):\n         from scrapy.robotstxt import RerpRobotParser\n \n         super()._setUp(RerpRobotParser)\n \n     def test_length_based_precedence(self):\n-        raise unittest.SkipTest(\n-            \"Rerp does not support length based directives precedence.\"\n-        )\n+        pytest.skip(\"Rerp does not support length based directives precedence.\")\n \n \n-class ProtegoRobotParserTest(BaseRobotParserTest, unittest.TestCase):\n-    if not protego_available():\n-        skip = \"Protego parser is not installed\"\n-\n-    def setUp(self):\n+@pytest.mark.skipif(not protego_available(), reason=\"Protego parser is not installed\")\n+class TestProtegoRobotParser(BaseRobotParserTest):\n+    def setup_method(self):\n         from scrapy.robotstxt import ProtegoRobotParser\n \n         super()._setUp(ProtegoRobotParser)\n \n     def test_order_based_precedence(self):\n-        raise unittest.SkipTest(\n-            \"Protego does not support order based directives precedence.\"\n-        )\n+        pytest.skip(\"Protego does not support order based directives precedence.\")\n\n@@ -2,7 +2,7 @@ from __future__ import annotations\n \n import shutil\n import tempfile\n-import unittest\n+from abc import ABC, abstractmethod\n from typing import Any, NamedTuple\n \n import pytest\n@@ -65,10 +65,14 @@ class MockCrawler(Crawler):\n         self.stats = load_object(self.settings[\"STATS_CLASS\"])(self)\n \n \n-class SchedulerHandler:\n-    priority_queue_cls: str | None = None\n+class SchedulerHandler(ABC):\n     jobdir = None\n \n+    @property\n+    @abstractmethod\n+    def priority_queue_cls(self) -> str:\n+        raise NotImplementedError\n+\n     def create_scheduler(self):\n         self.mock_crawler = MockCrawler(self.priority_queue_cls, self.jobdir)\n         self.scheduler = Scheduler.from_crawler(self.mock_crawler)\n@@ -80,10 +84,10 @@ class SchedulerHandler:\n         self.mock_crawler.stop()\n         self.mock_crawler.engine.downloader.close()\n \n-    def setUp(self):\n+    def setup_method(self):\n         self.create_scheduler()\n \n-    def tearDown(self):\n+    def teardown_method(self):\n         self.close_scheduler()\n \n \n@@ -99,16 +103,16 @@ _PRIORITIES = [\n _URLS = {\"http://foo.com/a\", \"http://foo.com/b\", \"http://foo.com/c\"}\n \n \n-class BaseSchedulerInMemoryTester(SchedulerHandler):\n+class TestSchedulerInMemoryBase(SchedulerHandler):\n     def test_length(self):\n-        self.assertFalse(self.scheduler.has_pending_requests())\n-        self.assertEqual(len(self.scheduler), 0)\n+        assert not self.scheduler.has_pending_requests()\n+        assert len(self.scheduler) == 0\n \n         for url in _URLS:\n             self.scheduler.enqueue_request(Request(url))\n \n-        self.assertTrue(self.scheduler.has_pending_requests())\n-        self.assertEqual(len(self.scheduler), len(_URLS))\n+        assert self.scheduler.has_pending_requests()\n+        assert len(self.scheduler) == len(_URLS)\n \n     def test_dequeue(self):\n         for url in _URLS:\n@@ -118,7 +122,7 @@ class BaseSchedulerInMemoryTester(SchedulerHandler):\n         while self.scheduler.has_pending_requests():\n             urls.add(self.scheduler.next_request().url)\n \n-        self.assertEqual(urls, _URLS)\n+        assert urls == _URLS\n \n     def test_dequeue_priorities(self):\n         for url, priority in _PRIORITIES:\n@@ -128,25 +132,23 @@ class BaseSchedulerInMemoryTester(SchedulerHandler):\n         while self.scheduler.has_pending_requests():\n             priorities.append(self.scheduler.next_request().priority)\n \n-        self.assertEqual(\n-            priorities, sorted([x[1] for x in _PRIORITIES], key=lambda x: -x)\n-        )\n+        assert priorities == sorted([x[1] for x in _PRIORITIES], key=lambda x: -x)\n \n \n-class BaseSchedulerOnDiskTester(SchedulerHandler):\n-    def setUp(self):\n+class TestSchedulerOnDiskBase(SchedulerHandler):\n+    def setup_method(self):\n         self.jobdir = tempfile.mkdtemp()\n         self.create_scheduler()\n \n-    def tearDown(self):\n+    def teardown_method(self):\n         self.close_scheduler()\n \n         shutil.rmtree(self.jobdir)\n         self.jobdir = None\n \n     def test_length(self):\n-        self.assertFalse(self.scheduler.has_pending_requests())\n-        self.assertEqual(len(self.scheduler), 0)\n+        assert not self.scheduler.has_pending_requests()\n+        assert len(self.scheduler) == 0\n \n         for url in _URLS:\n             self.scheduler.enqueue_request(Request(url))\n@@ -154,8 +156,8 @@ class BaseSchedulerOnDiskTester(SchedulerHandler):\n         self.close_scheduler()\n         self.create_scheduler()\n \n-        self.assertTrue(self.scheduler.has_pending_requests())\n-        self.assertEqual(len(self.scheduler), len(_URLS))\n+        assert self.scheduler.has_pending_requests()\n+        assert len(self.scheduler) == len(_URLS)\n \n     def test_dequeue(self):\n         for url in _URLS:\n@@ -168,7 +170,7 @@ class BaseSchedulerOnDiskTester(SchedulerHandler):\n         while self.scheduler.has_pending_requests():\n             urls.add(self.scheduler.next_request().url)\n \n-        self.assertEqual(urls, _URLS)\n+        assert urls == _URLS\n \n     def test_dequeue_priorities(self):\n         for url, priority in _PRIORITIES:\n@@ -181,17 +183,19 @@ class BaseSchedulerOnDiskTester(SchedulerHandler):\n         while self.scheduler.has_pending_requests():\n             priorities.append(self.scheduler.next_request().priority)\n \n-        self.assertEqual(\n-            priorities, sorted([x[1] for x in _PRIORITIES], key=lambda x: -x)\n-        )\n+        assert priorities == sorted([x[1] for x in _PRIORITIES], key=lambda x: -x)\n \n \n-class TestSchedulerInMemory(BaseSchedulerInMemoryTester, unittest.TestCase):\n-    priority_queue_cls = \"scrapy.pqueues.ScrapyPriorityQueue\"\n+class TestSchedulerInMemory(TestSchedulerInMemoryBase):\n+    @property\n+    def priority_queue_cls(self) -> str:\n+        return \"scrapy.pqueues.ScrapyPriorityQueue\"\n \n \n-class TestSchedulerOnDisk(BaseSchedulerOnDiskTester, unittest.TestCase):\n-    priority_queue_cls = \"scrapy.pqueues.ScrapyPriorityQueue\"\n+class TestSchedulerOnDisk(TestSchedulerOnDiskBase):\n+    @property\n+    def priority_queue_cls(self) -> str:\n+        return \"scrapy.pqueues.ScrapyPriorityQueue\"\n \n \n _URLS_WITH_SLOTS = [\n@@ -204,37 +208,34 @@ _URLS_WITH_SLOTS = [\n ]\n \n \n-class TestMigration(unittest.TestCase):\n-    def setUp(self):\n-        self.tmpdir = tempfile.mkdtemp()\n+class TestMigration:\n+    def test_migration(self, tmpdir):\n+        class PrevSchedulerHandler(SchedulerHandler):\n+            jobdir = tmpdir\n \n-    def tearDown(self):\n-        shutil.rmtree(self.tmpdir)\n+            @property\n+            def priority_queue_cls(self) -> str:\n+                return \"scrapy.pqueues.ScrapyPriorityQueue\"\n \n-    def _migration(self, tmp_dir):\n-        prev_scheduler_handler = SchedulerHandler()\n-        prev_scheduler_handler.priority_queue_cls = \"scrapy.pqueues.ScrapyPriorityQueue\"\n-        prev_scheduler_handler.jobdir = tmp_dir\n+        class NextSchedulerHandler(SchedulerHandler):\n+            jobdir = tmpdir\n \n+            @property\n+            def priority_queue_cls(self) -> str:\n+                return \"scrapy.pqueues.DownloaderAwarePriorityQueue\"\n+\n+        prev_scheduler_handler = PrevSchedulerHandler()\n         prev_scheduler_handler.create_scheduler()\n         for url in _URLS:\n             prev_scheduler_handler.scheduler.enqueue_request(Request(url))\n         prev_scheduler_handler.close_scheduler()\n \n-        next_scheduler_handler = SchedulerHandler()\n-        next_scheduler_handler.priority_queue_cls = (\n-            \"scrapy.pqueues.DownloaderAwarePriorityQueue\"\n-        )\n-        next_scheduler_handler.jobdir = tmp_dir\n-\n-        next_scheduler_handler.create_scheduler()\n-\n-    def test_migration(self):\n+        next_scheduler_handler = NextSchedulerHandler()\n         with pytest.raises(\n             ValueError,\n             match=\"DownloaderAwarePriorityQueue accepts ``slot_startprios`` as a dict\",\n         ):\n-            self._migration(self.tmpdir)\n+            next_scheduler_handler.create_scheduler()\n \n \n def _is_scheduling_fair(enqueued_slots, dequeued_slots):\n@@ -263,9 +264,12 @@ def _is_scheduling_fair(enqueued_slots, dequeued_slots):\n \n \n class DownloaderAwareSchedulerTestMixin:\n-    priority_queue_cls: str | None = \"scrapy.pqueues.DownloaderAwarePriorityQueue\"\n     reopen = False\n \n+    @property\n+    def priority_queue_cls(self) -> str:\n+        return \"scrapy.pqueues.DownloaderAwarePriorityQueue\"\n+\n     def test_logic(self):\n         for url, slot in _URLS_WITH_SLOTS:\n             request = Request(url)\n@@ -290,20 +294,18 @@ class DownloaderAwareSchedulerTestMixin:\n             slot = downloader.get_slot_key(request)\n             downloader.decrement(slot)\n \n-        self.assertTrue(\n-            _is_scheduling_fair([s for u, s in _URLS_WITH_SLOTS], dequeued_slots)\n-        )\n-        self.assertEqual(sum(len(s.active) for s in downloader.slots.values()), 0)\n+        assert _is_scheduling_fair([s for u, s in _URLS_WITH_SLOTS], dequeued_slots)\n+        assert sum(len(s.active) for s in downloader.slots.values()) == 0\n \n \n class TestSchedulerWithDownloaderAwareInMemory(\n-    DownloaderAwareSchedulerTestMixin, BaseSchedulerInMemoryTester, unittest.TestCase\n+    DownloaderAwareSchedulerTestMixin, TestSchedulerInMemoryBase\n ):\n     pass\n \n \n class TestSchedulerWithDownloaderAwareOnDisk(\n-    DownloaderAwareSchedulerTestMixin, BaseSchedulerOnDiskTester, unittest.TestCase\n+    DownloaderAwareSchedulerTestMixin, TestSchedulerOnDiskBase\n ):\n     reopen = True\n \n@@ -337,13 +339,12 @@ class TestIntegrationWithDownloaderAwareInMemory(TestCase):\n             url = mockserver.url(\"/status?n=200\", is_secure=False)\n             start_urls = [url] * 6\n             yield self.crawler.crawl(start_urls)\n-            self.assertEqual(\n-                self.crawler.stats.get_value(\"downloader/response_count\"),\n-                len(start_urls),\n+            assert self.crawler.stats.get_value(\"downloader/response_count\") == len(\n+                start_urls\n             )\n \n \n-class TestIncompatibility(unittest.TestCase):\n+class TestIncompatibility:\n     def _incompatible(self):\n         settings = {\n             \"SCHEDULER_PRIORITY_QUEUE\": \"scrapy.pqueues.DownloaderAwarePriorityQueue\",\n\n@@ -1,12 +1,11 @@\n from __future__ import annotations\n \n-from unittest import TestCase\n from urllib.parse import urljoin\n \n import pytest\n from testfixtures import LogCapture\n from twisted.internet import defer\n-from twisted.trial.unittest import TestCase as TwistedTestCase\n+from twisted.trial.unittest import TestCase\n \n from scrapy.core.scheduler import BaseScheduler\n from scrapy.http import Request\n@@ -65,17 +64,17 @@ class PathsSpider(Spider):\n \n class InterfaceCheckMixin:\n     def test_scheduler_class(self):\n-        self.assertTrue(isinstance(self.scheduler, BaseScheduler))\n-        self.assertTrue(issubclass(self.scheduler.__class__, BaseScheduler))\n+        assert isinstance(self.scheduler, BaseScheduler)\n+        assert issubclass(self.scheduler.__class__, BaseScheduler)\n \n \n-class BaseSchedulerTest(TestCase, InterfaceCheckMixin):\n-    def setUp(self):\n+class TestBaseScheduler(InterfaceCheckMixin):\n+    def setup_method(self):\n         self.scheduler = BaseScheduler()\n \n     def test_methods(self):\n-        self.assertIsNone(self.scheduler.open(Spider(\"foo\")))\n-        self.assertIsNone(self.scheduler.close(\"finished\"))\n+        assert self.scheduler.open(Spider(\"foo\")) is None\n+        assert self.scheduler.close(\"finished\") is None\n         with pytest.raises(NotImplementedError):\n             self.scheduler.has_pending_requests()\n         with pytest.raises(NotImplementedError):\n@@ -84,8 +83,8 @@ class BaseSchedulerTest(TestCase, InterfaceCheckMixin):\n             self.scheduler.next_request()\n \n \n-class MinimalSchedulerTest(TestCase, InterfaceCheckMixin):\n-    def setUp(self):\n+class TestMinimalScheduler(InterfaceCheckMixin):\n+    def setup_method(self):\n         self.scheduler = MinimalScheduler()\n \n     def test_open_close(self):\n@@ -101,51 +100,51 @@ class MinimalSchedulerTest(TestCase, InterfaceCheckMixin):\n             len(self.scheduler)\n \n     def test_enqueue_dequeue(self):\n-        self.assertFalse(self.scheduler.has_pending_requests())\n+        assert not self.scheduler.has_pending_requests()\n         for url in URLS:\n-            self.assertTrue(self.scheduler.enqueue_request(Request(url)))\n-            self.assertFalse(self.scheduler.enqueue_request(Request(url)))\n-        self.assertTrue(self.scheduler.has_pending_requests)\n+            assert self.scheduler.enqueue_request(Request(url))\n+            assert not self.scheduler.enqueue_request(Request(url))\n+        assert self.scheduler.has_pending_requests\n \n         dequeued = []\n         while self.scheduler.has_pending_requests():\n             request = self.scheduler.next_request()\n             dequeued.append(request.url)\n-        self.assertEqual(set(dequeued), set(URLS))\n-        self.assertFalse(self.scheduler.has_pending_requests())\n+        assert set(dequeued) == set(URLS)\n+        assert not self.scheduler.has_pending_requests()\n \n \n-class SimpleSchedulerTest(TwistedTestCase, InterfaceCheckMixin):\n+class SimpleSchedulerTest(TestCase, InterfaceCheckMixin):\n     def setUp(self):\n         self.scheduler = SimpleScheduler()\n \n     @defer.inlineCallbacks\n     def test_enqueue_dequeue(self):\n         open_result = yield self.scheduler.open(Spider(\"foo\"))\n-        self.assertEqual(open_result, \"open\")\n-        self.assertFalse(self.scheduler.has_pending_requests())\n+        assert open_result == \"open\"\n+        assert not self.scheduler.has_pending_requests()\n \n         for url in URLS:\n-            self.assertTrue(self.scheduler.enqueue_request(Request(url)))\n-            self.assertFalse(self.scheduler.enqueue_request(Request(url)))\n+            assert self.scheduler.enqueue_request(Request(url))\n+            assert not self.scheduler.enqueue_request(Request(url))\n \n-        self.assertTrue(self.scheduler.has_pending_requests())\n-        self.assertEqual(len(self.scheduler), len(URLS))\n+        assert self.scheduler.has_pending_requests()\n+        assert len(self.scheduler) == len(URLS)\n \n         dequeued = []\n         while self.scheduler.has_pending_requests():\n             request = self.scheduler.next_request()\n             dequeued.append(request.url)\n-        self.assertEqual(set(dequeued), set(URLS))\n+        assert set(dequeued) == set(URLS)\n \n-        self.assertFalse(self.scheduler.has_pending_requests())\n-        self.assertEqual(len(self.scheduler), 0)\n+        assert not self.scheduler.has_pending_requests()\n+        assert len(self.scheduler) == 0\n \n         close_result = yield self.scheduler.close(\"\")\n-        self.assertEqual(close_result, \"close\")\n+        assert close_result == \"close\"\n \n \n-class MinimalSchedulerCrawlTest(TwistedTestCase):\n+class MinimalSchedulerCrawlTest(TestCase):\n     scheduler_cls = MinimalScheduler\n \n     @defer.inlineCallbacks\n@@ -158,8 +157,8 @@ class MinimalSchedulerCrawlTest(TwistedTestCase):\n                 crawler = get_crawler(PathsSpider, settings)\n                 yield crawler.crawl(mockserver)\n             for path in PATHS:\n-                self.assertIn(f\"{{'path': '{path}'}}\", str(log))\n-            self.assertIn(f\"'item_scraped_count': {len(PATHS)}\", str(log))\n+                assert f\"{{'path': '{path}'}}\" in str(log)\n+            assert f\"'item_scraped_count': {len(PATHS)}\" in str(log)\n \n \n class SimpleSchedulerCrawlTest(MinimalSchedulerCrawlTest):\n\n@@ -3,7 +3,6 @@ import weakref\n import parsel\n import pytest\n from packaging import version\n-from twisted.trial import unittest\n \n from scrapy.http import HtmlResponse, TextResponse, XmlResponse\n from scrapy.selector import Selector\n@@ -12,7 +11,7 @@ PARSEL_VERSION = version.parse(getattr(parsel, \"__version__\", \"0.0\"))\n PARSEL_18_PLUS = PARSEL_VERSION >= version.parse(\"1.8.0\")\n \n \n-class SelectorTestCase(unittest.TestCase):\n+class TestSelector:\n     def test_simple_selection(self):\n         \"\"\"Simple selector tests\"\"\"\n         body = b\"<p><input name='a'value='1'/><input name='b'value='2'/></p>\"\n@@ -20,57 +19,46 @@ class SelectorTestCase(unittest.TestCase):\n         sel = Selector(response)\n \n         xl = sel.xpath(\"//input\")\n-        self.assertEqual(2, len(xl))\n+        assert len(xl) == 2\n         for x in xl:\n             assert isinstance(x, Selector)\n \n-        self.assertEqual(\n-            sel.xpath(\"//input\").getall(), [x.get() for x in sel.xpath(\"//input\")]\n-        )\n-        self.assertEqual(\n-            [x.get() for x in sel.xpath(\"//input[@name='a']/@name\")], [\"a\"]\n-        )\n-        self.assertEqual(\n-            [\n+        assert sel.xpath(\"//input\").getall() == [x.get() for x in sel.xpath(\"//input\")]\n+        assert [x.get() for x in sel.xpath(\"//input[@name='a']/@name\")] == [\"a\"]\n+        assert [\n             x.get()\n             for x in sel.xpath(\n                 \"number(concat(//input[@name='a']/@value, //input[@name='b']/@value))\"\n             )\n-            ],\n-            [\"12.0\"],\n-        )\n-        self.assertEqual(sel.xpath(\"concat('xpath', 'rules')\").getall(), [\"xpathrules\"])\n-        self.assertEqual(\n-            [\n+        ] == [\"12.0\"]\n+        assert sel.xpath(\"concat('xpath', 'rules')\").getall() == [\"xpathrules\"]\n+        assert [\n             x.get()\n             for x in sel.xpath(\n                 \"concat(//input[@name='a']/@value, //input[@name='b']/@value)\"\n             )\n-            ],\n-            [\"12\"],\n-        )\n+        ] == [\"12\"]\n \n     def test_root_base_url(self):\n         body = b'<html><form action=\"/path\"><input name=\"a\" /></form></html>'\n         url = \"http://example.com\"\n         response = TextResponse(url=url, body=body, encoding=\"utf-8\")\n         sel = Selector(response)\n-        self.assertEqual(url, sel.root.base)\n+        assert url == sel.root.base\n \n     def test_flavor_detection(self):\n         text = b'<div><img src=\"a.jpg\"><p>Hello</div>'\n         sel = Selector(XmlResponse(\"http://example.com\", body=text, encoding=\"utf-8\"))\n-        self.assertEqual(sel.type, \"xml\")\n-        self.assertEqual(\n-            sel.xpath(\"//div\").getall(),\n-            ['<div><img src=\"a.jpg\"><p>Hello</p></img></div>'],\n-        )\n+        assert sel.type == \"xml\"\n+        assert sel.xpath(\"//div\").getall() == [\n+            '<div><img src=\"a.jpg\"><p>Hello</p></img></div>'\n+        ]\n \n         sel = Selector(HtmlResponse(\"http://example.com\", body=text, encoding=\"utf-8\"))\n-        self.assertEqual(sel.type, \"html\")\n-        self.assertEqual(\n-            sel.xpath(\"//div\").getall(), ['<div><img src=\"a.jpg\"><p>Hello</p></div>']\n-        )\n+        assert sel.type == \"html\"\n+        assert sel.xpath(\"//div\").getall() == [\n+            '<div><img src=\"a.jpg\"><p>Hello</p></div>'\n+        ]\n \n     def test_http_header_encoding_precedence(self):\n         # '\\xa3'     = pound symbol in unicode\n@@ -92,7 +80,7 @@ class SelectorTestCase(unittest.TestCase):\n             url=\"http://example.com\", headers=headers, body=html_utf8\n         )\n         x = Selector(response)\n-        self.assertEqual(x.xpath(\"//span[@id='blank']/text()\").getall(), [\"\\xa3\"])\n+        assert x.xpath(\"//span[@id='blank']/text()\").getall() == [\"\\xa3\"]\n \n     def test_badly_encoded_body(self):\n         # \\xe9 alone isn't valid utf8 sequence\n@@ -116,7 +104,7 @@ class SelectorTestCase(unittest.TestCase):\n             Selector(TextResponse(url=\"http://example.com\", body=b\"\"), text=\"\")\n \n \n-class JMESPathTestCase(unittest.TestCase):\n+class TestJMESPath:\n     @pytest.mark.skipif(\n         not PARSEL_18_PLUS, reason=\"parsel < 1.8 doesn't support jmespath\"\n     )\n@@ -149,16 +137,13 @@ class JMESPathTestCase(unittest.TestCase):\n         }\n         \"\"\"\n         resp = TextResponse(url=\"http://example.com\", body=body, encoding=\"utf-8\")\n-        self.assertEqual(\n-            resp.jmespath(\"html\").get(),\n-            \"<div><a>a<br>b</a>c</div><div><a>d</a>e<b>f</b></div>\",\n+        assert (\n+            resp.jmespath(\"html\").get()\n+            == \"<div><a>a<br>b</a>c</div><div><a>d</a>e<b>f</b></div>\"\n         )\n-        self.assertEqual(\n-            resp.jmespath(\"html\").xpath(\"//div/a/text()\").getall(),\n-            [\"a\", \"b\", \"d\"],\n-        )\n-        self.assertEqual(resp.jmespath(\"html\").css(\"div > b\").getall(), [\"<b>f</b>\"])\n-        self.assertEqual(resp.jmespath(\"content\").jmespath(\"name.age\").get(), \"18\")\n+        assert resp.jmespath(\"html\").xpath(\"//div/a/text()\").getall() == [\"a\", \"b\", \"d\"]\n+        assert resp.jmespath(\"html\").css(\"div > b\").getall() == [\"<b>f</b>\"]\n+        assert resp.jmespath(\"content\").jmespath(\"name.age\").get() == \"18\"\n \n     @pytest.mark.skipif(\n         not PARSEL_18_PLUS, reason=\"parsel < 1.8 doesn't support jmespath\"\n@@ -194,15 +179,19 @@ class JMESPathTestCase(unittest.TestCase):\n         </div>\n         \"\"\"\n         resp = TextResponse(url=\"http://example.com\", body=body, encoding=\"utf-8\")\n-        self.assertEqual(\n-            resp.xpath(\"//div/content/text()\").jmespath(\"user[*].name\").getall(),\n-            [\"A\", \"B\", \"C\", \"D\"],\n-        )\n-        self.assertEqual(\n-            resp.xpath(\"//div/content\").jmespath(\"user[*].name\").getall(),\n-            [\"A\", \"B\", \"C\", \"D\"],\n-        )\n-        self.assertEqual(resp.xpath(\"//div/content\").jmespath(\"total\").get(), \"4\")\n+        assert resp.xpath(\"//div/content/text()\").jmespath(\"user[*].name\").getall() == [\n+            \"A\",\n+            \"B\",\n+            \"C\",\n+            \"D\",\n+        ]\n+        assert resp.xpath(\"//div/content\").jmespath(\"user[*].name\").getall() == [\n+            \"A\",\n+            \"B\",\n+            \"C\",\n+            \"D\",\n+        ]\n+        assert resp.xpath(\"//div/content\").jmespath(\"total\").get() == \"4\"\n \n     @pytest.mark.skipif(\n         not PARSEL_18_PLUS, reason=\"parsel < 1.8 doesn't support jmespath\"\n@@ -238,30 +227,26 @@ class JMESPathTestCase(unittest.TestCase):\n             </div>\n             \"\"\"\n         resp = TextResponse(url=\"http://example.com\", body=body, encoding=\"utf-8\")\n-        self.assertEqual(\n-            resp.xpath(\"//div/content/text()\").jmespath(\"user[*].name\").re(r\"(\\w+)\"),\n-            [\"A\", \"B\", \"C\", \"D\"],\n-        )\n-        self.assertEqual(\n-            resp.xpath(\"//div/content\").jmespath(\"user[*].name\").re(r\"(\\w+)\"),\n-            [\"A\", \"B\", \"C\", \"D\"],\n+        assert resp.xpath(\"//div/content/text()\").jmespath(\"user[*].name\").re(\n+            r\"(\\w+)\"\n+        ) == [\"A\", \"B\", \"C\", \"D\"]\n+        assert resp.xpath(\"//div/content\").jmespath(\"user[*].name\").re(r\"(\\w+)\") == [\n+            \"A\",\n+            \"B\",\n+            \"C\",\n+            \"D\",\n+        ]\n+\n+        assert resp.xpath(\"//div/content\").jmespath(\"unavailable\").re(r\"(\\d+)\") == []\n+\n+        assert (\n+            resp.xpath(\"//div/content\").jmespath(\"unavailable\").re_first(r\"(\\d+)\")\n+            is None\n         )\n \n-        self.assertEqual(\n-            resp.xpath(\"//div/content\").jmespath(\"unavailable\").re(r\"(\\d+)\"), []\n-        )\n-\n-        self.assertEqual(\n-            resp.xpath(\"//div/content\").jmespath(\"unavailable\").re_first(r\"(\\d+)\"),\n-            None,\n-        )\n-\n-        self.assertEqual(\n-            resp.xpath(\"//div/content\")\n-            .jmespath(\"user[*].age.to_string(@)\")\n-            .re(r\"(\\d+)\"),\n-            [\"18\", \"32\", \"22\", \"25\"],\n-        )\n+        assert resp.xpath(\"//div/content\").jmespath(\"user[*].age.to_string(@)\").re(\n+            r\"(\\d+)\"\n+        ) == [\"18\", \"32\", \"22\", \"25\"]\n \n     @pytest.mark.skipif(PARSEL_18_PLUS, reason=\"parsel >= 1.8 supports jmespath\")\n     def test_jmespath_not_available(self) -> None:\n\n@@ -20,7 +20,7 @@ class ItemSpider(Spider):\n         return {\"index\": response.meta[\"index\"]}\n \n \n-class AsyncSignalTestCase(unittest.TestCase):\n+class TestAsyncSignal(unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls.mockserver = MockServer()\n@@ -43,6 +43,6 @@ class AsyncSignalTestCase(unittest.TestCase):\n         crawler = get_crawler(ItemSpider)\n         crawler.signals.connect(self._on_item_scraped, signals.item_scraped)\n         yield crawler.crawl(mockserver=self.mockserver)\n-        self.assertEqual(len(self.items), 10)\n+        assert len(self.items) == 10\n         for index in range(10):\n-            self.assertIn({\"index\": index}, self.items)\n+            assert {\"index\": index} in self.items\n\n@@ -1,33 +1,31 @@\n-from unittest import TestCase\n-\n import scrapy\n \n \n-class ToplevelTestCase(TestCase):\n+class TestToplevel:\n     def test_version(self):\n-        self.assertIs(type(scrapy.__version__), str)\n+        assert isinstance(scrapy.__version__, str)\n \n     def test_version_info(self):\n-        self.assertIs(type(scrapy.version_info), tuple)\n+        assert isinstance(scrapy.version_info, tuple)\n \n     def test_request_shortcut(self):\n         from scrapy.http import FormRequest, Request\n \n-        self.assertIs(scrapy.Request, Request)\n-        self.assertIs(scrapy.FormRequest, FormRequest)\n+        assert scrapy.Request is Request\n+        assert scrapy.FormRequest is FormRequest\n \n     def test_spider_shortcut(self):\n         from scrapy.spiders import Spider\n \n-        self.assertIs(scrapy.Spider, Spider)\n+        assert scrapy.Spider is Spider\n \n     def test_selector_shortcut(self):\n         from scrapy.selector import Selector\n \n-        self.assertIs(scrapy.Selector, Selector)\n+        assert scrapy.Selector is Selector\n \n     def test_item_shortcut(self):\n         from scrapy.item import Field, Item\n \n-        self.assertIs(scrapy.Item, Item)\n-        self.assertIs(scrapy.Field, Field)\n+        assert scrapy.Item is Item\n+        assert scrapy.Field is Field\n\n@@ -1,11 +1,10 @@\n-import unittest\n from urllib.parse import urlparse\n \n \n-class UrlparseTestCase(unittest.TestCase):\n+class TestUrlparse:\n     def test_s3_url(self):\n         p = urlparse(\"s3://bucket/key/name?param=value\")\n-        self.assertEqual(p.scheme, \"s3\")\n-        self.assertEqual(p.hostname, \"bucket\")\n-        self.assertEqual(p.path, \"/key/name\")\n-        self.assertEqual(p.query, \"param=value\")\n+        assert p.scheme == \"s3\"\n+        assert p.hostname == \"bucket\"\n+        assert p.path == \"/key/name\"\n+        assert p.query == \"param=value\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
