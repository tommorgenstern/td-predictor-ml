{"custom_id": "scrapy#2f436c05e088b063b70f000a638409772df27138", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 8 | Churn Cumulative: 1741 | Contributors (this commit): 22 | Commits (past 90d): 12 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -524,23 +524,25 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         self.assertIn(\"The value of FOO is 42\", log)\n \n     def test_shutdown_graceful(self):\n+        sig = signal.SIGINT if sys.platform != \"win32\" else signal.SIGBREAK\n         args = self.get_script_args(\"sleeping.py\")\n         p = PopenSpawn(args, timeout=5)\n         p.expect_exact(\"Spider opened\")\n         p.expect_exact(\"Crawled (200)\")\n-        p.kill(signal.SIGINT)\n+        p.kill(sig)\n         p.expect_exact(\"shutting down gracefully\")\n         p.expect_exact(\"Spider closed (shutdown)\")\n         p.wait()\n \n     def test_shutdown_forced(self):\n+        sig = signal.SIGINT if sys.platform != \"win32\" else signal.SIGBREAK\n         args = self.get_script_args(\"sleeping.py\")\n         p = PopenSpawn(args, timeout=5)\n         p.expect_exact(\"Spider opened\")\n         p.expect_exact(\"Crawled (200)\")\n-        p.kill(signal.SIGINT)\n+        p.kill(sig)\n         p.expect_exact(\"shutting down gracefully\")\n-        p.kill(signal.SIGINT)\n+        p.kill(sig)\n         p.expect_exact(\"forcing unclean shutdown\")\n         p.wait()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#029a56384dcac0a52fe40f1bebaaa53eda1cc902", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 26 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -14,7 +14,7 @@ class SleepingSpider(scrapy.Spider):\n         from twisted.internet import reactor\n \n         d = Deferred()\n-        reactor.callLater(2, d.callback, None)\n+        reactor.callLater(3, d.callback, None)\n         await maybe_deferred_to_future(d)\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5807970a22b9781e291a623c9a713d3124265295", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 137 | Lines Deleted: 60 | Files Changed: 7 | Hunks: 57 | Methods Changed: 67 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 197 | Churn Cumulative: 6337 | Contributors (this commit): 40 | Commits (past 90d): 10 | Contributors (cumulative): 104 | DMM Complexity: 1.0\n\nDIFF:\n@@ -228,8 +228,8 @@ class Stream:\n \n         content_length_name = self._request.headers.normkey(b\"Content-Length\")\n         for name, values in self._request.headers.items():\n-            for value in values:\n-                value = str(value, \"utf-8\")\n+            for value_bytes in values:\n+                value = str(value_bytes, \"utf-8\")\n                 if name == content_length_name:\n                     if value != content_length:\n                         logger.warning(\n\n@@ -1,41 +1,73 @@\n+from __future__ import annotations\n+\n from collections.abc import Mapping\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    AnyStr,\n+    Dict,\n+    Iterable,\n+    List,\n+    Optional,\n+    Tuple,\n+    Union,\n+    cast,\n+)\n \n from w3lib.http import headers_dict_to_raw\n \n from scrapy.utils.datatypes import CaseInsensitiveDict, CaselessDict\n from scrapy.utils.python import to_unicode\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n \n+\n+_RawValueT = Union[bytes, str, int]\n+\n+\n+# isn't fully compatible typing-wise with either dict or CaselessDict,\n+# but it needs refactoring anyway, see also https://github.com/scrapy/scrapy/pull/5146\n class Headers(CaselessDict):\n     \"\"\"Case insensitive http headers dictionary\"\"\"\n \n-    def __init__(self, seq=None, encoding=\"utf-8\"):\n-        self.encoding = encoding\n+    def __init__(\n+        self,\n+        seq: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n+        encoding: str = \"utf-8\",\n+    ):\n+        self.encoding: str = encoding\n         super().__init__(seq)\n \n-    def update(self, seq):\n+    def update(  # type: ignore[override]\n+        self, seq: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]]]\n+    ) -> None:\n         seq = seq.items() if isinstance(seq, Mapping) else seq\n-        iseq = {}\n+        iseq: Dict[bytes, List[bytes]] = {}\n         for k, v in seq:\n             iseq.setdefault(self.normkey(k), []).extend(self.normvalue(v))\n         super().update(iseq)\n \n-    def normkey(self, key):\n+    def normkey(self, key: AnyStr) -> bytes:  # type: ignore[override]\n         \"\"\"Normalize key to bytes\"\"\"\n         return self._tobytes(key.title())\n \n-    def normvalue(self, value):\n+    def normvalue(self, value: Union[_RawValueT, Iterable[_RawValueT]]) -> List[bytes]:\n         \"\"\"Normalize values to bytes\"\"\"\n+        _value: Iterable[_RawValueT]\n         if value is None:\n-            value = []\n+            _value = []\n         elif isinstance(value, (str, bytes)):\n-            value = [value]\n-        elif not hasattr(value, \"__iter__\"):\n-            value = [value]\n+            _value = [value]\n+        elif hasattr(value, \"__iter__\"):\n+            _value = value\n+        else:\n+            _value = [value]\n \n-        return [self._tobytes(x) for x in value]\n+        return [self._tobytes(x) for x in _value]\n \n-    def _tobytes(self, x):\n+    def _tobytes(self, x: _RawValueT) -> bytes:\n         if isinstance(x, bytes):\n             return x\n         if isinstance(x, str):\n@@ -44,49 +76,52 @@ class Headers(CaselessDict):\n             return str(x).encode(self.encoding)\n         raise TypeError(f\"Unsupported value type: {type(x)}\")\n \n-    def __getitem__(self, key):\n+    def __getitem__(self, key: AnyStr) -> Optional[bytes]:\n         try:\n-            return super().__getitem__(key)[-1]\n+            return cast(List[bytes], super().__getitem__(key))[-1]\n         except IndexError:\n             return None\n \n-    def get(self, key, def_val=None):\n+    def get(self, key: AnyStr, def_val: Any = None) -> Optional[bytes]:\n         try:\n-            return super().get(key, def_val)[-1]\n+            return cast(List[bytes], super().get(key, def_val))[-1]\n         except IndexError:\n             return None\n \n-    def getlist(self, key, def_val=None):\n+    def getlist(self, key: AnyStr, def_val: Any = None) -> List[bytes]:\n         try:\n-            return super().__getitem__(key)\n+            return cast(List[bytes], super().__getitem__(key))\n         except KeyError:\n             if def_val is not None:\n                 return self.normvalue(def_val)\n             return []\n \n-    def setlist(self, key, list_):\n+    def setlist(self, key: AnyStr, list_: Iterable[_RawValueT]) -> None:\n         self[key] = list_\n \n-    def setlistdefault(self, key, default_list=()):\n+    def setlistdefault(\n+        self, key: AnyStr, default_list: Iterable[_RawValueT] = ()\n+    ) -> Any:\n         return self.setdefault(key, default_list)\n \n-    def appendlist(self, key, value):\n+    def appendlist(self, key: AnyStr, value: Iterable[_RawValueT]) -> None:\n         lst = self.getlist(key)\n         lst.extend(self.normvalue(value))\n         self[key] = lst\n \n-    def items(self):\n+    def items(self) -> Iterable[Tuple[bytes, List[bytes]]]:  # type: ignore[override]\n         return ((k, self.getlist(k)) for k in self.keys())\n \n-    def values(self):\n+    def values(self) -> List[Optional[bytes]]:  # type: ignore[override]\n         return [self[k] for k in self.keys()]\n \n-    def to_string(self):\n-        return headers_dict_to_raw(self)\n+    def to_string(self) -> bytes:\n+        # cast() can be removed if the headers_dict_to_raw() hint is improved\n+        return cast(bytes, headers_dict_to_raw(self))\n \n-    def to_unicode_dict(self):\n-        \"\"\"Return headers as a CaselessDict with unicode keys\n-        and unicode values. Multiple values are joined with ','.\n+    def to_unicode_dict(self) -> CaseInsensitiveDict:\n+        \"\"\"Return headers as a CaseInsensitiveDict with str keys\n+        and str values. Multiple values are joined with ','.\n         \"\"\"\n         return CaseInsensitiveDict(\n             (\n@@ -96,7 +131,7 @@ class Headers(CaselessDict):\n             for key, value in self.items()\n         )\n \n-    def __copy__(self):\n+    def __copy__(self) -> Self:\n         return self.__class__(self)\n \n     copy = __copy__\n\n@@ -5,7 +5,19 @@ requests in Scrapy.\n See documentation in docs/topics/request-response.rst\n \"\"\"\n import inspect\n-from typing import Callable, List, Optional, Tuple, Type, TypeVar, Union\n+from typing import (\n+    Any,\n+    AnyStr,\n+    Callable,\n+    Iterable,\n+    List,\n+    Mapping,\n+    Optional,\n+    Tuple,\n+    Type,\n+    TypeVar,\n+    Union,\n+)\n \n from w3lib.url import safe_url_string\n \n@@ -77,7 +89,7 @@ class Request(object_ref):\n         url: str,\n         callback: Optional[Callable] = None,\n         method: str = \"GET\",\n-        headers: Optional[dict] = None,\n+        headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n         body: Optional[Union[bytes, str]] = None,\n         cookies: Optional[Union[dict, List[dict]]] = None,\n         meta: Optional[dict] = None,\n\n@@ -4,7 +4,7 @@ responses in Scrapy.\n \n See documentation in docs/topics/request-response.rst\n \"\"\"\n-from typing import Generator, Tuple\n+from typing import Any, AnyStr, Generator, Iterable, Mapping, Tuple, Union\n from urllib.parse import urljoin\n \n from scrapy.exceptions import NotSupported\n@@ -42,7 +42,7 @@ class Response(object_ref):\n         self,\n         url: str,\n         status=200,\n-        headers=None,\n+        headers: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n         body=b\"\",\n         flags=None,\n         request=None,\n\n@@ -8,7 +8,7 @@ from __future__ import annotations\n \n import json\n from contextlib import suppress\n-from typing import TYPE_CHECKING, Any, Generator, Optional, Tuple\n+from typing import TYPE_CHECKING, Any, Generator, Optional, Tuple, cast\n from urllib.parse import urljoin\n \n import parsel\n@@ -102,14 +102,14 @@ class TextResponse(Response):\n         return urljoin(get_base_url(self), url)\n \n     @memoizemethod_noargs\n-    def _headers_encoding(self):\n-        content_type = self.headers.get(b\"Content-Type\", b\"\")\n+    def _headers_encoding(self) -> Optional[str]:\n+        content_type = cast(bytes, self.headers.get(b\"Content-Type\", b\"\"))\n         return http_content_type_encoding(to_unicode(content_type, encoding=\"latin-1\"))\n \n     def _body_inferred_encoding(self):\n         if self._cached_benc is None:\n             content_type = to_unicode(\n-                self.headers.get(b\"Content-Type\", b\"\"), encoding=\"latin-1\"\n+                cast(bytes, self.headers.get(b\"Content-Type\", b\"\")), encoding=\"latin-1\"\n             )\n             benc, ubody = html_to_unicode(\n                 content_type,\n\n@@ -5,14 +5,32 @@ Python Standard Library.\n This module must not depend on any module outside the Standard Library.\n \"\"\"\n \n+from __future__ import annotations\n+\n import collections\n import warnings\n import weakref\n from collections.abc import Mapping\n-from typing import Any, AnyStr, Optional, OrderedDict, Sequence, TypeVar\n+from typing import (\n+    TYPE_CHECKING,\n+    Any,\n+    AnyStr,\n+    Iterable,\n+    Optional,\n+    OrderedDict,\n+    Sequence,\n+    Tuple,\n+    TypeVar,\n+    Union,\n+)\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n \n+if TYPE_CHECKING:\n+    # typing.Self requires Python 3.11\n+    from typing_extensions import Self\n+\n+\n _KT = TypeVar(\"_KT\")\n _VT = TypeVar(\"_VT\")\n \n@@ -20,7 +38,7 @@ _VT = TypeVar(\"_VT\")\n class CaselessDict(dict):\n     __slots__ = ()\n \n-    def __new__(cls, *args, **kwargs):\n+    def __new__(cls, *args: Any, **kwargs: Any) -> Self:\n         from scrapy.http.headers import Headers\n \n         if issubclass(cls, CaselessDict) and not issubclass(cls, Headers):\n@@ -32,54 +50,58 @@ class CaselessDict(dict):\n             )\n         return super().__new__(cls, *args, **kwargs)\n \n-    def __init__(self, seq=None):\n+    def __init__(\n+        self,\n+        seq: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]], None] = None,\n+    ):\n         super().__init__()\n         if seq:\n             self.update(seq)\n \n-    def __getitem__(self, key):\n+    def __getitem__(self, key: AnyStr) -> Any:\n         return dict.__getitem__(self, self.normkey(key))\n \n-    def __setitem__(self, key, value):\n+    def __setitem__(self, key: AnyStr, value: Any) -> None:\n         dict.__setitem__(self, self.normkey(key), self.normvalue(value))\n \n-    def __delitem__(self, key):\n+    def __delitem__(self, key: AnyStr) -> None:\n         dict.__delitem__(self, self.normkey(key))\n \n-    def __contains__(self, key):\n+    def __contains__(self, key: AnyStr) -> bool:  # type: ignore[override]\n         return dict.__contains__(self, self.normkey(key))\n \n     has_key = __contains__\n \n-    def __copy__(self):\n+    def __copy__(self) -> Self:\n         return self.__class__(self)\n \n     copy = __copy__\n \n-    def normkey(self, key):\n+    def normkey(self, key: AnyStr) -> AnyStr:\n         \"\"\"Method to normalize dictionary key access\"\"\"\n         return key.lower()\n \n-    def normvalue(self, value):\n+    def normvalue(self, value: Any) -> Any:\n         \"\"\"Method to normalize values prior to be set\"\"\"\n         return value\n \n-    def get(self, key, def_val=None):\n+    def get(self, key: AnyStr, def_val: Any = None) -> Any:\n         return dict.get(self, self.normkey(key), self.normvalue(def_val))\n \n-    def setdefault(self, key, def_val=None):\n-        return dict.setdefault(self, self.normkey(key), self.normvalue(def_val))\n+    def setdefault(self, key: AnyStr, def_val: Any = None) -> Any:\n+        return dict.setdefault(self, self.normkey(key), self.normvalue(def_val))  # type: ignore[arg-type]\n \n-    def update(self, seq):\n+    # doesn't fully implement MutableMapping.update()\n+    def update(self, seq: Union[Mapping[AnyStr, Any], Iterable[Tuple[AnyStr, Any]]]) -> None:  # type: ignore[override]\n         seq = seq.items() if isinstance(seq, Mapping) else seq\n         iseq = ((self.normkey(k), self.normvalue(v)) for k, v in seq)\n         super().update(iseq)\n \n     @classmethod\n-    def fromkeys(cls, keys, value=None):\n-        return cls((k, value) for k in keys)\n+    def fromkeys(cls, keys: Iterable[AnyStr], value: Any = None) -> Self:  # type: ignore[override]\n+        return cls((k, value) for k in keys)  # type: ignore[misc]\n \n-    def pop(self, key, *args):\n+    def pop(self, key: AnyStr, *args: Any) -> Any:\n         return dict.pop(self, self.normkey(key), *args)\n \n \n\n@@ -275,7 +275,9 @@ class Https2ClientProtocolTestCase(TestCase):\n             self.assertEqual(response.body, expected_body)\n             self.assertEqual(response.request, request)\n \n-            content_length = int(response.headers.get(\"Content-Length\"))\n+            content_length_header = response.headers.get(\"Content-Length\")\n+            assert content_length_header is not None\n+            content_length = int(content_length_header)\n             self.assertEqual(len(response.body), content_length)\n \n         d = self.make_request(request)\n@@ -320,11 +322,15 @@ class Https2ClientProtocolTestCase(TestCase):\n             self.assertEqual(response.status, expected_status)\n             self.assertEqual(response.request, request)\n \n-            content_length = int(response.headers.get(\"Content-Length\"))\n+            content_length_header = response.headers.get(\"Content-Length\")\n+            assert content_length_header is not None\n+            content_length = int(content_length_header)\n             self.assertEqual(len(response.body), content_length)\n \n             # Parse the body\n-            content_encoding = str(response.headers[b\"Content-Encoding\"], \"utf-8\")\n+            content_encoding_header = response.headers[b\"Content-Encoding\"]\n+            assert content_encoding_header is not None\n+            content_encoding = str(content_encoding_header, \"utf-8\")\n             body = json.loads(str(response.body, content_encoding))\n             self.assertIn(\"request-body\", body)\n             self.assertIn(\"extra-data\", body)\n@@ -562,7 +568,9 @@ class Https2ClientProtocolTestCase(TestCase):\n         request = Request(self.get_url(f\"/query-params?{urlencode(params)}\"))\n \n         def assert_query_params(response: Response):\n-            content_encoding = str(response.headers[b\"Content-Encoding\"], \"utf-8\")\n+            content_encoding_header = response.headers[b\"Content-Encoding\"]\n+            assert content_encoding_header is not None\n+            content_encoding = str(content_encoding_header, \"utf-8\")\n             data = json.loads(str(response.body, content_encoding))\n             self.assertEqual(data, params)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#991121fa91aee4d428ae09e75427d4e91970a41b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 579 | Contributors (this commit): 10 | Commits (past 90d): 2 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -41,7 +41,9 @@ class Contract:\n                     cb_result = cb(response, **cb_kwargs)\n                     if isinstance(cb_result, (AsyncGenerator, CoroutineType)):\n                         raise TypeError(\"Contracts don't support async callbacks\")\n-                    return list(iterate_spider_output(cb_result))\n+                    return list(  # pylint: disable=return-in-finally\n+                        iterate_spider_output(cb_result)\n+                    )\n \n             request.callback = wrapper\n \n@@ -68,7 +70,7 @@ class Contract:\n                 else:\n                     results.addSuccess(self.testcase_post)\n                 finally:\n-                    return output\n+                    return output  # pylint: disable=return-in-finally\n \n             request.callback = wrapper\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2cb1e10c764175a9d7deee387feafaee0dfaa70b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1773 | Contributors (this commit): 22 | Commits (past 90d): 16 | Contributors (cumulative): 23 | DMM Complexity: None\n\nDIFF:\n@@ -14,7 +14,7 @@ class SleepingSpider(scrapy.Spider):\n         from twisted.internet import reactor\n \n         d = Deferred()\n-        reactor.callLater(3, d.callback, None)\n+        reactor.callLater(int(self.sleep), d.callback, None)\n         await maybe_deferred_to_future(d)\n \n \n\n@@ -525,7 +525,7 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n \n     def test_shutdown_graceful(self):\n         sig = signal.SIGINT if sys.platform != \"win32\" else signal.SIGBREAK\n-        args = self.get_script_args(\"sleeping.py\")\n+        args = self.get_script_args(\"sleeping.py\", \"-a\", \"sleep=3\")\n         p = PopenSpawn(args, timeout=5)\n         p.expect_exact(\"Spider opened\")\n         p.expect_exact(\"Crawled (200)\")\n@@ -536,7 +536,7 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n \n     def test_shutdown_forced(self):\n         sig = signal.SIGINT if sys.platform != \"win32\" else signal.SIGBREAK\n-        args = self.get_script_args(\"sleeping.py\")\n+        args = self.get_script_args(\"sleeping.py\", \"-a\", \"sleep=10\")\n         p = PopenSpawn(args, timeout=5)\n         p.expect_exact(\"Spider opened\")\n         p.expect_exact(\"Crawled (200)\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7a5cefbcfa1c7b7b0207263bf0d3d6b5e7dcc49a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 147 | Files Changed: 2 | Hunks: 2 | Methods Changed: 12 | Complexity Δ (Sum/Max): -21/0 | Churn Δ: 147 | Churn Cumulative: 245 | Contributors (this commit): 14 | Commits (past 90d): 2 | Contributors (cumulative): 19 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,94 +0,0 @@\n-\"\"\" This module implements the DecompressionMiddleware which tries to recognise\n-and extract the potentially compressed responses that may arrive.\n-\"\"\"\n-\n-import bz2\n-import gzip\n-import logging\n-import tarfile\n-import zipfile\n-from io import BytesIO\n-from tempfile import mktemp\n-from warnings import warn\n-\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.responsetypes import responsetypes\n-\n-warn(\n-    \"scrapy.downloadermiddlewares.decompression is deprecated\",\n-    ScrapyDeprecationWarning,\n-    stacklevel=2,\n-)\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-class DecompressionMiddleware:\n-    \"\"\"This middleware tries to recognise and extract the possibly compressed\n-    responses that may arrive.\"\"\"\n-\n-    def __init__(self):\n-        self._formats = {\n-            \"tar\": self._is_tar,\n-            \"zip\": self._is_zip,\n-            \"gz\": self._is_gzip,\n-            \"bz2\": self._is_bzip2,\n-        }\n-\n-    def _is_tar(self, response):\n-        archive = BytesIO(response.body)\n-        try:\n-            tar_file = tarfile.open(name=mktemp(), fileobj=archive)\n-        except tarfile.ReadError:\n-            return\n-\n-        body = tar_file.extractfile(tar_file.members[0]).read()\n-        respcls = responsetypes.from_args(filename=tar_file.members[0].name, body=body)\n-        return response.replace(body=body, cls=respcls)\n-\n-    def _is_zip(self, response):\n-        archive = BytesIO(response.body)\n-        try:\n-            zip_file = zipfile.ZipFile(archive)\n-        except zipfile.BadZipFile:\n-            return\n-\n-        namelist = zip_file.namelist()\n-        body = zip_file.read(namelist[0])\n-        respcls = responsetypes.from_args(filename=namelist[0], body=body)\n-        return response.replace(body=body, cls=respcls)\n-\n-    def _is_gzip(self, response):\n-        archive = BytesIO(response.body)\n-        try:\n-            body = gzip.GzipFile(fileobj=archive).read()\n-        except OSError:\n-            return\n-\n-        respcls = responsetypes.from_args(body=body)\n-        return response.replace(body=body, cls=respcls)\n-\n-    def _is_bzip2(self, response):\n-        try:\n-            body = bz2.decompress(response.body)\n-        except OSError:\n-            return\n-\n-        respcls = responsetypes.from_args(body=body)\n-        return response.replace(body=body, cls=respcls)\n-\n-    def process_response(self, request, response, spider):\n-        if not response.body:\n-            return response\n-\n-        for fmt, func in self._formats.items():\n-            new_response = func(response)\n-            if new_response:\n-                logger.debug(\n-                    \"Decompressed response with format: %(responsefmt)s\",\n-                    {\"responsefmt\": fmt},\n-                    extra={\"spider\": spider},\n-                )\n-                return new_response\n-        return response\n\n@@ -1,53 +0,0 @@\n-from unittest import TestCase, main\n-\n-from scrapy.downloadermiddlewares.decompression import DecompressionMiddleware\n-from scrapy.http import Response, XmlResponse\n-from scrapy.spiders import Spider\n-from scrapy.utils.test import assert_samelines\n-from tests import get_testdata\n-\n-\n-def _test_data(formats):\n-    uncompressed_body = get_testdata(\"compressed\", \"feed-sample1.xml\")\n-    test_responses = {}\n-    for format in formats:\n-        body = get_testdata(\"compressed\", \"feed-sample1.\" + format)\n-        test_responses[format] = Response(\"http://foo.com/bar\", body=body)\n-    return uncompressed_body, test_responses\n-\n-\n-class DecompressionMiddlewareTest(TestCase):\n-    test_formats = [\"tar\", \"xml.bz2\", \"xml.gz\", \"zip\"]\n-    uncompressed_body, test_responses = _test_data(test_formats)\n-\n-    def setUp(self):\n-        self.mw = DecompressionMiddleware()\n-        self.spider = Spider(\"foo\")\n-\n-    def test_known_compression_formats(self):\n-        for fmt in self.test_formats:\n-            rsp = self.test_responses[fmt]\n-            new = self.mw.process_response(None, rsp, self.spider)\n-            error_msg = f\"Failed {fmt}, response type {type(new).__name__}\"\n-            assert isinstance(new, XmlResponse), error_msg\n-            assert_samelines(self, new.body, self.uncompressed_body, fmt)\n-\n-    def test_plain_response(self):\n-        rsp = Response(url=\"http://test.com\", body=self.uncompressed_body)\n-        new = self.mw.process_response(None, rsp, self.spider)\n-        assert new is rsp\n-        assert_samelines(self, new.body, rsp.body)\n-\n-    def test_empty_response(self):\n-        rsp = Response(url=\"http://test.com\", body=b\"\")\n-        new = self.mw.process_response(None, rsp, self.spider)\n-        assert new is rsp\n-        assert not rsp.body\n-        assert not new.body\n-\n-    def tearDown(self):\n-        del self.mw\n-\n-\n-if __name__ == \"__main__\":\n-    main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bdb4abcc7a0c5b1d452557bc74087c7dbb3ff06f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 13 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 13 | Churn Cumulative: 2357 | Contributors (this commit): 32 | Commits (past 90d): 15 | Contributors (cumulative): 32 | DMM Complexity: 0.0\n\nDIFF:\n@@ -12,7 +12,6 @@ from twisted.internet.defer import (\n     inlineCallbacks,\n     maybeDeferred,\n )\n-from zope.interface.exceptions import DoesNotImplement\n \n try:\n     # zope >= 5.0 only supports MultipleInvalid\n@@ -205,19 +204,7 @@ class CrawlerRunner:\n         \"\"\"Get SpiderLoader instance from settings\"\"\"\n         cls_path = settings.get(\"SPIDER_LOADER_CLASS\")\n         loader_cls = load_object(cls_path)\n-        excs = (\n-            (DoesNotImplement, MultipleInvalid) if MultipleInvalid else DoesNotImplement\n-        )\n-        try:\n         verifyClass(ISpiderLoader, loader_cls)\n-        except excs:\n-            warnings.warn(\n-                \"SPIDER_LOADER_CLASS (previously named SPIDER_MANAGER_CLASS) does \"\n-                \"not fully implement scrapy.interfaces.ISpiderLoader interface. \"\n-                \"Please add all missing methods to avoid unexpected runtime errors.\",\n-                category=ScrapyDeprecationWarning,\n-                stacklevel=2,\n-            )\n         return loader_cls.from_settings(settings.frozencopy())\n \n     def __init__(self, settings: Union[Dict[str, Any], Settings, None] = None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#83f500a352c038fe6afb7b04f0de00e20fe1b887", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 20 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 20 | Churn Cumulative: 431 | Contributors (this commit): 16 | Commits (past 90d): 1 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -44,9 +44,6 @@ class MediaPipeline:\n         self.allow_redirects = settings.getbool(resolve(\"MEDIA_ALLOW_REDIRECTS\"), False)\n         self._handle_statuses(self.allow_redirects)\n \n-        # Check if deprecated methods are being used and make them compatible\n-        self._make_compatible()\n-\n     def _handle_statuses(self, allow_redirects):\n         self.handle_httpstatus_list = None\n         if allow_redirects:\n@@ -126,23 +123,6 @@ class MediaPipeline:\n         )\n         return dfd.addBoth(lambda _: wad)  # it must return wad at last\n \n-    def _make_compatible(self):\n-        \"\"\"Make overridable methods of MediaPipeline and subclasses backwards compatible\"\"\"\n-        methods = [\n-            \"file_path\",\n-            \"thumb_path\",\n-            \"media_to_download\",\n-            \"media_downloaded\",\n-            \"file_downloaded\",\n-            \"image_downloaded\",\n-            \"get_images\",\n-        ]\n-\n-        for method_name in methods:\n-            method = getattr(self, method_name, None)\n-            if callable(method):\n-                setattr(self, method_name, self._compatible(method))\n-\n     def _compatible(self, func):\n         \"\"\"Wrapper for overridable methods to allow backwards compatibility\"\"\"\n         self._check_signature(func)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#85c57778b5ac701f30721c4aa968efc8474c6e1e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 853 | Contributors (this commit): 20 | Commits (past 90d): 3 | Contributors (cumulative): 26 | DMM Complexity: None\n\nDIFF:\n@@ -200,7 +200,7 @@ class Request(object_ref):\n         ``ignore_unknown_options=False``.\n \n         .. caution:: Using :meth:`from_curl` from :class:`~scrapy.http.Request`\n-                     subclasses, such as :class:`~scrapy.http.JSONRequest`, or\n+                     subclasses, such as :class:`~scrapy.http.JsonRequest`, or\n                      :class:`~scrapy.http.XmlRpcRequest`, as well as having\n                      :ref:`downloader middlewares <topics-downloader-middleware>`\n                      and\n\n@@ -11,7 +11,6 @@ import warnings\n from typing import Optional, Tuple\n \n from scrapy.http.request import Request\n-from scrapy.utils.deprecate import create_deprecated_class\n \n \n class JsonRequest(Request):\n@@ -58,6 +57,3 @@ class JsonRequest(Request):\n     def _dumps(self, data: dict) -> str:\n         \"\"\"Convert to JSON\"\"\"\n         return json.dumps(data, **self._dumps_kwargs)\n-\n-\n-JSONRequest = create_deprecated_class(\"JSONRequest\", JsonRequest)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#aa0a4258264716ad5f99d9fca7c64bde8cf3576b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 102 | Files Changed: 4 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): -10/0 | Churn Δ: 102 | Churn Cumulative: 2607 | Contributors (this commit): 41 | Commits (past 90d): 4 | Contributors (cumulative): 61 | DMM Complexity: 0.5531914893617021\n\nDIFF:\n@@ -1,21 +0,0 @@\n-\"\"\"Common functions used in Item Loaders code\"\"\"\n-\n-import warnings\n-\n-from itemloaders import common\n-\n-from scrapy.utils.deprecate import ScrapyDeprecationWarning\n-\n-\n-def wrap_loader_context(function, context):\n-    \"\"\"Wrap functions that receive loader_context to contain the context\n-    \"pre-loaded\" and expose a interface that receives only one argument\n-    \"\"\"\n-    warnings.warn(\n-        \"scrapy.loader.common.wrap_loader_context has moved to a new library.\"\n-        \"Please update your reference to itemloaders.common.wrap_loader_context\",\n-        ScrapyDeprecationWarning,\n-        stacklevel=2,\n-    )\n-\n-    return common.wrap_loader_context(function, context)\n\n@@ -1,20 +0,0 @@\n-\"\"\"\n-This module provides some commonly used processors for Item Loaders.\n-\n-See documentation in docs/topics/loaders.rst\n-\"\"\"\n-from itemloaders import processors\n-\n-from scrapy.utils.deprecate import create_deprecated_class\n-\n-MapCompose = create_deprecated_class(\"MapCompose\", processors.MapCompose)\n-\n-Compose = create_deprecated_class(\"Compose\", processors.Compose)\n-\n-TakeFirst = create_deprecated_class(\"TakeFirst\", processors.TakeFirst)\n-\n-Identity = create_deprecated_class(\"Identity\", processors.Identity)\n-\n-SelectJmes = create_deprecated_class(\"SelectJmes\", processors.SelectJmes)\n-\n-Join = create_deprecated_class(\"Join\", processors.Join)\n\n@@ -21,17 +21,12 @@ from typing import (\n     Iterable,\n     List,\n     Optional,\n-    Pattern,\n     Union,\n     cast,\n )\n \n-from w3lib.html import replace_entities\n-\n from scrapy.item import Item\n from scrapy.utils.datatypes import LocalWeakReferencedCache\n-from scrapy.utils.deprecate import ScrapyDeprecationWarning\n-from scrapy.utils.python import flatten, to_unicode\n \n if TYPE_CHECKING:\n     from scrapy import Spider\n@@ -108,39 +103,6 @@ def walk_modules(path: str) -> List[ModuleType]:\n     return mods\n \n \n-def extract_regex(\n-    regex: Union[str, Pattern], text: str, encoding: str = \"utf-8\"\n-) -> List[str]:\n-    \"\"\"Extract a list of unicode strings from the given text/encoding using the following policies:\n-\n-    * if the regex contains a named group called \"extract\" that will be returned\n-    * if the regex contains multiple numbered groups, all those will be returned (flattened)\n-    * if the regex doesn't contain any group the entire regex matching is returned\n-    \"\"\"\n-    warnings.warn(\n-        \"scrapy.utils.misc.extract_regex has moved to parsel.utils.extract_regex.\",\n-        ScrapyDeprecationWarning,\n-        stacklevel=2,\n-    )\n-\n-    if isinstance(regex, str):\n-        regex = re.compile(regex, re.UNICODE)\n-\n-    try:\n-        # named group\n-        strings = [regex.search(text).group(\"extract\")]  # type: ignore[union-attr]\n-    except Exception:\n-        # full regex or numbered groups\n-        strings = regex.findall(text)\n-    strings = flatten(strings)\n-\n-    if isinstance(text, str):\n-        return [replace_entities(s, keep=[\"lt\", \"amp\"]) for s in strings]\n-    return [\n-        replace_entities(to_unicode(s, encoding), keep=[\"lt\", \"amp\"]) for s in strings\n-    ]\n-\n-\n def md5sum(file: IO) -> str:\n     \"\"\"Calculate the md5 checksum of a file-like object without reading its\n     whole content in memory.\n\n@@ -4,7 +4,6 @@ Once we remove the references from scrapy, we can remove these tests.\n \"\"\"\n \n import unittest\n-import warnings\n from functools import partial\n \n from itemloaders.processors import (\n@@ -18,9 +17,6 @@ from itemloaders.processors import (\n \n from scrapy.item import Field, Item\n from scrapy.loader import ItemLoader\n-from scrapy.loader.common import wrap_loader_context\n-from scrapy.utils.deprecate import ScrapyDeprecationWarning\n-from scrapy.utils.misc import extract_regex\n \n \n # test items\n@@ -722,24 +718,5 @@ class FunctionProcessorTestCase(unittest.TestCase):\n         self.assertEqual(dict(lo.load_item()), {\"foo\": [\"BAR\", \"ASDF\", \"QWERTY\"]})\n \n \n-class DeprecatedUtilityFunctionsTestCase(unittest.TestCase):\n-    def test_deprecated_wrap_loader_context(self):\n-        def function(*args):\n-            return None\n-\n-        with warnings.catch_warnings(record=True) as w:\n-            wrap_loader_context(function, context={})\n-\n-            assert len(w) == 1\n-            assert issubclass(w[0].category, ScrapyDeprecationWarning)\n-\n-    def test_deprecated_extract_regex(self):\n-        with warnings.catch_warnings(record=True) as w:\n-            extract_regex(r\"\\w+\", \"this is a test\")\n-\n-            assert len(w) == 1\n-            assert issubclass(w[0].category, ScrapyDeprecationWarning)\n-\n-\n if __name__ == \"__main__\":\n     unittest.main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ad4e8b64d4343e01e944c9698afaa29802d557a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 1752 | Contributors (this commit): 22 | Commits (past 90d): 14 | Contributors (cumulative): 22 | DMM Complexity: 0.0\n\nDIFF:\n@@ -15,6 +15,7 @@ from pytest import mark, raises\n from twisted.internet import defer\n from twisted.trial import unittest\n from w3lib import __version__ as w3lib_version\n+from zope.interface.exceptions import MultipleInvalid\n \n import scrapy\n from scrapy.crawler import Crawler, CrawlerProcess, CrawlerRunner\n@@ -179,11 +180,7 @@ class CrawlerRunnerTestCase(BaseCrawlerTest):\n                 \"SPIDER_LOADER_CLASS\": SpiderLoaderWithWrongInterface,\n             }\n         )\n-        with warnings.catch_warnings(record=True) as w:\n-            self.assertRaises(AttributeError, CrawlerRunner, settings)\n-            self.assertEqual(len(w), 1)\n-            self.assertIn(\"SPIDER_LOADER_CLASS\", str(w[0].message))\n-            self.assertIn(\"scrapy.interfaces.ISpiderLoader\", str(w[0].message))\n+        self.assertRaises(MultipleInvalid, CrawlerRunner, settings)\n \n     def test_crawler_runner_accepts_dict(self):\n         runner = CrawlerRunner({\"foo\": \"bar\"})\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2de95f1fc01fa8835f85c2554faba6bb536e71f8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 111 | Files Changed: 2 | Hunks: 3 | Methods Changed: 11 | Complexity Δ (Sum/Max): -15/0 | Churn Δ: 111 | Churn Cumulative: 1443 | Contributors (this commit): 21 | Commits (past 90d): 6 | Contributors (cumulative): 32 | DMM Complexity: 0.0\n\nDIFF:\n@@ -123,20 +123,6 @@ class MediaPipeline:\n         )\n         return dfd.addBoth(lambda _: wad)  # it must return wad at last\n \n-    def _compatible(self, func):\n-        \"\"\"Wrapper for overridable methods to allow backwards compatibility\"\"\"\n-        self._check_signature(func)\n-\n-        @functools.wraps(func)\n-        def wrapper(*args, **kwargs):\n-            if self._expects_item[func.__name__]:\n-                return func(*args, **kwargs)\n-\n-            kwargs.pop(\"item\", None)\n-            return func(*args, **kwargs)\n-\n-        return wrapper\n-\n     def _check_signature(self, func):\n         sig = signature(func)\n         self._expects_item[func.__name__] = True\n\n@@ -15,7 +15,6 @@ from scrapy.pipelines.images import ImagesPipeline\n from scrapy.pipelines.media import MediaPipeline\n from scrapy.settings import Settings\n from scrapy.spiders import Spider\n-from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.signal import disconnect_all\n from scrapy.utils.test import get_crawler\n@@ -427,102 +426,6 @@ class MockedMediaPipelineDeprecatedMethods(ImagesPipeline):\n         return super().image_downloaded(response, request, info)\n \n \n-class MediaPipelineDeprecatedMethodsTestCase(unittest.TestCase):\n-    skip = skip_pillow\n-\n-    def setUp(self):\n-        settings_dict = {\n-            \"IMAGES_STORE\": \"store-uri\",\n-            \"IMAGES_THUMBS\": {\"small\": (50, 50)},\n-        }\n-        crawler = get_crawler(spidercls=None, settings_dict=settings_dict)\n-        self.pipe = MockedMediaPipelineDeprecatedMethods.from_crawler(crawler)\n-        self.pipe.download_func = _mocked_download_func\n-        self.pipe.open_spider(None)\n-        self.item = dict(image_urls=[\"http://picsum.photos/id/1014/200/300\"], images=[])\n-\n-    def _assert_method_called_with_warnings(self, method, message, warnings):\n-        self.assertIn(method, self.pipe._mockcalled)\n-        warningShown = False\n-        for warning in warnings:\n-            if (\n-                warning[\"message\"] == message\n-                and warning[\"category\"] == ScrapyDeprecationWarning\n-            ):\n-                warningShown = True\n-        self.assertTrue(warningShown)\n-\n-    @inlineCallbacks\n-    def test_media_to_download_called(self):\n-        yield self.pipe.process_item(self.item, None)\n-        warnings = self.flushWarnings([MediaPipeline._compatible])\n-        message = (\n-            \"media_to_download(self, request, info) is deprecated, \"\n-            \"please use media_to_download(self, request, info, *, item=None)\"\n-        )\n-        self._assert_method_called_with_warnings(\"media_to_download\", message, warnings)\n-\n-    @inlineCallbacks\n-    def test_media_downloaded_called(self):\n-        yield self.pipe.process_item(self.item, None)\n-        warnings = self.flushWarnings([MediaPipeline._compatible])\n-        message = (\n-            \"media_downloaded(self, response, request, info) is deprecated, \"\n-            \"please use media_downloaded(self, response, request, info, *, item=None)\"\n-        )\n-        self._assert_method_called_with_warnings(\"media_downloaded\", message, warnings)\n-\n-    @inlineCallbacks\n-    def test_file_downloaded_called(self):\n-        yield self.pipe.process_item(self.item, None)\n-        warnings = self.flushWarnings([MediaPipeline._compatible])\n-        message = (\n-            \"file_downloaded(self, response, request, info) is deprecated, \"\n-            \"please use file_downloaded(self, response, request, info, *, item=None)\"\n-        )\n-        self._assert_method_called_with_warnings(\"file_downloaded\", message, warnings)\n-\n-    @inlineCallbacks\n-    def test_file_path_called(self):\n-        yield self.pipe.process_item(self.item, None)\n-        warnings = self.flushWarnings([MediaPipeline._compatible])\n-        message = (\n-            \"file_path(self, request, response=None, info=None) is deprecated, \"\n-            \"please use file_path(self, request, response=None, info=None, *, item=None)\"\n-        )\n-        self._assert_method_called_with_warnings(\"file_path\", message, warnings)\n-\n-    @inlineCallbacks\n-    def test_thumb_path_called(self):\n-        yield self.pipe.process_item(self.item, None)\n-        warnings = self.flushWarnings([MediaPipeline._compatible])\n-        message = (\n-            \"thumb_path(self, request, thumb_id, response=None, info=None) is deprecated, \"\n-            \"please use thumb_path(self, request, thumb_id, response=None, info=None, *, item=None)\"\n-        )\n-        self._assert_method_called_with_warnings(\"thumb_path\", message, warnings)\n-\n-    @inlineCallbacks\n-    def test_get_images_called(self):\n-        yield self.pipe.process_item(self.item, None)\n-        warnings = self.flushWarnings([MediaPipeline._compatible])\n-        message = (\n-            \"get_images(self, response, request, info) is deprecated, \"\n-            \"please use get_images(self, response, request, info, *, item=None)\"\n-        )\n-        self._assert_method_called_with_warnings(\"get_images\", message, warnings)\n-\n-    @inlineCallbacks\n-    def test_image_downloaded_called(self):\n-        yield self.pipe.process_item(self.item, None)\n-        warnings = self.flushWarnings([MediaPipeline._compatible])\n-        message = (\n-            \"image_downloaded(self, response, request, info) is deprecated, \"\n-            \"please use image_downloaded(self, response, request, info, *, item=None)\"\n-        )\n-        self._assert_method_called_with_warnings(\"image_downloaded\", message, warnings)\n-\n-\n class MediaPipelineAllowRedirectSettingsTestCase(unittest.TestCase):\n     def _assert_request_no3xx(self, pipeline_class, settings):\n         pipe = pipeline_class(settings=Settings(settings))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4ad727f0b566f2e50587e7bd2c9c691f10700daf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 18 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 18 | Churn Cumulative: 463 | Contributors (this commit): 16 | Commits (past 90d): 3 | Contributors (cumulative): 16 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,8 +1,6 @@\n import functools\n import logging\n from collections import defaultdict\n-from inspect import signature\n-from warnings import warn\n \n from twisted.internet.defer import Deferred, DeferredList\n from twisted.python.failure import Failure\n@@ -11,7 +9,6 @@ from scrapy.http.request import NO_CALLBACK\n from scrapy.settings import Settings\n from scrapy.utils.datatypes import SequenceExclude\n from scrapy.utils.defer import defer_result, mustbe_deferred\n-from scrapy.utils.deprecate import ScrapyDeprecationWarning\n from scrapy.utils.log import failure_to_exc_info\n from scrapy.utils.misc import arg_to_iter\n \n@@ -123,21 +120,6 @@ class MediaPipeline:\n         )\n         return dfd.addBoth(lambda _: wad)  # it must return wad at last\n \n-    def _check_signature(self, func):\n-        sig = signature(func)\n-        self._expects_item[func.__name__] = True\n-\n-        if \"item\" not in sig.parameters:\n-            old_params = str(sig)[1:-1]\n-            new_params = old_params + \", *, item=None\"\n-            warn(\n-                f\"{func.__name__}(self, {old_params}) is deprecated, \"\n-                f\"please use {func.__name__}(self, {new_params})\",\n-                ScrapyDeprecationWarning,\n-                stacklevel=2,\n-            )\n-            self._expects_item[func.__name__] = False\n-\n     def _modify_media_request(self, request):\n         if self.handle_httpstatus_list:\n             request.meta[\"handle_httpstatus_list\"] = self.handle_httpstatus_list\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a2b9351f04f0b9f81edbf3c3ebdf2ad1d5ba847e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 50 | Files Changed: 2 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 51 | Churn Cumulative: 294 | Contributors (this commit): 9 | Commits (past 90d): 2 | Contributors (cumulative): 14 | DMM Complexity: 0.24324324324324326\n\nDIFF:\n@@ -3,13 +3,10 @@ HTTP basic auth downloader middleware\n \n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n-import warnings\n \n from w3lib.http import basic_auth_header\n \n from scrapy import signals\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.url import url_is_from_any_domain\n \n \n@@ -28,25 +25,10 @@ class HttpAuthMiddleware:\n         pwd = getattr(spider, \"http_pass\", \"\")\n         if usr or pwd:\n             self.auth = basic_auth_header(usr, pwd)\n-            if not hasattr(spider, \"http_auth_domain\"):\n-                warnings.warn(\n-                    \"Using HttpAuthMiddleware without http_auth_domain is deprecated and can cause security \"\n-                    \"problems if the spider makes requests to several different domains. http_auth_domain \"\n-                    \"will be set to the domain of the first request, please set it to the correct value \"\n-                    \"explicitly.\",\n-                    category=ScrapyDeprecationWarning,\n-                )\n-                self.domain_unset = True\n-            else:\n             self.domain = spider.http_auth_domain\n-                self.domain_unset = False\n \n     def process_request(self, request, spider):\n         auth = getattr(self, \"auth\", None)\n         if auth and b\"Authorization\" not in request.headers:\n-            domain = urlparse_cached(request).hostname\n-            if self.domain_unset:\n-                self.domain = domain\n-                self.domain_unset = False\n             if not self.domain or url_is_from_any_domain(request.url, [self.domain]):\n                 request.headers[b\"Authorization\"] = auth\n\n@@ -1,10 +1,8 @@\n import unittest\n \n-import pytest\n from w3lib.http import basic_auth_header\n \n from scrapy.downloadermiddlewares.httpauth import HttpAuthMiddleware\n-from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Request\n from scrapy.spiders import Spider\n \n@@ -31,39 +29,10 @@ class HttpAuthMiddlewareLegacyTest(unittest.TestCase):\n         self.spider = TestSpiderLegacy(\"foo\")\n \n     def test_auth(self):\n-        with pytest.warns(\n-            ScrapyDeprecationWarning,\n-            match=\"Using HttpAuthMiddleware without http_auth_domain is deprecated\",\n-        ):\n+        with self.assertRaises(AttributeError):\n             mw = HttpAuthMiddleware()\n             mw.spider_opened(self.spider)\n \n-        # initial request, sets the domain and sends the header\n-        req = Request(\"http://example.com/\")\n-        assert mw.process_request(req, self.spider) is None\n-        self.assertEqual(req.headers[\"Authorization\"], basic_auth_header(\"foo\", \"bar\"))\n-\n-        # subsequent request to the same domain, should send the header\n-        req = Request(\"http://example.com/\")\n-        assert mw.process_request(req, self.spider) is None\n-        self.assertEqual(req.headers[\"Authorization\"], basic_auth_header(\"foo\", \"bar\"))\n-\n-        # subsequent request to a different domain, shouldn't send the header\n-        req = Request(\"http://example-noauth.com/\")\n-        assert mw.process_request(req, self.spider) is None\n-        self.assertNotIn(\"Authorization\", req.headers)\n-\n-    def test_auth_already_set(self):\n-        with pytest.warns(\n-            ScrapyDeprecationWarning,\n-            match=\"Using HttpAuthMiddleware without http_auth_domain is deprecated\",\n-        ):\n-            mw = HttpAuthMiddleware()\n-            mw.spider_opened(self.spider)\n-        req = Request(\"http://example.com/\", headers=dict(Authorization=\"Digest 123\"))\n-        assert mw.process_request(req, self.spider) is None\n-        self.assertEqual(req.headers[\"Authorization\"], b\"Digest 123\")\n-\n \n class HttpAuthMiddlewareTest(unittest.TestCase):\n     def setUp(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
