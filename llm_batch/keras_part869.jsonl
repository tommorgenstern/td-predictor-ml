{"custom_id": "keras#e072189ff2cde45ad5874d738efbfdb6d5fcd5f3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 215 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -143,7 +143,7 @@ class Equalization(BaseImagePreprocessingLayer):\n     def _apply_equalization(self, channel, hist):\n         cdf = self.backend.numpy.cumsum(hist)\n \n-        if backend.backend() == \"jax\":\n+        if self.backend.name == \"jax\":\n             mask = cdf > 0\n             first_nonzero_idx = self.backend.numpy.argmax(mask)\n             cdf_min = self.backend.numpy.take(cdf, first_nonzero_idx)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1aff3792403844160bfa184005947e4d52743a2e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 13 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -572,7 +572,6 @@ class OneHotIoU(IoU):\n             is used to determine each sample's most likely associated label.\n         axis: (Optional) The dimension containing the logits. Defaults to `-1`.\n \n-    Example:\n \n     Example:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5354cf99f81f7c1e79fad5ae0bbf07b5b07307d3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 112 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 8 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 112 | Churn Cumulative: 130 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -168,3 +168,115 @@ class TextVectorizationTest(testing.TestCase):\n                 vocabulary=[\"baz\", \"bar\", \"foo\"],\n                 ragged=True,\n             )\n+\n+    def test_multi_hot_output(self):\n+        layer = layers.TextVectorization(\n+            output_mode=\"multi_hot\", vocabulary=[\"foo\", \"bar\", \"baz\"]\n+        )\n+        input_data = [[\"foo bar\"], [\"baz foo foo\"]]\n+        output = layer(input_data)\n+\n+        \"\"\"\n+        First batch\n+        Tokens present: [\"foo\", \"bar\"]\n+            For each token in vocabulary:\n+            foo (index 1): present -> 1\n+            bar (index 2): present -> 1\n+            baz (index 3): absent -> 0\n+            Result: [0, 1, 1, 0]\n+        \n+        Second batch\n+            Tokens: [\"baz\", \"foo\", \"foo\"]\n+            For each token in vocabulary:\n+            foo (index 1): present -> 1\n+            bar (index 2): absent -> 0\n+            baz (index 3): present -> 1\n+            Result: [0, 1, 0, 1]\n+        \"\"\"\n+        self.assertAllClose(output, [[0, 1, 1, 0], [0, 1, 0, 1]])\n+\n+    def test_output_mode_count_output(self):\n+        layer = layers.TextVectorization(\n+            output_mode=\"count\", vocabulary=[\"foo\", \"bar\", \"baz\"]\n+        )\n+        output = layer([\"foo bar\", \"baz foo foo\"])\n+        self.assertAllClose(output, [[0, 1, 1, 0], [0, 2, 0, 1]])\n+\n+    def test_output_mode_tf_idf_output(self):\n+        layer = layers.TextVectorization(\n+            output_mode=\"tf_idf\",\n+            vocabulary=[\"foo\", \"bar\", \"baz\"],\n+            idf_weights=[0.3, 0.5, 0.2],\n+        )\n+        output = layer([\"foo bar\", \"baz foo foo\"])\n+        self.assertAllClose(\n+            output, [[0.0, 0.3, 0.5, 0.0], [0.0, 0.6, 0.0, 0.2]]\n+        )\n+\n+    def test_lower_and_strip_punctuation_standardization(self):\n+        layer = layers.TextVectorization(\n+            standardize=\"lower_and_strip_punctuation\",\n+            vocabulary=[\"hello\", \"world\", \"this\", \"is\", \"nice\", \"test\"],\n+        )\n+        output = layer([\"Hello, World!. This is just a nice test!\"])\n+        self.assertTrue(backend.is_tensor(output))\n+\n+        # test output sequence length, taking first batch.\n+        self.assertEqual(len(output[0]), 8)\n+\n+        self.assertAllEqual(output, [[2, 3, 4, 5, 1, 1, 6, 7]])\n+\n+    def test_lower_standardization(self):\n+        layer = layers.TextVectorization(\n+            standardize=\"lower\",\n+            vocabulary=[\n+                \"hello,\",\n+                \"hello\",\n+                \"world\",\n+                \"this\",\n+                \"is\",\n+                \"nice\",\n+                \"test\",\n+            ],\n+        )\n+        output = layer([\"Hello, World!. This is just a nice test!\"])\n+        self.assertTrue(backend.is_tensor(output))\n+        self.assertEqual(len(output[0]), 8)\n+        \"\"\"\n+        The input is lowercased and tokenized into words. The vocab is:\n+        {0: '',\n+        1: '[UNK]',\n+        2: 'hello,',\n+        3: 'hello',\n+        4: 'world',\n+        5: 'this',\n+        6: 'is',\n+        7: 'nice',\n+        8: 'test'}\n+        \"\"\"\n+        self.assertAllEqual(output, [[2, 1, 5, 6, 1, 1, 7, 1]])\n+\n+    def test_char_splitting(self):\n+        layer = layers.TextVectorization(\n+            split=\"character\", vocabulary=list(\"abcde\"), output_mode=\"int\"\n+        )\n+        output = layer([\"abcf\"])\n+        self.assertTrue(backend.is_tensor(output))\n+        self.assertEqual(len(output[0]), 4)\n+        self.assertAllEqual(output, [[2, 3, 4, 1]])\n+\n+    def test_custom_splitting(self):\n+        def custom_split(text):\n+            return tf.strings.split(text, sep=\"|\")\n+\n+        layer = layers.TextVectorization(\n+            split=custom_split,\n+            vocabulary=[\"foo\", \"bar\", \"foobar\"],\n+            output_mode=\"int\",\n+        )\n+        output = layer([\"foo|bar\"])\n+        self.assertTrue(backend.is_tensor(output))\n+\n+        # after custom split, the outputted index should be the last\n+        # token in the vocab.\n+        self.assertAllEqual(output, [[4]])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#44e0723527c6ac57bec693f4168085744cd6ad7b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 231 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -93,6 +93,7 @@ class ExceptionPyDataset(py_dataset_adapter.PyDataset):\n         raise ValueError(\"Expected exception\")\n \n \n+@pytest.mark.skipif(testing.tensorflow_uses_gpu(), reason=\"Flaky on GPU\")\n class PyDatasetAdapterTest(testing.TestCase):\n     @parameterized.named_parameters(\n         named_product(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e53e61d2a17d36600810414e3cf3cc6f55b7d90a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 30 | Churn Cumulative: 800 | Contributors (this commit): 8 | Commits (past 90d): 7 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -872,6 +872,12 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n                         mask = tree.map_structure(backend.get_keras_mask, v)\n                         kwargs[expected_mask_arg_name] = mask\n \n+        # We need to cache the `previous_mask` before `__call__` because the\n+        # mask might be removed during the call, such as `MultiHeadAttention`.\n+        previous_mask = tree.map_structure(\n+            backend.get_keras_mask, call_spec.first_arg\n+        )\n+\n         ####################\n         # 7. Call the layer.\n         try:\n@@ -918,12 +924,9 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n                         if backend.is_tensor(output):\n                             self.add_loss(self.activity_regularizer(output))\n \n-            # Set masks on outputs,\n-            # provided only the first positional input arg and its mask.\n+            # Set `previous_mask` on outputs if available. It is provided only\n+            # for the first positional input arg and its mask.\n             # TODO: consider extending this to all args and kwargs.\n-            previous_mask = tree.map_structure(\n-                backend.get_keras_mask, call_spec.first_arg\n-            )\n             if self.supports_masking:\n                 self._set_mask_metadata(\n                     call_spec.first_arg, outputs, previous_mask\n\n@@ -673,6 +673,23 @@ class LayerTest(testing.TestCase):\n         layer((x1_1, x1_2), x2)\n         layer(x1=(x1_1, x1_2), x2=x2)\n \n+        class MaskUnsetDuringCallLayer(layers.Layer):\n+            def __init__(self):\n+                super().__init__()\n+                self.supports_masking = True\n+\n+            def call(self, x, mask=None):\n+                assert mask is not None\n+                backend.set_keras_mask(x, None)  # Unset mask\n+                return x\n+\n+        layer = MaskUnsetDuringCallLayer()\n+        x = backend.numpy.ones((4, 4))\n+        mask = backend.numpy.ones((4,))\n+        backend.set_keras_mask(x, mask)\n+        y = layer(x)\n+        self.assertAllClose(y._keras_mask, mask)\n+\n     def test_stateless_call(self):\n         class TestLayer(layers.Layer):\n             def __init__(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#033d96790df08b5942dca98d0c75eaad9740a7ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 2 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 43 | Churn Cumulative: 1350 | Contributors (this commit): 12 | Commits (past 90d): 17 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -369,7 +369,9 @@ class Variable:\n         raise NotImplementedError\n \n     def _initialize_with_initializer(self, initializer):\n-        value = initializer(self._shape, dtype=self._dtype)\n+        value = self._convert_to_tensor(\n+            initializer(self._shape, dtype=self._dtype)\n+        )\n         self._initialize(value)\n \n     def _convert_to_tensor(self, value, dtype=None):\n\n@@ -61,6 +61,43 @@ class VariableInitializationTest(test_case.TestCase):\n         self.assertAllClose(v.value, np.ones((2, 2)))\n         self.assertEqual(v.dtype, \"float32\")\n \n+    def test_variable_initialization_with_lambda_expression(self):\n+        # Test Python number\n+        v = backend.Variable(\n+            initializer=lambda *a, **kw: 1.0,\n+            shape=(),\n+            dtype=\"float32\",\n+        )\n+        self.assertAllClose(v.value, 1.0)\n+        self.assertEqual(v.dtype, \"float32\")\n+\n+        # Test Python array\n+        v = backend.Variable(\n+            initializer=lambda *a, **kw: [1.0],\n+            shape=(1,),\n+            dtype=\"float32\",\n+        )\n+        self.assertAllClose(v.value, np.ones((1,)))\n+        self.assertEqual(v.dtype, \"float32\")\n+\n+        # Test numpy array\n+        v = backend.Variable(\n+            initializer=lambda *a, **kw: np.ones((1,)),\n+            shape=(1,),\n+            dtype=\"float32\",\n+        )\n+        self.assertAllClose(v.value, np.ones((1,)))\n+        self.assertEqual(v.dtype, \"float32\")\n+\n+        # Test backend array\n+        v = backend.Variable(\n+            initializer=lambda *a, **kw: ops.ones((1,)),\n+            shape=(1,),\n+            dtype=\"float32\",\n+        )\n+        self.assertAllClose(v.value, np.ones((1,)))\n+        self.assertEqual(v.dtype, \"float32\")\n+\n     def test_variable_initialization_with_strings(self):\n         \"\"\"Test variable init with non-callable initializer.\"\"\"\n         v = backend.Variable(initializer=\"ones\", shape=(2, 2))\n\n@@ -21,7 +21,7 @@ IS_THREAD_SAFE = True\n class Variable(KerasVariable):\n     def _initialize(self, value):\n         # Note that variable.shape is needed by distribution_lib\n-        self._shape = tuple(value.shape)\n+        self._shape = self._validate_shape(value.shape)\n         # We can't import the keras/distribution/distribution_lib\n         # due to circular dependency.\n         distribution = global_state.get_global_attribute(\"distribution\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1597013645bfcd31207d31b969193b458ce85626", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): -4/1 | Churn Δ: 48 | Churn Cumulative: 2088 | Contributors (this commit): 10 | Commits (past 90d): 22 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -140,7 +140,6 @@ class Trainer:\n                 wrapped in a `LossScaleOptimizer`, which will dynamically\n                 scale the loss to prevent underflow.\n         \"\"\"\n-        self._clear_previous_trainer_metrics()\n         optimizer = optimizers.get(optimizer)\n         self.optimizer = optimizer\n         if (\n@@ -287,21 +286,6 @@ class Trainer:\n         metrics.extend(self._metrics)\n         return metrics\n \n-    def _clear_previous_trainer_metrics(self):\n-        for layer in self._flatten_layers(include_self=False):\n-            if not isinstance(layer, Trainer):\n-                continue\n-            # A sublayer might be a Trainer. In that case, we need to clear\n-            # the Trainer-related metrics, as they are not usable when a\n-            # new Trainer is instantiated.\n-            for m in self._get_own_metrics():\n-                layer._tracker.untrack(m)\n-            layer._loss_tracker = None\n-            layer._compile_metrics = None\n-            if layer._compile_loss is not None:\n-                layer._compile_loss._metrics.clear()\n-            layer._metrics.clear()\n-\n     def compute_loss(\n         self,\n         x=None,\n\n@@ -407,6 +407,38 @@ class TestTrainer(testing.TestCase):\n         self.assertEqual(new_model.metrics[0], new_model._loss_tracker)\n         self.assertEqual(new_model.metrics[1], new_model._compile_metrics)\n \n+    def test_multiple_compiles(self):\n+        # https://github.com/keras-team/keras/issues/20474\n+        model1 = ExampleModel(units=3)\n+        model2 = ExampleModel(units=3)\n+        model1.compile(\n+            optimizer=optimizers.SGD(),\n+            loss=losses.MeanSquaredError(),\n+            metrics=[metrics.MeanSquaredError()],\n+        )\n+\n+        # Combine these 2 models into `combined`.\n+        inputs = keras.Input(shape=(4,))\n+        x = model1(inputs)\n+        outputs = model2(x)\n+        combined = models.Model(inputs, outputs)\n+        combined.compile(\n+            optimizer=optimizers.SGD(),\n+            loss=losses.MeanSquaredError(),\n+            metrics=[metrics.MeanSquaredError()],\n+        )\n+\n+        self.assertLen(model1.metrics, 2)\n+        self.assertIsNotNone(model1._loss_tracker)\n+        self.assertEqual(model1.metrics[0], model1._loss_tracker)\n+        self.assertEqual(model1.metrics[1], model1._compile_metrics)\n+\n+        # `combined.metrics` will not include `model1.metrics`.\n+        self.assertLen(combined.metrics, 2)\n+        self.assertIsNotNone(combined._loss_tracker)\n+        self.assertEqual(combined.metrics[0], combined._loss_tracker)\n+        self.assertEqual(combined.metrics[1], combined._compile_metrics)\n+\n     @pytest.mark.skipif(\n         backend.backend() != \"torch\",\n         reason=\"torch backend runs in eager mode for jit_compile='auto'\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#38b9b9ca8611ce3ca7d9613a5a65ef63ac6c1375", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 88 | Lines Deleted: 1 | Files Changed: 4 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 10/3 | Churn Δ: 89 | Churn Cumulative: 1222 | Contributors (this commit): 15 | Commits (past 90d): 25 | Contributors (cumulative): 27 | DMM Complexity: 0.9821428571428571\n\nDIFF:\n@@ -5,6 +5,7 @@ import pytest\n import tensorflow as tf\n from tensorflow.python.eager import context\n \n+import keras\n from keras.src import backend\n from keras.src import layers\n from keras.src import models\n@@ -135,3 +136,42 @@ class DistributeTest(testing.TestCase):\n             v2 = backend.Variable(x, dtype=\"float32\", aggregation=\"sum\")\n             self.assertEqual(v2.aggregation, \"sum\")\n             self.assertEqual(v2.value.aggregation, tf.VariableAggregation.SUM)\n+\n+    def test_correctness_with_fit_and_regularizer(self):\n+        strategy = tf.distribute.MirroredStrategy([\"CPU:0\", \"CPU:1\"])\n+\n+        batch_size = 12\n+        x = keras.ops.ones((batch_size, 1))\n+        y = keras.ops.zeros((batch_size, 1))\n+\n+        # Runs without a strategy to get expected weights.\n+        inputs = layers.Input(shape=(1,))\n+        layer = layers.Dense(\n+            1,\n+            use_bias=False,\n+            kernel_initializer=keras.initializers.Constant(1),\n+            kernel_regularizer=keras.regularizers.L1L2(l1=0.01, l2=0.01),\n+        )\n+        model = models.Model(inputs, layer(inputs))\n+        model.compile(loss=\"mse\", optimizer=\"sgd\")\n+        model.fit(x, y, batch_size=batch_size, epochs=1)\n+\n+        expected_weights = keras.ops.convert_to_numpy(layer.kernel)\n+\n+        # Runs with a mirrored strategy.\n+        with strategy.scope():\n+            inputs = layers.Input(shape=(1,))\n+            layer = layers.Dense(\n+                1,\n+                use_bias=False,\n+                kernel_initializer=keras.initializers.Constant(1),\n+                kernel_regularizer=keras.regularizers.L1L2(l1=0.01, l2=0.01),\n+            )\n+            model = models.Model(inputs, layer(inputs))\n+            model.compile(loss=\"mse\", optimizer=\"sgd\")\n+            model.fit(x, y, batch_size=batch_size, epochs=1)\n+            weights = strategy.run(lambda: layer.kernel.value).values\n+            for w in weights:\n+                self.assertAllClose(\n+                    keras.ops.convert_to_numpy(w), expected_weights\n+                )\n\n@@ -5,8 +5,10 @@ import numpy as np\n import tensorflow as tf\n from tensorflow.python.eager import context as tf_context\n \n+from keras.src import backend as backend_module\n from keras.src import callbacks as callbacks_module\n from keras.src import metrics as metrics_module\n+from keras.src import ops as ops_module\n from keras.src import optimizers as optimizers_module\n from keras.src import tree\n from keras.src.trainers import trainer as base_trainer\n@@ -707,6 +709,19 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         with self.distribute_strategy.scope():\n             self._symbolic_build(data_batch=data_batch)\n \n+    def _aggregate_additional_loss(self, loss):\n+        if not backend_module.is_float_dtype(loss.dtype):\n+            loss = ops_module.cast(loss, dtype=backend_module.floatx())\n+        loss = ops_module.sum(loss)\n+\n+        # Scales the loss by the number of replicas in the strategy.\n+        num_replicas = tf.distribute.get_strategy().num_replicas_in_sync\n+        if num_replicas > 1:\n+            loss = ops_module.multiply(\n+                loss, ops_module.cast(1.0 / num_replicas, loss.dtype)\n+            )\n+        return loss\n+\n \n class TFEpochIterator(EpochIterator):\n     def __init__(self, distribute_strategy=None, *args, **kwargs):\n\n@@ -157,6 +157,7 @@ def reduce_values(values, sample_weight=None, reduction=\"sum_over_batch_size\"):\n                 loss.dtype,\n             )\n         loss = ops.divide_no_nan(loss, divisor)\n+        loss = scale_loss_for_distribution(loss)\n     return loss\n \n \n@@ -221,3 +222,20 @@ def apply_mask(sample_weight, mask, dtype, reduction):\n         else:\n             sample_weight = mask\n     return sample_weight\n+\n+\n+def scale_loss_for_distribution(value):\n+    \"\"\"Scales the given value by the number of replicas in the strategy.\n+\n+    Currently, this function is only effective when using the tensorflow backend\n+    and `tf.distribute`.\n+    \"\"\"\n+    if backend.backend() == \"tensorflow\":\n+        import tensorflow as tf\n+\n+        num_replicas = tf.distribute.get_strategy().num_replicas_in_sync\n+        if num_replicas > 1:\n+            value = ops.multiply(\n+                value, ops.cast(1.0 / num_replicas, value.dtype)\n+            )\n+    return value\n\n@@ -352,7 +352,7 @@ class Trainer:\n             if loss is not None:\n                 losses.append(loss)\n         for loss in self.losses:\n-            losses.append(ops.sum(ops.cast(loss, dtype=backend.floatx())))\n+            losses.append(self._aggregate_additional_loss(loss))\n         if backend.backend() != \"jax\" and len(losses) == 0:\n             raise ValueError(\n                 \"No loss to compute. Provide a `loss` argument in `compile()`.\"\n@@ -386,6 +386,20 @@ class Trainer:\n         else:\n             return self.compute_loss(x, y, y_pred, sample_weight)\n \n+    def _aggregate_additional_loss(self, loss):\n+        \"\"\"Aggregates losses from `add_loss`, regularizers and sublayers.\n+\n+        Args:\n+            loss: A tensor representing the additional loss to aggregate.\n+\n+        Returns:\n+            A tensor representing the summed loss, cast to the `floatx()` if\n+            necessary.\n+        \"\"\"\n+        if not backend.is_float_dtype(loss.dtype):\n+            loss = ops.cast(loss, dtype=backend.floatx())\n+        return ops.sum(loss)\n+\n     def stateless_compute_loss(\n         self,\n         trainable_variables,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9b1159f1c62edc6a21cbe0aaedfc1c27b41cd1fb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 212 | Lines Deleted: 0 | Files Changed: 5 | Hunks: 5 | Methods Changed: 12 | Complexity Δ (Sum/Max): 15/10 | Churn Δ: 212 | Churn Cumulative: 988 | Contributors (this commit): 6 | Commits (past 90d): 24 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -151,6 +151,7 @@ from keras.src.layers.preprocessing.image_preprocessing.equalization import (\n from keras.src.layers.preprocessing.image_preprocessing.max_num_bounding_box import (\n     MaxNumBoundingBoxes,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.mix_up import MixUp\n from keras.src.layers.preprocessing.image_preprocessing.random_brightness import (\n     RandomBrightness,\n )\n\n@@ -151,6 +151,7 @@ from keras.src.layers.preprocessing.image_preprocessing.equalization import (\n from keras.src.layers.preprocessing.image_preprocessing.max_num_bounding_box import (\n     MaxNumBoundingBoxes,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.mix_up import MixUp\n from keras.src.layers.preprocessing.image_preprocessing.random_brightness import (\n     RandomBrightness,\n )\n\n@@ -94,6 +94,7 @@ from keras.src.layers.preprocessing.image_preprocessing.equalization import (\n from keras.src.layers.preprocessing.image_preprocessing.max_num_bounding_box import (\n     MaxNumBoundingBoxes,\n )\n+from keras.src.layers.preprocessing.image_preprocessing.mix_up import MixUp\n from keras.src.layers.preprocessing.image_preprocessing.random_brightness import (\n     RandomBrightness,\n )\n\n@@ -0,0 +1,146 @@\n+import keras.src.random.random\n+from keras.src.api_export import keras_export\n+from keras.src.layers.preprocessing.image_preprocessing.base_image_preprocessing_layer import (  # noqa: E501\n+    BaseImagePreprocessingLayer,\n+)\n+from keras.src.random import SeedGenerator\n+\n+\n+@keras_export(\"keras.layers.MixUp\")\n+class MixUp(BaseImagePreprocessingLayer):\n+    \"\"\"MixUp implements the MixUp data augmentation technique.\n+\n+    Args:\n+        alpha: Float between 0 and 1. Controls the blending strength.\n+               Smaller values mean less mixing, while larger values allow\n+               for more  blending between images. Defaults to 0.2,\n+               recommended for ImageNet1k classification.\n+        seed: Integer. Used to create a random seed.\n+\n+    References:\n+        - [MixUp paper](https://arxiv.org/abs/1710.09412).\n+        - [MixUp for Object Detection paper](https://arxiv.org/pdf/1902.04103).\n+\n+    Example:\n+    ```python\n+    (images, labels), _ = keras.datasets.cifar10.load_data()\n+    images, labels = images[:10], labels[:10]\n+    # Labels must be floating-point and one-hot encoded\n+    labels = tf.cast(tf.one_hot(labels, 10), tf.float32)\n+    mixup = keras.layers.MixUp(alpha=0.2)\n+    augmented_images, updated_labels = mixup(\n+        {'images': images, 'labels': labels}\n+    )\n+    # output == {'images': updated_images, 'labels': updated_labels}\n+    ```\n+    \"\"\"\n+\n+    def __init__(self, alpha=0.2, data_format=None, seed=None, **kwargs):\n+        super().__init__(data_format=None, **kwargs)\n+        self.alpha = alpha\n+        self.seed = seed\n+        self.generator = SeedGenerator(seed)\n+\n+    def get_random_transformation(self, data, training=True, seed=None):\n+        if isinstance(data, dict):\n+            images = data[\"images\"]\n+        else:\n+            images = data\n+\n+        images_shape = self.backend.shape(images)\n+\n+        if len(images_shape) == 3:\n+            batch_size = 1\n+        else:\n+            batch_size = self.backend.shape(images)[0]\n+\n+        permutation_order = self.backend.random.shuffle(\n+            self.backend.numpy.arange(0, batch_size, dtype=\"int64\"),\n+            seed=self.generator,\n+        )\n+\n+        mix_weight = keras.src.random.random.beta(\n+            (1,), self.alpha, self.alpha, seed=self.generator\n+        )\n+        return {\n+            \"mix_weight\": mix_weight,\n+            \"permutation_order\": permutation_order,\n+        }\n+\n+    def transform_images(self, images, transformation=None, training=True):\n+        mix_weight = transformation[\"mix_weight\"]\n+        permutation_order = transformation[\"permutation_order\"]\n+\n+        mix_weight = self.backend.cast(\n+            self.backend.numpy.reshape(mix_weight, [-1, 1, 1, 1]),\n+            dtype=self.compute_dtype,\n+        )\n+\n+        mixup_images = self.backend.cast(\n+            self.backend.numpy.take(images, permutation_order, axis=0),\n+            dtype=self.compute_dtype,\n+        )\n+\n+        images = mix_weight * images + (1.0 - mix_weight) * mixup_images\n+\n+        return images\n+\n+    def transform_labels(self, labels, transformation, training=True):\n+        mix_weight = transformation[\"mix_weight\"]\n+        permutation_order = transformation[\"permutation_order\"]\n+\n+        labels_for_mixup = self.backend.numpy.take(\n+            labels, permutation_order, axis=0\n+        )\n+\n+        mix_weight = self.backend.numpy.reshape(mix_weight, [-1, 1])\n+\n+        labels = mix_weight * labels + (1.0 - mix_weight) * labels_for_mixup\n+\n+        return labels\n+\n+    def transform_bounding_boxes(\n+        self,\n+        bounding_boxes,\n+        transformation,\n+        training=True,\n+    ):\n+        permutation_order = transformation[\"permutation_order\"]\n+        boxes, classes = bounding_boxes[\"boxes\"], bounding_boxes[\"classes\"]\n+        boxes_for_mixup = self.backend.numpy.take(boxes, permutation_order)\n+        classes_for_mixup = self.backend.numpy.take(classes, permutation_order)\n+        boxes = self.backend.numpy.concat([boxes, boxes_for_mixup], axis=1)\n+        classes = self.backend.numpy.concat(\n+            [classes, classes_for_mixup], axis=1\n+        )\n+        return {\"boxes\": boxes, \"classes\": classes}\n+\n+    def transform_segmentation_masks(\n+        self, segmentation_masks, transformation, training=True\n+    ):\n+        mix_weight = transformation[\"mix_weight\"]\n+        permutation_order = transformation[\"permutation_order\"]\n+\n+        mix_weight = self.backend.numpy.reshape(mix_weight, [-1, 1, 1, 1])\n+\n+        segmentation_masks_for_mixup = self.backend.numpy.take(\n+            segmentation_masks, permutation_order\n+        )\n+\n+        segmentation_masks = (\n+            mix_weight * segmentation_masks\n+            + (1.0 - mix_weight) * segmentation_masks_for_mixup\n+        )\n+\n+        return segmentation_masks\n+\n+    def compute_output_shape(self, input_shape):\n+        return input_shape\n+\n+    def get_config(self):\n+        config = {\n+            \"alpha\": self.alpha,\n+            \"seed\": self.seed,\n+        }\n+        base_config = super().get_config()\n+        return {**base_config, **config}\n\n@@ -0,0 +1,63 @@\n+import numpy as np\n+import pytest\n+from tensorflow import data as tf_data\n+\n+from keras.src import layers\n+from keras.src import testing\n+\n+\n+class MixUpTest(testing.TestCase):\n+    @pytest.mark.requires_trainable_backend\n+    def test_layer(self):\n+        self.run_layer_test(\n+            layers.MixUp,\n+            init_kwargs={\n+                \"alpha\": 0.2,\n+            },\n+            input_shape=(8, 3, 4, 3),\n+            supports_masking=False,\n+            expected_output_shape=(8, 3, 4, 3),\n+        )\n+\n+    def test_mix_up_basic_functionality(self):\n+        image = np.random.random((64, 64, 3))\n+        mix_up_layer = layers.MixUp(alpha=1)\n+        transformation = {\"mix_weight\": 1, \"permutation_order\": [0]}\n+        output = mix_up_layer.transform_images(\n+            image, transformation=transformation\n+        )[0]\n+        self.assertAllClose(output, image)\n+\n+        image = np.random.random((4, 64, 64, 3))\n+        mix_up_layer = layers.MixUp(alpha=0.2)\n+        transformation = {\"mix_weight\": 0.2, \"permutation_order\": [1, 0, 2, 3]}\n+        output = mix_up_layer.transform_images(\n+            image, transformation=transformation\n+        )\n+        self.assertNotAllClose(output, image)\n+        self.assertAllClose(output.shape, image.shape)\n+\n+    def test_mix_up_basic_functionality_channel_first(self):\n+        image = np.random.random((3, 64, 64))\n+        mix_up_layer = layers.MixUp(alpha=1)\n+        transformation = {\"mix_weight\": 1, \"permutation_order\": [0]}\n+        output = mix_up_layer.transform_images(\n+            image, transformation=transformation\n+        )[0]\n+        self.assertAllClose(output, image)\n+\n+        image = np.random.random((4, 3, 64, 64))\n+        mix_up_layer = layers.MixUp(alpha=0.2)\n+        transformation = {\"mix_weight\": 0.2, \"permutation_order\": [1, 0, 2, 3]}\n+        output = mix_up_layer.transform_images(\n+            image, transformation=transformation\n+        )\n+        self.assertNotAllClose(output, image)\n+        self.assertAllClose(output.shape, image.shape)\n+\n+    def test_tf_data_compatibility(self):\n+        layer = layers.MixUp()\n+        input_data = np.random.random((2, 8, 8, 3))\n+        ds = tf_data.Dataset.from_tensor_slices(input_data).batch(2).map(layer)\n+        for output in ds.take(1):\n+            output.numpy()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#811d0042739ec428fb17f1ae96294344851399e3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 10 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -45,8 +45,8 @@ class ConstraintsTest(testing.TestCase):\n         output = constraint_fn(get_example_array())\n         output = backend.convert_to_numpy(output)\n         l2 = np.sqrt(np.sum(np.square(output), axis=0))\n-        self.assertFalse(l2[l2 < 0.2])\n-        self.assertFalse(l2[l2 > 0.5 + 1e-6])\n+        self.assertTrue(np.all(l2 >= 0.2))\n+        self.assertTrue(np.all(l2 <= 0.5 + 1e-6))\n \n     def test_get_method(self):\n         obj = constraints.get(\"unit_norm\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b1e405710e18f2805fb60e5b678208b25f76a36d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 7 | Files Changed: 3 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 17 | Churn Cumulative: 564 | Contributors (this commit): 11 | Commits (past 90d): 13 | Contributors (cumulative): 19 | DMM Complexity: 0.5\n\nDIFF:\n@@ -44,13 +44,7 @@ class Variable(\n         )\n \n     def _initialize_with_initializer(self, initializer):\n-        self._value = tf.Variable(\n-            lambda: initializer(self._shape, dtype=self._dtype),\n-            dtype=self._dtype,\n-            trainable=self.trainable,\n-            name=self.name,\n-            aggregation=self._map_aggregation(self.aggregation),\n-        )\n+        self._initialize(lambda: initializer(self._shape, dtype=self._dtype))\n \n     def _deferred_initialize(self):\n         if self._value is not None:\n\n@@ -137,6 +137,14 @@ class DistributeTest(testing.TestCase):\n             self.assertEqual(v2.aggregation, \"sum\")\n             self.assertEqual(v2.value.aggregation, tf.VariableAggregation.SUM)\n \n+    def test_seed_generator(self):\n+        strategy = tf.distribute.MirroredStrategy([\"CPU:0\", \"CPU:1\"])\n+        with strategy.scope():\n+            seed_generator = keras.random.SeedGenerator(42)\n+            states = strategy.run(lambda: seed_generator.state.value).values\n+            for s in states:\n+                self.assertAllClose(keras.ops.convert_to_numpy(s), (42, 0))\n+\n     def test_correctness_with_fit_and_regularizer(self):\n         strategy = tf.distribute.MirroredStrategy([\"CPU:0\", \"CPU:1\"])\n \n\n@@ -89,6 +89,7 @@ class SeedGenerator:\n                 shape=(2,),\n                 dtype=self.backend.random_seed_dtype(),\n                 trainable=False,\n+                aggregation=\"none\",\n                 name=\"seed_generator_state\",\n             )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c7898055e61b3c8020a5b98e10c5f23627392cd8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 19 | Files Changed: 4 | Hunks: 12 | Methods Changed: 6 | Complexity Δ (Sum/Max): 1/3 | Churn Δ: 49 | Churn Cumulative: 1687 | Contributors (this commit): 16 | Commits (past 90d): 34 | Contributors (cumulative): 28 | DMM Complexity: 0.8\n\nDIFF:\n@@ -162,8 +162,8 @@ class DistributeTest(testing.TestCase):\n         )\n         model = models.Model(inputs, layer(inputs))\n         model.compile(loss=\"mse\", optimizer=\"sgd\")\n-        model.fit(x, y, batch_size=batch_size, epochs=1)\n-\n+        history = model.fit(x, y, batch_size=batch_size, epochs=1)\n+        expected_loss = history.history[\"loss\"]\n         expected_weights = keras.ops.convert_to_numpy(layer.kernel)\n \n         # Runs with a mirrored strategy.\n@@ -177,8 +177,10 @@ class DistributeTest(testing.TestCase):\n             )\n             model = models.Model(inputs, layer(inputs))\n             model.compile(loss=\"mse\", optimizer=\"sgd\")\n-            model.fit(x, y, batch_size=batch_size, epochs=1)\n+            history = model.fit(x, y, batch_size=batch_size, epochs=1)\n             weights = strategy.run(lambda: layer.kernel.value).values\n+\n+            self.assertAllClose(history.history[\"loss\"], expected_loss)\n             for w in weights:\n                 self.assertAllClose(\n                     keras.ops.convert_to_numpy(w), expected_weights\n\n@@ -5,12 +5,11 @@ import numpy as np\n import tensorflow as tf\n from tensorflow.python.eager import context as tf_context\n \n-from keras.src import backend as backend_module\n from keras.src import callbacks as callbacks_module\n from keras.src import metrics as metrics_module\n-from keras.src import ops as ops_module\n from keras.src import optimizers as optimizers_module\n from keras.src import tree\n+from keras.src.losses import loss as loss_module\n from keras.src.trainers import trainer as base_trainer\n from keras.src.trainers.data_adapters import array_slicing\n from keras.src.trainers.data_adapters import data_adapter_utils\n@@ -66,7 +65,8 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 training=True,\n             )\n             self._loss_tracker.update_state(\n-                loss, sample_weight=tf.shape(tree.flatten(x)[0])[0]\n+                loss_module.unscale_loss_for_distribution(loss),\n+                sample_weight=tf.shape(tree.flatten(x)[0])[0],\n             )\n             if self.optimizer is not None:\n                 loss = self.optimizer.scale_loss(loss)\n@@ -93,7 +93,8 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             x=x, y=y, y_pred=y_pred, sample_weight=sample_weight, training=False\n         )\n         self._loss_tracker.update_state(\n-            loss, sample_weight=tf.shape(tree.flatten(x)[0])[0]\n+            loss_module.unscale_loss_for_distribution(loss),\n+            sample_weight=tf.shape(tree.flatten(x)[0])[0],\n         )\n         return self.compute_metrics(x, y, y_pred, sample_weight=sample_weight)\n \n@@ -710,17 +711,8 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             self._symbolic_build(data_batch=data_batch)\n \n     def _aggregate_additional_loss(self, loss):\n-        if not backend_module.is_float_dtype(loss.dtype):\n-            loss = ops_module.cast(loss, dtype=backend_module.floatx())\n-        loss = ops_module.sum(loss)\n-\n-        # Scales the loss by the number of replicas in the strategy.\n-        num_replicas = tf.distribute.get_strategy().num_replicas_in_sync\n-        if num_replicas > 1:\n-            loss = ops_module.multiply(\n-                loss, ops_module.cast(1.0 / num_replicas, loss.dtype)\n-            )\n-        return loss\n+        loss = super()._aggregate_additional_loss(loss)\n+        return loss_module.scale_loss_for_distribution(loss)\n \n \n class TFEpochIterator(EpochIterator):\n\n@@ -239,3 +239,18 @@ def scale_loss_for_distribution(value):\n                 value, ops.cast(1.0 / num_replicas, value.dtype)\n             )\n     return value\n+\n+\n+def unscale_loss_for_distribution(value):\n+    \"\"\"Unscales the given value by the number of replicas in the strategy.\n+\n+    Currently, this function is only effective when using the tensorflow backend\n+    and `tf.distribute`.\n+    \"\"\"\n+    if backend.backend() == \"tensorflow\":\n+        import tensorflow as tf\n+\n+        num_replicas = tf.distribute.get_strategy().num_replicas_in_sync\n+        if num_replicas > 1:\n+            value = ops.multiply(value, ops.cast(num_replicas, value.dtype))\n+    return value\n\n@@ -5,6 +5,7 @@ from keras.src import metrics as metrics_module\n from keras.src import ops\n from keras.src import tree\n from keras.src.backend.common.keras_tensor import KerasTensor\n+from keras.src.losses import loss as loss_module\n from keras.src.utils.naming import get_object_name\n from keras.src.utils.tracking import Tracker\n \n@@ -799,7 +800,8 @@ class CompileLoss(losses_module.Loss):\n             # Record *unweighted* individual losses.\n             if metric:\n                 metric.update_state(\n-                    value, sample_weight=tree.flatten(y_p)[0].shape[0]\n+                    loss_module.unscale_loss_for_distribution(value),\n+                    sample_weight=tree.flatten(y_p)[0].shape[0],\n                 )\n             if loss_weight is not None:\n                 value = ops.multiply(value, loss_weight)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#00d0f492c475fcd1f30e201219c80ad8e96c2678", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/3 | Churn Δ: 24 | Churn Cumulative: 736 | Contributors (this commit): 10 | Commits (past 90d): 18 | Contributors (cumulative): 16 | DMM Complexity: 0.65\n\nDIFF:\n@@ -305,7 +305,13 @@ class Functional(Function, Model):\n                     raise_exception = True\n             else:\n                 raise_exception = True\n-\n+        if (\n+            isinstance(self._inputs_struct, dict)\n+            and not isinstance(inputs, dict)\n+            and list(self._inputs_struct.keys())\n+            != sorted(self._inputs_struct.keys())\n+        ):\n+            raise_exception = True\n         self._maybe_warn_inputs_struct_mismatch(\n             inputs, raise_exception=raise_exception\n         )\n\n@@ -15,6 +15,7 @@ from keras.src.layers.input_spec import InputSpec\n from keras.src.models import Functional\n from keras.src.models import Model\n from keras.src.models import Sequential\n+from keras.src import ops\n \n \n class FunctionalTest(testing.TestCase):\n@@ -573,7 +574,6 @@ class FunctionalTest(testing.TestCase):\n         with pytest.warns() as warning_logs:\n             model.predict([np.ones((2, 2)), np.zeros((2, 2))], verbose=0)\n             self.assertLen(list(filter(is_input_warning, warning_logs)), 1)\n-\n         # No warning for mismatched tuples and lists.\n         model = Model([i1, i2], outputs)\n         with warnings.catch_warnings(record=True) as warning_logs:\n@@ -699,3 +699,17 @@ class FunctionalTest(testing.TestCase):\n                 \"tags\": tags_data,\n             }\n         )\n+\n+    def test_list_input_with_dict_build(self):\n+        x1 = Input((10,), name=\"IT\")\n+        x2 = Input((10,), name=\"IS\")\n+        y = layers.subtract([x1, x2])\n+        model = Model(inputs={\"IT\": x1, \"IS\": x2}, outputs=y)\n+        x1 = ops.ones((1, 10))\n+        x2 = ops.zeros((1, 10))\n+        r1 = model({\"IT\": x1, \"IS\": x2})\n+        with self.assertRaisesRegex(\n+            ValueError,\n+            \"The structure of `inputs` doesn't match the expected structure\",\n+        ):\n+            r2 = model([x1, x2])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#5b6b9b0b9543b6d87e13fab7d621eb6fde9bb8ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 417 | Contributors (this commit): 7 | Commits (past 90d): 10 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -8,6 +8,7 @@ from absl.testing import parameterized\n from keras.src import applications\n from keras.src import backend\n from keras.src import layers\n+from keras.src import ops\n from keras.src import saving\n from keras.src import testing\n from keras.src.layers.core.input_layer import Input\n@@ -15,7 +16,6 @@ from keras.src.layers.input_spec import InputSpec\n from keras.src.models import Functional\n from keras.src.models import Model\n from keras.src.models import Sequential\n-from keras.src import ops\n \n \n class FunctionalTest(testing.TestCase):\n@@ -707,9 +707,10 @@ class FunctionalTest(testing.TestCase):\n         model = Model(inputs={\"IT\": x1, \"IS\": x2}, outputs=y)\n         x1 = ops.ones((1, 10))\n         x2 = ops.zeros((1, 10))\n-        r1 = model({\"IT\": x1, \"IS\": x2})\n+        # Works\n+        _ = model({\"IT\": x1, \"IS\": x2})\n         with self.assertRaisesRegex(\n             ValueError,\n             \"The structure of `inputs` doesn't match the expected structure\",\n         ):\n-            r2 = model([x1, x2])\n+            model([x1, x2])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c6c07204789d2bc811f450bf8c46d77833109665", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 12 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 22 | Churn Cumulative: 1159 | Contributors (this commit): 13 | Commits (past 90d): 30 | Contributors (cumulative): 20 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1096,12 +1096,14 @@ def _apply_masks(logits, mask, is_causal):\n \n \n def _dot_product_attention_xla(query, key, value, bias, mask, is_causal, scale):\n+    original_dtype = key.dtype\n     logits_dtype = np.promote_types(query.dtype, np.float32)\n-    logits = np.einsum(\n-        \"BTNH,BSNH->BNTS\",\n-        query.astype(logits_dtype),\n-        key.astype(logits_dtype),\n-    )\n+    if backend.standardize_dtype(key.dtype) == \"bfloat16\":\n+        # `np.einsum` doesn't support bfloat16\n+        key = key.astype(\"float32\")\n+        value = value.astype(\"float32\")\n+    logits = np.einsum(\"BTNH,BSNH->BNTS\", query, key)\n+    logits = logits.astype(logits_dtype)\n     logits *= np.array(scale, dtype=logits.dtype)\n \n     if bias is not None:\n@@ -1111,7 +1113,7 @@ def _dot_product_attention_xla(query, key, value, bias, mask, is_causal, scale):\n \n     # Softmax and it is always carried out in fp32.\n     padded_logits = padded_logits.astype(np.float32)\n-    probs = softmax(padded_logits, axis=-1).astype(key.dtype)\n+    probs = softmax(padded_logits, axis=-1).astype(original_dtype)\n     encoded_dtype = probs.dtype\n     if backend.standardize_dtype(probs.dtype) == \"bfloat16\":\n         # `np.einsum` doesn't support bfloat16\n\n@@ -1015,12 +1015,8 @@ def _apply_masks(logits, mask, is_causal):\n \n def _dot_product_attention_xla(query, key, value, bias, mask, is_causal, scale):\n     logits_dtype = backend.result_type(query.dtype, \"float32\")\n-    logits = tf.einsum(\n-        \"BTNH,BSNH->BNTS\",\n-        tf.cast(query, dtype=logits_dtype),\n-        tf.cast(key, dtype=logits_dtype),\n-        optimize=\"optimal\",\n-    )\n+    logits = tf.einsum(\"BTNH,BSNH->BNTS\", query, key, optimize=\"optimal\")\n+    logits = tf.cast(logits, logits_dtype)\n     logits = tf.multiply(logits, tf.cast(scale, logits.dtype))\n \n     if bias is not None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9294db1974066eb3383d8a8d988baafbcb6bf246", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 154 | Lines Deleted: 18 | Files Changed: 2 | Hunks: 21 | Methods Changed: 12 | Complexity Δ (Sum/Max): 8/5 | Churn Δ: 172 | Churn Cumulative: 193 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 0.8921568627450981\n\nDIFF:\n@@ -1,3 +1,5 @@\n+import warnings\n+\n from keras.src import backend\n from keras.src import initializers\n from keras.src import ops\n@@ -55,8 +57,8 @@ class _IoUBase(Metric):\n         sparse_y_pred=True,\n         axis=-1,\n     ):\n-        # defaulting to float32 to avoid issues with confusion matrix\n-        super().__init__(name=name, dtype=dtype or \"float32\")\n+        # defaulting to int to avoid issues with confusion matrix\n+        super().__init__(name=name, dtype=dtype or \"int\")\n         # Metric should be maximized during optimization.\n         self._direction = \"up\"\n         self.num_classes = num_classes\n@@ -69,6 +71,7 @@ class _IoUBase(Metric):\n             name=\"total_confusion_matrix\",\n             shape=(num_classes, num_classes),\n             initializer=initializers.Zeros(),\n+            dtype=self.dtype,\n         )\n \n     def update_state(self, y_true, y_pred, sample_weight=None):\n@@ -102,7 +105,17 @@ class _IoUBase(Metric):\n \n         if sample_weight is None:\n             sample_weight = 1\n-\n+        else:\n+            if (\n+                hasattr(sample_weight, \"dtype\")\n+                and \"float\" in str(sample_weight.dtype)\n+                and \"int\" in str(self.dtype)\n+            ):\n+                warnings.warn(\n+                    \"You are passing weight as `float`, but dtype is `int`. \"\n+                    \"This may result in an incorrect weight due to type casting\"\n+                    \" Consider using integer weights.\"\n+                )\n         sample_weight = ops.convert_to_tensor(sample_weight, dtype=self.dtype)\n \n         if len(sample_weight.shape) > 1:\n@@ -131,7 +144,7 @@ class _IoUBase(Metric):\n             y_pred,\n             self.num_classes,\n             weights=sample_weight,\n-            dtype=\"float32\",\n+            dtype=self.dtype,\n         )\n \n         return self.total_cm.assign(self.total_cm + current_cm)\n@@ -272,10 +285,11 @@ class IoU(_IoUBase):\n         denominator = ops.take_along_axis(\n             denominator, target_class_ids, axis=-1\n         )\n+        denominator = ops.cast(denominator, dtype=\"float32\")\n \n         # If the denominator is 0, we need to ignore the class.\n         num_valid_entries = ops.sum(\n-            ops.cast(ops.greater(denominator, 1e-9), dtype=self.dtype)\n+            ops.cast(ops.greater(denominator, 1e-9), dtype=\"float32\")\n         )\n \n         iou = ops.divide(true_positives, denominator + backend.epsilon())\n@@ -406,7 +420,8 @@ class BinaryIoU(IoU):\n             Update op.\n         \"\"\"\n         y_true = ops.convert_to_tensor(y_true, dtype=self.dtype)\n-        y_pred = ops.convert_to_tensor(y_pred, dtype=self.dtype)\n+        # convert y_pred on float 32 and cast just after to dtype\n+        y_pred = ops.convert_to_tensor(y_pred, dtype=\"float32\")\n         y_pred = ops.cast(y_pred >= self.threshold, self.dtype)\n         return super().update_state(y_true, y_pred, sample_weight)\n \n\n@@ -5,6 +5,7 @@ from keras.src import layers\n from keras.src import models\n from keras.src import testing\n from keras.src.metrics import iou_metrics as metrics\n+from keras.src.ops import convert_to_tensor\n \n \n class IoUTest(testing.TestCase):\n@@ -25,9 +26,7 @@ class IoUTest(testing.TestCase):\n         y_pred = [0, 1, 0, 1]\n         y_true = [0, 0, 1, 1]\n \n-        obj = metrics.IoU(\n-            num_classes=2, target_class_ids=[0, 1], dtype=\"float32\"\n-        )\n+        obj = metrics.IoU(num_classes=2, target_class_ids=[0, 1])\n \n         result = obj(y_true, y_pred)\n \n@@ -64,7 +63,9 @@ class IoUTest(testing.TestCase):\n         y_true = np.array([[0, 0], [1, 1]])\n         sample_weight = np.array([[0.2, 0.3], [0.4, 0.1]])\n \n-        obj = metrics.IoU(num_classes=2, target_class_ids=[0, 1])\n+        obj = metrics.IoU(\n+            num_classes=2, target_class_ids=[0, 1], dtype=\"float32\"\n+        )\n \n         result = obj(y_true, y_pred, sample_weight=sample_weight)\n \n@@ -136,7 +137,9 @@ class BinaryIoUTest(testing.TestCase):\n         expected_result = (\n             0.2 / (0.6 + 0.5 - 0.2) + 0.1 / (0.4 + 0.5 - 0.1)\n         ) / 2\n-        obj = metrics.BinaryIoU(target_class_ids=[0, 1], threshold=0.3)\n+        obj = metrics.BinaryIoU(\n+            target_class_ids=[0, 1], threshold=0.3, dtype=\"float32\"\n+        )\n         result = obj(y_true, y_pred, sample_weight=sample_weight)\n         self.assertAllClose(result, expected_result, atol=1e-3)\n \n@@ -150,7 +153,9 @@ class BinaryIoUTest(testing.TestCase):\n         expected_result = (\n             0.5 / (0.5 + 0.7 - 0.5) + 0.3 / (0.5 + 0.3 - 0.3)\n         ) / 2\n-        obj = metrics.BinaryIoU(target_class_ids=[0, 1], threshold=0.5)\n+        obj = metrics.BinaryIoU(\n+            target_class_ids=[0, 1], threshold=0.5, dtype=\"float32\"\n+        )\n         result = obj(y_true, y_pred, sample_weight=sample_weight)\n         self.assertAllClose(result, expected_result, atol=1e-3)\n \n@@ -191,7 +196,9 @@ class BinaryIoUTest(testing.TestCase):\n         expected_result = (\n             0.2 / (0.6 + 0.3 - 0.2) + 0.3 / (0.4 + 0.7 - 0.3)\n         ) / 2\n-        obj = metrics.BinaryIoU(target_class_ids=[0, 1], threshold=threshold)\n+        obj = metrics.BinaryIoU(\n+            target_class_ids=[0, 1], threshold=threshold, dtype=\"float32\"\n+        )\n         result = obj(y_true, y_pred, sample_weight=sample_weight)\n         self.assertAllClose(result, expected_result, atol=1e-3)\n \n@@ -281,7 +288,7 @@ class MeanIoUTest(testing.TestCase):\n         y_true = np.array([0, 0, 1, 1])\n         sample_weight = np.array([0.2, 0.3, 0.4, 0.1])\n \n-        m_obj = metrics.MeanIoU(num_classes=2)\n+        m_obj = metrics.MeanIoU(num_classes=2, dtype=\"float32\")\n \n         result = m_obj(y_true, y_pred, sample_weight=sample_weight)\n \n@@ -300,7 +307,7 @@ class MeanIoUTest(testing.TestCase):\n         y_true = np.array([0, 0, 1, -1])\n         sample_weight = np.array([0.2, 0.3, 0.4, 0.1])\n \n-        m_obj = metrics.MeanIoU(num_classes=2, ignore_class=-1)\n+        m_obj = metrics.MeanIoU(num_classes=2, ignore_class=-1, dtype=\"float32\")\n \n         result = m_obj(y_true, y_pred, sample_weight=sample_weight)\n \n@@ -319,7 +326,7 @@ class MeanIoUTest(testing.TestCase):\n         y_true = np.array([[0, 0], [1, 1]])\n         sample_weight = np.array([[0.2, 0.3], [0.4, 0.1]])\n \n-        m_obj = metrics.MeanIoU(num_classes=2)\n+        m_obj = metrics.MeanIoU(num_classes=2, dtype=\"float32\")\n \n         result = m_obj(y_true, y_pred, sample_weight=sample_weight)\n \n@@ -351,6 +358,112 @@ class MeanIoUTest(testing.TestCase):\n         expected_result = (0 + 1 / (1 + 1 - 1)) / 1\n         self.assertAllClose(result, expected_result, atol=1e-3)\n \n+    @staticmethod\n+    def _confusion_matrix(y_true, y_pred, num_classes):\n+        \"\"\"\n+        Creates a confusion matrix as a numpy array using vectorized operations.\n+\n+        Parameters:\n+        - y_true: array-like, true class labels.\n+        - y_pred: array-like, predicted class labels.\n+        - num_classes: int, number of classes.\n+\n+        Returns:\n+        - conf_matrix: np.ndarray, confusion matrix of shape (num_classes,\n+                                                              num_classes).\n+        \"\"\"\n+        # Map pairs of (y_true, y_pred) to indices in the confusion matrix\n+        indices = y_true * num_classes + y_pred\n+        # Count occurrences of each index\n+        conf_matrix = np.bincount(indices, minlength=num_classes * num_classes)\n+        # Reshape the flat array into a 2D confusion matrix\n+        conf_matrix = conf_matrix.reshape((num_classes, num_classes))\n+        return conf_matrix\n+\n+    @staticmethod\n+    def _get_big_chunk(dtype):\n+        np.random.seed(14)\n+        all_y_true = np.random.choice([0, 1, 2], size=(10, 530, 530))\n+        # Generate random probabilities for each channel\n+        random_probs = np.random.rand(10, 530, 530, 3)\n+        # Normalize to ensure the last dimension sums to 1\n+        all_y_pred = random_probs / random_probs.sum(axis=-1, keepdims=True)\n+        # Convert predictions to class indices\n+        all_y_pred_arg = np.argmax(all_y_pred, axis=-1)\n+        mean_iou_metric = metrics.MeanIoU(num_classes=3, dtype=dtype)\n+        conf_matrix_start_point = np.array(\n+            [\n+                [18729664, 18728760, 18731196],\n+                [18727297, 18726105, 18728071],\n+                [18727917, 18717835, 18723155],\n+            ]\n+        )\n+        mean_iou_metric.total_cm = mean_iou_metric.add_variable(\n+            name=\"total_confusion_matrix\",\n+            shape=(3, 3),\n+            initializer=convert_to_tensor(conf_matrix_start_point),\n+            dtype=dtype or \"int\",\n+        )\n+        mean_iou_metric.update_state(all_y_true, all_y_pred_arg)\n+        tmp_true = np.reshape(all_y_true, -1)\n+        tmp_pred = np.reshape(all_y_pred_arg, -1)\n+        return (\n+            all_y_true,\n+            all_y_pred_arg,\n+            mean_iou_metric,\n+            tmp_true,\n+            tmp_pred,\n+            conf_matrix_start_point,\n+        )\n+\n+    def test_big_chunk(self):\n+        # Init. process with dtype=None which will default to int\n+        (\n+            all_y_true,\n+            all_y_pred_arg,\n+            mean_iou_metric_all,\n+            tmp_true,\n+            tmp_pred,\n+            conf_matrix_start_point,\n+        ) = self._get_big_chunk(dtype=None)\n+        conf_matrix_from_keras = np.array(mean_iou_metric_all.total_cm)\n+        # Validate confusion matrices and results\n+        conf_matrix_manual = (\n+            self._confusion_matrix(tmp_true, tmp_pred, 3)\n+            + conf_matrix_start_point\n+        )\n+        self.assertTrue(\n+            np.array_equal(conf_matrix_from_keras, conf_matrix_manual),\n+            msg=\"Confusion matrices do not match!\",\n+        )\n+        # Now same but with float32 dtype, in here the confusion matrix\n+        # should not match. Likely this can be removed\n+        (\n+            all_y_true,\n+            all_y_pred_arg,\n+            mean_iou_metric_all,\n+            tmp_true,\n+            tmp_pred,\n+            conf_matrix_start_point,\n+        ) = self._get_big_chunk(dtype=\"float32\")\n+        conf_matrix_from_keras = np.array(mean_iou_metric_all.total_cm)\n+        # Validate confusion matrices and results\n+        conf_matrix_manual = (\n+            self._confusion_matrix(tmp_true, tmp_pred, 3)\n+            + conf_matrix_start_point\n+        )\n+        self.assertFalse(\n+            np.array_equal(conf_matrix_from_keras, conf_matrix_manual),\n+            msg=\"Confusion matrices match, but they should not!\",\n+        )\n+\n+    def test_user_warning_float_weight(self):\n+        y_pred = [0, 1, 1, 1]\n+        y_true = [0, 1, 1, 0]\n+        m_obj = metrics.MeanIoU(num_classes=3)\n+        with pytest.warns(Warning, match=r\"weight.*float.*int.*casting\"):\n+            m_obj(y_true, y_pred, sample_weight=np.array([0.2, 0.3, 0.4, 0.1]))\n+\n \n class OneHotIoUTest(testing.TestCase):\n     def test_unweighted(self):\n@@ -385,7 +498,9 @@ class OneHotIoUTest(testing.TestCase):\n         # true_positives = [0, 0, 0.1]\n         # iou = true_positives / (sum_row + sum_col - true_positives))\n         expected_result = (0 / (0.3 + 0.6 - 0) + 0.1 / (0.7 + 0.1 - 0.1)) / 2\n-        obj = metrics.OneHotIoU(num_classes=3, target_class_ids=[0, 2])\n+        obj = metrics.OneHotIoU(\n+            num_classes=3, target_class_ids=[0, 2], dtype=\"float32\"\n+        )\n         result = obj(y_true, y_pred, sample_weight=sample_weight)\n         self.assertAllClose(result, expected_result, atol=1e-3)\n \n@@ -439,6 +554,12 @@ class OneHotMeanIoUTest(testing.TestCase):\n         expected_result = (\n             0.1 / (0.4 + 0.6 - 0.1) + 0 + 0.1 / (0.6 + 0.1 - 0.1)\n         ) / 3\n-        obj = metrics.OneHotMeanIoU(num_classes=3)\n+        obj = metrics.OneHotMeanIoU(num_classes=3, dtype=\"float32\")\n         result = obj(y_true, y_pred, sample_weight=sample_weight)\n         self.assertAllClose(result, expected_result, atol=1e-3)\n+\n+        # Check same result with int weights\n+        sample_weight_int = [1, 2, 3, 3, 1]\n+        obj_int = metrics.OneHotMeanIoU(num_classes=3)\n+        result_int = obj_int(y_true, y_pred, sample_weight=sample_weight_int)\n+        self.assertAllClose(result_int, expected_result, atol=1e-3)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
