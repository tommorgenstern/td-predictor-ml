{"custom_id": "keras#fab6abfff53be5bec3ef19d0c6275e7897291d4e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 19 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 34 | Churn Cumulative: 3033 | Contributors (this commit): 4 | Commits (past 90d): 41 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -222,9 +222,6 @@ class JAXTrainer(base_trainer.Trainer):\n                     metrics_variables,\n                 ) = state\n \n-                # Update `iterations` var (necessary to make LR schedules work)\n-                # It's always the first variable tracked by the optimizer.\n-                self.optimizer.iterations.assign(optimizer_variables[0])\n                 # Setting _jax_state enables callbacks to force a state sync\n                 # if they need to.\n                 self._jax_state = {\n\n@@ -5,7 +5,6 @@ import math\n import numpy as np\n \n from keras_core import backend\n-from keras_core import callbacks\n from keras_core import layers\n from keras_core import optimizers\n from keras_core import testing\n@@ -15,7 +14,13 @@ from keras_core.optimizers import schedules\n \n class TestFitLRSchedulesFlow(testing.TestCase):\n     def test_fit_lr_correctness(self):\n-        model = Sequential([layers.Dense(3)])\n+        model = Sequential(\n+            [\n+                layers.Dense(\n+                    2, kernel_initializer=\"ones\", bias_initializer=\"ones\"\n+                )\n+            ]\n+        )\n         optimizer = optimizers.Adam(\n             learning_rate=schedules.ExponentialDecay(\n                 initial_learning_rate=0.05, decay_steps=1, decay_rate=0.9\n@@ -24,21 +29,15 @@ class TestFitLRSchedulesFlow(testing.TestCase):\n         self.assertEqual(len(optimizer.variables), 1)\n         self.assertEqual(optimizer.variables[0], 0)\n \n-        class LRTracker(callbacks.Callback):\n-            def __init__(self, optimizer):\n-                self.optimizer = optimizer\n-                self.logs = []\n-\n-            def on_batch_end(self, *args, **kwargs):\n-                self.logs.append(float(self.optimizer.learning_rate))\n-\n         model.compile(optimizer=optimizer, loss=\"mse\")\n-        x = np.random.random((16, 2))\n-        y = np.random.random((16, 3))\n-        tracker = LRTracker(optimizer)\n-        model.fit(x, y, epochs=1, batch_size=4, callbacks=[tracker])\n-        self.assertEqual(optimizer.variables[0], 4)\n-        self.assertAllClose(tracker.logs, [0.045, 0.0405, 0.03645, 0.032805])\n+        x = np.arange(32).reshape((16, 2))\n+        y = np.arange(32).reshape((16, 2))\n+        history = model.fit(x, y, epochs=3, batch_size=4, shuffle=False)\n+        self.assertEqual(optimizer.variables[0], 4 * 3)\n+        self.assertAllClose(\n+            history.history[\"loss\"],\n+            [230.79457092285156, 128.30319213867188, 79.33648681640625],\n+        )\n \n \n class ExponentialDecayTest(testing.TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b02c29d3a07c632298cf12d73379a7d5082617da", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 41 | Files Changed: 2 | Hunks: 9 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 62 | Churn Cumulative: 7488 | Contributors (this commit): 5 | Commits (past 90d): 44 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+import numpy as np\n import torch\n \n from keras_core.backend.torch.core import cast\n@@ -246,16 +247,12 @@ def count_nonzero(x, axis=None):\n     return torch.count_nonzero(x, dim=axis).T\n \n \n-def cross(x1, x2, axisa=-1, axisb=-1, axisc=-1, axis=None):\n-    # TODO: There is API divergence between np.cross and torch.cross,\n-    # preventing `axisa`, `axisb`, and `axisc` parameters from\n-    # being used.\n-    # https://github.com/pytorch/pytorch/issues/50273\n+def cross(x1, x2, axisa=-1, axisb=-1, axisc=-1, axis=-1):\n     if axisa != -1 or axisb != -1 or axisc != -1:\n-        raise NotImplementedError(\n-            \"Due to API divergence between `torch.cross()` and \"\n-            \"`np.cross`, the following arguments are not supported: \"\n-            f\"axisa={axisa}, axisb={axisb}, axisc={axisc}\"\n+        raise ValueError(\n+            \"Torch backend does not support `axisa`, `axisb`, or `axisc`. \"\n+            f\"Received: axisa={axisa}, axisb={axisb}, axisc={axisc}. Please \"\n+            \"use `axis` arg in torch backend.\"\n         )\n     x1, x2 = convert_to_tensor(x1), convert_to_tensor(x2)\n     return torch.cross(x1, x2, dim=axis)\n@@ -340,24 +337,18 @@ def floor(x):\n \n def full(shape, fill_value, dtype=None):\n     dtype = to_torch_dtype(dtype)\n-    if hasattr(fill_value, \"__len__\"):\n-        fill_value = convert_to_tensor(fill_value)\n-        reps = shape[-1] // fill_value.shape[-1]  # channels-last reduction\n-        reps_by_dim = (*shape[:-1], reps)\n-        return torch.tile(fill_value, reps_by_dim)\n+    fill_value = convert_to_tensor(fill_value, dtype=dtype)\n+    if len(fill_value.shape) > 0:\n+        # `torch.full` only supports scala `fill_value`.\n+        expand_size = len(shape) - len(fill_value.shape)\n+        tile_shape = tuple(shape[:expand_size]) + (1,) * len(fill_value.shape)\n+        return torch.tile(fill_value, tile_shape)\n+\n     return torch.full(size=shape, fill_value=fill_value, dtype=dtype)\n \n \n def full_like(x, fill_value, dtype=None):\n-    dtype = to_torch_dtype(dtype)\n-    if hasattr(fill_value, \"__len__\"):\n-        fill_value = convert_to_tensor(fill_value)\n-        reps_by_dim = tuple(\n-            [x.shape[i] // fill_value.shape[i] for i in range(x.ndim)]\n-        )\n-        return torch.tile(fill_value, reps_by_dim)\n-    x = convert_to_tensor(x)\n-    return torch.full_like(input=x, fill_value=fill_value, dtype=dtype)\n+    return full(shape=x.shape, fill_value=fill_value, dtype=dtype)\n \n \n def greater(x1, x2):\n@@ -832,15 +823,11 @@ def sum(x, axis=None, keepdims=False):\n \n \n def eye(N, M=None, k=None, dtype=\"float32\"):\n-    # TODO: implement support for `k` diagonal arg,\n-    # does not exist in torch.eye()\n-    if k is not None:\n-        raise NotImplementedError(\n-            \"Due to API divergence bewtween `torch.eye` \"\n-            \"and `np.eye`, the argument k is not supported: \"\n-            f\"Received: k={k}\"\n-        )\n     dtype = to_torch_dtype(dtype)\n-    if M is not None:\n-        return torch.eye(n=N, m=M, dtype=dtype)\n-    return torch.eye(n=N, dtype=dtype)\n+    M = N if M is None else M\n+    k = 0 if k is None else k\n+    if k == 0:\n+        return torch.eye(N, M, dtype=dtype)\n+    diag_length = np.maximum(N, M)\n+    diag = torch.ones(diag_length, dtype=dtype)\n+    return torch.diag(diag, diagonal=k)[:N, :M]\n\n@@ -3032,14 +3032,7 @@ class NumpyArrayCreateOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(np.array(knp.zeros([2, 3])), np.zeros([2, 3]))\n         self.assertAllClose(np.array(knp.Zeros()([2, 3])), np.zeros([2, 3]))\n \n-    @pytest.mark.skipif(\n-        backend.backend() == \"torch\",\n-        reason=\"`torch.eye` does not support arg `k`.\",\n-    )\n     def test_eye(self):\n-        # TODO: implement support for `k` diagonal arg,\n-        # does not exist in torch.eye()\n-\n         self.assertAllClose(np.array(knp.eye(3)), np.eye(3))\n         self.assertAllClose(np.array(knp.eye(3, 4)), np.eye(3, 4))\n         self.assertAllClose(np.array(knp.eye(3, 4, 1)), np.eye(3, 4, 1))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a460a35362bd03be865367c799f9e0725c7b3e89", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 28 | Files Changed: 2 | Hunks: 14 | Methods Changed: 8 | Complexity Δ (Sum/Max): 10/6 | Churn Δ: 68 | Churn Cumulative: 7556 | Contributors (this commit): 5 | Commits (past 90d): 46 | Contributors (cumulative): 8 | DMM Complexity: 0.0\n\nDIFF:\n@@ -343,7 +343,6 @@ def full(shape, fill_value, dtype=None):\n         expand_size = len(shape) - len(fill_value.shape)\n         tile_shape = tuple(shape[:expand_size]) + (1,) * len(fill_value.shape)\n         return torch.tile(fill_value, tile_shape)\n-\n     return torch.full(size=shape, fill_value=fill_value, dtype=dtype)\n \n \n@@ -596,8 +595,16 @@ def outer(x1, x2):\n def pad(x, pad_width, mode=\"constant\"):\n     x = convert_to_tensor(x)\n     pad_sum = ()\n+    pad_width = list(pad_width)[::-1]  # torch uses reverse order\n     for pad in pad_width:\n         pad_sum += pad\n+    if mode == \"symmetric\":\n+        mode = \"replicate\"\n+    if mode != \"constant\" and x.ndim < 3:\n+        new_dims = [1] * (3 - x.ndim)\n+        x = cast(x, torch.float32) if x.dtype == torch.int else x\n+        x = x.view(*new_dims, *x.shape)\n+        return torch.nn.functional.pad(x, pad=pad_sum, mode=mode).squeeze()\n     return torch.nn.functional.pad(x, pad=pad_sum, mode=mode)\n \n \n@@ -665,9 +672,20 @@ def sort(x, axis=-1):\n \n def split(x, indices_or_sections, axis=0):\n     x = convert_to_tensor(x)\n+    if isinstance(indices_or_sections, list):\n+        idxs = convert_to_tensor(indices_or_sections)\n+        start_size = indices_or_sections[0]\n+        end_size = x.shape[axis] - indices_or_sections[-1]\n+        chunk_sizes = (\n+            [start_size]\n+            + torch.diff(idxs).type(torch.int).tolist()\n+            + [end_size]\n+        )\n+    else:\n+        chunk_sizes = x.shape[axis] // indices_or_sections\n     return torch.split(\n         tensor=x,\n-        split_size_or_sections=indices_or_sections,\n+        split_size_or_sections=chunk_sizes,\n         dim=axis,\n     )\n \n@@ -694,7 +712,7 @@ def take(x, indices, axis=None):\n     x = convert_to_tensor(x)\n     indices = convert_to_tensor(indices).long()\n     if axis is not None:\n-        return torch.index_select(x, dim=axis, index=indices)\n+        return torch.index_select(x, dim=axis, index=indices).squeeze(axis)\n     return torch.take(x, index=indices)\n \n \n@@ -734,7 +752,9 @@ def trace(x, offset=None, axis1=None, axis2=None):\n \n def tri(N, M=None, k=0, dtype=\"float32\"):\n     dtype = to_torch_dtype(dtype)\n-    pass\n+    M = M or N\n+    x = torch.ones((N, M), dtype=dtype)\n+    return torch.tril(x, diagonal=k)\n \n \n def tril(x, k=0):\n\n@@ -1,5 +1,4 @@\n import numpy as np\n-import pytest\n from tensorflow.python.ops.numpy_ops import np_config\n \n from keras_core import backend\n@@ -2003,11 +2002,6 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase):\n         self.assertAllClose(np.array(knp.outer(x, y)), np.outer(x, y))\n         self.assertAllClose(np.array(knp.Outer()(x, y)), np.outer(x, y))\n \n-    # TODO: Fix numpy compatibility (squeeze by one dimension only)\n-    @pytest.mark.skipif(\n-        backend.backend() == \"torch\",\n-        reason=\"`torch.take` and `np.take` have return shape divergence.\",\n-    )\n     def test_take(self):\n         x = np.arange(24).reshape([1, 2, 3, 4])\n         indices = np.array([0, 1])\n@@ -2780,14 +2774,6 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n             np.pad(x, ((1, 1), (1, 1))),\n         )\n \n-    # TODO: implement padding with non-constant padding,\n-    # bypass NotImplementedError for PyTorch\n-    @pytest.mark.skipif(\n-        backend.backend() == \"torch\",\n-        reason=\"padding not implemented for non-constant use case\",\n-    )\n-    def test_pad_without_torch(self):\n-        x = np.array([[1, 2], [3, 4]])\n         self.assertAllClose(\n             np.array(knp.pad(x, ((1, 1), (1, 1)), mode=\"reflect\")),\n             np.pad(x, ((1, 1), (1, 1)), mode=\"reflect\"),\n@@ -2919,14 +2905,24 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(np.array(knp.sort(x, axis=0)), np.sort(x, axis=0))\n         self.assertAllClose(np.array(knp.Sort(axis=0)(x)), np.sort(x, axis=0))\n \n-    # TODO: implement split for `torch` with support for conversion\n-    # of numpy.split args.\n-    @pytest.mark.skipif(\n-        backend.backend() == \"torch\",\n-        reason=\"`torch.split` and `np.split` have return arg divergence.\",\n-    )\n     def test_split(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n+        if backend.backend() == \"torch\":\n+            self.assertAllClose(\n+                [t.numpy() for t in knp.split(x, 2)], np.split(x, 2)\n+            )\n+            self.assertAllClose(\n+                [t.numpy() for t in knp.Split(2)(x)], np.split(x, 2)\n+            )\n+            self.assertAllClose(\n+                [t.numpy() for t in knp.split(x, [1, 2], axis=1)],\n+                np.split(x, [1, 2], axis=1),\n+            )\n+            self.assertAllClose(\n+                [t.numpy() for t in knp.Split([1, 2], axis=1)(x)],\n+                np.split(x, [1, 2], axis=1),\n+            )\n+        else:\n             self.assertAllClose(np.array(knp.split(x, 2)), np.split(x, 2))\n             self.assertAllClose(np.array(knp.Split(2)(x)), np.split(x, 2))\n             self.assertAllClose(\n@@ -3073,11 +3069,7 @@ class NumpyArrayCreateOpsCorrectnessTest(testing.TestCase):\n         self.assertAllClose(np.array(knp.identity(3)), np.identity(3))\n         self.assertAllClose(np.array(knp.Identity()(3)), np.identity(3))\n \n-    @pytest.mark.skipif(\n-        backend.backend() == \"torch\", reason=\"No torch equivalent for `np.tri`\"\n-    )\n     def test_tri(self):\n-        # TODO: create a manual implementation, as PyTorch has no equivalent\n         self.assertAllClose(np.array(knp.tri(3)), np.tri(3))\n         self.assertAllClose(np.array(knp.tri(3, 4)), np.tri(3, 4))\n         self.assertAllClose(np.array(knp.tri(3, 4, 1)), np.tri(3, 4, 1))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#31cd921e5034d5f7e8ce40b0b309f164cebe0580", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 64 | Lines Deleted: 44 | Files Changed: 2 | Hunks: 26 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 108 | Churn Cumulative: 6731 | Contributors (this commit): 8 | Commits (past 90d): 43 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -47,7 +47,7 @@ class MeanSquaredError(LossFunctionWrapper):\n     Args:\n         reduction: Type of reduction to apply to the loss. In almost all cases\n             this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n     \"\"\"\n \n@@ -73,7 +73,7 @@ class MeanAbsoluteError(LossFunctionWrapper):\n     Args:\n         reduction: Type of reduction to apply to the loss. In almost all cases\n             this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n     \"\"\"\n \n@@ -99,7 +99,7 @@ class MeanAbsolutePercentageError(LossFunctionWrapper):\n     Args:\n         reduction: Type of reduction to apply to the loss. In almost all cases\n             this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n     \"\"\"\n \n@@ -129,7 +129,7 @@ class MeanSquaredLogarithmicError(LossFunctionWrapper):\n     Args:\n         reduction: Type of reduction to apply to the loss. In almost all cases\n             this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n     \"\"\"\n \n@@ -168,7 +168,7 @@ class CosineSimilarity(LossFunctionWrapper):\n             (the features axis). Defaults to -1.\n         reduction: Type of reduction to apply to the loss. In almost all cases\n             this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n     \"\"\"\n \n@@ -257,7 +257,7 @@ class Hinge(LossFunctionWrapper):\n     Args:\n         reduction: Type of reduction to apply to the loss. In almost all cases\n             this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n     \"\"\"\n \n@@ -284,7 +284,7 @@ class SquaredHinge(LossFunctionWrapper):\n     Args:\n         reduction: Type of reduction to apply to the loss. In almost all cases\n             this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n     \"\"\"\n \n@@ -310,7 +310,7 @@ class CategoricalHinge(LossFunctionWrapper):\n     Args:\n         reduction: Type of reduction to apply to the loss. In almost all cases\n             this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n     \"\"\"\n \n@@ -336,7 +336,7 @@ class KLDivergence(LossFunctionWrapper):\n     Args:\n         reduction: Type of reduction to apply to the loss. In almost all cases\n             this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n     \"\"\"\n \n@@ -360,7 +360,7 @@ class Poisson(LossFunctionWrapper):\n     Args:\n         reduction: Type of reduction to apply to the loss. In almost all cases\n             this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n     \"\"\"\n \n@@ -388,18 +388,17 @@ class BinaryCrossentropy(LossFunctionWrapper):\n     Args:\n         from_logits: Whether to interpret `y_pred` as a tensor of\n             [logit](https://en.wikipedia.org/wiki/Logit) values. By default, we\n-            assume that `y_pred` contains probabilities (i.e., values in [0,\n-            1]).\n+            assume that `y_pred` is probabilities (i.e., values in [0, 1]).\n         label_smoothing: Float in range [0, 1]. When 0, no smoothing occurs.\n             When > 0, we compute the loss between the predicted labels\n             and a smoothed version of the true labels, where the smoothing\n             squeezes the labels towards 0.5. Larger values of\n             `label_smoothing` correspond to heavier smoothing.\n-        axis: The axis along which to compute crossentropy (the features\n-            axis).  Defaults to -1.\n+        axis: The axis along which to compute crossentropy (the features axis).\n+            Defaults to -1.\n         reduction: Type of reduction to apply to the loss. In almost all cases\n             this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n \n     Examples:\n@@ -529,7 +528,7 @@ class BinaryFocalCrossentropy(LossFunctionWrapper):\n             Defaults to `-1`.\n         reduction: Type of reduction to apply to the loss. In almost all cases\n             this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n \n     Examples:\n@@ -665,11 +664,8 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n     Use this crossentropy loss function when there are two or more label\n     classes. We expect labels to be provided in a `one_hot` representation. If\n     you want to provide labels as integers, please use\n-    `SparseCategoricalCrossentropy` loss.  There should be `# classes` floating\n-    point values per feature.\n-\n-    In the snippet below, there is `# classes` floating pointing values per\n-    example. The shape of both `y_pred` and `y_true` are\n+    `SparseCategoricalCrossentropy` loss. There should be `num_classes` floating\n+    point values per feature, i.e., the shape of both `y_pred` and `y_true` are\n     `[batch_size, num_classes]`.\n \n     Args:\n@@ -683,7 +679,7 @@ class CategoricalCrossentropy(LossFunctionWrapper):\n             axis). Defaults to -1.\n         reduction: Type of reduction to apply to the loss. In almost all cases\n             this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n \n     Examples:\n@@ -791,7 +787,7 @@ class CategoricalFocalCrossentropy(LossFunctionWrapper):\n     Extending this to multi-class case is straightforward:\n     `FL(p_t) = alpha * (1 - p_t) ** gamma * CategoricalCE(y_true, y_pred)`\n \n-    In the snippet below, there is `# classes` floating pointing values per\n+    In the snippet below, there is `num_classes` floating pointing values per\n     example. The shape of both `y_pred` and `y_true` are\n     `(batch_size, num_classes)`.\n \n@@ -814,7 +810,7 @@ class CategoricalFocalCrossentropy(LossFunctionWrapper):\n             axis). Defaults to -1.\n         reduction: Type of reduction to apply to the loss. In almost all cases\n             this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n \n     Examples:\n@@ -903,16 +899,16 @@ class SparseCategoricalCrossentropy(LossFunctionWrapper):\n     feature for `y_true`.\n \n     In the snippet below, there is a single floating point value per example for\n-    `y_true` and `# classes` floating pointing values per example for `y_pred`.\n-    The shape of `y_true` is `[batch_size]` and the shape of `y_pred` is\n-    `[batch_size, num_classes]`.\n+    `y_true` and `num_classes` floating pointing values per example for\n+    `y_pred`. The shape of `y_true` is `[batch_size]` and the shape of `y_pred`\n+    is `[batch_size, num_classes]`.\n \n     Args:\n         from_logits: Whether `y_pred` is expected to be a logits tensor. By\n             default, we assume that `y_pred` encodes a probability distribution.\n         reduction: Type of reduction to apply to the loss. In almost all cases\n             this should be `\"sum_over_batch_size\"`.\n-            Suuported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n+            Supported options are `\"sum\"`, `\"sum_over_batch_size\"` or `None`.\n         name: Optional name for the loss instance.\n \n     Examples:\n\n@@ -37,8 +37,16 @@ class MeanSquaredErrorTest(testing.TestCase):\n         self.assertAlmostEqual(loss, 767.8 / 6)\n \n     def test_timestep_weighted(self):\n-        # TODO\n-        pass\n+        mse_obj = losses.MeanSquaredError()\n+        y_true = np.asarray([1, 9, 2, -5, -2, 6]).reshape(2, 3, 1)\n+        y_pred = np.asarray([4, 8, 12, 8, 1, 3]).reshape(2, 3, 1)\n+        sample_weight = np.array([3, 6, 5, 0, 4, 2]).reshape((2, 3))\n+        loss = mse_obj(\n+            y_true,\n+            y_pred,\n+            sample_weight=sample_weight,\n+        )\n+        self.assertAlmostEqual(loss, 97.833336)\n \n     def test_zero_weighted(self):\n         mse_obj = losses.MeanSquaredError()\n@@ -47,10 +55,6 @@ class MeanSquaredErrorTest(testing.TestCase):\n         loss = mse_obj(y_true, y_pred, sample_weight=0)\n         self.assertAlmostEqual(loss, 0.0)\n \n-    def test_invalid_sample_weight(self):\n-        # TODO\n-        pass\n-\n     def test_no_reduction(self):\n         mse_obj = losses.MeanSquaredError(reduction=None)\n         y_true = np.array([[1, 9, 2], [-5, -2, 6]])\n@@ -101,8 +105,16 @@ class MeanAbsoluteErrorTest(testing.TestCase):\n         self.assertAlmostEqual(loss, 81.4 / 6)\n \n     def test_timestep_weighted(self):\n-        # TODO\n-        pass\n+        mae_obj = losses.MeanAbsoluteError()\n+        y_true = np.asarray([1, 9, 2, -5, -2, 6]).reshape(2, 3, 1)\n+        y_pred = np.asarray([4, 8, 12, 8, 1, 3]).reshape(2, 3, 1)\n+        sample_weight = np.array([3, 6, 5, 0, 4, 2]).reshape((2, 3))\n+        loss = mae_obj(\n+            y_true,\n+            y_pred,\n+            sample_weight=sample_weight,\n+        )\n+        self.assertAlmostEqual(loss, 13.833333)\n \n     def test_zero_weighted(self):\n         mae_obj = losses.MeanAbsoluteError()\n@@ -111,10 +123,6 @@ class MeanAbsoluteErrorTest(testing.TestCase):\n         loss = mae_obj(y_true, y_pred, sample_weight=0)\n         self.assertAlmostEqual(loss, 0.0)\n \n-    def test_invalid_sample_weight(self):\n-        # TODO\n-        pass\n-\n     def test_no_reduction(self):\n         mae_obj = losses.MeanAbsoluteError(reduction=None)\n         y_true = np.array([[1, 9, 2], [-5, -2, 6]])\n@@ -165,8 +173,16 @@ class MeanAbsolutePercentageErrorTest(testing.TestCase):\n         self.assertAlmostEqual(loss, 422.8888, 3)\n \n     def test_timestep_weighted(self):\n-        # TODO\n-        pass\n+        mape_obj = losses.MeanAbsolutePercentageError()\n+        y_true = np.asarray([1, 9, 2, -5, -2, 6]).reshape(2, 3, 1)\n+        y_pred = np.asarray([4, 8, 12, 8, 1, 3]).reshape(2, 3, 1)\n+        sample_weight = np.array([3, 6, 5, 0, 4, 2]).reshape((2, 3))\n+        loss = mape_obj(\n+            y_true,\n+            y_pred,\n+            sample_weight=sample_weight,\n+        )\n+        self.assertAlmostEqual(loss, 694.4444)\n \n     def test_zero_weighted(self):\n         mape_obj = losses.MeanAbsolutePercentageError()\n@@ -212,8 +228,16 @@ class MeanSquaredLogarithmicErrorTest(testing.TestCase):\n         self.assertAlmostEqual(loss, 3.7856, 3)\n \n     def test_timestep_weighted(self):\n-        # TODO\n-        pass\n+        msle_obj = losses.MeanSquaredLogarithmicError()\n+        y_true = np.asarray([1, 9, 2, -5, -2, 6]).reshape(2, 3, 1)\n+        y_pred = np.asarray([4, 8, 12, 8, 1, 3]).reshape(2, 3, 1)\n+        sample_weight = np.array([3, 6, 5, 0, 4, 2]).reshape((2, 3))\n+        loss = msle_obj(\n+            y_true,\n+            y_pred,\n+            sample_weight=sample_weight,\n+        )\n+        self.assertAlmostEqual(loss, 2.647374)\n \n     def test_zero_weighted(self):\n         msle_obj = losses.MeanSquaredLogarithmicError()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6c917959e86a4d5bc9f100d6763a7a2524714dd0", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 152 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 152 | Churn Cumulative: 1435 | Contributors (this commit): 9 | Commits (past 90d): 70 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -40,6 +40,158 @@ from keras_core.utils.tracking import Tracker\n \n @keras_core_export([\"keras_core.Layer\", \"keras_core.layers.Layer\"])\n class Layer(Operation):\n+    \"\"\"This is the class from which all layers inherit.\n+\n+    A layer is a callable object that takes as input one or more tensors and\n+    that outputs one or more tensors. It involves *computation*, defined\n+    in the `call()` method, and a *state* (weight variables). State can be\n+    created:\n+\n+    * in `__init__()`, for instancce via `self.add_weight()`;\n+    * in the optional `build()` method, which is invoked by the first\n+      `__call__()` to the layer, and supplies the shape(s) of the input(s),\n+      which may not have been known at initialization time.\n+\n+    Layers are recursively composable: If you assign a Layer instance as an\n+    attribute of another Layer, the outer layer will start tracking the weights\n+    created by the inner layer. Nested layers should be instantiated in the\n+    `__init__()` method or `build()` method.\n+\n+    Users will just instantiate a layer and then treat it as a callable.\n+\n+    Args:\n+        trainable: Boolean, whether the layer's variables should be trainable.\n+        name: String name of the layer.\n+        dtype: The dtype of the layer's computations and weights. Can also be a\n+            `keras_core.mixed_precision.DTypePolicy`,\n+            which allows the computation and\n+            weight dtype to differ. Default of `None` means to use\n+            `keras_core.mixed_precision.dtype_policy()`,\n+            which is a `float32` policy unless set to different value\n+            (via `keras_core.mixed_precision.set_dtype_policy()`).\n+\n+    Attributes:\n+        name: The name of the layer (string).\n+        dtype: The dtype of the layer's weights.\n+        variable_dtype: Dtype of the layer's variables.\n+        compute_dtype: The dtype of the layer's computations.\n+            Layers automatically cast inputs to this dtype, which causes\n+            the computations and output to also be in this dtype.\n+            When mixed precision is used with a\n+            `keras_core.mixed_precision.DTypePolicy`, this will be different\n+            than `variable_dtype`.\n+        trainable_weights: List of variables to be included in backprop.\n+        non_trainable_weights: List of variables that should not be\n+            included in backprop.\n+        weights: The concatenation of the lists trainable_weights and\n+            non_trainable_weights (in this order).\n+        trainable: Whether the layer should be trained (boolean), i.e.\n+            whether its potentially-trainable weights should be returned\n+            as part of `layer.trainable_weights`.\n+        input_spec: Optional (list of) `InputSpec` object(s) specifying the\n+            constraints on inputs that can be accepted by the layer.\n+\n+    We recommend that descendants of `Layer` implement the following methods:\n+\n+    * `__init__()`: Defines custom layer attributes, and creates layer weights\n+        that do not depend on input shapes, using `add_weight()`,\n+        or other state.\n+    * `build(self, input_shape)`: This method can be used to create weights that\n+        depend on the shape(s) of the input(s), using `add_weight()`, or other\n+        state. `__call__()` will automatically build the layer\n+        (if it has not been built yet) by calling `build()`.\n+    * `call(self, *args, **kwargs)`: Called in `__call__` after making\n+        sure `build()` has been called. `call()` performs the logic of applying\n+        the layer to the input arguments.\n+        Two reserved keyword arguments you can optionally use in `call()` are:\n+            1. `training` (boolean, whether the call is in inference mode or\n+                training mode).\n+            2. `mask` (boolean tensor encoding masked timesteps in the input,\n+                used e.g. in RNN layers).\n+        A typical signature for this method is `call(self, inputs)`, and user\n+        could optionally add `training` and `mask` if the layer need them.\n+    * `get_config(self)`: Returns a dictionary containing the configuration\n+        used to initialize this layer. If the keys differ from the arguments\n+        in `__init__()`, then override `from_config(self)` as well.\n+        This method is used when saving\n+        the layer or a model that contains this layer.\n+\n+    Examples:\n+\n+    Here's a basic example: a layer with two variables, `w` and `b`,\n+    that returns `y = w . x + b`.\n+    It shows how to implement `build()` and `call()`.\n+    Variables set as attributes of a layer are tracked as weights\n+    of the layers (in `layer.weights`).\n+\n+    ```python\n+    class SimpleDense(Layer):\n+        def __init__(self, units=32):\n+            super().__init__()\n+            self.units = units\n+\n+        # Create the state of the layer (weights)\n+        def build(self, input_shape):\n+            self.kernel = self.add_weight(\n+                shape=(input_shape[-1], self.units),\n+                initializer=\"glorot_uniform\",\n+                trainable=True,\n+                name=\"kernel\",\n+            )\n+            self.bias = self.add_weight(\n+                shape=(self.units,),\n+                initializer=\"zeros\",\n+                trainable=True,\n+                name=\"bias\",\n+            )\n+\n+        # Defines the computation\n+        def call(self, inputs):\n+            return ops.matmul(inputs, self.kernel) + self.bias\n+\n+    # Instantiates the layer.\n+    linear_layer = SimpleDense(4)\n+\n+    # This will also call `build(input_shape)` and create the weights.\n+    y = linear_layer(ops.ones((2, 2)))\n+    assert len(linear_layer.weights) == 2\n+\n+    # These weights are trainable, so they're listed in `trainable_weights`:\n+    assert len(linear_layer.trainable_weights) == 2\n+    ```\n+\n+    Besides trainable weights, updated via backpropagation during training,\n+    layers can also have non-trainable weights. These weights are meant to\n+    be updated manually during `call()`. Here's a example layer that computes\n+    the running sum of its inputs:\n+\n+    ```python\n+    class ComputeSum(Layer):\n+\n+      def __init__(self, input_dim):\n+          super(ComputeSum, self).__init__()\n+          # Create a non-trainable weight.\n+          self.total = self.add_weight(\n+            shape=(),\n+            initializer=\"zeros\",\n+            trainable=False,\n+            name=\"total\",\n+          )\n+\n+      def call(self, inputs):\n+          self.total.assign(self.total + ops.sum(inputs))\n+          return self.total\n+\n+    my_sum = ComputeSum(2)\n+    x = ops.ones((2, 2))\n+    y = my_sum(x)\n+\n+    assert my_sum.weights == [my_sum.total]\n+    assert my_sum.non_trainable_weights == [my_sum.total]\n+    assert my_sum.trainable_weights == []\n+    ```\n+    \"\"\"\n+\n     def __init__(\n         self,\n         *,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ac4c8ea1fdb3e2ee2e06a7d6748b966d398fb614", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 767 | Contributors (this commit): 15 | Commits (past 90d): 6 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -269,8 +269,6 @@ def image_dataset_from_directory(\n             interpolation=interpolation,\n             crop_to_aspect_ratio=crop_to_aspect_ratio,\n         )\n-        train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n-        val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n \n         if batch_size is not None:\n             if shuffle:\n@@ -286,6 +284,9 @@ def image_dataset_from_directory(\n                     buffer_size=1024, seed=seed\n                 )\n \n+        train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n+        val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n+\n         # Users may need to reference `class_names`.\n         train_dataset.class_names = class_names\n         val_dataset.class_names = class_names\n@@ -314,7 +315,7 @@ def image_dataset_from_directory(\n             interpolation=interpolation,\n             crop_to_aspect_ratio=crop_to_aspect_ratio,\n         )\n-        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n+\n         if batch_size is not None:\n             if shuffle:\n                 # Shuffle locally at each iteration\n@@ -324,6 +325,8 @@ def image_dataset_from_directory(\n             if shuffle:\n                 dataset = dataset.shuffle(buffer_size=1024, seed=seed)\n \n+        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n+\n         # Users may need to reference `class_names`.\n         dataset.class_names = class_names\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#93f5c6cd7dc0de471bece73509cfb6bbcf8e639e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 80 | Lines Deleted: 3 | Files Changed: 4 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 83 | Churn Cumulative: 5318 | Contributors (this commit): 10 | Commits (past 90d): 51 | Contributors (cumulative): 20 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,7 @@ from keras_core import backend\n from keras_core.optimizers import base_optimizer\n \n \n-class TFOptimizer(base_optimizer.Optimizer):\n+class TFOptimizer(base_optimizer.BaseOptimizer):\n     \"\"\"A class for Tensorflow specific optimizer logic.\n \n     The major behavior change for this class is for tf.distribute.\n\n@@ -7,6 +7,79 @@ from keras_core.utils.tracking import Tracker\n \n @keras_core_export([\"keras_core.Metric\", \"keras_core.metrics.Metric\"])\n class Metric:\n+    \"\"\"Encapsulates metric logic and state.\n+\n+    Args:\n+        name: (Optional) string name of the metric instance.\n+        dtype: (Optional) data type of the metric result.\n+\n+    Standalone usage:\n+\n+    ```python\n+    m = SomeMetric(...)\n+    for input in ...:\n+        m.update_state(input)\n+    print('Final result: ', m.result())\n+    ```\n+\n+    Usage with `compile()` API:\n+\n+    ```python\n+    model = keras_core.Sequential()\n+    model.add(keras_core.layers.Dense(64, activation='relu'))\n+    model.add(keras_core.layers.Dense(64, activation='relu'))\n+    model.add(keras_core.layers.Dense(10, activation='softmax'))\n+\n+    model.compile(optimizer=keras_core.optimizers.RMSprop(0.01),\n+                  loss=keras_core.losses.CategoricalCrossentropy(),\n+                  metrics=[keras_core.metrics.CategoricalAccuracy()])\n+\n+    data = np.random.random((1000, 32))\n+    labels = np.random.random((1000, 10))\n+\n+    model.fit(data, labels, epochs=10)\n+    ```\n+\n+    To be implemented by subclasses:\n+\n+    * `__init__()`: All state variables should be created in this method by\n+      calling `self.add_variable()` like: `self.var = self.add_variable(...)`\n+    * `update_state()`: Has all updates to the state variables like:\n+      `self.var.assign(...)`.\n+    * `result()`: Computes and returns a scalar value or a dict of scalar values\n+      for the metric from the state variables.\n+\n+    Example subclass implementation:\n+\n+    ```python\n+    class BinaryTruePositives(Metric):\n+\n+        def __init__(self, name='binary_true_positives', **kwargs):\n+            super().__init__(name=name, **kwargs)\n+            self.true_positives = self.add_variable(\n+                shape=(),\n+                initializer='zeros',\n+                name='tp'\n+            )\n+\n+        def update_state(self, y_true, y_pred, sample_weight=None):\n+            y_true = ops.cast(y_true, \"bool\")\n+            y_pred = ops.cast(y_pred, \"bool\")\n+\n+            values = ops.logical_and(\n+                ops.equal(y_true, True), ops.equal(y_pred, True))\n+            values = ops.cast(values, self.dtype)\n+            if sample_weight is not None:\n+                sample_weight = ops.cast(sample_weight, self.dtype)\n+                sample_weight = ops.broadcast_to(sample_weight, values.shape)\n+                values = ops.multiply(values, sample_weight)\n+            self.true_positives.assign(self.true_positives + ops.sum(values))\n+\n+        def result(self):\n+            return self.true_positives\n+    ```\n+    \"\"\"\n+\n     def __init__(self, dtype=None, name=None):\n         self.name = name or auto_name(self.__class__.__name__)\n         self._dtype = dtype\n@@ -62,6 +135,10 @@ class Metric:\n         self._tracker.stored_ids[\"variables\"].add(id(variable))\n         return variable\n \n+    def add_weight(self, *args, **kwargs):\n+        # Backwards compatibility alias\n+        return self.add_variable(*args, **kwargs)\n+\n     @property\n     def variables(self):\n         variables = self._variables[:]\n\n@@ -12,7 +12,7 @@ from keras_core.utils import tracking\n from keras_core.utils.naming import auto_name\n \n \n-class Optimizer:\n+class BaseOptimizer:\n     def __init__(\n         self,\n         learning_rate,\n\n@@ -7,7 +7,7 @@ if backend.backend() == \"tensorflow\":\n \n     BackendOptimizer = tf_optimizer.TFOptimizer\n else:\n-    BackendOptimizer = base_optimizer.Optimizer\n+    BackendOptimizer = base_optimizer.BaseOptimizer\n \n \n keras_core_export([\"keras_core.Optimizer\", \"keras_core.optimizers.Optimizer\"])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#083c8177887945332a7ec21d91222d541e5a9e40", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 230 | Lines Deleted: 23 | Files Changed: 7 | Hunks: 15 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 253 | Churn Cumulative: 3830 | Contributors (this commit): 7 | Commits (past 90d): 130 | Contributors (cumulative): 25 | DMM Complexity: None\n\nDIFF:\n@@ -10,9 +10,8 @@ class Callback:\n     `predict()` in order to hook into the various stages of the model training,\n     evaluation, and inference lifecycle.\n \n-    To create a custom callback, subclass `keras.callbacks.Callback` and\n-    override the method associated with the stage of interest. See\n-    https://www.tensorflow.org/guide/keras/custom_callback for more information.\n+    To create a custom callback, subclass `keras_core.callbacks.Callback` and\n+    override the method associated with the stage of interest.\n \n     Example:\n \n\n@@ -7,7 +7,7 @@ from keras_core.api_export import keras_core_export\n class Initializer:\n     \"\"\"Initializer base class: all Keras initializers inherit from this class.\n \n-    Initializers should implement a `__call__` method with the following\n+    Initializers should implement a `__call__()` method with the following\n     signature:\n \n     ```python\n@@ -29,7 +29,7 @@ class Initializer:\n             self.stddev = stddev\n \n         def __call__(self, shape, dtype=None, **kwargs):\n-            return knp.random.normal(\n+            return keras_core.random.normal(\n                 shape, mean=self.mean, stddev=self.stddev, dtype=dtype\n             )\n \n\n@@ -9,6 +9,22 @@ from keras_core.utils.naming import auto_name\n \n @keras_core_export([\"keras_core.Loss\", \"keras_core.losses.Loss\"])\n class Loss:\n+    \"\"\"Loss base class.\n+\n+    To be implemented by subclasses:\n+\n+    * `call()`: Contains the logic for loss calculation using `y_true`,\n+        `y_pred`.\n+\n+    Example subclass implementation:\n+\n+    ```python\n+    class MeanSquaredError(Loss):\n+        def call(self, y_true, y_pred):\n+            return ops.mean(ops.square(y_pred - y_true), axis=-1)\n+    ```\n+    \"\"\"\n+\n     def __init__(self, name=None, reduction=\"sum_over_batch_size\"):\n         self.name = name or auto_name(self.__class__.__name__)\n         self.reduction = standardize_reduction(reduction)\n\n@@ -59,7 +59,7 @@ class Metric:\n             self.true_positives = self.add_variable(\n                 shape=(),\n                 initializer='zeros',\n-                name='tp'\n+                name='true_positives'\n             )\n \n         def update_state(self, y_true, y_pred, sample_weight=None):\n\n@@ -15,17 +15,75 @@ from keras_core.utils import tracking\n \n \n class Functional(Function, Model):\n-    \"\"\"\n-    Add support for extra call arguments compared to Function:\n-    training, masks\n+    \"\"\"A `Functional` model is a `Model` defined as a directed graph of layers.\n \n-    Add support for arg standardization:\n-    - list/dict duality\n-    - upranking\n+    Three types of `Model` exist: subclassed `Model`, `Functional` model,\n+    and `Sequential` (a special case of `Functional`).\n \n-    Override .layers\n+    A `Functional` model can be instantiated by passing two arguments to\n+    `__init__()`. The first argument is the `keras_core.Input` objects\n+    that represent the inputs to the model.\n+    The second argument specifies the output tensors that represent\n+    the outputs of this model. Both arguments can be a nested structure\n+    of tensors.\n \n-    Symbolic add_loss\n+    Example:\n+\n+    ```\n+    inputs = {'x1': keras_core.Input(shape=(10,)),\n+              'x2': keras_core.Input(shape=(1,))}\n+    t = keras_core.layers.Dense(1, activation='relu')(inputs['x1'])\n+    outputs = keras_core.layers.Add()([t, inputs['x2'])\n+    model = keras_core.Model(inputs, outputs)\n+    ```\n+\n+    A `Functional` model constructed using the Functional API can also\n+    include raw Keras Core ops.\n+\n+    Example:\n+\n+    ```python\n+    inputs = keras_core.Input(shape=(10,))\n+    x = keras_core.layers.Dense(1)(inputs)\n+    outputs = ops.nn.relu(x)\n+    model = keras_core.Model(inputs, outputs)\n+    ```\n+\n+    A new `Functional` model can also be created by using the\n+    intermediate tensors. This enables you to quickly extract sub-components\n+    of the model.\n+\n+    Example:\n+\n+    ```python\n+    inputs = keras_core.Input(shape=(None, None, 3))\n+    processed = keras_core.layers.RandomCrop(width=32, height=32)(inputs)\n+    conv = keras_core.layers.Conv2D(filters=2, kernel_size=3)(processed)\n+    pooling = keras_core.layers.GlobalAveragePooling2D()(conv)\n+    feature = keras_core.layers.Dense(10)(pooling)\n+\n+    full_model = keras_core.Model(inputs, feature)\n+    backbone = keras_core.Model(processed, conv)\n+    activations = keras_core.Model(conv, feature)\n+    ```\n+\n+    Note that the `backbone` and `activations` models are not\n+    created with `keras_core.Input` objects, but with the tensors\n+    that are originated from `keras_core.Input` objects.\n+    Under the hood, the layers and weights will\n+    be shared across these models, so that user can train the `full_model`, and\n+    use `backbone` or `activations` to do feature extraction.\n+    The inputs and outputs of the model can be nested structures of tensors as\n+    well, and the created models are standard `Functional` model that support\n+    all the existing API.\n+\n+    Args:\n+        inputs: List of input tensors (must be created via `keras_core.Input()`\n+            or originated from `keras_core.Input()`).\n+        outputs: List of output tensors.\n+        name: String, optional. Name of the model.\n+        trainable: Boolean, optional. If the model's variables should be\n+            trainable.\n     \"\"\"\n \n     @tracking.no_automatic_dependency_tracking\n\n@@ -26,19 +26,108 @@ else:\n \n @keras_core_export([\"keras_core.Model\", \"keras_core.models.Model\"])\n class Model(Trainer, Layer):\n-    \"\"\"\n+    \"\"\"A model grouping layers into an object with training/inference features.\n \n-    Combination of a Layer and Trainer. Adds:\n+    There are three ways to instantiate a `Model`:\n \n-    - layer surfacing\n-    - saving\n-    - export\n-    - summary\n+    ## With the \"Functional API\"\n \n-    Limitations:\n+    You start from `Input`,\n+    you chain layer calls to specify the model's forward pass,\n+    and finally you create your model from inputs and outputs:\n \n-    - call must have a single inputs argument\n-    - no masking support\n+    ```python\n+    inputs = keras_core.Input(shape=(37,))\n+    x = keras_core.layers.Dense(32, activation=\"relu\")(inputs)\n+    outputs = keras_core.layers.Dense(5, activation=\"softmax\")(x)\n+    model = keras_core.Model(inputs=inputs, outputs=outputs)\n+    ```\n+\n+    Note: Only dicts, lists, and tuples of input tensors are supported. Nested\n+    inputs are not supported (e.g. lists of list or dicts of dict).\n+\n+    A new Functional API model can also be created by using the\n+    intermediate tensors. This enables you to quickly extract sub-components\n+    of the model.\n+\n+    Example:\n+\n+    ```python\n+    inputs = keras_core.Input(shape=(None, None, 3))\n+    processed = keras_core.layers.RandomCrop(width=128, height=128)(inputs)\n+    conv = keras_core.layers.Conv2D(filters=32, kernel_size=3)(processed)\n+    pooling = keras_core.layers.GlobalAveragePooling2D()(conv)\n+    feature = keras_core.layers.Dense(10)(pooling)\n+\n+    full_model = keras_core.Model(inputs, feature)\n+    backbone = keras_core.Model(processed, conv)\n+    activations = keras_core.Model(conv, feature)\n+    ```\n+\n+    Note that the `backbone` and `activations` models are not\n+    created with `keras_core.Input` objects, but with the tensors that originate\n+    from `keras_core.Input` objects. Under the hood, the layers and weights will\n+    be shared across these models, so that user can train the `full_model`, and\n+    use `backbone` or `activations` to do feature extraction.\n+    The inputs and outputs of the model can be nested structures of tensors as\n+    well, and the created models are standard Functional API models that support\n+    all the existing APIs.\n+\n+    ## By subclassing the `Model` class\n+\n+    In that case, you should define your\n+    layers in `__init__()` and you should implement the model's forward pass\n+    in `call()`.\n+\n+    ```python\n+    class MyModel(keras_core.Model):\n+        def __init__(self):\n+            super().__init__()\n+            self.dense1 = keras_core.layers.Dense(32, activation=\"relu\")\n+            self.dense2 = keras_core.layers.Dense(5, activation=\"softmax\")\n+\n+        def call(self, inputs):\n+            x = self.dense1(inputs)\n+            return self.dense2(x)\n+\n+    model = MyModel()\n+    ```\n+\n+    If you subclass `Model`, you can optionally have\n+    a `training` argument (boolean) in `call()`, which you can use to specify\n+    a different behavior in training and inference:\n+\n+    ```python\n+    class MyModel(keras_core.Model):\n+        def __init__(self):\n+            super().__init__()\n+            self.dense1 = keras_core.layers.Dense(32, activation=\"relu\")\n+            self.dense2 = keras_core.layers.Dense(5, activation=\"softmax\")\n+            self.dropout = keras_core.layers.Dropout(0.5)\n+\n+        def call(self, inputs, training=False):\n+            x = self.dense1(inputs)\n+            x = self.dropout(x, training=training)\n+            return self.dense2(x)\n+\n+    model = MyModel()\n+    ```\n+\n+    Once the model is created, you can config the model with losses and metrics\n+    with `model.compile()`, train the model with `model.fit()`, or use the model\n+    to do prediction with `model.predict()`.\n+\n+    ## With the `Sequential` class\n+\n+    In addition, `keras_core.Sequential` is a special case of model where\n+    the model is purely a stack of single-input, single-output layers.\n+\n+    ```python\n+    model = keras_core.Sequential([\n+        keras_core.Input(shape=(None, None, 3)),\n+        keras_core.layers.Conv2D(filters=32, kernel_size=3),\n+    ])\n+    ```\n     \"\"\"\n \n     def __new__(cls, *args, **kwargs):\n\n@@ -12,6 +12,51 @@ from keras_core.utils import tracking\n \n @keras_core_export([\"keras_core.Sequential\", \"keras_core.models.Sequential\"])\n class Sequential(Model):\n+    \"\"\"`Sequential` groups a linear stack of layers into a `Model`.\n+\n+    Examples:\n+\n+    ```python\n+    model = keras_core.Sequential()\n+    model.add(keras_core.Input(shape=(16,)))\n+    model.add(keras_core.layers.Dense(8))\n+\n+    # Note that you can also omit the initial `Input`.\n+    # In that case the model doesn't have any weights until the first call\n+    # to a training/evaluation method (since it isn't yet built):\n+    model = keras_core.Sequential()\n+    model.add(keras_core.layers.Dense(8))\n+    model.add(keras_core.layers.Dense(4))\n+    # model.weights not created yet\n+\n+    # Whereas if you specify an `Input`, the model gets built\n+    # continuously as you are adding layers:\n+    model = keras_core.Sequential()\n+    model.add(keras_core.Input(shape=(16,)))\n+    model.add(keras_core.layers.Dense(8))\n+    len(model.weights)  # Returns \"2\"\n+\n+    # When using the delayed-build pattern (no input shape specified), you can\n+    # choose to manually build your model by calling\n+    # `build(batch_input_shape)`:\n+    model = keras_core.Sequential()\n+    model.add(keras_core.layers.Dense(8))\n+    model.add(keras_core.layers.Dense(4))\n+    model.build((None, 16))\n+    len(model.weights)  # Returns \"4\"\n+\n+    # Note that when using the delayed-build pattern (no input shape specified),\n+    # the model gets built the first time you call `fit`, `eval`, or `predict`,\n+    # or the first time you call the model on some input data.\n+    model = keras_core.Sequential()\n+    model.add(keras_core.layers.Dense(8))\n+    model.add(keras_core.layers.Dense(1))\n+    model.compile(optimizer='sgd', loss='mse')\n+    # This builds the model for the first time:\n+    model.fit(x, y, batch_size=32, epochs=10)\n+    ```\n+    \"\"\"\n+\n     @tracking.no_automatic_dependency_tracking\n     def __init__(self, layers=None, trainable=True, name=None):\n         super().__init__(trainable=trainable, name=name)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
