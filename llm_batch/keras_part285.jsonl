{"custom_id": "keras#8cc7a9757dab71b21e74e70d563869461f017d52", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 193 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 1 | Methods Changed: 4 | Complexity Δ (Sum/Max): 28/28 | Churn Δ: 199 | Churn Cumulative: 13746 | Contributors (this commit): 129 | Commits (past 90d): 16 | Contributors (cumulative): 129 | DMM Complexity: 0.18181818181818182\n\nDIFF:\n@@ -2780,12 +2780,199 @@ def rnn(step_function, inputs, initial_states,\n \n     {{np_implementation}}\n     \"\"\"\n-    return tf_keras_backend.rnn(step_function, inputs, initial_states,\n-                                go_backwards=go_backwards,\n-                                mask=mask,\n-                                constants=constants,\n-                                unroll=unroll,\n-                                input_length=input_length)\n+    ndim = len(inputs.shape)\n+    if ndim < 3:\n+        raise ValueError('Input should be at least 3D.')\n+\n+    # Transpose to time-major, i.e.\n+    # from (batch, time, ...) to (time, batch, ...)\n+    axes = [1, 0] + list(range(2, ndim))\n+    inputs = tf.transpose(inputs, (axes))\n+\n+    if mask is not None:\n+        if mask.dtype != tf.bool:\n+            mask = tf.cast(mask, tf.bool)\n+        if len(mask.shape) != 2:\n+            raise ValueError(\n+                'mask should have `shape=(samples, time)`, '\n+                'got {}'.format(mask.shape))\n+        mask = tf.transpose(mask, [1, 0])\n+\n+        def get_matching_mask(mask_t, ref_tensor_t):\n+            # tf.where needs its condition tensor\n+            # to be the same shape as its two\n+            # result tensors\n+            ndim = len(ref_tensor_t.shape)\n+            for _ in range(ndim - 1):\n+                mask_t = expand_dims(mask_t)\n+            add_shape = tf.shape(ref_tensor_t)[1:]\n+            multiple = tf.concat([[1], add_shape], 0)\n+            return tf.tile(mask_t, multiple)\n+\n+    if constants is None:\n+        constants = []\n+\n+    uses_learning_phase = [False]\n+\n+    if unroll:\n+        if not inputs.shape[0]:\n+            raise ValueError('Unrolling requires a '\n+                             'fixed number of timesteps.')\n+        states = initial_states\n+        successive_states = []\n+        successive_outputs = []\n+\n+        input_list = tf.unstack(inputs)\n+        if go_backwards:\n+            input_list.reverse()\n+\n+        if mask is not None:\n+            mask_list = tf.unstack(mask)\n+            if go_backwards:\n+                mask_list.reverse()\n+\n+            for inp, mask_t in zip(input_list, mask_list):\n+                output, new_states = step_function(inp, states + constants)\n+                if getattr(output, '_uses_learning_phase', False):\n+                    uses_learning_phase[0] = True\n+\n+                if not successive_outputs:\n+                    prev_output = zeros_like(output)\n+                else:\n+                    prev_output = successive_outputs[-1]\n+\n+                output_mask_t = get_matching_mask(mask_t, output)\n+                output = tf.where(output_mask_t, output, prev_output)\n+\n+                return_states = []\n+                for state, new_state in zip(states, new_states):\n+                    state_mask_t = get_matching_mask(mask_t, new_state)\n+                    return_states.append(tf.where(state_mask_t,\n+                                                  new_state,\n+                                                  state))\n+                states = return_states\n+                successive_outputs.append(output)\n+                successive_states.append(states)\n+            last_output = successive_outputs[-1]\n+            new_states = successive_states[-1]\n+            outputs = tf.stack(successive_outputs)\n+        else:\n+            for inp in input_list:\n+                output, states = step_function(inp, states + constants)\n+                if getattr(output, '_uses_learning_phase', False):\n+                    uses_learning_phase[0] = True\n+                successive_outputs.append(output)\n+                successive_states.append(states)\n+            last_output = successive_outputs[-1]\n+            new_states = successive_states[-1]\n+            outputs = tf.stack(successive_outputs)\n+\n+    else:\n+        if go_backwards:\n+            inputs = reverse(inputs, 0)\n+\n+        states = tuple(initial_states)\n+\n+        time_steps = tf.shape(inputs)[0]\n+        output, _ = step_function(inputs[0], initial_states + constants)\n+        output_ta = tensor_array_ops.TensorArray(\n+            dtype=output.dtype,\n+            size=time_steps,\n+            tensor_array_name='output_ta')\n+        initial_output = zeros_like(output)\n+        input_ta = tensor_array_ops.TensorArray(\n+            dtype=inputs.dtype,\n+            size=time_steps,\n+            tensor_array_name='input_ta')\n+        input_ta = input_ta.unstack(inputs)\n+        time = tf.constant(0, dtype='int32', name='time')\n+        while_loop_kwargs = {\n+            'cond': lambda time, *_: time < time_steps,\n+            'parallel_iterations': 32,\n+            'swap_memory': True,\n+            'maximum_iterations': input_length}\n+\n+        if mask is not None:\n+            if go_backwards:\n+                mask = reverse(mask, 0)\n+\n+            mask_ta = tensor_array_ops.TensorArray(\n+                dtype=tf.bool,\n+                size=time_steps,\n+                tensor_array_name='mask_ta')\n+            mask_ta = mask_ta.unstack(mask)\n+\n+            def _step(time, output_ta_t, output_tm1, *states):\n+                \"\"\"RNN step function.\n+                # Arguments\n+                    time: Current timestep value.\n+                    output_ta_t: TensorArray.\n+                    output_tm1: output Tensor from previous timestep\n+                    *states: List of states.\n+                # Returns\n+                    Tuple: `(time + 1,output_ta_t) + tuple(new_states)`\n+                \"\"\"\n+                current_input = input_ta.read(time)\n+                mask_t = mask_ta.read(time)\n+                output, new_states = step_function(current_input,\n+                                                   tuple(states) +\n+                                                   tuple(constants))\n+                if getattr(output, '_uses_learning_phase', False):\n+                    uses_learning_phase[0] = True\n+                for state, new_state in zip(states, new_states):\n+                    new_state.set_shape(state.shape)\n+\n+                output_mask_t = get_matching_mask(mask_t, output)\n+                output = tf.where(output_mask_t, output, output_tm1)\n+\n+                new_states = [tf.where(get_matching_mask(mask_t, new_states[i]),\n+                                       new_states[i],\n+                                       states[i]) for i in range(len(states))]\n+\n+                output_ta_t = output_ta_t.write(time, output)\n+                return (time + 1, output_ta_t, output) + tuple(new_states)\n+\n+            final_outputs = control_flow_ops.while_loop(\n+                body=_step,\n+                loop_vars=(time, output_ta, initial_output) + states,\n+                **while_loop_kwargs)\n+            new_states = final_outputs[3:]  # skip output_tm1\n+        else:\n+            def _step(time, output_ta_t, *states):\n+                \"\"\"RNN step function.\n+                # Arguments\n+                    time: Current timestep value.\n+                    output_ta_t: TensorArray.\n+                    *states: List of states.\n+                # Returns\n+                    Tuple: `(time + 1,output_ta_t) + tuple(new_states)`\n+                \"\"\"\n+                current_input = input_ta.read(time)\n+                output, new_states = step_function(current_input,\n+                                                   tuple(states) +\n+                                                   tuple(constants))\n+                if getattr(output, '_uses_learning_phase', False):\n+                    uses_learning_phase[0] = True\n+                for state, new_state in zip(states, new_states):\n+                    new_state.set_shape(state.shape)\n+                output_ta_t = output_ta_t.write(time, output)\n+                return (time + 1, output_ta_t) + tuple(new_states)\n+\n+            final_outputs = control_flow_ops.while_loop(\n+                body=_step,\n+                loop_vars=(time, output_ta) + states,\n+                **while_loop_kwargs)\n+            new_states = final_outputs[2:]\n+\n+        last_time = final_outputs[0]\n+        output_ta = final_outputs[1]\n+        outputs = output_ta.stack()\n+        last_output = output_ta.read(last_time - 1)\n+\n+    axes = [1, 0] + list(range(2, len(outputs.shape)))\n+    outputs = tf.transpose(outputs, axes)\n+    last_output._uses_learning_phase = uses_learning_phase[0]\n+    return last_output, outputs, new_states\n \n \n @symbolic\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c4d29cdf21abb94736781acc3c2d73ac6746e280", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 53 | Lines Deleted: 59 | Files Changed: 4 | Hunks: 29 | Methods Changed: 28 | Complexity Δ (Sum/Max): 3/4 | Churn Δ: 112 | Churn Cumulative: 21142 | Contributors (this commit): 143 | Commits (past 90d): 31 | Contributors (cumulative): 187 | DMM Complexity: 0.0\n\nDIFF:\n@@ -6,6 +6,8 @@ import tensorflow as tf\n from tensorflow.python.eager import context\n from tensorflow.python.framework import ops as tf_ops\n from tensorflow.python.ops import image_ops as tf_image_ops\n+from tensorflow.python.ops import math_ops as tf_math_ops\n+from tensorflow.python.ops import state_ops as tf_state_ops\n from tensorflow.python.keras import backend as tf_keras_backend\n from tensorflow.python.training import moving_averages\n from tensorflow.python.ops import tensor_array_ops\n@@ -66,21 +68,27 @@ name_scope = tf.name_scope\n \n \n def symbolic(func):\n+    if _is_tf_1():\n+        return func\n+\n     @functools.wraps(func)\n-    def wrapper(*args, **kwargs):\n+    def symbolic_fn_wrapper(*args, **kwargs):\n         if _SYMBOLIC_SCOPE.value:\n             with get_graph().as_default():\n                 return func(*args, **kwargs)\n         else:\n             return func(*args, **kwargs)\n-    return wrapper\n+    return symbolic_fn_wrapper\n \n \n def eager(func):\n+    if _is_tf_1():\n+        return func\n+\n     global _SYMBOLIC_SCOPE\n \n     @functools.wraps(func)\n-    def wrapper(*args, **kwargs):\n+    def eager_fn_wrapper(*args, **kwargs):\n         prev_value = _SYMBOLIC_SCOPE.value\n         try:\n             _SYMBOLIC_SCOPE.value = False\n@@ -89,7 +97,7 @@ def eager(func):\n         finally:\n             _SYMBOLIC_SCOPE.value = prev_value\n         return out\n-    return wrapper\n+    return eager_fn_wrapper\n \n \n @symbolic\n@@ -934,7 +942,7 @@ def update(x, new_x):\n     # Returns\n         The variable `x` updated.\n     \"\"\"\n-    return tf_keras_backend.update(x, new_x)\n+    return tf_state_ops.assign(x, new_x)\n \n \n @symbolic\n@@ -948,7 +956,7 @@ def update_add(x, increment):\n     # Returns\n         The variable `x` updated.\n     \"\"\"\n-    return tf_keras_backend.update_add(x, increment)\n+    return tf_state_ops.assign_add(x, increment)\n \n \n @symbolic\n@@ -962,7 +970,7 @@ def update_sub(x, decrement):\n     # Returns\n         The variable `x` updated.\n     \"\"\"\n-    return tf_keras_backend.update_sub(x, decrement)\n+    return tf_state_ops.assign_sub(x, decrement)\n \n \n @symbolic\n@@ -977,10 +985,12 @@ def moving_average_update(x, value, momentum):\n     # Returns\n         An operation to update the variable.\n     \"\"\"\n-    if value.dtype != x.dtype:\n-        value = tf.cast(value, x.dtype)\n-    return moving_averages.assign_moving_average(\n-        x, value, momentum, zero_debias=True)\n+    with tf_ops.colocate_with(x):\n+        decay = tf_ops.convert_to_tensor(1.0 - momentum)\n+        if decay.dtype != x.dtype.base_dtype:\n+            decay = tf_math_ops.cast(decay, x.dtype.base_dtype)\n+        update_delta = (x - tf_math_ops.cast(value, x.dtype)) * decay\n+        return tf_state_ops.assign_sub(x, update_delta)\n \n \n # LINEAR ALGEBRA\n@@ -1640,7 +1650,7 @@ def log(x):\n     # Returns\n         A tensor.\n     \"\"\"\n-    return tf.log(x)\n+    return tf_math_ops.log(x)\n \n \n @symbolic\n@@ -2018,7 +2028,9 @@ def normalize_batch_in_training(x, gamma, beta,\n     # Returns\n         A tuple length of 3, `(normalized_tensor, mean, variance)`.\n     \"\"\"\n-    if ndim(x) == 4 and list(reduction_axes) in [[0, 1, 2], [0, 2, 3]]:\n+    if (ndim(x) == 4 and\n+            list(reduction_axes) in [[0, 1, 2], [0, 2, 3]] and\n+            _is_tf_1()):\n         if not _has_nchw_support() and list(reduction_axes) == [0, 2, 3]:\n             return _broadcast_normalize_batch_in_training(x, gamma, beta,\n                                                           reduction_axes,\n@@ -2066,9 +2078,10 @@ def batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=1e-3):\n         else:\n             tf_data_format = None\n \n-        if (tf_data_format == 'NHWC' or\n-                tf_data_format == 'NCHW' and\n-                _has_nchw_support()):\n+        if ((tf_data_format == 'NHWC' or\n+                (tf_data_format == 'NCHW' and\n+                 _has_nchw_support())) and\n+                _is_tf_1()):\n             # The mean / var / beta / gamma may be processed by broadcast\n             # so it may have extra axes with 1,\n             # it is not needed and should be removed\n@@ -2611,7 +2624,7 @@ def get_value(x):\n         A Numpy array.\n     \"\"\"\n     if _is_tf_1():\n-        return x.eval(session=get_session((x,)))\n+        return x.eval(session=get_session())\n     else:\n         return x.numpy()\n \n@@ -3279,6 +3292,7 @@ def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n         ValueError: if `axis` is neither -1 nor one of\n             the axes of `output`.\n     \"\"\"\n+    print('context:', context.executing_eagerly())\n     return tf_keras_backend.sparse_categorical_crossentropy(\n         target, output, from_logits=from_logits, axis=axis)\n \n@@ -4358,7 +4372,7 @@ def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n     input_length = tf.cast(tf.squeeze(input_length, axis=-1), tf.int32)\n     sparse_labels = tf.cast(\n         ctc_label_dense_to_sparse(y_true, label_length), tf.int32)\n-    y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())\n+    y_pred = tf_math_ops.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())\n     return tf.expand_dims(ctc.ctc_loss(inputs=y_pred,\n                                        labels=sparse_labels,\n                                        sequence_length=input_length), 1)\n@@ -4396,7 +4410,7 @@ def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n             Tensor `(top_paths, )` that contains\n                 the log probability of each decoded sequence.\n     \"\"\"\n-    y_pred = tf.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())\n+    y_pred = tf_math_ops.log(tf.transpose(y_pred, perm=[1, 0, 2]) + epsilon())\n     input_length = tf.cast(input_length, tf.int32)\n \n     if greedy:\n\n@@ -51,8 +51,7 @@ class MaxNorm(Constraint):\n     def __call__(self, w):\n         norms = K.sqrt(K.sum(K.square(w), axis=self.axis, keepdims=True))\n         desired = K.clip(norms, 0, self.max_value)\n-        w *= (desired / (K.epsilon() + norms))\n-        return w\n+        return w * (desired / (K.epsilon() + norms))\n \n     def get_config(self):\n         return {'max_value': self.max_value,\n@@ -64,8 +63,7 @@ class NonNeg(Constraint):\n     \"\"\"\n \n     def __call__(self, w):\n-        w *= K.cast(K.greater_equal(w, 0.), K.floatx())\n-        return w\n+        return w * K.cast(K.greater_equal(w, 0.), K.floatx())\n \n \n class UnitNorm(Constraint):\n@@ -136,8 +134,7 @@ class MinMaxNorm(Constraint):\n         norms = K.sqrt(K.sum(K.square(w), axis=self.axis, keepdims=True))\n         desired = (self.rate * K.clip(norms, self.min_value, self.max_value) +\n                    (1 - self.rate) * norms)\n-        w *= (desired / (K.epsilon() + norms))\n-        return w\n+        return w * (desired / (K.epsilon() + norms))\n \n     def get_config(self):\n         return {'min_value': self.min_value,\n\n@@ -2065,7 +2065,8 @@ class TestBackend(object):\n         # Restore old value\n         set_floatx(old_floatx)\n \n-    def test_setfloatx_correct_values(self):\n+    def DISABLED_test_setfloatx_correct_values(self):\n+        \"\"\"Disabled because it is not thread safe at this time.\"\"\"\n         # Keep track of the old value\n         old_floatx = floatx()\n         # Check correct values\n@@ -2077,8 +2078,9 @@ class TestBackend(object):\n \n     @pytest.mark.skipif((K.backend() == 'cntk'),\n                         reason='cntk does not support float16')\n-    def test_set_floatx(self):\n-        \"\"\"\n+    def DISABLED_test_set_floatx(self):\n+        \"\"\"Disabled because it is not thread safe at this time.\n+\n         Make sure that changes to the global floatx are effectively\n         taken into account by the backend.\n         \"\"\"\n@@ -2118,7 +2120,7 @@ class TestBackend(object):\n         assert np.allclose(K.eval(K.clip(x, min_value, max_value)),\n                            np.asarray([-5., -4., 0., 4., 9.], dtype=np.float32))\n \n-    @pytest.mark.skipif(K.backend() != 'tensorflow',\n+    @pytest.mark.skipif(K.backend() != 'tensorflow' or not KTF._is_tf_1(),\n                         reason='This test is for tensorflow parallelism.')\n     def test_tensorflow_session_parallelism_settings(self, monkeypatch):\n         for threads in [0, 1, 4]:\n\n@@ -36,7 +36,7 @@ def _test_optimizer(optimizer, target=0.75):\n                   optimizer=optimizer,\n                   metrics=['accuracy'])\n \n-    history = model.fit(x_train, y_train, epochs=2, batch_size=16, verbose=0)\n+    history = model.fit(x_train, y_train, epochs=3, batch_size=16, verbose=0)\n     assert history.history['acc'][-1] >= target\n     config = optimizers.serialize(optimizer)\n     optim = optimizers.deserialize(config)\n@@ -69,7 +69,8 @@ def _test_optimizer(optimizer, target=0.75):\n def test_no_grad():\n     inp = Input([3])\n     x = Dense(10)(inp)\n-    x = Lambda(lambda l: 1.0 * K.reshape(K.cast(K.argmax(l), 'float32'), [-1, 1]),\n+    x = Lambda(\n+        lambda l: 1.0 * K.reshape(K.cast(K.argmax(l), 'float32'), [-1, 1]),\n         output_shape=lambda x: [x[0], 1])(x)\n     mod = Model(inp, x)\n     mod.compile('sgd', 'mse')\n@@ -131,15 +132,20 @@ def test_clipvalue():\n                     reason='Requires TensorFlow backend')\n def test_tfoptimizer():\n     from keras import constraints\n-    from tensorflow import train\n-    optimizer = optimizers.TFOptimizer(train.AdamOptimizer())\n+    import tensorflow as tf\n+    if tf.__version__.startswith('1.'):\n+        optimizer = optimizers.TFOptimizer(tf.train.AdamOptimizer())\n+    else:\n+        optimizer = tf.keras.optimizers.Adam()\n+\n     model = Sequential()\n     model.add(Dense(num_classes, input_shape=(3,),\n                     kernel_constraint=constraints.MaxNorm(1)))\n     model.compile(loss='mean_squared_error', optimizer=optimizer)\n     model.fit(np.random.random((5, 3)), np.random.random((5, num_classes)),\n               epochs=1, batch_size=5, verbose=0)\n-    # not supported\n+\n+    if tf.__version__.startswith('1.'):\n         with pytest.raises(NotImplementedError):\n             optimizer.weights\n         with pytest.raises(NotImplementedError):\n@@ -148,30 +154,5 @@ def test_tfoptimizer():\n             optimizer.from_config(None)\n \n \n-@pytest.mark.skipif((K.backend() != 'tensorflow'),\n-                    reason='Requires TensorFlow backend')\n-def test_tfoptimizer_pass_correct_named_params_to_native_tensorflow_optimizer():\n-    from keras import constraints\n-    from tensorflow import train\n-\n-    class MyTfOptimizer(train.Optimizer):\n-        wrapping_optimizer = train.AdamOptimizer()\n-\n-        def compute_gradients(self, loss, **kwargs):\n-            return super(MyTfOptimizer, self).compute_gradients(loss, **kwargs)\n-\n-        def apply_gradients(self, grads_and_vars, **kwargs):\n-            return self.wrapping_optimizer.apply_gradients(grads_and_vars,\n-                                                           **kwargs)\n-    my_tf_optimizer = MyTfOptimizer(use_locking=False, name='MyTfOptimizer')\n-    optimizer = optimizers.TFOptimizer(my_tf_optimizer)\n-    model = Sequential()\n-    model.add(Dense(num_classes, input_shape=(3,),\n-                    kernel_constraint=constraints.MaxNorm(1)))\n-    model.compile(loss='mean_squared_error', optimizer=optimizer)\n-    model.fit(np.random.random((5, 3)), np.random.random((5, num_classes)),\n-              epochs=1, batch_size=5, verbose=0)\n-\n-\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0ca37c9879ae36774ca508c7093c3840c37eb910", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 14 | Methods Changed: 9 | Complexity Δ (Sum/Max): 11/11 | Churn Δ: 59 | Churn Cumulative: 17492 | Contributors (this commit): 194 | Commits (past 90d): 23 | Contributors (cumulative): 215 | DMM Complexity: 0.0\n\nDIFF:\n@@ -111,10 +111,7 @@ def learning_phase():\n     # Returns\n         Learning phase (scalar integer tensor or Python integer).\n     \"\"\"\n-    lp = tf_keras_backend.learning_phase()\n-    if isinstance(lp, tf_ops.Tensor) and lp.dtype.name == 'bool':\n-        return tf.cast(lp, 'int32')\n-    return lp\n+    return tf_keras_backend.learning_phase()\n \n \n @symbolic\n@@ -174,6 +171,28 @@ def set_session(session):\n     tf_keras_backend.set_session(session)\n \n \n+def v1_variable_initialization():\n+    session = get_session()\n+    with session.graph.as_default():\n+        variables = tf.global_variables()\n+        candidate_vars = []\n+        for v in variables:\n+            if not getattr(v, '_keras_initialized', False):\n+                candidate_vars.append(v)\n+        if candidate_vars:\n+            # This step is expensive, so we only run it on variables\n+            # not already marked as initialized.\n+            is_initialized = session.run(\n+                [tf.is_variable_initialized(v) for v in candidate_vars])\n+            uninitialized_vars = []\n+            for flag, v in zip(is_initialized, candidate_vars):\n+                if not flag:\n+                    uninitialized_vars.append(v)\n+                v._keras_initialized = True\n+            if uninitialized_vars:\n+                session.run(tf.variables_initializer(uninitialized_vars))\n+\n+\n # DEVICE MANIPULATION AND PROBING\n \n class _TfDeviceCaptureOp(object):\n@@ -314,7 +333,7 @@ def to_dense(tensor):\n     ```\n     \"\"\"\n     if is_sparse(tensor):\n-        return tf.sparse_tensor_to_dense(tensor)\n+        return tf.sparse.to_dense(tensor)\n     else:\n         return tensor\n \n@@ -625,7 +644,10 @@ def eval(x):\n     ```\n     {{np_implementation}}\n     \"\"\"\n+    if _is_tf_1():\n+        return to_dense(x).eval(session=get_session())\n     if hasattr(x, 'numpy'):\n+        with context.eager_mode():\n             return x.numpy()\n     eval_fn = function([], [x])\n     return eval_fn([])[0]\n@@ -1061,7 +1083,7 @@ def dot(x, y):\n         return tf.reshape(tf.matmul(xt, yt),\n                           x_shape[:-1] + y_shape[:-2] + y_shape[-1:])\n     if is_sparse(x):\n-        out = tf.matmul(x, y)\n+        out = tf.sparse.sparse_dense_matmul(x, y)\n     else:\n         out = tf.matmul(x, y)\n     return out\n@@ -2141,7 +2163,10 @@ def concatenate(tensors, axis=-1):\n             axis %= rank\n         else:\n             axis = 0\n-    return tf.concat(tensors, axis=axis)\n+    if py_all([is_sparse(x) for x in tensors]):\n+        return tf.sparse.concat(axis, tensors)\n+    else:\n+        return tf.concat([to_dense(x) for x in tensors], axis)\n \n \n @symbolic\n@@ -2702,6 +2727,8 @@ def print_tensor(x, message=''):\n # GRAPH MANIPULATION\n \n def function(inputs, outputs, updates=None, **kwargs):\n+    if _is_tf_1():\n+        v1_variable_initialization()\n     return tf_keras_backend.function(inputs, outputs,\n                                      updates=updates,\n                                      **kwargs)\n@@ -4173,8 +4200,9 @@ def bias_add(x, bias, data_format=None):\n             new_shape = (1, 1, 1, 1, bias_shape[0])\n         else:\n             new_shape = (1,) + bias_shape\n-        new_shape = transpose_shape(new_shape, data_format, spatial_axes=(1, 2, 3))\n-        x += reshape(bias, new_shape)\n+        new_shape = transpose_shape(new_shape, data_format,\n+                                    spatial_axes=(1, 2, 3))\n+        x = x + reshape(bias, new_shape)\n     elif ndim(x) == 4:\n         if data_format == 'channels_first':\n             if len(bias_shape) == 1:\n@@ -4182,22 +4210,23 @@ def bias_add(x, bias, data_format=None):\n                     x = tf.nn.bias_add(x, bias,\n                                        data_format='NCHW')\n                 else:\n-                    x += reshape(bias, (1, bias_shape[0], 1, 1))\n+                    x = x + reshape(bias, (1, bias_shape[0], 1, 1))\n             else:\n-                x += reshape(bias, (1, bias_shape[2]) + bias_shape[:2])\n+                x = x + reshape(bias, (1, bias_shape[2]) + bias_shape[:2])\n         elif data_format == 'channels_last':\n             if len(bias_shape) == 1:\n                 x = tf.nn.bias_add(x, bias,\n                                    data_format='NHWC')\n             else:\n-                x += reshape(bias, (1,) + bias_shape)\n+                x = x + reshape(bias, (1,) + bias_shape)\n     elif ndim(x) == 3:\n         if len(bias_shape) == 1:\n             new_shape = (1, 1, bias_shape[0])\n         else:\n             new_shape = (1,) + bias_shape\n-        new_shape = transpose_shape(new_shape, data_format, spatial_axes=(1,))\n-        x += reshape(bias, new_shape)\n+        new_shape = transpose_shape(new_shape, data_format,\n+                                    spatial_axes=(1,))\n+        x = x + reshape(bias, new_shape)\n     else:\n         x = tf.nn.bias_add(x, bias)\n     return x\n\n@@ -1149,7 +1149,7 @@ class TensorBoard(Callback):\n                     embedding_input = tf.reshape(embedding_input,\n                                                  (step, int(embedding_size)))\n                     shape = (self.embeddings_data[0].shape[0], int(embedding_size))\n-                    embedding = tf.Variable(tf.zeros(shape),\n+                    embedding = K.variable(K.zeros(shape),\n                                            name=layer.name + '_embedding')\n                     embeddings_vars[layer.name] = embedding\n                     batch = tf.assign(embedding[batch_id:batch_id + step],\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#14625e57af7fe85eae501582ce1da135fc04c8e8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 979 | Contributors (this commit): 24 | Commits (past 90d): 2 | Contributors (cumulative): 29 | DMM Complexity: None\n\nDIFF:\n@@ -1,4 +1,5 @@\n-'''Example script to generate text from Nietzsche's writings.\n+'''\n+#Example script to generate text from Nietzsche's writings.\n \n At least 20 epochs are required before the generated text\n starts sounding coherent.\n\n@@ -1,7 +1,8 @@\n # -*- coding: utf-8 -*-\n \"\"\"\n-Train an Auxiliary Classifier Generative Adversarial Network (ACGAN) on the\n-MNIST dataset. See https://arxiv.org/abs/1610.09585 for more details.\n+#Train an Auxiliary Classifier GAN (ACGAN) on the MNIST dataset.\n+\n+[More details on Auxiliary Classifier GANs.](https://arxiv.org/abs/1610.09585)\n \n You should start to see reasonable images after ~5 epochs, and good images\n by ~15 epochs. You should use a GPU, as the convolution-heavy operations are\n@@ -11,13 +12,13 @@ as the compilation time can be a blocker using Theano.\n Timings:\n \n Hardware           | Backend | Time / Epoch\n--------------------------------------------\n+:------------------|:--------|------------:\n  CPU               | TF      | 3 hrs\n  Titan X (maxwell) | TF      | 4 min\n  Titan X (maxwell) | TH      | 7 min\n \n-Consult https://github.com/lukedeo/keras-acgan for more information and\n-example output\n+Consult [Auxiliary Classifier Generative Adversarial Networks in Keras\n+](https://github.com/lukedeo/keras-acgan) for more information and example output.\n \"\"\"\n from __future__ import print_function\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c30223e510486179282b3480fc923dcb9863e3d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 70 | Lines Deleted: 300 | Files Changed: 5 | Hunks: 115 | Methods Changed: 18 | Complexity Δ (Sum/Max): -14/0 | Churn Δ: 370 | Churn Cumulative: 27917 | Contributors (this commit): 205 | Commits (past 90d): 34 | Contributors (cumulative): 279 | DMM Complexity: 1.0\n\nDIFF:\n@@ -9,7 +9,7 @@ from tensorflow.python.ops import image_ops as tf_image_ops\n from tensorflow.python.ops import math_ops as tf_math_ops\n from tensorflow.python.ops import state_ops as tf_state_ops\n from tensorflow.python.keras import backend as tf_keras_backend\n-from tensorflow.python.training import moving_averages\n+from tensorflow.python.keras.utils import tf_utils\n from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import control_flow_ops\n from tensorflow.python.ops import functional_ops\n@@ -18,7 +18,6 @@ from .common import floatx, epsilon, image_data_format\n \n import functools\n import threading\n-import contextlib\n \n import numpy as np\n from distutils.version import StrictVersion\n@@ -40,6 +39,7 @@ _LOCAL_DEVICES = None\n \n _SYMBOLIC_SCOPE = threading.local()\n _SYMBOLIC_SCOPE.value = True\n+_LEARNING_PHASE_CACHE = {}\n \n \n def _is_tf_1():\n@@ -59,7 +59,6 @@ set_image_data_format = tf_keras_backend.set_image_data_format\n get_graph = tf_keras_backend.get_graph\n get_uid = tf_keras_backend.get_uid\n reset_uids = tf_keras_backend.reset_uids\n-clear_session = tf_keras_backend.clear_session\n manual_variable_initialization = tf_keras_backend.manual_variable_initialization\n \n # TODO\n@@ -81,6 +80,10 @@ def symbolic(func):\n     return symbolic_fn_wrapper\n \n \n+def is_symbolic(x):\n+    return isinstance(x, tf.Tensor) and hasattr(x, 'op')\n+\n+\n def eager(func):\n     if _is_tf_1():\n         return func\n@@ -111,7 +114,18 @@ def learning_phase():\n     # Returns\n         Learning phase (scalar integer tensor or Python integer).\n     \"\"\"\n-    return tf_keras_backend.learning_phase()\n+    lp = tf_keras_backend.learning_phase()\n+    if _is_tf_1():\n+        return lp\n+    else:\n+        if isinstance(lp, int):\n+            return lp\n+        if lp in _LEARNING_PHASE_CACHE:\n+            return _LEARNING_PHASE_CACHE[lp]\n+        with name_scope(''):\n+            int_lp = tf.cast(lp, 'int32', name='learning_phase')\n+        _LEARNING_PHASE_CACHE[lp] = int_lp\n+        return int_lp\n \n \n @symbolic\n@@ -171,6 +185,16 @@ def set_session(session):\n     tf_keras_backend.set_session(session)\n \n \n+def clear_session():\n+    \"\"\"Destroys the current Keras graph and creates a new one.\n+\n+    Useful to avoid clutter from old models / layers.\n+    \"\"\"\n+    tf_keras_backend.clear_session()\n+    global _LEARNING_PHASE_CACHE\n+    _LEARNING_PHASE_CACHE = {}\n+\n+\n def v1_variable_initialization():\n     session = get_session()\n     with session.graph.as_default():\n@@ -588,10 +612,7 @@ def ndim(x):\n \n     {{np_implementation}}\n     \"\"\"\n-    dims = x.shape._dims\n-    if dims is not None:\n-        return len(dims)\n-    return None\n+    return x.shape.rank\n \n \n def dtype(x):\n@@ -1017,7 +1038,6 @@ def moving_average_update(x, value, momentum):\n \n # LINEAR ALGEBRA\n \n-@symbolic\n def dot(x, y):\n     \"\"\"Multiplies 2 tensors (and/or variables) and returns a *tensor*.\n \n@@ -1089,7 +1109,6 @@ def dot(x, y):\n     return out\n \n \n-@symbolic\n def batch_dot(x, y, axes=None):\n     \"\"\"Batchwise dot product.\n \n@@ -1294,7 +1313,6 @@ def batch_dot(x, y, axes=None):\n     return result\n \n \n-@symbolic\n def transpose(x):\n     \"\"\"Transposes a tensor and returns it.\n \n@@ -1331,7 +1349,6 @@ def transpose(x):\n     return tf.transpose(x)\n \n \n-@symbolic\n def gather(reference, indices):\n     \"\"\"Retrieves the elements of indices `indices` in the tensor `reference`.\n \n@@ -1350,7 +1367,6 @@ def gather(reference, indices):\n # ELEMENT-WISE OPERATIONS\n \n \n-@symbolic\n def max(x, axis=None, keepdims=False):\n     \"\"\"Maximum value in a tensor.\n \n@@ -1372,7 +1388,6 @@ def max(x, axis=None, keepdims=False):\n     return tf.reduce_max(x, axis, keepdims)\n \n \n-@symbolic\n def min(x, axis=None, keepdims=False):\n     \"\"\"Minimum value in a tensor.\n \n@@ -1394,7 +1409,6 @@ def min(x, axis=None, keepdims=False):\n     return tf.reduce_min(x, axis, keepdims)\n \n \n-@symbolic\n def sum(x, axis=None, keepdims=False):\n     \"\"\"Sum of the values in a tensor, alongside the specified axis.\n \n@@ -1416,7 +1430,6 @@ def sum(x, axis=None, keepdims=False):\n     return tf.reduce_sum(x, axis, keepdims)\n \n \n-@symbolic\n def prod(x, axis=None, keepdims=False):\n     \"\"\"Multiplies the values in a tensor, alongside the specified axis.\n \n@@ -1438,7 +1451,6 @@ def prod(x, axis=None, keepdims=False):\n     return tf.reduce_prod(x, axis, keepdims)\n \n \n-@symbolic\n def cumsum(x, axis=0):\n     \"\"\"Cumulative sum of the values in a tensor, alongside the specified axis.\n \n@@ -1450,10 +1462,9 @@ def cumsum(x, axis=0):\n         A tensor of the cumulative sum of values of `x` along `axis`.\n     {{np_implementation}}\n     \"\"\"\n-    return tf.cumsum(x, axis=axis)\n+    return tf_math_ops.cumsum(x, axis=axis)\n \n \n-@symbolic\n def cumprod(x, axis=0):\n     \"\"\"Cumulative product of the values in a tensor, alongside the specified axis.\n \n@@ -1465,10 +1476,9 @@ def cumprod(x, axis=0):\n         A tensor of the cumulative product of values of `x` along `axis`.\n     {{np_implementation}}\n     \"\"\"\n-    return tf.cumprod(x, axis=axis)\n+    return tf_math_ops.cumprod(x, axis=axis)\n \n \n-@symbolic\n def var(x, axis=None, keepdims=False):\n     \"\"\"Variance of a tensor, alongside the specified axis.\n \n@@ -1495,7 +1505,6 @@ def var(x, axis=None, keepdims=False):\n                           keepdims)\n \n \n-@symbolic\n def std(x, axis=None, keepdims=False):\n     \"\"\"Standard deviation of a tensor, alongside the specified axis.\n \n@@ -1516,7 +1525,6 @@ def std(x, axis=None, keepdims=False):\n     return tf.sqrt(var(x, axis=axis, keepdims=keepdims))\n \n \n-@symbolic\n def mean(x, axis=None, keepdims=False):\n     \"\"\"Mean of a tensor, alongside the specified axis.\n \n@@ -1539,7 +1547,6 @@ def mean(x, axis=None, keepdims=False):\n     return tf.reduce_mean(x, axis, keepdims)\n \n \n-@symbolic\n def any(x, axis=None, keepdims=False):\n     \"\"\"Bitwise reduction (logical OR).\n \n@@ -1576,7 +1583,6 @@ def all(x, axis=None, keepdims=False):\n     return tf.reduce_all(x, axis, keepdims)\n \n \n-@symbolic\n def argmax(x, axis=-1):\n     \"\"\"Returns the index of the maximum value along an axis.\n \n@@ -1591,7 +1597,6 @@ def argmax(x, axis=-1):\n     return tf.argmax(x, axis)\n \n \n-@symbolic\n def argmin(x, axis=-1):\n     \"\"\"Returns the index of the minimum value along an axis.\n \n@@ -1606,7 +1611,6 @@ def argmin(x, axis=-1):\n     return tf.argmin(x, axis)\n \n \n-@symbolic\n def square(x):\n     \"\"\"Element-wise square.\n \n@@ -1619,7 +1623,6 @@ def square(x):\n     return tf.square(x)\n \n \n-@symbolic\n def abs(x):\n     \"\"\"Element-wise absolute value.\n \n@@ -1632,7 +1635,6 @@ def abs(x):\n     return tf.abs(x)\n \n \n-@symbolic\n def sqrt(x):\n     \"\"\"Element-wise square root.\n \n@@ -1649,7 +1651,6 @@ def sqrt(x):\n     return tf.sqrt(x)\n \n \n-@symbolic\n def exp(x):\n     \"\"\"Element-wise exponential.\n \n@@ -1662,7 +1663,6 @@ def exp(x):\n     return tf.exp(x)\n \n \n-@symbolic\n def log(x):\n     \"\"\"Element-wise log.\n \n@@ -1675,7 +1675,6 @@ def log(x):\n     return tf_math_ops.log(x)\n \n \n-@symbolic\n def logsumexp(x, axis=None, keepdims=False):\n     \"\"\"Computes log(sum(exp(elements across dimensions of a tensor))).\n \n@@ -1700,7 +1699,6 @@ def logsumexp(x, axis=None, keepdims=False):\n     return tf.reduce_logsumexp(x, axis, keepdims)\n \n \n-@symbolic\n def round(x):\n     \"\"\"Element-wise rounding to the closest integer.\n \n@@ -1715,7 +1713,6 @@ def round(x):\n     return tf.round(x)\n \n \n-@symbolic\n def sign(x):\n     \"\"\"Element-wise sign.\n \n@@ -1728,7 +1725,6 @@ def sign(x):\n     return tf.sign(x)\n \n \n-@symbolic\n def pow(x, a):\n     \"\"\"Element-wise exponentiation.\n \n@@ -1743,7 +1739,6 @@ def pow(x, a):\n     return tf.pow(x, a)\n \n \n-@symbolic\n def clip(x, min_value, max_value):\n     \"\"\"Element-wise value clipping.\n \n@@ -1767,7 +1762,6 @@ def clip(x, min_value, max_value):\n     return tf.clip_by_value(x, min_value, max_value)\n \n \n-@symbolic\n def equal(x, y):\n     \"\"\"Element-wise equality between two tensors.\n \n@@ -1783,7 +1777,6 @@ def equal(x, y):\n     return tf.equal(x, y)\n \n \n-@symbolic\n def not_equal(x, y):\n     \"\"\"Element-wise inequality between two tensors.\n \n@@ -1799,7 +1792,6 @@ def not_equal(x, y):\n     return tf.not_equal(x, y)\n \n \n-@symbolic\n def greater(x, y):\n     \"\"\"Element-wise truth value of (x > y).\n \n@@ -1815,7 +1807,6 @@ def greater(x, y):\n     return tf.greater(x, y)\n \n \n-@symbolic\n def greater_equal(x, y):\n     \"\"\"Element-wise truth value of (x >= y).\n \n@@ -1831,7 +1822,6 @@ def greater_equal(x, y):\n     return tf.greater_equal(x, y)\n \n \n-@symbolic\n def less(x, y):\n     \"\"\"Element-wise truth value of (x < y).\n \n@@ -1847,7 +1837,6 @@ def less(x, y):\n     return tf.less(x, y)\n \n \n-@symbolic\n def less_equal(x, y):\n     \"\"\"Element-wise truth value of (x <= y).\n \n@@ -1863,7 +1852,6 @@ def less_equal(x, y):\n     return tf.less_equal(x, y)\n \n \n-@symbolic\n def maximum(x, y):\n     \"\"\"Element-wise maximum of two tensors.\n \n@@ -1879,7 +1867,6 @@ def maximum(x, y):\n     return tf.maximum(x, y)\n \n \n-@symbolic\n def minimum(x, y):\n     \"\"\"Element-wise minimum of two tensors.\n \n@@ -1895,7 +1882,6 @@ def minimum(x, y):\n     return tf.minimum(x, y)\n \n \n-@symbolic\n def sin(x):\n     \"\"\"Computes sin of x element-wise.\n \n@@ -1908,7 +1894,6 @@ def sin(x):\n     return tf.sin(x)\n \n \n-@symbolic\n def cos(x):\n     \"\"\"Computes cos of x element-wise.\n \n@@ -2034,7 +2019,6 @@ def _fused_normalize_batch_in_training(x, gamma, beta, reduction_axes,\n         data_format=tf_data_format)\n \n \n-@symbolic\n def normalize_batch_in_training(x, gamma, beta,\n                                 reduction_axes, epsilon=1e-3):\n     \"\"\"Computes mean and std for batch then apply batch_normalization on batch.\n@@ -2071,7 +2055,6 @@ def normalize_batch_in_training(x, gamma, beta,\n                                                           epsilon=epsilon)\n \n \n-@symbolic\n def batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=1e-3):\n     \"\"\"Applies batch normalization on x given mean, var, beta and gamma.\n \n@@ -2146,7 +2129,6 @@ def batch_normalization(x, mean, var, beta, gamma, axis=-1, epsilon=1e-3):\n \n # SHAPE OPERATIONS\n \n-@symbolic\n def concatenate(tensors, axis=-1):\n     \"\"\"Concatenates a list of tensors alongside the specified axis.\n \n@@ -2169,7 +2151,6 @@ def concatenate(tensors, axis=-1):\n         return tf.concat([to_dense(x) for x in tensors], axis)\n \n \n-@symbolic\n def reshape(x, shape):\n     \"\"\"Reshapes a tensor to the specified shape.\n \n@@ -2183,7 +2164,6 @@ def reshape(x, shape):\n     return tf.reshape(x, shape)\n \n \n-@symbolic\n def permute_dimensions(x, pattern):\n     \"\"\"Permutes axes in a tensor.\n \n@@ -2198,7 +2178,6 @@ def permute_dimensions(x, pattern):\n     return tf.transpose(x, perm=pattern)\n \n \n-@symbolic\n def resize_images(x,\n                   height_factor,\n                   width_factor,\n@@ -2257,7 +2236,6 @@ def resize_images(x,\n     return x\n \n \n-@symbolic\n def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n     \"\"\"Resizes the volume contained in a 5D tensor.\n \n@@ -2289,7 +2267,6 @@ def resize_volumes(x, depth_factor, height_factor, width_factor, data_format):\n         raise ValueError('Unknown data_format: ' + str(data_format))\n \n \n-@symbolic\n def repeat_elements(x, rep, axis):\n     \"\"\"Repeats the elements of a tensor along an axis, like `np.repeat`.\n \n@@ -2340,7 +2317,6 @@ def repeat_elements(x, rep, axis):\n     return x_rep\n \n \n-@symbolic\n def repeat(x, n):\n     \"\"\"Repeats a 2D tensor.\n \n@@ -2360,7 +2336,6 @@ def repeat(x, n):\n     return tf.tile(x, pattern)\n \n \n-@symbolic\n def arange(start, stop=None, step=1, dtype='int32'):\n     \"\"\"Creates a 1D tensor containing a sequence of integers.\n \n@@ -2398,7 +2373,6 @@ def arange(start, stop=None, step=1, dtype='int32'):\n     return result\n \n \n-@symbolic\n def tile(x, n):\n     \"\"\"Creates a tensor by tiling `x` by `n`.\n \n@@ -2415,7 +2389,6 @@ def tile(x, n):\n     return tf.tile(x, n)\n \n \n-@symbolic\n def flatten(x):\n     \"\"\"Flatten a tensor.\n \n@@ -2428,7 +2401,6 @@ def flatten(x):\n     return tf.reshape(x, [-1])\n \n \n-@symbolic\n def batch_flatten(x):\n     \"\"\"Turn a nD tensor into a 2D tensor with same 0th dimension.\n \n@@ -2444,7 +2416,6 @@ def batch_flatten(x):\n     return x\n \n \n-@symbolic\n def expand_dims(x, axis=-1):\n     \"\"\"Adds a 1-sized dimension at index \"axis\".\n \n@@ -2458,7 +2429,6 @@ def expand_dims(x, axis=-1):\n     return tf.expand_dims(x, axis)\n \n \n-@symbolic\n def squeeze(x, axis):\n     \"\"\"Removes a 1-dimension from the tensor at index \"axis\".\n \n@@ -2472,7 +2442,6 @@ def squeeze(x, axis):\n     return tf.squeeze(x, [axis])\n \n \n-@symbolic\n def temporal_padding(x, padding=(1, 1)):\n     \"\"\"Pads the middle dimension of a 3D tensor.\n \n@@ -2489,7 +2458,6 @@ def temporal_padding(x, padding=(1, 1)):\n     return tf.pad(x, pattern)\n \n \n-@symbolic\n def spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n     \"\"\"Pads the 2nd and 3rd dimensions of a 4D tensor.\n \n@@ -2518,7 +2486,6 @@ def spatial_2d_padding(x, padding=((1, 1), (1, 1)), data_format=None):\n     return tf.pad(x, pattern)\n \n \n-@symbolic\n def spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n     \"\"\"Pads 5D tensor with zeros along the depth, height, width dimensions.\n \n@@ -2561,7 +2528,6 @@ def spatial_3d_padding(x, padding=((1, 1), (1, 1), (1, 1)), data_format=None):\n     return tf.pad(x, pattern)\n \n \n-@symbolic\n def stack(x, axis=0):\n     \"\"\"Stacks a list of rank `R` tensors into a rank `R+1` tensor.\n \n@@ -2577,7 +2543,6 @@ def stack(x, axis=0):\n     return tf.stack(x, axis=axis)\n \n \n-@symbolic\n def one_hot(indices, num_classes):\n     \"\"\"Computes the one-hot representation of an integer tensor.\n \n@@ -2593,7 +2558,6 @@ def one_hot(indices, num_classes):\n     return tf.one_hot(indices, depth=num_classes, axis=-1)\n \n \n-@symbolic\n def reverse(x, axes):\n     \"\"\"Reverses a tensor along the specified axes.\n \n@@ -2612,7 +2576,6 @@ def reverse(x, axes):\n     return tf.reverse(x, axes)\n \n \n-@symbolic\n def slice(x, start, size):\n     \"\"\"Extracts a slice from a tensor.\n \n@@ -2700,7 +2663,6 @@ def get_variable_shape(x):\n     return int_shape(x)\n \n \n-@symbolic\n def print_tensor(x, message=''):\n     \"\"\"Prints `message` and the tensor value when evaluated.\n \n@@ -2770,7 +2732,6 @@ def stop_gradient(variables):\n \n # CONTROL FLOW\n \n-@symbolic\n def rnn(step_function, inputs, initial_states,\n         go_backwards=False, mask=None, constants=None,\n         unroll=False, input_length=None):\n@@ -2820,198 +2781,20 @@ def rnn(step_function, inputs, initial_states,\n \n     {{np_implementation}}\n     \"\"\"\n-    ndim = len(inputs.shape)\n-    if ndim < 3:\n-        raise ValueError('Input should be at least 3D.')\n-\n-    # Transpose to time-major, i.e.\n-    # from (batch, time, ...) to (time, batch, ...)\n-    axes = [1, 0] + list(range(2, ndim))\n-    inputs = tf.transpose(inputs, (axes))\n-\n-    if mask is not None:\n-        if mask.dtype != tf.bool:\n-            mask = tf.cast(mask, tf.bool)\n-        if len(mask.shape) != 2:\n-            raise ValueError(\n-                'mask should have `shape=(samples, time)`, '\n-                'got {}'.format(mask.shape))\n-        mask = tf.transpose(mask, [1, 0])\n-\n-        def get_matching_mask(mask_t, ref_tensor_t):\n-            # tf.where needs its condition tensor\n-            # to be the same shape as its two\n-            # result tensors\n-            ndim = len(ref_tensor_t.shape)\n-            for _ in range(ndim - 1):\n-                mask_t = expand_dims(mask_t)\n-            add_shape = tf.shape(ref_tensor_t)[1:]\n-            multiple = tf.concat([[1], add_shape], 0)\n-            return tf.tile(mask_t, multiple)\n-\n-    if constants is None:\n-        constants = []\n-\n-    uses_learning_phase = [False]\n-\n-    if unroll:\n-        if not inputs.shape[0]:\n-            raise ValueError('Unrolling requires a '\n-                             'fixed number of timesteps.')\n-        states = initial_states\n-        successive_states = []\n-        successive_outputs = []\n-\n-        input_list = tf.unstack(inputs)\n-        if go_backwards:\n-            input_list.reverse()\n-\n-        if mask is not None:\n-            mask_list = tf.unstack(mask)\n-            if go_backwards:\n-                mask_list.reverse()\n-\n-            for inp, mask_t in zip(input_list, mask_list):\n-                output, new_states = step_function(inp, states + constants)\n-                if getattr(output, '_uses_learning_phase', False):\n-                    uses_learning_phase[0] = True\n-\n-                if not successive_outputs:\n-                    prev_output = zeros_like(output)\n-                else:\n-                    prev_output = successive_outputs[-1]\n-\n-                output_mask_t = get_matching_mask(mask_t, output)\n-                output = tf.where(output_mask_t, output, prev_output)\n-\n-                return_states = []\n-                for state, new_state in zip(states, new_states):\n-                    state_mask_t = get_matching_mask(mask_t, new_state)\n-                    return_states.append(tf.where(state_mask_t,\n-                                                  new_state,\n-                                                  state))\n-                states = return_states\n-                successive_outputs.append(output)\n-                successive_states.append(states)\n-            last_output = successive_outputs[-1]\n-            new_states = successive_states[-1]\n-            outputs = tf.stack(successive_outputs)\n-        else:\n-            for inp in input_list:\n-                output, states = step_function(inp, states + constants)\n-                if getattr(output, '_uses_learning_phase', False):\n-                    uses_learning_phase[0] = True\n-                successive_outputs.append(output)\n-                successive_states.append(states)\n-            last_output = successive_outputs[-1]\n-            new_states = successive_states[-1]\n-            outputs = tf.stack(successive_outputs)\n-\n-    else:\n-        if go_backwards:\n-            inputs = reverse(inputs, 0)\n-\n-        states = tuple(initial_states)\n-\n-        time_steps = tf.shape(inputs)[0]\n-        output, _ = step_function(inputs[0], initial_states + constants)\n-        output_ta = tensor_array_ops.TensorArray(\n-            dtype=output.dtype,\n-            size=time_steps,\n-            tensor_array_name='output_ta')\n-        initial_output = zeros_like(output)\n-        input_ta = tensor_array_ops.TensorArray(\n-            dtype=inputs.dtype,\n-            size=time_steps,\n-            tensor_array_name='input_ta')\n-        input_ta = input_ta.unstack(inputs)\n-        time = tf.constant(0, dtype='int32', name='time')\n-        while_loop_kwargs = {\n-            'cond': lambda time, *_: time < time_steps,\n-            'parallel_iterations': 32,\n-            'swap_memory': True,\n-            'maximum_iterations': input_length}\n-\n-        if mask is not None:\n-            if go_backwards:\n-                mask = reverse(mask, 0)\n-\n-            mask_ta = tensor_array_ops.TensorArray(\n-                dtype=tf.bool,\n-                size=time_steps,\n-                tensor_array_name='mask_ta')\n-            mask_ta = mask_ta.unstack(mask)\n-\n-            def _step(time, output_ta_t, output_tm1, *states):\n-                \"\"\"RNN step function.\n-                # Arguments\n-                    time: Current timestep value.\n-                    output_ta_t: TensorArray.\n-                    output_tm1: output Tensor from previous timestep\n-                    *states: List of states.\n-                # Returns\n-                    Tuple: `(time + 1,output_ta_t) + tuple(new_states)`\n-                \"\"\"\n-                current_input = input_ta.read(time)\n-                mask_t = mask_ta.read(time)\n-                output, new_states = step_function(current_input,\n-                                                   tuple(states) +\n-                                                   tuple(constants))\n-                if getattr(output, '_uses_learning_phase', False):\n-                    uses_learning_phase[0] = True\n-                for state, new_state in zip(states, new_states):\n-                    new_state.set_shape(state.shape)\n-\n-                output_mask_t = get_matching_mask(mask_t, output)\n-                output = tf.where(output_mask_t, output, output_tm1)\n-\n-                new_states = [tf.where(get_matching_mask(mask_t, new_states[i]),\n-                                       new_states[i],\n-                                       states[i]) for i in range(len(states))]\n-\n-                output_ta_t = output_ta_t.write(time, output)\n-                return (time + 1, output_ta_t, output) + tuple(new_states)\n-\n-            final_outputs = control_flow_ops.while_loop(\n-                body=_step,\n-                loop_vars=(time, output_ta, initial_output) + states,\n-                **while_loop_kwargs)\n-            new_states = final_outputs[3:]  # skip output_tm1\n-        else:\n-            def _step(time, output_ta_t, *states):\n-                \"\"\"RNN step function.\n-                # Arguments\n-                    time: Current timestep value.\n-                    output_ta_t: TensorArray.\n-                    *states: List of states.\n-                # Returns\n-                    Tuple: `(time + 1,output_ta_t) + tuple(new_states)`\n-                \"\"\"\n-                current_input = input_ta.read(time)\n-                output, new_states = step_function(current_input,\n-                                                   tuple(states) +\n-                                                   tuple(constants))\n-                if getattr(output, '_uses_learning_phase', False):\n-                    uses_learning_phase[0] = True\n-                for state, new_state in zip(states, new_states):\n-                    new_state.set_shape(state.shape)\n-                output_ta_t = output_ta_t.write(time, output)\n-                return (time + 1, output_ta_t) + tuple(new_states)\n-\n-            final_outputs = control_flow_ops.while_loop(\n-                body=_step,\n-                loop_vars=(time, output_ta) + states,\n-                **while_loop_kwargs)\n-            new_states = final_outputs[2:]\n-\n-        last_time = final_outputs[0]\n-        output_ta = final_outputs[1]\n-        outputs = output_ta.stack()\n-        last_output = output_ta.read(last_time - 1)\n-\n-    axes = [1, 0] + list(range(2, len(outputs.shape)))\n-    outputs = tf.transpose(outputs, axes)\n-    last_output._uses_learning_phase = uses_learning_phase[0]\n+    last_output, outputs, new_states = tf_keras_backend.rnn(\n+        step_function, inputs, initial_states,\n+        go_backwards=go_backwards,\n+        mask=mask,\n+        constants=constants,\n+        unroll=unroll,\n+        input_length=input_length)\n+    reachable = tf_utils.get_reachable_from_inputs([learning_phase()],\n+                                                   targets=[last_output])\n+    print('learning_phase:', learning_phase())\n+    print('reachable:', reachable)\n+    print('last_output:', last_output)\n+    if last_output in reachable:\n+        last_output._uses_learning_phase = True\n     return last_output, outputs, new_states\n \n \n@@ -3147,7 +2930,6 @@ def in_test_phase(x, alt, training=None):\n \n # NN OPERATIONS\n \n-@symbolic\n def relu(x, alpha=0., max_value=None, threshold=0.):\n     \"\"\"Rectified linear unit.\n \n@@ -3202,7 +2984,6 @@ def relu(x, alpha=0., max_value=None, threshold=0.):\n     return x\n \n \n-@symbolic\n def elu(x, alpha=1.):\n     \"\"\"Exponential linear unit.\n \n@@ -3222,7 +3003,6 @@ def elu(x, alpha=1.):\n         return tf.where(x > 0, res, alpha * res)\n \n \n-@symbolic\n def softmax(x, axis=-1):\n     \"\"\"Softmax of a tensor.\n \n@@ -3239,7 +3019,6 @@ def softmax(x, axis=-1):\n     return tf.nn.softmax(x, axis=axis)\n \n \n-@symbolic\n def softplus(x):\n     \"\"\"Softplus of a tensor.\n \n@@ -3254,7 +3033,6 @@ def softplus(x):\n     return tf.nn.softplus(x)\n \n \n-@symbolic\n def softsign(x):\n     \"\"\"Softsign of a tensor.\n \n@@ -3269,7 +3047,6 @@ def softsign(x):\n     return tf.nn.softsign(x)\n \n \n-@symbolic\n def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n     \"\"\"Categorical crossentropy between an output tensor and a target tensor.\n \n@@ -3296,7 +3073,6 @@ def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n         target, output, from_logits=from_logits, axis=axis)\n \n \n-@symbolic\n def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n     \"\"\"Categorical crossentropy with integer targets.\n \n@@ -3319,12 +3095,10 @@ def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n         ValueError: if `axis` is neither -1 nor one of\n             the axes of `output`.\n     \"\"\"\n-    print('context:', context.executing_eagerly())\n     return tf_keras_backend.sparse_categorical_crossentropy(\n         target, output, from_logits=from_logits, axis=axis)\n \n \n-@symbolic\n def binary_crossentropy(target, output, from_logits=False):\n     \"\"\"Binary crossentropy between an output tensor and a target tensor.\n \n@@ -3342,7 +3116,6 @@ def binary_crossentropy(target, output, from_logits=False):\n         target, output, from_logits=from_logits)\n \n \n-@symbolic\n def sigmoid(x):\n     \"\"\"Element-wise sigmoid.\n \n@@ -3357,7 +3130,6 @@ def sigmoid(x):\n     return tf.nn.sigmoid(x)\n \n \n-@symbolic\n def hard_sigmoid(x):\n     \"\"\"Segment-wise linear approximation of sigmoid.\n \n@@ -3376,7 +3148,6 @@ def hard_sigmoid(x):\n     return tf_keras_backend.hard_sigmoid(x)\n \n \n-@symbolic\n def tanh(x):\n     \"\"\"Element-wise tanh.\n \n@@ -3391,7 +3162,6 @@ def tanh(x):\n     return tf.nn.tanh(x)\n \n \n-@symbolic\n def dropout(x, level, noise_shape=None, seed=None):\n     \"\"\"Sets entries in `x` to zero at random, while scaling the entire tensor.\n \n@@ -3412,7 +3182,6 @@ def dropout(x, level, noise_shape=None, seed=None):\n     return tf.nn.dropout(x, rate=level, noise_shape=noise_shape, seed=seed)\n \n \n-@symbolic\n def l2_normalize(x, axis=None):\n     \"\"\"Normalizes a tensor wrt the L2 norm alongside the specified axis.\n \n@@ -3428,7 +3197,6 @@ def l2_normalize(x, axis=None):\n     return tf.nn.l2_normalize(x, axis=axis)\n \n \n-@symbolic\n def in_top_k(predictions, targets, k):\n     \"\"\"Returns whether the `targets` are in the top `k` `predictions`.\n \n@@ -3540,7 +3308,6 @@ def _preprocess_padding(padding):\n     return padding\n \n \n-@symbolic\n def conv1d(x, kernel, strides=1, padding='valid',\n            data_format=None, dilation_rate=1):\n     \"\"\"1D convolution.\n@@ -3594,7 +3361,6 @@ def conv1d(x, kernel, strides=1, padding='valid',\n     return x\n \n \n-@symbolic\n def conv2d(x, kernel, strides=(1, 1), padding='valid',\n            data_format=None, dilation_rate=(1, 1)):\n     \"\"\"2D convolution.\n@@ -3641,7 +3407,6 @@ def conv2d(x, kernel, strides=(1, 1), padding='valid',\n     return x\n \n \n-@symbolic\n def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n                      padding='valid', data_format=None, dilation_rate=(1, 1)):\n     \"\"\"2D deconvolution (i.e. transposed convolution).\n@@ -3705,7 +3470,6 @@ def conv2d_transpose(x, kernel, output_shape, strides=(1, 1),\n     return x\n \n \n-@symbolic\n def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n                      padding='valid', data_format=None, dilation_rate=1):\n     \"\"\"1D convolution with separable filters.\n@@ -3770,7 +3534,6 @@ def separable_conv1d(x, depthwise_kernel, pointwise_kernel, strides=1,\n     return x\n \n \n-@symbolic\n def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n                      padding='valid', data_format=None, dilation_rate=(1, 1)):\n     \"\"\"2D convolution with separable filters.\n@@ -3818,7 +3581,6 @@ def separable_conv2d(x, depthwise_kernel, pointwise_kernel, strides=(1, 1),\n     return x\n \n \n-@symbolic\n def depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n                      data_format=None, dilation_rate=(1, 1)):\n     \"\"\"2D convolution with separable filters.\n@@ -3865,7 +3627,6 @@ def depthwise_conv2d(x, depthwise_kernel, strides=(1, 1), padding='valid',\n     return x\n \n \n-@symbolic\n def conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n            data_format=None, dilation_rate=(1, 1, 1)):\n     \"\"\"3D convolution.\n@@ -3910,7 +3671,6 @@ def conv3d(x, kernel, strides=(1, 1, 1), padding='valid',\n     return x\n \n \n-@symbolic\n def conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n                      padding='valid', data_format=None):\n     \"\"\"3D deconvolution (i.e. transposed convolution).\n@@ -3962,7 +3722,6 @@ def conv3d_transpose(x, kernel, output_shape, strides=(1, 1, 1),\n     return x\n \n \n-@symbolic\n def pool2d(x, pool_size, strides=(1, 1),\n            padding='valid', data_format=None,\n            pool_mode='max'):\n@@ -4011,7 +3770,6 @@ def pool2d(x, pool_size, strides=(1, 1),\n     return x\n \n \n-@symbolic\n def pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n            data_format=None, pool_mode='max'):\n     \"\"\"3D Pooling.\n@@ -4059,7 +3817,6 @@ def pool3d(x, pool_size, strides=(1, 1, 1), padding='valid',\n     return x\n \n \n-@symbolic\n def local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n     \"\"\"Apply 1D conv with un-shared weights.\n \n@@ -4099,7 +3856,6 @@ def local_conv1d(inputs, kernel, kernel_size, strides, data_format=None):\n     return permute_dimensions(output, (1, 0, 2))\n \n \n-@symbolic\n def local_conv2d(inputs,\n                  kernel,\n                  kernel_size,\n@@ -4169,7 +3925,6 @@ def local_conv2d(inputs,\n     return output\n \n \n-@symbolic\n def bias_add(x, bias, data_format=None):\n     \"\"\"Adds a bias vector to a tensor.\n \n@@ -4253,6 +4008,10 @@ def random_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n         dtype = floatx()\n     if seed is None:\n         seed = np.random.randint(10e6)\n+    if py_any(list(is_symbolic(x) for x in (shape, mean, stddev))):\n+        with get_graph().as_default():\n+            return tf_keras_backend.random_normal(\n+                shape, mean=mean, stddev=stddev, dtype=dtype, seed=seed)\n     with tf_ops.init_scope():\n         return tf_keras_backend.random_normal(\n             shape, mean=mean, stddev=stddev, dtype=dtype, seed=seed)\n@@ -4277,6 +4036,10 @@ def random_uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n         dtype = floatx()\n     if seed is None:\n         seed = np.random.randint(10e6)\n+    if py_any(list(is_symbolic(x) for x in (shape, minval, maxval))):\n+        with get_graph().as_default():\n+            return tf_keras_backend.random_uniform(\n+                shape, minval=minval, maxval=maxval, dtype=dtype, seed=seed)\n     with tf_ops.init_scope():\n         return tf_keras_backend.random_uniform(\n             shape, minval=minval, maxval=maxval, dtype=dtype, seed=seed)\n@@ -4298,6 +4061,10 @@ def random_binomial(shape, p=0.0, dtype=None, seed=None):\n         dtype = floatx()\n     if seed is None:\n         seed = np.random.randint(10e6)\n+    if py_any(list(is_symbolic(x) for x in (shape, p))):\n+        with get_graph().as_default():\n+            return tf_keras_backend.random_binomial(\n+                shape, p=p, dtype=dtype, seed=seed)\n     with tf_ops.init_scope():\n         return tf_keras_backend.random_binomial(\n             shape, p=p, dtype=dtype, seed=seed)\n@@ -4325,6 +4092,10 @@ def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n         dtype = floatx()\n     if seed is None:\n         seed = np.random.randint(10e6)\n+    if py_any(list(is_symbolic(x) for x in (shape, mean, stddev))):\n+        with get_graph().as_default():\n+            return tf_keras_backend.truncated_normal(\n+                shape, mean=mean, stddev=stddev, dtype=dtype, seed=seed)\n     with tf_ops.init_scope():\n         return tf_keras_backend.truncated_normal(\n             shape, mean=mean, stddev=stddev, dtype=dtype, seed=seed)\n@@ -4337,7 +4108,6 @@ def truncated_normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n # in TensorFlow's CTC implementation\n \n \n-@symbolic\n def ctc_label_dense_to_sparse(labels, label_lengths):\n     \"\"\"Converts CTC labels from dense to sparse.\n \n@@ -4379,7 +4149,6 @@ def ctc_label_dense_to_sparse(labels, label_lengths):\n     return tf.SparseTensor(indices, vals_sparse, label_shape)\n \n \n-@symbolic\n def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n     \"\"\"Runs CTC loss algorithm on each batch element.\n \n@@ -4407,7 +4176,6 @@ def ctc_batch_cost(y_true, y_pred, input_length, label_length):\n                                        sequence_length=input_length), 1)\n \n \n-@symbolic\n def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n                top_paths=1, merge_repeated=False):\n     \"\"\"Decodes the output of a softmax.\n@@ -4461,7 +4229,6 @@ def ctc_decode(y_pred, input_length, greedy=True, beam_width=100,\n \n # HIGH ORDER FUNCTIONS\n \n-@symbolic\n def map_fn(fn, elems, name=None, dtype=None):\n     \"\"\"Map the function fn over the elements elems and return the outputs.\n \n@@ -4477,7 +4244,6 @@ def map_fn(fn, elems, name=None, dtype=None):\n     return tf.map_fn(fn, elems, name=name, dtype=dtype)\n \n \n-@symbolic\n def foldl(fn, elems, initializer=None, name=None):\n     \"\"\"Reduce elems using fn to combine them from left to right.\n \n@@ -4494,7 +4260,6 @@ def foldl(fn, elems, initializer=None, name=None):\n     return tf.foldl(fn, elems, initializer=initializer, name=name)\n \n \n-@symbolic\n def foldr(fn, elems, initializer=None, name=None):\n     \"\"\"Reduce elems using fn to combine them from right to left.\n \n\n@@ -34,6 +34,7 @@ class Model(Network):\n     \"\"\"The `Model` class adds training & evaluation routines to a `Network`.\n     \"\"\"\n \n+    @K.symbolic\n     def compile(self, optimizer,\n                 loss=None,\n                 metrics=None,\n\n@@ -597,7 +597,6 @@ class ConvLSTM2DCell(Layer):\n         self._recurrent_dropout_mask = None\n \n     def build(self, input_shape):\n-\n         if self.data_format == 'channels_first':\n             channel_axis = 1\n         else:\n@@ -623,6 +622,7 @@ class ConvLSTM2DCell(Layer):\n             constraint=self.recurrent_constraint)\n         if self.use_bias:\n             if self.unit_forget_bias:\n+                @K.eager\n                 def bias_initializer(_, *args, **kwargs):\n                     return K.concatenate([\n                         self.bias_initializer((self.filters,), *args, **kwargs),\n\n@@ -18,7 +18,6 @@ from keras.utils.generic_utils import slice_arrays\n from keras.models import Sequential\n from keras import backend as K\n from keras.utils import Sequence\n-from keras.callbacks import LambdaCallback\n from keras.callbacks import Callback\n \n \n\n@@ -21,7 +21,11 @@ sequence_len = 2\n @pytest.mark.parametrize('data_format', ['channels_first', 'channels_last'])\n @pytest.mark.parametrize('return_sequences', [True, False])\n @pytest.mark.parametrize('use_mask', [True, False])\n-def test_convolutional_recurrent(data_format, return_sequences, use_mask):\n+@pytest.mark.parametrize('use_bias', [True, False])\n+def test_convolutional_recurrent(data_format='channels_last',\n+                                 return_sequences=False,\n+                                 use_mask=False,\n+                                 use_bias=True):\n \n     class Masking5D(Masking):\n         \"\"\"Regular masking layer returns wrong shape of mask for RNN\"\"\"\n@@ -45,7 +49,8 @@ def test_convolutional_recurrent(data_format, return_sequences, use_mask):\n               'stateful': True,\n               'filters': filters,\n               'kernel_size': (num_row, num_col),\n-              'padding': 'valid'}\n+              'padding': 'valid',\n+              'use_bias': use_bias}\n     layer = convolutional_recurrent.ConvLSTM2D(**kwargs)\n     layer.build(inputs.shape)\n     if use_mask:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
