{"custom_id": "scrapy#6d989e3fb01c8a184b84d34a3c083ed40006b0ae", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 45 | Lines Deleted: 75 | Files Changed: 4 | Hunks: 32 | Methods Changed: 31 | Complexity Δ (Sum/Max): -10/0 | Churn Δ: 120 | Churn Cumulative: 2353 | Contributors (this commit): 5 | Commits (past 90d): 36 | Contributors (cumulative): 9 | DMM Complexity: 0.0625\n\nDIFF:\n@@ -1,14 +1,8 @@\n \"\"\"\n Dupe Filter classes implement a mechanism for filtering duplicate requests.\n-They must implement the following methods:\n+They must implement the following method:\n \n-* open_spider(spider)\n-  open a spider for tracking duplicates (typically used to reserve resources)\n-\n-* close_spider(spider)\n-  close a spider (typically used for freeing resources)\n-\n-* request_seen(spider, request, dont_record=False)\n+* request_seen(request, dont_record=False)\n   return ``True`` if the request was seen before, or ``False`` otherwise. If\n   ``dont_record`` is ``True`` the request must not be recorded as seen.\n \n@@ -17,33 +11,27 @@ They must implement the following methods:\n from scrapy.utils.request import request_fingerprint\n \n \n-class NullDupeFilter(dict):\n-    def open_spider(self, spider):\n-        pass\n+class BaseDupeFilter(object):\n \n-    def close_spider(self, spider):\n-        pass\n+    @classmethod\n+    def from_settings(cls, settings):\n+        return cls()\n \n-    def request_seen(self, spider, request, dont_record=False):\n+    def request_seen(self, request, dont_record=False):\n         return False\n \n \n-class RequestFingerprintDupeFilter(object):\n+class RequestFingerprintDupeFilter(BaseDupeFilter):\n     \"\"\"Duplicate filter using scrapy.utils.request.request_fingerprint\"\"\"\n \n     def __init__(self):\n-        self.fingerprints = {}\n+        super(RequestFingerprintDupeFilter, self).__init__()\n+        self.fingerprints = set()\n \n-    def open_spider(self, spider):\n-        self.fingerprints[spider] = set()\n-\n-    def close_spider(self, spider):\n-        del self.fingerprints[spider]\n-\n-    def request_seen(self, spider, request, dont_record=False):\n+    def request_seen(self, request, dont_record=False):\n         fp = request_fingerprint(request)\n-        if fp in self.fingerprints[spider]:\n+        if fp in self.fingerprints:\n             return True\n         if not dont_record:\n-            self.fingerprints[spider].add(fp)\n+            self.fingerprints.add(fp)\n         return False\n\n@@ -17,18 +17,18 @@ from scrapy.exceptions import DontCloseSpider\n from scrapy.http import Response, Request\n from scrapy.utils.misc import load_object\n from scrapy.utils.signal import send_catch_log, send_catch_log_deferred\n-from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.reactor import CallLaterOnce\n \n \n class Slot(object):\n \n-    def __init__(self, start_requests, close_if_idle, nextcall):\n+    def __init__(self, start_requests, close_if_idle, nextcall, scheduler):\n         self.closing = False\n         self.inprogress = set() # requests in progress\n         self.start_requests = iter(start_requests)\n         self.close_if_idle = close_if_idle\n         self.nextcall = nextcall\n+        self.scheduler = scheduler\n \n     def add_request(self, request):\n         self.inprogress.add(request)\n@@ -56,7 +56,7 @@ class ExecutionEngine(object):\n         self.slots = {}\n         self.running = False\n         self.paused = False\n-        self.scheduler = load_object(settings['SCHEDULER'])()\n+        self.scheduler_cls = load_object(settings['SCHEDULER'])\n         self.downloader = Downloader(self.settings)\n         self.scraper = Scraper(self, self.settings)\n         self._concurrent_spiders = settings.getint('CONCURRENT_SPIDERS')\n@@ -85,10 +85,6 @@ class ExecutionEngine(object):\n         \"\"\"Resume the execution engine\"\"\"\n         self.paused = False\n \n-    def is_idle(self):\n-        return self.scheduler.is_idle() and self.downloader.is_idle() and \\\n-            self.scraper.is_idle()\n-\n     def _next_request(self, spider):\n         try:\n             slot = self.slots[spider]\n@@ -121,13 +117,13 @@ class ExecutionEngine(object):\n             or self.scraper.slots[spider].needs_backout()\n \n     def _next_request_from_scheduler(self, spider):\n-        request = self.scheduler.next_request(spider)\n+        slot = self.slots[spider]\n+        request = slot.scheduler.next_request()\n         if not request:\n             return\n         d = self._download(request, spider)\n         d.addBoth(self._handle_downloader_output, request, spider)\n         d.addErrback(log.msg, spider=spider)\n-        slot = self.slots[spider]\n         d.addBoth(lambda _: slot.remove_request(request))\n         d.addErrback(log.msg, spider=spider)\n         d.addBoth(lambda _: slot.nextcall.schedule())\n@@ -148,7 +144,7 @@ class ExecutionEngine(object):\n     def spider_is_idle(self, spider):\n         scraper_idle = spider in self.scraper.slots \\\n             and self.scraper.slots[spider].is_idle()\n-        pending = self.scheduler.spider_has_pending_requests(spider)\n+        pending = self.slots[spider].scheduler.has_pending_requests()\n         downloading = bool(self.downloader.slots)\n         idle = scraper_idle and not (pending or downloading)\n         return idle\n@@ -168,7 +164,7 @@ class ExecutionEngine(object):\n         self.slots[spider].nextcall.schedule()\n \n     def schedule(self, request, spider):\n-        return self.scheduler.enqueue_request(spider, request)\n+        return self.slots[spider].scheduler.enqueue_request(request)\n \n     def download(self, request, spider):\n         slot = self.slots[spider]\n@@ -210,9 +206,10 @@ class ExecutionEngine(object):\n             spider.name\n         log.msg(\"Spider opened\", spider=spider)\n         nextcall = CallLaterOnce(self._next_request, spider)\n-        slot = Slot(start_requests or (), close_if_idle, nextcall)\n+        scheduler = self.scheduler_cls.from_settings(self.settings)\n+        slot = Slot(start_requests or (), close_if_idle, nextcall, scheduler)\n         self.slots[spider] = slot\n-        yield self.scheduler.open_spider(spider)\n+        yield scheduler.open(spider)\n         yield self.scraper.open_spider(spider)\n         stats.open_spider(spider)\n         yield send_catch_log_deferred(signals.spider_opened, spider=spider)\n@@ -244,14 +241,12 @@ class ExecutionEngine(object):\n             return slot.closing\n         log.msg(\"Closing spider (%s)\" % reason, spider=spider)\n \n-        self.scheduler.clear_pending_requests(spider)\n-\n         dfd = slot.close()\n \n         dfd.addBoth(lambda _: self.scraper.close_spider(spider))\n         dfd.addErrback(log.err, spider=spider)\n \n-        dfd.addBoth(lambda _: self.scheduler.close_spider(spider))\n+        dfd.addBoth(lambda _: slot.scheduler.close(reason))\n         dfd.addErrback(log.err, spider=spider)\n \n         dfd.addBoth(lambda _: send_catch_log_deferred(signal=signals.spider_closed, \\\n\n@@ -1,45 +1,33 @@\n from scrapy.utils.datatypes import PriorityQueue, PriorityStack\n from scrapy.utils.misc import load_object\n-from scrapy.conf import settings\n \n class Scheduler(object):\n \n-    def __init__(self):\n-        self.pending_requests = {}\n-        self.dfo = settings['SCHEDULER_ORDER'].upper() == 'DFO'\n-        self.dupefilter = load_object(settings['DUPEFILTER_CLASS'])()\n+    def __init__(self, dupefilter, dfo=False):\n+        self.dupefilter = dupefilter\n+        Queue = PriorityStack if dfo else PriorityQueue\n+        self.pending_requests = Queue()\n \n-    def spider_has_pending_requests(self, spider):\n-        if spider in self.pending_requests:\n-            return bool(self.pending_requests[spider])\n+    @classmethod\n+    def from_settings(cls, settings):\n+        dfo = settings['SCHEDULER_ORDER'].upper() == 'DFO'\n+        dupefilter_cls = load_object(settings['DUPEFILTER_CLASS'])\n+        dupefilter = dupefilter_cls.from_settings(settings)\n+        return cls(dupefilter, dfo=dfo)\n \n-    def open_spider(self, spider):\n-        if spider in self.pending_requests:\n-            raise RuntimeError('Scheduler spider already opened: %s' % spider)\n+    def has_pending_requests(self):\n+        return bool(self.pending_requests)\n \n-        Priority = PriorityStack if self.dfo else PriorityQueue\n-        self.pending_requests[spider] = Priority()\n-        return self.dupefilter.open_spider(spider)\n+    def enqueue_request(self, request):\n+        if request.dont_filter or not self.dupefilter.request_seen(request):\n+            self.pending_requests.push(request, -request.priority)\n \n-    def close_spider(self, spider):\n-        if spider not in self.pending_requests:\n-            raise RuntimeError('Scheduler spider is not open: %s' % spider)\n-        self.pending_requests.pop(spider, None)\n-        return self.dupefilter.close_spider(spider)\n+    def next_request(self):\n+        if self.pending_requests:\n+            return self.pending_requests.pop()[0]\n \n-    def enqueue_request(self, spider, request):\n-        if request.dont_filter or not self.dupefilter.request_seen(spider, request):\n-            self.pending_requests[spider].push(request, -request.priority)\n-\n-    def clear_pending_requests(self, spider):\n-        # TODO: flush queue here or discard enqueued requests, depending on how\n-        # the spider is being closed.\n+    def open(self, spider):\n         pass\n \n-    def next_request(self, spider):\n-        q = self.pending_requests[spider]\n-        if q:\n-            return q.pop()[0]\n-\n-    def is_idle(self):\n-        return not self.pending_requests\n+    def close(self, reason):\n+        pass\n\n@@ -11,7 +11,6 @@ def get_engine_status(engine=None):\n \n     global_tests = [\n         \"time()-engine.start_time\",\n-        \"engine.is_idle()\",\n         \"engine.has_capacity()\",\n         \"engine.scheduler.is_idle()\",\n         \"len(engine.scheduler.pending_requests)\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c6f29f02c4d5da5ec989d3034597b9133fe04aad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 1909 | Contributors (this commit): 5 | Commits (past 90d): 20 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -191,12 +191,16 @@ class ExecutionEngine(object):\n                     response=response, request=request, spider=spider)\n             return response\n \n+        def _on_error(failure):\n+            failure.request = request\n+            return failure\n+\n         def _on_complete(_):\n             slot.nextcall.schedule()\n             return _\n \n         dwld = self.downloader.fetch(request, spider)\n-        dwld.addCallback(_on_success)\n+        dwld.addCallbacks(_on_success, _on_error)\n         dwld.addBoth(_on_complete)\n         return dwld\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fbf0e9ef438306cb44ab32d01295b486956cc679", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 36 | Lines Deleted: 111 | Files Changed: 2 | Hunks: 2 | Methods Changed: 13 | Complexity Δ (Sum/Max): -21/10 | Churn Δ: 147 | Churn Cumulative: 272 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -0,0 +1,36 @@\n+\"\"\"\n+Link extractor based on lxml.html\n+\"\"\"\n+\n+import lxml.html\n+\n+from scrapy.link import Link\n+from scrapy.utils.python import unique as unique_list\n+\n+class LxmlParserLinkExtractor(object):\n+    def __init__(self, tag=\"a\", attr=\"href\", process=None, unique=False):\n+        self.scan_tag = tag if callable(tag) else lambda t: t == tag\n+        self.scan_attr = attr if callable(attr) else lambda a: a == attr\n+        self.process_attr = process if callable(process) else lambda v: v\n+        self.unique = unique\n+\n+        self.links = []\n+\n+    def _extract_links(self, response_text, response_url):\n+        html = lxml.html.fromstring(response_text)\n+        html.make_links_absolute(response_url)\n+        for e, a, l, p in html.iterlinks():\n+            if self.scan_tag(e.tag):\n+                if self.scan_attr(a):\n+                    link = Link(self.process_attr(l), text=e.text)\n+                    self.links.append(link)\n+\n+        links = unique_list(self.links, key=lambda link: link.url) \\\n+                if self.unique else self.links\n+\n+        return links\n+\n+    def extract_links(self, response):\n+        return self._extract_links(response.body, response.url)\n+\n+\n\n@@ -1,111 +0,0 @@\n-\"\"\"\n-lxml-based link extractors (experimental)\n-\n-NOTE: The ideal name for this module would be `lxml`, but that's not possible\n-because it collides with the lxml library module.\n-\"\"\"\n-\n-from lxml import etree\n-import lxml.html\n-from w3lib.url import safe_url_string, urljoin_rfc\n-\n-from scrapy.link import Link\n-from scrapy.utils.python import unique as unique_list, str_to_unicode\n-\n-class LxmlLinkExtractor(object):\n-    def __init__(self, tag=\"a\", attr=\"href\", process=None, unique=False):\n-        scan_tag = tag if callable(tag) else lambda t: t == tag\n-        scan_attr = attr if callable(attr) else lambda a: a == attr\n-        process_attr = process if callable(process) else lambda v: v\n-\n-        self.unique = unique\n-\n-        target = LinkTarget(scan_tag, scan_attr, process_attr)\n-        self.parser = etree.HTMLParser(target=target)\n-\n-    def _extract_links(self, response_text, response_url, response_encoding):\n-        self.base_url, self.links = etree.HTML(response_text, self.parser) \n-\n-        links = unique_list(self.links, key=lambda link: link.url) if self.unique else self.links\n-\n-        ret = []\n-        base_url = urljoin_rfc(response_url, self.base_url) if self.base_url else response_url\n-        for link in links:\n-            link.url = urljoin_rfc(base_url, link.url, response_encoding)\n-            link.url = safe_url_string(link.url, response_encoding)\n-            link.text = str_to_unicode(link.text, response_encoding, errors='replace')\n-            ret.append(link)\n-\n-        return ret\n-\n-    def extract_links(self, response):\n-        # wrapper needed to allow to work directly with text\n-        return self._extract_links(response.body, response.url, \n-                                   response.encoding)\n-\n-    def matches(self, url):\n-        \"\"\"This extractor matches with any url, since it doesn't contain any patterns\"\"\"\n-        return True\n-\n-\n-class LinkTarget(object):\n-    def __init__(self, scan_tag, scan_attr, process_attr):\n-        self.scan_tag = scan_tag\n-        self.scan_attr = scan_attr\n-        self.process_attr = process_attr\n-\n-        self.base_url = None\n-        self.links = []\n-\n-        self.current_link = None\n-\n-    def start(self, tag, attrs):\n-        if tag == 'base':\n-            self.base_url = dict(attrs).get('href')\n-        if self.scan_tag(tag):\n-            for attr, value in attrs.iteritems():\n-                if self.scan_attr(attr):\n-                    url = self.process_attr(value)\n-                    link = Link(url=url)\n-                    self.links.append(link)\n-                    self.current_link = link\n-\n-    def end(self, tag):\n-        self.current_link = None\n-\n-    def data(self, data):\n-        if self.current_link and not self.current_link.text:\n-            self.current_link.text = data.strip()\n-\n-    def close(self):\n-        return self.base_url, self.links\n-\n-\n-class LxmlParserLinkExtractor(object):\n-    def __init__(self, tag=\"a\", attr=\"href\", process=None, unique=False):\n-        self.scan_tag = tag if callable(tag) else lambda t: t == tag\n-        self.scan_attr = attr if callable(attr) else lambda a: a == attr\n-        self.process_attr = process if callable(process) else lambda v: v\n-        self.unique = unique\n-\n-        self.links = []\n-\n-    def _extract_links(self, response_text, response_url):\n-        html = lxml.html.fromstring(response_text)\n-        html.make_links_absolute(response_url)\n-        for e, a, l, p in html.iterlinks():\n-            if self.scan_tag(e.tag):\n-                if self.scan_attr(a):\n-                    link = Link(self.process_attr(l), text=e.text)\n-                    self.links.append(link)\n-\n-        links = unique_list(self.links, key=lambda link: link.url) \\\n-                if self.unique else self.links\n-\n-        return links\n-\n-    def extract_links(self, response):\n-        # wrapper needed to allow to work directly with text\n-        return self._extract_links(response.body, response.url)\n-\n-\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f8c9b40345bd61a2dab2445f7eead17587e5d69d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 232 | Contributors (this commit): 2 | Commits (past 90d): 9 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -50,6 +50,9 @@ class Scheduler(object):\n     def next_request(self):\n         return self.mqs.pop() or self._dqpop()\n \n+    def __len__(self):\n+        return len(self.dqs) + len(self.mqs)\n+\n     def _dqpush(self, request):\n         if self.dqs is None:\n             return\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#888dac867ff13a71478c3c27566592bb09c547b2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 234 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -51,7 +51,7 @@ class Scheduler(object):\n         return self.mqs.pop() or self._dqpop()\n \n     def __len__(self):\n-        return len(self.dqs) + len(self.mqs)\n+        return len(self.dqs) + len(self.mqs) if self.dqs else len(self.mqs)\n \n     def _dqpush(self, request):\n         if self.dqs is None:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3191a0856003e8298d9d4b74aec56ffb5ed38432", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 30 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -11,6 +11,9 @@ def start_python_console(namespace=None, noipython=False):\n             if noipython:\n                 raise ImportError\n             import IPython\n+            try:\n+                shell = IPython.embed(user_ns=namespace)\n+            except AttributeError:\n                 shell = IPython.Shell.IPShellEmbed(argv=[], user_ns=namespace)\n             shell()\n         except ImportError:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cb95d7a5af416208ba94948ae6d982658f16e85a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 14 | Churn Cumulative: 448 | Contributors (this commit): 5 | Commits (past 90d): 17 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,6 +4,7 @@ Item Exporters are used to export/serialize items into different formats.\n \n import csv\n import pprint\n+import marshal\n from cPickle import Pickler\n from xml.sax.saxutils import XMLGenerator\n \n@@ -12,7 +13,7 @@ from scrapy.utils.py26 import json\n \n __all__ = ['BaseItemExporter', 'PprintItemExporter', 'PickleItemExporter', \\\n     'CsvItemExporter', 'XmlItemExporter', 'JsonLinesItemExporter', \\\n-    'JsonItemExporter']\n+    'JsonItemExporter', 'MarshalItemExporter']\n \n class BaseItemExporter(object):\n \n@@ -185,6 +186,16 @@ class PickleItemExporter(BaseItemExporter):\n         self.pickler.dump(dict(self._get_serialized_fields(item)))\n \n \n+class MarshalItemExporter(BaseItemExporter):\n+\n+    def __init__(self, file, **kwargs):\n+        self._configure(kwargs)\n+        self.file = file\n+\n+    def export_item(self, item):\n+        marshal.dump(dict(self._get_serialized_fields(item)), self.file)\n+\n+\n class PprintItemExporter(BaseItemExporter):\n \n     def __init__(self, file, **kwargs):\n\n@@ -162,6 +162,7 @@ FEED_EXPORTERS_BASE = {\n     'jsonlines': 'scrapy.contrib.exporter.JsonLinesItemExporter',\n     'csv': 'scrapy.contrib.exporter.CsvItemExporter',\n     'xml': 'scrapy.contrib.exporter.XmlItemExporter',\n+    'marshal': 'scrapy.contrib.exporter.MarshalItemExporter',\n }\n \n HTTPCACHE_ENABLED = False\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cd8470b3092a4d84a883edf1b7e1221a1e6b3104", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 125 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -53,7 +53,7 @@ class CrawlSpider(BaseSpider):\n                 links = rule.process_links(links)\n             seen = seen.union(links)\n             for link in links:\n-                r = Request(url=link.url, callback='_response_downloaded')\n+                r = Request(url=link.url, callback=self._response_downloaded)\n                 r.meta.update(rule=n, link_text=link.text)\n                 yield rule.process_request(r)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ebb892e554d299687cf9df4fd3af781c7b8a6a4c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 206 | Contributors (this commit): 1 | Commits (past 90d): 11 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -24,8 +24,8 @@ def get_engine_status(engine=None):\n         \"engine.spider_is_idle(spider)\",\n         \"engine.slots[spider].closing\",\n         \"len(engine.slots[spider].inprogress)\",\n-        \"len(engine.slots[spider].scheduler.dq)\",\n-        \"len(engine.slots[spider].scheduler.mq)\",\n+        \"len(engine.slots[spider].scheduler.dqs or [])\",\n+        \"len(engine.slots[spider].scheduler.mqs)\",\n         \"len(engine.scraper.slots[spider].queue)\",\n         \"len(engine.scraper.slots[spider].active)\",\n         \"engine.scraper.slots[spider].active_size\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#76cbb6a2e6246852090d0a4d448c138ccfcecebd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/1 | Churn Δ: 18 | Churn Cumulative: 589 | Contributors (this commit): 2 | Commits (past 90d): 14 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,4 +1,3 @@\n-import socket\n import random\n import warnings\n from time import time\n@@ -8,12 +7,10 @@ from functools import partial\n from twisted.internet import reactor, defer\n from twisted.python.failure import Failure\n \n-from scrapy.utils.python import setattr_default\n from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.signal import send_catch_log\n-from scrapy.utils.reactor import CallLaterOnce\n from scrapy.utils.httpobj import urlparse_cached\n-from scrapy.resolver import gethostbyname\n+from scrapy.resolver import dnscache\n from scrapy import signals\n from scrapy import log\n from .middleware import DownloaderMiddlewareManager\n@@ -95,14 +92,12 @@ class Downloader(object):\n         return len(self.active) >= self.total_concurrency\n \n     def _get_slot(self, request, spider):\n-        key = urlparse_cached(request).hostname\n+        key = urlparse_cached(request).hostname or ''\n+        if self.ip_concurrency:\n+            key = dnscache.get(key, key)\n         if key not in self.slots:\n             if self.ip_concurrency:\n                 concurrency = self.ip_concurrency\n-                try:\n-                    key = gethostbyname(key)\n-                except socket.error: # resolution error\n-                    pass\n             else:\n                 concurrency = self.domain_concurrency\n             concurrency, delay = _get_concurrency_delay(concurrency, spider, self.settings)\n\n@@ -10,11 +10,6 @@ from scrapy.utils.datatypes import LocalCache\n \n dnscache = LocalCache(10000)\n \n-def gethostbyname(hostname):\n-    if hostname not in dnscache:\n-        dnscache[hostname] = socket.gethostbyname(hostname)\n-    return dnscache[hostname]\n-\n class CachingThreadedResolver(ThreadedResolver):\n \n     def getHostByName(self, name, timeout = (1, 3, 11, 45)):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1ce84046d8dea81b3e26988c0765b507ce7b1cac", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 236 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -58,10 +58,10 @@ class Scheduler(object):\n             return\n         try:\n             reqd = request_to_dict(request, self.spider)\n+            self.dqs.push(reqd, request.priority)\n         except ValueError: # non serializable request\n             return\n         else:\n-            self.dqs.push(reqd, request.priority)\n             stats.inc_value('scheduler/disk_enqueued', spider=self.spider)\n             return True\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#38e193d4809b8e43aea56744683e2209fb3dbb55", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 19 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,4 +12,6 @@ class MarshalDiskQueue(DiskQueue):\n         super(MarshalDiskQueue, self).push(marshal.dumps(obj))\n \n     def pop(self):\n-        return marshal.loads(super(MarshalDiskQueue, self).pop())\n+        s = super(MarshalDiskQueue, self).pop()\n+        if s:\n+            return marshal.loads(s)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5c938cc0290e83c0f07df3b5966ffe4e5250ad88", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 208 | Contributors (this commit): 1 | Commits (past 90d): 12 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,8 +12,6 @@ def get_engine_status(engine=None):\n     global_tests = [\n         \"time()-engine.start_time\",\n         \"engine.has_capacity()\",\n-        \"engine.scheduler.is_idle()\",\n-        \"len(engine.scheduler.pending_requests)\",\n         \"engine.downloader.is_idle()\",\n         \"len(engine.downloader.slots)\",\n         \"len(engine.downloader.active)\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bb67cfd955c0b53640a938bb71f4afeb210b6c47", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 31 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 31 | Churn Cumulative: 31 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,31 @@\n+from scrapy.tests import test_utils_queue as t\n+from scrapy.squeue import MarshalDiskQueue\n+\n+\n+class MarshalDiskQueueTest(t.DiskQueueTest):\n+\n+    chunksize = 100000\n+\n+    def queue(self):\n+        return MarshalDiskQueue(self.qdir, chunksize=self.chunksize)\n+\n+    def test_serialize(self):\n+        q = self.queue()\n+        q.push('a')\n+        q.push(123)\n+        q.push({'a': 'dict'})\n+        self.assertEqual(q.pop(), 'a')\n+        self.assertEqual(q.pop(), 123)\n+        self.assertEqual(q.pop(), {'a': 'dict'})\n+\n+class ChunkSize1MarshalDiskQueueTest(MarshalDiskQueueTest):\n+    chunksize = 1\n+\n+class ChunkSize2MarshalDiskQueueTest(MarshalDiskQueueTest):\n+    chunksize = 2\n+\n+class ChunkSize3MarshalDiskQueueTest(MarshalDiskQueueTest):\n+    chunksize = 3\n+\n+class ChunkSize4MarshalDiskQueueTest(MarshalDiskQueueTest):\n+    chunksize = 4\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9f60c276127c94b2e136cd83d171aa98c229932c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 3 | Churn Cumulative: 463 | Contributors (this commit): 3 | Commits (past 90d): 20 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -70,6 +70,7 @@ class CrawlerProcess(Crawler):\n \n     def start(self):\n         super(CrawlerProcess, self).start()\n+        if self.settings.getbool('DNSCACHE_ENABLED'):\n             reactor.installResolver(CachingThreadedResolver(reactor))\n         reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n         reactor.run(installSignalHandlers=False) # blocking call\n\n@@ -49,6 +49,8 @@ DEPTH_LIMIT = 0\n DEPTH_STATS = True\n DEPTH_PRIORITY = 1\n \n+DNSCACHE_ENABLED = True\n+\n DOWNLOAD_DELAY = 0\n \n DOWNLOAD_HANDLERS = {}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#259dccaf5840af03375ab372a68df0bbba7d233e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 10 | Files Changed: 8 | Hunks: 9 | Methods Changed: 1 | Complexity Δ (Sum/Max): 28/28 | Churn Δ: 18 | Churn Cumulative: 891 | Contributors (this commit): 4 | Commits (past 90d): 13 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -11,7 +11,7 @@ from scrapy.xlib.pydispatch import dispatcher\n from scrapy import signals\n from scrapy.http import Headers\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n-from scrapy.core.downloader.responsetypes import responsetypes\n+from scrapy.responsetypes import responsetypes\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.misc import load_object\n\n@@ -2,7 +2,7 @@ import zlib\n \n from scrapy.utils.gz import gunzip\n from scrapy.http import Response, TextResponse\n-from scrapy.core.downloader.responsetypes import responsetypes\n+from scrapy.responsetypes import responsetypes\n \n \n class HttpCompressionMiddleware(object):\n\n@@ -5,7 +5,7 @@ from time import time\n import cPickle as pickle\n \n from scrapy.http import Headers\n-from scrapy.core.downloader.responsetypes import responsetypes\n+from scrapy.responsetypes import responsetypes\n from scrapy.utils.request import request_fingerprint\n from scrapy.utils.project import data_path\n from scrapy import conf\n\n@@ -11,7 +11,7 @@ from tempfile import mktemp\n \n from scrapy import log\n from scrapy.http import Response\n-from scrapy.core.downloader.responsetypes import responsetypes\n+from scrapy.responsetypes import responsetypes\n \n \n class DecompressionMiddleware(object):\n\n@@ -1,5 +1,5 @@\n from w3lib.url import file_uri_to_path\n-from scrapy.core.downloader.responsetypes import responsetypes\n+from scrapy.responsetypes import responsetypes\n from scrapy.utils.decorator import defers\n \n class FileDownloadHandler(object):\n\n@@ -8,7 +8,7 @@ from twisted.internet import defer\n \n from scrapy.http import Headers\n from scrapy.utils.httpobj import urlparse_cached\n-from scrapy.core.downloader.responsetypes import responsetypes\n+from scrapy.responsetypes import responsetypes\n \n \n def _parsed_url_args(parsed):\n\n@@ -14,8 +14,6 @@ from scrapy.utils.python import isbinarytext\n from scrapy.utils.py26 import get_data\n from scrapy.conf import settings\n \n-__package__ = 'scrapy.core.downloader.responsetypes' # required for python 2.5\n-\n class ResponseTypes(object):\n \n     CLASSES = {\n@@ -36,7 +34,7 @@ class ResponseTypes(object):\n         self.CLASSES.update(settings.get('RESPONSE_CLASSES', {}))\n         self.classes = {}\n         self.mimetypes = MimeTypes()\n-        mimedata = get_data(__package__, 'mime.types')\n+        mimedata = get_data('scrapy', 'mime.types')\n         self.mimetypes.readfp(StringIO(mimedata))\n         for mimetype, cls in self.CLASSES.iteritems():\n             self.classes[mimetype] = load_object(cls)\n\n@@ -1,5 +1,5 @@\n import unittest\n-from scrapy.core.downloader.responsetypes import responsetypes\n+from scrapy.responsetypes import responsetypes\n \n from scrapy.http import Response, TextResponse, XmlResponse, HtmlResponse, Headers\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cea0dae1b2fe52629ab2be26e11876473b3c5b3a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 25 | Churn Cumulative: 154 | Contributors (this commit): 1 | Commits (past 90d): 8 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -56,6 +56,28 @@ class SitemapTest(unittest.TestCase):\n     <lastmod />\n   </url>\n </urlset>\n+\"\"\")\n+        self.assertEqual(list(s),\n+            [{'priority': '1', 'loc': 'http://www.example.com/', 'lastmod': '2009-08-16', 'changefreq': 'daily'},\n+             {'loc': 'http://www.example.com/2', 'lastmod': ''},\n+            ])\n+\n+    def test_sitemap_wrong_ns(self):\n+        \"\"\"We have seen sitemaps with wrongs ns. Presumably, Google still works\n+        with these, though is not 100% confirmed\"\"\"\n+        s = Sitemap(\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<urlset xmlns=\"http://www.google.com/schemas/sitemap/0.84\">\n+  <url xmlns=\"\">\n+    <loc> http://www.example.com/</loc>\n+    <lastmod>2009-08-16</lastmod>\n+    <changefreq>daily</changefreq>\n+    <priority>1</priority>\n+  </url>\n+  <url xmlns=\"\">\n+    <loc> http://www.example.com/2</loc>\n+    <lastmod />\n+  </url>\n+</urlset>\n \"\"\")\n         self.assertEqual(list(s),\n             [{'priority': '1', 'loc': 'http://www.example.com/', 'lastmod': '2009-08-16', 'changefreq': 'daily'},\n\n@@ -22,7 +22,8 @@ class Sitemap(object):\n         for elem in self._root.getchildren():\n             d = {}\n             for el in elem.getchildren():\n-                _, name = el.tag.split('}', 1)\n+                tag = el.tag\n+                name = tag.split('}', 1)[1] if '}' in tag else tag\n                 d[name] = el.text.strip() if el.text else ''\n             yield d\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a2b0737a1d77b12a8a2f07c3329d855e19df611d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 26 | Churn Cumulative: 180 | Contributors (this commit): 1 | Commits (past 90d): 10 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -84,6 +84,29 @@ class SitemapTest(unittest.TestCase):\n              {'loc': 'http://www.example.com/2', 'lastmod': ''},\n             ])\n \n+    def test_sitemap_wrong_ns2(self):\n+        \"\"\"We have seen sitemaps with wrongs ns. Presumably, Google still works\n+        with these, though is not 100% confirmed\"\"\"\n+        s = Sitemap(\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<urlset>\n+  <url xmlns=\"\">\n+    <loc> http://www.example.com/</loc>\n+    <lastmod>2009-08-16</lastmod>\n+    <changefreq>daily</changefreq>\n+    <priority>1</priority>\n+  </url>\n+  <url xmlns=\"\">\n+    <loc> http://www.example.com/2</loc>\n+    <lastmod />\n+  </url>\n+</urlset>\n+\"\"\")\n+        assert s.type == 'urlset'\n+        self.assertEqual(list(s),\n+            [{'priority': '1', 'loc': 'http://www.example.com/', 'lastmod': '2009-08-16', 'changefreq': 'daily'},\n+             {'loc': 'http://www.example.com/2', 'lastmod': ''},\n+            ])\n+\n     def test_sitemap_urls_from_robots(self):\n         robots = \"\"\"User-agent: *\n Disallow: /aff/\n\n@@ -16,7 +16,8 @@ class Sitemap(object):\n         tree = ElementTree()\n         tree.parse(StringIO(xmltext))\n         self._root = tree.getroot()\n-        _, self.type = self._root.tag.split('}', 1)\n+        rt = self._root.tag\n+        self.type = self._root.tag.split('}', 1)[1] if '}' in rt else rt\n \n     def __iter__(self):\n         for elem in self._root.getchildren():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0eaa1d95f6dea80f807fce979d8f521729905906", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 10 | Files Changed: 10 | Hunks: 19 | Methods Changed: 11 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 36 | Churn Cumulative: 1697 | Contributors (this commit): 5 | Commits (past 90d): 28 | Contributors (cumulative): 17 | DMM Complexity: None\n\nDIFF:\n@@ -11,7 +11,7 @@ from scrapy.crawler import CrawlerProcess\n from scrapy.xlib import lsprofcalltree\n from scrapy.conf import settings\n from scrapy.command import ScrapyCommand\n-from scrapy.exceptions import UsageError\n+from scrapy.exceptions import UsageError, ScrapyDeprecationWarning\n from scrapy.utils.misc import walk_modules\n from scrapy.utils.project import inside_project\n \n@@ -82,7 +82,7 @@ def _check_deprecated_scrapy_ctl(argv, inproject):\n         return\n     import warnings\n     warnings.warn(\"`scrapy-ctl.py` command-line tool is deprecated and will be removed in Scrapy 0.11, use `scrapy` instead\",\n-        DeprecationWarning, stacklevel=3)\n+        ScrapyDeprecationWarning, stacklevel=3)\n     if inproject:\n         projpath = os.path.abspath(os.path.dirname(os.path.dirname(settings.settings_module.__file__)))\n         cfg_path = os.path.join(projpath, 'scrapy.cfg')\n\n@@ -12,6 +12,7 @@ from twisted.python import log as txlog\n from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy import signals, log\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.project import crawler\n from scrapy.conf import settings\n \n@@ -22,7 +23,7 @@ class CloseSpider(object):\n         self.itemcount = settings.getint('CLOSESPIDER_ITEMCOUNT')\n         # XXX: legacy support - remove for future releases\n         if settings.getint('CLOSESPIDER_ITEMPASSED'):\n-            warnings.warn(\"CLOSESPIDER_ITEMPASSED setting is deprecated, use CLOSESPIDER_ITEMCOUNT instead\", DeprecationWarning)\n+            warnings.warn(\"CLOSESPIDER_ITEMPASSED setting is deprecated, use CLOSESPIDER_ITEMCOUNT instead\", ScrapyDeprecationWarning)\n             self.pagecount = settings.getint('CLOSESPIDER_ITEMPASSED')\n         self.pagecount = settings.getint('CLOSESPIDER_PAGECOUNT')\n         self.errorcount = settings.getint('CLOSESPIDER_ERRORCOUNT')\n\n@@ -1,5 +1,6 @@\n from scrapy.contrib.exporter import JsonLinesItemExporter\n+from scrapy.exceptions import ScrapyDeprecationWarning\n \n import warnings\n warnings.warn(\"Module `scrapy.contrib.exporter.jsonlines` is deprecated - use `scrapy.contrib.exporter` instead\",\n-    DeprecationWarning, stacklevel=2)\n+    ScrapyDeprecationWarning, stacklevel=2)\n\n@@ -8,6 +8,7 @@ import warnings\n \n from scrapy import log\n from scrapy.http import Request\n+from scrapy.exceptions import ScrapyDeprecationWarning\n \n class DepthMiddleware(object):\n \n@@ -27,7 +28,7 @@ class DepthMiddleware(object):\n             # XXX: backwards compatibility with old SCHEDULER_ORDER setting\n             # will be removed on Scrapy 0.15\n             warnings.warn(\"SCHEDULER_ORDER setting is deprecated, \" \\\n-                \"use DEPTH_PRIORITY instead\", DeprecationWarning)\n+                \"use DEPTH_PRIORITY instead\", ScrapyDeprecationWarning)\n             if sorder == 'BFO':\n                 prio = 1\n             elif sorder == 'DFO':\n\n@@ -11,6 +11,7 @@ from scrapy.utils.defer import mustbe_deferred\n from scrapy.utils.signal import send_catch_log\n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.resolver import dnscache\n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy import signals\n from scrapy import log\n from .middleware import DownloaderMiddlewareManager\n@@ -46,7 +47,7 @@ def _get_concurrency_delay(concurrency, spider, settings):\n     c = settings.getint('CONCURRENT_REQUESTS_PER_SPIDER')\n     if c:\n         warnings.warn(\"CONCURRENT_REQUESTS_PER_SPIDER setting is deprecated, \" \\\n-            \"use CONCURRENT_REQUESTS_PER_DOMAIN instead\", DeprecationWarning)\n+            \"use CONCURRENT_REQUESTS_PER_DOMAIN instead\", ScrapyDeprecationWarning)\n         concurrency = c\n     # ----------------------------\n \n\n@@ -43,3 +43,10 @@ class UsageError(Exception):\n     def __init__(self, *a, **kw):\n         self.print_help = kw.pop('print_help', True)\n         super(UsageError, self).__init__(*a, **kw)\n+\n+class ScrapyDeprecationWarning(Warning):\n+    \"\"\"Warning category for deprecated features, since the default\n+    DeprecationWarning is silenced on Python 2.7+\n+    \"\"\"\n+    pass\n+\n\n@@ -1,9 +1,10 @@\n import warnings\n+from scrapy.exceptions import ScrapyDeprecationWarning\n \n def deprecated_setter(setter, attrname):\n     def newsetter(self, value):\n         c = self.__class__.__name__\n         warnings.warn(\"Don't modify %s.%s attribute, use %s.replace() instead\" % \\\n-            (c, attrname, c), DeprecationWarning, stacklevel=2)\n+            (c, attrname, c), ScrapyDeprecationWarning, stacklevel=2)\n         return setter(self, value)\n     return newsetter\n\n@@ -13,6 +13,7 @@ import scrapy\n from scrapy.conf import settings\n from scrapy.utils.python import unicode_to_str\n from scrapy.utils.misc import load_object\n+from scrapy.exceptions import ScrapyDeprecationWarning\n  \n # Logging levels\n DEBUG = logging.DEBUG\n@@ -114,7 +115,7 @@ def start(logfile=None, loglevel=None, logstdout=None):\n def msg(message, level=INFO, **kw):\n     if 'component' in kw:\n         warnings.warn(\"Argument `component` of scrapy.log.msg() is deprecated\", \\\n-            DeprecationWarning, stacklevel=2)\n+            ScrapyDeprecationWarning, stacklevel=2)\n     kw.setdefault('system', 'scrapy')\n     kw['logLevel'] = level\n     log.msg(message, **kw)\n\n@@ -3,6 +3,7 @@ from functools import wraps\n \n from twisted.internet import defer, threads\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n \n def deprecated(use_instead=None):\n     \"\"\"This is a decorator which can be used to mark functions\n@@ -15,7 +16,7 @@ def deprecated(use_instead=None):\n             message = \"Call to deprecated function %s.\" % func.__name__\n             if use_instead:\n                 message += \" Use %s instead.\" % use_instead\n-            warnings.warn(message, category=DeprecationWarning, stacklevel=2)\n+            warnings.warn(message, category=ScrapyDeprecationWarning, stacklevel=2)\n             return func(*args, **kwargs)\n         return new_func\n     return wrapped\n\n@@ -2,8 +2,10 @@\n \n import warnings\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n+\n def attribute(obj, oldattr, newattr, version='0.12'):\n     cname = obj.__class__.__name__\n     warnings.warn(\"%s.%s attribute is deprecated and will be no longer supported \"\n         \"in Scrapy %s, use %s.%s attribute instead\" % \\\n-        (cname, oldattr, version, cname, newattr), DeprecationWarning, stacklevel=3)\n+        (cname, oldattr, version, cname, newattr), ScrapyDeprecationWarning, stacklevel=3)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5c63b2307fb349b441f14a81091de837628e96f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 124 | Lines Deleted: 105 | Files Changed: 17 | Hunks: 70 | Methods Changed: 55 | Complexity Δ (Sum/Max): 6/3 | Churn Δ: 229 | Churn Cumulative: 5285 | Contributors (this commit): 9 | Commits (past 90d): 78 | Contributors (cumulative): 32 | DMM Complexity: 0.9473684210526315\n\nDIFF:\n@@ -13,12 +13,12 @@ from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy import signals, log\n from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.project import crawler\n from scrapy.conf import settings\n \n class CloseSpider(object):\n \n-    def __init__(self):\n+    def __init__(self, crawler):\n+        self.crawler = crawler\n         self.timeout = settings.getint('CLOSESPIDER_TIMEOUT')\n         self.itemcount = settings.getint('CLOSESPIDER_ITEMCOUNT')\n         # XXX: legacy support - remove for future releases\n@@ -43,28 +43,32 @@ class CloseSpider(object):\n             dispatcher.connect(self.item_scraped, signal=signals.item_scraped)\n         dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n \n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler)\n+\n     def catch_log(self, event):\n         if event.get('logLevel') == log.ERROR:\n             spider = event.get('spider')\n             if spider:\n                 self.errorcounts[spider] += 1\n                 if self.errorcounts[spider] == self.errorcount:\n-                    crawler.engine.close_spider(spider, 'closespider_errorcount')\n+                    self.crawler.engine.close_spider(spider, 'closespider_errorcount')\n \n     def page_count(self, response, request, spider):\n         self.pagecounts[spider] += 1\n         if self.pagecounts[spider] == self.pagecount:\n-            crawler.engine.close_spider(spider, 'closespider_pagecount')\n+            self.crawler.engine.close_spider(spider, 'closespider_pagecount')\n \n     def spider_opened(self, spider):\n         self.tasks[spider] = reactor.callLater(self.timeout, \\\n-            crawler.engine.close_spider, spider=spider, \\\n+            self.crawler.engine.close_spider, spider=spider, \\\n             reason='closespider_timeout')\n \n     def item_scraped(self, item, spider):\n         self.counts[spider] += 1\n         if self.counts[spider] == self.itemcount:\n-            crawler.engine.close_spider(spider, 'closespider_itemcount')\n+            self.crawler.engine.close_spider(spider, 'closespider_itemcount')\n \n     def spider_closed(self, spider):\n         self.counts.pop(spider, None)\n\n@@ -9,25 +9,28 @@ import robotparser\n from scrapy.xlib.pydispatch import dispatcher\n \n from scrapy import signals, log\n-from scrapy.project import crawler\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n-from scrapy.conf import settings\n \n class RobotsTxtMiddleware(object):\n     DOWNLOAD_PRIORITY = 1000\n \n-    def __init__(self):\n-        if not settings.getbool('ROBOTSTXT_OBEY'):\n+    def __init__(self, crawler):\n+        if not crawler.settings.getbool('ROBOTSTXT_OBEY'):\n             raise NotConfigured\n \n+        self.crawler = crawler\n         self._parsers = {}\n         self._spider_netlocs = {}\n         self._useragents = {}\n         dispatcher.connect(self.spider_opened, signals.spider_opened)\n         dispatcher.connect(self.spider_closed, signals.spider_closed)\n \n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler)\n+\n     def process_request(self, request, spider):\n         useragent = self._useragents[spider]\n         rp = self.robot_parser(request, spider)\n@@ -42,7 +45,7 @@ class RobotsTxtMiddleware(object):\n             self._parsers[netloc] = None\n             robotsurl = \"%s://%s/robots.txt\" % (url.scheme, url.netloc)\n             robotsreq = Request(robotsurl, priority=self.DOWNLOAD_PRIORITY)\n-            dfd = crawler.engine.download(robotsreq, spider)\n+            dfd = self.crawler.engine.download(robotsreq, spider)\n             dfd.addCallback(self._parse_robots)\n             self._spider_netlocs[spider].add(netloc)\n         return self._parsers[netloc]\n\n@@ -12,31 +12,34 @@ from twisted.internet import task\n from scrapy.xlib.pydispatch import dispatcher\n from scrapy import signals\n from scrapy import log\n-from scrapy.project import crawler\n from scrapy.exceptions import NotConfigured\n from scrapy.mail import MailSender\n-from scrapy.conf import settings\n from scrapy.stats import stats\n from scrapy.utils.memory import get_vmvalue_from_procfs, procfs_supported\n from scrapy.utils.engine import get_engine_status\n \n class MemoryUsage(object):\n     \n-    def __init__(self):\n-        if not settings.getbool('MEMUSAGE_ENABLED'):\n+    def __init__(self, crawler):\n+        if not crawler.settings.getbool('MEMUSAGE_ENABLED'):\n             raise NotConfigured\n         if not procfs_supported():\n             raise NotConfigured\n \n+        self.crawler = crawler\n         self.warned = False\n-        self.notify_mails = settings.getlist('MEMUSAGE_NOTIFY')\n-        self.limit = settings.getint('MEMUSAGE_LIMIT_MB')*1024*1024\n-        self.warning = settings.getint('MEMUSAGE_WARNING_MB')*1024*1024\n-        self.report = settings.getbool('MEMUSAGE_REPORT')\n+        self.notify_mails = crawler.settings.getlist('MEMUSAGE_NOTIFY')\n+        self.limit = crawler.settings.getint('MEMUSAGE_LIMIT_MB')*1024*1024\n+        self.warning = crawler.settings.getint('MEMUSAGE_WARNING_MB')*1024*1024\n+        self.report = crawler.settings.getbool('MEMUSAGE_REPORT')\n         self.mail = MailSender()\n         dispatcher.connect(self.engine_started, signal=signals.engine_started)\n         dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n \n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler)\n+\n     def get_virtual_size(self):\n         return get_vmvalue_from_procfs('VmSize')\n \n@@ -70,10 +73,10 @@ class MemoryUsage(object):\n             log.msg(\"Memory usage exceeded %dM. Shutting down Scrapy...\" % mem, level=log.ERROR)\n             if self.notify_mails:\n                 subj = \"%s terminated: memory usage exceeded %dM at %s\" % \\\n-                        (settings['BOT_NAME'], mem, socket.gethostname())\n+                        (self.crawler.settings['BOT_NAME'], mem, socket.gethostname())\n                 self._send_report(self.notify_mails, subj)\n                 stats.set_value('memusage/limit_notified', 1)\n-            crawler.stop()\n+            self.crawler.stop()\n \n     def _check_warning(self):\n         if self.warned: # warn only once\n@@ -84,7 +87,7 @@ class MemoryUsage(object):\n             log.msg(\"Memory usage reached %dM\" % mem, level=log.WARNING)\n             if self.notify_mails:\n                 subj = \"%s warning: memory usage reached %dM at %s\" % \\\n-                        (settings['BOT_NAME'], mem, socket.gethostname())\n+                        (self.crawler.settings['BOT_NAME'], mem, socket.gethostname())\n                 self._send_report(self.notify_mails, subj)\n                 stats.set_value('memusage/warning_notified', 1)\n             self.warned = True\n\n@@ -17,13 +17,15 @@ class MediaPipeline(object):\n             self.downloaded = {}\n             self.waiting = defaultdict(list)\n \n-    def __init__(self, download_func=None):\n+    def __init__(self, download_func=None, crawler=None):\n         self.spiderinfo = {}\n         self.download_func = download_func\n-        if not download_func:\n-            from scrapy.project import crawler\n         self.crawler = crawler\n \n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler=crawler)\n+\n     def open_spider(self, spider):\n         self.spiderinfo[spider] = self.SpiderInfo(spider)\n \n\n@@ -1,10 +1,8 @@\n from scrapy.webservice import JsonRpcResource\n-from scrapy.project import crawler\n \n class CrawlerResource(JsonRpcResource):\n \n     ws_name = 'crawler'\n \n-    def __init__(self, _crawler=crawler):\n-        JsonRpcResource.__init__(self)\n-        self._target = _crawler\n+    def __init__(self, crawler):\n+        JsonRpcResource.__init__(self, crawler, crawler)\n\n@@ -1,19 +1,17 @@\n from scrapy.webservice import JsonResource\n-from scrapy.project import crawler\n from scrapy.utils.engine import get_engine_status\n \n class EngineStatusResource(JsonResource):\n \n     ws_name = 'enginestatus'\n \n-    def __init__(self, spider_name=None, _crawler=crawler):\n-        JsonResource.__init__(self)\n+    def __init__(self, crawler, spider_name=None):\n+        JsonResource.__init__(self, crawler)\n         self._spider_name = spider_name\n         self.isLeaf = spider_name is not None\n-        self._crawler = _crawler\n \n     def render_GET(self, txrequest):\n-        status = get_engine_status(self._crawler.engine)\n+        status = get_engine_status(self.crawler.engine)\n         if self._spider_name is None:\n             return status\n         for sp, st in status['spiders'].items():\n@@ -21,4 +19,4 @@ class EngineStatusResource(JsonResource):\n                 return st\n \n     def getChild(self, name, txrequest):\n-        return EngineStatusResource(name, self._crawler)\n+        return EngineStatusResource(name, self.crawler)\n\n@@ -5,6 +5,5 @@ class StatsResource(JsonRpcResource):\n \n     ws_name = 'stats'\n \n-    def __init__(self, _stats=stats):\n-        JsonRpcResource.__init__(self)\n-        self._target = _stats\n+    def __init__(self, crawler):\n+        JsonRpcResource.__init__(self, crawler, stats)\n\n@@ -62,15 +62,15 @@ def _get_concurrency_delay(concurrency, spider, settings):\n \n class Downloader(object):\n \n-    def __init__(self, settings):\n-        self.settings = settings\n+    def __init__(self, crawler):\n+        self.settings = crawler.settings\n         self.slots = {}\n         self.active = set()\n         self.handlers = DownloadHandlers()\n-        self.total_concurrency = settings.getint('CONCURRENT_REQUESTS')\n-        self.domain_concurrency = settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')\n-        self.ip_concurrency = settings.getint('CONCURRENT_REQUESTS_PER_IP')\n-        self.middleware = DownloaderMiddlewareManager.from_settings(settings)\n+        self.total_concurrency = self.settings.getint('CONCURRENT_REQUESTS')\n+        self.domain_concurrency = self.settings.getint('CONCURRENT_REQUESTS_PER_DOMAIN')\n+        self.ip_concurrency = self.settings.getint('CONCURRENT_REQUESTS_PER_IP')\n+        self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\n \n \n     def fetch(self, request, spider):\n\n@@ -51,15 +51,15 @@ class Slot(object):\n \n class ExecutionEngine(object):\n \n-    def __init__(self, settings, spider_closed_callback):\n-        self.settings = settings\n+    def __init__(self, crawler, spider_closed_callback):\n+        self.settings = crawler.settings\n         self.slots = {}\n         self.running = False\n         self.paused = False\n-        self.scheduler_cls = load_object(settings['SCHEDULER'])\n-        self.downloader = Downloader(self.settings)\n-        self.scraper = Scraper(self, self.settings)\n-        self._concurrent_spiders = settings.getint('CONCURRENT_SPIDERS')\n+        self.scheduler_cls = load_object(self.settings['SCHEDULER'])\n+        self.downloader = Downloader(crawler)\n+        self.scraper = Scraper(crawler)\n+        self._concurrent_spiders = self.settings.getint('CONCURRENT_SPIDERS')\n         self._spider_closed_callback = spider_closed_callback\n \n     @defer.inlineCallbacks\n\n@@ -61,13 +61,13 @@ class Slot(object):\n \n class Scraper(object):\n \n-    def __init__(self, engine, settings):\n+    def __init__(self, crawler):\n         self.slots = {}\n-        self.spidermw = SpiderMiddlewareManager.from_settings(settings)\n-        itemproc_cls = load_object(settings['ITEM_PROCESSOR'])\n-        self.itemproc = itemproc_cls.from_settings(settings)\n-        self.concurrent_items = settings.getint('CONCURRENT_ITEMS')\n-        self.engine = engine\n+        self.spidermw = SpiderMiddlewareManager.from_crawler(crawler)\n+        itemproc_cls = load_object(crawler.settings['ITEM_PROCESSOR'])\n+        self.itemproc = itemproc_cls.from_crawler(crawler)\n+        self.concurrent_items = crawler.settings.getint('CONCURRENT_ITEMS')\n+        self.crawler = crawler\n \n     @defer.inlineCallbacks\n     def open_spider(self, spider):\n@@ -143,7 +143,7 @@ class Scraper(object):\n     def handle_spider_error(self, _failure, request, response, spider):\n         exc = _failure.value\n         if isinstance(exc, CloseSpider):\n-            self.engine.close_spider(spider, exc.reason or 'cancelled')\n+            self.crawler.engine.close_spider(spider, exc.reason or 'cancelled')\n             return\n         log.err(_failure, \"Spider error processing %s\" % request, spider=spider)\n         send_catch_log(signal=signals.spider_error, failure=_failure, response=response, \\\n@@ -166,7 +166,7 @@ class Scraper(object):\n         if isinstance(output, Request):\n             send_catch_log(signal=signals.request_received, request=output, \\\n                 spider=spider)\n-            self.engine.crawl(request=output, spider=spider)\n+            self.crawler.engine.crawl(request=output, spider=spider)\n         elif isinstance(output, BaseItem):\n             self.slots[spider].itemproc_size += 1\n             dfd = self.itemproc.process_item(output, spider)\n\n@@ -31,10 +31,10 @@ class Crawler(object):\n         if self.configured:\n             return\n         self.configured = True\n-        self.extensions = ExtensionManager.from_settings(self.settings)\n+        self.extensions = ExtensionManager.from_crawler(self)\n         spman_cls = load_object(self.settings['SPIDER_MANAGER_CLASS'])\n         self.spiders = spman_cls.from_settings(self.settings)\n-        self.engine = ExecutionEngine(self.settings, self._spider_closed)\n+        self.engine = ExecutionEngine(self, self._spider_closed)\n \n     def crawl(self, spider, requests=None):\n         spider.set_crawler(self)\n\n@@ -21,13 +21,15 @@ class MiddlewareManager(object):\n         raise NotImplementedError\n \n     @classmethod\n-    def from_settings(cls, settings):\n+    def from_settings(cls, settings, crawler=None):\n         mwlist = cls._get_mwlist_from_settings(settings)\n         middlewares = []\n         for clspath in mwlist:\n             try:\n                 mwcls = load_object(clspath)\n-                if hasattr(mwcls, 'from_settings'):\n+                if crawler and hasattr(mwcls, 'from_crawler'):\n+                    mw = mwcls.from_crawler(crawler)\n+                elif hasattr(mwcls, 'from_settings'):\n                     mw = mwcls.from_settings(settings)\n                 else:\n                     mw = mwcls()\n@@ -41,6 +43,10 @@ class MiddlewareManager(object):\n             level=log.DEBUG)\n         return cls(*middlewares)\n \n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls.from_settings(crawler.settings, crawler)\n+\n     def _add_middleware(self, mw):\n         if hasattr(mw, 'open_spider'):\n             self.methods['open_spider'].append(mw.open_spider)\n\n@@ -1,11 +1,13 @@\n \"\"\"\n-This module contains the Scrapy Crawler once installed by calling the crawler\n-``install`` method, like this::\n+--------- WARNING: THIS MODULE IS DEPRECATED -----------\n \n-    crawler.install()\n+This module is deprecated. If you want to get the Scrapy crawler from your\n+extension, middleware or pipeline implement the `from_crawler` class method.\n \n-After that, you can import the (singleton) crawler like this::\n+For example:\n \n-    from scrapy.project import crawler\n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler)\n \n \"\"\"\n\n@@ -12,14 +12,12 @@ from twisted.internet import protocol\n \n from scrapy.xlib.pydispatch import dispatcher\n from scrapy.exceptions import NotConfigured\n-from scrapy.project import crawler\n from scrapy.stats import stats\n from scrapy import log, signals\n from scrapy.utils.signal import send_catch_log\n from scrapy.utils.trackref import print_live_refs\n from scrapy.utils.engine import print_engine_status\n from scrapy.utils.reactor import listen_tcp\n-from scrapy.conf import settings\n \n try:\n     import guppy\n@@ -34,15 +32,20 @@ update_telnet_vars = object()\n \n class TelnetConsole(protocol.ServerFactory):\n \n-    def __init__(self):\n-        if not settings.getbool('TELNETCONSOLE_ENABLED'):\n+    def __init__(self, crawler):\n+        if not crawler.settings.getbool('TELNETCONSOLE_ENABLED'):\n             raise NotConfigured\n+        self.crawler = crawler\n         self.noisy = False\n-        self.portrange = map(int, settings.getlist('TELNETCONSOLE_PORT'))\n-        self.host = settings['TELNETCONSOLE_HOST']\n+        self.portrange = map(int, crawler.settings.getlist('TELNETCONSOLE_PORT'))\n+        self.host = crawler.settings['TELNETCONSOLE_HOST']\n         dispatcher.connect(self.start_listening, signals.engine_started)\n         dispatcher.connect(self.stop_listening, signals.engine_stopped)\n \n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler)\n+\n     def start_listening(self):\n         self.port = listen_tcp(self.portrange, self.host, self)\n         h = self.port.getHost()\n@@ -59,13 +62,13 @@ class TelnetConsole(protocol.ServerFactory):\n     def _get_telnet_vars(self):\n         # Note: if you add entries here also update topics/telnetconsole.rst\n         telnet_vars = {\n-            'engine': crawler.engine,\n-            'manager': crawler,\n-            'extensions': crawler.extensions,\n+            'engine': self.crawler.engine,\n+            'manager': self.crawler,\n+            'extensions': self.crawler.extensions,\n             'stats': stats,\n-            'spiders': crawler.spiders,\n-            'settings': settings,\n-            'est': print_engine_status,\n+            'spiders': self.crawler.spiders,\n+            'settings': self.crawler.settings,\n+            'est': lambda x: print_engine_status(self.crawler),\n             'p': pprint.pprint,\n             'prefs': print_live_refs,\n             'hpy': hpy,\n\n@@ -2,13 +2,8 @@\n \n from time import time # used in global tests code\n \n-from scrapy.project import crawler\n-\n-def get_engine_status(engine=None):\n+def get_engine_status(engine):\n     \"\"\"Return a report of the current engine status\"\"\"\n-    if engine is None:\n-        engine = crawler.engine\n-\n     global_tests = [\n         \"time()-engine.start_time\",\n         \"engine.has_capacity()\",\n@@ -59,6 +54,6 @@ def format_engine_status(engine=None):\n             s += \"  %-50s : %s\\n\" % (test, result)\n     return s\n \n-def print_engine_status(engine=None):\n+def print_engine_status(engine):\n     print format_engine_status(engine)\n \n\n@@ -19,9 +19,7 @@ class SpiderReferencer(object):\n \n     spider_ref_re = re.compile('^spider:([0-9a-f]+)?:?(.+)?$')\n \n-    def __init__(self, crawler=None):\n-        if crawler is None:\n-            from scrapy.project import crawler\n+    def __init__(self, crawler):\n         self.crawler = crawler\n \n     def get_reference_from_spider(self, spider):\n@@ -80,7 +78,8 @@ class ScrapyJSONEncoder(json.JSONEncoder):\n     TIME_FORMAT = \"%H:%M:%S\"\n \n     def __init__(self, *a, **kw):\n-        self.spref = kw.pop('spref', None) or SpiderReferencer()\n+        crawler = kw.pop('crawler', None)\n+        self.spref = kw.pop('spref', None) or SpiderReferencer(crawler)\n         super(ScrapyJSONEncoder, self).__init__(*a, **kw)\n \n     def encode(self, o):\n\n@@ -15,19 +15,21 @@ from scrapy.utils.misc import load_object\n from scrapy.utils.txweb import JsonResource as JsonResource_\n from scrapy.utils.reactor import listen_tcp\n from scrapy.utils.conf import build_component_list\n-from scrapy.conf import settings\n \n \n class JsonResource(JsonResource_):\n \n-    json_encoder = ScrapyJSONEncoder()\n+    def __init__(self, crawler, target=None):\n+        JsonResource_.__init__(self)\n+        self.crawler = crawler\n+        self.json_encoder = ScrapyJSONEncoder(crawler=crawler)\n \n class JsonRpcResource(JsonResource):\n \n-    json_decoder = ScrapyJSONDecoder()\n-\n-    def __init__(self, target=None):\n-        JsonResource.__init__(self)\n+    def __init__(self, crawler, target=None):\n+        JsonResource.__init__(self, crawler, target)\n+        self.json_decoder = ScrapyJSONDecoder(crawler=crawler)\n+        self.crawler = crawler\n         self._target = target\n \n     def render_GET(self, txrequest):\n@@ -63,23 +65,28 @@ class RootResource(JsonResource):\n \n class WebService(server.Site):\n \n-    def __init__(self):\n-        if not settings.getbool('WEBSERVICE_ENABLED'):\n+    def __init__(self, crawler):\n+        if not crawler.settings.getbool('WEBSERVICE_ENABLED'):\n             raise NotConfigured\n-        logfile = settings['WEBSERVICE_LOGFILE']\n-        self.portrange = map(int, settings.getlist('WEBSERVICE_PORT'))\n-        self.host = settings['WEBSERVICE_HOST']\n-        root = RootResource()\n-        reslist = build_component_list(settings['WEBSERVICE_RESOURCES_BASE'], \\\n-            settings['WEBSERVICE_RESOURCES'])\n+        self.crawler = crawler\n+        logfile = crawler.settings['WEBSERVICE_LOGFILE']\n+        self.portrange = map(int, crawler.settings.getlist('WEBSERVICE_PORT'))\n+        self.host = crawler.settings['WEBSERVICE_HOST']\n+        root = RootResource(crawler)\n+        reslist = build_component_list(crawler.settings['WEBSERVICE_RESOURCES_BASE'], \\\n+            crawler.settings['WEBSERVICE_RESOURCES'])\n         for res_cls in map(load_object, reslist):\n-            res = res_cls()\n+            res = res_cls(crawler)\n             root.putChild(res.ws_name, res)\n         server.Site.__init__(self, root, logPath=logfile)\n         self.noisy = False\n         dispatcher.connect(self.start_listening, signals.engine_started)\n         dispatcher.connect(self.stop_listening, signals.engine_stopped)\n \n+    @classmethod\n+    def from_crawler(cls, crawler):\n+        return cls(crawler)\n+\n     def start_listening(self):\n         self.port = listen_tcp(self.portrange, self.host, self)\n         h = self.port.getHost()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
