{"custom_id": "scrapy#6d2a3e093b5d863b555480df436efd1a2b4c091f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 15 | Churn Cumulative: 25 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,4 +1,5 @@\n from scrapy.contrib_exp.adaptors.extraction import extract, ExtractImageLinks\n from scrapy.contrib_exp.adaptors.markup import remove_tags, remove_root, unquote\n from scrapy.contrib_exp.adaptors.misc import to_unicode, clean_spaces, strip, drop_empty, delist, Regex\n+from scrapy.contrib_exp.adaptors.date import to_date\n \n\n@@ -0,0 +1,14 @@\n+import time\n+\n+def to_date(format):\n+    \"\"\"\n+    Converts a date string to a YYYY-MM-DD one suitable for DateField\n+\n+    format is the format string passed to time.strptime\n+    \n+    Input: string/unicode\n+    Output: string\n+    \"\"\"\n+    def _to_date(value):\n+        return time.strftime('%Y-%m-%d', time.strptime(value, format))\n+    return _to_date\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0a598ff0b1cf1bca2b7169691269cbed15e37233", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 35 | Contributors (this commit): 2 | Commits (past 90d): 6 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -32,7 +32,7 @@ class ItemAdaptor(object):\n             # for ItemAdaptor's\n             fa = bounded_fa.__func__.__call__ \n         except KeyError:\n-            raise AttributeError(name)\n+            return setattr(self.item_instance, name, value)\n \n         adaptor_args = {'response': self._response}\n         ovalue = fa(value, adaptor_args=adaptor_args)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fdaa2ec3ffeb14a1cdf90f828f4510beca3ce9ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 22 | Churn Cumulative: 134 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -15,14 +15,16 @@ class ItemAdaptor(object):\n     def _get_field_adaptors(self):\n         def get_field_adaptor(field, cls):\n             if field in cls.__dict__:\n-                fa[field] = cls.__dict__[field]\n+                return cls.__dict__[field]\n+            else:\n+                for class_ in cls.__bases__:\n+                    return get_field_adaptor(field, class_)\n \n         fa = {}\n         for field in self.item_instance._fields.keys():\n-            for base in self.__class__.__bases__:\n-                get_field_adaptor(field, base)\n-\n-            get_field_adaptor(field, self.__class__)\n+            adaptor = get_field_adaptor(field, self.__class__)\n+            if adaptor:\n+                fa[field] = adaptor\n \n         return fa\n \n\n@@ -7,6 +7,7 @@ from scrapy.contrib_exp.newitem import Item, StringField\n class TestItem(Item):\n     name = StringField()\n     url = StringField()\n+    summary = StringField()\n \n class TestAdaptor(ItemAdaptor):\n     item_class = TestItem\n@@ -30,6 +31,15 @@ class ItemAdaptorTest(unittest.TestCase):\n         assert 'name' in ia._field_adaptors\n \n \n+        class ChildChildTestAdaptor(ChildTestAdaptor):\n+            summary = lambda v, adaptor_args: v\n+\n+        ia = ChildChildTestAdaptor()\n+        assert 'url' in ia._field_adaptors\n+        assert 'name' in ia._field_adaptors\n+        assert 'summary' in ia._field_adaptors\n+\n+\n         ia.url = 'HTTP://scrapy.ORG'\n         self.assertEqual(ia.url, 'http://scrapy.org')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bfcde7e0610e82b88e8c94bd24196a4223970f11", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 77 | Contributors (this commit): 1 | Commits (past 90d): 6 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,8 +30,15 @@ class ItemAdaptorTest(unittest.TestCase):\n         assert 'url' in ia._field_adaptors\n         assert 'name' in ia._field_adaptors\n \n+        ia.url = 'HTTP://scrapy.ORG'\n+        self.assertEqual(ia.url, 'http://scrapy.org')\n+\n+        ia.name = 'marta'\n+        self.assertEqual(ia.name, 'Marta')\n+\n \n         class ChildChildTestAdaptor(ChildTestAdaptor):\n+            url = lambda v, adaptor_args: v.upper()\n             summary = lambda v, adaptor_args: v\n \n         ia = ChildChildTestAdaptor()\n@@ -41,7 +48,7 @@ class ItemAdaptorTest(unittest.TestCase):\n \n \n         ia.url = 'HTTP://scrapy.ORG'\n-        self.assertEqual(ia.url, 'http://scrapy.org')\n+        self.assertEqual(ia.url, 'HTTP://SCRAPY.ORG')\n \n         ia.name = 'marta'\n         self.assertEqual(ia.name, 'Marta')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bf960fb102f685e9608ff3f619750994f016a7fd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 8 | Files Changed: 6 | Hunks: 6 | Methods Changed: 0 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 13 | Churn Cumulative: 500 | Contributors (this commit): 4 | Commits (past 90d): 28 | Contributors (cumulative): 12 | DMM Complexity: None\n\nDIFF:\n@@ -61,7 +61,7 @@ DOWNLOADER_MIDDLEWARES = (\n     'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware',\n     'scrapy.contrib.downloadermiddleware.common.CommonMiddleware',\n     'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware',\n-    'scrapy.contrib.downloadermiddleware.compression.CompressionMiddleware',\n+    'scrapy.contrib.downloadermiddleware.http_compression.HTTPCompressionMiddleware',\n     'scrapy.contrib.downloadermiddleware.debug.CrawlDebug',\n     'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware',\n     # Downloader side\n\n@@ -61,7 +61,7 @@ DOWNLOADER_MIDDLEWARES = (\n     'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware',\n     'scrapy.contrib.downloadermiddleware.common.CommonMiddleware',\n     'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware',\n-    'scrapy.contrib.downloadermiddleware.compression.CompressionMiddleware',\n+    'scrapy.contrib.downloadermiddleware.http_compression.HTTPCompressionMiddleware',\n     'scrapy.contrib.downloadermiddleware.debug.CrawlDebug',\n     'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware',\n     # Downloader side\n\n@@ -67,7 +67,7 @@ DOWNLOADER_MIDDLEWARES = [\n     'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware',\n     'scrapy.contrib.downloadermiddleware.common.CommonMiddleware',\n     'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware',\n-    'scrapy.contrib.downloadermiddleware.compression.CompressionMiddleware',\n+    'scrapy.contrib.downloadermiddleware.http_compression.HTTPCompressionMiddleware',\n     'scrapy.contrib.downloadermiddleware.debug.CrawlDebug',\n     'scrapy.contrib.downloadermiddleware.stats.DownloaderStats',\n     'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware',\n\n@@ -5,7 +5,7 @@ from cStringIO import StringIO\n from scrapy.http import Response\n \n \n-class CompressionMiddleware(object):\n+class HTTPCompressionMiddleware(object):\n     \"\"\"This middleware allows compressed (gzip, deflate) traffic to be\n     sent/received from web sites\"\"\"\n \n\n@@ -1,8 +1,5 @@\n \"\"\" This module implements the DecompressionMiddleware which tries to recognise\n and extract the potentially compressed responses that may arrive. \n-\n-NOTE: This middleware needs a better name to avoid confusiong it with the\n-CompressinMiddleware (in contrib.downloadermiddleware.compression).\n \"\"\"\n \n import zipfile\n\n@@ -61,7 +61,7 @@ DOWNLOADER_MIDDLEWARES = (\n     'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware',\n     'scrapy.contrib.downloadermiddleware.common.CommonMiddleware',\n     'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware',\n-    'scrapy.contrib.downloadermiddleware.compression.CompressionMiddleware',\n+    'scrapy.contrib.downloadermiddleware.http_compression.HTTPCompressionMiddleware',\n     'scrapy.contrib.downloadermiddleware.debug.CrawlDebug',\n     'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware',\n     # Downloader side\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7215ffa1eb4f282e90a07bf38b70e90d85ef83cf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 5 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 8/8 | Churn Δ: 10 | Churn Cumulative: 454 | Contributors (this commit): 4 | Commits (past 90d): 29 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -61,7 +61,7 @@ DOWNLOADER_MIDDLEWARES = (\n     'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware',\n     'scrapy.contrib.downloadermiddleware.common.CommonMiddleware',\n     'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware',\n-    'scrapy.contrib.downloadermiddleware.http_compression.HTTPCompressionMiddleware',\n+    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware',\n     'scrapy.contrib.downloadermiddleware.debug.CrawlDebug',\n     'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware',\n     # Downloader side\n\n@@ -61,7 +61,7 @@ DOWNLOADER_MIDDLEWARES = (\n     'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware',\n     'scrapy.contrib.downloadermiddleware.common.CommonMiddleware',\n     'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware',\n-    'scrapy.contrib.downloadermiddleware.http_compression.HTTPCompressionMiddleware',\n+    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware',\n     'scrapy.contrib.downloadermiddleware.debug.CrawlDebug',\n     'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware',\n     # Downloader side\n\n@@ -67,7 +67,7 @@ DOWNLOADER_MIDDLEWARES = [\n     'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware',\n     'scrapy.contrib.downloadermiddleware.common.CommonMiddleware',\n     'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware',\n-    'scrapy.contrib.downloadermiddleware.http_compression.HTTPCompressionMiddleware',\n+    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware',\n     'scrapy.contrib.downloadermiddleware.debug.CrawlDebug',\n     'scrapy.contrib.downloadermiddleware.stats.DownloaderStats',\n     'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware',\n\n@@ -5,7 +5,7 @@ from cStringIO import StringIO\n from scrapy.http import Response\n \n \n-class HTTPCompressionMiddleware(object):\n+class HttpCompressionMiddleware(object):\n     \"\"\"This middleware allows compressed (gzip, deflate) traffic to be\n     sent/received from web sites\"\"\"\n \n\n@@ -61,7 +61,7 @@ DOWNLOADER_MIDDLEWARES = (\n     'scrapy.contrib.downloadermiddleware.retry.RetryMiddleware',\n     'scrapy.contrib.downloadermiddleware.common.CommonMiddleware',\n     'scrapy.contrib.downloadermiddleware.redirect.RedirectMiddleware',\n-    'scrapy.contrib.downloadermiddleware.http_compression.HTTPCompressionMiddleware',\n+    'scrapy.contrib.downloadermiddleware.httpcompression.HttpCompressionMiddleware',\n     'scrapy.contrib.downloadermiddleware.debug.CrawlDebug',\n     'scrapy.contrib.downloadermiddleware.cache.CacheMiddleware',\n     # Downloader side\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a88119aa8b841cf59010e8ad64ff58fff13bc350", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 20 | Files Changed: 2 | Hunks: 7 | Methods Changed: 6 | Complexity Δ (Sum/Max): 7/6 | Churn Δ: 49 | Churn Cumulative: 192 | Contributors (this commit): 2 | Commits (past 90d): 16 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,22 +1,31 @@\n-from functools import wraps\n from scrapy.utils.python import get_func_args\n \n+def is_adaptor(func):\n+    return callable(func) and 'adaptor_args' in get_func_args(func)\n+\n+class ItemAdaptorMetaClass(type):\n+    def __new__(meta, name, bases, dct):\n+        # defines adaptor fields as static methods\n+        for key, func in dct.items():\n+            if not key.startswith('_') and is_adaptor(func):\n+                dct[key] = staticmethod(func)\n+        return type.__new__(meta, name, bases, dct)\n+\n \n class ItemAdaptor(object):\n-    def __init__(self, response=None, item=None):\n-        if item:\n-            self.item_instance = item\n-        else:\n-            self.item_instance = self.item_class()\n+    __metaclass__ = ItemAdaptorMetaClass\n \n+    def __init__(self, response=None, item=None):\n+        self.item_instance = item if item else self.item_class()\n         self._response = response\n         self._field_adaptors = self._get_field_adaptors()\n \n     def _get_field_adaptors(self):\n         def get_field_adaptor(field, cls):\n-            if field in cls.__dict__:\n-                return cls.__dict__[field]\n-            else:\n+            func = getattr(cls, field, None)\n+            if func:\n+                return func\n+\n             for class_ in cls.__bases__:\n                 return get_field_adaptor(field, class_)\n \n\n@@ -1,5 +1,6 @@\n \n import unittest\n+import string\n from scrapy.contrib_exp.newitem.adaptors import adaptor, ItemAdaptor\n from scrapy.contrib_exp.newitem import Item, StringField\n \n@@ -53,17 +54,16 @@ class ItemAdaptorTest(unittest.TestCase):\n         ia.name = 'marta'\n         self.assertEqual(ia.name, 'Marta')\n \n-    # FIXME: This test is fine but fails\n-#     def test_inheritance_2(self):\n-#         class ParentAdaptor(TestAdaptor):\n-#             name = adaptor(lambda v, adaptor_args: v)\n-# \n-#         class ChildAdaptor(ParentAdaptor):\n-#             name = adaptor(lambda v: v.swapcase, ParentAdaptor.name)\n-# \n-#         ia = ChildAdaptor()\n-#         ia.name = 'marta'\n-#         self.assertEqual(ia.name, 'mARTA')\n+    def test_staticmethods(self):\n+        class ParentAdaptor(TestAdaptor):\n+            name = adaptor(lambda v, adaptor_args: v)\n+\n+        class ChildAdaptor(ParentAdaptor):\n+            name = adaptor(ParentAdaptor.name, string.swapcase)\n+\n+        ia = ChildAdaptor()\n+        ia.name = 'Marta'\n+        self.assertEqual(ia.name, 'mARTA')\n \n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bfe5a554c269c5703315cdab36ceeec81a28ea11", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 101 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -3,6 +3,7 @@ from scrapy.utils.python import get_func_args\n def is_adaptor(func):\n     return callable(func) and 'adaptor_args' in get_func_args(func)\n \n+\n class ItemAdaptorMetaClass(type):\n     def __new__(meta, name, bases, dct):\n         # defines adaptor fields as static methods\n@@ -22,10 +23,9 @@ class ItemAdaptor(object):\n \n     def _get_field_adaptors(self):\n         def get_field_adaptor(field, cls):\n-            func = getattr(cls, field, None)\n-            if func:\n-                return func\n-\n+            try:\n+                return getattr(cls, field)\n+            except AttributeError:\n                 for class_ in cls.__bases__:\n                     return get_field_adaptor(field, class_)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d16169e9ab6521198a3182913acb23dea32539f9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 33 | Files Changed: 8 | Hunks: 26 | Methods Changed: 10 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 55 | Churn Cumulative: 2022 | Contributors (this commit): 6 | Commits (past 90d): 59 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,14 +5,11 @@ useful in some Scrapy implementations\n \n import hashlib\n \n-from pydispatch import dispatcher\n from pprint import PrettyPrinter\n \n from scrapy.item import ScrapedItem, ItemDelta\n from scrapy.item.adaptors import AdaptorPipe\n-from scrapy.spider import spiders\n-from scrapy.core import signals\n-from scrapy.core.exceptions import UsageError, DropItem\n+from scrapy.core.exceptions import DropItem\n from scrapy.utils.python import unique\n \n class ValidationError(DropItem):\n@@ -141,7 +138,7 @@ class RobustScrapedItem(ScrapedItem):\n             return ret\n \n         if not values:\n-            raise UsageError(\"You must specify at least one value when setting an attribute\")\n+            raise ValueError(\"You must specify at least one value when setting an attribute\")\n         if attrname not in self.ATTRIBUTES:\n             raise AttributeError('Attribute \"%s\" is not a valid attribute name. You must add it to %s.ATTRIBUTES' % (attrname, self.__class__.__name__))\n \n\n@@ -10,7 +10,7 @@ from scrapy.item import ScrapedItem\n from scrapy.http import Request\n from scrapy.utils.iterators import xmliter, csviter\n from scrapy.xpath.selector import XmlXPathSelector, HtmlXPathSelector\n-from scrapy.core.exceptions import UsageError, NotConfigured, NotSupported\n+from scrapy.core.exceptions import NotConfigured, NotSupported\n \n class XMLFeedSpider(BaseSpider):\n     \"\"\"\n@@ -55,7 +55,7 @@ class XMLFeedSpider(BaseSpider):\n             if isinstance(ret, (ScrapedItem, Request)):\n                 ret = [ret]\n             if not isinstance(ret, (list, tuple)):\n-                raise UsageError('You cannot return an \"%s\" object from a spider' % type(ret).__name__)\n+                raise TypeError('You cannot return an \"%s\" object from a spider' % type(ret).__name__)\n             for result_item in self.process_results(response, ret):\n                 yield result_item\n \n@@ -107,7 +107,7 @@ class CSVFeedSpider(BaseSpider):\n             if isinstance(ret, (ScrapedItem, Request)):\n                 ret = [ret]\n             if not isinstance(ret, (list, tuple)):\n-                raise UsageError('You cannot return an \"%s\" object from a spider' % type(ret).__name__)\n+                raise TypeError('You cannot return an \"%s\" object from a spider' % type(ret).__name__)\n             for result_item in self.process_results(response, ret):\n                 yield result_item\n \n\n@@ -3,10 +3,9 @@ Download handlers for different schemes\n \"\"\"\n from __future__ import with_statement\n \n-import os\n import urlparse\n \n-from twisted.internet import defer, reactor\n+from twisted.internet import reactor\n from twisted.web import error as web_error\n \n try:\n@@ -16,8 +15,8 @@ except ImportError:\n \n from scrapy import optional_features\n from scrapy.core import signals\n-from scrapy.http import Request, Response, Headers\n-from scrapy.core.exceptions import UsageError, HttpException, NotSupported\n+from scrapy.http import Headers\n+from scrapy.core.exceptions import HttpException, NotSupported\n from scrapy.utils.defer import defer_succeed\n from scrapy.conf import settings\n \n\n@@ -7,10 +7,6 @@ new exceptions here without documenting them there.\n \n # Internal\n \n-class UsageError(Exception):\n-    \"\"\"Incorrect usage of the core API\"\"\"\n-    pass\n-\n class NotConfigured(Exception):\n     \"\"\"Indicates a missing configuration situation\"\"\"\n     pass\n\n@@ -8,20 +8,19 @@ from twisted.plugin import IPlugin\n \n from scrapy import log\n from scrapy.http import Request\n-from scrapy.core.exceptions import UsageError\n \n def _valid_domain_name(obj):\n     \"\"\"Check the domain name specified is valid\"\"\"\n     if not obj.domain_name:\n-        raise UsageError(\"A site domain name is required\")\n+        raise ValueError(\"A site domain name is required\")\n \n def _valid_download_delay(obj):\n     \"\"\"Check the download delay is valid, if specified\"\"\"\n     delay = getattr(obj, 'download_delay', 0)\n     if not type(delay) in (int, long, float):\n-        raise UsageError(\"download_delay must be numeric\")\n+        raise ValueError(\"download_delay must be numeric\")\n     if float(delay) < 0.0:\n-        raise UsageError(\"download_delay must be positive\")\n+        raise ValueError(\"download_delay must be positive\")\n \n class ISpider(Interface, IPlugin) :\n     \"\"\"Interface to be implemented by site-specific web spiders\"\"\"\n\n@@ -2,9 +2,8 @@\n import unittest\n \n from scrapy.contrib.item import RobustScrapedItem\n-from scrapy.item.adaptors import AdaptorPipe, AdaptorFunc\n+from scrapy.item.adaptors import AdaptorPipe\n from scrapy.contrib_exp import adaptors\n-from scrapy.core.exceptions import UsageError\n \n class MyItem(RobustScrapedItem):\n     ATTRIBUTES = {\n@@ -20,7 +19,7 @@ class RobustScrapedItemTestCase(unittest.TestCase):\n         self.item = MyItem()\n \n     def test_attribute_basic(self):\n-        self.assertRaises(UsageError, self.item.attribute, 'foo')\n+        self.assertRaises(ValueError, self.item.attribute, 'foo')\n         self.assertRaises(AttributeError, self.item.attribute, 'foo', 'something')\n \n         self.item.attribute('name', 'John')\n\n@@ -2,7 +2,6 @@ import unittest\n from cStringIO import StringIO\n \n from scrapy.utils.misc import hash_values, items_to_csv, load_object, to_list\n-from scrapy.core.exceptions import UsageError\n from scrapy.item import ScrapedItem\n \n class UtilsMiscTestCase(unittest.TestCase):\n@@ -10,7 +9,7 @@ class UtilsMiscTestCase(unittest.TestCase):\n         self.assertEqual(hash_values('some', 'values', 'to', 'hash'),\n                          'f37f5dc65beaaea35af05e16e26d439fd150c576')\n \n-        self.assertRaises(UsageError, hash_values, 'some', None, 'value')\n+        self.assertRaises(ValueError, hash_values, 'some', None, 'value')\n \n     def test_items_to_csv(self):\n         item_1 = ScrapedItem()\n@@ -59,6 +58,8 @@ class UtilsMiscTestCase(unittest.TestCase):\n     def test_load_object(self):\n         obj = load_object('scrapy.utils.misc.load_object')\n         assert obj is load_object\n+        self.assertRaises(ImportError, load_object, 'nomodule999.mod.function')\n+        self.assertRaises(NameError, load_object, 'scrapy.utils.misc.load_object999')\n \n     def test_to_list(self):\n         self.assertEqual(to_list(None), [])\n\n@@ -10,7 +10,6 @@ import csv\n \n from twisted.internet import defer\n \n-from scrapy.core.exceptions import UsageError\n from scrapy.utils.python import flatten, unicode_to_str\n from scrapy.utils.markup import remove_entities\n from scrapy.utils.defer import defer_succeed\n@@ -78,18 +77,18 @@ def load_object(path):\n     try:\n         dot = path.rindex('.')\n     except ValueError:\n-        raise UsageError, '%s isn\\'t a module' % path\n+        raise ValueError, \"Error loading object '%s': not a full path\" % path\n \n     module, name = path[:dot], path[dot+1:]\n     try:\n         mod = __import__(module, {}, {}, [''])\n     except ImportError, e:\n-        raise UsageError, 'Error importing %s: \"%s\"' % (module, e)\n+        raise ImportError, \"Error loading object '%s': %s\" % (path, e)\n \n     try:\n         obj = getattr(mod, name)\n     except AttributeError:\n-        raise UsageError, 'module \"%s\" does not define any object named \"%s\"' % (module, name)\n+        raise NameError, \"Module '%s' doesn't define any object named '%s'\" % (module, name)\n \n     return obj\n load_class = load_object # backwards compatibility, but isnt going to be available for too long.\n@@ -117,7 +116,7 @@ def extract_regex(regex, text, encoding):\n         return [remove_entities(unicode(s, encoding), keep=['lt', 'amp']) for s in strings]\n \n def hash_values(*values):\n-    \"\"\"Hash a series of values. \n+    \"\"\"Hash a series of non-None values. \n     \n     For example:\n     >>> hash_values('some', 'values', 'to', 'hash')\n@@ -126,9 +125,8 @@ def hash_values(*values):\n     hash = hashlib.sha1()\n     for value in values:\n         if value is None:\n-            message = \"hash_values was passed None at argument index %d. This is a bug in the calling code\" \\\n-                    % list(values).index(None)\n-            raise UsageError(message)\n+            message = \"hash_values was passed None at argument index %d\" % list(values).index(None)\n+            raise ValueError(message)\n         hash.update(value)\n     return hash.hexdigest()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ca22e332d14199786ace8422384954cb072ae4d2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 17 | Lines Deleted: 10 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 27 | Churn Cumulative: 776 | Contributors (this commit): 4 | Commits (past 90d): 20 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,7 @@\n import unittest\n from cStringIO import StringIO\n \n-from scrapy.utils.misc import hash_values, items_to_csv, load_object, to_list\n+from scrapy.utils.misc import hash_values, items_to_csv, load_object, arg_to_list\n from scrapy.item import ScrapedItem\n \n class UtilsMiscTestCase(unittest.TestCase):\n@@ -61,10 +61,11 @@ class UtilsMiscTestCase(unittest.TestCase):\n         self.assertRaises(ImportError, load_object, 'nomodule999.mod.function')\n         self.assertRaises(NameError, load_object, 'scrapy.utils.misc.load_object999')\n \n-    def test_to_list(self):\n-        self.assertEqual(to_list(None), [])\n-        self.assertEqual(to_list('lala'), ['lala'])\n-        self.assertEqual(to_list(('lala', 'poo')), ['lala', 'poo'])\n+    def test_arg_to_list(self):\n+        self.assertEqual(arg_to_list(None), [])\n+        self.assertEqual(arg_to_list('lala'), ['lala'])\n+        self.assertEqual(arg_to_list(100), [100])\n+        self.assertEqual(arg_to_list(('lala', 'poo')), ['lala', 'poo'])\n \n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -14,13 +14,19 @@ from scrapy.utils.python import flatten, unicode_to_str\n from scrapy.utils.markup import remove_entities\n from scrapy.utils.defer import defer_succeed\n \n-def to_list(obj):\n-    if obj is None:\n+def arg_to_list(arg):\n+    \"\"\"Convert an argument to list, so that it can be iterated. The argument\n+    can be a None, single value, or an iterable. \n+    \n+    This is meant to be used in functions for arguments which can be either\n+    None, single valued or multi-valued.\n+    \"\"\"\n+    if arg is None:\n         return []\n-    elif hasattr(obj, '__iter__'):\n-        return list(obj)\n+    elif hasattr(arg, '__iter__'):\n+        return list(arg)\n     else:\n-        return [obj]\n+        return [arg]\n \n def dict_updatedefault(D, E, **F):\n     \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#f145e850ad3935cc7523fcb88471ff441b7c7eec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 10 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -3/0 | Churn Δ: 14 | Churn Cumulative: 115 | Contributors (this commit): 2 | Commits (past 90d): 11 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -22,18 +22,12 @@ class ItemAdaptor(object):\n         self._field_adaptors = self._get_field_adaptors()\n \n     def _get_field_adaptors(self):\n-        def get_field_adaptor(field, cls):\n-            try:\n-                return getattr(cls, field)\n-            except AttributeError:\n-                for class_ in cls.__bases__:\n-                    return get_field_adaptor(field, class_)\n-\n         fa = {}\n         for field in self.item_instance._fields.keys():\n-            adaptor = get_field_adaptor(field, self.__class__)\n-            if adaptor:\n-                fa[field] = adaptor\n+            try:\n+                fa[field] = object.__getattribute__(self, field)\n+            except:\n+                pass\n \n         return fa\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#16c20f5c4cc03345067d79267756272b3c1a146e", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 23 | Churn Cumulative: 237 | Contributors (this commit): 2 | Commits (past 90d): 20 | Contributors (cumulative): 4 | DMM Complexity: 0.2222222222222222\n\nDIFF:\n@@ -20,6 +20,7 @@ class ItemAdaptor(object):\n         self.item_instance = item if item else self.item_class()\n         self._response = response\n         self._field_adaptors = self._get_field_adaptors()\n+        self._default_adaptor = self._get_default_adaptor()\n \n     def _get_field_adaptors(self):\n         fa = {}\n@@ -31,13 +32,23 @@ class ItemAdaptor(object):\n \n         return fa\n \n+    def _get_default_adaptor(self):\n+        try:\n+            return object.__getattribute__(self, 'default_adaptor')\n+        except:\n+            return None\n+\n     def __setattr__(self, name, value):\n-        if name.startswith('_') or name == 'item_instance':\n+        if name.startswith('_') or name == 'item_instance' \\\n+                or name == 'default_adaptor':\n             return object.__setattr__(self, name, value)\n \n         try:\n             fa = self._field_adaptors[name]\n         except KeyError:\n+            if self._default_adaptor:\n+                fa = self._default_adaptor\n+            else:\n                 return setattr(self.item_instance, name, value)\n \n         adaptor_args = {'response': self._response}\n\n@@ -23,6 +23,16 @@ class ItemAdaptorTest(unittest.TestCase):\n         self.assertEqual(ia.item_instance.name, 'Marta')\n         self.assertEqual(ia.name, 'Marta')\n \n+    def test_defaultadaptor(self):\n+        class DefaultedTestAdaptor(ItemAdaptor):\n+            item_class = TestItem\n+            default_adaptor = lambda v, adaptor_args: v.title()\n+\n+        dta = DefaultedTestAdaptor()\n+        assert dta._default_adaptor\n+        dta.name = 'marta'\n+        assert dta.name == 'Marta'\n+\n     def test_inheritance(self):\n         class ChildTestAdaptor(TestAdaptor):\n             url = lambda v, adaptor_args: v.lower()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d8f31f5678fe99505b94f3a81f148bb32cd8647f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 130 | Contributors (this commit): 2 | Commits (past 90d): 13 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -51,7 +51,7 @@ class ItemAdaptor(object):\n             else:\n                 return setattr(self.item_instance, name, value)\n \n-        adaptor_args = {'response': self._response}\n+        adaptor_args = {'response': self._response, 'item': self.item_instance}\n         ovalue = fa(value, adaptor_args=adaptor_args)\n         setattr(self.item_instance, name, ovalue)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4b0d69cc6d7ae6be44a7e308c29702e6278ac3e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 14 | Files Changed: 8 | Hunks: 11 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 802 | Contributors (this commit): 4 | Commits (past 90d): 24 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -1,7 +1,6 @@\n from pydispatch import dispatcher\n \n from scrapy.core import signals\n-from scrapy.core.engine import scrapyengine\n from scrapy.utils.misc import dict_updatedefault\n \n class CookiesMiddleware(object):\n\n@@ -1,6 +1,5 @@\n from scrapy import log\n from scrapy.core.exceptions import NotConfigured\n-from scrapy.conf import settings\n \n class CrawlDebug(object):\n \n\n@@ -1,6 +1,4 @@\n import time\n-import hmac\n-import base64\n import hashlib\n import rfc822\n from cStringIO import StringIO\n@@ -9,14 +7,13 @@ import Image\n \n from scrapy import log\n from scrapy.http import Request\n-from scrapy.stats import stats\n from scrapy.core.engine import scrapyengine\n-from scrapy.core.exceptions import DropItem, NotConfigured, HttpException\n+from scrapy.core.exceptions import NotConfigured\n from scrapy.contrib.pipeline.media import MediaPipeline\n from scrapy.contrib.aws import sign_request\n from scrapy.conf import settings\n \n-from .images import BaseImagesPipeline, NoimagesDrop, ImageException\n+from .images import BaseImagesPipeline, ImageException\n \n \n def md5sum(buffer):\n\n@@ -1,7 +1,6 @@\n from urlparse import urlunparse\n \n from twisted.web.client import HTTPClientFactory\n-from twisted.internet import reactor\n \n from scrapy.http import Url\n \n\n@@ -8,7 +8,7 @@ docs/topics/scheduler-middleware.rst\n from twisted.internet.defer import Deferred\n \n from scrapy import log\n-from scrapy.http import Request, Response\n+from scrapy.http import Response\n from scrapy.core.exceptions import NotConfigured\n from scrapy.utils.misc import load_object\n from scrapy.utils.defer import mustbe_deferred\n\n@@ -7,8 +7,6 @@ See documentation in docs/ref/request-response.rst\n \n import re\n \n-from BeautifulSoup import UnicodeDammit\n-\n from scrapy.http.response.text import TextResponse\n from scrapy.utils.python import memoizemethod\n \n\n@@ -7,8 +7,6 @@ See documentation in docs/ref/request-response.rst\n \n import re\n \n-from BeautifulSoup import UnicodeDammit\n-\n from scrapy.http.response.text import TextResponse\n from scrapy.utils.python import memoizemethod\n \n\n@@ -4,7 +4,6 @@ Monkey patches\n These are generally a bad idea.\n \"\"\"\n import twisted\n-from twisted.web.client import HTTPClientFactory\n \n \n # Extend limit for BeautifulSoup parsing loops\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#57c7c5e9f626dab4974b9de5fa99b6ca56390cfc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 359 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,7 +11,7 @@ import Image\n from scrapy import log\n from scrapy.stats import stats\n from scrapy.core.exceptions import DropItem, NotConfigured\n-from scrapy.core.exceptions import HttpException\n+from scrapy.core.exceptions import HttpException, IgnoreRequest\n from scrapy.conf import settings\n \n from scrapy.contrib.pipeline.media import MediaPipeline\n@@ -47,7 +47,11 @@ class BaseImagesPipeline(MediaPipeline):\n \n     def media_failed(self, failure, request, info):\n         referer = request.headers.get('Referer')\n-        errmsg = str(failure.value) if isinstance(failure.value, HttpException) else str(failure)\n+        if isinstance(failure.value, (HttpException, IgnoreRequest)):\n+            errmsg = str(failure.value)\n+        else:\n+            errmsg = str(failure)\n+\n         msg = 'Image (http-error): Error downloading %s from %s referred in <%s>: %s' \\\n                 % (self.MEDIA_TYPE, request, referer, errmsg)\n         log.msg(msg, level=log.WARNING, domain=info.domain)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#315934bf6ba9b4e9baa1468f45f7842b8bf1046f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 98 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 9 | Complexity Δ (Sum/Max): 20/20 | Churn Δ: 98 | Churn Cumulative: 98 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,98 @@\n+from HTMLParser import HTMLParser\n+\n+from scrapy.link import Link\n+from scrapy.utils.python import unique as unique_list\n+from scrapy.utils.url import safe_url_string, urljoin_rfc as urljoin\n+\n+class LinkExtractor(HTMLParser):\n+\n+    \"\"\"LinkExtractor are used to extract links from web pages. They are\n+    instantiated and later \"applied\" to a Response using the extract_links\n+    method which must receive a Response object and return a list of Link objects\n+    containing the (absolute) urls to follow, and the links texts.\n+\n+    This is the base LinkExtractor class that provides enough basic\n+    functionality for extracting links to follow, but you could override this\n+    class or create a new one if you need some additional functionality. The\n+    only requisite is that the new (or overrided) class must provide a\n+    extract_links method that receives a Response and returns a list of Link objects.\n+\n+    This LinkExtractor always returns percent-encoded URLs, using the detected encoding\n+    from the response.\n+\n+    The constructor arguments are:\n+\n+    * tag (string or function)\n+      * a tag name which is used to search for links (defaults to \"a\")\n+      * a function which receives a tag name and returns whether to scan it\n+    * attr (string or function)\n+      * an attribute name which is used to search for links (defaults to \"href\")\n+      * a function which receives an attribute name and returns whether to scan it\n+    * unique - if True the same urls won't be extracted twice, otherwise the\n+      same urls will be extracted multiple times (with potentially different link texts)\n+    \"\"\"\n+\n+    def __init__(self, tag=\"a\", attr=\"href\", unique=False):\n+        HTMLParser.__init__(self)\n+\n+        self.scan_tag = tag if callable(tag) else lambda t: t == tag\n+        self.scan_attr = attr if callable(attr) else lambda a: a == attr\n+        self.unique = unique\n+\n+    def _extract_links(self, response_text, response_url, response_encoding):\n+        self.reset()\n+        self.feed(response_text)\n+        self.close()\n+\n+        links = unique_list(self.links, key=lambda link: link.url) if self.unique else self.links\n+\n+        ret = []\n+        base_url = self.base_url if self.base_url else response_url\n+        for link in links:\n+            link.url = urljoin(base_url, link.url)\n+            link.url = safe_url_string(link.url, response_encoding)\n+            link.text = link.text.decode(response_encoding)\n+            ret.append(link)\n+\n+        return ret\n+\n+    def extract_links(self, response):\n+        # wrapper needed to allow to work directly with text\n+        return self._extract_links(response.body, response.url, \n+                                   response.encoding)\n+\n+    def reset(self):\n+        HTMLParser.reset(self)\n+\n+        self.base_url = None\n+        self.current_link = None\n+        self.links = []\n+\n+    def handle_starttag(self, tag, attrs):\n+        if tag == 'base':\n+            self.base_url = dict(attrs).get('href')\n+        if self.scan_tag(tag):\n+            for attr, value in attrs:\n+                if self.scan_attr(attr):\n+                    url = self.process_attr(value)\n+                    link = Link(url=url)\n+                    self.links.append(link)\n+                    self.current_link = link\n+\n+    def handle_endtag(self, tag):\n+        self.current_link = None\n+\n+    def handle_data(self, data):\n+        if self.current_link and not self.current_link.text:\n+            self.current_link.text = data.strip()\n+\n+    def process_attr(self, value):\n+        \"\"\"Hook to process the value of the attribute before asigning\n+        it to the link\"\"\"\n+        return value\n+\n+    def matches(self, url):\n+        \"\"\"This extractor matches with any url, since\n+        it doesn't contain any patterns\"\"\"\n+        return True\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2acdd9b4091766de75209a02136374874a25e349", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 8/5 | Churn Δ: 33 | Churn Cumulative: 809 | Contributors (this commit): 4 | Commits (past 90d): 22 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,7 +1,7 @@\n import unittest\n from cStringIO import StringIO\n \n-from scrapy.utils.misc import hash_values, items_to_csv, load_object, arg_to_list\n+from scrapy.utils.misc import hash_values, items_to_csv, load_object, arg_to_list, arg_to_iter\n from scrapy.item import ScrapedItem\n \n class UtilsMiscTestCase(unittest.TestCase):\n@@ -67,5 +67,25 @@ class UtilsMiscTestCase(unittest.TestCase):\n         self.assertEqual(arg_to_list(100), [100])\n         self.assertEqual(arg_to_list(('lala', 'poo')), ['lala', 'poo'])\n \n+    def test_arg_to_list(self):\n+        self.assertEqual(arg_to_list(None), [])\n+        self.assertEqual(arg_to_list('lala'), ['lala'])\n+        self.assertEqual(arg_to_list(100), [100])\n+        self.assertEqual(arg_to_list(('lala', 'poo')), ['lala', 'poo'])\n+        self.assertEqual(arg_to_list(s for s in ['lala', 'poo']), ['lala', 'poo'])\n+\n+    def test_arg_to_iter(self):\n+        assert hasattr(arg_to_iter(None), '__iter__')\n+        assert hasattr(arg_to_iter(100), '__iter__')\n+        assert hasattr(arg_to_iter('lala'), '__iter__')\n+        assert hasattr(arg_to_iter([1,2,3]), '__iter__')\n+        assert hasattr(arg_to_iter(l for l in 'abcd'), '__iter__')\n+\n+        self.assertEqual(list(arg_to_iter(None)), [])\n+        self.assertEqual(list(arg_to_iter('lala')), ['lala'])\n+        self.assertEqual(list(arg_to_iter(100)), [100])\n+        self.assertEqual(list(arg_to_iter(l for l in 'abc')), ['a', 'b', 'c'])\n+        self.assertEqual(list(arg_to_iter([1,2,3])), [1,2,3])\n+\n if __name__ == \"__main__\":\n     unittest.main()\n\n@@ -28,6 +28,17 @@ def arg_to_list(arg):\n     else:\n         return [arg]\n \n+def arg_to_iter(arg):\n+    \"\"\"Convert an argument to an iterable. The argument can be a None, single\n+    value, or an iterable.\n+    \"\"\"\n+    if arg is None:\n+        return []\n+    elif hasattr(arg, '__iter__'):\n+        return arg\n+    else:\n+        return [arg]\n+\n def dict_updatedefault(D, E, **F):\n     \"\"\"\n     updatedefault(D, E, **F) -> None.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1f38460ba405879cf918318cfd8883f8fe78d9bf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 231 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,7 +8,7 @@ docs/topics/spider-middleware.rst\n \n from scrapy import log\n from scrapy.core.exceptions import NotConfigured\n-from scrapy.utils.misc import load_object\n+from scrapy.utils.misc import load_object, arg_to_iter\n from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy.conf import settings\n \n@@ -89,4 +89,5 @@ class SpiderMiddlewareManager(object):\n \n     def call(self, request, response, spider):\n         defer_result(response).chainDeferred(request.deferred)\n+        request.deferred.addCallback(arg_to_iter)\n         return request.deferred\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#127e5ae02932af67c6157939cff6ab388c89c677", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 108 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -28,15 +28,18 @@ class LinkExtractor(HTMLParser):\n     * attr (string or function)\n       * an attribute name which is used to search for links (defaults to \"href\")\n       * a function which receives an attribute name and returns whether to scan it\n+    * process (funtion)\n+      * a function wich receives the attribute value before assigning it\n     * unique - if True the same urls won't be extracted twice, otherwise the\n       same urls will be extracted multiple times (with potentially different link texts)\n     \"\"\"\n \n-    def __init__(self, tag=\"a\", attr=\"href\", unique=False):\n+    def __init__(self, tag=\"a\", attr=\"href\", process=None, unique=False):\n         HTMLParser.__init__(self)\n \n         self.scan_tag = tag if callable(tag) else lambda t: t == tag\n         self.scan_attr = attr if callable(attr) else lambda a: a == attr\n+        self.process_attr = process if callable(process) else lambda v: v\n         self.unique = unique\n \n     def _extract_links(self, response_text, response_url, response_encoding):\n@@ -86,11 +89,6 @@ class LinkExtractor(HTMLParser):\n         if self.current_link and not self.current_link.text:\n             self.current_link.text = data.strip()\n \n-    def process_attr(self, value):\n-        \"\"\"Hook to process the value of the attribute before asigning\n-        it to the link\"\"\"\n-        return value\n-\n     def matches(self, url):\n         \"\"\"This extractor matches with any url, since\n         it doesn't contain any patterns\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#875e8e57d3cd20f4df2f0f7e8cb4b94bc6b7485a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 70 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 5 | Methods Changed: 8 | Complexity Δ (Sum/Max): 20/20 | Churn Δ: 74 | Churn Cumulative: 182 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,10 +1,11 @@\n from HTMLParser import HTMLParser\n+from lxml import etree\n \n from scrapy.link import Link\n from scrapy.utils.python import unique as unique_list\n from scrapy.utils.url import safe_url_string, urljoin_rfc as urljoin\n \n-class LinkExtractor(HTMLParser):\n+class HtmlParserLinkExtractor(HTMLParser):\n \n     \"\"\"LinkExtractor are used to extract links from web pages. They are\n     instantiated and later \"applied\" to a Response using the extract_links\n@@ -26,10 +27,10 @@ class LinkExtractor(HTMLParser):\n       * a tag name which is used to search for links (defaults to \"a\")\n       * a function which receives a tag name and returns whether to scan it\n     * attr (string or function)\n-      * an attribute name which is used to search for links (defaults to \"href\")\n-      * a function which receives an attribute name and returns whether to scan it\n+      * an attrsute name which is used to search for links (defaults to \"href\")\n+      * a function which receives an attrsute name and returns whether to scan it\n     * process (funtion)\n-      * a function wich receives the attribute value before assigning it\n+      * a function wich receives the attrsute value before assigning it\n     * unique - if True the same urls won't be extracted twice, otherwise the\n       same urls will be extracted multiple times (with potentially different link texts)\n     \"\"\"\n@@ -94,3 +95,68 @@ class LinkExtractor(HTMLParser):\n         it doesn't contain any patterns\"\"\"\n         return True\n \n+\n+class LxmlLinkExtractor(object):\n+    def __init__(self, tag=\"a\", attr=\"href\", process=None, unique=False):\n+        scan_tag = tag if callable(tag) else lambda t: t == tag\n+        scan_attr = attr if callable(attr) else lambda a: a == attr\n+        process_attr = process if callable(process) else lambda v: v\n+\n+        self.unique = unique\n+\n+        target = LinkTarget(scan_tag, scan_attr, process_attr)\n+        self.parser = etree.HTMLParser(target=target)\n+\n+\n+    def _extract_links(self, response_text, response_url, response_encoding):\n+        self.base_url, self.links = etree.HTML(response_text, self.parser) \n+\n+        links = unique_list(self.links, key=lambda link: link.url) if self.unique else self.links\n+\n+        ret = []\n+        base_url = self.base_url if self.base_url else response_url\n+        for link in links:\n+            link.url = urljoin(base_url, link.url)\n+            link.url = safe_url_string(link.url, response_encoding)\n+            link.text = link.text.decode(response_encoding)\n+            ret.append(link)\n+\n+        return ret\n+\n+    def extract_links(self, response):\n+        # wrapper needed to allow to work directly with text\n+        return self._extract_links(response.body, response.url, \n+                                   response.encoding)\n+\n+\n+class LinkTarget(object):\n+    def __init__(self, scan_tag, scan_attr, process_attr):\n+        self.scan_tag = scan_tag\n+        self.scan_attr = scan_attr\n+        self.process_attr = process_attr\n+\n+        self.base_url = None\n+        self.links = []\n+\n+        self.current_link = None\n+\n+    def start(self, tag, attrs):\n+        if tag == 'base':\n+            self.base_url = dict(attrs).get('href')\n+        if self.scan_tag(tag):\n+            for attr, value in attrs.iteritems():\n+                if self.scan_attr(attr):\n+                    url = self.process_attr(value)\n+                    link = Link(url=url)\n+                    self.links.append(link)\n+                    self.current_link = link\n+\n+    def end(self, tag):\n+        self.current_link = None\n+\n+    def data(self, data):\n+        if self.current_link and not self.current_link.text:\n+            self.current_link.text = data.strip()\n+\n+    def close(self):\n+        return self.base_url, self.links\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
