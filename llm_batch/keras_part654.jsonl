{"custom_id": "keras#1fa5e2a600849e042717515991893cf03cf83464", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 432 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 9 | Complexity Δ (Sum/Max): 13/13 | Churn Δ: 432 | Churn Cumulative: 432 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,432 @@\n+\"\"\"\n+Title: Image classification with modern MLP models\n+Author: [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)\n+Date created: 2021/05/30\n+Last modified: 2021/05/30\n+Description: Implementing the MLP-Mixer, FNet, and gMLP models for CIFAR-100 image classification.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Introduction\n+\n+This example implements three modern attention-free, multi-layer perceptron (MLP) based models for image\n+classification, demonstrated on the CIFAR-100 dataset:\n+\n+1. The [MLP-Mixer](https://arxiv.org/abs/2105.01601) model, by Ilya Tolstikhin et al., based on two types of MLPs.\n+3. The [FNet](https://arxiv.org/abs/2105.03824) model, by James Lee-Thorp et al., based on unparameterized\n+Fourier Transform.\n+2. The [gMLP](https://arxiv.org/abs/2105.08050) model, by Hanxiao Liu et al., based on MLP with gating.\n+\n+The purpose of the example is not to compare between these models, as they might perform differently on\n+different datasets with well-tuned hyperparameters. Rather, it is to show simple implementations of their\n+main building blocks.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import numpy as np\n+import tensorflow as tf\n+import keras_core as keras\n+from keras_core import layers\n+from keras_core.layers import Lambda\n+    \n+\"\"\"\n+## Prepare the data\n+\"\"\"\n+\n+num_classes = 100\n+input_shape = (32, 32, 3)\n+\n+(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n+\n+print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n+print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n+\n+\"\"\"\n+## Configure the hyperparameters\n+\"\"\"\n+\n+weight_decay = 0.0001\n+batch_size = 128\n+num_epochs = 50\n+dropout_rate = 0.2\n+image_size = 64  # We'll resize input images to this size.\n+patch_size = 8  # Size of the patches to be extracted from the input images.\n+num_patches = (image_size // patch_size) ** 2  # Size of the data array.\n+embedding_dim = 256  # Number of hidden units.\n+num_blocks = 4  # Number of blocks.\n+\n+print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n+print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2} \")\n+print(f\"Patches per image: {num_patches}\")\n+print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n+\n+\"\"\"\n+## Build a classification model\n+\n+We implement a method that builds a classifier given the processing blocks.\n+\"\"\"\n+\n+\n+def build_classifier(blocks, positional_encoding=False):\n+    inputs = layers.Input(shape=input_shape)\n+    # Augment data.\n+    augmented = data_augmentation(inputs)\n+    # Create patches.\n+    patches = Patches(patch_size, num_patches)(augmented)\n+    # Encode patches to generate a [batch_size, num_patches, embedding_dim] tensor.\n+    x = layers.Dense(units=embedding_dim)(patches)\n+    if positional_encoding:\n+        positions = tf.range(start=0, limit=num_patches, delta=1)\n+        position_embedding = layers.Embedding(\n+            input_dim=num_patches, output_dim=embedding_dim\n+        )(positions)\n+        x = x + position_embedding\n+    # Process x using the module blocks.\n+    x = blocks(x)\n+    # Apply global average pooling to generate a [batch_size, embedding_dim] representation tensor.\n+    representation = layers.GlobalAveragePooling1D()(x)\n+    # Apply dropout.\n+    representation = layers.Dropout(rate=dropout_rate)(representation)\n+    # Compute logits outputs.\n+    logits = layers.Dense(num_classes)(representation)\n+    # Create the Keras model.\n+    return keras.Model(inputs=inputs, outputs=logits)\n+\n+\n+\"\"\"\n+## Define an experiment\n+\n+We implement a utility function to compile, train, and evaluate a given model.\n+\"\"\"\n+\n+\n+def run_experiment(model):\n+    # Create Adam optimizer with weight decay.\n+    optimizer = keras.optimizers.AdamW(\n+        learning_rate=learning_rate,\n+        weight_decay=weight_decay,\n+    )\n+    # Compile the model.\n+    model.compile(\n+        optimizer=optimizer,\n+        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n+        metrics=[\n+            keras.metrics.SparseCategoricalAccuracy(name=\"acc\"),\n+            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top5-acc\"),\n+        ],\n+    )\n+    # Create a learning rate scheduler callback.\n+    reduce_lr = keras.callbacks.ReduceLROnPlateau(\n+        monitor=\"val_loss\", factor=0.5, patience=5\n+    )\n+    # Create an early stopping callback.\n+    early_stopping = keras.callbacks.EarlyStopping(\n+        monitor=\"val_loss\", patience=10, restore_best_weights=True\n+    )\n+    # Fit the model.\n+    history = model.fit(\n+        x=x_train,\n+        y=y_train,\n+        batch_size=batch_size,\n+        epochs=num_epochs,\n+        validation_split=0.1,\n+        callbacks=[early_stopping, reduce_lr],\n+    )\n+\n+    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n+    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n+    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n+\n+    # Return history to plot learning curves.\n+    return history\n+\n+\n+\"\"\"\n+## Use data augmentation\n+\"\"\"\n+\n+data_augmentation = keras.Sequential(\n+    [\n+        layers.Normalization(),\n+        layers.Resizing(image_size, image_size),\n+        layers.RandomFlip(\"horizontal\"),\n+        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n+    ],\n+    name=\"data_augmentation\",\n+)\n+# Compute the mean and the variance of the training data for normalization.\n+data_augmentation.layers[0].adapt(x_train)\n+\n+\n+\"\"\"\n+## Implement patch extraction as a layer\n+\"\"\"\n+\n+\n+class Patches(layers.Layer):\n+    def __init__(self, patch_size, num_patches):\n+        super().__init__()\n+        self.patch_size = patch_size\n+        self.num_patches = num_patches\n+\n+    def call(self, images):\n+        batch_size = tf.shape(images)[0]\n+        patches = tf.image.extract_patches(\n+            images=images,\n+            sizes=[1, self.patch_size, self.patch_size, 1],\n+            strides=[1, self.patch_size, self.patch_size, 1],\n+            rates=[1, 1, 1, 1],\n+            padding=\"VALID\",\n+        )\n+        patch_dims = patches.shape[-1]\n+        patches = tf.reshape(patches, [batch_size, self.num_patches, patch_dims])\n+        return patches\n+\n+\n+\"\"\"\n+## The MLP-Mixer model\n+\n+The MLP-Mixer is an architecture based exclusively on\n+multi-layer perceptrons (MLPs), that contains two types of MLP layers:\n+\n+1. One applied independently to image patches, which mixes the per-location features.\n+2. The other applied across patches (along channels), which mixes spatial information.\n+\n+This is similar to a [depthwise separable convolution based model](https://arxiv.org/pdf/1610.02357.pdf)\n+such as the Xception model, but with two chained dense transforms, no max pooling, and layer normalization\n+instead of batch normalization.\n+\"\"\"\n+\n+\"\"\"\n+### Implement the MLP-Mixer module\n+\"\"\"\n+\n+\n+class MLPMixerLayer(layers.Layer):\n+    def __init__(self, num_patches, hidden_units, dropout_rate, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+\n+        self.mlp1 = keras.Sequential(\n+            [\n+                layers.Dense(units=num_patches, activation=\"gelu\"),\n+                layers.Dense(units=num_patches),\n+                layers.Dropout(rate=dropout_rate),\n+            ]\n+        )\n+        self.mlp2 = keras.Sequential(\n+            [\n+                layers.Dense(units=num_patches, activation=\"gelu\"),\n+                layers.Dense(units=embedding_dim),\n+                layers.Dropout(rate=dropout_rate),\n+            ]\n+        )\n+        self.normalize = layers.LayerNormalization(epsilon=1e-6)\n+\n+    def build(self, input_shape):\n+        return super().build(input_shape)\n+\n+    def call(self, inputs):\n+        # Apply layer normalization.\n+        x = self.normalize(inputs)\n+        # Transpose inputs from [num_batches, num_patches, hidden_units] to [num_batches, hidden_units, num_patches].\n+        x_channels = tf.linalg.matrix_transpose(x)\n+        # Apply mlp1 on each channel independently.\n+        mlp1_outputs = self.mlp1(x_channels)\n+        # Transpose mlp1_outputs from [num_batches, hidden_dim, num_patches] to [num_batches, num_patches, hidden_units].\n+        mlp1_outputs = tf.linalg.matrix_transpose(mlp1_outputs)\n+        # Add skip connection.\n+        x = mlp1_outputs + inputs\n+        # Apply layer normalization.\n+        x_patches = self.normalize(x)\n+        # Apply mlp2 on each patch independtenly.\n+        mlp2_outputs = self.mlp2(x_patches)\n+        # Add skip connection.\n+        x = x + mlp2_outputs\n+        return x\n+\n+\n+\"\"\"\n+### Build, train, and evaluate the MLP-Mixer model\n+\n+Note that training the model with the current settings on a V100 GPUs\n+takes around 8 seconds per epoch.\n+\"\"\"\n+\n+mlpmixer_blocks = keras.Sequential(\n+    [MLPMixerLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]\n+)\n+learning_rate = 0.005\n+mlpmixer_classifier = build_classifier(mlpmixer_blocks)\n+history = run_experiment(mlpmixer_classifier)\n+\n+\"\"\"\n+The MLP-Mixer model tends to have much less number of parameters compared\n+to convolutional and transformer-based models, which leads to less training and\n+serving computational cost.\n+\n+As mentioned in the [MLP-Mixer](https://arxiv.org/abs/2105.01601) paper,\n+when pre-trained on large datasets, or with modern regularization schemes,\n+the MLP-Mixer attains competitive scores to state-of-the-art models.\n+You can obtain better results by increasing the embedding dimensions,\n+increasing, increasing the number of mixer blocks, and training the model for longer.\n+You may also try to increase the size of the input images and use different patch sizes.\n+\"\"\"\n+\n+\"\"\"\n+## The FNet model\n+\n+The FNet uses a similar block to the Transformer block. However, FNet replaces the self-attention layer\n+in the Transformer block with a parameter-free 2D Fourier transformation layer:\n+\n+1. One 1D Fourier Transform is applied along the patches.\n+2. One 1D Fourier Transform is applied along the channels.\n+\"\"\"\n+\n+\"\"\"\n+### Implement the FNet module\n+\"\"\"\n+\n+\n+class FNetLayer(layers.Layer):\n+    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+\n+        self.ffn = keras.Sequential(\n+            [\n+                layers.Dense(units=embedding_dim, activation=\"gelu\"),\n+                layers.Dropout(rate=dropout_rate),\n+                layers.Dense(units=embedding_dim),\n+            ]\n+        )\n+\n+        self.normalize1 = layers.LayerNormalization(epsilon=1e-6)\n+        self.normalize2 = layers.LayerNormalization(epsilon=1e-6)\n+\n+    def call(self, inputs):\n+        # Apply fourier transformations.\n+        x = tf.cast(\n+            tf.signal.fft2d(tf.cast(inputs, dtype=tf.dtypes.complex64)),\n+            dtype=tf.dtypes.float32,\n+        )\n+        # Add skip connection.\n+        x = x + inputs\n+        # Apply layer normalization.\n+        x = self.normalize1(x)\n+        # Apply Feedfowrad network.\n+        x_ffn = self.ffn(x)\n+        # Add skip connection.\n+        x = x + x_ffn\n+        # Apply layer normalization.\n+        return self.normalize2(x)\n+\n+\n+\"\"\"\n+### Build, train, and evaluate the FNet model\n+\n+Note that training the model with the current settings on a V100 GPUs\n+takes around 8 seconds per epoch.\n+\"\"\"\n+\n+fnet_blocks = keras.Sequential(\n+    [FNetLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]\n+)\n+learning_rate = 0.001\n+fnet_classifier = build_classifier(fnet_blocks, positional_encoding=True)\n+history = run_experiment(fnet_classifier)\n+\n+\"\"\"\n+As shown in the [FNet](https://arxiv.org/abs/2105.03824) paper,\n+better results can be achieved by increasing the embedding dimensions,\n+increasing the number of FNet blocks, and training the model for longer.\n+You may also try to increase the size of the input images and use different patch sizes.\n+The FNet scales very efficiently to long inputs, runs much faster than attention-based\n+Transformer models, and produces competitive accuracy results.\n+\"\"\"\n+\n+\"\"\"\n+## The gMLP model\n+\n+The gMLP is a MLP architecture that features a Spatial Gating Unit (SGU).\n+The SGU enables cross-patch interactions across the spatial (channel) dimension, by:\n+\n+1. Transforming the input spatially by applying linear projection across patches (along channels).\n+2. Applying element-wise multiplication of the input and its spatial transformation.\n+\"\"\"\n+\n+\"\"\"\n+### Implement the gMLP module\n+\"\"\"\n+\n+\n+class gMLPLayer(layers.Layer):\n+    def __init__(self, num_patches, embedding_dim, dropout_rate, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+\n+        self.channel_projection1 = keras.Sequential(\n+            [\n+                layers.Dense(units=embedding_dim * 2, activation=\"gelu\"),\n+                layers.Dropout(rate=dropout_rate),\n+            ]\n+        )\n+\n+        self.channel_projection2 = layers.Dense(units=embedding_dim)\n+\n+        self.spatial_projection = layers.Dense(\n+            units=num_patches, bias_initializer=\"Ones\"\n+        )\n+\n+        self.normalize1 = layers.LayerNormalization(epsilon=1e-6)\n+        self.normalize2 = layers.LayerNormalization(epsilon=1e-6)\n+\n+    def spatial_gating_unit(self, x):\n+        # Split x along the channel dimensions.\n+        # Tensors u and v will in th shape of [batch_size, num_patchs, embedding_dim].\n+        u, v = tf.split(x, num_or_size_splits=2, axis=2)\n+        # Apply layer normalization.\n+        v = self.normalize2(v)\n+        # Apply spatial projection.\n+        v_channels = tf.linalg.matrix_transpose(v)\n+        v_projected = self.spatial_projection(v_channels)\n+        v_projected = tf.linalg.matrix_transpose(v_projected)\n+        # Apply element-wise multiplication.\n+        return u * v_projected\n+\n+    def call(self, inputs):\n+        # Apply layer normalization.\n+        x = self.normalize1(inputs)\n+        # Apply the first channel projection. x_projected shape: [batch_size, num_patches, embedding_dim * 2].\n+        x_projected = self.channel_projection1(x)\n+        # Apply the spatial gating unit. x_spatial shape: [batch_size, num_patches, embedding_dim].\n+        x_spatial = self.spatial_gating_unit(x_projected)\n+        # Apply the second channel projection. x_projected shape: [batch_size, num_patches, embedding_dim].\n+        x_projected = self.channel_projection2(x_spatial)\n+        # Add skip connection.\n+        return x + x_projected\n+\n+\n+\"\"\"\n+### Build, train, and evaluate the gMLP model\n+\n+Note that training the model with the current settings on a V100 GPUs\n+takes around 9 seconds per epoch.\n+\"\"\"\n+\n+gmlp_blocks = keras.Sequential(\n+    [gMLPLayer(num_patches, embedding_dim, dropout_rate) for _ in range(num_blocks)]\n+)\n+learning_rate = 0.003\n+gmlp_classifier = build_classifier(gmlp_blocks)\n+history = run_experiment(gmlp_classifier)\n+\n+\"\"\"\n+As shown in the [gMLP](https://arxiv.org/abs/2105.08050) paper,\n+better results can be achieved by increasing the embedding dimensions,\n+increasing the number of gMLP blocks, and training the model for longer.\n+You may also try to increase the size of the input images and use different patch sizes.\n+Note that, the paper used advanced regularization strategies, such as MixUp and CutMix,\n+as well as AutoAugment.\n+\"\"\"\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2350e681eb92f98c943782213187eb04e877a521", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 99 | Lines Deleted: 2 | Files Changed: 5 | Hunks: 8 | Methods Changed: 8 | Complexity Δ (Sum/Max): 7/2 | Churn Δ: 101 | Churn Cumulative: 2122 | Contributors (this commit): 6 | Commits (past 90d): 62 | Contributors (cumulative): 21 | DMM Complexity: 1.0\n\nDIFF:\n@@ -21,6 +21,17 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     )\n \n \n+def categorical(logits, num_samples, dtype=\"int32\", seed=None):\n+    seed = draw_seed(seed)\n+    output_shape = list(logits.shape)\n+    output_shape[1] = num_samples\n+    output_shape = tuple(output_shape)\n+    output = jax.random.categorical(\n+        seed, logits[..., None], shape=output_shape, axis=1\n+    )\n+    return output.astype(dtype)\n+\n+\n def randint(shape, minval, maxval, dtype=\"int32\", seed=None):\n     seed = draw_seed(seed)\n     return jax.random.randint(\n\n@@ -32,6 +32,14 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     )\n \n \n+def categorical(logits, num_samples, dtype=\"int64\", seed=None):\n+    seed = tf_draw_seed(seed)\n+    output = tf.random.stateless_categorical(\n+        logits, num_samples, seed=seed\n+    )\n+    return tf.cast(output, dtype)\n+\n+\n def randint(shape, minval, maxval, dtype=\"int32\", seed=None):\n     intemediate_dtype = dtype\n     if standardize_dtype(dtype) not in [\"int32\", \"int64\"]:\n\n@@ -2,6 +2,7 @@ import torch\n import torch.nn.functional as tnn\n \n from keras_core.backend.config import floatx\n+from keras_core.backend.torch.core import convert_to_tensor\n from keras_core.backend.torch.core import get_device\n from keras_core.backend.torch.core import to_torch_dtype\n from keras_core.random.seed_generator import SeedGenerator\n@@ -9,9 +10,9 @@ from keras_core.random.seed_generator import draw_seed\n from keras_core.random.seed_generator import make_default_seed\n \n \n-def torch_seed_generator(seed):\n+def torch_seed_generator(seed, device=\"cpu\"):\n     seed_val, _ = draw_seed(seed)\n-    generator = torch.Generator()\n+    generator = torch.Generator(device=device)\n     generator.manual_seed(int(seed_val))\n     return generator\n \n@@ -25,6 +26,15 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     ).to(get_device())\n \n \n+def categorical(logits, num_samples, dtype=\"int32\", seed=None):\n+    logits = convert_to_tensor(logits)\n+    dtype = to_torch_dtype(dtype)\n+    generator = torch_seed_generator(seed, device=get_device())\n+    return torch.multinomial(\n+        logits, num_samples, replacement=True, generator=generator,\n+    ).type(dtype)\n+\n+\n def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     dtype = dtype or floatx()\n     dtype = to_torch_dtype(dtype)\n\n@@ -29,6 +29,51 @@ def normal(shape, mean=0.0, stddev=1.0, dtype=None, seed=None):\n     )\n \n \n+@keras_core_export(\"keras_core.random.categorical\")\n+def categorical(logits, num_samples, dtype=\"int32\", seed=None):\n+    \"\"\"Draws samples from a categorical distribution.\n+\n+    This function takes as input `logits`, a 2-D input tensor with shape\n+    (batch_size, num_classes). Each row of the input represents a categorical\n+    distribution, with each column index containing the log-probability for a\n+    given class.\n+\n+    The function will output a 2-D tensor with shape (batch_size, num_samples),\n+    where each row contains samples from the corresponding row in `logits`.\n+    Each column index contains an independent samples drawn from the input\n+    distribution.\n+\n+    Args:\n+        logits: 2-D Tensor with shape (batch_size, num_classes). Each row\n+            should define a categorical distibution with the unnormalized\n+            log-probabilities for all classes.\n+        num_samples: Int, the number of independent samples to draw for each\n+            row of the input. This will be the second dimension of the output\n+            tensor's shape.\n+        dtype: Optional dtype of the output tensor.\n+        seed: A Python integer or instance of\n+            `keras_core.random.SeedGenerator`.\n+            Used to make the behavior of the initializer\n+            deterministic. Note that an initializer seeded with an integer\n+            or None (unseeded) will produce the same random values\n+            across multiple calls. To get different random values\n+            across multiple calls, use as seed an instance\n+            of `keras_core.random.SeedGenerator`.\n+\n+    Returns:\n+        A 2-D tensor with (batch_size, num_samples).\n+    \"\"\"\n+    logits_shape = list(backend.convert_to_tensor(logits).shape)\n+    if len(logits_shape) != 2:\n+        raise ValueError(\n+            \"`logits` should be a 2-D tensor with shape \"\n+            f\"[batch_size, num_classes]. Received: logits={logits}\"\n+        )\n+    return backend.random.categorical(\n+        logits, num_samples, dtype=dtype, seed=seed\n+    )\n+\n+\n @keras_core_export(\"keras_core.random.uniform\")\n def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n     \"\"\"Draw samples from a uniform distribution.\n\n@@ -40,6 +40,29 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n         self.assertLessEqual(knp.max(res), maxval)\n         self.assertGreaterEqual(knp.max(res), minval)\n \n+    @parameterized.parameters(\n+        {\"seed\": 10, \"num_samples\": 1, \"batch_size\": 1},\n+        {\"seed\": 10, \"num_samples\": 5, \"batch_size\": 2},\n+        {\"seed\": 10, \"num_samples\": 10, \"batch_size\": 4},\n+        {\"seed\": 10, \"num_samples\": 15, \"batch_size\": 8},\n+    )\n+    def test_categorical(self, seed, num_samples, batch_size):\n+        np.random.seed(seed)\n+        # Definitively favor the batch index.\n+        logits = np.eye(batch_size) * 1e9\n+        res = random.categorical(logits, num_samples, seed=seed)\n+        # Outputs should have shape `(batch_size, num_samples)`, where each\n+        # output index matches the batch index.\n+        self.assertEqual(res.shape, (batch_size, num_samples))\n+        expected = np.tile(np.arange(batch_size)[:, None], (1, num_samples))\n+        self.assertAllClose(res, expected)\n+\n+    def test_categorical_errors(self):\n+        with self.assertRaises(ValueError):\n+            random.categorical(np.ones((5,)), 5)\n+        with self.assertRaises(ValueError):\n+            random.categorical(np.ones((5, 5, 5)), 5)\n+\n     @parameterized.parameters(\n         {\"seed\": 10, \"shape\": (5,), \"min\": 0, \"max\": 10, \"dtype\": \"uint16\"},\n         {\"seed\": 10, \"shape\": (2, 3), \"min\": 0, \"max\": 10, \"dtype\": \"uint32\"},\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d955292989548ef6a1bde00235fc6b14f9a12d6f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 6 | Files Changed: 3 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 18 | Churn Cumulative: 1228 | Contributors (this commit): 6 | Commits (past 90d): 35 | Contributors (cumulative): 13 | DMM Complexity: 0.25\n\nDIFF:\n@@ -34,9 +34,7 @@ def uniform(shape, minval=0.0, maxval=1.0, dtype=None, seed=None):\n \n def categorical(logits, num_samples, dtype=\"int64\", seed=None):\n     seed = tf_draw_seed(seed)\n-    output = tf.random.stateless_categorical(\n-        logits, num_samples, seed=seed\n-    )\n+    output = tf.random.stateless_categorical(logits, num_samples, seed=seed)\n     return tf.cast(output, dtype)\n \n \n\n@@ -31,7 +31,10 @@ def categorical(logits, num_samples, dtype=\"int32\", seed=None):\n     dtype = to_torch_dtype(dtype)\n     generator = torch_seed_generator(seed, device=get_device())\n     return torch.multinomial(\n-        logits, num_samples, replacement=True, generator=generator,\n+        logits,\n+        num_samples,\n+        replacement=True,\n+        generator=generator,\n     ).type(dtype)\n \n \n\n@@ -1,3 +1,5 @@\n+from tensorflow import nest\n+\n from keras_core import backend\n from keras_core.layers.layer import Layer\n from keras_core.utils import backend_utils\n@@ -22,8 +24,11 @@ class TFDataLayer(Layer):\n         ):\n             # We're in a TF graph, e.g. a tf.data pipeline.\n             self.backend.set_backend(\"tensorflow\")\n-            inputs = self.backend.convert_to_tensor(\n-                inputs, dtype=self.compute_dtype\n+            inputs = nest.map_structure(\n+                lambda x: self.backend.convert_to_tensor(\n+                    x, dtype=self.compute_dtype\n+                ),\n+                inputs,\n             )\n             switch_convert_input_args = False\n             if self._convert_input_args:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#664cf6f4f55afd84550a6b6d8cfb5cbb64284749", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 10 | Files Changed: 3 | Hunks: 10 | Methods Changed: 6 | Complexity Δ (Sum/Max): 11/10 | Churn Δ: 54 | Churn Cumulative: 1662 | Contributors (this commit): 7 | Commits (past 90d): 45 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -61,7 +61,7 @@ class Masking(Layer):\n         try:\n             outputs._keras_mask = ops.squeeze(boolean_mask, axis=-1)\n         except AttributeError:\n-            # outputs in a C type.\n+            # tensor is a C type.\n             pass\n         return outputs\n \n\n@@ -175,8 +175,8 @@ class CompileMetrics(metrics_module.Metric):\n         if output_names:\n             num_outputs = len(output_names)\n \n-        y_pred = nest.flatten(y_pred)\n-        y_true = nest.flatten(y_true)\n+        y_pred = self._flatten_y(y_pred)\n+        y_true = self._flatten_y(y_true)\n \n         metrics = self._user_metrics\n         weighted_metrics = self._user_weighted_metrics\n@@ -314,16 +314,25 @@ class CompileMetrics(metrics_module.Metric):\n                         flat_metrics.append(None)\n         return flat_metrics\n \n+    def _flatten_y(self, y):\n+        if isinstance(y, dict) and self.output_names:\n+            result = []\n+            for name in self.output_names:\n+                if name in y:\n+                    result.append(y[name])\n+            return result\n+        return nest.flatten(y)\n+\n     def update_state(self, y_true, y_pred, sample_weight=None):\n         if not self.built:\n             self.build(y_true, y_pred)\n-        y_true = nest.flatten(y_true)\n-        y_pred = nest.flatten(y_pred)\n+        y_true = self._flatten_y(y_true)\n+        y_pred = self._flatten_y(y_pred)\n         for m, y_t, y_p in zip(self._flat_metrics, y_true, y_pred):\n             if m:\n                 m.update_state(y_t, y_p)\n         if sample_weight is not None:\n-            sample_weight = nest.flatten(sample_weight)\n+            sample_weight = self._flatten_y(sample_weight)\n             # For multi-outputs, repeat sample weights for n outputs.\n             if len(sample_weight) < len(y_true):\n                 sample_weight = [sample_weight[0] for _ in range(len(y_true))]\n@@ -427,7 +436,7 @@ class CompileLoss(losses_module.Loss):\n         if output_names:\n             num_outputs = len(output_names)\n \n-        y_pred = nest.flatten(y_pred)\n+        y_pred = self._flatten_y(y_pred)\n         loss = self._user_loss\n         loss_weights = self._user_loss_weights\n         flat_losses = []\n@@ -577,15 +586,24 @@ class CompileLoss(losses_module.Loss):\n         with ops.name_scope(self.name):\n             return self.call(y_true, y_pred, sample_weight)\n \n+    def _flatten_y(self, y):\n+        if isinstance(y, dict) and self.output_names:\n+            result = []\n+            for name in self.output_names:\n+                if name in y:\n+                    result.append(y[name])\n+            return result\n+        return nest.flatten(y)\n+\n     def call(self, y_true, y_pred, sample_weight=None):\n         if not self.built:\n             self.build(y_true, y_pred)\n \n-        y_true = nest.flatten(y_true)\n-        y_pred = nest.flatten(y_pred)\n+        y_true = self._flatten_y(y_true)\n+        y_pred = self._flatten_y(y_pred)\n \n         if sample_weight is not None:\n-            sample_weight = nest.flatten(sample_weight)\n+            sample_weight = self._flatten_y(sample_weight)\n             # For multi-outputs, repeat sample weights for n outputs.\n             if len(sample_weight) < len(y_true):\n                 sample_weight = [sample_weight[0] for _ in range(len(y_true))]\n\n@@ -294,3 +294,19 @@ class TestCompileLoss(testing.TestCase, parameterized.TestCase):\n         compile_loss.build(y_true, y_pred)\n         value = compile_loss(y_true, y_pred, sample_weight)\n         self.assertAllClose(value, 1.266666, atol=1e-5)\n+\n+    def test_list_loss_dict_data(self):\n+        compile_loss = CompileLoss(loss=[\"mse\", \"mae\"], output_names=[\"b\", \"a\"])\n+        y_true = [backend.KerasTensor((3, 4)), backend.KerasTensor((3, 4))]\n+        y_pred = [backend.KerasTensor((3, 4)), backend.KerasTensor((3, 4))]\n+        compile_loss.build(y_true, y_pred)\n+        y_true = {\n+            \"a\": np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]),\n+            \"b\": np.array([[0.7, 0.8], [0.9, 1.0], [1.1, 1.2]]),\n+        }\n+        y_pred = {\n+            \"a\": np.array([[1.2, 1.1], [1.0, 0.9], [0.8, 0.7]]),\n+            \"b\": np.array([[0.6, 0.5], [0.4, 0.3], [0.2, 0.1]]),\n+        }\n+        value = compile_loss(y_true, y_pred)\n+        self.assertAllClose(value, 1.07666, atol=1e-5)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#556d6052142b28be5f262a2a5a06c8e838e9912b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 958 | Contributors (this commit): 6 | Commits (past 90d): 27 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -30,8 +30,9 @@ def categorical(logits, num_samples, dtype=\"int32\", seed=None):\n     logits = convert_to_tensor(logits)\n     dtype = to_torch_dtype(dtype)\n     generator = torch_seed_generator(seed, device=get_device())\n+    probs = torch.softmax(logits, dim=-1)\n     return torch.multinomial(\n-        logits,\n+        probs,\n         num_samples,\n         replacement=True,\n         generator=generator,\n\n@@ -48,8 +48,9 @@ class RandomTest(testing.TestCase, parameterized.TestCase):\n     )\n     def test_categorical(self, seed, num_samples, batch_size):\n         np.random.seed(seed)\n-        # Definitively favor the batch index.\n-        logits = np.eye(batch_size) * 1e9\n+        # Create logits that definitely favors the batch index after a softmax\n+        # is applied. Without a softmax, this would be close to random.\n+        logits = np.eye(batch_size) * 1e5 + 1e6\n         res = random.categorical(logits, num_samples, seed=seed)\n         # Outputs should have shape `(batch_size, num_samples)`, where each\n         # output index matches the batch index.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#530a1572f4b2f419a73a4f0dd0b47382e2b3b672", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 229 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 229 | Churn Cumulative: 229 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,229 @@\n+\"\"\"\n+Title: MixUp augmentation for image classification\n+Author: [Sayak Paul](https://twitter.com/RisingSayak)\n+Date created: 2021/03/06\n+Last modified: 2021/03/06\n+Description: Data augmentation using the mixup technique for image classification.\n+Accelerator: GPU\n+\"\"\"\n+\"\"\"\n+## Introduction\n+\"\"\"\n+\n+\"\"\"\n+_mixup_ is a *domain-agnostic* data augmentation technique proposed in [mixup: Beyond Empirical Risk Minimization](https://arxiv.org/abs/1710.09412)\n+by Zhang et al. It's implemented with the following formulas:\n+\n+![](https://i.ibb.co/DRyHYww/image.png)\n+\n+(Note that the lambda values are values with the [0, 1] range and are sampled from the\n+[Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution).)\n+\n+The technique is quite systematically named. We are literally mixing up the features and\n+their corresponding labels. Implementation-wise it's simple. Neural networks are prone\n+to [memorizing corrupt labels](https://arxiv.org/abs/1611.03530). mixup relaxes this by\n+combining different features with one another (same happens for the labels too) so that\n+a network does not get overconfident about the relationship between the features and\n+their labels.\n+\n+mixup is specifically useful when we are not sure about selecting a set of augmentation\n+transforms for a given dataset, medical imaging datasets, for example. mixup can be\n+extended to a variety of data modalities such as computer vision, naturallanguage\n+processing, speech, and so on.\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import numpy as np\n+import tensorflow as tf\n+import matplotlib.pyplot as plt\n+from keras_core import layers\n+import keras_core as keras\n+\n+\"\"\"\n+## Prepare the dataset\n+\n+In this example, we will be using the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. But this same recipe can\n+be used for other classification datasets as well.\n+\"\"\"\n+\n+(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n+\n+x_train = x_train.astype(\"float32\") / 255.0\n+x_train = np.reshape(x_train, (-1, 28, 28, 1))\n+y_train = tf.one_hot(y_train, 10)\n+\n+x_test = x_test.astype(\"float32\") / 255.0\n+x_test = np.reshape(x_test, (-1, 28, 28, 1))\n+y_test = tf.one_hot(y_test, 10)\n+\n+\"\"\"\n+## Define hyperparameters\n+\"\"\"\n+\n+AUTO = tf.data.AUTOTUNE\n+BATCH_SIZE = 64\n+EPOCHS = 10\n+\n+\"\"\"\n+## Convert the data into TensorFlow `Dataset` objects\n+\"\"\"\n+\n+# Put aside a few samples to create our validation set\n+val_samples = 2000\n+x_val, y_val = x_train[:val_samples], y_train[:val_samples]\n+new_x_train, new_y_train = x_train[val_samples:], y_train[val_samples:]\n+\n+train_ds_one = (\n+    tf.data.Dataset.from_tensor_slices((new_x_train, new_y_train))\n+    .shuffle(BATCH_SIZE * 100)\n+    .batch(BATCH_SIZE)\n+)\n+train_ds_two = (\n+    tf.data.Dataset.from_tensor_slices((new_x_train, new_y_train))\n+    .shuffle(BATCH_SIZE * 100)\n+    .batch(BATCH_SIZE)\n+)\n+# Because we will be mixing up the images and their corresponding labels, we will be\n+# combining two shuffled datasets from the same training data.\n+train_ds = tf.data.Dataset.zip((train_ds_one, train_ds_two))\n+\n+val_ds = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(BATCH_SIZE)\n+\n+test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)\n+\n+\"\"\"\n+## Define the mixup technique function\n+\n+To perform the mixup routine, we create new virtual datasets using the training data from\n+the same dataset, and apply a lambda value within the [0, 1] range sampled from a [Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution)\n+— such that, for example, `new_x = lambda * x1 + (1 - lambda) * x2` (where\n+`x1` and `x2` are images) and the same equation is applied to the labels as well.\n+\"\"\"\n+\n+\n+def sample_beta_distribution(size, concentration_0=0.2, concentration_1=0.2):\n+    gamma_1_sample = tf.random.gamma(shape=[size], alpha=concentration_1)\n+    gamma_2_sample = tf.random.gamma(shape=[size], alpha=concentration_0)\n+    return gamma_1_sample / (gamma_1_sample + gamma_2_sample)\n+\n+\n+def mix_up(ds_one, ds_two, alpha=0.2):\n+    # Unpack two datasets\n+    images_one, labels_one = ds_one\n+    images_two, labels_two = ds_two\n+    batch_size = tf.shape(images_one)[0]\n+\n+    # Sample lambda and reshape it to do the mixup\n+    l = sample_beta_distribution(batch_size, alpha, alpha)\n+    x_l = tf.reshape(l, (batch_size, 1, 1, 1))\n+    y_l = tf.reshape(l, (batch_size, 1))\n+\n+    # Perform mixup on both images and labels by combining a pair of images/labels\n+    # (one from each dataset) into one image/label\n+    images = images_one * x_l + images_two * (1 - x_l)\n+    labels = labels_one * y_l + labels_two * (1 - y_l)\n+    return (images, labels)\n+\n+\n+\"\"\"\n+**Note** that here , we are combining two images to create a single one. Theoretically,\n+we can combine as many we want but that comes at an increased computation cost. In\n+certain cases, it may not help improve the performance as well.\n+\"\"\"\n+\n+\"\"\"\n+## Visualize the new augmented dataset\n+\"\"\"\n+\n+# First create the new dataset using our `mix_up` utility\n+train_ds_mu = train_ds.map(\n+    lambda ds_one, ds_two: mix_up(ds_one, ds_two, alpha=0.2), num_parallel_calls=AUTO\n+)\n+\n+# Let's preview 9 samples from the dataset\n+sample_images, sample_labels = next(iter(train_ds_mu))\n+plt.figure(figsize=(10, 10))\n+for i, (image, label) in enumerate(zip(sample_images[:9], sample_labels[:9])):\n+    ax = plt.subplot(3, 3, i + 1)\n+    plt.imshow(image.numpy().squeeze())\n+    print(label.numpy().tolist())\n+    plt.axis(\"off\")\n+\n+\"\"\"\n+## Model building\n+\"\"\"\n+\n+\n+def get_training_model():\n+    model = keras.Sequential(\n+        [\n+            layers.Conv2D(16, (5, 5), activation=\"relu\", input_shape=(28, 28, 1)),\n+            layers.MaxPooling2D(pool_size=(2, 2)),\n+            layers.Conv2D(32, (5, 5), activation=\"relu\"),\n+            layers.MaxPooling2D(pool_size=(2, 2)),\n+            layers.Dropout(0.2),\n+            layers.GlobalAveragePooling2D(),\n+            layers.Dense(128, activation=\"relu\"),\n+            layers.Dense(10, activation=\"softmax\"),\n+        ]\n+    )\n+    return model\n+\n+\n+\"\"\"\n+For the sake of reproducibility, we serialize the initial random weights of our shallow\n+network.\n+\"\"\"\n+\n+initial_model = get_training_model()\n+initial_model.save_weights(\"initial_weights.weights.h5\")\n+\n+\"\"\"\n+## 1. Train the model with the mixed up dataset\n+\"\"\"\n+\n+model = get_training_model()\n+model.load_weights(\"initial_weights.weights.h5\")\n+model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n+model.fit(train_ds_mu, validation_data=val_ds, epochs=EPOCHS)\n+_, test_acc = model.evaluate(test_ds)\n+print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))\n+\n+\"\"\"\n+## 2. Train the model *without* the mixed up dataset\n+\"\"\"\n+\n+model = get_training_model()\n+model.load_weights(\"initial_weights.weights.h5\")\n+model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n+# Notice that we are NOT using the mixed up dataset here\n+model.fit(train_ds_one, validation_data=val_ds, epochs=EPOCHS)\n+_, test_acc = model.evaluate(test_ds)\n+print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))\n+\n+\"\"\"\n+Readers are encouraged to try out mixup on different datasets from different domains and\n+experiment with the lambda parameter. You are strongly advised to check out the\n+[original paper](https://arxiv.org/abs/1710.09412) as well - the authors present several ablation studies on mixup\n+showing how it can improve generalization, as well as show their results of combining\n+more than two images to create a single one.\n+\"\"\"\n+\n+\"\"\"\n+## Notes\n+\n+* With mixup, you can create synthetic examples — especially when you lack a large\n+dataset - without incurring high computational costs.\n+* [Label smoothing](https://www.pyimagesearch.com/2019/12/30/label-smoothing-with-keras-tensorflow-and-deep-learning/) and mixup usually do not work well together because label smoothing\n+already modifies the hard labels by some factor.\n+* mixup does not work well when you are using [Supervised Contrastive\n+Learning](https://arxiv.org/abs/2004.11362) (SCL) since SCL expects the true labels\n+during its pre-training phase.\n+* A few other benefits of mixup include (as described in the [paper](https://arxiv.org/abs/1710.09412)) robustness to\n+adversarial examples and stabilized GAN (Generative Adversarial Networks) training.\n+* There are a number of data augmentation techniques that extend mixup such as\n+[CutMix](https://arxiv.org/abs/1905.04899) and [AugMix](https://arxiv.org/abs/1912.02781).\n+\"\"\"\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d5ef287ed23361e349fba1c34742ff858ad0f658", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 706 | Contributors (this commit): 7 | Commits (past 90d): 20 | Contributors (cumulative): 7 | DMM Complexity: 0.0\n\nDIFF:\n@@ -11,6 +11,8 @@ convert_to_tensor\n convert_to_numpy\n \"\"\"\n \n+import numpy as np\n+\n from keras_core import backend\n from keras_core.api_export import keras_core_export\n from keras_core.backend import KerasTensor\n@@ -296,9 +298,7 @@ def convert_to_tensor(x, dtype=None):\n def convert_to_numpy(x):\n     \"\"\"Convert a tensor to a NumPy array.\"\"\"\n     if any_symbolic_tensors((x,)):\n-        raise ValueError(\n-            \"A symbolic tensor (usually the result of applying layers or \"\n-            \"operations to a `keras.Input`), cannot be converted to a numpy \"\n-            \"array. There is no concrete value for the input.\"\n-        )\n+        # This will raise a `ValueError` defined in the `KerasTensor` class. We\n+        # trigger it rather than duplicate it here.\n+        return np.array(x)\n     return backend.convert_to_numpy(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#58a870eba8479b2c0c9d82e64e5e984e63c31b4b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 291 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 7 | Complexity Δ (Sum/Max): 9/9 | Churn Δ: 291 | Churn Cumulative: 291 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,291 @@\n+\"\"\"\n+Title: Image classification with Vision Transformer\n+Author: [Khalid Salama](https://www.linkedin.com/in/khalid-salama-24403144/)\n+Date created: 2021/01/18\n+Last modified: 2021/01/18\n+Description: Implementing the Vision Transformer (ViT) model for image classification.\n+Accelerator: GPU\n+\"\"\"\n+\n+\"\"\"\n+## Introduction\n+\n+This example implements the [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929)\n+model by Alexey Dosovitskiy et al. for image classification,\n+and demonstrates it on the CIFAR-100 dataset.\n+The ViT model applies the Transformer architecture with self-attention to sequences of\n+image patches, without using convolution layers.\n+\n+\"\"\"\n+\n+\"\"\"\n+## Setup\n+\"\"\"\n+\n+import numpy as np\n+import tensorflow as tf\n+import keras_core as keras\n+from keras_core import layers\n+\n+\"\"\"\n+## Prepare the data\n+\"\"\"\n+\n+num_classes = 100\n+input_shape = (32, 32, 3)\n+\n+(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()\n+\n+print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n+print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n+\n+\n+\"\"\"\n+## Configure the hyperparameters\n+\"\"\"\n+\n+learning_rate = 0.001\n+weight_decay = 0.0001\n+batch_size = 256\n+num_epochs = 100\n+image_size = 72  # We'll resize input images to this size\n+patch_size = 6  # Size of the patches to be extract from the input images\n+num_patches = (image_size // patch_size) ** 2\n+projection_dim = 64\n+num_heads = 4\n+transformer_units = [\n+    projection_dim * 2,\n+    projection_dim,\n+]  # Size of the transformer layers\n+transformer_layers = 8\n+mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier\n+\n+\n+\"\"\"\n+## Use data augmentation\n+\"\"\"\n+\n+data_augmentation = keras.Sequential(\n+    [\n+        layers.Normalization(),\n+        layers.Resizing(image_size, image_size),\n+        layers.RandomFlip(\"horizontal\"),\n+        layers.RandomRotation(factor=0.02),\n+        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n+    ],\n+    name=\"data_augmentation\",\n+)\n+# Compute the mean and the variance of the training data for normalization.\n+data_augmentation.layers[0].adapt(x_train)\n+\n+\n+\"\"\"\n+## Implement multilayer perceptron (MLP)\n+\"\"\"\n+\n+\n+def mlp(x, hidden_units, dropout_rate):\n+    for units in hidden_units:\n+        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n+        x = layers.Dropout(dropout_rate)(x)\n+    return x\n+\n+\n+\"\"\"\n+## Implement patch creation as a layer\n+\"\"\"\n+\n+\n+class Patches(layers.Layer):\n+    def __init__(self, patch_size):\n+        super().__init__()\n+        self.patch_size = patch_size\n+\n+    def call(self, images):\n+        batch_size = tf.shape(images)[0]\n+        patches = tf.image.extract_patches(\n+            images=images,\n+            sizes=[1, self.patch_size, self.patch_size, 1],\n+            strides=[1, self.patch_size, self.patch_size, 1],\n+            rates=[1, 1, 1, 1],\n+            padding=\"VALID\",\n+        )\n+        patch_dims = patches.shape[-1]\n+        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n+        return patches\n+\n+\n+\"\"\"\n+Let's display patches for a sample image\n+\"\"\"\n+\n+import matplotlib.pyplot as plt\n+\n+plt.figure(figsize=(4, 4))\n+image = x_train[np.random.choice(range(x_train.shape[0]))]\n+plt.imshow(image.astype(\"uint8\"))\n+plt.axis(\"off\")\n+\n+resized_image = tf.image.resize(\n+    tf.convert_to_tensor([image]), size=(image_size, image_size)\n+)\n+patches = Patches(patch_size)(resized_image)\n+print(f\"Image size: {image_size} X {image_size}\")\n+print(f\"Patch size: {patch_size} X {patch_size}\")\n+print(f\"Patches per image: {patches.shape[1]}\")\n+print(f\"Elements per patch: {patches.shape[-1]}\")\n+\n+n = int(np.sqrt(patches.shape[1]))\n+plt.figure(figsize=(4, 4))\n+for i, patch in enumerate(patches[0]):\n+    ax = plt.subplot(n, n, i + 1)\n+    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n+    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n+    plt.axis(\"off\")\n+\n+\"\"\"\n+## Implement the patch encoding layer\n+\n+The `PatchEncoder` layer will linearly transform a patch by projecting it into a\n+vector of size `projection_dim`. In addition, it adds a learnable position\n+embedding to the projected vector.\n+\"\"\"\n+\n+\n+class PatchEncoder(layers.Layer):\n+    def __init__(self, num_patches, projection_dim):\n+        super().__init__()\n+        self.num_patches = num_patches\n+        self.projection = layers.Dense(units=projection_dim)\n+        self.position_embedding = layers.Embedding(\n+            input_dim=num_patches, output_dim=projection_dim\n+        )\n+\n+    def call(self, patch):\n+        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n+        encoded = self.projection(patch) + self.position_embedding(positions)\n+        return encoded\n+\n+\n+\"\"\"\n+## Build the ViT model\n+\n+The ViT model consists of multiple Transformer blocks,\n+which use the `layers.MultiHeadAttention` layer as a self-attention mechanism\n+applied to the sequence of patches. The Transformer blocks produce a\n+`[batch_size, num_patches, projection_dim]` tensor, which is processed via an\n+classifier head with softmax to produce the final class probabilities output.\n+\n+Unlike the technique described in the [paper](https://arxiv.org/abs/2010.11929),\n+which prepends a learnable embedding to the sequence of encoded patches to serve\n+as the image representation, all the outputs of the final Transformer block are\n+reshaped with `layers.Flatten()` and used as the image\n+representation input to the classifier head.\n+Note that the `layers.GlobalAveragePooling1D` layer\n+could also be used instead to aggregate the outputs of the Transformer block,\n+especially when the number of patches and the projection dimensions are large.\n+\"\"\"\n+\n+\n+def create_vit_classifier():\n+    inputs = layers.Input(shape=input_shape)\n+    # Augment data.\n+    augmented = data_augmentation(inputs)\n+    # Create patches.\n+    patches = Patches(patch_size)(augmented)\n+    # Encode patches.\n+    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n+\n+    # Create multiple layers of the Transformer block.\n+    for _ in range(transformer_layers):\n+        # Layer normalization 1.\n+        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n+        # Create a multi-head attention layer.\n+        attention_output = layers.MultiHeadAttention(\n+            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n+        )(x1, x1)\n+        # Skip connection 1.\n+        x2 = layers.Add()([attention_output, encoded_patches])\n+        # Layer normalization 2.\n+        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n+        # MLP.\n+        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n+        # Skip connection 2.\n+        encoded_patches = layers.Add()([x3, x2])\n+\n+    # Create a [batch_size, projection_dim] tensor.\n+    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n+    representation = layers.Flatten()(representation)\n+    representation = layers.Dropout(0.5)(representation)\n+    # Add MLP.\n+    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n+    # Classify outputs.\n+    logits = layers.Dense(num_classes)(features)\n+    # Create the Keras model.\n+    model = keras.Model(inputs=inputs, outputs=logits)\n+    return model\n+\n+\n+\"\"\"\n+## Compile, train, and evaluate the mode\n+\"\"\"\n+\n+\n+def run_experiment(model):\n+    optimizer = keras.optimizers.AdamW(\n+        learning_rate=learning_rate, weight_decay=weight_decay\n+    )\n+\n+    model.compile(\n+        optimizer=optimizer,\n+        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n+        metrics=[\n+            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n+            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n+        ],\n+    )\n+\n+    checkpoint_filepath = \"/tmp/checkpoint\"\n+    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n+        checkpoint_filepath,\n+        monitor=\"val_accuracy\",\n+        save_best_only=True,\n+        save_weights_only=True,\n+    )\n+\n+    history = model.fit(\n+        x=x_train,\n+        y=y_train,\n+        batch_size=batch_size,\n+        epochs=num_epochs,\n+        validation_split=0.1,\n+        callbacks=[checkpoint_callback],\n+    )\n+\n+    model.load_weights(checkpoint_filepath)\n+    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n+    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n+    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n+\n+    return history\n+\n+\n+vit_classifier = create_vit_classifier()\n+history = run_experiment(vit_classifier)\n+\n+\n+\"\"\"\n+After 100 epochs, the ViT model achieves around 55% accuracy and\n+82% top-5 accuracy on the test data. These are not competitive results on the CIFAR-100 dataset,\n+as a ResNet50V2 trained from scratch on the same data can achieve 67% accuracy.\n+\n+Note that the state of the art results reported in the\n+[paper](https://arxiv.org/abs/2010.11929) are achieved by pre-training the ViT model using\n+the JFT-300M dataset, then fine-tuning it on the target dataset. To improve the model quality\n+without pre-training, you can try to train the model for more epochs, use a larger number of\n+Transformer layers, resize the input images, change the patch size, or increase the projection dimensions. \n+Besides, as mentioned in the paper, the quality of the model is affected not only by architecture choices, \n+but also by parameters such as the learning rate schedule, optimizer, weight decay, etc.\n+In practice, it's recommended to fine-tune a ViT model\n+that was pre-trained using a large, high-resolution dataset.\n+\"\"\"\n\\ No newline at end of file\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
