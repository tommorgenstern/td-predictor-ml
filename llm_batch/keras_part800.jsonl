{"custom_id": "keras#68e0368c680decbc7c9e1da57b56b3a8212b3ec2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 966 | Contributors (this commit): 7 | Commits (past 90d): 3 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -107,7 +107,7 @@ def save_model(model, filepath, overwrite=True, **kwargs):\n             \"Invalid filepath extension for saving. \"\n             \"Please add either a `.keras` extension for the native Keras \"\n             f\"format (recommended) or a `.h5` extension. \"\n-            \"Use `tf.saved_model.save()` if you want to export a SavedModel \"\n+            \"Use `model.export(filepath)` if you want to export a SavedModel \"\n             \"for use with TFLite/TFServing/etc. \"\n             f\"Received: filepath={filepath}.\"\n         )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b01bb52936945d8518b655f2f826334c3352b308", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 232 | Lines Deleted: 159 | Files Changed: 2 | Hunks: 50 | Methods Changed: 45 | Complexity Δ (Sum/Max): 14/8 | Churn Δ: 391 | Churn Cumulative: 2493 | Contributors (this commit): 6 | Commits (past 90d): 17 | Contributors (cumulative): 10 | DMM Complexity: 0.45454545454545453\n\nDIFF:\n@@ -1,6 +1,8 @@\n \"\"\"Library for exporting inference-only Keras models/layers.\"\"\"\n \n import inspect\n+import itertools\n+import string\n \n from absl import logging\n \n@@ -215,10 +217,12 @@ class ExportArchive:\n                 per positional input argument of `fn`). Nested arguments are\n                 allowed (see below for an example showing a Functional model\n                 with 2 input arguments).\n-            jax2tf_kwargs: Dict for additional arguments to pass to `jax2tf`.\n+            jax2tf_kwargs: Optional. A dict for arguments to pass to `jax2tf`.\n                 Supported only when the backend is JAX. See documentation for\n                 [`jax2tf.convert`](\n                     https://github.com/google/jax/blob/main/jax/experimental/jax2tf/README.md).\n+                The values for `native_serialization` and `polymorphic_shapes`,\n+                if not provided, are automatically computed.\n \n         Returns:\n             The `tf.function` wrapping `fn` that was added to the archive.\n@@ -335,13 +339,9 @@ class ExportArchive:\n                     with StatelessScope(state_mapping=state_mapping):\n                         return fn(*args, **kwargs)\n \n-                variables_spec = [\n-                    _make_tensor_spec(v) for v in self._backend_variables\n-                ]\n-\n                 jax2tf_stateless_fn = self._convert_jax2tf_function(\n                     stateless_fn,\n-                    [variables_spec] + input_signature,\n+                    input_signature,\n                     jax2tf_kwargs=jax2tf_kwargs,\n                 )\n \n@@ -543,27 +543,51 @@ class ExportArchive:\n \n         if jax2tf_kwargs is None:\n             jax2tf_kwargs = {}\n+\n         if \"native_serialization\" not in jax2tf_kwargs:\n             jax2tf_kwargs[\"native_serialization\"] = (\n                 self._check_device_compatible()\n             )\n \n-        shapes = []\n-        for spec in input_signature:\n-            shapes.append(self._spec_to_poly_shape(spec))\n+        variables_shapes = self._to_polymorphic_shape(\n+            self._backend_variables, allow_none=False\n+        )\n+        if \"polymorphic_shapes\" in jax2tf_kwargs:\n+            input_shapes = jax2tf_kwargs[\"polymorphic_shapes\"]\n+        else:\n+            input_shapes = self._to_polymorphic_shape(input_signature)\n+        jax2tf_kwargs[\"polymorphic_shapes\"] = [variables_shapes] + input_shapes\n \n-        return jax2tf.convert(\n-            fn,\n-            polymorphic_shapes=shapes,\n-            **jax2tf_kwargs,\n+        return jax2tf.convert(fn, **jax2tf_kwargs)\n+\n+    def _to_polymorphic_shape(self, struct, allow_none=True):\n+        if allow_none:\n+            # Generates unique names: a, b, ... z, aa, ab, ... az, ba, ... zz\n+            # for unknown non-batch dims. Defined here to be scope per endpoint.\n+            dim_names = itertools.chain(\n+                string.ascii_lowercase,\n+                itertools.starmap(\n+                    lambda a, b: a + b,\n+                    itertools.product(string.ascii_lowercase, repeat=2),\n+                ),\n             )\n \n-    def _spec_to_poly_shape(self, spec):\n-        if isinstance(spec, (dict, list)):\n-            return tree.map_structure(self._spec_to_poly_shape, spec)\n-        spec_shape = spec.shape\n-        spec_shape = str(spec_shape).replace(\"None\", \"b\")\n-        return spec_shape\n+        def convert_shape(x):\n+            poly_shape = []\n+            for index, dim in enumerate(list(x.shape)):\n+                if dim is not None:\n+                    poly_shape.append(str(dim))\n+                elif not allow_none:\n+                    raise ValueError(\n+                        f\"Illegal None dimension in {x} with shape {x.shape}\"\n+                    )\n+                elif index == 0:\n+                    poly_shape.append(\"batch\")\n+                else:\n+                    poly_shape.append(next(dim_names))\n+            return \"(\" + \", \".join(poly_shape) + \")\"\n+\n+        return tree.map_structure(convert_shape, struct)\n \n     def _check_device_compatible(self):\n         from jax import default_backend as jax_device\n@@ -612,14 +636,16 @@ def _get_save_spec(model):\n         return None\n \n     if len(shapes_dict) == 1:\n-        return tf.TensorSpec(\n-            shape=list(shapes_dict.values())[0], dtype=model.input_dtype\n-        )\n+        shape = list(shapes_dict.values())[0]\n+        shape = (None,) + shape[1:]\n+        return tf.TensorSpec(shape=shape, dtype=model.input_dtype)\n \n     specs = {}\n-    for key, value in shapes_dict.items():\n+    for key, shape in shapes_dict.items():\n         key = key.rstrip(\"_shape\")\n-        specs[key] = tf.TensorSpec(shape=value, dtype=model.input_dtype)\n+        shape = (None,) + shape[1:]\n+        specs[key] = tf.TensorSpec(shape=shape, dtype=model.input_dtype)\n+\n     return specs\n \n \n@@ -758,7 +784,8 @@ class TFSMLayer(Layer):\n \n \n def _make_tensor_spec(x):\n-    return tf.TensorSpec(x.shape, dtype=x.dtype, name=x.name)\n+    shape = (None,) + x.shape[1:]\n+    return tf.TensorSpec(shape, dtype=x.dtype, name=x.name)\n \n \n def _print_signature(fn, name):\n\n@@ -5,67 +5,103 @@ import os\n import numpy as np\n import pytest\n import tensorflow as tf\n+from absl.testing import parameterized\n \n from keras import backend\n from keras import layers\n from keras import models\n+from keras import ops\n from keras import random\n from keras import testing\n from keras import utils\n from keras.export import export_lib\n from keras.saving import saving_lib\n+from keras.testing.test_utils import named_product\n+from keras.utils import tree\n \n \n-class RandomLayer(layers.Layer):\n-    def __init__(self, **kwargs):\n-        super().__init__(**kwargs)\n-        self.seed_generator = backend.random.SeedGenerator()\n+class CustomModel(models.Model):\n+    def __init__(self, layer_list):\n+        super().__init__()\n+        self.layer_list = layer_list\n \n-    def call(self, inputs):\n-        return inputs + random.uniform(inputs.shape, seed=self.seed_generator)\n+    def call(self, input):\n+        output = input\n+        for layer in self.layer_list:\n+            output = layer(output)\n+        return output\n \n \n-def get_model():\n-    layer_list = [\n+def get_model(type=\"sequential\", input_shape=(10,), layer_list=None):\n+    layer_list = layer_list or [\n         layers.Dense(10, activation=\"relu\"),\n         layers.BatchNormalization(),\n         layers.Dense(1, activation=\"sigmoid\"),\n     ]\n-    model = models.Sequential(layer_list)\n-    return model\n+    if type == \"sequential\":\n+        return models.Sequential(layer_list)\n+    elif type == \"functional\":\n+        input = output = tree.map_shape_structure(layers.Input, input_shape)\n+        for layer in layer_list:\n+            output = layer(output)\n+        return models.Model(inputs=input, outputs=output)\n+    elif type == \"subclass\":\n+        return CustomModel(layer_list)\n \n \n @pytest.mark.skipif(\n     backend.backend() not in (\"tensorflow\", \"jax\"),\n     reason=\"Export only currently supports the TF and JAX backends.\",\n )\n-class ExportArchiveTest(testing.TestCase):\n-    def test_standard_model_export(self):\n+class ExportArchiveTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.named_parameters(\n+        named_product(model_type=[\"sequential\", \"functional\", \"subclass\"])\n+    )\n+    def test_standard_model_export(self, model_type):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n-        model = get_model()\n+        model = get_model(model_type)\n         ref_input = tf.random.normal((3, 10))\n         ref_output = model(ref_input)\n \n         export_lib.export_model(model, temp_filepath)\n         revived_model = tf.saved_model.load(temp_filepath)\n-        self.assertAllClose(\n-            ref_output, revived_model.serve(ref_input), atol=1e-6\n+        self.assertAllClose(ref_output, revived_model.serve(ref_input))\n+        # Test with a different batch size\n+        revived_model.serve(tf.random.normal((6, 10)))\n+\n+    @parameterized.named_parameters(\n+        named_product(model_type=[\"sequential\", \"functional\", \"subclass\"])\n+    )\n+    def test_model_with_rng_export(self, model_type):\n+\n+        class RandomLayer(layers.Layer):\n+            def __init__(self):\n+                super().__init__()\n+                self.seed_generator = backend.random.SeedGenerator()\n+\n+            def call(self, inputs):\n+                return inputs + random.uniform(\n+                    ops.shape(inputs), seed=self.seed_generator\n                 )\n \n-    def test_model_with_rng_export(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n-        model = models.Sequential([RandomLayer()])\n+        model = get_model(model_type, layer_list=[RandomLayer()])\n         ref_input = tf.random.normal((3, 10))\n         ref_output = model(ref_input)\n \n         export_lib.export_model(model, temp_filepath)\n         revived_model = tf.saved_model.load(temp_filepath)\n         self.assertEqual(ref_output.shape, revived_model.serve(ref_input).shape)\n+        # Test with a different batch size\n+        revived_model.serve(tf.random.normal((6, 10)))\n \n-    def test_low_level_model_export(self):\n+    @parameterized.named_parameters(\n+        named_product(model_type=[\"sequential\", \"functional\", \"subclass\"])\n+    )\n+    def test_low_level_model_export(self, model_type):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n \n-        model = get_model()\n+        model = get_model(model_type)\n         ref_input = tf.random.normal((3, 10))\n         ref_output = model(ref_input)\n \n@@ -85,9 +121,9 @@ class ExportArchiveTest(testing.TestCase):\n         )\n         export_archive.write_out(temp_filepath)\n         revived_model = tf.saved_model.load(temp_filepath)\n-        self.assertAllClose(\n-            ref_output, revived_model.call(ref_input), atol=1e-6\n-        )\n+        self.assertAllClose(ref_output, revived_model.call(ref_input))\n+        # Test with a different batch size\n+        revived_model.call(tf.random.normal((6, 10)))\n \n     def test_low_level_model_export_with_alias(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n@@ -114,10 +150,49 @@ class ExportArchiveTest(testing.TestCase):\n             ),\n         )\n         self.assertAllClose(\n-            ref_output,\n-            revived_model.function_aliases[\"call_alias\"](ref_input),\n-            atol=1e-6,\n+            ref_output, revived_model.function_aliases[\"call_alias\"](ref_input)\n         )\n+        # Test with a different batch size\n+        revived_model.function_aliases[\"call_alias\"](tf.random.normal((6, 10)))\n+\n+    @parameterized.named_parameters(\n+        named_product(model_type=[\"sequential\", \"functional\", \"subclass\"])\n+    )\n+    def test_low_level_model_export_with_dynamic_dims(self, model_type):\n+\n+        class ReductionLayer(layers.Layer):\n+            def call(self, inputs):\n+                return ops.max(inputs, axis=1)\n+\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n+\n+        model = get_model(\n+            model_type,\n+            input_shape=[(None,), (None,)],\n+            layer_list=[layers.Concatenate(), ReductionLayer()],\n+        )\n+        ref_input = [tf.random.normal((3, 8)), tf.random.normal((3, 6))]\n+        ref_output = model(ref_input)\n+\n+        export_archive = export_lib.ExportArchive()\n+        export_archive.track(model)\n+        export_archive.add_endpoint(\n+            \"call\",\n+            model.__call__,\n+            input_signature=[\n+                [\n+                    tf.TensorSpec(shape=(None, None), dtype=tf.float32),\n+                    tf.TensorSpec(shape=(None, None), dtype=tf.float32),\n+                ]\n+            ],\n+        )\n+        export_archive.write_out(temp_filepath)\n+        revived_model = tf.saved_model.load(temp_filepath)\n+        self.assertAllClose(ref_output, revived_model.call(ref_input))\n+        # Test with a different batch size\n+        revived_model.call([tf.random.normal((6, 8)), tf.random.normal((6, 6))])\n+        # Test with a different batch size and different dynamic sizes\n+        revived_model.call([tf.random.normal((6, 3)), tf.random.normal((6, 5))])\n \n     @pytest.mark.skipif(\n         backend.backend() != \"jax\",\n@@ -156,15 +231,51 @@ class ExportArchiveTest(testing.TestCase):\n                 },\n             )\n         export_archive.write_out(temp_filepath)\n-        revived_model = tf.saved_model.load(\n-            temp_filepath,\n-            options=tf.saved_model.LoadOptions(\n-                experimental_load_function_aliases=True\n-            ),\n+        revived_model = tf.saved_model.load(temp_filepath)\n+        self.assertAllClose(ref_output, revived_model.call(ref_input))\n+\n+    @pytest.mark.skipif(\n+        backend.backend() != \"jax\",\n+        reason=\"This test is only for the JAX backend.\",\n     )\n-        self.assertAllClose(\n-            ref_output, revived_model.call(ref_input), atol=1e-6\n+    def test_low_level_model_export_with_jax2tf_polymorphic_shapes(self):\n+\n+        class SquareLayer(layers.Layer):\n+            def call(self, inputs):\n+                return ops.matmul(inputs, inputs)\n+\n+        temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n+\n+        model = CustomModel([SquareLayer()])\n+        ref_input = tf.random.normal((3, 10, 10))\n+        ref_output = model(ref_input)\n+        signature = [tf.TensorSpec(shape=(None, None, None), dtype=tf.float32)]\n+\n+        with self.assertRaises(TypeError):\n+            # This will fail because the polymorphic_shapes that is\n+            # automatically generated will not account for the fact that\n+            # dynamic dimensions 1 and 2 must have the same value.\n+            export_archive = export_lib.ExportArchive()\n+            export_archive.track(model)\n+            export_archive.add_endpoint(\n+                \"call\",\n+                model.__call__,\n+                input_signature=signature,\n+                jax2tf_kwargs={},\n             )\n+            export_archive.write_out(temp_filepath)\n+\n+        export_archive = export_lib.ExportArchive()\n+        export_archive.track(model)\n+        export_archive.add_endpoint(\n+            \"call\",\n+            model.__call__,\n+            input_signature=signature,\n+            jax2tf_kwargs={\"polymorphic_shapes\": [\"(batch, a, a)\"]},\n+        )\n+        export_archive.write_out(temp_filepath)\n+        revived_model = tf.saved_model.load(temp_filepath)\n+        self.assertAllClose(ref_output, revived_model.call(ref_input))\n \n     @pytest.mark.skipif(\n         backend.backend() != \"tensorflow\",\n@@ -199,9 +310,7 @@ class ExportArchiveTest(testing.TestCase):\n \n         revived_model = tf.saved_model.load(temp_filepath)\n         self.assertFalse(hasattr(revived_model, \"_tracked\"))\n-        self.assertAllClose(\n-            ref_output, revived_model.call(ref_input), atol=1e-6\n-        )\n+        self.assertAllClose(ref_output, revived_model.call(ref_input))\n         self.assertLen(revived_model.variables, 8)\n         self.assertLen(revived_model.trainable_variables, 6)\n         self.assertLen(revived_model.non_trainable_variables, 2)\n@@ -253,9 +362,7 @@ class ExportArchiveTest(testing.TestCase):\n         # Reload and verify outputs\n         revived_model = tf.saved_model.load(temp_filepath)\n         self.assertFalse(hasattr(revived_model, \"_tracked\"))\n-        self.assertAllClose(\n-            ref_output, revived_model.serve(ref_input), atol=1e-6\n-        )\n+        self.assertAllClose(ref_output, revived_model.serve(ref_input))\n         self.assertLen(revived_model.variables, 8)\n         self.assertLen(revived_model.trainable_variables, 6)\n         self.assertLen(revived_model.non_trainable_variables, 2)\n@@ -330,9 +437,7 @@ class ExportArchiveTest(testing.TestCase):\n         # Reload and verify outputs\n         revived_model = tf.saved_model.load(temp_filepath)\n         self.assertFalse(hasattr(revived_model, \"_tracked\"))\n-        self.assertAllClose(\n-            ref_output, revived_model.serve(ref_input), atol=1e-6\n-        )\n+        self.assertAllClose(ref_output, revived_model.serve(ref_input))\n         self.assertLen(revived_model.variables, 6)\n         self.assertLen(revived_model.trainable_variables, 6)\n         self.assertLen(revived_model.non_trainable_variables, 0)\n@@ -357,9 +462,7 @@ class ExportArchiveTest(testing.TestCase):\n         )\n         export_archive.write_out(temp_filepath)\n         revived_layer = tf.saved_model.load(temp_filepath)\n-        self.assertAllClose(\n-            ref_output, revived_layer.call(ref_input), atol=1e-6\n-        )\n+        self.assertAllClose(ref_output, revived_layer.call(ref_input))\n \n     def test_multi_input_output_functional_model(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n@@ -386,15 +489,11 @@ class ExportArchiveTest(testing.TestCase):\n         )\n         export_archive.write_out(temp_filepath)\n         revived_model = tf.saved_model.load(temp_filepath)\n-        self.assertAllClose(\n-            ref_outputs[0],\n-            revived_model.serve(ref_inputs)[0],\n-            atol=1e-6,\n-        )\n-        self.assertAllClose(\n-            ref_outputs[1],\n-            revived_model.serve(ref_inputs)[1],\n-            atol=1e-6,\n+        self.assertAllClose(ref_outputs[0], revived_model.serve(ref_inputs)[0])\n+        self.assertAllClose(ref_outputs[1], revived_model.serve(ref_inputs)[1])\n+        # Test with a different batch size\n+        revived_model.serve(\n+            [tf.random.normal((6, 2)), tf.random.normal((6, 2))]\n         )\n \n         # Now test dict inputs\n@@ -420,15 +519,14 @@ class ExportArchiveTest(testing.TestCase):\n         )\n         export_archive.write_out(temp_filepath)\n         revived_model = tf.saved_model.load(temp_filepath)\n-        self.assertAllClose(\n-            ref_outputs[0],\n-            revived_model.serve(ref_inputs)[0],\n-            atol=1e-6,\n-        )\n-        self.assertAllClose(\n-            ref_outputs[1],\n-            revived_model.serve(ref_inputs)[1],\n-            atol=1e-6,\n+        self.assertAllClose(ref_outputs[0], revived_model.serve(ref_inputs)[0])\n+        self.assertAllClose(ref_outputs[1], revived_model.serve(ref_inputs)[1])\n+        # Test with a different batch size\n+        revived_model.serve(\n+            {\n+                \"x1\": tf.random.normal((6, 2)),\n+                \"x2\": tf.random.normal((6, 2)),\n+            }\n         )\n \n     # def test_model_with_lookup_table(self):\n@@ -449,9 +547,7 @@ class ExportArchiveTest(testing.TestCase):\n \n     #     export_lib.export_model(model, temp_filepath)\n     #     revived_model = tf.saved_model.load(temp_filepath)\n-    #     self.assertAllClose(\n-    #         ref_output, revived_model.serve(ref_input), atol=1e-6\n-    #     )\n+    #     self.assertAllClose(ref_output, revived_model.serve(ref_input))\n \n     def test_track_multiple_layers(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n@@ -475,16 +571,8 @@ class ExportArchiveTest(testing.TestCase):\n         )\n         export_archive.write_out(temp_filepath)\n         revived_layer = tf.saved_model.load(temp_filepath)\n-        self.assertAllClose(\n-            ref_output_1,\n-            revived_layer.call_1(ref_input_1),\n-            atol=1e-6,\n-        )\n-        self.assertAllClose(\n-            ref_output_2,\n-            revived_layer.call_2(ref_input_2),\n-            atol=1e-6,\n-        )\n+        self.assertAllClose(ref_output_1, revived_layer.call_1(ref_input_1))\n+        self.assertAllClose(ref_output_2, revived_layer.call_2(ref_input_2))\n \n     def test_non_standard_layer_signature(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"exported_layer\")\n@@ -505,11 +593,7 @@ class ExportArchiveTest(testing.TestCase):\n         )\n         export_archive.write_out(temp_filepath)\n         revived_layer = tf.saved_model.load(temp_filepath)\n-        self.assertAllClose(\n-            ref_output,\n-            revived_layer.call(x1, x2),\n-            atol=1e-6,\n-        )\n+        self.assertAllClose(ref_output, revived_layer.call(x1, x2))\n \n     def test_non_standard_layer_signature_with_kwargs(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"exported_layer\")\n@@ -530,10 +614,10 @@ class ExportArchiveTest(testing.TestCase):\n         )\n         export_archive.write_out(temp_filepath)\n         revived_layer = tf.saved_model.load(temp_filepath)\n-        self.assertAllClose(\n-            ref_output,\n-            revived_layer.call(query=x1, value=x2),\n-            atol=1e-6,\n+        self.assertAllClose(ref_output, revived_layer.call(query=x1, value=x2))\n+        # Test with a different batch size\n+        revived_layer.call(\n+            query=tf.random.normal((6, 2, 2)), value=tf.random.normal((6, 2, 2))\n         )\n \n     def test_variable_collection(self):\n@@ -573,20 +657,8 @@ class ExportArchiveTest(testing.TestCase):\n             export_lib.export_model(model, temp_filepath)\n \n         # Subclassed model has not been called\n-        class MyModel(models.Model):\n-            def __init__(self, **kwargs):\n-                super().__init__(**kwargs)\n-                self.dense = layers.Dense(2)\n-\n-            def build(self, input_shape):\n-                self.dense.build(input_shape)\n-                self.built = True\n-\n-            def call(self, x):\n-                return self.dense(x)\n-\n-        model = MyModel()\n-        model.build((2, 3))\n+        model = get_model(\"subclass\")\n+        model.build((2, 10))\n         with self.assertRaisesRegex(ValueError, \"It must be called\"):\n             export_lib.export_model(model, temp_filepath)\n \n@@ -629,10 +701,7 @@ class ExportArchiveTest(testing.TestCase):\n         with self.assertRaisesRegex(\n             ValueError, \"you must provide an `input_signature`\"\n         ):\n-            export_archive.add_endpoint(\n-                \"call\",\n-                model.__call__,\n-            )\n+            export_archive.add_endpoint(\"call\", model.__call__)\n \n         # Set endpoint that has never been called\n         export_archive = export_lib.ExportArchive()\n@@ -647,33 +716,7 @@ class ExportArchiveTest(testing.TestCase):\n         with self.assertRaisesRegex(\n             ValueError, \"you must either provide a function\"\n         ):\n-            export_archive.add_endpoint(\n-                \"call\",\n-                my_endpoint,\n-            )\n-\n-    def test_subclassed_model_export(self):\n-        class CustomModelX(models.Model):\n-            def __init__(self, *args, **kwargs):\n-                super().__init__(*args, **kwargs)\n-                self.dense1 = layers.Dense(1)\n-                self.dense2 = layers.Dense(1)\n-\n-            def call(self, inputs):\n-                out = self.dense1(inputs)\n-                return self.dense2(out)\n-\n-        temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n-        x = np.random.random((100, 32))\n-        model = CustomModelX()\n-        model.compile(\n-            optimizer=\"adam\",\n-            loss=\"mse\",\n-        )\n-        ref_output = model(x)\n-        model.export(temp_filepath)\n-        revived_model = tf.saved_model.load(temp_filepath)\n-        self.assertAllClose(ref_output, revived_model.serve(x), atol=1e-6)\n+            export_archive.add_endpoint(\"call\", my_endpoint)\n \n     def test_export_no_assets(self):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n@@ -689,17 +732,20 @@ class ExportArchiveTest(testing.TestCase):\n         )\n         export_archive.write_out(temp_filepath)\n \n-    def test_model_export_method(self):\n+    @parameterized.named_parameters(\n+        named_product(model_type=[\"sequential\", \"functional\", \"subclass\"])\n+    )\n+    def test_model_export_method(self, model_type):\n         temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n-        model = get_model()\n+        model = get_model(model_type)\n         ref_input = tf.random.normal((3, 10))\n         ref_output = model(ref_input)\n \n         model.export(temp_filepath)\n         revived_model = tf.saved_model.load(temp_filepath)\n-        self.assertAllClose(\n-            ref_output, revived_model.serve(ref_input), atol=1e-6\n-        )\n+        self.assertAllClose(ref_output, revived_model.serve(ref_input))\n+        # Test with a different batch size\n+        revived_model.serve(tf.random.normal((6, 10)))\n \n \n @pytest.mark.skipif(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#04cad40272cb4cf6d754b9abc699bdd90971edca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 31 | Files Changed: 1 | Hunks: 7 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 46 | Churn Cumulative: 244 | Contributors (this commit): 3 | Commits (past 90d): 6 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -682,13 +682,20 @@ def _cudnn_gru(\n         _assert_valid_mask(mask)\n         sequence_lengths = _compute_sequence_length_from_mask(mask, time_major)\n     else:\n-        sequence_lengths = None\n+        if time_major:\n+            batch_dim = tf.shape(inputs)[1]\n+            max_sequence_length = tf.shape(inputs)[0]\n+        else:\n+            batch_dim = tf.shape(inputs)[0]\n+            max_sequence_length = tf.shape(inputs)[1]\n+        sequence_lengths = tf.fill([batch_dim], max_sequence_length)\n \n     if not time_major and sequence_lengths is None:\n         inputs = tf.transpose(inputs, perm=(1, 0, 2))\n         seq_axis, batch_axis = (0, 1)\n     else:\n         seq_axis, batch_axis = (0, 1) if time_major else (1, 0)\n+\n     # For init_h, cuDNN expects one more dim of num_layers before or after batch\n     # dim for time major or batch major inputs respectively\n     init_h = tf.expand_dims(initial_state, axis=seq_axis)\n@@ -719,7 +726,6 @@ def _cudnn_gru(\n         transpose_weights=True,\n     )\n \n-    if sequence_lengths is not None:\n     if go_backwards:\n         # Three reversals are required. E.g.,\n         # normal input = [1, 2, 3, 0, 0]  # where 0 need to be masked\n@@ -750,18 +756,6 @@ def _cudnn_gru(\n             batch_axis=batch_axis,\n         )\n         outputs = tf.reverse(outputs, axis=[seq_axis])\n-    else:\n-        if go_backwards:\n-            # Reverse axis 0 since the input is already convert to time major.\n-            inputs = tf.reverse(inputs, axis=[0])\n-        outputs, h, _, _ = tf.raw_ops.CudnnRNN(\n-            input=inputs,\n-            input_h=init_h,\n-            input_c=0,\n-            params=params,\n-            is_training=True,\n-            rnn_mode=\"gru\",\n-        )\n \n     last_output = outputs[-1]\n     if not time_major and sequence_lengths is None and return_sequences:\n@@ -880,7 +874,13 @@ def _cudnn_lstm(\n         _assert_valid_mask(mask)\n         sequence_lengths = _compute_sequence_length_from_mask(mask, time_major)\n     else:\n-        sequence_lengths = None\n+        if time_major:\n+            batch_dim = tf.shape(inputs)[1]\n+            max_sequence_length = tf.shape(inputs)[0]\n+        else:\n+            batch_dim = tf.shape(inputs)[0]\n+            max_sequence_length = tf.shape(inputs)[1]\n+        sequence_lengths = tf.fill([batch_dim], max_sequence_length)\n \n     if not time_major and sequence_lengths is None:\n         inputs = tf.transpose(inputs, perm=(1, 0, 2))\n@@ -918,7 +918,6 @@ def _cudnn_lstm(\n         transpose_weights=True,\n     )\n \n-    if sequence_lengths is not None:\n     if go_backwards:\n         # Three reversals are required. E.g.,\n         # normal input = [1, 2, 3, 0, 0]  # where 0 need to be masked\n@@ -949,21 +948,6 @@ def _cudnn_lstm(\n             batch_axis=batch_axis,\n         )\n         outputs = tf.reverse(outputs, axis=[seq_axis])\n-    else:\n-        # # Fill the array with shape [batch] with value of max timesteps.\n-        # sequence_length = array_ops.fill([array_ops.shape(inputs)[1]],\n-        #                                  array_ops.shape(inputs)[0])\n-        if go_backwards:\n-            # Reverse axis 0 since the input is already convert to time major.\n-            inputs = tf.reverse(inputs, axis=[0])\n-        outputs, h, c, _ = tf.raw_ops.CudnnRNN(\n-            input=inputs,\n-            input_h=init_h,\n-            input_c=init_c,\n-            params=params,\n-            is_training=True,\n-            rnn_mode=\"lstm\",\n-        )\n \n     last_output = outputs[-1]\n     if not time_major and sequence_lengths is None and return_sequences:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#b37be22bb1fa39816cf7751fc95de5ea9c3dbf19", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 6/4 | Churn Δ: 39 | Churn Cumulative: 1569 | Contributors (this commit): 11 | Commits (past 90d): 51 | Contributors (cumulative): 21 | DMM Complexity: 0.0\n\nDIFF:\n@@ -51,6 +51,10 @@ class JAXTrainer(base_trainer.Trainer):\n             return_losses=True,\n             **kwargs,\n         )\n+        if losses:\n+            # Make forward pass losses available to compute_loss.\n+            self._losses_override.clear()\n+            self._losses_override = losses\n \n         loss, variables = self.stateless_compute_loss(\n             trainable_variables,\n@@ -61,14 +65,12 @@ class JAXTrainer(base_trainer.Trainer):\n             y_pred=y_pred,\n             sample_weight=sample_weight,\n         )\n+        if losses:\n+            self._losses_override.clear()\n         (trainable_variables, non_trainable_variables, metrics_variables) = (\n             variables\n         )\n \n-        # Sum forward pass losses\n-        if losses:\n-            loss += ops.sum(losses)\n-\n         # Handle loss scaling\n         unscaled_loss = loss\n         if training and self.optimizer is not None:\n\n@@ -274,6 +274,7 @@ class Layer(BackendLayer, Operation):\n         self._trainable = trainable\n         self._losses = []\n         self._loss_ids = set()\n+        self._losses_override = []\n \n         self._call_signature = inspect.signature(self.call)\n         call_signature_parameters = [\n@@ -1091,6 +1092,8 @@ class Layer(BackendLayer, Operation):\n     @property\n     def losses(self):\n         \"\"\"List of scalar losses from `add_loss`, regularizers and sublayers.\"\"\"\n+        if self._losses_override:\n+            return self._losses_override\n         losses = self._get_own_losses()\n         for layer in self._flatten_layers(include_self=False):\n             losses.extend(layer._get_own_losses())\n\n@@ -1319,3 +1319,29 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         self.assertAlmostEqual(\n             history.history[\"custom\"][0], history.history[\"loss\"][0] * 4\n         )\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_fwd_pass_loss_presence_in_compute_loss(self):\n+\n+        class MyModel(keras.Model):\n+            def __init__(self):\n+                super().__init__()\n+                self.custom_metric = keras.metrics.Mean(name=\"custom\")\n+                self.dense = keras.layers.Dense(2, activity_regularizer=\"l2\")\n+\n+            def call(self, x):\n+                return self.dense(x)\n+\n+            def compute_loss(\n+                self, x=None, y=None, y_pred=None, sample_weight=None\n+            ):\n+                loss = super().compute_loss(x, y, y_pred, sample_weight)\n+                self.custom_metric.update_state(sum(self.losses))\n+                return loss\n+\n+        model = MyModel()\n+        model.compile(optimizer=\"sgd\", loss=\"mse\")\n+        x = np.ones((32, 4))\n+        y = np.ones((32, 2)) * 2\n+        history = model.fit(x, y)\n+        self.assertGreater(history.history[\"custom\"][0], 0.0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f77b020e497a353b644df3aeebc97c831c8057fc", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 800 | Contributors (this commit): 8 | Commits (past 90d): 24 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -7,7 +7,6 @@ import numpy as np\n \n from keras import backend\n from keras import callbacks as callbacks_module\n-from keras import ops\n from keras import optimizers as optimizers_module\n from keras.backend import distribution_lib as jax_distribution_lib\n from keras.distribution import distribution_lib\n\n@@ -967,6 +967,7 @@ class Trainer:\n                 y,\n                 sample_weight,\n             ) = data_adapter_utils.unpack_x_y_sample_weight(data_batch)\n+\n             # Build all model state with `backend.compute_output_spec`.\n             try:\n                 y_pred = backend.compute_output_spec(self, x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8f05c11b274590be5000cb325d272b9898bfec9f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 40 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 44 | Churn Cumulative: 465 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -294,8 +294,43 @@ def plot_nested_functional_model():\n     )\n \n \n+def plot_functional_model_with_splits_and_merges():\n+    class SplitLayer(keras.Layer):\n+        def call(self, x):\n+            return list(keras.ops.split(x, 2, axis=1))\n+\n+    class ConcatLayer(keras.Layer):\n+        def call(self, xs):\n+            return keras.ops.concatenate(xs, axis=1)\n+\n+    inputs = keras.Input((2,))\n+    a, b = SplitLayer()(inputs)\n+\n+    a = keras.layers.Dense(2)(a)\n+    b = keras.layers.Dense(2)(b)\n+\n+    outputs = ConcatLayer()([a, b])\n+    model = keras.Model(inputs, outputs)\n+\n+    plot_model(model, \"split-functional.png\", expand_nested=True)\n+    plot_model(\n+        model,\n+        \"split-functional-show_shapes.png\",\n+        show_shapes=True,\n+        expand_nested=True,\n+    )\n+    plot_model(\n+        model,\n+        \"split-functional-show_shapes-show_dtype.png\",\n+        show_shapes=True,\n+        show_dtype=True,\n+        expand_nested=True,\n+    )\n+\n+\n if __name__ == \"__main__\":\n     plot_sequential_model()\n     plot_functional_model()\n     plot_subclassed_model()\n     plot_nested_functional_model()\n+    plot_functional_model_with_splits_and_merges()\n\n@@ -5,6 +5,7 @@ import sys\n \n from keras.api_export import keras_export\n from keras.utils import io_utils\n+from keras.utils import tree\n \n try:\n     # pydot-ng is a fork of pydot that is better maintained.\n@@ -105,8 +106,8 @@ def make_layer_label(layer, **kwargs):\n     if show_shapes:\n         shape = None\n         try:\n-            shape = layer.output.shape\n-        except ValueError:\n+            shape = tree.map_structure(lambda x: x.shape, layer.output)\n+        except (ValueError, AttributeError):\n             pass\n         cols.append(\n             (\n@@ -118,8 +119,8 @@ def make_layer_label(layer, **kwargs):\n     if show_dtype:\n         dtype = None\n         try:\n-            dtype = layer.output.dtype\n-        except ValueError:\n+            dtype = tree.map_structure(lambda x: x.dtype, layer.output)\n+        except (ValueError, AttributeError):\n             pass\n         cols.append(\n             (\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f7bc67e6c105c116a2ba7f5412137acf78174b1a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 709 | Contributors (this commit): 3 | Commits (past 90d): 7 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -448,7 +448,7 @@ class FlaxLayer(JaxLayer):\n \n     This layer enables the use of Flax components in the form of\n     [`flax.linen.Module`](\n-            https://flax.readthedocs.io/en/latest/flax.linen.html#module)\n+        https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)\n     instances within Keras when using JAX as the backend for Keras.\n \n     The module method to use for the forward pass can be specified via the\n@@ -464,7 +464,7 @@ class FlaxLayer(JaxLayer):\n     `FlaxLayer` handles the non-trainable state of your model and required RNGs\n     automatically. Note that the `mutable` parameter of\n     [`flax.linen.Module.apply()`](\n-      https://flax.readthedocs.io/en/latest/flax.linen.html#flax.linen.Module.apply)\n+        https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.apply)\n     is set to `DenyList([\"params\"])`, therefore making the assumption that all\n     the variables outside of the \"params\" collection are non-trainable weights.\n \n@@ -521,10 +521,10 @@ class FlaxLayer(JaxLayer):\n             must take the `Module` as the first argument. It is used for both\n             `Module.init` and `Module.apply`. Details are documented in the\n             `method` argument of [`flax.linen.Module.apply()`](\n-              https://flax.readthedocs.io/en/latest/flax.linen.html#flax.linen.Module.apply).\n+              https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.apply).\n         variables: A `dict` containing all the variables of the module in the\n             same format as what is returned by [`flax.linen.Module.init()`](\n-              https://flax.readthedocs.io/en/latest/flax.linen.html#flax.linen.Module.init).\n+              https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html#flax.linen.init).\n             It should contain a \"params\" key and, if applicable, other keys for\n             collections of variables for non-trainable state. This allows\n             passing trained parameters and learned non-trainable state or\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#504716cb71973d4d4e485eb1724a3c4d3b621a69", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 10 | Files Changed: 3 | Hunks: 8 | Methods Changed: 3 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 21 | Churn Cumulative: 616 | Contributors (this commit): 5 | Commits (past 90d): 11 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,5 +1,6 @@\n import copy\n import inspect\n+import typing\n import warnings\n \n from keras import backend\n@@ -94,6 +95,9 @@ class Functional(Function, Model):\n             trainable.\n     \"\"\"\n \n+    def __new__(cls, *args, **kwargs):\n+        return typing.cast(Functional, super().__new__(cls))\n+\n     @tracking.no_automatic_dependency_tracking\n     def __init__(self, inputs, outputs, name=None, **kwargs):\n         if isinstance(inputs, dict):\n\n@@ -1,5 +1,6 @@\n import inspect\n import json\n+import typing\n import warnings\n \n from keras import backend\n@@ -27,7 +28,7 @@ else:\n \n \n @keras_export([\"keras.Model\", \"keras.models.Model\"])\n-class Model(Trainer, Layer):\n+class Model(Trainer, base_trainer.Trainer, Layer):\n     \"\"\"A model grouping layers into an object with training/inference features.\n \n     There are three ways to instantiate a `Model`:\n@@ -138,7 +139,7 @@ class Model(Trainer, Layer):\n             from keras.models import functional\n \n             return functional.Functional(*args, **kwargs)\n-        return super().__new__(cls)\n+        return typing.cast(Model, super().__new__(cls))\n \n     def __init__(self, *args, **kwargs):\n         Trainer.__init__(self)\n@@ -599,11 +600,3 @@ def inject_functional_model_class(cls):\n     cls.__new__(cls)\n \n     return cls\n-\n-\n-Model.fit.__doc__ = base_trainer.Trainer.fit.__doc__\n-Model.predict.__doc__ = base_trainer.Trainer.predict.__doc__\n-Model.evaluate.__doc__ = base_trainer.Trainer.evaluate.__doc__\n-Model.train_on_batch.__doc__ = base_trainer.Trainer.train_on_batch.__doc__\n-Model.test_on_batch.__doc__ = base_trainer.Trainer.test_on_batch.__doc__\n-Model.predict_on_batch.__doc__ = base_trainer.Trainer.predict_on_batch.__doc__\n\n@@ -1,5 +1,6 @@\n import copy\n import inspect\n+import typing\n \n from keras.api_export import keras_export\n from keras.backend.common import global_state\n@@ -60,6 +61,9 @@ class Sequential(Model):\n     ```\n     \"\"\"\n \n+    def __new__(cls, *args, **kwargs):\n+        return typing.cast(Sequential, super().__new__(cls))\n+\n     def __init__(self, layers=None, trainable=True, name=None):\n         super().__init__(trainable=trainable, name=name)\n         self._functional = None\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#30659522780ea2b5f7cf4f359103154087fd4882", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 67 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 67 | Churn Cumulative: 293 | Contributors (this commit): 10 | Commits (past 90d): 8 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -12,6 +12,73 @@ from keras.utils.naming import auto_name\n \n \n class KerasVariable:\n+    \"\"\"Represents a backend-agnostic variable in Keras.\n+\n+    A `Variable` acts as a container for state. It holds a tensor value and can\n+     be updated. With the JAX backend, variables are used to implement\n+    'functionalization', the pattern of lifting stateful operations out of\n+     a piece of computation to turn it into a stateless function.\n+\n+    Args:\n+        initializer: Initial value or callable for initialization. Requires\n+            `shape` if a callable is used.\n+        shape: Optional. Tuple for the variable's shape, needed if initializer\n+            is callable.\n+        dtype: Optional. Data type of the variable, defaults to Keras's float\n+            type.\n+        trainable: Optional. Boolean indicating if variable is trainable.\n+            Defaults to True.\n+        name: Optional. A unique name for the variable. Automatically generated\n+            if not set.\n+\n+    Attributes:\n+        name: The name of the variable (string).\n+        path: The path of the variable within the Keras model or layer (string).\n+        dtype: The data type of the variable (string).\n+        shape: The shape of the variable (tuple of integers).\n+        ndim: The number of dimensions of the variable (integer).\n+        trainable: Whether the variable is trainable (boolean).\n+        value: The current value of the variable (NumPy array or tensor).\n+\n+    Examples:\n+\n+\n+    # Initializing a KerasVariable with a numpy array\n+    >>> import numpy as np\n+    >>> from keras.backend.common.variables import KerasVariable\n+    >>> initial_array = np.ones((3, 3), dtype='float32')\n+    >>> variable_from_array = KerasVariable(initializer=initial_array)\n+    >>> print(variable_from_array.value)\n+    ... [[1. 1. 1.]\n+    ... [1. 1. 1.]\n+    ... [1. 1. 1.]]\n+\n+    # Using a Keras initializer to create a variable\n+    >>> from keras.initializers import Ones\n+    >>> variable_from_initializer = KerasVariable(\n+        initializer=Ones(), shape=(3, 3), dtype=\"float32\"\n+    )\n+    >>> print(variable_from_initializer.value)\n+    ... [[1. 1. 1.]\n+    ... [1. 1. 1.]\n+    ... [1. 1. 1.]]\n+\n+    # Updating the value of a variable\n+    >>> new_value = np.zeros((3, 3), dtype='float32')\n+    >>> variable_from_array.assign(new_value)\n+    >>> print(variable_from_array.value)\n+    ... [[0. 0. 0.]\n+    ... [0. 0. 0.]\n+    ... [0. 0. 0.]]\n+\n+    # Marking a variable as non-trainable\n+    >>> non_trainable_variable = KerasVariable(\n+            initializer=np.ones((3, 3), dtype=\"float32\"), trainable=False\n+        )\n+    >>> print(non_trainable_variable.trainable)\n+    False\n+    \"\"\"\n+\n     def __init__(\n         self,\n         initializer,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#71d98233b30b8d7005db56def301d5554de264ea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 35 | Files Changed: 1 | Hunks: 10 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 68 | Churn Cumulative: 361 | Contributors (this commit): 10 | Commits (past 90d): 9 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -16,18 +16,19 @@ class KerasVariable:\n \n     A `Variable` acts as a container for state. It holds a tensor value and can\n     be updated. With the JAX backend, variables are used to implement\n-    'functionalization', the pattern of lifting stateful operations out of\n+    \"functionalization\", the pattern of lifting stateful operations out of\n     a piece of computation to turn it into a stateless function.\n \n     Args:\n-        initializer: Initial value or callable for initialization. Requires\n-            `shape` if a callable is used.\n-        shape: Optional. Tuple for the variable's shape, needed if initializer\n-            is callable.\n-        dtype: Optional. Data type of the variable, defaults to Keras's float\n-            type.\n+        initializer: Initial value or callable for initialization.\n+            If a callable is used, it should take the arguments\n+            `shape` and `dtype`.\n+        shape: Optional. Tuple for the variable's shape. Required if `initializer`\n+            is a callable.\n+        dtype: Optional. Data type of the variable. Defaults to the global float\n+            dtype type (`\"float32\"` if never configured).\n         trainable: Optional. Boolean indicating if variable is trainable.\n-            Defaults to True.\n+            Defaults to `True`.\n         name: Optional. A unique name for the variable. Automatically generated\n             if not set.\n \n@@ -42,41 +43,38 @@ class KerasVariable:\n \n     Examples:\n \n+    **Initializing a `Variable` with a NumPy array:**\n \n-    # Initializing a KerasVariable with a numpy array\n-    >>> import numpy as np\n-    >>> from keras.backend.common.variables import KerasVariable\n-    >>> initial_array = np.ones((3, 3), dtype='float32')\n-    >>> variable_from_array = KerasVariable(initializer=initial_array)\n-    >>> print(variable_from_array.value)\n-    ... [[1. 1. 1.]\n-    ... [1. 1. 1.]\n-    ... [1. 1. 1.]]\n+    ```python\n+    import numpy as np\n+    import keras\n+    initial_array = np.ones((3, 3))\n+    variable_from_array = keras.Variable(initializer=initial_array)\n+    ```\n \n-    # Using a Keras initializer to create a variable\n-    >>> from keras.initializers import Ones\n-    >>> variable_from_initializer = KerasVariable(\n+    **Using a Keras initializer to create a `Variable`:**\n+\n+    ```python\n+    from keras.initializers import Ones\n+    variable_from_initializer = keras.Variable(\n         initializer=Ones(), shape=(3, 3), dtype=\"float32\"\n     )\n-    >>> print(variable_from_initializer.value)\n-    ... [[1. 1. 1.]\n-    ... [1. 1. 1.]\n-    ... [1. 1. 1.]]\n+    ```\n \n-    # Updating the value of a variable\n-    >>> new_value = np.zeros((3, 3), dtype='float32')\n-    >>> variable_from_array.assign(new_value)\n-    >>> print(variable_from_array.value)\n-    ... [[0. 0. 0.]\n-    ... [0. 0. 0.]\n-    ... [0. 0. 0.]]\n+    **Updating the value of a `Variable`:**\n \n-    # Marking a variable as non-trainable\n-    >>> non_trainable_variable = KerasVariable(\n+    ```python\n+    new_value = np.zeros((3, 3), dtype=\"float32\")\n+    variable_from_array.assign(new_value)\n+    ```\n+\n+    **Marking a `Variable` as non-trainable:**\n+\n+    ```python\n+    non_trainable_variable = keras.Variable(\n         initializer=np.ones((3, 3), dtype=\"float32\"), trainable=False\n     )\n-    >>> print(non_trainable_variable.trainable)\n-    False\n+    ```\n     \"\"\"\n \n     def __init__(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
