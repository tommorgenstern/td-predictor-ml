{"custom_id": "scrapy#5f2827efe7b069e514a87bd0f7a9589f49f0a97a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 4 | Churn Cumulative: 314 | Contributors (this commit): 15 | Commits (past 90d): 13 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -37,8 +37,10 @@ class HttpCompressionMiddleware:\n     \"\"\"This middleware allows compressed (gzip, deflate) traffic to be\n     sent/received from web sites\"\"\"\n \n-    def __init__(self, *, crawler=None):\n+    def __init__(self, stats=None, *, crawler=None):\n         if not crawler:\n+            if stats:\n+                self.stats = stats\n             return\n         self.stats = crawler.stats\n         self._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6c278e1862c5453ec45a8f6a5c2472710895cad9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 16 | Files Changed: 2 | Hunks: 11 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 34 | Churn Cumulative: 410 | Contributors (this commit): 15 | Commits (past 90d): 8 | Contributors (cumulative): 16 | DMM Complexity: 0.5\n\nDIFF:\n@@ -1,6 +1,5 @@\n import zlib\n from io import BytesIO\n-from typing import List\n \n try:\n     import brotli\n@@ -21,7 +20,7 @@ def _inflate(data: bytes, *, max_size: int = 0) -> bytes:\n     decompressor = zlib.decompressobj()\n     raw_decompressor = zlib.decompressobj(wbits=-15)\n     input_stream = BytesIO(data)\n-    output_list: List[bytes] = []\n+    output_stream = BytesIO()\n     output_chunk = b\".\"\n     decompressed_size = 0\n     CHUNK_SIZE = 8196\n@@ -47,14 +46,15 @@ def _inflate(data: bytes, *, max_size: int = 0) -> bytes:\n                 f\"({decompressed_size} B) exceed the specified maximum \"\n                 f\"({max_size} B).\"\n             )\n-        output_list.append(output_chunk)\n-    return b\"\".join(output_list)\n+        output_stream.write(output_chunk)\n+    output_stream.seek(0)\n+    return output_stream.read()\n \n \n def _unbrotli(data: bytes, *, max_size: int = 0) -> bytes:\n     decompressor = brotli.Decompressor()\n     input_stream = BytesIO(data)\n-    output_list: List[bytes] = []\n+    output_stream = BytesIO()\n     output_chunk = b\".\"\n     decompressed_size = 0\n     CHUNK_SIZE = 8196\n@@ -68,14 +68,15 @@ def _unbrotli(data: bytes, *, max_size: int = 0) -> bytes:\n                 f\"({decompressed_size} B) exceed the specified maximum \"\n                 f\"({max_size} B).\"\n             )\n-        output_list.append(output_chunk)\n-    return b\"\".join(output_list)\n+        output_stream.write(output_chunk)\n+    output_stream.seek(0)\n+    return output_stream.read()\n \n \n def _unzstd(data: bytes, *, max_size: int = 0) -> bytes:\n     decompressor = zstandard.ZstdDecompressor()\n     stream_reader = decompressor.stream_reader(BytesIO(data))\n-    output_list: List[bytes] = []\n+    output_stream = BytesIO()\n     output_chunk = b\".\"\n     decompressed_size = 0\n     CHUNK_SIZE = 8196\n@@ -88,5 +89,6 @@ def _unzstd(data: bytes, *, max_size: int = 0) -> bytes:\n                 f\"({decompressed_size} B) exceed the specified maximum \"\n                 f\"({max_size} B).\"\n             )\n-        output_list.append(output_chunk)\n-    return b\"\".join(output_list)\n+        output_stream.write(output_chunk)\n+    output_stream.seek(0)\n+    return output_stream.read()\n\n@@ -1,7 +1,6 @@\n import struct\n from gzip import GzipFile\n from io import BytesIO\n-from typing import List\n \n from scrapy.http import Response\n \n@@ -14,7 +13,7 @@ def gunzip(data: bytes, *, max_size: int = 0) -> bytes:\n     This is resilient to CRC checksum errors.\n     \"\"\"\n     f = GzipFile(fileobj=BytesIO(data))\n-    output_list: List[bytes] = []\n+    output_stream = BytesIO()\n     chunk = b\".\"\n     decompressed_size = 0\n     while chunk:\n@@ -23,8 +22,8 @@ def gunzip(data: bytes, *, max_size: int = 0) -> bytes:\n         except (OSError, EOFError, struct.error):\n             # complete only if there is some data, otherwise re-raise\n             # see issue 87 about catching struct.error\n-            # some pages are quite small so output_list is empty\n-            if output_list:\n+            # some pages are quite small so output_stream is empty\n+            if output_stream:\n                 break\n             raise\n         decompressed_size += len(chunk)\n@@ -34,8 +33,9 @@ def gunzip(data: bytes, *, max_size: int = 0) -> bytes:\n                 f\"({decompressed_size} B) exceed the specified maximum \"\n                 f\"({max_size} B).\"\n             )\n-        output_list.append(chunk)\n-    return b\"\".join(output_list)\n+        output_stream.write(chunk)\n+    output_stream.seek(0)\n+    return output_stream.read()\n \n \n def gzip_magic_number(response: Response) -> bool:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#62398e424c0a3d66d0bdd3908e47284cbe797ef9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 8 | Files Changed: 2 | Hunks: 9 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 16 | Churn Cumulative: 426 | Contributors (this commit): 15 | Commits (past 90d): 10 | Contributors (cumulative): 16 | DMM Complexity: 0.3333333333333333\n\nDIFF:\n@@ -12,6 +12,9 @@ except ImportError:\n     pass\n \n \n+_CHUNK_SIZE = 65536  # 64 KiB\n+\n+\n class _DecompressionMaxSizeExceeded(ValueError):\n     pass\n \n@@ -23,9 +26,8 @@ def _inflate(data: bytes, *, max_size: int = 0) -> bytes:\n     output_stream = BytesIO()\n     output_chunk = b\".\"\n     decompressed_size = 0\n-    CHUNK_SIZE = 8196\n     while output_chunk:\n-        input_chunk = input_stream.read(CHUNK_SIZE)\n+        input_chunk = input_stream.read(_CHUNK_SIZE)\n         try:\n             output_chunk = decompressor.decompress(input_chunk)\n         except zlib.error:\n@@ -57,9 +59,8 @@ def _unbrotli(data: bytes, *, max_size: int = 0) -> bytes:\n     output_stream = BytesIO()\n     output_chunk = b\".\"\n     decompressed_size = 0\n-    CHUNK_SIZE = 8196\n     while output_chunk:\n-        input_chunk = input_stream.read(CHUNK_SIZE)\n+        input_chunk = input_stream.read(_CHUNK_SIZE)\n         output_chunk = decompressor.process(input_chunk)\n         decompressed_size += len(output_chunk)\n         if max_size and decompressed_size > max_size:\n@@ -79,9 +80,8 @@ def _unzstd(data: bytes, *, max_size: int = 0) -> bytes:\n     output_stream = BytesIO()\n     output_chunk = b\".\"\n     decompressed_size = 0\n-    CHUNK_SIZE = 8196\n     while output_chunk:\n-        output_chunk = stream_reader.read(CHUNK_SIZE)\n+        output_chunk = stream_reader.read(_CHUNK_SIZE)\n         decompressed_size += len(output_chunk)\n         if max_size and decompressed_size > max_size:\n             raise _DecompressionMaxSizeExceeded(\n\n@@ -4,7 +4,7 @@ from io import BytesIO\n \n from scrapy.http import Response\n \n-from ._compression import _DecompressionMaxSizeExceeded\n+from ._compression import _CHUNK_SIZE, _DecompressionMaxSizeExceeded\n \n \n def gunzip(data: bytes, *, max_size: int = 0) -> bytes:\n@@ -18,7 +18,7 @@ def gunzip(data: bytes, *, max_size: int = 0) -> bytes:\n     decompressed_size = 0\n     while chunk:\n         try:\n-            chunk = f.read1(8196)\n+            chunk = f.read1(_CHUNK_SIZE)\n         except (OSError, EOFError, struct.error):\n             # complete only if there is some data, otherwise re-raise\n             # see issue 87 about catching struct.error\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8a73c6c90c5984292d39a0a9d4be86e792d81cb4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 3 | Churn Cumulative: 317 | Contributors (this commit): 15 | Commits (past 90d): 14 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -39,8 +39,9 @@ class HttpCompressionMiddleware:\n \n     def __init__(self, stats=None, *, crawler=None):\n         if not crawler:\n-            if stats:\n             self.stats = stats\n+            self._max_size = 1073741824\n+            self._warn_size = 33554432\n             return\n         self.stats = crawler.stats\n         self._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a113208a0643263fdd7198238bf9213f9148bc1a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 302 | Contributors (this commit): 15 | Commits (past 90d): 5 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -23,7 +23,7 @@ def gunzip(data: bytes, *, max_size: int = 0) -> bytes:\n             # complete only if there is some data, otherwise re-raise\n             # see issue 87 about catching struct.error\n             # some pages are quite small so output_stream is empty\n-            if output_stream:\n+            if output_stream.getbuffer().nbytes > 0:\n                 break\n             raise\n         decompressed_size += len(chunk)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c947f51077e1ab246f30764cc4cc7a1cc5835d40", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 9 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 5 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 16 | Churn Cumulative: 1119 | Contributors (this commit): 30 | Commits (past 90d): 4 | Contributors (cumulative): 42 | DMM Complexity: 0.0\n\nDIFF:\n@@ -40,10 +40,10 @@ def xmliter(\n     \"\"\"\n     nodename_patt = re.escape(nodename)\n \n-    DOCUMENT_HEADER_RE = re.compile(r\"<\\?xml[^>]+>\\s*\", re.S)\n+    DOCUMENT_HEADER_RE = re.compile(r\"<\\?xml[^>]{1,1024}>\\s*\", re.S)\n     HEADER_END_RE = re.compile(rf\"<\\s*/{nodename_patt}\\s*>\", re.S)\n-    END_TAG_RE = re.compile(r\"<\\s*/([^\\s>]+)\\s*>\", re.S)\n-    NAMESPACE_RE = re.compile(r\"((xmlns[:A-Za-z]*)=[^>\\s]+)\", re.S)\n+    END_TAG_RE = re.compile(r\"<\\s*/([^\\s>]{1,1024})\\s*>\", re.S)\n+    NAMESPACE_RE = re.compile(r\"((xmlns[:A-Za-z]{,1024})=[^>\\s]+)\", re.S)\n     text = _body_or_str(obj)\n \n     document_header_match = re.search(DOCUMENT_HEADER_RE, text)\n@@ -57,13 +57,15 @@ def xmliter(\n         for tagname in reversed(re.findall(END_TAG_RE, header_end)):\n             assert header_end_idx\n             tag = re.search(\n-                rf\"<\\s*{tagname}.*?xmlns[:=][^>]*>\", text[: header_end_idx[1]], re.S\n+                rf\"<\\s*{tagname}.{{,1024}}?xmlns[:=][^>]{{,1024}}>\",\n+                text[: header_end_idx[1]],\n+                re.S,\n             )\n             if tag:\n                 for x in re.findall(NAMESPACE_RE, tag.group()):\n                     namespaces[x[1]] = x[0]\n \n-    r = re.compile(rf\"<{nodename_patt}[\\s>].*?</{nodename_patt}>\", re.DOTALL)\n+    r = re.compile(rf\"<{nodename_patt}[\\s>].{{,1024}}?</{nodename_patt}>\", re.DOTALL)\n     for match in r.finditer(text):\n         nodetext = (\n             document_header\n\n@@ -91,8 +91,8 @@ def open_in_browser(\n     if isinstance(response, HtmlResponse):\n         if b\"<base\" not in body:\n             repl = rf'\\1<base href=\"{response.url}\">'\n-            body = re.sub(b\"<!--.*?-->\", b\"\", body, flags=re.DOTALL)\n-            body = re.sub(rb\"(<head(?:>|\\s.*?>))\", to_bytes(repl), body)\n+            body = re.sub(b\"<!--.{,1024}?-->\", b\"\", body, flags=re.DOTALL)\n+            body = re.sub(rb\"(<head(?:>|\\s.{,1024}?>))\", to_bytes(repl), body)\n         ext = \".html\"\n     elif isinstance(response, TextResponse):\n         ext = \".txt\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eb8b2c5197f3dd8570bad1945b23886ee57d045f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 17 | Churn Cumulative: 511 | Contributors (this commit): 18 | Commits (past 90d): 3 | Contributors (cumulative): 18 | DMM Complexity: 1.0\n\nDIFF:\n@@ -81,8 +81,21 @@ def open_in_browser(\n     ],\n     _openfunc: Callable[[str], Any] = webbrowser.open,\n ) -> Any:\n-    \"\"\"Open the given response in a local web browser, populating the <base>\n-    tag for external links to work\n+    \"\"\"Open *response* in a local web browser, adjusting the `base tag`_ for\n+    external links to work, e.g. so that images and styles are displayed.\n+\n+    .. _base tag: https://www.w3schools.com/tags/tag_base.asp\n+\n+    For example:\n+\n+    .. code-block:: python\n+\n+        from scrapy.utils.response import open_in_browser\n+\n+\n+        def parse_details(self, response):\n+            if \"item name\" not in response.body:\n+                open_in_browser(response)\n     \"\"\"\n     from scrapy.http import HtmlResponse, TextResponse\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c66b51770637d3d72347ab41a201f672618aeaf2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 3 | Churn Cumulative: 1122 | Contributors (this commit): 43 | Commits (past 90d): 6 | Contributors (cumulative): 54 | DMM Complexity: 1.0\n\nDIFF:\n@@ -91,8 +91,6 @@ def requires_uvloop(request):\n         pytest.skip(\"uvloop does not support Windows\")\n     if twisted_version == Version(\"twisted\", 21, 2, 0):\n         pytest.skip(\"https://twistedmatrix.com/trac/ticket/10106\")\n-    if sys.version_info >= (3, 12):\n-        pytest.skip(\"uvloop doesn't support Python 3.12 yet\")\n \n \n def pytest_configure(config):\n\n@@ -63,6 +63,7 @@ setup(\n         \"Programming Language :: Python :: 3.10\",\n         \"Programming Language :: Python :: 3.11\",\n         \"Programming Language :: Python :: 3.12\",\n+        \"Programming Language :: Python :: 3.13\",\n         \"Programming Language :: Python :: Implementation :: CPython\",\n         \"Programming Language :: Python :: Implementation :: PyPy\",\n         \"Topic :: Internet :: WWW/HTTP\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#bb74badd1bd66c59a63268c52342507c76d290b8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 329 | Contributors (this commit): 15 | Commits (past 90d): 15 | Contributors (cumulative): 15 | DMM Complexity: None\n\nDIFF:\n@@ -61,12 +61,12 @@ class HttpCompressionMiddleware:\n                 \"reimplement their 'from_crawler' method.\",\n                 ScrapyDeprecationWarning,\n             )\n-            spider = cls()\n-            spider.stats = crawler.stats\n-            spider._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\n-            spider._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")\n-            crawler.signals.connect(spider.open_spider, signals.spider_opened)\n-            return spider\n+            mw = cls()\n+            mw.stats = crawler.stats\n+            mw._max_size = crawler.settings.getint(\"DOWNLOAD_MAXSIZE\")\n+            mw._warn_size = crawler.settings.getint(\"DOWNLOAD_WARNSIZE\")\n+            crawler.signals.connect(mw.open_spider, signals.spider_opened)\n+            return mw\n \n     def open_spider(self, spider):\n         if hasattr(spider, \"download_maxsize\"):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1533b69032e2fb5e495e88a3fed57c0d98502612", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 39 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 42 | Churn Cumulative: 878 | Contributors (this commit): 19 | Commits (past 90d): 6 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -103,9 +103,9 @@ def open_in_browser(\n     body = response.body\n     if isinstance(response, HtmlResponse):\n         if b\"<base\" not in body:\n-            repl = rf'\\1<base href=\"{response.url}\">'\n-            body = re.sub(b\"<!--.{,1024}?-->\", b\"\", body, flags=re.DOTALL)\n-            body = re.sub(rb\"(<head(?:>|\\s.{,1024}?>))\", to_bytes(repl), body)\n+            repl = rf'\\0<base href=\"{response.url}\">'\n+            body = re.sub(b\"(?s)<!--.*?(?:-->|$)\", b\"\", body)\n+            body = re.sub(rb\"<head(?:[^<>]*?>)\", to_bytes(repl), body, count=1)\n         ext = \".html\"\n     elif isinstance(response, TextResponse):\n         ext = \".txt\"\n\n@@ -1,10 +1,12 @@\n import unittest\n import warnings\n from pathlib import Path\n+from time import process_time\n from urllib.parse import urlparse\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import HtmlResponse, Response, TextResponse\n+from scrapy.settings.default_settings import DOWNLOAD_MAXSIZE\n from scrapy.utils.python import to_bytes\n from scrapy.utils.response import (\n     get_base_url,\n@@ -198,3 +200,37 @@ class ResponseUtilsTest(unittest.TestCase):\n         assert open_in_browser(\n             r5, _openfunc=check_base_url\n         ), \"Inject unique base url with conditional comment\"\n+\n+    def test_open_in_browser_redos_comment(self):\n+        MAX_CPU_TIME = 30\n+\n+        # Exploit input from\n+        # https://makenowjust-labs.github.io/recheck/playground/\n+        # for /<!--.*?-->/ (old pattern to remove comments).\n+        body = b\"-><!--\\x00\" * (int(DOWNLOAD_MAXSIZE / 7) - 10) + b\"->\\n<!---->\"\n+\n+        response = HtmlResponse(\"https://example.com\", body=body)\n+\n+        start_time = process_time()\n+\n+        open_in_browser(response, lambda url: True)\n+\n+        end_time = process_time()\n+        self.assertLess(end_time - start_time, MAX_CPU_TIME)\n+\n+    def test_open_in_browser_redos_head(self):\n+        MAX_CPU_TIME = 15\n+\n+        # Exploit input from\n+        # https://makenowjust-labs.github.io/recheck/playground/\n+        # for /(<head(?:>|\\s.*?>))/ (old pattern to find the head element).\n+        body = b\"<head\\t\" * int(DOWNLOAD_MAXSIZE / 6)\n+\n+        response = HtmlResponse(\"https://example.com\", body=body)\n+\n+        start_time = process_time()\n+\n+        open_in_browser(response, lambda url: True)\n+\n+        end_time = process_time()\n+        self.assertLess(end_time - start_time, MAX_CPU_TIME)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b9c4ee26d7816b483b6b52ad1e74c3d8d9e19c36", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 147 | Files Changed: 2 | Hunks: 2 | Methods Changed: 12 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 147 | Churn Cumulative: 392 | Contributors (this commit): 14 | Commits (past 90d): 4 | Contributors (cumulative): 19 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,94 +0,0 @@\n-\"\"\" This module implements the DecompressionMiddleware which tries to recognise\n-and extract the potentially compressed responses that may arrive.\n-\"\"\"\n-\n-import bz2\n-import gzip\n-import logging\n-import tarfile\n-import zipfile\n-from io import BytesIO\n-from tempfile import mktemp\n-from warnings import warn\n-\n-from scrapy.exceptions import ScrapyDeprecationWarning\n-from scrapy.responsetypes import responsetypes\n-\n-warn(\n-    \"scrapy.downloadermiddlewares.decompression is deprecated\",\n-    ScrapyDeprecationWarning,\n-    stacklevel=2,\n-)\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-class DecompressionMiddleware:\n-    \"\"\"This middleware tries to recognise and extract the possibly compressed\n-    responses that may arrive.\"\"\"\n-\n-    def __init__(self):\n-        self._formats = {\n-            \"tar\": self._is_tar,\n-            \"zip\": self._is_zip,\n-            \"gz\": self._is_gzip,\n-            \"bz2\": self._is_bzip2,\n-        }\n-\n-    def _is_tar(self, response):\n-        archive = BytesIO(response.body)\n-        try:\n-            tar_file = tarfile.open(name=mktemp(), fileobj=archive)\n-        except tarfile.ReadError:\n-            return\n-\n-        body = tar_file.extractfile(tar_file.members[0]).read()\n-        respcls = responsetypes.from_args(filename=tar_file.members[0].name, body=body)\n-        return response.replace(body=body, cls=respcls)\n-\n-    def _is_zip(self, response):\n-        archive = BytesIO(response.body)\n-        try:\n-            zip_file = zipfile.ZipFile(archive)\n-        except zipfile.BadZipFile:\n-            return\n-\n-        namelist = zip_file.namelist()\n-        body = zip_file.read(namelist[0])\n-        respcls = responsetypes.from_args(filename=namelist[0], body=body)\n-        return response.replace(body=body, cls=respcls)\n-\n-    def _is_gzip(self, response):\n-        archive = BytesIO(response.body)\n-        try:\n-            body = gzip.GzipFile(fileobj=archive).read()\n-        except OSError:\n-            return\n-\n-        respcls = responsetypes.from_args(body=body)\n-        return response.replace(body=body, cls=respcls)\n-\n-    def _is_bzip2(self, response):\n-        try:\n-            body = bz2.decompress(response.body)\n-        except OSError:\n-            return\n-\n-        respcls = responsetypes.from_args(body=body)\n-        return response.replace(body=body, cls=respcls)\n-\n-    def process_response(self, request, response, spider):\n-        if not response.body:\n-            return response\n-\n-        for fmt, func in self._formats.items():\n-            new_response = func(response)\n-            if new_response:\n-                logger.debug(\n-                    \"Decompressed response with format: %(responsefmt)s\",\n-                    {\"responsefmt\": fmt},\n-                    extra={\"spider\": spider},\n-                )\n-                return new_response\n-        return response\n\n@@ -1,53 +0,0 @@\n-from unittest import TestCase, main\n-\n-from scrapy.downloadermiddlewares.decompression import DecompressionMiddleware\n-from scrapy.http import Response, XmlResponse\n-from scrapy.spiders import Spider\n-from scrapy.utils.test import assert_samelines\n-from tests import get_testdata\n-\n-\n-def _test_data(formats):\n-    uncompressed_body = get_testdata(\"compressed\", \"feed-sample1.xml\")\n-    test_responses = {}\n-    for format in formats:\n-        body = get_testdata(\"compressed\", \"feed-sample1.\" + format)\n-        test_responses[format] = Response(\"http://foo.com/bar\", body=body)\n-    return uncompressed_body, test_responses\n-\n-\n-class DecompressionMiddlewareTest(TestCase):\n-    test_formats = [\"tar\", \"xml.bz2\", \"xml.gz\", \"zip\"]\n-    uncompressed_body, test_responses = _test_data(test_formats)\n-\n-    def setUp(self):\n-        self.mw = DecompressionMiddleware()\n-        self.spider = Spider(\"foo\")\n-\n-    def test_known_compression_formats(self):\n-        for fmt in self.test_formats:\n-            rsp = self.test_responses[fmt]\n-            new = self.mw.process_response(None, rsp, self.spider)\n-            error_msg = f\"Failed {fmt}, response type {type(new).__name__}\"\n-            assert isinstance(new, XmlResponse), error_msg\n-            assert_samelines(self, new.body, self.uncompressed_body, fmt)\n-\n-    def test_plain_response(self):\n-        rsp = Response(url=\"http://test.com\", body=self.uncompressed_body)\n-        new = self.mw.process_response(None, rsp, self.spider)\n-        assert new is rsp\n-        assert_samelines(self, new.body, rsp.body)\n-\n-    def test_empty_response(self):\n-        rsp = Response(url=\"http://test.com\", body=b\"\")\n-        new = self.mw.process_response(None, rsp, self.spider)\n-        assert new is rsp\n-        assert not rsp.body\n-        assert not new.body\n-\n-    def tearDown(self):\n-        del self.mw\n-\n-\n-if __name__ == \"__main__\":\n-    main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4f72b49f975a406784779dc19ede31364f92235f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 7 | Files Changed: 2 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/5 | Churn Δ: 28 | Churn Cumulative: 1551 | Contributors (this commit): 29 | Commits (past 90d): 4 | Contributors (cumulative): 44 | DMM Complexity: 0.0\n\nDIFF:\n@@ -12,11 +12,14 @@ from typing import (\n     List,\n     Literal,\n     Optional,\n+    Tuple,\n     Union,\n     cast,\n     overload,\n )\n \n+from lxml import etree\n+\n from scrapy.http import Response, TextResponse\n from scrapy.selector import Selector\n from scrapy.utils.python import re_rsearch, to_unicode\n@@ -77,15 +80,31 @@ def xmliter(\n         yield Selector(text=nodetext, type=\"xml\")\n \n \n+def _resolve_xml_namespace(element_name: str, data: bytes) -> Tuple[str, str]:\n+    if \":\" not in element_name:\n+        return element_name, None, None\n+    reader: \"SupportsReadClose[bytes]\" = _StreamReader(data)\n+    node_prefix, element_name = element_name.split(\":\", maxsplit=1)\n+    ns_iterator = etree.iterparse(\n+        reader, encoding=reader.encoding, events=(\"start-ns\",)\n+    )\n+    for event, (_prefix, _namespace) in ns_iterator:\n+        if _prefix != node_prefix:\n+            continue\n+        return element_name, _prefix, _namespace\n+    return f\"{node_prefix}:{element_name}\", None, None\n+\n+\n def xmliter_lxml(\n     obj: Union[Response, str, bytes],\n     nodename: str,\n     namespace: Optional[str] = None,\n     prefix: str = \"x\",\n ) -> Generator[Selector, Any, None]:\n-    from lxml import etree\n+    if not namespace:\n+        nodename, prefix, namespace = _resolve_xml_namespace(nodename, obj)\n \n-    reader = _StreamReader(obj)\n+    reader: \"SupportsReadClose[bytes]\" = _StreamReader(obj)\n     tag = f\"{{{namespace}}}{nodename}\" if namespace else nodename\n     iterable = etree.iterparse(\n         cast(\"SupportsReadClose[bytes]\", reader), tag=tag, encoding=reader.encoding\n\n@@ -1,4 +1,3 @@\n-from pytest import mark\n from twisted.trial import unittest\n \n from scrapy.http import Response, TextResponse, XmlResponse\n@@ -247,10 +246,6 @@ class XmliterTestCase(unittest.TestCase):\n class LxmlXmliterTestCase(XmliterTestCase):\n     xmliter = staticmethod(xmliter_lxml)\n \n-    @mark.xfail(reason=\"known bug of the current implementation\")\n-    def test_xmliter_namespaced_nodename(self):\n-        super().test_xmliter_namespaced_nodename()\n-\n     def test_xmliter_iterate_namespace(self):\n         body = b\"\"\"\n             <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d50f436a73ef13fca8d3d9c302ae1a48e984a4a5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 658 | Contributors (this commit): 24 | Commits (past 90d): 4 | Contributors (cumulative): 24 | DMM Complexity: 0.5\n\nDIFF:\n@@ -86,7 +86,10 @@ def _resolve_xml_namespace(element_name: str, data: bytes) -> Tuple[str, str]:\n     reader: \"SupportsReadClose[bytes]\" = _StreamReader(data)\n     node_prefix, element_name = element_name.split(\":\", maxsplit=1)\n     ns_iterator = etree.iterparse(\n-        reader, encoding=reader.encoding, events=(\"start-ns\",)\n+        reader,\n+        encoding=reader.encoding,\n+        events=(\"start-ns\",),\n+        huge_tree=True,\n     )\n     for event, (_prefix, _namespace) in ns_iterator:\n         if _prefix != node_prefix:\n@@ -107,7 +110,10 @@ def xmliter_lxml(\n     reader: \"SupportsReadClose[bytes]\" = _StreamReader(obj)\n     tag = f\"{{{namespace}}}{nodename}\" if namespace else nodename\n     iterable = etree.iterparse(\n-        cast(\"SupportsReadClose[bytes]\", reader), tag=tag, encoding=reader.encoding\n+        reader,\n+        tag=tag,\n+        encoding=reader.encoding,\n+        huge_tree=True,\n     )\n     selxpath = \"//\" + (f\"{prefix}:{nodename}\" if namespace else nodename)\n     for _, node in iterable:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#9655b0b8eb4bbc66b0fe540a19265b6342cf371b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 5 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 375 | Contributors (this commit): 10 | Commits (past 90d): 3 | Contributors (cumulative): 10 | DMM Complexity: None\n\nDIFF:\n@@ -4,6 +4,8 @@ from pathlib import Path\n from time import process_time\n from urllib.parse import urlparse\n \n+import pytest\n+\n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import HtmlResponse, Response, TextResponse\n from scrapy.settings.default_settings import DOWNLOAD_MAXSIZE\n@@ -201,7 +203,9 @@ class ResponseUtilsTest(unittest.TestCase):\n             r5, _openfunc=check_base_url\n         ), \"Inject unique base url with conditional comment\"\n \n-    def test_open_in_browser_redos_comment(self):\n+\n+@pytest.mark.slow\n+def test_open_in_browser_redos_comment():\n     MAX_CPU_TIME = 30\n \n     # Exploit input from\n@@ -216,9 +220,11 @@ class ResponseUtilsTest(unittest.TestCase):\n     open_in_browser(response, lambda url: True)\n \n     end_time = process_time()\n-        self.assertLess(end_time - start_time, MAX_CPU_TIME)\n+    assert (end_time - start_time) < MAX_CPU_TIME\n \n-    def test_open_in_browser_redos_head(self):\n+\n+@pytest.mark.slow\n+def test_open_in_browser_redos_head():\n     MAX_CPU_TIME = 15\n \n     # Exploit input from\n@@ -233,4 +239,4 @@ class ResponseUtilsTest(unittest.TestCase):\n     open_in_browser(response, lambda url: True)\n \n     end_time = process_time()\n-        self.assertLess(end_time - start_time, MAX_CPU_TIME)\n+    assert (end_time - start_time) < MAX_CPU_TIME\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2538c0e8629b46bd34bb0555aeca6714a6f9e571", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 4 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 670 | Contributors (this commit): 24 | Commits (past 90d): 5 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -43,10 +43,10 @@ def xmliter(\n     \"\"\"\n     nodename_patt = re.escape(nodename)\n \n-    DOCUMENT_HEADER_RE = re.compile(r\"<\\?xml[^>]{1,1024}>\\s*\", re.S)\n+    DOCUMENT_HEADER_RE = re.compile(r\"<\\?xml[^>]+>\\s*\", re.S)\n     HEADER_END_RE = re.compile(rf\"<\\s*/{nodename_patt}\\s*>\", re.S)\n-    END_TAG_RE = re.compile(r\"<\\s*/([^\\s>]{1,1024})\\s*>\", re.S)\n-    NAMESPACE_RE = re.compile(r\"((xmlns[:A-Za-z]{,1024})=[^>\\s]+)\", re.S)\n+    END_TAG_RE = re.compile(r\"<\\s*/([^\\s>]+)\\s*>\", re.S)\n+    NAMESPACE_RE = re.compile(r\"((xmlns[:A-Za-z]*)=[^>\\s]+)\", re.S)\n     text = _body_or_str(obj)\n \n     document_header_match = re.search(DOCUMENT_HEADER_RE, text)\n@@ -60,15 +60,13 @@ def xmliter(\n         for tagname in reversed(re.findall(END_TAG_RE, header_end)):\n             assert header_end_idx\n             tag = re.search(\n-                rf\"<\\s*{tagname}.{{,1024}}?xmlns[:=][^>]{{,1024}}>\",\n-                text[: header_end_idx[1]],\n-                re.S,\n+                rf\"<\\s*{tagname}.*?xmlns[:=][^>]*>\", text[: header_end_idx[1]], re.S\n             )\n             if tag:\n                 for x in re.findall(NAMESPACE_RE, tag.group()):\n                     namespaces[x[1]] = x[0]\n \n-    r = re.compile(rf\"<{nodename_patt}[\\s>].{{,1024}}?</{nodename_patt}>\", re.DOTALL)\n+    r = re.compile(rf\"<{nodename_patt}[\\s>].*?</{nodename_patt}>\", re.DOTALL)\n     for match in r.finditer(text):\n         nodetext = (\n             document_header\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#150d96764b5a455c75315596ca8ba5ded0f416dd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 14 | Files Changed: 4 | Hunks: 27 | Methods Changed: 9 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 72 | Churn Cumulative: 2101 | Contributors (this commit): 34 | Commits (past 90d): 13 | Contributors (cumulative): 65 | DMM Complexity: 0.5714285714285714\n\nDIFF:\n@@ -7,7 +7,7 @@ See documentation in docs/topics/spiders.rst\n from scrapy.exceptions import NotConfigured, NotSupported\n from scrapy.selector import Selector\n from scrapy.spiders import Spider\n-from scrapy.utils.iterators import csviter, xmliter\n+from scrapy.utils.iterators import csviter, xmliter_lxml\n from scrapy.utils.spider import iterate_spider_output\n \n \n@@ -84,7 +84,7 @@ class XMLFeedSpider(Spider):\n         return self.parse_nodes(response, nodes)\n \n     def _iternodes(self, response):\n-        for node in xmliter(response, self.itertag):\n+        for node in xmliter_lxml(response, self.itertag):\n             self._register_namespaces(node)\n             yield node\n \n\n@@ -17,9 +17,12 @@ from typing import (\n     cast,\n     overload,\n )\n+from warnings import warn\n \n from lxml import etree\n+from packaging.version import Version\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Response, TextResponse\n from scrapy.selector import Selector\n from scrapy.utils.python import re_rsearch, to_unicode\n@@ -29,6 +32,12 @@ if TYPE_CHECKING:\n \n logger = logging.getLogger(__name__)\n \n+_LXML_VERSION = Version(etree.__version__)\n+_LXML_HUGE_TREE_VERSION = Version(\"4.2\")\n+_ITERPARSE_KWARGS = {}\n+if _LXML_VERSION >= _LXML_HUGE_TREE_VERSION:\n+    _ITERPARSE_KWARGS[\"huge_tree\"] = True\n+\n \n def xmliter(\n     obj: Union[Response, str, bytes], nodename: str\n@@ -41,6 +50,16 @@ def xmliter(\n     - a unicode string\n     - a string encoded as utf-8\n     \"\"\"\n+    warn(\n+        (\n+            \"xmliter is deprecated and its use strongly discouraged because \"\n+            \"it is vulnerable to ReDoS attacks. Use xmliter_lxml instead. See \"\n+            \"https://github.com/scrapy/scrapy/security/advisories/GHSA-cc65-xxvf-f7r9\"\n+        ),\n+        ScrapyDeprecationWarning,\n+        stacklevel=2,\n+    )\n+\n     nodename_patt = re.escape(nodename)\n \n     DOCUMENT_HEADER_RE = re.compile(r\"<\\?xml[^>]+>\\s*\", re.S)\n@@ -87,7 +106,7 @@ def _resolve_xml_namespace(element_name: str, data: bytes) -> Tuple[str, str]:\n         reader,\n         encoding=reader.encoding,\n         events=(\"start-ns\",),\n-        huge_tree=True,\n+        **_ITERPARSE_KWARGS,\n     )\n     for event, (_prefix, _namespace) in ns_iterator:\n         if _prefix != node_prefix:\n@@ -111,7 +130,7 @@ def xmliter_lxml(\n         reader,\n         tag=tag,\n         encoding=reader.encoding,\n-        huge_tree=True,\n+        **_ITERPARSE_KWARGS,\n     )\n     selxpath = \"//\" + (f\"{prefix}:{nodename}\" if namespace else nodename)\n     for _, node in iterable:\n\n@@ -1,13 +1,14 @@\n+import pytest\n from twisted.trial import unittest\n \n+from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Response, TextResponse, XmlResponse\n from scrapy.utils.iterators import _body_or_str, csviter, xmliter, xmliter_lxml\n from tests import get_testdata\n \n \n-class XmliterTestCase(unittest.TestCase):\n-    xmliter = staticmethod(xmliter)\n-\n+class XmliterBaseTestCase:\n+    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n     def test_xmliter(self):\n         body = b\"\"\"\n             <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n@@ -39,6 +40,7 @@ class XmliterTestCase(unittest.TestCase):\n             attrs, [(\"001\", [\"Name 1\"], [\"Type 1\"]), (\"002\", [\"Name 2\"], [\"Type 2\"])]\n         )\n \n+    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n     def test_xmliter_unusual_node(self):\n         body = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n             <root>\n@@ -52,6 +54,7 @@ class XmliterTestCase(unittest.TestCase):\n         ]\n         self.assertEqual(nodenames, [[\"matchme...\"]])\n \n+    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n     def test_xmliter_unicode(self):\n         # example taken from https://github.com/scrapy/scrapy/issues/1665\n         body = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n@@ -111,6 +114,7 @@ class XmliterTestCase(unittest.TestCase):\n                 [(\"26\", [\"-\"], [\"80\"]), (\"21\", [\"Ab\"], [\"76\"]), (\"27\", [\"A\"], [\"27\"])],\n             )\n \n+    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n     def test_xmliter_text(self):\n         body = (\n             '<?xml version=\"1.0\" encoding=\"UTF-8\"?>'\n@@ -122,6 +126,7 @@ class XmliterTestCase(unittest.TestCase):\n             [[\"one\"], [\"two\"]],\n         )\n \n+    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n     def test_xmliter_namespaces(self):\n         body = b\"\"\"\n             <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n@@ -161,6 +166,7 @@ class XmliterTestCase(unittest.TestCase):\n         self.assertEqual(node.xpath(\"id/text()\").getall(), [])\n         self.assertEqual(node.xpath(\"price/text()\").getall(), [])\n \n+    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n     def test_xmliter_namespaced_nodename(self):\n         body = b\"\"\"\n             <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n@@ -189,6 +195,7 @@ class XmliterTestCase(unittest.TestCase):\n             [\"http://www.mydummycompany.com/images/item1.jpg\"],\n         )\n \n+    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n     def test_xmliter_namespaced_nodename_missing(self):\n         body = b\"\"\"\n             <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n@@ -213,6 +220,7 @@ class XmliterTestCase(unittest.TestCase):\n         with self.assertRaises(StopIteration):\n             next(my_iter)\n \n+    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n     def test_xmliter_exception(self):\n         body = (\n             '<?xml version=\"1.0\" encoding=\"UTF-8\"?>'\n@@ -225,10 +233,12 @@ class XmliterTestCase(unittest.TestCase):\n \n         self.assertRaises(StopIteration, next, iter)\n \n+    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n     def test_xmliter_objtype_exception(self):\n         i = self.xmliter(42, \"product\")\n         self.assertRaises(TypeError, next, i)\n \n+    @pytest.mark.filterwarnings(\"ignore::scrapy.exceptions.ScrapyDeprecationWarning\")\n     def test_xmliter_encoding(self):\n         body = (\n             b'<?xml version=\"1.0\" encoding=\"ISO-8859-9\"?>\\n'\n@@ -243,7 +253,24 @@ class XmliterTestCase(unittest.TestCase):\n         )\n \n \n-class LxmlXmliterTestCase(XmliterTestCase):\n+class XmliterTestCase(XmliterBaseTestCase, unittest.TestCase):\n+    xmliter = staticmethod(xmliter)\n+\n+    def test_deprecation(self):\n+        body = b\"\"\"\n+            <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+            <products>\n+              <product></product>\n+            </products>\n+        \"\"\"\n+        with pytest.warns(\n+            ScrapyDeprecationWarning,\n+            match=\"xmliter\",\n+        ):\n+            next(self.xmliter(body, \"product\"))\n+\n+\n+class LxmlXmliterTestCase(XmliterBaseTestCase, unittest.TestCase):\n     xmliter = staticmethod(xmliter_lxml)\n \n     def test_xmliter_iterate_namespace(self):\n\n@@ -203,9 +203,8 @@ class ResponseUtilsTest(unittest.TestCase):\n             r5, _openfunc=check_base_url\n         ), \"Inject unique base url with conditional comment\"\n \n-\n     @pytest.mark.slow\n-def test_open_in_browser_redos_comment():\n+    def test_open_in_browser_redos_comment(self):\n         MAX_CPU_TIME = 30\n \n         # Exploit input from\n@@ -220,11 +219,10 @@ def test_open_in_browser_redos_comment():\n         open_in_browser(response, lambda url: True)\n \n         end_time = process_time()\n-    assert (end_time - start_time) < MAX_CPU_TIME\n-\n+        self.assertLess(end_time - start_time, MAX_CPU_TIME)\n \n     @pytest.mark.slow\n-def test_open_in_browser_redos_head():\n+    def test_open_in_browser_redos_head(self):\n         MAX_CPU_TIME = 15\n \n         # Exploit input from\n@@ -239,4 +237,4 @@ def test_open_in_browser_redos_head():\n         open_in_browser(response, lambda url: True)\n \n         end_time = process_time()\n-    assert (end_time - start_time) < MAX_CPU_TIME\n+        self.assertLess(end_time - start_time, MAX_CPU_TIME)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b0dbd0e9affff23ef7f68caa3a84734aec4dde54", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 703 | Contributors (this commit): 24 | Commits (past 90d): 7 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -101,18 +101,18 @@ def _resolve_xml_namespace(element_name: str, data: bytes) -> Tuple[str, str]:\n     if \":\" not in element_name:\n         return element_name, None, None\n     reader: \"SupportsReadClose[bytes]\" = _StreamReader(data)\n-    node_prefix, element_name = element_name.split(\":\", maxsplit=1)\n+    input_prefix, element_name = element_name.split(\":\", maxsplit=1)\n     ns_iterator = etree.iterparse(\n         reader,\n         encoding=reader.encoding,\n         events=(\"start-ns\",),\n         **_ITERPARSE_KWARGS,\n     )\n-    for event, (_prefix, _namespace) in ns_iterator:\n-        if _prefix != node_prefix:\n+    for event, (prefix, namespace) in ns_iterator:\n+        if prefix != input_prefix:\n             continue\n-        return element_name, _prefix, _namespace\n-    return f\"{node_prefix}:{element_name}\", None, None\n+        return element_name, prefix, namespace\n+    return f\"{input_prefix}:{element_name}\", None, None\n \n \n def xmliter_lxml(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a49c8762dd163b60cc73c4486a662471cfa7ac7d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 18 | Lines Deleted: 24 | Files Changed: 1 | Hunks: 6 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 42 | Churn Cumulative: 745 | Contributors (this commit): 24 | Commits (past 90d): 8 | Contributors (cumulative): 24 | DMM Complexity: 0.0\n\nDIFF:\n@@ -12,7 +12,6 @@ from typing import (\n     List,\n     Literal,\n     Optional,\n-    Tuple,\n     Union,\n     cast,\n     overload,\n@@ -97,43 +96,38 @@ def xmliter(\n         yield Selector(text=nodetext, type=\"xml\")\n \n \n-def _resolve_xml_namespace(element_name: str, data: bytes) -> Tuple[str, str]:\n-    if \":\" not in element_name:\n-        return element_name, None, None\n-    reader: \"SupportsReadClose[bytes]\" = _StreamReader(data)\n-    input_prefix, element_name = element_name.split(\":\", maxsplit=1)\n-    ns_iterator = etree.iterparse(\n-        reader,\n-        encoding=reader.encoding,\n-        events=(\"start-ns\",),\n-        **_ITERPARSE_KWARGS,\n-    )\n-    for event, (prefix, namespace) in ns_iterator:\n-        if prefix != input_prefix:\n-            continue\n-        return element_name, prefix, namespace\n-    return f\"{input_prefix}:{element_name}\", None, None\n-\n-\n def xmliter_lxml(\n     obj: Union[Response, str, bytes],\n     nodename: str,\n     namespace: Optional[str] = None,\n     prefix: str = \"x\",\n ) -> Generator[Selector, Any, None]:\n-    if not namespace:\n-        nodename, prefix, namespace = _resolve_xml_namespace(nodename, obj)\n-\n     reader: \"SupportsReadClose[bytes]\" = _StreamReader(obj)\n     tag = f\"{{{namespace}}}{nodename}\" if namespace else nodename\n     iterable = etree.iterparse(\n         reader,\n-        tag=tag,\n         encoding=reader.encoding,\n+        events=(\"end\", \"start-ns\"),\n         **_ITERPARSE_KWARGS,\n     )\n     selxpath = \"//\" + (f\"{prefix}:{nodename}\" if namespace else nodename)\n-    for _, node in iterable:\n+    needs_namespace_resolution = not namespace and \":\" in nodename\n+    if needs_namespace_resolution:\n+        prefix, nodename = nodename.split(\":\", maxsplit=1)\n+    for event, data in iterable:\n+        if event == \"start-ns\":\n+            if needs_namespace_resolution:\n+                _prefix, _namespace = data\n+                if _prefix != prefix:\n+                    continue\n+                namespace = _namespace\n+                needs_namespace_resolution = False\n+                selxpath = f\"//{prefix}:{nodename}\"\n+                tag = f\"{{{namespace}}}{nodename}\"\n+            continue\n+        node = data\n+        if node.tag != tag:\n+            continue\n         nodetext = etree.tostring(node, encoding=\"unicode\")\n         node.clear()\n         xs = Selector(text=nodetext, type=\"xml\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ce9d290eff8b5992023ffa5e833881b83a0669c3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 8 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 754 | Contributors (this commit): 24 | Commits (past 90d): 9 | Contributors (cumulative): 24 | DMM Complexity: None\n\nDIFF:\n@@ -19,7 +19,6 @@ from typing import (\n from warnings import warn\n \n from lxml import etree\n-from packaging.version import Version\n \n from scrapy.exceptions import ScrapyDeprecationWarning\n from scrapy.http import Response, TextResponse\n@@ -31,12 +30,6 @@ if TYPE_CHECKING:\n \n logger = logging.getLogger(__name__)\n \n-_LXML_VERSION = Version(etree.__version__)\n-_LXML_HUGE_TREE_VERSION = Version(\"4.2\")\n-_ITERPARSE_KWARGS = {}\n-if _LXML_VERSION >= _LXML_HUGE_TREE_VERSION:\n-    _ITERPARSE_KWARGS[\"huge_tree\"] = True\n-\n \n def xmliter(\n     obj: Union[Response, str, bytes], nodename: str\n@@ -108,7 +101,7 @@ def xmliter_lxml(\n         reader,\n         encoding=reader.encoding,\n         events=(\"end\", \"start-ns\"),\n-        **_ITERPARSE_KWARGS,\n+        huge_tree=True,\n     )\n     selxpath = \"//\" + (f\"{prefix}:{nodename}\" if namespace else nodename)\n     needs_namespace_resolution = not namespace and \":\" in nodename\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
