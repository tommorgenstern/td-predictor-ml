{"custom_id": "keras#0f0d8be884a5383f29c945d7fe3a1baf4d3b11e2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 112 | Lines Deleted: 80 | Files Changed: 6 | Hunks: 61 | Methods Changed: 17 | Complexity Δ (Sum/Max): 7/7 | Churn Δ: 192 | Churn Cumulative: 8064 | Contributors (this commit): 64 | Commits (past 90d): 32 | Contributors (cumulative): 82 | DMM Complexity: 0.0\n\nDIFF:\n@@ -81,7 +81,7 @@ def set_floatx(floatx):\n     '''\n     global _FLOATX\n     if floatx not in {'float16', 'float32', 'float64'}:\n-        raise Exception('Unknown floatx type: ' + str(floatx))\n+        raise ValueError('Unknown floatx type: ' + str(floatx))\n     _FLOATX = str(floatx)\n \n \n@@ -147,7 +147,7 @@ def set_image_dim_ordering(dim_ordering):\n     '''\n     global _IMAGE_DIM_ORDERING\n     if dim_ordering not in {'tf', 'th'}:\n-        raise Exception('Unknown dim_ordering:', dim_ordering)\n+        raise ValueError('Unknown dim_ordering:', dim_ordering)\n     _IMAGE_DIM_ORDERING = str(dim_ordering)\n \n \n\n@@ -480,7 +480,7 @@ class TensorBoard(Callback):\n     def __init__(self, log_dir='./logs', histogram_freq=0, write_graph=True, write_images=False):\n         super(TensorBoard, self).__init__()\n         if K._BACKEND != 'tensorflow':\n-            raise Exception('TensorBoard callback only works '\n+            raise RuntimeError('TensorBoard callback only works '\n                                'with the TensorFlow backend.')\n         self.log_dir = log_dir\n         self.histogram_freq = histogram_freq\n\n@@ -8,10 +8,10 @@ import os\n \n def load_data(label_mode='fine'):\n     if label_mode not in ['fine', 'coarse']:\n-        raise Exception('label_mode must be one of \"fine\" \"coarse\".')\n+        raise ValueError('label_mode must be one of \"fine\" \"coarse\".')\n \n-    dirname = \"cifar-100-python\"\n-    origin = \"http://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n+    dirname = 'cifar-100-python'\n+    origin = 'http://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz'\n     path = get_file(dirname, origin=origin, untar=True)\n \n     fpath = os.path.join(path, 'train')\n\n@@ -10,7 +10,8 @@ import sys\n def load_data(path='imdb_full.pkl', nb_words=None, skip_top=0,\n               maxlen=None, seed=113,\n               start_char=1, oov_char=2, index_from=3):\n-    '''\n+    '''Loads IMDB dataset.\n+\n     # Arguments\n         path: where to store the data (in `/.keras/dataset`)\n         nb_words: max number of words to include. Words are ranked\n@@ -72,7 +73,7 @@ def load_data(path='imdb_full.pkl', nb_words=None, skip_top=0,\n         X = new_X\n         labels = new_labels\n     if not X:\n-        raise Exception('After filtering for sequences shorter than maxlen=' +\n+        raise ValueError('After filtering for sequences shorter than maxlen=' +\n                          str(maxlen) + ', no sequence was kept. '\n                          'Increase maxlen.')\n     if not nb_words:\n\n@@ -88,7 +88,7 @@ def orthogonal(shape, scale=1.1, name=None):\n \n def identity(shape, scale=1, name=None):\n     if len(shape) != 2 or shape[0] != shape[1]:\n-        raise Exception('Identity matrix initialization can only be used '\n+        raise ValueError('Identity matrix initialization can only be used '\n                          'for 2D square matrices.')\n     else:\n         return K.variable(scale * np.identity(shape[0]), name=name)\n\n@@ -84,7 +84,7 @@ class Convolution1D(Layer):\n                  bias=True, input_dim=None, input_length=None, **kwargs):\n \n         if border_mode not in {'valid', 'same', 'full'}:\n-            raise Exception('Invalid border mode for Convolution1D:', border_mode)\n+            raise ValueError('Invalid border mode for Convolution1D:', border_mode)\n         self.nb_filter = nb_filter\n         self.filter_length = filter_length\n         self.init = initializations.get(init, dim_ordering='th')\n@@ -257,7 +257,7 @@ class AtrousConvolution1D(Convolution1D):\n                  bias=True, **kwargs):\n \n         if border_mode not in {'valid', 'same', 'full'}:\n-            raise Exception('Invalid border mode for AtrousConv1D:', border_mode)\n+            raise ValueError('Invalid border mode for AtrousConv1D:', border_mode)\n \n         self.atrous_rate = int(atrous_rate)\n \n@@ -374,7 +374,7 @@ class Convolution2D(Layer):\n         if dim_ordering == 'default':\n             dim_ordering = K.image_dim_ordering()\n         if border_mode not in {'valid', 'same', 'full'}:\n-            raise Exception('Invalid border mode for Convolution2D:', border_mode)\n+            raise ValueError('Invalid border mode for Convolution2D:', border_mode)\n         self.nb_filter = nb_filter\n         self.nb_row = nb_row\n         self.nb_col = nb_col\n@@ -382,7 +382,8 @@ class Convolution2D(Layer):\n         self.activation = activations.get(activation)\n         self.border_mode = border_mode\n         self.subsample = tuple(subsample)\n-        assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n+        if dim_ordering not in {'tf', 'th'}:\n+            raise ValueError('dim_ordering must be in {tf, th}.')\n         self.dim_ordering = dim_ordering\n \n         self.W_regularizer = regularizers.get(W_regularizer)\n@@ -405,7 +406,7 @@ class Convolution2D(Layer):\n             stack_size = input_shape[3]\n             self.W_shape = (self.nb_row, self.nb_col, stack_size, self.nb_filter)\n         else:\n-            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n         self.W = self.init(self.W_shape, name='{}_W'.format(self.name))\n         if self.bias:\n             self.b = K.zeros((self.nb_filter,), name='{}_b'.format(self.name))\n@@ -445,7 +446,7 @@ class Convolution2D(Layer):\n             rows = input_shape[1]\n             cols = input_shape[2]\n         else:\n-            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n \n         rows = conv_output_length(rows, self.nb_row,\n                                   self.border_mode, self.subsample[0])\n@@ -456,8 +457,6 @@ class Convolution2D(Layer):\n             return (input_shape[0], self.nb_filter, rows, cols)\n         elif self.dim_ordering == 'tf':\n             return (input_shape[0], rows, cols, self.nb_filter)\n-        else:\n-            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n \n     def call(self, x, mask=None):\n         output = K.conv2d(x, self.W, strides=self.subsample,\n@@ -470,7 +469,7 @@ class Convolution2D(Layer):\n             elif self.dim_ordering == 'tf':\n                 output += K.reshape(self.b, (1, 1, 1, self.nb_filter))\n             else:\n-                raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+                raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n         output = self.activation(output)\n         return output\n \n@@ -618,18 +617,24 @@ class Deconvolution2D(Convolution2D):\n         if dim_ordering == 'default':\n             dim_ordering = K.image_dim_ordering()\n         if border_mode not in {'valid', 'same', 'full'}:\n-            raise Exception('Invalid border mode for Deconvolution2D:', border_mode)\n+            raise ValueError('Invalid border mode for Deconvolution2D:', border_mode)\n \n         self.output_shape_ = output_shape\n \n         super(Deconvolution2D, self).__init__(nb_filter, nb_row, nb_col,\n-                                              init=init, activation=activation,\n-                                              weights=weights, border_mode=border_mode,\n-                                              subsample=subsample, dim_ordering=dim_ordering,\n-                                              W_regularizer=W_regularizer, b_regularizer=b_regularizer,\n+                                              init=init,\n+                                              activation=activation,\n+                                              weights=weights,\n+                                              border_mode=border_mode,\n+                                              subsample=subsample,\n+                                              dim_ordering=dim_ordering,\n+                                              W_regularizer=W_regularizer,\n+                                              b_regularizer=b_regularizer,\n                                               activity_regularizer=activity_regularizer,\n-                                              W_constraint=W_constraint, b_constraint=b_constraint,\n-                                              bias=bias, **kwargs)\n+                                              W_constraint=W_constraint,\n+                                              b_constraint=b_constraint,\n+                                              bias=bias,\n+                                              **kwargs)\n \n     def get_output_shape_for(self, input_shape):\n         if self.dim_ordering == 'th':\n@@ -639,14 +644,12 @@ class Deconvolution2D(Convolution2D):\n             rows = self.output_shape_[1]\n             cols = self.output_shape_[2]\n         else:\n-            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n \n         if self.dim_ordering == 'th':\n             return (input_shape[0], self.nb_filter, rows, cols)\n         elif self.dim_ordering == 'tf':\n             return (input_shape[0], rows, cols, self.nb_filter)\n-        else:\n-            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n \n     def call(self, x, mask=None):\n         output = K.deconv2d(x, self.W, self.output_shape_,\n@@ -660,7 +663,7 @@ class Deconvolution2D(Convolution2D):\n             elif self.dim_ordering == 'tf':\n                 output += K.reshape(self.b, (1, 1, 1, self.nb_filter))\n             else:\n-                raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+                raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n         output = self.activation(output)\n         return output\n \n@@ -752,18 +755,24 @@ class AtrousConvolution2D(Convolution2D):\n             dim_ordering = K.image_dim_ordering()\n \n         if border_mode not in {'valid', 'same', 'full'}:\n-            raise Exception('Invalid border mode for AtrousConv2D:', border_mode)\n+            raise ValueError('Invalid border mode for AtrousConv2D:', border_mode)\n \n         self.atrous_rate = tuple(atrous_rate)\n \n         super(AtrousConvolution2D, self).__init__(nb_filter, nb_row, nb_col,\n-                                                  init=init, activation=activation,\n-                                                  weights=weights, border_mode=border_mode,\n-                                                  subsample=subsample, dim_ordering=dim_ordering,\n-                                                  W_regularizer=W_regularizer, b_regularizer=b_regularizer,\n+                                                  init=init,\n+                                                  activation=activation,\n+                                                  weights=weights,\n+                                                  border_mode=border_mode,\n+                                                  subsample=subsample,\n+                                                  dim_ordering=dim_ordering,\n+                                                  W_regularizer=W_regularizer,\n+                                                  b_regularizer=b_regularizer,\n                                                   activity_regularizer=activity_regularizer,\n-                                                  W_constraint=W_constraint, b_constraint=b_constraint,\n-                                                  bias=bias, **kwargs)\n+                                                  W_constraint=W_constraint,\n+                                                  b_constraint=b_constraint,\n+                                                  bias=bias,\n+                                                  **kwargs)\n \n     def get_output_shape_for(self, input_shape):\n         if self.dim_ordering == 'th':\n@@ -773,19 +782,19 @@ class AtrousConvolution2D(Convolution2D):\n             rows = input_shape[1]\n             cols = input_shape[2]\n         else:\n-            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n \n         rows = conv_output_length(rows, self.nb_row, self.border_mode,\n-                                  self.subsample[0], dilation=self.atrous_rate[0])\n+                                  self.subsample[0],\n+                                  dilation=self.atrous_rate[0])\n         cols = conv_output_length(cols, self.nb_col, self.border_mode,\n-                                  self.subsample[1], dilation=self.atrous_rate[1])\n+                                  self.subsample[1],\n+                                  dilation=self.atrous_rate[1])\n \n         if self.dim_ordering == 'th':\n             return (input_shape[0], self.nb_filter, rows, cols)\n         elif self.dim_ordering == 'tf':\n             return (input_shape[0], rows, cols, self.nb_filter)\n-        else:\n-            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n \n     def call(self, x, mask=None):\n         output = K.conv2d(x, self.W, strides=self.subsample,\n@@ -799,7 +808,7 @@ class AtrousConvolution2D(Convolution2D):\n             elif self.dim_ordering == 'tf':\n                 output += K.reshape(self.b, (1, 1, 1, self.nb_filter))\n             else:\n-                raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+                raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n         output = self.activation(output)\n         return output\n \n@@ -898,28 +907,30 @@ class SeparableConvolution2D(Layer):\n                  b_constraint=None,\n                  bias=True, **kwargs):\n \n-        if K._BACKEND != 'tensorflow':\n-            raise Exception('SeparableConv2D is only available '\n+        if K.backend() != 'tensorflow':\n+            raise RuntimeError('SeparableConv2D is only available '\n                                'with TensorFlow for the time being.')\n \n         if dim_ordering == 'default':\n             dim_ordering = K.image_dim_ordering()\n \n         if border_mode not in {'valid', 'same'}:\n-            raise Exception('Invalid border mode for SeparableConv2D:', border_mode)\n+            raise ValueError('Invalid border mode for SeparableConv2D:', border_mode)\n \n         if border_mode not in {'valid', 'same'}:\n-            raise Exception('Invalid border mode for SeparableConv2D:', border_mode)\n+            raise ValueError('Invalid border mode for SeparableConv2D:', border_mode)\n         self.nb_filter = nb_filter\n         self.nb_row = nb_row\n         self.nb_col = nb_col\n         self.init = initializations.get(init, dim_ordering=dim_ordering)\n         self.activation = activations.get(activation)\n-        assert border_mode in {'valid', 'same'}, 'border_mode must be in {valid, same}'\n+        if border_mode not in {'valid', 'same'}:\n+            raise ValueError('border_mode must be in {valid, same}.')\n         self.border_mode = border_mode\n         self.subsample = tuple(subsample)\n         self.depth_multiplier = depth_multiplier\n-        assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n+        if dim_ordering not in {'tf', 'th'}:\n+            raise ValueError('dim_ordering must be in {tf, th}.')\n         self.dim_ordering = dim_ordering\n \n         self.depthwise_regularizer = regularizers.get(depthwise_regularizer)\n@@ -946,7 +957,7 @@ class SeparableConvolution2D(Layer):\n             depthwise_shape = (self.nb_row, self.nb_col, stack_size, self.depth_multiplier)\n             pointwise_shape = (1, 1, self.depth_multiplier * stack_size, self.nb_filter)\n         else:\n-            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n         self.depthwise_kernel = self.init(depthwise_shape,\n                                           name='{}_depthwise_kernel'.format(self.name))\n         self.pointwise_kernel = self.init(pointwise_shape,\n@@ -994,7 +1005,7 @@ class SeparableConvolution2D(Layer):\n             rows = input_shape[1]\n             cols = input_shape[2]\n         else:\n-            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n \n         rows = conv_output_length(rows, self.nb_row,\n                                   self.border_mode, self.subsample[0])\n@@ -1006,7 +1017,7 @@ class SeparableConvolution2D(Layer):\n         elif self.dim_ordering == 'tf':\n             return (input_shape[0], rows, cols, self.nb_filter)\n         else:\n-            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n \n     def call(self, x, mask=None):\n         output = K.separable_conv2d(x, self.depthwise_kernel,\n@@ -1020,7 +1031,7 @@ class SeparableConvolution2D(Layer):\n             elif self.dim_ordering == 'tf':\n                 output += K.reshape(self.b, (1, 1, 1, self.nb_filter))\n             else:\n-                raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+                raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n         output = self.activation(output)\n         return output\n \n@@ -1114,7 +1125,7 @@ class Convolution3D(Layer):\n             dim_ordering = K.image_dim_ordering()\n \n         if border_mode not in {'valid', 'same', 'full'}:\n-            raise Exception('Invalid border mode for Convolution3D:', border_mode)\n+            raise ValueError('Invalid border mode for Convolution3D:', border_mode)\n         self.nb_filter = nb_filter\n         self.kernel_dim1 = kernel_dim1\n         self.kernel_dim2 = kernel_dim2\n@@ -1123,7 +1134,8 @@ class Convolution3D(Layer):\n         self.activation = activations.get(activation)\n         self.border_mode = border_mode\n         self.subsample = tuple(subsample)\n-        assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n+        if dim_ordering not in {'tf', 'th'}:\n+            raise ValueError('dim_ordering must be in {tf, th}.')\n         self.dim_ordering = dim_ordering\n \n         self.W_regularizer = regularizers.get(W_regularizer)\n@@ -1151,7 +1163,7 @@ class Convolution3D(Layer):\n             self.W_shape = (self.kernel_dim1, self.kernel_dim2, self.kernel_dim3,\n                             stack_size, self.nb_filter)\n         else:\n-            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n \n         self.W = self.init(self.W_shape, name='{}_W'.format(self.name))\n         if self.bias:\n@@ -1194,7 +1206,7 @@ class Convolution3D(Layer):\n             conv_dim2 = input_shape[2]\n             conv_dim3 = input_shape[3]\n         else:\n-            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n \n         conv_dim1 = conv_output_length(conv_dim1, self.kernel_dim1,\n                                        self.border_mode, self.subsample[0])\n@@ -1208,7 +1220,7 @@ class Convolution3D(Layer):\n         elif self.dim_ordering == 'tf':\n             return (input_shape[0], conv_dim1, conv_dim2, conv_dim3, self.nb_filter)\n         else:\n-            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n \n     def call(self, x, mask=None):\n         input_shape = self.input_spec[0].shape\n@@ -1223,7 +1235,7 @@ class Convolution3D(Layer):\n             elif self.dim_ordering == 'tf':\n                 output += K.reshape(self.b, (1, 1, 1, 1, self.nb_filter))\n             else:\n-                raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+                raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n         output = self.activation(output)\n         return output\n \n@@ -1309,7 +1321,8 @@ class UpSampling2D(Layer):\n         if dim_ordering == 'default':\n             dim_ordering = K.image_dim_ordering()\n         self.size = tuple(size)\n-        assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n+        if dim_ordering not in {'tf', 'th'}:\n+            raise ValueError('dim_ordering must be in {tf, th}.')\n         self.dim_ordering = dim_ordering\n         self.input_spec = [InputSpec(ndim=4)]\n         super(UpSampling2D, self).__init__(**kwargs)\n@@ -1330,7 +1343,7 @@ class UpSampling2D(Layer):\n                     height,\n                     input_shape[3])\n         else:\n-            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n \n     def call(self, x, mask=None):\n         return K.resize_images(x, self.size[0], self.size[1],\n@@ -1372,7 +1385,8 @@ class UpSampling3D(Layer):\n         if dim_ordering == 'default':\n             dim_ordering = K.image_dim_ordering()\n         self.size = tuple(size)\n-        assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n+        if dim_ordering not in {'tf', 'th'}:\n+            raise ValueError('dim_ordering must be in {tf, th}.')\n         self.dim_ordering = dim_ordering\n         self.input_spec = [InputSpec(ndim=5)]\n         super(UpSampling3D, self).__init__(**kwargs)\n@@ -1397,7 +1411,7 @@ class UpSampling3D(Layer):\n                     dim3,\n                     input_shape[4])\n         else:\n-            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n \n     def call(self, x, mask=None):\n         return K.resize_volumes(x, self.size[0], self.size[1], self.size[2],\n@@ -1544,7 +1558,8 @@ class ZeroPadding2D(Layer):\n                                 'of length 2 or 4, or dict. '\n                                 'Found: ' + str(padding))\n \n-        assert dim_ordering in {'tf', 'th'}, '`dim_ordering` must be in {\"tf\", \"th\"}.'\n+        if dim_ordering not in {'tf', 'th'}:\n+            raise ValueError('dim_ordering must be in {tf, th}.')\n         self.dim_ordering = dim_ordering\n         self.input_spec = [InputSpec(ndim=4)]\n \n@@ -1564,7 +1579,7 @@ class ZeroPadding2D(Layer):\n                     cols,\n                     input_shape[3])\n         else:\n-            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n \n     def call(self, x, mask=None):\n         return K.asymmetric_spatial_2d_padding(x,\n@@ -1609,7 +1624,8 @@ class ZeroPadding3D(Layer):\n         if dim_ordering == 'default':\n             dim_ordering = K.image_dim_ordering()\n         self.padding = tuple(padding)\n-        assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n+        if dim_ordering not in {'tf', 'th'}:\n+            raise ValueError('dim_ordering must be in {tf, th}.')\n         self.dim_ordering = dim_ordering\n         self.input_spec = [InputSpec(ndim=5)]\n \n@@ -1633,7 +1649,7 @@ class ZeroPadding3D(Layer):\n                     dim3,\n                     input_shape[4])\n         else:\n-            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n \n     def call(self, x, mask=None):\n         return K.spatial_3d_padding(x, padding=self.padding,\n@@ -1644,6 +1660,7 @@ class ZeroPadding3D(Layer):\n         base_config = super(ZeroPadding3D, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n+\n class Cropping1D(Layer):\n     '''Cropping layer for 1D input (e.g. temporal sequence).\n     It crops along the time dimension (axis 1).\n@@ -1663,7 +1680,8 @@ class Cropping1D(Layer):\n     def __init__(self, cropping=(1, 1), **kwargs):\n         super(Cropping1D, self).__init__(**kwargs)\n         self.cropping = tuple(cropping)\n-        assert len(self.cropping) == 2, 'cropping must be a tuple length of 2'\n+        if len(self.cropping) != 2:\n+            raise ValueError('`cropping` must be a tuple length of 2.')\n         self.input_spec = [InputSpec(ndim=3)]\n \n     def build(self, input_shape):\n@@ -1671,7 +1689,10 @@ class Cropping1D(Layer):\n         self.built = True\n \n     def get_output_shape_for(self, input_shape):\n-        length = input_shape[1] - self.cropping[0] - self.cropping[1] if input_shape[1] is not None else None\n+        if input_shape[1] is not None:\n+            length = input_shape[1] - self.cropping[0] - self.cropping[1]\n+        else:\n+            length = None\n         return (input_shape[0],\n                 length,\n                 input_shape[2])\n@@ -1729,10 +1750,14 @@ class Cropping2D(Layer):\n         if dim_ordering == 'default':\n             dim_ordering = K.image_dim_ordering()\n         self.cropping = tuple(cropping)\n-        assert len(self.cropping) == 2, 'cropping must be a tuple length of 2'\n-        assert len(self.cropping[0]) == 2, 'cropping[0] must be a tuple length of 2'\n-        assert len(self.cropping[1]) == 2, 'cropping[1] must be a tuple length of 2'\n-        assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n+        if len(self.cropping) != 2:\n+            raise ValueError('`cropping` must be a tuple length of 2.')\n+        if len(self.cropping[0]) != 2:\n+            raise ValueError('`cropping[0]` must be a tuple length of 2.')\n+        if len(self.cropping[1]) != 2:\n+            raise ValueError('`cropping[1]` must be a tuple length of 2.')\n+        if dim_ordering not in {'tf', 'th'}:\n+            raise ValueError('dim_ordering must be in {tf, th}.')\n         self.dim_ordering = dim_ordering\n         self.input_spec = [InputSpec(ndim=4)]\n \n@@ -1752,7 +1777,7 @@ class Cropping2D(Layer):\n                     input_shape[2] - self.cropping[1][0] - self.cropping[1][1],\n                     input_shape[3])\n         else:\n-            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n \n     def call(self, x, mask=None):\n         input_shape = self.input_spec[0].shape\n@@ -1797,16 +1822,22 @@ class Cropping3D(Layer):\n \n     '''\n \n-    def __init__(self, cropping=((1, 1), (1, 1), (1, 1)), dim_ordering='default', **kwargs):\n+    def __init__(self, cropping=((1, 1), (1, 1), (1, 1)),\n+                 dim_ordering='default', **kwargs):\n         super(Cropping3D, self).__init__(**kwargs)\n         if dim_ordering == 'default':\n             dim_ordering = K.image_dim_ordering()\n         self.cropping = tuple(cropping)\n-        assert len(self.cropping) == 3, 'cropping must be a tuple length of 3'\n-        assert len(self.cropping[0]) == 2, 'cropping[0] must be a tuple length of 2'\n-        assert len(self.cropping[1]) == 2, 'cropping[1] must be a tuple length of 2'\n-        assert len(self.cropping[2]) == 2, 'cropping[2] must be a tuple length of 2'\n-        assert dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n+        if len(self.cropping) != 3:\n+            raise ValueError('`cropping` must be a tuple length of 3.')\n+        if len(self.cropping[0]) != 2:\n+            raise ValueError('`cropping[0]` must be a tuple length of 2.')\n+        if len(self.cropping[1]) != 2:\n+            raise ValueError('`cropping[1]` must be a tuple length of 2.')\n+        if len(self.cropping[2]) != 2:\n+            raise ValueError('`cropping[2]` must be a tuple length of 2.')\n+        if dim_ordering not in {'tf', 'th'}:\n+            raise ValueError('dim_ordering must be in {tf, th}.')\n         self.dim_ordering = dim_ordering\n         self.input_spec = [InputSpec(ndim=5)]\n \n@@ -1834,7 +1865,7 @@ class Cropping3D(Layer):\n                     dim3,\n                     input_shape[4])\n         else:\n-            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n \n     def call(self, x, mask=None):\n         input_shape = self.input_spec[0].shape\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2b336756b661fe6d96856723f3d804c4db954c97", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 7 | Files Changed: 1 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 12 | Churn Cumulative: 128 | Contributors (this commit): 7 | Commits (past 90d): 2 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -23,10 +23,7 @@ _keras_dir = os.path.join(_keras_base_dir, '.keras')\n if not os.path.exists(_keras_dir):\n     os.makedirs(_keras_dir)\n \n-# Set theano as default backend for Windows users since tensorflow is not available for Windows yet.\n-if os.name == 'nt':\n-    _BACKEND = 'theano'\n-else:\n+# Default backend: TensorFlow.\n _BACKEND = 'tensorflow'\n \n _config_path = os.path.expanduser(os.path.join(_keras_dir, 'keras.json'))\n@@ -35,10 +32,11 @@ if os.path.exists(_config_path):\n     _floatx = _config.get('floatx', floatx())\n     assert _floatx in {'float16', 'float32', 'float64'}\n     _epsilon = _config.get('epsilon', epsilon())\n-    assert type(_epsilon) == float\n+    assert isinstance(_epsilon, float)\n     _backend = _config.get('backend', _BACKEND)\n     assert _backend in {'theano', 'tensorflow'}\n-    _image_dim_ordering = _config.get('image_dim_ordering', image_dim_ordering())\n+    _image_dim_ordering = _config.get('image_dim_ordering',\n+                                      image_dim_ordering())\n     assert _image_dim_ordering in {'tf', 'th'}\n \n     set_floatx(_floatx)\n@@ -68,7 +66,7 @@ elif _BACKEND == 'tensorflow':\n     sys.stderr.write('Using TensorFlow backend.\\n')\n     from .tensorflow_backend import *\n else:\n-    raise Exception('Unknown backend: ' + str(_BACKEND))\n+    raise ValueError('Unknown backend: ' + str(_BACKEND))\n \n \n def backend():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ff62eb251b04b8301e71aee970bdb157f2649fa9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 505 | Lines Deleted: 520 | Files Changed: 15 | Hunks: 109 | Methods Changed: 57 | Complexity Δ (Sum/Max): -65/24 | Churn Δ: 1025 | Churn Cumulative: 42824 | Contributors (this commit): 170 | Commits (past 90d): 121 | Contributors (cumulative): 329 | DMM Complexity: 1.0\n\nDIFF:\n@@ -57,7 +57,7 @@ def to_dense(tensor):\n \n \n def variable(value, dtype=_FLOATX, name=None):\n-    '''Instantiate a tensor variable.\n+    '''Instantiates a variable.\n     '''\n     if hasattr(value, 'tocoo'):\n         _assert_sparse_module()\n\n@@ -13,6 +13,7 @@ import inspect\n from six.moves import zip\n \n from .. import backend as K\n+from .. import initializations\n from ..utils.io_utils import ask_to_proceed_with_overwrite\n from ..utils.generic_utils import func_dump, func_load\n \n@@ -28,6 +29,11 @@ def to_list(x):\n     return [x]\n \n \n+def object_list_uid(object_list):\n+    object_list = to_list(object_list)\n+    return ', '.join([str(abs(id(x))) for x in object_list])\n+\n+\n class InputSpec(object):\n     '''This specifies the ndim, dtype and shape of every input to a layer.\n     Every layer should expose (if appropriate) an `input_spec` attribute:\n@@ -239,7 +245,6 @@ class Layer(object):\n         non_trainable_weights: List of variables.\n         weights: The concatenation of the lists trainable_weights and\n             non_trainable_weights (in this order).\n-        regularizers: List of regularizers.\n         constraints: Dict mapping weights to constraints.\n \n     # Methods\n@@ -294,8 +299,8 @@ class Layer(object):\n             self.trainable_weights = []\n         if not hasattr(self, 'non_trainable_weights'):\n             self.non_trainable_weights = []\n-        if not hasattr(self, 'regularizers'):\n-            self.regularizers = []\n+        if not hasattr(self, 'losses'):\n+            self.losses = []\n         if not hasattr(self, 'constraints'):\n             self.constraints = {}  # dict {tensor: constraint instance}\n         self.built = False\n@@ -354,6 +359,19 @@ class Layer(object):\n     def non_trainable_weights(self, weights):\n         self._non_trainable_weights = weights\n \n+    @property\n+    def regularizers(self):\n+        warnings.warn('The `regularizers` property of layers/models is deprecated. '\n+                      'Regularization losses are now managed via the `losses` '\n+                      'layer/model property.')\n+        return []\n+\n+    @regularizers.setter\n+    def regularizers(self, _):\n+        warnings.warn('The `regularizers` property of layers/models is deprecated. '\n+                      'Regularization losses are now managed via the `losses` '\n+                      'layer/model property.')\n+\n     def create_input_layer(self, batch_input_shape,\n                            input_dtype=None, name=None):\n         if not name:\n@@ -373,6 +391,32 @@ class Layer(object):\n         # to the input layer we just created.\n         self(x)\n \n+    def add_weight(self, shape, initializer, name=None,\n+                   trainable=True,\n+                   regularizer=None,\n+                   constraint=None):\n+        '''Adds a weight variable to the layer.\n+\n+        # Arguments:\n+            shape: The shape tuple of the weight.\n+            initializer: An Initializer instance (callable).\n+            trainable: A boolean, whether the weight should\n+                be trained via backprop or not (assuming\n+                that the layer itself is also trainable).\n+            regularizer: An optional Regularizer instance.\n+        '''\n+        initializer = initializations.get(initializer)\n+        weight = initializer(shape, name=name)\n+        if regularizer is not None:\n+            self.add_loss(regularizer(weight))\n+        if constraint is not None:\n+            self.constraints[weight] = constraint\n+        if trainable:\n+            self.trainable_weights.append(weight)\n+        else:\n+            self.non_trainable_weights.append(weight)\n+        return weight\n+\n     def assert_input_compatibility(self, input):\n         '''This checks that the tensor(s) `input`\n         verify the input assumptions of the layer\n@@ -519,15 +563,21 @@ class Layer(object):\n             self.add_inbound_node(inbound_layers, node_indices, tensor_indices)\n             # Outputs were already computed when calling self.add_inbound_node.\n             outputs = self.inbound_nodes[-1].output_tensors\n+        else:\n+            # This case appears if the input was not a Keras tensor.\n+            outputs = to_list(self.call(x, mask))\n+\n+        # Apply activity regularizer if any:\n+        if hasattr(self, 'activity_regularizer') and self.activity_regularizer is not None:\n+            regularization_losses = [self.activity_regularizer(x) for x in outputs]\n+            self.add_loss(regularization_losses, input_tensors)\n+\n         # If single output tensor: return it,\n         # else return a list (at least 2 elements).\n         if len(outputs) == 1:\n             return outputs[0]\n         else:\n             return outputs\n-        else:\n-            # This case appears if the input was not a Keras tensor.\n-            return self.call(x, mask)\n \n     def add_inbound_node(self, inbound_layers,\n                          node_indices=None, tensor_indices=None):\n@@ -806,20 +856,58 @@ class Layer(object):\n                             'ill-defined for the layer. ' +\n                             'Use `get_output_shape_at(node_index)` instead.')\n \n-    def add_updates(self, updates, inputs):\n+    def add_loss(self, losses, inputs=None):\n+        if losses is None:\n+            return\n+        # Update self.losses\n+        losses = to_list(losses)\n+        if not hasattr(self, 'losses'):\n+            self.losses = []\n+        try:\n+            self.losses += losses\n+        except AttributeError:\n+            # In case self.losses isn't settable\n+            # (i.e. it's a getter method).\n+            # In that case the `losses` property is\n+            # auto-computed and shouldn't be set.\n+            pass\n+        # Update self._per_input_updates\n+        if not hasattr(self, '_per_input_losses'):\n+            self._per_input_losses = {}\n+        if inputs is not None:\n+            inputs_hash = object_list_uid(inputs)\n+        else:\n+            # Updates indexed by None are unconditional\n+            # rather than input-dependent\n+            inputs_hash = None\n+        if inputs_hash not in self._per_input_losses:\n+            self._per_input_losses[inputs_hash] = []\n+        self._per_input_losses[inputs_hash] += losses\n+\n+    def add_update(self, updates, inputs=None):\n+        if updates is None:\n+            return\n         # Update self.updates\n+        updates = to_list(updates)\n         if not hasattr(self, 'updates'):\n             self.updates = []\n         try:\n             self.updates += updates\n         except AttributeError:\n+            # In case self.updates isn't settable\n+            # (i.e. it's a getter method).\n+            # In that case the `updates` property is\n+            # auto-computed and shouldn't be set.\n             pass\n         # Update self._per_input_updates\n         if not hasattr(self, '_per_input_updates'):\n             self._per_input_updates = {}\n-        inputs = to_list(inputs)\n-        updates = to_list(updates)\n-        inputs_hash = ', '.join([str(abs(id(x))) for x in inputs])\n+        if inputs is not None:\n+            inputs_hash = object_list_uid(inputs)\n+        else:\n+            # Updates indexed by None are unconditional\n+            # rather than input-dependent\n+            inputs_hash = None\n         if inputs_hash not in self._per_input_updates:\n             self._per_input_updates[inputs_hash] = []\n         self._per_input_updates[inputs_hash] += updates\n@@ -827,12 +915,19 @@ class Layer(object):\n     def get_updates_for(self, inputs):\n         if not hasattr(self, '_per_input_updates'):\n             return []\n-        inputs = to_list(inputs)\n-        inputs_hash = ', '.join([str(abs(id(x))) for x in inputs])\n+        inputs_hash = object_list_uid(inputs)\n         if inputs_hash in self._per_input_updates:\n             return self._per_input_updates[inputs_hash]\n         return []\n \n+    def get_losses_for(self, inputs):\n+        if not hasattr(self, '_per_input_losses'):\n+            return []\n+        inputs_hash = object_list_uid(inputs)\n+        if inputs_hash in self._per_input_losses:\n+            return self._per_input_losses[inputs_hash]\n+        return []\n+\n     @property\n     def weights(self):\n         return self.trainable_weights + self.non_trainable_weights\n@@ -950,7 +1045,6 @@ class InputLayer(Layer):\n \n         self.trainable_weights = []\n         self.non_trainable_weights = []\n-        self.regularizers = []\n         self.constraints = {}\n \n         self.sparse = sparse\n@@ -1151,7 +1245,6 @@ class Merge(Layer):\n         self.inbound_nodes = []\n         self.outbound_nodes = []\n         self.constraints = {}\n-        self.regularizers = []\n         self.trainable_weights = []\n         self.non_trainable_weights = []\n         self.supports_masking = True\n@@ -1587,7 +1680,6 @@ class Container(Layer):\n         supports_masking (boolean)\n         trainable_weights (list of variables)\n         non_trainable_weights (list of variables)\n-        regularizers (list of regularizers)\n         constraints (list of tuples (weight, constraint))\n \n     # Methods\n@@ -1901,7 +1993,6 @@ class Container(Layer):\n         self.supports_masking = False\n         # The following are implemented as property functions:\n         # self.constraints\n-        # self.regularizers\n         # self.trainable_weights\n         # self.non_trainable_weights\n         # self.input_spec\n@@ -1946,14 +2037,38 @@ class Container(Layer):\n                 if len(layer.inbound_nodes) == 1:\n                     updates += layer.updates\n                 else:\n+                    # Collect updates that are dependent on inputs\n+                    # that are part of the model.\n                     for node_index, node in enumerate(layer.inbound_nodes):\n                         node_key = layer.name + '_ib-' + str(node_index)\n                         if node_key in self.container_nodes:\n                             # The model owns this layer node.\n                             inputs = node.input_tensors\n                             updates += layer.get_updates_for(inputs)\n+                    # Collect unconditional updates.\n+                    updates += layer.get_updates_for(None)\n         return updates\n \n+    @property\n+    def losses(self):\n+        losses = []\n+        for layer in self.layers:\n+            if hasattr(layer, 'losses'):\n+                if len(layer.inbound_nodes) == 1:\n+                    losses += layer.losses\n+                else:\n+                    # Collect losses that are dependent on inputs\n+                    # that are part of the model.\n+                    for node_index, node in enumerate(layer.inbound_nodes):\n+                        node_key = layer.name + '_ib-' + str(node_index)\n+                        if node_key in self.container_nodes:\n+                            # The model owns this layer node.\n+                            inputs = node.input_tensors\n+                            losses += layer.get_losses_for(inputs)\n+                    # Collect unconditional losses.\n+                    losses += layer.get_losses_for(None)\n+        return losses\n+\n     @property\n     def stateful(self):\n         return any([(hasattr(layer, 'stateful') and layer.stateful) for layer in self.layers])\n@@ -1990,10 +2105,13 @@ class Container(Layer):\n \n     @property\n     def regularizers(self):\n-        regs = []\n-        for layer in self.layers:\n-            regs += layer.regularizers\n-        return regs\n+        warnings.warn('The `regularizers` attribute of layers/models '\n+                      'is deprecated. '\n+                      'Regularization losses are now managed via the `losses` '\n+                      'layer/model property.\\n'\n+                      'The `regularizers` attribute will be removed '\n+                      'after 06/2017.')\n+        return []\n \n     @property\n     def trainable_weights(self):\n@@ -2061,8 +2179,7 @@ class Container(Layer):\n         '''True if any layer in the graph uses it.\n         '''\n         layers_learning_phase = any([layer.uses_learning_phase for layer in self.layers])\n-        regs_learning_phase = any([reg.uses_learning_phase for reg in self.regularizers])\n-        return layers_learning_phase or regs_learning_phase\n+        return layers_learning_phase\n \n     def call(self, input, mask=None):\n         '''`call` just reapplies all ops in the graph to the new inputs\n@@ -2239,9 +2356,16 @@ class Container(Layer):\n                         output_tensors = to_list(layer.call(computed_tensors, computed_masks))\n                         output_masks = to_list(layer.compute_mask(computed_tensors, computed_masks))\n \n-                    # update model updates\n+                    # Update model updates and losses:\n                     layer_inputs = [x[0] for x in computed_data]\n-                    self.add_updates(layer.get_updates_for(layer_inputs), inputs)\n+                    # Keep track of updates that depend on the inputs (e.g. BN updates).\n+                    self.add_update(layer.get_updates_for(layer_inputs), inputs)\n+                    # Keep track of unconditional updates (e.g. a counter).\n+                    self.add_update(layer.get_updates_for(None), None)\n+                    # Keep track of losses that depend on the inputs (e.g. activity regularizers).\n+                    self.add_loss(layer.get_losses_for(layer_inputs), inputs)\n+                    # Keep track of unconditional losses (e.g. weight regularizers).\n+                    self.add_loss(layer.get_losses_for(None), None)\n \n                     # Update _keras_shape.\n                     if all([hasattr(x, '_keras_shape') for x in computed_tensors]):\n\n@@ -611,9 +611,10 @@ class Model(Container):\n             else:\n                 total_loss += loss_weight * output_loss\n \n-        # add regularization penalties to the loss\n-        for r in self.regularizers:\n-            total_loss = r(total_loss)\n+        # add regularization penalties\n+        # and other layer-specific losses\n+        for loss_tensor in self.losses:\n+            total_loss += loss_tensor\n \n         # list of same size as output_names.\n         # contains tuples (metrics for output, names of metrics)\n\n@@ -113,31 +113,20 @@ class Convolution1D(Layer):\n     def build(self, input_shape):\n         input_dim = input_shape[2]\n         self.W_shape = (self.filter_length, 1, input_dim, self.nb_filter)\n-        self.W = self.init(self.W_shape, name='{}_W'.format(self.name))\n+\n+        self.W = self.add_weight(self.W_shape,\n+                                 initializer=self.init,\n+                                 name='{}_W'.format(self.name),\n+                                 regularizer=self.W_regularizer,\n+                                 constraint=self.W_constraint)\n         if self.bias:\n-            self.b = K.zeros((self.nb_filter,), name='{}_b'.format(self.name))\n-            self.trainable_weights = [self.W, self.b]\n+            self.b = self.add_weight((self.nb_filter,),\n+                                     initializer='zero',\n+                                     name='{}_b'.format(self.name),\n+                                     regularizer=self.b_regularizer,\n+                                     constraint=self.b_constraint)\n         else:\n-            self.trainable_weights = [self.W]\n-        self.regularizers = []\n-\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(self.W)\n-            self.regularizers.append(self.W_regularizer)\n-\n-        if self.bias and self.b_regularizer:\n-            self.b_regularizer.set_param(self.b)\n-            self.regularizers.append(self.b_regularizer)\n-\n-        if self.activity_regularizer:\n-            self.activity_regularizer.set_layer(self)\n-            self.regularizers.append(self.activity_regularizer)\n-\n-        self.constraints = {}\n-        if self.W_constraint:\n-            self.constraints[self.W] = self.W_constraint\n-        if self.bias and self.b_constraint:\n-            self.constraints[self.b] = self.b_constraint\n+            self.b = None\n \n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n@@ -406,32 +395,20 @@ class Convolution2D(Layer):\n             stack_size = input_shape[3]\n             self.W_shape = (self.nb_row, self.nb_col, stack_size, self.nb_filter)\n         else:\n-            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n-        self.W = self.init(self.W_shape, name='{}_W'.format(self.name))\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+        self.W = self.add_weight(self.W_shape,\n+                                 initializer=self.init,\n+                                 name='{}_W'.format(self.name),\n+                                 regularizer=self.W_regularizer,\n+                                 constraint=self.W_constraint)\n         if self.bias:\n-            self.b = K.zeros((self.nb_filter,), name='{}_b'.format(self.name))\n-            self.trainable_weights = [self.W, self.b]\n+            self.b = self.add_weight((self.nb_filter,),\n+                                     initializer='zero',\n+                                     name='{}_b'.format(self.name),\n+                                     regularizer=self.b_regularizer,\n+                                     constraint=self.b_constraint)\n         else:\n-            self.trainable_weights = [self.W]\n-        self.regularizers = []\n-\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(self.W)\n-            self.regularizers.append(self.W_regularizer)\n-\n-        if self.bias and self.b_regularizer:\n-            self.b_regularizer.set_param(self.b)\n-            self.regularizers.append(self.b_regularizer)\n-\n-        if self.activity_regularizer:\n-            self.activity_regularizer.set_layer(self)\n-            self.regularizers.append(self.activity_regularizer)\n-\n-        self.constraints = {}\n-        if self.W_constraint:\n-            self.constraints[self.W] = self.W_constraint\n-        if self.bias and self.b_constraint:\n-            self.constraints[self.b] = self.b_constraint\n+            self.b = None\n \n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n@@ -957,40 +934,26 @@ class SeparableConvolution2D(Layer):\n             depthwise_shape = (self.nb_row, self.nb_col, stack_size, self.depth_multiplier)\n             pointwise_shape = (1, 1, self.depth_multiplier * stack_size, self.nb_filter)\n         else:\n-            raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n-        self.depthwise_kernel = self.init(depthwise_shape,\n+            raise Exception('Invalid dim_ordering: ' + self.dim_ordering)\n+\n+        self.depthwise_kernel = self.add_weight(depthwise_shape,\n+                                                initializer=self.init,\n+                                                regularizer=self.depthwise_regularizer,\n+                                                constraint=self.depthwise_constraint,\n                                                 name='{}_depthwise_kernel'.format(self.name))\n-        self.pointwise_kernel = self.init(pointwise_shape,\n+        self.pointwise_kernel = self.add_weight(pointwise_shape,\n+                                                initializer=self.init,\n+                                                regularizer=self.pointwise_regularizer,\n+                                                constraint=self.pointwise_constraint,\n                                                 name='{}_pointwise_kernel'.format(self.name))\n         if self.bias:\n-            self.b = K.zeros((self.nb_filter,), name='{}_b'.format(self.name))\n-            self.trainable_weights = [self.depthwise_kernel,\n-                                      self.pointwise_kernel,\n-                                      self.b]\n+            self.b = self.add_weight((self.nb_filter,),\n+                                     initializer='zero',\n+                                     name='{}_b'.format(self.name),\n+                                     regularizer=self.b_regularizer,\n+                                     constraint=self.b_constraint)\n         else:\n-            self.trainable_weights = [self.depthwise_kernel,\n-                                      self.pointwise_kernel]\n-        self.regularizers = []\n-        if self.depthwise_regularizer:\n-            self.depthwise_regularizer.set_param(self.depthwise_kernel)\n-            self.regularizers.append(self.depthwise_regularizer)\n-        if self.pointwise_regularizer:\n-            self.pointwise_regularizer.set_param(self.pointwise_kernel)\n-            self.regularizers.append(self.pointwise_regularizer)\n-        if self.bias and self.b_regularizer:\n-            self.b_regularizer.set_param(self.b)\n-            self.regularizers.append(self.b_regularizer)\n-        if self.activity_regularizer:\n-            self.activity_regularizer.set_layer(self)\n-            self.regularizers.append(self.activity_regularizer)\n-\n-        self.constraints = {}\n-        if self.depthwise_constraint:\n-            self.constraints[self.depthwise_kernel] = self.depthwise_constraint\n-        if self.pointwise_constraint:\n-            self.constraints[self.pointwise_kernel] = self.pointwise_constraint\n-        if self.bias and self.b_constraint:\n-            self.constraints[self.b] = self.b_constraint\n+            self.b = None\n \n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n@@ -1165,31 +1128,19 @@ class Convolution3D(Layer):\n         else:\n             raise ValueError('Invalid dim_ordering:', self.dim_ordering)\n \n-        self.W = self.init(self.W_shape, name='{}_W'.format(self.name))\n+        self.W = self.add_weight(self.W_shape,\n+                                 initializer=self.init,\n+                                 name='{}_W'.format(self.name),\n+                                 regularizer=self.W_regularizer,\n+                                 constraint=self.W_constraint)\n         if self.bias:\n-            self.b = K.zeros((self.nb_filter,), name='{}_b'.format(self.name))\n-            self.trainable_weights = [self.W, self.b]\n+            self.b = self.add_weight((self.nb_filter,),\n+                                     initializer='zero',\n+                                     name='{}_b'.format(self.name),\n+                                     regularizer=self.b_regularizer,\n+                                     constraint=self.b_constraint)\n         else:\n-            self.trainable_weights = [self.W]\n-\n-        self.regularizers = []\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(self.W)\n-            self.regularizers.append(self.W_regularizer)\n-\n-        if self.bias and self.b_regularizer:\n-            self.b_regularizer.set_param(self.b)\n-            self.regularizers.append(self.b_regularizer)\n-\n-        if self.activity_regularizer:\n-            self.activity_regularizer.set_layer(self)\n-            self.regularizers.append(self.activity_regularizer)\n-\n-        self.constraints = {}\n-        if self.W_constraint:\n-            self.constraints[self.W] = self.W_constraint\n-        if self.bias and self.b_constraint:\n-            self.constraints[self.b] = self.b_constraint\n+            self.b = None\n \n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n\n@@ -728,33 +728,19 @@ class Dense(Layer):\n         self.input_spec = [InputSpec(dtype=K.floatx(),\n                                      shape=(None, input_dim))]\n \n-        self.W = self.init((input_dim, self.output_dim),\n-                           name='{}_W'.format(self.name))\n+        self.W = self.add_weight((input_dim, self.output_dim),\n+                                 initializer=self.init,\n+                                 name='{}_W'.format(self.name),\n+                                 regularizer=self.W_regularizer,\n+                                 constraint=self.W_constraint)\n         if self.bias:\n-            self.b = K.zeros((self.output_dim,),\n-                             name='{}_b'.format(self.name))\n-            self.trainable_weights = [self.W, self.b]\n+            self.b = self.add_weight((self.output_dim,),\n+                                     initializer='zero',\n+                                     name='{}_b'.format(self.name),\n+                                     regularizer=self.b_regularizer,\n+                                     constraint=self.b_constraint)\n         else:\n-            self.trainable_weights = [self.W]\n-\n-        self.regularizers = []\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(self.W)\n-            self.regularizers.append(self.W_regularizer)\n-\n-        if self.bias and self.b_regularizer:\n-            self.b_regularizer.set_param(self.b)\n-            self.regularizers.append(self.b_regularizer)\n-\n-        if self.activity_regularizer:\n-            self.activity_regularizer.set_layer(self)\n-            self.regularizers.append(self.activity_regularizer)\n-\n-        self.constraints = {}\n-        if self.W_constraint:\n-            self.constraints[self.W] = self.W_constraint\n-        if self.bias and self.b_constraint:\n-            self.constraints[self.b] = self.b_constraint\n+            self.b = None\n \n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n@@ -808,9 +794,8 @@ class ActivityRegularization(Layer):\n         self.l2 = l2\n \n         super(ActivityRegularization, self).__init__(**kwargs)\n-        activity_regularizer = ActivityRegularizer(l1=l1, l2=l2)\n-        activity_regularizer.set_layer(self)\n-        self.regularizers = [activity_regularizer]\n+        self.activity_regularizer = regularizers.L1L2Regularizer(l1=l1, l2=l2)\n+        self.regularizers = [self.activity_regularizer]\n \n     def get_config(self):\n         config = {'l1': self.l1,\n@@ -897,33 +882,19 @@ class MaxoutDense(Layer):\n         self.input_spec = [InputSpec(dtype=K.floatx(),\n                                      shape=(None, input_dim))]\n \n-        self.W = self.init((self.nb_feature, input_dim, self.output_dim),\n-                           name='{}_W'.format(self.name))\n+        self.W = self.add_weight((self.nb_feature, input_dim, self.output_dim),\n+                                 initializer=self.init,\n+                                 name='{}_W'.format(self.name),\n+                                 regularizer=self.W_regularizer,\n+                                 constraint=self.W_constraint)\n         if self.bias:\n-            self.b = K.zeros((self.nb_feature, self.output_dim),\n-                             name='{}_b'.format(self.name))\n-            self.trainable_weights = [self.W, self.b]\n+            self.b = self.add_weight((self.nb_feature, self.output_dim,),\n+                                     initializer='zero',\n+                                     name='{}_b'.format(self.name),\n+                                     regularizer=self.b_regularizer,\n+                                     constraint=self.b_constraint)\n         else:\n-            self.trainable_weights = [self.W]\n-\n-        self.regularizers = []\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(self.W)\n-            self.regularizers.append(self.W_regularizer)\n-\n-        if self.bias and self.b_regularizer:\n-            self.b_regularizer.set_param(self.b)\n-            self.regularizers.append(self.b_regularizer)\n-\n-        if self.activity_regularizer:\n-            self.activity_regularizer.set_layer(self)\n-            self.regularizers.append(self.activity_regularizer)\n-\n-        self.constraints = {}\n-        if self.W_constraint:\n-            self.constraints[self.W] = self.W_constraint\n-        if self.bias and self.b_constraint:\n-            self.constraints[self.b] = self.b_constraint\n+            self.b = None\n \n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n@@ -1030,38 +1001,25 @@ class Highway(Layer):\n         self.input_spec = [InputSpec(dtype=K.floatx(),\n                                      shape=(None, input_dim))]\n \n-        self.W = self.init((input_dim, input_dim),\n-                           name='{}_W'.format(self.name))\n-        self.W_carry = self.init((input_dim, input_dim),\n+        self.W = self.add_weight((input_dim, input_dim),\n+                                 initializer=self.init,\n+                                 name='{}_W'.format(self.name),\n+                                 regularizer=self.W_regularizer,\n+                                 constraint=self.W_constraint)\n+        self.W_carry = self.add_weight((input_dim, input_dim),\n+                                       initializer=self.init,\n                                        name='{}_W_carry'.format(self.name))\n-\n         if self.bias:\n-            self.b = K.zeros((input_dim,), name='{}_b'.format(self.name))\n-            # initialize with a vector of values `transform_bias`\n-            self.b_carry = K.variable(np.ones((input_dim,)) * self.transform_bias,\n+            self.b = self.add_weight((input_dim,),\n+                                     initializer='zero',\n+                                     name='{}_b'.format(self.name),\n+                                     regularizer=self.b_regularizer,\n+                                     constraint=self.b_constraint)\n+            self.b_carry = self.add_weight((input_dim,),\n+                                           initializer='one',\n                                            name='{}_b_carry'.format(self.name))\n-            self.trainable_weights = [self.W, self.b, self.W_carry, self.b_carry]\n         else:\n-            self.trainable_weights = [self.W, self.W_carry]\n-\n-        self.regularizers = []\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(self.W)\n-            self.regularizers.append(self.W_regularizer)\n-\n-        if self.bias and self.b_regularizer:\n-            self.b_regularizer.set_param(self.b)\n-            self.regularizers.append(self.b_regularizer)\n-\n-        if self.activity_regularizer:\n-            self.activity_regularizer.set_layer(self)\n-            self.regularizers.append(self.activity_regularizer)\n-\n-        self.constraints = {}\n-        if self.W_constraint:\n-            self.constraints[self.W] = self.W_constraint\n-        if self.bias and self.b_constraint:\n-            self.constraints[self.b] = self.b_constraint\n+            self.b_carry = None\n \n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n@@ -1178,31 +1136,19 @@ class TimeDistributedDense(Layer):\n                                      shape=(None,) + input_shape[1:])]\n         input_dim = input_shape[2]\n \n-        self.W = self.init((input_dim, self.output_dim),\n-                           name='{}_W'.format(self.name))\n+        self.W = self.add_weight((input_dim, self.output_dim),\n+                                 initializer=self.init,\n+                                 name='{}_W'.format(self.name),\n+                                 regularizer=self.W_regularizer,\n+                                 constraint=self.W_constraint)\n         if self.bias:\n-            self.b = K.zeros((self.output_dim,),\n-                             name='{}_b'.format(self.name))\n-            self.trainable_weights = [self.W, self.b]\n-        self.regularizers = []\n-\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(self.W)\n-            self.regularizers.append(self.W_regularizer)\n-\n-        if self.bias and self.b_regularizer:\n-            self.b_regularizer.set_param(self.b)\n-            self.regularizers.append(self.b_regularizer)\n-\n-        if self.activity_regularizer:\n-            self.activity_regularizer.set_layer(self)\n-            self.regularizers.append(self.activity_regularizer)\n-\n-        self.constraints = {}\n-        if self.W_constraint:\n-            self.constraints[self.W] = self.W_constraint\n-        if self.bias and self.b_constraint:\n-            self.constraints[self.b] = self.b_constraint\n+            self.b = self.add_weight((self.output_dim,),\n+                                     initializer='zero',\n+                                     name='{}_b'.format(self.name),\n+                                     regularizer=self.b_regularizer,\n+                                     constraint=self.b_constraint)\n+        else:\n+            self.b = None\n \n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n\n@@ -91,22 +91,11 @@ class Embedding(Layer):\n         super(Embedding, self).__init__(**kwargs)\n \n     def build(self, input_shape):\n-        self.W = self.init((self.input_dim, self.output_dim),\n-                           name='{}_W'.format(self.name))\n-        self.trainable_weights = [self.W]\n-\n-        self.constraints = {}\n-        if self.W_constraint:\n-            self.constraints[self.W] = self.W_constraint\n-\n-        self.regularizers = []\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(self.W)\n-            self.regularizers.append(self.W_regularizer)\n-\n-        if self.activity_regularizer:\n-            self.activity_regularizer.set_layer(self)\n-            self.regularizers.append(self.activity_regularizer)\n+        self.W = self.add_weight((self.input_dim, self.output_dim),\n+                                 initializer=self.init,\n+                                 name='{}_W'.format(self.name),\n+                                 regularizer=self.W_regularizer,\n+                                 constraint=self.W_constraint)\n \n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n\n@@ -110,31 +110,21 @@ class LocallyConnected1D(Layer):\n     def build(self, input_shape):\n         input_dim = input_shape[2]\n         _, output_length, nb_filter = self.get_output_shape_for(input_shape)\n-\n         self.W_shape = (output_length, self.filter_length * input_dim, nb_filter)\n-        self.W = self.init(self.W_shape, name='{}_W'.format(self.name))\n+\n+        self.W = self.add_weight(self.W_shape,\n+                                 initializer=self.init,\n+                                 name='{}_W'.format(self.name),\n+                                 regularizer=self.W_regularizer,\n+                                 constraint=self.W_constraint)\n         if self.bias:\n-            self.b = K.zeros((output_length, self.nb_filter), name='{}_b'.format(self.name))\n-            self.trainable_weights = [self.W, self.b]\n+            self.b = self.add_weight((output_length, self.nb_filter),\n+                                     initializer='zero',\n+                                     name='{}_b'.format(self.name),\n+                                     regularizer=self.b_regularizer,\n+                                     constraint=self.b_constraint)\n         else:\n-            self.trainable_weights = [self.W]\n-\n-        self.regularizers = []\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(self.W)\n-            self.regularizers.append(self.W_regularizer)\n-        if self.b_regularizer:\n-            self.b_regularizer.set_param(self.b)\n-            self.regularizers.append(self.b_regularizer)\n-        if self.activity_regularizer:\n-            self.activity_regularizer.set_layer(self)\n-            self.regularizers.append(self.activity_regularizer)\n-\n-        self.constraints = {}\n-        if self.W_constraint:\n-            self.constraints[self.W] = self.W_constraint\n-        if self.b_constraint:\n-            self.constraints[self.b] = self.b_constraint\n+            self.b = None\n \n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n@@ -306,30 +296,20 @@ class LocallyConnected2D(Layer):\n         self.output_row = output_row\n         self.output_col = output_col\n         self.W_shape = (output_row * output_col, self.nb_row * self.nb_col * input_filter, nb_filter)\n-        self.W = self.init(self.W_shape, name='{}_W'.format(self.name))\n \n+        self.W = self.add_weight(self.W_shape,\n+                                 initializer=self.init,\n+                                 name='{}_W'.format(self.name),\n+                                 regularizer=self.W_regularizer,\n+                                 constraint=self.W_constraint)\n         if self.bias:\n-            self.b = K.zeros((output_row, output_col, nb_filter), name='{}_b'.format(self.name))\n-            self.trainable_weights = [self.W, self.b]\n+            self.b = self.add_weight((output_row, output_col, nb_filter),\n+                                     initializer='zero',\n+                                     name='{}_b'.format(self.name),\n+                                     regularizer=self.b_regularizer,\n+                                     constraint=self.b_constraint)\n         else:\n-            self.trainable_weights = [self.W]\n-\n-        self.regularizers = []\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(self.W)\n-            self.regularizers.append(self.W_regularizer)\n-        if self.bias and self.b_regularizer:\n-            self.b_regularizer.set_param(self.b)\n-            self.regularizers.append(self.b_regularizer)\n-        if self.activity_regularizer:\n-            self.activity_regularizer.set_layer(self)\n-            self.regularizers.append(self.activity_regularizer)\n-\n-        self.constraints = {}\n-        if self.W_constraint:\n-            self.constraints[self.W] = self.W_constraint\n-        if self.bias and self.b_constraint:\n-            self.constraints[self.b] = self.b_constraint\n+            self.b = None\n \n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n\n@@ -82,24 +82,20 @@ class BatchNormalization(Layer):\n         self.input_spec = [InputSpec(shape=input_shape)]\n         shape = (input_shape[self.axis],)\n \n-        self.gamma = self.gamma_init(shape, name='{}_gamma'.format(self.name))\n-        self.beta = self.beta_init(shape, name='{}_beta'.format(self.name))\n-        self.trainable_weights = [self.gamma, self.beta]\n-\n-        self.regularizers = []\n-        if self.gamma_regularizer:\n-            self.gamma_regularizer.set_param(self.gamma)\n-            self.regularizers.append(self.gamma_regularizer)\n-\n-        if self.beta_regularizer:\n-            self.beta_regularizer.set_param(self.beta)\n-            self.regularizers.append(self.beta_regularizer)\n-\n-        self.running_mean = K.zeros(shape,\n-                                    name='{}_running_mean'.format(self.name))\n-        self.running_std = K.ones(shape,\n-                                  name='{}_running_std'.format(self.name))\n-        self.non_trainable_weights = [self.running_mean, self.running_std]\n+        self.gamma = self.add_weight(shape,\n+                                     initializer=self.gamma_init,\n+                                     regularizer=self.gamma_regularizer,\n+                                     name='{}_gamma'.format(self.name))\n+        self.beta = self.add_weight(shape,\n+                                    initializer=self.beta_init,\n+                                    regularizer=self.beta_regularizer,\n+                                    name='{}_beta'.format(self.name))\n+        self.running_mean = self.add_weight(shape, initializer='zero',\n+                                            name='{}_running_mean'.format(self.name),\n+                                            trainable=False)\n+        self.running_std = self.add_weight(shape, initializer='one',\n+                                           name='{}_running_std'.format(self.name),\n+                                           trainable=False)\n \n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n@@ -121,7 +117,7 @@ class BatchNormalization(Layer):\n                 epsilon=self.epsilon)\n \n             if self.mode == 0:\n-                self.add_updates([K.moving_average_update(self.running_mean, mean, self.momentum),\n+                self.add_update([K.moving_average_update(self.running_mean, mean, self.momentum),\n                                  K.moving_average_update(self.running_std, std, self.momentum)], x)\n \n                 if sorted(reduction_axes) == range(K.ndim(x))[:-1]:\n\n@@ -229,7 +229,7 @@ class Recurrent(Layer):\n             updates = []\n             for i in range(len(states)):\n                 updates.append((self.states[i], states[i]))\n-            self.add_updates(updates, x)\n+            self.add_update(updates, x)\n \n         if self.return_sequences:\n             return outputs\n@@ -288,7 +288,8 @@ class SimpleRNN(Recurrent):\n         self.W_regularizer = regularizers.get(W_regularizer)\n         self.U_regularizer = regularizers.get(U_regularizer)\n         self.b_regularizer = regularizers.get(b_regularizer)\n-        self.dropout_W, self.dropout_U = dropout_W, dropout_U\n+        self.dropout_W = dropout_W\n+        self.dropout_U = dropout_U\n \n         if self.dropout_W or self.dropout_U:\n             self.uses_learning_phase = True\n@@ -304,24 +305,18 @@ class SimpleRNN(Recurrent):\n         input_dim = input_shape[2]\n         self.input_dim = input_dim\n \n-        self.W = self.init((input_dim, self.output_dim),\n-                           name='{}_W'.format(self.name))\n-        self.U = self.inner_init((self.output_dim, self.output_dim),\n-                                 name='{}_U'.format(self.name))\n-        self.b = K.zeros((self.output_dim,), name='{}_b'.format(self.name))\n-\n-        self.regularizers = []\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(self.W)\n-            self.regularizers.append(self.W_regularizer)\n-        if self.U_regularizer:\n-            self.U_regularizer.set_param(self.U)\n-            self.regularizers.append(self.U_regularizer)\n-        if self.b_regularizer:\n-            self.b_regularizer.set_param(self.b)\n-            self.regularizers.append(self.b_regularizer)\n-\n-        self.trainable_weights = [self.W, self.U, self.b]\n+        self.W = self.add_weight((input_dim, self.output_dim),\n+                                 initializer=self.init,\n+                                 name='{}_W'.format(self.name),\n+                                 regularizer=self.W_regularizer)\n+        self.U = self.add_weight((self.output_dim, self.output_dim),\n+                                 initializer=self.inner_init,\n+                                 name='{}_U'.format(self.name),\n+                                 regularizer=self.U_regularizer)\n+        self.b = self.add_weight((self.output_dim,),\n+                                 initializer='zero',\n+                                 name='{}_b'.format(self.name),\n+                                 regularizer=self.b_regularizer)\n \n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n@@ -446,7 +441,8 @@ class GRU(Recurrent):\n         self.W_regularizer = regularizers.get(W_regularizer)\n         self.U_regularizer = regularizers.get(U_regularizer)\n         self.b_regularizer = regularizers.get(b_regularizer)\n-        self.dropout_W, self.dropout_U = dropout_W, dropout_U\n+        self.dropout_W = dropout_W\n+        self.dropout_U = dropout_U\n \n         if self.dropout_W or self.dropout_U:\n             self.uses_learning_phase = True\n@@ -463,57 +459,59 @@ class GRU(Recurrent):\n             self.states = [None]\n \n         if self.consume_less == 'gpu':\n-\n-            self.W = self.init((self.input_dim, 3 * self.output_dim),\n-                               name='{}_W'.format(self.name))\n-            self.U = self.inner_init((self.output_dim, 3 * self.output_dim),\n-                                     name='{}_U'.format(self.name))\n-\n-            self.b = K.variable(np.hstack((np.zeros(self.output_dim),\n-                                           np.zeros(self.output_dim),\n-                                           np.zeros(self.output_dim))),\n-                                name='{}_b'.format(self.name))\n-\n-            self.trainable_weights = [self.W, self.U, self.b]\n+            self.W = self.add_weight((self.input_dim, 3 * self.output_dim),\n+                                     initializer=self.init,\n+                                     name='{}_W'.format(self.name),\n+                                     regularizer=self.W_regularizer)\n+            self.U = self.add_weight((self.output_dim, 3 * self.output_dim),\n+                                     initializer=self.inner_init,\n+                                     name='{}_U'.format(self.name),\n+                                     regularizer=self.U_regularizer)\n+            self.b = self.add_weight((self.output_dim * 3,),\n+                                     initializer='zero',\n+                                     name='{}_b'.format(self.name),\n+                                     regularizer=self.b_regularizer)\n         else:\n-\n-            self.W_z = self.init((self.input_dim, self.output_dim),\n-                                 name='{}_W_z'.format(self.name))\n-            self.U_z = self.inner_init((self.output_dim, self.output_dim),\n-                                       name='{}_U_z'.format(self.name))\n-            self.b_z = K.zeros((self.output_dim,), name='{}_b_z'.format(self.name))\n-\n-            self.W_r = self.init((self.input_dim, self.output_dim),\n-                                 name='{}_W_r'.format(self.name))\n-            self.U_r = self.inner_init((self.output_dim, self.output_dim),\n-                                       name='{}_U_r'.format(self.name))\n-            self.b_r = K.zeros((self.output_dim,), name='{}_b_r'.format(self.name))\n-\n-            self.W_h = self.init((self.input_dim, self.output_dim),\n-                                 name='{}_W_h'.format(self.name))\n-            self.U_h = self.inner_init((self.output_dim, self.output_dim),\n-                                       name='{}_U_h'.format(self.name))\n-            self.b_h = K.zeros((self.output_dim,), name='{}_b_h'.format(self.name))\n-\n-            self.trainable_weights = [self.W_z, self.U_z, self.b_z,\n-                                      self.W_r, self.U_r, self.b_r,\n-                                      self.W_h, self.U_h, self.b_h]\n-\n+            self.W_z = self.add_weight((self.input_dim, self.output_dim),\n+                                       initializer=self.init,\n+                                       name='{}_W_z'.format(self.name),\n+                                       regularizer=self.W_regularizer)\n+            self.U_z = self.add_weight((self.output_dim, self.output_dim),\n+                                       initializer=self.init,\n+                                       name='{}_U_z'.format(self.name),\n+                                       regularizer=self.W_regularizer)\n+            self.b_z = self.add_weight((self.output_dim,),\n+                                       initializer='zero',\n+                                       name='{}_b_z'.format(self.name),\n+                                       regularizer=self.b_regularizer)\n+            self.W_r = self.add_weight((self.input_dim, self.output_dim),\n+                                       initializer=self.init,\n+                                       name='{}_W_r'.format(self.name),\n+                                       regularizer=self.W_regularizer)\n+            self.U_r = self.add_weight((self.output_dim, self.output_dim),\n+                                       initializer=self.init,\n+                                       name='{}_U_r'.format(self.name),\n+                                       regularizer=self.W_regularizer)\n+            self.b_r = self.add_weight((self.output_dim,),\n+                                       initializer='zero',\n+                                       name='{}_b_r'.format(self.name),\n+                                       regularizer=self.b_regularizer)\n+            self.W_h = self.add_weight((self.input_dim, self.output_dim),\n+                                       initializer=self.init,\n+                                       name='{}_W_h'.format(self.name),\n+                                       regularizer=self.W_regularizer)\n+            self.U_h = self.add_weight((self.output_dim, self.output_dim),\n+                                       initializer=self.init,\n+                                       name='{}_U_h'.format(self.name),\n+                                       regularizer=self.W_regularizer)\n+            self.b_h = self.add_weight((self.output_dim,),\n+                                       initializer='zero',\n+                                       name='{}_b_h'.format(self.name),\n+                                       regularizer=self.b_regularizer)\n             self.W = K.concatenate([self.W_z, self.W_r, self.W_h])\n             self.U = K.concatenate([self.U_z, self.U_r, self.U_h])\n             self.b = K.concatenate([self.b_z, self.b_r, self.b_h])\n \n-        self.regularizers = []\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(self.W)\n-            self.regularizers.append(self.W_regularizer)\n-        if self.U_regularizer:\n-            self.U_regularizer.set_param(self.U)\n-            self.regularizers.append(self.U_regularizer)\n-        if self.b_regularizer:\n-            self.b_regularizer.set_param(self.b)\n-            self.regularizers.append(self.b_regularizer)\n-\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n@@ -671,7 +669,8 @@ class LSTM(Recurrent):\n         self.W_regularizer = regularizers.get(W_regularizer)\n         self.U_regularizer = regularizers.get(U_regularizer)\n         self.b_regularizer = regularizers.get(b_regularizer)\n-        self.dropout_W, self.dropout_U = dropout_W, dropout_U\n+        self.dropout_W = dropout_W\n+        self.dropout_U = dropout_U\n \n         if self.dropout_W or self.dropout_U:\n             self.uses_learning_phase = True\n@@ -688,63 +687,83 @@ class LSTM(Recurrent):\n             self.states = [None, None]\n \n         if self.consume_less == 'gpu':\n-            self.W = self.init((self.input_dim, 4 * self.output_dim),\n-                               name='{}_W'.format(self.name))\n-            self.U = self.inner_init((self.output_dim, 4 * self.output_dim),\n-                                     name='{}_U'.format(self.name))\n+            self.W = self.add_weight((self.input_dim, 4 * self.output_dim),\n+                                     initializer=self.init,\n+                                     name='{}_W'.format(self.name),\n+                                     regularizer=self.W_regularizer)\n+            self.U = self.add_weight((self.output_dim, 4 * self.output_dim),\n+                                     initializer=self.inner_init,\n+                                     name='{}_U'.format(self.name),\n+                                     regularizer=self.U_regularizer)\n \n-            self.b = K.variable(np.hstack((np.zeros(self.output_dim),\n+            def b_reg(shape, name=None):\n+                return K.variable(np.hstack((np.zeros(self.output_dim),\n                                              K.get_value(self.forget_bias_init((self.output_dim,))),\n                                              np.zeros(self.output_dim),\n                                              np.zeros(self.output_dim))),\n                                   name='{}_b'.format(self.name))\n-            self.trainable_weights = [self.W, self.U, self.b]\n+            self.b = self.add_weight((self.output_dim * 4,),\n+                                     initializer=b_reg,\n+                                     name='{}_b'.format(self.name),\n+                                     regularizer=self.b_regularizer)\n         else:\n-            self.W_i = self.init((self.input_dim, self.output_dim),\n-                                 name='{}_W_i'.format(self.name))\n-            self.U_i = self.inner_init((self.output_dim, self.output_dim),\n-                                       name='{}_U_i'.format(self.name))\n-            self.b_i = K.zeros((self.output_dim,), name='{}_b_i'.format(self.name))\n-\n-            self.W_f = self.init((self.input_dim, self.output_dim),\n-                                 name='{}_W_f'.format(self.name))\n-            self.U_f = self.inner_init((self.output_dim, self.output_dim),\n-                                       name='{}_U_f'.format(self.name))\n-            self.b_f = self.forget_bias_init((self.output_dim,),\n-                                             name='{}_b_f'.format(self.name))\n-\n-            self.W_c = self.init((self.input_dim, self.output_dim),\n-                                 name='{}_W_c'.format(self.name))\n-            self.U_c = self.inner_init((self.output_dim, self.output_dim),\n-                                       name='{}_U_c'.format(self.name))\n-            self.b_c = K.zeros((self.output_dim,), name='{}_b_c'.format(self.name))\n-\n-            self.W_o = self.init((self.input_dim, self.output_dim),\n-                                 name='{}_W_o'.format(self.name))\n-            self.U_o = self.inner_init((self.output_dim, self.output_dim),\n-                                       name='{}_U_o'.format(self.name))\n-            self.b_o = K.zeros((self.output_dim,), name='{}_b_o'.format(self.name))\n+            self.W_i = self.add_weight((self.input_dim, self.output_dim),\n+                                       initializer=self.init,\n+                                       name='{}_W_i'.format(self.name),\n+                                       regularizer=self.W_regularizer)\n+            self.U_i = self.add_weight((self.output_dim, self.output_dim),\n+                                       initializer=self.init,\n+                                       name='{}_U_i'.format(self.name),\n+                                       regularizer=self.W_regularizer)\n+            self.b_i = self.add_weight((self.output_dim,),\n+                                       initializer='zero',\n+                                       name='{}_b_i'.format(self.name),\n+                                       regularizer=self.b_regularizer)\n+            self.W_f = self.add_weight((self.input_dim, self.output_dim),\n+                                       initializer=self.init,\n+                                       name='{}_W_f'.format(self.name),\n+                                       regularizer=self.W_regularizer)\n+            self.U_f = self.add_weight((self.output_dim, self.output_dim),\n+                                       initializer=self.init,\n+                                       name='{}_U_f'.format(self.name),\n+                                       regularizer=self.W_regularizer)\n+            self.b_f = self.add_weight((self.output_dim,),\n+                                       initializer=self.forget_bias_init,\n+                                       name='{}_b_f'.format(self.name),\n+                                       regularizer=self.b_regularizer)\n+            self.W_c = self.add_weight((self.input_dim, self.output_dim),\n+                                       initializer=self.init,\n+                                       name='{}_W_c'.format(self.name),\n+                                       regularizer=self.W_regularizer)\n+            self.U_c = self.add_weight((self.output_dim, self.output_dim),\n+                                       initializer=self.init,\n+                                       name='{}_U_c'.format(self.name),\n+                                       regularizer=self.W_regularizer)\n+            self.b_c = self.add_weight((self.output_dim,),\n+                                       initializer='zero',\n+                                       name='{}_b_c'.format(self.name),\n+                                       regularizer=self.b_regularizer)\n+            self.W_o = self.add_weight((self.input_dim, self.output_dim),\n+                                       initializer=self.init,\n+                                       name='{}_W_o'.format(self.name),\n+                                       regularizer=self.W_regularizer)\n+            self.U_o = self.add_weight((self.output_dim, self.output_dim),\n+                                       initializer=self.init,\n+                                       name='{}_U_o'.format(self.name),\n+                                       regularizer=self.W_regularizer)\n+            self.b_o = self.add_weight((self.output_dim,),\n+                                       initializer='zero',\n+                                       name='{}_b_o'.format(self.name),\n+                                       regularizer=self.b_regularizer)\n \n             self.trainable_weights = [self.W_i, self.U_i, self.b_i,\n                                       self.W_c, self.U_c, self.b_c,\n                                       self.W_f, self.U_f, self.b_f,\n                                       self.W_o, self.U_o, self.b_o]\n-\n             self.W = K.concatenate([self.W_i, self.W_f, self.W_c, self.W_o])\n             self.U = K.concatenate([self.U_i, self.U_f, self.U_c, self.U_o])\n             self.b = K.concatenate([self.b_i, self.b_f, self.b_c, self.b_o])\n \n-        self.regularizers = []\n-        if self.W_regularizer:\n-            self.W_regularizer.set_param(self.W)\n-            self.regularizers.append(self.W_regularizer)\n-        if self.U_regularizer:\n-            self.U_regularizer.set_param(self.U)\n-            self.regularizers.append(self.U_regularizer)\n-        if self.b_regularizer:\n-            self.b_regularizer.set_param(self.b)\n-            self.regularizers.append(self.b_regularizer)\n-\n         if self.initial_weights is not None:\n             self.set_weights(self.initial_weights)\n             del self.initial_weights\n\n@@ -17,7 +17,7 @@ class Wrapper(Layer):\n         self.trainable_weights = getattr(self.layer, 'trainable_weights', [])\n         self.non_trainable_weights = getattr(self.layer, 'non_trainable_weights', [])\n         self.updates = getattr(self.layer, 'updates', [])\n-        self.regularizers = getattr(self.layer, 'regularizers', [])\n+        self.losses = getattr(self.layer, 'losses', [])\n         self.constraints = getattr(self.layer, 'constraints', {})\n \n         # properly attribute the current layer to\n@@ -130,6 +130,11 @@ class TimeDistributed(Wrapper):\n             # (nb_samples, timesteps, ...)\n             output_shape = self.get_output_shape_for(input_shape)\n             y = K.reshape(y, (-1, input_length) + output_shape[2:])\n+\n+        # Apply activity regularizer if any:\n+        if hasattr(self.layer, 'activity_regularizer') and self.layer.activity_regularizer is not None:\n+            regularization_loss = self.layer.activity_regularizer(y)\n+            self.add_loss(regularization_loss, X)\n         return y\n \n \n@@ -246,9 +251,9 @@ class Bidirectional(Wrapper):\n         return []\n \n     @property\n-    def regularizers(self):\n-        if hasattr(self.forward_layer, 'regularizers'):\n-            return self.forward_layer.regularizers + self.backward_layer.regularizers\n+    def losses(self):\n+        if hasattr(self.forward_layer, 'losses'):\n+            return self.forward_layer.losses + self.backward_layer.losses\n         return []\n \n     @property\n\n@@ -497,6 +497,13 @@ class Sequential(Model):\n     def get_updates_for(self, inputs):\n         return self.model.get_updates_for(inputs)\n \n+    @property\n+    def losses(self):\n+        return self.model.losses\n+\n+    def get_losses_for(self, inputs):\n+        return self.model.get_losses_for(inputs)\n+\n     @property\n     def regularizers(self):\n         # support for legacy behavior\n\n@@ -1,22 +1,27 @@\n from __future__ import absolute_import\n from . import backend as K\n from .utils.generic_utils import get_from_module\n+import warnings\n \n \n class Regularizer(object):\n \n-    def set_param(self, p):\n-        self.p = p\n-\n-    def set_layer(self, layer):\n-        self.layer = layer\n-\n-    def __call__(self, loss):\n-        return loss\n+    def __call__(self, x):\n+        return 0\n \n     def get_config(self):\n         return {'name': self.__class__.__name__}\n \n+    def set_param(self, _):\n+        warnings.warn('The `set_param` method on regularizers is deprecated. '\n+                      'It no longer does anything, '\n+                      'and it will be removed after 06/2017.')\n+\n+    def set_layer(self, _):\n+        warnings.warn('The `set_layer` method on regularizers is deprecated. '\n+                      'It no longer does anything, '\n+                      'and it will be removed after 06/2017.')\n+\n \n class EigenvalueRegularizer(Regularizer):\n     '''This takes a constant that controls\n@@ -28,71 +33,43 @@ class EigenvalueRegularizer(Regularizer):\n     '''\n     def __init__(self, k):\n         self.k = k\n-        self.uses_learning_phase = True\n \n-    def set_param(self, p):\n-        if hasattr(self, 'p'):\n-            raise Exception('Regularizers cannot be reused. '\n-                            'Instantiate one regularizer per layer.')\n-        self.p = p\n+    def __call__(self, x):\n+        if K.ndim(x) > 2:\n+            raise Exception('EigenvalueRegularizer '\n+                            'is only available for tensors of rank 2.')\n+        covariance = K.dot(K.transpose(x), x)\n+        dim1, dim2 = K.eval(K.shape(covariance))\n \n-    def __call__(self, loss):\n-        power = 9  # number of iterations of the power method\n-        W = self.p\n-        if K.ndim(W) > 2:\n-            raise Exception('Eigenvalue Decay regularizer '\n-                            'is only available for dense '\n-                            'and embedding layers.')\n-        WW = K.dot(K.transpose(W), W)\n-        dim1, dim2 = K.eval(K.shape(WW))  # number of neurons in the layer\n-\n-        # power method for approximating the dominant eigenvector:\n-        o = K.ones([dim1, 1])  # initial values for the dominant eigenvector\n-        main_eigenvect = K.dot(WW, o)\n+        # Power method for approximating the dominant eigenvector:\n+        power = 9  # Number of iterations of the power method.\n+        o = K.ones([dim1, 1])  # Initial values for the dominant eigenvector.\n+        main_eigenvect = K.dot(covariance, o)\n         for n in range(power - 1):\n-            main_eigenvect = K.dot(WW, main_eigenvect)\n+            main_eigenvect = K.dot(covariance, main_eigenvect)\n+        covariance_d = K.dot(covariance, main_eigenvect)\n \n-        WWd = K.dot(WW, main_eigenvect)\n-\n-        # the corresponding dominant eigenvalue:\n-        main_eigenval = (K.dot(K.transpose(WWd), main_eigenvect) /\n+        # The corresponding dominant eigenvalue:\n+        main_eigenval = (K.dot(K.transpose(covariance_d), main_eigenvect) /\n                          K.dot(K.transpose(main_eigenvect), main_eigenvect))\n-        # multiplied by the given regularization gain\n-        regularized_loss = loss + (main_eigenval ** 0.5) * self.k\n-\n-        return K.in_train_phase(regularized_loss[0, 0], loss)\n+        # Multiply by the given regularization gain.\n+        regularization = (main_eigenval ** 0.5) * self.k\n+        return K.sum(regularization)\n \n \n-class WeightRegularizer(Regularizer):\n+class L1L2Regularizer(Regularizer):\n \n     def __init__(self, l1=0., l2=0.):\n         self.l1 = K.cast_to_floatx(l1)\n         self.l2 = K.cast_to_floatx(l2)\n-        self.uses_learning_phase = True\n-        self.p = None\n \n-    def set_param(self, p):\n-        if self.p is not None:\n-            raise Exception('Regularizers cannot be reused. '\n-                            'Instantiate one regularizer per layer.')\n-        self.p = p\n-\n-    def __call__(self, loss):\n-        if self.p is None:\n-            raise Exception('Need to call `set_param` on '\n-                            'WeightRegularizer instance '\n-                            'before calling the instance. '\n-                            'Check that you are not passing '\n-                            'a WeightRegularizer instead of an '\n-                            'ActivityRegularizer '\n-                            '(i.e. activity_regularizer=\"l2\" instead '\n-                            'of activity_regularizer=\"activity_l2\".')\n-        regularized_loss = loss\n+    def __call__(self, x):\n+        regularization = 0\n         if self.l1:\n-            regularized_loss += K.sum(self.l1 * K.abs(self.p))\n+            regularization += K.sum(self.l1 * K.abs(x))\n         if self.l2:\n-            regularized_loss += K.sum(self.l2 * K.square(self.p))\n-        return K.in_train_phase(regularized_loss, loss)\n+            regularization += K.sum(self.l2 * K.square(x))\n+        return regularization\n \n     def get_config(self):\n         return {'name': self.__class__.__name__,\n@@ -100,61 +77,34 @@ class WeightRegularizer(Regularizer):\n                 'l2': float(self.l2)}\n \n \n-class ActivityRegularizer(Regularizer):\n+# Aliases.\n \n-    def __init__(self, l1=0., l2=0.):\n-        self.l1 = K.cast_to_floatx(l1)\n-        self.l2 = K.cast_to_floatx(l2)\n-        self.uses_learning_phase = True\n-        self.layer = None\n-\n-    def set_layer(self, layer):\n-        if self.layer is not None:\n-            raise Exception('Regularizers cannot be reused')\n-        self.layer = layer\n-\n-    def __call__(self, loss):\n-        if self.layer is None:\n-            raise Exception('Need to call `set_layer` on '\n-                            'ActivityRegularizer instance '\n-                            'before calling the instance.')\n-        regularized_loss = loss\n-        for i in range(len(self.layer.inbound_nodes)):\n-            output = self.layer.get_output_at(i)\n-            if self.l1:\n-                regularized_loss += K.sum(self.l1 * K.abs(output))\n-            if self.l2:\n-                regularized_loss += K.sum(self.l2 * K.square(output))\n-        return K.in_train_phase(regularized_loss, loss)\n-\n-    def get_config(self):\n-        return {'name': self.__class__.__name__,\n-                'l1': float(self.l1),\n-                'l2': float(self.l2)}\n+WeightRegularizer = L1L2Regularizer\n+ActivityRegularizer = L1L2Regularizer\n \n \n def l1(l=0.01):\n-    return WeightRegularizer(l1=l)\n+    return L1L2Regularizer(l1=l)\n \n \n def l2(l=0.01):\n-    return WeightRegularizer(l2=l)\n+    return L1L2Regularizer(l2=l)\n \n \n def l1l2(l1=0.01, l2=0.01):\n-    return WeightRegularizer(l1=l1, l2=l2)\n+    return L1L2Regularizer(l1=l1, l2=l2)\n \n \n def activity_l1(l=0.01):\n-    return ActivityRegularizer(l1=l)\n+    return L1L2Regularizer(l1=l)\n \n \n def activity_l2(l=0.01):\n-    return ActivityRegularizer(l2=l)\n+    return L1L2Regularizer(l2=l)\n \n \n def activity_l1l2(l1=0.01, l2=0.01):\n-    return ActivityRegularizer(l1=l1, l2=l2)\n+    return L1L2Regularizer(l1=l1, l2=l2)\n \n \n def get(identifier, kwargs=None):\n\n@@ -132,6 +132,12 @@ def test_regularizer(layer_class):\n     layer.build(shape)\n     output = layer(K.variable(np.ones(shape)))\n     K.eval(output)\n+    if layer_class == recurrent.SimpleRNN:\n+        assert len(layer.losses) == 3\n+    if layer_class == recurrent.GRU:\n+        assert len(layer.losses) == 9\n+    if layer_class == recurrent.LSTM:\n+        assert len(layer.losses) == 12\n \n \n @keras_test\n\n@@ -76,6 +76,15 @@ def test_TimeDistributed():\n     outer_model.fit(np.random.random((10, 3, 2)), np.random.random((10, 3, 3)), nb_epoch=1, batch_size=10)\n \n \n+@keras_test\n+def test_regularizers():\n+    model = Sequential()\n+    model.add(wrappers.TimeDistributed(core.Dense(2, W_regularizer='l1'), input_shape=(3, 4)))\n+    model.add(core.Activation('relu'))\n+    model.compile(optimizer='rmsprop', loss='mse')\n+    assert len(model.losses) == 1\n+\n+\n @keras_test\n def test_Bidirectional():\n     rnn = recurrent.SimpleRNN\n\n@@ -67,6 +67,7 @@ def test_W_reg():\n                 regularizers.l1l2()]:\n         model = create_model(weight_reg=reg)\n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+        assert len(model.losses) == 1\n         model.fit(X_train, Y_train, batch_size=batch_size,\n                   nb_epoch=nb_epoch, verbose=0)\n         model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n@@ -77,6 +78,7 @@ def test_A_reg():\n     for reg in [regularizers.activity_l1(), regularizers.activity_l2()]:\n         model = create_model(activity_reg=reg)\n         model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n+        assert len(model.losses) == 1\n         model.fit(X_train, Y_train, batch_size=batch_size,\n                   nb_epoch=nb_epoch, verbose=0)\n         model.evaluate(X_test[test_ids, :], Y_test[test_ids, :], verbose=0)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
