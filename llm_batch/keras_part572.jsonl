{"custom_id": "keras#f3d099d8aa38b2b40664f794ff276b52e6dc523d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1055 | Lines Deleted: 0 | Files Changed: 8 | Hunks: 8 | Methods Changed: 42 | Complexity Δ (Sum/Max): 63/11 | Churn Δ: 1055 | Churn Cumulative: 3165 | Contributors (this commit): 2 | Commits (past 90d): 24 | Contributors (cumulative): 16 | DMM Complexity: 0.899236641221374\n\nDIFF:\n@@ -0,0 +1,190 @@\n+from keras_core import backend\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.optimizers import optimizer\n+\n+\n+@keras_core_export([\"keras_core.optimizers.Adafactor\"])\n+class Adafactor(optimizer.Optimizer):\n+    \"\"\"Optimizer that implements the Adafactor algorithm.\n+\n+    Adafactor is commonly used in NLP tasks, and has the advantage\n+    of taking less memory because it only saves partial information of previous\n+    gradients.\n+\n+    The default argument setup is based on the original paper (see reference).\n+    When gradients are of dimension > 2, Adafactor optimizer will delete the\n+    last 2 dimensions separately in its accumulator variables.\n+\n+    Args:\n+        learning_rate: Initial value for the learning rate:\n+            a floating point value, Defaults to 0.001.\n+        beta_2_decay: float, defaults to -0.8. The decay rate of `beta_2`.\n+        epsilon_1: float, defaults to 1e-30. A small offset to keep demoninator\n+            away from 0.\n+        epsilon_2: float, defaults to 1e-3. A small offset to avoid learning\n+            rate becoming too small by time.\n+        clip_threshold: float, defaults to 1.0. Clipping threshold. This is a\n+            part of Adafactor algorithm, independent from `clipnorm`,\n+            `clipvalue`, and `global_clipnorm`.\n+        relative_step: bool, defaults to True. If `learning_rate` is a\n+            constant and `relative_step=True`, learning rate will be adjusted\n+            based on current iterations. This is a default learning rate decay\n+            in Adafactor.\n+        {{base_optimizer_keyword_args}}\n+\n+    Reference:\n+\n+    - [Shazeer, Noam et al., 2018](https://arxiv.org/abs/1804.04235).\n+\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        learning_rate=0.001,\n+        beta_2_decay=-0.8,\n+        epsilon_1=1e-30,\n+        epsilon_2=1e-3,\n+        clip_threshold=1.0,\n+        relative_step=True,\n+        weight_decay=None,\n+        clipnorm=None,\n+        clipvalue=None,\n+        global_clipnorm=None,\n+        use_ema=False,\n+        ema_momentum=0.99,\n+        ema_overwrite_frequency=None,\n+        name=\"adafactor\",\n+    ):\n+        super().__init__(\n+            learning_rate=learning_rate,\n+            name=name,\n+            weight_decay=weight_decay,\n+            clipnorm=clipnorm,\n+            clipvalue=clipvalue,\n+            global_clipnorm=global_clipnorm,\n+            use_ema=use_ema,\n+            ema_momentum=ema_momentum,\n+            ema_overwrite_frequency=ema_overwrite_frequency,\n+        )\n+        self.beta_2_decay = beta_2_decay\n+        self.epsilon_1 = epsilon_1\n+        self.epsilon_2 = epsilon_2\n+        self.clip_threshold = clip_threshold\n+        self.relative_step = relative_step\n+\n+    def build(self, var_list):\n+        \"\"\"Initialize optimizer variables.\n+\n+        Adam optimizer has 3 types of variables: momentums, velocities and\n+        velocity_hat (only set when amsgrad is applied),\n+\n+        Args:\n+            var_list: list of model variables to build Adam variables on.\n+        \"\"\"\n+        if self.built:\n+            return\n+        super().build(var_list)\n+        self._r = []\n+        self._c = []\n+        self._v = []\n+        for var in var_list:\n+            if len(var.shape) < 2:\n+                # Don't factor if variable is of dimension < 2, but we still\n+                # need to create dummy variables as placeholder.\n+                self._r.append(backend.Variable(0, name=var.name))\n+                self._c.append(backend.Variable(0, name=var.name))\n+            else:\n+                # Always factor the last 2 dimenstions.\n+                r_shape = var.shape[:-1]\n+                c_shape = var.shape[:-2] + var.shape[-1]\n+                self._r.append(\n+                    self.add_variable(\n+                        shape=r_shape,\n+                        dtype=var.dtype,\n+                        name=var.name,\n+                    )\n+                )\n+                self._c.append(\n+                    self.add_variable(\n+                        shape=c_shape,\n+                        dtype=var.dtype,\n+                        name=var.name,\n+                    )\n+                )\n+            self._v.append(\n+                self.add_variable_from_reference(\n+                    reference_variable=var, name=\"v\"\n+                )\n+            )\n+\n+    def _rms(self, x):\n+        return ops.sqrt(ops.mean(ops.square(x)))\n+\n+    def update_step(self, gradient, variable, learning_rate):\n+        \"\"\"Update step given gradient and the associated model variable.\"\"\"\n+\n+        lr = ops.cast(learning_rate, variable.dtype)\n+        gradient = ops.cast(gradient, variable.dtype)\n+        epsilon_2 = ops.cast(self.epsilon_2, variable.dtype)\n+        one = ops.cast(1.0, variable.dtype)\n+        local_step = ops.cast(self.iterations + 1, variable.dtype)\n+        if self.relative_step:  # TODO: add learning_rate_schedule logic\n+            # If `relative_step=True` and learning rate is a constant, we\n+            # apply the relative step algorithm.\n+            lr = ops.minimum(lr, 1 / ops.sqrt(local_step))\n+\n+        r = self._r[self._get_variable_index(variable)]\n+        c = self._c[self._get_variable_index(variable)]\n+        v = self._v[self._get_variable_index(variable)]\n+\n+        rho_t = ops.minimum(lr, 1 / ops.sqrt(local_step))\n+        alpha_t = ops.maximum(epsilon_2, self._rms(variable)) * rho_t\n+        regulated_grad_square = ops.square(gradient) + self.epsilon_1\n+        beta_2_t = 1 - ops.power(local_step, self.beta_2_decay)\n+\n+        if len(variable.shape) >= 2:\n+            # `r` deletes the last dimension of gradient, so it is of shape\n+            # `gradient.shape[:-1]`.\n+            r.assign(\n+                beta_2_t * r\n+                + (1 - beta_2_t) * ops.mean(regulated_grad_square, axis=-1)\n+            )\n+            # `c` deletes the second last dimension of gradient, so it is of\n+            # shape `gradient.shape[:-2] + gradient.shape[-1]`.\n+            c.assign(\n+                beta_2_t * c\n+                + (1 - beta_2_t) * ops.mean(regulated_grad_square, axis=-2)\n+            )\n+            v.assign(\n+                ops.expand_dims(\n+                    r / ops.mean(r, axis=-1, keepdims=True), axis=-1\n+                )\n+                * ops.expand_dims(c, -2)\n+            )\n+        else:\n+            v.assign(beta_2_t * v + (1 - beta_2_t) * regulated_grad_square)\n+\n+        # `convert_to_tensor` unifies the handling of sparse and dense grads.\n+        u_t = gradient / ops.sqrt(v)\n+        u_t_hat = u_t / ops.maximum(one, (self._rms(u_t) / self.clip_threshold))\n+        variable.assign(variable - alpha_t * u_t_hat)\n+\n+    def get_config(self):\n+        config = super().get_config()\n+\n+        config.update(\n+            {\n+                \"beta_2_decay\": self.beta_2_decay,\n+                \"epsilon_1\": self.epsilon_1,\n+                \"epsilon_2\": self.epsilon_2,\n+                \"clip_threshold\": self.clip_threshold,\n+                \"relative_step\": self.relative_step,\n+            }\n+        )\n+        return config\n+\n+\n+Adafactor.__doc__ = Adafactor.__doc__.replace(\n+    \"{{base_optimizer_keyword_args}}\", optimizer.base_optimizer_keyword_args\n+)\n\n@@ -0,0 +1,93 @@\n+# flake8: noqa\n+\n+\n+import numpy as np\n+\n+from keras_core import backend\n+from keras_core import testing\n+from keras_core.optimizers.adafactor import Adafactor\n+\n+\n+class AdafactorTest(testing.TestCase):\n+    def test_config(self):\n+        optimizer = Adafactor(\n+            learning_rate=0.5,\n+            beta_2_decay=-0.65,\n+            epsilon_1=1e-15,\n+            epsilon_2=1e-4,\n+            clip_threshold=0.9,\n+            relative_step=False,\n+        )\n+        self.run_class_serialization_test(optimizer)\n+\n+    def test_single_step(self):\n+        optimizer = Adafactor(learning_rate=0.5)\n+        grads = np.array([1.0, 6.0, 7.0, 2.0])\n+        vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n+        optimizer.apply_gradients(zip([grads], [vars]))\n+        self.assertAllClose(\n+            vars, [-0.3693, 0.6307, 1.6307, 2.6307], rtol=1e-4, atol=1e-4\n+        )\n+\n+    def test_weight_decay(self):\n+        grads, var1, var2, var3 = (\n+            np.zeros(()),\n+            backend.Variable(2.0),\n+            backend.Variable(2.0, name=\"exclude\"),\n+            backend.Variable(2.0),\n+        )\n+        optimizer_1 = Adafactor(learning_rate=1.0, weight_decay=0.004)\n+        optimizer_1.apply_gradients(zip([grads], [var1]))\n+\n+        optimizer_2 = Adafactor(learning_rate=1.0, weight_decay=0.004)\n+        optimizer_2.exclude_from_weight_decay(var_names=[\"exclude\"])\n+        optimizer_2.apply_gradients(zip([grads, grads], [var1, var2]))\n+\n+        optimizer_3 = Adafactor(learning_rate=1.0, weight_decay=0.004)\n+        optimizer_3.exclude_from_weight_decay(var_list=[var3])\n+        optimizer_3.apply_gradients(zip([grads, grads], [var1, var3]))\n+\n+        self.assertAlmostEqual(var1.numpy(), 1.9760959, decimal=6)\n+        self.assertAlmostEqual(var2.numpy(), 2.0, decimal=6)\n+        self.assertAlmostEqual(var3.numpy(), 2.0, decimal=6)\n+\n+    def test_correctness_with_golden(self):\n+        optimizer = Adafactor(\n+            learning_rate=0.5,\n+            beta_2_decay=-0.65,\n+            epsilon_1=1e-15,\n+            epsilon_2=1e-4,\n+            clip_threshold=0.9,\n+            relative_step=False,\n+        )\n+\n+        x = backend.Variable(np.ones([10]))\n+        grads = np.arange(0.1, 1.1, 0.1)\n+        first_grads = np.full((10,), 0.01)\n+\n+        # fmt: off\n+        golden = np.array(\n+            [[0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55, 0.55],\n+            [0.3031, 0.3026, 0.3025, 0.3024, 0.3024, 0.3024, 0.3024, 0.3024, 0.3024, 0.3024],\n+            [0.1671, 0.1665, 0.1663, 0.1663, 0.1663, 0.1663, 0.1663, 0.1663, 0.1663, 0.1663],\n+            [0.0923, 0.0916, 0.0915, 0.0914, 0.0914, 0.0914, 0.0914, 0.0914, 0.0914, 0.0914],\n+            [0.0554, 0.0548, 0.0546, 0.0546, 0.0546, 0.0546, 0.0546, 0.0545, 0.0545, 0.0545]]\n+        )\n+        # fmt: on\n+\n+        optimizer.apply_gradients(zip([first_grads], [x]))\n+        for i in range(5):\n+            self.assertAllClose(x, golden[i], rtol=5e-4, atol=5e-4)\n+            optimizer.apply_gradients(zip([grads], [x]))\n+\n+    def test_clip_norm(self):\n+        optimizer = Adafactor(clipnorm=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [2**0.5 / 2, 2**0.5 / 2])\n+\n+    def test_clip_value(self):\n+        optimizer = Adafactor(clipvalue=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [1.0, 1.0])\n\n@@ -0,0 +1,141 @@\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.optimizers import optimizer\n+\n+\n+@keras_core_export([\"keras_core.optimizers.Adamax\"])\n+class Adamax(optimizer.Optimizer):\n+    \"\"\"Optimizer that implements the Adamax algorithm.\n+\n+    Adamax, a variant of Adam based on the infinity norm, is a first-order\n+    gradient-based optimization method. Due to its capability of adjusting the\n+    learning rate based on data characteristics, it is suited to learn\n+    time-variant process, e.g., speech data with dynamically changed noise\n+    conditions. Default parameters follow those provided in the paper (see\n+    references below).\n+\n+    Initialization:\n+\n+    ```python\n+    m = 0  # Initialize initial 1st moment vector\n+    u = 0  # Initialize the exponentially weighted infinity norm\n+    t = 0  # Initialize timestep\n+    ```\n+\n+    The update rule for parameter `w` with gradient `g` is described at the end\n+    of section 7.1 of the paper (see the referenece section):\n+\n+    ```python\n+    t += 1\n+    m = beta1 * m + (1 - beta) * g\n+    u = max(beta2 * u, abs(g))\n+    current_lr = learning_rate / (1 - beta1 ** t)\n+    w = w - current_lr * m / (u + epsilon)\n+    ```\n+\n+    Args:\n+        learning_rate: A floating point value, or a callable\n+            that takes no arguments and returns the actual value to use. The\n+            learning rate. Defaults to `0.001`.\n+        beta_1: A float value or a constant float tensor. The exponential decay\n+            rate for the 1st moment estimates.\n+        beta_2: A float value or a constant float tensor. The exponential decay\n+            rate for the exponentially weighted infinity norm.\n+        epsilon: A small constant for numerical stability.\n+            {{base_optimizer_keyword_args}}\n+\n+    Reference:\n+\n+    - [Kingma et al., 2014](http://arxiv.org/abs/1412.6980)\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        learning_rate=0.001,\n+        beta_1=0.9,\n+        beta_2=0.999,\n+        epsilon=1e-7,\n+        weight_decay=None,\n+        clipnorm=None,\n+        clipvalue=None,\n+        global_clipnorm=None,\n+        use_ema=False,\n+        ema_momentum=0.99,\n+        ema_overwrite_frequency=None,\n+        name=\"adamax\",\n+    ):\n+        super().__init__(\n+            learning_rate=learning_rate,\n+            name=name,\n+            weight_decay=weight_decay,\n+            clipnorm=clipnorm,\n+            clipvalue=clipvalue,\n+            global_clipnorm=global_clipnorm,\n+            use_ema=use_ema,\n+            ema_momentum=ema_momentum,\n+            ema_overwrite_frequency=ema_overwrite_frequency,\n+        )\n+        self.beta_1 = beta_1\n+        self.beta_2 = beta_2\n+        self.epsilon = epsilon\n+\n+    def build(self, var_list):\n+        \"\"\"Initialize optimizer variables.\n+\n+        Adamax optimizer has 2 types of variables: momentums (denoted as m),\n+        exponentially weighted infinity norm (denoted as u).\n+\n+        Args:\n+            var_list: list of model variables to build Adamax variables on.\n+        \"\"\"\n+        if self.built:\n+            return\n+        super().build(var_list)\n+        self._m = []\n+        self._u = []\n+        for var in var_list:\n+            self._m.append(\n+                self.add_variable_from_reference(\n+                    reference_variable=var, name=\"m\"\n+                )\n+            )\n+            self._u.append(\n+                self.add_variable_from_reference(\n+                    reference_variable=var, name=\"u\"\n+                )\n+            )\n+\n+    def update_step(self, gradient, variable, learning_rate):\n+        \"\"\"Update step given gradient and the associated model variable.\"\"\"\n+        lr = ops.cast(learning_rate, variable.dtype)\n+        gradient = ops.cast(gradient, variable.dtype)\n+        local_step = ops.cast(self.iterations + 1, variable.dtype)\n+        beta_1_power = ops.power(\n+            ops.cast(self.beta_1, variable.dtype), local_step\n+        )\n+\n+        m = self._m[self._get_variable_index(variable)]\n+        u = self._u[self._get_variable_index(variable)]\n+\n+        m.assign(m + (gradient - m) * (1 - self.beta_1))\n+        u.assign(ops.maximum(self.beta_2 * u, ops.abs(gradient)))\n+        variable.assign(\n+            variable - (lr * m) / ((1 - beta_1_power) * (u + self.epsilon))\n+        )\n+\n+    def get_config(self):\n+        config = super().get_config()\n+\n+        config.update(\n+            {\n+                \"beta_1\": self.beta_1,\n+                \"beta_2\": self.beta_2,\n+                \"epsilon\": self.epsilon,\n+            }\n+        )\n+        return config\n+\n+\n+Adamax.__doc__ = Adamax.__doc__.replace(\n+    \"{{base_optimizer_keyword_args}}\", optimizer.base_optimizer_keyword_args\n+)\n\n@@ -0,0 +1,84 @@\n+# flake8: noqa\n+\n+\n+import numpy as np\n+\n+from keras_core import backend\n+from keras_core import testing\n+from keras_core.optimizers.adamax import Adamax\n+\n+\n+class AdamaxTest(testing.TestCase):\n+    def test_config(self):\n+        optimizer = Adamax(\n+            learning_rate=0.5,\n+            beta_1=0.8,\n+            beta_2=0.95,\n+            epsilon=1e-5,\n+        )\n+        self.run_class_serialization_test(optimizer)\n+\n+    def test_single_step(self):\n+        optimizer = Adamax(learning_rate=0.5)\n+        grads = np.array([1.0, 6.0, 7.0, 2.0])\n+        vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n+        optimizer.apply_gradients(zip([grads], [vars]))\n+        self.assertAllClose(vars, [0.5, 1.5, 2.5, 3.5], rtol=1e-4, atol=1e-4)\n+\n+    def test_weight_decay(self):\n+        grads, var1, var2, var3 = (\n+            np.zeros(()),\n+            backend.Variable(2.0),\n+            backend.Variable(2.0, name=\"exclude\"),\n+            backend.Variable(2.0),\n+        )\n+        optimizer_1 = Adamax(learning_rate=1.0, weight_decay=0.004)\n+        optimizer_1.apply_gradients(zip([grads], [var1]))\n+\n+        optimizer_2 = Adamax(learning_rate=1.0, weight_decay=0.004)\n+        optimizer_2.exclude_from_weight_decay(var_names=[\"exclude\"])\n+        optimizer_2.apply_gradients(zip([grads, grads], [var1, var2]))\n+\n+        optimizer_3 = Adamax(learning_rate=1.0, weight_decay=0.004)\n+        optimizer_3.exclude_from_weight_decay(var_list=[var3])\n+        optimizer_3.apply_gradients(zip([grads, grads], [var1, var3]))\n+\n+        self.assertAlmostEqual(var1.numpy(), 1.9760959, decimal=6)\n+        self.assertAlmostEqual(var2.numpy(), 2.0, decimal=6)\n+        self.assertAlmostEqual(var3.numpy(), 2.0, decimal=6)\n+\n+    def test_correctness_with_golden(self):\n+        optimizer = Adamax(\n+            learning_rate=0.2, beta_1=0.85, beta_2=0.95, epsilon=1e-6\n+        )\n+\n+        x = backend.Variable(np.ones([10]))\n+        grads = np.arange(0.1, 1.1, 0.1)\n+        first_grads = np.full((10,), 0.01)\n+\n+        # fmt: off\n+        golden = np.array(\n+            [[0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8],\n+            [0.6827, 0.6873, 0.6888, 0.6896, 0.6901, 0.6904, 0.6906, 0.6908, 0.6909, 0.691],\n+            [0.5333, 0.5407, 0.5431, 0.5444, 0.5451, 0.5456, 0.546, 0.5462, 0.5464, 0.5466],\n+            [0.368, 0.3773, 0.3804, 0.382, 0.3829, 0.3835, 0.384, 0.3843, 0.3846, 0.3848],\n+            [0.1933, 0.204, 0.2076, 0.2094, 0.2105, 0.2112, 0.2117, 0.2121, 0.2124, 0.2126]]\n+        )\n+        # fmt: on\n+\n+        optimizer.apply_gradients(zip([first_grads], [x]))\n+        for i in range(5):\n+            self.assertAllClose(x, golden[i], rtol=5e-4, atol=5e-4)\n+            optimizer.apply_gradients(zip([grads], [x]))\n+\n+    def test_clip_norm(self):\n+        optimizer = Adamax(clipnorm=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [2**0.5 / 2, 2**0.5 / 2])\n+\n+    def test_clip_value(self):\n+        optimizer = Adamax(clipvalue=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [1.0, 1.0])\n\n@@ -0,0 +1,227 @@\n+from keras_core import initializers\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.optimizers import optimizer\n+\n+\n+@keras_core_export([\"keras_core.optimizers.Ftrl\"])\n+class Ftrl(optimizer.Optimizer):\n+    r\"\"\"Optimizer that implements the FTRL algorithm.\n+\n+    \"Follow The Regularized Leader\" (FTRL) is an optimization algorithm\n+    developed at Google for click-through rate prediction in the early 2010s. It\n+    is most suitable for shallow models with large and sparse feature spaces.\n+    The algorithm is described by\n+    [McMahan et al., 2013](https://research.google.com/pubs/archive/41159.pdf).\n+    The Keras version has support for both online L2 regularization\n+    (the L2 regularization described in the paper\n+    above) and shrinkage-type L2 regularization\n+    (which is the addition of an L2 penalty to the loss function).\n+\n+    Initialization:\n+\n+    ```python\n+    n = 0\n+    sigma = 0\n+    z = 0\n+    ```\n+\n+    Update rule for one variable `w`:\n+\n+    ```python\n+    prev_n = n\n+    n = n + g ** 2\n+    sigma = (n ** -lr_power - prev_n ** -lr_power) / lr\n+    z = z + g - sigma * w\n+    if abs(z) < lambda_1:\n+      w = 0\n+    else:\n+      w = (sgn(z) * lambda_1 - z) / ((beta + sqrt(n)) / alpha + lambda_2)\n+    ```\n+\n+    Notation:\n+\n+    - `lr` is the learning rate\n+    - `g` is the gradient for the variable\n+    - `lambda_1` is the L1 regularization strength\n+    - `lambda_2` is the L2 regularization strength\n+    - `lr_power` is the power to scale n.\n+\n+    Check the documentation for the `l2_shrinkage_regularization_strength`\n+    parameter for more details when shrinkage is enabled, in which case gradient\n+    is replaced with a gradient with shrinkage.\n+\n+    Args:\n+        learning_rate: A floating point value, or a callable that\n+            takes no arguments and returns the actual value to use. The learning\n+            rate.  Defaults to `0.001`.\n+        learning_rate_power: A float value, must be less or equal to zero.\n+            Controls how the learning rate decreases during training. Use zero\n+            for a fixed learning rate.\n+        initial_accumulator_value: The starting value for accumulators. Only\n+            zero or positive values are allowed.\n+        l1_regularization_strength: A float value, must be greater than or equal\n+            to zero. Defaults to `0.0`.\n+        l2_regularization_strength: A float value, must be greater than or equal\n+            to zero. Defaults to `0.0`.\n+        l2_shrinkage_regularization_strength: A float value, must be greater\n+            than or equal to zero. This differs from L2 above in that the L2\n+            above is a stabilization penalty, whereas this L2 shrinkage is a\n+            magnitude penalty. When input is sparse shrinkage will only happen\n+            on the active weights.\n+        beta: A float value, representing the beta value from the paper.\n+            Defaults to 0.0.\n+        {{base_optimizer_keyword_args}}\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        learning_rate=0.001,\n+        learning_rate_power=-0.5,\n+        initial_accumulator_value=0.1,\n+        l1_regularization_strength=0.0,\n+        l2_regularization_strength=0.0,\n+        l2_shrinkage_regularization_strength=0.0,\n+        beta=0.0,\n+        weight_decay=None,\n+        clipnorm=None,\n+        clipvalue=None,\n+        global_clipnorm=None,\n+        use_ema=False,\n+        ema_momentum=0.99,\n+        ema_overwrite_frequency=None,\n+        name=\"ftrl\",\n+    ):\n+        super().__init__(\n+            learning_rate=learning_rate,\n+            name=name,\n+            weight_decay=weight_decay,\n+            clipnorm=clipnorm,\n+            clipvalue=clipvalue,\n+            global_clipnorm=global_clipnorm,\n+            use_ema=use_ema,\n+            ema_momentum=ema_momentum,\n+            ema_overwrite_frequency=ema_overwrite_frequency,\n+        )\n+\n+        if initial_accumulator_value < 0.0:\n+            raise ValueError(\n+                \"`initial_accumulator_value` needs to be positive or zero. \"\n+                \"Received: initial_accumulator_value=\"\n+                f\"{initial_accumulator_value}.\"\n+            )\n+        if learning_rate_power > 0.0:\n+            raise ValueError(\n+                \"`learning_rate_power` needs to be negative or zero. Received: \"\n+                f\"learning_rate_power={learning_rate_power}.\"\n+            )\n+        if l1_regularization_strength < 0.0:\n+            raise ValueError(\n+                \"`l1_regularization_strength` needs to be positive or zero. \"\n+                \"Received: l1_regularization_strength=\"\n+                f\"{l1_regularization_strength}.\"\n+            )\n+        if l2_regularization_strength < 0.0:\n+            raise ValueError(\n+                \"`l2_regularization_strength` needs to be positive or zero. \"\n+                \"Received: l2_regularization_strength=\"\n+                f\"{l2_regularization_strength}.\"\n+            )\n+        if l2_shrinkage_regularization_strength < 0.0:\n+            raise ValueError(\n+                \"`l2_shrinkage_regularization_strength` needs to be positive \"\n+                \"or zero. Received: l2_shrinkage_regularization_strength\"\n+                f\"={l2_shrinkage_regularization_strength}.\"\n+            )\n+\n+        self.learning_rate_power = learning_rate_power\n+        self.initial_accumulator_value = initial_accumulator_value\n+        self.l1_regularization_strength = l1_regularization_strength\n+        self.l2_regularization_strength = l2_regularization_strength\n+        self.l2_shrinkage_regularization_strength = (\n+            l2_shrinkage_regularization_strength\n+        )\n+        self.beta = beta\n+\n+    def build(self, var_list):\n+        \"\"\"Initialize optimizer variables.\n+\n+        Args:\n+            var_list: list of model variables to build Ftrl variables on.\n+        \"\"\"\n+        if self.built:\n+            return\n+        super().build(var_list)\n+        self._accumulators = []\n+        self._linears = []\n+        for var in var_list:\n+            self._accumulators.append(\n+                self.add_variable(\n+                    shape=var.shape,\n+                    dtype=var.dtype,\n+                    name=\"accumulator\",\n+                    initializer=initializers.Constant(\n+                        self.initial_accumulator_value,\n+                    ),\n+                )\n+            )\n+            self._linears.append(\n+                self.add_variable_from_reference(\n+                    reference_variable=var, name=\"linear\"\n+                )\n+            )\n+\n+    def update_step(self, gradient, variable, learning_rate):\n+        \"\"\"Update step given gradient and the associated model variable.\"\"\"\n+\n+        lr = ops.cast(learning_rate, variable.dtype)\n+        gradient = ops.cast(gradient, variable.dtype)\n+\n+        accum = self._accumulators[self._get_variable_index(variable)]\n+        linear = self._linears[self._get_variable_index(variable)]\n+\n+        lr_power = self.learning_rate_power\n+        l2_reg = self.l2_regularization_strength\n+        l2_reg = l2_reg + self.beta / (2.0 * lr)\n+\n+        # Ftrl optimizer has the same implementation for sparse and dense\n+        # gradients update.\n+        grad_to_use = (\n+            gradient + 2 * self.l2_shrinkage_regularization_strength * variable\n+        )\n+        new_accum = accum + ops.power(gradient, 2)\n+        linear.assign(\n+            linear\n+            + grad_to_use\n+            - (ops.power(new_accum, -lr_power) - ops.power(accum, -lr_power))\n+            / lr\n+            * variable\n+        )\n+        quadratic = ops.power(new_accum, (-lr_power)) / lr + 2 * l2_reg\n+        linear_clipped = ops.clip(\n+            linear,\n+            -self.l1_regularization_strength,\n+            self.l1_regularization_strength,\n+        )\n+        variable.assign((linear_clipped - linear) / quadratic)\n+        accum.assign(new_accum)\n+\n+    def get_config(self):\n+        config = super().get_config()\n+\n+        config.update(\n+            {\n+                \"learning_rate_power\": self.learning_rate_power,\n+                \"initial_accumulator_value\": self.initial_accumulator_value,\n+                \"l1_regularization_strength\": self.l1_regularization_strength,\n+                \"l2_regularization_strength\": self.l2_regularization_strength,\n+                \"l2_shrinkage_regularization_strength\": self.l2_shrinkage_regularization_strength,  # noqa: E501\n+                \"beta\": self.beta,\n+            }\n+        )\n+        return config\n+\n+\n+Ftrl.__doc__ = Ftrl.__doc__.replace(\n+    \"{{base_optimizer_keyword_args}}\", optimizer.base_optimizer_keyword_args\n+)\n\n@@ -0,0 +1,73 @@\n+# flake8: noqa\n+\n+\n+import numpy as np\n+\n+from keras_core import backend\n+from keras_core import testing\n+from keras_core.optimizers.ftrl import Ftrl\n+\n+\n+class FtrlTest(testing.TestCase):\n+    def test_config(self):\n+        optimizer = Ftrl(\n+            learning_rate=0.05,\n+            learning_rate_power=-0.2,\n+            initial_accumulator_value=0.4,\n+            l1_regularization_strength=0.05,\n+            l2_regularization_strength=0.15,\n+            l2_shrinkage_regularization_strength=0.01,\n+            beta=0.3,\n+        )\n+        self.run_class_serialization_test(optimizer)\n+\n+    def test_single_step(self):\n+        optimizer = Ftrl(learning_rate=0.5)\n+        grads = np.array([1.0, 6.0, 7.0, 2.0])\n+        vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n+        optimizer.apply_gradients(zip([grads], [vars]))\n+        self.assertAllClose(\n+            vars, [0.2218, 1.3954, 2.3651, 2.8814], rtol=1e-4, atol=1e-4\n+        )\n+\n+    def test_correctness_with_golden(self):\n+        optimizer = Ftrl(\n+            learning_rate=0.05,\n+            learning_rate_power=-0.2,\n+            initial_accumulator_value=0.4,\n+            l1_regularization_strength=0.05,\n+            l2_regularization_strength=0.15,\n+            l2_shrinkage_regularization_strength=0.01,\n+            beta=0.3,\n+        )\n+\n+        x = backend.Variable(np.ones([10]))\n+        grads = np.arange(0.1, 1.1, 0.1)\n+        first_grads = np.full((10,), 0.01)\n+\n+        # fmt: off\n+        golden = np.array(\n+            [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n+            [-0.0034, -0.0077, -0.0118, -0.0157, -0.0194, -0.023, -0.0263, -0.0294, -0.0325, -0.0354],\n+            [-0.0078, -0.0162, -0.0242, -0.0317, -0.0387, -0.0454, -0.0516, -0.0575, -0.0631, -0.0685],\n+            [-0.0121, -0.0246, -0.0363, -0.0472, -0.0573, -0.0668, -0.0757, -0.0842, -0.0922, -0.0999],\n+            [-0.0164, -0.0328, -0.0481, -0.0623, -0.0753, -0.0875, -0.099, -0.1098, -0.1201, -0.1299]]\n+        )\n+        # fmt: on\n+\n+        optimizer.apply_gradients(zip([first_grads], [x]))\n+        for i in range(5):\n+            self.assertAllClose(x, golden[i], rtol=5e-4, atol=5e-4)\n+            optimizer.apply_gradients(zip([grads], [x]))\n+\n+    def test_clip_norm(self):\n+        optimizer = Ftrl(clipnorm=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [2**0.5 / 2, 2**0.5 / 2])\n+\n+    def test_clip_value(self):\n+        optimizer = Ftrl(clipvalue=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [1.0, 1.0])\n\n@@ -0,0 +1,158 @@\n+from keras_core import backend\n+from keras_core import operations as ops\n+from keras_core.api_export import keras_core_export\n+from keras_core.optimizers import optimizer\n+\n+\n+@keras_core_export([\"keras_core.optimizers.Nadam\"])\n+class Nadam(optimizer.Optimizer):\n+    \"\"\"Optimizer that implements the Nadam algorithm.\n+\n+    Much like Adam is essentially RMSprop with momentum, Nadam is Adam with\n+    Nesterov momentum.\n+\n+    Args:\n+        learning_rate: A floating point value or a callable\n+            that takes no arguments and returns the actual value to use. The\n+            learning rate. Defaults to `0.001`.\n+        beta_1: A float value or a constant float tensor, or a callable\n+            that takes no arguments and returns the actual value to use. The\n+            exponential decay rate for the 1st moment estimates.\n+            Defaults to `0.9`.\n+        beta_2: A float value or a constant float tensor, or a callable\n+            that takes no arguments and returns the actual value to use. The\n+            exponential decay rate for the 2nd moment estimates. Defaults to\n+            `0.999`.\n+        epsilon: A small constant for numerical stability. This epsilon is\n+            \"epsilon hat\" in the Kingma and Ba paper (in the formula just before\n+            Section 2.1), not the epsilon in Algorithm 1 of the paper.\n+            Defaults to `1e-7`.\n+        {{base_optimizer_keyword_args}}\n+\n+    Reference:\n+\n+    - [Dozat, 2015](http://cs229.stanford.edu/proj2015/054_report.pdf).\n+\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        learning_rate=0.001,\n+        beta_1=0.9,\n+        beta_2=0.999,\n+        epsilon=1e-7,\n+        weight_decay=None,\n+        clipnorm=None,\n+        clipvalue=None,\n+        global_clipnorm=None,\n+        use_ema=False,\n+        ema_momentum=0.99,\n+        ema_overwrite_frequency=None,\n+        name=\"nadam\",\n+    ):\n+        super().__init__(\n+            learning_rate=learning_rate,\n+            name=name,\n+            weight_decay=weight_decay,\n+            clipnorm=clipnorm,\n+            clipvalue=clipvalue,\n+            global_clipnorm=global_clipnorm,\n+            use_ema=use_ema,\n+            ema_momentum=ema_momentum,\n+            ema_overwrite_frequency=ema_overwrite_frequency,\n+        )\n+        self.beta_1 = beta_1\n+        self.beta_2 = beta_2\n+        self.epsilon = epsilon\n+\n+    def build(self, var_list):\n+        \"\"\"Initialize optimizer variables.\n+\n+        Nadam optimizer has 2 types of variables: momentums and velocities.\n+\n+        Args:\n+            var_list: list of model variables to build Nadam variables on.\n+        \"\"\"\n+        if self.built:\n+            return\n+        super().build(var_list)\n+        self._momentums = []\n+        self._velocities = []\n+        self._u_product = backend.Variable(1.0, dtype=var_list[0].dtype)\n+        # Keep a counter on how many times of _u_product has been computed to\n+        # avoid duplicated computations.\n+        self._u_product_counter = 1\n+\n+        for var in var_list:\n+            self._momentums.append(\n+                self.add_variable_from_reference(\n+                    reference_variable=var, name=\"m\"\n+                )\n+            )\n+            self._velocities.append(\n+                self.add_variable_from_reference(\n+                    reference_variable=var, name=\"v\"\n+                )\n+            )\n+\n+    def update_step(self, gradient, variable, learning_rate):\n+        \"\"\"Update step given gradient and the associated model variable.\"\"\"\n+        var_dtype = variable.dtype\n+        lr = ops.cast(learning_rate, var_dtype)\n+        gradient = ops.cast(gradient, var_dtype)\n+\n+        local_step = ops.cast(self.iterations + 1, var_dtype)\n+        next_step = ops.cast(self.iterations + 2, var_dtype)\n+        decay = ops.cast(0.96, var_dtype)\n+        beta_1 = ops.cast(self.beta_1, var_dtype)\n+        beta_2 = ops.cast(self.beta_2, var_dtype)\n+        u_t = beta_1 * (1.0 - 0.5 * (ops.power(decay, local_step)))\n+        u_t_1 = beta_1 * (1.0 - 0.5 * (ops.power(decay, next_step)))\n+\n+        def get_cached_u_product():\n+            return self._u_product\n+\n+        def compute_new_u_product():\n+            u_product_t = self._u_product * u_t\n+            self._u_product.assign(u_product_t)\n+            self._u_product_counter += 1\n+            return u_product_t\n+\n+        if self._u_product_counter == (self.iterations + 2):\n+            u_product_t = get_cached_u_product()\n+        else:\n+            u_product_t = compute_new_u_product()\n+\n+        u_product_t_1 = u_product_t * u_t_1\n+        beta_2_power = ops.power(beta_2, local_step)\n+\n+        m = self._momentums[self._get_variable_index(variable)]\n+        v = self._velocities[self._get_variable_index(variable)]\n+\n+        m.assign(m + (gradient - m) * (1 - beta_1))\n+        v.assign(v + (ops.square(gradient) - v) * (1 - beta_2))\n+        m_hat = u_t_1 * m / (1 - u_product_t_1) + (1 - u_t) * gradient / (\n+            1 - u_product_t\n+        )\n+        v_hat = v / (1 - beta_2_power)\n+\n+        variable.assign(\n+            variable - (m_hat * lr) / (ops.sqrt(v_hat) + self.epsilon)\n+        )\n+\n+    def get_config(self):\n+        config = super().get_config()\n+\n+        config.update(\n+            {\n+                \"beta_1\": self.beta_1,\n+                \"beta_2\": self.beta_2,\n+                \"epsilon\": self.epsilon,\n+            }\n+        )\n+        return config\n+\n+\n+Nadam.__doc__ = Nadam.__doc__.replace(\n+    \"{{base_optimizer_keyword_args}}\", optimizer.base_optimizer_keyword_args\n+)\n\n@@ -0,0 +1,89 @@\n+# flake8: noqa\n+\n+\n+import numpy as np\n+\n+from keras_core import backend\n+from keras_core import testing\n+from keras_core.optimizers.nadam import Nadam\n+\n+\n+class NadamTest(testing.TestCase):\n+    def test_config(self):\n+        optimizer = Nadam(\n+            learning_rate=0.5,\n+            beta_1=0.5,\n+            beta_2=0.67,\n+            epsilon=1e-5,\n+        )\n+        self.run_class_serialization_test(optimizer)\n+\n+    def test_single_step(self):\n+        optimizer = Nadam(learning_rate=0.5)\n+        grads = np.array([1.0, 6.0, 7.0, 2.0])\n+        vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n+        optimizer.apply_gradients(zip([grads], [vars]))\n+        self.assertAllClose(\n+            vars, [0.4686, 1.4686, 2.4686, 3.4686], rtol=1e-4, atol=1e-4\n+        )\n+\n+    def test_weight_decay(self):\n+        grads, var1, var2, var3 = (\n+            np.zeros(()),\n+            backend.Variable(2.0),\n+            backend.Variable(2.0, name=\"exclude\"),\n+            backend.Variable(2.0),\n+        )\n+        optimizer_1 = Nadam(learning_rate=1.0, weight_decay=0.004)\n+        optimizer_1.apply_gradients(zip([grads], [var1]))\n+\n+        optimizer_2 = Nadam(learning_rate=1.0, weight_decay=0.004)\n+        optimizer_2.exclude_from_weight_decay(var_names=[\"exclude\"])\n+        optimizer_2.apply_gradients(zip([grads, grads], [var1, var2]))\n+\n+        optimizer_3 = Nadam(learning_rate=1.0, weight_decay=0.004)\n+        optimizer_3.exclude_from_weight_decay(var_list=[var3])\n+        optimizer_3.apply_gradients(zip([grads, grads], [var1, var3]))\n+\n+        self.assertAlmostEqual(var1.numpy(), 1.9760959, decimal=6)\n+        self.assertAlmostEqual(var2.numpy(), 2.0, decimal=6)\n+        self.assertAlmostEqual(var3.numpy(), 2.0, decimal=6)\n+\n+    def test_correctness_with_golden(self):\n+        optimizer = Nadam(\n+            learning_rate=0.5,\n+            beta_1=0.5,\n+            beta_2=0.67,\n+            epsilon=1e-5,\n+        )\n+\n+        x = backend.Variable(np.ones([10]))\n+        grads = np.arange(0.1, 1.1, 0.1)\n+        first_grads = np.full((10,), 0.01)\n+\n+        # fmt: off\n+        golden = np.array(\n+            [[0.4281, 0.4281, 0.4281, 0.4281, 0.4281, 0.4281, 0.4281, 0.4281, 0.4281, 0.4281],\n+            [-0.1738, -0.1731, -0.1726, -0.1723, -0.1721, -0.172, -0.1719, -0.1718, -0.1718, -0.1717],\n+            [-0.7115, -0.7103, -0.7096, -0.7092, -0.709, -0.7088, -0.7086, -0.7085, -0.7085, -0.7084],\n+            [-1.2335, -1.2322, -1.2313, -1.2309, -1.2306, -1.2304, -1.2302, -1.2301, -1.23, -1.2299],\n+            [-1.7492, -1.7478, -1.7469, -1.7464, -1.7461, -1.7459, -1.7457, -1.7456, -1.7455, -1.7454]]\n+        )\n+        # fmt: on\n+\n+        optimizer.apply_gradients(zip([first_grads], [x]))\n+        for i in range(5):\n+            self.assertAllClose(x, golden[i], rtol=5e-4, atol=5e-4)\n+            optimizer.apply_gradients(zip([grads], [x]))\n+\n+    def test_clip_norm(self):\n+        optimizer = Nadam(clipnorm=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [2**0.5 / 2, 2**0.5 / 2])\n+\n+    def test_clip_value(self):\n+        optimizer = Nadam(clipvalue=1)\n+        grad = [np.array([100.0, 100.0])]\n+        clipped_grad = optimizer._clip_gradients(grad)\n+        self.assertAllClose(clipped_grad[0], [1.0, 1.0])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#459b1b13fcc1e423ef62d2d49c7edffd33cb353a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 3 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 19 | Churn Cumulative: 253 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -48,13 +48,13 @@ class BatchNormalization(Layer):\n     inference data*.\n \n     Args:\n-        axis: Integer, the axis that should be normalized (typically the features\n-            axis). For instance, after a `Conv2D` layer with\n-            `data_format=\"channels_first\"`, set `axis=1` in `BatchNormalization`.\n+        axis: Integer, the axis that should be normalized\n+            (typically the features axis). For instance, after a `Conv2D` layer\n+            with `data_format=\"channels_first\"`, use `axis=1`.\n         momentum: Momentum for the moving average.\n         epsilon: Small float added to variance to avoid dividing by zero.\n-        center: If `True`, add offset of `beta` to normalized tensor. If False,\n-            `beta` is ignored.\n+        center: If `True`, add offset of `beta` to normalized tensor.\n+            If `False`, `beta` is ignored.\n         scale: If `True`, multiply by `gamma`. If `False`, `gamma` is not used.\n             When the next layer is linear this can be disabled\n             since the scaling will be done by the next layer.\n@@ -72,10 +72,11 @@ class BatchNormalization(Layer):\n         inputs: Input tensor (of any rank).\n         training: Python boolean indicating whether the layer should behave in\n             training mode or in inference mode.\n-            - `training=True`: The layer will normalize its inputs using the mean\n-            and variance of the current batch of inputs.\n-            - `training=False`: The layer will normalize its inputs using the mean\n-            and variance of its moving statistics, learned during training.\n+            - `training=True`: The layer will normalize its inputs using\n+            the mean and variance of the current batch of inputs.\n+            - `training=False`: The layer will normalize its inputs using\n+            the mean and variance of its moving statistics,\n+            learned during training.\n \n     Reference:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a0f94646c3f079ffc34b51e2649deb4ebd04fbcd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 216 | Lines Deleted: 20 | Files Changed: 5 | Hunks: 23 | Methods Changed: 32 | Complexity Δ (Sum/Max): 32/26 | Churn Δ: 236 | Churn Cumulative: 6187 | Contributors (this commit): 7 | Commits (past 90d): 43 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -267,7 +267,7 @@ def silu(x):\n \n \n @keras_core_export(\"keras_core.activations.gelu\")\n-def gelu(x):\n+def gelu(x, approximate=False):\n     \"\"\"Gaussian error linear unit (GELU) activation function.\n \n     The Gaussian error linear unit (GELU) is defined as:\n@@ -280,12 +280,13 @@ def gelu(x):\n \n     Args:\n         x: Input tensor.\n+        approximate: A `bool`, whether to enable approximation.\n \n     Reference:\n \n     - [Hendrycks et al., 2016](https://arxiv.org/abs/1606.08415)\n     \"\"\"\n-    return ops.gelu(x)\n+    return ops.gelu(x, approximate=approximate)\n \n \n @keras_core_export(\"keras_core.activations.tanh\")\n@@ -380,7 +381,7 @@ class Mish(ops.Operation):\n \n     @staticmethod\n     def static_call(x):\n-        return x * backend.tanh(backend.softplus(x))\n+        return x * backend.nn.tanh(backend.nn.softplus(x))\n \n \n @keras_core_export(\"keras_core.activations.mish\")\n\n@@ -1 +1,174 @@\n-# TODO\n+import numpy as np\n+\n+from keras_core import activations\n+from keras_core import backend\n+from keras_core import testing\n+\n+\n+def _ref_softmax(values):\n+    m = np.max(values)\n+    e = np.exp(values - m)\n+    return e / np.sum(e)\n+\n+\n+def _ref_softplus(x):\n+    return np.log(np.ones_like(x) + np.exp(x))\n+\n+\n+class ActivationsTest(testing.TestCase):\n+    def test_softmax(self):\n+        x = np.random.random((2, 5))\n+\n+        result = activations.softmax(x[np.newaxis, :])[0]\n+        expected = _ref_softmax(x[0])\n+        self.assertAllClose(result[0], expected, rtol=1e-05)\n+\n+    def test_softmax_2d_axis_0(self):\n+        x = np.random.random((2, 5))\n+        result = activations.softmax(x[np.newaxis, :], axis=1)[0]\n+        expected = np.zeros((2, 5))\n+        for i in range(5):\n+            expected[:, i] = _ref_softmax(x[:, i])\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+    # TODO: Fails on Tuple Axis\n+    # ops/nn_ops.py:3824: TypeError:\n+    # '<=' not supported between instances of 'int' and 'tuple'\n+    # def test_softmax_3d_axis_tuple(self):\n+    #     x = np.random.random((2, 3, 5))\n+    #     result = activations.softmax([x], axis=(1, 2))[0]\n+    #     expected = np.zeros((2, 3, 5))\n+    #     for i in range(2):\n+    #         expected[i, :, :] = _ref_softmax(x[i, :, :])\n+    #     self.assertAllClose(result, expected, rtol=1e-05)\n+\n+    def test_temporal_softmax(self):\n+        x = np.random.random((2, 2, 3)) * 10\n+        result = activations.softmax(x[np.newaxis, :])[0]\n+        expected = _ref_softmax(x[0, 0])\n+        self.assertAllClose(result[0, 0], expected, rtol=1e-05)\n+\n+    def test_selu(self):\n+        alpha = 1.6732632423543772848170429916717\n+        scale = 1.0507009873554804934193349852946\n+\n+        positive_values = np.array([[1, 2]], dtype=backend.floatx())\n+        result = activations.selu(positive_values[np.newaxis, :])[0]\n+        self.assertAllClose(result, positive_values * scale, rtol=1e-05)\n+\n+        negative_values = np.array([[-1, -2]], dtype=backend.floatx())\n+        result = activations.selu(negative_values[np.newaxis, :])[0]\n+        true_result = (np.exp(negative_values) - 1) * scale * alpha\n+        self.assertAllClose(result, true_result)\n+\n+    def test_softplus(self):\n+        x = np.random.random((2, 5))\n+        result = activations.softplus(x[np.newaxis, :])[0]\n+        expected = _ref_softplus(x)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+    def test_softsign(self):\n+        def softsign(x):\n+            return np.divide(x, np.ones_like(x) + np.absolute(x))\n+\n+        x = np.random.random((2, 5))\n+        result = activations.softsign(x[np.newaxis, :])[0]\n+        expected = softsign(x)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+    def test_sigmoid(self):\n+        def ref_sigmoid(x):\n+            if x >= 0:\n+                return 1 / (1 + np.exp(-x))\n+            else:\n+                z = np.exp(x)\n+                return z / (1 + z)\n+\n+        sigmoid = np.vectorize(ref_sigmoid)\n+\n+        x = np.random.random((2, 5))\n+        result = activations.sigmoid(x[np.newaxis, :])[0]\n+        expected = sigmoid(x)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+    def test_hard_sigmoid(self):\n+        def ref_hard_sigmoid(x):\n+            x = (x / 6.0) + 0.5\n+            z = 0.0 if x <= 0 else (1.0 if x >= 1 else x)\n+            return z\n+\n+        hard_sigmoid = np.vectorize(ref_hard_sigmoid)\n+        x = np.random.random((2, 5))\n+        result = activations.hard_sigmoid(x[np.newaxis, :])[0]\n+        expected = hard_sigmoid(x)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+    def test_relu(self):\n+        positive_values = np.random.random((2, 5))\n+        result = activations.relu(positive_values[np.newaxis, :])[0]\n+        self.assertAllClose(result, positive_values, rtol=1e-05)\n+\n+        negative_values = np.random.uniform(-1, 0, (2, 5))\n+        result = activations.relu(negative_values[np.newaxis, :])[0]\n+        expected = np.zeros((2, 5))\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+    def test_gelu(self):\n+        def gelu(x, approximate=False):\n+            if approximate:\n+                return (\n+                    0.5\n+                    * x\n+                    * (\n+                        1.0\n+                        + np.tanh(\n+                            np.sqrt(2.0 / np.pi)\n+                            * (x + 0.044715 * np.power(x, 3))\n+                        )\n+                    )\n+                )\n+            else:\n+                from scipy.stats import norm\n+\n+                return x * norm.cdf(x)\n+\n+        x = np.random.random((2, 5))\n+        result = activations.gelu(x[np.newaxis, :])[0]\n+        expected = gelu(x)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+        x = np.random.random((2, 5))\n+        result = activations.gelu(x[np.newaxis, :], approximate=True)[0]\n+        expected = gelu(x, True)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+    def test_elu(self):\n+        x = np.random.random((2, 5))\n+        result = activations.elu(x[np.newaxis, :])[0]\n+        self.assertAllClose(result, x, rtol=1e-05)\n+        negative_values = np.array([[-1, -2]], dtype=backend.floatx())\n+        result = activations.elu(negative_values[np.newaxis, :])[0]\n+        true_result = np.exp(negative_values) - 1\n+        self.assertAllClose(result, true_result)\n+\n+    def test_tanh(self):\n+        x = np.random.random((2, 5))\n+        result = activations.tanh(x[np.newaxis, :])[0]\n+        expected = np.tanh(x)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+    def test_exponential(self):\n+        x = np.random.random((2, 5))\n+        result = activations.exponential(x[np.newaxis, :])[0]\n+        expected = np.exp(x)\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+    def test_mish(self):\n+        x = np.random.random((2, 5))\n+        result = activations.mish(x[np.newaxis, :])[0]\n+        expected = x * np.tanh(_ref_softplus(x))\n+        self.assertAllClose(result, expected, rtol=1e-05)\n+\n+    def test_linear(self):\n+        x = np.random.random((10, 5))\n+        self.assertAllClose(x, activations.linear(x))\n\n@@ -19,6 +19,10 @@ def sigmoid(x):\n     return jnn.sigmoid(x)\n \n \n+def tanh(x):\n+    return jnn.tanh(x)\n+\n+\n def softplus(x):\n     return jnn.softplus(x)\n \n@@ -59,8 +63,8 @@ def gelu(x, approximate=True):\n     return jnn.gelu(x, approximate)\n \n \n-def softmax(x):\n-    return jnn.softmax(x)\n+def softmax(x, axis=None):\n+    return jnn.softmax(x, axis=axis)\n \n \n def log_softmax(x, axis=-1):\n\n@@ -20,6 +20,10 @@ def sigmoid(x):\n     return tf.nn.sigmoid(x)\n \n \n+def tanh(x):\n+    return tf.nn.tanh(x)\n+\n+\n def softplus(x):\n     return tf.math.softplus(x)\n \n\n@@ -74,7 +74,7 @@ class Sigmoid(Operation):\n         return backend.nn.sigmoid(x)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape)\n+        return KerasTensor(x.shape, x.dtype)\n \n \n def sigmoid(x):\n@@ -83,12 +83,26 @@ def sigmoid(x):\n     return backend.nn.sigmoid(x)\n \n \n+class Tanh(Operation):\n+    def call(self, x):\n+        return backend.nn.tanh(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, x.dtype)\n+\n+\n+def tanh(x):\n+    if any_symbolic_tensors((x,)):\n+        return Tanh().symbolic_call(x)\n+    return backend.nn.tanh(x)\n+\n+\n class Softplus(Operation):\n     def call(self, x):\n         return backend.nn.softplus(x)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape)\n+        return KerasTensor(x.shape, x.dtype)\n \n \n def softplus(x):\n@@ -102,7 +116,7 @@ class Softsign(Operation):\n         return backend.nn.softsign(x)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape)\n+        return KerasTensor(x.shape, x.dtype)\n \n \n def softsign(x):\n@@ -116,7 +130,7 @@ class Silu(Operation):\n         return backend.nn.silu(x)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape)\n+        return KerasTensor(x.shape, x.dtype)\n \n \n def silu(x):\n@@ -130,7 +144,7 @@ class Swish(Operation):\n         return backend.nn.swish(x)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape)\n+        return KerasTensor(x.shape, x.dtype)\n \n \n def swish(x):\n@@ -144,7 +158,7 @@ class LogSigmoid(Operation):\n         return backend.nn.log_sigmoid(x)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape)\n+        return KerasTensor(x.shape, x.dtype)\n \n \n def log_sigmoid(x):\n@@ -162,7 +176,7 @@ class LeakyRelu(Operation):\n         return backend.nn.leaky_relu(x, self.negative_slope)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape)\n+        return KerasTensor(x.shape, x.dtype)\n \n \n def leaky_relu(x, negative_slope=0.2):\n@@ -176,7 +190,7 @@ class HardSigmoid(Operation):\n         return backend.nn.hard_sigmoid(x)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape)\n+        return KerasTensor(x.shape, x.dtype)\n \n \n def hard_sigmoid(x):\n@@ -190,7 +204,7 @@ class Elu(Operation):\n         return backend.nn.elu(x)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape)\n+        return KerasTensor(x.shape, x.dtype)\n \n \n def elu(x):\n@@ -204,7 +218,7 @@ class Selu(Operation):\n         return backend.nn.selu(x)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape)\n+        return KerasTensor(x.shape, x.dtype)\n \n \n def selu(x):\n@@ -222,7 +236,7 @@ class Gelu(Operation):\n         return backend.nn.gelu(x, self.approximate)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape)\n+        return KerasTensor(x.shape, x.dtype)\n \n \n def gelu(x, approximate=True):\n@@ -240,13 +254,13 @@ class Softmax(Operation):\n         return backend.nn.softmax(x, axis=self.axis)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape)\n+        return KerasTensor(x.shape, x.dtype)\n \n \n def softmax(x, axis=None):\n     if any_symbolic_tensors((x,)):\n         return Softmax(axis).symbolic_call(x)\n-    return backend.nn.softmax(x)\n+    return backend.nn.softmax(x, axis=axis)\n \n \n class LogSoftmax(Operation):\n@@ -258,7 +272,7 @@ class LogSoftmax(Operation):\n         return backend.nn.log_softmax(x, axis=self.axis)\n \n     def compute_output_spec(self, x):\n-        return KerasTensor(x.shape)\n+        return KerasTensor(x.shape, x.dtype)\n \n \n def log_softmax(x, axis=None):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
