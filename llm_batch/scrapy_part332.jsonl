{"custom_id": "scrapy#5d7c0a5f861327c1a51ffcb3a39e283eb999fae7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 123 | Contributors (this commit): 3 | Commits (past 90d): 1 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -19,11 +19,11 @@ import scrapy\n \n class CheckSpider(scrapy.Spider):\n     name = '{self.spider_name}'\n-    start_urls = ['http://example.com']\n+    start_urls = ['http://toscrape.com']\n \n     def parse(self, response, **cb_kwargs):\n         \\\"\\\"\\\"\n-        @url http://example.com\n+        @url http://toscrape.com\n         {contracts}\n         \\\"\\\"\\\"\n         {parse_def}\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#115d35270691176e128bd30f6a2a8bcf1289fe5c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 839 | Contributors (this commit): 13 | Commits (past 90d): 5 | Contributors (cumulative): 13 | DMM Complexity: None\n\nDIFF:\n@@ -86,7 +86,7 @@ class Downloader:\n         self.middleware = DownloaderMiddlewareManager.from_crawler(crawler)\n         self._slot_gc_loop = task.LoopingCall(self._slot_gc)\n         self._slot_gc_loop.start(60)\n-        self.per_slot_settings = self.settings.getdict('PER_SLOT_SETTINGS', {})\n+        self.per_slot_settings = self.settings.getdict('DOWNLOAD_SLOTS', {})\n \n     def fetch(self, request, spider):\n         def _deactivate(response):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#be97402e46c5e4793e23b713728e1777fbe4d49c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 843 | Contributors (this commit): 13 | Commits (past 90d): 6 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -109,8 +109,6 @@ class Downloader:\n             randomize_delay = self.per_slot_settings.get(key, {}).get('randomize_delay', self.randomize_delay)\n             new_slot = Slot(conc, delay, randomize_delay)\n             self.slots[key] = new_slot\n-            logger.debug(\n-                f\"Downloader slot '{key}' - created {'(from per slot settings)' if key in self.per_slot_settings.keys() else ''}: {new_slot}\")\n \n         return key, self.slots[key]\n \n@@ -209,5 +207,3 @@ class Downloader:\n             if not slot.active and slot.lastseen + slot.delay < mintime:\n                 inactive_slot = self.slots.pop(key)\n                 inactive_slot.close()\n-                logger.debug(\n-                    f\"Downloader slot '{key}' - closed {'(from per slot settings)' if key in self.per_slot_settings.keys() else ''} : {inactive_slot}\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#befb6df119058db8a6a340b8235ccb565a60f3ca", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 6 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 7 | Churn Cumulative: 244 | Contributors (this commit): 14 | Commits (past 90d): 2 | Contributors (cumulative): 19 | DMM Complexity: 0.0\n\nDIFF:\n@@ -142,10 +142,6 @@ class WrappedRequest:\n         \"\"\"\n         return self.request.meta.get('is_unverifiable', False)\n \n-    def get_origin_req_host(self):\n-        return urlparse_cached(self.request).hostname\n-\n-    # python3 uses attributes instead of methods\n     @property\n     def full_url(self):\n         return self.get_full_url()\n@@ -164,7 +160,7 @@ class WrappedRequest:\n \n     @property\n     def origin_req_host(self):\n-        return self.get_origin_req_host()\n+        return urlparse_cached(self.request).hostname\n \n     def has_header(self, name):\n         return name in self.request.headers\n\n@@ -34,7 +34,6 @@ class WrappedRequestTest(TestCase):\n         self.assertTrue(self.wrapped.unverifiable)\n \n     def test_get_origin_req_host(self):\n-        self.assertEqual(self.wrapped.get_origin_req_host(), 'www.example.com')\n         self.assertEqual(self.wrapped.origin_req_host, 'www.example.com')\n \n     def test_has_header(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#aa0306a167ef34b23cc2ec407a48359a4b5a8d0a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 1000 | Contributors (this commit): 33 | Commits (past 90d): 3 | Contributors (cumulative): 39 | DMM Complexity: None\n\nDIFF:\n@@ -303,10 +303,14 @@ intersphinx_mapping = {\n hoverxref_auto_ref = True\n hoverxref_role_types = {\n     \"class\": \"tooltip\",\n+    \"command\": \"tooltip\",\n     \"confval\": \"tooltip\",\n     \"hoverxref\": \"tooltip\",\n     \"mod\": \"tooltip\",\n     \"ref\": \"tooltip\",\n+    \"reqmeta\": \"tooltip\",\n+    \"setting\": \"tooltip\",\n+    \"signal\": \"tooltip\",\n }\n hoverxref_roles = ['command', 'reqmeta', 'setting', 'signal']\n \n\n@@ -182,7 +182,7 @@ def maybeDeferred_coro(f: Callable, *args, **kw) -> Deferred:\n \n def deferred_to_future(d: Deferred) -> Future:\n     \"\"\"\n-    .. versionadded:: VERSION\n+    .. versionadded:: 2.6.0\n \n     Return an :class:`asyncio.Future` object that wraps *d*.\n \n@@ -203,7 +203,7 @@ def deferred_to_future(d: Deferred) -> Future:\n \n def maybe_deferred_to_future(d: Deferred) -> Union[Deferred, Future]:\n     \"\"\"\n-    .. versionadded:: VERSION\n+    .. versionadded:: 2.6.0\n \n     Return *d* as an object that can be awaited from a :ref:`Scrapy callable\n     defined as a coroutine <coroutine-support>`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#8ce01b3b76d4634f55067d6cfdf632ec70ba304a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 181 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 9 | Methods Changed: 15 | Complexity Δ (Sum/Max): 20/17 | Churn Δ: 186 | Churn Cumulative: 685 | Contributors (this commit): 19 | Commits (past 90d): 2 | Contributors (cumulative): 25 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,6 +4,7 @@ from urllib.parse import urljoin, urlparse\n from w3lib.url import safe_url_string\n \n from scrapy.http import HtmlResponse\n+from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.response import get_meta_refresh\n from scrapy.exceptions import IgnoreRequest, NotConfigured\n \n@@ -11,6 +12,21 @@ from scrapy.exceptions import IgnoreRequest, NotConfigured\n logger = logging.getLogger(__name__)\n \n \n+def _build_redirect_request(source_request, *, url, method=None, body=None):\n+    redirect_request = source_request.replace(\n+        url=url,\n+        method=method,\n+        body=body,\n+        cookies=None,\n+    )\n+    if 'Cookie' in redirect_request.headers:\n+        source_request_netloc = urlparse_cached(source_request).netloc\n+        redirect_request_netloc = urlparse_cached(redirect_request).netloc\n+        if source_request_netloc != redirect_request_netloc:\n+            del redirect_request.headers['Cookie']\n+    return redirect_request\n+\n+\n class BaseRedirectMiddleware:\n \n     enabled_setting = 'REDIRECT_ENABLED'\n@@ -47,10 +63,15 @@ class BaseRedirectMiddleware:\n             raise IgnoreRequest(\"max redirections reached\")\n \n     def _redirect_request_using_get(self, request, redirect_url):\n-        redirected = request.replace(url=redirect_url, method='GET', body='')\n-        redirected.headers.pop('Content-Type', None)\n-        redirected.headers.pop('Content-Length', None)\n-        return redirected\n+        redirect_request = _build_redirect_request(\n+            request,\n+            url=redirect_url,\n+            method='GET',\n+            body='',\n+        )\n+        redirect_request.headers.pop('Content-Type', None)\n+        redirect_request.headers.pop('Content-Length', None)\n+        return redirect_request\n \n \n class RedirectMiddleware(BaseRedirectMiddleware):\n@@ -80,7 +101,7 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n         redirected_url = urljoin(request.url, location)\n \n         if response.status in (301, 307, 308) or request.method == 'HEAD':\n-            redirected = request.replace(url=redirected_url)\n+            redirected = _build_redirect_request(request, url=redirected_url)\n             return self._redirect(redirected, request, spider, response.status)\n \n         redirected = self._redirect_request_using_get(request, redirected_url)\n\n@@ -6,8 +6,10 @@ import pytest\n \n from scrapy.downloadermiddlewares.cookies import CookiesMiddleware\n from scrapy.downloadermiddlewares.defaultheaders import DefaultHeadersMiddleware\n+from scrapy.downloadermiddlewares.redirect import RedirectMiddleware\n from scrapy.exceptions import NotConfigured\n from scrapy.http import Response, Request\n+from scrapy.settings import Settings\n from scrapy.spiders import Spider\n from scrapy.utils.python import to_bytes\n from scrapy.utils.test import get_crawler\n@@ -23,9 +25,11 @@ class CookiesMiddlewareTest(TestCase):\n     def setUp(self):\n         self.spider = Spider('foo')\n         self.mw = CookiesMiddleware()\n+        self.redirect_middleware = RedirectMiddleware(settings=Settings())\n \n     def tearDown(self):\n         del self.mw\n+        del self.redirect_middleware\n \n     def test_basic(self):\n         req = Request('http://scrapytest.org/')\n@@ -368,3 +372,154 @@ class CookiesMiddlewareTest(TestCase):\n         req4 = Request('http://example.org', cookies={'a': 'b'})\n         assert self.mw.process_request(req4, self.spider) is None\n         self.assertCookieValEqual(req4.headers['Cookie'], b'a=b')\n+\n+    def _test_cookie_redirect(\n+        self,\n+        source,\n+        target,\n+        *,\n+        cookies1,\n+        cookies2,\n+    ):\n+        input_cookies = {'a': 'b'}\n+\n+        if not isinstance(source, dict):\n+            source = {'url': source}\n+        if not isinstance(target, dict):\n+            target = {'url': target}\n+        target.setdefault('status', 301)\n+\n+        request1 = Request(cookies=input_cookies, **source)\n+        self.mw.process_request(request1, self.spider)\n+        cookies = request1.headers.get('Cookie')\n+        self.assertEqual(cookies, b\"a=b\" if cookies1 else None)\n+\n+        response = Response(\n+            headers={\n+                'Location': target['url'],\n+            },\n+            **target,\n+        )\n+        self.assertEqual(\n+            self.mw.process_response(request1, response, self.spider),\n+            response,\n+        )\n+\n+        request2 = self.redirect_middleware.process_response(\n+            request1,\n+            response,\n+            self.spider,\n+        )\n+        self.assertIsInstance(request2, Request)\n+\n+        self.mw.process_request(request2, self.spider)\n+        cookies = request2.headers.get('Cookie')\n+        self.assertEqual(cookies, b\"a=b\" if cookies2 else None)\n+\n+    def test_cookie_redirect_same_domain(self):\n+        self._test_cookie_redirect(\n+            'https://toscrape.com',\n+            'https://toscrape.com',\n+            cookies1=True,\n+            cookies2=True,\n+        )\n+\n+    def test_cookie_redirect_same_domain_forcing_get(self):\n+        self._test_cookie_redirect(\n+            'https://toscrape.com',\n+            {'url': 'https://toscrape.com', 'status': 302},\n+            cookies1=True,\n+            cookies2=True,\n+        )\n+\n+    def test_cookie_redirect_different_domain(self):\n+        self._test_cookie_redirect(\n+            'https://toscrape.com',\n+            'https://example.com',\n+            cookies1=True,\n+            cookies2=False,\n+        )\n+\n+    def test_cookie_redirect_different_domain_forcing_get(self):\n+        self._test_cookie_redirect(\n+            'https://toscrape.com',\n+            {'url': 'https://example.com', 'status': 302},\n+            cookies1=True,\n+            cookies2=False,\n+        )\n+\n+    def _test_cookie_header_redirect(\n+        self,\n+        source,\n+        target,\n+        *,\n+        cookies2,\n+    ):\n+        \"\"\"Test the handling of a user-defined Cookie header when building a\n+        redirect follow-up request.\n+\n+        We follow RFC 6265 for cookie handling. The Cookie header can only\n+        contain a list of key-value pairs (i.e. no additional cookie\n+        parameters like Domain or Path). Because of that, we follow the same\n+        rules that we would follow for the handling of the Set-Cookie response\n+        header when the Domain is not set: the cookies must be limited to the\n+        target URL domain (not even subdomains can receive those cookies).\n+\n+        .. note:: This method tests the scenario where the cookie middleware is\n+                  disabled. Because of known issue #1992, when the cookies\n+                  middleware is enabled we do not need to be concerned about\n+                  the Cookie header getting leaked to unintended domains,\n+                  because the middleware empties the header from every request.\n+        \"\"\"\n+        if not isinstance(source, dict):\n+            source = {'url': source}\n+        if not isinstance(target, dict):\n+            target = {'url': target}\n+        target.setdefault('status', 301)\n+\n+        request1 = Request(headers={'Cookie': b'a=b'}, **source)\n+\n+        response = Response(\n+            headers={\n+                'Location': target['url'],\n+            },\n+            **target,\n+        )\n+\n+        request2 = self.redirect_middleware.process_response(\n+            request1,\n+            response,\n+            self.spider,\n+        )\n+        self.assertIsInstance(request2, Request)\n+\n+        cookies = request2.headers.get('Cookie')\n+        self.assertEqual(cookies, b\"a=b\" if cookies2 else None)\n+\n+    def test_cookie_header_redirect_same_domain(self):\n+        self._test_cookie_header_redirect(\n+            'https://toscrape.com',\n+            'https://toscrape.com',\n+            cookies2=True,\n+        )\n+\n+    def test_cookie_header_redirect_same_domain_forcing_get(self):\n+        self._test_cookie_header_redirect(\n+            'https://toscrape.com',\n+            {'url': 'https://toscrape.com', 'status': 302},\n+            cookies2=True,\n+        )\n+\n+    def test_cookie_header_redirect_different_domain(self):\n+        self._test_cookie_header_redirect(\n+            'https://toscrape.com',\n+            'https://example.com',\n+            cookies2=False,\n+        )\n+\n+    def test_cookie_header_redirect_different_domain_forcing_get(self):\n+        self._test_cookie_header_redirect(\n+            'https://toscrape.com',\n+            {'url': 'https://example.com', 'status': 302},\n+            cookies2=False,\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e865c4430e58a4faa0e0766b23830f8423d6167a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 202 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 9 | Methods Changed: 16 | Complexity Δ (Sum/Max): 31/25 | Churn Δ: 205 | Churn Cumulative: 1510 | Contributors (this commit): 37 | Commits (past 90d): 4 | Contributors (cumulative): 50 | DMM Complexity: 0.8164556962025317\n\nDIFF:\n@@ -1,15 +1,26 @@\n import logging\n from collections import defaultdict\n \n+from tldextract import TLDExtract\n+\n from scrapy.exceptions import NotConfigured\n from scrapy.http import Response\n from scrapy.http.cookies import CookieJar\n+from scrapy.utils.httpobj import urlparse_cached\n from scrapy.utils.python import to_unicode\n \n \n logger = logging.getLogger(__name__)\n \n \n+_split_domain = TLDExtract(include_psl_private_domains=True)\n+\n+\n+def _is_public_domain(domain):\n+    parts = _split_domain(domain)\n+    return not parts.domain\n+\n+\n class CookiesMiddleware:\n     \"\"\"This middleware enables working with sites that need cookies\"\"\"\n \n@@ -23,14 +34,29 @@ class CookiesMiddleware:\n             raise NotConfigured\n         return cls(crawler.settings.getbool('COOKIES_DEBUG'))\n \n+    def _process_cookies(self, cookies, *, jar, request):\n+        for cookie in cookies:\n+            cookie_domain = cookie.domain\n+            if cookie_domain.startswith('.'):\n+                cookie_domain = cookie_domain[1:]\n+\n+            request_domain = urlparse_cached(request).hostname.lower()\n+\n+            if cookie_domain and _is_public_domain(cookie_domain):\n+                if cookie_domain != request_domain:\n+                    continue\n+                cookie.domain = request_domain\n+\n+            jar.set_cookie_if_ok(cookie, request)\n+\n     def process_request(self, request, spider):\n         if request.meta.get('dont_merge_cookies', False):\n             return\n \n         cookiejarkey = request.meta.get(\"cookiejar\")\n         jar = self.jars[cookiejarkey]\n-        for cookie in self._get_request_cookies(jar, request):\n-            jar.set_cookie_if_ok(cookie, request)\n+        cookies = self._get_request_cookies(jar, request)\n+        self._process_cookies(cookies, jar=jar, request=request)\n \n         # set Cookie header\n         request.headers.pop('Cookie', None)\n@@ -44,7 +70,9 @@ class CookiesMiddleware:\n         # extract cookies from Set-Cookie and drop invalid/expired cookies\n         cookiejarkey = request.meta.get(\"cookiejar\")\n         jar = self.jars[cookiejarkey]\n-        jar.extract_cookies(response, request)\n+        cookies = jar.make_cookies(response, request)\n+        self._process_cookies(cookies, jar=jar, request=request)\n+\n         self._debug_set_cookie(response, spider)\n \n         return response\n\n@@ -32,6 +32,7 @@ install_requires = [\n     'protego>=0.1.15',\n     'itemadapter>=0.1.0',\n     'setuptools',\n+    'tldextract',\n ]\n extras_require = {}\n cpython_dependencies = [\n\n@@ -15,6 +15,48 @@ from scrapy.utils.python import to_bytes\n from scrapy.utils.test import get_crawler\n \n \n+def _cookie_to_set_cookie_value(cookie):\n+    \"\"\"Given a cookie defined as a dictionary with name and value keys, and\n+    optional path and domain keys, return the equivalent string that can be\n+    associated to a ``Set-Cookie`` header.\"\"\"\n+    decoded = {}\n+    for key in (\"name\", \"value\", \"path\", \"domain\"):\n+        if cookie.get(key) is None:\n+            if key in (\"name\", \"value\"):\n+                return\n+            continue\n+        if isinstance(cookie[key], (bool, float, int, str)):\n+            decoded[key] = str(cookie[key])\n+        else:\n+            try:\n+                decoded[key] = cookie[key].decode(\"utf8\")\n+            except UnicodeDecodeError:\n+                decoded[key] = cookie[key].decode(\"latin1\", errors=\"replace\")\n+\n+    cookie_str = f\"{decoded.pop('name')}={decoded.pop('value')}\"\n+    for key, value in decoded.items():  # path, domain\n+        cookie_str += f\"; {key.capitalize()}={value}\"\n+    return cookie_str\n+\n+\n+def _cookies_to_set_cookie_list(cookies):\n+    \"\"\"Given a group of cookie defined either as a dictionary or as a list of\n+    dictionaries (i.e. in a format supported by the cookies parameter of\n+    Request), return the equivalen list of strings that can be associated to a\n+    ``Set-Cookie`` header.\"\"\"\n+    if not cookies:\n+        return []\n+    if isinstance(cookies, dict):\n+        cookies = ({\"name\": k, \"value\": v} for k, v in cookies.items())\n+    return filter(\n+        None,\n+        (\n+            _cookie_to_set_cookie_value(cookie)\n+            for cookie in cookies\n+        )\n+    )\n+\n+\n class CookiesMiddlewareTest(TestCase):\n \n     def assertCookieValEqual(self, first, second, msg=None):\n@@ -523,3 +565,131 @@ class CookiesMiddlewareTest(TestCase):\n             {'url': 'https://example.com', 'status': 302},\n             cookies2=False,\n         )\n+\n+    def _test_user_set_cookie_domain_followup(\n+        self,\n+        url1,\n+        url2,\n+        domain,\n+        *,\n+        cookies1,\n+        cookies2,\n+    ):\n+        input_cookies = [\n+            {\n+                'name': 'a',\n+                'value': 'b',\n+                'domain': domain,\n+            }\n+        ]\n+\n+        request1 = Request(url1, cookies=input_cookies)\n+        self.mw.process_request(request1, self.spider)\n+        cookies = request1.headers.get('Cookie')\n+        self.assertEqual(cookies, b\"a=b\" if cookies1 else None)\n+\n+        request2 = Request(url2)\n+        self.mw.process_request(request2, self.spider)\n+        cookies = request2.headers.get('Cookie')\n+        self.assertEqual(cookies, b\"a=b\" if cookies2 else None)\n+\n+    def test_user_set_cookie_domain_suffix_private(self):\n+        self._test_user_set_cookie_domain_followup(\n+            'https://books.toscrape.com',\n+            'https://quotes.toscrape.com',\n+            'toscrape.com',\n+            cookies1=True,\n+            cookies2=True,\n+        )\n+\n+    def test_user_set_cookie_domain_suffix_public_period(self):\n+        self._test_user_set_cookie_domain_followup(\n+            'https://foo.co.uk',\n+            'https://bar.co.uk',\n+            'co.uk',\n+            cookies1=False,\n+            cookies2=False,\n+        )\n+\n+    def test_user_set_cookie_domain_suffix_public_private(self):\n+        self._test_user_set_cookie_domain_followup(\n+            'https://foo.blogspot.com',\n+            'https://bar.blogspot.com',\n+            'blogspot.com',\n+            cookies1=False,\n+            cookies2=False,\n+        )\n+\n+    def test_user_set_cookie_domain_public_period(self):\n+        self._test_user_set_cookie_domain_followup(\n+            'https://co.uk',\n+            'https://co.uk',\n+            'co.uk',\n+            cookies1=True,\n+            cookies2=True,\n+        )\n+\n+    def _test_server_set_cookie_domain_followup(\n+        self,\n+        url1,\n+        url2,\n+        domain,\n+        *,\n+        cookies,\n+    ):\n+        request1 = Request(url1)\n+        self.mw.process_request(request1, self.spider)\n+\n+        input_cookies = [\n+            {\n+                'name': 'a',\n+                'value': 'b',\n+                'domain': domain,\n+            }\n+        ]\n+\n+        headers = {\n+            'Set-Cookie': _cookies_to_set_cookie_list(input_cookies),\n+        }\n+        response = Response(url1, status=200, headers=headers)\n+        self.assertEqual(\n+            self.mw.process_response(request1, response, self.spider),\n+            response,\n+        )\n+\n+        request2 = Request(url2)\n+        self.mw.process_request(request2, self.spider)\n+        actual_cookies = request2.headers.get('Cookie')\n+        self.assertEqual(actual_cookies, b\"a=b\" if cookies else None)\n+\n+    def test_server_set_cookie_domain_suffix_private(self):\n+        self._test_server_set_cookie_domain_followup(\n+            'https://books.toscrape.com',\n+            'https://quotes.toscrape.com',\n+            'toscrape.com',\n+            cookies=True,\n+        )\n+\n+    def test_server_set_cookie_domain_suffix_public_period(self):\n+        self._test_server_set_cookie_domain_followup(\n+            'https://foo.co.uk',\n+            'https://bar.co.uk',\n+            'co.uk',\n+            cookies=False,\n+        )\n+\n+    def test_server_set_cookie_domain_suffix_public_private(self):\n+        self._test_server_set_cookie_domain_followup(\n+            'https://foo.blogspot.com',\n+            'https://bar.blogspot.com',\n+            'blogspot.com',\n+            cookies=False,\n+        )\n+\n+    def test_server_set_cookie_domain_public_period(self):\n+        self._test_server_set_cookie_domain_followup(\n+            'https://co.uk',\n+            'https://co.uk',\n+            'co.uk',\n+            cookies=True,\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d60636d0de94c5a08c25d1d6820faed0b45506b7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 156 | Contributors (this commit): 14 | Commits (past 90d): 2 | Contributors (cumulative): 14 | DMM Complexity: 0.0\n\nDIFF:\n@@ -12,11 +12,10 @@ from scrapy.exceptions import IgnoreRequest, NotConfigured\n logger = logging.getLogger(__name__)\n \n \n-def _build_redirect_request(source_request, *, url, method=None, body=None):\n+def _build_redirect_request(source_request, *, url, **kwargs):\n     redirect_request = source_request.replace(\n         url=url,\n-        method=method,\n-        body=body,\n+        **kwargs,\n         cookies=None,\n     )\n     if 'Cookie' in redirect_request.headers:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3ecbea4b876ed084f30bb4063e1993dd9c3cdb8a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 1841 | Contributors (this commit): 28 | Commits (past 90d): 4 | Contributors (cumulative): 28 | DMM Complexity: 1.0\n\nDIFF:\n@@ -290,6 +290,7 @@ class CrawlerProcess(CrawlerRunner):\n         super().__init__(settings)\n         configure_logging(self.settings, install_root_handler)\n         log_scrapy_info(self.settings)\n+        self._initiated_reactor = False\n \n     def _signal_shutdown(self, signum, _):\n         from twisted.internet import reactor\n@@ -310,7 +311,9 @@ class CrawlerProcess(CrawlerRunner):\n     def _create_crawler(self, spidercls):\n         if isinstance(spidercls, str):\n             spidercls = self.spider_loader.load(spidercls)\n-        return Crawler(spidercls, self.settings, init_reactor=True)\n+        init_reactor = not self._initiated_reactor\n+        self._initiated_reactor = True\n+        return Crawler(spidercls, self.settings, init_reactor=init_reactor)\n \n     def start(self, stop_after_crawl=True, install_signal_handlers=True):\n         \"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#96fc4dae15181695c58040389fa502857a2b0df8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 22 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 22 | Churn Cumulative: 963 | Contributors (this commit): 15 | Commits (past 90d): 8 | Contributors (cumulative): 16 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,16 @@\n+import scrapy\n+from scrapy.crawler import CrawlerProcess\n+\n+\n+class NoRequestsSpider(scrapy.Spider):\n+    name = 'no_request'\n+\n+    def start_requests(self):\n+        return []\n+\n+\n+process = CrawlerProcess(settings={})\n+\n+process.crawl(NoRequestsSpider)\n+process.crawl(NoRequestsSpider)\n+process.start()\n\n@@ -302,6 +302,12 @@ class CrawlerProcessSubprocess(ScriptRunnerMixin, unittest.TestCase):\n         self.assertIn('Spider closed (finished)', log)\n         self.assertNotIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n \n+    def test_multi(self):\n+        log = self.run_script('multi.py')\n+        self.assertIn('Spider closed (finished)', log)\n+        self.assertNotIn(\"Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\", log)\n+        self.assertNotIn(\"ReactorAlreadyInstalledError\", log)\n+\n     def test_asyncio_enabled_no_reactor(self):\n         log = self.run_script('asyncio_enabled_no_reactor.py')\n         self.assertIn('Spider closed (finished)', log)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3bf6baeaa705ddc2d3417206f3db816dabadf5a9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1847 | Contributors (this commit): 28 | Commits (past 90d): 5 | Contributors (cumulative): 28 | DMM Complexity: None\n\nDIFF:\n@@ -290,7 +290,7 @@ class CrawlerProcess(CrawlerRunner):\n         super().__init__(settings)\n         configure_logging(self.settings, install_root_handler)\n         log_scrapy_info(self.settings)\n-        self._initiated_reactor = False\n+        self._initialized_reactor = False\n \n     def _signal_shutdown(self, signum, _):\n         from twisted.internet import reactor\n@@ -311,8 +311,8 @@ class CrawlerProcess(CrawlerRunner):\n     def _create_crawler(self, spidercls):\n         if isinstance(spidercls, str):\n             spidercls = self.spider_loader.load(spidercls)\n-        init_reactor = not self._initiated_reactor\n-        self._initiated_reactor = True\n+        init_reactor = not self._initialized_reactor\n+        self._initialized_reactor = True\n         return Crawler(spidercls, self.settings, init_reactor=init_reactor)\n \n     def start(self, stop_after_crawl=True, install_signal_handlers=True):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#62a00812669aa0906aa0d4e9a4c2e87be3b74975", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 2789 | Contributors (this commit): 27 | Commits (past 90d): 1 | Contributors (cumulative): 27 | DMM Complexity: 0.5\n\nDIFF:\n@@ -136,7 +136,9 @@ class ExecutionEngine:\n         self.paused = False\n \n     def _next_request(self) -> None:\n-        assert self.slot is not None  # typing\n+        if self.slot is None:\n+            return\n+\n         assert self.spider is not None  # typing\n \n         if self.paused:\n@@ -184,7 +186,8 @@ class ExecutionEngine:\n         d.addErrback(lambda f: logger.info('Error while removing request from slot',\n                                            exc_info=failure_to_exc_info(f),\n                                            extra={'spider': self.spider}))\n-        d.addBoth(lambda _: self.slot.nextcall.schedule())\n+        slot = self.slot\n+        d.addBoth(lambda _: slot.nextcall.schedule())\n         d.addErrback(lambda f: logger.info('Error while scheduling new request',\n                                            exc_info=failure_to_exc_info(f),\n                                            extra={'spider': self.spider}))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c1d4be8cb5ed610fef4a2af60f9b654faab7a243", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 3/2 | Churn Δ: 26 | Churn Cumulative: 2268 | Contributors (this commit): 33 | Commits (past 90d): 6 | Contributors (cumulative): 43 | DMM Complexity: 1.0\n\nDIFF:\n@@ -14,6 +14,15 @@ from scrapy.utils.project import inside_project, get_project_settings\n from scrapy.utils.python import garbage_collect\n \n \n+class ScrapyArgumentParser(argparse.ArgumentParser):\n+    def _parse_optional(self, arg_string):\n+        # if starts with -: it means that is a parameter not a argument\n+        if arg_string[:2] == '-:':\n+            return None\n+\n+        return super()._parse_optional(arg_string)\n+\n+\n def _iter_command_classes(module_name):\n     # TODO: add `name` attribute to commands and and merge this function with\n     # scrapy.utils.spider.iter_spider_classes\n@@ -131,7 +140,7 @@ def execute(argv=None, settings=None):\n         sys.exit(2)\n \n     cmd = cmds[cmdname]\n-    parser = argparse.ArgumentParser(formatter_class=ScrapyHelpFormatter,\n+    parser = ScrapyArgumentParser(formatter_class=ScrapyHelpFormatter,\n                                   usage=f\"scrapy {cmdname} {cmd.syntax()}\",\n                                   conflict_handler='resolve',\n                                   description=cmd.long_desc())\n\n@@ -770,6 +770,21 @@ class MySpider(scrapy.Spider):\n         log = self.get_log(spider_code, args=args)\n         self.assertIn(\"error: Please use only one of -o/--output and -O/--overwrite-output\", log)\n \n+    def test_output_stdout(self):\n+        spider_code = \"\"\"\n+import scrapy\n+\n+class MySpider(scrapy.Spider):\n+    name = 'myspider'\n+\n+    def start_requests(self):\n+        self.logger.debug('FEEDS: {}'.format(self.settings.getdict('FEEDS')))\n+        return []\n+\"\"\"\n+        args = ['-o', '-:json']\n+        log = self.get_log(spider_code, args=args)\n+        self.assertIn(\"[myspider] DEBUG: FEEDS: {'stdout:': {'format': 'json'}}\", log)\n+\n \n @skipIf(platform.system() != 'Windows', \"Windows required for .pyw files\")\n class WindowsRunSpiderCommandTest(RunSpiderCommandTest):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b59a69be1790f138afe89df3dfed17ac48384d8c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 33 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 33 | Churn Cumulative: 721 | Contributors (this commit): 17 | Commits (past 90d): 1 | Contributors (cumulative): 17 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,9 +12,11 @@ module with the ``runserver`` argument::\n \n import os\n import re\n+import subprocess\n import sys\n import warnings\n from collections import defaultdict\n+from threading import Timer\n from urllib.parse import urlparse\n \n import attr\n@@ -502,6 +504,37 @@ class EngineTest(unittest.TestCase):\n             self.assertEqual(warning_list[0].category, ScrapyDeprecationWarning)\n             self.assertEqual(str(warning_list[0].message), \"ExecutionEngine.has_capacity is deprecated\")\n \n+    def test_short_timeout(self):\n+        args = (\n+            sys.executable,\n+            '-m',\n+            'scrapy.cmdline',\n+            'fetch',\n+            '-s',\n+            'CLOSESPIDER_TIMEOUT=0.001',\n+            '-s',\n+            'LOG_LEVEL=DEBUG',\n+            'http://toscrape.com',\n+        )\n+        p = subprocess.Popen(\n+            args,\n+            stderr=subprocess.PIPE,\n+        )\n+\n+        def kill_proc():\n+            p.kill()\n+            p.communicate()\n+            assert False, 'Command took too much time to complete'\n+\n+        timer = Timer(15, kill_proc)\n+        try:\n+            timer.start()\n+            _, stderr = p.communicate()\n+        finally:\n+            timer.cancel()\n+\n+        self.assertNotIn(b'Traceback', stderr)\n+\n \n if __name__ == \"__main__\":\n     if len(sys.argv) > 1 and sys.argv[1] == 'runserver':\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#78ba4b033b016be7fbb22bfa9e6d5d389380e6d4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 5 | Churn Cumulative: 1117 | Contributors (this commit): 21 | Commits (past 90d): 1 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -5,7 +5,6 @@ library.\n Some of the functions that used to be imported from this module have been moved\n to the w3lib.url module. Always import those from there instead.\n \"\"\"\n-import posixpath\n import re\n from urllib.parse import ParseResult, urldefrag, urlparse, urlunparse\n \n@@ -31,8 +30,8 @@ def url_is_from_spider(url, spider):\n \n \n def url_has_any_extension(url, extensions):\n-    return posixpath.splitext(parse_url(url).path)[1].lower() in extensions\n-\n+    \"\"\"Return True if the url ends with one of the extensions provided\"\"\"\n+    return any(parse_url(url).path.lower().endswith(ext) for ext in extensions)\n \n def parse_url(url, encoding=None):\n     \"\"\"Return urlparsed url from the given argument (which could be an already\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#c961438d5d9998344460d930ea502fae40553043", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 15 | Files Changed: 2 | Hunks: 8 | Methods Changed: 7 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 73 | Churn Cumulative: 2726 | Contributors (this commit): 11 | Commits (past 90d): 4 | Contributors (cumulative): 14 | DMM Complexity: 0.7096774193548387\n\nDIFF:\n@@ -4,7 +4,7 @@ Spider Middleware manager\n See documentation in docs/topics/spider-middleware.rst\n \"\"\"\n import logging\n-from inspect import isasyncgenfunction\n+from inspect import isasyncgenfunction, iscoroutine\n from itertools import islice\n from typing import Any, AsyncGenerator, AsyncIterable, Callable, Generator, Iterable, Tuple, Union, cast\n \n@@ -61,7 +61,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             try:\n                 result = method(response=response, spider=spider)\n                 if result is not None:\n-                    msg = (f\"Middleware {method.__qualname__} must return None \"\n+                    msg = (f\"{method.__qualname__} must return None \"\n                            f\"or raise an exception, got {type(result)}\")\n                     raise _InvalidOutput(msg)\n             except _InvalidOutput:\n@@ -129,7 +129,7 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             elif result is None:\n                 continue\n             else:\n-                msg = (f\"Middleware {method.__qualname__} must return None \"\n+                msg = (f\"{method.__qualname__} must return None \"\n                        f\"or an iterable, got {type(result)}\")\n                 raise _InvalidOutput(msg)\n         return _failure\n@@ -197,8 +197,17 @@ class SpiderMiddlewareManager(MiddlewareManager):\n             if _isiterable(result):\n                 result = self._evaluate_iterable(response, spider, result, method_index + 1, recovered)\n             else:\n-                msg = (f\"Middleware {method.__qualname__} must return an \"\n-                       f\"iterable, got {type(result)}\")\n+                if iscoroutine(result):\n+                    result.close()  # Silence warning about not awaiting\n+                    msg = (\n+                        f\"{method.__qualname__} must be an asynchronous \"\n+                        f\"generator (i.e. use yield)\"\n+                    )\n+                else:\n+                    msg = (\n+                        f\"{method.__qualname__} must return an iterable, got \"\n+                        f\"{type(result)}\"\n+                    )\n                 raise _InvalidOutput(msg)\n             last_result_is_async = isinstance(result, AsyncIterable)\n \n\n@@ -123,6 +123,11 @@ class BaseAsyncSpiderMiddlewareTestCase(SpiderMiddlewareTestCase):\n             start_index = 10\n         return {i: c for c, i in enumerate(mw_classes, start=start_index)}\n \n+    def _scrape_func(self, *args, **kwargs):\n+        yield {'foo': 1}\n+        yield {'foo': 2}\n+        yield {'foo': 3}\n+\n     @defer.inlineCallbacks\n     def _get_middleware_result(self, *mw_classes, start_index: Optional[int] = None):\n         setting = self._construct_mw_setting(*mw_classes, start_index=start_index)\n@@ -201,11 +206,6 @@ class ProcessSpiderOutputSimple(BaseAsyncSpiderMiddlewareTestCase):\n     MW_ASYNCGEN = ProcessSpiderOutputAsyncGenMiddleware\n     MW_UNIVERSAL = ProcessSpiderOutputUniversalMiddleware\n \n-    def _scrape_func(self, *args, **kwargs):\n-        yield {'foo': 1}\n-        yield {'foo': 2}\n-        yield {'foo': 3}\n-\n     def test_simple(self):\n         \"\"\" Simple mw \"\"\"\n         return self._test_simple_base(self.MW_SIMPLE)\n@@ -285,6 +285,45 @@ class ProcessSpiderOutputAsyncGen(ProcessSpiderOutputSimple):\n                                       downgrade=True)\n \n \n+class ProcessSpiderOutputNonIterableMiddleware:\n+    def process_spider_output(self, response, result, spider):\n+        return\n+\n+\n+class ProcessSpiderOutputCoroutineMiddleware:\n+    async def process_spider_output(self, response, result, spider):\n+        results = []\n+        for r in result:\n+            results.append(r)\n+        return results\n+\n+\n+class ProcessSpiderOutputInvalidResult(BaseAsyncSpiderMiddlewareTestCase):\n+\n+    @defer.inlineCallbacks\n+    def test_non_iterable(self):\n+        with self.assertRaisesRegex(\n+            _InvalidOutput,\n+            (\n+                \"\\.process_spider_output must return an iterable, got <class \"\n+                \"'NoneType'>\"\n+            ),\n+        ):\n+            yield self._get_middleware_result(\n+                ProcessSpiderOutputNonIterableMiddleware,\n+            )\n+\n+    @defer.inlineCallbacks\n+    def test_coroutine(self):\n+        with self.assertRaisesRegex(\n+            _InvalidOutput,\n+            \"\\.process_spider_output must be an asynchronous generator\",\n+        ):\n+            yield self._get_middleware_result(\n+                ProcessSpiderOutputCoroutineMiddleware,\n+            )\n+\n+\n class ProcessStartRequestsSimpleMiddleware:\n     def process_start_requests(self, start_requests, spider):\n         for r in start_requests:\n@@ -387,11 +426,6 @@ class BuiltinMiddlewareSimpleTest(BaseAsyncSpiderMiddlewareTestCase):\n     MW_ASYNCGEN = ProcessSpiderOutputAsyncGenMiddleware\n     MW_UNIVERSAL = ProcessSpiderOutputUniversalMiddleware\n \n-    def _scrape_func(self, *args, **kwargs):\n-        yield {'foo': 1}\n-        yield {'foo': 2}\n-        yield {'foo': 3}\n-\n     @defer.inlineCallbacks\n     def _get_middleware_result(self, *mw_classes, start_index: Optional[int] = None):\n         setting = self._construct_mw_setting(*mw_classes, start_index=start_index)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5b4b8b6fb12874d4a0a11c261341639c9af95b10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 13 | Churn Cumulative: 1145 | Contributors (this commit): 12 | Commits (past 90d): 1 | Contributors (cumulative): 12 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,6 +1,8 @@\n import unittest\n \n+from scrapy.linkextractors import IGNORED_EXTENSIONS\n from scrapy.spiders import Spider\n+from scrapy.utils.misc import arg_to_iter\n from scrapy.utils.url import (\n     add_http_if_no_scheme,\n     guess_scheme,\n@@ -8,9 +10,9 @@ from scrapy.utils.url import (\n     strip_url,\n     url_is_from_any_domain,\n     url_is_from_spider,\n+    url_has_any_extension,\n )\n \n-\n __doctests__ = ['scrapy.utils.url']\n \n \n@@ -81,6 +83,15 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertTrue(url_is_from_spider('http://www.example.net/some/page.html', MySpider))\n         self.assertFalse(url_is_from_spider('http://www.example.us/some/page.html', MySpider))\n \n+    def test_url_has_any_extension(self):\n+        deny_extensions = {'.' + e for e in arg_to_iter(IGNORED_EXTENSIONS)}\n+        self.assertTrue(url_has_any_extension(\"http://www.example.com/archive.tar.gz\", deny_extensions))\n+        self.assertTrue(url_has_any_extension(\"http://www.example.com/page.doc\", deny_extensions))\n+        self.assertTrue(url_has_any_extension(\"http://www.example.com/page.pdf\", deny_extensions))\n+        self.assertFalse(url_has_any_extension(\"http://www.example.com/page.htm\", deny_extensions))\n+        self.assertFalse(url_has_any_extension(\"http://www.example.com/\", deny_extensions))\n+        self.assertFalse(url_has_any_extension(\"http://www.example.com/page.doc.html\", deny_extensions))\n+\n \n class AddHttpIfNoScheme(unittest.TestCase):\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0905d42e33871e976760d880316210d4953cd5df", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1120 | Contributors (this commit): 22 | Commits (past 90d): 2 | Contributors (cumulative): 22 | DMM Complexity: 1.0\n\nDIFF:\n@@ -31,7 +31,8 @@ def url_is_from_spider(url, spider):\n \n def url_has_any_extension(url, extensions):\n     \"\"\"Return True if the url ends with one of the extensions provided\"\"\"\n-    return any(parse_url(url).path.lower().endswith(ext) for ext in extensions)\n+    lowercase_path = parse_url(url).path.lower()\n+    return any(lowercase_path.endswith(ext) for ext in extensions)\n \n def parse_url(url, encoding=None):\n     \"\"\"Return urlparsed url from the given argument (which could be an already\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0beed7055cdec3683bf67ec01b573ddf87df723f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 2 | Churn Cumulative: 221 | Contributors (this commit): 13 | Commits (past 90d): 1 | Contributors (cumulative): 13 | DMM Complexity: 0.0\n\nDIFF:\n@@ -161,4 +161,6 @@ class LxmlLinkExtractor(FilteringLinkExtractor):\n         for doc in docs:\n             links = self._extract_links(doc, response.url, response.encoding, base_url)\n             all_links.extend(self._process_links(links))\n+        if self.link_extractor.unique:\n             return unique_list(all_links)\n+        return all_links\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#636127ec1ea2b8949438015c2167ab5d009ff1bf", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 14 | Churn Cumulative: 1539 | Contributors (this commit): 24 | Commits (past 90d): 3 | Contributors (cumulative): 24 | DMM Complexity: 1.0\n\nDIFF:\n@@ -930,3 +930,17 @@ class MySpider(scrapy.Spider):\n         args = ['-o', 'example1.json', '-O', 'example2.json']\n         log = self.get_log(spider_code, args=args)\n         self.assertIn(\"error: Please use only one of -o/--output and -O/--overwrite-output\", log)\n+\n+\n+class HelpMessageTest(CommandTest):\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.commands = [\"parse\", \"startproject\", \"view\", \"crawl\", \"edit\",\n+                         \"list\", \"fetch\", \"settings\", \"shell\", \"runspider\",\n+                         \"version\", \"genspider\", \"check\", \"bench\"]\n+\n+    def test_help_messages(self):\n+        for command in self.commands:\n+            _, out, _ = self.proc(command, \"-h\")\n+            self.assertIn(\"Usage\", out)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
