{"custom_id": "scrapy#2de4b8dc146bcc6056434984fc7bb2d773c98645", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 502 | Contributors (this commit): 8 | Commits (past 90d): 2 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -26,9 +26,9 @@ class ProjectTest(unittest.TestCase):\n         rmtree(self.temp_path)\n \n     def call(self, *new_args, **kwargs):\n-        out = tempfile.TemporaryFile()\n+        with tempfile.TemporaryFile() as out:\n             args = (sys.executable, '-m', 'scrapy.cmdline') + new_args\n-        return subprocess.call(args, stdout=out, stderr=out, cwd=self.cwd, \\\n+            return subprocess.call(args, stdout=out, stderr=out, cwd=self.cwd,\n                 env=self.env, **kwargs)\n \n     def proc(self, *new_args, **kwargs):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6f7efa1d1df5e1042fd654fe5aae74ab864b1c4d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 43 | Lines Deleted: 40 | Files Changed: 29 | Hunks: 39 | Methods Changed: 11 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 83 | Churn Cumulative: 8715 | Contributors (this commit): 37 | Commits (past 90d): 43 | Contributors (cumulative): 149 | DMM Complexity: None\n\nDIFF:\n@@ -13,7 +13,7 @@ command).\n \n from __future__ import print_function\n import sys, optparse, urllib, json\n-from urlparse import urljoin\n+from six.moves.urllib.parse import urljoin\n \n from scrapy.utils.jsonrpc import jsonrpc_client_call, JsonRpcError\n \n\n@@ -8,7 +8,7 @@ import time\n import urllib2\n import netrc\n import json\n-from urlparse import urlparse, urljoin\n+from six.moves.urllib.parse import urlparse, urljoin\n from subprocess import Popen, PIPE, check_call\n \n from w3lib.form import encode_multipart\n\n@@ -1,7 +1,7 @@\n import base64\n from urllib import getproxies, unquote, proxy_bypass\n from urllib2 import _parse_proxy\n-from urlparse import urlunparse\n+from six.moves.urllib.parse import urlunparse\n \n from scrapy.utils.httpobj import urlparse_cached\n from scrapy.exceptions import NotConfigured\n\n@@ -1,4 +1,4 @@\n-from urlparse import urljoin\n+from six.moves.urllib.parse import urljoin\n \n from scrapy import log\n from scrapy.http import HtmlResponse\n\n@@ -4,7 +4,7 @@ enable this middleware and enable the ROBOTSTXT_OBEY setting.\n \n \"\"\"\n \n-import robotparser\n+from six.moves.urllib import robotparser\n \n from scrapy import signals, log\n from scrapy.exceptions import NotConfigured, IgnoreRequest\n\n@@ -7,7 +7,7 @@ See documentation in docs/topics/feed-exports.rst\n import sys, os, posixpath\n from tempfile import TemporaryFile\n from datetime import datetime\n-from urlparse import urlparse\n+from six.moves.urllib.parse import urlparse\n from ftplib import FTP\n \n from zope.interface import Interface, implements\n\n@@ -3,7 +3,7 @@ HTMLParser-based link extractor\n \"\"\"\n \n from HTMLParser import HTMLParser\n-from urlparse import urljoin\n+from six.moves.urllib.parse import urljoin\n \n from w3lib.url import safe_url_string\n \n\n@@ -3,7 +3,7 @@ Link extractor based on lxml.html\n \"\"\"\n \n import re\n-from urlparse import urlparse, urljoin\n+from six.moves.urllib.parse import urlparse, urljoin\n \n import lxml.etree as etree\n \n\n@@ -1,5 +1,5 @@\n import re\n-from urlparse import urljoin\n+from six.moves.urllib.parse import urljoin\n \n from w3lib.html import remove_tags, remove_entities, replace_escape_chars\n \n\n@@ -1,7 +1,7 @@\n \"\"\"\n SGMLParser-based Link extractors\n \"\"\"\n-from urlparse import urljoin\n+from six.moves.urllib.parse import urljoin\n import warnings\n from sgmllib import SGMLParser\n \n\n@@ -7,7 +7,7 @@ import os\n import os.path\n import rfc822\n import time\n-import urlparse\n+from six.moves.urllib.parse import urlparse\n from collections import defaultdict\n from cStringIO import StringIO\n import six\n@@ -167,7 +167,7 @@ class FilesPipeline(MediaPipeline):\n         if os.path.isabs(uri):  # to support win32 paths like: C:\\\\some\\dir\n             scheme = 'file'\n         else:\n-            scheme = urlparse.urlparse(uri).scheme\n+            scheme = urlparse(uri).scheme\n         store_cls = self.STORE_SCHEMES[scheme]\n         return store_cls(uri)\n \n\n@@ -29,7 +29,7 @@ In case of status 200 request, response.headers will come with two keys:\n \"\"\"\n \n import re\n-from urlparse import urlparse\n+from six.moves.urllib.parse import urlparse\n from cStringIO import StringIO\n \n from twisted.internet import reactor\n\n@@ -4,7 +4,7 @@ import re\n \n from time import time\n from cStringIO import StringIO\n-from urlparse import urldefrag\n+from six.moves.urllib.parse import urldefrag\n \n from zope.interface import implements\n from twisted.internet import defer, reactor, protocol\n\n@@ -1,5 +1,5 @@\n from time import time\n-from urlparse import urlparse, urlunparse, urldefrag\n+from six.moves.urllib.parse import urlparse, urlunparse, urldefrag\n \n from twisted.web.client import HTTPClientFactory\n from twisted.web.http import HTTPClient\n\n@@ -5,7 +5,8 @@ This module implements the FormRequest class which is a more covenient class\n See documentation in docs/topics/request-response.rst\n \"\"\"\n \n-import urllib, urlparse\n+import urllib\n+from six.moves.urllib.parse import urljoin\n import lxml.html\n import six\n from scrapy.http.request import Request\n@@ -43,7 +44,7 @@ class FormRequest(Request):\n def _get_form_url(form, url):\n     if url is None:\n         return form.action or form.base_url\n-    return urlparse.urljoin(form.base_url, url)\n+    return urljoin(form.base_url, url)\n \n def _urlencode(seq, enc):\n     values = [(unicode_to_str(k, enc), unicode_to_str(v, enc))\n\n@@ -3,7 +3,7 @@ Common code and definitions used by Link extractors (located in\n scrapy.contrib.linkextractor).\n \"\"\"\n import re\n-from urlparse import urlparse\n+from six.moves.urllib.parse import urlparse\n \n from scrapy.utils.url import url_is_from_any_domain\n from scrapy.utils.url import canonicalize_url, url_is_from_any_domain, url_has_any_extension\n\n@@ -1,4 +1,5 @@\n-import os, urlparse\n+import os\n+from six.moves.urllib.parse import urlparse\n from cStringIO import StringIO\n \n from zope.interface.verify import verifyObject\n@@ -84,7 +85,7 @@ class S3FeedStorageTest(unittest.TestCase):\n         file = storage.open(Spider(\"default\"))\n         file.write(\"content\")\n         yield storage.store(file)\n-        u = urlparse.urlparse(uri)\n+        u = urlparse(uri)\n         key = connect_s3().get_bucket(u.hostname, validate=False).get_key(u.path)\n         self.failUnlessEqual(key.get_contents_as_string(), \"content\")\n \n\n@@ -11,7 +11,8 @@ module with the ``runserver`` argument::\n \"\"\"\n \n from __future__ import print_function\n-import sys, os, re, urlparse\n+import sys, os, re\n+from six.moves.urllib.parse import urlparse\n \n from twisted.internet import reactor, defer\n from twisted.web import server, static, util\n@@ -117,7 +118,7 @@ class CrawlerRun(object):\n         return \"http://localhost:%s%s\" % (self.portno, path)\n \n     def getpath(self, url):\n-        u = urlparse.urlparse(url)\n+        u = urlparse(url)\n         return u.path\n \n     def item_scraped(self, item, spider, response):\n\n@@ -1,4 +1,4 @@\n-from urlparse import urlparse\n+from six.moves.urllib.parse import urlparse\n from unittest import TestCase\n \n from scrapy.http import Request, Response\n\n@@ -1,7 +1,7 @@\n import cgi\n import unittest\n import xmlrpclib\n-from urlparse import urlparse\n+from six.moves.urllib.parse import urlparse\n \n from scrapy.http import Request, FormRequest, XmlRpcRequest, Headers, HtmlResponse\n \n\n@@ -5,7 +5,7 @@ from scrapy.spider import Spider\n from scrapy.contrib.spidermiddleware.offsite import OffsiteMiddleware\n from scrapy.utils.test import get_crawler\n \n-from urlparse import urlparse\n+from six.moves.urllib.parse import urlparse\n \n class TestOffsiteMiddleware(TestCase):\n \n\n@@ -1,4 +1,4 @@\n-from urlparse import urlparse\n+from six.moves.urllib.parse import urlparse\n import unittest\n \n \n\n@@ -1,5 +1,5 @@\n import unittest\n-import urlparse\n+from six.moves.urllib.parse import urlparse\n \n from scrapy.http import Request\n from scrapy.utils.httpobj import urlparse_cached\n@@ -13,7 +13,7 @@ class HttpobjUtilsTest(unittest.TestCase):\n         req1a = urlparse_cached(request1)\n         req1b = urlparse_cached(request1)\n         req2 = urlparse_cached(request2)\n-        urlp = urlparse.urlparse(url)\n+        urlp = urlparse(url)\n \n         assert req1a == req2\n         assert req1a == urlp\n\n@@ -1,6 +1,6 @@\n import os\n import unittest\n-import urlparse\n+from six.moves.urllib.parse import urlparse\n \n from scrapy.http import Response, TextResponse, HtmlResponse\n from scrapy.utils.response import response_httprepr, open_in_browser, get_meta_refresh\n@@ -24,7 +24,7 @@ class ResponseUtilsTest(unittest.TestCase):\n         url = \"http:///www.example.com/some/page.html\"\n         body = \"<html> <head> <title>test page</title> </head> <body>test body</body> </html>\"\n         def browser_open(burl):\n-            path = urlparse.urlparse(burl).path\n+            path = urlparse(burl).path\n             if not os.path.exists(path):\n                 path = burl.replace('file://', '')\n             bbody = open(path).read()\n\n@@ -3,7 +3,7 @@ from twisted.internet import defer\n Tests borrowed from the twisted.web.client tests.\n \"\"\"\n import os\n-from urlparse import urlparse\n+from six.moves.urllib.parse import urlparse\n \n from twisted.trial import unittest\n from twisted.web import server, static, error, util\n\n@@ -2,7 +2,7 @@\n \n import weakref\n \n-from urlparse import urlparse\n+from six.moves.urllib.parse import urlparse\n \n _urlparse_cache = weakref.WeakKeyDictionary()\n def urlparse_cached(request_or_response):\n\n@@ -6,7 +6,7 @@ scrapy.http.Request objects\n from __future__ import print_function\n import hashlib\n import weakref\n-from urlparse import urlunparse\n+from six.moves.urllib.parse import urlunparse\n \n from twisted.internet.defer import Deferred\n from w3lib.http import basic_auth_header\n\n@@ -1,5 +1,5 @@\n from __future__ import print_function\n-import urlparse\n+from six.moves.urllib.parse import urljoin\n \n from twisted.internet import reactor\n from twisted.web import server, resource, static, util\n@@ -14,7 +14,7 @@ class SiteTest(object):\n         self.site.stopListening()\n \n     def url(self, path):\n-        return urlparse.urljoin(self.baseurl, path)\n+        return urljoin(self.baseurl, path)\n \n def test_site():\n     r = resource.Resource()\n\n@@ -6,7 +6,7 @@ Some of the functions that used to be imported from this module have been moved\n to the w3lib.url module. Always import those from there instead.\n \"\"\"\n import posixpath\n-import urlparse\n+from six.moves.urllib.parse import ParseResult, urlunparse, urldefrag, urlparse\n import urllib\n import cgi\n \n@@ -59,7 +59,7 @@ def canonicalize_url(url, keep_blank_values=True, keep_fragments=False,\n     query = urllib.urlencode(keyvals)\n     path = safe_url_string(_unquotepath(path)) or '/'\n     fragment = '' if not keep_fragments else fragment\n-    return urlparse.urlunparse((scheme, netloc.lower(), path, params, query, fragment))\n+    return urlunparse((scheme, netloc.lower(), path, params, query, fragment))\n \n \n def _unquotepath(path):\n@@ -72,8 +72,8 @@ def parse_url(url, encoding=None):\n     \"\"\"Return urlparsed url from the given argument (which could be an already\n     parsed url)\n     \"\"\"\n-    return url if isinstance(url, urlparse.ParseResult) else \\\n-        urlparse.urlparse(unicode_to_str(url, encoding))\n+    return url if isinstance(url, ParseResult) else \\\n+        urlparse(unicode_to_str(url, encoding))\n \n \n def escape_ajax(url):\n@@ -99,7 +99,7 @@ def escape_ajax(url):\n     >>> escape_ajax(\"www.example.com/ajax.html\")\n     'www.example.com/ajax.html'\n     \"\"\"\n-    defrag, frag = urlparse.urldefrag(url)\n+    defrag, frag = urldefrag(url)\n     if not frag.startswith('!'):\n         return url\n     return add_or_replace_parameter(defrag, '_escaped_fragment_', frag[1:])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0d758e4b304d389b52546d82681b7f007ab7b53a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 3 | Files Changed: 3 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 15 | Churn Cumulative: 1431 | Contributors (this commit): 14 | Commits (past 90d): 5 | Contributors (cumulative): 21 | DMM Complexity: None\n\nDIFF:\n@@ -4,7 +4,7 @@ This module implements the XmlRpcRequest class which is a more convenient class\n \n See documentation in docs/topics/request-response.rst\n \"\"\"\n-import xmlrpclib\n+from six.moves import xmlrpc_client as xmlrpclib\n \n from scrapy.http.request import Request\n from scrapy.utils.python import get_func_args\n\n@@ -4,12 +4,21 @@ Mail sending helpers\n See documentation in docs/topics/email.rst\n \"\"\"\n from cStringIO import StringIO\n+import six\n+\n+from email.utils import COMMASPACE, formatdate\n+if six.PY2:\n     from email.MIMEMultipart import MIMEMultipart\n     from email.MIMENonMultipart import MIMENonMultipart\n     from email.MIMEBase import MIMEBase\n     from email.MIMEText import MIMEText\n-from email.Utils import COMMASPACE, formatdate\n     from email import Encoders\n+else:\n+    from email.mime.multipart import MIMEMultipart\n+    from email.mime.nonmultipart import MIMENonMultipart\n+    from email.mime.base import MIMEBase\n+    from email.mime.text import MIMEText\n+    from email import encoders as Encoders\n \n from twisted.internet import defer, reactor, ssl\n from twisted.mail.smtp import ESMTPSenderFactory\n\n@@ -1,6 +1,6 @@\n import cgi\n import unittest\n-import xmlrpclib\n+from six.moves import xmlrpc_client as xmlrpclib\n from six.moves.urllib.parse import urlparse\n \n from scrapy.http import Request, FormRequest, XmlRpcRequest, Headers, HtmlResponse\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#61717ca0754a892b1281bfd485c200ff89e2bfed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 6 | Files Changed: 1 | Hunks: 5 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 179 | Contributors (this commit): 6 | Commits (past 90d): 2 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -7,17 +7,14 @@ from cStringIO import StringIO\n import six\n \n from email.utils import COMMASPACE, formatdate\n+from six.moves.email_mime_multipart import MIMEMultipart\n+from six.moves.email_mime_text import MIMEText\n+from six.moves.email_mime_base import MIMEBase\n if six.PY2:\n-    from email.MIMEMultipart import MIMEMultipart\n     from email.MIMENonMultipart import MIMENonMultipart\n-    from email.MIMEBase import MIMEBase\n-    from email.MIMEText import MIMEText\n     from email import Encoders\n else:\n-    from email.mime.multipart import MIMEMultipart\n     from email.mime.nonmultipart import MIMENonMultipart\n-    from email.mime.base import MIMEBase\n-    from email.mime.text import MIMEText\n     from email import encoders as Encoders\n \n from twisted.internet import defer, reactor, ssl\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2999fc75b123a1c9ce6766c9580034bf58f2726f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 65 | Files Changed: 18 | Hunks: 63 | Methods Changed: 28 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 123 | Churn Cumulative: 5666 | Contributors (this commit): 24 | Commits (past 90d): 33 | Contributors (cumulative): 87 | DMM Complexity: None\n\nDIFF:\n@@ -9,7 +9,6 @@ import rfc822\n import time\n from six.moves.urllib.parse import urlparse\n from collections import defaultdict\n-from cStringIO import StringIO\n import six\n \n from twisted.internet import defer, threads\n@@ -257,7 +256,7 @@ class FilesPipeline(MediaPipeline):\n \n     def file_downloaded(self, response, request, info):\n         path = self.file_path(request, response=response, info=info)\n-        buf = StringIO(response.body)\n+        buf = six.BytesIO(response.body)\n         self.store.persist_file(path, buf, info)\n         checksum = md5sum(buf)\n         return checksum\n\n@@ -5,7 +5,6 @@ See documentation in topics/images.rst\n \"\"\"\n \n import hashlib\n-from cStringIO import StringIO\n import six\n \n from PIL import Image\n@@ -70,7 +69,7 @@ class ImagesPipeline(FilesPipeline):\n \n     def get_images(self, response, request, info):\n         path = self.file_path(request, response=response, info=info)\n-        orig_image = Image.open(StringIO(response.body))\n+        orig_image = Image.open(six.BytesIO(response.body))\n \n         width, height = orig_image.size\n         if width < self.MIN_WIDTH or height < self.MIN_HEIGHT:\n@@ -97,7 +96,7 @@ class ImagesPipeline(FilesPipeline):\n             image = image.copy()\n             image.thumbnail(size, Image.ANTIALIAS)\n \n-        buf = StringIO()\n+        buf = six.BytesIO()\n         image.save(buf, 'JPEG')\n         return image, buf\n \n\n@@ -6,7 +6,6 @@ import bz2\n import gzip\n import zipfile\n import tarfile\n-from cStringIO import StringIO\n from tempfile import mktemp\n import six\n \n@@ -27,7 +26,7 @@ class DecompressionMiddleware(object):\n         }\n \n     def _is_tar(self, response):\n-        archive = StringIO(response.body)\n+        archive = six.BytesIO(response.body)\n         try:\n             tar_file = tarfile.open(name=mktemp(), fileobj=archive)\n         except tarfile.ReadError:\n@@ -38,7 +37,7 @@ class DecompressionMiddleware(object):\n         return response.replace(body=body, cls=respcls)\n \n     def _is_zip(self, response):\n-        archive = StringIO(response.body)\n+        archive = six.BytesIO(response.body)\n         try:\n             zip_file = zipfile.ZipFile(archive)\n         except zipfile.BadZipfile:\n@@ -50,7 +49,7 @@ class DecompressionMiddleware(object):\n         return response.replace(body=body, cls=respcls)\n \n     def _is_gzip(self, response):\n-        archive = StringIO(response.body)\n+        archive = six.BytesIO(response.body)\n         try:\n             body = gzip.GzipFile(fileobj=archive).read()\n         except IOError:\n\n@@ -29,8 +29,8 @@ In case of status 200 request, response.headers will come with two keys:\n \"\"\"\n \n import re\n+import six\n from six.moves.urllib.parse import urlparse\n-from cStringIO import StringIO\n \n from twisted.internet import reactor\n from twisted.protocols.ftp import FTPClient, CommandFailed\n@@ -42,7 +42,7 @@ from scrapy.responsetypes import responsetypes\n class ReceivedDataProtocol(Protocol):\n     def __init__(self, filename=None):\n         self.__filename = filename\n-        self.body = open(filename, \"w\") if filename else StringIO()\n+        self.body = open(filename, \"w\") if filename else six.BytesIO()\n         self.size = 0\n \n     def dataReceived(self, data):\n\n@@ -1,9 +1,9 @@\n \"\"\"Download handlers for http and https schemes\"\"\"\n \n import re\n+import six\n \n from time import time\n-from cStringIO import StringIO\n from six.moves.urllib.parse import urldefrag\n \n from zope.interface import implements\n@@ -234,7 +234,7 @@ class _ResponseReader(protocol.Protocol):\n         self._finished = finished\n         self._txresponse = txresponse\n         self._request = request\n-        self._bodybuf = StringIO()\n+        self._bodybuf = six.BytesIO()\n \n     def dataReceived(self, bodyBytes):\n         self._bodybuf.write(bodyBytes)\n\n@@ -3,7 +3,7 @@ Mail sending helpers\n \n See documentation in docs/topics/email.rst\n \"\"\"\n-from cStringIO import StringIO\n+from six.moves import cStringIO as StringIO\n import six\n \n from email.utils import COMMASPACE, formatdate\n\n@@ -6,7 +6,6 @@ based on different criteria.\n \n from mimetypes import MimeTypes\n from pkgutil import get_data\n-from cStringIO import StringIO\n import six\n \n from scrapy.http import Response\n@@ -34,7 +33,7 @@ class ResponseTypes(object):\n         self.classes = {}\n         self.mimetypes = MimeTypes()\n         mimedata = get_data('scrapy', 'mime.types')\n-        self.mimetypes.readfp(StringIO(mimedata))\n+        self.mimetypes.readfp(six.BytesIO(mimedata))\n         for mimetype, cls in six.iteritems(self.CLASSES):\n             self.classes[mimetype] = load_object(cls)\n \n\n@@ -1,6 +1,6 @@\n import unittest, json\n+import six\n from six.moves import cPickle as pickle\n-from cStringIO import StringIO\n import lxml.etree\n import re\n \n@@ -19,7 +19,7 @@ class BaseItemExporterTest(unittest.TestCase):\n \n     def setUp(self):\n         self.i = TestItem(name=u'John\\xa3', age='22')\n-        self.output = StringIO()\n+        self.output = six.BytesIO()\n         self.ie = self._get_exporter()\n \n     def _get_exporter(self, **kwargs):\n@@ -126,7 +126,7 @@ class PickleItemExporterTest(BaseItemExporterTest):\n     def test_export_multiple_items(self):\n         i1 = TestItem(name='hello', age='world')\n         i2 = TestItem(name='bye', age='world')\n-        f = StringIO()\n+        f = six.BytesIO()\n         ie = PickleItemExporter(f)\n         ie.start_exporting()\n         ie.export_item(i1)\n@@ -151,21 +151,21 @@ class CsvItemExporterTest(BaseItemExporterTest):\n         self.assertCsvEqual(self.output.getvalue(), 'age,name\\r\\n22,John\\xc2\\xa3\\r\\n')\n \n     def test_header(self):\n-        output = StringIO()\n+        output = six.BytesIO()\n         ie = CsvItemExporter(output, fields_to_export=self.i.fields.keys())\n         ie.start_exporting()\n         ie.export_item(self.i)\n         ie.finish_exporting()\n         self.assertCsvEqual(output.getvalue(), 'age,name\\r\\n22,John\\xc2\\xa3\\r\\n')\n \n-        output = StringIO()\n+        output = six.BytesIO()\n         ie = CsvItemExporter(output, fields_to_export=['age'])\n         ie.start_exporting()\n         ie.export_item(self.i)\n         ie.finish_exporting()\n         self.assertCsvEqual(output.getvalue(), 'age\\r\\n22\\r\\n')\n \n-        output = StringIO()\n+        output = six.BytesIO()\n         ie = CsvItemExporter(output)\n         ie.start_exporting()\n         ie.export_item(self.i)\n@@ -173,7 +173,7 @@ class CsvItemExporterTest(BaseItemExporterTest):\n         ie.finish_exporting()\n         self.assertCsvEqual(output.getvalue(), 'age,name\\r\\n22,John\\xc2\\xa3\\r\\n22,John\\xc2\\xa3\\r\\n')\n \n-        output = StringIO()\n+        output = six.BytesIO()\n         ie = CsvItemExporter(output, include_headers_line=False)\n         ie.start_exporting()\n         ie.export_item(self.i)\n@@ -186,7 +186,7 @@ class CsvItemExporterTest(BaseItemExporterTest):\n             friends = Field()\n \n         i = TestItem2(name='John', friends=['Mary', 'Paul'])\n-        output = StringIO()\n+        output = six.BytesIO()\n         ie = CsvItemExporter(output, include_headers_line=False)\n         ie.start_exporting()\n         ie.export_item(i)\n@@ -216,7 +216,7 @@ class XmlItemExporterTest(BaseItemExporterTest):\n         self.assertXmlEquivalent(self.output.getvalue(), expected_value)\n \n     def test_multivalued_fields(self):\n-        output = StringIO()\n+        output = six.BytesIO()\n         item = TestItem(name=[u'John\\xa3', u'Doe'])\n         ie = XmlItemExporter(output)\n         ie.start_exporting()\n@@ -226,7 +226,7 @@ class XmlItemExporterTest(BaseItemExporterTest):\n         self.assertXmlEquivalent(output.getvalue(), expected_value)\n \n     def test_nested_item(self):\n-        output = StringIO()\n+        output = six.BytesIO()\n         i1 = TestItem(name=u'foo\\xa3hoo', age='22')\n         i2 = TestItem(name=u'bar', age=i1)\n         i3 = TestItem(name=u'buz', age=i2)\n@@ -248,7 +248,7 @@ class XmlItemExporterTest(BaseItemExporterTest):\n         self.assertXmlEquivalent(output.getvalue(), expected_value)\n \n     def test_nested_list_item(self):\n-        output = StringIO()\n+        output = six.BytesIO()\n         i1 = TestItem(name=u'foo')\n         i2 = TestItem(name=u'bar')\n         i3 = TestItem(name=u'buz', age=[i1, i2])\n\n@@ -1,6 +1,6 @@\n import os\n+import six\n from six.moves.urllib.parse import urlparse\n-from cStringIO import StringIO\n \n from zope.interface.verify import verifyObject\n from twisted.trial import unittest\n@@ -62,13 +62,13 @@ class FTPFeedStorageTest(unittest.TestCase):\n     def _assert_stores(self, storage, path):\n         spider = Spider(\"default\")\n         file = storage.open(spider)\n-        file.write(\"content\")\n+        file.write(b\"content\")\n         yield storage.store(file)\n         self.failUnless(os.path.exists(path))\n-        self.failUnlessEqual(open(path).read(), \"content\")\n+        self.failUnlessEqual(open(path).read(), b\"content\")\n         # again, to check s3 objects are overwritten\n-        yield storage.store(StringIO(\"new content\"))\n-        self.failUnlessEqual(open(path).read(), \"new content\")\n+        yield storage.store(six.BytesIO(b\"new content\"))\n+        self.failUnlessEqual(open(path).read(), b\"new content\")\n \n \n class S3FeedStorageTest(unittest.TestCase):\n@@ -93,9 +93,9 @@ class StdoutFeedStorageTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_store(self):\n-        out = StringIO()\n+        out = six.BytesIO()\n         storage = StdoutFeedStorage('stdout:', _stdout=out)\n         file = storage.open(Spider(\"default\"))\n-        file.write(\"content\")\n+        file.write(b\"content\")\n         yield storage.store(file)\n-        self.assertEqual(out.getvalue(), \"content\")\n+        self.assertEqual(out.getvalue(), b\"content\")\n\n@@ -1,6 +1,6 @@\n+import six\n from unittest import TestCase\n from os.path import join, abspath, dirname\n-from cStringIO import StringIO\n from gzip import GzipFile\n \n from scrapy.spider import Spider\n@@ -104,8 +104,8 @@ class HttpCompressionTest(TestCase):\n             'Content-Type': 'text/html',\n             'Content-Encoding': 'gzip',\n         }\n-        f = StringIO()\n-        plainbody = \"\"\"<html><head><title>Some page</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\">\"\"\"\n+        f = six.BytesIO()\n+        plainbody = b\"\"\"<html><head><title>Some page</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\">\"\"\"\n         zf = GzipFile(fileobj=f, mode='wb')\n         zf.write(plainbody)\n         zf.close()\n@@ -122,8 +122,8 @@ class HttpCompressionTest(TestCase):\n             'Content-Type': 'text/html',\n             'Content-Encoding': 'gzip',\n         }\n-        f = StringIO()\n-        plainbody = \"\"\"<html><head><title>Some page</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\">\"\"\"\n+        f = six.BytesIO()\n+        plainbody = b\"\"\"<html><head><title>Some page</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\">\"\"\"\n         zf = GzipFile(fileobj=f, mode='wb')\n         zf.write(plainbody)\n         zf.close()\n\n@@ -1,4 +1,4 @@\n-from cStringIO import StringIO\n+import six\n \n from twisted.python import log as txlog, failure\n from twisted.trial import unittest\n@@ -21,7 +21,7 @@ class ScrapyFileLogObserverTest(unittest.TestCase):\n     encoding = 'utf-8'\n \n     def setUp(self):\n-        self.f = StringIO()\n+        self.f = six.BytesIO()\n         self.log_observer = log.ScrapyFileLogObserver(self.f, self.level, self.encoding)\n         self.log_observer.start()\n \n\n@@ -1,6 +1,6 @@\n import unittest\n \n-from cStringIO import StringIO\n+import six\n from scrapy.mail import MailSender\n \n class MailSenderTest(unittest.TestCase):\n@@ -30,8 +30,8 @@ class MailSenderTest(unittest.TestCase):\n         self.assertEqual(msg.get('Content-Type'), 'text/html')\n \n     def test_send_attach(self):\n-        attach = StringIO()\n-        attach.write('content')\n+        attach = six.BytesIO()\n+        attach.write(b'content')\n         attach.seek(0)\n         attachs = [('attachment', 'text/plain', attach)]\n \n\n@@ -1,9 +1,9 @@\n import os\n import hashlib\n import warnings\n-from cStringIO import StringIO\n from tempfile import mkdtemp\n from shutil import rmtree\n+import six\n \n from twisted.trial import unittest\n \n@@ -201,7 +201,7 @@ class ImagesPipelineTestCaseFields(unittest.TestCase):\n \n \n def _create_image(format, *a, **kw):\n-    buf = StringIO()\n+    buf = six.BytesIO()\n     Image.new(*a, **kw).save(buf, format)\n     buf.seek(0)\n     return Image.open(buf)\n\n@@ -1,8 +1,8 @@\n import gzip\n import inspect\n import warnings\n-from cStringIO import StringIO\n from scrapy.utils.trackref import object_ref\n+import six\n \n from twisted.trial import unittest\n \n@@ -57,7 +57,7 @@ class XMLFeedSpiderTest(SpiderTest):\n     spider_class = XMLFeedSpider\n \n     def test_register_namespace(self):\n-        body = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+        body = b\"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n         <urlset xmlns:x=\"http://www.google.com/schemas/sitemap/0.84\"\n                 xmlns:y=\"http://www.example.com/schemas/extras/1.0\">\n         <url><x:loc>http://www.example.com/Special-Offers.html</loc><y:updated>2009-08-16</updated><other value=\"bar\" y:custom=\"fuu\"/></url>\n@@ -103,7 +103,7 @@ class CSVFeedSpiderTest(SpiderTest):\n \n class CrawlSpiderTest(SpiderTest):\n \n-    test_body = \"\"\"<html><head><title>Page title<title>\n+    test_body = b\"\"\"<html><head><title>Page title<title>\n     <body>\n     <p><a href=\"item/12.html\">Item 12</a></p>\n     <div class='links'>\n@@ -195,8 +195,8 @@ class SitemapSpiderTest(SpiderTest):\n \n     spider_class = SitemapSpider\n \n-    BODY = \"SITEMAP\"\n-    f = StringIO()\n+    BODY = b\"SITEMAP\"\n+    f = six.BytesIO()\n     g = gzip.GzipFile(fileobj=f, mode='w+b')\n     g.write(BODY)\n     g.close()\n\n@@ -1,5 +1,4 @@\n-import unittest, json\n-from cStringIO import StringIO\n+import unittest, json, six\n \n from scrapy.utils.jsonrpc import jsonrpc_client_call, jsonrpc_server_call, \\\n     JsonRpcError, jsonrpc_errors\n@@ -19,7 +18,7 @@ class urllib_mock(object):\n     def urlopen(self, url, request):\n         self.url = url\n         self.request = request\n-        return StringIO(self.response)\n+        return six.BytesIO(self.response)\n \n class TestTarget(object):\n \n\n@@ -1,7 +1,6 @@\n import sys\n import os\n import unittest\n-from cStringIO import StringIO\n \n from scrapy.item import Item, Field\n from scrapy.utils.misc import load_object, arg_to_iter, walk_modules\n\n@@ -1,5 +1,5 @@\n import struct\n-from cStringIO import StringIO\n+import six\n from gzip import GzipFile\n \n def gunzip(data):\n@@ -7,9 +7,9 @@ def gunzip(data):\n \n     This is resilient to CRC checksum errors.\n     \"\"\"\n-    f = GzipFile(fileobj=StringIO(data))\n-    output = ''\n-    chunk = '.'\n+    f = GzipFile(fileobj=six.BytesIO(data))\n+    output = b''\n+    chunk = b'.'\n     while chunk:\n         try:\n             chunk = f.read(8196)\n\n@@ -1,5 +1,4 @@\n-import re, csv\n-from cStringIO import StringIO\n+import re, csv, six\n \n from scrapy.http import TextResponse, Response\n from scrapy.selector import Selector\n@@ -48,7 +47,7 @@ def csviter(obj, delimiter=None, headers=None, encoding=None):\n     def _getrow(csv_r):\n         return [str_to_unicode(field, encoding) for field in next(csv_r)]\n \n-    lines = StringIO(_body_or_str(obj, unicode=False))\n+    lines = six.BytesIO(_body_or_str(obj, unicode=False))\n     if delimiter:\n         csv_r = csv.reader(lines, delimiter=delimiter)\n     else:\n@@ -68,7 +67,7 @@ def csviter(obj, delimiter=None, headers=None, encoding=None):\n \n \n def _body_or_str(obj, unicode=True):\n-    assert isinstance(obj, (Response, basestring)), \\\n+    assert isinstance(obj, (Response, six.string_types)), \\\n         \"obj must be Response or basestring, not %s\" % type(obj).__name__\n     if isinstance(obj, Response):\n         if not unicode:\n@@ -77,7 +76,7 @@ def _body_or_str(obj, unicode=True):\n             return obj.body_as_unicode()\n         else:\n             return obj.body.decode('utf-8')\n-    elif type(obj) is type(u''):\n+    elif type(obj) is six.text_type:\n         return obj if unicode else obj.encode('utf-8')\n     else:\n         return obj.decode('utf-8') if unicode else obj\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ef8872a518af65714deb2f8767a8dfa55d1558cb", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 926 | Contributors (this commit): 7 | Commits (past 90d): 6 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -54,7 +54,7 @@ class ReceivedDataProtocol(Protocol):\n         return self.__filename\n \n     def close(self):\n-        self.body.close() if self.filename else self.body.reset()\n+        self.body.close() if self.filename else self.body.seek(0)\n \n _CODE_RE = re.compile(\"\\d+\")\n class FTPDownloadHandler(object):\n\n@@ -132,7 +132,7 @@ class PickleItemExporterTest(BaseItemExporterTest):\n         ie.export_item(i1)\n         ie.export_item(i2)\n         ie.finish_exporting()\n-        f.reset()\n+        f.seek(0)\n         self.assertEqual(pickle.load(f), i1)\n         self.assertEqual(pickle.load(f), i2)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0786e84a33155ebc8d8d3502e3a7f3060b86a4ec", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 69 | Lines Deleted: 41 | Files Changed: 16 | Hunks: 49 | Methods Changed: 26 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 110 | Churn Cumulative: 5517 | Contributors (this commit): 23 | Commits (past 90d): 47 | Contributors (cumulative): 78 | DMM Complexity: None\n\nDIFF:\n@@ -11,6 +11,11 @@ from six.moves.urllib.parse import urlparse\n from collections import defaultdict\n import six\n \n+try:\n+    from cStringIO import StringIO as BytesIO\n+except ImportError:\n+    from io import BytesIO\n+\n from twisted.internet import defer, threads\n \n from scrapy import log\n@@ -256,7 +261,7 @@ class FilesPipeline(MediaPipeline):\n \n     def file_downloaded(self, response, request, info):\n         path = self.file_path(request, response=response, info=info)\n-        buf = six.BytesIO(response.body)\n+        buf = BytesIO(response.body)\n         self.store.persist_file(path, buf, info)\n         checksum = md5sum(buf)\n         return checksum\n\n@@ -7,6 +7,11 @@ See documentation in topics/images.rst\n import hashlib\n import six\n \n+try:\n+    from cStringIO import StringIO as BytesIO\n+except ImportError:\n+    from io import BytesIO\n+\n from PIL import Image\n \n from scrapy.utils.misc import md5sum\n@@ -69,7 +74,7 @@ class ImagesPipeline(FilesPipeline):\n \n     def get_images(self, response, request, info):\n         path = self.file_path(request, response=response, info=info)\n-        orig_image = Image.open(six.BytesIO(response.body))\n+        orig_image = Image.open(BytesIO(response.body))\n \n         width, height = orig_image.size\n         if width < self.MIN_WIDTH or height < self.MIN_HEIGHT:\n@@ -96,7 +101,7 @@ class ImagesPipeline(FilesPipeline):\n             image = image.copy()\n             image.thumbnail(size, Image.ANTIALIAS)\n \n-        buf = six.BytesIO()\n+        buf = BytesIO()\n         image.save(buf, 'JPEG')\n         return image, buf\n \n\n@@ -7,8 +7,14 @@ import gzip\n import zipfile\n import tarfile\n from tempfile import mktemp\n+\n import six\n \n+try:\n+    from cStringIO import StringIO as BytesIO\n+except ImportError:\n+    from io import BytesIO\n+\n from scrapy import log\n from scrapy.responsetypes import responsetypes\n \n@@ -26,7 +32,7 @@ class DecompressionMiddleware(object):\n         }\n \n     def _is_tar(self, response):\n-        archive = six.BytesIO(response.body)\n+        archive = BytesIO(response.body)\n         try:\n             tar_file = tarfile.open(name=mktemp(), fileobj=archive)\n         except tarfile.ReadError:\n@@ -37,7 +43,7 @@ class DecompressionMiddleware(object):\n         return response.replace(body=body, cls=respcls)\n \n     def _is_zip(self, response):\n-        archive = six.BytesIO(response.body)\n+        archive = BytesIO(response.body)\n         try:\n             zip_file = zipfile.ZipFile(archive)\n         except zipfile.BadZipfile:\n@@ -49,7 +55,7 @@ class DecompressionMiddleware(object):\n         return response.replace(body=body, cls=respcls)\n \n     def _is_gzip(self, response):\n-        archive = six.BytesIO(response.body)\n+        archive = BytesIO(response.body)\n         try:\n             body = gzip.GzipFile(fileobj=archive).read()\n         except IOError:\n\n@@ -29,7 +29,7 @@ In case of status 200 request, response.headers will come with two keys:\n \"\"\"\n \n import re\n-import six\n+from io import BytesIO\n from six.moves.urllib.parse import urlparse\n \n from twisted.internet import reactor\n@@ -42,7 +42,7 @@ from scrapy.responsetypes import responsetypes\n class ReceivedDataProtocol(Protocol):\n     def __init__(self, filename=None):\n         self.__filename = filename\n-        self.body = open(filename, \"w\") if filename else six.BytesIO()\n+        self.body = open(filename, \"w\") if filename else BytesIO()\n         self.size = 0\n \n     def dataReceived(self, data):\n\n@@ -1,8 +1,8 @@\n \"\"\"Download handlers for http and https schemes\"\"\"\n \n import re\n-import six\n \n+from io import BytesIO\n from time import time\n from six.moves.urllib.parse import urldefrag\n \n@@ -234,7 +234,7 @@ class _ResponseReader(protocol.Protocol):\n         self._finished = finished\n         self._txresponse = txresponse\n         self._request = request\n-        self._bodybuf = six.BytesIO()\n+        self._bodybuf = BytesIO()\n \n     def dataReceived(self, bodyBytes):\n         self._bodybuf.write(bodyBytes)\n\n@@ -6,6 +6,7 @@ based on different criteria.\n \n from mimetypes import MimeTypes\n from pkgutil import get_data\n+from io import BytesIO\n import six\n \n from scrapy.http import Response\n@@ -33,7 +34,7 @@ class ResponseTypes(object):\n         self.classes = {}\n         self.mimetypes = MimeTypes()\n         mimedata = get_data('scrapy', 'mime.types')\n-        self.mimetypes.readfp(six.BytesIO(mimedata))\n+        self.mimetypes.readfp(BytesIO(mimedata))\n         for mimetype, cls in six.iteritems(self.CLASSES):\n             self.classes[mimetype] = load_object(cls)\n \n\n@@ -1,5 +1,5 @@\n import unittest, json\n-import six\n+from io import BytesIO\n from six.moves import cPickle as pickle\n import lxml.etree\n import re\n@@ -19,7 +19,7 @@ class BaseItemExporterTest(unittest.TestCase):\n \n     def setUp(self):\n         self.i = TestItem(name=u'John\\xa3', age='22')\n-        self.output = six.BytesIO()\n+        self.output = BytesIO()\n         self.ie = self._get_exporter()\n \n     def _get_exporter(self, **kwargs):\n@@ -126,7 +126,7 @@ class PickleItemExporterTest(BaseItemExporterTest):\n     def test_export_multiple_items(self):\n         i1 = TestItem(name='hello', age='world')\n         i2 = TestItem(name='bye', age='world')\n-        f = six.BytesIO()\n+        f = BytesIO()\n         ie = PickleItemExporter(f)\n         ie.start_exporting()\n         ie.export_item(i1)\n@@ -151,21 +151,21 @@ class CsvItemExporterTest(BaseItemExporterTest):\n         self.assertCsvEqual(self.output.getvalue(), 'age,name\\r\\n22,John\\xc2\\xa3\\r\\n')\n \n     def test_header(self):\n-        output = six.BytesIO()\n+        output = BytesIO()\n         ie = CsvItemExporter(output, fields_to_export=self.i.fields.keys())\n         ie.start_exporting()\n         ie.export_item(self.i)\n         ie.finish_exporting()\n         self.assertCsvEqual(output.getvalue(), 'age,name\\r\\n22,John\\xc2\\xa3\\r\\n')\n \n-        output = six.BytesIO()\n+        output = BytesIO()\n         ie = CsvItemExporter(output, fields_to_export=['age'])\n         ie.start_exporting()\n         ie.export_item(self.i)\n         ie.finish_exporting()\n         self.assertCsvEqual(output.getvalue(), 'age\\r\\n22\\r\\n')\n \n-        output = six.BytesIO()\n+        output = BytesIO()\n         ie = CsvItemExporter(output)\n         ie.start_exporting()\n         ie.export_item(self.i)\n@@ -173,7 +173,7 @@ class CsvItemExporterTest(BaseItemExporterTest):\n         ie.finish_exporting()\n         self.assertCsvEqual(output.getvalue(), 'age,name\\r\\n22,John\\xc2\\xa3\\r\\n22,John\\xc2\\xa3\\r\\n')\n \n-        output = six.BytesIO()\n+        output = BytesIO()\n         ie = CsvItemExporter(output, include_headers_line=False)\n         ie.start_exporting()\n         ie.export_item(self.i)\n@@ -186,7 +186,7 @@ class CsvItemExporterTest(BaseItemExporterTest):\n             friends = Field()\n \n         i = TestItem2(name='John', friends=['Mary', 'Paul'])\n-        output = six.BytesIO()\n+        output = BytesIO()\n         ie = CsvItemExporter(output, include_headers_line=False)\n         ie.start_exporting()\n         ie.export_item(i)\n@@ -216,7 +216,7 @@ class XmlItemExporterTest(BaseItemExporterTest):\n         self.assertXmlEquivalent(self.output.getvalue(), expected_value)\n \n     def test_multivalued_fields(self):\n-        output = six.BytesIO()\n+        output = BytesIO()\n         item = TestItem(name=[u'John\\xa3', u'Doe'])\n         ie = XmlItemExporter(output)\n         ie.start_exporting()\n@@ -226,7 +226,7 @@ class XmlItemExporterTest(BaseItemExporterTest):\n         self.assertXmlEquivalent(output.getvalue(), expected_value)\n \n     def test_nested_item(self):\n-        output = six.BytesIO()\n+        output = BytesIO()\n         i1 = TestItem(name=u'foo\\xa3hoo', age='22')\n         i2 = TestItem(name=u'bar', age=i1)\n         i3 = TestItem(name=u'buz', age=i2)\n@@ -248,7 +248,7 @@ class XmlItemExporterTest(BaseItemExporterTest):\n         self.assertXmlEquivalent(output.getvalue(), expected_value)\n \n     def test_nested_list_item(self):\n-        output = six.BytesIO()\n+        output = BytesIO()\n         i1 = TestItem(name=u'foo')\n         i2 = TestItem(name=u'bar')\n         i3 = TestItem(name=u'buz', age=[i1, i2])\n\n@@ -1,5 +1,5 @@\n import os\n-import six\n+from io import BytesIO\n from six.moves.urllib.parse import urlparse\n \n from zope.interface.verify import verifyObject\n@@ -67,7 +67,7 @@ class FTPFeedStorageTest(unittest.TestCase):\n         self.failUnless(os.path.exists(path))\n         self.failUnlessEqual(open(path).read(), b\"content\")\n         # again, to check s3 objects are overwritten\n-        yield storage.store(six.BytesIO(b\"new content\"))\n+        yield storage.store(BytesIO(b\"new content\"))\n         self.failUnlessEqual(open(path).read(), b\"new content\")\n \n \n@@ -93,7 +93,7 @@ class StdoutFeedStorageTest(unittest.TestCase):\n \n     @defer.inlineCallbacks\n     def test_store(self):\n-        out = six.BytesIO()\n+        out = BytesIO()\n         storage = StdoutFeedStorage('stdout:', _stdout=out)\n         file = storage.open(Spider(\"default\"))\n         file.write(b\"content\")\n\n@@ -1,4 +1,4 @@\n-import six\n+from io import BytesIO\n from unittest import TestCase\n from os.path import join, abspath, dirname\n from gzip import GzipFile\n@@ -104,7 +104,7 @@ class HttpCompressionTest(TestCase):\n             'Content-Type': 'text/html',\n             'Content-Encoding': 'gzip',\n         }\n-        f = six.BytesIO()\n+        f = BytesIO()\n         plainbody = b\"\"\"<html><head><title>Some page</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\">\"\"\"\n         zf = GzipFile(fileobj=f, mode='wb')\n         zf.write(plainbody)\n@@ -122,7 +122,7 @@ class HttpCompressionTest(TestCase):\n             'Content-Type': 'text/html',\n             'Content-Encoding': 'gzip',\n         }\n-        f = six.BytesIO()\n+        f = BytesIO()\n         plainbody = b\"\"\"<html><head><title>Some page</title><meta http-equiv=\"Content-Type\" content=\"text/html; charset=gb2312\">\"\"\"\n         zf = GzipFile(fileobj=f, mode='wb')\n         zf.write(plainbody)\n\n@@ -1,4 +1,4 @@\n-import six\n+from io import BytesIO\n \n from twisted.python import log as txlog, failure\n from twisted.trial import unittest\n@@ -21,7 +21,7 @@ class ScrapyFileLogObserverTest(unittest.TestCase):\n     encoding = 'utf-8'\n \n     def setUp(self):\n-        self.f = six.BytesIO()\n+        self.f = BytesIO()\n         self.log_observer = log.ScrapyFileLogObserver(self.f, self.level, self.encoding)\n         self.log_observer.start()\n \n\n@@ -1,6 +1,6 @@\n import unittest\n+from io import BytesIO\n \n-import six\n from scrapy.mail import MailSender\n \n class MailSenderTest(unittest.TestCase):\n@@ -30,7 +30,7 @@ class MailSenderTest(unittest.TestCase):\n         self.assertEqual(msg.get('Content-Type'), 'text/html')\n \n     def test_send_attach(self):\n-        attach = six.BytesIO()\n+        attach = BytesIO()\n         attach.write(b'content')\n         attach.seek(0)\n         attachs = [('attachment', 'text/plain', attach)]\n\n@@ -3,7 +3,7 @@ import hashlib\n import warnings\n from tempfile import mkdtemp\n from shutil import rmtree\n-import six\n+from io import BytesIO\n \n from twisted.trial import unittest\n \n@@ -201,7 +201,7 @@ class ImagesPipelineTestCaseFields(unittest.TestCase):\n \n \n def _create_image(format, *a, **kw):\n-    buf = six.BytesIO()\n+    buf = BytesIO()\n     Image.new(*a, **kw).save(buf, format)\n     buf.seek(0)\n     return Image.open(buf)\n\n@@ -2,7 +2,7 @@ import gzip\n import inspect\n import warnings\n from scrapy.utils.trackref import object_ref\n-import six\n+from io import BytesIO\n \n from twisted.trial import unittest\n \n@@ -196,7 +196,7 @@ class SitemapSpiderTest(SpiderTest):\n     spider_class = SitemapSpider\n \n     BODY = b\"SITEMAP\"\n-    f = six.BytesIO()\n+    f = BytesIO()\n     g = gzip.GzipFile(fileobj=f, mode='w+b')\n     g.write(BODY)\n     g.close()\n\n@@ -1,4 +1,5 @@\n-import unittest, json, six\n+import unittest, json\n+from io import BytesIO\n \n from scrapy.utils.jsonrpc import jsonrpc_client_call, jsonrpc_server_call, \\\n     JsonRpcError, jsonrpc_errors\n@@ -18,7 +19,7 @@ class urllib_mock(object):\n     def urlopen(self, url, request):\n         self.url = url\n         self.request = request\n-        return six.BytesIO(self.response)\n+        return BytesIO(self.response)\n \n class TestTarget(object):\n \n\n@@ -1,5 +1,10 @@\n import struct\n-import six\n+\n+try:\n+    from cStringIO import StringIO as BytesIO\n+except ImportError:\n+    from io import BytesIO\n+\n from gzip import GzipFile\n \n def gunzip(data):\n@@ -7,7 +12,7 @@ def gunzip(data):\n \n     This is resilient to CRC checksum errors.\n     \"\"\"\n-    f = GzipFile(fileobj=six.BytesIO(data))\n+    f = GzipFile(fileobj=BytesIO(data))\n     output = b''\n     chunk = b'.'\n     while chunk:\n\n@@ -1,5 +1,10 @@\n import re, csv, six\n \n+try:\n+    from cStringIO import StringIO as BytesIO\n+except ImportError:\n+    from io import BytesIO\n+\n from scrapy.http import TextResponse, Response\n from scrapy.selector import Selector\n from scrapy import log\n@@ -47,7 +52,7 @@ def csviter(obj, delimiter=None, headers=None, encoding=None):\n     def _getrow(csv_r):\n         return [str_to_unicode(field, encoding) for field in next(csv_r)]\n \n-    lines = six.BytesIO(_body_or_str(obj, unicode=False))\n+    lines = BytesIO(_body_or_str(obj, unicode=False))\n     if delimiter:\n         csv_r = csv.reader(lines, delimiter=delimiter)\n     else:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1cd9c4d6531f0d0d2b322f193c5c554c615fce68", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 7 | Churn Cumulative: 500 | Contributors (this commit): 10 | Commits (past 90d): 6 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -1,9 +1,8 @@\n import os\n import hashlib\n import warnings\n-from tempfile import mkdtemp\n+from tempfile import mkdtemp, TemporaryFile\n from shutil import rmtree\n-from io import BytesIO\n \n from twisted.trial import unittest\n \n@@ -201,7 +200,7 @@ class ImagesPipelineTestCaseFields(unittest.TestCase):\n \n \n def _create_image(format, *a, **kw):\n-    buf = BytesIO()\n+    buf = TemporaryFile()\n     Image.new(*a, **kw).save(buf, format)\n     buf.seek(0)\n     return Image.open(buf)\n\n@@ -81,7 +81,7 @@ def _body_or_str(obj, unicode=True):\n             return obj.body_as_unicode()\n         else:\n             return obj.body.decode('utf-8')\n-    elif type(obj) is six.text_type:\n+    elif isinstance(obj, six.text_type):\n         return obj if unicode else obj.encode('utf-8')\n     else:\n         return obj.decode('utf-8') if unicode else obj\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
