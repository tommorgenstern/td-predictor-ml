{"custom_id": "keras#d155a8525574db1c0bf3450cb95bec6655a1b6af", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1400 | Lines Deleted: 21 | Files Changed: 1 | Hunks: 128 | Methods Changed: 125 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1421 | Churn Cumulative: 3132 | Contributors (this commit): 9 | Commits (past 90d): 25 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -203,10 +203,10 @@ def shape_equal(shape1, shape2, axis=None, allow_none=True):\n     Args:\n         shape1: A tuple or list of integers.\n         shape2: A tuple or list of integers.\n-        axis: int or list/tuple of ints, defaults to None. If specified, the\n+        axis: int or list/tuple of ints, defaults to `None`. If specified, the\n             shape check will ignore the axes specified by `axis`.\n-        allow_none: bool, defaults to True. If True, None in the shape will\n-            match any value.\n+        allow_none: bool, defaults to `True`. If `True`, `None` in the shape\n+            will match any value.\n     \"\"\"\n     if len(shape1) != len(shape2):\n         return False\n@@ -244,7 +244,7 @@ def absolute(x):\n         x: Input tensor.\n \n     Returns:\n-        An array containing the absolute value of each element in x.\n+        An array containing the absolute value of each element in `x`.\n \n     Example:\n \n@@ -405,7 +405,7 @@ class Any(Operation):\n \n @keras_core_export([\"keras_core.ops.any\", \"keras_core.ops.numpy.any\"])\n def any(x, axis=None, keepdims=False):\n-    \"\"\"Test whether any array element along a given axis evaluates to True.\n+    \"\"\"Test whether any array element along a given axis evaluates to `True`.\n \n     Args:\n         x: Input tensor.\n@@ -472,7 +472,7 @@ def amax(x, axis=None, keepdims=False):\n         axis: Axis along which to compute the maximum.\n             By default (`axis=None`), find the maximum value in all the\n             dimensions of the input array.\n-        keep_dims: If `True`, axes which are reduced are left in the result as\n+        keepdims: If `True`, axes which are reduced are left in the result as\n             dimensions that are broadcast to the size of the original\n             input tensor. Defaults to `False`.\n \n@@ -528,7 +528,7 @@ def amin(x, axis=None, keepdims=False):\n         axis: Axis along which to compute the minimum.\n             By default (`axis=None`), find the minimum value in all the\n             dimensions of the input array.\n-        keep_dims: If `True`, axes which are reduced are left in the result as\n+        keepdims: If `True`, axes which are reduced are left in the result as\n             dimensions that are broadcast to the size of the original\n             input tensor. Defaults to `False`.\n \n@@ -701,15 +701,16 @@ class Arccos(Operation):\n \n @keras_core_export([\"keras_core.ops.arccos\", \"keras_core.ops.numpy.arccos\"])\n def arccos(x):\n-    \"\"\"Trigonometric inverse cosine, element-wise. The inverse of `cos` so that,\n-    if `y = cos(x)`, then `x = arccos(y)`.\n+    \"\"\"Trigonometric inverse cosine, element-wise.\n \n-    Arg:\n+    The inverse of `cos` so that, if `y = cos(x)`, then `x = arccos(y)`.\n+\n+    Args:\n         x: Input tensor.\n \n     Returns:\n         Tensor of the angle of the ray intersecting the unit circle at the given\n-        x-coordinate in radians [0, pi].\n+        x-coordinate in radians `[0, pi]`.\n \n     Example:\n     >>> x = keras_core.ops.convert_to_tensor([1, -1])\n@@ -754,7 +755,7 @@ class Arcsin(Operation):\n def arcsin(x):\n     \"\"\"Inverse sine, element-wise.\n \n-    Arg:\n+    Args:\n         x: Input tensor.\n \n     Returns:\n@@ -787,7 +788,7 @@ def arcsinh(x):\n         x: Input tensor.\n \n     Returns:\n-        Output tensor of same shape as x.\n+        Output tensor of same shape as `x`.\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Arcsinh().symbolic_call(x)\n@@ -806,7 +807,7 @@ class Arctan(Operation):\n def arctan(x):\n     \"\"\"Trigonometric inverse tangent, element-wise.\n \n-    Arg:\n+    Args:\n         x: Input tensor.\n \n     Returns:\n@@ -894,7 +895,7 @@ def arctanh(x):\n         x: Input tensor.\n \n     Returns:\n-        Output tensor of same shape as x.\n+        Output tensor of same shape as `x`.\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Arctanh().symbolic_call(x)\n@@ -1009,7 +1010,7 @@ class Argsort(Operation):\n \n @keras_core_export([\"keras_core.ops.argsort\", \"keras_core.ops.numpy.argsort\"])\n def argsort(x, axis=-1):\n-    \"\"\"Returns the indices that would sort an tensor.\n+    \"\"\"Returns the indices that would sort a tensor.\n \n     Args:\n         x: Input tensor.\n@@ -1258,6 +1259,23 @@ class BroadcastTo(Operation):\n     ]\n )\n def broadcast_to(x, shape):\n+    \"\"\"Broadcast a tensor to a new shape.\n+\n+    Args:\n+        x: The tensor to broadcast.\n+        shape: The shape of the desired tensor. A single integer `i` is\n+            interpreted as `(i,)`.\n+\n+    Returns:\n+        A tensor with the desired shape.\n+\n+    Examples:\n+    >>> x = keras_core.ops.array([1, 2, 3])\n+    >>> keras_core.ops.broadcast_to(x, (3, 3))\n+    array([[1, 2, 3],\n+           [1, 2, 3],\n+           [1, 2, 3]])\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return BroadcastTo(shape=shape).symbolic_call(x)\n     return backend.numpy.broadcast_to(x, shape)\n@@ -1273,6 +1291,17 @@ class Ceil(Operation):\n \n @keras_core_export([\"keras_core.ops.ceil\", \"keras_core.ops.numpy.ceil\"])\n def ceil(x):\n+    \"\"\"Return the ceiling of the input, element-wise.\n+\n+    The ceil of the scalar `x` is the smallest integer `i`, such that\n+    `i >= x`.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        The ceiling of each element in `x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Ceil().symbolic_call(x)\n     return backend.numpy.ceil(x)\n@@ -1293,6 +1322,20 @@ class Clip(Operation):\n \n @keras_core_export([\"keras_core.ops.clip\", \"keras_core.ops.numpy.clip\"])\n def clip(x, x_min, x_max):\n+    \"\"\"Clip (limit) the values in a tensor.\n+\n+    Given an interval, values outside the interval are clipped to the\n+    interval edges. For example, if an interval of `[0, 1]` is specified,\n+    values smaller than 0 become 0, and values larger than 1 become 1.\n+\n+    Args:\n+        x: Input tensor.\n+        x_min: Minimum value.\n+        x_max: Maximum value.\n+\n+    Returns:\n+        The clipped tensor.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Clip(x_min, x_max).symbolic_call(x)\n     return backend.numpy.clip(x, x_min, x_max)\n@@ -1337,6 +1380,15 @@ class Concatenate(Operation):\n     ]\n )\n def concatenate(xs, axis=0):\n+    \"\"\"Join a sequence of tensors along an existing axis.\n+\n+    Args:\n+        xs: The sequence of tensors to concatenate.\n+        axis: The axis along which the tensors will be joined. Defaults to 0.\n+\n+    Returns:\n+        The concatenated tensor.\n+    \"\"\"\n     if any_symbolic_tensors(xs):\n         return Concatenate(axis=axis).symbolic_call(xs)\n     return backend.numpy.concatenate(xs, axis=axis)\n@@ -1354,6 +1406,19 @@ class Conjugate(Operation):\n     [\"keras_core.ops.conjugate\", \"keras_core.ops.numpy.conjugate\"]\n )\n def conjugate(x):\n+    \"\"\"Returns the complex conjugate, element-wise.\n+\n+    The complex conjugate of a complex number is obtained by changing the sign\n+    of its imaginary part.\n+\n+    `keras_core.ops.conj` is a shorthand for this function.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        The complex conjugate of each element in `x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Conjugate().symbolic_call(x)\n     return backend.numpy.conjugate(x)\n@@ -1365,6 +1430,7 @@ class Conj(Conjugate):\n \n @keras_core_export([\"keras_core.ops.conj\", \"keras_core.ops.numpy.conj\"])\n def conj(x):\n+    \"\"\"Shorthand for `keras_core.ops.conjugate`.\"\"\"\n     return conjugate(x)\n \n \n@@ -1378,6 +1444,14 @@ class Copy(Operation):\n \n @keras_core_export([\"keras_core.ops.copy\", \"keras_core.ops.numpy.copy\"])\n def copy(x):\n+    \"\"\"Returns a copy of `x`.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        A copy of `x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Copy().symbolic_call(x)\n     return backend.numpy.copy(x)\n@@ -1393,6 +1467,14 @@ class Cos(Operation):\n \n @keras_core_export([\"keras_core.ops.cos\", \"keras_core.ops.numpy.cos\"])\n def cos(x):\n+    \"\"\"Cosine, element-wise.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        The corresponding cosine values.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Cos().symbolic_call(x)\n     return backend.numpy.cos(x)\n@@ -1414,7 +1496,7 @@ def cosh(x):\n         x: Input tensor.\n \n     Returns:\n-        Output tensor of same shape as x.\n+        Output tensor of same shape as `x`.\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Cosh().symbolic_call(x)\n@@ -1446,6 +1528,27 @@ class CountNonzero(Operation):\n     ]\n )\n def count_nonzero(x, axis=None):\n+    \"\"\"Counts the number of non-zero values in `x` along the given `axis`.\n+\n+    If no axis is specified then all non-zeros in the tensor are counted.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: Axis or tuple of axes along which to count the number of\n+            non-zeros. Defaults to `None`.\n+\n+    Returns:\n+        int or tensor of ints.\n+\n+    Examples:\n+    >>> x = keras_core.ops.array([[0, 1, 7, 0], [3, 0, 2, 19]])\n+    >>> keras_core.ops.count_nonzero(x)\n+    5\n+    >>> keras_core.ops.count_nonzero(x, axis=0)\n+    array([1, 1, 2, 1], dtype=int64)\n+    >>> keras_core.ops.count_nonzero(x, axis=1)\n+    array([2, 3], dtype=int64)\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return CountNonzero(axis=axis).symbolic_call(x)\n     return backend.numpy.count_nonzero(x, axis=axis)\n@@ -1500,6 +1603,39 @@ class Cross(Operation):\n \n @keras_core_export([\"keras_core.ops.cross\", \"keras_core.ops.numpy.cross\"])\n def cross(x1, x2, axisa=-1, axisb=-1, axisc=-1, axis=None):\n+    \"\"\"Returns the cross product of two (arrays of) vectors.\n+\n+    The cross product of `x1` and `x2` in R^3 is a vector\n+    perpendicular to both `x1` and `x2`. If `x1` and `x2` are arrays of\n+    vectors, the vectors are defined by the last axis of `x1` and `x2`\n+    by default, and these axes can have dimensions 2 or 3.\n+\n+    Where the dimension of either `x1` or `x2` is 2, the third component of\n+    the input vector is assumed to be zero and the cross product calculated\n+    accordingly.\n+\n+    In cases where both input vectors have dimension 2, the z-component of\n+    the cross product is returned.\n+\n+    Args:\n+        x1: Components of the first vector(s).\n+        x2: Components of the second vector(s).\n+        axisa: Axis of `x1` that defines the vector(s). Defaults to -1.\n+        axisb: Axis of `x2` that defines the vector(s). Defaults to -1.\n+        axisc: Axis of the result containing the cross product vector(s).\n+            Ignored if both input vectors have dimension 2, as the return is\n+            scalar. By default, the last axis.\n+        axis: If defined, the axis of `x1`, `x2` and the result that\n+            defines the vector(s) and cross product(s). Overrides `axisa`,\n+            `axisb` and `axisc`.\n+\n+    Note:\n+        Torch backend does not support two dimensional vectors, or the\n+        arguments `axisa`, `axisb` and `axisc`. Use `axis` instead.\n+\n+    Returns:\n+        Vector cross product(s).\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return Cross(\n             axisa=axisa, axisb=axisb, axisc=axisc, axis=axis\n@@ -1534,6 +1670,16 @@ class Cumprod(Operation):\n \n @keras_core_export([\"keras_core.ops.cumprod\", \"keras_core.ops.numpy.cumprod\"])\n def cumprod(x, axis=None):\n+    \"\"\"Return the cumulative product of elements along a given axis.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: Axis along which the cumulative product is computed.\n+            By default the input is flattened.\n+\n+    Returns:\n+        Output tensor.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Cumprod(axis=axis).symbolic_call(x)\n     return backend.numpy.cumprod(x, axis=axis)\n@@ -1559,6 +1705,16 @@ class Cumsum(Operation):\n \n @keras_core_export([\"keras_core.ops.cumsum\", \"keras_core.ops.numpy.cumsum\"])\n def cumsum(x, axis=None):\n+    \"\"\"Returns the cumulative sum of elements along a given axis.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: Axis along which the cumulative sum is computed.\n+            By default the input is flattened.\n+\n+    Returns:\n+        Output tensor.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Cumsum(axis=axis).symbolic_call(x)\n     return backend.numpy.cumsum(x, axis=axis)\n@@ -1603,6 +1759,38 @@ class Diag(Operation):\n \n @keras_core_export([\"keras_core.ops.diag\", \"keras_core.ops.numpy.diag\"])\n def diag(x, k=0):\n+    \"\"\"Extract a diagonal or construct a diagonal array.\n+\n+    Args:\n+        x: Input tensor. If `x` is 2-D, returns the k-th diagonal of `x`.\n+            If `x` is 1-D, return a 2-D tensor with `x` on the k-th diagonal.\n+        k: The diagonal to consider. Defaults to 0. Use `k > 0` for diagonals\n+            above the main diagonal, and `k < 0` for diagonals below\n+            the main diagonal.\n+\n+    Returns:\n+        The extracted diagonal or constructed diagonal tensor.\n+\n+    Examples:\n+    >>> from keras_core import ops\n+    >>> x = ops.arange(9).reshape((3, 3))\n+    >>> x\n+    array([[0, 1, 2],\n+           [3, 4, 5],\n+           [6, 7, 8]])\n+\n+    >>> ops.diag(x)\n+    array([0, 4, 8])\n+    >>> ops.diag(x, k=1)\n+    array([1, 5])\n+    >>> ops.diag(x, k=-1)\n+    array([3, 7])\n+\n+    >>> ops.diag(ops.diag(x)))\n+    array([[0, 0, 0],\n+           [0, 4, 0],\n+           [0, 0, 8]])\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Diag(k=k).symbolic_call(x)\n     return backend.numpy.diag(x, k=k)\n@@ -1652,6 +1840,52 @@ class Diagonal(Operation):\n \n @keras_core_export([\"keras_core.ops.diagonal\", \"keras_core.ops.numpy.diagonal\"])\n def diagonal(x, offset=0, axis1=0, axis2=1):\n+    \"\"\"Return specified diagonals.\n+\n+    If `x` is 2-D, returns the diagonal of `x` with the given offset, i.e., the\n+    collection of elements of the form `x[i, i+offset]`.\n+\n+    If `x` has more than two dimensions, the axes specified by `axis1`\n+    and `axis2` are used to determine the 2-D sub-array whose diagonal\n+    is returned.\n+\n+    The shape of the resulting array can be determined by removing `axis1`\n+    and `axis2` and appending an index to the right equal to the size of\n+    the resulting diagonals.\n+\n+    Args:\n+        x: Input tensor.\n+        offset: Offset of the diagonal from the main diagonal.\n+            Can be positive or negative. Defaults to 0 (main diagonal).\n+        axis1: Axis to be used as the first axis of the 2-D sub-arrays.\n+            Defaults to 0 (first axis).\n+        axis2: Axis to be used as the second axis of the 2-D sub-arrays.\n+            Defaults to 1 (second axis).\n+\n+    Returns:\n+        Tensor of diagonals.\n+\n+    Examples:\n+    >>> from keras_core import ops\n+    >>> x = ops.arange(4).reshape((2, 2))\n+    >>> x\n+    array([[0, 1],\n+           [2, 3]])\n+    >>> x.diagonal()\n+    array([0, 3])\n+    >>> x.diagonal(1)\n+    array([1])\n+\n+    >>> x = ops.arange(8).reshape((2, 2, 2))\n+    >>> x\n+    array([[[0, 1],\n+            [2, 3]],\n+           [[4, 5],\n+            [6, 7]]])\n+    >>> x.diagonal(0, 0, 1)\n+    array([[0, 6],\n+           [1, 7]])\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Diagonal(\n             offset=offset,\n@@ -1690,7 +1924,7 @@ def digitize(x, bins):\n             increasing.\n \n     Returns:\n-        Output array of indices, of same shape as x.\n+        Output array of indices, of same shape as `x`.\n \n     Example:\n \n@@ -1742,6 +1976,28 @@ class Dot(Operation):\n \n @keras_core_export([\"keras_core.ops.dot\", \"keras_core.ops.numpy.dot\"])\n def dot(x1, x2):\n+    \"\"\"Dot product of two tensors.\n+\n+    - If both `x1` and `x2` are 1-D tensors, it is inner product of vectors\n+      (without complex conjugation).\n+    - If both `x1` and `x2` are 2-D tensors, it is matrix multiplication.\n+    - If either `x1` or `x2` is 0-D (scalar), it is equivalent to `x1 * x2`.\n+    - If `x1` is an N-D tensor and `x2` is a 1-D tensor, it is a sum product\n+      over the last axis of `x1` and `x2`.\n+    - If `x1` is an N-D tensor and `x2` is an M-D tensor (where `M>=2`),\n+      it is a sum product over the last axis of `x1` and the second-to-last\n+      axis of `x2`: `dot(x1, x2)[i,j,k,m] = sum(a[i,j,:] * b[k,:,m])`.\n+\n+    Args:\n+        x1: First argument.\n+        x2: Second argument.\n+\n+    Note:\n+        Torch backend does not accept 0-D tensors as arguments.\n+\n+    Returns:\n+        Dot product of `x1` and `x2`.\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return Dot().symbolic_call(x1, x2)\n     return backend.numpy.dot(x1, x2)\n@@ -1776,7 +2032,8 @@ class Einsum(Operation):\n             the index 0. For dims not represented by any letter, insert to the\n             wildcard part. For each letter in output target not appearing in\n             input spec, the dim will be 1 for broadcasting. After 4, each\n-            operand should have a target shape containing only number and None.\n+            operand should have a target shape containing only number and\n+            `None`.\n         5. Broadcast all shapes computed from 4, and the result is the output\n             shape.\n \n@@ -1922,6 +2179,88 @@ class Einsum(Operation):\n \n @keras_core_export([\"keras_core.ops.einsum\", \"keras_core.ops.numpy.einsum\"])\n def einsum(subscripts, *operands):\n+    \"\"\"Evaluates the Einstein summation convention on the operands.\n+\n+    Args:\n+        subscripts: Specifies the subscripts for summation as comma separated\n+            list of subscript labels. An implicit (classical Einstein\n+            summation) calculation is performed unless the explicit indicator\n+            `->` is included as well as subscript labels of the precise\n+            output form.\n+        operands: The operands to compute the Einstein sum of.\n+\n+    Returns:\n+        The calculation based on the Einstein summation convention.\n+\n+    Example:\n+    >>> from keras_core import ops\n+    >>> a = ops.arange(25).reshape(5, 5)\n+    >>> b = ops.arange(5)\n+    >>> c = ops.arange(6).reshape(2, 3)\n+\n+    Trace of a matrix:\n+\n+    >>> ops.einsum(\"ii\", a)\n+    60\n+    >>> ops.einsum(a, [0, 0])\n+    60\n+    >>> ops.trace(a)\n+    60\n+\n+    Extract the diagonal:\n+\n+    >>> ops.einsum(\"ii -> i\", a)\n+    array([ 0,  6, 12, 18, 24])\n+    >>> ops.einsum(a, [0, 0], [0])\n+    array([ 0,  6, 12, 18, 24])\n+    >>> ops.diag(a)\n+    array([ 0,  6, 12, 18, 24])\n+\n+    Sum over an axis:\n+\n+    >>> ops.einsum(\"ij -> i\", a)\n+    array([ 10,  35,  60,  85, 110])\n+    >>> ops.einsum(a, [0, 1], [0])\n+    array([ 10,  35,  60,  85, 110])\n+    >>> ops.sum(a, axis=1)\n+    array([ 10,  35,  60,  85, 110])\n+\n+    For higher dimensional tensors summing a single axis can be done\n+    with ellipsis:\n+\n+    >>> ops.einsum(\"...j -> ...\", a)\n+    array([ 10,  35,  60,  85, 110])\n+    >>> np.einsum(a, [..., 1], [...])\n+    array([ 10,  35,  60,  85, 110])\n+\n+    Compute a matrix transpose or reorder any number of axes:\n+\n+    >>> ops.einsum(\"ji\", c)\n+    array([[0, 3],\n+           [1, 4],\n+           [2, 5]])\n+    >>> ops.einsum(\"ij -> ji\", c)\n+    array([[0, 3],\n+           [1, 4],\n+           [2, 5]])\n+    >>> ops.einsum(c, [1, 0])\n+    array([[0, 3],\n+           [1, 4],\n+           [2, 5]])\n+    >>> ops.transpose(c)\n+    array([[0, 3],\n+           [1, 4],\n+           [2, 5]])\n+\n+    Matrix vector multiplication:\n+\n+    >>> ops.einsum(\"ij, j\", a, b)\n+    array([ 30,  80, 130, 180, 230])\n+    >>> ops.einsum(a, [0, 1], b, [1])\n+    array([ 30,  80, 130, 180, 230])\n+    >>> ops.einsum(\"...j, j\", a, b)\n+    array([ 30,  80, 130, 180, 230])\n+    \"\"\"\n     if any_symbolic_tensors(operands):\n         return Einsum(subscripts).symbolic_call(*operands)\n     return backend.numpy.einsum(subscripts, *operands)\n@@ -1937,6 +2276,15 @@ class Empty(Operation):\n \n @keras_core_export([\"keras_core.ops.empty\", \"keras_core.ops.numpy.empty\"])\n def empty(shape, dtype=\"float32\"):\n+    \"\"\"Return a tensor of given shape and type filled with uninitialized data.\n+\n+    Args:\n+        shape: Shape of the empty tensor.\n+        dtype: Desired data type of the empty tensor.\n+\n+    Returns:\n+        The empty tensor.\n+    \"\"\"\n     return backend.numpy.empty(shape, dtype=dtype)\n \n \n@@ -1953,6 +2301,15 @@ class Equal(Operation):\n \n @keras_core_export([\"keras_core.ops.equal\", \"keras_core.ops.numpy.equal\"])\n def equal(x1, x2):\n+    \"\"\"Returns `(x1 == x2)` element-wise.\n+\n+    Args:\n+        x1: Tensor to compare.\n+        x2: Tensor to compare.\n+\n+    Returns:\n+        Output tensor, element-wise comparison of `x1` and `x2`.\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return Equal().symbolic_call(x1, x2)\n     return backend.numpy.equal(x1, x2)\n@@ -1968,6 +2325,14 @@ class Exp(Operation):\n \n @keras_core_export([\"keras_core.ops.exp\", \"keras_core.ops.numpy.exp\"])\n def exp(x):\n+    \"\"\"Calculate the exponential of all elements in the input tensor.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output tensor, element-wise exponential of `x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Exp().symbolic_call(x)\n     return backend.numpy.exp(x)\n@@ -2003,6 +2368,18 @@ class ExpandDims(Operation):\n     ]\n )\n def expand_dims(x, axis):\n+    \"\"\"Expand the shape of a tensor.\n+\n+    Insert a new axis at the `axis` position in the expanded tensor shape.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: Position in the expanded axes where the new axis\n+            (or axes) is placed.\n+\n+    Returns:\n+        Output tensor with the number of dimensions increased.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return ExpandDims(axis=axis).symbolic_call(x)\n     return backend.numpy.expand_dims(x, axis)\n@@ -2018,6 +2395,14 @@ class Expm1(Operation):\n \n @keras_core_export([\"keras_core.ops.expm1\", \"keras_core.ops.numpy.expm1\"])\n def expm1(x):\n+    \"\"\"Calculate `exp(x) - 1` for all elements in the tensor.\n+\n+    Args:\n+        x: Input values.\n+\n+    Returns:\n+        Output tensor, element-wise exponential minus one.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Expm1().symbolic_call(x)\n     return backend.numpy.expm1(x)\n@@ -2037,6 +2422,18 @@ class Flip(Operation):\n \n @keras_core_export([\"keras_core.ops.flip\", \"keras_core.ops.numpy.flip\"])\n def flip(x, axis=None):\n+    \"\"\"Reverse the order of elements in the tensor along the given axis.\n+\n+    The shape of the tensor is preserved, but the elements are reordered.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: Axis or axes along which to flip the tensor. The default,\n+            `axis=None`, will flip over all of the axes of the input tensor.\n+\n+    Returns:\n+        Output tensor with entries of `axis` reversed.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Flip(axis=axis).symbolic_call(x)\n     return backend.numpy.flip(x, axis=axis)\n@@ -2052,6 +2449,16 @@ class Floor(Operation):\n \n @keras_core_export([\"keras_core.ops.floor\", \"keras_core.ops.numpy.floor\"])\n def floor(x):\n+    \"\"\"Return the floor of the input, element-wise.\n+\n+    The floor of the scalar `x` is the largest integer `i`, such that `i <= x`.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output tensor, element-wise floor of `x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Floor().symbolic_call(x)\n     return backend.numpy.floor(x)\n@@ -2067,6 +2474,16 @@ class Full(Operation):\n \n @keras_core_export([\"keras_core.ops.full\", \"keras_core.ops.numpy.full\"])\n def full(shape, fill_value, dtype=None):\n+    \"\"\"Return a new tensor of given shape and type, filled with `fill_value`.\n+\n+    Args:\n+        shape: Shape of the new tensor.\n+        fill_value: Fill value.\n+        dtype: Desired data type of the tensor.\n+\n+    Returns:\n+        Output tensor.\n+    \"\"\"\n     return backend.numpy.full(shape, fill_value, dtype=dtype)\n \n \n@@ -2082,6 +2499,16 @@ class FullLike(Operation):\n     [\"keras_core.ops.full_like\", \"keras_core.ops.numpy.full_like\"]\n )\n def full_like(x, fill_value, dtype=None):\n+    \"\"\"Return a full tensor with the same shape and type as the given tensor.\n+\n+    Args:\n+        x: Input tensor.\n+        fill_value: Fill value.\n+        dtype: Overrides data type of the result.\n+\n+    Returns:\n+        Tensor of `fill_value` with the same shape and type as `x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return FullLike().symbolic_call(x, fill_value, dtype=dtype)\n     return backend.numpy.full_like(x, fill_value, dtype=dtype)\n@@ -2158,6 +2585,7 @@ class GetItem(Operation):\n \n @keras_core_export([\"keras_core.ops.get_item\", \"keras_core.ops.numpy.get_item\"])\n def get_item(x, key):\n+    \"\"\"Return `x[key]`.\"\"\"\n     if any_symbolic_tensors((x,)):\n         return GetItem().symbolic_call(x, key)\n     return x[key]\n@@ -2176,6 +2604,15 @@ class Greater(Operation):\n \n @keras_core_export([\"keras_core.ops.greater\", \"keras_core.ops.numpy.greater\"])\n def greater(x1, x2):\n+    \"\"\"Return the truth value of `x1 > x2` element-wise.\n+\n+    Args:\n+        x1: First input tensor.\n+        x2: Second input tensor.\n+\n+    Returns:\n+        Output tensor, element-wise comparison of `x1` and `x2`.\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return Greater().symbolic_call(x1, x2)\n     return backend.numpy.greater(x1, x2)\n@@ -2199,6 +2636,15 @@ class GreaterEqual(Operation):\n     ]\n )\n def greater_equal(x1, x2):\n+    \"\"\"Return the truth value of `x1 >= x2` element-wise.\n+\n+    Args:\n+        x1: First input tensor.\n+        x2: Second input tensor.\n+\n+    Returns:\n+        Output tensor, element-wise comparison of `x1` and `x2`.\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return GreaterEqual().symbolic_call(x1, x2)\n     return backend.numpy.greater_equal(x1, x2)\n@@ -2230,6 +2676,17 @@ class Hstack(Operation):\n \n @keras_core_export([\"keras_core.ops.hstack\", \"keras_core.ops.numpy.hstack\"])\n def hstack(xs):\n+    \"\"\"Stack tensors in sequence horizontally (column wise).\n+\n+    This is equivalent to concatenation along the first axis for 1-D tensors,\n+    and along the second axis for all other tensors.\n+\n+    Args:\n+        xs: Sequence of tensors.\n+\n+    Returns:\n+        The tensor formed by stacking the given tensors.\n+    \"\"\"\n     if any_symbolic_tensors((xs,)):\n         return Hstack().symbolic_call(xs)\n     return backend.numpy.hstack(xs)\n@@ -2245,6 +2702,18 @@ class Identity(Operation):\n \n @keras_core_export([\"keras_core.ops.identity\", \"keras_core.ops.numpy.identity\"])\n def identity(n, dtype=\"float32\"):\n+    \"\"\"Return the identity tensor.\n+\n+    The identity tensor is a square tensor with ones on the main diagonal and\n+    zeros elsewhere.\n+\n+    Args:\n+        n: Number of rows (and columns) in the `n x n` output tensor.\n+        dtype: Data type of the output tensor.\n+\n+    Returns:\n+        The identity tensor.\n+    \"\"\"\n     return backend.numpy.identity(n, dtype=dtype)\n \n \n@@ -2258,6 +2727,14 @@ class Imag(Operation):\n \n @keras_core_export([\"keras_core.ops.imag\", \"keras_core.ops.numpy.imag\"])\n def imag(x):\n+    \"\"\"Return the imaginary part of the complex argument.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        The imaginary component of the complex argument.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Imag().symbolic_call(x)\n     return backend.numpy.imag(x)\n@@ -2276,6 +2753,15 @@ class Isclose(Operation):\n \n @keras_core_export([\"keras_core.ops.isclose\", \"keras_core.ops.numpy.isclose\"])\n def isclose(x1, x2):\n+    \"\"\"Return whether two tensors are element-wise almost equal.\n+\n+    Args:\n+        x1: First input tensor.\n+        x2: Second input tensor.\n+\n+    Returns:\n+        Output boolean tensor.\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return Isclose().symbolic_call(x1, x2)\n     return backend.numpy.isclose(x1, x2)\n@@ -2291,6 +2777,18 @@ class Isfinite(Operation):\n \n @keras_core_export([\"keras_core.ops.isfinite\", \"keras_core.ops.numpy.isfinite\"])\n def isfinite(x):\n+    \"\"\"Return whether a tensor is finite, element-wise.\n+\n+    Real values are finite when they are not NaN, not positive infinity, and\n+    not negative infinity. Complex values are finite when both their real\n+    and imaginary parts are finite.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output boolean tensor.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Isfinite().symbolic_call(x)\n     return backend.numpy.isfinite(x)\n@@ -2306,6 +2804,14 @@ class Isinf(Operation):\n \n @keras_core_export([\"keras_core.ops.isinf\", \"keras_core.ops.numpy.isinf\"])\n def isinf(x):\n+    \"\"\"Test element-wise for positive or negative infinity.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output boolean tensor.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Isinf().symbolic_call(x)\n     return backend.numpy.isinf(x)\n@@ -2321,6 +2827,14 @@ class Isnan(Operation):\n \n @keras_core_export([\"keras_core.ops.isnan\", \"keras_core.ops.numpy.isnan\"])\n def isnan(x):\n+    \"\"\"Test element-wise for NaN and return result as a boolean tensor.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output boolean tensor.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Isnan().symbolic_call(x)\n     return backend.numpy.isnan(x)\n@@ -2339,6 +2853,15 @@ class Less(Operation):\n \n @keras_core_export([\"keras_core.ops.less\", \"keras_core.ops.numpy.less\"])\n def less(x1, x2):\n+    \"\"\"Return the truth value of `x1 < x2` element-wise.\n+\n+    Args:\n+        x1: First input tensor.\n+        x2: Second input tensor.\n+\n+    Returns:\n+        Output tensor, element-wise comparison of `x1` and `x2`.\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return Less().symbolic_call(x1, x2)\n     return backend.numpy.less(x1, x2)\n@@ -2362,6 +2885,15 @@ class LessEqual(Operation):\n     ]\n )\n def less_equal(x1, x2):\n+    \"\"\"Return the truth value of `x1 <= x2` element-wise.\n+\n+    Args:\n+        x1: First input tensor.\n+        x2: Second input tensor.\n+\n+    Returns:\n+        Output tensor, element-wise comparison of `x1` and `x2`.\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return LessEqual().symbolic_call(x1, x2)\n     return backend.numpy.less_equal(x1, x2)\n@@ -2418,6 +2950,36 @@ class Linspace(Operation):\n def linspace(\n     start, stop, num=50, endpoint=True, retstep=False, dtype=None, axis=0\n ):\n+    \"\"\"Return evenly spaced numbers over a specified interval.\n+\n+    Returns `num` evenly spaced samples, calculated over the interval\n+    `[start, stop]`.\n+\n+    The endpoint of the interval can optionally be excluded.\n+\n+    Args:\n+        start: The starting value of the sequence.\n+        stop: The end value of the sequence, unless `endpoint` is set to\n+            `False`. In that case, the sequence consists of all but the last\n+            of `num + 1` evenly spaced samples, so that `stop` is excluded.\n+            Note that the step size changes when `endpoint` is `False`.\n+        num: Number of samples to generate. Default is 50. Must be\n+            non-negative.\n+        endpoint: If `True`, `stop` is the last sample. Otherwise, it is\n+            not included. Default is `True`.\n+        retstep: If `True`, return `(samples, step)`, where `step` is the\n+            spacing between samples.\n+        dtype: The type of the output tensor.\n+        axis: The axis in the result to store the samples. Relevant only if\n+            start or stop are array-like. Default is 0.\n+\n+    Note:\n+        Torch backend does not support `axis` argument.\n+\n+    Returns:\n+        A tensor of evenly spaced numbers.\n+        If `retstep` is `True`, returns `(samples, step)`\n+    \"\"\"\n     if any_symbolic_tensors((start, stop)):\n         return Linspace(num, endpoint, retstep, dtype, axis)(start, stop)\n     return backend.numpy.linspace(\n@@ -2441,6 +3003,14 @@ class Log(Operation):\n \n @keras_core_export([\"keras_core.ops.log\", \"keras_core.ops.numpy.log\"])\n def log(x):\n+    \"\"\"Natural logarithm, element-wise.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output tensor, element-wise natural logarithm of `x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Log().symbolic_call(x)\n     return backend.numpy.log(x)\n@@ -2456,6 +3026,14 @@ class Log10(Operation):\n \n @keras_core_export([\"keras_core.ops.log10\", \"keras_core.ops.numpy.log10\"])\n def log10(x):\n+    \"\"\"Return the base 10 logarithm of the input tensor, element-wise.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output tensor, element-wise base 10 logarithm of `x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Log10().symbolic_call(x)\n     return backend.numpy.log10(x)\n@@ -2471,6 +3049,16 @@ class Log1p(Operation):\n \n @keras_core_export([\"keras_core.ops.log1p\", \"keras_core.ops.numpy.log1p\"])\n def log1p(x):\n+    \"\"\"Returns the natural logarithm of one plus the `x`, element-wise.\n+\n+    Calculates `log(1 + x)`.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output tensor, element-wise natural logarithm of `1 + x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Log1p().symbolic_call(x)\n     return backend.numpy.log1p(x)\n@@ -2486,6 +3074,14 @@ class Log2(Operation):\n \n @keras_core_export([\"keras_core.ops.log2\", \"keras_core.ops.numpy.log2\"])\n def log2(x):\n+    \"\"\"Base-2 logarithm of `x`, element-wise.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output tensor, element-wise base-2 logarithm of `x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Log2().symbolic_call(x)\n     return backend.numpy.log2(x)\n@@ -2506,6 +3102,18 @@ class Logaddexp(Operation):\n     [\"keras_core.ops.logaddexp\", \"keras_core.ops.numpy.logaddexp\"]\n )\n def logaddexp(x1, x2):\n+    \"\"\"Logarithm of the sum of exponentiations of the inputs.\n+\n+    Calculates `log(exp(x1) + exp(x2))`.\n+\n+    Args:\n+        x1: Input tensor.\n+        x2: Input tensor.\n+\n+    Returns:\n+        Output tensor, element-wise logarithm of the sum of exponentiations\n+        of the inputs.\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return Logaddexp().symbolic_call(x1, x2)\n     return backend.numpy.logaddexp(x1, x2)\n@@ -2529,6 +3137,17 @@ class LogicalAnd(Operation):\n     ]\n )\n def logical_and(x1, x2):\n+    \"\"\"Computes the element-wise logical AND of the given input tensors.\n+\n+    Zeros are treated as `False` and non-zeros are treated as `True`.\n+\n+    Args:\n+        x1: Input tensor.\n+        x2: Input tensor.\n+\n+    Returns:\n+        Output tensor, element-wise logical AND of the inputs.\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return LogicalAnd().symbolic_call(x1, x2)\n     return backend.numpy.logical_and(x1, x2)\n@@ -2549,6 +3168,16 @@ class LogicalNot(Operation):\n     ]\n )\n def logical_not(x):\n+    \"\"\"Computes the element-wise NOT of the given input tensor.\n+\n+    Zeros are treated as `False` and non-zeros are treated as `True`.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output tensor, element-wise logical NOT of the input.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return LogicalNot().symbolic_call(x)\n     return backend.numpy.logical_not(x)\n@@ -2572,6 +3201,17 @@ class LogicalOr(Operation):\n     ]\n )\n def logical_or(x1, x2):\n+    \"\"\"Computes the element-wise logical OR of the given input tensors.\n+\n+    Zeros are treated as `False` and non-zeros are treated as `True`.\n+\n+    Args:\n+        x1: Input tensor.\n+        x2: Input tensor.\n+\n+    Returns:\n+        Output tensor, element-wise logical OR of the inputs.\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return LogicalOr().symbolic_call(x1, x2)\n     return backend.numpy.logical_or(x1, x2)\n@@ -2622,6 +3262,31 @@ class Logspace(Operation):\n \n @keras_core_export([\"keras_core.ops.logspace\", \"keras_core.ops.numpy.logspace\"])\n def logspace(start, stop, num=50, endpoint=True, base=10, dtype=None, axis=0):\n+    \"\"\"Returns numbers spaced evenly on a log scale.\n+\n+    In linear space, the sequence starts at `base ** start` and ends with\n+    `base ** stop` (see `endpoint` below).\n+\n+    Args:\n+        start: The starting value of the sequence.\n+        stop: The final value of the sequence, unless `endpoint` is `False`.\n+            In that case, `num + 1` values are spaced over the interval in\n+            log-space, of which all but the last (a sequence of length `num`)\n+            are returned.\n+        num: Number of samples to generate. Default is 50.\n+        endpoint: If `True`, `stop` is the last sample. Otherwise, it is not\n+            included. Default is `True`.\n+        base: The base of the log space. Default is 10\n+        dtype: The type of the output tensor.\n+        axis: The axis in the result to store the samples. Relevant only\n+            if start or stop are array-like.\n+\n+    Note:\n+        Torch backend does not support `axis` argument.\n+\n+    Returns:\n+        A tensor of evenly spaced samples on a log scale.\n+    \"\"\"\n     if any_symbolic_tensors((start, stop)):\n         return Logspace(num, endpoint, base, dtype, axis)(start, stop)\n     return backend.numpy.logspace(\n@@ -2669,6 +3334,24 @@ class Matmul(Operation):\n \n @keras_core_export([\"keras_core.ops.matmul\", \"keras_core.ops.numpy.matmul\"])\n def matmul(x1, x2):\n+    \"\"\"Matrix product of two tensors.\n+\n+    - If both tensors are 1-dimensional, the dot product (scalar) is returned.\n+    - If either tensor is N-D, N > 2, it is treated as a stack of matrices\n+      residing in the last two indexes and broadcast accordingly.\n+    - If the first tensor is 1-D, it is promoted to a matrix by prepending\n+      a 1 to its dimensions. After matrix multiplication the prepended\n+      1 is removed.\n+    - If the second tensor is 1-D, it is promoted to a matrix by appending a 1\n+      to its dimensions. After matrix multiplication the appended 1 is removed.\n+\n+    Args:\n+        x1: First tensor.\n+        x2: Second tensor.\n+\n+    Returns:\n+        Output tensor, matrix product of the inputs.\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return Matmul().symbolic_call(x1, x2)\n     # The below conversion works around an outstanding JAX bug.\n@@ -2701,6 +3384,19 @@ class Max(Operation):\n \n @keras_core_export([\"keras_core.ops.max\", \"keras_core.ops.numpy.max\"])\n def max(x, axis=None, keepdims=False, initial=None):\n+    \"\"\"Return the maximum of a tensor or maximum along an axis.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: Axis or axes along which to operate. By default, flattened input\n+            is used.\n+        keepdims: If this is set to `True`, the axes which are reduced are left\n+            in the result as dimensions with size one. Default is `False`.\n+        initial: The minimum value of an output element. Default is `None`.\n+\n+    Returns:\n+        Maximum of `x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Max(axis=axis, keepdims=keepdims, initial=initial).symbolic_call(\n             x\n@@ -2721,6 +3417,15 @@ class Maximum(Operation):\n \n @keras_core_export([\"keras_core.ops.maximum\", \"keras_core.ops.numpy.maximum\"])\n def maximum(x1, x2):\n+    \"\"\"Element-wise maximum of `x1` and `x2`.\n+\n+    Args:\n+        x1: First tensor.\n+        x2: Second tensor.\n+\n+    Returns:\n+        Output tensor, element-wise maximum of `x1` and `x2`.\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return Maximum().symbolic_call(x1, x2)\n     return backend.numpy.maximum(x1, x2)\n@@ -2760,6 +3465,36 @@ class Meshgrid(Operation):\n \n @keras_core_export([\"keras_core.ops.meshgrid\", \"keras_core.ops.numpy.meshgrid\"])\n def meshgrid(*x, indexing=\"xy\"):\n+    \"\"\"Creates grids of coordinates from coordinate vectors.\n+\n+    Given `N` 1-D tensors `T0, T1, ..., TN-1` as inputs with corresponding\n+    lengths `S0, S1, ..., SN-1`, this creates an `N` N-dimensional tensors\n+    `G0, G1, ..., GN-1` each with shape `(S0, ..., SN-1)` where the output\n+    `Gi` is constructed by expanding `Ti` to the result shape.\n+\n+    Args:\n+        x: 1-D tensors representing the coordinates of a grid.\n+        indexing: Cartesian (`\"xy\"`, default) or matrix (`\"ij\"`) indexing\n+            of output.\n+\n+    Returns:\n+        Sequence of N tensors.\n+\n+    Example:\n+    >>> from keras_core import ops\n+    >>> x = ops.array([1, 2, 3])\n+    >>> y = ops.array([4, 5, 6])\n+\n+    >>> grid_x, grid_y = ops.meshgrid(x, y, indexing=\"ij\")\n+    >>> grid_x\n+    array([[1, 1, 1],\n+           [2, 2, 2],\n+           [3, 3, 3]])\n+    >>> grid_y\n+    array([[4, 5, 6],\n+           [4, 5, 6],\n+           [4, 5, 6]])\n+    \"\"\"\n     if any_symbolic_tensors(x):\n         return Meshgrid(indexing=indexing).symbolic_call(*x)\n     return backend.numpy.meshgrid(*x, indexing=indexing)\n@@ -2788,6 +3523,19 @@ class Min(Operation):\n \n @keras_core_export([\"keras_core.ops.min\", \"keras_core.ops.numpy.min\"])\n def min(x, axis=None, keepdims=False, initial=None):\n+    \"\"\"Return the minimum of a tensor or minimum along an axis.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: Axis or axes along which to operate. By default, flattened input\n+            is used.\n+        keepdims: If this is set to `True`, the axes which are reduced are left\n+            in the result as dimensions with size one. Default is `False`.\n+        initial: The maximum value of an output element. Default is `None`.\n+\n+    Returns:\n+        Minimum of `x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Min(axis=axis, keepdims=keepdims, initial=initial).symbolic_call(\n             x\n@@ -2808,6 +3556,15 @@ class Minimum(Operation):\n \n @keras_core_export([\"keras_core.ops.minimum\", \"keras_core.ops.numpy.minimum\"])\n def minimum(x1, x2):\n+    \"\"\"Element-wise minimum of `x1` and `x2`.\n+\n+    Args:\n+        x1: First tensor.\n+        x2: Second tensor.\n+\n+    Returns:\n+        Output tensor, element-wise minimum of `x1` and `x2`.\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return Minimum().symbolic_call(x1, x2)\n     return backend.numpy.minimum(x1, x2)\n@@ -2826,6 +3583,15 @@ class Mod(Operation):\n \n @keras_core_export([\"keras_core.ops.mod\", \"keras_core.ops.numpy.mod\"])\n def mod(x1, x2):\n+    \"\"\"Returns the element-wise remainder of division.\n+\n+    Args:\n+        x1: First tensor.\n+        x2: Second tensor.\n+\n+    Returns:\n+        Output tensor, element-wise remainder of division.\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return Mod().symbolic_call(x1, x2)\n     return backend.numpy.mod(x1, x2)\n@@ -2877,6 +3643,19 @@ class Moveaxis(Operation):\n \n @keras_core_export([\"keras_core.ops.moveaxis\", \"keras_core.ops.numpy.moveaxis\"])\n def moveaxis(x, source, destination):\n+    \"\"\"Move axes of a tensor to new positions.\n+\n+    Other axes remain in their original order.\n+\n+    Args:\n+        x: Tensor whose axes should be reordered.\n+        source: Original positions of the axes to move. These must be unique.\n+        destination: Destinations positions for each of the original axes.\n+            These must also be unique.\n+\n+    Returns:\n+        Tensor with moved axes.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Moveaxis(source, destination).symbolic_call(x)\n     return backend.numpy.moveaxis(x, source=source, destination=destination)\n@@ -2894,6 +3673,14 @@ class NanToNum(Operation):\n     ]\n )\n def nan_to_num(x):\n+    \"\"\"Replace NaN with zero and infinity with large finite numbers.\n+\n+    Args:\n+        x: Input data.\n+\n+    Returns:\n+        `x`, with non-finite values replaced.\n+    \"\"\"\n     return backend.numpy.nan_to_num(x)\n \n \n@@ -2909,6 +3696,14 @@ class Ndim(Operation):\n \n @keras_core_export([\"keras_core.ops.ndim\", \"keras_core.ops.numpy.ndim\"])\n def ndim(x):\n+    \"\"\"Return the number of dimensions of a tensor.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        The number of dimensions in `x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Ndim().symbolic_call(x)\n     return backend.numpy.ndim(x)\n@@ -2921,6 +3716,14 @@ class Nonzero(Operation):\n \n @keras_core_export([\"keras_core.ops.nonzero\", \"keras_core.ops.numpy.nonzero\"])\n def nonzero(x):\n+    \"\"\"Return the indices of the elements that are non-zero.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        Indices of elements that are non-zero.\n+    \"\"\"\n     return backend.numpy.nonzero(x)\n \n \n@@ -2939,6 +3742,15 @@ class NotEqual(Operation):\n     [\"keras_core.ops.not_equal\", \"keras_core.ops.numpy.not_equal\"]\n )\n def not_equal(x1, x2):\n+    \"\"\"Return `(x1 != x2)` element-wise.\n+\n+    Args:\n+        x1: First input tensor.\n+        x2: Second input tensor.\n+\n+    Returns:\n+        Output tensor, element-wise comparsion of `x1` and `x2`.\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return NotEqual().symbolic_call(x1, x2)\n     return backend.numpy.not_equal(x1, x2)\n@@ -2958,6 +3770,15 @@ class OnesLike(Operation):\n     [\"keras_core.ops.ones_like\", \"keras_core.ops.numpy.ones_like\"]\n )\n def ones_like(x, dtype=None):\n+    \"\"\"Return a tensor of ones with the same shape and type of `x`.\n+\n+    Args:\n+        x: Input tensor.\n+        dtype: Overrides the data type of the result.\n+\n+    Returns:\n+        A tensor of ones with the same shape and type as `x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return OnesLike().symbolic_call(x, dtype=dtype)\n     return backend.numpy.ones_like(x, dtype=dtype)\n@@ -2980,6 +3801,15 @@ class ZerosLike(Operation):\n     ]\n )\n def zeros_like(x, dtype=None):\n+    \"\"\"Return a tensor of zeros with the same shape and type as `x`.\n+\n+    Args:\n+        x: Input tensor.\n+        dtype: Overrides the data type of the result.\n+\n+    Returns:\n+        A tensor of zeros with the same shape and type as `x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return ZerosLike().symbolic_call(x, dtype=dtype)\n     return backend.numpy.zeros_like(x, dtype=dtype)\n@@ -3006,6 +3836,21 @@ class Outer(Operation):\n \n @keras_core_export([\"keras_core.ops.outer\", \"keras_core.ops.numpy.outer\"])\n def outer(x1, x2):\n+    \"\"\"Compute the outer product of two vectors.\n+\n+    Given two vectors `x1` and `x2`, the outer product is:\n+\n+    ```\n+    out[i, j] = x1[i] * x2[j]\n+    ```\n+\n+    Args:\n+        x1: First input tensor.\n+        x2: Second input tensor.\n+\n+    Returns:\n+        Outer product of `x1` and `x2`.\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return Outer().symbolic_call(x1, x2)\n     return backend.numpy.outer(x1, x2)\n@@ -3060,6 +3905,34 @@ class Pad(Operation):\n \n @keras_core_export([\"keras_core.ops.pad\", \"keras_core.ops.numpy.pad\"])\n def pad(x, pad_width, mode=\"constant\"):\n+    \"\"\"Pad a tensor.\n+\n+    Args:\n+        x: Tensor to pad.\n+        pad_width: Number of values padded to the edges of each axis.\n+            `((before_1, after_1), ...(before_N, after_N))` unique pad\n+            widths for each axis.\n+            `((before, after),)` yields same before and after pad for\n+            each axis.\n+            `(pad,)` or `int` is a shortcut for `before = after = pad`\n+            width for all axes.\n+        mode: One of `\"constant\"`, `\"edge\"`, `\"linear_ramp\"`,\n+            `\"maximum\"`, `\"mean\"`, `\"median\"`, `\"minimum\"`,\n+            `\"reflect\"`, `\"symmetric\"`, `\"wrap\"`, `\"empty\"`,\n+            `\"circular\"`. Default is `\"constant\"`.\n+\n+    Note:\n+        Torch backend only supports modes `\"constant\"`, `\"reflect\"`,\n+        `\"symmetric\"` and `\"circular\"`.\n+        Only Torch backend supports `\"circular\"` mode.\n+\n+    Note:\n+        Tensorflow backend only supports modes `\"constant\"`, `\"reflect\"`\n+        and `\"symmetric\"`.\n+\n+    Returns:\n+        Padded tensor.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Pad(pad_width, mode=mode).symbolic_call(x)\n     return backend.numpy.pad(x, pad_width, mode=mode)\n@@ -3092,6 +3965,20 @@ class Prod(Operation):\n \n @keras_core_export([\"keras_core.ops.prod\", \"keras_core.ops.numpy.prod\"])\n def prod(x, axis=None, keepdims=False, dtype=None):\n+    \"\"\"Return the product of tensor elements over a given axis.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: Axis or axes along which a product is performed. The default,\n+            `axis=None`, will compute the product of all elements\n+            in the input tensor.\n+        keepdims: If this is set to `True`, the axes which are reduce\n+            are left in the result as dimensions with size one.\n+        dtype: Data type of the returned tensor.\n+\n+    Returns:\n+        Product of elements of `x` over the given axis or axes.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Prod(axis=axis, keepdims=keepdims, dtype=dtype).symbolic_call(x)\n     return backend.numpy.prod(x, axis=axis, keepdims=keepdims, dtype=dtype)\n@@ -3113,6 +4000,16 @@ class Ravel(Operation):\n \n @keras_core_export([\"keras_core.ops.ravel\", \"keras_core.ops.numpy.ravel\"])\n def ravel(x):\n+    \"\"\"Return a contiguous flattened tensor.\n+\n+    A 1-D tensor, containing the elements of the input, is returned.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output tensor.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Ravel().symbolic_call(x)\n     return backend.numpy.ravel(x)\n@@ -3128,6 +4025,14 @@ class Real(Operation):\n \n @keras_core_export([\"keras_core.ops.real\", \"keras_core.ops.numpy.real\"])\n def real(x):\n+    \"\"\"Return the real part of the complex argument.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        The real component of the complex argument.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Real().symbolic_call(x)\n     return backend.numpy.real(x)\n@@ -3148,6 +4053,16 @@ class Reciprocal(Operation):\n     ]\n )\n def reciprocal(x):\n+    \"\"\"Return the reciprocal of the argument, element-wise.\n+\n+    Calculates `1/x`.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output tensor, element-wise reciprocal of `x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Reciprocal().symbolic_call(x)\n     return backend.numpy.reciprocal(x)\n@@ -3189,6 +4104,17 @@ class Repeat(Operation):\n \n @keras_core_export([\"keras_core.ops.repeat\", \"keras_core.ops.numpy.repeat\"])\n def repeat(x, repeats, axis=None):\n+    \"\"\"Repeat each element of a tensor after themselves.\n+\n+    Args:\n+        x: Input tensor.\n+        repeats: The number of repetitions for each element.\n+        axis: The axis along which to repeat values. By default, use\n+            the flattened input array, and return a flat output array.\n+\n+    Returns:\n+        Output tensor.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Repeat(repeats, axis=axis).symbolic_call(x)\n     return backend.numpy.repeat(x, repeats, axis=axis)\n@@ -3211,6 +4137,17 @@ class Reshape(Operation):\n \n @keras_core_export([\"keras_core.ops.reshape\", \"keras_core.ops.numpy.reshape\"])\n def reshape(x, new_shape):\n+    \"\"\"Gives a new shape to a tensor without changing its data.\n+\n+    Args:\n+        x: Input tensor.\n+        new_shape: The new shape should be compatible with the original shape.\n+            One shape dimension can be -1 in which case the value is\n+            inferred from the length of the array and remaining dimensions.\n+\n+    Returns:\n+        The reshaped tensor.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Reshape(new_shape).symbolic_call(x)\n     return backend.numpy.reshape(x, new_shape)\n@@ -3231,6 +4168,20 @@ class Roll(Operation):\n \n @keras_core_export([\"keras_core.ops.roll\", \"keras_core.ops.numpy.roll\"])\n def roll(x, shift, axis=None):\n+    \"\"\"Roll tensor elements along a given axis.\n+\n+    Elements that roll beyond the last position are re-introduced at the first.\n+\n+    Args:\n+        x: Input tensor.\n+        shift: The number of places by which elements are shifted.\n+        axis: The axis along which elements are shifted. By default, the\n+            array is flattened before shifting, after which the original\n+            shape is restored.\n+\n+    Returns:\n+        Output tensor.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Roll(shift, axis=axis).symbolic_call(x)\n     return backend.numpy.roll(x, shift, axis=axis)\n@@ -3250,6 +4201,15 @@ class Round(Operation):\n \n @keras_core_export([\"keras_core.ops.round\", \"keras_core.ops.numpy.round\"])\n def round(x, decimals=0):\n+    \"\"\"Evenly round to the given number of decimals.\n+\n+    Args:\n+        x: Input tensor.\n+        decimals: Number of decimal places to round to. Default is 0.\n+\n+    Returns:\n+        Output tensor.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Round(decimals).symbolic_call(x)\n     return backend.numpy.round(x, decimals)\n@@ -3265,6 +4225,14 @@ class Sign(Operation):\n \n @keras_core_export([\"keras_core.ops.sign\", \"keras_core.ops.numpy.sign\"])\n def sign(x):\n+    \"\"\"Returns a tensor with the signs of the elements of `x`.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output tensor of same shape as `x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Sign().symbolic_call(x)\n     return backend.numpy.sign(x)\n@@ -3280,6 +4248,14 @@ class Sin(Operation):\n \n @keras_core_export([\"keras_core.ops.sin\", \"keras_core.ops.numpy.sin\"])\n def sin(x):\n+    \"\"\"Trigonomeric sine, element-wise.\n+\n+    Arguments:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output tensor of same shape as `x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Sin().symbolic_call(x)\n     return backend.numpy.sin(x)\n@@ -3301,7 +4277,7 @@ def sinh(x):\n         x: Input tensor.\n \n     Returns:\n-        Output tensor of same shape as x.\n+        Output tensor of same shape as `x`.\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Sinh().symbolic_call(x)\n@@ -3318,6 +4294,14 @@ class Size(Operation):\n \n @keras_core_export([\"keras_core.ops.size\", \"keras_core.ops.numpy.size\"])\n def size(x):\n+    \"\"\"Return the number of elements in a tensor.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        Number of elements in `x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Size().symbolic_call(x)\n     return backend.numpy.size(x)\n@@ -3337,6 +4321,16 @@ class Sort(Operation):\n \n @keras_core_export([\"keras_core.ops.sort\", \"keras_core.ops.numpy.sort\"])\n def sort(x, axis=-1):\n+    \"\"\"Sorts the elements of `x` along a given axis in ascending order.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: Axis along which to sort. If `None`, the tensor is flattened\n+            before sorting. Default is the last axis.\n+\n+    Returns:\n+        Sorted tensor.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Sort(axis=axis).symbolic_call(x)\n     return backend.numpy.sort(x, axis=axis)\n@@ -3387,6 +4381,26 @@ class Split(Operation):\n \n @keras_core_export([\"keras_core.ops.split\", \"keras_core.ops.numpy.split\"])\n def split(x, indices_or_sections, axis=0):\n+    \"\"\"Split a tensor into chunks.\n+\n+    Args:\n+        x: Input tensor.\n+        indices_or_sections: Either an integer indicating the number of\n+            sections along `axis` or a list of integers indicating the indices\n+            along `axis` at which the tensor is split.\n+        indices_or_sections: If an integer, N, the tensor will be split into N\n+            equal sections along `axis`. If a 1-D array of sorted integers,\n+            the entries indicate indices at which the tensor will be split\n+            along `axis`.\n+        axis: Axis along which to split. Default is 0.\n+\n+    Note:\n+        A split does not have to result in equal division when using\n+        Torch backend.\n+\n+    Returns:\n+        A list of tensors.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Split(indices_or_sections, axis=axis).symbolic_call(x)\n     return backend.numpy.split(x, indices_or_sections, axis=axis)\n@@ -3423,6 +4437,18 @@ class Stack(Operation):\n \n @keras_core_export([\"keras_core.ops.stack\", \"keras_core.ops.numpy.stack\"])\n def stack(x, axis=0):\n+    \"\"\"Join a sequence of tensors along a new axis.\n+\n+    The `axis` parameter specifies the index of the new axis in the\n+    dimensions of the result.\n+\n+    Args:\n+        x: A sequence of tensors.\n+        axis: Axis along which to stack. Default is 0.\n+\n+    Returns:\n+        The stacked tensor.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Stack(axis=axis).symbolic_call(x)\n     return backend.numpy.stack(x, axis=axis)\n@@ -3448,6 +4474,19 @@ class Std(Operation):\n \n @keras_core_export([\"keras_core.ops.std\", \"keras_core.ops.numpy.std\"])\n def std(x, axis=None, keepdims=False):\n+    \"\"\"Compute the standard deviation along the specified axis.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: Axis along which to compute standard deviation.\n+            Default is to compute the standard deviation of the\n+            flattened tensor.\n+        keepdims: If this is set to `True`, the axes which are reduced are left\n+            in the result as dimensions with size one.\n+\n+    Returns:\n+        Output tensor containing the standard deviation values.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Std(axis=axis, keepdims=keepdims).symbolic_call(x)\n     return backend.numpy.std(x, axis=axis, keepdims=keepdims)\n@@ -3473,6 +4512,16 @@ class Swapaxes(Operation):\n \n @keras_core_export([\"keras_core.ops.swapaxes\", \"keras_core.ops.numpy.swapaxes\"])\n def swapaxes(x, axis1, axis2):\n+    \"\"\"Interchange two axes of a tensor.\n+\n+    Args:\n+        x: Input tensor.\n+        axis1: First axis.\n+        axis2: Second axis.\n+\n+    Returns:\n+        A tensor with the axes swapped.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Swapaxes(axis1, axis2).symbolic_call(x)\n     return backend.numpy.swapaxes(x, axis1=axis1, axis2=axis2)\n@@ -3503,6 +4552,17 @@ class Take(Operation):\n \n @keras_core_export([\"keras_core.ops.take\", \"keras_core.ops.numpy.take\"])\n def take(x, indices, axis=None):\n+    \"\"\"Take elements from a tensor along an axis.\n+\n+    Args:\n+        x: Source tensor.\n+        indices: The indices of the values to extract.\n+        axis: The axis over which to select values. By default, the\n+            flattened input tensor is used.\n+\n+    Returns:\n+        The corresponding tensor of values.\n+    \"\"\"\n     if any_symbolic_tensors((x, indices)):\n         return Take(axis=axis).symbolic_call(x, indices)\n     return backend.numpy.take(x, indices, axis=axis)\n@@ -3549,6 +4609,17 @@ class TakeAlongAxis(Operation):\n     ]\n )\n def take_along_axis(x, indices, axis=None):\n+    \"\"\"Select values from `x` at the 1-D `indices` along the given axis.\n+\n+    Args:\n+        x: Source tensor.\n+        indices: The indices of the values to extract.\n+        axis: The axis over which to select values. By default, the flattened\n+            input tensor is used.\n+\n+    Returns:\n+        The corresponding tensor of values.\n+    \"\"\"\n     if any_symbolic_tensors((x, indices)):\n         return TakeAlongAxis(axis=axis).symbolic_call(x, indices)\n     return backend.numpy.take_along_axis(x, indices, axis=axis)\n@@ -3564,6 +4635,14 @@ class Tan(Operation):\n \n @keras_core_export([\"keras_core.ops.tan\", \"keras_core.ops.numpy.tan\"])\n def tan(x):\n+    \"\"\"Compute tangent, element-wise.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output tensor of same shape as `x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Tan().symbolic_call(x)\n     return backend.numpy.tan(x)\n@@ -3585,7 +4664,7 @@ def tanh(x):\n         x: Input tensor.\n \n     Returns:\n-        Output tensor of same shape as x.\n+        Output tensor of same shape as `x`.\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Tanh().symbolic_call(x)\n@@ -3637,6 +4716,21 @@ class Tensordot(Operation):\n     [\"keras_core.ops.tensordot\", \"keras_core.ops.numpy.tensordot\"]\n )\n def tensordot(x1, x2, axes=2):\n+    \"\"\"Compute the tensor dot product along specified axes.\n+\n+    Args:\n+        x1: First tensor.\n+        x2: Second tensor.\n+        axes: - If an integer, N, sum over the last N axes of `x1` and the\n+                first N axes of `x2` in order. The sizes of the corresponding\n+                axes must match.\n+              - Or, a list of axes to be summed over, first sequence applying\n+                to `x1`, second to `x2`. Both sequences must be of the\n+                same length.\n+\n+    Returns:\n+        The tensor dot product of the inputs.\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return Tensordot(axes=axes).symbolic_call(x1, x2)\n     return backend.numpy.tensordot(x1, x2, axes=axes)\n@@ -3669,6 +4763,23 @@ class Tile(Operation):\n \n @keras_core_export([\"keras_core.ops.tile\", \"keras_core.ops.numpy.tile\"])\n def tile(x, repeats):\n+    \"\"\"Repeat `x` the number of times given by `repeats`.\n+\n+    If `repeats` has length `d`, the result will have dimension of\n+    `max(d, x.ndim)`.\n+\n+    If `x.ndim < d`, `x` is promoted to be d-dimensional by prepending\n+    new axes.\n+\n+    If `x.ndim > d`, `repeats` is promoted to `x.ndim` by prepending 1's to it.\n+\n+    Args:\n+        x: Input tensor.\n+        repeats: The number of repetitions of `x` along each axis.\n+\n+    Returns:\n+        The tiled output tensor.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Tile(\n             repeats,\n@@ -3698,6 +4809,32 @@ class Trace(Operation):\n \n @keras_core_export([\"keras_core.ops.trace\", \"keras_core.ops.numpy.trace\"])\n def trace(x, offset=0, axis1=0, axis2=1):\n+    \"\"\"Return the sum along diagonals of the tensor.\n+\n+    If `x` is 2-D, the sum along its diagonal with the given offset is\n+    returned, i.e., the sum of elements `x[i, i+offset]` for all `i`.\n+\n+    If a has more than two dimensions, then the axes specified by `axis1`\n+    and `axis2` are used to determine the 2-D sub-arrays whose traces are\n+    returned.\n+\n+    The shape of the resulting tensor is the same as that of `x` with `axis1`\n+    and `axis2` removed.\n+\n+    Args:\n+        x: Input tensor.\n+        offset: Offset of the diagonal from the main diagonal. Can be\n+            both positive and negative. Defaults to 0.\n+        axis1: Axis to be used as the first axis of the 2-D sub-arrays.\n+            Defaults to 0 (first axis).\n+        axis2: Axis to be used as the second axis of the 2-D sub-arrays.\n+            Defaults to 1 (second axis).\n+\n+    Returns:\n+        If `x` is 2-D, the sum of the diagonal is returned. If `x` has\n+        larger dimensions, then a tensor of sums along diagonals is\n+        returned.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Trace(offset, axis1, axis2).symbolic_call(x)\n     return backend.numpy.trace(x, offset=offset, axis1=axis1, axis2=axis2)\n@@ -3715,6 +4852,20 @@ class Tri(Operation):\n \n @keras_core_export([\"keras_core.ops.tri\", \"keras_core.ops.numpy.tri\"])\n def tri(N, M=None, k=0, dtype=\"float32\"):\n+    \"\"\"Return a tensor with ones at and below a diagonal and zeros elsewhere.\n+\n+    Args:\n+        N: Number of rows in the tensor.\n+        M: Number of columns in the tensor.\n+        k: The sub-diagonal at and below which the array is filled.\n+            `k = 0` is the main diagonal, while `k < 0` is below it, and\n+            `k > 0` is above. The default is 0.\n+        dtype: Data type of the returned tensor. The default is \"float32\".\n+\n+    Returns:\n+        Tensor with its lower triangle filled with ones and zeros elsewhere.\n+        `T[i, j] == 1` for `j <= i + k`, 0 otherwise.\n+    \"\"\"\n     return backend.numpy.tri(N, M=M, k=k, dtype=dtype)\n \n \n@@ -3732,6 +4883,19 @@ class Tril(Operation):\n \n @keras_core_export([\"keras_core.ops.tril\", \"keras_core.ops.numpy.tril\"])\n def tril(x, k=0):\n+    \"\"\"Return lower triangle of a tensor.\n+\n+    For tensors with `ndim` exceeding 2, `tril` will apply to the\n+    final two axes.\n+\n+    Args:\n+        x: Input tensor.\n+        k: Diagonal above which to zero elements. Defaults to 0, the\n+            main diagonal. `k < 0` is below it, and `k > 0` is above it.\n+\n+    Returns:\n+        Lower triangle of `x`, of same shape and data type as `x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Tril(k=k).symbolic_call(x)\n     return backend.numpy.tril(x, k=k)\n@@ -3751,6 +4915,19 @@ class Triu(Operation):\n \n @keras_core_export([\"keras_core.ops.triu\", \"keras_core.ops.numpy.triu\"])\n def triu(x, k=0):\n+    \"\"\"Return upper triangle of a tensor.\n+\n+    For tensors with `ndim` exceeding 2, `triu` will apply to the\n+    final two axes.\n+\n+    Args:\n+        x: Input tensor.\n+        k: Diagonal below which to zero elements. Defaults to 0, the\n+            main diagonal. `k < 0` is below it, and `k > 0` is above it.\n+\n+    Returns:\n+        Upper triangle of `x`, of same shape and data type as `x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Triu(k=k).symbolic_call(x)\n     return backend.numpy.triu(x, k=k)\n@@ -3766,6 +4943,21 @@ class Vdot(Operation):\n \n @keras_core_export([\"keras_core.ops.vdot\", \"keras_core.ops.numpy.vdot\"])\n def vdot(x1, x2):\n+    \"\"\"Return the dot product of two vectors.\n+\n+    If the first argument is complex, the complex conjugate of the first\n+    argument is used for the calculation of the dot product.\n+\n+    Multidimensional tensors are flattened before the dot product is taken.\n+\n+    Args:\n+        x1: First input tensor. If complex, its complex conjugate is taken\n+            before calculation of the dot product.\n+        x2: Second input tensor.\n+\n+    Returns:\n+        Output tensor.\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return Vdot().symbolic_call(x1, x2)\n     return backend.numpy.vdot(x1, x2)\n@@ -3797,6 +4989,14 @@ class Vstack(Operation):\n \n @keras_core_export([\"keras_core.ops.vstack\", \"keras_core.ops.numpy.vstack\"])\n def vstack(xs):\n+    \"\"\"Stack tensors in sequence vertically (row wise).\n+\n+    Args:\n+        xs: Sequence of tensors.\n+\n+    Returns:\n+        Tensor formed by stacking the given tensors.\n+    \"\"\"\n     if any_symbolic_tensors((xs,)):\n         return Vstack().symbolic_call(xs)\n     return backend.numpy.vstack(xs)\n@@ -3817,6 +5017,17 @@ class Where(Operation):\n \n @keras_core_export([\"keras_core.ops.where\", \"keras_core.ops.numpy.where\"])\n def where(condition, x1, x2):\n+    \"\"\"Return elements chosen from `x1` or `x2` depending on `condition`.\n+\n+    Args:\n+        condition: Where `True`, yield `x1`, otherwise yield `x2`.\n+        x1: Values from which to choose when `condition` is `True`.\n+        x2: Values from which to choose when `condition` is `False`.\n+\n+    Returns:\n+        A tensor with elements from `x1` where `condition` is `True`, and\n+        elements from `x2` where `condition` is `False`.\n+    \"\"\"\n     if any_symbolic_tensors((condition, x1, x2)):\n         return Where().symbolic_call(condition, x1, x2)\n     return backend.numpy.where(condition, x1, x2)\n@@ -3835,6 +5046,15 @@ class Subtract(Operation):\n \n @keras_core_export([\"keras_core.ops.subtract\", \"keras_core.ops.numpy.subtract\"])\n def subtract(x1, x2):\n+    \"\"\"Subtract arguments element-wise.\n+\n+    Args:\n+        x1: First input tensor.\n+        x2: Second input tensor.\n+\n+    Returns:\n+        Output tensor, element-wise difference of `x1` and `x2`.\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return Subtract().symbolic_call(x1, x2)\n     return backend.numpy.subtract(x1, x2)\n@@ -3853,6 +5073,15 @@ class Multiply(Operation):\n \n @keras_core_export([\"keras_core.ops.multiply\", \"keras_core.ops.numpy.multiply\"])\n def multiply(x1, x2):\n+    \"\"\"Multiply arguments element-wise.\n+\n+    Args:\n+        x1: First input tensor.\n+        x2: Second input tensor.\n+\n+    Returns:\n+        Output tensor, element-wise product of `x1` and `x2`.\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return Multiply().symbolic_call(x1, x2)\n     return backend.numpy.multiply(x1, x2)\n@@ -3871,6 +5100,17 @@ class Divide(Operation):\n \n @keras_core_export([\"keras_core.ops.divide\", \"keras_core.ops.numpy.divide\"])\n def divide(x1, x2):\n+    \"\"\"Divide arguments element-wise.\n+\n+    `keras_core.ops.true_divide` is an alias for this function.\n+\n+    Args:\n+        x1: First input tensor.\n+        x2: Second input tensor.\n+\n+    Returns:\n+        Output tensor, the quotient `x1/x2`, element-wise.\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return Divide().symbolic_call(x1, x2)\n     return backend.numpy.divide(x1, x2)\n@@ -3894,6 +5134,7 @@ class TrueDivide(Operation):\n     ]\n )\n def true_divide(x1, x2):\n+    \"\"\"Alias for `keras_core.ops.divide`.\"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return TrueDivide().symbolic_call(x1, x2)\n     return backend.numpy.true_divide(x1, x2)\n@@ -3912,6 +5153,15 @@ class Power(Operation):\n \n @keras_core_export([\"keras_core.ops.power\", \"keras_core.ops.numpy.power\"])\n def power(x1, x2):\n+    \"\"\"First tensor elements raised to powers from second tensor, element-wise.\n+\n+    Args:\n+        x1: The bases.\n+        x2: The exponents.\n+\n+    Returns:\n+        Output tensor, the bases in `x1` raised to the exponents in `x2`.\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return Power().symbolic_call(x1, x2)\n     return backend.numpy.power(x1, x2)\n@@ -3927,6 +5177,14 @@ class Negative(Operation):\n \n @keras_core_export([\"keras_core.ops.negative\", \"keras_core.ops.numpy.negative\"])\n def negative(x):\n+    \"\"\"Numerical negative, element-wise.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output tensor, `y = -x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Negative().symbolic_call(x)\n     return backend.numpy.negative(x)\n@@ -3942,6 +5200,14 @@ class Square(Operation):\n \n @keras_core_export([\"keras_core.ops.square\", \"keras_core.ops.numpy.square\"])\n def square(x):\n+    \"\"\"Return the element-wise square of the input.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output tensor, the square of `x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Square().symbolic_call(x)\n     return backend.numpy.square(x)\n@@ -3958,6 +5224,14 @@ class Sqrt(Operation):\n \n @keras_core_export([\"keras_core.ops.sqrt\", \"keras_core.ops.numpy.sqrt\"])\n def sqrt(x):\n+    \"\"\"Return the non-negative square root of a tensor, element-wise.\n+\n+    Args:\n+        x: Input tensor.\n+\n+    Returns:\n+        Output tensor, the non-negative square root of `x`.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Sqrt().symbolic_call(x)\n     x = backend.convert_to_tensor(x)\n@@ -3989,6 +5263,16 @@ class Squeeze(Operation):\n \n @keras_core_export([\"keras_core.ops.squeeze\", \"keras_core.ops.numpy.squeeze\"])\n def squeeze(x, axis=None):\n+    \"\"\"Remove axes of length one from `x`.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: Select a subset of the entries of length one in the shape.\n+\n+    Returns:\n+        The input tensor with all or a subset of the dimensions of\n+        length 1 removed.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Squeeze().symbolic_call(x, axis=axis)\n     return backend.numpy.squeeze(x, axis=axis)\n@@ -4022,6 +5306,16 @@ class Transpose(Operation):\n     [\"keras_core.ops.transpose\", \"keras_core.ops.numpy.transpose\"]\n )\n def transpose(x, axes=None):\n+    \"\"\"Returns a tensor with `axes` transposed.\n+\n+    Args:\n+        x: Input tensor.\n+        axes: Sequence of integers. Permutation of the dimensions of `x`.\n+            By default, the order of the axes are reversed.\n+\n+    Returns:\n+        `x` with its axes permuted.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Transpose(axes=axes).symbolic_call(x)\n     return backend.numpy.transpose(x, axes=axes)\n@@ -4047,6 +5341,18 @@ class Mean(Operation):\n \n @keras_core_export([\"keras_core.ops.mean\", \"keras_core.ops.numpy.mean\"])\n def mean(x, axis=None, keepdims=False):\n+    \"\"\"Compute the arithmetic mean along the specified axes.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: Axis or axes along which the means are computed. The default\n+            is to compute the mean of the flattened tensor.\n+        keepdims: If this is set to `True`, the axes which are reduced are left\n+            in the result as dimensions with size one.\n+\n+    Returns:\n+        Output tensor containing the mean values.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Mean(axis=axis, keepdims=keepdims).symbolic_call(x)\n     return backend.numpy.mean(x, axis=axis, keepdims=keepdims)\n@@ -4072,6 +5378,18 @@ class Var(Operation):\n \n @keras_core_export([\"keras_core.ops.var\", \"keras_core.ops.numpy.var\"])\n def var(x, axis=None, keepdims=False):\n+    \"\"\"Compute the variance along the specified axes.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: Axis or axes along which the variance is computed. The default\n+            is to compute the variance of the flattened tensor.\n+        keepdims: If this is set to `True`, the axes which are reduced are left\n+            in the result as dimensions with size one.\n+\n+    Returns:\n+        Output tensor containing the variance.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Var(axis=axis, keepdims=keepdims).symbolic_call(x)\n     return backend.numpy.var(x, axis=axis, keepdims=keepdims)\n@@ -4097,6 +5415,18 @@ class Sum(Operation):\n \n @keras_core_export([\"keras_core.ops.sum\", \"keras_core.ops.numpy.sum\"])\n def sum(x, axis=None, keepdims=False):\n+    \"\"\"Sum of a tensor over the given axes.\n+\n+    Args:\n+        x: Input tensor.\n+        axis: Axis or axes along which the sum is computed. The default is to\n+            compute the sum of the flattened tensor.\n+        keepdims: If this is set to `True`, the axes which are reduced are left\n+            in the result as dimensions with size one.\n+\n+    Returns:\n+        Output tensor containing the sum.\n+    \"\"\"\n     if any_symbolic_tensors((x,)):\n         return Sum(axis=axis, keepdims=keepdims).symbolic_call(x)\n     return backend.numpy.sum(x, axis=axis, keepdims=keepdims)\n@@ -4112,6 +5442,15 @@ class Zeros(Operation):\n \n @keras_core_export([\"keras_core.ops.zeros\", \"keras_core.ops.numpy.zeros\"])\n def zeros(shape, dtype=\"float32\"):\n+    \"\"\"Return a new tensor of given shape and type, filled with zeros.\n+\n+    Args:\n+        shape: Shape of the new tensor.\n+        dtype: Desired data type of the tensor.\n+\n+    Returns:\n+        Tensor of zeros with the given shape and dtype.\n+    \"\"\"\n     return backend.numpy.zeros(shape, dtype=dtype)\n \n \n@@ -4125,6 +5464,15 @@ class Ones(Operation):\n \n @keras_core_export([\"keras_core.ops.ones\", \"keras_core.ops.numpy.ones\"])\n def ones(shape, dtype=\"float32\"):\n+    \"\"\"Return a new tensor of given shape and type, filled with ones.\n+\n+    Args:\n+        shape: Shape of the new tensor.\n+        dtype: Desired data type of the tensor.\n+\n+    Returns:\n+        Tensor of ones with the given shape and dtype.\n+    \"\"\"\n     return backend.numpy.ones(shape, dtype=dtype)\n \n \n@@ -4140,6 +5488,19 @@ class Eye(Operation):\n \n @keras_core_export([\"keras_core.ops.eye\", \"keras_core.ops.numpy.eye\"])\n def eye(N, M=None, k=0, dtype=\"float32\"):\n+    \"\"\"Return a 2-D tensor with ones on the diagonal and zeros elsewhere.\n+\n+    Args:\n+        N: Number of rows in the output.\n+        M: Number of columns in the output. If `None`, defaults to `N`.\n+        k: Index of the diagonal: 0 (the default) refers to the main\n+            diagonal, a positive value refers to an upper diagonal,\n+            and a negative value to a lower diagonal.\n+        dtype: Data type of the returned tensor.\n+\n+    Returns:\n+        Tensor with ones on the k-th diagonal and zeros elsewhere.\n+    \"\"\"\n     return backend.numpy.eye(N, M=M, k=k, dtype=dtype)\n \n \n@@ -4158,6 +5519,15 @@ class FloorDivide(Operation):\n     [\"keras_core.ops.floor_divide\", \"keras_core.ops.numpy.floor_divide\"]\n )\n def floor_divide(x1, x2):\n+    \"\"\"Returns the largest integer smaller or equal to the division of inputs.\n+\n+    Args:\n+        x1: Numerator.\n+        x2: Denominator.\n+\n+    Returns:\n+        Output tensor, `y = floor(x1/x2)`\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return FloorDivide().symbolic_call(x1, x2)\n     return backend.numpy.floor_divide(x1, x2)\n@@ -4178,6 +5548,15 @@ class LogicalXor(Operation):\n     [\"keras_core.ops.logical_xor\", \"keras_core.ops.numpy.logical_xor\"]\n )\n def logical_xor(x1, x2):\n+    \"\"\"Compute the truth value of `x1 XOR x2`, element-wise.\n+\n+    Args:\n+        x1: First input tensor.\n+        x2: Second input tensor.\n+\n+    Returns:\n+        Output boolean tensor.\n+    \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n         return LogicalXor().symbolic_call(x1, x2)\n     return backend.numpy.logical_xor(x1, x2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
