{"custom_id": "keras#8cabbae5ba7fe93eacbfeba7530ca94fee74ad45", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 451 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 17 | Complexity Δ (Sum/Max): 42/42 | Churn Δ: 451 | Churn Cumulative: 451 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.8487972508591065\n\nDIFF:\n@@ -0,0 +1,451 @@\n+# Copyright 2019 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests that Custom Training Loop docs match actual behavior.\n+\n+The tutorial at https://www.tensorflow.org/tutorials/distribute/custom_training,\n+defined at\n+https://github.com/tensorflow/docs/blob/master/site/en/tutorials/distribute/custom_training.ipynb\n+makes several statements about\n+\n+  * ways to reduce loss terms to the actual training loss, and\n+  * how they compare to the built-in behavior of Keras Model.fit().\n+\n+This test verifies that these statements match the actual behavior,\n+under a variety of distribution strategies.\n+\"\"\"\n+\n+import numpy as np\n+import tensorflow.compat.v2 as tf\n+from absl.testing import parameterized\n+\n+from keras.distribute import strategy_combinations\n+\n+\n+def make_compute_loss_fn(variant, loss_object, GLOBAL_BATCH_SIZE):\n+    \"\"\"Returns the `compute_loss()` function as defined in the tutorial.\"\"\"\n+\n+    if variant == \"basic\":\n+        # The basic form of the loss function, shown verbatim in the tutorial.\n+        def compute_loss(labels, predictions, model_losses):\n+            per_example_loss = loss_object(labels, predictions)\n+            loss = tf.nn.compute_average_loss(per_example_loss)\n+            if model_losses:\n+                loss += tf.nn.scale_regularization_loss(tf.add_n(model_losses))\n+            return loss\n+\n+    elif variant == \"fixed_batch_size\":\n+        # The variant that adds a fixed `global_batch_size=` arg\n+        # (described but not shown verbatim).\n+        def compute_loss(labels, predictions, model_losses):\n+            per_example_loss = loss_object(labels, predictions)\n+            loss = tf.nn.compute_average_loss(\n+                per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE\n+            )\n+            if model_losses:\n+                loss += tf.nn.scale_regularization_loss(tf.add_n(model_losses))\n+            return loss\n+\n+    elif variant == \"balanced\":\n+        # The variant that scales the loss to balance out varying batch sizes\n+        # (described but not shown verbatim).\n+        def compute_loss(labels, predictions, model_losses):\n+            per_example_loss = loss_object(labels, predictions)\n+            loss = tf.nn.compute_average_loss(per_example_loss)\n+            if model_losses:\n+                loss += tf.nn.scale_regularization_loss(tf.add_n(model_losses))\n+            observed_global_batch_size = (\n+                tf.distribute.get_strategy().num_replicas_in_sync\n+                * tf.shape(per_example_loss)[0]\n+            )\n+            loss *= tf.math.divide(\n+                tf.cast(observed_global_batch_size, tf.float32),\n+                tf.cast(GLOBAL_BATCH_SIZE, tf.float32),\n+            )\n+            return loss\n+\n+    else:\n+        raise ValueError(f\"Unknown {variant=}\")\n+\n+    return compute_loss\n+\n+\n+def create_dataset(global_batch_size):\n+    \"\"\"Creates the dataset for ImpliedExampleWeightsTest.\n+\n+    It contains two batches: the first has full size, the second just 1 element.\n+    The i-th element `(x,y)` has model input `x = onehot(i)` and label `y = 0`.\n+    \"\"\"\n+    n = global_batch_size + 1\n+    ds = tf.data.Dataset.from_tensor_slices((tf.eye(n), tf.zeros([n, 1])))\n+    ds = ds.batch(global_batch_size)\n+    return ds\n+\n+\n+def create_model(n):\n+    \"\"\"Creates the model for ImpliedExampleWeightsTest.\n+\n+    The model has three trainable weights of interest, all initialized to 1.0:\n+\n+      * \"predicting/kernel:0\" of shape [n, 1] maps a one-hot encoded input to\n+        the model output. When used with the MeanAbsoluteError loss, an input\n+        onehot(i) produces a gradient onehot(i) for this weight, subject to\n+        the training loop's loss reduction across examples.\n+      * \"activity_regularized/kernel:0\" of shape [n, 1] has an activity\n+        regularizer loss in the model so that input onehot(i) produces a\n+        gradient of 1/batch_size * onehot(i) for this weight.\n+      * \"weight_regularized:0\" of shape [1] has a weight regularizer loss in\n+        the model that produces a gradient of 1 for this weight, independent\n+        of batch size.\n+    \"\"\"\n+    inputs = tf.keras.Input(shape=(n,), name=\"inputs\")\n+\n+    predicting = tf.keras.layers.Dense(\n+        1, use_bias=False, kernel_initializer=\"ones\", name=\"predicting\"\n+    )\n+    activity_regularized = tf.keras.layers.Dense(\n+        1,\n+        use_bias=False,\n+        kernel_initializer=\"ones\",\n+        activity_regularizer=tf.keras.regularizers.L1(l1=1.0),\n+        name=\"activity_regularized\",\n+    )\n+    weight_regularized = tf.keras.layers.Dense(\n+        1,\n+        kernel_initializer=\"zeros\",\n+        bias_initializer=\"ones\",\n+        bias_regularizer=tf.keras.regularizers.L1(l1=1.0),\n+        name=\"weight_regularized\",\n+    )\n+\n+    # Make outputs = predicting(inputs), depending on the other Layers as well.\n+    add = tf.keras.layers.Add(name=\"add\")\n+    multiply = tf.keras.layers.Multiply(name=\"multiply\")\n+    outputs = add(\n+        [\n+            predicting(inputs),\n+            multiply(\n+                [np.array([[0.0]], np.float32), activity_regularized(inputs)]\n+            ),\n+            multiply(\n+                [np.array([[0.0]], np.float32), weight_regularized(inputs)]\n+            ),\n+        ]\n+    )\n+\n+    model = tf.keras.Model(inputs, outputs)\n+    return model\n+\n+\n+def create_loss(**kwargs):\n+    \"\"\"Returns the loss to be used with the model from create_model().\"\"\"\n+    return tf.keras.losses.MeanAbsoluteError(**kwargs)\n+\n+\n+def create_optimizer(learning_rate):\n+    \"\"\"Returns the optimizer that applies gradients in the most obvious way.\"\"\"\n+    return tf.keras.optimizers.SGD(learning_rate)\n+\n+\n+def get_expected_example_weights(\n+    ctl_variant, *, local_batch_size, num_replicas_in_sync\n+):\n+    \"\"\"Returns the weights that examples have in the gradient updates seen.\"\"\"\n+\n+    global_batch_size = local_batch_size * num_replicas_in_sync\n+    n = global_batch_size + 1\n+    num_batches = 2\n+\n+    expected = dict(\n+        # Examples in a full batch receive the expected gradient weight,\n+        # independent of the CTL variant.\n+        example_prediction_fullbatch=1.0,\n+        example_activity_fullbatch=1.0,\n+    )\n+    if ctl_variant == \"basic\":\n+        # In the basic variant of the CTL, when a batch of size 1 hits a\n+        # replica, the singleton example receives the weight that is\n+        # normally spread evenly across the local_batch_size.\n+        expected[\"example_prediction_singleton\"] = local_batch_size\n+        expected[\"example_activity_singleton\"] = local_batch_size\n+        # Weight regularization applies equally in each batch,\n+        # irrespective of its size.\n+        expected[\"total_weight_regularization\"] = num_batches\n+    elif ctl_variant == \"fixed_batch_size\":\n+        # In the CTL variant that fixes GLOBAL_BATCH_SIZE for the reduction\n+        # of prediction losses, the weight of a singleton example is\n+        # reverted to normal for prediction, but activity and weight\n+        # regularization behaves as in the \"basic\" variant.\n+        expected[\"example_prediction_singleton\"] = 1.0\n+        expected[\"example_activity_singleton\"] = local_batch_size\n+        expected[\"total_weight_regularization\"] = num_batches\n+    elif ctl_variant == \"balanced\":\n+        # The CTL variant that corrects both prediction and regularization\n+        # losses for the batch size achieves equal weights of examples\n+        # both for the prediction and for an activity regularizer\n+        expected[\"example_prediction_singleton\"] = 1.0\n+        expected[\"example_activity_singleton\"] = 1.0\n+        # Weight regularization, in sync with the other loss terms,\n+        # applies proportional to the number of examples.\n+        expected[\"total_weight_regularization\"] = n / global_batch_size\n+    return expected\n+\n+\n+class MaybeStrategyScope:\n+    \"\"\"Provides a context allowing no distribution strategy.\"\"\"\n+\n+    def __init__(self, strategy):\n+        self._strategy = strategy\n+        self._scope = None\n+\n+    def __enter__(self):\n+        if self._strategy:\n+            self._scope = self._strategy.scope()\n+            self._scope.__enter__()\n+\n+    def __exit__(self, exc_type, value, traceback):\n+        if self._strategy:\n+            self._scope.__exit__(exc_type, value, traceback)\n+            self._scope = None\n+\n+\n+class ImpliedExampleWeightsTest(tf.test.TestCase, parameterized.TestCase):\n+    \"\"\"Tests weights of loss terms depending on batch size and training loop.\"\"\"\n+\n+    @tf.__internal__.distribute.combinations.generate(\n+        tf.__internal__.test.combinations.combine(\n+            strategy=strategy_combinations.all_strategies\n+            + strategy_combinations.multiworker_strategies\n+            + [None],\n+            ctl_variant=[\"basic\", \"fixed_batch_size\", \"balanced\"],\n+        )\n+    )\n+    def test_ctl(self, strategy, ctl_variant):\n+        \"\"\"Tests a variant of the CTL under a distribution strategy.\"\"\"\n+        if strategy is None:\n+            num_replicas_in_sync = 1\n+        else:\n+            num_replicas_in_sync = strategy.num_replicas_in_sync\n+\n+        local_batch_size = 2  # For a full batch; greater than 1.\n+        global_batch_size = local_batch_size * num_replicas_in_sync\n+        ds = create_dataset(global_batch_size)\n+        if strategy is not None:\n+            ds = strategy.experimental_distribute_dataset(ds)\n+\n+        n = global_batch_size + 1\n+        learning_rate = 0.01\n+        with MaybeStrategyScope(strategy):\n+            model = create_model(n)\n+            loss_object = create_loss(reduction=tf.keras.losses.Reduction.NONE)\n+            compute_loss = make_compute_loss_fn(\n+                ctl_variant, loss_object, global_batch_size\n+            )\n+            optimizer = create_optimizer(learning_rate)\n+\n+            def train_step(inputs):\n+                x, labels = inputs\n+                with tf.GradientTape() as tape:\n+                    predictions = model(x, training=True)\n+                    loss = compute_loss(labels, predictions, model.losses)\n+                gradients = tape.gradient(loss, model.trainable_variables)\n+                optimizer.apply_gradients(\n+                    zip(gradients, model.trainable_variables)\n+                )\n+                return loss\n+\n+            @tf.function\n+            def wrapped_train_step(inputs):\n+                if strategy is None:\n+                    return train_step(inputs)\n+                else:\n+                    per_replica_losses = strategy.run(\n+                        train_step, args=(inputs,)\n+                    )\n+                    return strategy.reduce(\n+                        tf.distribute.ReduceOp.SUM,\n+                        per_replica_losses,\n+                        axis=None,\n+                    )\n+\n+            num_epochs = 1\n+            num_batches = 0\n+            for epoch in range(num_epochs):\n+                total_loss = 0.0\n+                for x in ds:\n+                    total_loss += wrapped_train_step(x)\n+                    num_batches += 1\n+                train_loss = total_loss / num_batches\n+                self.assertTrue(tf.math.is_finite(train_loss).numpy())\n+\n+        self.assertEqual(num_batches, 2)\n+\n+        expected = get_expected_example_weights(\n+            ctl_variant,\n+            local_batch_size=local_batch_size,\n+            num_replicas_in_sync=num_replicas_in_sync,\n+        )\n+        self.assert_implied_example_weights(\n+            model,\n+            **expected,\n+            rtol=1e-6 if strategy is None else 1e-4,\n+            learning_rate=learning_rate,\n+            global_batch_size=global_batch_size,\n+        )\n+\n+    @tf.__internal__.distribute.combinations.generate(\n+        tf.__internal__.test.combinations.combine(\n+            strategy=strategy_combinations.all_strategies\n+            + strategy_combinations.multiworker_strategies\n+            + [None],\n+        )\n+    )\n+    def test_fit(self, strategy):\n+        \"\"\"Tests Model.fit().\"\"\"\n+        if strategy is None:\n+            num_replicas_in_sync = 1\n+        else:\n+            num_replicas_in_sync = strategy.num_replicas_in_sync\n+\n+        local_batch_size = 2  # For a full batch; greater than 1.\n+        global_batch_size = local_batch_size * num_replicas_in_sync\n+        ds = create_dataset(global_batch_size)\n+\n+        n = global_batch_size + 1\n+        learning_rate = 0.01\n+        with MaybeStrategyScope(strategy):\n+            model = create_model(n)\n+            model.compile(\n+                optimizer=create_optimizer(learning_rate), loss=create_loss()\n+            )\n+        epochs = 1\n+        steps_per_epoch = 2\n+        model.fit(ds, epochs=epochs, steps_per_epoch=steps_per_epoch)\n+\n+        expected = get_expected_example_weights(\n+            ctl_variant=\"basic\",  # The tutorial claims this consistency!\n+            local_batch_size=local_batch_size,\n+            num_replicas_in_sync=num_replicas_in_sync,\n+        )\n+        self.assert_implied_example_weights(\n+            model,\n+            **expected,\n+            rtol=1e-6 if strategy is None else 1e-4,\n+            learning_rate=learning_rate,\n+            global_batch_size=global_batch_size,\n+        )\n+\n+    def assert_implied_example_weights(\n+        self,\n+        model,\n+        *,\n+        learning_rate,\n+        global_batch_size,\n+        rtol,\n+        example_prediction_fullbatch,\n+        example_prediction_singleton,\n+        example_activity_fullbatch,\n+        example_activity_singleton,\n+        total_weight_regularization,\n+    ):\n+        \"\"\"Checks model.weights for the expected effects of training.\"\"\"\n+        model_weights = {\n+            v.name: self._get_var_value(v).numpy()\n+            for v in model.trainable_variables\n+        }\n+\n+        # The total weight received by each one-hot example in the prediction\n+        # loss is the change of its corresponding weight from the initial\n+        # value 1, adjusted for the expected averaging by global_batch_size and\n+        # scaling by SGD's learning_rate.\n+        predicting_kernel = model_weights[\"predicting/kernel:0\"]\n+        example_prediction_weights = (\n+            (1.0 - predicting_kernel) / learning_rate * global_batch_size\n+        )\n+        # There was one full batch of examples, followed by a singleton.\n+        self.assertEqual(predicting_kernel.shape, (global_batch_size + 1, 1))\n+        # Check the examples in the full batch.\n+        actual_example_prediction_fullbatch = self.reduce_assert_equal(\n+            example_prediction_weights[:-1, 0]\n+        )\n+        self.assertAllClose(\n+            example_prediction_fullbatch,\n+            actual_example_prediction_fullbatch,\n+            rtol=rtol,\n+        )\n+        # Check the singleton example after the full batch.\n+        actual_example_prediction_singleton = example_prediction_weights[-1, 0]\n+        self.assertAllClose(\n+            example_prediction_singleton,\n+            actual_example_prediction_singleton,\n+            rtol=rtol,\n+        )\n+\n+        # Analogous to predictions, check weights for acticity regularization.\n+        activity_regularized_kernel = model_weights[\n+            \"activity_regularized/kernel:0\"\n+        ]\n+        example_activity_weights = (\n+            (1.0 - activity_regularized_kernel)\n+            / learning_rate\n+            * global_batch_size\n+        )\n+        self.assertEqual(\n+            activity_regularized_kernel.shape, (global_batch_size + 1, 1)\n+        )\n+        actual_example_activity_fullbatch = self.reduce_assert_equal(\n+            example_activity_weights[:-1, 0]\n+        )\n+        self.assertAllClose(\n+            example_activity_fullbatch,\n+            actual_example_activity_fullbatch,\n+            rtol=rtol,\n+        )\n+        actual_example_activity_singleton = example_activity_weights[-1, 0]\n+        self.assertAllClose(\n+            example_activity_singleton,\n+            actual_example_activity_singleton,\n+            rtol=rtol,\n+        )\n+\n+        # The total weight of weight regularization is the change of this\n+        # (otherwise unused) bias term from its initial value 1,\n+        # adjusted for the expected scaling by SGD's learning_rate.\n+        actual_total_weight_reguarization = (\n+            1.0 - model_weights[\"weight_regularized/bias:0\"][0]\n+        ) / learning_rate\n+        self.assertAllClose(\n+            total_weight_regularization,\n+            actual_total_weight_reguarization,\n+            rtol=rtol,\n+        )\n+\n+    def reduce_assert_equal(self, x):\n+        \"\"\"Returns first element of x and asserts all others are equal.\"\"\"\n+        result = x[0]\n+        for i, value in enumerate(x[1:]):\n+            self.assertAllEqual(result, value, msg=f\"at position {i=}\")\n+        return result\n+\n+    def _get_var_value(self, var):\n+        \"\"\"Returns the (unique) value of a (possibly distributed) Variable.\"\"\"\n+        if hasattr(var, \"values\"):  # Distributed.\n+            result = self.reduce_assert_equal([v.value() for v in var.values])\n+        else:\n+            result = var.value()\n+        return result\n+\n+\n+if __name__ == \"__main__\":\n+    tf.__internal__.distribute.multi_process_runner.test_main()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bf0bd94515d9e3c1e4778e30866e45a8ee20ca48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 80 | Lines Deleted: 5 | Files Changed: 3 | Hunks: 9 | Methods Changed: 5 | Complexity Δ (Sum/Max): 8/4 | Churn Δ: 85 | Churn Cumulative: 1870 | Contributors (this commit): 8 | Commits (past 90d): 61 | Contributors (cumulative): 14 | DMM Complexity: 0.9285714285714286\n\nDIFF:\n@@ -38,7 +38,7 @@ class Loss:\n                 lambda x: ops.convert_to_tensor(x, dtype=dtype), y_pred\n             )\n             y_true = nest.map_structure(\n-                lambda x: ops.convert_to_tensor(x, dtype=y_pred.dtype), y_true\n+                lambda x: ops.convert_to_tensor(x, dtype=dtype), y_true\n             )\n \n             losses = self.call(y_true, y_pred)\n\n@@ -407,13 +407,22 @@ class CompileLoss(losses_module.Loss):\n         loss_weights = self._user_loss_weights\n         flat_losses = []\n         flat_loss_weights = []\n-        if num_outputs == 1:\n-            if not is_function_like(loss):\n+\n+        if num_outputs == 1 and not is_function_like(loss):\n             raise ValueError(\n                 \"When there is only a single output, the `loss` argument \"\n                 \"must be a callable. \"\n                 f\"Received instead:\\nloss={loss} of type {type(loss)}\"\n             )\n+\n+        if is_function_like(loss) and nest.is_nested(y_pred):\n+            # The model has multiple outputs but only one loss fn\n+            # was provided. Broadcast loss to all outputs.\n+            loss = nest.map_structure(lambda x: loss, y_pred)\n+\n+        # Iterate over all possible loss formats:\n+        # plain function, list/tuple, dict\n+        if is_function_like(loss):\n             flat_losses.append(get_loss(loss, y_true, y_pred))\n             if loss_weights:\n                 if not isinstance(loss_weights, float):\n@@ -500,7 +509,7 @@ class CompileLoss(losses_module.Loss):\n             for name, yt, yp in zip(output_names, y_true, y_pred):\n                 if name in loss:\n                     if loss[name]:\n-                        flat_losses.append(get_metric(loss[name], yt, yp))\n+                        flat_losses.append(get_loss(loss[name], yt, yp))\n                     else:\n                         flat_losses.append(None)\n                 else:\n@@ -533,6 +542,8 @@ class CompileLoss(losses_module.Loss):\n                         flat_loss_weights.append(loss_weights[name])\n                     else:\n                         flat_loss_weights.append(1.0)\n+            else:\n+                flat_loss_weights = [1.0 for _ in flat_losses]\n         self.flat_losses = flat_losses\n         self.flat_loss_weights = flat_loss_weights\n         self.built = True\n@@ -544,6 +555,7 @@ class CompileLoss(losses_module.Loss):\n         y_true = nest.flatten(y_true)\n         y_pred = nest.flatten(y_pred)\n         loss_values = []\n+\n         for loss, y_t, y_p, w in zip(\n             self.flat_losses, y_true, y_pred, self.flat_loss_weights\n         ):\n\n@@ -1,4 +1,5 @@\n import numpy as np\n+from absl.testing import parameterized\n \n from keras_core import backend\n from keras_core import metrics as losses_module\n@@ -194,7 +195,7 @@ class TestCompileMetrics(testing.TestCase):\n         self.assertAllClose(result[\"accuracy\"], 0.333333)\n \n \n-class TestCompileLoss(testing.TestCase):\n+class TestCompileLoss(testing.TestCase, parameterized.TestCase):\n     def test_single_output_case(self):\n         compile_loss = CompileLoss(\n             loss=losses_module.MeanSquaredError(),\n@@ -210,3 +211,65 @@ class TestCompileLoss(testing.TestCase):\n         compile_loss.build(y_true, y_pred)\n         value = compile_loss(y_true, y_pred)\n         self.assertAllClose(value, 0.068333, atol=1e-5)\n+\n+    @parameterized.parameters(True, False)\n+    def test_list_output_case(self, broadcast):\n+        if broadcast:\n+            # Test broadcasting single loss to all outputs\n+            compile_loss = CompileLoss(\n+                loss=\"mse\",\n+            )\n+        else:\n+            compile_loss = CompileLoss(\n+                loss=[\"mse\", \"mse\"],\n+            )\n+        # Test symbolic build\n+        y_true = [backend.KerasTensor((3, 4)), backend.KerasTensor((3, 4))]\n+        y_pred = [backend.KerasTensor((3, 4)), backend.KerasTensor((3, 4))]\n+        compile_loss.build(y_true, y_pred)\n+        # Test eager build\n+        y_true = [\n+            np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]),\n+            np.array([[0.7, 0.8], [0.9, 1.0], [1.1, 1.2]]),\n+        ]\n+        y_pred = [\n+            np.array([[1.2, 1.1], [1.0, 0.9], [0.8, 0.7]]),\n+            np.array([[0.6, 0.5], [0.4, 0.3], [0.2, 0.1]]),\n+        ]\n+        compile_loss.build(y_true, y_pred)\n+        value = compile_loss(y_true, y_pred)\n+        self.assertAllClose(value, 0.953333, atol=1e-5)\n+\n+    @parameterized.parameters(True, False)\n+    def test_dict_output_case(self, broadcast):\n+        if broadcast:\n+            # Test broadcasting single loss to all outputs\n+            compile_loss = CompileLoss(\n+                loss=\"mse\",\n+            )\n+        else:\n+            compile_loss = CompileLoss(\n+                loss={\"a\": \"mse\", \"b\": \"mse\"},\n+            )\n+        # Test symbolic build\n+        y_true = {\n+            \"a\": backend.KerasTensor((3, 4)),\n+            \"b\": backend.KerasTensor((3, 4)),\n+        }\n+        y_pred = {\n+            \"a\": backend.KerasTensor((3, 4)),\n+            \"b\": backend.KerasTensor((3, 4)),\n+        }\n+        compile_loss.build(y_true, y_pred)\n+        # Test eager build\n+        y_true = {\n+            \"a\": np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]),\n+            \"b\": np.array([[0.7, 0.8], [0.9, 1.0], [1.1, 1.2]]),\n+        }\n+        y_pred = {\n+            \"a\": np.array([[1.2, 1.1], [1.0, 0.9], [0.8, 0.7]]),\n+            \"b\": np.array([[0.6, 0.5], [0.4, 0.3], [0.2, 0.1]]),\n+        }\n+        compile_loss.build(y_true, y_pred)\n+        value = compile_loss(y_true, y_pred)\n+        self.assertAllClose(value, 0.953333, atol=1e-5)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#41f66af80cfc2b245d80c6ac7bcec4fc02a701e7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 1161 | Contributors (this commit): 5 | Commits (past 90d): 16 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -82,8 +82,10 @@ def format_layer_shape(layer):\n             lambda x: format_shape(x.shape), outputs\n         )\n     if len(output_shapes) == 1:\n-        output_shapes = output_shapes[0]\n-    return str(output_shapes)\n+        return output_shapes[0]\n+    out = str(output_shapes)\n+    out = out.replace(\"'\", \"\")\n+    return out\n \n \n def print_summary(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#586c1d9f7bf28ee16149da94f73d62c70d8b2cd8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 8 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 652 | Contributors (this commit): 1 | Commits (past 90d): 3 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -49,6 +49,14 @@ ATTR_SKIPLIST = frozenset(\n         \"variables\",\n         \"non_trainable_variables\",\n         \"trainable_variables\",\n+        # TF trackable attrs\n+        \"_unconditional_checkpoint_dependencies\",\n+        \"_unconditional_dependency_names\",\n+        \"_checkpoint_dependencies\",\n+        \"_deferred_dependencies\",\n+        \"_deserialization_dependencies\",\n+        \"_lookup_dependency\",\n+        \"_self_unconditional_dependency_names\",\n     }\n )\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#83636fc1917654564a7eff6e58edf52bb24e318d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 14 | Lines Deleted: 2 | Files Changed: 7 | Hunks: 12 | Methods Changed: 8 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 16 | Churn Cumulative: 3371 | Contributors (this commit): 7 | Commits (past 90d): 53 | Contributors (cumulative): 24 | DMM Complexity: 0.7777777777777778\n\nDIFF:\n@@ -416,6 +416,7 @@ def mish(x):\n \n     - [Misra, 2019](https://arxiv.org/abs/1908.08681)\n     \"\"\"\n+    x = backend.convert_to_tensor(x)\n     return Mish.static_call(x)\n \n \n\n@@ -58,7 +58,7 @@ def logsumexp(x, axis=None, keepdims=False):\n         max_x = torch.max(x)\n         return torch.log(torch.sum(torch.exp(x - max_x))) + max_x\n \n-    max_x = torch.max(x, dim=axis, keepdim=True).values\n+    max_x = torch.amax(x, dim=axis, keepdim=True)\n     result = (\n         torch.log(torch.sum(torch.exp(x - max_x), dim=axis, keepdim=True))\n         + max_x\n\n@@ -134,6 +134,8 @@ def append(\n \n def arange(start, stop=None, step=1, dtype=None):\n     dtype = to_torch_dtype(dtype)\n+    if stop is None:\n+        return torch.arange(end=start, dtype=dtype)\n     return torch.arange(start, stop, step=step, dtype=dtype)\n \n \n\n@@ -1,5 +1,6 @@\n import numpy as np\n \n+from keras_core import backend\n from keras_core import constraints\n from keras_core import testing\n \n@@ -35,12 +36,14 @@ class ConstraintsTest(testing.TestCase):\n     def test_unit_norm(self):\n         constraint_fn = constraints.UnitNorm()\n         output = constraint_fn(get_example_array())\n+        output = backend.convert_to_numpy(output)\n         l2 = np.sqrt(np.sum(np.square(output), axis=0))\n         self.assertAllClose(l2, 1.0)\n \n     def test_min_max_norm(self):\n         constraint_fn = constraints.MinMaxNorm(min_value=0.2, max_value=0.5)\n         output = constraint_fn(get_example_array())\n+        output = backend.convert_to_numpy(output)\n         l2 = np.sqrt(np.sum(np.square(output), axis=0))\n         self.assertFalse(l2[l2 < 0.2])\n         self.assertFalse(l2[l2 > 0.5 + 1e-6])\n\n@@ -68,6 +68,7 @@ class NormalizationTest(testing.TestCase, parameterized.TestCase):\n         layer.adapt(data)\n         self.assertTrue(layer.built)\n         output = layer(x)\n+        output = backend.convert_to_numpy(output)\n         self.assertAllClose(np.var(output, axis=0), 1.0, atol=1e-5)\n         self.assertAllClose(np.mean(output, axis=0), 0.0, atol=1e-5)\n \n@@ -84,6 +85,7 @@ class NormalizationTest(testing.TestCase, parameterized.TestCase):\n         layer.adapt(data)\n         self.assertTrue(layer.built)\n         output = layer(x)\n+        output = backend.convert_to_numpy(output)\n         self.assertAllClose(np.var(output, axis=(0, 3)), 1.0, atol=1e-5)\n         self.assertAllClose(np.mean(output, axis=(0, 3)), 0.0, atol=1e-5)\n \n\n@@ -1,6 +1,7 @@\n import numpy as np\n import tensorflow as tf\n \n+from keras_core import backend\n from keras_core import layers\n from keras_core import testing\n \n@@ -36,6 +37,7 @@ class RandomBrightnessTest(testing.TestCase):\n         inputs = np.random.randint(0, 255, size=(224, 224, 3))\n         output = layer(inputs)\n         diff = output - inputs\n+        diff = backend.convert_to_numpy(diff)\n         self.assertTrue(np.amin(diff) >= 0)\n         self.assertTrue(np.mean(diff) > 0)\n \n@@ -45,6 +47,7 @@ class RandomBrightnessTest(testing.TestCase):\n         inputs = np.random.randint(0, 255, size=(224, 224, 3))\n         output = layer(inputs)\n         diff = output - inputs\n+        diff = backend.convert_to_numpy(diff)\n         self.assertTrue(np.amax(diff) <= 0)\n         self.assertTrue(np.mean(diff) < 0)\n \n\n@@ -9,7 +9,8 @@ from keras_core.operations import operation\n \n class OpWithMultipleInputs(operation.Operation):\n     def call(self, x, y, z=None):\n-        return x + 2 * y + 3 * z\n+        return 3 * z + x + 2 * y\n+        # Order of operations issue with torch backend\n \n     def compute_output_spec(self, x, y, z=None):\n         return keras_tensor.KerasTensor(x.shape, x.dtype)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#8183eee4abf2420380006475cdd7f7735309395b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 11 | Files Changed: 3 | Hunks: 5 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 23 | Churn Cumulative: 1958 | Contributors (this commit): 7 | Commits (past 90d): 63 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -430,18 +430,11 @@ def standardize_shape(\n                 f\"Cannot convert '{shape}' to a shape. \"\n                 f\"Found invalid entry '{e}'. \"\n             )\n-            if allow_dynamic_batch_size:\n+            if not allow_dynamic_batch_size and e is None:\n                 msg += (\n                     \"Dynamic shapes (shapes with `None` entries) \"\n-                    f\"are not allowed with the {config.backend()}, \"\n-                    \"except for the batch size (axis 0).\"\n-                )\n-            else:\n-                msg += (\n-                    \"Dynamic shapes (shapes with `None` entries) \"\n-                    f\"are not allowed with the {config.backend()}. \"\n-                    \"All dimensions should be positive integers, \"\n-                    \"including the batch size (axis 0).\"\n+                    f\"are not allowed with the {config.backend()} \"\n+                    \"backend.\"\n                 )\n             raise ValueError(msg)\n         if e < 0:\n\n@@ -612,6 +612,10 @@ def get_temp_dir():\n     return temp_dir\n \n \n+# def get_attr_skiplist_for_layers():\n+#     if global_state.get_shared_attribute(\"saving_attr_skiplist_last\", None) is None:\n+\n+\n def _is_keras_trackable(obj):\n     return isinstance(\n         obj,\n\n@@ -12,10 +12,14 @@ from keras_core.callbacks.callback import Callback\n \n if backend.backend() == \"jax\":\n     from keras_core.backend.jax.trainer import JAXTrainer as Trainer\n-else:\n+elif backend.backend() == \"torch\":\n+    from keras_core.backend.torch.trainer import TorchTrainer as Trainer\n+elif backend.backend() == \"tensorflow\":\n     from keras_core.backend.tensorflow.trainer import (\n         TensorFlowTrainer as Trainer,\n     )\n+else:\n+    raise ImportError(f\"Invalid backend: {backend.backend()}\")\n \n \n # A model is just a layer mixed in with a Trainer.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2b10523385b524983b98e3d1138083cb7a0b4214", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 4 | Churn Cumulative: 660 | Contributors (this commit): 1 | Commits (past 90d): 5 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -612,10 +612,6 @@ def get_temp_dir():\n     return temp_dir\n \n \n-# def get_attr_skiplist_for_layers():\n-#     if global_state.get_shared_attribute(\"saving_attr_skiplist_last\", None) is None:\n-\n-\n def _is_keras_trackable(obj):\n     return isinstance(\n         obj,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d5d679b0a95d8b4c717c1e9ac5027a028d9d1887", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 13 | Files Changed: 2 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): -2/3 | Churn Δ: 37 | Churn Cumulative: 1666 | Contributors (this commit): 10 | Commits (past 90d): 84 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -989,19 +989,9 @@ class Layer(BackendLayer, Operation):\n \n     def _build_by_run_for_single_pos_arg(self, input_shape):\n         # Case: all inputs are in the first arg (possibly nested).\n-        if is_shape_tuple(input_shape):\n-            input_shape = tuple(input_shape)\n-        if isinstance(input_shape, list):\n-            input_tensors = [\n-                backend.KerasTensor(shape) for shape in input_shape\n-            ]\n-        elif isinstance(input_shape, dict):\n-            input_tensors = {\n-                k: backend.KerasTensor(shape)\n-                for k, shape in input_shape.items()\n-            }\n-        else:\n-            input_tensors = backend.KerasTensor(input_shape)\n+        input_tensors = map_shape_structure(\n+            lambda s: backend.KerasTensor(s), input_shape\n+        )\n         try:\n             backend.compute_output_spec(self.call, input_tensors)\n             return True\n\n@@ -1,3 +1,5 @@\n+import numpy as np\n+\n from keras_core import layers\n from keras_core import testing\n from keras_core.layers.core.input_layer import Input\n@@ -25,3 +27,22 @@ class ModelTest(testing.TestCase):\n         json_string = model.to_json()\n         new_model = model_from_json(json_string)\n         self.assertEqual(json_string, new_model.to_json())\n+\n+    def test_tuple_input_model_subclass(self):\n+        # https://github.com/keras-team/keras-core/issues/324\n+\n+        class MultiInputModel(Model):\n+            def __init__(self, **kwargs):\n+                super().__init__(**kwargs)\n+                self.dense1 = layers.Dense(4)\n+\n+            def call(self, inputs):\n+                a, b = inputs\n+                r = self.dense1(a)\n+                return layers.concatenate([r, b])\n+\n+        model = MultiInputModel()\n+        x1 = np.random.rand(3, 3)\n+        x2 = np.random.rand(3, 2)\n+        out = model((x1, x2))\n+        self.assertEqual(out.shape, (3, 6))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2aea261d7f9f5786d2a3f1a634fb86cf13a61465", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 9 | Files Changed: 4 | Hunks: 8 | Methods Changed: 10 | Complexity Δ (Sum/Max): 288/220 | Churn Δ: 33 | Churn Cumulative: 9553 | Contributors (this commit): 20 | Commits (past 90d): 11 | Contributors (cumulative): 35 | DMM Complexity: 0.9230769230769231\n\nDIFF:\n@@ -224,7 +224,7 @@ class MeanTest(test_combinations.TestCase):\n         @tf.function\n         def reset_in_fn():\n             m.reset_state()\n-            return m.update_state(100)\n+            m.update_state(100)\n \n         for _ in range(5):\n             self.evaluate(reset_in_fn())\n\n@@ -2465,10 +2465,14 @@ class ResetStatesTest(test_combinations.TestCase):\n         @tf.function\n         def reset_in_fn():\n             m.reset_state()\n-            return m.update_state(100)\n+            m.update_state(100)\n \n         for _ in range(5):\n-            self.evaluate(reset_in_fn())\n+            reset_in_fn()\n+            if not tf.executing_eagerly():\n+                self.evaluate(\n+                    tf.compat.v1.get_default_graph().get_operations()[-1]\n+                )\n         self.assertEqual(self.evaluate(m.count), 1)\n \n \n\n@@ -445,7 +445,7 @@ class NonFusedAdam(optimizer_v2.OptimizerV2):\n         super().set_weights(weights)\n \n     @tf.function(jit_compile=True)\n-    def _resource_apply_dense(self, grad, var, apply_state=None):\n+    def _resource_apply_dense_impl(self, grad, var, apply_state):\n         var_device, var_dtype = var.device, var.dtype.base_dtype\n         coefficients = (apply_state or {}).get(\n             (var_device, var_dtype)\n@@ -467,8 +467,13 @@ class NonFusedAdam(optimizer_v2.OptimizerV2):\n             v = vhat\n         var.assign_sub((m * alpha) / (tf.sqrt(v) + coefficients[\"epsilon\"]))\n \n+    def _resource_apply_dense(self, grad, var, apply_state=None):\n+        self._resource_apply_dense_impl(grad, var, apply_state)\n+        if not tf.executing_eagerly():\n+            return tf.compat.v1.get_default_graph().get_operations()[-1]\n+\n     @tf.function(jit_compile=True)\n-    def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n+    def _resource_apply_sparse_impl(self, grad, var, indices, apply_state):\n         var_device, var_dtype = var.device, var.dtype.base_dtype\n         coefficients = (apply_state or {}).get(\n             (var_device, var_dtype)\n@@ -499,6 +504,11 @@ class NonFusedAdam(optimizer_v2.OptimizerV2):\n                 / (tf.sqrt(v_hat) + coefficients[\"epsilon\"])\n             )\n \n+    def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n+        self._resource_apply_sparse_impl(grad, var, indices, apply_state)\n+        if not tf.executing_eagerly():\n+            return tf.compat.v1.get_default_graph().get_operations()[-1]\n+\n     def get_config(self):\n         config = super().get_config()\n         config.update(\n\n@@ -74,10 +74,11 @@ def update_state_wrapper(update_state_fn):\n                 )\n \n         with tf_utils.graph_context_for_symbolic_tensors(*args, **kwargs):\n-            update_op = update_state_fn(*args, **kwargs)\n-        if update_op is not None:  # update_op will be None in eager execution.\n-            metric_obj.add_update(update_op)\n-        return update_op\n+            result = update_state_fn(*args, **kwargs)\n+        if not tf.executing_eagerly():\n+            result = tf.compat.v1.get_default_graph().get_operations()[-1]\n+            metric_obj.add_update(result)\n+        return result\n \n     return tf.__internal__.decorator.make_decorator(update_state_fn, decorated)\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#3f6c2e9bccdef66d7e7b6d837011c4477d9d3aa6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 32 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 10 | Methods Changed: 5 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 34 | Churn Cumulative: 2659 | Contributors (this commit): 9 | Commits (past 90d): 67 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -8,6 +8,7 @@ from keras_core import optimizers as optimizers_module\n from keras_core.trainers import trainer as base_trainer\n from keras_core.trainers.data_adapters import data_adapter_utils\n from keras_core.trainers.epoch_iterator import EpochIterator\n+from keras_core.utils import traceback_utils\n \n \n class JAXTrainer(base_trainer.Trainer):\n@@ -227,8 +228,10 @@ class JAXTrainer(base_trainer.Trainer):\n             kwargs = {}\n             if self._call_has_training_arg():\n                 kwargs[\"training\"] = False\n+\n+            x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data[0])\n             outputs, _ = self.stateless_call(\n-                trainable_variables, non_trainable_variables, data[0], **kwargs\n+                trainable_variables, non_trainable_variables, x, **kwargs\n             )\n             return outputs\n \n@@ -271,6 +274,7 @@ class JAXTrainer(base_trainer.Trainer):\n         else:\n             self.predict_function = predict_step\n \n+    @traceback_utils.filter_traceback\n     def fit(\n         self,\n         x=None,\n@@ -447,6 +451,7 @@ class JAXTrainer(base_trainer.Trainer):\n         self._jax_state = None\n         return self.history\n \n+    @traceback_utils.filter_traceback\n     def evaluate(\n         self,\n         x=None,\n@@ -540,6 +545,7 @@ class JAXTrainer(base_trainer.Trainer):\n             return logs\n         return self._flatten_metrics_in_order(logs)\n \n+    @traceback_utils.filter_traceback\n     def predict(\n         self, x, batch_size=None, verbose=\"auto\", steps=None, callbacks=None\n     ):\n@@ -556,8 +562,9 @@ class JAXTrainer(base_trainer.Trainer):\n             # Build the model on one batch of data.\n             for _, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n                 # Build model\n+                x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data[0])\n                 with backend.StatelessScope():\n-                    self(data[0])\n+                    self(x)\n                 break\n \n         # Container that configures and calls callbacks.\n\n@@ -1,6 +1,7 @@\n import numpy as np\n from absl.testing import parameterized\n \n+import keras_core\n from keras_core import backend\n from keras_core import initializers\n from keras_core import layers\n@@ -319,3 +320,25 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         # With class weights\n         logs = model.train_on_batch(x, y, class_weight={1: 0.3, 0: 0.2})\n         self.assertAlmostEqual(logs[0], 12.899)\n+\n+    def test_nested_input_predict(self):\n+        # https://github.com/keras-team/keras-core/issues/325\n+\n+        class TupleInputModel(keras_core.Model):\n+            def call(self, inputs):\n+                a, b = inputs\n+                return a + b\n+\n+        model = TupleInputModel()\n+        x1, x2 = np.random.rand(2, 3, 4)\n+        out = model.predict((x1, x2))\n+        self.assertEqual(out.shape, (3, 4))\n+\n+        class DictInputModel(keras_core.Model):\n+            def call(self, inputs):\n+                return inputs[\"a\"] + inputs[\"b\"]\n+\n+        model = DictInputModel()\n+        x1, x2 = np.random.rand(2, 3, 4)\n+        out = model.predict({\"a\": x1, \"b\": x2})\n+        self.assertEqual(out.shape, (3, 4))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e3c556f6df54e66cceac4737c748c4616c590ad8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 4 | Methods Changed: 4 | Complexity Δ (Sum/Max): 5/4 | Churn Δ: 24 | Churn Cumulative: 2161 | Contributors (this commit): 8 | Commits (past 90d): 53 | Contributors (cumulative): 12 | DMM Complexity: 0.9047619047619048\n\nDIFF:\n@@ -152,6 +152,9 @@ class Functional(Function, Model):\n \n         self._layers = self.layers\n         self.built = True\n+        # We will convert directly (to the correct dtype per input).\n+        self._convert_input_args = False\n+        self._allow_non_tensor_positional_args = True\n         self._post_build()\n \n     @property\n@@ -232,6 +235,16 @@ class Functional(Function, Model):\n         # Otherwise both ref inputs and inputs will already be in same order.\n         return nest.flatten(inputs)\n \n+    def _convert_inputs_to_tensors(self, flat_inputs):\n+        flat_dtypes = [x.dtype for x in self._inputs]\n+        converted = []\n+        for x, dtype in zip(flat_inputs, flat_dtypes):\n+            if backend.is_tensor(x):\n+                converted.append(backend.cast(x, dtype=dtype))\n+            else:\n+                converted.append(backend.convert_to_tensor(x, dtype=dtype))\n+        return converted\n+\n     def _adjust_input_rank(self, flat_inputs):\n         flat_ref_shapes = [x.shape for x in self._inputs]\n         adjusted = []\n@@ -263,6 +276,7 @@ class Functional(Function, Model):\n \n     def _standardize_inputs(self, inputs):\n         flat_inputs = self._flatten_to_reference_inputs(inputs)\n+        flat_inputs = self._convert_inputs_to_tensors(flat_inputs)\n         return self._adjust_input_rank(flat_inputs)\n \n     @property\n\n@@ -171,6 +171,16 @@ class FunctionalTest(testing.TestCase):\n         out_val = model(np.random.random((2, 3)))\n         self.assertEqual(out_val.shape, (2, 3, 3))\n \n+    def test_dtype_standardization(self):\n+        float_input = Input(shape=(2,), dtype=\"float16\")\n+        int_input = Input(shape=(2,), dtype=\"int32\")\n+        float_output = float_input + 2\n+        int_output = int_input + 2\n+        model = Functional((float_input, int_input), (float_output, int_output))\n+        float_data, int_data = model((np.ones((2, 2)), np.ones((2, 2))))\n+        self.assertEqual(backend.standardize_dtype(float_data.dtype), \"float16\")\n+        self.assertEqual(backend.standardize_dtype(int_data.dtype), \"int32\")\n+\n     def test_serialization(self):\n         # Test basic model\n         inputs = Input(shape=(3,), batch_size=2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#d579f815cd2819ce63f095f1d1d68ab84809df69", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 262 | Lines Deleted: 34 | Files Changed: 3 | Hunks: 16 | Methods Changed: 18 | Complexity Δ (Sum/Max): 19/16 | Churn Δ: 296 | Churn Cumulative: 1919 | Contributors (this commit): 6 | Commits (past 90d): 54 | Contributors (cumulative): 9 | DMM Complexity: 0.9895833333333334\n\nDIFF:\n@@ -29,6 +29,11 @@ history = model.fit(\n model.evaluate(x, y, verbose=0)\n model.predict(x, verbose=0)\n \n+# Test on batch functions\n+model.train_on_batch(x, y)\n+model.test_on_batch(x, y)\n+model.predict_on_batch(x)\n+\n # Test functional model.\n inputs = keras_core.Input(shape=(32, 32, 3))\n outputs = layers.Conv2D(filters=10, kernel_size=3)(inputs)\n\n@@ -18,9 +18,16 @@ class TorchTrainer(base_trainer.Trainer):\n         self.test_function = None\n         self.predict_function = None\n \n-    def train_step(self, data):\n+    def make_train_function(self, force=False):\n+        if self.train_function is not None and not force:\n+            return self.train_function\n+\n+        def one_step_on_data(data):\n+            \"\"\"Runs a single training step on a batch of data.\"\"\"\n             data = data[0]\n-        x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(data)\n+            x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(\n+                data\n+            )\n \n             # Compute prediction error\n             if self._call_has_training_arg():\n@@ -28,8 +35,8 @@ class TorchTrainer(base_trainer.Trainer):\n             else:\n                 y_pred = self(x)\n \n-        # Call torch.nn.Module.zero_grad() to clear the leftover gradients for\n-        # the weights from the previous train step.\n+            # Call torch.nn.Module.zero_grad() to clear the leftover gradients\n+            # for the weights from the previous train step.\n             self.zero_grad()\n \n             loss = self.compute_loss(\n@@ -42,8 +49,8 @@ class TorchTrainer(base_trainer.Trainer):\n                 # Backpropagation\n                 trainable_weights = [v for v in self.trainable_weights]\n \n-            # Call torch.Tensor.backward() on the loss to compute gradients for\n-            # the weights.\n+                # Call torch.Tensor.backward() on the loss to compute gradients\n+                # for the weights.\n                 loss.backward()\n \n                 gradients = [v.value.grad for v in trainable_weights]\n@@ -56,7 +63,55 @@ class TorchTrainer(base_trainer.Trainer):\n             else:\n                 warnings.warn(\"The model does not have any trainable weights.\")\n \n-        return self.compute_metrics(x, y, y_pred, sample_weight=sample_weight)\n+            return self.compute_metrics(\n+                x, y, y_pred, sample_weight=sample_weight\n+            )\n+\n+        self.train_function = one_step_on_data\n+\n+    def make_test_function(self, force=False):\n+        if self.test_function is not None and not force:\n+            return self.test_function\n+\n+        def one_step_on_data(data):\n+            \"\"\"Runs a single test step on a batch of data.\"\"\"\n+            with torch.no_grad():\n+                data = data[0]\n+                (\n+                    x,\n+                    y,\n+                    sample_weight,\n+                ) = data_adapter_utils.unpack_x_y_sample_weight(data)\n+                if self._call_has_training_arg():\n+                    y_pred = self(x, training=False)\n+                else:\n+                    y_pred = self(x)\n+                loss = self.compute_loss(\n+                    x=x, y=y, y_pred=y_pred, sample_weight=sample_weight\n+                )\n+                self._loss_tracker.update_state(loss)\n+                return self.compute_metrics(\n+                    x, y, y_pred, sample_weight=sample_weight\n+                )\n+\n+        self.test_function = one_step_on_data\n+\n+    def make_predict_function(self, force=False):\n+        if self.predict_function is not None and not force:\n+            return self.predict_function\n+\n+        def one_step_on_data(data):\n+            \"\"\"Runs a predict test step on a batch of data.\"\"\"\n+            with torch.no_grad():\n+                data = data[0]\n+                x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data)\n+                if self._call_has_training_arg():\n+                    y_pred = self(x, training=False)\n+                else:\n+                    y_pred = self(x)\n+                return y_pred\n+\n+        self.predict_function = one_step_on_data\n \n     def fit(\n         self,\n@@ -127,6 +182,7 @@ class TorchTrainer(base_trainer.Trainer):\n             )\n \n         self.stop_training = False\n+        self.make_train_function()\n         callbacks.on_train_begin()\n \n         for epoch in range(initial_epoch, epochs):\n@@ -141,7 +197,7 @@ class TorchTrainer(base_trainer.Trainer):\n                 # Callbacks\n                 callbacks.on_train_batch_begin(step)\n \n-                logs = self.train_step(data)\n+                logs = self.train_function(data)\n \n                 # Callbacks\n                 callbacks.on_train_batch_end(step, self._pythonify_logs(logs))\n@@ -197,19 +253,6 @@ class TorchTrainer(base_trainer.Trainer):\n         callbacks.on_train_end(logs=training_logs)\n         return self.history\n \n-    def test_step(self, data):\n-        data = data[0]\n-        x, y, sample_weight = data_adapter_utils.unpack_x_y_sample_weight(data)\n-        if self._call_has_training_arg():\n-            y_pred = self(x, training=False)\n-        else:\n-            y_pred = self(x)\n-        loss = self.compute_loss(\n-            x=x, y=y, y_pred=y_pred, sample_weight=sample_weight\n-        )\n-        self._loss_tracker.update_state(loss)\n-        return self.compute_metrics(x, y, y_pred, sample_weight=sample_weight)\n-\n     def evaluate(\n         self,\n         x=None,\n@@ -256,13 +299,13 @@ class TorchTrainer(base_trainer.Trainer):\n         # Switch the torch Module back to testing mode.\n         self.eval()\n \n+        self.make_test_function()\n         callbacks.on_test_begin()\n         logs = None\n         self.reset_metrics()\n         for step, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n             callbacks.on_test_batch_begin(step)\n-            with torch.no_grad():\n-                logs = self.test_step(data)\n+            logs = self.test_function(data)\n             callbacks.on_test_batch_end(step, self._pythonify_logs(logs))\n         logs = self.get_metrics_result()\n         callbacks.on_test_end(logs)\n@@ -271,15 +314,6 @@ class TorchTrainer(base_trainer.Trainer):\n             return logs\n         return self._flatten_metrics_in_order(logs)\n \n-    def predict_step(self, data):\n-        data = data[0]\n-        x, _, _ = data_adapter_utils.unpack_x_y_sample_weight(data)\n-        if self._call_has_training_arg():\n-            y_pred = self(x, training=False)\n-        else:\n-            y_pred = self(x)\n-        return y_pred\n-\n     def predict(\n         self, x, batch_size=None, verbose=\"auto\", steps=None, callbacks=None\n     ):\n@@ -322,15 +356,126 @@ class TorchTrainer(base_trainer.Trainer):\n         # Switch the torch Module back to testing mode.\n         self.eval()\n \n+        self.make_predict_function()\n         callbacks.on_predict_begin()\n         outputs = None\n         for step, data in epoch_iterator.enumerate_epoch(return_type=\"np\"):\n             callbacks.on_predict_batch_begin(step)\n-            with torch.no_grad():\n-                batch_outputs = self.predict_step(data)\n+            batch_outputs = self.predict_function(data)\n             outputs = append_to_outputs(batch_outputs, outputs)\n             callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n         callbacks.on_predict_end()\n         return tf.__internal__.nest.map_structure_up_to(\n             batch_outputs, np.concatenate, outputs\n         )\n+\n+    def train_on_batch(\n+        self,\n+        x,\n+        y=None,\n+        sample_weight=None,\n+        class_weight=None,\n+        return_dict=False,\n+    ):\n+        \"\"\"Runs a single gradient update on a single batch of data.\n+\n+        Args:\n+            x: Input data. Must be array-like.\n+            y: Target data. Must be array-like.\n+            sample_weight: Optional array of the same length as x, containing\n+                weights to apply to the model's loss for each sample.\n+                In the case of temporal data, you can pass a 2D array\n+                with shape `(samples, sequence_length)`, to apply a different\n+                weight to every timestep of every sample.\n+            class_weight: Optional dictionary mapping class indices (integers)\n+                to a weight (float) to apply to the model's loss for the samples\n+                from this class during training. This can be useful to tell the\n+                model to \"pay more attention\" to samples from an\n+                under-represented class. When `class_weight` is specified\n+                and targets have a rank of 2 or greater, either `y` must\n+                be one-hot encoded, or an explicit final dimension of 1\n+                must be included for sparse class labels.\n+            return_dict: If `True`, loss and metric results are returned as a\n+                dict, with each key being the name of the metric. If `False`,\n+                they are returned as a list.\n+\n+        Returns:\n+            A scalar loss value (when no metrics and `return_dict=False`),\n+            a list of loss and metric values\n+            (if there are metrics and `return_dict=False`), or a dict of\n+            metric and loss values (if `return_dict=True`).\n+        \"\"\"\n+        self._assert_compile_called(\"train_on_batch\")\n+        self.make_train_function()\n+        if class_weight is not None:\n+            if sample_weight is not None:\n+                raise ValueError(\n+                    \"Arguments `sample_weight` and `class_weight` \"\n+                    \"cannot be specified at the same time. \"\n+                    f\"Received: sample_weight={sample_weight}, \"\n+                    f\"class_weight={class_weight}\"\n+                )\n+            sample_weight = data_adapter_utils.class_weight_to_sample_weights(\n+                y, class_weight\n+            )\n+\n+        data = (x, y, sample_weight)\n+        logs = self.train_function([data])\n+        logs = tf.nest.map_structure(lambda x: np.array(x), logs)\n+        if return_dict:\n+            return logs\n+        return self._flatten_metrics_in_order(logs)\n+\n+    def test_on_batch(\n+        self,\n+        x,\n+        y=None,\n+        sample_weight=None,\n+        return_dict=False,\n+    ):\n+        \"\"\"Test the model on a single batch of samples.\n+\n+        Args:\n+            x: Input data. Must be array-like.\n+            y: Target data. Must be array-like.\n+            sample_weight: Optional array of the same length as x, containing\n+                weights to apply to the model's loss for each sample.\n+                In the case of temporal data, you can pass a 2D array\n+                with shape `(samples, sequence_length)`, to apply a different\n+                weight to every timestep of every sample.\n+            return_dict: If `True`, loss and metric results are returned as a\n+                dict, with each key being the name of the metric. If `False`,\n+                they are returned as a list.\n+\n+        Returns:\n+            A scalar loss value (when no metrics and `return_dict=False`),\n+            a list of loss and metric values\n+            (if there are metrics and `return_dict=False`), or a dict of\n+            metric and loss values (if `return_dict=True`).\n+        \"\"\"\n+        self._assert_compile_called(\"test_on_batch\")\n+        self.make_test_function()\n+\n+        data = (x, y, sample_weight)\n+\n+        logs = self.test_function([data])\n+        logs = tf.nest.map_structure(lambda x: np.array(x), logs)\n+        if return_dict:\n+            return logs\n+        return self._flatten_metrics_in_order(logs)\n+\n+    def predict_on_batch(self, x):\n+        \"\"\"Returns predictions for a single batch of samples.\n+\n+        Args:\n+            x: Input data. It must be array-like.\n+\n+        Returns:\n+            NumPy array(s) of predictions.\n+        \"\"\"\n+        self.make_predict_function()\n+        batch_outputs = self.predict_function((x,))\n+        batch_outputs = tf.nest.map_structure(\n+            lambda x: np.array(x), batch_outputs\n+        )\n+        return batch_outputs\n\n@@ -277,6 +277,84 @@ class Trainer:\n     ):\n         raise NotImplementedError\n \n+    def train_on_batch(\n+        self,\n+        x,\n+        y=None,\n+        sample_weight=None,\n+        class_weight=None,\n+        return_dict=False,\n+    ):\n+        \"\"\"Runs a single gradient update on a single batch of data.\n+\n+        Args:\n+            x: Input data. Must be array-like.\n+            y: Target data. Must be array-like.\n+            sample_weight: Optional array of the same length as x, containing\n+                weights to apply to the model's loss for each sample.\n+                In the case of temporal data, you can pass a 2D array\n+                with shape `(samples, sequence_length)`, to apply a different\n+                weight to every timestep of every sample.\n+            class_weight: Optional dictionary mapping class indices (integers)\n+                to a weight (float) to apply to the model's loss for the samples\n+                from this class during training. This can be useful to tell the\n+                model to \"pay more attention\" to samples from an\n+                under-represented class. When `class_weight` is specified\n+                and targets have a rank of 2 or greater, either `y` must\n+                be one-hot encoded, or an explicit final dimension of 1\n+                must be included for sparse class labels.\n+            return_dict: If `True`, loss and metric results are returned as a\n+                dict, with each key being the name of the metric. If `False`,\n+                they are returned as a list.\n+\n+        Returns:\n+            A scalar loss value (when no metrics and `return_dict=False`),\n+            a list of loss and metric values\n+            (if there are metrics and `return_dict=False`), or a dict of\n+            metric and loss values (if `return_dict=True`).\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def test_on_batch(\n+        self,\n+        x,\n+        y=None,\n+        sample_weight=None,\n+        return_dict=False,\n+    ):\n+        \"\"\"Test the model on a single batch of samples.\n+\n+        Args:\n+            x: Input data. Must be array-like.\n+            y: Target data. Must be array-like.\n+            sample_weight: Optional array of the same length as x, containing\n+                weights to apply to the model's loss for each sample.\n+                In the case of temporal data, you can pass a 2D array\n+                with shape `(samples, sequence_length)`, to apply a different\n+                weight to every timestep of every sample.\n+            return_dict: If `True`, loss and metric results are returned as a\n+                dict, with each key being the name of the metric. If `False`,\n+                they are returned as a list.\n+\n+        Returns:\n+            A scalar loss value (when no metrics and `return_dict=False`),\n+            a list of loss and metric values\n+            (if there are metrics and `return_dict=False`), or a dict of\n+            metric and loss values (if `return_dict=True`).\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def predict_on_batch(self, x):\n+        \"\"\"Returns predictions for a single batch of samples.\n+\n+        Args:\n+            x: Input data. It must be array-like.\n+\n+        Returns:\n+            NumPy array(s) of predictions.\n+        \"\"\"\n+        raise NotImplementedError\n+\n     def get_compile_config(self):\n         \"\"\"Returns a serialized config with information for compiling the model.\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
