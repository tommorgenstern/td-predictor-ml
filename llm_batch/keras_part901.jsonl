{"custom_id": "keras#08ad93b77488f911328da6a676f04709b6e9f788", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 118 | Lines Deleted: 21 | Files Changed: 3 | Hunks: 7 | Methods Changed: 7 | Complexity Δ (Sum/Max): 2/4 | Churn Δ: 139 | Churn Cumulative: 194 | Contributors (this commit): 5 | Commits (past 90d): 5 | Contributors (cumulative): 10 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1346,25 +1346,6 @@ class AUC(Metric):\n         if not self._built:\n             self._build(y_pred.shape)\n \n-        if self.multi_label or (self.label_weights is not None):\n-            # y_true should have shape (number of examples, number of labels).\n-            shapes = [(y_true, (\"N\", \"L\"))]\n-            if self.multi_label:\n-                # TP, TN, FP, and FN should all have shape\n-                # (number of thresholds, number of labels).\n-                shapes.extend(\n-                    [\n-                        (self.true_positives, (\"T\", \"L\")),\n-                        (self.true_negatives, (\"T\", \"L\")),\n-                        (self.false_positives, (\"T\", \"L\")),\n-                        (self.false_negatives, (\"T\", \"L\")),\n-                    ]\n-                )\n-            if self.label_weights is not None:\n-                # label_weights should be of length equal to the number of\n-                # labels.\n-                shapes.append((self.label_weights, (\"L\",)))\n-\n         # Only forward label_weights to update_confusion_matrix_variables when\n         # multi_label is False. Otherwise the averaging of individual label AUCs\n         # is handled in AUC.result\n@@ -1500,13 +1481,53 @@ class AUC(Metric):\n             )\n             x = fp_rate\n             y = recall\n-        else:  # curve == 'PR'.\n+        elif self.curve == metrics_utils.AUCCurve.PR:  # curve == 'PR'.\n             precision = ops.divide_no_nan(\n                 self.true_positives,\n                 ops.add(self.true_positives, self.false_positives),\n             )\n             x = recall\n             y = precision\n+        else:  # curve == 'PRGAIN'.\n+            # Due to the hyperbolic transform, this formula is less robust than\n+            # ROC and PR values. In particular\n+            # 1) Both measures diverge when there are no negative values;\n+            # 2) Both measures diverge when there are no true positives;\n+            # 3) Recall gain becomes negative when the recall is lower than the\n+            #    label average (i.e. when more negative exampless are\n+            #    classified positive than real positives).\n+            #\n+            # We ignore case 1 as it is easily understood that metrics would be\n+            # badly defined then. For case 2 we set recall_gain to 0 and\n+            # precision_gain to 1. For case 3 we set recall_gain to 0. These\n+            # fixes will result in an overstimation of the AUCfor estimators\n+            # that are anti-correlated with the label (at some threshold).\n+\n+            # The scaling factor $\\frac{P}{N}$ that is used to for mboth gain\n+            # values.\n+            scaling_factor = ops.divide_no_nan(\n+                ops.add(self.true_positives, self.false_negatives),\n+                ops.add(self.true_negatives, self.false_positives),\n+            )\n+\n+            recall_gain = 1.0 - scaling_factor * ops.divide_no_nan(\n+                self.false_negatives, self.true_positives\n+            )\n+            precision_gain = 1.0 - scaling_factor * ops.divide_no_nan(\n+                self.false_positives, self.true_positives\n+            )\n+            # Handle case 2.\n+            recall_gain = ops.where(\n+                ops.equal(self.true_positives, 0.0), 0.0, recall_gain\n+            )\n+            precision_gain = ops.where(\n+                ops.equal(self.true_positives, 0.0), 1.0, precision_gain\n+            )\n+            # Handle case 3.\n+            recall_gain = ops.maximum(recall_gain, 0.0)\n+\n+            x = recall_gain\n+            y = precision_gain\n \n         # Find the rectangle heights based on `summation_method`.\n         if (\n\n@@ -1396,6 +1396,79 @@ class AUCTest(testing.TestCase):\n         # produce all zeros.\n         self.assertAllClose(result, 0.0, 1e-3)\n \n+    def test_weighted_prgain_majoring(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds,\n+            curve=\"PRGAIN\",\n+            summation_method=\"majoring\",\n+        )\n+        result = auc_obj(\n+            self.y_true, self.y_pred, sample_weight=self.sample_weight\n+        )\n+\n+        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n+        # scaling_facor (P/N) = 7/3\n+        # recall_gain = 1 - 7/3 [0/7, 3/4, 7/0] = [1, -3/4, -inf] -> [1, 0, 0]\n+        # precision_gain = 1 - 7/3 [3/7, 0/4, 0/0] = [0, 1, NaN] -> [0, 1, 1]\n+        # heights = [max(0, 1), max(1, 1)] = [1, 1]\n+        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n+        expected_result = 1 * 1 + 0 * 1\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_weighted_prgain_minoring(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds,\n+            curve=\"PRGAIN\",\n+            summation_method=\"minoring\",\n+        )\n+        result = auc_obj(\n+            self.y_true, self.y_pred, sample_weight=self.sample_weight\n+        )\n+\n+        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n+        # scaling_facor (P/N) = 7/3\n+        # recall_gain = 1 - 7/3 [0/7, 3/4, 7/0] = [1, -3/4, -inf] -> [1, 0, 0]\n+        # precision_gain = 1 - 7/3 [3/7, 0/4, 0/0] = [0, 1, NaN] -> [0, 1, 1]\n+        # heights = [min(0, 1), min(1, 1)] = [0, 1]\n+        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n+        expected_result = 1 * 0 + 0 * 1\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_weighted_prgain_interpolation(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, curve=\"PRGAIN\"\n+        )\n+        result = auc_obj(\n+            self.y_true, self.y_pred, sample_weight=self.sample_weight\n+        )\n+\n+        # tp = [7, 4, 0], fp = [3, 0, 0], fn = [0, 3, 7], tn = [0, 3, 3]\n+        # scaling_facor (P/N) = 7/3\n+        # recall_gain = 1 - 7/3 [0/7, 3/4, 7/0] = [1, -3/4, -inf] -> [1, 0, 0]\n+        # precision_gain = 1 - 7/3 [3/7, 0/4, 0/0] = [0, 1, NaN] -> [0, 1, 1]\n+        # heights = [(0+1)/2, (1+1)/2] = [0.5, 1]\n+        # widths = [(1 - 0), (0 - 0)] = [1, 0]\n+        expected_result = 1 * 0.5 + 0 * 1\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n+    def test_prgain_interpolation(self):\n+        auc_obj = metrics.AUC(\n+            num_thresholds=self.num_thresholds, curve=\"PRGAIN\"\n+        )\n+\n+        y_true = np.array([0, 0, 0, 1, 0, 1, 0, 1, 1, 1])\n+        y_pred = np.array([0.1, 0.2, 0.3, 0.3, 0.4, 0.4, 0.6, 0.6, 0.8, 0.9])\n+        result = auc_obj(y_true, y_pred)\n+\n+        # tp = [5, 3, 0], fp = [5, 1, 0], fn = [0, 2, 5], tn = [0, 4, 4]\n+        # scaling_facor (P/N) = 5/5 = 1\n+        # recall_gain = 1 - [0/5, 2/3, 5/0] = [1, 1/3, -inf] -> [1, 1/3, 0]\n+        # precision_gain = 1 - [5/5, 1/3, 0/0] = [1, 1/3, NaN] -> [0, 2/3, 1]\n+        # heights = [(0+2/3)/2, (2/3+1)/2] = [0.333333, 0.833333]\n+        # widths = [(1 - 1/3), (1/3 - 0)] = [0.666666, 0.333333]\n+        expected_result = 0.666666 * 0.333333 + 0.333333 * 0.833333\n+        self.assertAllClose(result, expected_result, 1e-3)\n+\n     def test_invalid_num_thresholds(self):\n         with self.assertRaisesRegex(\n             ValueError, \"Argument `num_thresholds` must be an integer > 1\"\n\n@@ -43,6 +43,7 @@ class AUCCurve(Enum):\n \n     ROC = \"ROC\"\n     PR = \"PR\"\n+    PRGAIN = \"PRGAIN\"\n \n     @staticmethod\n     def from_str(key):\n@@ -50,10 +51,12 @@ class AUCCurve(Enum):\n             return AUCCurve.PR\n         elif key in (\"roc\", \"ROC\"):\n             return AUCCurve.ROC\n+        elif key in (\"prgain\", \"PRGAIN\"):\n+            return AUCCurve.PRGAIN\n         else:\n             raise ValueError(\n                 f'Invalid AUC curve value: \"{key}\". '\n-                'Expected values are [\"PR\", \"ROC\"]'\n+                'Expected values are [\"PR\", \"ROC\", \"PRGAIN\"]'\n             )\n \n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#11f737ea3192ffc25227adac3e331cca1357ef4b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 153 | Lines Deleted: 18 | Files Changed: 2 | Hunks: 11 | Methods Changed: 16 | Complexity Δ (Sum/Max): 18/17 | Churn Δ: 171 | Churn Cumulative: 423 | Contributors (this commit): 8 | Commits (past 90d): 5 | Contributors (cumulative): 11 | DMM Complexity: 0.5376344086021505\n\nDIFF:\n@@ -171,13 +171,16 @@ class Operation:\n         # In this case the subclass doesn't implement get_config():\n         # Let's see if we can autogenerate it.\n         if getattr(self, \"_auto_config\", None) is not None:\n-            xtra_args = set(config.keys())\n             config.update(self._auto_config.config)\n-            # Remove args non explicitly supported\n-            argspec = inspect.getfullargspec(self.__init__)\n-            if argspec.varkw != \"kwargs\":\n-                for key in xtra_args - xtra_args.intersection(argspec.args[1:]):\n-                    config.pop(key, None)\n+            init_params = inspect.signature(self.__init__).parameters\n+            init_has_name = \"name\" in init_params\n+            init_has_kwargs = (\n+                \"kwargs\" in init_params\n+                and init_params[\"kwargs\"].kind == inspect.Parameter.VAR_KEYWORD\n+            )\n+            if not init_has_name and not init_has_kwargs:\n+                # We can't pass `name` back to `__init__`, remove it.\n+                config.pop(\"name\", None)\n             return config\n         else:\n             raise NotImplementedError(\n\n@@ -30,20 +30,71 @@ class OpWithMultipleOutputs(operation.Operation):\n \n \n class OpWithCustomConstructor(operation.Operation):\n-    def __init__(self, alpha, mode=\"foo\"):\n-        super().__init__()\n+    def __init__(self, alpha, *, name=None):\n+        super().__init__(name=name)\n         self.alpha = alpha\n-        self.mode = mode\n \n     def call(self, x):\n-        if self.mode == \"foo\":\n-            return x\n         return self.alpha * x\n \n     def compute_output_spec(self, x):\n         return keras_tensor.KerasTensor(x.shape, x.dtype)\n \n \n+class OpWithCustomConstructorNoName(operation.Operation):\n+    def __init__(self, alpha):\n+        super().__init__()\n+        self.alpha = alpha\n+\n+    def call(self, x):\n+        return self.alpha * x\n+\n+    def compute_output_spec(self, x):\n+        return keras_tensor.KerasTensor(x.shape, x.dtype)\n+\n+\n+class OpWithKwargsInConstructor(operation.Operation):\n+    def __init__(self, alpha, **kwargs):\n+        super().__init__(**kwargs)\n+        self.alpha = alpha\n+\n+    def call(self, x):\n+        return self.alpha * x\n+\n+    def compute_output_spec(self, x):\n+        return keras_tensor.KerasTensor(x.shape, x.dtype)\n+\n+\n+class OpWithCustomConstructorGetConfig(operation.Operation):\n+    def __init__(self, alpha, *, name=None):\n+        super().__init__(name=name)\n+        self.alpha = alpha\n+\n+    def call(self, x):\n+        return self.alpha * x\n+\n+    def compute_output_spec(self, x):\n+        return keras_tensor.KerasTensor(x.shape, x.dtype)\n+\n+    def get_config(self):\n+        return {**super().get_config(), \"alpha\": self.alpha}\n+\n+\n+class OpWithKwargsInConstructorGetConfig(operation.Operation):\n+    def __init__(self, alpha, **kwargs):\n+        super().__init__(**kwargs)\n+        self.alpha = alpha\n+\n+    def call(self, x):\n+        return self.alpha * x\n+\n+    def compute_output_spec(self, x):\n+        return keras_tensor.KerasTensor(x.shape, x.dtype)\n+\n+    def get_config(self):\n+        return {**super().get_config(), \"alpha\": self.alpha}\n+\n+\n class OperationTest(testing.TestCase):\n     def test_symbolic_call(self):\n         x = keras_tensor.KerasTensor(shape=(2, 3), name=\"x\")\n@@ -129,19 +180,100 @@ class OperationTest(testing.TestCase):\n         self.assertAllClose(out[0], np.ones((2, 3)))\n         self.assertAllClose(out[1], np.ones((2, 3)) + 1)\n \n-    def test_serialization(self):\n-        op = OpWithMultipleOutputs(name=\"test_op\")\n+    def test_serialization_with_default_init_and_get_config(self):\n+        # Explicit name passed in constructor is serialized and deserialized.\n+        op = OpWithMultipleInputs(name=\"test_op\")\n         config = op.get_config()\n         self.assertEqual(config, {\"name\": \"test_op\"})\n-        op = OpWithMultipleOutputs.from_config(config)\n-        self.assertEqual(op.name, \"test_op\")\n+        revived = OpWithMultipleInputs.from_config(config)\n+        self.assertEqual(revived.get_config(), config)\n+        self.assertEqual(revived.name, op.name)\n \n-    def test_autoconfig(self):\n-        op = OpWithCustomConstructor(alpha=0.2, mode=\"bar\")\n+        # Auto generated name is serialized and deserialized.\n+        op = OpWithMultipleInputs()\n         config = op.get_config()\n-        self.assertEqual(config, {\"alpha\": 0.2, \"mode\": \"bar\"})\n+        self.assertEqual(config, {\"name\": op.name})\n+        revived = OpWithMultipleInputs.from_config(config)\n+        self.assertEqual(revived.get_config(), config)\n+        self.assertEqual(revived.name, op.name)\n+\n+    def test_serialization_custom_constructor_with_name_auto_config(self):\n+        # Explicit name passed in constructor is serialized and deserialized.\n+        op = OpWithCustomConstructor(alpha=0.2, name=\"test_op\")\n+        config = op.get_config()\n+        self.assertEqual(config, {\"alpha\": 0.2, \"name\": \"test_op\"})\n         revived = OpWithCustomConstructor.from_config(config)\n         self.assertEqual(revived.get_config(), config)\n+        self.assertEqual(revived.name, op.name)\n+\n+        # Auto generated name is serialized and deserialized.\n+        op = OpWithCustomConstructor(alpha=0.2)\n+        config = op.get_config()\n+        self.assertEqual(config, {\"alpha\": 0.2, \"name\": op.name})\n+        revived = OpWithCustomConstructor.from_config(config)\n+        self.assertEqual(revived.get_config(), config)\n+        self.assertEqual(revived.name, op.name)\n+\n+    def test_serialization_custom_constructor_with_no_name_auto_config(self):\n+        # Auto generated name is not serialized.\n+        op = OpWithCustomConstructorNoName(alpha=0.2)\n+        config = op.get_config()\n+        self.assertEqual(config, {\"alpha\": 0.2})\n+        revived = OpWithCustomConstructorNoName.from_config(config)\n+        self.assertEqual(revived.get_config(), config)\n+\n+    def test_serialization_custom_constructor_with_kwargs_auto_config(self):\n+        # Explicit name passed in constructor is serialized and deserialized.\n+        op = OpWithKwargsInConstructor(alpha=0.2, name=\"test_op\")\n+        config = op.get_config()\n+        self.assertEqual(config, {\"alpha\": 0.2, \"name\": \"test_op\"})\n+        revived = OpWithKwargsInConstructor.from_config(config)\n+        self.assertEqual(revived.get_config(), config)\n+        self.assertEqual(revived.name, op.name)\n+\n+        # Auto generated name is serialized and deserialized.\n+        op = OpWithKwargsInConstructor(alpha=0.2)\n+        config = op.get_config()\n+        self.assertEqual(config, {\"alpha\": 0.2, \"name\": op.name})\n+        revived = OpWithKwargsInConstructor.from_config(config)\n+        self.assertEqual(revived.get_config(), config)\n+        self.assertEqual(revived.name, op.name)\n+\n+    def test_serialization_custom_constructor_custom_get_config(self):\n+        # Explicit name passed in constructor is serialized and deserialized.\n+        op = OpWithCustomConstructorGetConfig(alpha=0.2, name=\"test_op\")\n+        config = op.get_config()\n+        self.assertEqual(config, {\"alpha\": 0.2, \"name\": \"test_op\"})\n+        revived = OpWithCustomConstructorGetConfig.from_config(config)\n+        self.assertEqual(revived.get_config(), config)\n+        self.assertEqual(revived.name, op.name)\n+\n+        # Auto generated name is serialized and deserialized.\n+        op = OpWithCustomConstructorGetConfig(alpha=0.2)\n+        config = op.get_config()\n+        self.assertEqual(config, {\"alpha\": 0.2, \"name\": op.name})\n+        revived = OpWithCustomConstructorGetConfig.from_config(config)\n+        self.assertEqual(revived.get_config(), config)\n+        self.assertEqual(revived.name, op.name)\n+\n+    def test_serialization_custom_constructor_with_kwargs_custom_get_config(\n+        self,\n+    ):\n+        # Explicit name passed in constructor is serialized and deserialized.\n+        op = OpWithKwargsInConstructorGetConfig(alpha=0.2, name=\"test_op\")\n+        config = op.get_config()\n+        self.assertEqual(config, {\"alpha\": 0.2, \"name\": \"test_op\"})\n+        revived = OpWithKwargsInConstructorGetConfig.from_config(config)\n+        self.assertEqual(revived.get_config(), config)\n+        self.assertEqual(revived.name, op.name)\n+\n+        # Auto generated name is serialized and deserialized.\n+        op = OpWithKwargsInConstructorGetConfig(alpha=0.2)\n+        config = op.get_config()\n+        self.assertEqual(config, {\"alpha\": 0.2, \"name\": op.name})\n+        revived = OpWithKwargsInConstructorGetConfig.from_config(config)\n+        self.assertEqual(revived.get_config(), config)\n+        self.assertEqual(revived.name, op.name)\n \n     @skip_if_backend(\n         \"openvino\", \"Can not constant fold eltwise node by CPU plugin\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#fb0b1e62a3e6d3907f67f3ddbf8a9782b3cc47ff", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 195 | Lines Deleted: 219 | Files Changed: 6 | Hunks: 97 | Methods Changed: 92 | Complexity Δ (Sum/Max): -2/3 | Churn Δ: 414 | Churn Cumulative: 6431 | Contributors (this commit): 26 | Commits (past 90d): 37 | Contributors (cumulative): 66 | DMM Complexity: 0.0\n\nDIFF:\n@@ -78,24 +78,30 @@ def map(f, xs):\n \n \n class Scan(Operation):\n-    def __init__(self, reverse=False, unroll=1):\n+    def __init__(self, length=None, reverse=False, unroll=1):\n         super().__init__()\n+        self.length = length\n         self.reverse = reverse\n         self.unroll = unroll\n \n-    def call(self, f, init, xs=None, length=None):\n+    def call(self, f, init, xs=None):\n         return backend.core.scan(\n-            f, init, xs, length, reverse=self.reverse, unroll=self.unroll\n+            f,\n+            init,\n+            xs,\n+            length=self.length,\n+            reverse=self.reverse,\n+            unroll=self.unroll,\n         )\n \n-    def compute_output_spec(self, f, init, xs=None, length=None):\n+    def compute_output_spec(self, f, init, xs=None):\n         if xs is None:\n-            n = int(length)\n+            n = int(self.length)\n             x = None\n         else:\n             n = (\n-                int(length)\n-                if length is not None\n+                int(self.length)\n+                if self.length is not None\n                 else tree.flatten(xs)[0].shape[0]\n             )\n             x = xs[0]\n@@ -176,9 +182,9 @@ def scan(f, init, xs=None, length=None, reverse=False, unroll=1):\n     [1, 3, 6, 10, 15]\n     \"\"\"\n     if any_symbolic_tensors((init, xs)):\n-        return Scan(reverse=reverse, unroll=unroll).symbolic_call(\n-            f, init, xs, length\n-        )\n+        return Scan(\n+            length=length, reverse=reverse, unroll=unroll\n+        ).symbolic_call(f, init, xs)\n     return backend.core.scan(\n         f, init, xs, length, reverse=reverse, unroll=unroll\n     )\n@@ -283,11 +289,15 @@ def associative_scan(f, elems, reverse=False, axis=0):\n \n \n class Scatter(Operation):\n-    def call(self, indices, values, shape):\n-        return backend.core.scatter(indices, values, shape)\n+    def __init__(self, shape):\n+        super().__init__()\n+        self.shape = shape\n \n-    def compute_output_spec(self, indices, values, shape):\n-        return KerasTensor(shape, dtype=values.dtype)\n+    def call(self, indices, values):\n+        return backend.core.scatter(indices, values, self.shape)\n+\n+    def compute_output_spec(self, indices, values):\n+        return KerasTensor(self.shape, dtype=values.dtype)\n \n \n @keras_export(\"keras.ops.scatter\")\n@@ -316,8 +326,8 @@ def scatter(indices, values, shape):\n     array([[0., 1.],\n            [0., 1.]])\n     \"\"\"\n-    if any_symbolic_tensors((indices, values, shape)):\n-        return Scatter().symbolic_call(indices, values, shape)\n+    if any_symbolic_tensors((indices, values)):\n+        return Scatter(shape=shape).symbolic_call(indices, values)\n     return backend.core.scatter(indices, values, shape)\n \n \n@@ -382,11 +392,15 @@ def scatter_update(inputs, indices, updates):\n \n \n class Slice(Operation):\n-    def call(self, inputs, start_indices, shape):\n-        return backend.core.slice(inputs, start_indices, shape)\n+    def __init__(self, shape):\n+        super().__init__()\n+        self.shape = shape\n \n-    def compute_output_spec(self, inputs, start_indices, shape):\n-        return KerasTensor(shape, dtype=inputs.dtype)\n+    def call(self, inputs, start_indices):\n+        return backend.core.slice(inputs, start_indices, self.shape)\n+\n+    def compute_output_spec(self, inputs, start_indices):\n+        return KerasTensor(self.shape, dtype=inputs.dtype)\n \n \n @keras_export(\"keras.ops.slice\")\n@@ -415,8 +429,8 @@ def slice(inputs, start_indices, shape):\n     Returns:\n         A tensor, has the same shape and dtype as `inputs`.\n     \"\"\"\n-    if any_symbolic_tensors((inputs, start_indices, shape)):\n-        return Slice().symbolic_call(inputs, start_indices, shape)\n+    if any_symbolic_tensors((inputs, start_indices)):\n+        return Slice(shape=shape).symbolic_call(inputs, start_indices)\n     return backend.core.slice(inputs, start_indices, shape)\n \n \n@@ -916,7 +930,7 @@ def _saturate_cast(x, dtype, backend_module=None):\n class ConvertToTensor(Operation):\n     def __init__(self, dtype=None, sparse=None, ragged=None):\n         super().__init__()\n-        self.dtype = backend.standardize_dtype(dtype)\n+        self.dtype = None if dtype is None else backend.standardize_dtype(dtype)\n         self.sparse = sparse\n         self.ragged = ragged\n \n@@ -926,7 +940,11 @@ class ConvertToTensor(Operation):\n         )\n \n     def compute_output_spec(self, x):\n-        dtype = x.dtype if self.dtype is None else self.dtype\n+        dtype = (\n+            backend.standardize_dtype(x.dtype)\n+            if self.dtype is None\n+            else self.dtype\n+        )\n         sparse = (\n             False if self.sparse is not None and not self.sparse else x.sparse\n         )\n\n@@ -1010,7 +1010,7 @@ class CoreOpsCallsTests(testing.TestCase):\n         init = np.array(0, dtype=\"float32\")\n         xs = np.array([1, 2, 3, 4, 10, 20], dtype=\"float32\")\n         scan_op = core.Scan()\n-        carry, result = scan_op.call(cumsum, init, xs, None)\n+        carry, result = scan_op.call(cumsum, init, xs)\n         self.assertAllClose(carry, 40.0)\n         self.assertAllClose(result, ops.cumsum(xs))\n \n@@ -1025,8 +1025,8 @@ class CoreOpsCallsTests(testing.TestCase):\n         indices = np.array([[1, 0], [0, 1]])\n         values = np.array([10, 20])\n         shape = (2, 2)\n-        scatter = core.Scatter()\n-        result = scatter.call(indices, values, shape)\n+        scatter = core.Scatter(shape)\n+        result = scatter.call(indices, values)\n         expected_output = np.array([[0, 20], [10, 0]])\n         self.assertAllClose(core.convert_to_numpy(result), expected_output)\n \n@@ -1043,8 +1043,8 @@ class CoreOpsCallsTests(testing.TestCase):\n         inputs = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n         start_indices = np.array([1, 1])\n         shape = (2, 2)\n-        slice_op = core.Slice()\n-        result = slice_op.call(inputs, start_indices, shape)\n+        slice_op = core.Slice(shape)\n+        result = slice_op.call(inputs, start_indices)\n         expected_output = np.array([[5, 6], [8, 9]])\n         self.assertAllClose(core.convert_to_numpy(result), expected_output)\n \n@@ -1052,8 +1052,8 @@ class CoreOpsCallsTests(testing.TestCase):\n         inputs = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=np.float32)\n         start_indices = np.array([1, 1])\n         shape = (2, 2)\n-        slice_op = core.Slice()\n-        output_spec = slice_op.compute_output_spec(inputs, start_indices, shape)\n+        slice_op = core.Slice(shape)\n+        output_spec = slice_op.compute_output_spec(inputs, start_indices)\n         self.assertEqual(output_spec.shape, shape)\n         self.assertEqual(output_spec.dtype, inputs.dtype)\n \n\n@@ -1693,7 +1693,7 @@ class OneHot(Operation):\n         super().__init__()\n         self.num_classes = num_classes\n         self.axis = axis\n-        self.dtype = dtype or backend.floatx()\n+        self.dtype = backend.standardize_dtype(dtype)\n         self.sparse = sparse\n \n     def call(self, x):\n@@ -2580,9 +2580,13 @@ def psnr(\n \n \n class DotProductAttention(Operation):\n-    def __init__(self, is_causal=False):\n+    def __init__(\n+        self, is_causal=False, flash_attention=None, attn_logits_soft_cap=None\n+    ):\n         super().__init__()\n         self.is_causal = is_causal\n+        self.flash_attention = flash_attention\n+        self.attn_logits_soft_cap = attn_logits_soft_cap\n \n     def call(\n         self,\n@@ -2592,8 +2596,6 @@ class DotProductAttention(Operation):\n         bias=None,\n         mask=None,\n         scale=None,\n-        flash_attention=None,\n-        attn_logits_soft_cap=None,\n     ):\n         return backend.nn.dot_product_attention(\n             query,\n@@ -2603,8 +2605,8 @@ class DotProductAttention(Operation):\n             mask=mask,\n             scale=scale,\n             is_causal=self.is_causal,\n-            flash_attention=flash_attention,\n-            attn_logits_soft_cap=attn_logits_soft_cap,\n+            flash_attention=self.flash_attention,\n+            attn_logits_soft_cap=self.attn_logits_soft_cap,\n         )\n \n     def compute_output_spec(\n@@ -2615,8 +2617,6 @@ class DotProductAttention(Operation):\n         bias=None,\n         mask=None,\n         scale=None,\n-        flash_attention=None,\n-        attn_logits_soft_cap=None,\n     ):\n         return KerasTensor(query.shape, dtype=query.dtype)\n \n@@ -2703,15 +2703,17 @@ def dot_product_attention(\n             )\n \n     if any_symbolic_tensors((query, key, value)):\n-        return DotProductAttention(is_causal=is_causal).symbolic_call(\n+        return DotProductAttention(\n+            is_causal=is_causal,\n+            flash_attention=flash_attention,\n+            attn_logits_soft_cap=attn_logits_soft_cap,\n+        ).symbolic_call(\n             query,\n             key,\n             value,\n             bias=bias,\n             mask=mask,\n             scale=scale,\n-            flash_attention=flash_attention,\n-            attn_logits_soft_cap=attn_logits_soft_cap,\n         )\n     return backend.nn.dot_product_attention(\n         query,\n\n@@ -591,13 +591,18 @@ def append(\n \n \n class Arange(Operation):\n-    def call(self, start, stop=None, step=1, dtype=None):\n-        return backend.numpy.arange(start, stop, step=step, dtype=dtype)\n+    def __init__(self, dtype=None):\n+        super().__init__()\n+        self.dtype = None if dtype is None else backend.standardize_dtype(dtype)\n \n-    def compute_output_spec(self, start, stop=None, step=1, dtype=None):\n+    def call(self, start, stop=None, step=1):\n+        return backend.numpy.arange(start, stop, step=step, dtype=self.dtype)\n+\n+    def compute_output_spec(self, start, stop=None, step=1):\n         if stop is None:\n             start, stop = 0, start\n         output_shape = [int(np.ceil((stop - start) / step))]\n+        dtype = self.dtype\n         if dtype is None:\n             dtypes_to_resolve = [\n                 getattr(start, \"dtype\", type(start)),\n@@ -655,6 +660,8 @@ def arange(start, stop=None, step=1, dtype=None):\n     >>> keras.ops.arange(3, 7, 2)\n     array([3, 5], dtype=int32)\n     \"\"\"\n+    if any_symbolic_tensors((start, stop, step)):\n+        return Arange(dtype=dtype).symbolic_call(start, stop, step=step)\n     return backend.numpy.arange(start, stop, step=step, dtype=dtype)\n \n \n@@ -1077,10 +1084,19 @@ def argsort(x, axis=-1):\n \n \n class Array(Operation):\n-    def call(self, x, dtype=None):\n-        return backend.numpy.array(x, dtype=dtype)\n+    def __init__(self, dtype=None):\n+        super().__init__()\n+        self.dtype = None if dtype is None else backend.standardize_dtype(dtype)\n+\n+    def call(self, x):\n+        return backend.numpy.array(x, dtype=self.dtype)\n \n     def compute_output_spec(self, x, dtype=None):\n+        dtype = (\n+            backend.standardize_dtype(x.dtype)\n+            if self.dtype is None\n+            else self.dtype\n+        )\n         return KerasTensor(x.shape, dtype=dtype)\n \n \n@@ -1103,7 +1119,7 @@ def array(x, dtype=None):\n     array([1., 2., 3.], dtype=float32)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n-        return Array().symbolic_call(x, dtype=dtype)\n+        return Array(dtype=dtype).symbolic_call(x)\n     return backend.numpy.array(x, dtype=dtype)\n \n \n@@ -2187,7 +2203,7 @@ class Cumprod(Operation):\n     def __init__(self, axis=None, dtype=None):\n         super().__init__()\n         self.axis = axis\n-        self.dtype = dtype\n+        self.dtype = None if dtype is None else backend.standardize_dtype(dtype)\n \n     def call(self, x):\n         return backend.numpy.cumprod(x, axis=self.axis, dtype=self.dtype)\n@@ -2200,7 +2216,11 @@ class Cumprod(Operation):\n                 output_shape = (int(np.prod(x.shape)),)\n         else:\n             output_shape = x.shape\n-        output_dtype = backend.standardize_dtype(self.dtype or x.dtype)\n+        output_dtype = (\n+            backend.standardize_dtype(x.dtype)\n+            if self.dtype is None\n+            else self.dtype\n+        )\n         if output_dtype == \"bool\":\n             output_dtype = \"int32\"\n         return KerasTensor(output_shape, output_dtype)\n@@ -2226,7 +2246,7 @@ class Cumsum(Operation):\n     def __init__(self, axis=None, dtype=None):\n         super().__init__()\n         self.axis = axis\n-        self.dtype = dtype\n+        self.dtype = None if dtype is None else backend.standardize_dtype(dtype)\n \n     def call(self, x):\n         return backend.numpy.cumsum(x, axis=self.axis, dtype=self.dtype)\n@@ -2239,7 +2259,11 @@ class Cumsum(Operation):\n                 output_shape = (int(np.prod(x.shape)),)\n         else:\n             output_shape = x.shape\n-        output_dtype = backend.standardize_dtype(self.dtype or x.dtype)\n+        output_dtype = (\n+            backend.standardize_dtype(x.dtype)\n+            if self.dtype is None\n+            else self.dtype\n+        )\n         if output_dtype == \"bool\":\n             output_dtype = \"int32\"\n         return KerasTensor(output_shape, output_dtype)\n@@ -2924,15 +2948,6 @@ def einsum(subscripts, *operands, **kwargs):\n     return backend.numpy.einsum(subscripts, *operands, **kwargs)\n \n \n-class Empty(Operation):\n-    def call(self, shape, dtype=None):\n-        return backend.numpy.empty(shape, dtype=dtype)\n-\n-    def compute_output_spec(self, shape, dtype=None):\n-        dtype = dtype or backend.floatx()\n-        return KerasTensor(shape, dtype=dtype)\n-\n-\n @keras_export([\"keras.ops.empty\", \"keras.ops.numpy.empty\"])\n def empty(shape, dtype=None):\n     \"\"\"Return a tensor of given shape and type filled with uninitialized data.\n@@ -3161,12 +3176,17 @@ def floor(x):\n \n \n class Full(Operation):\n-    def call(self, shape, fill_value, dtype=None):\n-        return backend.numpy.full(shape, fill_value, dtype=dtype)\n+    def __init__(self, shape, dtype=None):\n+        super().__init__()\n+        self.shape = shape\n+        self.dtype = None if dtype is None else backend.standardize_dtype(dtype)\n \n-    def compute_output_spec(self, shape, fill_value, dtype=None):\n-        dtype = dtype or backend.floatx()\n-        return KerasTensor(shape, dtype=dtype)\n+    def call(self, fill_value):\n+        return backend.numpy.full(self.shape, fill_value, dtype=self.dtype)\n+\n+    def compute_output_spec(self, fill_value):\n+        dtype = backend.floatx() if self.dtype is None else self.dtype\n+        return KerasTensor(self.shape, dtype=dtype)\n \n \n @keras_export([\"keras.ops.full\", \"keras.ops.numpy.full\"])\n@@ -3181,15 +3201,25 @@ def full(shape, fill_value, dtype=None):\n     Returns:\n         Output tensor.\n     \"\"\"\n+    if any_symbolic_tensors((fill_value,)):\n+        return Full(shape=shape, dtype=dtype).symbolic_call(fill_value)\n     return backend.numpy.full(shape, fill_value, dtype=dtype)\n \n \n class FullLike(Operation):\n-    def call(self, x, fill_value, dtype=None):\n-        return backend.numpy.full_like(x, fill_value, dtype=dtype)\n+    def __init__(self, dtype=None):\n+        super().__init__()\n+        self.dtype = None if dtype is None else backend.standardize_dtype(dtype)\n \n-    def compute_output_spec(self, x, fill_value, dtype=None):\n-        dtype = dtype or x.dtype\n+    def call(self, x, fill_value):\n+        return backend.numpy.full_like(x, fill_value, dtype=self.dtype)\n+\n+    def compute_output_spec(self, x, fill_value):\n+        dtype = (\n+            backend.standardize_dtype(x.dtype)\n+            if self.dtype is None\n+            else self.dtype\n+        )\n         return KerasTensor(x.shape, dtype=dtype)\n \n \n@@ -3205,8 +3235,8 @@ def full_like(x, fill_value, dtype=None):\n     Returns:\n         Tensor of `fill_value` with the same shape and type as `x`.\n     \"\"\"\n-    if any_symbolic_tensors((x,)):\n-        return FullLike().symbolic_call(x, fill_value, dtype=dtype)\n+    if any_symbolic_tensors((x, fill_value)):\n+        return FullLike(dtype=dtype).symbolic_call(x, fill_value)\n     return backend.numpy.full_like(x, fill_value, dtype=dtype)\n \n \n@@ -3395,15 +3425,6 @@ def hstack(xs):\n     return backend.numpy.hstack(xs)\n \n \n-class Identity(Operation):\n-    def call(self, n, dtype=None):\n-        return backend.numpy.identity(n, dtype=dtype)\n-\n-    def compute_output_spec(self, n, dtype=None):\n-        dtype = dtype or backend.floatx()\n-        return KerasTensor([n, n], dtype=dtype)\n-\n-\n @keras_export([\"keras.ops.identity\", \"keras.ops.numpy.identity\"])\n def identity(n, dtype=None):\n     \"\"\"Return the identity tensor.\n@@ -3446,12 +3467,14 @@ def imag(x):\n \n \n class Isclose(Operation):\n-    def call(self, x1, x2, rtol=1e-5, atol=1e-8, equal_nan=False):\n-        return backend.numpy.isclose(x1, x2, rtol, atol, equal_nan)\n+    def __init__(self, equal_nan=False):\n+        super().__init__()\n+        self.equal_nan = equal_nan\n \n-    def compute_output_spec(\n-        self, x1, x2, rtol=1e-5, atol=1e-8, equal_nan=False\n-    ):\n+    def call(self, x1, x2, rtol=1e-5, atol=1e-8):\n+        return backend.numpy.isclose(x1, x2, rtol, atol, self.equal_nan)\n+\n+    def compute_output_spec(self, x1, x2, rtol=1e-5, atol=1e-8):\n         x1_shape = getattr(x1, \"shape\", [])\n         x2_shape = getattr(x2, \"shape\", [])\n         output_shape = broadcast_shapes(x1_shape, x2_shape)\n@@ -3473,7 +3496,7 @@ def isclose(x1, x2, rtol=1e-5, atol=1e-8, equal_nan=False):\n         Output boolean tensor.\n     \"\"\"\n     if any_symbolic_tensors((x1, x2)):\n-        return Isclose().symbolic_call(x1, x2, rtol, atol, equal_nan)\n+        return Isclose(equal_nan=equal_nan).symbolic_call(x1, x2, rtol, atol)\n     return backend.numpy.isclose(x1, x2, rtol, atol, equal_nan)\n \n \n@@ -3653,7 +3676,7 @@ class Linspace(Operation):\n         dtype = (\n             self.dtype\n             if self.dtype is not None\n-            else getattr(start, \"dtype\", type(start))\n+            else backend.standardize_dtype(getattr(start, \"dtype\", type(start)))\n         )\n         dtype = backend.result_type(dtype, float)\n         if self.retstep:\n@@ -3962,7 +3985,7 @@ class Logspace(Operation):\n         self.num = num\n         self.endpoint = endpoint\n         self.base = base\n-        self.dtype = dtype\n+        self.dtype = None if dtype is None else backend.standardize_dtype(dtype)\n         self.axis = axis\n \n     def call(self, start, stop):\n@@ -3997,7 +4020,7 @@ class Logspace(Operation):\n         dtype = (\n             self.dtype\n             if self.dtype is not None\n-            else getattr(start, \"dtype\", type(start))\n+            else backend.standardize_dtype(getattr(start, \"dtype\", type(start)))\n         )\n         dtype = backend.result_type(dtype, float)\n         return KerasTensor(output_shape, dtype=dtype)\n@@ -4581,15 +4604,17 @@ def not_equal(x1, x2):\n class OnesLike(Operation):\n     def __init__(self, dtype=None):\n         super().__init__()\n-        self.dtype = (\n-            backend.standardize_dtype(dtype) if dtype is not None else None\n-        )\n+        self.dtype = None if dtype is None else backend.standardize_dtype(dtype)\n \n     def call(self, x):\n         return backend.numpy.ones_like(x, dtype=self.dtype)\n \n     def compute_output_spec(self, x):\n-        dtype = x.dtype if self.dtype is None else self.dtype\n+        dtype = (\n+            backend.standardize_dtype(x.dtype)\n+            if self.dtype is None\n+            else self.dtype\n+        )\n         sparse = getattr(x, \"sparse\", False)\n         return KerasTensor(x.shape, dtype=dtype, sparse=sparse)\n \n@@ -4613,15 +4638,17 @@ def ones_like(x, dtype=None):\n class ZerosLike(Operation):\n     def __init__(self, dtype=None):\n         super().__init__()\n-        self.dtype = (\n-            backend.standardize_dtype(dtype) if dtype is not None else None\n-        )\n+        self.dtype = None if dtype is None else backend.standardize_dtype(dtype)\n \n     def call(self, x):\n         return backend.numpy.zeros_like(x, dtype=self.dtype)\n \n     def compute_output_spec(self, x, dtype=None):\n-        dtype = x.dtype if self.dtype is None else self.dtype\n+        dtype = (\n+            backend.standardize_dtype(x.dtype)\n+            if self.dtype is None\n+            else self.dtype\n+        )\n         sparse = getattr(x, \"sparse\", False)\n         return KerasTensor(x.shape, dtype=dtype, sparse=sparse)\n \n@@ -4797,7 +4824,7 @@ class Prod(Operation):\n         else:\n             self.axis = axis\n         self.keepdims = keepdims\n-        self.dtype = dtype\n+        self.dtype = None if dtype is None else backend.standardize_dtype(dtype)\n \n     def call(self, x):\n         return backend.numpy.prod(\n@@ -4811,7 +4838,7 @@ class Prod(Operation):\n         if self.dtype is not None:\n             dtype = self.dtype\n         else:\n-            dtype = backend.result_type(x.dtype)\n+            dtype = backend.standardize_dtype(x.dtype)\n             if dtype == \"bool\":\n                 dtype = \"int32\"\n             elif dtype in (\"int8\", \"int16\"):\n@@ -5219,12 +5246,18 @@ def round(x, decimals=0):\n \n \n class SearchSorted(Operation):\n-    def call(self, sorted_sequence, values, side=\"left\"):\n+    def __init__(self, side=\"left\"):\n+        super().__init__()\n+        self.side = side\n+\n+    def call(self, sorted_sequence, values):\n         sorted_sequence = backend.convert_to_tensor(sorted_sequence)\n         values = backend.convert_to_tensor(values)\n-        return backend.numpy.searchsorted(sorted_sequence, values, side=side)\n+        return backend.numpy.searchsorted(\n+            sorted_sequence, values, side=self.side\n+        )\n \n-    def compute_output_spec(self, sorted_sequence, values, side=\"left\"):\n+    def compute_output_spec(self, sorted_sequence, values):\n         if len(sorted_sequence.shape) != 1:\n             raise ValueError(\n                 \"searchsorted only supports 1-D sorted sequences. Use\"\n@@ -5254,7 +5287,7 @@ def searchsorted(sorted_sequence, values, side=\"left\"):\n         Tensor of insertion indices of same shape as `values`.\n     \"\"\"\n     if any_symbolic_tensors((sorted_sequence, values)):\n-        return SearchSorted().symbolic_call(sorted_sequence, values, side=side)\n+        return SearchSorted(side=side).symbolic_call(sorted_sequence, values)\n \n     sorted_sequence = backend.convert_to_tensor(sorted_sequence)\n     values = backend.convert_to_tensor(values)\n@@ -5934,21 +5967,6 @@ def trace(x, offset=0, axis1=0, axis2=1):\n     return backend.numpy.trace(x, offset=offset, axis1=axis1, axis2=axis2)\n \n \n-class Tri(Operation):\n-    def __init__(self, k=0, dtype=None):\n-        super().__init__()\n-        self.k = k\n-        self.dtype = dtype or backend.floatx()\n-\n-    def call(self, N, M=None):\n-        return backend.numpy.tri(N=N, M=M, k=self.k, dtype=self.dtype)\n-\n-    def compute_output_spec(self, N, M=None):\n-        if M is None:\n-            M = N\n-        return KerasTensor((N, M), dtype=self.dtype)\n-\n-\n @keras_export([\"keras.ops.tri\", \"keras.ops.numpy.tri\"])\n def tri(N, M=None, k=0, dtype=None):\n     \"\"\"Return a tensor with ones at and below a diagonal and zeros elsewhere.\n@@ -6763,15 +6781,6 @@ def sum(x, axis=None, keepdims=False):\n     return backend.numpy.sum(x, axis=axis, keepdims=keepdims)\n \n \n-class Zeros(Operation):\n-    def call(self, shape, dtype=None):\n-        return backend.numpy.zeros(shape, dtype=dtype)\n-\n-    def compute_output_spec(self, shape, dtype=None):\n-        dtype = dtype or backend.floatx()\n-        return KerasTensor(shape, dtype=dtype)\n-\n-\n @keras_export([\"keras.ops.zeros\", \"keras.ops.numpy.zeros\"])\n def zeros(shape, dtype=None):\n     \"\"\"Return a new tensor of given shape and type, filled with zeros.\n@@ -6786,15 +6795,6 @@ def zeros(shape, dtype=None):\n     return backend.numpy.zeros(shape, dtype=dtype)\n \n \n-class Ones(Operation):\n-    def call(self, shape, dtype=None):\n-        return backend.numpy.ones(shape, dtype=dtype)\n-\n-    def compute_output_spec(self, shape, dtype=None):\n-        dtype = dtype or backend.floatx()\n-        return KerasTensor(shape, dtype=dtype)\n-\n-\n @keras_export([\"keras.ops.ones\", \"keras.ops.numpy.ones\"])\n def ones(shape, dtype=None):\n     \"\"\"Return a new tensor of given shape and type, filled with ones.\n@@ -6809,21 +6809,6 @@ def ones(shape, dtype=None):\n     return backend.numpy.ones(shape, dtype=dtype)\n \n \n-class Eye(Operation):\n-    def __init__(self, k=0, dtype=None):\n-        super().__init__()\n-        self.k = k\n-        self.dtype = dtype or backend.floatx()\n-\n-    def call(self, N, M=None):\n-        return backend.numpy.eye(N, M=M, k=self.k, dtype=self.dtype)\n-\n-    def compute_output_spec(self, N, M=None):\n-        if M is None:\n-            M = N\n-        return KerasTensor((N, M), dtype=self.dtype)\n-\n-\n @keras_export([\"keras.ops.eye\", \"keras.ops.numpy.eye\"])\n def eye(N, M=None, k=0, dtype=None):\n     \"\"\"Return a 2-D tensor with ones on the diagonal and zeros elsewhere.\n\n@@ -2744,7 +2744,7 @@ class NumpyTwoInputOpsCorrectnessTest(testing.TestCase):\n \n         self.assertAllClose(knp.FullLike()(x, 2), np.full_like(x, 2))\n         self.assertAllClose(\n-            knp.FullLike()(x, 2, dtype=\"float32\"),\n+            knp.FullLike(dtype=\"float32\")(x, 2),\n             np.full_like(x, 2, dtype=\"float32\"),\n         )\n         self.assertAllClose(\n@@ -4920,35 +4920,29 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase):\n class NumpyArrayCreateOpsCorrectnessTest(testing.TestCase):\n     def test_ones(self):\n         self.assertAllClose(knp.ones([2, 3]), np.ones([2, 3]))\n-        self.assertAllClose(knp.Ones()([2, 3]), np.ones([2, 3]))\n \n     def test_zeros(self):\n         self.assertAllClose(knp.zeros([2, 3]), np.zeros([2, 3]))\n-        self.assertAllClose(knp.Zeros()([2, 3]), np.zeros([2, 3]))\n \n     def test_eye(self):\n         self.assertAllClose(knp.eye(3), np.eye(3))\n         self.assertAllClose(knp.eye(3, 4), np.eye(3, 4))\n         self.assertAllClose(knp.eye(3, 4, 1), np.eye(3, 4, 1))\n \n-        self.assertAllClose(knp.Eye()(3), np.eye(3))\n-        self.assertAllClose(knp.Eye()(3, 4), np.eye(3, 4))\n-        self.assertAllClose(knp.Eye(k=1)(3, 4), np.eye(3, 4, k=1))\n-\n         # Test k >= N\n-        self.assertAllClose(knp.Eye(k=3)(3), np.eye(3, k=3))\n+        self.assertAllClose(knp.eye(3, k=3), np.eye(3, k=3))\n \n         # Test k > 0 and N >= M\n-        self.assertAllClose(knp.Eye(k=1)(3), np.eye(3, k=1))\n+        self.assertAllClose(knp.eye(3, k=1), np.eye(3, k=1))\n \n         # Test k > 0 and N < M and N + k > M\n-        self.assertAllClose(knp.Eye(k=2)(3, 4), np.eye(3, 4, k=2))\n+        self.assertAllClose(knp.eye(3, 4, k=2), np.eye(3, 4, k=2))\n \n         # Test k < 0 and M >= N\n-        self.assertAllClose(knp.Eye(k=-1)(3), np.eye(3, k=-1))\n+        self.assertAllClose(knp.eye(3, k=-1), np.eye(3, k=-1))\n \n         # Test k < 0 and M < N and M - k > N\n-        self.assertAllClose(knp.Eye(k=-2)(4, 3), np.eye(4, 3, k=-2))\n+        self.assertAllClose(knp.eye(4, 3, k=-2), np.eye(4, 3, k=-2))\n \n     def test_arange(self):\n         self.assertAllClose(knp.arange(3), np.arange(3))\n@@ -4972,34 +4966,29 @@ class NumpyArrayCreateOpsCorrectnessTest(testing.TestCase):\n             np.full([2, 3], np.array([1, 4, 5])),\n         )\n \n-        self.assertAllClose(knp.Full()([2, 3], 0), np.full([2, 3], 0))\n-        self.assertAllClose(knp.Full()([2, 3], 0.1), np.full([2, 3], 0.1))\n+        self.assertAllClose(knp.Full([2, 3])(0), np.full([2, 3], 0))\n+        self.assertAllClose(knp.Full([2, 3])(0.1), np.full([2, 3], 0.1))\n         self.assertAllClose(\n-            knp.Full()([2, 3], np.array([1, 4, 5])),\n+            knp.Full([2, 3])(np.array([1, 4, 5])),\n             np.full([2, 3], np.array([1, 4, 5])),\n         )\n \n     def test_identity(self):\n         self.assertAllClose(knp.identity(3), np.identity(3))\n-        self.assertAllClose(knp.Identity()(3), np.identity(3))\n \n     def test_tri(self):\n         self.assertAllClose(knp.tri(3), np.tri(3))\n         self.assertAllClose(knp.tri(3, 4), np.tri(3, 4))\n         self.assertAllClose(knp.tri(3, 4, 1), np.tri(3, 4, 1))\n \n-        self.assertAllClose(knp.Tri()(3), np.tri(3))\n-        self.assertAllClose(knp.Tri()(3, 4), np.tri(3, 4))\n-        self.assertAllClose(knp.Tri(k=1)(3, 4), np.tri(3, 4, 1))\n-\n         # Test k < 0\n-        self.assertAllClose(knp.Tri(k=-1)(3), np.tri(3, k=-1))\n+        self.assertAllClose(knp.tri(3, k=-1), np.tri(3, k=-1))\n \n         # Test -k-1 > N\n-        self.assertAllClose(knp.Tri(k=-5)(3), np.tri(3, k=-5))\n+        self.assertAllClose(knp.tri(3, k=-5), np.tri(3, k=-5))\n \n         # Test k > M\n-        self.assertAllClose(knp.Tri(k=4)(3), np.tri(3, k=4))\n+        self.assertAllClose(knp.tri(3, k=4), np.tri(3, k=4))\n \n \n def create_sparse_tensor(x, indices_from=None, start=0, delta=2):\n@@ -5951,12 +5940,6 @@ class NumpyDtypeTest(testing.TestCase):\n             standardize_dtype(knp.ones([2, 3], dtype=dtype).dtype),\n             expected_dtype,\n         )\n-        self.assertEqual(\n-            standardize_dtype(\n-                knp.Ones().symbolic_call([2, 3], dtype=dtype).dtype\n-            ),\n-            expected_dtype,\n-        )\n \n     @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n     def test_zeros(self, dtype):\n@@ -5968,12 +5951,6 @@ class NumpyDtypeTest(testing.TestCase):\n             standardize_dtype(knp.zeros([2, 3], dtype=dtype).dtype),\n             expected_dtype,\n         )\n-        self.assertEqual(\n-            standardize_dtype(\n-                knp.Zeros().symbolic_call([2, 3], dtype=dtype).dtype\n-            ),\n-            expected_dtype,\n-        )\n \n     @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n     def test_absolute(self, dtype):\n@@ -6163,7 +6140,7 @@ class NumpyDtypeTest(testing.TestCase):\n         )\n         self.assertEqual(\n             standardize_dtype(\n-                knp.Arange().symbolic_call(start, stop, step, dtype).dtype\n+                knp.Arange(dtype).symbolic_call(start, stop, step).dtype\n             ),\n             expected_dtype,\n         )\n@@ -6982,12 +6959,6 @@ class NumpyDtypeTest(testing.TestCase):\n             standardize_dtype(knp.empty([2, 3], dtype=dtype).dtype),\n             expected_dtype,\n         )\n-        self.assertEqual(\n-            standardize_dtype(\n-                knp.Empty().symbolic_call([2, 3], dtype=dtype).dtype\n-            ),\n-            expected_dtype,\n-        )\n \n     @parameterized.named_parameters(\n         named_product(dtypes=itertools.combinations(ALL_DTYPES, 2))\n@@ -7082,10 +7053,6 @@ class NumpyDtypeTest(testing.TestCase):\n             standardize_dtype(knp.eye(3, dtype=dtype).dtype),\n             expected_dtype,\n         )\n-        self.assertEqual(\n-            standardize_dtype(knp.Eye(dtype=dtype).symbolic_call(3).dtype),\n-            expected_dtype,\n-        )\n \n         expected_dtype = standardize_dtype(jnp.eye(3, 4, 1, dtype=dtype).dtype)\n \n@@ -7093,12 +7060,6 @@ class NumpyDtypeTest(testing.TestCase):\n             standardize_dtype(knp.eye(3, 4, k=1, dtype=dtype).dtype),\n             expected_dtype,\n         )\n-        self.assertEqual(\n-            standardize_dtype(\n-                knp.Eye(k=1, dtype=dtype).symbolic_call(3, 4).dtype\n-            ),\n-            expected_dtype,\n-        )\n \n     @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n     def test_flip(self, dtype):\n@@ -7214,9 +7175,7 @@ class NumpyDtypeTest(testing.TestCase):\n             expected_dtype,\n         )\n         self.assertEqual(\n-            standardize_dtype(\n-                knp.Full().symbolic_call((), 0, dtype=dtype).dtype\n-            ),\n+            standardize_dtype(knp.Full((), dtype=dtype).symbolic_call(0).dtype),\n             expected_dtype,\n         )\n \n@@ -7311,12 +7270,6 @@ class NumpyDtypeTest(testing.TestCase):\n             standardize_dtype(knp.identity(3, dtype=dtype).dtype),\n             expected_dtype,\n         )\n-        self.assertEqual(\n-            standardize_dtype(\n-                knp.Identity().symbolic_call(3, dtype=dtype).dtype\n-            ),\n-            expected_dtype,\n-        )\n \n     @parameterized.named_parameters(\n         named_product(dtypes=itertools.combinations(ALL_DTYPES, 2))\n@@ -8642,10 +8595,6 @@ class NumpyDtypeTest(testing.TestCase):\n             standardize_dtype(knp.tri(3, dtype=dtype).dtype),\n             expected_dtype,\n         )\n-        self.assertEqual(\n-            standardize_dtype(knp.Tri(dtype=dtype).symbolic_call(3).dtype),\n-            expected_dtype,\n-        )\n \n     @parameterized.named_parameters(named_product(dtype=ALL_DTYPES))\n     def test_tril(self, dtype):\n\n@@ -16,6 +16,11 @@ from keras.src.utils.naming import to_snake_case\n \n OPS_MODULES = (\"core\", \"image\", \"linalg\", \"math\", \"nn\", \"numpy\")\n \n+# Parameters with these names are known to always be static (non-tensors).\n+STATIC_PARAMETER_NAMES = frozenset(\n+    {\"axis\", \"axes\", \"dtype\", \"shape\", \"newshape\", \"sparse\", \"ragged\"}\n+)\n+\n \n def op_functions_and_classes(ops_module):\n     \"\"\"Enumerate pairs of op function and op classes in a module.\n@@ -104,7 +109,7 @@ class OperationTest(testing.TestCase):\n             op_signature = inspect.signature(op_function)\n \n             for p in dynamic_parameters + static_parameters:\n-                # Check the same name appeas in the op signature\n+                # Check the same name appears in the op signature\n                 self.assertIn(\n                     p.name,\n                     op_signature.parameters,\n@@ -119,10 +124,26 @@ class OperationTest(testing.TestCase):\n                     f\"function `{name}` and op class `{op_class.__name__}`\",\n                 )\n \n-            # Check order of parameters.\n             dynamic_parameter_names = [p.name for p in dynamic_parameters]\n             static_parameter_names = [p.name for p in static_parameters]\n \n+            # Check for obvious mistakes in parameters that were made dynamic\n+            # but should be static.\n+            for p in dynamic_parameters:\n+                self.assertNotIn(\n+                    p.name,\n+                    STATIC_PARAMETER_NAMES,\n+                    f\"`{p.name}` should not be a dynamic parameter in op class \"\n+                    f\"`{op_class.__name__}` based on its name.\",\n+                )\n+                self.assertNotIsInstance(\n+                    p.default,\n+                    (bool, str),\n+                    f\"`{p.name}` should not be a dynamic parameter in op class \"\n+                    f\"`{op_class.__name__}` based on default `{p.default}`.\",\n+                )\n+\n+            # Check order of parameters.\n             if name in (\n                 \"fori_loop\",\n                 \"while_loop\",\n@@ -130,6 +151,7 @@ class OperationTest(testing.TestCase):\n                 \"dot_product_attention\",\n                 \"average\",\n                 \"einsum\",\n+                \"full\",\n                 \"pad\",\n             ):\n                 # Loose case:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
