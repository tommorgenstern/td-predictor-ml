{"custom_id": "keras#4bf04920f260e4bb3070c70673ea510e82beed9a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 10 | Lines Deleted: 3 | Files Changed: 4 | Hunks: 5 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 886 | Contributors (this commit): 1 | Commits (past 90d): 15 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -55,7 +55,11 @@ model = MyModel(hidden_dim=256, output_dim=16)\n optimizer = optimizers.SGD(learning_rate=0.0001)\n dataset = Dataset()\n \n-# # Build model\n+\n+######### Custom JAX workflow ###############\n+\n+\n+# Build model\n x = ops.convert_to_tensor(np.random.random((1, 128)))\n model(x)\n # Build optimizer\n@@ -68,7 +72,6 @@ def compute_loss_and_updates(\n     y_pred, non_trainable_variables = model.stateless_call(\n         trainable_variables, non_trainable_variables, x\n     )\n-\n     loss = loss_fn(y, y_pred)\n     return loss, non_trainable_variables\n \n\n@@ -19,6 +19,7 @@ class MyDense(layers.Layer):\n         input_dim = input_shape[-1]\n         w_shape = (input_dim, self.units)\n         w_value = initializers.GlorotUniform()(w_shape)\n+        # State must be stored in backend.Variable objects.\n         self.w = backend.Variable(w_value, name=\"kernel\")\n \n         b_shape = (self.units,)\n\n@@ -59,6 +59,9 @@ optimizer = optimizers.SGD(learning_rate=0.0001)\n dataset = Dataset()\n \n \n+######### Custom TF workflow ###############\n+\n+\n @tf.function(jit_compile=True)\n def train_step(data):\n     x, y = data\n\n@@ -154,7 +154,7 @@ class Trainer(base_trainer.Trainer):\n             )\n             return logs, state\n         \n-        if not self.run_eagerly and not self.jit_compile:\n+        if not self.run_eagerly and self.jit_compile:\n             @jax.jit\n             def train_step(state, data):\n                 return _train_step(state, data)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#ee5be68ce9bd993f8d66805cfe3638fffdb6e903", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 24 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 17 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 41 | Churn Cumulative: 915 | Contributors (this commit): 1 | Commits (past 90d): 12 | Contributors (cumulative): 2 | DMM Complexity: 0.4444444444444444\n\nDIFF:\n@@ -9,7 +9,6 @@ from keras_core.trainers.epoch_iterator import EpochIterator\n \n \n class Trainer(base_trainer.Trainer):\n-\n     def stateless_compute_loss_and_updates(\n         self, trainable_variables, non_trainable_variables, x, y, sample_weight\n     ):\n@@ -104,7 +103,9 @@ class Trainer(base_trainer.Trainer):\n                 model=self,\n             )\n \n-        grad_fn = jax.value_and_grad(self.stateless_compute_loss_and_updates, has_aux=True)\n+        grad_fn = jax.value_and_grad(\n+            self.stateless_compute_loss_and_updates, has_aux=True\n+        )\n \n         def _train_step(state, data):\n             (\n@@ -117,7 +118,11 @@ class Trainer(base_trainer.Trainer):\n                 data\n             )\n             (loss, (y_pred, non_trainable_variables)), grads = grad_fn(\n-                trainable_variables, non_trainable_variables, x, y, sample_weight\n+                trainable_variables,\n+                non_trainable_variables,\n+                x,\n+                y,\n+                sample_weight,\n             )\n \n             (\n@@ -155,9 +160,11 @@ class Trainer(base_trainer.Trainer):\n             return logs, state\n \n         if not self.run_eagerly and self.jit_compile:\n+\n             @jax.jit\n             def train_step(state, data):\n                 return _train_step(state, data)\n+\n         else:\n             train_step = _train_step\n \n\n@@ -49,7 +49,6 @@ class Progbar:\n             or \"posix\" in sys.modules\n             or \"PYCHARM_HOSTED\" in os.environ\n         )\n-        self._total_width = 0\n         self._seen_so_far = 0\n         # We use a dict + list to avoid garbage collection\n         # issues found in OrderedDict\n@@ -59,6 +58,7 @@ class Progbar:\n         self._last_update = 0\n         self._time_at_epoch_start = self._start\n         self._time_after_first_step = None\n+        self._prev_total_width = 0\n \n     def update(self, current, values=None, finalize=None):\n         \"\"\"Updates the progress bar.\n@@ -102,6 +102,7 @@ class Progbar:\n         self._seen_so_far = current\n \n         message = \"\"\n+        special_char_len = 0\n         now = time.time()\n         time_per_unit = self._estimate_step_duration(current, now)\n \n@@ -109,9 +110,8 @@ class Progbar:\n             if now - self._last_update < self.interval and not finalize:\n                 return\n \n-            prev_total_width = self._total_width\n             if self._dynamic_display:\n-                message += \"\\b\" * prev_total_width\n+                message += \"\\b\" * self._prev_total_width\n                 message += \"\\r\"\n             else:\n                 message += \"\\n\"\n@@ -125,13 +125,12 @@ class Progbar:\n \n                 if prog_width > 0:\n                     bar += \"\\33[32m\" + \"━\" * prog_width + \"\\x1b[0m\"\n-\n+                    special_char_len += 17\n                 bar += \"\\33[37m\" + \"━\" * (self.width - prog_width) + \"\\x1b[0m\"\n+                special_char_len += 9\n \n             else:\n                 bar = \"%7d/Unknown\" % current\n-\n-            self._total_width = len(bar)\n             message += bar\n \n             # Add ETA if applicable\n@@ -151,6 +150,7 @@ class Progbar:\n             else:\n                 # Time elapsed since start, in seconds\n                 info = f\" \\x1b[1m{now - self._start:.0f}s\\x1b[0m\"\n+            special_char_len += 8\n \n             # Add time/step\n             info += self._format_time(time_per_unit, self.unit_name)\n@@ -168,16 +168,16 @@ class Progbar:\n                         info += f\" {avg:.4e}\"\n                 else:\n                     info += f\" {self._values[k]}\"\n-\n-            self._total_width += len(info)\n-            if prev_total_width > self._total_width:\n-                info += \" \" * (prev_total_width - self._total_width)\n-\n-            if finalize:\n-                info += \"\\n\"\n-\n             message += info\n+\n+            total_width = len(bar) + len(info) - special_char_len\n+            if self._prev_total_width > total_width:\n+                message += \" \" * (self._prev_total_width - total_width)\n+            if finalize:\n+                message += \"\\n\"\n+\n             io_utils.print_msg(message, line_break=False)\n+            self._prev_total_width = total_width\n             message = \"\"\n \n         elif self.verbose == 2:\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f0525e5c07f443e038f0d41e90691548c21d5228", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 25 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 5 | Methods Changed: 6 | Complexity Δ (Sum/Max): 12/12 | Churn Δ: 26 | Churn Cumulative: 5585 | Contributors (this commit): 13 | Commits (past 90d): 18 | Contributors (cumulative): 15 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1245,6 +1245,7 @@ class BatchNormalizationBase(Layer):\n             mask_weights = tf.expand_dims(\n                 mask_weights, axis=-1, name=\"mask_weights_broadcasted\"\n             )\n+            mask_weights = _expand_tensor_with_local_replica_group(mask_weights)\n             mean, var = tf.nn.weighted_moments(\n                 replica_tensor,\n                 axes=updated_reduction_axes,\n\n@@ -67,9 +67,10 @@ class BatchNormalizationDTensorTest(test_util.DTensorBaseTest):\n         training=[True, False],\n         synchronized=[True, False],\n         renorm=[True, False],\n+        use_mask=[True, False],\n     )\n     def test_batch_normalization_with_dtensor_strategy(\n-        self, training, synchronized, renorm\n+        self, training, synchronized, renorm, use_mask\n     ):\n         num_replica = 2\n         local_batch_size = 4\n@@ -81,6 +82,26 @@ class BatchNormalizationDTensorTest(test_util.DTensorBaseTest):\n         replica_inputs = tf.reshape(\n             global_inputs, [num_replica, local_batch_size, *feature_shape]\n         )\n+        if use_mask:\n+            mask = tf.concat(\n+                [\n+                    tf.ones(shape=[global_batch_size, 2]),\n+                    tf.zeros(shape=[global_batch_size, 1]),\n+                ],\n+                axis=-1,\n+            )\n+            mask = tf.cast(mask, tf.bool)\n+            mask = tf.reshape(mask, [num_replica, local_batch_size, 3])\n+\n+            def value_fn(value_context):\n+                return {\n+                    \"inputs\": replica_inputs[\n+                        value_context.replica_id_in_sync_group\n+                    ],\n+                    \"mask\": mask[value_context.replica_id_in_sync_group],\n+                }\n+\n+        else:\n \n             def value_fn(value_context):\n                 return replica_inputs[value_context.replica_id_in_sync_group]\n@@ -121,6 +142,8 @@ class BatchNormalizationDTensorTest(test_util.DTensorBaseTest):\n     ):\n         @tf.function\n         def run_fn(inputs):\n+            if isinstance(inputs, dict):\n+                return bn_layer(**inputs, **run_kwargs)\n             return bn_layer(inputs, **run_kwargs)\n \n         distributed_inputs = (\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7fd5cfcf2d8b18265cc2141e529965b75adba494", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 17 | Files Changed: 2 | Hunks: 13 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 52 | Churn Cumulative: 573 | Contributors (this commit): 4 | Commits (past 90d): 12 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,5 @@\n-from keras_core import operations as ops\n from keras_core import backend\n+from keras_core import operations as ops\n from keras_core.losses.loss import Loss\n from keras_core.losses.loss import squeeze_to_same_rank\n \n@@ -27,7 +27,11 @@ class LossFunctionWrapper(Loss):\n class MeanSquaredError(LossFunctionWrapper):\n     \"\"\"Computes the mean of squares of errors between labels and predictions.\n \n-    `loss = mean(square(y_true - y_pred))`\n+    Formula:\n+\n+    ```python\n+    loss = mean(square(y_true - y_pred))\n+    ```\n \n     Args:\n         reduction: Type of reduction to apply to loss. For almost all cases\n@@ -45,7 +49,11 @@ class MeanSquaredError(LossFunctionWrapper):\n class MeanAbsoluteError(LossFunctionWrapper):\n     \"\"\"Computes the mean of absolute difference between labels and predictions.\n \n-    `loss = mean(abs(y_true - y_pred))`\n+    Formula:\n+\n+    ```python\n+    loss = mean(abs(y_true - y_pred))\n+    ```\n \n     Args:\n         reduction: Type of reduction to apply to loss. For almost all cases\n@@ -65,7 +73,9 @@ class MeanAbsolutePercentageError(LossFunctionWrapper):\n \n     Formula:\n \n-    `loss = 100 * mean(abs((y_true - y_pred) / y_true))`\n+    ```python\n+    loss = 100 * mean(abs((y_true - y_pred) / y_true))\n+    ```\n \n     Args:\n         reduction: Type of reduction to apply to loss. For almost all cases\n@@ -87,7 +97,11 @@ class MeanAbsolutePercentageError(LossFunctionWrapper):\n class MeanSquaredLogarithmicError(LossFunctionWrapper):\n     \"\"\"Computes the mean squared logarithmic error between `y_true` & `y_pred`.\n \n-    `loss = mean(square(log(y_true + 1.) - log(y_pred + 1.)))`\n+    Formula:\n+\n+    ```python\n+    loss = mean(square(log(y_true + 1) - log(y_pred + 1)))\n+    ```\n \n     Args:\n         reduction: Type of reduction to apply to loss. For almost all cases\n@@ -109,10 +123,11 @@ class MeanSquaredLogarithmicError(LossFunctionWrapper):\n def mean_squared_error(y_true, y_pred):\n     \"\"\"Computes the mean squared error between labels and predictions.\n \n-    After computing the squared distance between the inputs, the mean value over\n-    the last dimension is returned.\n+    Formula:\n \n-    `loss = mean(square(y_true - y_pred), axis=-1)`\n+    ```python\n+    loss = mean(square(y_true - y_pred), axis=-1)\n+    ```\n \n     Standalone usage:\n \n@@ -136,10 +151,9 @@ def mean_squared_error(y_true, y_pred):\n def mean_absolute_error(y_true, y_pred):\n     \"\"\"Computes the mean absolute error between labels and predictions.\n \n-    After computing the absolute distance between the inputs, the mean value\n-    over the last dimension is returned.\n-\n-    `loss = mean(abs(y_true - y_pred), axis=-1)`\n+    ```python\n+    loss = mean(abs(y_true - y_pred), axis=-1)\n+    ```\n \n     Standalone usage:\n \n@@ -166,9 +180,8 @@ def mean_absolute_percentage_error(y_true, y_pred):\n     `loss = 100 * mean(abs((y_true - y_pred) / y_true), axis=-1)`\n \n     Division by zero is prevented by dividing by `maximum(y_true, epsilon)`\n-    where epsilon defaults to `1e-7` and is equal to:\n-\n-    >>> epsilon = keras_core.backend.epsilon()\n+    where `epsilon = keras_core.backend.epsilon()`\n+    (default to `1e-7`).\n \n     Standalone usage:\n \n@@ -195,7 +208,11 @@ def mean_absolute_percentage_error(y_true, y_pred):\n def mean_squared_logarithmic_error(y_true, y_pred):\n     \"\"\"Computes the mean squared logarithmic error between `y_true` & `y_pred`.\n \n-    `loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1)`\n+    Formula:\n+\n+    ```python\n+    loss = mean(square(log(y_true + 1) - log(y_pred + 1)), axis=-1)\n+    ```\n \n     Note that `y_pred` and `y_true` cannot be less or equal to 0. Negative\n     values and 0 values will be replaced with `keras_core.backend.epsilon()`\n@@ -205,7 +222,7 @@ def mean_squared_logarithmic_error(y_true, y_pred):\n \n     >>> y_true = np.random.randint(0, 2, size=(2, 3))\n     >>> y_pred = np.random.random(size=(2, 3))\n-    >>> loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)\n+    >>> loss = keras_core.losses.mean_squared_logarithmic_error(y_true, y_pred)\n \n     Args:\n         y_true: Ground truth values with shape = `[batch_size, d0, .. dN]`.\n\n@@ -182,6 +182,7 @@ class MeanAbsolutePercentageErrorTest(testing.TestCase):\n         loss = mape_obj(y_true, y_pred, sample_weight=2.3)\n         self.assertAlmostEqual(loss, [621.8518, 352.6666])\n \n+\n class MeanSquaredLogarithmicErrorTest(testing.TestCase):\n     def test_config(self):\n         # TODO\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#73c0b8a442ee7fd2d8e92d95fe881c3c6e5eee6d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 854 | Contributors (this commit): 1 | Commits (past 90d): 13 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -185,7 +185,6 @@ class Trainer:\n         \"\"\"\n         del x  # The default implementation does not use `x`.\n         if self._compile_metrics is not None:\n-            print(\"Call compile_metrics.update\")\n             self._compile_metrics.update_state(y, y_pred, sample_weight)\n         return self.get_metrics_result()\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6d271fd7914cfc92cdcc267aa6e1ee42fce7c3f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 5 | Files Changed: 2 | Hunks: 4 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 10 | Churn Cumulative: 2305 | Contributors (this commit): 29 | Commits (past 90d): 13 | Contributors (cumulative): 47 | DMM Complexity: None\n\nDIFF:\n@@ -64,7 +64,7 @@ def load_data(\n           `oov_char` value in the dataset. When 0, no words are\n           skipped. Defaults to `0`.\n       maxlen: int or None. Maximum sequence length.\n-          Any longer sequence will be truncated. None, means no truncation.\n+          Any longer sequence will be truncated. `None`, means no truncation.\n           Defaults to `None`.\n       seed: int. Seed for reproducible data shuffling.\n       start_char: int. The start of a sequence will be marked with this\n\n@@ -68,14 +68,14 @@ def load_data(\n           all words are kept. Defaults to `None`.\n       skip_top: skip the top N most frequently occurring words\n           (which may not be informative). These words will appear as\n-          `oov_char` value in the dataset. 0 means no words are\n-          skipped. Defaults to 0\n+          `oov_char` value in the dataset. `0` means no words are\n+          skipped. Defaults to `0`.\n       maxlen: int or None. Maximum sequence length.\n-          Any longer sequence will be truncated. None means no truncation.\n+          Any longer sequence will be truncated. `None` means no truncation.\n           Defaults to `None`.\n       test_split: Float between 0 and 1. Fraction of the dataset to be used\n         as test data. 0.2 means that 20% of the dataset is used as\n-        test data. Defaults to 0.2\n+        test data. Defaults to `0.2`.\n       seed: int. Seed for reproducible data shuffling.\n       start_char: int. The start of a sequence will be marked with this\n           character. 0 is usually the padding character. Defaults to `1`.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#46449dca70b6cdc164097e073c6b6359a8dba47d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 2 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 325 | Contributors (this commit): 6 | Commits (past 90d): 6 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -65,10 +65,10 @@ class ReLU(Layer):\n       Same shape as the input.\n \n     Args:\n-      max_value: Float >= 0. Maximum activation value. None means unlimited.\n+      max_value: Float >= `0.`. Maximum activation value. `None` means unlimited.\n         Defaults to `None`.\n-      negative_slope: Float >= 0. Negative slope coefficient. Defaults to `0.`.\n-      threshold: Float >= 0. Threshold value for thresholded activation.\n+      negative_slope: Float >= `0.`. Negative slope coefficient. Defaults to `0.`.\n+      threshold: Float >= `0.`. Threshold value for thresholded activation.\n         Defaults to `0.`.\n     \"\"\"\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#63a2de2b8191abb8d0a7b910c8bad122b502e200", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 1722 | Contributors (this commit): 9 | Commits (past 90d): 5 | Contributors (cumulative): 9 | DMM Complexity: None\n\nDIFF:\n@@ -405,8 +405,7 @@ class PolynomialDecay(LearningRateSchedule):\n           end_learning_rate: A scalar `float32` or `float64` `Tensor` or a\n             Python number.  The minimal end learning rate.\n           power: A scalar `float32` or `float64` `Tensor` or a\n-            Python number. The power of the polynomial. Linear default.\n-            Defaults to `1.0`.\n+            Python number. The power of the polynomial. Defaults to `1.0`.\n           cycle: A boolean, whether it should cycle beyond decay_steps.\n           name: String.  Optional name of the operation. Defaults to\n             'PolynomialDecay'.\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0ebca04adac6604cb12babf2c5b5ef7ac95ba0ea", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 2850 | Contributors (this commit): 9 | Commits (past 90d): 7 | Contributors (cumulative): 11 | DMM Complexity: None\n\nDIFF:\n@@ -692,12 +692,12 @@ class OptimizerV2(tf.__internal__.tracking.Trackable):\n \n         Args:\n           grads_and_vars: List of (gradient, variable) pairs.\n-          name: Optional name for the returned operation. When None, uses the\n+          name: Optional name for the returned operation. When `None`, uses the\n             name passed to the `Optimizer` constructor. Defaults to `None`.\n           experimental_aggregate_gradients: Whether to sum gradients from\n             different replicas in the presence of `tf.distribute.Strategy`. If\n             False, it's user responsibility to aggregate the gradients. Default\n-            to True.\n+            to `True`.\n \n         Returns:\n           An `Operation` that applies the specified gradients. The `iterations`\n\n@@ -264,7 +264,7 @@ def polynomial_decay(\n       end_learning_rate: A scalar `float32` or `float64` `Tensor` or a Python\n         number.  The minimal end learning rate.\n       power: A scalar `float32` or `float64` `Tensor` or a Python number.  The\n-        power of the polynomial. Linear is default. Defaults to `1.0`.\n+        power of the polynomial. Defaults to `1.0`.\n       cycle: A boolean, whether it should cycle beyond decay_steps. Defaults to\n         `False`.\n       name: String. Optional name of the operation. Defaults to\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#524068db0ea27734e5ab49c80c0995586049c714", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 4 | Files Changed: 2 | Hunks: 4 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 8 | Churn Cumulative: 5017 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: None\n\nDIFF:\n@@ -40,7 +40,7 @@ class _ConfusionMatrixConditionCount(base_metric.Metric):\n         threshold values in [0, 1]. A threshold is compared with prediction\n         values to determine the truth value of predictions\n         (i.e., above the threshold is `true`, below is `false`). One metric\n-        value is generated for each threshold value. Defaults to 0.5.\n+        value is generated for each threshold value. Defaults to `0.5`.\n       name: (Optional) string name of the metric instance.\n       dtype: (Optional) data type of the metric result.\n     \"\"\"\n\n@@ -467,7 +467,7 @@ class MeanIoU(IoU):\n       sparse_y_pred: Whether predictions are encoded using integers or\n         dense floating point vectors. If `False`, the `tf.argmax` function\n         will be used to determine each sample's most likely associated label.\n-      axis: (Optional) -1 dimension contains the logits. Defaults to `-1`.\n+      axis: (Optional) The dimension containing the logits. Defaults to `-1`.\n \n     Standalone usage:\n \n@@ -583,7 +583,7 @@ class OneHotIoU(IoU):\n       sparse_y_pred: Whether predictions are encoded using natural numbers or\n         probability distribution vectors. If `False`, the `tf.argmax` function\n         will be used to determine each sample's most likely associated label.\n-      axis: (Optional) -1 dimension contains the logits. Defaults to `-1`.\n+      axis: (Optional) The dimension containing the logits. Defaults to `-1`.\n \n     Standalone usage:\n \n@@ -697,7 +697,7 @@ class OneHotMeanIoU(MeanIoU):\n       sparse_y_pred: Whether predictions are encoded using natural numbers or\n         probability distribution vectors. If `False`, the `tf.argmax` function\n         will be used to determine each sample's most likely associated label.\n-      axis: (Optional) -1 dimension contains the logits. Defaults to `-1`.\n+      axis: (Optional) The dimension containing the logits. Defaults to `-1`.\n \n     Standalone usage:\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#633f63c58df64dd24556f73577c7b4a0841cf3c5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 617 | Lines Deleted: 6 | Files Changed: 6 | Hunks: 8 | Methods Changed: 65 | Complexity Δ (Sum/Max): 141/66 | Churn Δ: 623 | Churn Cumulative: 1382 | Contributors (this commit): 3 | Commits (past 90d): 29 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -5,6 +5,7 @@ from tensorflow import nest\n \n from keras_core.backend.common import KerasVariable\n from keras_core.backend.common import standardize_dtype\n+from keras_core.backend.jax import nn\n from keras_core.backend.jax import numpy\n from keras_core.backend.jax import random\n from keras_core.backend.keras_tensor import KerasTensor\n@@ -240,9 +241,7 @@ class Variable(KerasVariable):\n         return self.value.__round__(ndigits)\n \n \n-### Shape / dtype inference util\n-\n-\n+# Shape / dtype inference util\n def compute_output_spec(fn, *args, **kwargs):\n     with StatelessScope():\n \n@@ -289,9 +288,7 @@ def compute_output_spec(fn, *args, **kwargs):\n         return nest.map_structure(convert_jax_spec_to_keras_tensor, jax_out)\n \n \n-### NumPy op delegation\n-\n-\n+# NumPy op delegation\n def execute(op_name, *args, **kwargs):\n     if hasattr(jnp, op_name):\n         op = getattr(jnp, op_name)\n\n@@ -0,0 +1,63 @@\n+import jax\n+from jax import nn as jnn\n+from jax import numpy as jnp\n+\n+\n+def relu(x):\n+    return jnn.relu(x)\n+\n+\n+def relu6(x):\n+    return jnn.relu6(x)\n+\n+\n+def sigmoid(x):\n+    return jnn.sigmoid(x)\n+\n+\n+def softplus(x):\n+    return jnn.softplus(x)\n+\n+\n+def softsign(x):\n+    return jnn.soft_sign(x)\n+\n+\n+def silu(x):\n+    return jnn.silu(x)\n+\n+\n+def swish(x):\n+    return jnn.swish(x)\n+\n+\n+def log_sigmoid(x):\n+    return jnn.log_sigmoid(x)\n+\n+\n+def leaky_relu(x, negative_slope=0.2):\n+    return jnn.leaky_relu(x, negative_slope=negative_slope)\n+\n+\n+def hard_sigmoid(x):\n+    return jnn.hard_sigmoid(x)\n+\n+\n+def elu(x):\n+    return jnn.elu(x)\n+\n+\n+def selu(x):\n+    return jnn.selu(x)\n+\n+\n+def gelu(x, approximate=True):\n+    return jnn.gelu(x, approximate)\n+\n+\n+def softmax(x):\n+    return jnn.softmax(x)\n+\n+\n+def log_softmax(x, axis=-1):\n+    return jnn.log_softmax(x, axis=axis)\n\n@@ -6,6 +6,7 @@ from keras_core.backend.common import standardize_dtype\n from keras_core.backend.keras_tensor import KerasTensor\n from keras_core.backend.stateless_scope import get_stateless_scope\n from keras_core.backend.stateless_scope import in_stateless_scope\n+from keras_core.backend.tensorflow import nn\n from keras_core.backend.tensorflow import numpy\n from keras_core.backend.tensorflow import random\n from keras_core.utils.naming import auto_name\n\n@@ -0,0 +1,63 @@\n+import tensorflow as tf\n+from tensorflow import nn as tfnn\n+\n+\n+def relu(x):\n+    return tfnn.relu(x)\n+\n+\n+def relu6(x):\n+    return tfnn.relu6(x)\n+\n+\n+def sigmoid(x):\n+    return tfnn.sigmoid(x)\n+\n+\n+def softplus(x):\n+    return tf.math.softplus(x)\n+\n+\n+def softsign(x):\n+    return tfnn.softsign(x)\n+\n+\n+def silu(x, beta=1.0):\n+    return tfnn.silu(x, beta=beta)\n+\n+\n+def swish(x):\n+    return x * sigmoid(x)\n+\n+\n+def log_sigmoid(x):\n+    return tf.math.log_sigmoid(x)\n+\n+\n+def leaky_relu(x, negative_slope=0.2):\n+    return tfnn.leaky_relu(x, alpha=negative_slope)\n+\n+\n+def hard_sigmoid(x):\n+    x = x / 6.0 + 0.5\n+    return tf.clip_by_value(x, 0.0, 1.0)\n+\n+\n+def elu(x):\n+    return tfnn.elu(x)\n+\n+\n+def selu(x):\n+    return tfnn.selu(x)\n+\n+\n+def gelu(x, approximate=True):\n+    return tfnn.gelu(x, approximate)\n+\n+\n+def softmax(x, axis=None):\n+    return tfnn.softmax(x, axis=axis)\n+\n+\n+def log_softmax(x, axis=None):\n+    return tfnn.log_softmax(x, axis=axis)\n\n@@ -24,3 +24,234 @@ conv_transpose\n \n ctc ??\n \"\"\"\n+\n+from keras_core import backend\n+from keras_core.backend import KerasTensor\n+from keras_core.backend import any_symbolic_tensors\n+from keras_core.operations.operation import Operation\n+\n+\n+class Relu(Operation):\n+    def call(self, x):\n+        return backend.nn.relu(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, x.dtype)\n+\n+\n+def relu(x):\n+    if any_symbolic_tensors((x,)):\n+        return Relu().symbolic_call(x)\n+    return backend.nn.relu(x)\n+\n+\n+class Relu6(Operation):\n+    def call(self, x):\n+        return backend.nn.relu6(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape, x.dtype)\n+\n+\n+def relu6(x):\n+    if any_symbolic_tensors((x,)):\n+        return Relu6().symbolic_call(x)\n+    return backend.nn.relu6(x)\n+\n+\n+class Sigmoid(Operation):\n+    def call(self, x):\n+        return backend.nn.sigmoid(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape)\n+\n+\n+def sigmoid(x):\n+    if any_symbolic_tensors((x,)):\n+        return Sigmoid().symbolic_call(x)\n+    return backend.nn.sigmoid(x)\n+\n+\n+class Softplus(Operation):\n+    def call(self, x):\n+        return backend.nn.softplus(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape)\n+\n+\n+def softplus(x):\n+    if any_symbolic_tensors((x,)):\n+        return Softplus().symbolic_call(x)\n+    return backend.nn.softplus(x)\n+\n+\n+class Softsign(Operation):\n+    def call(self, x):\n+        return backend.nn.softsign(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape)\n+\n+\n+def softsign(x):\n+    if any_symbolic_tensors((x,)):\n+        return Softsign().symbolic_call(x)\n+    return backend.nn.softsign(x)\n+\n+\n+class Silu(Operation):\n+    def call(self, x):\n+        return backend.nn.silu(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape)\n+\n+\n+def silu(x):\n+    if any_symbolic_tensors((x,)):\n+        return Silu().symbolic_call(x)\n+    return backend.nn.silu(x)\n+\n+\n+class Swish(Operation):\n+    def call(self, x):\n+        return backend.nn.swish(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape)\n+\n+\n+def swish(x):\n+    if any_symbolic_tensors((x,)):\n+        return Swish().symbolic_call(x)\n+    return backend.nn.swish(x)\n+\n+\n+class LogSigmoid(Operation):\n+    def call(self, x):\n+        return backend.nn.log_sigmoid(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape)\n+\n+\n+def log_sigmoid(x):\n+    if any_symbolic_tensors((x,)):\n+        return LogSigmoid().symbolic_call(x)\n+    return backend.nn.log_sigmoid(x)\n+\n+\n+class LeakyRelu(Operation):\n+    def __init__(self, negative_slope=0.2):\n+        super().__init__()\n+        self.negative_slope = negative_slope\n+\n+    def call(self, x):\n+        return backend.nn.leaky_relu(x, self.negative_slope)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape)\n+\n+\n+def leaky_relu(x, negative_slope=0.2):\n+    if any_symbolic_tensors((x,)):\n+        return LeakyRelu(negative_slope).symbolic_call(x)\n+    return backend.nn.leaky_relu(x, negative_slope=negative_slope)\n+\n+\n+class HardSigmoid(Operation):\n+    def call(self, x):\n+        return backend.nn.hard_sigmoid(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape)\n+\n+\n+def hard_sigmoid(x):\n+    if any_symbolic_tensors((x,)):\n+        return HardSigmoid().symbolic_call(x)\n+    return backend.nn.hard_sigmoid(x)\n+\n+\n+class Elu(Operation):\n+    def call(self, x):\n+        return backend.nn.elu(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape)\n+\n+\n+def elu(x):\n+    if any_symbolic_tensors((x,)):\n+        return Elu().symbolic_call(x)\n+    return backend.nn.elu(x)\n+\n+\n+class Selu(Operation):\n+    def call(self, x):\n+        return backend.nn.selu(x)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape)\n+\n+\n+def selu(x):\n+    if any_symbolic_tensors((x,)):\n+        return Selu().symbolic_call(x)\n+    return backend.nn.selu(x)\n+\n+\n+class Gelu(Operation):\n+    def __init__(self, approximate=True):\n+        super().__init__()\n+        self.approximate = approximate\n+\n+    def call(self, x):\n+        return backend.nn.gelu(x, self.approximate)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape)\n+\n+\n+def gelu(x, approximate=True):\n+    if any_symbolic_tensors((x,)):\n+        return Gelu(approximate).symbolic_call(x)\n+    return backend.nn.gelu(x, approximate)\n+\n+\n+class Softmax(Operation):\n+    def __init__(self, axis=None):\n+        super().__init__()\n+        self.axis = axis if axis is not None else -1\n+\n+    def call(self, x):\n+        return backend.nn.softmax(x, axis=self.axis)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape)\n+\n+\n+def softmax(x, axis=None):\n+    if any_symbolic_tensors((x,)):\n+        return Softmax(axis).symbolic_call(x)\n+    return backend.nn.softmax(x)\n+\n+\n+class LogSoftmax(Operation):\n+    def __init__(self, axis=None):\n+        super().__init__()\n+        self.axis = axis if axis is not None else -1\n+\n+    def call(self, x):\n+        return backend.nn.log_softmax(x, axis=self.axis)\n+\n+    def compute_output_spec(self, x):\n+        return KerasTensor(x.shape)\n+\n+\n+def log_softmax(x, axis=None):\n+    if any_symbolic_tensors((x,)):\n+        return LogSoftmax(axis).symbolic_call(x)\n+    return backend.nn.log_softmax(x, axis=axis)\n\n@@ -0,0 +1,256 @@\n+import numpy as np\n+import pytest\n+from tensorflow.python.ops.numpy_ops import np_config\n+\n+from keras_core import testing\n+from keras_core.backend import backend\n+from keras_core.backend.keras_tensor import KerasTensor\n+from keras_core.operations import nn as knn\n+\n+\n+@pytest.mark.skipif(\n+    backend() != \"tensorflow\",\n+    reason=\"Dynamic shapes are only supported in TensorFlow backend.\",\n+)\n+class NNOpsDynamicShapeTest(testing.TestCase):\n+    def test_relu(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.relu(x).shape, (None, 2, 3))\n+\n+    def test_relu6(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.relu6(x).shape, (None, 2, 3))\n+\n+    def test_sigmoid(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.sigmoid(x).shape, (None, 2, 3))\n+\n+    def test_softplus(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.softplus(x).shape, (None, 2, 3))\n+\n+    def test_softsign(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.softsign(x).shape, (None, 2, 3))\n+\n+    def test_silu(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.silu(x).shape, (None, 2, 3))\n+\n+    def test_swish(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.swish(x).shape, (None, 2, 3))\n+\n+    def test_log_sigmoid(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.log_sigmoid(x).shape, (None, 2, 3))\n+\n+    def test_leaky_relu(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.leaky_relu(x).shape, (None, 2, 3))\n+\n+    def test_hard_sigmoid(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.hard_sigmoid(x).shape, (None, 2, 3))\n+\n+    def test_elu(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.elu(x).shape, (None, 2, 3))\n+\n+    def test_selu(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.selu(x).shape, (None, 2, 3))\n+\n+    def test_gelu(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.gelu(x).shape, (None, 2, 3))\n+\n+    def test_softmax(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.softmax(x).shape, (None, 2, 3))\n+        self.assertEqual(knn.softmax(x, axis=1).shape, (None, 2, 3))\n+        self.assertEqual(knn.softmax(x, axis=-1).shape, (None, 2, 3))\n+\n+    def test_log_softmax(self):\n+        x = KerasTensor([None, 2, 3])\n+        self.assertEqual(knn.log_softmax(x).shape, (None, 2, 3))\n+        self.assertEqual(knn.log_softmax(x, axis=1).shape, (None, 2, 3))\n+        self.assertEqual(knn.log_softmax(x, axis=-1).shape, (None, 2, 3))\n+\n+\n+class NNOpsStaticShapeTest(testing.TestCase):\n+    def test_relu(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.relu(x).shape, (1, 2, 3))\n+\n+    def test_relu6(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.relu6(x).shape, (1, 2, 3))\n+\n+    def test_sigmoid(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.sigmoid(x).shape, (1, 2, 3))\n+\n+    def test_softplus(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.softplus(x).shape, (1, 2, 3))\n+\n+    def test_softsign(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.softsign(x).shape, (1, 2, 3))\n+\n+    def test_silu(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.silu(x).shape, (1, 2, 3))\n+\n+    def test_swish(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.swish(x).shape, (1, 2, 3))\n+\n+    def test_log_sigmoid(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.log_sigmoid(x).shape, (1, 2, 3))\n+\n+    def test_leaky_relu(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.leaky_relu(x).shape, (1, 2, 3))\n+\n+    def test_hard_sigmoid(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.hard_sigmoid(x).shape, (1, 2, 3))\n+\n+    def test_elu(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.elu(x).shape, (1, 2, 3))\n+\n+    def test_selu(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.selu(x).shape, (1, 2, 3))\n+\n+    def test_gelu(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.gelu(x).shape, (1, 2, 3))\n+\n+    def test_softmax(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.softmax(x).shape, (1, 2, 3))\n+        self.assertEqual(knn.softmax(x, axis=1).shape, (1, 2, 3))\n+        self.assertEqual(knn.softmax(x, axis=-1).shape, (1, 2, 3))\n+\n+    def test_log_softmax(self):\n+        x = KerasTensor([1, 2, 3])\n+        self.assertEqual(knn.log_softmax(x).shape, (1, 2, 3))\n+        self.assertEqual(knn.log_softmax(x, axis=1).shape, (1, 2, 3))\n+        self.assertEqual(knn.log_softmax(x, axis=-1).shape, (1, 2, 3))\n+\n+\n+class NNOpsCorrectnessTest(testing.TestCase):\n+    def test_relu(self):\n+        x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n+        self.assertAllClose(knn.relu(x), [0, 0, 1, 2, 3])\n+\n+    def test_relu6(self):\n+        x = np.array([-1, 0, 1, 2, 3, 4, 5, 6, 7], dtype=np.float32)\n+        self.assertAllClose(knn.relu6(x), [0, 0, 1, 2, 3, 4, 5, 6, 6])\n+\n+    def test_sigmoid(self):\n+        x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.sigmoid(x), [0.26894143, 0.5, 0.7310586, 0.880797, 0.95257413]\n+        )\n+\n+    def test_softplus(self):\n+        x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.softplus(x),\n+            [0.31326166, 0.6931472, 1.3132616, 2.126928, 3.0485873],\n+        )\n+\n+    def test_softsign(self):\n+        x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n+        self.assertAllClose(knn.softsign(x), [-0.5, 0, 0.5, 0.6666667, 0.75])\n+\n+    def test_silu(self):\n+        x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.silu(x),\n+            [-0.26894143, 0, 0.7310586, 1.7615942, 2.8577223],\n+        )\n+\n+    def test_swish(self):\n+        x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.swish(x), [-0.26894143, 0.0, 0.7310586, 1.7615943, 2.8577223]\n+        )\n+\n+    def test_log_sigmoid(self):\n+        x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.log_sigmoid(x),\n+            [-1.3132616, -0.6931472, -0.31326166, -0.126928, -0.04858732],\n+        )\n+\n+    def test_leaky_relu(self):\n+        x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.leaky_relu(x),\n+            [-0.2, 0, 1, 2, 3],\n+        )\n+\n+    def test_hard_sigmoid(self):\n+        x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.hard_sigmoid(x),\n+            [0.33333334, 0.5, 0.6666667, 0.8333334, 1.0],\n+        )\n+\n+    def test_elu(self):\n+        x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.elu(x),\n+            [-0.63212055, 0, 1, 2, 3],\n+        )\n+\n+    def test_selu(self):\n+        x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.selu(x),\n+            [-1.1113307, 0.0, 1.050701, 2.101402, 3.152103],\n+        )\n+\n+    def test_gelu(self):\n+        x = np.array([-1, 0, 1, 2, 3], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.gelu(x),\n+            [-0.15880796, 0.0, 0.841192, 1.9545977, 2.9963627],\n+        )\n+\n+    def test_softmax(self):\n+        x = np.array([1, 2, 3], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.softmax(x),\n+            [0.09003057, 0.24472848, 0.66524094],\n+        )\n+        self.assertAllClose(\n+            knn.softmax(x, axis=0),\n+            [0.09003057, 0.24472848, 0.66524094],\n+        )\n+        self.assertAllClose(\n+            knn.softmax(x, axis=-1),\n+            [0.09003057, 0.24472848, 0.66524094],\n+        )\n+\n+    def test_log_softmax(self):\n+        x = np.array([1, 2, 3], dtype=np.float32)\n+        self.assertAllClose(\n+            knn.log_softmax(x),\n+            [-2.407606, -1.4076059, -0.4076059],\n+        )\n+        self.assertAllClose(\n+            knn.log_softmax(x, axis=0),\n+            [-2.407606, -1.4076059, -0.4076059],\n+        )\n+        self.assertAllClose(\n+            knn.log_softmax(x, axis=-1),\n+            [-2.407606, -1.4076059, -0.4076059],\n+        )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0fa15a4b12b6b435cfc0ec526b5ff1bf603c32cd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 111 | Lines Deleted: 10 | Files Changed: 4 | Hunks: 11 | Methods Changed: 11 | Complexity Δ (Sum/Max): 9/6 | Churn Δ: 121 | Churn Cumulative: 1161 | Contributors (this commit): 2 | Commits (past 90d): 26 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -36,21 +36,21 @@ class MyModel(Model):\n         self.dense3 = MyDense(output_dim)\n \n     def call(self, x):\n-        x = self.dense1(x)\n-        x = self.dense2(x)\n+        x = jax.nn.relu(self.dense1(x))\n+        x = jax.nn.relu(self.dense2(x))\n         return self.dense3(x)\n \n \n def Dataset():\n     for _ in range(20):\n-        yield (np.random.random((32, 128)), np.random.random((32, 16)))\n+        yield (np.random.random((32, 128)), np.random.random((32, 4)))\n \n \n def loss_fn(y_true, y_pred):\n     return ops.sum((y_true - y_pred) ** 2)\n \n \n-model = MyModel(hidden_dim=256, output_dim=16)\n+model = MyModel(hidden_dim=256, output_dim=4)\n \n optimizer = optimizers.SGD(learning_rate=0.0001)\n dataset = Dataset()\n\n@@ -36,8 +36,8 @@ class MyModel(Model):\n         self.dense3 = MyDense(output_dim)\n \n     def call(self, x):\n-        x = self.dense1(x)\n-        x = self.dense2(x)\n+        x = tf.nn.relu(self.dense1(x))\n+        x = tf.nn.relu(self.dense2(x))\n         return self.dense3(x)\n \n \n@@ -45,7 +45,7 @@ def Dataset():\n     for _ in range(20):\n         yield (\n             np.random.random((32, 128)).astype(\"float32\"),\n-            np.random.random((32, 16)).astype(\"float32\"),\n+            np.random.random((32, 4)).astype(\"float32\"),\n         )\n \n \n@@ -53,9 +53,9 @@ def loss_fn(y_true, y_pred):\n     return ops.sum((y_true - y_pred) ** 2)\n \n \n-model = MyModel(hidden_dim=256, output_dim=16)\n+model = MyModel(hidden_dim=256, output_dim=4)\n \n-optimizer = optimizers.SGD(learning_rate=0.0001)\n+optimizer = optimizers.SGD(learning_rate=0.001)\n dataset = Dataset()\n \n \n\n@@ -1,6 +1,7 @@\n import contextlib\n import warnings\n \n+import numpy as np\n import tensorflow as tf\n from tensorflow.python.eager import context as tf_context\n \n@@ -330,7 +331,52 @@ class Trainer(base_trainer.Trainer):\n     def predict(\n         self, x, batch_size=None, verbose=\"auto\", steps=None, callbacks=None\n     ):\n-        raise NotImplementedError\n+        # Create an iterator that yields batches of input data.\n+        epoch_iterator = TFEpochIterator(\n+            x=x,\n+            batch_size=batch_size,\n+            steps_per_epoch=steps,\n+            shuffle=False,\n+        )\n+\n+        # Container that configures and calls callbacks.\n+        if not isinstance(callbacks, callbacks_module.CallbackList):\n+            callbacks = callbacks_module.CallbackList(\n+                callbacks,\n+                add_history=True,\n+                add_progbar=verbose != 0,\n+                verbose=verbose,\n+                epochs=1,\n+                steps=epoch_iterator.num_batches,\n+                model=self,\n+            )\n+\n+        self.make_predict_function()\n+        callbacks.on_predict_begin()\n+        outputs = None\n+        with epoch_iterator.catch_stop_iteration():\n+            for step, iterator in epoch_iterator.enumerate_epoch():\n+                callbacks.on_predict_batch_begin(step)\n+                batch_outputs = self.predict_function(iterator)\n+                if outputs is None:\n+                    outputs = tf.nest.map_structure(\n+                        lambda batch_output: [batch_output],\n+                        batch_outputs,\n+                    )\n+                else:\n+                    tf.__internal__.nest.map_structure_up_to(\n+                        batch_outputs,\n+                        lambda output, batch_output: output.append(\n+                            batch_output\n+                        ),\n+                        outputs,\n+                        batch_outputs,\n+                    )\n+                callbacks.on_predict_batch_end(step, {\"outputs\": batch_outputs})\n+        callbacks.on_predict_end()\n+        return tf.__internal__.nest.map_structure_up_to(\n+            batch_outputs, np.concatenate, outputs\n+        )\n \n \n class TFEpochIterator(EpochIterator):\n\n@@ -26,6 +26,28 @@ class ExampleModel(layers.Dense, Trainer):\n         Trainer.__init__(self)\n \n \n+class OutputStructModel(layers.Layer, Trainer):\n+    def __init__(self, units):\n+        layers.Layer.__init__(self)\n+        Trainer.__init__(self)\n+        self.dense_1 = layers.Dense(\n+            units,\n+            use_bias=False,\n+            kernel_initializer=initializers.Ones(),\n+        )\n+        self.dense_2 = layers.Dense(\n+            units,\n+            use_bias=False,\n+            kernel_initializer=initializers.Ones(),\n+        )\n+\n+    def call(self, x):\n+        return {\n+            \"y_one\": self.dense_1(x),\n+            \"y_two\": self.dense_2(x),\n+        }\n+\n+\n class TestTrainer(testing.TestCase):\n     def test_metric_tracking(self):\n         class ModelWithMetric(layers.Dense, Trainer):\n@@ -124,3 +146,36 @@ class TestTrainer(testing.TestCase):\n \n     def test_evaluate_flow_jit(self):\n         self._test_evaluate_flow(run_eagerly=False, jit_compile=True)\n+\n+    def _test_predict_flow(self, run_eagerly, jit_compile):\n+        # Test basic example\n+        model = ExampleModel(units=3)\n+        model.run_eagerly = run_eagerly\n+        model.jit_compile = jit_compile\n+\n+        x = np.ones((100, 4))\n+        batch_size = 16\n+        outputs = model.predict(x, batch_size=batch_size)\n+        self.assertAllClose(outputs, 4 * np.ones((100, 3)))\n+\n+        # Test with output struct\n+        model = OutputStructModel(units=3)\n+        model.run_eagerly = run_eagerly\n+        model.jit_compile = jit_compile\n+\n+        x = np.ones((100, 4))\n+        batch_size = 16\n+        outputs = model.predict(x, batch_size=batch_size)\n+        self.assertTrue(isinstance(outputs, dict))\n+        self.assertEqual(len(outputs), 2)\n+        self.assertAllClose(outputs[\"y_one\"], 4 * np.ones((100, 3)))\n+        self.assertAllClose(outputs[\"y_two\"], 4 * np.ones((100, 3)))\n+\n+    def test_predicte_flow_eager(self):\n+        self._test_predict_flow(run_eagerly=True, jit_compile=False)\n+\n+    def test_predict_flow_graph_fn(self):\n+        self._test_predict_flow(run_eagerly=False, jit_compile=False)\n+\n+    def test_predict_flow_jit(self):\n+        self._test_predict_flow(run_eagerly=False, jit_compile=True)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
