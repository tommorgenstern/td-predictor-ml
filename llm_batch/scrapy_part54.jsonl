{"custom_id": "scrapy#abda5edf091383c96e7c5ceef17f79f44a922334", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 18/18 | Churn Δ: 2 | Churn Cumulative: 2 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -83,7 +83,7 @@ class DecompressionMiddleware(object):\n         return (response, None)\n \n     def process_response(self, request, response, spider):\n-        if isinstance(response, Response):\n+        if isinstance(response, Response) and response.body:\n             response, format = self.extract(response)\n             if format:\n                 log.msg('Decompressed response with format: %s' % format, log.DEBUG, domain=spider.domain_name)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#0f690b03dc216424f7c92bf06a2ba0864bfa390f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 6 | Churn Cumulative: 30 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,5 @@\n+import warnings\n+\n from scrapy.core.exceptions import IgnoreRequest\n from scrapy.utils.response import response_status_message\n \n@@ -8,6 +10,10 @@ class ErrorPagesMiddleware(object):\n     TODO: move this mw to spidermiddleware and remove me\n     \"\"\"\n \n+    def __init__(self):\n+        warnings.warn(\"scrapy.contrib.downloadermiddleware.errorpages.ErrorPagesMiddleware downloader middleware is deprecated - use scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware spider middleware instead\",\n+            DeprecationWarning, stacklevel=2)\n+\n     def process_response(self, request, response, spider):\n         status = response.status\n         if 200 <= status < 300 or status in getattr(spider, 'handle_httpstatus_list', []):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#cfafa011090cb4fc02776d6f4de11ba5e86dd439", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 16 | Files Changed: 1 | Hunks: 4 | Methods Changed: 6 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 28 | Churn Cumulative: 33 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -14,10 +14,7 @@ from scrapy.utils.middleware import build_middleware_list\n from scrapy.conf import settings\n \n def _isiterable(possible_iterator):\n-    try:\n-        return iter(possible_iterator)\n-    except TypeError:\n-        return None\n+    return hasattr(possible_iterator, '__iter__')\n \n class SpiderMiddlewareManager(object):\n     def __init__(self):\n@@ -63,16 +60,7 @@ class SpiderMiddlewareManager(object):\n                     (fname(method), type(result))\n                 if result is not None:\n                     return result\n-            return self.call(request=request, response=response, spider=spider)\n-\n-\n-        def process_spider_output(result):\n-            for method in self.result_middleware:\n-                result = method(response=response, result=result, spider=spider)\n-                assert _isiterable(result), \\\n-                    'Middleware %s must returns an iterable object, got %s ' % \\\n-                    (fname(method), type(result))\n-            return result\n+            return self.call(request, response, spider)\n \n         def process_spider_exception(_failure):\n             exception = _failure.value\n@@ -85,12 +73,20 @@ class SpiderMiddlewareManager(object):\n                     return result\n             return _failure\n \n+        def process_spider_output(result):\n+            for method in self.result_middleware:\n+                result = method(response=response, result=result, spider=spider)\n+                assert _isiterable(result), \\\n+                    'Middleware %s must returns an iterable object, got %s ' % \\\n+                    (fname(method), type(result))\n+            return result\n+\n         dfd = mustbe_deferred(process_spider_input, response)\n         dfd.addErrback(process_spider_exception)\n         dfd.addCallback(process_spider_output)\n         return dfd\n \n-    def call(self, request, response, spider):\n-        defer_result(response).chainDeferred(request.deferred)\n+    def call(self, request, result, spider):\n+        defer_result(result).chainDeferred(request.deferred)\n         request.deferred.addCallback(arg_to_iter)\n         return request.deferred\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#727e67af5e82128839f084196809fa745bc8921f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 16 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 3 | Methods Changed: 3 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 17 | Churn Cumulative: 50 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -11,6 +11,7 @@ from scrapy.core.exceptions import NotConfigured\n from scrapy.utils.misc import load_object, arg_to_iter\n from scrapy.utils.defer import mustbe_deferred, defer_result\n from scrapy.utils.middleware import build_middleware_list\n+from scrapy.http import Request\n from scrapy.conf import settings\n \n def _isiterable(possible_iterator):\n@@ -79,7 +80,8 @@ class SpiderMiddlewareManager(object):\n                 assert _isiterable(result), \\\n                     'Middleware %s must returns an iterable object, got %s ' % \\\n                     (fname(method), type(result))\n-            return result\n+\n+            return self._validate_output(request, result)\n \n         dfd = mustbe_deferred(process_spider_input, response)\n         dfd.addErrback(process_spider_exception)\n@@ -90,3 +92,16 @@ class SpiderMiddlewareManager(object):\n         defer_result(result).chainDeferred(request.deferred)\n         request.deferred.addCallback(arg_to_iter)\n         return request.deferred\n+\n+    def _validate_output(self, request, result):\n+        \"\"\"Every request returned by spiders must be instanciate with a callback\"\"\"\n+        for r in result:\n+            if isinstance(r, Request) and not r.deferred.callbacks:\n+                log.msg('Ignoring %s returned from spider while processing %s: ' \\\n+                        'Request has no callback, try adding callback=self.parse ' \\\n+                        'argument when instanciating Request objects inside your spiders' \\\n+                        % (r, request), level=log.WARNING)\n+                continue\n+            yield r\n+\n+\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#87fbc9c58c6877c1e600354dd898e0014e6ef687", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 4 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 56 | Contributors (this commit): 2 | Commits (past 90d): 5 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -81,7 +81,7 @@ class SpiderMiddlewareManager(object):\n                     'Middleware %s must returns an iterable object, got %s ' % \\\n                     (fname(method), type(result))\n \n-            return self._validate_output(request, result)\n+            return self._validate_output(request, result, spider)\n \n         dfd = mustbe_deferred(process_spider_input, response)\n         dfd.addErrback(process_spider_exception)\n@@ -93,14 +93,14 @@ class SpiderMiddlewareManager(object):\n         request.deferred.addCallback(arg_to_iter)\n         return request.deferred\n \n-    def _validate_output(self, request, result):\n+    def _validate_output(self, request, result, spider):\n         \"\"\"Every request returned by spiders must be instanciate with a callback\"\"\"\n         for r in result:\n             if isinstance(r, Request) and not r.deferred.callbacks:\n                 log.msg('Ignoring %s returned from spider while processing %s: ' \\\n                         'Request has no callback, try adding callback=self.parse ' \\\n                         'argument when instanciating Request objects inside your spiders' \\\n-                        % (r, request), level=log.WARNING)\n+                        % (r, request), level=log.WARNING, domain=spider.domain_name)\n                 continue\n             yield r\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6524def4b8e6d46037a6b5f4020490826fffde1d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 76/76 | Churn Δ: 3 | Churn Cumulative: 3 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -196,8 +196,7 @@ class RobustScrapedItem(ScrapedItem):\n \n     def validate(self):\n         \"\"\"Method used to validate item attributes data\"\"\"\n-        if not self.guid:\n-            raise ValidationError('A guid is required')\n+        raise NotImplemented\n \n     @property\n     def version(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#eca05c9e12bf7184f309a2d407eac10206ef762c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 15 | Churn Cumulative: 15 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3,16 +3,13 @@ OffsiteMiddleware: Filters out Requests for URLs outside the domains covered by\n the spider.\n \"\"\"\n \n-from scrapy import log\n-from scrapy.http import Request\n+from itertools import ifilter\n+\n+from scrapy.item import ScrapedItem\n from scrapy.utils.url import url_is_from_spider\n \n class OffsiteMiddleware(object):\n-    def process_spider_output(self, response, result, spider):\n-        def _filter(r):\n-            if isinstance(r, Request) and not url_is_from_spider(r.url, spider):\n-                log.msg(\"Ignoring link (offsite): %s \" % r.url, level=log.DEBUG, domain=spider.domain_name)\n-                return False\n-            return True\n-        return (r for r in result or () if _filter(r))\n \n+    def process_spider_output(self, response, result, spider):\n+        filter = lambda x: isinstance(x, ScrapedItem) or url_is_from_spider(x.url, spider)\n+        return ifilter(filter, result or ())\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a74b0b1764ef72e0b7d4f618dbbde0f9e0adf278", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 7 | Churn Cumulative: 22 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -3,13 +3,10 @@ OffsiteMiddleware: Filters out Requests for URLs outside the domains covered by\n the spider.\n \"\"\"\n \n-from itertools import ifilter\n-\n-from scrapy.item import ScrapedItem\n+from scrapy.http import Request\n from scrapy.utils.url import url_is_from_spider\n \n class OffsiteMiddleware(object):\n \n     def process_spider_output(self, response, result, spider):\n-        filter = lambda x: isinstance(x, ScrapedItem) or url_is_from_spider(x.url, spider)\n-        return ifilter(filter, result or ())\n+        return (x for x in result if not isinstance(x, Request) or url_is_from_spider(x.url, spider))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#962dbeba88fd9a2e28ac5bfe109268eb307d3e34", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 2 | Churn Cumulative: 2 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -1,5 +1,5 @@\n \"\"\"\n-This module implements the XmlRpcRequest class which is a more covenient class\n+This module implements the XmlRpcRequest class which is a more convenient class\n (that Request) to generate xml-rpc requests.\n \n See documentation in docs/ref/request-response.rst\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#4a1a01354bd96bb0cf418056679cb3de2df3906c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 26 | Lines Deleted: 28 | Files Changed: 7 | Hunks: 22 | Methods Changed: 22 | Complexity Δ (Sum/Max): 69/30 | Churn Δ: 54 | Churn Cumulative: 71 | Contributors (this commit): 2 | Commits (past 90d): 17 | Contributors (cumulative): 14 | DMM Complexity: 0.5\n\nDIFF:\n@@ -9,7 +9,7 @@ from scrapy.dupefilter import dupefilter\n class DuplicatesFilterMiddleware(object):\n     \"\"\"Filter out already seen requests to avoid visiting pages more than once.\"\"\"\n \n-    def enqueue_request(self, domain, request, priority):\n+    def enqueue_request(self, domain, request):\n         added = dupefilter.add(domain, request)\n         if not (added or request.dont_filter):\n             raise IgnoreRequest('Skipped (already seen request)')\n\n@@ -33,7 +33,7 @@ class RulesScheduler(Scheduler):\n     # How often we should process pages that have not changed (need to include depth)\n     MIN_PROCESS_UNCHANGED_DAYS = 12\n \n-    def enqueue_request(self, domain, request, priority=1):\n+    def enqueue_request(self, domain, request):\n         \"\"\"Add a page to be scraped for a domain that is currently being scraped.\n \n         The url will only be added if we have not checked it already within\n@@ -55,7 +55,7 @@ class RulesScheduler(Scheduler):\n                     return\n             # put the version in the pending pages to avoid querying DB again\n             record = (request, version, now)\n-            self.pending_requests[domain].put(record, priority)\n+            self.pending_requests[domain].push(record, request.priority)\n \n     def next_request(self, domain):\n         \"\"\"Get the next page from the superclass. This will add a callback\n\n@@ -219,7 +219,7 @@ class ExecutionEngine(object):\n     def open_domains(self):\n         return self.downloader.sites.keys()\n \n-    def crawl(self, request, spider, priority=0, domain_priority=0):\n+    def crawl(self, request, spider, domain_priority=0):\n         domain = spider.domain_name\n \n         def _process(response):\n@@ -242,7 +242,7 @@ class ExecutionEngine(object):\n                     piped.addBoth(_onpipelinefinish, item)\n                 elif isinstance(item, Request):\n                     signals.send_catch_log(signal=signals.request_received, sender=self.__class__, request=item, spider=spider, response=response)\n-                    self.crawl(request=item, spider=spider, priority=priority)\n+                    self.crawl(request=item, spider=spider)\n                 elif item is None:\n                     pass # may be next time.\n                 else:\n@@ -276,14 +276,14 @@ class ExecutionEngine(object):\n             request.deferred.addErrback(lambda _:None)\n             request.deferred.errback(_failure) #TODO: merge into spider middleware.\n \n-        schd = self.schedule(request, spider, priority, domain_priority)\n+        schd = self.schedule(request, spider, domain_priority)\n         schd.addCallbacks(_process, _cleanfailure)\n         return schd\n \n     def scrape(self, request, response, spider):\n         return self.spidermiddleware.scrape(request, response, spider)\n \n-    def schedule(self, request, spider, priority=0, domain_priority=0):\n+    def schedule(self, request, spider, domain_priority=0):\n         domain = spider.domain_name\n         if not self.scheduler.domain_is_open(domain):\n             if self.debug_mode: \n@@ -291,7 +291,7 @@ class ExecutionEngine(object):\n             return self._add_starter(request, spider, domain_priority)\n         if self.debug_mode: \n             log.msg('Scheduling %s (now)' % request_info(request), log.DEBUG)\n-        schd = self.schedulermiddleware.enqueue_request(domain, request, priority)\n+        schd = self.schedulermiddleware.enqueue_request(domain, request)\n         self.next_request(spider)\n         return schd\n \n@@ -343,7 +343,8 @@ class ExecutionEngine(object):\n                 return response\n             elif isinstance(response, Request):\n                 redirected = response # proper alias\n-                schd = self.schedule(redirected, spider, priority=self.REDIRECTION_PRIORITY)\n+                redirected.priority = self.REDIRECTION_PRIORITY\n+                schd = self.schedule(redirected, spider)\n                 chain_deferred(schd, redirected.deferred)\n                 return schd\n \n\n@@ -44,16 +44,16 @@ class SchedulerMiddlewareManager(object):\n         log.msg(\"Enabled scheduler middlewares: %s\" % \", \".join([type(m).__name__ for m in mws]))\n         self.loaded = True\n \n-    def enqueue_request(self, domain, request, priority):\n+    def enqueue_request(self, domain, request):\n         def _enqueue_request(request):\n             for method in self.mw_enqueue_request:\n-                result = method(domain=domain, request=request, priority=priority)\n+                result = method(domain=domain, request=request)\n                 assert result is None or isinstance(result, (Response, Deferred)), \\\n                         'Middleware %s.enqueue_request must return None, Response or Deferred, got %s' % \\\n                         (method.im_self.__class__.__name__, result.__class__.__name__)\n                 if result:\n                     return result\n-            return self.scheduler.enqueue_request(domain=domain, request=request, priority=priority)\n+            return self.scheduler.enqueue_request(domain=domain, request=request)\n \n         deferred = mustbe_deferred(_enqueue_request, request)\n         return deferred\n\n@@ -62,7 +62,7 @@ class Scheduler(object) :\n     def next_domain(self) :\n         \"\"\"Return next domain available to scrape and remove it from available domains queue\"\"\"\n         if self.pending_domains_count:\n-            domain, priority = self.domains_queue.pop()\n+            domain = self.domains_queue.pop()[0]\n             if self.pending_domains_count[domain] == 1:\n                 del self.pending_domains_count[domain]\n             else:\n@@ -90,10 +90,10 @@ class Scheduler(object) :\n         Priority = PriorityStack if self.dfo else PriorityQueue\n         self.pending_requests[domain] = Priority()\n \n-    def enqueue_request(self, domain, request, priority=0):\n+    def enqueue_request(self, domain, request):\n         \"\"\"Enqueue a request to be downloaded for a domain that is currently being scraped.\"\"\"\n         dfd = defer.Deferred()\n-        self.pending_requests[domain].push((request, dfd), priority)\n+        self.pending_requests[domain].push((request, dfd), request.priority)\n         return dfd\n \n     def next_request(self, domain):\n@@ -107,9 +107,8 @@ class Scheduler(object) :\n \n         \"\"\"\n         try:\n-            # The second value is the request scheduled priority, returns the first one.\n-            return self.pending_requests[domain].pop()[0]\n-        except (KeyError, IndexError), ex:\n+            return self.pending_requests[domain].pop()[0] # [1] is priority\n+        except (KeyError, IndexError):\n             return (None, None)\n \n     def close_domain(self, domain) :\n\n@@ -5,7 +5,6 @@ requests in Scrapy.\n See documentation in docs/ref/request-response.rst\n \"\"\"\n \n-import urllib\n import copy\n \n from twisted.internet import defer\n@@ -13,18 +12,18 @@ from twisted.internet import defer\n from scrapy.http.url import Url\n from scrapy.http.headers import Headers\n from scrapy.utils.url import safe_url_string\n-from scrapy.utils.defer import chain_deferred\n \n class Request(object):\n \n     def __init__(self, url, callback=None, method='GET', headers=None, body=None, \n-                 cookies=None, meta=None, encoding='utf-8', dont_filter=False,\n-                 errback=None):\n+                 cookies=None, meta=None, encoding='utf-8', priority=0.0,\n+                 dont_filter=False, errback=None):\n \n         self._encoding = encoding  # this one has to be set first\n         self.method = method.upper()\n         self.set_url(url)\n         self.set_body(body)\n+        self.priority = priority\n \n         if callable(callback):\n             callback = defer.Deferred().addCallbacks(callback, errback)\n\n@@ -1,7 +1,6 @@\n import unittest\n \n-from scrapy.spider import spiders\n-from scrapy.http import Request, Response\n+from scrapy.http import Request\n from scrapy.core.exceptions import IgnoreRequest\n from scrapy.contrib.schedulermiddleware.duplicatesfilter import DuplicatesFilterMiddleware\n from scrapy.dupefilter import dupefilter\n@@ -25,7 +24,7 @@ class DuplicatesFilterMiddlewareTest(unittest.TestCase):\n         r3 = Request('http://scrapytest.org/2')\n         r4 = Request('http://scrapytest.org/1')\n \n-        assert not mw.enqueue_request(domain, r1, 1)\n-        assert not mw.enqueue_request(domain, r2, 1)\n-        self.assertRaises(IgnoreRequest, mw.enqueue_request, domain, r3, 1)\n-        self.assertRaises(IgnoreRequest, mw.enqueue_request, domain, r4, 1)\n+        assert not mw.enqueue_request(domain, r1)\n+        assert not mw.enqueue_request(domain, r2)\n+        self.assertRaises(IgnoreRequest, mw.enqueue_request, domain, r3)\n+        self.assertRaises(IgnoreRequest, mw.enqueue_request, domain, r4)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#7c2476bb25fbfd40118d660a64fb2d81c5e14cd1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 3 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 39/27 | Churn Δ: 7 | Churn Cumulative: 7 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 4 | DMM Complexity: 1.0\n\nDIFF:\n@@ -44,8 +44,8 @@ class RobotsTxtMiddleware(object):\n         else:\n             self._parsers[urldomain] = None\n             robotsurl = \"%s://%s/robots.txt\" % parsedurl[0:2]\n-            robotsreq = Request(robotsurl)\n-            dfd = scrapyengine.schedule(robotsreq, spiders.fromdomain(spiderdomain), priority=self.DOWNLOAD_PRIORITY)\n+            robotsreq = Request(robotsurl, priority=self.DOWNLOAD_PRIORITY)\n+            dfd = scrapyengine.schedule(robotsreq, spiders.fromdomain(spiderdomain))\n             dfd.addCallbacks(callback=self._parse_robots, callbackArgs=[urldomain])\n             self._spiderdomains[spiderdomain].add(urldomain)\n \n\n@@ -122,7 +122,8 @@ class MediaPipeline(object):\n         request fingerprint is used as cache key.\n \n         \"\"\"\n-        return scrapyengine.schedule(request, info.spider, priority=self.DOWNLOAD_PRIORITY)\n+        request.priority = self.DOWNLOAD_PRIORITY\n+        return scrapyengine.schedule(request, info.spider)\n \n     def media_to_download(self, request, info):\n         \"\"\" Ongoing request hook pre-cache\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#fd0e490157f0ec5c6594347ff5d9437d14dbf13f", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 30 | Churn Cumulative: 120 | Contributors (this commit): 2 | Commits (past 90d): 10 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -189,6 +189,8 @@ STATS_DEBUG = False\n STATS_WSPORT = 8089\n STATS_WSTIMEOUT = 15\n \n+STATSMAILER_RCPTS = []\n+\n TEMPLATES_DIR = abspath(join(dirname(__file__), '..', 'templates'))\n \n URLLENGTH_LIMIT = 2083\n\n@@ -0,0 +1,28 @@\n+\"\"\"\n+StatsMailer extension sends an email when a domain finishes scraping.\n+\n+Use STATSMAILER_RCPTS setting to enable and give the recipient mail address\n+\"\"\"\n+\n+import pprint\n+\n+from pydispatch import dispatcher\n+\n+from scrapy.stats import stats\n+from scrapy.mail import MailSender\n+from scrapy.conf import settings\n+from scrapy.core import signals\n+from scrapy.core.exceptions import NotConfigured\n+\n+class StatsMailer(object):\n+\n+    def __init__(self):\n+        self.recipients = settings.getlist(\"STATSMAILER_RCPTS\")\n+        if not self.recipients:\n+            raise NotConfigured\n+        dispatcher.connect(self.send_stats, signal=signals.domain_closed)\n+        \n+    def send_stats(self, domain):\n+        mail = MailSender()\n+        body = pprint.pformat(stats[domain])\n+        mail.send(self.recipients, \"Scrapy stats for: %s\" % domain, body)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a8d430b4ddd5e4cda41065eac3f12b9a459f7876", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 215 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -117,7 +117,7 @@ class Cache(object):\n         if expiration_secs >= 0:\n             expiration_date = metadata['timestamp'] + datetime.timedelta(seconds=expiration_secs)\n             if datetime.datetime.utcnow() > expiration_date:\n-                log.msg('dropping old cached response from %s' % metadata['timestamp'], level=log.DEBUG)\n+                log.msg('dropping old cached response from %s' % metadata['timestamp'], level=log.DEBUG, domain=domain)\n                 return None\n         return metadata\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#1d8cec63d16c96ac76484f75874af4e528e58b6c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 4 | Files Changed: 1 | Hunks: 6 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 10 | Churn Cumulative: 35 | Contributors (this commit): 2 | Commits (past 90d): 3 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -34,18 +34,20 @@ def start(logfile=None, loglevel=None, log_stdout=None):\n     \"\"\"Initialize and start logging facility\"\"\"\n     global log_level, started\n \n+    # set loglevel\n+    loglevel = loglevel or settings['LOGLEVEL']\n+    log_level = globals()[loglevel] if loglevel else DEBUG\n     if started or not settings.getbool('LOG_ENABLED'):\n         return\n+    started = True\n \n+    # set log observer\n+    if log.defaultObserver: # check twisted log not already started\n         logfile = logfile or settings['LOGFILE']\n-    loglevel = loglevel or settings['LOGLEVEL']\n         log_stdout = log_stdout or settings.getbool('LOG_STDOUT')\n \n         file = open(logfile, 'a') if logfile else sys.stderr\n-    level = int(getattr(sys.modules[__name__], loglevel)) if loglevel else DEBUG\n         log.startLogging(file, setStdout=log_stdout)\n-    log_level = level\n-    started = True\n \n def msg(message, level=INFO, component=BOT_NAME, domain=None):\n     \"\"\"Log message according to the level\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ff76f46d5a677c9a3213d84b4638fd506c4a5211", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 35/35 | Churn Δ: 4 | Churn Cumulative: 4 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -3,6 +3,7 @@ from __future__ import with_statement\n import sys\n import os\n import optparse\n+import cProfile\n \n import scrapy\n from scrapy import log\n@@ -134,10 +135,9 @@ def execute_with_args(argv):\n \n     cmd.process_options(args, opts)\n     spiders.load()\n-    log.start() # start logging\n+    log.start()\n     if opts.profile:\n         log.msg(\"Profiling enabled. Analyze later with: python -m pstats %s\" % opts.profile)\n-        import cProfile\n         loc = locals()\n         p = cProfile.Profile()\n         p.runctx('ret = cmd.run(args, opts)', globals(), loc)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#aeb9734a802f44a42b0e73a5776eac303c58c39b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 3 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 51/51 | Churn Δ: 3 | Churn Cumulative: 3 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -73,7 +73,10 @@ class Downloader(object):\n         domain = spider.domain_name\n         site = self.sites[domain]\n         if not site or site.closed:\n+            if self.debug_mode:\n                 raise IgnoreRequest('Unable to fetch (domain already closed): %s' % request)\n+            else:\n+                raise IgnoreRequest\n \n         site.active.add(request)\n         def _deactivate(_):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5e3ef5a2fd0f056e04354d81b33327dffdf8681b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 19/19 | Churn Δ: 4 | Churn Cumulative: 4 | Contributors (this commit): 2 | Commits (past 90d): 2 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -51,10 +51,10 @@ class ItemPipelineManager(object):\n \n         This pipeline is configurable with the ITEM_PIPELINES setting\n         \"\"\"\n-        if not self.pipeline:\n+        domain = spider.domain_name\n+        if not self.pipeline or domain not in self.domaininfo:\n             return defer_succeed(item)\n \n-        domain = spider.domain_name\n         pipeline = self.pipeline[:]\n         current_stage = pipeline[0]\n         info = self.domaininfo[domain]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3c919f256292d23c2c5092969f7e405bba03880d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 23 | Lines Deleted: 9 | Files Changed: 3 | Hunks: 15 | Methods Changed: 11 | Complexity Δ (Sum/Max): 10/6 | Churn Δ: 32 | Churn Cumulative: 55 | Contributors (this commit): 2 | Commits (past 90d): 8 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -57,4 +57,4 @@ class Command(ScrapyCommand):\n                 log.msg('Cannot create any requests with the provided arguments', log.ERROR)\n                 return\n \n-        scrapymanager.runonce(*args, **opts.__dict__)\n+        scrapymanager.runonce(*args)\n\n@@ -13,4 +13,4 @@ class Command(ScrapyCommand):\n         ScrapyCommand.add_options(self, parser)\n \n     def run(self, args, opts):\n-        scrapymanager.start(*args, **opts.__dict__)\n+        scrapymanager.start(*args)\n\n@@ -1,5 +1,7 @@\n import signal\n \n+from twisted.internet import reactor\n+\n from scrapy.extension import extensions\n from scrapy import log\n from scrapy.http import Request\n@@ -20,11 +22,21 @@ class ExecutionManager(object):\n     def __init__(self):\n         self.interrupted = False\n         self.configured = False\n+        self.control_reactor = True\n \n-    def configure(self, *args, **opts):\n+    def configure(self, control_reactor=True):\n+        self.control_reactor = control_reactor\n+        if control_reactor:\n             self._install_signals()\n+        else:\n+            reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n \n+        if not log.started:\n+            log.start()\n+        if not extensions.loaded:\n             extensions.load()\n+        if not spiders.loaded:\n+            spiders.load()\n         log.msg(\"Enabled extensions: %s\" % \", \".join(extensions.enabled.iterkeys()))\n \n         scheduler = load_object(settings['SCHEDULER'])()\n@@ -35,24 +47,25 @@ class ExecutionManager(object):\n         \"\"\"Schedule the given args for crawling. args is a list of urls or domains\"\"\"\n \n         requests = self._parse_args(args)\n-        # schedule initial requets to be scraped at engine start\n+        # schedule initial requests to be scraped at engine start\n         for domain in requests or ():\n             spider = spiders.fromdomain(domain) \n             priority = self.domainprio.get_priority(domain)\n             for request in requests[domain]:\n                 scrapyengine.crawl(request, spider, domain_priority=priority)\n \n-    def runonce(self, *args, **opts):\n+    def runonce(self, *args):\n         \"\"\"Run the engine until it finishes scraping all domains and then exit\"\"\"\n-        self.configure(*args, **opts)\n+        self.configure()\n         self.crawl(*args)\n         scrapyengine.start()\n \n-    def start(self, **opts):\n+    def start(self, control_reactor=True):\n         \"\"\"Start the scrapy server, without scheduling any domains\"\"\"\n-        self.configure(**opts)\n+        self.configure(control_reactor)\n         scrapyengine.keep_alive = True\n-        scrapyengine.start()# blocking call\n+        scrapyengine.start(control_reactor=control_reactor)\n+        if control_reactor:\n             self.stop()\n \n     def stop(self):\n@@ -60,6 +73,7 @@ class ExecutionManager(object):\n         self.interrupted = True\n         scrapyengine.stop()\n         log.log_level = -999 # disable logging\n+        if self.control_reactor:\n             signal.signal(signal.SIGTERM, signal.SIG_IGN)\n             signal.signal(signal.SIGINT, signal.SIG_IGN)\n             if hasattr(signal, \"SIGBREAK\"):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e5b99a56c42a843cf799e63b81ad64aa085c8967", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 81 | Lines Deleted: 121 | Files Changed: 3 | Hunks: 50 | Methods Changed: 39 | Complexity Δ (Sum/Max): 3/4 | Churn Δ: 202 | Churn Cumulative: 285 | Contributors (this commit): 2 | Commits (past 90d): 12 | Contributors (cumulative): 6 | DMM Complexity: 0.0\n\nDIFF:\n@@ -56,6 +56,7 @@ class ExecutionEngine(object):\n         self.ports = []\n         self.running = False\n         self.paused = False\n+        self.control_reactor = True\n \n     def configure(self, scheduler=None, downloader=None):\n         \"\"\"\n@@ -114,10 +115,11 @@ class ExecutionEngine(object):\n                 p.stopListening()\n             self.ports = []\n \n-    def start(self):\n+    def start(self, control_reactor=True):\n         \"\"\"Start the execution engine\"\"\"\n         if not self.running:\n-            reactor.callWhenRunning(self._mainloop)\n+            self.control_reactor = control_reactor\n+            reactor.callLater(0, self._mainloop)\n             self.start_time = datetime.now()\n             signals.send_catch_log(signal=signals.engine_started, sender=self.__class__)\n             self.addtask(self._mainloop, 5.0)\n@@ -126,6 +128,7 @@ class ExecutionEngine(object):\n             for args, kwargs in [t for t in self.ports if isinstance(t, tuple)]:\n                 reactor.listenTCP(*args, **kwargs)\n             self.running = True\n+            if control_reactor:\n                 reactor.run() # blocking call\n \n     def stop(self):\n@@ -141,7 +144,7 @@ class ExecutionEngine(object):\n             self.tasks = []\n             for p in [p for p in self.ports if not isinstance(p, tuple)]:\n                 p.stopListening()\n-            if reactor.running:\n+            if self.control_reactor and reactor.running:\n                 reactor.stop()\n             signals.send_catch_log(signal=signals.engine_stopped, sender=self.__class__)\n \n@@ -185,7 +188,7 @@ class ExecutionEngine(object):\n             return\n \n         # backout enqueing downloads if domain needs it\n-        if self.downloader.needs_backout(domain):\n+        if domain in self.cancelled or self.downloader.needs_backout(domain):\n             return\n \n         # Next pending request from scheduler\n@@ -206,7 +209,7 @@ class ExecutionEngine(object):\n \n     def domain_is_idle(self, domain):\n         scraping = self._scraping.get(domain)\n-        pending = self.scheduler.domain_has_pending(domain)\n+        pending = self.scheduler.domain_has_pending_requests(domain)\n         downloading = not self.downloader.domain_is_idle(domain)\n         haspipe = not self.pipeline.domain_is_idle(domain)\n         oninit = domain in self.initializing\n@@ -222,48 +225,49 @@ class ExecutionEngine(object):\n     def crawl(self, request, spider, domain_priority=0):\n         domain = spider.domain_name\n \n-        def _process(response):\n-            assert isinstance(response, (Response, Exception))\n+        def _process_response(response):\n+            assert isinstance(response, (Response, Exception)), \"Expecting Response or Exception, got %s\" % type(response).__name__\n \n-            def _onpipelinefinish(pipe_result, item):\n-                # _ can only be an item or a DropItem failure, since other\n-                # failures are caught in ItemPipeine (item/pipeline.py)\n+            def cb_spidermiddleware_output(spmw_result):\n+                def cb_spider_output(output):\n+                    def cb_pipeline_output(pipe_result, item):\n                         if isinstance(pipe_result, Failure):\n+                            # can only be a DropItem exception, since other exceptions are caught in the Item Pipeline (item/pipeline.py)\n                             signals.send_catch_log(signal=signals.item_dropped, sender=self.__class__, item=item, spider=spider, response=response, exception=pipe_result.value)\n                         else:\n                             signals.send_catch_log(signal=signals.item_passed, sender=self.__class__, item=item, spider=spider, response=response, pipe_output=pipe_result)\n                         self.next_request(spider)\n \n-            def _onsuccess_per_item(item):\n-                if isinstance(item, ScrapedItem):\n-                    log.msg(\"Scraped %s in <%s>\" % (item, request.url), log.INFO, domain=domain)\n-                    signals.send_catch_log(signal=signals.item_scraped, sender=self.__class__, item=item, spider=spider, response=response)\n-                    piped = self.pipeline.pipe(item, spider)\n-                    piped.addBoth(_onpipelinefinish, item)\n-                elif isinstance(item, Request):\n-                    signals.send_catch_log(signal=signals.request_received, sender=self.__class__, request=item, spider=spider, response=response)\n-                    self.crawl(request=item, spider=spider)\n-                elif item is None:\n+                    if domain in self.cancelled:\n+                        return\n+                    elif isinstance(output, ScrapedItem):\n+                        log.msg(\"Scraped %s in <%s>\" % (output, request.url), log.INFO, domain=domain)\n+                        signals.send_catch_log(signal=signals.item_scraped, sender=self.__class__, item=output, spider=spider, response=response)\n+                        piped = self.pipeline.pipe(output, spider)\n+                        piped.addBoth(cb_pipeline_output, output)\n+                    elif isinstance(output, Request):\n+                        signals.send_catch_log(signal=signals.request_received, sender=self.__class__, request=output, spider=spider, response=response)\n+                        self.crawl(request=output, spider=spider)\n+                    elif output is None:\n                         pass # may be next time.\n                     else:\n-                    log.msg(\"Spider must return Request, ScrapedItem or None, got '%s' while processing %s\" % (type(item).__name__, request), log.WARNING, domain=domain)\n+                        log.msg(\"Spider must return Request, ScrapedItem or None, got '%s' while processing %s\" % (type(output).__name__, request), log.WARNING, domain=domain)\n \n-            def _onsuccess(result):\n-                return deferred_imap(_onsuccess_per_item, result)\n+                return deferred_imap(cb_spider_output, spmw_result)\n \n-            def _onerror(_failure):\n+            def eb_user(_failure):\n                 if not isinstance(_failure.value, IgnoreRequest):\n                     referer = None if not isinstance(response, Response) else response.request.headers.get('Referer', None)\n                     log.msg(\"Error while spider was processing <%s> from <%s>: %s\" % (request.url, referer, _failure), log.ERROR, domain=domain)\n                     stats.incpath(\"%s/spider_exceptions/%s\" % (domain, _failure.value.__class__.__name__))\n \n-            def _bugtrap(_failure):\n+            def eb_framework(_failure):\n                 log.msg('FRAMEWORK BUG processing %s: %s' % (request, _failure), log.ERROR, domain=domain)\n \n \n-            scd = self.scrape(request, response, spider)\n-            scd.addCallbacks(_onsuccess, _onerror)\n-            scd.addErrback(_bugtrap)\n+            scd = self.spidermiddleware.scrape(request, response, spider)\n+            scd.addCallbacks(cb_spidermiddleware_output, eb_user)\n+            scd.addErrback(eb_framework)\n \n             self._scraping[domain].add(response)\n             scd.addBoth(lambda _: self._scraping[domain].remove(response))\n@@ -277,12 +281,9 @@ class ExecutionEngine(object):\n             request.deferred.errback(_failure) # TODO: merge into spider middleware.\n \n         schd = self.schedule(request, spider, domain_priority)\n-        schd.addCallbacks(_process, _cleanfailure)\n+        schd.addCallbacks(_process_response, _cleanfailure)\n         return schd\n \n-    def scrape(self, request, response, spider):\n-        return self.spidermiddleware.scrape(request, response, spider)\n-\n     def schedule(self, request, spider, domain_priority=0):\n         domain = spider.domain_name\n         if not self.scheduler.domain_is_open(domain):\n@@ -310,7 +311,7 @@ class ExecutionEngine(object):\n \n     def _add_starter(self, request, spider, domain_priority):\n         domain = spider.domain_name\n-        if not self.scheduler.is_pending(domain):\n+        if not self.scheduler.domain_is_pending(domain):\n             self.scheduler.add_domain(domain, priority=domain_priority)\n             self.starters[domain] = []\n         deferred = defer.Deferred()\n@@ -420,6 +421,7 @@ class ExecutionEngine(object):\n         log.msg(\"Domain opened\", domain=domain)\n         spider = spider or spiders.fromdomain(domain)\n \n+        self.cancelled.discard(domain)\n         self.scheduler.open_domain(domain)\n         self.downloader.open_domain(domain)\n         self.pipeline.open_domain(domain)\n@@ -436,27 +438,14 @@ class ExecutionEngine(object):\n                 self._domain_idle(domain)\n         dfd.addCallback(_state)\n \n-    def close_domain(self, domain):\n-        \"\"\"Close (cancel) domain and clear all its outstanding requests\"\"\"\n-        if domain not in self.cancelled:\n-            self.cancelled.add(domain)\n-            self._close_domain(domain)\n-\n-    def _close_domain(self, domain):\n-        self.downloader.close_domain(domain)\n-\n     def _domain_idle(self, domain):\n+        \"\"\"Called when a domain gets idle. This function is called when there are no\n+        remaining pages to download or schedule. It can be called multiple\n+        times. If some extension raises a DontCloseDomain exception (in the\n+        domain_idle signal handler) the domain is not closed until the next\n+        loop and this function is guaranteed to be called (at least) once again\n+        for this domain.\n         \"\"\"\n-        Called when a domain gets idle. This function is called when there are no\n-        remaining pages to download or scheduled. It can be called multiple\n-        times. If the some extensions raises a DontCloseDomain exception the\n-        domain won't be closed and this function is garanteed to be called\n-        again (at least once) for this domain.\n-        \"\"\"\n-        # we get a callback from the downloader which completes closing\n-        #log.msg(\"Finishing scraping %s\" % domain, domain=domain)\n-        #from traceback import print_stack; print_stack()\n-\n         spider = spiders.fromdomain(domain)\n         try:\n             dispatcher.send(signal=signals.domain_idle, sender=self.__class__, domain=domain, spider=spider)\n@@ -474,6 +463,16 @@ class ExecutionEngine(object):\n         if self.is_idle() and not self.keep_alive:\n             self.stop()\n \n+    def close_domain(self, domain):\n+        \"\"\"Close (cancel) domain and clear all its outstanding requests\"\"\"\n+        if domain not in self.cancelled:\n+            log.msg(\"Closing domain\", domain=domain)\n+            self.cancelled.add(domain)\n+            self._close_domain(domain)\n+\n+    def _close_domain(self, domain):\n+        self.downloader.close_domain(domain)\n+\n     def closed_domain(self, domain):\n         \"\"\"\n         This function is called after the domain has been closed, and throws\n@@ -489,6 +488,7 @@ class ExecutionEngine(object):\n         signals.send_catch_log(signal=signals.domain_closed, sender=self.__class__, domain=domain, spider=spider, status=status)\n         log.msg(\"Domain closed (%s)\" % status, domain=domain) \n         self.cancelled.discard(domain)\n+        self.starters.pop(domain, None)\n         self._mainloop()\n \n     def getstatus(self):\n@@ -501,22 +501,24 @@ class ExecutionEngine(object):\n             \"datetime.now()-self.start_time\", \n             \"self.is_idle()\", \n             \"self.scheduler.is_idle()\",\n-            \"len(self.scheduler.pending_domains_count)\",\n+            \"len(self.scheduler.pending_domains)\",\n             \"self.downloader.is_idle()\",\n             \"len(self.downloader.sites)\",\n             \"self.downloader.has_capacity()\",\n             \"self.pipeline.is_idle()\",\n             \"len(self.pipeline.domaininfo)\",\n+            \"len(self._scraping)\",\n             ]\n         domain_tests = [\n             \"self.domain_is_idle(domain)\",\n-            \"self.scheduler.domain_has_pending(domain)\",\n+            \"self.scheduler.domain_has_pending_requests(domain)\",\n             \"len(self.scheduler.pending_requests[domain])\",\n             \"self.downloader.outstanding(domain)\",\n             \"len(self.downloader.request_queue(domain))\",\n             \"len(self.downloader.active_requests(domain))\",\n             \"self.pipeline.domain_is_idle(domain)\",\n             \"len(self.pipeline.domaininfo[domain])\",\n+            \"len(self._scraping[domain])\",\n             ]\n \n         for test in global_tests:\n@@ -525,8 +527,10 @@ class ExecutionEngine(object):\n         for domain in self.downloader.sites:\n             s += \"%s\\n\" % domain\n             for test in domain_tests:\n-                s += \"  %-45s : %s\\n\" % (test, eval(test))\n-\n+                try:\n+                    s += \"  %-50s : %s\\n\" % (test, eval(test))\n+                except Exception, e:\n+                    s += \"  %-50s : %s (exception)\\n\" % (test, type(e).__name__)\n         return s\n \n     def st(self): # shortcut for printing engine status (useful in telnet console)\n\n@@ -28,7 +28,6 @@ class ExecutionManager(object):\n         self.control_reactor = control_reactor\n         if control_reactor:\n             self._install_signals()\n-        else:\n         reactor.addSystemEventTrigger('before', 'shutdown', self.stop)\n \n         if not log.started:\n\n@@ -4,89 +4,53 @@ The Scrapy Scheduler\n \n from twisted.internet import defer\n \n-from scrapy import log\n from scrapy.utils.datatypes import PriorityQueue, PriorityStack\n from scrapy.conf import settings\n \n-\n class Scheduler(object):\n     \"\"\"The scheduler decides what to scrape next. In other words, it defines the\n-    crawling order.\n-\n-    The scheduler schedules websites and requests to be scraped.  Individual\n-    web pages that are to be scraped are batched up into a \"run\" for a website.\n-\n-    As the domain is being scraped, pages that are discovered are added to the\n-    scheduler.\n-\n-    Typical usage:\n-\n-        * next_domain() called each time a domain slot is freed, and return\n-        next domain to be scraped.\n-\n-        * open_domain() called to commence scraping a website\n-\n-        * enqueue_request() called multiple times to enqueue new requests to be downloaded\n-\n-        * next_request() called multiple times when there is capacity to download requests\n-\n-        * close_domain() called when there are no more pages for a website\n-\n-    Notes:\n-\n-        1. The order in which you get back the list of pages to scrape is not\n-        necesarily the order you put them in.\n-\n-    ``pending_domains_count`` contains the names of all domains that are to be scheduled.\n+    crawling order. The scheduler schedules websites and requests to be\n+    scraped. Individual web pages that are to be scraped are batched up into a\n+    \"run\" for a website. New pages discovered through the crawling process are\n+    also added to the scheduler.\n     \"\"\"\n \n     def __init__(self):\n-        self.pending_domains_count = {}\n+        self.pending_domains = set()\n         self.domains_queue = PriorityQueue()\n         self.pending_requests = {}\n-        self.dfo = settings.get('SCHEDULER_ORDER', '').upper() == 'DFO'\n+        self.dfo = settings['SCHEDULER_ORDER'].upper() == 'DFO'\n \n     def domain_is_open(self, domain):\n         \"\"\"Check if scheduler's resources were allocated for a domain\"\"\"\n         return domain in self.pending_requests\n \n-    def is_pending(self, domain):\n+    def domain_is_pending(self, domain):\n         \"\"\"Check if a domain is waiting to be scraped in domain's queue.\"\"\"\n-        return domain in self.pending_domains_count\n+        return domain in self.pending_domains\n \n-    def domain_has_pending(self, domain):\n+    def domain_has_pending_requests(self, domain):\n         \"\"\"Check if are there pending requests for a domain\"\"\"\n         if domain in self.pending_requests:\n             return bool(self.pending_requests[domain])\n \n     def next_domain(self) :\n         \"\"\"Return next domain available to scrape and remove it from available domains queue\"\"\"\n-        if self.pending_domains_count:\n+        if self.pending_domains:\n             domain = self.domains_queue.pop()[0]\n-            if self.pending_domains_count[domain] == 1:\n-                del self.pending_domains_count[domain]\n-            else:\n-                self.pending_domains_count[domain] -= 1\n+            self.pending_domains.remove(domain)\n             return domain\n-        return None\n \n     def add_domain(self, domain, priority=0):\n-        \"\"\"This functions schedules a new domain to be scraped, with the given priority.\n-\n-        It doesn't check if the domain is already scheduled.\n-\n-        A domain can be scheduled twice, either with the same or with different\n-        priority.\n-\n+        \"\"\"Add a new domain to be scraped, with the given priority. If the\n+        domain is already scheduled, it does nothing.\n         \"\"\"\n+        if domain not in self.pending_domains:\n             self.domains_queue.push(domain, priority)\n-        if domain not in self.pending_domains_count:\n-            self.pending_domains_count[domain] = 1\n-        else:\n-            self.pending_domains_count[domain] += 1\n+            self.pending_domains.add(domain)\n \n     def open_domain(self, domain):\n-        \"\"\"Allocates resources for maintaining a schedule for domain.\"\"\"\n+        \"\"\"Allocates scheduling resources for the given domain\"\"\"\n         Priority = PriorityStack if self.dfo else PriorityQueue\n         self.pending_requests[domain] = Priority()\n \n@@ -104,7 +68,6 @@ class Scheduler(object) :\n \n         ``(None, None)`` should be returned if there aren't requests pending\n         for the domain.\n-\n         \"\"\"\n         try:\n             return self.pending_requests[domain].pop()[0] # [1] is priority\n@@ -112,16 +75,10 @@ class Scheduler(object) :\n             return (None, None)\n \n     def close_domain(self, domain):\n-        \"\"\"Called once we are finished scraping a domain.\n-\n-        The scheduler will free any resources associated with the domain.\n-\n+        \"\"\"Called when a spider has finished scraping to free any resources\n+        associated with the domain.\n         \"\"\"\n-        try :\n-            del self.pending_requests[domain]\n-        except Exception, inst:\n-            msg = \"Could not clear pending pages for domain %s, %s\" % (domain, inst)\n-            log.msg(msg, level=log.WARNING)\n+        self.pending_requests.pop(domain, None)\n \n     def remove_pending_domain(self, domain):\n         \"\"\"\n@@ -135,8 +92,8 @@ class Scheduler(object) :\n         need to call close_domain for open domains.\n \n         \"\"\"\n-        if not self.domain_is_open(domain):\n-            return self.pending_domains_count.pop(domain, 0)\n+        if domain in self.pending_domains and not self.domain_is_open(domain):\n+            return self.pending_domains.remove(domain)\n \n     def is_idle(self):\n         \"\"\"Checks if the schedulers has any request pendings\"\"\"\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
