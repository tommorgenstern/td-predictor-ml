{"custom_id": "keras#d229c4784505346e6a039cf2bdd5cbca3ce9e648", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 238 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 7 | Complexity Δ (Sum/Max): 19/19 | Churn Δ: 238 | Churn Cumulative: 238 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 0.7464788732394366\n\nDIFF:\n@@ -0,0 +1,238 @@\n+# -*- coding: utf-8 -*-\n+from __future__ import print_function\n+import re\n+import inspect\n+import os\n+import shutil\n+\n+from keras.layers import convolutional\n+from keras.layers import recurrent\n+from keras.layers import core\n+from keras.layers import noise\n+from keras.layers import normalization\n+from keras.layers import advanced_activations\n+from keras.layers import containers\n+from keras.layers import embeddings\n+from keras import optimizers\n+from keras import callbacks\n+from keras import models\n+\n+MODULES = [(convolutional, 'keras.layers.convolutional'),\n+           (recurrent, 'keras.layers.recurrent'),\n+           (noise, 'keras.layers.noise'),\n+           (normalization, 'keras.layers.normalization'),\n+           (advanced_activations, 'keras.layers.advanced_activations'),\n+           (containers, 'keras.layers.containers'),\n+           (core, 'keras.layers.core'),\n+           (embeddings, 'keras.layers.embeddings'),\n+           (optimizers, 'keras.optimizers'),\n+           (callbacks, 'keras.callbacks'),\n+           (models, 'keras.models')]\n+\n+SKIP = ['build', 'get_params', 'MaskedLayer',\n+        'SiameseHead', 'MaskedLambda',\n+        'CallbackList']\n+ROOT = 'http://keras.io/'\n+INCLUDE_METHODS_FOR = [\n+    'Layer',\n+    'Graph',\n+    'Sequential',\n+    'Callback',\n+]\n+\n+\n+def get_earliest_class_that_defined_member(member, cls):\n+    ancestors = get_classes_ancestors([cls])\n+    result = None\n+    for ancestor in ancestors:\n+        if member in dir(ancestor):\n+            result = ancestor\n+    if not result:\n+        return cls\n+    return result\n+\n+\n+def get_classes_ancestors(classes):\n+    ancestors = []\n+    for cls in classes:\n+        ancestors += cls.__bases__\n+    filtered_ancestors = []\n+    for ancestor in ancestors:\n+        if ancestor.__name__ in ['object']:\n+            continue\n+        filtered_ancestors.append(ancestor)\n+    if filtered_ancestors:\n+        return filtered_ancestors + get_classes_ancestors(filtered_ancestors)\n+    else:\n+        return filtered_ancestors\n+\n+\n+def get_method_signature(method):\n+    signature = inspect.getargspec(method)\n+    defaults = signature.defaults\n+    args = signature.args[1:]\n+    if defaults:\n+        kwargs = zip(args[-len(defaults):], defaults)\n+        args = args[:-len(defaults)]\n+    else:\n+        kwargs = []\n+    st = '%s.%s(' % (method.__module__, method.__name__)\n+    for a in args:\n+        st += str(a) + ', '\n+    for a, v in kwargs:\n+        st += str(a) + '=' + str(v) + ', '\n+    if kwargs or args:\n+        return st[:-2] + ')'\n+    else:\n+        return st + ')'\n+\n+\n+def class_to_link(cls):\n+    module_name = cls.__module__\n+    assert module_name[:6] == 'keras.'\n+    module_name = module_name[6:]\n+    link = ROOT + module_name.replace('.', '/') + '#' + cls.__name__.lower()\n+    return link\n+\n+\n+def code_snippet(snippet):\n+    result = '```python\\n'\n+    result += snippet + '\\n'\n+    result += '```\\n'\n+    return result\n+\n+\n+def process_class_docstring(docstring):\n+    docstring = re.sub(r'    # (.*)\\n',\n+                       r'    __\\1__\\n\\n',\n+                       docstring)\n+\n+    docstring = re.sub(r'    ([^\\s\\\\]+):(.*)\\n',\n+                       r'    - __\\1__:\\2\\n',\n+                       docstring)\n+    docstring = docstring.replace('    ' * 3, '\\t')\n+    docstring = docstring.replace('    ', '')\n+    return docstring\n+\n+\n+def process_method_docstring(docstring):\n+    docstring = re.sub(r'    # (.*)\\n',\n+                       r'    __\\1__\\n\\n',\n+                       docstring)\n+\n+    docstring = re.sub(r'    ([^\\s\\\\]+):(.*)\\n',\n+                       r'    - __\\1__:\\2\\n',\n+                       docstring)\n+    docstring = docstring.replace('    ' * 4, '\\t')\n+    docstring = docstring.replace('    ', '')\n+    return docstring\n+\n+print('Cleaning up existing sources directory.')\n+if os.path.exists('sources'):\n+    shutil.rmtree('sources')\n+print('Populating sources directory with templates.')\n+for subdir, dirs, fnames in os.walk('templates'):\n+    for fname in fnames:\n+        new_subdir = subdir.replace('templates', 'sources')\n+        if not os.path.exists(new_subdir):\n+            os.makedirs(new_subdir)\n+        if fname[-3:] == '.md':\n+            fpath = os.path.join(subdir, fname)\n+            new_fpath = fpath.replace('templates', 'sources')\n+            shutil.copy(fpath, new_fpath)\n+\n+print('Starting autogeneration.')\n+covered_so_far = set()\n+for module, module_name in MODULES:\n+    class_pages = []\n+    for name in dir(module):\n+        if name in SKIP:\n+            continue\n+        if name[0] == '_':\n+            continue\n+        module_member = getattr(module, name)\n+        if module_member in covered_so_far:\n+            continue\n+        if inspect.isclass(module_member):\n+            cls = module_member\n+            if cls.__module__ == module_name:\n+\n+                try:\n+                    class_signature = get_method_signature(cls.__init__)\n+                    class_signature = class_signature.replace('__init__', cls.__name__)\n+                except:\n+                    # in case the class inherits from object and does not\n+                    # define __init__\n+                    class_signature = module_name + '.' + cls.__name__ + '()'\n+\n+                methods = []\n+                methods_not_defined_here = []\n+                for name in dir(cls):\n+                    if name in SKIP:\n+                        continue\n+                    if name[0] == '_':\n+                        continue\n+                    cls_member = getattr(cls, name)\n+                    if inspect.ismethod(cls_member):\n+                        method = cls_member\n+                        signature = inspect.getargspec(method)\n+                        defaults = signature.defaults\n+                        args = signature.args[1:]\n+                        if defaults:\n+                            kwargs = zip(args[-len(defaults):], defaults)\n+                            args = args[:-len(defaults)]\n+                        else:\n+                            kwargs = []\n+\n+                        defined_by = get_earliest_class_that_defined_member(method.__name__, cls)\n+                        if cls == defined_by:\n+                            methods.append(method)\n+                        else:\n+                            methods_not_defined_here.append((method, defined_by))\n+\n+                blocks = []\n+                blocks.append('# ' + cls.__name__ + '\\n')\n+                blocks.append(code_snippet(class_signature))\n+                docstring = cls.__doc__\n+                if docstring:\n+                    blocks.append(process_class_docstring(docstring))\n+\n+                if cls.__name__ in INCLUDE_METHODS_FOR:\n+                    if methods or methods_not_defined_here:\n+                        blocks.append('### Methods\\n')\n+                        for method in methods:\n+                            signature = get_method_signature(method)\n+                            signature = signature.replace(module_name + '.', '')\n+                            blocks.append(code_snippet(signature))\n+                            docstring = method.__doc__\n+                            if docstring:\n+                                blocks.append(process_method_docstring(docstring))\n+                        for method, defined_by in methods_not_defined_here:\n+                            signature = get_method_signature(method)\n+                            method_module_name = method.__module__\n+                            signature = signature.replace(method_module_name + '.', '')\n+                            link = '[' + defined_by.__name__ + '](' + class_to_link(defined_by) + ')'\n+                            blocks.append(code_snippet(signature))\n+                            blocks.append('Defined by ' + link + '.\\n')\n+\n+                mkdown = '\\n'.join(blocks)\n+                class_pages.append((id(cls), mkdown))\n+                covered_so_far.add(module_member)\n+\n+    class_pages.sort(key=lambda x: x[0])\n+    class_pages = [x[1] for x in class_pages]\n+    module_page = '\\n----\\n\\n'.join(class_pages)\n+\n+    # save module page.\n+    # Either insert content into existing page,\n+    # or create page otherwise\n+    path = 'sources/' + module_name.replace('.', '/')[6:] + '.md'\n+    if os.path.exists(path):\n+        template = open(path).read()\n+        assert '{{autogenerated}}' in template, ('Template found for ' + path +\n+                                                 ' but missing {{autogenerated}} tag.')\n+        module_page = template.replace('{{autogenerated}}', module_page)\n+        print('...inserting autogenerated content into template:', path)\n+    else:\n+        print('...creating new page with autogenerated content:', path)\n+    open(path, 'w').write(module_page)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#537e2d2123edd5954aa96a59df4f485e4f829a01", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 12 | Lines Deleted: 9 | Files Changed: 1 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 21 | Churn Cumulative: 453 | Contributors (this commit): 3 | Commits (past 90d): 8 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -113,9 +113,10 @@ def test_maxout_dense():\n     _runner(layer)\n \n \n-@pytest.mark.skipif(K._BACKEND == 'tensorflow', reason=\"currently not working with TensorFlow\")\n+@pytest.mark.skipif(K._BACKEND == 'tensorflow',\n+                    reason='currently not working with TensorFlow')\n def test_sequences():\n-    \"\"\"Test masking sequences with zeroes as padding\"\"\"\n+    '''Test masking sequences with zeroes as padding'''\n     # integer inputs, one per timestep, like embeddings\n     layer = core.Masking()\n     func = K.function([layer.input], [layer.get_output_mask()])\n@@ -127,12 +128,13 @@ def test_sequences():\n \n     # get mask for this input\n     output = func([input_data])[0]\n-    assert np.all(output == expected), \"Output not as expected\"\n+    assert np.all(output == expected), 'Output not as expected'\n \n \n-@pytest.mark.skipif(K._BACKEND == 'tensorflow', reason=\"currently not working with TensorFlow\")\n+@pytest.mark.skipif(K._BACKEND == 'tensorflow',\n+                    reason='currently not working with TensorFlow')\n def test_non_zero():\n-    \"\"\"Test masking with non-zero mask value\"\"\"\n+    '''Test masking with non-zero mask value'''\n     layer = core.Masking(5)\n     func = K.function([layer.input], [layer.get_output_mask()])\n     input_data = np.array([[[1, 1], [2, 1], [3, 1], [5, 5]],\n@@ -140,12 +142,13 @@ def test_non_zero():\n                           dtype=np.int32)\n     output = func([input_data])[0]\n     expected = np.array([[1, 1, 1, 0], [1, 1, 1, 1]])\n-    assert np.all(output == expected), \"Output not as expected\"\n+    assert np.all(output == expected), 'Output not as expected'\n \n \n-@pytest.mark.skipif(K._BACKEND == 'tensorflow', reason=\"currently not working with TensorFlow\")\n+@pytest.mark.skipif(K._BACKEND == 'tensorflow',\n+                    reason='currently not working with TensorFlow')\n def test_non_zero_output():\n-    \"\"\"Test output of masking layer with non-zero mask value\"\"\"\n+    '''Test output of masking layer with non-zero mask value'''\n     layer = core.Masking(5)\n     func = K.function([layer.input], [layer.get_output()])\n \n@@ -155,7 +158,7 @@ def test_non_zero_output():\n     output = func([input_data])[0]\n     expected = np.array([[[1, 1], [2, 1], [3, 1], [0, 0]],\n                         [[1, 5], [5, 0], [0, 0], [0, 0]]])\n-    assert np.all(output == expected), \"Output not as expected\"\n+    assert np.all(output == expected), 'Output not as expected'\n \n \n def _runner(layer):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#0f86b45918bc68821905f6ac0894874eef564fed", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 15 | Churn Cumulative: 253 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -87,7 +87,7 @@ def get_method_signature(method):\n         return st + ')'\n \n \n-def class_to_link(cls):\n+def class_to_docs_link(cls):\n     module_name = cls.__module__\n     assert module_name[:6] == 'keras.'\n     module_name = module_name[6:]\n@@ -95,6 +95,16 @@ def class_to_link(cls):\n     return link\n \n \n+def class_to_source_link(cls):\n+    module_name = cls.__module__\n+    assert module_name[:6] == 'keras.'\n+    path = module_name.replace('.', '/')\n+    path += '.py'\n+    line = inspect.getsourcelines(cls)[-1]\n+    link = 'https://github.com/fchollet/keras/blob/master/' + path + '#L' + str(line)\n+    return '[[source]](' + link + ')'\n+\n+\n def code_snippet(snippet):\n     result = '```python\\n'\n     result += snippet + '\\n'\n@@ -191,6 +201,7 @@ for module, module_name in MODULES:\n                             methods_not_defined_here.append((method, defined_by))\n \n                 blocks = []\n+                blocks.append('<span style=\"float:right;\">' + class_to_source_link(cls) + '</span>')\n                 blocks.append('# ' + cls.__name__ + '\\n')\n                 blocks.append(code_snippet(class_signature))\n                 docstring = cls.__doc__\n@@ -211,7 +222,7 @@ for module, module_name in MODULES:\n                             signature = get_method_signature(method)\n                             method_module_name = method.__module__\n                             signature = signature.replace(method_module_name + '.', '')\n-                            link = '[' + defined_by.__name__ + '](' + class_to_link(defined_by) + ')'\n+                            link = '[' + defined_by.__name__ + '](' + class_to_docs_link(defined_by) + ')'\n                             blocks.append(code_snippet(signature))\n                             blocks.append('Defined by ' + link + '.\\n')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#efff160cea608891b9b5ed25b6c5f56869964f63", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 804 | Contributors (this commit): 4 | Commits (past 90d): 17 | Contributors (cumulative): 4 | DMM Complexity: None\n\nDIFF:\n@@ -412,7 +412,7 @@ def rnn(step_function, inputs, initial_states,\n         if masking:\n             # if all-zero input timestep, return\n             # all-zero output and unchanged states\n-            switch = T.any(input)\n+            switch = T.any(input, axis=-1, keepdims=True)\n             output = T.switch(switch, output, 0. * output)\n             return_states = []\n             for state, new_state in zip(states, new_states):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#90b441c5875b3f90570fbf8a7e5db938f3c611e4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 19 | Lines Deleted: 15 | Files Changed: 3 | Hunks: 15 | Methods Changed: 6 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 34 | Churn Cumulative: 961 | Contributors (this commit): 3 | Commits (past 90d): 14 | Contributors (cumulative): 8 | DMM Complexity: None\n\nDIFF:\n@@ -90,7 +90,8 @@ def test_elu():\n         assert config['alpha'] == alpha\n \n \n-@pytest.mark.skipif(K._BACKEND == 'tensorflow', reason=\"currently not working with TensorFlow\")\n+@pytest.mark.skipif(K._BACKEND == 'tensorflow',\n+                    reason='currently not working with TensorFlow')\n def test_parametric_softplus():\n     from keras.layers.advanced_activations import ParametricSoftplus\n     np.random.seed(1337)\n@@ -114,7 +115,8 @@ def test_parametric_softplus():\n             assert config['beta_init'] == beta\n \n \n-@pytest.mark.skipif(K._BACKEND == 'tensorflow', reason=\"currently not working with TensorFlow\")\n+@pytest.mark.skipif(K._BACKEND == 'tensorflow',\n+                    reason='currently not working with TensorFlow')\n def test_thresholded_linear():\n     from keras.layers.advanced_activations import ThresholdedLinear\n     np.random.seed(1337)\n@@ -135,7 +137,8 @@ def test_thresholded_linear():\n         assert config['theta'] == theta\n \n \n-@pytest.mark.skipif(K._BACKEND == 'tensorflow', reason=\"currently not working with TensorFlow\")\n+@pytest.mark.skipif(K._BACKEND == 'tensorflow',\n+                    reason='currently not working with TensorFlow')\n def test_thresholded_relu():\n     from keras.layers.advanced_activations import ThresholdedReLU\n     np.random.seed(1337)\n\n@@ -137,7 +137,8 @@ def test_merge_sum():\n     assert(loss == nloss)\n \n \n-@pytest.mark.skipif(K._BACKEND=='tensorflow', reason=\"currently not working with TensorFlow\")\n+@pytest.mark.skipif(K._BACKEND == 'tensorflow',\n+                    reason='currently not working with TensorFlow')\n def test_merge_dot():\n     left = Sequential()\n     left.add(Dense(input_dim=input_dim, output_dim=nb_hidden))\n@@ -427,7 +428,7 @@ def test_1o_1i():\n                      merge_mode='sum')\n     graph.compile('rmsprop', {'output1': 'mse'})\n \n-    history = graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n+    graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n               nb_epoch=10)\n     out = graph.predict({'input1': X_test_graph})\n     assert(type(out == dict))\n@@ -438,10 +439,10 @@ def test_1o_1i():\n     assert(loss < 2.5)\n \n     # test validation split\n-    history = graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n+    graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n               validation_split=0.2, nb_epoch=1)\n     # test validation data\n-    history = graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n+    graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n               validation_data={'input1': X_train_graph, 'output1': y_train_graph},\n               nb_epoch=1)\n \n@@ -463,7 +464,7 @@ def test_1o_1i_2():\n                      merge_mode='sum')\n     graph.compile('rmsprop', {'output1': 'mse'})\n \n-    history = graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n+    graph.fit({'input1': X_train_graph, 'output1': y_train_graph},\n               nb_epoch=10)\n     out = graph.predict({'input1': X_train_graph})\n     assert(type(out == dict))\n@@ -492,7 +493,7 @@ def test_1o_2i():\n                      merge_mode='sum')\n     graph.compile('rmsprop', {'output1': 'mse'})\n \n-    history = graph.fit({'input1': X_train_graph, 'input2': X2_train_graph, 'output1': y_train_graph},\n+    graph.fit({'input1': X_train_graph, 'input2': X2_train_graph, 'output1': y_train_graph},\n               nb_epoch=10)\n     out = graph.predict({'input1': X_test_graph, 'input2': X2_test_graph})\n     assert(type(out == dict))\n@@ -519,7 +520,7 @@ def test_2o_1i_weights():\n     graph.add_output(name='output2', input='dense3')\n     graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n \n-    history = graph.fit({'input1': X_train_graph, 'output1': y_train_graph, 'output2': y2_train_graph},\n+    graph.fit({'input1': X_train_graph, 'output1': y_train_graph, 'output2': y2_train_graph},\n               nb_epoch=10)\n     out = graph.predict({'input1': X_test_graph})\n     assert(type(out == dict))\n@@ -567,7 +568,7 @@ def test_2o_1i_sample_weights():\n \n     graph.compile('rmsprop', {'output1': 'mse', 'output2': 'mse'})\n \n-    history = graph.fit({'input1': X_train_graph, 'output1': y_train_graph, 'output2': y2_train_graph},\n+    graph.fit({'input1': X_train_graph, 'output1': y_train_graph, 'output2': y2_train_graph},\n               nb_epoch=10,\n               sample_weight={'output1': weights1, 'output2': weights2})\n     out = graph.predict({'input1': X_test_graph})\n@@ -599,12 +600,12 @@ def test_recursive():\n \n     seq.compile('rmsprop', 'mse')\n \n-    history = seq.fit(X_train_graph, y_train_graph, batch_size=10, nb_epoch=10)\n+    seq.fit(X_train_graph, y_train_graph, batch_size=10, nb_epoch=10)\n     loss = seq.evaluate(X_test_graph, y_test_graph)\n     assert(loss < 2.5)\n \n     loss = seq.evaluate(X_test_graph, y_test_graph, show_accuracy=True)\n-    pred = seq.predict(X_test_graph)\n+    seq.predict(X_test_graph)\n     seq.get_config(verbose=1)\n \n \n@@ -661,6 +662,5 @@ def test_count_params():\n     assert(n == graph.count_params())\n \n \n-\n if __name__ == '__main__':\n     pytest.main([__file__])\n\n@@ -7,7 +7,8 @@ from keras import objectives\n from keras import backend as K\n \n \n-@pytest.mark.skipif(K._BACKEND=='tensorflow', reason=\"currently not working with TensorFlow\")\n+@pytest.mark.skipif(K._BACKEND == 'tensorflow',\n+                    reason='currently not working with TensorFlow')\n def test_masking():\n     np.random.seed(1337)\n     X = np.array(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#bab230c0d6debd6b1ab2c521f21d133841c1a7f5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 69 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 69 | Churn Cumulative: 69 | Contributors (this commit): 1 | Commits (past 90d): 1 | Contributors (cumulative): 1 | DMM Complexity: 1.0\n\nDIFF:\n@@ -0,0 +1,69 @@\n+import theano\n+import unittest\n+from numpy.testing import assert_allclose\n+import numpy as np\n+from keras.layers.recurrent import SimpleRNN\n+from mock import Mock\n+\n+floatX = theano.config.floatX\n+\n+__author__ = \"Jeff Ye\"\n+\n+\n+class TestSimpleRNN(unittest.TestCase):\n+    left_padding_data = np.array(\n+        [\n+            [  # batch 1\n+               [0], [1], [2], [3]\n+               ],\n+            [  # batch 2\n+               [0], [0], [1], [2]\n+               ]\n+        ], dtype=floatX)\n+    left_padding_mask = np.array(  # n_sample x n_time\n+        [\n+            [  # batch 1\n+               0, 1, 1, 1\n+               ],\n+            [  # batch 2\n+               0, 0, 1, 1\n+               ]\n+        ], dtype=np.int32)\n+\n+    def setUp(self):\n+        W = np.array([[1]], dtype=floatX)\n+        U = np.array([[1]], dtype=floatX)\n+        b = np.array([0], dtype=floatX)\n+        weights = [W, U, b]\n+        self.forward = SimpleRNN(output_dim=1, activation='linear', weights=weights)\n+        self.backward = SimpleRNN(output_dim=1, activation='linear', weights=weights)\n+\n+        previous = Mock()\n+        previous.nb_input = 1\n+        previous.nb_output = 1\n+        previous.output_shape = self.left_padding_data.shape\n+        previous.get_output_mask = Mock()\n+        self.previous = previous\n+\n+    def test_left_padding(self):\n+        forward = self.forward\n+        forward.go_backwards = False\n+        forward.return_sequences = True\n+        self.previous.get_output.return_value = theano.shared(value=self.left_padding_data)\n+        self.previous.get_output_mask.return_value = theano.shared(value=self.left_padding_mask)\n+        forward.set_previous(self.previous)\n+        np.testing.assert_allclose(forward.get_output().eval(),\n+                                   np.array([\n+                                       [[0], [1], [3], [6]],\n+                                       [[0], [0], [1], [3]]]))\n+\n+        backward = self.backward\n+        backward.go_backwards = True\n+        backward.return_sequences = True\n+        self.previous.get_output.return_value = theano.shared(value=self.left_padding_data)\n+        self.previous.get_output_mask.return_value = theano.shared(value=self.left_padding_mask)\n+        backward.set_previous(self.previous)\n+        np.testing.assert_allclose(backward.get_output().eval(),\n+                                   np.array([\n+                                       [[3], [5], [6], [0]],\n+                                       [[2], [3], [0], [0]]]))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#9987b8314edafa7d8ae047cc55d035f1c0e21ba7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 3 | Churn Cumulative: 975 | Contributors (this commit): 11 | Commits (past 90d): 7 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -276,8 +276,7 @@ class ModelCheckpoint(Callback):\n             else:\n                 if self.monitor_op(current, self.best):\n                     if self.verbose > 0:\n-                        print('Epoch %05d: %s improved from %0.5f to %0.5f, ' +\n-                              'saving model to %s'\n+                        print('Epoch %05d: %s improved from %0.5f to %0.5f, saving model to %s'\n                               % (epoch, self.monitor, self.best,\n                                  current, filepath))\n                     self.best = current\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#52976242f0c58a178805f882fb8142c3fb1db832", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 200 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 3 | Methods Changed: 9 | Complexity Δ (Sum/Max): 33/31 | Churn Δ: 200 | Churn Cumulative: 6949 | Contributors (this commit): 33 | Commits (past 90d): 107 | Contributors (cumulative): 36 | DMM Complexity: 0.8\n\nDIFF:\n@@ -1048,6 +1048,107 @@ class TimeDistributedDense(MaskedLayer):\n         return dict(list(base_config.items()) + list(config.items()))\n \n \n+class TimeDistributedHighway(MaskedLayer):\n+    '''\n+       Apply a same Highway layer for each dimension[1] (time_dimension) input.\n+       Especially useful after a recurrent network with 'return_sequence=True'.\n+       Tensor input dimensions:   (nb_sample, time_dimension, input_dim)\n+       Tensor output dimensions:  (nb_sample, time_dimension, input_dim)\n+\n+       NOTE that this layer carries it's input dimension, so an output dimension \n+       isn't necessary.\n+\n+    '''\n+    input_ndim = 3\n+\n+    def __init__(self, \n+                 init='glorot_uniform', transform_bias=-1, activation='linear', weights=None,\n+                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n+                 W_constraint=None, b_constraint=None,\n+                 input_dim=None, input_length=None, **kwargs):\n+        self.init = initializations.get(init)\n+        self.transform_bias = transform_bias\n+        self.activation = activations.get(activation)\n+        \n+        self.W_regularizer = regularizers.get(W_regularizer)\n+        self.b_regularizer = regularizers.get(b_regularizer)\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n+\n+        self.W_constraint = constraints.get(W_constraint)\n+        self.b_constraint = constraints.get(b_constraint)\n+        self.constraints = [self.W_constraint, self.b_constraint]\n+\n+        self.initial_weights = weights\n+\n+        self.input_dim = input_dim\n+        self.input_length = input_length\n+        if self.input_dim:\n+            kwargs['input_shape'] = (self.input_length, self.input_dim)\n+        self.input = K.placeholder(ndim=3)\n+        super(TimeDistributedHighway, self).__init__(**kwargs)\n+\n+    def build(self):\n+        input_dim = self.input_shape[2]\n+\n+        self.W = self.init((input_dim, input_dim))\n+        self.b = K.zeros((input_dim))\n+\n+        self.W_carry = self.init((input_dim, input_dim))\n+        self.b_carry = K.variable(np.ones((input_dim)) * self.transform_bias)\n+\n+        self.params = [self.W, self.b, self.W_carry, self.b_carry]\n+        self.regularizers = []\n+\n+        if self.W_regularizer:\n+            self.W_regularizer.set_param(self.W)\n+            self.regularizers.append(self.W_regularizer)\n+\n+        if self.b_regularizer:\n+            self.b_regularizer.set_param(self.b)\n+            self.regularizers.append(self.b_regularizer)\n+\n+        if self.activity_regularizer:\n+            self.activity_regularizer.set_layer(self)\n+            self.regularizers.append(self.activity_regularizer)\n+\n+        if self.initial_weights is not None:\n+            self.set_weights(self.initial_weights)\n+            del self.initial_weights\n+\n+    @property\n+    def output_shape(self):\n+        input_shape = self.input_shape\n+        return (input_shape[0], input_shape[1], input_shape[2])\n+\n+    def get_output(self, train=False):\n+        X = self.get_input(train)\n+\n+        def step(x, states):\n+            output = self.activation(K.dot(x, self.W) + self.b)\n+            transform_weight = activations.sigmoid(K.dot(x, self.W_carry) + self.b_carry)\n+            output *= transform_weight\n+            output = output + (1 - transform_weight) * x\n+            return output, []\n+\n+        last_output, outputs, states = K.rnn(step, X, [], masking=False)\n+        return outputs\n+\n+    def get_config(self):\n+        config = {\"name\": self.__class__.__name__,\n+                  \"init\": self.init.__name__,\n+                  \"transform_bias\": self.transform_bias,\n+                  \"activation\": self.activation.__name__,\n+                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n+                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n+                  \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n+                  \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None,\n+                  \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None,\n+                  \"input_dim\": self.input_dim,\n+                  \"input_length\": self.input_length}\n+        base_config = super(TimeDistributedHighway, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n+\n+\n class AutoEncoder(Layer):\n     '''A customizable autoencoder model.\n \n@@ -1694,3 +1795,92 @@ def add_shared_layer(layer, inputs):\n         sh = SiameseHead(i)\n         inputs[i].add(s)\n         inputs[i].add(sh)\n+\n+class Highway(Layer):\n+    '''Densely connected highway network, \n+    a natural extension of LSTMs to feedforward networks\n+\n+    This layer has identical output shape to input shape, and\n+    thus doesn't need an output_dim specified\n+\n+    cite: http://arxiv.org/pdf/1505.00387v2.pdf\n+    '''\n+    input_ndim = 2\n+\n+    def __init__(self, init='glorot_uniform', transform_bias=-2, activation='linear', weights=None,\n+                 W_regularizer=None, b_regularizer=None, activity_regularizer=None,\n+                 W_constraint=None, b_constraint=None, input_dim=None, **kwargs):\n+        self.init = initializations.get(init)\n+        self.transform_bias = transform_bias\n+        self.activation = activations.get(activation)\n+\n+        self.W_regularizer = regularizers.get(W_regularizer)\n+        self.b_regularizer = regularizers.get(b_regularizer)\n+        self.activity_regularizer = regularizers.get(activity_regularizer)\n+\n+        self.W_constraint = constraints.get(W_constraint)\n+        self.b_constraint = constraints.get(b_constraint)\n+        self.constraints = [self.W_constraint, self.b_constraint]\n+\n+        self.initial_weights = weights\n+\n+        self.input_dim = input_dim\n+        if self.input_dim:\n+            kwargs['input_shape'] = (self.input_dim,)\n+        self.input = K.placeholder(ndim=2)\n+        super(Highway, self).__init__(**kwargs)\n+\n+    def build(self):\n+        input_dim = self.input_shape[1]\n+\n+        self.W = self.init((input_dim, input_dim))\n+        self.W_carry = self.init((input_dim, input_dim))\n+        \n+        self.b = K.zeros((input_dim,))\n+        # -- initialize with a vector of values `transform_bias`\n+        self.b_carry = K.variable(np.ones((input_dim,)) * self.transform_bias)\n+\n+        self.params = [self.W, self.b, self.W_carry, self.b_carry]\n+\n+        self.regularizers = []\n+        if self.W_regularizer:\n+            self.W_regularizer.set_param(self.W)\n+            self.regularizers.append(self.W_regularizer)\n+\n+        if self.b_regularizer:\n+            self.b_regularizer.set_param(self.b)\n+            self.regularizers.append(self.b_regularizer)\n+\n+        if self.activity_regularizer:\n+            self.activity_regularizer.set_layer(self)\n+            self.regularizers.append(self.activity_regularizer)\n+\n+        if self.initial_weights is not None:\n+            self.set_weights(self.initial_weights)\n+            del self.initial_weights\n+\n+    @property\n+    def output_shape(self):\n+        return (self.input_shape[0], self.input_shape[1])\n+\n+    def get_output(self, train=False):\n+        X = self.get_input(train)\n+        transform_weight = activations.sigmoid(K.dot(X, self.W_carry) + self.b_carry)\n+        act = self.activation(K.dot(X, self.W) + self.b)\n+        act *= transform_weight\n+        output = act + (1 - transform_weight) * X\n+        return output\n+\n+    def get_config(self):\n+        config = {\"name\": self.__class__.__name__,\n+                  \"init\": self.init.__name__,\n+                  \"transform_bias\": self.transform_bias,\n+                  \"activation\": self.activation.__name__,\n+                  \"W_regularizer\": self.W_regularizer.get_config() if self.W_regularizer else None,\n+                  \"b_regularizer\": self.b_regularizer.get_config() if self.b_regularizer else None,\n+                  \"activity_regularizer\": self.activity_regularizer.get_config() if self.activity_regularizer else None,\n+                  \"W_constraint\": self.W_constraint.get_config() if self.W_constraint else None,\n+                  \"b_constraint\": self.b_constraint.get_config() if self.b_constraint else None,\n+                  \"input_dim\": self.input_dim}\n+        base_config = super(Highway, self).get_config()\n+        return dict(list(base_config.items()) + list(config.items()))\n\n@@ -100,6 +100,16 @@ def test_time_dist_merge():\n     _runner(layer)\n \n \n+def test_time_dist_highway():\n+    layer = core.TimeDistributedHighway(input_shape=(None, 10))\n+    _runner(layer)\n+\n+\n+def test_highway():\n+    layer = core.Highway(input_shape=(10,))\n+    _runner(layer)\n+\n+\n def test_autoencoder():\n     layer_1 = core.Layer()\n     layer_2 = core.Layer()\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#090ac0d1387a9f370f18afbc25314eeec2568d0c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 69 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): -2/0 | Churn Δ: 69 | Churn Cumulative: 138 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,69 +0,0 @@\n-import theano\n-import unittest\n-from numpy.testing import assert_allclose\n-import numpy as np\n-from keras.layers.recurrent import SimpleRNN\n-from mock import Mock\n-\n-floatX = theano.config.floatX\n-\n-__author__ = \"Jeff Ye\"\n-\n-\n-class TestSimpleRNN(unittest.TestCase):\n-    left_padding_data = np.array(\n-        [\n-            [  # batch 1\n-               [0], [1], [2], [3]\n-               ],\n-            [  # batch 2\n-               [0], [0], [1], [2]\n-               ]\n-        ], dtype=floatX)\n-    left_padding_mask = np.array(  # n_sample x n_time\n-        [\n-            [  # batch 1\n-               0, 1, 1, 1\n-               ],\n-            [  # batch 2\n-               0, 0, 1, 1\n-               ]\n-        ], dtype=np.int32)\n-\n-    def setUp(self):\n-        W = np.array([[1]], dtype=floatX)\n-        U = np.array([[1]], dtype=floatX)\n-        b = np.array([0], dtype=floatX)\n-        weights = [W, U, b]\n-        self.forward = SimpleRNN(output_dim=1, activation='linear', weights=weights)\n-        self.backward = SimpleRNN(output_dim=1, activation='linear', weights=weights)\n-\n-        previous = Mock()\n-        previous.nb_input = 1\n-        previous.nb_output = 1\n-        previous.output_shape = self.left_padding_data.shape\n-        previous.get_output_mask = Mock()\n-        self.previous = previous\n-\n-    def test_left_padding(self):\n-        forward = self.forward\n-        forward.go_backwards = False\n-        forward.return_sequences = True\n-        self.previous.get_output.return_value = theano.shared(value=self.left_padding_data)\n-        self.previous.get_output_mask.return_value = theano.shared(value=self.left_padding_mask)\n-        forward.set_previous(self.previous)\n-        np.testing.assert_allclose(forward.get_output().eval(),\n-                                   np.array([\n-                                       [[0], [1], [3], [6]],\n-                                       [[0], [0], [1], [3]]]))\n-\n-        backward = self.backward\n-        backward.go_backwards = True\n-        backward.return_sequences = True\n-        self.previous.get_output.return_value = theano.shared(value=self.left_padding_data)\n-        self.previous.get_output_mask.return_value = theano.shared(value=self.left_padding_mask)\n-        backward.set_previous(self.previous)\n-        np.testing.assert_allclose(backward.get_output().eval(),\n-                                   np.array([\n-                                       [[3], [5], [6], [0]],\n-                                       [[2], [3], [0], [0]]]))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f0cba6ec83f802c6984bbaf90c4286048215aaa5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 11 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): -29/0 | Churn Δ: 16 | Churn Cumulative: 6502 | Contributors (this commit): 32 | Commits (past 90d): 98 | Contributors (cumulative): 32 | DMM Complexity: 1.0\n\nDIFF:\n@@ -47,13 +47,19 @@ class Layer(object):\n         if not hasattr(self, 'params'):\n             self.params = []\n \n-    def __call__(self, X, train=False):\n-        # set temporary input\n-        tmp = self.get_input\n+    def __call__(self, X, mask=None, train=False):\n+        # set temporary input and mask\n+        tmp_input = self.get_input\n+        tmp_mask = None\n+        if hasattr(self, 'get_input_mask'):\n+            tmp_mask = self.get_input_mask\n+            self.get_input_mask = lambda _: mask\n         self.get_input = lambda _: X\n         Y = self.get_output(train=train)\n-        # return input to what it was\n-        self.get_input = tmp\n+        # return input and mask to what it was\n+        self.get_input = tmp_input\n+        if hasattr(self, 'get_input_mask'):\n+            self.get_input_mask = tmp_mask\n         return Y\n \n     def set_previous(self, layer, connection_map={}):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#4d3c6c9bbe2cddd650aa32559eff1f03774a2d47", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 15 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 17/17 | Churn Δ: 20 | Churn Cumulative: 1094 | Contributors (this commit): 14 | Commits (past 90d): 31 | Contributors (cumulative): 14 | DMM Complexity: 1.0\n\nDIFF:\n@@ -24,13 +24,23 @@ class Sequential(Layer):\n         for layer in layers:\n             self.add(layer)\n \n-    def __call__(self, X, train=False):\n+    def __call__(self, X, mask=None, train=False):\n+        #recursively search for a layer which is not a Sequential model\n+        layer = self\n+        while issubclass(layer.__class__, Sequential):\n+            layer = layer.layers[0]\n         # set temporary input to first layer\n-        tmp = self.layers[0].get_input\n-        self.layers[0].get_input = lambda _: X\n+        tmp_input = layer.get_input\n+        tmp_mask = None\n+        layer.get_input = lambda _: X\n+        if hasattr(layer, 'get_input_mask'):\n+            tmp_mask = layer.get_input_mask\n+            layer.get_input_mask = lambda _: mask\n         Y = self.get_output(train=train)\n-        # return input to first layer to what it was\n-        self.layers[0].get_input = tmp\n+        # return input from first layer to what it was\n+        layer.get_input = tmp_input\n+        if hasattr(layer, 'get_input_mask'):\n+            layer.get_input_mask = tmp_mask\n         return Y\n \n     def set_previous(self, layer):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f51982359577e726730e3c94aa382df2d63247f2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 13 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 13 | Churn Cumulative: 133 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -31,6 +31,7 @@ def test_sequential_call():\n     model.add(Dense(output_dim=output_dim, input_dim=input_dim))\n     model.compile('sgd', 'mse')\n \n+    # test flat model\n     X = K.placeholder(ndim=2)\n     Y = model(X)\n     F = K.function([X], [Y])\n@@ -41,6 +42,18 @@ def test_sequential_call():\n     # results of __call__ should match model.predict\n     assert_allclose(y1, y2)\n \n+    # test nested model\n+    model2 = Sequential()\n+    model2.add(model)\n+    model2.compile('sgd', 'mse')\n+        \n+    Y2 = model2(X)\n+    F = K.function([X], [Y2])\n+\n+    y1 = F([x])[0].astype(K.floatx())\n+    y2 = model2.predict(x)\n+    # results of __call__ should match model.predict\n+    assert_allclose(y1, y2)\n \n if __name__ == '__main__':\n     pytest.main([__file__])\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#1cf7036d1cf44a0514cee24ceda896e8f4038863", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 2 | Lines Deleted: 3 | Files Changed: 1 | Hunks: 3 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 5 | Churn Cumulative: 1099 | Contributors (this commit): 14 | Commits (past 90d): 32 | Contributors (cumulative): 14 | DMM Complexity: None\n\nDIFF:\n@@ -385,9 +385,7 @@ class Graph(Layer):\n             dot_axes: Same meaning as `dot_axes` argument of `add_node()`\n             outputs: Used when `merge_mode=None`. Names for the output nodes.\n             create_output: Same meaning as `create_output` argument of `add_node()`.\n-                When creating an output, `merge_mode` must be specified.\n         '''\n-        layer.layer_cache = self.layer_cache\n         if name in self.namespace:\n             raise Exception('Duplicate node identifier: ' + name)\n         for o in outputs:\n@@ -418,7 +416,8 @@ class Graph(Layer):\n                 raise Exception('Unknown identifier: ' + input)\n         s = Siamese(layer, layers, merge_mode,\n                     concat_axis=concat_axis,\n-                    dot_axes=dot_axes)\n+                    dot_axes=dot_axes,\n+                    is_graph=True)\n         self.namespace.add(name)\n         self.nodes[name] = s\n         self.node_config.append({'name': name,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#060fcd3ed072eb81a768d9e8da97ecb7b95c4702", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 20 | Lines Deleted: 14 | Files Changed: 1 | Hunks: 10 | Methods Changed: 8 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 34 | Churn Cumulative: 6536 | Contributors (this commit): 32 | Commits (past 90d): 99 | Contributors (cumulative): 32 | DMM Complexity: 0.6666666666666666\n\nDIFF:\n@@ -1450,7 +1450,7 @@ class Siamese(Layer):\n         dot_axes: Same meaning as `dot_axes` argument of Merge layer\n     '''\n     def __init__(self, layer, inputs, merge_mode='concat',\n-                 concat_axis=1, dot_axes=-1):\n+                 concat_axis=1, dot_axes=-1, is_graph=False):\n         if merge_mode not in ['sum', 'mul', 'concat', 'ave',\n                               'join', 'cos', 'dot', None]:\n             raise Exception('Invalid merge mode: ' + str(merge_mode))\n@@ -1460,17 +1460,19 @@ class Siamese(Layer):\n                 raise Exception(merge_mode + ' merge takes exactly 2 layers')\n \n         self.layer = layer\n+        self.trainable = layer.trainable\n+        self.is_graph = is_graph\n         self.inputs = inputs\n-        self.params = []\n+        self.layer.set_previous(inputs[0])\n         self.merge_mode = merge_mode\n         self.concat_axis = concat_axis\n         self.dot_axes = dot_axes\n-        layer.set_previous(inputs[0])\n+        self.params = []\n         self.regularizers = []\n         self.constraints = []\n         self.updates = []\n         layers = [layer]\n-        if merge_mode:\n+        if merge_mode and not is_graph:\n             layers += inputs\n         for l in layers:\n             params, regs, consts, updates = l.get_params()\n@@ -1518,15 +1520,18 @@ class Siamese(Layer):\n     def get_params(self):\n         return self.params, self.regularizers, self.constraints, self.updates\n \n-    def set_layer_input(self, index):\n-        l = self.layer\n-        while not hasattr(l, 'previous'):\n-            l = l.layers[0]\n-        l.previous = self.inputs[index]\n+    def set_layer_input(self, head):\n+        layer = self.layer\n+        from ..layers.containers import Sequential\n+        while issubclass(layer.__class__, Sequential):\n+            layer = layer.layers[0]\n+        layer.previous = self.inputs[head]\n \n     def get_output_at(self, head, train=False):\n-        self.set_layer_input(head)\n-        return self.layer.get_output(train)\n+        X = self.inputs[head].get_output(train)\n+        mask = self.inputs[head].get_output_mask(train)\n+        Y = self.layer(X), mask)\n+        return Y\n \n     def get_output_shape(self, head, train=False):\n         self.set_layer_input(head)\n@@ -1627,7 +1632,7 @@ class Siamese(Layer):\n \n     def get_weights(self):\n         weights = self.layer.get_weights()\n-        if self.merge_mode:\n+        if self.merge_mode and not self.is_graph:\n             for m in self.inputs:\n                 weights += m.get_weights()\n         return weights\n@@ -1636,7 +1641,7 @@ class Siamese(Layer):\n         nb_param = len(self.layer.params)\n         self.layer.set_weights(weights[:nb_param])\n         weights = weights[nb_param:]\n-        if self.merge_mode:\n+        if self.merge_mode and not self.is_graph:\n             for i in range(len(self.inputs)):\n                 nb_param = len(self.inputs[i].params)\n                 self.inputs[i].set_weights(weights[:nb_param])\n@@ -1648,7 +1653,8 @@ class Siamese(Layer):\n                   'inputs': [m.get_config() for m in self.inputs],\n                   'merge_mode': self.merge_mode,\n                   'concat_axis': self.concat_axis,\n-                  'dot_axes': self.dot_axes}\n+                  'dot_axes': self.dot_axes,\n+                  'is_graph': self.is_graph}\n         base_config = super(Siamese, self).get_config()\n         return dict(list(base_config.items()) + list(config.items()))\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
