{"custom_id": "keras#6d2d38ca0a2494f65234896b35344a754d648833", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 1 | Files Changed: 2 | Hunks: 2 | Methods Changed: 4 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 31 | Churn Cumulative: 844 | Contributors (this commit): 6 | Commits (past 90d): 10 | Contributors (cumulative): 9 | DMM Complexity: 0.7407407407407407\n\nDIFF:\n@@ -865,7 +865,14 @@ class Trainer:\n \n     def _flatten_metrics_in_order(self, logs):\n         \"\"\"Turns `logs` dict into a list as per key order of `metrics_names`.\"\"\"\n-        metric_names = [m.name for m in self.metrics]\n+        metric_names = []\n+        for metric in self.metrics:\n+            if isinstance(metric, CompileMetrics):\n+                metric_names += [\n+                    sub_metric.name for sub_metric in metric.metrics\n+                ]\n+            else:\n+                metric_names.append(metric.name)\n         results = []\n         for name in metric_names:\n             if name in logs:\n\n@@ -1046,6 +1046,28 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n             sorted(list(eval_out_2.keys())), [\"loss\", \"mean_absolute_error\"]\n         )\n \n+    def test_evaluate_return_list_respect_metrics_order(self):\n+        def metrics_zero(y_true, y_pred):\n+            return 0.0\n+\n+        def metrics_one(y_true, y_pred):\n+            return 1.0\n+\n+        model = ExampleModel(units=3)\n+        model.compile(\n+            optimizer=\"sgd\", loss=\"mse\", metrics=[metrics_zero, metrics_one]\n+        )\n+        eval_out = model.evaluate(np.ones((3, 2)), np.ones((3, 3)))\n+        self.assertEqual(eval_out[1], 0.0)\n+        self.assertEqual(eval_out[2], 1.0)\n+\n+        model.compile(\n+            optimizer=\"sgd\", loss=\"mse\", metrics=[metrics_one, metrics_zero]\n+        )\n+        eval_out = model.evaluate(np.ones((3, 2)), np.ones((3, 3)))\n+        self.assertEqual(eval_out[1], 1.0)\n+        self.assertEqual(eval_out[2], 0.0)\n+\n     @pytest.mark.requires_trainable_backend\n     def test_nested_inputs(self):\n         model = ListModel(units=2)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a9f67cf7e1c57128abe4f51f9e1ed282f8d2f567", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 41 | Lines Deleted: 7 | Files Changed: 5 | Hunks: 8 | Methods Changed: 9 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 48 | Churn Cumulative: 1472 | Contributors (this commit): 9 | Commits (past 90d): 41 | Contributors (cumulative): 31 | DMM Complexity: 1.0\n\nDIFF:\n@@ -104,7 +104,9 @@ class JAXTrainer(base_trainer.Trainer):\n                 for ref_v, v in zip(self.metrics_variables, metrics_variables)\n             ]\n         ) as scope:\n-            self._loss_tracker.update_state(unscaled_loss)\n+            self._loss_tracker.update_state(\n+                unscaled_loss, sample_weight=tree.flatten(x)[0].shape[0]\n+            )\n             logs = self.compute_metrics(x, y, y_pred, sample_weight)\n \n         new_metrics_variables = []\n@@ -146,7 +148,9 @@ class JAXTrainer(base_trainer.Trainer):\n                 for ref_v, v in zip(self.metrics_variables, metrics_variables)\n             ]\n         ) as scope:\n-            self._loss_tracker.update_state(unscaled_loss)\n+            self._loss_tracker.update_state(\n+                unscaled_loss, sample_weight=tree.flatten(x)[0].shape[0]\n+            )\n             logs = self.compute_metrics(x, y, y_pred, sample_weight)\n \n         new_metrics_variables = []\n\n@@ -31,7 +31,9 @@ class NumpyTrainer(base_trainer.Trainer):\n         loss = self.compute_loss(\n             x=x, y=y, y_pred=y_pred, sample_weight=sample_weight\n         )\n-        self._loss_tracker.update_state(loss)\n+        self._loss_tracker.update_state(\n+            loss, sample_weight=tree.flatten(x)[0].shape[0]\n+        )\n         return self.compute_metrics(x, y, y_pred, sample_weight=sample_weight)\n \n     def predict_step(self, data):\n\n@@ -60,7 +60,9 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             loss = self.compute_loss(\n                 x=x, y=y, y_pred=y_pred, sample_weight=sample_weight\n             )\n-            self._loss_tracker.update_state(loss)\n+            self._loss_tracker.update_state(\n+                loss, sample_weight=tf.shape(tree.flatten(x)[0])[0]\n+            )\n             if self.optimizer is not None:\n                 loss = self.optimizer.scale_loss(loss)\n \n@@ -85,7 +87,9 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         loss = self.compute_loss(\n             x=x, y=y, y_pred=y_pred, sample_weight=sample_weight\n         )\n-        self._loss_tracker.update_state(loss)\n+        self._loss_tracker.update_state(\n+            loss, sample_weight=tf.shape(tree.flatten(x)[0])[0]\n+        )\n         return self.compute_metrics(x, y, y_pred, sample_weight=sample_weight)\n \n     def predict_step(self, data):\n\n@@ -52,7 +52,9 @@ class TorchTrainer(base_trainer.Trainer):\n         loss = self.compute_loss(\n             x=x, y=y, y_pred=y_pred, sample_weight=sample_weight\n         )\n-        self._loss_tracker.update_state(loss)\n+        self._loss_tracker.update_state(\n+            loss, sample_weight=tree.flatten(x)[0].shape[0]\n+        )\n         if self.optimizer is not None:\n             loss = self.optimizer.scale_loss(loss)\n \n@@ -86,7 +88,9 @@ class TorchTrainer(base_trainer.Trainer):\n         loss = self.compute_loss(\n             x=x, y=y, y_pred=y_pred, sample_weight=sample_weight\n         )\n-        self._loss_tracker.update_state(loss)\n+        self._loss_tracker.update_state(\n+            loss, sample_weight=tree.flatten(x)[0].shape[0]\n+        )\n         return self.compute_metrics(x, y, y_pred, sample_weight=sample_weight)\n \n     def predict_step(self, data):\n\n@@ -586,6 +586,26 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n         model.evaluate(x, y, batch_size=batch_size, callbacks=[step_count])\n         self.assertEqual(step_count.test_count, 3)\n \n+    @pytest.mark.requires_trainable_backend\n+    def test_fit_with_different_batch_size_same_loss(self):\n+        x = np.random.rand(100, 4)\n+        y = np.ones((100, 1))\n+        model = ExampleModel(units=1)\n+        model.trainable = False\n+        model.compile(loss=\"mse\")\n+        loss1 = model.fit(x, y, batch_size=80).history[\"loss\"]\n+        loss2 = model.fit(x, y, batch_size=100).history[\"loss\"]\n+        self.assertAllClose(loss1, loss2)\n+\n+    def test_evaluate_with_different_batch_size_same_loss(self):\n+        x = np.random.rand(100, 4)\n+        y = np.ones((100, 1))\n+        model = ExampleModel(units=1)\n+        model.compile(loss=\"mse\")\n+        loss1 = model.evaluate(x, y, batch_size=80)\n+        loss2 = model.evaluate(x, y, batch_size=100)\n+        self.assertAllClose(loss1, loss2)\n+\n     @pytest.mark.requires_trainable_backend\n     def test_adds_loss_scaling_optimizer(self):\n         model = TrainingTestingLayer(dtype=\"mixed_float16\")\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#01ddde786b86fc51d01681b365dd908251c0749d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 59 | Lines Deleted: 27 | Files Changed: 1 | Hunks: 15 | Methods Changed: 8 | Complexity Δ (Sum/Max): 4/4 | Churn Δ: 86 | Churn Cumulative: 274 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 1.0\n\nDIFF:\n@@ -7,6 +7,8 @@ keras.backend.set_image_data_format(\"channels_last\")\n tf_keras.backend.set_image_data_format(\"channels_last\")\n \n NUM_CLASSES = 10\n+BATCH_SIZE = 32\n+EPOCHS = 1\n \n \n def build_mnist_data(num_classes):\n@@ -23,11 +25,7 @@ def build_mnist_data(num_classes):\n     y_train = keras.utils.to_categorical(y_train, num_classes)\n     y_test = keras.utils.to_categorical(y_test, num_classes)\n \n-    print(\"x_train shape:\", x_train.shape)\n-    print(x_train.shape[0], \"train samples\")\n-    print(x_test.shape[0], \"test samples\")\n-\n-    return x_train, y_train, x_test, y_test\n+    return x_train[:100], y_train[:100]\n \n \n def build_keras_model(keras_module, num_classes):\n@@ -44,45 +42,60 @@ def build_keras_model(keras_module, num_classes):\n             keras_module.layers.Conv2D(\n                 64, kernel_size=(3, 3), activation=\"relu\"\n             ),\n-            # TODO: Renable the following line.\n-            # keras_module.layers.BatchNormalization(scale=False, center=True),\n+            keras_module.layers.BatchNormalization(scale=False, center=True),\n             keras_module.layers.MaxPooling2D(pool_size=(2, 2)),\n             keras_module.layers.Flatten(),\n             keras_module.layers.Dense(num_classes, activation=\"softmax\"),\n         ]\n     )\n-\n-    model.summary()\n     return model\n \n \n-def train_model(model, x, y):\n-    batch_size = 256\n-    epochs = 1\n-\n+def compile_model(model):\n     model.compile(\n-        loss=\"mse\", optimizer=\"adam\", metrics=[\"accuracy\"], jit_compile=False\n+        loss=\"categorical_crossentropy\",\n+        optimizer=\"adam\",\n+        metrics=[\"mae\", \"accuracy\"],\n+        jit_compile=False,\n+        run_eagerly=True,\n     )\n \n+\n+def train_model(model, x, y):\n     return model.fit(\n         x,\n         y,\n-        batch_size=batch_size,\n-        epochs=epochs,\n-        validation_split=0.1,\n+        batch_size=BATCH_SIZE,\n+        epochs=EPOCHS,\n         shuffle=False,\n+        verbose=0,\n     )\n \n \n def eval_model(model, x, y):\n-    score = model.evaluate(x, y, verbose=0)\n-    print(\"Test loss:\", score[0])\n-    print(\"Test accuracy:\", score[1])\n+    score = model.evaluate(x, y, verbose=0, batch_size=BATCH_SIZE)\n+    print(score)\n     return score\n \n \n+def check_history(h1, h2):\n+    for key in h1.history.keys():\n+        print(f\"{key}:\")\n+        print(h1.history[key])\n+        print(h2.history[key])\n+        np.testing.assert_allclose(\n+            h1.history[key],\n+            h2.history[key],\n+            atol=1e-3,\n+        )\n+\n+\n+def predict_model(model, x):\n+    return model.predict(x, batch_size=BATCH_SIZE, verbose=0)\n+\n+\n def numerical_test():\n-    x_train, y_train, x_test, y_test = build_mnist_data(NUM_CLASSES)\n+    x_train, y_train = build_mnist_data(NUM_CLASSES)\n     keras_model = build_keras_model(keras, NUM_CLASSES)\n     tf_keras_model = build_keras_model(tf_keras, NUM_CLASSES)\n \n@@ -93,15 +106,34 @@ def numerical_test():\n     for kw, kcw in zip(keras_model.weights, tf_keras_model.weights):\n         np.testing.assert_allclose(kw.numpy(), kcw.numpy())\n \n+    compile_model(keras_model)\n+    compile_model(tf_keras_model)\n+\n+    print(\"Checking training histories:\")\n     keras_history = train_model(keras_model, x_train, y_train)\n     tf_keras_history = train_model(tf_keras_model, x_train, y_train)\n+    check_history(keras_history, tf_keras_history)\n+    print(\"Training histories match.\")\n+    print()\n \n-    for key in keras_history.history.keys():\n-        np.testing.assert_allclose(\n-            keras_history.history[key],\n-            tf_keras_history.history[key],\n-            atol=1e-3,\n-        )\n+    print(\"Checking trained weights:\")\n+    for kw, kcw in zip(keras_model.weights, tf_keras_model.weights):\n+        np.testing.assert_allclose(kw.numpy(), kcw.numpy(), atol=1e-3)\n+    print(\"Trained weights match.\")\n+    print()\n+\n+    print(\"Checking predict:\")\n+    outputs1 = predict_model(keras_model, x_train)\n+    outputs2 = predict_model(tf_keras_model, x_train)\n+    np.testing.assert_allclose(outputs1, outputs2, atol=1e-3)\n+    print(\"Predict results match.\")\n+    print()\n+\n+    print(\"Checking evaluate:\")\n+    score1 = eval_model(keras_model, x_train, y_train)\n+    score2 = eval_model(tf_keras_model, x_train, y_train)\n+    np.testing.assert_allclose(score1, score2, atol=1e-3)\n+    print(\"Evaluate results match.\")\n \n \n if __name__ == \"__main__\":\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f022476665b8325e745ff13de8288dc99daa0f5b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 27 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 1/1 | Churn Δ: 27 | Churn Cumulative: 31 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 1.0\n\nDIFF:\n@@ -22,6 +22,33 @@ class PadSequencesTest(testing.TestCase):\n         b = sequence_utils.pad_sequences(a, maxlen=3, value=1)\n         self.assertAllClose(b, [[1, 1, 1], [1, 1, 2], [1, 2, 3]])\n \n+    def test_pad_sequences_float(self):\n+        a = [[1.2], [1.2, 2.3], [1.2, 2.3, 3.4]]\n+\n+        # test padding\n+        b = sequence_utils.pad_sequences(\n+            a, maxlen=3, padding=\"pre\", dtype=\"float32\"\n+        )\n+        self.assertAllClose(b, [[0, 0, 1.2], [0, 1.2, 2.3], [1.2, 2.3, 3.4]])\n+        b = sequence_utils.pad_sequences(\n+            a, maxlen=3, padding=\"post\", dtype=\"float32\"\n+        )\n+        self.assertAllClose(b, [[1.2, 0, 0], [1.2, 2.3, 0], [1.2, 2.3, 3.4]])\n+\n+        # test truncating\n+        b = sequence_utils.pad_sequences(\n+            a, maxlen=2, truncating=\"pre\", dtype=\"float32\"\n+        )\n+        self.assertAllClose(b, [[0, 1.2], [1.2, 2.3], [2.3, 3.4]])\n+        b = sequence_utils.pad_sequences(\n+            a, maxlen=2, truncating=\"post\", dtype=\"float32\"\n+        )\n+        self.assertAllClose(b, [[0, 1.2], [1.2, 2.3], [1.2, 2.3]])\n+\n+        # test value\n+        b = sequence_utils.pad_sequences(a, maxlen=3, value=1, dtype=\"float32\")\n+        self.assertAllClose(b, [[1, 1, 1.2], [1, 1.2, 2.3], [1.2, 2.3, 3.4]])\n+\n     def test_pad_sequences_str(self):\n         a = [[\"1\"], [\"1\", \"2\"], [\"1\", \"2\", \"3\"]]\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#e74436bcfad49f310e1c2e583123bc7d7e3ac668", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 430 | Lines Deleted: 21 | Files Changed: 6 | Hunks: 29 | Methods Changed: 24 | Complexity Δ (Sum/Max): 33/22 | Churn Δ: 451 | Churn Cumulative: 6097 | Contributors (this commit): 18 | Commits (past 90d): 48 | Contributors (cumulative): 49 | DMM Complexity: 0.9792899408284024\n\nDIFF:\n@@ -331,6 +331,12 @@ class Dense(Layer):\n     def quantize(self, mode):\n         import gc\n \n+        # Prevent quantization of the subclasses\n+        if type(self) is not Dense:\n+            raise NotImplementedError(\n+                f\"Layer {self.__class__.__name__} does not have a `quantize()` \"\n+                \"method implemented.\"\n+            )\n         self._check_quantize_args(mode, self.compute_dtype)\n         if mode == \"int8\":\n             if backend.standardize_dtype(self._kernel.dtype) == \"int8\":\n@@ -344,9 +350,11 @@ class Dense(Layer):\n             kernel_scale = ops.squeeze(kernel_scale, axis=0)\n             self._tracker.unlock()\n             self._untrack_variable(self._kernel)\n+            kernel_shape = self._kernel.shape\n+            del self._kernel\n             self._kernel = self.add_weight(\n                 name=\"kernel\",\n-                shape=self._kernel.shape,\n+                shape=kernel_shape,\n                 # Prevent adding a large constant to the computation graph\n                 initializer=lambda shape, dtype: kernel_value,\n                 dtype=\"int8\",\n\n@@ -313,6 +313,8 @@ class DenseTest(testing.TestCase):\n     def test_quantize_int8(self):\n         layer = layers.Dense(units=16)\n         layer.build((None, 8))\n+        x = np.random.random((2, 8))\n+        y_float = layer(x)\n         layer.quantize(\"int8\")\n \n         # Verify weights dtype\n@@ -322,9 +324,10 @@ class DenseTest(testing.TestCase):\n             layer.variable_dtype,\n         )\n \n-        # Try eager call\n-        x = np.random.random((2, 8))\n-        _ = layer(x)\n+        # Try eager call and verify output correctness\n+        y_quantized = layer(x)\n+        mse = ops.mean(ops.square(y_float - y_quantized))\n+        self.assertLess(mse, 1e-3)  # A weak correctness test\n \n         # Try saving and reloading the model\n         model = models.Sequential([layer])\n@@ -381,6 +384,15 @@ class DenseTest(testing.TestCase):\n         ):\n             layer.quantize(\"int8\")\n \n+    def test_quantize_on_subclass(self):\n+        class MyDense(layers.Dense):\n+            pass\n+\n+        layer = MyDense(units=16)\n+        layer.build((None, 8))\n+        with self.assertRaises(NotImplementedError):\n+            layer.quantize(\"int8\")\n+\n     def test_quantize_when_already_quantized(self):\n         layer = layers.Dense(units=2)\n         layer.build((None, 2))\n\n@@ -424,7 +424,7 @@ class EinsumDense(Layer):\n                 )\n             # De-scale outputs\n             x = ops.cast(x, self.compute_dtype)\n-            x = ops.divide(x, ops.multiply(inputs_scale, self.kernel_scale))\n+            x = ops.divide(x, ops.multiply(inputs_scale, kernel_scale))\n             return x, grad_fn\n \n         x = einsum_with_inputs_gradient(\n@@ -433,8 +433,8 @@ class EinsumDense(Layer):\n             ops.convert_to_tensor(self.kernel_scale),\n         )\n         if self.lora_enabled:\n-            lora_kernel = ops.matmul(self.lora_kernel_a, self.lora_kernel_b)\n-            lora_x = ops.einsum(self.equation, inputs, lora_kernel)\n+            lora_x = ops.einsum(self.equation, inputs, self.lora_kernel_a)\n+            lora_x = ops.matmul(lora_x, self.lora_kernel_b)\n             x = ops.add(x, lora_x)\n         if self.bias is not None:\n             x += self.bias\n@@ -445,6 +445,12 @@ class EinsumDense(Layer):\n     def quantize(self, mode):\n         import gc\n \n+        # Prevent quantization of the subclasses\n+        if type(self) is not EinsumDense:\n+            raise NotImplementedError(\n+                f\"Layer {self.__class__.__name__} does not have a `quantize()` \"\n+                \"method implemented.\"\n+            )\n         self._check_quantize_args(mode, self.compute_dtype)\n         if mode == \"int8\":\n             if backend.standardize_dtype(self._kernel.dtype) == \"int8\":\n@@ -482,9 +488,11 @@ class EinsumDense(Layer):\n                 )\n             self._tracker.unlock()\n             self._untrack_variable(self._kernel)\n+            kernel_shape = self._kernel.shape\n+            del self._kernel\n             self._kernel = self.add_weight(\n                 name=\"kernel\",\n-                shape=self._kernel.shape,\n+                shape=kernel_shape,\n                 # Prevent adding a large constant to the computation graph\n                 initializer=lambda shape, dtype: kernel_value,\n                 dtype=\"int8\",\n\n@@ -8,6 +8,7 @@ from keras import backend\n from keras import constraints\n from keras import layers\n from keras import models\n+from keras import ops\n from keras import saving\n from keras import testing\n from keras.export import export_lib\n@@ -386,6 +387,8 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n             bias_axes=\"d\",\n         )\n         layer.build((None, 3))\n+        x = np.random.random((2, 3))\n+        y_float = layer(x)\n         layer.quantize(\"int8\")\n \n         # Verify weights dtype\n@@ -395,9 +398,10 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n             layer.variable_dtype,\n         )\n \n-        # Try eager call\n-        x = np.random.random((2, 3))\n-        _ = layer(x)\n+        # Try eager call and verify output correctness\n+        y_quantized = layer(x)\n+        mse = ops.mean(ops.square(y_float - y_quantized))\n+        self.assertLess(mse, 1e-3)  # A weak correctness test\n \n         # Try saving and reloading the model\n         model = models.Sequential([layer])\n@@ -489,6 +493,19 @@ class EinsumDenseTest(testing.TestCase, parameterized.TestCase):\n         ):\n             layer.quantize(\"int8\")\n \n+    def test_quantize_on_subclass(self):\n+        class MyEinsumDense(layers.EinsumDense):\n+            pass\n+\n+        layer = MyEinsumDense(\n+            equation=\"ab,bcd->acd\",\n+            output_shape=(8, 32),\n+            bias_axes=\"d\",\n+        )\n+        layer.build((None, 3))\n+        with self.assertRaises(NotImplementedError):\n+            layer.quantize(\"int8\")\n+\n     def test_quantize_when_already_quantized(self):\n         layer = layers.EinsumDense(\n             equation=\"ab,bcd->acd\",\n\n@@ -1,8 +1,9 @@\n-import numpy as np\n-\n+from keras import backend\n from keras import constraints\n+from keras import dtype_policies\n from keras import initializers\n from keras import ops\n+from keras import quantizers\n from keras import regularizers\n from keras.api_export import keras_export\n from keras.layers.layer import Layer\n@@ -91,6 +92,11 @@ class Embedding(Layer):\n         self.lora_enabled = False\n \n     def build(self, input_shape=None):\n+        if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+            self.quantized_build(\n+                input_shape, mode=self.dtype_policy.quantization_mode\n+            )\n+        else:\n             self._embeddings = self.add_weight(\n                 shape=(self.input_dim, self.output_dim),\n                 initializer=self.embeddings_initializer,\n@@ -159,20 +165,39 @@ class Embedding(Layer):\n         self.embeddings.trainable = False\n         self._tracker.lock()\n         self.lora_enabled = True\n+        self.lora_rank = rank\n \n     def save_own_variables(self, store):\n-        if not self.lora_enabled:\n-            return super().save_own_variables(store)\n-\n-        embeddings_value = self.embeddings\n+        # Do nothing if the layer isn't yet built\n+        if not self.built:\n+            return\n+        # The keys of the `store` will be saved as determined because the\n+        # default ordering will change after quantization\n+        embeddings_value, embeddings_scale = (\n+            self._get_embeddings_with_merged_lora()\n+        )\n         store[\"0\"] = embeddings_value\n+        if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+            store[\"1\"] = embeddings_scale\n \n     def load_own_variables(self, store):\n         if not self.lora_enabled:\n-            return super().load_own_variables(store)\n+            self._check_load_own_variables(store)\n+        # Do nothing if the layer isn't yet built\n+        if not self.built:\n+            return\n+        # The keys of the `store` will be saved as determined because the\n+        # default ordering will change after quantization\n         self._embeddings.assign(store[\"0\"])\n-        self.lora_embeddings_a.assign(np.zeros(self.lora_embeddings_a.shape))\n-        self.lora_embeddings_b.assign(np.zeros(self.lora_embeddings_b.shape))\n+        if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+            self.embeddings_scale.assign(store[\"1\"])\n+        if self.lora_enabled:\n+            self.lora_embeddings_a.assign(\n+                ops.zeros(self.lora_embeddings_a.shape)\n+            )\n+            self.lora_embeddings_b.assign(\n+                ops.zeros(self.lora_embeddings_b.shape)\n+            )\n \n     def get_config(self):\n         base_config = super().get_config()\n@@ -196,3 +221,150 @@ class Embedding(Layer):\n         if self.lora_rank:\n             config[\"lora_rank\"] = self.lora_rank\n         return {**base_config, **config}\n+\n+    def _check_load_own_variables(self, store):\n+        all_vars = self._trainable_variables + self._non_trainable_variables\n+        if len(store.keys()) != len(all_vars):\n+            if len(all_vars) == 0 and not self.built:\n+                raise ValueError(\n+                    f\"Layer '{self.name}' was never built \"\n+                    \"and thus it doesn't have any variables. \"\n+                    f\"However the weights file lists {len(store.keys())} \"\n+                    \"variables for this layer.\\n\"\n+                    \"In most cases, this error indicates that either:\\n\\n\"\n+                    \"1. The layer is owned by a parent layer that \"\n+                    \"implements a `build()` method, but calling the \"\n+                    \"parent's `build()` method did NOT create the state of \"\n+                    f\"the child layer '{self.name}'. A `build()` method \"\n+                    \"must create ALL state for the layer, including \"\n+                    \"the state of any children layers.\\n\\n\"\n+                    \"2. You need to implement \"\n+                    \"the `def build_from_config(self, config)` method \"\n+                    f\"on layer '{self.name}', to specify how to rebuild \"\n+                    \"it during loading. \"\n+                    \"In this case, you might also want to implement the \"\n+                    \"method that generates the build config at saving time, \"\n+                    \"`def get_build_config(self)`. \"\n+                    \"The method `build_from_config()` is meant \"\n+                    \"to create the state \"\n+                    \"of the layer (i.e. its variables) upon deserialization.\",\n+                )\n+            raise ValueError(\n+                f\"Layer '{self.name}' expected {len(all_vars)} variables, \"\n+                \"but received \"\n+                f\"{len(store.keys())} variables during loading. \"\n+                f\"Expected: {[v.name for v in all_vars]}\"\n+            )\n+\n+    \"\"\"Quantization-related methods\"\"\"\n+\n+    def quantized_build(self, input_shape, mode):\n+        if mode == \"int8\":\n+            self.inputs_quantizer = quantizers.AbsMaxQuantizer(axis=-1)\n+            self._embeddings = self.add_weight(\n+                name=\"embeddings\",\n+                shape=(self.input_dim, self.output_dim),\n+                initializer=\"zeros\",\n+                dtype=\"int8\",\n+                trainable=False,\n+            )\n+            self.embeddings_scale = self.add_weight(\n+                name=\"embeddings_scale\",\n+                shape=(self.output_dim,),\n+                initializer=\"ones\",\n+                trainable=False,\n+            )\n+\n+    def quantized_call(self, inputs):\n+        # We cannot update quantized self._embeddings, so the custom gradient is\n+        # not needed\n+        if backend.standardize_dtype(inputs.dtype) not in (\"int32\", \"int64\"):\n+            inputs = ops.cast(inputs, \"int32\")\n+        outputs = ops.take(self._embeddings, inputs, axis=0)\n+        # De-scale outputs\n+        outputs = ops.cast(outputs, self.compute_dtype)\n+        outputs = ops.divide(\n+            outputs, ops.expand_dims(self.embeddings_scale, axis=0)\n+        )\n+        if self.lora_enabled:\n+            lora_outputs = ops.take(self.lora_embeddings_a, inputs, axis=0)\n+            lora_outputs = ops.matmul(lora_outputs, self.lora_embeddings_b)\n+            outputs = ops.add(outputs, lora_outputs)\n+        return outputs\n+\n+    def quantize(self, mode):\n+        import gc\n+\n+        # Prevent quantization of the subclasses\n+        if type(self) is not Embedding:\n+            raise NotImplementedError(\n+                f\"Layer {self.__class__.__name__} does not have a `quantize()` \"\n+                \"method implemented.\"\n+            )\n+        self._check_quantize_args(mode, self.compute_dtype)\n+        if mode == \"int8\":\n+            if backend.standardize_dtype(self._embeddings.dtype) == \"int8\":\n+                raise ValueError(\"`quantize` can only be done once per layer.\")\n+            # Configure `self.inputs_quantizer`\n+            self.inputs_quantizer = quantizers.AbsMaxQuantizer(axis=-1)\n+            # Quantize `self._embeddings` to int8 and compute corresponding\n+            # scale\n+            embeddings_value, embeddings_scale = quantizers.abs_max_quantize(\n+                self._embeddings, axis=0\n+            )\n+            embeddings_scale = ops.squeeze(embeddings_scale, axis=0)\n+            self._tracker.unlock()\n+            self._untrack_variable(self._embeddings)\n+            del self._embeddings\n+            self._embeddings = self.add_weight(\n+                name=\"embeddings\",\n+                shape=(self.input_dim, self.output_dim),\n+                # Prevent adding a large constant to the computation graph\n+                initializer=lambda shape, dtype: embeddings_value,\n+                dtype=\"int8\",\n+                trainable=False,\n+            )\n+            self.embeddings_scale = self.add_weight(\n+                name=\"embeddings_scale\",\n+                shape=(self.output_dim,),\n+                # Prevent adding a large constant to the computation graph\n+                initializer=lambda shape, dtype: embeddings_scale,\n+                trainable=False,\n+            )\n+            self._tracker.lock()\n+        else:\n+            NotImplementedError(\n+                \"Invalid quantization mode. Expected 'int8'. \"\n+                f\"Received: mode={mode}\"\n+            )\n+\n+        # Set new dtype policy\n+        if not isinstance(\n+            self.dtype_policy, dtype_policies.QuantizedDTypePolicy\n+        ):\n+            quantized_dtype = f\"{mode}_from_{self.dtype_policy.name}\"\n+            self.dtype_policy = dtype_policies.get(quantized_dtype)\n+\n+        # Release memory manually because sometimes the backend doesn't\n+        gc.collect()\n+\n+    def _get_embeddings_with_merged_lora(self):\n+        if isinstance(self.dtype_policy, dtype_policies.QuantizedDTypePolicy):\n+            embeddings_value = self._embeddings\n+            embeddings_scale = self.embeddings_scale\n+            if self.lora_enabled:\n+                # Dequantize & quantize to merge lora weights into embeddings\n+                # Note that this is a lossy compression\n+                embeddings_value = ops.divide(\n+                    embeddings_value, embeddings_scale\n+                )\n+                embeddings_value = ops.add(\n+                    embeddings_value,\n+                    ops.matmul(self.lora_embeddings_a, self.lora_embeddings_b),\n+                )\n+                embeddings_value, embeddings_scale = (\n+                    quantizers.abs_max_quantize(embeddings_value, axis=0)\n+                )\n+                embeddings_scale = ops.squeeze(embeddings_scale, axis=0)\n+            return embeddings_value, embeddings_scale\n+        return self.embeddings, None\n\n@@ -7,7 +7,9 @@ from keras import backend\n from keras import constraints\n from keras import layers\n from keras import models\n+from keras import ops\n from keras import saving\n+from keras.export import export_lib\n from keras.testing import test_case\n \n \n@@ -152,7 +154,7 @@ class EmbeddingTest(test_case.TestCase):\n         model.save(temp_filepath)\n \n         new_model = saving.load_model(temp_filepath)\n-        self.assertFalse(new_model.layers[0].lora_enabled)\n+        self.assertTrue(new_model.layers[0].lora_enabled)\n         self.assertAllClose(model.predict(x), new_model.predict(x))\n \n         # Try saving and reloading the model's weights only\n@@ -213,3 +215,193 @@ class EmbeddingTest(test_case.TestCase):\n         layer.enable_lora(rank=2)\n         with self.assertRaisesRegex(ValueError, \"lora is already enabled\"):\n             layer.enable_lora(rank=2)\n+\n+    def test_quantize_int8(self):\n+        layer = layers.Embedding(10, 16)\n+        layer.build()\n+        x = np.random.randint(0, 9, size=(64, 3))\n+        y_float = layer(x)\n+        layer.quantize(\"int8\")\n+\n+        # Verify weights dtype\n+        self.assertEqual(\n+            backend.standardize_dtype(layer._embeddings.dtype), \"int8\"\n+        )\n+        self.assertEqual(\n+            backend.standardize_dtype(layer.embeddings_scale.dtype),\n+            layer.variable_dtype,\n+        )\n+\n+        # Try eager call and verify output correctness\n+        y_quantized = layer(x)\n+        mse = ops.mean(ops.square(y_float - y_quantized))\n+        self.assertLess(mse, 1e-3)  # A weak correctness test\n+\n+        # Try saving and reloading the model\n+        model = models.Sequential([layer])\n+        temp_filepath = os.path.join(\n+            self.get_temp_dir(), \"quantized_model.keras\"\n+        )\n+        model.save(temp_filepath)\n+        new_model = saving.load_model(temp_filepath)\n+        self.assertAllClose(model.predict(x), new_model.predict(x))\n+\n+        # Try saving and reloading the model's weights only\n+        temp_filepath = os.path.join(\n+            self.get_temp_dir(), \"quantized_model.weights.h5\"\n+        )\n+        model.save_weights(temp_filepath)\n+\n+        # Try lora\n+        layer = layers.Embedding(10, 16)\n+        layer.build()\n+        layer.enable_lora(4)\n+        layer.quantize(\"int8\")\n+        _ = layer(x)\n+\n+        # Try building with quantized dtype policy\n+        layer = layers.Embedding(10, 16, dtype=\"int8_from_mixed_bfloat16\")\n+        layer.build()\n+        self.assertEqual(\n+            backend.standardize_dtype(layer._embeddings.dtype), \"int8\"\n+        )\n+        self.assertEqual(\n+            backend.standardize_dtype(layer.embeddings_scale.dtype), \"float32\"\n+        )\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_quantize_dtype_argument(self):\n+        self.run_layer_test(\n+            layers.Embedding,\n+            {\n+                \"input_dim\": 4,\n+                \"output_dim\": 3,\n+                \"dtype\": \"int8_from_mixed_bfloat16\",\n+            },\n+            input_shape=(2,),\n+            input_dtype=\"int32\",\n+            expected_output_shape=(2, 3),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=2,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=False,\n+        )\n+        self.run_layer_test(\n+            layers.Embedding,\n+            {\n+                \"input_dim\": 5,\n+                \"output_dim\": 4,\n+                \"mask_zero\": True,\n+                \"dtype\": \"int8_from_float32\",\n+            },\n+            input_shape=(2, 3),\n+            input_dtype=\"int64\",\n+            expected_output_shape=(2, 3, 4),\n+            expected_num_trainable_weights=0,\n+            expected_num_non_trainable_weights=2,\n+            expected_num_seed_generators=0,\n+            expected_num_losses=0,\n+            supports_masking=True,\n+        )\n+\n+    def test_quantize_on_unbuilt_layer(self):\n+        layer = layers.Embedding(10, 16)\n+        with self.assertRaisesRegex(\n+            ValueError, \"Cannot quantize a layer that isn't yet built.\"\n+        ):\n+            layer.quantize(\"int8\")\n+\n+    def test_quantize_on_subclass(self):\n+        class MyEmbedding(layers.Embedding):\n+            pass\n+\n+        layer = MyEmbedding(10, 16)\n+        with self.assertRaises(NotImplementedError):\n+            layer.quantize(\"int8\")\n+\n+    def test_quantize_when_already_quantized(self):\n+        layer = layers.Embedding(10, 16)\n+        layer.build()\n+        layer.quantize(\"int8\")\n+        with self.assertRaisesRegex(\n+            ValueError, \"`quantize` can only be done once per layer.\"\n+        ):\n+            layer.quantize(\"int8\")\n+\n+    @pytest.mark.requires_trainable_backend\n+    def test_quantize_when_lora_enabled(self):\n+        layer = layers.Embedding(10, 16)\n+        layer.build()\n+        layer.enable_lora(4)\n+        layer.quantize(\"int8\")\n+        self.assertLen(layer.trainable_weights, 2)\n+        self.assertLen(layer.non_trainable_weights, 2)\n+\n+        # Try calling fit()\n+        init_lora_a_embeddings_value = layer.lora_embeddings_a.numpy()\n+        init_lora_b_embeddings_value = layer.lora_embeddings_b.numpy()\n+        x = np.random.randint(0, 9, size=(64, 3))\n+        y = np.random.random((64, 3, 16))\n+        model = models.Sequential([layer])\n+        model.compile(optimizer=\"sgd\", loss=\"mse\")\n+        model.fit(x, y)\n+\n+        final_lora_a_embeddings_value = layer.lora_embeddings_a.numpy()\n+        final_lora_b_embeddings_value = layer.lora_embeddings_b.numpy()\n+        diff_a = np.max(\n+            np.abs(init_lora_a_embeddings_value - final_lora_a_embeddings_value)\n+        )\n+        diff_b = np.max(\n+            np.abs(init_lora_b_embeddings_value - final_lora_b_embeddings_value)\n+        )\n+        self.assertGreater(diff_a, 0.0)\n+        self.assertGreater(diff_b, 0.0)\n+\n+        # Try saving and reloading the model\n+        temp_filepath = os.path.join(\n+            self.get_temp_dir(), \"quantized_lora_model.keras\"\n+        )\n+        model.save(temp_filepath)\n+        new_model = saving.load_model(temp_filepath)\n+        self.assertTrue(new_model.layers[0].lora_enabled)\n+        self.assertAllClose(model.predict(x), new_model.predict(x), atol=0.5)\n+\n+        # Try saving and reloading the model's weights only\n+        temp_filepath = os.path.join(\n+            self.get_temp_dir(), \"quantized_lora_model.weights.h5\"\n+        )\n+        model.save_weights(temp_filepath)\n+        new_model = models.Sequential(\n+            [layers.Input((3,), dtype=\"int32\"), layers.Embedding(10, 16)]\n+        )\n+        new_model.quantize(\"int8\")\n+        new_model.load_weights(temp_filepath)\n+        self.assertFalse(new_model.layers[0].lora_enabled)\n+        self.assertAllClose(model.predict(x), new_model.predict(x), atol=0.5)\n+\n+        # Try loading a normal checkpoint into a lora model\n+        new_model.save_weights(temp_filepath)\n+        model.load_weights(temp_filepath)\n+        self.assertAllClose(model.predict(x), new_model.predict(x), atol=0.5)\n+\n+        # Test export and TFSMLayer reloading when using tensorflow backend\n+        if backend.backend() == \"tensorflow\":\n+            import tensorflow as tf\n+\n+            temp_filepath = os.path.join(self.get_temp_dir(), \"exported_model\")\n+            ref_input = tf.random.normal((32, 3))\n+            ref_output = model(ref_input)\n+            export_lib.export_model(model, temp_filepath)\n+            reloaded_layer = export_lib.TFSMLayer(temp_filepath)\n+            self.assertAllClose(\n+                reloaded_layer(ref_input), ref_output, atol=1e-7\n+            )\n+            self.assertLen(reloaded_layer.weights, len(model.weights))\n+            self.assertLen(\n+                reloaded_layer.trainable_weights, len(model.trainable_weights)\n+            )\n+            self.assertLen(\n+                reloaded_layer.non_trainable_weights,\n+                len(model.non_trainable_weights),\n+            )\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#2958e0a8e61b01523b6232486683e21e22c85465", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 0 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 7 | Churn Cumulative: 17 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -111,6 +111,13 @@ class CenterCrop(TFDataLayer):\n \n     def compute_output_shape(self, input_shape):\n         input_shape = list(input_shape)\n+        if isinstance(input_shape[0], (list, tuple)) or len(\n+            input_shape\n+        ) not in (3, 4):\n+            raise ValueError(\n+                \"`input_shape` must be a non-nested tuple or list \"\n+                \"of rank-1 with size 3 (unbatched) or 4 (batched). \"\n+            )\n         if len(input_shape) == 4:\n             if self.data_format == \"channels_last\":\n                 input_shape[1] = self.height\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c9a1073d8248e8d65942280c41005296fb88b1c3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 30 | Lines Deleted: 0 | Files Changed: 2 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/1 | Churn Δ: 30 | Churn Cumulative: 391 | Contributors (this commit): 3 | Commits (past 90d): 10 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -69,6 +69,10 @@ class DTypePolicy:\n             return FloatDTypePolicy(name)\n         return super().__new__(cls)\n \n+    def __getnewargs__(self):\n+        # To support `copy`, `deepcopy` and `pickle`\n+        return (self._name,)\n+\n     def __init__(self, name):\n         self._name = name\n         self._compute_dtype = backend.floatx()\n\n@@ -61,6 +61,32 @@ class DTypePolicyTest(test_case.TestCase):\n         new_policy = DTypePolicy.from_config(config)\n         self.assertEqual(new_policy.name, \"mixed_float16\")\n \n+    def test_deepcopy(self):\n+        \"\"\"Test builtin serialization methods.\"\"\"\n+        import copy\n+        import pickle\n+\n+        # copy.deepcopy\n+        policy = DTypePolicy(\"mixed_float16\")\n+        copied_policy = copy.deepcopy(policy)\n+        self.assertEqual(\n+            repr(copied_policy), '<FloatDTypePolicy \"mixed_float16\">'\n+        )\n+        # copy.copy\n+        copied_policy = copy.copy(policy)\n+        self.assertEqual(\n+            repr(copied_policy), '<FloatDTypePolicy \"mixed_float16\">'\n+        )\n+        # pickle\n+        temp_dir = self.get_temp_dir()\n+        with open(f\"{temp_dir}/policy.pickle\", \"wb\") as f:\n+            pickle.dump(policy, f)\n+        with open(f\"{temp_dir}/policy.pickle\", \"rb\") as f:\n+            copied_policy = pickle.load(f)\n+        self.assertEqual(\n+            repr(copied_policy), '<FloatDTypePolicy \"mixed_float16\">'\n+        )\n+\n \n class FloatDTypePolicyTest(test_case.TestCase):\n     def test_initialization_valid_name(self):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#6f0d63e5dbe9cea6daec35f67055a95dc13d256c", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 445 | Lines Deleted: 173 | Files Changed: 18 | Hunks: 152 | Methods Changed: 81 | Complexity Δ (Sum/Max): 38/7 | Churn Δ: 618 | Churn Cumulative: 23456 | Contributors (this commit): 31 | Commits (past 90d): 122 | Contributors (cumulative): 137 | DMM Complexity: 0.5677083333333334\n\nDIFF:\n@@ -120,7 +120,6 @@ class VariablePropertiesTest(test_case.TestCase, parameterized.TestCase):\n         )\n         self.assertEqual(v.dtype, \"float32\")\n         self.assertEqual(backend.standardize_dtype(v.value.dtype), \"float32\")\n-        print(\"open scope\")\n         with AutocastScope(\"float16\"):\n             self.assertEqual(\n                 backend.standardize_dtype(v.value.dtype), \"float16\"\n\n@@ -1,4 +1,5 @@\n import jax\n+import jax.experimental.sparse as jax_sparse\n import jax.numpy as jnp\n import numpy as np\n from jax import lax\n@@ -404,19 +405,55 @@ def conv_transpose(\n     )\n \n \n-def one_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n+def one_hot(x, num_classes, axis=-1, dtype=\"float32\", sparse=False):\n     x = convert_to_tensor(x)\n+    if sparse:\n+        if axis < 0:\n+            axis = axis + len(x.shape) + 1\n+        if dtype is None:\n+            dtype = \"float32\"\n+        # We deal with negative inputs by having zeros in the output although\n+        # it's useless. It makes shapes static.\n+        values = jnp.greater_equal(jnp.ravel(x), 0).astype(dtype)\n+        values_count = values.shape[0]\n+        indices = [jnp.arange(dim) for dim in x.shape]\n+        indices = jnp.meshgrid(*indices, indexing=\"ij\")\n+        indices.insert(axis, jnp.maximum(x, 0))  # Deal with negative indices\n+        indices = [a.reshape(values_count, 1).astype(\"int32\") for a in indices]\n+        indices = jnp.concatenate(indices, axis=1)\n+        shape = list(x.shape)\n+        shape.insert(axis, num_classes)\n+        shape = tuple(shape)\n+        return jax_sparse.BCOO(\n+            (values, indices),\n+            shape=shape,\n+            indices_sorted=True,\n+            unique_indices=True,\n+        )\n     return jnn.one_hot(x, num_classes, axis=axis, dtype=dtype)\n \n \n-def multi_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n+def multi_hot(x, num_classes, axis=-1, dtype=\"float32\", sparse=False):\n     x = convert_to_tensor(x)\n     reduction_axis = 1 if len(x.shape) > 1 else 0\n-    outputs = jnp.max(\n+    if sparse:\n+        result = one_hot(\n+            x, num_classes, axis=axis, dtype=\"int32\", sparse=sparse\n+        )\n+        # JAX's BCOO does not support max reduction, use sum and compare with 0.\n+        result = jax_sparse.bcoo_reduce_sum(result, axes=(reduction_axis,))\n+        result = jax_sparse.bcoo_sum_duplicates(result)\n+        values = jnp.greater_equal(result.data, 0).astype(dtype)\n+        return jax_sparse.BCOO(\n+            (values, result.indices),\n+            shape=result.shape,\n+            indices_sorted=True,\n+            unique_indices=True,\n+        )\n+    return jnp.max(\n         one_hot(cast(x, \"int32\"), num_classes, axis=axis, dtype=dtype),\n         axis=reduction_axis,\n     )\n-    return outputs\n \n \n def categorical_crossentropy(target, output, from_logits=False, axis=-1):\n\n@@ -9,6 +9,7 @@ from keras.backend.common import dtypes\n from keras.backend.common.backend_utils import canonicalize_axis\n from keras.backend.common.backend_utils import to_tuple_or_list\n from keras.backend.common.variables import standardize_dtype\n+from keras.backend.jax import nn\n from keras.backend.jax import sparse\n from keras.backend.jax.core import cast\n from keras.backend.jax.core import convert_to_tensor\n@@ -21,7 +22,25 @@ def add(x1, x2):\n     return jnp.add(x1, x2)\n \n \n-def bincount(x, weights=None, minlength=0):\n+def bincount(x, weights=None, minlength=0, sparse=False):\n+    if sparse:\n+        reduction_axis = 1 if len(x.shape) > 1 else 0\n+        maxlength = jnp.maximum(jnp.max(x) + 1, minlength)\n+        one_hot_encoding = nn.one_hot(x, maxlength, sparse=sparse)\n+        if weights is not None:\n+            split_weights = split(\n+                weights,\n+                weights.shape[reduction_axis],\n+                reduction_axis,\n+            )\n+            stacked_weights = stack(split_weights, axis=reduction_axis)\n+            one_hot_encoding = one_hot_encoding * stacked_weights\n+\n+        outputs = jax_sparse.bcoo_reduce_sum(\n+            one_hot_encoding,\n+            axes=(reduction_axis,),\n+        )\n+        return outputs\n     if len(x.shape) == 2:\n         if weights is None:\n \n@@ -1069,6 +1088,26 @@ def var(x, axis=None, keepdims=False):\n \n def sum(x, axis=None, keepdims=False):\n     x = convert_to_tensor(x)\n+    if isinstance(x, jax_sparse.BCOO):\n+        if axis is None:\n+            axis = tuple(range(len(x.shape)))\n+        (\n+            canonical_axis,\n+            keep_dims_shape,\n+            broadcast_dimensions,\n+        ) = sparse.axis_shape_dims_for_broadcast_in_dim(\n+            axis, x.shape, insert_dims=False\n+        )\n+        output = jax_sparse.bcoo_reduce_sum(x, axes=canonical_axis)\n+        if keepdims:\n+            # `bcoo_reduce_sum` does not support keepdims, neither does\n+            # sparsify(jnp.sum), so we recreate the empty dimensions.\n+            output = jax_sparse.bcoo_broadcast_in_dim(\n+                output,\n+                shape=keep_dims_shape,\n+                broadcast_dimensions=broadcast_dimensions,\n+            )\n+        return output\n     return jnp.sum(x, axis=axis, keepdims=keepdims)\n \n \n\n@@ -445,7 +445,9 @@ def conv_transpose(\n     )\n \n \n-def one_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n+def one_hot(x, num_classes, axis=-1, dtype=\"float32\", sparse=False):\n+    if sparse:\n+        raise ValueError(\"Unsupported value `sparse=True` with numpy backend\")\n     x = convert_to_tensor(x)\n     input_shape = x.shape\n \n@@ -473,7 +475,9 @@ def one_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n     return categorical\n \n \n-def multi_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n+def multi_hot(x, num_classes, axis=-1, dtype=\"float32\", sparse=False):\n+    if sparse:\n+        raise ValueError(\"Unsupported value `sparse=True` with numpy backend\")\n     x = convert_to_tensor(x)\n     reduction_axis = 1 if len(x.shape) > 1 else 0\n     outputs = np.max(\n\n@@ -260,7 +260,9 @@ def average(x, axis=None, weights=None):\n     return np.average(x, weights=weights, axis=axis)\n \n \n-def bincount(x, weights=None, minlength=0):\n+def bincount(x, weights=None, minlength=0, sparse=False):\n+    if sparse:\n+        raise ValueError(\"Unsupported value `sparse=True` with numpy backend\")\n     x = convert_to_tensor(x)\n     dtypes_to_resolve = [x.dtype]\n     if weights is not None:\n\n@@ -1,3 +1,4 @@\n+import math\n import warnings\n \n import tensorflow as tf\n@@ -444,19 +445,44 @@ def conv_transpose(\n     )\n \n \n-def one_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n+def one_hot(x, num_classes, axis=-1, dtype=\"float32\", sparse=False):\n     x = convert_to_tensor(x)\n+    if dtype is None:\n+        dtype = \"float32\"\n+    if sparse:\n+        # We don't use `tf.sparse.bincount`, it doesn't handle negative indices\n+        # and only support rank 1 and 2 tensors (`one_hot` adds a dimension).\n+        if axis < 0:\n+            axis = axis + len(x.shape) + 1\n+        values_count = math.prod(x.shape)\n+        values = tf.reshape(x, (values_count,))\n+        # We deal with negative inputs by having zeros in the output although\n+        # it's useless. It makes shapes static.\n+        values = tf.cast(tf.greater_equal(values, 0), dtype=dtype)\n+        indices = [tf.range(dim) for dim in x.shape]\n+        indices = tf.meshgrid(*indices, indexing=\"ij\")\n+        indices.insert(axis, tf.maximum(x, 0))  # Deal with negative indices\n+        indices = [tf.reshape(a, (values_count, 1)) for a in indices]\n+        indices = [tf.cast(a, tf.int64) for a in indices]\n+        indices = tf.concat(indices, axis=1)\n+        shape = list(x.shape)\n+        shape.insert(axis, num_classes)\n+        return tf.SparseTensor(indices, values, shape)\n     return tf.one_hot(x, num_classes, axis=axis, dtype=dtype)\n \n \n-def multi_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n+def multi_hot(x, num_classes, axis=-1, dtype=\"float32\", sparse=False):\n     x = convert_to_tensor(x)\n     reduction_axis = 1 if len(x.shape) > 1 else 0\n-    outputs = tf.reduce_max(\n-        one_hot(cast(x, \"int32\"), num_classes, axis=axis, dtype=dtype),\n-        axis=reduction_axis,\n+    one_hot_outputs = one_hot(\n+        cast(x, \"int32\"), num_classes, axis=axis, dtype=dtype, sparse=sparse\n     )\n-    return outputs\n+    if sparse:\n+        # We don't use `tf.sparse.bincount`, it doesn't handle negative indices.\n+        return tf.sparse.reduce_max(\n+            one_hot_outputs, axis=reduction_axis, output_is_sparse=True\n+        )\n+    return tf.reduce_max(one_hot_outputs, axis=reduction_axis)\n \n \n def _get_logits(output, from_logits, op_type, fn_name):\n\n@@ -16,6 +16,7 @@ from keras.backend.common import dtypes\n from keras.backend.common.backend_utils import canonicalize_axis\n from keras.backend.common.backend_utils import to_tuple_or_list\n from keras.backend.tensorflow import sparse\n+from keras.backend.tensorflow.core import cast\n from keras.backend.tensorflow.core import convert_to_tensor\n from keras.utils import tree\n \n@@ -35,7 +36,7 @@ def add(x1, x2):\n     return tf.add(x1, x2)\n \n \n-def bincount(x, weights=None, minlength=0):\n+def bincount(x, weights=None, minlength=0, sparse=False):\n     x = convert_to_tensor(x)\n     dtypes_to_resolve = [x.dtype]\n     if standardize_dtype(x.dtype) not in [\"int32\", \"int64\"]:\n@@ -56,19 +57,24 @@ def bincount(x, weights=None, minlength=0):\n                 weights = tf.cast(weights, tf.float32)\n     else:\n         dtype = \"int32\"\n-    if isinstance(x, tf.SparseTensor):\n+    if sparse or isinstance(x, tf.SparseTensor):\n         output = tf.sparse.bincount(\n             x,\n             weights=weights,\n             minlength=minlength,\n             axis=-1,\n         )\n-        output = tf.cast(output, dtype)\n+        actual_length = output.shape[-1]\n+        if actual_length is None:\n+            actual_length = tf.shape(output)[-1]\n+        output = cast(output, dtype)\n         if x.shape.rank == 1:\n-            output_shape = (minlength,)\n+            output_shape = (actual_length,)\n         else:\n+            batch_size = output.shape[0]\n+            if batch_size is None:\n                 batch_size = tf.shape(output)[0]\n-            output_shape = (batch_size, minlength)\n+            output_shape = (batch_size, actual_length)\n         return tf.SparseTensor(\n             indices=output.indices,\n             values=output.values,\n@@ -946,11 +952,11 @@ def digitize(x, bins):\n     # int16, uint8, uint16, uint32\n     ori_dtype = standardize_dtype(x.dtype)\n     if ori_dtype in (\"bool\", \"int8\", \"int16\", \"uint8\", \"uint16\"):\n-        x = tf.cast(x, \"int32\")\n+        x = cast(x, \"int32\")\n     elif ori_dtype == \"uint32\":\n-        x = tf.cast(x, \"int64\")\n+        x = cast(x, \"int64\")\n     elif ori_dtype in (\"bfloat16\", \"float16\"):\n-        x = tf.cast(x, \"float32\")\n+        x = cast(x, \"float32\")\n \n     if isinstance(x, tf.RaggedTensor):\n         return tf.ragged.map_flat_values(\n@@ -2032,7 +2038,11 @@ def sum(x, axis=None, keepdims=False):\n         dtype = \"int32\"\n     elif dtype in (\"uint8\", \"uint16\"):\n         dtype = \"uint32\"\n-    x = tf.cast(x, dtype)\n+    x = cast(x, dtype)\n+    if isinstance(x, tf.SparseTensor):\n+        return tf.sparse.reduce_sum(\n+            x, axis=axis, keepdims=keepdims, output_is_sparse=True\n+        )\n     return tf.reduce_sum(x, axis=axis, keepdims=keepdims)\n \n \n\n@@ -552,7 +552,9 @@ def conv_transpose(\n     return outputs\n \n \n-def one_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n+def one_hot(x, num_classes, axis=-1, dtype=\"float32\", sparse=False):\n+    if sparse:\n+        raise ValueError(\"Unsupported value `sparse=True` with torch backend\")\n     # Axis is the output axis. By default, PyTorch, outputs to last axis.\n     # If axis is not last, change output to axis and shift remaining elements.\n     x = convert_to_tensor(x, dtype=torch.long)\n@@ -575,7 +577,9 @@ def one_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n     return output\n \n \n-def multi_hot(x, num_classes, axis=-1, dtype=\"float32\"):\n+def multi_hot(x, num_classes, axis=-1, dtype=\"float32\", sparse=False):\n+    if sparse:\n+        raise ValueError(\"Unsupported value `sparse=True` with torch backend\")\n     x = convert_to_tensor(x)\n     reduction_axis = 1 if len(x.shape) > 1 else 0\n     outputs = torch.amax(\n\n@@ -375,7 +375,9 @@ def average(x, axis=None, weights=None):\n     return torch.mean(x, axis)\n \n \n-def bincount(x, weights=None, minlength=0):\n+def bincount(x, weights=None, minlength=0, sparse=False):\n+    if sparse:\n+        raise ValueError(\"Unsupported value `sparse=True` with torch backend\")\n     x = convert_to_tensor(x)\n     dtypes_to_resolve = [x.dtype]\n     if weights is not None:\n\n@@ -273,14 +273,12 @@ class MergingLayersTest(testing.TestCase, parameterized.TestCase):\n \n             x1 = tf.SparseTensor([[0, 0], [1, 2]], [1.0, 2.0], (2, 3))\n             x3 = tf.SparseTensor([[0, 0], [1, 1]], [4.0, 5.0], (2, 3))\n-            sparse_class = tf.SparseTensor\n         elif backend.backend() == \"jax\":\n             import jax.experimental.sparse as jax_sparse\n \n             # Use n_batch of 1 to be compatible with all ops.\n             x1 = jax_sparse.BCOO(([[1.0, 2.0]], [[[0], [2]]]), shape=(2, 3))\n             x3 = jax_sparse.BCOO(([[4.0, 5.0]], [[[0], [1]]]), shape=(2, 3))\n-            sparse_class = jax_sparse.JAXSparse\n         else:\n             self.fail(f\"Sparse is unsupported with backend {backend.backend()}\")\n \n@@ -292,5 +290,5 @@ class MergingLayersTest(testing.TestCase, parameterized.TestCase):\n         # Merging a sparse tensor with a sparse tensor produces a sparse tensor\n         x3_np = backend.convert_to_numpy(x3)\n \n-        self.assertIsInstance(layer([x1, x3]), sparse_class)\n+        self.assertSparse(layer([x1, x3]))\n         self.assertAllClose(layer([x1, x3]), np_op(x1_np, x3_np, **init_kwargs))\n\n@@ -1,4 +1,5 @@\n from keras.api_export import keras_export\n+from keras.backend import KerasTensor\n from keras.layers.preprocessing.tf_data_layer import TFDataLayer\n from keras.utils import backend_utils\n \n@@ -73,6 +74,8 @@ class CategoryEncoding(TFDataLayer):\n             For all output modes, currently only output up to rank 2 is\n             supported.\n             Defaults to `\"multi_hot\"`.\n+        sparse: Whether to return a sparse tensor; for backends that support\n+            sparse tensors.\n \n     Call arguments:\n         inputs: A 1D or 2D tensor of integer inputs.\n@@ -81,7 +84,9 @@ class CategoryEncoding(TFDataLayer):\n             Not used in `\"multi_hot\"` or `\"one_hot\"` modes.\n     \"\"\"\n \n-    def __init__(self, num_tokens=None, output_mode=\"multi_hot\", **kwargs):\n+    def __init__(\n+        self, num_tokens=None, output_mode=\"multi_hot\", sparse=False, **kwargs\n+    ):\n         super().__init__(**kwargs)\n \n         # Support deprecated names for output_modes.\n@@ -104,14 +109,26 @@ class CategoryEncoding(TFDataLayer):\n             )\n         self.num_tokens = num_tokens\n         self.output_mode = output_mode\n+        self.sparse = sparse\n         self._allow_non_tensor_positional_args = True\n         self._convert_input_args = False\n \n     def _count(self, inputs, axis=-1, count_weights=None):\n+        # We don't use `ops.bincount` here because its output has a dynamic\n+        # shape (the last dimension is calculated from the highest value of\n+        # `inputs`). Instead, we reimplement a narrower use case where\n+        # `minlength` and `maxlength` (not supported by `ops.bincount`) are a\n+        # static value and the same value of `self.num_tokens`. Also, we don't\n+        # need to support indices that are negative or greater than\n+        # `self.num_tokens`.\n         reduction_axis = 1 if len(inputs.shape) > 1 else 0\n \n         one_hot_encoding = self.backend.nn.one_hot(\n-            inputs, self.num_tokens, axis=axis, dtype=self.dtype\n+            inputs,\n+            self.num_tokens,\n+            axis=axis,\n+            dtype=self.dtype,\n+            sparse=self.sparse,\n         )\n         if count_weights is not None:\n             split_weights = self.backend.numpy.split(\n@@ -132,17 +149,15 @@ class CategoryEncoding(TFDataLayer):\n \n     def _encode(self, inputs, count_weights=None):\n         if self.output_mode == \"multi_hot\":\n-            outputs = self.backend.nn.multi_hot(\n-                inputs, self.num_tokens, dtype=self.dtype\n+            return self.backend.nn.multi_hot(\n+                inputs, self.num_tokens, dtype=self.dtype, sparse=self.sparse\n             )\n         elif self.output_mode == \"one_hot\":\n-            outputs = self.backend.nn.one_hot(\n-                inputs, self.num_tokens, dtype=self.dtype\n+            return self.backend.nn.one_hot(\n+                inputs, self.num_tokens, dtype=self.dtype, sparse=self.sparse\n             )\n         elif self.output_mode == \"count\":\n-            outputs = self._count(inputs, count_weights=count_weights)\n-\n-        return outputs\n+            return self._count(inputs, count_weights=count_weights)\n \n     def compute_output_shape(self, input_shape):\n         if self.output_mode == \"one_hot\":\n@@ -152,6 +167,12 @@ class CategoryEncoding(TFDataLayer):\n                 return tuple(input_shape[:-1] + (self.num_tokens,))\n         return tuple(input_shape[:-1] + (self.num_tokens,))\n \n+    def compute_output_spec(self, inputs, count_weights=None):\n+        output_shape = self.compute_output_shape(inputs.shape)\n+        return KerasTensor(\n+            output_shape, dtype=self.compute_dtype, sparse=self.sparse\n+        )\n+\n     def get_config(self):\n         config = {\n             \"num_tokens\": self.num_tokens,\n\n@@ -1,22 +1,32 @@\n import numpy as np\n+from absl.testing import parameterized\n from tensorflow import data as tf_data\n \n+from keras import backend\n from keras import layers\n from keras import testing\n \n+TEST_CASES = [{\"testcase_name\": \"dense\", \"sparse\": False}]\n+if backend.SUPPORTS_SPARSE_TENSORS:\n+    TEST_CASES += [{\"testcase_name\": \"sparse\", \"sparse\": True}]\n \n-class CategoryEncodingTest(testing.TestCase):\n-    def test_count_output(self):\n+\n+class CategoryEncodingTest(testing.TestCase, parameterized.TestCase):\n+    @parameterized.named_parameters(TEST_CASES)\n+    def test_count_output(self, sparse):\n         input_array = np.array([1, 2, 3, 1])\n         expected_output = np.array([0, 2, 1, 1, 0, 0])\n \n         num_tokens = 6\n         expected_output_shape = (num_tokens,)\n \n-        layer = layers.CategoryEncoding(num_tokens=6, output_mode=\"count\")\n+        layer = layers.CategoryEncoding(\n+            num_tokens=num_tokens, output_mode=\"count\", sparse=sparse\n+        )\n         int_data = layer(input_array)\n         self.assertEqual(expected_output_shape, int_data.shape)\n         self.assertAllClose(int_data, expected_output)\n+        self.assertSparse(int_data, sparse)\n \n         # Test symbolic call.\n         output = layer(\n@@ -24,47 +34,60 @@ class CategoryEncodingTest(testing.TestCase):\n         )\n         self.assertEqual(expected_output_shape, output.shape)\n         self.assertEqual(\"float32\", output.dtype)\n+        self.assertSparse(output, sparse)\n \n-    def test_count_weighted_output(self):\n+    @parameterized.named_parameters(TEST_CASES)\n+    def test_count_weighted_output(self, sparse):\n         input_array = np.array([[0, 1], [0, 0], [1, 2], [3, 1]])\n         count_weights = np.array(\n             [[0.1, 0.2], [0.1, 0.1], [0.2, 0.3], [0.4, 0.2]]\n         )\n         expected_output = np.array(\n             [\n-                [0.1, 0.2, 0.0, 0.0],\n-                [0.2, 0.0, 0.0, 0.0],\n-                [0.0, 0.2, 0.3, 0.0],\n-                [0.0, 0.2, 0.0, 0.4],\n+                [0.1, 0.2, 0.0, 0.0, 0.0, 0.0],\n+                [0.2, 0.0, 0.0, 0.0, 0.0, 0.0],\n+                [0.0, 0.2, 0.3, 0.0, 0.0, 0.0],\n+                [0.0, 0.2, 0.0, 0.4, 0.0, 0.0],\n             ]\n         )\n \n-        num_tokens = 4\n-        expected_output_shape = (num_tokens, num_tokens)\n+        num_tokens = 6\n+        expected_output_shape = (input_array.shape[0], num_tokens)\n \n-        layer = layers.CategoryEncoding(num_tokens=4, output_mode=\"count\")\n+        layer = layers.CategoryEncoding(\n+            num_tokens=num_tokens, output_mode=\"count\", sparse=sparse\n+        )\n         int_data = layer(input_array, count_weights=count_weights)\n         self.assertEqual(expected_output_shape, int_data.shape)\n         self.assertAllClose(int_data, expected_output)\n+        self.assertSparse(int_data, sparse)\n \n         # Test symbolic call.\n         output = layer(\n-            layers.Input(batch_shape=input_array.shape, dtype=\"int32\")\n+            layers.Input(batch_shape=input_array.shape, dtype=\"int32\"),\n+            count_weights=layers.Input(\n+                batch_shape=input_array.shape, dtype=\"float32\"\n+            ),\n         )\n         self.assertEqual(expected_output_shape, output.shape)\n         self.assertEqual(\"float32\", output.dtype)\n+        self.assertSparse(output, sparse)\n \n-    def test_batched_count_output(self):\n+    @parameterized.named_parameters(TEST_CASES)\n+    def test_batched_count_output(self, sparse):\n         input_array = np.array([[1, 2, 3, 1], [0, 3, 1, 0]])\n         expected_output = np.array([[0, 2, 1, 1, 0, 0], [2, 1, 0, 1, 0, 0]])\n \n         num_tokens = 6\n         expected_output_shape = (2, num_tokens)\n \n-        layer = layers.CategoryEncoding(num_tokens=6, output_mode=\"count\")\n+        layer = layers.CategoryEncoding(\n+            num_tokens=num_tokens, output_mode=\"count\", sparse=sparse\n+        )\n         int_data = layer(input_array)\n         self.assertEqual(expected_output_shape, int_data.shape)\n         self.assertAllClose(int_data, expected_output)\n+        self.assertSparse(int_data, sparse)\n \n         # Test symbolic call.\n         output = layer(\n@@ -72,8 +95,10 @@ class CategoryEncodingTest(testing.TestCase):\n         )\n         self.assertEqual(expected_output_shape, output.shape)\n         self.assertEqual(\"float32\", output.dtype)\n+        self.assertSparse(output, sparse)\n \n-    def test_multi_hot(self):\n+    @parameterized.named_parameters(TEST_CASES)\n+    def test_multi_hot(self, sparse):\n         input_data = np.array([3, 2, 0, 1])\n         expected_output = np.array([1, 1, 1, 1, 0, 0])\n         num_tokens = 6\n@@ -81,11 +106,12 @@ class CategoryEncodingTest(testing.TestCase):\n \n         # Test call on layer directly.\n         layer = layers.CategoryEncoding(\n-            num_tokens=num_tokens, output_mode=\"multi_hot\"\n+            num_tokens=num_tokens, output_mode=\"multi_hot\", sparse=sparse\n         )\n         output_data = layer(input_data)\n         self.assertAllClose(expected_output, output_data)\n         self.assertEqual(expected_output_shape, output_data.shape)\n+        self.assertSparse(output_data, sparse)\n \n         # Test symbolic call.\n         output = layer(\n@@ -93,20 +119,23 @@ class CategoryEncodingTest(testing.TestCase):\n         )\n         self.assertEqual(expected_output_shape, output.shape)\n         self.assertEqual(\"float32\", output.dtype)\n+        self.assertSparse(output, sparse)\n \n-    def test_batched_multi_hot(self):\n+    @parameterized.named_parameters(TEST_CASES)\n+    def test_batched_multi_hot(self, sparse):\n         input_data = np.array([[3, 2, 0, 1], [3, 2, 0, 1]])\n         expected_output = np.array([[1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 0, 0]])\n         num_tokens = 6\n-        expected_output_shape = (2, num_tokens)\n+        expected_output_shape = (input_data.shape[0], num_tokens)\n \n         # Test call on layer directly.\n         layer = layers.CategoryEncoding(\n-            num_tokens=num_tokens, output_mode=\"multi_hot\"\n+            num_tokens=num_tokens, output_mode=\"multi_hot\", sparse=sparse\n         )\n         output_data = layer(input_data)\n         self.assertAllClose(expected_output, output_data)\n         self.assertEqual(expected_output_shape, output_data.shape)\n+        self.assertSparse(output_data, sparse)\n \n         # Test symbolic call.\n         output = layer(\n@@ -114,27 +143,30 @@ class CategoryEncodingTest(testing.TestCase):\n         )\n         self.assertEqual(expected_output_shape, output.shape)\n         self.assertEqual(\"float32\", output.dtype)\n+        self.assertSparse(output, sparse)\n \n-    def test_one_hot(self):\n+    @parameterized.named_parameters(TEST_CASES)\n+    def test_one_hot(self, sparse):\n         input_data = np.array([3, 2, 0, 1])\n         expected_output = np.array(\n             [\n-                [0, 0, 0, 1],\n-                [0, 0, 1, 0],\n-                [1, 0, 0, 0],\n-                [0, 1, 0, 0],\n+                [0, 0, 0, 1, 0, 0],\n+                [0, 0, 1, 0, 0, 0],\n+                [1, 0, 0, 0, 0, 0],\n+                [0, 1, 0, 0, 0, 0],\n             ]\n         )\n-        num_tokens = 4\n-        expected_output_shape = (num_tokens, num_tokens)\n+        num_tokens = 6\n+        expected_output_shape = (input_data.shape[0], num_tokens)\n \n         # Test call on layer directly.\n         layer = layers.CategoryEncoding(\n-            num_tokens=num_tokens, output_mode=\"one_hot\"\n+            num_tokens=num_tokens, output_mode=\"one_hot\", sparse=sparse\n         )\n         output_data = layer(input_data)\n         self.assertAllClose(expected_output, output_data)\n         self.assertEqual(expected_output_shape, output_data.shape)\n+        self.assertSparse(output_data, sparse)\n \n         # Test symbolic call.\n         output = layer(\n@@ -142,35 +174,38 @@ class CategoryEncodingTest(testing.TestCase):\n         )\n         self.assertEqual(expected_output_shape, output.shape)\n         self.assertEqual(\"float32\", output.dtype)\n+        self.assertSparse(output, sparse)\n \n-    def test_batched_one_hot(self):\n+    @parameterized.named_parameters(TEST_CASES)\n+    def test_batched_one_hot(self, sparse):\n         input_data = np.array([[3, 2, 0, 1], [3, 2, 0, 1]])\n         expected_output = np.array(\n             [\n                 [\n-                    [0, 0, 0, 1],\n-                    [0, 0, 1, 0],\n-                    [1, 0, 0, 0],\n-                    [0, 1, 0, 0],\n+                    [0, 0, 0, 1, 0, 0],\n+                    [0, 0, 1, 0, 0, 0],\n+                    [1, 0, 0, 0, 0, 0],\n+                    [0, 1, 0, 0, 0, 0],\n                 ],\n                 [\n-                    [0, 0, 0, 1],\n-                    [0, 0, 1, 0],\n-                    [1, 0, 0, 0],\n-                    [0, 1, 0, 0],\n+                    [0, 0, 0, 1, 0, 0],\n+                    [0, 0, 1, 0, 0, 0],\n+                    [1, 0, 0, 0, 0, 0],\n+                    [0, 1, 0, 0, 0, 0],\n                 ],\n             ]\n         )\n-        num_tokens = 4\n-        expected_output_shape = (2, num_tokens, num_tokens)\n+        num_tokens = 6\n+        expected_output_shape = input_data.shape[0:2] + (num_tokens,)\n \n         # Test call on layer directly.\n         layer = layers.CategoryEncoding(\n-            num_tokens=num_tokens, output_mode=\"one_hot\"\n+            num_tokens=num_tokens, output_mode=\"one_hot\", sparse=sparse\n         )\n         output_data = layer(input_data)\n         self.assertAllClose(expected_output, output_data)\n         self.assertEqual(expected_output_shape, output_data.shape)\n+        self.assertSparse(output_data, sparse)\n \n         # Test symbolic call.\n         output = layer(\n@@ -178,6 +213,7 @@ class CategoryEncodingTest(testing.TestCase):\n         )\n         self.assertEqual(expected_output_shape, output.shape)\n         self.assertEqual(\"float32\", output.dtype)\n+        self.assertSparse(output, sparse)\n \n     def test_tf_data_compatibility(self):\n         layer = layers.CategoryEncoding(\n\n@@ -392,23 +392,21 @@ class CoreOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             import tensorflow as tf\n \n             x = tf.SparseTensor([[0, 0], [1, 2]], [1.0, 2.0], (2, 3))\n-            sparse_class = tf.SparseTensor\n         elif backend.backend() == \"jax\":\n             import jax.experimental.sparse as jax_sparse\n \n             x = jax_sparse.BCOO(([1.0, 2.0], [[0, 0], [1, 2]]), shape=(2, 3))\n-            sparse_class = jax_sparse.JAXSparse\n         else:\n             self.fail(f\"Sparse is unsupported with backend {backend.backend()}\")\n \n         x_default = ops.convert_to_tensor(x)\n-        self.assertIsInstance(x_default, sparse_class)\n+        self.assertSparse(x_default)\n         self.assertAllClose(x, x_default)\n         x_sparse = ops.convert_to_tensor(x, sparse=True)\n-        self.assertIsInstance(x_sparse, sparse_class)\n+        self.assertSparse(x_sparse)\n         self.assertAllClose(x, x_sparse)\n         x_dense = ops.convert_to_tensor(x, sparse=False)\n-        self.assertNotIsInstance(x_dense, sparse_class)\n+        self.assertSparse(x_dense, False)\n         self.assertAllClose(x, x_dense)\n \n         x_numpy = ops.convert_to_numpy(x)\n\n@@ -1255,15 +1255,20 @@ def conv_transpose(\n \n \n class OneHot(Operation):\n-    def __init__(self, num_classes, axis=-1, dtype=None):\n+    def __init__(self, num_classes, axis=-1, dtype=None, sparse=False):\n         super().__init__()\n         self.num_classes = num_classes\n         self.axis = axis\n         self.dtype = dtype or backend.floatx()\n+        self.sparse = sparse\n \n     def call(self, x):\n         return backend.nn.one_hot(\n-            x, self.num_classes, axis=self.axis, dtype=self.dtype\n+            x,\n+            self.num_classes,\n+            axis=self.axis,\n+            dtype=self.dtype,\n+            sparse=self.sparse,\n         )\n \n     def compute_output_spec(self, x):\n@@ -1277,11 +1282,11 @@ class OneHot(Operation):\n                 f\"axis must be -1 or between [0, {len(x.shape)}), but \"\n                 f\"received {self.axis}.\"\n             )\n-        return KerasTensor(x_shape, dtype=self.dtype)\n+        return KerasTensor(x_shape, dtype=self.dtype, sparse=self.sparse)\n \n \n @keras_export([\"keras.ops.one_hot\", \"keras.ops.nn.one_hot\"])\n-def one_hot(x, num_classes, axis=-1, dtype=None):\n+def one_hot(x, num_classes, axis=-1, dtype=None, sparse=False):\n     \"\"\"Converts integer tensor `x` into a one-hot tensor.\n \n     The one-hot encoding is a representation where each integer value is\n@@ -1297,6 +1302,8 @@ def one_hot(x, num_classes, axis=-1, dtype=None):\n             `-1`, which represents the last axis.\n         dtype: (Optional) Data type of the output tensor. If not\n             provided, it defaults to the default data type of the backend.\n+        sparse: Whether to return a sparse tensor; for backends that support\n+            sparse tensors.\n \n     Returns:\n         Integer tensor: One-hot encoded tensor with the same shape as `x`\n@@ -1314,9 +1321,15 @@ def one_hot(x, num_classes, axis=-1, dtype=None):\n            [1. 0. 0. 0.]], shape=(4, 4), dtype=float32)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n-        return OneHot(num_classes, axis=axis, dtype=dtype).symbolic_call(x)\n+        return OneHot(\n+            num_classes, axis=axis, dtype=dtype, sparse=sparse\n+        ).symbolic_call(x)\n     return backend.nn.one_hot(\n-        x, num_classes, axis=axis, dtype=dtype or backend.floatx()\n+        x,\n+        num_classes,\n+        axis=axis,\n+        dtype=dtype or backend.floatx(),\n+        sparse=sparse,\n     )\n \n \n@@ -1558,16 +1571,17 @@ def sparse_categorical_crossentropy(target, output, from_logits=False, axis=-1):\n \n class MultiHot(Operation):\n     def __init__(\n-        self, num_classes=None, axis=-1, dtype=None, name=None, **kwargs\n+        self, num_classes=None, axis=-1, dtype=None, sparse=False, **kwargs\n     ):\n         if num_classes is None and \"num_tokens\" in kwargs:\n             num_classes = kwargs.pop(\"num_tokens\")\n         if num_classes is None:\n             raise ValueError(\"Argument `num_classes` must be specified.\")\n-        super().__init__(name, **kwargs)\n+        super().__init__(**kwargs)\n         self.num_classes = num_classes\n         self.axis = axis\n         self.dtype = dtype or backend.floatx()\n+        self.sparse = sparse\n \n     def call(self, inputs):\n         return backend.nn.multi_hot(\n@@ -1594,7 +1608,7 @@ class MultiHot(Operation):\n         else:\n             x_shape = [x_shape[0]] + x_shape[2:]\n \n-        return KerasTensor(x_shape, dtype=inputs.dtype)\n+        return KerasTensor(x_shape, dtype=inputs.dtype, sparse=self.sparse)\n \n \n @keras_export(\n@@ -1603,7 +1617,9 @@ class MultiHot(Operation):\n         \"keras.ops.nn.multi_hot\",\n     ]\n )\n-def multi_hot(inputs, num_classes=None, axis=-1, dtype=None, **kwargs):\n+def multi_hot(\n+    inputs, num_classes=None, axis=-1, dtype=None, sparse=False, **kwargs\n+):\n     \"\"\"Encodes integer labels as multi-hot vectors.\n \n     This function encodes integer labels as multi-hot vectors, where each label\n@@ -1616,6 +1632,8 @@ def multi_hot(inputs, num_classes=None, axis=-1, dtype=None, **kwargs):\n             added. Defaults to `-1`, which corresponds to the last dimension.\n         dtype: (optional) The data type of the resulting tensor. Default\n             is backend's float type.\n+        sparse: Whether to return a sparse tensor; for backends that support\n+            sparse tensors.\n \n     Returns:\n         Tensor: The multi-hot encoded tensor.\n@@ -1633,14 +1651,14 @@ def multi_hot(inputs, num_classes=None, axis=-1, dtype=None, **kwargs):\n         raise ValueError(\"Argument `num_classes` must be specified.\")\n \n     if any_symbolic_tensors((inputs,)):\n-        return MultiHot(num_classes, axis, dtype).symbolic_call(inputs)\n+        return MultiHot(num_classes, axis, dtype, sparse).symbolic_call(inputs)\n \n-    return backend.nn.multi_hot(inputs, num_classes, axis, dtype)\n+    return backend.nn.multi_hot(inputs, num_classes, axis, dtype, sparse)\n \n \n class Moments(Operation):\n-    def __init__(self, axes, keepdims=False, synchronized=False, name=None):\n-        super().__init__(name)\n+    def __init__(self, axes, keepdims=False, synchronized=False):\n+        super().__init__()\n         self.axes = axes\n         self.keepdims = keepdims\n         self.synchronized = synchronized\n@@ -1709,8 +1727,8 @@ def moments(x, axes, keepdims=False, synchronized=False):\n \n \n class BatchNorm(Operation):\n-    def __init__(self, axis, epsilon, name=None):\n-        super().__init__(name)\n+    def __init__(self, axis, epsilon):\n+        super().__init__()\n         self.axis = axis\n         self.epsilon = epsilon\n \n\n@@ -202,6 +202,7 @@ class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(knn.multi_hot(x, 5).shape, (None, 1, 5))\n         self.assertEqual(knn.multi_hot(x, 5, 1).shape, (None, 3, 1))\n         self.assertEqual(knn.multi_hot(x, 5, 2).shape, (None, 5, 1))\n+        self.assertSparse(knn.multi_hot(x, 5, sparse=True))\n \n     @parameterized.product(dtype=[\"float32\", \"int32\"])\n     def test_multi_hot_dtype(self, dtype):\n@@ -555,6 +556,7 @@ class NNOpsDynamicShapeTest(testing.TestCase, parameterized.TestCase):\n         self.assertEqual(knn.one_hot(x, 5).shape, (None, 3, 1, 5))\n         self.assertEqual(knn.one_hot(x, 5, 1).shape, (None, 5, 3, 1))\n         self.assertEqual(knn.one_hot(x, 5, 2).shape, (None, 3, 5, 1))\n+        self.assertSparse(knn.one_hot(x, 5, sparse=True))\n \n     @parameterized.product(dtype=[\"float32\", \"int32\"])\n     def test_one_hot_dtype(self, dtype):\n@@ -1012,6 +1014,7 @@ class NNOpsStaticShapeTest(testing.TestCase):\n         self.assertEqual(knn.one_hot(x, 5).shape, (2, 3, 1, 5))\n         self.assertEqual(knn.one_hot(x, 5, 1).shape, (2, 5, 3, 1))\n         self.assertEqual(knn.one_hot(x, 5, 2).shape, (2, 3, 5, 1))\n+        self.assertSparse(knn.one_hot(x, 5, sparse=True))\n \n     def test_binary_crossentropy(self):\n         x1 = KerasTensor([2, 3, 1])\n@@ -1552,54 +1555,63 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         )\n         self.assertAllClose(outputs, expected)\n \n-    def test_one_hot(self):\n+    @parameterized.named_parameters(\n+        [\n+            {\"testcase_name\": \"dense\", \"sparse\": False},\n+            {\"testcase_name\": \"sparse\", \"sparse\": True},\n+        ]\n+    )\n+    def test_one_hot(self, sparse):\n+        if sparse and not backend.SUPPORTS_SPARSE_TENSORS:\n+            pytest.skip(\"Backend does not support sparse tensors\")\n         # Test 1D one-hot.\n         indices_1d = np.array([0, 1, 2, 3])\n-        self.assertAllClose(knn.one_hot(indices_1d, 4), np.eye(4)[indices_1d])\n-        self.assertAllClose(\n-            knn.one_hot(indices_1d, 4, axis=0),\n-            np.eye(4)[indices_1d],\n-        )\n+        output_1d = knn.one_hot(indices_1d, 4, sparse=sparse)\n+        self.assertAllClose(output_1d, np.eye(4)[indices_1d])\n+        self.assertSparse(output_1d, sparse)\n+        output_1d = knn.one_hot(indices_1d, 4, axis=0, sparse=sparse)\n+        self.assertAllClose(output_1d, np.eye(4)[indices_1d])\n+        self.assertSparse(output_1d, sparse)\n \n         # Test 1D list one-hot.\n         indices_1d = [0, 1, 2, 3]\n-        self.assertAllClose(knn.one_hot(indices_1d, 4), np.eye(4)[indices_1d])\n-        self.assertAllClose(\n-            knn.one_hot(indices_1d, 4, axis=0),\n-            np.eye(4)[indices_1d],\n-        )\n+        output_1d = knn.one_hot(indices_1d, 4, sparse=sparse)\n+        self.assertAllClose(output_1d, np.eye(4)[indices_1d])\n+        self.assertSparse(output_1d, sparse)\n+        output_1d = knn.one_hot(indices_1d, 4, axis=0, sparse=sparse)\n+        self.assertAllClose(output_1d, np.eye(4)[indices_1d])\n+        self.assertSparse(output_1d, sparse)\n \n         # Test 2D one-hot.\n         indices_2d = np.array([[0, 1], [2, 3]])\n-        self.assertAllClose(knn.one_hot(indices_2d, 4), np.eye(4)[indices_2d])\n+        output_2d = knn.one_hot(indices_2d, 4, sparse=sparse)\n+        self.assertAllClose(output_2d, np.eye(4)[indices_2d])\n+        self.assertSparse(output_2d, sparse)\n+        output_2d = knn.one_hot(indices_2d, 4, axis=2, sparse=sparse)\n+        self.assertAllClose(output_2d, np.eye(4)[indices_2d])\n+        self.assertSparse(output_2d, sparse)\n+        output_2d = knn.one_hot(indices_2d, 4, axis=1, sparse=sparse)\n         self.assertAllClose(\n-            knn.one_hot(indices_2d, 4, axis=2),\n-            np.eye(4)[indices_2d],\n-        )\n-        self.assertAllClose(\n-            knn.one_hot(indices_2d, 4, axis=1),\n-            np.transpose(np.eye(4)[indices_2d], (0, 2, 1)),\n+            output_2d, np.transpose(np.eye(4)[indices_2d], (0, 2, 1))\n         )\n+        self.assertSparse(output_2d, sparse)\n \n         # Test 1D one-hot with negative inputs\n         indices_1d = np.array([0, -1, -1, 3])\n+        output_1d = knn.one_hot(indices_1d, 4, sparse=sparse)\n         self.assertAllClose(\n-            knn.one_hot(indices_1d, 4),\n+            output_1d,\n             np.array(\n                 [\n                     [1, 0, 0, 0],\n                     [0, 0, 0, 0],\n                     [0, 0, 0, 0],\n-                    [\n-                        0,\n-                        0,\n-                        0,\n-                        1,\n-                    ],\n+                    [0, 0, 0, 1],\n                 ],\n                 dtype=np.float32,\n             ),\n         )\n+        self.assertSparse(output_1d, sparse)\n \n     def test_binary_crossentropy(self):\n         # Test with from_logits=False\n@@ -1689,21 +1701,36 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         )\n         self.assertAllClose(result, [0.001822, 0.000459, 0.169846])\n \n-    def test_multi_hot(self):\n+    @parameterized.named_parameters(\n+        [\n+            {\"testcase_name\": \"dense\", \"sparse\": False},\n+            {\"testcase_name\": \"sparse\", \"sparse\": True},\n+        ]\n+    )\n+    def test_multi_hot(self, sparse):\n+        if sparse and not backend.SUPPORTS_SPARSE_TENSORS:\n+            pytest.skip(\"Backend does not support sparse tensors\")\n+\n         # Test 1D multi-hot.\n         indices_1d = np.array([0, 1, 2, 3])\n         expected_output_1d = np.array([1, 1, 1, 1])\n-        self.assertAllClose(knn.multi_hot(indices_1d, 4), expected_output_1d)\n+        output_1d = knn.multi_hot(indices_1d, 4, sparse=sparse)\n+        self.assertAllClose(output_1d, expected_output_1d)\n+        self.assertSparse(output_1d, sparse)\n \n         # Test 2D multi-hot.\n         indices_2d = np.array([[0, 1], [2, 3]])\n         expected_output_2d = np.array([[1, 1, 0, 0], [0, 0, 1, 1]])\n-        self.assertAllClose(knn.multi_hot(indices_2d, 4), expected_output_2d)\n+        output_2d = knn.multi_hot(indices_2d, 4, sparse=sparse)\n+        self.assertAllClose(output_2d, expected_output_2d)\n+        self.assertSparse(output_2d, sparse)\n \n         # Test 1D multi-hot with negative inputs\n         indices_1d = np.array([0, -1, -1, 3])\n         expected_output_1d = np.array([1, 0, 0, 1])\n-        self.assertAllClose(knn.multi_hot(indices_1d, 4), expected_output_1d)\n+        output_1d = knn.multi_hot(indices_1d, 4, sparse=sparse)\n+        self.assertAllClose(output_1d, expected_output_1d)\n+        self.assertSparse(output_1d, sparse)\n \n     def test_moments(self):\n         # Test 1D moments\n\n@@ -1240,14 +1240,18 @@ def average(x, axis=None, weights=None):\n \n \n class Bincount(Operation):\n-    def __init__(self, weights=None, minlength=0):\n+    def __init__(self, weights=None, minlength=0, sparse=False):\n         super().__init__()\n         self.weights = weights\n         self.minlength = minlength\n+        self.sparse = sparse\n \n     def call(self, x):\n         return backend.numpy.bincount(\n-            x, weights=self.weights, minlength=self.minlength\n+            x,\n+            weights=self.weights,\n+            minlength=self.minlength,\n+            sparse=self.sparse,\n         )\n \n     def compute_output_spec(self, x):\n@@ -1258,11 +1262,13 @@ class Bincount(Operation):\n             dtype = dtypes.result_type(*dtypes_to_resolve)\n         else:\n             dtype = \"int32\"\n-        return KerasTensor(list(x.shape[:-1]) + [None], dtype=dtype)\n+        return KerasTensor(\n+            list(x.shape[:-1]) + [None], dtype=dtype, sparse=self.sparse\n+        )\n \n \n @keras_export([\"keras.ops.bincount\", \"keras.ops.numpy.bincount\"])\n-def bincount(x, weights=None, minlength=0):\n+def bincount(x, weights=None, minlength=0, sparse=False):\n     \"\"\"Count the number of occurrences of each value in a tensor of integers.\n \n     Args:\n@@ -1278,6 +1284,8 @@ def bincount(x, weights=None, minlength=0):\n             this number of bins in the output tensor. If greater than\n             `max(x) + 1`, each value of the output at an index higher than\n             `max(x)` is set to 0.\n+        sparse: Whether to return a sparse tensor; for backends that support\n+            sparse tensors.\n \n     Returns:\n         1D tensor where each element gives the number of occurrence(s) of its\n@@ -1298,8 +1306,12 @@ def bincount(x, weights=None, minlength=0):\n     array([0, 1, 2, 1, 0, 0], dtype=int32)\n     \"\"\"\n     if any_symbolic_tensors((x,)):\n-        return Bincount(weights=weights, minlength=minlength).symbolic_call(x)\n-    return backend.numpy.bincount(x, weights=weights, minlength=minlength)\n+        return Bincount(\n+            weights=weights, minlength=minlength, sparse=sparse\n+        ).symbolic_call(x)\n+    return backend.numpy.bincount(\n+        x, weights=weights, minlength=minlength, sparse=sparse\n+    )\n \n \n class BroadcastTo(Operation):\n@@ -5913,9 +5925,11 @@ class Sum(Operation):\n         # TODO: torch doesn't support uint32\n         if backend.backend() == \"torch\" and dtype == \"uint32\":\n             dtype = \"int32\"\n+        sparse = getattr(x, \"sparse\", False)\n         return KerasTensor(\n             reduce_shape(x.shape, axis=self.axis, keepdims=self.keepdims),\n             dtype=dtype,\n+            sparse=sparse,\n         )\n \n \n\n@@ -2022,14 +2022,12 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase, parameterized.TestCase):\n                 )\n \n             dense_to_sparse = tf.sparse.from_dense\n-            sparse_class = tf.SparseTensor\n         elif backend.backend() == \"jax\":\n             import jax.experimental.sparse as jax_sparse\n \n             dense_to_sparse = functools.partial(\n                 jax_sparse.BCOO.fromdense, n_batch=len(x_shape) - 2\n             )\n-            sparse_class = jax_sparse.JAXSparse\n \n         rng = np.random.default_rng(0)\n \n@@ -2045,8 +2043,7 @@ class NumpyTwoInputOpsCorretnessTest(testing.TestCase, parameterized.TestCase):\n \n         atol = 0.1 if dtype == \"float16\" else 1e-4\n         self.assertAllClose(knp.matmul(x, y), np.matmul(x_np, y_np), atol=atol)\n-        if x_sparse and y_sparse:\n-            self.assertIsInstance(knp.matmul(x, y), sparse_class)\n+        self.assertSparse(knp.matmul(x, y), x_sparse and y_sparse)\n \n     def test_power(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n@@ -3082,7 +3079,15 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             np.average(x, axis=1, weights=weights_1d),\n         )\n \n-    def test_bincount(self):\n+    @parameterized.named_parameters(\n+        [\n+            {\"testcase_name\": \"dense\", \"sparse\": False},\n+            {\"testcase_name\": \"sparse\", \"sparse\": True},\n+        ]\n+    )\n+    def test_bincount(self, sparse):\n+        if sparse and not backend.SUPPORTS_SPARSE_TENSORS:\n+            pytest.skip(\"Backend does not support sparse tensors\")\n         if backend.backend() == \"tensorflow\":\n             import tensorflow as tf\n \n@@ -3092,31 +3097,42 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         x = np.array([1, 1, 2, 3, 2, 4, 4, 5])\n         weights = np.array([0, 0, 3, 2, 1, 1, 4, 2])\n         minlength = 3\n-        self.assertAllClose(\n-            knp.bincount(x, weights=weights, minlength=minlength),\n-            np.bincount(x, weights=weights, minlength=minlength),\n+        output = knp.bincount(\n+            x, weights=weights, minlength=minlength, sparse=sparse\n         )\n         self.assertAllClose(\n-            knp.Bincount(weights=weights, minlength=minlength)(x),\n-            np.bincount(x, weights=weights, minlength=minlength),\n+            output, np.bincount(x, weights=weights, minlength=minlength)\n         )\n+        self.assertSparse(output, sparse)\n+        output = knp.Bincount(\n+            weights=weights, minlength=minlength, sparse=sparse\n+        )(x)\n+        self.assertAllClose(\n+            output, np.bincount(x, weights=weights, minlength=minlength)\n+        )\n+        self.assertSparse(output, sparse)\n+\n         x = np.array([[1, 1, 2, 3, 2, 4, 4, 5]])\n         weights = np.array([[0, 0, 3, 2, 1, 1, 4, 2]])\n         expected_output = np.array([[0, 0, 4, 2, 5, 2]])\n-        self.assertAllClose(\n-            knp.bincount(x, weights=weights, minlength=minlength),\n-            expected_output,\n-        )\n-        self.assertAllClose(\n-            knp.Bincount(weights=weights, minlength=minlength)(x),\n-            expected_output,\n+        output = knp.bincount(\n+            x, weights=weights, minlength=minlength, sparse=sparse\n         )\n+        self.assertAllClose(output, expected_output)\n+        self.assertSparse(output, sparse)\n+        output = knp.Bincount(\n+            weights=weights, minlength=minlength, sparse=sparse\n+        )(x)\n+        self.assertAllClose(output, expected_output)\n+        self.assertSparse(output, sparse)\n+\n         # test with weights=None\n         expected_output = np.array([[0, 2, 2, 1, 2, 1]])\n-        self.assertAllClose(\n-            knp.Bincount(weights=None, minlength=minlength)(x),\n-            expected_output,\n+        output = knp.Bincount(weights=None, minlength=minlength, sparse=sparse)(\n+            x\n         )\n+        self.assertAllClose(output, expected_output)\n+        self.assertSparse(output, sparse)\n \n     def test_broadcast_to(self):\n         x = np.array([[1, 2, 3], [3, 2, 1]])\n@@ -3189,13 +3205,11 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n \n             x = tf.SparseTensor([[0, 0], [1, 2]], [1.0, 2.0], (2, 3))\n             y = tf.SparseTensor([[0, 0], [1, 1]], [4.0, 5.0], (2, 3))\n-            sparse_class = tf.SparseTensor\n         elif backend.backend() == \"jax\":\n             import jax.experimental.sparse as jax_sparse\n \n             x = jax_sparse.BCOO(([1.0, 2.0], [[0, 0], [1, 2]]), shape=(2, 3))\n             y = jax_sparse.BCOO(([4.0, 5.0], [[0, 0], [1, 1]]), shape=(2, 3))\n-            sparse_class = jax_sparse.JAXSparse\n \n         x_np = backend.convert_to_numpy(x)\n         y_np = backend.convert_to_numpy(y)\n@@ -3227,8 +3241,8 @@ class NumpyOneInputOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             np.concatenate([x_np, y_np], axis=axis),\n         )\n \n-        self.assertIsInstance(knp.concatenate([x, y], axis=axis), sparse_class)\n-        self.assertIsInstance(knp.Concatenate(axis=axis)([x, y]), sparse_class)\n+        self.assertSparse(knp.concatenate([x, y], axis=axis))\n+        self.assertSparse(knp.Concatenate(axis=axis)([x, y]))\n \n     def test_conjugate(self):\n         x = np.array([[1 + 2j, 2 + 3j], [3 + 4j, 4 + 5j]])\n@@ -4317,6 +4331,20 @@ class SparseTest(testing.TestCase, parameterized.TestCase):\n             (\"mean\", \"02_k\", {\"axis\": (1, 2), \"keepdims\": True}, (4, 2, 3)),\n             (\"mean\", \"all\", {\"axis\": (0, 1, 2)}, (4, 2, 3)),\n             (\"mean\", \"all_k\", {\"axis\": (0, 1, 2), \"keepdims\": True}, (4, 2, 3)),\n+            (\"sum\", \"none\", {\"axis\": None}, (4, 2, 3)),\n+            (\"sum\", \"none_k\", {\"axis\": None, \"keepdims\": True}, (4, 2, 3)),\n+            (\"sum\", \"empty\", {\"axis\": ()}, (4, 2, 3)),\n+            (\"sum\", \"empty_k\", {\"axis\": (), \"keepdims\": True}, (4, 2, 3)),\n+            (\"sum\", \"0\", {\"axis\": 0}, (4, 2, 3)),\n+            (\"sum\", \"0_k\", {\"axis\": 0, \"keepdims\": True}, (4, 2, 3)),\n+            (\"sum\", \"1\", {\"axis\": 1}, (4, 2, 3)),\n+            (\"sum\", \"1_k\", {\"axis\": 1, \"keepdims\": True}, (4, 2, 3)),\n+            (\"sum\", \"01\", {\"axis\": (0, 1)}, (4, 2, 3)),\n+            (\"sum\", \"01_k\", {\"axis\": (0, 1), \"keepdims\": True}, (4, 2, 3)),\n+            (\"sum\", \"02\", {\"axis\": (1, 2)}, (4, 2, 3)),\n+            (\"sum\", \"02_k\", {\"axis\": (1, 2), \"keepdims\": True}, (4, 2, 3)),\n+            (\"sum\", \"all\", {\"axis\": (0, 1, 2)}, (4, 2, 3)),\n+            (\"sum\", \"all_k\", {\"axis\": (0, 1, 2), \"keepdims\": True}, (4, 2, 3)),\n             (\"expand_dims\", \"zero\", {\"axis\": 0}, (2, 3)),\n             (\"expand_dims\", \"one\", {\"axis\": 1}, (2, 3)),\n             (\"expand_dims\", \"minus_two\", {\"axis\": -2}, (2, 3)),\n\n@@ -75,6 +75,29 @@ class TestCase(unittest.TestCase):\n     def assertLen(self, iterable, expected_len, msg=None):\n         self.assertEqual(len(iterable), expected_len, msg=msg)\n \n+    def assertSparse(self, x, sparse=True):\n+        if isinstance(x, KerasTensor):\n+            self.assertEqual(x.sparse, sparse)\n+        elif backend.backend() == \"tensorflow\":\n+            import tensorflow as tf\n+\n+            if sparse:\n+                self.assertIsInstance(x, tf.SparseTensor)\n+            else:\n+                self.assertNotIsInstance(x, tf.SparseTensor)\n+        elif backend.backend() == \"jax\":\n+            import jax.experimental.sparse as jax_sparse\n+\n+            if sparse:\n+                self.assertIsInstance(x, jax_sparse.JAXSparse)\n+            else:\n+                self.assertNotIsInstance(x, jax_sparse.JAXSparse)\n+        else:\n+            self.assertFalse(\n+                sparse,\n+                f\"Backend {backend.backend()} does not support sparse tensors\",\n+            )\n+\n     def run_class_serialization_test(self, instance, custom_objects=None):\n         from keras.saving import custom_object_scope\n         from keras.saving import deserialize_keras_object\n@@ -381,21 +404,7 @@ class TestCase(unittest.TestCase):\n                     )\n             if expected_output_sparse:\n                 for x in tree.flatten(output):\n-                    if isinstance(x, KerasTensor):\n-                        self.assertTrue(x.sparse)\n-                    elif backend.backend() == \"tensorflow\":\n-                        import tensorflow as tf\n-\n-                        self.assertIsInstance(x, tf.SparseTensor)\n-                    elif backend.backend() == \"jax\":\n-                        import jax.experimental.sparse as jax_sparse\n-\n-                        self.assertIsInstance(x, jax_sparse.JAXSparse)\n-                    else:\n-                        self.fail(\n-                            \"Sparse is unsupported with \"\n-                            f\"backend {backend.backend()}\"\n-                        )\n+                    self.assertSparse(x)\n             if eager:\n                 if expected_output is not None:\n                     self.assertEqual(type(expected_output), type(output))\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
