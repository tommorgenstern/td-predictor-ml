{"custom_id": "keras#de5d18b8e88227404127cef9ca9177dbb010f863", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 2 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 6 | Churn Cumulative: 662 | Contributors (this commit): 7 | Commits (past 90d): 21 | Contributors (cumulative): 13 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1324,7 +1324,10 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n \n     def add_metric(self, *args, **kwargs):\n         # Permanently disabled\n-        raise self._not_implemented_error(self.add_metric)\n+        raise NotImplementedError(\n+            \"Layer `add_metric()` method is deprecated\"\n+            \" add your metric in `Model.compile(metrics=[...]).`\"\n+        )\n \n     def count_params(self):\n         \"\"\"Count the total number of scalars composing the weights.\n\n@@ -144,7 +144,6 @@ class LayerTest(testing.TestCase, parameterized.TestCase):\n     @parameterized.named_parameters(\n         (\"call\", \"call\", None),\n         (\"compute_output_shape\", \"compute_output_shape\", None),\n-        (\"add_metric\", \"add_metric\", None),\n         (\n             \"quantized_build\",\n             \"quantized_build\",\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f6a81cc2f524bf3d2e9cb1923d101f1308e4711b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 286 | Lines Deleted: 38 | Files Changed: 18 | Hunks: 65 | Methods Changed: 43 | Complexity Δ (Sum/Max): 42/10 | Churn Δ: 324 | Churn Cumulative: 3315 | Contributors (this commit): 21 | Commits (past 90d): 76 | Contributors (cumulative): 75 | DMM Complexity: 0.7340425531914894\n\nDIFF:\n@@ -18,6 +18,7 @@ from keras.api import Quantizer\n from keras.api import Regularizer\n from keras.api import Sequential\n from keras.api import StatelessScope\n+from keras.api import SymbolicScope\n from keras.api import Variable\n from keras.api import __version__\n from keras.api import activations\n\n@@ -33,6 +33,7 @@ from keras.api import tree\n from keras.api import utils\n from keras.src.backend.common.keras_tensor import KerasTensor\n from keras.src.backend.common.stateless_scope import StatelessScope\n+from keras.src.backend.common.symbolic_scope import SymbolicScope\n from keras.src.backend.exports import Variable\n from keras.src.backend.exports import device\n from keras.src.backend.exports import name_scope\n\n@@ -31,6 +31,7 @@ from keras.api._tf_keras.keras import metrics\n from keras.api._tf_keras.keras import preprocessing\n from keras.src.backend.common.keras_tensor import KerasTensor\n from keras.src.backend.common.stateless_scope import StatelessScope\n+from keras.src.backend.common.symbolic_scope import SymbolicScope\n from keras.src.backend.exports import Variable\n from keras.src.backend.exports import device\n from keras.src.backend.exports import name_scope\n\n@@ -14,6 +14,8 @@ from keras.src.backend.common.name_scope import name_scope\n from keras.src.backend.common.stateless_scope import StatelessScope\n from keras.src.backend.common.stateless_scope import get_stateless_scope\n from keras.src.backend.common.stateless_scope import in_stateless_scope\n+from keras.src.backend.common.symbolic_scope import SymbolicScope\n+from keras.src.backend.common.symbolic_scope import in_symbolic_scope\n from keras.src.backend.common.variables import AutocastScope\n from keras.src.backend.common.variables import get_autocast_scope\n from keras.src.backend.common.variables import is_float_dtype\n\n@@ -0,0 +1,23 @@\n+from keras.src.api_export import keras_export\n+from keras.src.backend.common import global_state\n+\n+\n+@keras_export(\"keras.SymbolicScope\")\n+class SymbolicScope:\n+    \"\"\"Scope to indicate the symbolic stage.\"\"\"\n+\n+    def __enter__(self):\n+        self.original_scope = get_symbolic_scope()\n+        global_state.set_global_attribute(\"symbolic_scope\", self)\n+        return self\n+\n+    def __exit__(self, *args, **kwargs):\n+        global_state.set_global_attribute(\"symbolic_scope\", self.original_scope)\n+\n+\n+def in_symbolic_scope():\n+    return global_state.get_global_attribute(\"symbolic_scope\") is not None\n+\n+\n+def get_symbolic_scope():\n+    return global_state.get_global_attribute(\"symbolic_scope\")\n\n@@ -0,0 +1,26 @@\n+import numpy as np\n+\n+from keras.src import ops\n+from keras.src import testing\n+from keras.src.backend.common.symbolic_scope import SymbolicScope\n+from keras.src.backend.common.symbolic_scope import in_symbolic_scope\n+\n+\n+class TestSymbolicScope(testing.TestCase):\n+    def test_basic_flow(self):\n+\n+        # Define a function that behaves differently according to\n+        # `in_symbolic_scope`.\n+        def compute_loss(y, y_pred):\n+            if in_symbolic_scope():\n+                return ops.zeros_like(y)\n+            return ops.add(y, y_pred)\n+\n+        y = ops.ones(shape=(2,))\n+        y_pred = ops.ones(shape=(2,))\n+        with SymbolicScope():\n+            loss = compute_loss(y, y_pred)\n+        self.assertAllClose(loss, np.zeros((2,)))\n+\n+        loss = compute_loss(y, y_pred)\n+        self.assertAllClose(loss, 2 * np.ones((2,)))\n\n@@ -10,6 +10,7 @@ from keras.src.backend.common import global_state\n from keras.src.backend.common import standardize_dtype\n from keras.src.backend.common.keras_tensor import KerasTensor\n from keras.src.backend.common.stateless_scope import StatelessScope\n+from keras.src.backend.common.symbolic_scope import SymbolicScope\n from keras.src.backend.jax import distribution_lib\n \n SUPPORTS_SPARSE_TENSORS = True\n@@ -101,7 +102,7 @@ def cast(x, dtype):\n \n # Shape / dtype / sparseness inference util\n def compute_output_spec(fn, *args, **kwargs):\n-    with StatelessScope():\n+    with StatelessScope(), SymbolicScope():\n         built_in_types = (type(None), int, float, str, bool, complex, bytes)\n \n         # First, separate symbolic args from other args\n\n@@ -12,6 +12,7 @@ from keras.src.backend.common.backend_utils import slice_along_axis\n from keras.src.backend.common.dtypes import result_type\n from keras.src.backend.common.keras_tensor import KerasTensor\n from keras.src.backend.common.stateless_scope import StatelessScope\n+from keras.src.backend.common.symbolic_scope import SymbolicScope\n \n SUPPORTS_SPARSE_TENSORS = False\n \n@@ -88,7 +89,7 @@ def vectorized_map(function, elements):\n \n # Shape / dtype inference util\n def compute_output_spec(fn, *args, **kwargs):\n-    with StatelessScope():\n+    with StatelessScope(), SymbolicScope():\n \n         def has_none_shape(x):\n             if isinstance(x, KerasTensor):\n\n@@ -97,7 +97,10 @@ class NumpyTrainer(base_trainer.Trainer):\n             self._compile_metrics is not None\n             and not self._compile_metrics.built\n         )\n-        if model_unbuilt or compile_metrics_unbuilt:\n+        compile_loss_unbuilt = (\n+            self._compile_loss is not None and not self._compile_loss.built\n+        )\n+        if model_unbuilt or compile_metrics_unbuilt or compile_loss_unbuilt:\n             # Create symbolic tensors matching an input batch.\n \n             def to_symbolic_input(v):\n@@ -133,6 +136,15 @@ class NumpyTrainer(base_trainer.Trainer):\n                     y_pred,\n                     sample_weight=sample_weight,\n                 )\n+            if compile_loss_unbuilt:\n+                # Build `CompileLoss` state with `backend.compute_output_spec`.\n+                backend.compute_output_spec(\n+                    self._compute_loss,\n+                    x,\n+                    y,\n+                    y_pred,\n+                    sample_weight=sample_weight,\n+                )\n         self._post_build()\n \n     def fit(\n\n@@ -14,6 +14,7 @@ from keras.src.backend.common.keras_tensor import KerasTensor\n from keras.src.backend.common.name_scope import name_scope as base_name_scope\n from keras.src.backend.common.stateless_scope import StatelessScope\n from keras.src.backend.common.stateless_scope import in_stateless_scope\n+from keras.src.backend.common.symbolic_scope import SymbolicScope\n from keras.src.backend.tensorflow.sparse import sparse_to_dense\n from keras.src.utils.naming import auto_name\n \n@@ -182,7 +183,7 @@ def cast(x, dtype):\n \n \n def compute_output_spec(fn, *args, **kwargs):\n-    with StatelessScope():\n+    with StatelessScope(), SymbolicScope():\n         graph_name = auto_name(\"scratch_graph\")\n         with tf.__internal__.FuncGraph(graph_name).as_default():\n \n\n@@ -17,6 +17,7 @@ from keras.src.backend.common.keras_tensor import KerasTensor\n from keras.src.backend.common.stateless_scope import StatelessScope\n from keras.src.backend.common.stateless_scope import get_stateless_scope\n from keras.src.backend.common.stateless_scope import in_stateless_scope\n+from keras.src.backend.common.symbolic_scope import SymbolicScope\n from keras.src.backend.config import floatx\n \n SUPPORTS_SPARSE_TENSORS = False\n@@ -335,7 +336,7 @@ def compute_output_spec(fn, *args, **kwargs):\n                 )\n                 return fn(*eager_args, **eager_kwargs)\n \n-    with StatelessScope(), torch.no_grad():\n+    with StatelessScope(), SymbolicScope(), torch.no_grad():\n         outputs = symbolic_call(fn, args, kwargs, fill_value=83)\n \n         none_in_shape = any(\n\n@@ -32,6 +32,7 @@ from keras.src.api_export import keras_export\n from keras.src.backend import KerasTensor\n from keras.src.backend.common import global_state\n from keras.src.backend.common.name_scope import current_path\n+from keras.src.backend.common.symbolic_scope import in_symbolic_scope\n from keras.src.distribution import distribution_lib\n from keras.src.dtype_policies import DTypePolicyMap\n from keras.src.layers import input_spec\n@@ -1148,7 +1149,10 @@ class Layer(BackendLayer, Operation, KerasSaveable):\n         for variable in self.trainable_weights:\n             if variable.regularizer is None:\n                 continue\n-            if backend.in_stateless_scope():\n+            if backend.in_stateless_scope() and not in_symbolic_scope():\n+                # If in symbolic scope, we might get `None` from\n+                # `get_current_value` in `backend.compute_output_spec`. So we\n+                # assign `variable` instead.\n                 v = backend.get_stateless_scope().get_current_value(variable)\n             else:\n                 v = variable\n\n@@ -239,14 +239,13 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n         # Fit the model to make sure compile_metrics are built\n         hist = model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n         hist_keys = sorted(hist.history.keys())\n-        # TODO `tf.keras` also outputs individual losses for outputs\n         ref_keys = sorted(\n             [\n                 \"loss\",\n-                # \"output_a_loss\",\n+                \"output_a_loss\",\n                 \"output_a_mean_squared_error\",\n                 \"output_b_accuracy\",\n-                # \"output_b_loss\",\n+                \"output_b_loss\",\n                 \"output_b_mean_squared_error\",\n             ]\n         )\n@@ -270,16 +269,15 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n         # Fit the model to make sure compile_metrics are built\n         hist = model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n         hist_keys = sorted(hist.history.keys())\n-        # TODO `tf.keras` also outputs individual losses for outputs\n         ref_keys = sorted(\n             [\n                 \"loss\",\n-                # \"output_a_loss\",\n+                \"output_a_loss\",\n                 \"output_a_bce\",\n                 \"output_a_mae\",\n                 \"output_a_mse\",\n                 \"output_b_acc\",\n-                # \"output_b_loss\",\n+                \"output_b_loss\",\n                 \"output_b_mse\",\n             ]\n         )\n@@ -303,14 +301,13 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n         # Fit the model to make sure compile_metrics are built\n         hist = model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n         hist_keys = sorted(hist.history.keys())\n-        # TODO `tf.keras` also outputs individual losses for outputs\n         ref_keys = sorted(\n             [\n                 \"loss\",\n-                # \"output_a_loss\",\n+                \"output_a_loss\",\n                 \"output_a_mean_squared_error\",\n                 \"output_b_accuracy\",\n-                # \"output_b_loss\",\n+                \"output_b_loss\",\n                 \"output_b_mean_squared_error\",\n             ]\n         )\n@@ -351,15 +348,14 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n             verbose=0,\n         )\n         hist_keys = sorted(hist.history.keys())\n-        # TODO `tf.keras` also outputs individual losses for outputs\n         ref_keys = sorted(\n             [\n                 \"loss\",\n-                # \"output_a_loss\",\n+                \"output_a_loss\",\n                 \"output_a_mean_squared_error\",\n                 \"output_a_weighted_mean_squared_error\",\n                 \"output_b_accuracy\",\n-                # \"output_b_loss\",\n+                \"output_b_loss\",\n                 \"output_b_mean_squared_error\",\n                 \"output_b_weighted_accuracy\",\n                 \"output_b_weighted_mean_squared_error\",\n@@ -396,15 +392,14 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n         # Fit the model to make sure compile_metrics are built\n         hist = model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n         hist_keys = sorted(hist.history.keys())\n-        # TODO `tf.keras` also outputs individual losses for outputs\n         ref_keys = sorted(\n             [\n                 \"loss\",\n-                # \"output_a_loss\",\n+                \"output_a_loss\",\n                 \"output_a_mean_squared_error\",\n                 \"output_a_weighted_mean_squared_error\",\n                 \"output_b_accuracy\",\n-                # \"output_b_loss\",\n+                \"output_b_loss\",\n                 \"output_b_mean_squared_error\",\n                 \"output_b_weighted_accuracy\",\n                 \"output_b_weighted_mean_squared_error\",\n@@ -436,18 +431,17 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n         # Fit the model to make sure compile_metrics are built\n         hist = model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n         hist_keys = sorted(hist.history.keys())\n-        # TODO `tf.keras` also outputs individual losses for outputs\n         # `output_b_accuracy` doesn't have `weighted_` in metric name.\n         # When a metric is only in weighted metrics, it skips `weighted_`\n         # prefix. This behavior matches`tf.keras`.\n         ref_keys = sorted(\n             [\n                 \"loss\",\n-                # \"output_a_loss\",\n+                \"output_a_loss\",\n                 \"output_a_mean_squared_error\",\n                 \"output_a_weighted_mean_squared_error\",\n                 \"output_b_accuracy\",\n-                # \"output_b_loss\",\n+                \"output_b_loss\",\n                 \"output_b_mean_squared_error\",\n             ]\n         )\n@@ -472,13 +466,12 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n         # Fit the model to make sure compile_metrics are built\n         hist = model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n         hist_keys = sorted(hist.history.keys())\n-        # TODO `tf.keras` also outputs individual losses for outputs\n         ref_keys = sorted(\n             [\n                 \"loss\",\n-                # \"output_a_loss\",\n+                \"output_a_loss\",\n                 \"output_b_accuracy\",\n-                # \"output_b_loss\",\n+                \"output_b_loss\",\n                 \"output_b_mean_squared_error\",\n             ]\n         )\n@@ -500,7 +493,10 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n                 \"output_b\": \"binary_crossentropy\",\n             },\n         )\n-        model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n+        hist = model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n+        hist_keys = sorted(hist.history.keys())\n+        ref_keys = sorted([\"loss\", \"output_a_loss\", \"output_b_loss\"])\n+        self.assertListEqual(hist_keys, ref_keys)\n \n     def test_functional_list_outputs_with_custom_compute_loss(self):\n         model = _get_model_with_custom_compute_loss()\n@@ -514,7 +510,12 @@ class ModelTest(testing.TestCase, parameterized.TestCase):\n         model.compile(\n             optimizer=\"sgd\", loss=[\"mean_squared_error\", \"binary_crossentropy\"]\n         )\n-        model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n+        hist = model.fit(x, (y1, y2), batch_size=2, epochs=1, verbose=0)\n+        hist_keys = sorted(hist.history.keys())\n+        ref_keys = sorted(\n+            [\"binary_crossentropy_loss\", \"loss\", \"mean_squared_error_loss\"]\n+        )\n+        self.assertListEqual(hist_keys, ref_keys)\n \n     def test_functional_list_outputs_dict_losses_invalid_keys(self):\n         model = _get_model_multi_outputs_list()\n\n@@ -3,6 +3,7 @@ from keras.src import metrics as metrics_module\n from keras.src import ops\n from keras.src import tree\n from keras.src.utils.naming import get_object_name\n+from keras.src.utils.tracking import Tracker\n \n \n class MetricsList(metrics_module.Metric):\n@@ -431,6 +432,28 @@ class CompileLoss(losses_module.Loss):\n         # Inferred by `y_pred` and `output_names`\n         self.inferred_output_names = None\n \n+        # Use `Tracker` to track metrcis for individual losses.\n+        self._metrics = []\n+        self._tracker = Tracker(\n+            {\n+                \"metrics\": (\n+                    lambda x: isinstance(x, metrics_module.Metric),\n+                    self._metrics,\n+                )\n+            }\n+        )\n+\n+    @property\n+    def metrics(self):\n+        return self._metrics\n+\n+    @property\n+    def variables(self):\n+        vars = []\n+        for m in self.metrics:\n+            vars.extend(m.variables)\n+        return vars\n+\n     def build(self, y_true, y_pred):\n         loss = self._user_loss\n         loss_weights = self._user_loss_weights\n@@ -527,6 +550,21 @@ class CompileLoss(losses_module.Loss):\n             for identifier, _y_true, _y_pred in zip(flat_losses, y_true, y_pred)\n         ]\n \n+        # Add `Mean` metric to the tracker for each loss.\n+        if len(flat_losses) > 1:\n+            for i, _loss in enumerate(flat_losses):\n+                if _loss is not None:\n+                    if inferred_output_names is not None and len(\n+                        inferred_output_names\n+                    ) == len(flat_losses):\n+                        name = inferred_output_names[i]\n+                    else:\n+                        name = _loss.name\n+                    name += \"_loss\"\n+                    self._tracker.add_to_store(\n+                        \"metrics\", metrics_module.Mean(name=name)\n+                    )\n+\n         self.flat_losses = flat_losses\n         self.flat_loss_weights = flat_loss_weights\n         self.filtered_y_true_keys = filtered_y_true_keys\n@@ -596,22 +634,31 @@ class CompileLoss(losses_module.Loss):\n         else:\n             sample_weight = [None for _ in y_true]\n \n+        # We need to add a dummy `None` if the model has only a single output.\n+        metrics = [None] if len(self.metrics) == 0 else self.metrics\n+\n         # Iterate all losses in flat form.\n         loss_values = []\n-        for loss, y_t, y_p, loss_weight, sample_weight in zip(\n+        for loss_fn, y_t, y_p, loss_weight, sample_weight, metric in zip(\n             self.flat_losses,\n             y_true,\n             y_pred,\n             self.flat_loss_weights,\n             sample_weight,\n+            metrics,\n         ):\n-            if loss:\n+            if loss_fn:\n                 value = ops.cast(\n-                    loss(y_t, y_p, sample_weight), dtype=self.dtype\n+                    loss_fn(y_t, y_p, sample_weight), dtype=self.dtype\n                 )\n                 if loss_weight is not None:\n                     value = ops.multiply(value, loss_weight)\n                 loss_values.append(value)\n+                # Record individual losses.\n+                if metric:\n+                    metric.update_state(\n+                        value, sample_weight=tree.flatten(y_p)[0].shape[0]\n+                    )\n         if loss_values:\n             total_loss = sum(loss_values)\n             return total_loss\n\n@@ -250,6 +250,8 @@ class Trainer:\n         metrics.extend(super().metrics)\n         if self.compiled and self._compile_metrics is not None:\n             metrics += [self._compile_metrics]\n+        if self.compiled and self._compile_loss is not None:\n+            metrics.extend(self._compile_loss.metrics)\n         return metrics\n \n     @property\n@@ -1004,10 +1006,13 @@ class Trainer:\n             self._compile_metrics is not None\n             and not self._compile_metrics.built\n         )\n+        compile_loss_unbuilt = (\n+            self._compile_loss is not None and not self._compile_loss.built\n+        )\n         optimizer_unbuilt = (\n             self.optimizer is not None and not self.optimizer.built\n         )\n-        if model_unbuilt or compile_metrics_unbuilt:\n+        if model_unbuilt or compile_metrics_unbuilt or compile_loss_unbuilt:\n             # Create symbolic tensors matching an input batch.\n \n             def to_symbolic_input(v):\n@@ -1030,7 +1035,7 @@ class Trainer:\n \n             # Build all model state with `backend.compute_output_spec`.\n             try:\n-                y_pred = backend.compute_output_spec(self, x)\n+                y_pred = backend.compute_output_spec(self, x, training=False)\n             except Exception as e:\n                 raise RuntimeError(\n                     \"Unable to automatically build the model. \"\n@@ -1052,6 +1057,16 @@ class Trainer:\n                     y_pred,\n                     sample_weight=sample_weight,\n                 )\n+            if compile_loss_unbuilt:\n+                # Build `CompileLoss` state with `backend.compute_output_spec`.\n+                backend.compute_output_spec(\n+                    self._compute_loss,\n+                    x,\n+                    y,\n+                    y_pred,\n+                    sample_weight=sample_weight,\n+                    training=False,\n+                )\n         if optimizer_unbuilt:\n             # Build optimizer\n             self.optimizer.build(self.trainable_variables)\n\n@@ -14,6 +14,7 @@ from keras.src import models\n from keras.src import ops\n from keras.src import optimizers\n from keras.src import testing\n+from keras.src.backend.common.symbolic_scope import in_symbolic_scope\n from keras.src.callbacks.callback import Callback\n from keras.src.optimizers.rmsprop import RMSprop\n from keras.src.testing.test_utils import named_product\n@@ -1406,6 +1407,7 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n                 sample_weight=None,\n                 training=True,\n             ):\n+                if not in_symbolic_scope():\n                     test_self.assertTrue(training)\n                 loss = super().compute_loss(\n                     x, y, y_pred, sample_weight, training\n@@ -1443,6 +1445,7 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n                 sample_weight=None,\n                 training=True,\n             ):\n+                if not in_symbolic_scope():\n                     test_self.assertTrue(training)\n                 loss = super().compute_loss(\n                     x, y, y_pred, sample_weight, training\n@@ -1478,6 +1481,7 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n                 sample_weight=None,\n                 training=True,\n             ):\n+                if not in_symbolic_scope():\n                     test_self.assertFalse(training)\n                 loss = super().compute_loss(\n                     x, y, y_pred, sample_weight, training\n@@ -1613,6 +1617,65 @@ class TestTrainer(testing.TestCase, parameterized.TestCase):\n             atol=1e-3,\n         )\n \n+    def test_symbolic_build(self):\n+        class ExampleModelWithTrainingArgs(Trainer, layers.Layer):\n+            def __init__(self, units):\n+                layers.Layer.__init__(self)\n+                Trainer.__init__(self)\n+                self.dense = layers.Dense(units)\n+                self.bn = layers.BatchNormalization(axis=-1)\n+\n+            def build(self, input_shape):\n+                self.dense.build(input_shape)\n+                input_shape = self.dense.compute_output_shape(input_shape)\n+                self.bn.build(input_shape)\n+\n+            def call(self, x, training=None):\n+                outputs = self.bn(self.dense(x), training=training)\n+                return [outputs, outputs]\n+\n+        model = ExampleModelWithTrainingArgs(units=3)\n+        model.compile(\n+            optimizer=optimizers.SGD(),\n+            loss=[losses.MeanSquaredError(), losses.MeanSquaredError()],\n+            metrics=[metrics.MeanSquaredError(), metrics.MeanSquaredError()],\n+        )\n+        x = np.ones((4, 4))\n+        y = np.zeros((4, 3))\n+        model(x)  # Eager call to build model weights\n+        ref_weights = model.get_weights()\n+\n+        # Before `_symbolic_build`\n+        self.assertTrue(model.built)\n+        self.assertFalse(model._compile_metrics.built)\n+        self.assertFalse(model._compile_loss.built)\n+        self.assertLen(model._compile_loss.metrics, 0)\n+        self.assertLen(model.metrics, 2)\n+\n+        model._symbolic_build(data_batch=(x, (y, y)))\n+        weights = model.get_weights()\n+\n+        # Ensure weights are intact\n+        self.assertEqual(len(weights), len(ref_weights))\n+        for w, ref_w in zip(weights, ref_weights):\n+            self.assertAllClose(w, ref_w)\n+\n+        # Ensure `built`\n+        self.assertTrue(model.built)\n+        self.assertTrue(model._compile_metrics.built)\n+        self.assertTrue(model._compile_loss.built)\n+\n+        # Ensure the len of metrics (original metrics + loss trackers)\n+        self.assertLen(model._compile_metrics.metrics, 2)\n+        self.assertLen(model._compile_loss.metrics, 2)\n+        self.assertLen(model.metrics, 4)\n+\n+        # Ensure no values in metrics\n+        for v in model._compile_metrics.variables:\n+            self.assertAllClose(v, 0.0)\n+        for v in model._compile_loss.variables:\n+            self.assertAllClose(v, 0.0)\n+\n \n class TrainerDistributeTest(testing.TestCase):\n     @pytest.mark.skipif(\n\n@@ -112,7 +112,11 @@ class TorchModuleWrapper(Layer):\n             self._track_variable(variable)\n         self.built = True\n \n-    def call(self, *args, **kwargs):\n+    def call(self, *args, training=None, **kwargs):\n+        if training is False:\n+            self.eval()\n+        else:\n+            self.train()\n         return self.module(*args, **kwargs)\n \n     def save_own_variables(self, store):\n\n@@ -29,9 +29,9 @@ class Classifier(models.Model):\n             self.torch_wrappers.append(TorchModuleWrapper(torch_model))\n         self.fc = layers.Dense(1)\n \n-    def call(self, x):\n+    def call(self, x, training=None):\n         for wrapper in self.torch_wrappers:\n-            x = wrapper(x)\n+            x = wrapper(x, training=training)\n         return self.fc(x)\n \n     def get_config(self):\n@@ -49,7 +49,7 @@ class ClassifierWithNoSpecialCasing(models.Model):\n         self.fc2 = torch.nn.Linear(4, 4)\n         self.fc3 = layers.Dense(2)\n \n-    def call(self, x):\n+    def call(self, x, training=None):\n         return self.fc3(self.fc2(self.bn1(self.fc1(x))))\n \n \n@@ -82,6 +82,50 @@ class TorchUtilsTest(testing.TestCase, parameterized.TestCase):\n         model.compile(optimizer=\"sgd\", loss=\"mse\")\n         model.fit(np.random.random((3, 2)), np.random.random((3, 1)))\n \n+    @parameterized.named_parameters(\n+        (\n+            \"explicit_torch_wrapper\",\n+            Classifier,\n+            {\"use_batch_norm\": True, \"num_torch_layers\": 1},\n+        ),\n+        (\"implicit_torch_wrapper\", ClassifierWithNoSpecialCasing, {}),\n+    )\n+    def test_training_args(self, cls, kwargs):\n+        model = cls(**kwargs)\n+        model(np.random.random((3, 2)), training=False)  # Eager call to build\n+        ref_weights = model.get_weights()\n+        ref_running_mean = backend.convert_to_numpy(\n+            model.torch_wrappers[0].module[-1].running_mean\n+            if cls is Classifier\n+            else model.bn1.module.running_mean\n+        )\n+\n+        # Test training=False doesn't affect model weights\n+        model(np.random.random((3, 2)), training=False)\n+        weights = model.get_weights()\n+        for w, ref_w in zip(weights, ref_weights):\n+            self.assertAllClose(w, ref_w)\n+\n+        # Test training=None affects BN's stats\n+        model.set_weights(ref_weights)  # Restore previous weights\n+        model(np.random.random((3, 2)))\n+        running_mean = backend.convert_to_numpy(\n+            model.torch_wrappers[0].module[-1].running_mean\n+            if cls is Classifier\n+            else model.bn1.module.running_mean\n+        )\n+        self.assertNotAllClose(running_mean, ref_running_mean)\n+\n+        # Test training=True affects BN's stats\n+        model.set_weights(ref_weights)  # Restore previous weights\n+        model(np.random.random((3, 2)), training=True)\n+        running_mean = backend.convert_to_numpy(\n+            model.torch_wrappers[0].module[-1].running_mean\n+            if cls is Classifier\n+            else model.bn1.module.running_mean\n+        )\n+        self.assertNotAllClose(running_mean, ref_running_mean)\n+\n     def test_module_autowrapping(self):\n         model = ClassifierWithNoSpecialCasing()\n         self.assertIsInstance(model.fc1, TorchModuleWrapper)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7c35b3306d7bf77a9bc43e436c29153c1eabe9bd", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 21 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 6 | Methods Changed: 6 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 23 | Churn Cumulative: 582 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -237,12 +237,13 @@ class KerasVariable:\n             scope.add_update((self, value))\n         else:\n             self._direct_assign(value)\n+        return value\n \n     def assign_add(self, value):\n-        self.assign(self + value)\n+        return self.assign(self + value)\n \n     def assign_sub(self, value):\n-        self.assign(self - value)\n+        return self.assign(self - value)\n \n     @property\n     def dtype(self):\n\n@@ -297,18 +297,36 @@ class VariableNumpyValueAndAssignmentTest(test_case.TestCase):\n         v.assign(np.array([4, 5, 6]))\n         self.assertAllClose(v.value, np.array([4, 5, 6]))\n \n+    def test_variable_assign_return(self):\n+        \"\"\"Test assigning a new value and returning.\"\"\"\n+        v = backend.Variable(initializer=np.array([1, 2, 3]))\n+        r = v.assign(np.array([4, 5, 6]))\n+        self.assertAllClose(r, np.array([4, 5, 6]))\n+\n     def test_variable_assign_add(self):\n         \"\"\"Test the assign_add method on a variable.\"\"\"\n         v = backend.Variable(initializer=np.array([1, 2, 3]))\n         v.assign_add(np.array([1, 1, 1]))\n         self.assertAllClose(v.value, np.array([2, 3, 4]))\n \n+    def test_variable_assign_add_return(self):\n+        \"\"\"Test assign_add a new value and returning.\"\"\"\n+        v = backend.Variable(initializer=np.array([1, 2, 3]))\n+        r = v.assign_add(np.array([1, 1, 1]))\n+        self.assertAllClose(r, np.array([2, 3, 4]))\n+\n     def test_variable_assign_sub(self):\n         \"\"\"Test the assign_sub method on a variable.\"\"\"\n         v = backend.Variable(initializer=np.array([2, 3, 4]))\n         v.assign_sub(np.array([1, 1, 1]))\n         self.assertAllClose(v.value, np.array([1, 2, 3]))\n \n+    def test_variable_assign_sub_return(self):\n+        \"\"\"Test assign_sub a new value and returning.\"\"\"\n+        v = backend.Variable(initializer=np.array([2, 3, 4]))\n+        r = v.assign_sub(np.array([1, 1, 1]))\n+        self.assertAllClose(r, np.array([1, 2, 3]))\n+\n     def test_deferred_initialize_within_stateless_scope(self):\n         \"\"\"Test deferred init within a stateless scope.\"\"\"\n         with backend.StatelessScope():\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#c9bd4a8c80a97596e54a9bba8f2c8a449716ab04", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 58 | Lines Deleted: 14 | Files Changed: 2 | Hunks: 17 | Methods Changed: 11 | Complexity Δ (Sum/Max): 4/2 | Churn Δ: 72 | Churn Cumulative: 89 | Contributors (this commit): 3 | Commits (past 90d): 3 | Contributors (cumulative): 5 | DMM Complexity: 1.0\n\nDIFF:\n@@ -1,3 +1,4 @@\n+from keras.src import backend\n from keras.src import ops\n from keras.src.api_export import keras_export\n from keras.src.layers.input_spec import InputSpec\n@@ -39,16 +40,34 @@ class ZeroPadding1D(Layer):\n               the padding dimension (axis 1).\n             - If tuple of 2 ints: how many zeros to add at the beginning and the\n               end of the padding dimension (`(left_pad, right_pad)`).\n+        data_format: A string, one of `\"channels_last\"` (default) or\n+            `\"channels_first\"`. The ordering of the dimensions in the inputs.\n+            `\"channels_last\"` corresponds to inputs with shape\n+            `(batch_size, axis_to_pad, channels)` while `\"channels_first\"`\n+            corresponds to inputs with shape\n+            `(batch_size, channels, axis_to_pad)`.\n+            When unspecified, uses `image_data_format` value found in your Keras\n+            config file at `~/.keras/keras.json` (if exists). Defaults to\n+            `\"channels_last\"`.\n \n     Input shape:\n-        3D tensor with shape `(batch_size, axis_to_pad, features)`\n+        3D tensor with shape:\n+        - If `data_format` is `\"channels_last\"`:\n+          `(batch_size, axis_to_pad, features)`\n+        - If `data_format` is `\"channels_first\"`:\n+          `(batch_size, features, axis_to_pad)`\n \n     Output shape:\n-        3D tensor with shape `(batch_size, padded_axis, features)`\n+        3D tensor with shape:\n+        - If `data_format` is `\"channels_last\"`:\n+          `(batch_size, padded_axis, features)`\n+        - If `data_format` is `\"channels_first\"`:\n+          `(batch_size, features, padded_axis)`\n     \"\"\"\n \n-    def __init__(self, padding=1, **kwargs):\n+    def __init__(self, padding=1, data_format=None, **kwargs):\n         super().__init__(**kwargs)\n+        self.data_format = backend.standardize_data_format(data_format)\n         self.padding = argument_validation.standardize_tuple(\n             padding, 2, \"padding\", allow_zero=True\n         )\n@@ -56,15 +75,19 @@ class ZeroPadding1D(Layer):\n \n     def compute_output_shape(self, input_shape):\n         output_shape = list(input_shape)\n-        if output_shape[1] is not None:\n-            output_shape[1] += self.padding[0] + self.padding[1]\n+        padding_dim = 2 if self.data_format == \"channels_first\" else 1\n+        if output_shape[padding_dim] is not None:\n+            output_shape[padding_dim] += self.padding[0] + self.padding[1]\n         return tuple(output_shape)\n \n     def call(self, inputs):\n+        if self.data_format == \"channels_first\":\n+            all_dims_padding = ((0, 0), (0, 0), self.padding)\n+        else:\n             all_dims_padding = ((0, 0), self.padding, (0, 0))\n         return ops.pad(inputs, all_dims_padding)\n \n     def get_config(self):\n-        config = {\"padding\": self.padding}\n+        config = {\"padding\": self.padding, \"data_format\": self.data_format}\n         base_config = super().get_config()\n         return {**base_config, **config}\n\n@@ -7,18 +7,30 @@ from keras.src import testing\n \n \n class ZeroPadding1DTest(testing.TestCase, parameterized.TestCase):\n-    def test_zero_padding_1d(self):\n+    @parameterized.parameters(\n+        {\"data_format\": \"channels_first\"},\n+        {\"data_format\": \"channels_last\"},\n+    )\n+    def test_zero_padding_1d(self, data_format):\n         inputs = np.random.rand(1, 2, 3)\n-        outputs = layers.ZeroPadding1D(padding=(1, 2))(inputs)\n-\n+        outputs = layers.ZeroPadding1D(padding=(1, 2), data_format=data_format)(\n+            inputs\n+        )\n+        if data_format == \"channels_last\":\n             for index in [0, -1, -2]:\n                 self.assertAllClose(outputs[:, index, :], 0.0)\n             self.assertAllClose(outputs[:, 1:-2, :], inputs)\n+        else:\n+            for index in [0, -1, -2]:\n+                self.assertAllClose(outputs[:, :, index], 0.0)\n+            self.assertAllClose(outputs[:, :, 1:-2], inputs)\n \n     @parameterized.named_parameters((\"one_tuple\", (2, 2)), (\"one_int\", 2))\n     def test_zero_padding_1d_with_same_padding(self, padding):\n         inputs = np.random.rand(1, 2, 3)\n-        outputs = layers.ZeroPadding1D(padding=padding)(inputs)\n+        outputs = layers.ZeroPadding1D(\n+            padding=padding, data_format=\"channels_last\"\n+        )(inputs)\n \n         for index in [0, 1, -1, -2]:\n             self.assertAllClose(outputs[:, index, :], 0.0)\n@@ -26,11 +38,15 @@ class ZeroPadding1DTest(testing.TestCase, parameterized.TestCase):\n \n     def test_zero_padding_1d_with_dynamic_spatial_dim(self):\n         input_layer = layers.Input(batch_shape=(1, None, 3))\n-        padded = layers.ZeroPadding1D((1, 2))(input_layer)\n+        padded = layers.ZeroPadding1D((1, 2), data_format=\"channels_last\")(\n+            input_layer\n+        )\n         self.assertEqual(padded.shape, (1, None, 3))\n \n         input_layer = layers.Input(batch_shape=(1, 2, 3))\n-        padded = layers.ZeroPadding1D((1, 2))(input_layer)\n+        padded = layers.ZeroPadding1D((1, 2), data_format=\"channels_last\")(\n+            input_layer\n+        )\n         self.assertEqual(padded.shape, (1, 5, 3))\n \n     @parameterized.parameters(\n@@ -42,10 +58,15 @@ class ZeroPadding1DTest(testing.TestCase, parameterized.TestCase):\n         with self.assertRaises(ValueError):\n             layers.ZeroPadding1D(padding)\n \n-    def test_zero_padding_1d_get_config(self):\n-        layer = layers.ZeroPadding1D(padding=(1, 2))\n+    @parameterized.parameters(\n+        {\"data_format\": \"channels_first\"},\n+        {\"data_format\": \"channels_last\"},\n+    )\n+    def test_zero_padding_1d_get_config(self, data_format):\n+        layer = layers.ZeroPadding1D(padding=(1, 2), data_format=data_format)\n         expected_config = {\n             \"dtype\": dtype_policies.serialize(layer.dtype_policy),\n+            \"data_format\": data_format,\n             \"name\": layer.name,\n             \"padding\": (1, 2),\n             \"trainable\": layer.trainable,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#08e739479f2990eab2aeb145d7d9c7e79ef819ad", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 2 | Complexity Δ (Sum/Max): -1/0 | Churn Δ: 2 | Churn Cumulative: 271 | Contributors (this commit): 8 | Commits (past 90d): 11 | Contributors (cumulative): 8 | DMM Complexity: 1.0\n\nDIFF:\n@@ -240,7 +240,7 @@ def convert_to_numpy(x):\n             if x.requires_grad:\n                 x = x.detach()\n             # Tensor has to be moved to CPU before converting to numpy.\n-            if x.is_cuda or x.is_mps:\n+            if x.device != torch.device(\"cpu\"):\n                 x = x.cpu()\n             if x.dtype == torch.bfloat16:\n                 # Attempting to call .numpy() on a bfloat16 torch tensor leads\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#7d92e9eea354da51e7c2a3edd679839ca0315a02", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 6 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -64,7 +64,7 @@ class Callback:\n     \"\"\"\n \n     def __init__(self):\n-        self.validation_data = None\n+        self.params = None\n         self._model = None\n \n     def set_params(self, params):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#a226835ed0bfaedd454f40f5e9ecd16f2d6bd424", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 7 | Lines Deleted: 7 | Files Changed: 3 | Hunks: 7 | Methods Changed: 3 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 14 | Churn Cumulative: 36 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: None\n\nDIFF:\n@@ -157,20 +157,20 @@ class ModelCheckpoint(Callback):\n         if mode == \"min\":\n             self.monitor_op = np.less\n             if self.best is None:\n-                self.best = np.Inf\n+                self.best = np.inf\n         elif mode == \"max\":\n             self.monitor_op = np.greater\n             if self.best is None:\n-                self.best = -np.Inf\n+                self.best = -np.inf\n         else:\n             if \"acc\" in self.monitor or self.monitor.startswith(\"fmeasure\"):\n                 self.monitor_op = np.greater\n                 if self.best is None:\n-                    self.best = -np.Inf\n+                    self.best = -np.inf\n             else:\n                 self.monitor_op = np.less\n                 if self.best is None:\n-                    self.best = np.Inf\n+                    self.best = np.inf\n \n         if self.save_freq != \"epoch\" and not isinstance(self.save_freq, int):\n             raise ValueError(\n\n@@ -92,10 +92,10 @@ class ReduceLROnPlateau(Callback):\n             self.mode == \"auto\" and \"acc\" not in self.monitor\n         ):\n             self.monitor_op = lambda a, b: np.less(a, b - self.min_delta)\n-            self.best = np.Inf\n+            self.best = np.inf\n         else:\n             self.monitor_op = lambda a, b: np.greater(a, b + self.min_delta)\n-            self.best = -np.Inf\n+            self.best = -np.inf\n         self.cooldown_counter = 0\n         self.wait = 0\n \n\n@@ -101,7 +101,7 @@ def pad_sequences(\n         maxlen = np.max(lengths)\n \n     is_dtype_str = np.issubdtype(dtype, np.str_) or np.issubdtype(\n-        dtype, np.unicode_\n+        dtype, np.str_\n     )\n     if isinstance(value, str) and dtype != object and not is_dtype_str:\n         raise ValueError(\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "keras#f0e79cfa808e9de7981529efe3712cc868036ee4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 91 | Lines Deleted: 34 | Files Changed: 5 | Hunks: 27 | Methods Changed: 16 | Complexity Δ (Sum/Max): 10/5 | Churn Δ: 125 | Churn Cumulative: 1299 | Contributors (this commit): 13 | Commits (past 90d): 29 | Contributors (cumulative): 30 | DMM Complexity: 0.9473684210526315\n\nDIFF:\n@@ -116,18 +116,18 @@ class TFOptimizer(KerasAutoTrackable, base_optimizer.BaseOptimizer):\n             v.value if isinstance(v, backend.Variable) else v\n             for v in trainable_variables\n         ]\n+        grads_and_vars = list(zip(grads, trainable_variables))\n+        grads_and_vars = self._all_reduce_sum_gradients(grads_and_vars)\n         tf.__internal__.distribute.interim.maybe_merge_call(\n             self._distributed_tf_update_step,\n             self._distribution_strategy,\n-            list(zip(grads, trainable_variables)),\n+            grads_and_vars,\n             learning_rate,\n         )\n \n     def _distributed_tf_update_step(\n         self, distribution, grads_and_vars, learning_rate\n     ):\n-        grads_and_vars = self._all_reduce_sum_gradients(grads_and_vars)\n-\n         def apply_grad_to_update_var(var, grad, learning_rate):\n             return self.update_step(grad, var, learning_rate)\n \n\n@@ -3,6 +3,7 @@\n import numpy as np\n import pytest\n import tensorflow as tf\n+from absl.testing import parameterized\n from tensorflow.python.eager import context\n \n from keras.src import backend\n@@ -14,7 +15,7 @@ from keras.src.optimizers.sgd import SGD\n     backend.backend() != \"tensorflow\",\n     reason=\"The distribute test can only run with TF backend.\",\n )\n-class OptimizerDistributeTest(testing.TestCase):\n+class OptimizerDistributeTest(testing.TestCase, parameterized.TestCase):\n     def setUp(self):\n         super().setUp()\n         # Need at least 2 devices for distribution related tests.\n@@ -39,20 +40,32 @@ class OptimizerDistributeTest(testing.TestCase):\n             )\n         self.run_class_serialization_test(optimizer)\n \n-    def test_single_step(self):\n+    @parameterized.parameters([(\"keras_sgd\",), (\"tf_keras_sgd\",)])\n+    def test_single_step(self, optimizer_type):\n+        if optimizer_type == \"tf_keras_sgd\":\n+            try:\n+                import tf_keras\n+\n+                optimizer_fn = tf_keras.optimizers.SGD\n+            except (ImportError, AttributeError):\n+                self.skipTest(\"tf_keras not installed\")\n+        else:\n+            optimizer_fn = SGD\n         with self.strategy.scope():\n-            optimizer = SGD(\n+            optimizer = optimizer_fn(\n                 learning_rate=0.5,\n                 momentum=0.06,\n             )\n-            grads = tf.constant([1.0, 6.0, 7.0, 2.0])\n-            vars = backend.Variable([1.0, 2.0, 3.0, 4.0])\n+            # use tf variable to work both in k2 & k3.\n+            vars = tf.Variable([1.0, 2.0, 3.0, 4.0])\n \n-            self.strategy.run(\n-                lambda: optimizer.apply_gradients(zip([grads], [vars]))\n-            )\n+            def update():\n+                grads = tf.constant([1.0, 6.0, 7.0, 2.0])\n+                optimizer.apply_gradients(zip([grads], [vars]))\n+\n+            self.strategy.run(update)\n             self.assertAllClose(\n-                vars, [0.5, -1.0, -0.5, 3.0], rtol=1e-4, atol=1e-4\n+                vars, [0.0, -4.0, -4.0, 2.0], rtol=1e-4, atol=1e-4\n             )\n \n     def test_weight_decay(self):\n@@ -91,31 +104,32 @@ class OptimizerDistributeTest(testing.TestCase):\n     def test_correctness_with_golden(self):\n         with self.strategy.scope():\n             optimizer = SGD(nesterov=True)\n-\n             x = backend.Variable(np.ones([10]))\n-            grads = np.arange(0.1, 1.1, 0.1)\n-            first_grads = np.full((10,), 0.01)\n+\n+            def update_grads():\n+                grads = backend.convert_to_tensor(np.arange(0.1, 1.1, 0.1))\n+                optimizer.apply_gradients(zip([grads], [x]))\n+\n+            def update_first_grads():\n+                first_grads = backend.convert_to_tensor(np.full((10,), 0.01))\n+                optimizer.apply_gradients(zip([first_grads], [x]))\n \n         # fmt: off\n         golden = np.array(\n-            [[0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999, 0.9999,\n-            0.9999, 0.9999], [0.9989, 0.9979, 0.9969, 0.9959, 0.9949, 0.9939,\n-            0.9929, 0.9919, 0.9909, 0.9899], [0.9979, 0.9959, 0.9939, 0.9919,\n-            0.9899, 0.9879, 0.9859, 0.9839, 0.9819, 0.9799], [0.9969, 0.9939,\n-            0.9909, 0.9879, 0.9849, 0.9819, 0.9789, 0.9759, 0.9729, 0.9699],\n-            [0.9959, 0.9919, 0.9879, 0.9839, 0.9799, 0.9759, 0.9719, 0.9679,\n-            0.9639, 0.9599]]\n+            [\n+                [0.9980, 0.9960, 0.9940, 0.9920, 0.9900, 0.9880, 0.9860, 0.9840, 0.9820, 0.9800],\n+                [0.9978, 0.9958, 0.9938, 0.9918, 0.9898, 0.9878, 0.9858, 0.9838, 0.9818, 0.9798],\n+                [0.9976, 0.9956, 0.9936, 0.9916, 0.9896, 0.9876, 0.9856, 0.9836, 0.9816, 0.9796],\n+                [0.9974, 0.9954, 0.9934, 0.9914, 0.9894, 0.9874, 0.9854, 0.9834, 0.9814, 0.9794],\n+                [0.9972, 0.9952, 0.9932, 0.9912, 0.9892, 0.9872, 0.9852, 0.9832, 0.9812, 0.9792],\n+            ]\n         )\n         # fmt: on\n \n-        self.strategy.run(\n-            lambda: optimizer.apply_gradients(zip([first_grads], [x]))\n-        )\n+        self.strategy.run(update_grads)\n         for i in range(5):\n             self.assertAllClose(x, golden[i], rtol=5e-4, atol=5e-4)\n-            self.strategy.run(\n-                lambda: optimizer.apply_gradients(zip([grads], [x]))\n-            )\n+            self.strategy.run(update_first_grads)\n \n     def test_clip_norm(self):\n         with self.strategy.scope():\n@@ -190,7 +204,7 @@ class OptimizerDistributeTest(testing.TestCase):\n             self.assertAllClose(optimizer._iterations, 2)\n             self.assertAllClose(optimizer.iterations, 0)\n             self.strategy.run(lambda: optimizer.apply_gradients([(grads, v)]))\n-            self.assertAllClose(v, [[0.0, 1.0], [1.0, 2.0]])\n+            self.assertAllClose(v, [[-1.0, 0.0], [-1.0, 0.0]])\n             self.assertAllClose(\n                 optimizer._accumulated_gradients[0], [[0.0, 0.0], [0.0, 0.0]]\n             )\n\n@@ -291,6 +291,8 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             steps_per_execution=self.steps_per_execution,\n         )\n \n+        self._maybe_symbolic_build(iterator=epoch_iterator)\n+\n         # Container that configures and calls callbacks.\n         if not isinstance(callbacks, callbacks_module.CallbackList):\n             callbacks = callbacks_module.CallbackList(\n@@ -406,6 +408,8 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 steps_per_execution=self.steps_per_execution,\n             )\n \n+        self._maybe_symbolic_build(iterator=epoch_iterator)\n+\n         # Container that configures and calls callbacks.\n         if not isinstance(callbacks, callbacks_module.CallbackList):\n             callbacks = callbacks_module.CallbackList(\n@@ -525,7 +529,6 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         return_dict=False,\n     ):\n         self._assert_compile_called(\"train_on_batch\")\n-        self.make_train_function()\n         if class_weight is not None:\n             if sample_weight is not None:\n                 raise ValueError(\n@@ -538,6 +541,10 @@ class TensorFlowTrainer(base_trainer.Trainer):\n                 y, class_weight\n             )\n \n+        # Maybe build model\n+        self._maybe_symbolic_build(data_batch=(x, y, sample_weight))\n+        self.make_train_function()\n+\n         def data():\n             yield (x, y, sample_weight)\n \n@@ -555,11 +562,14 @@ class TensorFlowTrainer(base_trainer.Trainer):\n         return_dict=False,\n     ):\n         self._assert_compile_called(\"test_on_batch\")\n-        self.make_test_function()\n \n         def data():\n             yield (x, y, sample_weight)\n \n+        # Maybe build model\n+        self._maybe_symbolic_build(data_batch=(x, y, sample_weight))\n+        self.make_test_function()\n+\n         logs = self.test_function(data())\n         logs = tree.map_structure(lambda x: np.array(x), logs)\n         if return_dict:\n@@ -621,6 +631,35 @@ class TensorFlowTrainer(base_trainer.Trainer):\n             x=None, y=y, y_pred=y_pred, sample_weight=sample_weight\n         )\n \n+    def _maybe_symbolic_build(self, iterator=None, data_batch=None):\n+        # Only symbolic build when distribute strategy is created in tf trainer\n+        if self._distribute_strategy is None:\n+            # When no distribution strategy is set, defer building\n+            # to when the train/test/predict function gets traced.\n+            # This maximizes backwards compatibility.\n+            return\n+\n+        # Unlike jax/torch iterator, tf iterator returns an iterator instead\n+        # of data batch in `iterator.enumerate_epoch()`.\n+        if iterator is not None:\n+            for _, it in iterator.enumerate_epoch():\n+                maybe_distributed_data_batch = next(it)\n+                has_distributed_values = tree.map_structure(\n+                    lambda x: isinstance(x, tf.distribute.DistributedValues),\n+                    maybe_distributed_data_batch,\n+                )\n+                if all(tree.flatten(has_distributed_values)):\n+                    data_batch = self.distribute_strategy.reduce(\n+                        \"MEAN\",\n+                        maybe_distributed_data_batch,\n+                        axis=None,\n+                    )\n+                else:\n+                    data_batch = maybe_distributed_data_batch\n+                break\n+        with self.distribute_strategy.scope():\n+            self._symbolic_build(data_batch=data_batch)\n+\n \n class TFEpochIterator(EpochIterator):\n     def __init__(self, distribute_strategy=None, *args, **kwargs):\n\n@@ -1913,8 +1913,12 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n         reason=\"synchronized=True only implemented for TF backend\",\n     )\n     def test_moments_sync_with_distribution_strategy(self, dtype):\n+        from tensorflow.python.eager import context\n+\n         from keras.src.utils.module_utils import tensorflow as tf\n \n+        context._reset_context()\n+\n         # Config 2 CPUs for testing.\n         logical_cpus = tf.config.list_logical_devices(\"CPU\")\n         if len(logical_cpus) == 1:\n@@ -1944,6 +1948,8 @@ class NNOpsCorrectnessTest(testing.TestCase, parameterized.TestCase):\n             self.assertEqual(variance.values[0], 8.75)\n             self.assertEqual(variance.values[0], 8.75)\n \n+        context._reset_context()\n+\n     def test_batch_normalization(self):\n         x = np.array([[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]])\n         mean = np.array([0.2, 0.3, 0.4])\n\n@@ -1715,9 +1715,7 @@ class TrainerDistributeTest(testing.TestCase):\n         x = (np.arange(512) / 128).reshape((256, 2))\n         y = (np.arange(256) % 2).reshape((256, 1))\n         out_fit = model.fit(x, y)\n-            self.assertLess(\n-                out_fit.history[\"sparse_categorical_accuracy\"][0], 0.6\n-            )\n+        self.assertLess(out_fit.history[\"sparse_categorical_accuracy\"][0], 0.6)\n         out_eval = model.evaluate(x, y)\n         self.assertLess(out_eval[1], 0.6)\n         out_predict = model.predict(x)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
