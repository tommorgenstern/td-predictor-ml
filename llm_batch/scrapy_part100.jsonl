{"custom_id": "scrapy#0f342417b9a84ad5bc884ffbbaee093243081b04", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 108 | Files Changed: 1 | Hunks: 1 | Methods Changed: 6 | Complexity Δ (Sum/Max): -21/0 | Churn Δ: 108 | Churn Cumulative: 215 | Contributors (this commit): 3 | Commits (past 90d): 4 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -1,108 +0,0 @@\n-\"\"\"\n-This module provides a mechanism for collecting one (or more) sample items per\n-spider.\n-\n-The items are collected in a dict of guid->item and persisted by pickling that\n-dict into a file.\n-\n-This can be useful for testing changes made to the framework or other common\n-code that affects several spiders.\n-\n-It uses the scrapy stats service to keep track of which spiders are already\n-sampled.\n-\n-Settings that affect this module:\n-\n-ITEMSAMPLER_FILE\n-  file where to store the pickled dict of scraped items\n-ITEMSAMPLER_CLOSE_SPIDER\n-  wether to close the spider after enough products have been sampled\n-ITEMSAMPLER_MAX_RESPONSE_SIZE\n-  maximum response size to process\n-\"\"\"\n-\n-from __future__ import with_statement\n-\n-import cPickle as pickle\n-\n-from scrapy.xlib.pydispatch import dispatcher\n-\n-from scrapy.core.manager import scrapymanager\n-from scrapy.exceptions import NotConfigured\n-from scrapy import signals\n-from scrapy.utils.response import response_httprepr\n-from scrapy.stats import stats\n-from scrapy.http import Request\n-from scrapy import log\n-from scrapy.conf import settings\n-\n-items_per_spider = settings.getint('ITEMSAMPLER_COUNT', 1)\n-close_spider = settings.getbool('ITEMSAMPLER_CLOSE_SPIDER', False)\n-max_response_size = settings.getbool('ITEMSAMPLER_MAX_RESPONSE_SIZE', )\n-\n-class ItemSamplerPipeline(object):\n-\n-    def __init__(self):\n-        self.filename = settings['ITEMSAMPLER_FILE']\n-        if not self.filename:\n-            raise NotConfigured\n-        self.items = {}\n-        self.spiders_count = 0\n-        self.empty_spiders = set()\n-        dispatcher.connect(self.spider_closed, signal=signals.spider_closed)\n-        dispatcher.connect(self.engine_stopped, signal=signals.engine_stopped)\n-\n-    def process_item(self, spider, item):\n-        sampled = stats.get_value(\"items_sampled\", 0, spider=spider)\n-        if sampled < items_per_spider:\n-            self.items[item.guid] = item\n-            sampled += 1\n-            stats.set_value(\"items_sampled\", sampled, spider=spider)\n-            log.msg(\"Sampled %s\" % item, spider=spider, level=log.INFO)\n-            if close_spider and sampled == items_per_spider:\n-                scrapymanager.engine.close_spider(spider)\n-        return item\n-\n-    def engine_stopped(self):\n-        with open(self.filename, 'w') as f:\n-            pickle.dump(self.items, f)\n-        if self.empty_spiders:\n-            log.msg(\"No products sampled for: %s\" % \" \".join(self.empty_spiders), \\\n-                level=log.WARNING)\n-\n-    def spider_closed(self, spider, reason):\n-        if reason == 'finished' and not stats.get_value(\"items_sampled\", spider=spider):\n-            self.empty_spiders.add(spider.name)\n-        self.spiders_count += 1\n-        log.msg(\"Sampled %d spiders so far (%d empty)\" % (self.spiders_count, \\\n-            len(self.empty_spiders)), level=log.INFO)\n-\n-\n-class ItemSamplerMiddleware(object):\n-    \"\"\"This middleware drops items and requests (when spider sampling has been\n-    completed) to accelerate the processing of remaining spiders\"\"\"\n-\n-    def __init__(self):\n-        if not settings['ITEMSAMPLER_FILE']:\n-            raise NotConfigured\n-\n-    def process_spider_input(self, response, spider):\n-        if stats.get_value(\"items_sampled\", spider=spider) >= items_per_spider:\n-            return []\n-        elif max_response_size and max_response_size > len(response_httprepr(response)):  \n-            return []\n-\n-    def process_spider_output(self, response, result, spider):\n-        requests, items = [], []\n-        for r in result:\n-            if isinstance(r, Request):\n-                requests.append(r)\n-            else:\n-                items.append(r)\n-\n-        if stats.get_value(\"items_sampled\", spider=spider) >= items_per_spider:\n-            return []\n-        else:\n-            # TODO: this needs some revision, as keeping only the first item\n-            # may lead to differences when performing replays on sampled items\n-            return requests + items[0:]\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6a0ef0e41c9db8c95ff231b3ebf2e169389a4ea2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 11 | Contributors (this commit): 2 | Commits (past 90d): 1 | Contributors (cumulative): 2 | DMM Complexity: None\n\nDIFF:\n@@ -3,7 +3,6 @@ from unittest import TestCase\n from scrapy.spider import BaseSpider\n from scrapy.http import Request\n from scrapy.contrib.downloadermiddleware.useragent import UserAgentMiddleware\n-from scrapy.conf import settings\n \n \n class UserAgentMiddlewareTest(TestCase):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#68ae1da0d035a8523d5f9d6b4c509837d377fe6a", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 93 | Contributors (this commit): 3 | Commits (past 90d): 2 | Contributors (cumulative): 3 | DMM Complexity: None\n\nDIFF:\n@@ -6,7 +6,6 @@ from scrapy.contrib.downloadermiddleware.httpproxy import HttpProxyMiddleware\n from scrapy.exceptions import NotConfigured\n from scrapy.http import Response, Request\n from scrapy.spider import BaseSpider\n-from scrapy.conf import settings\n \n spider = BaseSpider('foo')\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#6d21f2f23a26c86f9a5bfc705ffc6ae9aa0516e1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 0 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 0 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 1 | Churn Cumulative: 353 | Contributors (this commit): 5 | Commits (past 90d): 10 | Contributors (cumulative): 5 | DMM Complexity: None\n\nDIFF:\n@@ -198,7 +198,6 @@ SPIDER_MIDDLEWARES = {}\n SPIDER_MIDDLEWARES_BASE = {\n     # Engine side\n     'scrapy.contrib.spidermiddleware.httperror.HttpErrorMiddleware': 50,\n-    'scrapy.contrib.itemsampler.ItemSamplerMiddleware': 100,\n     'scrapy.contrib.spidermiddleware.requestlimit.RequestLimitMiddleware': 200,\n     'scrapy.contrib.spidermiddleware.offsite.OffsiteMiddleware': 500,\n     'scrapy.contrib.spidermiddleware.referer.RefererMiddleware': 700,\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#192ec5e35b91622efbc517d1a323c0e3d766221d", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 4 | Lines Deleted: 5 | Files Changed: 1 | Hunks: 2 | Methods Changed: 2 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 9 | Churn Cumulative: 540 | Contributors (this commit): 3 | Commits (past 90d): 9 | Contributors (cumulative): 3 | DMM Complexity: 0.0\n\nDIFF:\n@@ -141,9 +141,9 @@ class Scraper(object):\n \n     def handle_spider_error(self, _failure, request, spider, propagated_failure=None):\n         referer = request.headers.get('Referer', None)\n-        msg = \"Spider exception caught while processing <%s> (referer: <%s>): %s\" % \\\n-            (request.url, referer, _failure)\n-        log.msg(msg, log.ERROR, spider=spider)\n+        msg = \"Spider exception caught while processing <%s> (referer: <%s>)\" % \\\n+            (request.url, referer)\n+        log.err(_failure, msg, spider=spider)\n         stats.inc_value(\"spider_exceptions/%s\" % _failure.value.__class__.__name__, \\\n             spider=spider)\n \n@@ -210,8 +210,7 @@ class Scraper(object):\n                 send_catch_log(signal=signals.item_dropped, \\\n                     item=item, spider=spider, exception=output.value)\n             else:\n-                log.msg('Error processing %s - %s' % (item, output), \\\n-                    log.ERROR, spider=spider)\n+                log.err(output, 'Error processing %s' % item, spider=spider)\n         else:\n             log.msg(\"Passed %s\" % item, log.INFO, spider=spider)\n             send_catch_log(signal=signals.item_passed, \\\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#ae692c05b78959ce43899526a3d14d6fdf031354", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 1 | Lines Deleted: 1 | Files Changed: 1 | Hunks: 1 | Methods Changed: 1 | Complexity Δ (Sum/Max): 0/0 | Churn Δ: 2 | Churn Cumulative: 65 | Contributors (this commit): 1 | Commits (past 90d): 2 | Contributors (cumulative): 1 | DMM Complexity: None\n\nDIFF:\n@@ -18,7 +18,7 @@ class MiddlewareManager(object):\n \n     @classmethod\n     def _get_mwlist_from_settings(cls, settings):\n-        return []\n+        raise NotImplementedError\n \n     @classmethod\n     def from_settings(cls, settings):\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#2167bfb20d186c1cc74299b01a95e96b6a785d1b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 5 | Lines Deleted: 2 | Files Changed: 1 | Hunks: 3 | Methods Changed: 2 | Complexity Δ (Sum/Max): 2/2 | Churn Δ: 7 | Churn Cumulative: 301 | Contributors (this commit): 2 | Commits (past 90d): 4 | Contributors (cumulative): 2 | DMM Complexity: 0.0\n\nDIFF:\n@@ -42,7 +42,7 @@ class ScrapyFileLogObserver(log.FileLogObserver):\n         if ev is not None:\n             log.FileLogObserver.emit(self, ev)\n \n-def _adapt_eventdict(eventDict, log_level=INFO, encoding='utf-8'):\n+def _adapt_eventdict(eventDict, log_level=INFO, encoding='utf-8', prepend_level=True):\n     \"\"\"Adapt Twisted log eventDict making it suitable for logging with a Scrapy\n     log observer. It may return None to indicate that the event should be\n     ignored by a Scrapy log observer.\n@@ -66,11 +66,14 @@ def _adapt_eventdict(eventDict, log_level=INFO, encoding='utf-8'):\n     lvlname = level_names.get(level, 'NOLEVEL')\n     if message:\n         message = [unicode_to_str(x, encoding) for x in message]\n+        if prepend_level:\n             message[0] = \"%s: %s\" % (lvlname, message[0])\n     ev['message'] = message\n     why = ev.get('why')\n     if why:\n-        why = \"%s: %s\" % (lvlname, unicode_to_str(why, encoding))\n+        why = unicode_to_str(why, encoding)\n+        if prepend_level:\n+            why = \"%s: %s\" % (lvlname, why)\n     ev['why'] = why\n     return ev\n \n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#5755bdcddc3b941eea0aaf031c89331b440bea8b", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 44 | Lines Deleted: 4 | Files Changed: 3 | Hunks: 6 | Methods Changed: 7 | Complexity Δ (Sum/Max): 11/7 | Churn Δ: 48 | Churn Cumulative: 865 | Contributors (this commit): 5 | Commits (past 90d): 16 | Contributors (cumulative): 11 | DMM Complexity: 1.0\n\nDIFF:\n@@ -4,7 +4,7 @@ extracts information from them\"\"\"\n from twisted.python.failure import Failure\n from twisted.internet import defer\n \n-from scrapy.utils.defer import defer_result, defer_succeed, parallel\n+from scrapy.utils.defer import defer_result, defer_succeed, parallel, iter_errback\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.misc import load_object\n from scrapy.utils.signal import send_catch_log\n@@ -143,7 +143,7 @@ class Scraper(object):\n \n     def handle_spider_error(self, _failure, request, spider, propagated_failure=None):\n         referer = request.headers.get('Referer', None)\n-        msg = \"Spider exception caught while processing <%s> (referer: <%s>)\" % \\\n+        msg = \"Spider error processing <%s> (referer: <%s>)\" % \\\n             (request.url, referer)\n         log.err(_failure, msg, spider=spider)\n         stats.inc_value(\"spider_exceptions/%s\" % _failure.value.__class__.__name__, \\\n@@ -152,7 +152,8 @@ class Scraper(object):\n     def handle_spider_output(self, result, request, response, spider):\n         if not result:\n             return defer_succeed(None)\n-        dfd = parallel(iter(result), self.concurrent_items,\n+        it = iter_errback(result, self.handle_spider_error, request, spider)\n+        dfd = parallel(it, self.concurrent_items,\n             self._process_spidermw_output, request, response, spider)\n         return dfd\n \n\n@@ -3,7 +3,7 @@ from twisted.internet import reactor, defer\n from twisted.python.failure import Failure\n \n from scrapy.utils.defer import mustbe_deferred, process_chain, \\\n-    process_chain_both, process_parallel\n+    process_chain_both, process_parallel, iter_errback\n \n \n class MustbeDeferredTest(unittest.TestCase):\n@@ -76,3 +76,29 @@ class DeferUtilsTest(unittest.TestCase):\n         self.failUnlessFailure(d, TypeError)\n         self.flushLoggedErrors()\n         return d\n+\n+\n+class IterErrbackTest(unittest.TestCase):\n+\n+    def test_iter_errback_good(self):\n+        def itergood():\n+            for x in xrange(10):\n+                yield x\n+\n+        errors = []\n+        out = list(iter_errback(itergood(), errors.append))\n+        self.failUnlessEqual(out, range(10))\n+        self.failIf(errors)\n+\n+    def test_iter_errback_bad(self):\n+        def iterbad():\n+            for x in xrange(10):\n+                if x == 5:\n+                    a = 1/0\n+                yield x\n+\n+        errors = []\n+        out = list(iter_errback(iterbad(), errors.append))\n+        self.failUnlessEqual(out, [0, 1, 2, 3, 4])\n+        self.failUnlessEqual(len(errors), 1)\n+        self.failUnless(isinstance(errors[0].value, ZeroDivisionError))\n\n@@ -85,3 +85,16 @@ def process_parallel(callbacks, input, *a, **kw):\n     d = defer.gatherResults(dfds)\n     d.addErrback(lambda _: _.value.subFailure)\n     return d\n+\n+def iter_errback(iterable, errback, *a, **kw):\n+    \"\"\"Wraps an iterable calling an errback if an error is caught while\n+    iterating it.\n+    \"\"\"\n+    it = iter(iterable)\n+    while 1:\n+        try:\n+            yield it.next()\n+        except StopIteration:\n+            break\n+        except:\n+            errback(failure.Failure(), *a, **kw)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#3e3a66620b278f7c03e36ebedb3591d0aaa7e8f4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 105 | Lines Deleted: 39 | Files Changed: 4 | Hunks: 28 | Methods Changed: 17 | Complexity Δ (Sum/Max): 12/7 | Churn Δ: 144 | Churn Cumulative: 2298 | Contributors (this commit): 5 | Commits (past 90d): 30 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,7 +19,7 @@ from scrapy.exceptions import IgnoreRequest, DontCloseSpider\n from scrapy.http import Response, Request\n from scrapy.spider import spiders\n from scrapy.utils.misc import load_object\n-from scrapy.utils.signal import send_catch_log\n+from scrapy.utils.signal import send_catch_log, send_catch_log_deferred\n from scrapy.utils.defer import mustbe_deferred\n \n class ExecutionEngine(object):\n@@ -44,11 +44,12 @@ class ExecutionEngine(object):\n         self.configured = True\n         self._spider_closed_callback = spider_closed_callback\n \n+    @defer.inlineCallbacks\n     def start(self):\n         \"\"\"Start the execution engine\"\"\"\n         assert not self.running, \"Engine already running\"\n         self.start_time = time()\n-        send_catch_log(signal=signals.engine_started)\n+        yield send_catch_log_deferred(signal=signals.engine_started)\n         self.running = True\n \n     def stop(self):\n@@ -218,7 +219,7 @@ class ExecutionEngine(object):\n         self.downloader.open_spider(spider)\n         yield self.scraper.open_spider(spider)\n         stats.open_spider(spider)\n-        send_catch_log(signals.spider_opened, spider=spider)\n+        yield send_catch_log_deferred(signals.spider_opened, spider=spider)\n         self.next_request(spider)\n \n     def _spider_idle(self, spider):\n@@ -268,11 +269,12 @@ class ExecutionEngine(object):\n     def _finish_closing_spider(self, spider):\n         \"\"\"This function is called after the spider has been closed\"\"\"\n         reason = self.closing.pop(spider, 'finished')\n-        send_catch_log(signal=signals.spider_closed, spider=spider, reason=reason)\n         call = self._next_request_calls.pop(spider, None)\n         if call and call.active():\n             call.cancel()\n-        dfd = defer.maybeDeferred(stats.close_spider, spider, reason=reason)\n+        dfd = send_catch_log_deferred(signal=signals.spider_closed, \\\n+            spider=spider, reason=reason)\n+        dfd.addBoth(lambda _: stats.close_spider(spider, reason=reason))\n         dfd.addErrback(log.err, \"Unhandled error in stats.close_spider()\",\n             spider=spider)\n         dfd.addBoth(lambda _: spiders.close_spider(spider))\n@@ -283,5 +285,6 @@ class ExecutionEngine(object):\n         dfd.addBoth(lambda _: self._spider_closed_callback(spider))\n         return dfd\n \n+    @defer.inlineCallbacks\n     def _finish_stopping_engine(self):\n-        send_catch_log(signal=signals.engine_stopped)\n+        yield send_catch_log_deferred(signal=signals.engine_stopped)\n\n@@ -7,7 +7,7 @@ from twisted.internet import defer\n from scrapy.utils.defer import defer_result, defer_succeed, parallel, iter_errback\n from scrapy.utils.spider import iterate_spider_output\n from scrapy.utils.misc import load_object\n-from scrapy.utils.signal import send_catch_log\n+from scrapy.utils.signal import send_catch_log, send_catch_log_deferred\n from scrapy.exceptions import IgnoreRequest, DropItem\n from scrapy import signals\n from scrapy.http import Request, Response\n@@ -171,14 +171,10 @@ class Scraper(object):\n         elif isinstance(output, BaseItem):\n             log.msg(\"Scraped %s in <%s>\" % (output, request.url), level=log.DEBUG, \\\n                 spider=spider)\n-            send_catch_log(signal=signals.item_scraped, \\\n-                item=output, spider=spider, response=response)\n             self.sites[spider].itemproc_size += 1\n-            # FIXME: this can't be called here because the stats spider may be\n-            # already closed\n-            #stats.max_value('scraper/max_itemproc_size', \\\n-            #        self.sites[spider].itemproc_size, spider=spider)\n-            dfd = self.itemproc.process_item(output, spider)\n+            dfd = send_catch_log_deferred(signal=signals.item_scraped, \\\n+                item=output, spider=spider, response=response)\n+            dfd.addBoth(lambda _: self.itemproc.process_item(output, spider))\n             dfd.addBoth(self._itemproc_finished, output, spider)\n             return dfd\n         elif output is None:\n@@ -210,12 +206,12 @@ class Scraper(object):\n             ex = output.value\n             if isinstance(ex, DropItem):\n                 log.msg(\"Dropped %s - %s\" % (item, str(ex)), level=log.WARNING, spider=spider)\n-                send_catch_log(signal=signals.item_dropped, \\\n+                return send_catch_log_deferred(signal=signals.item_dropped, \\\n                     item=item, spider=spider, exception=output.value)\n             else:\n                 log.err(output, 'Error processing %s' % item, spider=spider)\n         else:\n             log.msg(\"Passed %s\" % item, log.INFO, spider=spider)\n-            send_catch_log(signal=signals.item_passed, \\\n+            return send_catch_log_deferred(signal=signals.item_passed, \\\n                 item=item, spider=spider, output=output)\n \n\n@@ -1,45 +1,84 @@\n from twisted.trial import unittest\n from twisted.python import log as txlog\n from twisted.python.failure import Failure\n+from twisted.internet import defer, reactor\n \n from scrapy.xlib.pydispatch import dispatcher\n-from scrapy.utils.signal import send_catch_log\n+from scrapy.utils.signal import send_catch_log, send_catch_log_deferred\n from scrapy import log\n \n-test_signal = object()\n-\n-class SignalUtilsTest(unittest.TestCase):\n+class SendCatchLogTest(unittest.TestCase):\n \n+    @defer.inlineCallbacks\n     def test_send_catch_log(self):\n+        test_signal = object()\n         handlers_called = set()\n \n-        def test_handler_error(arg):\n-            handlers_called.add(test_handler_error)\n-            a = 1/0\n-\n-        def test_handler_check(arg):\n-            handlers_called.add(test_handler_check)\n-            assert arg == 'test'\n-            return \"OK\"\n-\n         def log_received(event):\n             handlers_called.add(log_received)\n-            assert \"test_handler_error\" in event['message'][0]\n+            assert \"error_handler\" in event['message'][0]\n             assert event['logLevel'] == log.ERROR\n \n         txlog.addObserver(log_received)\n-        dispatcher.connect(test_handler_error, signal=test_signal)\n-        dispatcher.connect(test_handler_check, signal=test_signal)\n-        result = send_catch_log(test_signal, arg='test')\n+        dispatcher.connect(self.error_handler, signal=test_signal)\n+        dispatcher.connect(self.ok_handler, signal=test_signal)\n+        result = yield defer.maybeDeferred(self._get_result, test_signal, arg='test', \\\n+            handlers_called=handlers_called)\n \n-        assert test_handler_error in handlers_called\n-        assert test_handler_check in handlers_called\n+        assert self.error_handler in handlers_called\n+        assert self.ok_handler in handlers_called\n         assert log_received in handlers_called\n-        self.assertEqual(result[0][0], test_handler_error)\n+        self.assertEqual(result[0][0], self.error_handler)\n         self.assert_(isinstance(result[0][1], Failure))\n-        self.assertEqual(result[1], (test_handler_check, \"OK\"))\n+        self.assertEqual(result[1], (self.ok_handler, \"OK\"))\n \n         txlog.removeObserver(log_received)\n         self.flushLoggedErrors()\n-        dispatcher.disconnect(test_handler_error, signal=test_signal)\n-        dispatcher.disconnect(test_handler_check, signal=test_signal)\n+        dispatcher.disconnect(self.error_handler, signal=test_signal)\n+        dispatcher.disconnect(self.ok_handler, signal=test_signal)\n+\n+    def _get_result(self, signal, *a, **kw):\n+        return send_catch_log(signal, *a, **kw)\n+\n+    def error_handler(self, arg, handlers_called):\n+        handlers_called.add(self.error_handler)\n+        a = 1/0\n+\n+    def ok_handler(self, arg, handlers_called):\n+        handlers_called.add(self.ok_handler)\n+        assert arg == 'test'\n+        return \"OK\"\n+\n+\n+class SendCatchLogDeferredTest(SendCatchLogTest):\n+\n+    def _get_result(self, signal, *a, **kw):\n+        return send_catch_log_deferred(signal, *a, **kw)\n+\n+\n+class SendCatchLogDeferredTest2(SendCatchLogTest):\n+\n+    def ok_handler(self, arg, handlers_called):\n+        handlers_called.add(self.ok_handler)\n+        assert arg == 'test'\n+        d = defer.Deferred()\n+        reactor.callLater(0, d.callback, \"OK\")\n+        return d\n+\n+    def _get_result(self, signal, *a, **kw):\n+        return send_catch_log_deferred(signal, *a, **kw)\n+\n+class SendCatchLogTest2(unittest.TestCase):\n+\n+    def test_error_logged_if_deferred_not_supported(self):\n+        test_signal = object()\n+        test_handler = lambda: defer.Deferred()\n+        log_events = []\n+        txlog.addObserver(log_events.append)\n+        dispatcher.connect(test_handler, test_signal)\n+        send_catch_log(test_signal)\n+        self.failUnless(log_events)\n+        self.failUnless(\"Cannot return deferreds from signal handler\" in str(log_events))\n+        txlog.removeObserver(log_events.append)\n+        self.flushLoggedErrors()\n+        dispatcher.disconnect(test_handler, test_signal)\n\n@@ -1,5 +1,6 @@\n \"\"\"Helper functinos for working with signals\"\"\"\n \n+from twisted.internet.defer import maybeDeferred, DeferredList, Deferred\n from twisted.python.failure import Failure\n \n from scrapy.xlib.pydispatch.dispatcher import Any, Anonymous, liveReceivers, \\\n@@ -19,6 +20,9 @@ def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):\n         try:\n             response = robustApply(receiver, signal=signal, sender=sender,\n                 *arguments, **named)\n+            if isinstance(response, Deferred):\n+                log.msg(\"Cannot return deferreds from signal handler: %s\" % \\\n+                    receiver, log.ERROR, spider=spider)\n         except dont_log:\n             result = Failure()\n         except Exception:\n@@ -30,6 +34,30 @@ def send_catch_log(signal=Any, sender=Anonymous, *arguments, **named):\n         responses.append((receiver, result))\n     return responses\n \n+def send_catch_log_deferred(signal=Any, sender=Anonymous, *arguments, **named):\n+    \"\"\"Like send_catch_log but supports returning deferreds on signal handlers.\n+    Returns a deferred that gets fired once all signal handlers deferreds were\n+    fired.\n+    \"\"\"\n+    def logerror(failure, recv):\n+        if dont_log is None or not isinstance(failure.value, dont_log):\n+            log.err(failure, \"Error caught on signal handler: %s\" % recv, \\\n+                spider=spider)\n+        return failure\n+\n+    dont_log = named.pop('dont_log', None)\n+    spider = named.get('spider', None)\n+    dfds = []\n+    for receiver in liveReceivers(getAllReceivers(sender, signal)):\n+        d = maybeDeferred(robustApply, receiver, signal=signal, sender=sender,\n+                *arguments, **named)\n+        d.addErrback(logerror, receiver)\n+        d.addBoth(lambda result: (receiver, result))\n+        dfds.append(d)\n+    d = DeferredList(dfds)\n+    d.addCallback(lambda out: [x[1] for x in out])\n+    return d\n+\n def disconnect_all(signal=Any, sender=Any):\n     \"\"\"Disconnect all signal handlers. Useful for cleaning up after running\n     tests\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#a28cf290081fe0c0ec422714baeceedd92fab472", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 6 | Lines Deleted: 71 | Files Changed: 2 | Hunks: 9 | Methods Changed: 3 | Complexity Δ (Sum/Max): -5/0 | Churn Δ: 77 | Churn Cumulative: 390 | Contributors (this commit): 4 | Commits (past 90d): 6 | Contributors (cumulative): 7 | DMM Complexity: 0.6774193548387096\n\nDIFF:\n@@ -4,8 +4,6 @@ Base class for Scrapy spiders\n See documentation in docs/topics/spiders.rst\n \"\"\"\n \n-import warnings\n-\n from scrapy import log\n from scrapy.http import Request\n from scrapy.utils.misc import arg_to_iter\n@@ -18,38 +16,18 @@ class BaseSpider(object_ref):\n     class.\n     \"\"\"\n \n-    # XXX: class attributes kept for backwards compatibility\n     name = None\n-    start_urls = []\n-    allowed_domains = []\n \n     def __init__(self, name=None, **kwargs):\n-        self.__dict__.update(kwargs)\n-        # XXX: SEP-12 backward compatibility (remove for 0.10)\n-        if hasattr(self, 'domain_name'):\n-            warnings.warn(\"Spider.domain_name attribute is deprecated, use Spider.name instead and Spider.allowed_domains\", \\\n-                DeprecationWarning, stacklevel=4)\n-            self.name = self.domain_name\n-            self.allowed_domains = [self.name]\n-            if hasattr(self, 'extra_domain_names'):\n-                warnings.warn(\"Spider.extra_domain_names attribute is deprecated - user Spider.allowed_domains instead\", \\\n-                    DeprecationWarning, stacklevel=4)\n-                self.allowed_domains += list(self.extra_domain_names)\n-\n         if name is not None:\n             self.name = name\n-        # XXX: create instance attributes (class attributes were kept for\n-        # backwards compatibility)\n-        if not self.start_urls:\n-            self.start_urls = []\n-        if not self.allowed_domains:\n-            self.allowed_domains = []\n-        if not self.name:\n+        elif not getattr(self, 'name', None):\n             raise ValueError(\"%s must have a name\" % type(self).__name__)\n-\n-        # XXX: SEP-12 forward compatibility (remove for 0.10)\n-        self.domain_name = self.name\n-        self.extra_domain_names = self.allowed_domains\n+        self.__dict__.update(kwargs)\n+        if not hasattr(self, 'start_urls'):\n+            self.start_urls = []\n+        if not hasattr(self, 'allowed_domains'):\n+            self.allowed_domains = []\n \n     def log(self, message, level=log.DEBUG):\n         \"\"\"Log the given messages at the given log level. Always use this\n\n@@ -1,6 +1,5 @@\n from __future__ import with_statement\n \n-import sys\n import warnings\n \n from twisted.trial import unittest\n@@ -15,54 +14,12 @@ class BaseSpiderTest(unittest.TestCase):\n \n     spider_class = BaseSpider\n \n-    class OldSpider(spider_class):\n-\n-        domain_name = 'example.com'\n-        extra_domain_names = ('example.org', 'example.net')\n-\n-\n-    class OldSpiderWithoutExtradomains(spider_class):\n-\n-        domain_name = 'example.com'\n-\n-\n-    class NewSpider(spider_class):\n-\n-        name = 'example.com'\n-        allowed_domains = ('example.org', 'example.net')\n-\n-\n     def setUp(self):\n         warnings.simplefilter(\"always\")\n \n     def tearDown(self):\n         warnings.resetwarnings()\n \n-    def test_sep12_deprecation_warnings(self):\n-        if sys.version_info[:2] < (2, 6):\n-            # warnings.catch_warnings() was added in Python 2.6\n-            raise unittest.SkipTest(\"This test requires Python 2.6+\")\n-        with warnings.catch_warnings(record=True) as w:\n-            spider = self.OldSpider()\n-            self.assertEqual(len(w), 2) # one for domain_name & one for extra_domain_names\n-            self.assert_(issubclass(w[-1].category, DeprecationWarning))\n-\n-    def test_sep12_backwards_compatibility(self):\n-        spider = self.OldSpider()\n-        self.assertEqual(spider.name, 'example.com')\n-        self.assert_('example.com' in spider.allowed_domains, spider.allowed_domains)\n-        self.assert_('example.org' in spider.allowed_domains, spider.allowed_domains)\n-        self.assert_('example.net' in spider.allowed_domains, spider.allowed_domains)\n-\n-        spider = self.OldSpiderWithoutExtradomains()\n-        self.assertEqual(spider.name, 'example.com')\n-        self.assert_('example.com' in spider.allowed_domains, spider.allowed_domains)\n-\n-        spider = self.NewSpider()\n-        self.assertEqual(spider.domain_name, 'example.com')\n-        self.assert_('example.org' in spider.extra_domain_names, spider.extra_domain_names)\n-        self.assert_('example.net' in spider.extra_domain_names, spider.extra_domain_names)\n-\n     def test_base_spider(self):\n         spider = self.spider_class(\"example.com\")\n         self.assertEqual(spider.name, 'example.com')\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#b563e56363aabe1d7fb6195151b0f5443f53e964", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 29 | Lines Deleted: 2 | Files Changed: 2 | Hunks: 3 | Methods Changed: 6 | Complexity Δ (Sum/Max): 3/3 | Churn Δ: 31 | Churn Cumulative: 251 | Contributors (this commit): 4 | Commits (past 90d): 4 | Contributors (cumulative): 7 | DMM Complexity: 1.0\n\nDIFF:\n@@ -19,7 +19,22 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertFalse(url_is_from_any_domain(url, ['testdomain.com']))\n         self.assertFalse(url_is_from_any_domain(url+'.testdomain.com', ['testdomain.com']))\n \n-    def test_url_is_from_any_domain(self):\n+    def test_url_is_from_spider(self):\n+        spider = BaseSpider(name='example.com')\n+        self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', spider))\n+        self.assertTrue(url_is_from_spider('http://sub.example.com/some/page.html', spider))\n+        self.assertFalse(url_is_from_spider('http://www.example.org/some/page.html', spider))\n+        self.assertFalse(url_is_from_spider('http://www.example.net/some/page.html', spider))\n+\n+    def test_url_is_from_spider_class_attributes(self):\n+        class MySpider(BaseSpider):\n+            name = 'example.com'\n+        self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', MySpider))\n+        self.assertTrue(url_is_from_spider('http://sub.example.com/some/page.html', MySpider))\n+        self.assertFalse(url_is_from_spider('http://www.example.org/some/page.html', MySpider))\n+        self.assertFalse(url_is_from_spider('http://www.example.net/some/page.html', MySpider))\n+\n+    def test_url_is_from_spider_with_allowed_domains(self):\n         spider = BaseSpider(name='example.com', allowed_domains=['example.org', 'example.net'])\n         self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', spider))\n         self.assertTrue(url_is_from_spider('http://sub.example.com/some/page.html', spider))\n@@ -28,6 +43,17 @@ class UrlUtilsTest(unittest.TestCase):\n         self.assertTrue(url_is_from_spider('http://www.example.net/some/page.html', spider))\n         self.assertFalse(url_is_from_spider('http://www.example.us/some/page.html', spider))\n \n+    def test_url_is_from_spider_with_allowed_domains_class_attributes(self):\n+        class MySpider(BaseSpider):\n+            name = 'example.com'\n+            allowed_domains = ['example.org', 'example.net']\n+        self.assertTrue(url_is_from_spider('http://www.example.com/some/page.html', MySpider))\n+        self.assertTrue(url_is_from_spider('http://sub.example.com/some/page.html', MySpider))\n+        self.assertTrue(url_is_from_spider('http://example.com/some/page.html', MySpider))\n+        self.assertTrue(url_is_from_spider('http://www.example.org/some/page.html', MySpider))\n+        self.assertTrue(url_is_from_spider('http://www.example.net/some/page.html', MySpider))\n+        self.assertFalse(url_is_from_spider('http://www.example.us/some/page.html', MySpider))\n+\n     def test_urljoin_rfc(self):\n         self.assertEqual(urljoin_rfc('http://example.com/some/path', 'newpath/test'),\n                                      'http://example.com/some/newpath/test')\n\n@@ -22,7 +22,8 @@ def url_is_from_any_domain(url, domains):\n \n def url_is_from_spider(url, spider):\n     \"\"\"Return True if the url belongs to the given spider\"\"\"\n-    return url_is_from_any_domain(url, [spider.name] + spider.allowed_domains)\n+    return url_is_from_any_domain(url, [spider.name] + \\\n+        getattr(spider, 'allowed_domains', []))\n \n def urljoin_rfc(base, ref, encoding='utf-8'):\n     \"\"\"Same as urlparse.urljoin but supports unicode values in base and ref\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#d17695ee4b1d74bd116a34876ddabb471b573b64", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 35 | Lines Deleted: 1 | Files Changed: 3 | Hunks: 4 | Methods Changed: 3 | Complexity Δ (Sum/Max): 13/7 | Churn Δ: 36 | Churn Cumulative: 145 | Contributors (this commit): 4 | Commits (past 90d): 9 | Contributors (cumulative): 6 | DMM Complexity: 1.0\n\nDIFF:\n@@ -12,6 +12,7 @@ from scrapy.utils.py26 import ignore_patterns, copytree\n TEMPLATES_PATH = join(scrapy.__path__[0], 'templates', 'project')\n \n TEMPLATES_TO_RENDER = (\n+    ('scrapy.cfg',),\n     ('scrapy-ctl.py',),\n     ('${project_name}', 'settings.py.tmpl'),\n     ('${project_name}', 'items.py.tmpl'),\n@@ -44,6 +45,7 @@ class Command(ScrapyCommand):\n \n         moduletpl = join(TEMPLATES_PATH, 'module')\n         copytree(moduletpl, join(project_name, project_name), ignore=IGNORE)\n+        shutil.copy(join(TEMPLATES_PATH, 'scrapy.cfg'), project_name)\n         shutil.copy(join(TEMPLATES_PATH, 'scrapy-ctl.py'), project_name)\n         for paths in TEMPLATES_TO_RENDER:\n             path = join(*paths)\n\n@@ -0,0 +1,32 @@\n+\"\"\"\n+Scrapy command-line tool\n+\"\"\"\n+\n+import sys, os\n+from ConfigParser import RawConfigParser\n+\n+def closest_scrapy_cfg(path='.', prevpath=None):\n+    if path == prevpath:\n+        return ''\n+    path = os.path.abspath(path)\n+    cfgfile = os.path.join(path, 'scrapy.cfg')\n+    if os.path.exists(cfgfile):\n+        return cfgfile\n+    return closest_scrapy_cfg(os.path.dirname(path), path)\n+\n+def main():\n+    scrapy_cfg = closest_scrapy_cfg()\n+    cfg_sources = [scrapy_cfg, os.path.expanduser('~/.scrapy.cfg'), '/etc/scrapy.cfg']\n+    cfg = RawConfigParser()\n+    cfg.read(cfg_sources)\n+    if cfg.has_option('default', 'settings'):\n+        os.environ['SCRAPY_SETTINGS_MODULE'] = cfg.get('default', 'settings')\n+        projdir = os.path.dirname(scrapy_cfg)\n+        if projdir not in sys.path:\n+            sys.path.append(projdir)\n+\n+    from scrapy.cmdline import execute\n+    execute()\n+\n+if __name__ == '__main__':\n+    main()\n\n@@ -90,7 +90,7 @@ setup_args = {\n     'packages': packages,\n     'cmdclass': cmdclasses,\n     'data_files': data_files,\n-    'scripts': ['bin/scrapy-ctl.py', 'bin/scrapy-ws.py', 'bin/scrapy-sqs.py'],\n+    'scripts': ['bin/scrapy', 'bin/scrapy-ctl.py', 'bin/scrapy-ws.py', 'bin/scrapy-sqs.py'],\n     'classifiers': [\n         'Programming Language :: Python',\n         'Programming Language :: Python :: 2.5',\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
{"custom_id": "scrapy#e741a807d2898d0ec92621acaf40d1b5a73e1ac6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "messages": [{"role": "user", "content": "You are a senior reviewer.\n\nCommit Summary:\nLines Added: 304 | Lines Deleted: 5 | Files Changed: 5 | Hunks: 5 | Methods Changed: 21 | Complexity Δ (Sum/Max): 54/40 | Churn Δ: 309 | Churn Cumulative: 732 | Contributors (this commit): 5 | Commits (past 90d): 19 | Contributors (cumulative): 9 | DMM Complexity: 1.0\n\nDIFF:\n@@ -114,6 +114,27 @@ EXTENSIONS_BASE = {\n     'scrapy.contrib.memusage.MemoryUsage': 0,\n     'scrapy.contrib.memdebug.MemoryDebugger': 0,\n     'scrapy.contrib.closespider.CloseSpider': 0,\n+    'scrapy.contrib.feedexport.FeedExporter': 0,\n+}\n+\n+FEED_URI = None\n+FEED_URI_ARGS = None # a function to extend uri arguments\n+FEED_FORMAT = 'jsonlines'\n+FEED_STORE_EMPTY = False\n+FEED_STORAGES = {}\n+FEED_STORAGES_BASE = {\n+    '': 'scrapy.contrib.feedexport.FileFeedStorage',\n+    'file': 'scrapy.contrib.feedexport.FileFeedStorage',\n+    'stdout': 'scrapy.contrib.feedexport.StdoutFeedStorage',\n+    's3': 'scrapy.contrib.feedexport.S3FeedStorage',\n+    'ftp': 'scrapy.contrib.feedexport.FTPFeedStorage',\n+}\n+FEED_EXPORTERS = {}\n+FEED_EXPORTERS_BASE = {\n+    'json': 'scrapy.contrib.exporter.JsonItemExporter',\n+    'jsonlines': 'scrapy.contrib.exporter.JsonLinesItemExporter',\n+    'csv': 'scrapy.contrib.exporter.CsvItemExporter',\n+    'xml': 'scrapy.contrib.exporter.XmlItemExporter',\n }\n \n GROUPSETTINGS_ENABLED = False\n\n@@ -0,0 +1,195 @@\n+\"\"\"\n+Feed Exports extension\n+\n+See documentation in docs/topics/feed-exports.rst\n+\"\"\"\n+\n+import sys, os, posixpath\n+from tempfile import TemporaryFile\n+from datetime import datetime\n+from urlparse import urlparse\n+from ftplib import FTP\n+from shutil import copyfileobj\n+\n+from twisted.internet import defer, threads\n+from scrapy import log, signals\n+from scrapy.xlib.pydispatch import dispatcher\n+from scrapy.utils.ftp import ftp_makedirs_cwd\n+from scrapy.exceptions import NotConfigured\n+from scrapy.utils.misc import load_object\n+from scrapy.conf import settings\n+\n+class BlockingFeedStorage(object):\n+\n+    def store(self, file, spider):\n+        return threads.deferToThread(self._store_in_thread, file, spider)\n+\n+    def _store_in_thread(self, file, spider):\n+        raise NotImplementedError\n+\n+\n+class StdoutFeedStorage(object):\n+\n+    def __init__(self, uri):\n+        pass\n+\n+    def store(self, file, spider):\n+        copyfileobj(file, sys.stdout)\n+\n+\n+class FileFeedStorage(BlockingFeedStorage):\n+\n+    def __init__(self, uri):\n+        u = urlparse(uri)\n+        self.path = u.path\n+\n+    def _store_in_thread(self, file, spider):\n+        dirname = os.path.dirname(self.path)\n+        if dirname and not os.path.exists(dirname):\n+            os.makedirs(dirname)\n+        f = open(self.path, 'wb')\n+        copyfileobj(file, f)\n+        f.close()\n+\n+\n+class S3FeedStorage(BlockingFeedStorage):\n+\n+    def __init__(self, uri):\n+        try:\n+            import boto\n+        except ImportError:\n+            raise NotConfigured\n+        self.connect_s3 = boto.connect_s3\n+        u = urlparse(uri)\n+        self.bucketname = u.hostname\n+        self.access_key = u.username or settings['AWS_ACCESS_KEY_ID']\n+        self.secret_key = u.password or settings['AWS_SECRET_ACCESS_KEY']\n+        self.keyname = u.path\n+\n+    def _store_in_thread(self, file, spider):\n+        conn = self.connect_s3(self.access_key, self.secret_key)\n+        bucket = conn.get_bucket(self.bucketname, validate=False)\n+        key = bucket.new_key(self.keyname)\n+        key.set_contents_from_file(file)\n+        key.close()\n+\n+\n+class FTPFeedStorage(BlockingFeedStorage):\n+\n+    def __init__(self, uri):\n+        u = urlparse(uri)\n+        self.host = u.hostname\n+        self.port = int(u.port or '21')\n+        self.username = u.username\n+        self.password = u.password\n+        self.path = u.path\n+\n+    def _store_in_thread(self, file, spider):\n+        ftp = FTP()\n+        ftp.connect(self.host, self.port)\n+        ftp.login(self.username, self.password)\n+        dirname, filename = posixpath.split(self.path)\n+        ftp_makedirs_cwd(ftp, dirname)\n+        ftp.storbinary('STOR %s' % filename, file)\n+        ftp.quit()\n+\n+\n+class SpiderSlot(object):\n+    def __init__(self, file, exp):\n+        self.file = file\n+        self.exporter = exp\n+        self.itemcount = 0\n+\n+class FeedExporter(object):\n+\n+    def __init__(self):\n+        self.urifmt = settings['FEED_URI']\n+        if not self.urifmt:\n+            raise NotConfigured\n+        self.format = settings['FEED_FORMAT'].lower()\n+        self.storages = self._load_components('FEED_STORAGES')\n+        self.exporters = self._load_components('FEED_EXPORTERS')\n+        if not self._storage_supported(self.urifmt):\n+            raise NotConfigured\n+        if not self._exporter_supported(self.format):\n+            raise NotConfigured\n+        self.store_empty = settings.getbool('FEED_STORE_EMPTY')\n+        uripar = settings['FEED_URI_PARAMS']\n+        self._uripar = load_object(uripar) if uripar else lambda x, y: None\n+        self.slots = {}\n+        dispatcher.connect(self.open_spider, signals.spider_opened)\n+        dispatcher.connect(self.close_spider, signals.spider_closed)\n+        dispatcher.connect(self.item_passed, signals.item_passed)\n+\n+    def open_spider(self, spider):\n+        file = TemporaryFile(prefix='feed-')\n+        exp = self._get_exporter(file)\n+        exp.start_exporting()\n+        self.slots[spider] = SpiderSlot(file, exp)\n+\n+    def close_spider(self, spider):\n+        slot = self.slots.pop(spider)\n+        if not slot.itemcount and not self.store_empty:\n+            return\n+        slot.exporter.finish_exporting()\n+        nbytes = slot.file.tell()\n+        slot.file.seek(0)\n+        uri = self.urifmt % self._get_uri_params(spider)\n+        storage = self._get_storage(uri)\n+        if not storage:\n+            return\n+        logfmt = \"%%s %s feed (%d items, %d bytes) in: %s\" % (self.format, \\\n+            slot.itemcount, nbytes, uri)\n+        d = defer.maybeDeferred(storage.store, slot.file, spider)\n+        d.addCallback(lambda _: log.msg(logfmt % \"Stored\", spider=spider))\n+        d.addErrback(log.err, logfmt % \"Error storing\", spider=spider)\n+        d.addBoth(lambda _: slot.file.close())\n+        return d\n+\n+    def item_passed(self, item, spider):\n+        slot = self.slots[spider]\n+        slot.exporter.export_item(item)\n+        slot.itemcount += 1\n+        return item\n+\n+    def _load_components(self, setting_prefix):\n+        conf = dict(settings['%s_BASE' % setting_prefix])\n+        conf.update(settings[setting_prefix])\n+        d = {}\n+        for k, v in conf.items():\n+            try:\n+                d[k] = load_object(v)\n+            except NotConfigured:\n+                pass\n+        return d\n+\n+    def _exporter_supported(self, format):\n+        if format in self.exporters:\n+            return True\n+        log.msg(\"Unknown feed format: %s\" % format, log.ERROR)\n+\n+    def _storage_supported(self, uri):\n+        scheme = urlparse(uri).scheme\n+        if scheme in self.storages:\n+            try:\n+                self._get_storage(uri)\n+                return True\n+            except NotConfigured:\n+                log.msg(\"Disabled feed storage scheme: %s\" % scheme, log.ERROR)\n+        else:\n+            log.msg(\"Unknown feed storage scheme: %s\" % scheme, log.ERROR)\n+\n+    def _get_exporter(self, *a, **kw):\n+        return self.exporters[self.format](*a, **kw)\n+\n+    def _get_storage(self, uri):\n+        return self.storages[urlparse(uri).scheme](uri)\n+\n+    def _get_uri_params(self, spider):\n+        params = {}\n+        for k in dir(spider):\n+            params[k] = getattr(spider, k)\n+        ts = datetime.utcnow().replace(microsecond=0).isoformat().replace(':', '-')\n+        params['time'] = ts\n+        self._uripar(params, spider)\n+        return params\n\n@@ -1,8 +1,6 @@\n-\"\"\"\n-File Export Pipeline\n-\n-See documentation in docs/topics/item-pipeline.rst\n-\"\"\"\n+import warnings\n+warnings.warn(\"File export pipeline is deprecated and will be removed in Scrapy 0.11, use Feed exports instead\", \\\n+    DeprecationWarning, stacklevel=2)\n \n from scrapy.xlib.pydispatch import dispatcher\n from scrapy import signals\n\n@@ -0,0 +1,70 @@\n+import os, urlparse\n+\n+from twisted.trial import unittest\n+from twisted.internet import defer\n+from cStringIO import StringIO\n+\n+from scrapy.spider import BaseSpider\n+from scrapy.contrib.feedexport import FileFeedStorage, FTPFeedStorage, S3FeedStorage\n+\n+class FeedStorageTest(unittest.TestCase):\n+\n+    @defer.inlineCallbacks\n+    def _assert_stores(self, storage, path):\n+        yield storage.store(StringIO(\"content\"), BaseSpider(\"default\"))\n+        self.failUnless(os.path.exists(path))\n+        self.failUnlessEqual(open(path).read(), \"content\")\n+        # again, to check files are overwritten properly\n+        yield storage.store(StringIO(\"new content\"), BaseSpider(\"default\"))\n+        self.failUnlessEqual(open(path).read(), \"new content\")\n+\n+\n+class FileFeedStorageTest(FeedStorageTest):\n+\n+    def test_store_file_uri(self):\n+        path = os.path.abspath(self.mktemp())\n+        uri = \"file://%s\" % path\n+        return self._assert_stores(FileFeedStorage(uri), path)\n+\n+    def test_store_file_uri_makedirs(self):\n+        path = os.path.abspath(self.mktemp())\n+        path = os.path.join(path, 'more', 'paths', 'file.txt')\n+        uri = \"file://%s\" % path\n+        return self._assert_stores(FileFeedStorage(uri), path)\n+\n+    def test_store_direct_path(self):\n+        path = os.path.abspath(self.mktemp())\n+        return self._assert_stores(FileFeedStorage(path), path)\n+\n+    def test_store_direct_path_relative(self):\n+        path = self.mktemp()\n+        return self._assert_stores(FileFeedStorage(path), path)\n+\n+\n+class FTPFeedStorageTest(FeedStorageTest):\n+\n+    def test_store(self):\n+        uri = os.environ.get('FEEDTEST_FTP_URI')\n+        path = os.environ.get('FEEDTEST_FTP_PATH')\n+        if not (uri and path):\n+            raise unittest.SkipTest(\"No FTP server available for testing\")\n+        return self._assert_stores(FTPFeedStorage(uri), path)\n+\n+\n+class S3FeedStorageTest(unittest.TestCase):\n+\n+    @defer.inlineCallbacks\n+    def test_store(self):\n+        uri = os.environ.get('FEEDTEST_S3_URI')\n+        if not uri:\n+            raise unittest.SkipTest(\"No S3 bucket available for testing\")\n+        try:\n+            from boto import connect_s3\n+        except ImportError:\n+            raise unittest.SkipTest(\"Missing library: boto\")\n+        storage = S3FeedStorage(uri)\n+        yield storage.store(StringIO(\"content\"), BaseSpider(\"default\"))\n+        u = urlparse.urlparse(uri)\n+        key = connect_s3().get_bucket(u.hostname, validate=False).get_key(u.path)\n+        self.failUnlessEqual(key.get_contents_as_string(), \"content\")\n+\n\n@@ -0,0 +1,15 @@\n+from ftplib import error_perm\n+from posixpath import dirname\n+\n+def ftp_makedirs_cwd(ftp, path, first_call=True):\n+    \"\"\"Set the current directory of the FTP connection given in the `ftp`\n+    argument (as a ftplib.FTP object), creating all parent directories if they\n+    don't exist. The ftplib.FTP object must be already connected and logged in.\n+    \"\"\"\n+    try:\n+        ftp.cwd(path)\n+    except error_perm:\n+        ftp_makedirs_cwd(ftp, dirname(path), False)\n+        ftp.mkd(path)\n+        if first_call:\n+            ftp.cwd(path)\n\n\nQuestion: Does this commit introduce technical debt? Answer yes or no."}], "max_tokens": 1, "temperature": 0}}
